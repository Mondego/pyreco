__FILENAME__ = autoProcessTV
#!/usr/bin/env python

# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import os.path
import sys

# Try importing Python 2 modules using new names
try:
    import ConfigParser as configparser
    import urllib2
    from urllib import urlencode

# On error import Python 3 modules
except ImportError:
    import configparser
    import urllib.request as urllib2
    from urllib.parse import urlencode

# workaround for broken urllib2 in python 2.6.5: wrong credentials lead to an infinite recursion
if sys.version_info >= (2, 6, 5) and sys.version_info < (2, 6, 6):
    class HTTPBasicAuthHandler(urllib2.HTTPBasicAuthHandler):
        def retry_http_basic_auth(self, host, req, realm):
            # don't retry if auth failed
            if req.get_header(self.auth_header, None) is not None:
                return None

            return urllib2.HTTPBasicAuthHandler.retry_http_basic_auth(self, host, req, realm)

else:
    HTTPBasicAuthHandler = urllib2.HTTPBasicAuthHandler


def processEpisode(dir_to_process, org_NZB_name=None):

    # Default values
    host = "localhost"
    port = "8081"
    username = ""
    password = ""
    ssl = 0
    web_root = "/"

    default_url = host + ":" + port + web_root
    if ssl:
        default_url = "https://" + default_url
    else:
        default_url = "http://" + default_url

    # Get values from config_file
    config = configparser.RawConfigParser()
    config_filename = os.path.join(os.path.dirname(sys.argv[0]), "autoProcessTV.cfg")

    if not os.path.isfile(config_filename):
        print ("ERROR: " + config_filename + " doesn\'t exist")
        print ("copy /rename " + config_filename + ".sample and edit\n")
        print ("Trying default url: " + default_url + "\n")

    else:
        try:
            print ("Loading config from " + config_filename + "\n")

            with open(config_filename, "r") as fp:
                config.readfp(fp)

            # Replace default values with config_file values
            host = config.get("SickBeard", "host")
            port = config.get("SickBeard", "port")
            username = config.get("SickBeard", "username")
            password = config.get("SickBeard", "password")

            try:
                ssl = int(config.get("SickBeard", "ssl"))

            except (configparser.NoOptionError, ValueError):
                pass

            try:
                web_root = config.get("SickBeard", "web_root")
                if not web_root.startswith("/"):
                    web_root = "/" + web_root

                if not web_root.endswith("/"):
                    web_root = web_root + "/"

            except configparser.NoOptionError:
                pass

        except EnvironmentError:
            e = sys.exc_info()[1]
            print ("Could not read configuration file: " + str(e))
            # There was a config_file, don't use default values but exit
            sys.exit(1)

    params = {}

    params['quiet'] = 1

    params['dir'] = dir_to_process
    if org_NZB_name != None:
        params['nzbName'] = org_NZB_name

    if ssl:
        protocol = "https://"
    else:
        protocol = "http://"

    url = protocol + host + ":" + port + web_root + "home/postprocess/processEpisode?" + urlencode(params)

    print ("Opening URL: " + url)

    try:
        password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()
        password_mgr.add_password(None, url, username, password)
        handler = HTTPBasicAuthHandler(password_mgr)
        opener = urllib2.build_opener(handler)
        urllib2.install_opener(opener)

        result = opener.open(url).readlines()

        for line in result:
            if line:
                print (line.strip())

    except IOError:
        e = sys.exc_info()[1]
        print ("Unable to open URL: " + str(e))
        sys.exit(1)


if __name__ == "__main__":
    print ("This module is supposed to be used as import in other scripts and not run standalone.")
    print ("Use sabToSickBeard instead.")
    sys.exit(1)

########NEW FILE########
__FILENAME__ = hellaToSickBeard
#!/usr/bin/env python

# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.


import sys

try:
    import autoProcessTV
except:
    print ("Can't import autoProcessTV.py, make sure it's in the same folder as " + sys.argv[0])
    sys.exit(1)

if len(sys.argv) < 4:
    print ("No folder supplied - is this being called from HellaVCR?")
    sys.exit(1)
else:
    autoProcessTV.processEpisode(sys.argv[3], sys.argv[2])

########NEW FILE########
__FILENAME__ = sabToSickBeard
#!/usr/bin/env python

# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.


import sys

try:
    import autoProcessTV
except:
    print ("Can't import autoProcessTV.py, make sure it's in the same folder as " + sys.argv[0])
    sys.exit(1)

# SABnzbd user script parameters - see: http://wiki.sabnzbd.org/user-scripts

# 0  sys.argv[0] is the name of this script

# 1  The final directory of the job (full path)
if len(sys.argv) < 2:
    print ("No folder supplied - is this being called from SABnzbd?")
    sys.exit(1)
else:
    download_final_dir = sys.argv[1]

# 2  The original name of the NZB file
org_NZB_name = sys.argv[2] if len(sys.argv) > 3 else None

# 3  Clean version of the job name (no path info and ".nzb" removed)
clean_NZB_file = sys.argv[3] if len(sys.argv) > 4 else None

# 4  Indexer's report number (if supported)
indexer_report = sys.argv[4] if len(sys.argv) > 5 else None

# 5  User-defined category
sab_user_category = sys.argv[5] if len(sys.argv) > 6 else None

# 6  Group that the NZB was posted in e.g. alt.binaries.x
group_NZB = sys.argv[6] if len(sys.argv) > 7 else None

# 7  Status of post processing. 0 = OK, 1=failed verification, 2=failed unpack, 3=1+2
sab_post_processing_status = sys.argv[7] if len(sys.argv) > 8 else None

# Only final_dir and org_NZB_name are being used to process episodes
autoProcessTV.processEpisode(download_final_dir, org_NZB_name)

########NEW FILE########
__FILENAME__ = auth
import cherrypy
from cherrypy.lib import httpauth


def check_auth(users, encrypt=None, realm=None):
    """If an authorization header contains credentials, return True, else False."""
    request = cherrypy.serving.request
    if 'authorization' in request.headers:
        # make sure the provided credentials are correctly set
        ah = httpauth.parseAuthorization(request.headers['authorization'])
        if ah is None:
            raise cherrypy.HTTPError(400, 'Bad Request')
        
        if not encrypt:
            encrypt = httpauth.DIGEST_AUTH_ENCODERS[httpauth.MD5]
        
        if hasattr(users, '__call__'):
            try:
                # backward compatibility
                users = users() # expect it to return a dictionary
                
                if not isinstance(users, dict):
                    raise ValueError("Authentication users must be a dictionary")
                
                # fetch the user password
                password = users.get(ah["username"], None)
            except TypeError:
                # returns a password (encrypted or clear text)
                password = users(ah["username"])
        else:
            if not isinstance(users, dict):
                raise ValueError("Authentication users must be a dictionary")
            
            # fetch the user password
            password = users.get(ah["username"], None)
        
        # validate the authorization by re-computing it here
        # and compare it with what the user-agent provided
        if httpauth.checkResponse(ah, password, method=request.method,
                                  encrypt=encrypt, realm=realm):
            request.login = ah["username"]
            return True
        
        request.login = False
    return False

def basic_auth(realm, users, encrypt=None, debug=False):
    """If auth fails, raise 401 with a basic authentication header.
    
    realm: a string containing the authentication realm.
    users: a dict of the form: {username: password} or a callable returning a dict.
    encrypt: callable used to encrypt the password returned from the user-agent.
             if None it defaults to a md5 encryption.
    """
    if check_auth(users, encrypt):
        if debug:
            cherrypy.log('Auth successful', 'TOOLS.BASIC_AUTH')
        return
    
    # inform the user-agent this path is protected
    cherrypy.serving.response.headers['www-authenticate'] = httpauth.basicAuth(realm)
    
    raise cherrypy.HTTPError(401, "You are not authorized to access that resource")

def digest_auth(realm, users, debug=False):
    """If auth fails, raise 401 with a digest authentication header.
    
    realm: a string containing the authentication realm.
    users: a dict of the form: {username: password} or a callable returning a dict.
    """
    if check_auth(users, realm=realm):
        if debug:
            cherrypy.log('Auth successful', 'TOOLS.DIGEST_AUTH')
        return
    
    # inform the user-agent this path is protected
    cherrypy.serving.response.headers['www-authenticate'] = httpauth.digestAuth(realm)
    
    raise cherrypy.HTTPError(401, "You are not authorized to access that resource")

########NEW FILE########
__FILENAME__ = auth_basic
# This file is part of CherryPy <http://www.cherrypy.org/>
# -*- coding: utf-8 -*-
# vim:ts=4:sw=4:expandtab:fileencoding=utf-8

__doc__ = """Module auth_basic.py provides a CherryPy 3.x tool which implements
the server-side of HTTP Basic Access Authentication, as described in RFC 2617.

Example usage, using the built-in checkpassword_dict function which uses a dict
as the credentials store:

userpassdict = {'bird' : 'bebop', 'ornette' : 'wayout'}
checkpassword = cherrypy.lib.auth_basic.checkpassword_dict(userpassdict)
basic_auth = {'tools.auth_basic.on': True,
              'tools.auth_basic.realm': 'earth',
              'tools.auth_basic.checkpassword': checkpassword,
}
app_config = { '/' : basic_auth }
"""

__author__ = 'visteya'
__date__ = 'April 2009'

import binascii
import base64
import cherrypy


def checkpassword_dict(user_password_dict):
    """Returns a checkpassword function which checks credentials
    against a dictionary of the form: {username : password}.

    If you want a simple dictionary-based authentication scheme, use
    checkpassword_dict(my_credentials_dict) as the value for the
    checkpassword argument to basic_auth().
    """
    def checkpassword(realm, user, password):
        p = user_password_dict.get(user)
        return p and p == password or False

    return checkpassword


def basic_auth(realm, checkpassword, debug=False):
    """basic_auth is a CherryPy tool which hooks at before_handler to perform
    HTTP Basic Access Authentication, as specified in RFC 2617.

    If the request has an 'authorization' header with a 'Basic' scheme, this
    tool attempts to authenticate the credentials supplied in that header.  If
    the request has no 'authorization' header, or if it does but the scheme is
    not 'Basic', or if authentication fails, the tool sends a 401 response with
    a 'WWW-Authenticate' Basic header.

    Arguments:
    realm: a string containing the authentication realm.

    checkpassword: a callable which checks the authentication credentials.
        Its signature is checkpassword(realm, username, password). where
        username and password are the values obtained from the request's
        'authorization' header.  If authentication succeeds, checkpassword
        returns True, else it returns False.
    """
    
    if '"' in realm:
        raise ValueError('Realm cannot contain the " (quote) character.')
    request = cherrypy.serving.request
    
    auth_header = request.headers.get('authorization')
    if auth_header is not None:
        try:
            scheme, params = auth_header.split(' ', 1)
            if scheme.lower() == 'basic':
                # since CherryPy claims compability with Python 2.3, we must use
                # the legacy API of base64
                username_password = base64.decodestring(params)
                username, password = username_password.split(':', 1)
                if checkpassword(realm, username, password):
                    if debug:
                        cherrypy.log('Auth succeeded', 'TOOLS.AUTH_BASIC')
                    request.login = username
                    return # successful authentication
        except (ValueError, binascii.Error): # split() error, base64.decodestring() error
            raise cherrypy.HTTPError(400, 'Bad Request')
    
    # Respond with 401 status and a WWW-Authenticate header
    cherrypy.serving.response.headers['www-authenticate'] = 'Basic realm="%s"' % realm
    raise cherrypy.HTTPError(401, "You are not authorized to access that resource")


########NEW FILE########
__FILENAME__ = auth_digest
# This file is part of CherryPy <http://www.cherrypy.org/>
# -*- coding: utf-8 -*-
# vim:ts=4:sw=4:expandtab:fileencoding=utf-8

__doc__ = """An implementation of the server-side of HTTP Digest Access
Authentication, which is described in RFC 2617.

Example usage, using the built-in get_ha1_dict_plain function which uses a dict
of plaintext passwords as the credentials store:

userpassdict = {'alice' : '4x5istwelve'}
get_ha1 = cherrypy.lib.auth_digest.get_ha1_dict_plain(userpassdict)
digest_auth = {'tools.auth_digest.on': True,
               'tools.auth_digest.realm': 'wonderland',
               'tools.auth_digest.get_ha1': get_ha1,
               'tools.auth_digest.key': 'a565c27146791cfb',
}
app_config = { '/' : digest_auth }
"""

__author__ = 'visteya'
__date__ = 'April 2009'


try:
    from hashlib import md5
except ImportError:
    # Python 2.4 and earlier
    from md5 import new as md5
md5_hex = lambda s: md5(s).hexdigest()

import time
import base64
from urllib2 import parse_http_list, parse_keqv_list

import cherrypy

qop_auth = 'auth'
qop_auth_int = 'auth-int'
valid_qops = (qop_auth, qop_auth_int)

valid_algorithms = ('MD5', 'MD5-sess')


def TRACE(msg):
    cherrypy.log(msg, context='TOOLS.AUTH_DIGEST')

# Three helper functions for users of the tool, providing three variants
# of get_ha1() functions for three different kinds of credential stores.
def get_ha1_dict_plain(user_password_dict):
    """Returns a get_ha1 function which obtains a plaintext password from a
    dictionary of the form: {username : password}.

    If you want a simple dictionary-based authentication scheme, with plaintext
    passwords, use get_ha1_dict_plain(my_userpass_dict) as the value for the
    get_ha1 argument to digest_auth().
    """
    def get_ha1(realm, username):
        password = user_password_dict.get(username)
        if password:
            return md5_hex('%s:%s:%s' % (username, realm, password))
        return None

    return get_ha1

def get_ha1_dict(user_ha1_dict):
    """Returns a get_ha1 function which obtains a HA1 password hash from a
    dictionary of the form: {username : HA1}.

    If you want a dictionary-based authentication scheme, but with
    pre-computed HA1 hashes instead of plain-text passwords, use
    get_ha1_dict(my_userha1_dict) as the value for the get_ha1
    argument to digest_auth().
    """
    def get_ha1(realm, username):
        return user_ha1_dict.get(user)

    return get_ha1

def get_ha1_file_htdigest(filename):
    """Returns a get_ha1 function which obtains a HA1 password hash from a
    flat file with lines of the same format as that produced by the Apache
    htdigest utility. For example, for realm 'wonderland', username 'alice',
    and password '4x5istwelve', the htdigest line would be:

    alice:wonderland:3238cdfe91a8b2ed8e39646921a02d4c

    If you want to use an Apache htdigest file as the credentials store,
    then use get_ha1_file_htdigest(my_htdigest_file) as the value for the
    get_ha1 argument to digest_auth().  It is recommended that the filename
    argument be an absolute path, to avoid problems.
    """
    def get_ha1(realm, username):
        result = None
        f = open(filename, 'r')
        for line in f:
            u, r, ha1 = line.rstrip().split(':')
            if u == username and r == realm:
                result = ha1
                break
        f.close()
        return result

    return get_ha1


def synthesize_nonce(s, key, timestamp=None):
    """Synthesize a nonce value which resists spoofing and can be checked for staleness.
    Returns a string suitable as the value for 'nonce' in the www-authenticate header.

    Args:
    s: a string related to the resource, such as the hostname of the server.
    key: a secret string known only to the server.
    timestamp: an integer seconds-since-the-epoch timestamp
    """
    if timestamp is None:
        timestamp = int(time.time())
    h = md5_hex('%s:%s:%s' % (timestamp, s, key))
    nonce = '%s:%s' % (timestamp, h)
    return nonce


def H(s):
    """The hash function H"""
    return md5_hex(s)


class HttpDigestAuthorization (object):
    """Class to parse a Digest Authorization header and perform re-calculation
    of the digest.
    """

    def errmsg(self, s):
        return 'Digest Authorization header: %s' % s

    def __init__(self, auth_header, http_method, debug=False):
        self.http_method = http_method
        self.debug = debug
        scheme, params = auth_header.split(" ", 1)
        self.scheme = scheme.lower()
        if self.scheme != 'digest':
            raise ValueError('Authorization scheme is not "Digest"')

        self.auth_header = auth_header

        # make a dict of the params
        items = parse_http_list(params)
        paramsd = parse_keqv_list(items)

        self.realm = paramsd.get('realm')
        self.username = paramsd.get('username')
        self.nonce = paramsd.get('nonce')
        self.uri = paramsd.get('uri')
        self.method = paramsd.get('method')
        self.response = paramsd.get('response') # the response digest
        self.algorithm = paramsd.get('algorithm', 'MD5')
        self.cnonce = paramsd.get('cnonce')
        self.opaque = paramsd.get('opaque')
        self.qop = paramsd.get('qop') # qop
        self.nc = paramsd.get('nc') # nonce count

        # perform some correctness checks
        if self.algorithm not in valid_algorithms:
            raise ValueError(self.errmsg("Unsupported value for algorithm: '%s'" % self.algorithm))

        has_reqd = self.username and \
                   self.realm and \
                   self.nonce and \
                   self.uri and \
                   self.response
        if not has_reqd:
            raise ValueError(self.errmsg("Not all required parameters are present."))

        if self.qop:
            if self.qop not in valid_qops:
                raise ValueError(self.errmsg("Unsupported value for qop: '%s'" % self.qop))
            if not (self.cnonce and self.nc):
                raise ValueError(self.errmsg("If qop is sent then cnonce and nc MUST be present"))
        else:
            if self.cnonce or self.nc:
                raise ValueError(self.errmsg("If qop is not sent, neither cnonce nor nc can be present"))


    def __str__(self):
        return 'authorization : %s' % self.auth_header

    def validate_nonce(self, s, key):
        """Validate the nonce.
        Returns True if nonce was generated by synthesize_nonce() and the timestamp
        is not spoofed, else returns False.

        Args:
        s: a string related to the resource, such as the hostname of the server.
        key: a secret string known only to the server.
        Both s and key must be the same values which were used to synthesize the nonce
        we are trying to validate.
        """
        try:
            timestamp, hashpart = self.nonce.split(':', 1)
            s_timestamp, s_hashpart = synthesize_nonce(s, key, timestamp).split(':', 1)
            is_valid = s_hashpart == hashpart
            if self.debug:
                TRACE('validate_nonce: %s' % is_valid)
            return is_valid
        except ValueError: # split() error
            pass
        return False


    def is_nonce_stale(self, max_age_seconds=600):
        """Returns True if a validated nonce is stale.  The nonce contains a
        timestamp in plaintext and also a secure hash of the timestamp.  You should
        first validate the nonce to ensure the plaintext timestamp is not spoofed.
        """
        try:
            timestamp, hashpart = self.nonce.split(':', 1)
            if int(timestamp) + max_age_seconds > int(time.time()):
                return False
        except ValueError: # int() error
            pass
        if self.debug:
            TRACE("nonce is stale")
        return True


    def HA2(self, entity_body=''):
        """Returns the H(A2) string.  See RFC 2617 3.2.2.3."""
        # RFC 2617 3.2.2.3
        # If the "qop" directive's value is "auth" or is unspecified, then A2 is:
        #    A2 = method ":" digest-uri-value
        #
        # If the "qop" value is "auth-int", then A2 is:
        #    A2 = method ":" digest-uri-value ":" H(entity-body)
        if self.qop is None or self.qop == "auth":
            a2 = '%s:%s' % (self.http_method, self.uri)
        elif self.qop == "auth-int":
            a2 = "%s:%s:%s" % (self.http_method, self.uri, H(entity_body))
        else:
            # in theory, this should never happen, since I validate qop in __init__()
            raise ValueError(self.errmsg("Unrecognized value for qop!"))
        return H(a2)


    def request_digest(self, ha1, entity_body=''):
        """Calculates the Request-Digest.  See RFC 2617 3.2.2.1.
        Arguments:

        ha1 : the HA1 string obtained from the credentials store.

        entity_body : if 'qop' is set to 'auth-int', then A2 includes a hash
            of the "entity body".  The entity body is the part of the
            message which follows the HTTP headers.  See RFC 2617 section
            4.3.  This refers to the entity the user agent sent in the request which
            has the Authorization header.  Typically GET requests don't have an entity,
            and POST requests do.
        """
        ha2 = self.HA2(entity_body)
        # Request-Digest -- RFC 2617 3.2.2.1
        if self.qop:
            req = "%s:%s:%s:%s:%s" % (self.nonce, self.nc, self.cnonce, self.qop, ha2)
        else:
            req = "%s:%s" % (self.nonce, ha2)

        # RFC 2617 3.2.2.2
        #
        # If the "algorithm" directive's value is "MD5" or is unspecified, then A1 is:
        # A1 = unq(username-value) ":" unq(realm-value) ":" passwd
        #
        # If the "algorithm" directive's value is "MD5-sess", then A1 is
        # calculated only once - on the first request by the client following
        # receipt of a WWW-Authenticate challenge from the server.
        # A1 = H( unq(username-value) ":" unq(realm-value) ":" passwd )
        #         ":" unq(nonce-value) ":" unq(cnonce-value)
        if self.algorithm == 'MD5-sess':
            ha1 = H('%s:%s:%s' % (ha1, self.nonce, self.cnonce))

        digest = H('%s:%s' % (ha1, req))
        return digest



def www_authenticate(realm, key, algorithm='MD5', nonce=None, qop=qop_auth, stale=False):
    """Constructs a WWW-Authenticate header for Digest authentication."""
    if qop not in valid_qops:
        raise ValueError("Unsupported value for qop: '%s'" % qop)
    if algorithm not in valid_algorithms:
        raise ValueError("Unsupported value for algorithm: '%s'" % algorithm)

    if nonce is None:
        nonce = synthesize_nonce(realm, key)
    s = 'Digest realm="%s", nonce="%s", algorithm="%s", qop="%s"' % (
                realm, nonce, algorithm, qop)
    if stale:
        s += ', stale="true"'
    return s


def digest_auth(realm, get_ha1, key, debug=False):
    """digest_auth is a CherryPy tool which hooks at before_handler to perform
    HTTP Digest Access Authentication, as specified in RFC 2617.
    
    If the request has an 'authorization' header with a 'Digest' scheme, this
    tool authenticates the credentials supplied in that header.  If
    the request has no 'authorization' header, or if it does but the scheme is
    not "Digest", or if authentication fails, the tool sends a 401 response with
    a 'WWW-Authenticate' Digest header.
    
    Arguments:
    realm: a string containing the authentication realm.
    
    get_ha1: a callable which looks up a username in a credentials store
        and returns the HA1 string, which is defined in the RFC to be
        MD5(username : realm : password).  The function's signature is:
            get_ha1(realm, username)
        where username is obtained from the request's 'authorization' header.
        If username is not found in the credentials store, get_ha1() returns
        None.
    
    key: a secret string known only to the server, used in the synthesis of nonces.
    """
    request = cherrypy.serving.request
    
    auth_header = request.headers.get('authorization')
    nonce_is_stale = False
    if auth_header is not None:
        try:
            auth = HttpDigestAuthorization(auth_header, request.method, debug=debug)
        except ValueError, e:
            raise cherrypy.HTTPError(400, 'Bad Request: %s' % e)
        
        if debug:
            TRACE(str(auth))
        
        if auth.validate_nonce(realm, key):
            ha1 = get_ha1(realm, auth.username)
            if ha1 is not None:
                # note that for request.body to be available we need to hook in at
                # before_handler, not on_start_resource like 3.1.x digest_auth does.
                digest = auth.request_digest(ha1, entity_body=request.body)
                if digest == auth.response: # authenticated
                    if debug:
                        TRACE("digest matches auth.response")
                    # Now check if nonce is stale.
                    # The choice of ten minutes' lifetime for nonce is somewhat arbitrary
                    nonce_is_stale = auth.is_nonce_stale(max_age_seconds=600)
                    if not nonce_is_stale:
                        request.login = auth.username
                        if debug:
                            TRACE("authentication of %s successful" % auth.username)
                        return
    
    # Respond with 401 status and a WWW-Authenticate header
    header = www_authenticate(realm, key, stale=nonce_is_stale)
    if debug:
        TRACE(header)
    cherrypy.serving.response.headers['WWW-Authenticate'] = header
    raise cherrypy.HTTPError(401, "You are not authorized to access that resource")


########NEW FILE########
__FILENAME__ = caching
import datetime
import threading
import time

import cherrypy
from cherrypy.lib import cptools, httputil


class Cache(object):
    
    def get(self):
        raise NotImplemented
    
    def put(self, obj, size):
        raise NotImplemented
    
    def delete(self):
        raise NotImplemented
    
    def clear(self):
        raise NotImplemented



# ------------------------------- Memory Cache ------------------------------- #


class AntiStampedeCache(dict):
    
    def wait(self, key, timeout=5, debug=False):
        """Return the cached value for the given key, or None.
        
        If timeout is not None (the default), and the value is already
        being calculated by another thread, wait until the given timeout has
        elapsed. If the value is available before the timeout expires, it is
        returned. If not, None is returned, and a sentinel placed in the cache
        to signal other threads to wait.
        
        If timeout is None, no waiting is performed nor sentinels used.
        """
        value = self.get(key)
        if isinstance(value, threading._Event):
            if timeout is None:
                # Ignore the other thread and recalc it ourselves.
                if debug:
                    cherrypy.log('No timeout', 'TOOLS.CACHING')
                return None
            
            # Wait until it's done or times out.
            if debug:
                cherrypy.log('Waiting up to %s seconds' % timeout, 'TOOLS.CACHING')
            value.wait(timeout)
            if value.result is not None:
                # The other thread finished its calculation. Use it.
                if debug:
                    cherrypy.log('Result!', 'TOOLS.CACHING')
                return value.result
            # Timed out. Stick an Event in the slot so other threads wait
            # on this one to finish calculating the value.
            if debug:
                cherrypy.log('Timed out', 'TOOLS.CACHING')
            e = threading.Event()
            e.result = None
            dict.__setitem__(self, key, e)
            
            return None
        elif value is None:
            # Stick an Event in the slot so other threads wait
            # on this one to finish calculating the value.
            if debug:
                cherrypy.log('Timed out', 'TOOLS.CACHING')
            e = threading.Event()
            e.result = None
            dict.__setitem__(self, key, e)
        return value
    
    def __setitem__(self, key, value):
        """Set the cached value for the given key."""
        existing = self.get(key)
        dict.__setitem__(self, key, value)
        if isinstance(existing, threading._Event):
            # Set Event.result so other threads waiting on it have
            # immediate access without needing to poll the cache again.
            existing.result = value
            existing.set()


class MemoryCache(Cache):
    """An in-memory cache for varying response content.
    
    Each key in self.store is a URI, and each value is an AntiStampedeCache.
    The response for any given URI may vary based on the values of
    "selecting request headers"; that is, those named in the Vary
    response header. We assume the list of header names to be constant
    for each URI throughout the lifetime of the application, and store
    that list in self.store[uri].selecting_headers.
    
    The items contained in self.store[uri] have keys which are tuples of request
    header values (in the same order as the names in its selecting_headers),
    and values which are the actual responses.
    """
    
    maxobjects = 1000
    maxobj_size = 100000
    maxsize = 10000000
    delay = 600
    antistampede_timeout = 5
    expire_freq = 0.1
    debug = False
    
    def __init__(self):
        self.clear()
        
        # Run self.expire_cache in a separate daemon thread.
        t = threading.Thread(target=self.expire_cache, name='expire_cache')
        self.expiration_thread = t
        if hasattr(threading.Thread, "daemon"):
            # Python 2.6+
            t.daemon = True
        else:
            t.setDaemon(True)
        t.start()
    
    def clear(self):
        """Reset the cache to its initial, empty state."""
        self.store = {}
        self.expirations = {}
        self.tot_puts = 0
        self.tot_gets = 0
        self.tot_hist = 0
        self.tot_expires = 0
        self.tot_non_modified = 0
        self.cursize = 0
    
    def expire_cache(self):
        # expire_cache runs in a separate thread which the servers are
        # not aware of. It's possible that "time" will be set to None
        # arbitrarily, so we check "while time" to avoid exceptions.
        # See tickets #99 and #180 for more information.
        while time:
            now = time.time()
            # Must make a copy of expirations so it doesn't change size
            # during iteration
            for expiration_time, objects in self.expirations.items():
                if expiration_time <= now:
                    for obj_size, uri, sel_header_values in objects:
                        try:
                            del self.store[uri][sel_header_values]
                            self.tot_expires += 1
                            self.cursize -= obj_size
                        except KeyError:
                            # the key may have been deleted elsewhere
                            pass
                    del self.expirations[expiration_time]
            time.sleep(self.expire_freq)
    
    def get(self):
        """Return the current variant if in the cache, else None."""
        request = cherrypy.serving.request
        self.tot_gets += 1
        
        uri = cherrypy.url(qs=request.query_string)
        uricache = self.store.get(uri)
        if uricache is None:
            return None
        
        header_values = [request.headers.get(h, '')
                         for h in uricache.selecting_headers]
        header_values.sort()
        variant = uricache.wait(key=tuple(header_values),
                                timeout=self.antistampede_timeout,
                                debug=self.debug)
        if variant is not None:
            self.tot_hist += 1
        return variant
    
    def put(self, variant, size):
        """Store the current variant in the cache."""
        request = cherrypy.serving.request
        response = cherrypy.serving.response
        
        uri = cherrypy.url(qs=request.query_string)
        uricache = self.store.get(uri)
        if uricache is None:
            uricache = AntiStampedeCache()
            uricache.selecting_headers = [
                e.value for e in response.headers.elements('Vary')]
            self.store[uri] = uricache
        
        if len(self.store) < self.maxobjects:
            total_size = self.cursize + size
            
            # checks if there's space for the object
            if (size < self.maxobj_size and total_size < self.maxsize):
                # add to the expirations list
                expiration_time = response.time + self.delay
                bucket = self.expirations.setdefault(expiration_time, [])
                bucket.append((size, uri, uricache.selecting_headers))
                
                # add to the cache
                header_values = [request.headers.get(h, '')
                                 for h in uricache.selecting_headers]
                header_values.sort()
                uricache[tuple(header_values)] = variant
                self.tot_puts += 1
                self.cursize = total_size
    
    def delete(self):
        """Remove ALL cached variants of the current resource."""
        uri = cherrypy.url(qs=cherrypy.serving.request.query_string)
        self.store.pop(uri, None)


def get(invalid_methods=("POST", "PUT", "DELETE"), debug=False, **kwargs):
    """Try to obtain cached output. If fresh enough, raise HTTPError(304).
    
    If POST, PUT, or DELETE:
        * invalidates (deletes) any cached response for this resource
        * sets request.cached = False
        * sets request.cacheable = False
    
    else if a cached copy exists:
        * sets request.cached = True
        * sets request.cacheable = False
        * sets response.headers to the cached values
        * checks the cached Last-Modified response header against the
            current If-(Un)Modified-Since request headers; raises 304
            if necessary.
        * sets response.status and response.body to the cached values
        * returns True
    
    otherwise:
        * sets request.cached = False
        * sets request.cacheable = True
        * returns False
    """
    request = cherrypy.serving.request
    response = cherrypy.serving.response
    
    if not hasattr(cherrypy, "_cache"):
        # Make a process-wide Cache object.
        cherrypy._cache = kwargs.pop("cache_class", MemoryCache)()
        
        # Take all remaining kwargs and set them on the Cache object.
        for k, v in kwargs.items():
            setattr(cherrypy._cache, k, v)
        cherrypy._cache.debug = debug
    
    # POST, PUT, DELETE should invalidate (delete) the cached copy.
    # See http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10.
    if request.method in invalid_methods:
        if debug:
            cherrypy.log('request.method %r in invalid_methods %r' % 
                         (request.method, invalid_methods), 'TOOLS.CACHING')
        cherrypy._cache.delete()
        request.cached = False
        request.cacheable = False
        return False
    
    if 'no-cache' in [e.value for e in request.headers.elements('Pragma')]:
        request.cached = False
        request.cacheable = True
        return False
    
    cache_data = cherrypy._cache.get()
    request.cached = bool(cache_data)
    request.cacheable = not request.cached
    if request.cached:
        # Serve the cached copy.
        max_age = cherrypy._cache.delay
        for v in [e.value for e in request.headers.elements('Cache-Control')]:
            atoms = v.split('=', 1)
            directive = atoms.pop(0)
            if directive == 'max-age':
                if len(atoms) != 1 or not atoms[0].isdigit():
                    raise cherrypy.HTTPError(400, "Invalid Cache-Control header")
                max_age = int(atoms[0])
                break
            elif directive == 'no-cache':
                if debug:
                    cherrypy.log('Ignoring cache due to Cache-Control: no-cache',
                                 'TOOLS.CACHING')
                request.cached = False
                request.cacheable = True
                return False
        
        if debug:
            cherrypy.log('Reading response from cache', 'TOOLS.CACHING')
        s, h, b, create_time = cache_data
        age = int(response.time - create_time)
        if (age > max_age):
            if debug:
                cherrypy.log('Ignoring cache due to age > %d' % max_age,
                             'TOOLS.CACHING')
            request.cached = False
            request.cacheable = True
            return False
        
        # Copy the response headers. See http://www.cherrypy.org/ticket/721.
        response.headers = rh = httputil.HeaderMap()
        for k in h:
            dict.__setitem__(rh, k, dict.__getitem__(h, k))
        
        # Add the required Age header
        response.headers["Age"] = str(age)
        
        try:
            # Note that validate_since depends on a Last-Modified header;
            # this was put into the cached copy, and should have been
            # resurrected just above (response.headers = cache_data[1]).
            cptools.validate_since()
        except cherrypy.HTTPRedirect, x:
            if x.status == 304:
                cherrypy._cache.tot_non_modified += 1
            raise
        
        # serve it & get out from the request
        response.status = s
        response.body = b
    else:
        if debug:
            cherrypy.log('request is not cached', 'TOOLS.CACHING')
    return request.cached


def tee_output():
    request = cherrypy.serving.request
    if 'no-store' in request.headers.values('Cache-Control'):
        return
    
    def tee(body):
        """Tee response.body into a list."""
        if ('no-cache' in response.headers.values('Pragma') or
            'no-store' in response.headers.values('Cache-Control')):
            for chunk in body:
                yield chunk
            return
        
        output = []
        for chunk in body:
            output.append(chunk)
            yield chunk
        
        # save the cache data
        body = ''.join(output)
        cherrypy._cache.put((response.status, response.headers or {},
                             body, response.time), len(body))
    
    response = cherrypy.serving.response
    response.body = tee(response.body)


def expires(secs=0, force=False, debug=False):
    """Tool for influencing cache mechanisms using the 'Expires' header.
    
    'secs' must be either an int or a datetime.timedelta, and indicates the
    number of seconds between response.time and when the response should
    expire. The 'Expires' header will be set to (response.time + secs).
    
    If 'secs' is zero, the 'Expires' header is set one year in the past, and
    the following "cache prevention" headers are also set:
       'Pragma': 'no-cache'
       'Cache-Control': 'no-cache, must-revalidate'
    
    If 'force' is False (the default), the following headers are checked:
    'Etag', 'Last-Modified', 'Age', 'Expires'. If any are already present,
    none of the above response headers are set.
    """
    
    response = cherrypy.serving.response
    headers = response.headers
    
    cacheable = False
    if not force:
        # some header names that indicate that the response can be cached
        for indicator in ('Etag', 'Last-Modified', 'Age', 'Expires'):
            if indicator in headers:
                cacheable = True
                break
    
    if not cacheable and not force:
        if debug:
            cherrypy.log('request is not cacheable', 'TOOLS.EXPIRES')
    else:
        if debug:
            cherrypy.log('request is cacheable', 'TOOLS.EXPIRES')
        if isinstance(secs, datetime.timedelta):
            secs = (86400 * secs.days) + secs.seconds
        
        if secs == 0:
            if force or ("Pragma" not in headers):
                headers["Pragma"] = "no-cache"
            if cherrypy.serving.request.protocol >= (1, 1):
                if force or "Cache-Control" not in headers:
                    headers["Cache-Control"] = "no-cache, must-revalidate"
            # Set an explicit Expires date in the past.
            expiry = httputil.HTTPDate(1169942400.0)
        else:
            expiry = httputil.HTTPDate(response.time + secs)
        if force or "Expires" not in headers:
            headers["Expires"] = expiry

########NEW FILE########
__FILENAME__ = covercp
"""Code-coverage tools for CherryPy.

To use this module, or the coverage tools in the test suite,
you need to download 'coverage.py', either Gareth Rees' original
implementation:
http://www.garethrees.org/2001/12/04/python-coverage/

or Ned Batchelder's enhanced version:
http://www.nedbatchelder.com/code/modules/coverage.html

To turn on coverage tracing, use the following code:

    cherrypy.engine.subscribe('start', covercp.start)

DO NOT subscribe anything on the 'start_thread' channel, as previously
recommended. Calling start once in the main thread should be sufficient
to start coverage on all threads. Calling start again in each thread
effectively clears any coverage data gathered up to that point.

Run your code, then use the covercp.serve() function to browse the
results in a web browser. If you run this module from the command line,
it will call serve() for you.
"""

import re
import sys
import cgi
from urllib import quote_plus
import os, os.path
localFile = os.path.join(os.path.dirname(__file__), "coverage.cache")

try:
    from coverage import the_coverage as coverage
    def start():
        coverage.start()
except ImportError:
    # Setting coverage to None will raise errors
    # that need to be trapped downstream.
    coverage = None
    
    import warnings
    warnings.warn("No code coverage will be performed; coverage.py could not be imported.")
    
    def start():
        pass
start.priority = 20

TEMPLATE_MENU = """<html>
<head>
    <title>CherryPy Coverage Menu</title>
    <style>
        body {font: 9pt Arial, serif;}
        #tree {
            font-size: 8pt;
            font-family: Andale Mono, monospace;
            white-space: pre;
            }
        #tree a:active, a:focus {
            background-color: black;
            padding: 1px;
            color: white;
            border: 0px solid #9999FF;
            -moz-outline-style: none;
            }
        .fail { color: red;}
        .pass { color: #888;}
        #pct { text-align: right;}
        h3 {
            font-size: small;
            font-weight: bold;
            font-style: italic;
            margin-top: 5px; 
            }
        input { border: 1px solid #ccc; padding: 2px; }
        .directory {
            color: #933;
            font-style: italic;
            font-weight: bold;
            font-size: 10pt;
            }
        .file {
            color: #400;
            }
        a { text-decoration: none; }
        #crumbs {
            color: white;
            font-size: 8pt;
            font-family: Andale Mono, monospace;
            width: 100%;
            background-color: black;
            }
        #crumbs a {
            color: #f88;
            }
        #options {
            line-height: 2.3em;
            border: 1px solid black;
            background-color: #eee;
            padding: 4px;
            }
        #exclude {
            width: 100%;
            margin-bottom: 3px;
            border: 1px solid #999;
            }
        #submit {
            background-color: black;
            color: white;
            border: 0;
            margin-bottom: -9px;
            }
    </style>
</head>
<body>
<h2>CherryPy Coverage</h2>"""

TEMPLATE_FORM = """
<div id="options">
<form action='menu' method=GET>
    <input type='hidden' name='base' value='%(base)s' />
    Show percentages <input type='checkbox' %(showpct)s name='showpct' value='checked' /><br />
    Hide files over <input type='text' id='pct' name='pct' value='%(pct)s' size='3' />%%<br />
    Exclude files matching<br />
    <input type='text' id='exclude' name='exclude' value='%(exclude)s' size='20' />
    <br />

    <input type='submit' value='Change view' id="submit"/>
</form>
</div>""" 

TEMPLATE_FRAMESET = """<html>
<head><title>CherryPy coverage data</title></head>
<frameset cols='250, 1*'>
    <frame src='menu?base=%s' />
    <frame name='main' src='' />
</frameset>
</html>
"""

TEMPLATE_COVERAGE = """<html>
<head>
    <title>Coverage for %(name)s</title>
    <style>
        h2 { margin-bottom: .25em; }
        p { margin: .25em; }
        .covered { color: #000; background-color: #fff; }
        .notcovered { color: #fee; background-color: #500; }
        .excluded { color: #00f; background-color: #fff; }
         table .covered, table .notcovered, table .excluded
             { font-family: Andale Mono, monospace;
               font-size: 10pt; white-space: pre; }

         .lineno { background-color: #eee;}
         .notcovered .lineno { background-color: #000;}
         table { border-collapse: collapse;
    </style>
</head>
<body>
<h2>%(name)s</h2>
<p>%(fullpath)s</p>
<p>Coverage: %(pc)s%%</p>"""

TEMPLATE_LOC_COVERED = """<tr class="covered">
    <td class="lineno">%s&nbsp;</td>
    <td>%s</td>
</tr>\n"""
TEMPLATE_LOC_NOT_COVERED = """<tr class="notcovered">
    <td class="lineno">%s&nbsp;</td>
    <td>%s</td>
</tr>\n"""
TEMPLATE_LOC_EXCLUDED = """<tr class="excluded">
    <td class="lineno">%s&nbsp;</td>
    <td>%s</td>
</tr>\n"""

TEMPLATE_ITEM = "%s%s<a class='file' href='report?name=%s' target='main'>%s</a>\n"

def _percent(statements, missing):
    s = len(statements)
    e = s - len(missing)
    if s > 0:
        return int(round(100.0 * e / s))
    return 0

def _show_branch(root, base, path, pct=0, showpct=False, exclude=""):
    
    # Show the directory name and any of our children
    dirs = [k for k, v in root.items() if v]
    dirs.sort()
    for name in dirs:
        newpath = os.path.join(path, name)
        
        if newpath.lower().startswith(base):
            relpath = newpath[len(base):]
            yield "| " * relpath.count(os.sep)
            yield "<a class='directory' href='menu?base=%s&exclude=%s'>%s</a>\n" % \
                   (newpath, quote_plus(exclude), name)
        
        for chunk in _show_branch(root[name], base, newpath, pct, showpct, exclude):
            yield chunk
    
    # Now list the files
    if path.lower().startswith(base):
        relpath = path[len(base):]
        files = [k for k, v in root.items() if not v]
        files.sort()
        for name in files:
            newpath = os.path.join(path, name)
            
            pc_str = ""
            if showpct:
                try:
                    _, statements, _, missing, _ = coverage.analysis2(newpath)
                except:
                    # Yes, we really want to pass on all errors.
                    pass
                else:
                    pc = _percent(statements, missing)
                    pc_str = ("%3d%% " % pc).replace(' ', '&nbsp;')
                    if pc < float(pct) or pc == -1:
                        pc_str = "<span class='fail'>%s</span>" % pc_str
                    else:
                        pc_str = "<span class='pass'>%s</span>" % pc_str
            
            yield TEMPLATE_ITEM % ("| " * (relpath.count(os.sep) + 1),
                                   pc_str, newpath, name)

def _skip_file(path, exclude):
    if exclude:
        return bool(re.search(exclude, path))

def _graft(path, tree):
    d = tree
    
    p = path
    atoms = []
    while True:
        p, tail = os.path.split(p)
        if not tail:
            break
        atoms.append(tail)
    atoms.append(p)
    if p != "/":
        atoms.append("/")
    
    atoms.reverse()
    for node in atoms:
        if node:
            d = d.setdefault(node, {})

def get_tree(base, exclude):
    """Return covered module names as a nested dict."""
    tree = {}
    coverage.get_ready()
    runs = list(coverage.cexecuted.keys())
    if runs:
        for path in runs:
            if not _skip_file(path, exclude) and not os.path.isdir(path):
                _graft(path, tree)
    return tree

class CoverStats(object):
    
    def __init__(self, root=None):
        if root is None:
            # Guess initial depth. Files outside this path will not be
            # reachable from the web interface.
            import cherrypy
            root = os.path.dirname(cherrypy.__file__)
        self.root = root
    
    def index(self):
        return TEMPLATE_FRAMESET % self.root.lower()
    index.exposed = True
    
    def menu(self, base="/", pct="50", showpct="",
             exclude=r'python\d\.\d|test|tut\d|tutorial'):
        
        # The coverage module uses all-lower-case names.
        base = base.lower().rstrip(os.sep)
        
        yield TEMPLATE_MENU
        yield TEMPLATE_FORM % locals()
        
        # Start by showing links for parent paths
        yield "<div id='crumbs'>"
        path = ""
        atoms = base.split(os.sep)
        atoms.pop()
        for atom in atoms:
            path += atom + os.sep
            yield ("<a href='menu?base=%s&exclude=%s'>%s</a> %s"
                   % (path, quote_plus(exclude), atom, os.sep))
        yield "</div>"
        
        yield "<div id='tree'>"
        
        # Then display the tree
        tree = get_tree(base, exclude)
        if not tree:
            yield "<p>No modules covered.</p>"
        else:
            for chunk in _show_branch(tree, base, "/", pct,
                                      showpct == 'checked', exclude):
                yield chunk
        
        yield "</div>"
        yield "</body></html>"
    menu.exposed = True
    
    def annotated_file(self, filename, statements, excluded, missing):
        source = open(filename, 'r')
        buffer = []
        for lineno, line in enumerate(source.readlines()):
            lineno += 1
            line = line.strip("\n\r")
            empty_the_buffer = True
            if lineno in excluded:
                template = TEMPLATE_LOC_EXCLUDED
            elif lineno in missing:
                template = TEMPLATE_LOC_NOT_COVERED
            elif lineno in statements:
                template = TEMPLATE_LOC_COVERED
            else:
                empty_the_buffer = False
                buffer.append((lineno, line))
            if empty_the_buffer:
                for lno, pastline in buffer:
                    yield template % (lno, cgi.escape(pastline))
                buffer = []
                yield template % (lineno, cgi.escape(line))
    
    def report(self, name):
        coverage.get_ready()
        filename, statements, excluded, missing, _ = coverage.analysis2(name)
        pc = _percent(statements, missing)
        yield TEMPLATE_COVERAGE % dict(name=os.path.basename(name),
                                       fullpath=name,
                                       pc=pc)
        yield '<table>\n'
        for line in self.annotated_file(filename, statements, excluded,
                                        missing):
            yield line
        yield '</table>'
        yield '</body>'
        yield '</html>'
    report.exposed = True


def serve(path=localFile, port=8080, root=None):
    if coverage is None:
        raise ImportError("The coverage module could not be imported.")
    coverage.cache_default = path
    
    import cherrypy
    cherrypy.config.update({'server.socket_port': int(port),
                            'server.thread_pool': 10,
                            'environment': "production",
                            })
    cherrypy.quickstart(CoverStats(root))

if __name__ == "__main__":
    serve(*tuple(sys.argv[1:]))


########NEW FILE########
__FILENAME__ = cptools
"""Functions for builtin CherryPy tools."""

import logging
try:
    # Python 2.5+
    from hashlib import md5
except ImportError:
    from md5 import new as md5
import re

try:
    set
except NameError:
    from sets import Set as set

import cherrypy
from cherrypy.lib import httputil as _httputil


#                     Conditional HTTP request support                     #

def validate_etags(autotags=False, debug=False):
    """Validate the current ETag against If-Match, If-None-Match headers.
    
    If autotags is True, an ETag response-header value will be provided
    from an MD5 hash of the response body (unless some other code has
    already provided an ETag header). If False (the default), the ETag
    will not be automatic.
    
    WARNING: the autotags feature is not designed for URL's which allow
    methods other than GET. For example, if a POST to the same URL returns
    no content, the automatic ETag will be incorrect, breaking a fundamental
    use for entity tags in a possibly destructive fashion. Likewise, if you
    raise 304 Not Modified, the response body will be empty, the ETag hash
    will be incorrect, and your application will break.
    See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.24
    """
    response = cherrypy.serving.response
    
    # Guard against being run twice.
    if hasattr(response, "ETag"):
        return
    
    status, reason, msg = _httputil.valid_status(response.status)
    
    etag = response.headers.get('ETag')
    
    # Automatic ETag generation. See warning in docstring.
    if etag:
        if debug:
            cherrypy.log('ETag already set: %s' % etag, 'TOOLS.ETAGS')
    elif not autotags:
        if debug:
            cherrypy.log('Autotags off', 'TOOLS.ETAGS')
    elif status != 200:
        if debug:
            cherrypy.log('Status not 200', 'TOOLS.ETAGS')
    else:
        etag = response.collapse_body()
        etag = '"%s"' % md5(etag).hexdigest()
        if debug:
            cherrypy.log('Setting ETag: %s' % etag, 'TOOLS.ETAGS')
        response.headers['ETag'] = etag
    
    response.ETag = etag
    
    # "If the request would, without the If-Match header field, result in
    # anything other than a 2xx or 412 status, then the If-Match header
    # MUST be ignored."
    if debug:
        cherrypy.log('Status: %s' % status, 'TOOLS.ETAGS')
    if status >= 200 and status <= 299:
        request = cherrypy.serving.request
        
        conditions = request.headers.elements('If-Match') or []
        conditions = [str(x) for x in conditions]
        if debug:
            cherrypy.log('If-Match conditions: %s' % repr(conditions),
                         'TOOLS.ETAGS')
        if conditions and not (conditions == ["*"] or etag in conditions):
            raise cherrypy.HTTPError(412, "If-Match failed: ETag %r did "
                                     "not match %r" % (etag, conditions))
        
        conditions = request.headers.elements('If-None-Match') or []
        conditions = [str(x) for x in conditions]
        if debug:
            cherrypy.log('If-None-Match conditions: %s' % repr(conditions),
                         'TOOLS.ETAGS')
        if conditions == ["*"] or etag in conditions:
            if debug:
                cherrypy.log('request.method: %s' % request.method, 'TOOLS.ETAGS')
            if request.method in ("GET", "HEAD"):
                raise cherrypy.HTTPRedirect([], 304)
            else:
                raise cherrypy.HTTPError(412, "If-None-Match failed: ETag %r "
                                         "matched %r" % (etag, conditions))

def validate_since():
    """Validate the current Last-Modified against If-Modified-Since headers.
    
    If no code has set the Last-Modified response header, then no validation
    will be performed.
    """
    response = cherrypy.serving.response
    lastmod = response.headers.get('Last-Modified')
    if lastmod:
        status, reason, msg = _httputil.valid_status(response.status)
        
        request = cherrypy.serving.request
        
        since = request.headers.get('If-Unmodified-Since')
        if since and since != lastmod:
            if (status >= 200 and status <= 299) or status == 412:
                raise cherrypy.HTTPError(412)
        
        since = request.headers.get('If-Modified-Since')
        if since and since == lastmod:
            if (status >= 200 and status <= 299) or status == 304:
                if request.method in ("GET", "HEAD"):
                    raise cherrypy.HTTPRedirect([], 304)
                else:
                    raise cherrypy.HTTPError(412)


#                                Tool code                                #

def proxy(base=None, local='X-Forwarded-Host', remote='X-Forwarded-For',
          scheme='X-Forwarded-Proto', debug=False):
    """Change the base URL (scheme://host[:port][/path]).
    
    For running a CP server behind Apache, lighttpd, or other HTTP server.
    
    If you want the new request.base to include path info (not just the host),
    you must explicitly set base to the full base path, and ALSO set 'local'
    to '', so that the X-Forwarded-Host request header (which never includes
    path info) does not override it. Regardless, the value for 'base' MUST
    NOT end in a slash.
    
    cherrypy.request.remote.ip (the IP address of the client) will be
    rewritten if the header specified by the 'remote' arg is valid.
    By default, 'remote' is set to 'X-Forwarded-For'. If you do not
    want to rewrite remote.ip, set the 'remote' arg to an empty string.
    """
    
    request = cherrypy.serving.request
    
    if scheme:
        s = request.headers.get(scheme, None)
        if debug:
            cherrypy.log('Testing scheme %r:%r' % (scheme, s), 'TOOLS.PROXY')
        if s == 'on' and 'ssl' in scheme.lower():
            # This handles e.g. webfaction's 'X-Forwarded-Ssl: on' header
            scheme = 'https'
        else:
            # This is for lighttpd/pound/Mongrel's 'X-Forwarded-Proto: https'
            scheme = s
    if not scheme:
        scheme = request.base[:request.base.find("://")]
    
    if local:
        lbase = request.headers.get(local, None)
        if debug:
            cherrypy.log('Testing local %r:%r' % (local, lbase), 'TOOLS.PROXY')
        if lbase is not None:
            base = lbase.split(',')[0]
    if not base:
        port = request.local.port
        if port == 80:
            base = '127.0.0.1'
        else:
            base = '127.0.0.1:%s' % port
    
    if base.find("://") == -1:
        # add http:// or https:// if needed
        base = scheme + "://" + base
    
    request.base = base
    
    if remote:
        xff = request.headers.get(remote)
        if debug:
            cherrypy.log('Testing remote %r:%r' % (remote, xff), 'TOOLS.PROXY')
        if xff:
            if remote == 'X-Forwarded-For':
                # See http://bob.pythonmac.org/archives/2005/09/23/apache-x-forwarded-for-caveat/
                xff = xff.split(',')[-1].strip()
            request.remote.ip = xff


def ignore_headers(headers=('Range',), debug=False):
    """Delete request headers whose field names are included in 'headers'.
    
    This is a useful tool for working behind certain HTTP servers;
    for example, Apache duplicates the work that CP does for 'Range'
    headers, and will doubly-truncate the response.
    """
    request = cherrypy.serving.request
    for name in headers:
        if name in request.headers:
            if debug:
                cherrypy.log('Ignoring request header %r' % name,
                             'TOOLS.IGNORE_HEADERS')
            del request.headers[name]


def response_headers(headers=None, debug=False):
    """Set headers on the response."""
    if debug:
        cherrypy.log('Setting response headers: %s' % repr(headers),
                     'TOOLS.RESPONSE_HEADERS')
    for name, value in (headers or []):
        cherrypy.serving.response.headers[name] = value
response_headers.failsafe = True


def referer(pattern, accept=True, accept_missing=False, error=403,
            message='Forbidden Referer header.', debug=False):
    """Raise HTTPError if Referer header does/does not match the given pattern.
    
    pattern: a regular expression pattern to test against the Referer.
    accept: if True, the Referer must match the pattern; if False,
        the Referer must NOT match the pattern.
    accept_missing: if True, permit requests with no Referer header.
    error: the HTTP error code to return to the client on failure.
    message: a string to include in the response body on failure.
    """
    try:
        ref = cherrypy.serving.request.headers['Referer']
        match = bool(re.match(pattern, ref))
        if debug:
            cherrypy.log('Referer %r matches %r' % (ref, pattern),
                         'TOOLS.REFERER')
        if accept == match:
            return
    except KeyError:
        if debug:
            cherrypy.log('No Referer header', 'TOOLS.REFERER')
        if accept_missing:
            return
    
    raise cherrypy.HTTPError(error, message)


class SessionAuth(object):
    """Assert that the user is logged in."""
    
    session_key = "username"
    debug = False
    
    def check_username_and_password(self, username, password):
        pass
    
    def anonymous(self):
        """Provide a temporary user name for anonymous users."""
        pass
    
    def on_login(self, username):
        pass
    
    def on_logout(self, username):
        pass
    
    def on_check(self, username):
        pass
    
    def login_screen(self, from_page='..', username='', error_msg='', **kwargs):
        return """<html><body>
Message: %(error_msg)s
<form method="post" action="do_login">
    Login: <input type="text" name="username" value="%(username)s" size="10" /><br />
    Password: <input type="password" name="password" size="10" /><br />
    <input type="hidden" name="from_page" value="%(from_page)s" /><br />
    <input type="submit" />
</form>
</body></html>""" % {'from_page': from_page, 'username': username,
                     'error_msg': error_msg}
    
    def do_login(self, username, password, from_page='..', **kwargs):
        """Login. May raise redirect, or return True if request handled."""
        response = cherrypy.serving.response
        error_msg = self.check_username_and_password(username, password)
        if error_msg:
            body = self.login_screen(from_page, username, error_msg)
            response.body = body
            if "Content-Length" in response.headers:
                # Delete Content-Length header so finalize() recalcs it.
                del response.headers["Content-Length"]
            return True
        else:
            cherrypy.serving.request.login = username
            cherrypy.session[self.session_key] = username
            self.on_login(username)
            raise cherrypy.HTTPRedirect(from_page or "/")
    
    def do_logout(self, from_page='..', **kwargs):
        """Logout. May raise redirect, or return True if request handled."""
        sess = cherrypy.session
        username = sess.get(self.session_key)
        sess[self.session_key] = None
        if username:
            cherrypy.serving.request.login = None
            self.on_logout(username)
        raise cherrypy.HTTPRedirect(from_page)
    
    def do_check(self):
        """Assert username. May raise redirect, or return True if request handled."""
        sess = cherrypy.session
        request = cherrypy.serving.request
        response = cherrypy.serving.response
        
        username = sess.get(self.session_key)
        if not username:
            sess[self.session_key] = username = self.anonymous()
            if self.debug:
                cherrypy.log('No session[username], trying anonymous', 'TOOLS.SESSAUTH')
        if not username:
            url = cherrypy.url(qs=request.query_string)
            if self.debug:
                cherrypy.log('No username, routing to login_screen with '
                             'from_page %r' % url, 'TOOLS.SESSAUTH')
            response.body = self.login_screen(url)
            if "Content-Length" in response.headers:
                # Delete Content-Length header so finalize() recalcs it.
                del response.headers["Content-Length"]
            return True
        if self.debug:
            cherrypy.log('Setting request.login to %r' % username, 'TOOLS.SESSAUTH')
        request.login = username
        self.on_check(username)
    
    def run(self):
        request = cherrypy.serving.request
        response = cherrypy.serving.response
        
        path = request.path_info
        if path.endswith('login_screen'):
            if self.debug:
                cherrypy.log('routing %r to login_screen' % path, 'TOOLS.SESSAUTH')
            return self.login_screen(**request.params)
        elif path.endswith('do_login'):
            if request.method != 'POST':
                response.headers['Allow'] = "POST"
                if self.debug:
                    cherrypy.log('do_login requires POST', 'TOOLS.SESSAUTH')
                raise cherrypy.HTTPError(405)
            if self.debug:
                cherrypy.log('routing %r to do_login' % path, 'TOOLS.SESSAUTH')
            return self.do_login(**request.params)
        elif path.endswith('do_logout'):
            if request.method != 'POST':
                response.headers['Allow'] = "POST"
                raise cherrypy.HTTPError(405)
            if self.debug:
                cherrypy.log('routing %r to do_logout' % path, 'TOOLS.SESSAUTH')
            return self.do_logout(**request.params)
        else:
            if self.debug:
                cherrypy.log('No special path, running do_check', 'TOOLS.SESSAUTH')
            return self.do_check()


def session_auth(**kwargs):
    sa = SessionAuth()
    for k, v in kwargs.items():
        setattr(sa, k, v)
    return sa.run()
session_auth.__doc__ = """Session authentication hook.

Any attribute of the SessionAuth class may be overridden via a keyword arg
to this function:

""" + "\n".join(["%s: %s" % (k, type(getattr(SessionAuth, k)).__name__)
                 for k in dir(SessionAuth) if not k.startswith("__")])


def log_traceback(severity=logging.ERROR, debug=False):
    """Write the last error's traceback to the cherrypy error log."""
    cherrypy.log("", "HTTP", severity=severity, traceback=True)

def log_request_headers(debug=False):
    """Write request headers to the cherrypy error log."""
    h = ["  %s: %s" % (k, v) for k, v in cherrypy.serving.request.header_list]
    cherrypy.log('\nRequest Headers:\n' + '\n'.join(h), "HTTP")

def log_hooks(debug=False):
    """Write request.hooks to the cherrypy error log."""
    request = cherrypy.serving.request
    
    msg = []
    # Sort by the standard points if possible.
    from cherrypy import _cprequest
    points = _cprequest.hookpoints
    for k in request.hooks.keys():
        if k not in points:
            points.append(k)
    
    for k in points:
        msg.append("    %s:" % k)
        v = request.hooks.get(k, [])
        v.sort()
        for h in v:
            msg.append("        %r" % h)
    cherrypy.log('\nRequest Hooks for ' + cherrypy.url() + 
                 ':\n' + '\n'.join(msg), "HTTP")

def redirect(url='', internal=True, debug=False):
    """Raise InternalRedirect or HTTPRedirect to the given url."""
    if debug:
        cherrypy.log('Redirecting %sto: %s' % 
                     ({True: 'internal ', False: ''}[internal], url),
                     'TOOLS.REDIRECT')
    if internal:
        raise cherrypy.InternalRedirect(url)
    else:
        raise cherrypy.HTTPRedirect(url)

def trailing_slash(missing=True, extra=False, status=None, debug=False):
    """Redirect if path_info has (missing|extra) trailing slash."""
    request = cherrypy.serving.request
    pi = request.path_info
    
    if debug:
        cherrypy.log('is_index: %r, missing: %r, extra: %r, path_info: %r' % 
                     (request.is_index, missing, extra, pi),
                     'TOOLS.TRAILING_SLASH')
    if request.is_index is True:
        if missing:
            if not pi.endswith('/'):
                new_url = cherrypy.url(pi + '/', request.query_string)
                raise cherrypy.HTTPRedirect(new_url, status=status or 301)
    elif request.is_index is False:
        if extra:
            # If pi == '/', don't redirect to ''!
            if pi.endswith('/') and pi != '/':
                new_url = cherrypy.url(pi[:-1], request.query_string)
                raise cherrypy.HTTPRedirect(new_url, status=status or 301)

def flatten(debug=False):
    """Wrap response.body in a generator that recursively iterates over body.
    
    This allows cherrypy.response.body to consist of 'nested generators';
    that is, a set of generators that yield generators.
    """
    import types
    def flattener(input):
        numchunks = 0
        for x in input:
            if not isinstance(x, types.GeneratorType):
                numchunks += 1
                yield x
            else:
                for y in flattener(x):
                    numchunks += 1
                    yield y
        if debug:
            cherrypy.log('Flattened %d chunks' % numchunks, 'TOOLS.FLATTEN')
    response = cherrypy.serving.response
    response.body = flattener(response.body)


def accept(media=None, debug=False):
    """Return the client's preferred media-type (from the given Content-Types).
    
    If 'media' is None (the default), no test will be performed.
    
    If 'media' is provided, it should be the Content-Type value (as a string)
    or values (as a list or tuple of strings) which the current resource
    can emit. The client's acceptable media ranges (as declared in the
    Accept request header) will be matched in order to these Content-Type
    values; the first such string is returned. That is, the return value
    will always be one of the strings provided in the 'media' arg (or None
    if 'media' is None).
    
    If no match is found, then HTTPError 406 (Not Acceptable) is raised.
    Note that most web browsers send */* as a (low-quality) acceptable
    media range, which should match any Content-Type. In addition, "...if
    no Accept header field is present, then it is assumed that the client
    accepts all media types."
    
    Matching types are checked in order of client preference first,
    and then in the order of the given 'media' values.
    
    Note that this function does not honor accept-params (other than "q").
    """
    if not media:
        return
    if isinstance(media, basestring):
        media = [media]
    request = cherrypy.serving.request
    
    # Parse the Accept request header, and try to match one
    # of the requested media-ranges (in order of preference).
    ranges = request.headers.elements('Accept')
    if not ranges:
        # Any media type is acceptable.
        if debug:
            cherrypy.log('No Accept header elements', 'TOOLS.ACCEPT')
        return media[0]
    else:
        # Note that 'ranges' is sorted in order of preference
        for element in ranges:
            if element.qvalue > 0:
                if element.value == "*/*":
                    # Matches any type or subtype
                    if debug:
                        cherrypy.log('Match due to */*', 'TOOLS.ACCEPT')
                    return media[0]
                elif element.value.endswith("/*"):
                    # Matches any subtype
                    mtype = element.value[:-1]  # Keep the slash
                    for m in media:
                        if m.startswith(mtype):
                            if debug:
                                cherrypy.log('Match due to %s' % element.value,
                                             'TOOLS.ACCEPT')
                            return m
                else:
                    # Matches exact value
                    if element.value in media:
                        if debug:
                            cherrypy.log('Match due to %s' % element.value,
                                         'TOOLS.ACCEPT')
                        return element.value
    
    # No suitable media-range found.
    ah = request.headers.get('Accept')
    if ah is None:
        msg = "Your client did not send an Accept header."
    else:
        msg = "Your client sent this Accept header: %s." % ah
    msg += (" But this resource only emits these media types: %s." % 
            ", ".join(media))
    raise cherrypy.HTTPError(406, msg)


class MonitoredHeaderMap(_httputil.HeaderMap):
    
    def __init__(self):
        self.accessed_headers = set()
    
    def __getitem__(self, key):
        self.accessed_headers.add(key)
        return _httputil.HeaderMap.__getitem__(self, key)
    
    def __contains__(self, key):
        self.accessed_headers.add(key)
        return _httputil.HeaderMap.__contains__(self, key)
    
    def get(self, key, default=None):
        self.accessed_headers.add(key)
        return _httputil.HeaderMap.get(self, key, default=default)
    
    def has_key(self, key):
        self.accessed_headers.add(key)
        return _httputil.HeaderMap.has_key(self, key)


def autovary(ignore=None, debug=False):
    """Auto-populate the Vary response header based on request.header access."""
    request = cherrypy.serving.request
    
    req_h = request.headers
    request.headers = MonitoredHeaderMap()
    request.headers.update(req_h)
    if ignore is None:
        ignore = set(['Content-Disposition', 'Content-Length', 'Content-Type'])
    
    def set_response_header():
        resp_h = cherrypy.serving.response.headers
        v = set([e.value for e in resp_h.elements('Vary')])
        if debug:
            cherrypy.log('Accessed headers: %s' % request.headers.accessed_headers,
                         'TOOLS.AUTOVARY')
        v = v.union(request.headers.accessed_headers)
        v = v.difference(ignore)
        v = list(v)
        v.sort()
        resp_h['Vary'] = ', '.join(v)
    request.hooks.attach('before_finalize', set_response_header, 95)


########NEW FILE########
__FILENAME__ = encoding
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO
try:
    set
except NameError:
    from sets import Set as set
import struct
import time
import types

import cherrypy
from cherrypy.lib import file_generator
from cherrypy.lib import set_vary_header


def decode(encoding=None, default_encoding='utf-8'):
    """Replace or extend the list of charsets used to decode a request entity.
    
    Either argument may be a single string or a list of strings.
    
    encoding: If not None, restricts the set of charsets attempted while decoding
    a request entity to the given set (even if a different charset is given in
    the Content-Type request header).
    
    default_encoding: Only in effect if the 'encoding' argument is not given.
    If given, the set of charsets attempted while decoding a request entity is
    *extended* with the given value(s).
    """
    body = cherrypy.request.body
    if encoding is not None:
        if not isinstance(encoding, list):
            encoding = [encoding]
        body.attempt_charsets = encoding
    elif default_encoding:
        if not isinstance(default_encoding, list):
            default_encoding = [default_encoding]
        body.attempt_charsets = body.attempt_charsets + default_encoding


class ResponseEncoder:
    
    default_encoding = 'utf-8'
    failmsg = "Response body could not be encoded with %r."
    encoding = None
    errors = 'strict'
    text_only = True
    add_charset = True
    debug = False
    
    def __init__(self, **kwargs):
        for k, v in kwargs.items():
            setattr(self, k, v)
        
        self.attempted_charsets = set()
        request = cherrypy.serving.request
        if request.handler is not None:
            # Replace request.handler with self
            if self.debug:
                cherrypy.log('Replacing request.handler', 'TOOLS.ENCODE')
            self.oldhandler = request.handler
            request.handler = self
    
    def encode_stream(self, encoding):
        """Encode a streaming response body.
        
        Use a generator wrapper, and just pray it works as the stream is
        being written out.
        """
        if encoding in self.attempted_charsets:
            return False
        self.attempted_charsets.add(encoding)
        
        def encoder(body):
            for chunk in body:
                if isinstance(chunk, unicode):
                    chunk = chunk.encode(encoding, self.errors)
                yield chunk
        self.body = encoder(self.body)
        return True
    
    def encode_string(self, encoding):
        """Encode a buffered response body."""
        if encoding in self.attempted_charsets:
            return False
        self.attempted_charsets.add(encoding)
        
        try:
            body = []
            for chunk in self.body:
                if isinstance(chunk, unicode):
                    chunk = chunk.encode(encoding, self.errors)
                body.append(chunk)
            self.body = body
        except (LookupError, UnicodeError):
            return False
        else:
            return True
    
    def find_acceptable_charset(self):
        request = cherrypy.serving.request
        response = cherrypy.serving.response
        
        if self.debug:
            cherrypy.log('response.stream %r' % response.stream, 'TOOLS.ENCODE')
        if response.stream:
            encoder = self.encode_stream
        else:
            encoder = self.encode_string
            if "Content-Length" in response.headers:
                # Delete Content-Length header so finalize() recalcs it.
                # Encoded strings may be of different lengths from their
                # unicode equivalents, and even from each other. For example:
                # >>> t = u"\u7007\u3040"
                # >>> len(t)
                # 2
                # >>> len(t.encode("UTF-8"))
                # 6
                # >>> len(t.encode("utf7"))
                # 8
                del response.headers["Content-Length"]
        
        # Parse the Accept-Charset request header, and try to provide one
        # of the requested charsets (in order of user preference).
        encs = request.headers.elements('Accept-Charset')
        charsets = [enc.value.lower() for enc in encs]
        if self.debug:
            cherrypy.log('charsets %s' % repr(charsets), 'TOOLS.ENCODE')
        
        if self.encoding is not None:
            # If specified, force this encoding to be used, or fail.
            encoding = self.encoding.lower()
            if self.debug:
                cherrypy.log('Specified encoding %r' % encoding, 'TOOLS.ENCODE')
            if (not charsets) or "*" in charsets or encoding in charsets:
                if self.debug:
                    cherrypy.log('Attempting encoding %r' % encoding, 'TOOLS.ENCODE')
                if encoder(encoding):
                    return encoding
        else:
            if not encs:
                if self.debug:
                    cherrypy.log('Attempting default encoding %r' % 
                                 self.default_encoding, 'TOOLS.ENCODE')
                # Any character-set is acceptable.
                if encoder(self.default_encoding):
                    return self.default_encoding
                else:
                    raise cherrypy.HTTPError(500, self.failmsg % self.default_encoding)
            else:
                if "*" not in charsets:
                    # If no "*" is present in an Accept-Charset field, then all
                    # character sets not explicitly mentioned get a quality
                    # value of 0, except for ISO-8859-1, which gets a quality
                    # value of 1 if not explicitly mentioned.
                    iso = 'iso-8859-1'
                    if iso not in charsets:
                        if self.debug:
                            cherrypy.log('Attempting ISO-8859-1 encoding',
                                         'TOOLS.ENCODE')
                        if encoder(iso):
                            return iso
                
                for element in encs:
                    if element.qvalue > 0:
                        if element.value == "*":
                            # Matches any charset. Try our default.
                            if self.debug:
                                cherrypy.log('Attempting default encoding due '
                                             'to %r' % element, 'TOOLS.ENCODE')
                            if encoder(self.default_encoding):
                                return self.default_encoding
                        else:
                            encoding = element.value
                            if self.debug:
                                cherrypy.log('Attempting encoding %r (qvalue >'
                                             '0)' % element, 'TOOLS.ENCODE')
                            if encoder(encoding):
                                return encoding
        
        # No suitable encoding found.
        ac = request.headers.get('Accept-Charset')
        if ac is None:
            msg = "Your client did not send an Accept-Charset header."
        else:
            msg = "Your client sent this Accept-Charset header: %s." % ac
        msg += " We tried these charsets: %s." % ", ".join(self.attempted_charsets)
        raise cherrypy.HTTPError(406, msg)
    
    def __call__(self, *args, **kwargs):
        response = cherrypy.serving.response
        self.body = self.oldhandler(*args, **kwargs)
        
        if isinstance(self.body, basestring):
            # strings get wrapped in a list because iterating over a single
            # item list is much faster than iterating over every character
            # in a long string.
            if self.body:
                self.body = [self.body]
            else:
                # [''] doesn't evaluate to False, so replace it with [].
                self.body = []
        elif isinstance(self.body, types.FileType):
            self.body = file_generator(self.body)
        elif self.body is None:
            self.body = []
        
        ct = response.headers.elements("Content-Type")
        if self.debug:
            cherrypy.log('Content-Type: %r' % ct, 'TOOLS.ENCODE')
        if ct:
            if self.text_only:
                ct = ct[0]
                if ct.value.lower().startswith("text/"):
                    if self.debug:
                        cherrypy.log('Content-Type %r starts with "text/"' % ct,
                                     'TOOLS.ENCODE')
                    do_find = True
                else:
                    if self.debug:
                        cherrypy.log('Not finding because Content-Type %r does '
                                     'not start with "text/"' % ct,
                                     'TOOLS.ENCODE')
                    do_find = False
            else:
                if self.debug:
                    cherrypy.log('Finding because not text_only', 'TOOLS.ENCODE')
                do_find = True
            
            if do_find:
                # Set "charset=..." param on response Content-Type header
                ct.params['charset'] = self.find_acceptable_charset()
                if self.add_charset:
                    if self.debug:
                        cherrypy.log('Setting Content-Type %r' % ct,
                                     'TOOLS.ENCODE')
                    response.headers["Content-Type"] = str(ct)
        
        return self.body

# GZIP

def compress(body, compress_level):
    """Compress 'body' at the given compress_level."""
    import zlib
    
    # See http://www.gzip.org/zlib/rfc-gzip.html
    yield '\x1f\x8b'       # ID1 and ID2: gzip marker
    yield '\x08'           # CM: compression method
    yield '\x00'           # FLG: none set
    # MTIME: 4 bytes
    yield struct.pack("<L", int(time.time()) & 0xFFFFFFFFL)
    yield '\x02'           # XFL: max compression, slowest algo
    yield '\xff'           # OS: unknown
    
    crc = zlib.crc32("")
    size = 0
    zobj = zlib.compressobj(compress_level,
                            zlib.DEFLATED, -zlib.MAX_WBITS,
                            zlib.DEF_MEM_LEVEL, 0)
    for line in body:
        size += len(line)
        crc = zlib.crc32(line, crc)
        yield zobj.compress(line)
    yield zobj.flush()
    
    # CRC32: 4 bytes
    yield struct.pack("<L", crc & 0xFFFFFFFFL)
    # ISIZE: 4 bytes
    yield struct.pack("<L", size & 0xFFFFFFFFL)

def decompress(body):
    import gzip
    
    zbuf = StringIO()
    zbuf.write(body)
    zbuf.seek(0)
    zfile = gzip.GzipFile(mode='rb', fileobj=zbuf)
    data = zfile.read()
    zfile.close()
    return data


def gzip(compress_level=5, mime_types=['text/html', 'text/plain'], debug=False):
    """Try to gzip the response body if Content-Type in mime_types.
    
    cherrypy.response.headers['Content-Type'] must be set to one of the
    values in the mime_types arg before calling this function.
    
    No compression is performed if any of the following hold:
        * The client sends no Accept-Encoding request header
        * No 'gzip' or 'x-gzip' is present in the Accept-Encoding header
        * No 'gzip' or 'x-gzip' with a qvalue > 0 is present
        * The 'identity' value is given with a qvalue > 0.
    """
    request = cherrypy.serving.request
    response = cherrypy.serving.response
    
    set_vary_header(response, "Accept-Encoding")
    
    if not response.body:
        # Response body is empty (might be a 304 for instance)
        if debug:
            cherrypy.log('No response body', context='TOOLS.GZIP')
        return
    
    # If returning cached content (which should already have been gzipped),
    # don't re-zip.
    if getattr(request, "cached", False):
        if debug:
            cherrypy.log('Not gzipping cached response', context='TOOLS.GZIP')
        return
    
    acceptable = request.headers.elements('Accept-Encoding')
    if not acceptable:
        # If no Accept-Encoding field is present in a request,
        # the server MAY assume that the client will accept any
        # content coding. In this case, if "identity" is one of
        # the available content-codings, then the server SHOULD use
        # the "identity" content-coding, unless it has additional
        # information that a different content-coding is meaningful
        # to the client.
        if debug:
            cherrypy.log('No Accept-Encoding', context='TOOLS.GZIP')
        return
    
    ct = response.headers.get('Content-Type', '').split(';')[0]
    for coding in acceptable:
        if coding.value == 'identity' and coding.qvalue != 0:
            if debug:
                cherrypy.log('Non-zero identity qvalue: %r' % coding,
                             context='TOOLS.GZIP')
            return
        if coding.value in ('gzip', 'x-gzip'):
            if coding.qvalue == 0:
                if debug:
                    cherrypy.log('Zero gzip qvalue: %r' % coding,
                                 context='TOOLS.GZIP')
                return
            
            if ct not in mime_types:
                if debug:
                    cherrypy.log('Content-Type %r not in mime_types %r' % 
                                 (ct, mime_types), context='TOOLS.GZIP')
                return
            
            if debug:
                cherrypy.log('Gzipping', context='TOOLS.GZIP')
            # Return a generator that compresses the page
            response.headers['Content-Encoding'] = 'gzip'
            response.body = compress(response.body, compress_level)
            if "Content-Length" in response.headers:
                # Delete Content-Length header so finalize() recalcs it.
                del response.headers["Content-Length"]
            
            return
    
    if debug:
        cherrypy.log('No acceptable encoding found.', context='GZIP')
    cherrypy.HTTPError(406, "identity, gzip").set_response()


########NEW FILE########
__FILENAME__ = http
import warnings
warnings.warn('cherrypy.lib.http has been deprecated and will be removed '
              'in CherryPy 3.3 use cherrypy.lib.httputil instead.',
              DeprecationWarning)

from cherrypy.lib.httputil import *


########NEW FILE########
__FILENAME__ = httpauth
"""
httpauth modules defines functions to implement HTTP Digest Authentication (RFC 2617).
This has full compliance with 'Digest' and 'Basic' authentication methods. In
'Digest' it supports both MD5 and MD5-sess algorithms.

Usage:

    First use 'doAuth' to request the client authentication for a
    certain resource. You should send an httplib.UNAUTHORIZED response to the
    client so he knows he has to authenticate itself.
    
    Then use 'parseAuthorization' to retrieve the 'auth_map' used in
    'checkResponse'.

    To use 'checkResponse' you must have already verified the password associated
    with the 'username' key in 'auth_map' dict. Then you use the 'checkResponse'
    function to verify if the password matches the one sent by the client.

SUPPORTED_ALGORITHM - list of supported 'Digest' algorithms
SUPPORTED_QOP - list of supported 'Digest' 'qop'.
"""
__version__ = 1, 0, 1
__author__ = "Tiago Cogumbreiro <cogumbreiro@users.sf.net>"
__credits__ = """
    Peter van Kampen for its recipe which implement most of Digest authentication:
    http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/302378
"""

__license__ = """
Copyright (c) 2005, Tiago Cogumbreiro <cogumbreiro@users.sf.net>
All rights reserved.

Redistribution and use in source and binary forms, with or without modification, 
are permitted provided that the following conditions are met:

    * Redistributions of source code must retain the above copyright notice, 
      this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above copyright notice, 
      this list of conditions and the following disclaimer in the documentation 
      and/or other materials provided with the distribution.
    * Neither the name of Sylvain Hellegouarch nor the names of his contributors 
      may be used to endorse or promote products derived from this software 
      without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND 
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED 
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE 
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE 
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL 
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR 
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER 
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, 
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE 
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"""

__all__ = ("digestAuth", "basicAuth", "doAuth", "checkResponse",
           "parseAuthorization", "SUPPORTED_ALGORITHM", "md5SessionKey",
           "calculateNonce", "SUPPORTED_QOP")

################################################################################
try:
    # Python 2.5+
    from hashlib import md5
except ImportError:
    from md5 import new as md5
import time
import base64
from urllib2 import parse_http_list, parse_keqv_list

MD5 = "MD5"
MD5_SESS = "MD5-sess"
AUTH = "auth"
AUTH_INT = "auth-int"

SUPPORTED_ALGORITHM = (MD5, MD5_SESS)
SUPPORTED_QOP = (AUTH, AUTH_INT)

################################################################################
# doAuth
#
DIGEST_AUTH_ENCODERS = {
    MD5: lambda val: md5(val).hexdigest(),
    MD5_SESS: lambda val: md5(val).hexdigest(),
#    SHA: lambda val: sha.new (val).hexdigest (),
}

def calculateNonce (realm, algorithm=MD5):
    """This is an auxaliary function that calculates 'nonce' value. It is used
    to handle sessions."""

    global SUPPORTED_ALGORITHM, DIGEST_AUTH_ENCODERS
    assert algorithm in SUPPORTED_ALGORITHM

    try:
        encoder = DIGEST_AUTH_ENCODERS[algorithm]
    except KeyError:
        raise NotImplementedError ("The chosen algorithm (%s) does not have "\
                                   "an implementation yet" % algorithm)

    return encoder ("%d:%s" % (time.time(), realm))

def digestAuth (realm, algorithm=MD5, nonce=None, qop=AUTH):
    """Challenges the client for a Digest authentication."""
    global SUPPORTED_ALGORITHM, DIGEST_AUTH_ENCODERS, SUPPORTED_QOP
    assert algorithm in SUPPORTED_ALGORITHM
    assert qop in SUPPORTED_QOP

    if nonce is None:
        nonce = calculateNonce (realm, algorithm)

    return 'Digest realm="%s", nonce="%s", algorithm="%s", qop="%s"' % (
        realm, nonce, algorithm, qop
    )

def basicAuth (realm):
    """Challengenes the client for a Basic authentication."""
    assert '"' not in realm, "Realms cannot contain the \" (quote) character."

    return 'Basic realm="%s"' % realm

def doAuth (realm):
    """'doAuth' function returns the challenge string b giving priority over
    Digest and fallback to Basic authentication when the browser doesn't
    support the first one.
    
    This should be set in the HTTP header under the key 'WWW-Authenticate'."""

    return digestAuth (realm) + " " + basicAuth (realm)


################################################################################
# Parse authorization parameters
#
def _parseDigestAuthorization (auth_params):
    # Convert the auth params to a dict
    items = parse_http_list(auth_params)
    params = parse_keqv_list(items)

    # Now validate the params

    # Check for required parameters
    required = ["username", "realm", "nonce", "uri", "response"]
    for k in required:
        if k not in params:
            return None

    # If qop is sent then cnonce and nc MUST be present
    if "qop" in params and not ("cnonce" in params \
                                      and "nc" in params):
        return None

    # If qop is not sent, neither cnonce nor nc can be present
    if ("cnonce" in params or "nc" in params) and \
       "qop" not in params:
        return None

    return params


def _parseBasicAuthorization (auth_params):
    username, password = base64.decodestring (auth_params).split (":", 1)
    return {"username": username, "password": password}

AUTH_SCHEMES = {
    "basic": _parseBasicAuthorization,
    "digest": _parseDigestAuthorization,
}

def parseAuthorization (credentials):
    """parseAuthorization will convert the value of the 'Authorization' key in
    the HTTP header to a map itself. If the parsing fails 'None' is returned.
    """

    global AUTH_SCHEMES

    auth_scheme, auth_params = credentials.split(" ", 1)
    auth_scheme = auth_scheme.lower ()

    parser = AUTH_SCHEMES[auth_scheme]
    params = parser (auth_params)

    if params is None:
        return

    assert "auth_scheme" not in params
    params["auth_scheme"] = auth_scheme
    return params


################################################################################
# Check provided response for a valid password
#
def md5SessionKey (params, password):
    """
    If the "algorithm" directive's value is "MD5-sess", then A1 
    [the session key] is calculated only once - on the first request by the
    client following receipt of a WWW-Authenticate challenge from the server.

    This creates a 'session key' for the authentication of subsequent
    requests and responses which is different for each "authentication
    session", thus limiting the amount of material hashed with any one
    key.

    Because the server need only use the hash of the user
    credentials in order to create the A1 value, this construction could
    be used in conjunction with a third party authentication service so
    that the web server would not need the actual password value.  The
    specification of such a protocol is beyond the scope of this
    specification.
"""

    keys = ("username", "realm", "nonce", "cnonce")
    params_copy = {}
    for key in keys:
        params_copy[key] = params[key]

    params_copy["algorithm"] = MD5_SESS
    return _A1 (params_copy, password)

def _A1(params, password):
    algorithm = params.get ("algorithm", MD5)
    H = DIGEST_AUTH_ENCODERS[algorithm]

    if algorithm == MD5:
        # If the "algorithm" directive's value is "MD5" or is
        # unspecified, then A1 is:
        # A1 = unq(username-value) ":" unq(realm-value) ":" passwd
        return "%s:%s:%s" % (params["username"], params["realm"], password)

    elif algorithm == MD5_SESS:

        # This is A1 if qop is set
        # A1 = H( unq(username-value) ":" unq(realm-value) ":" passwd )
        #         ":" unq(nonce-value) ":" unq(cnonce-value)
        h_a1 = H ("%s:%s:%s" % (params["username"], params["realm"], password))
        return "%s:%s:%s" % (h_a1, params["nonce"], params["cnonce"])


def _A2(params, method, kwargs):
    # If the "qop" directive's value is "auth" or is unspecified, then A2 is:
    # A2 = Method ":" digest-uri-value

    qop = params.get ("qop", "auth")
    if qop == "auth":
        return method + ":" + params["uri"]
    elif qop == "auth-int":
        # If the "qop" value is "auth-int", then A2 is:
        # A2 = Method ":" digest-uri-value ":" H(entity-body)
        entity_body = kwargs.get ("entity_body", "")
        H = kwargs["H"]

        return "%s:%s:%s" % (
            method,
            params["uri"],
            H(entity_body)
        )

    else:
        raise NotImplementedError ("The 'qop' method is unknown: %s" % qop)

def _computeDigestResponse(auth_map, password, method="GET", A1=None, **kwargs):
    """
    Generates a response respecting the algorithm defined in RFC 2617
    """
    params = auth_map

    algorithm = params.get ("algorithm", MD5)

    H = DIGEST_AUTH_ENCODERS[algorithm]
    KD = lambda secret, data: H(secret + ":" + data)

    qop = params.get ("qop", None)

    H_A2 = H(_A2(params, method, kwargs))

    if algorithm == MD5_SESS and A1 is not None:
        H_A1 = H(A1)
    else:
        H_A1 = H(_A1(params, password))

    if qop in ("auth", "auth-int"):
        # If the "qop" value is "auth" or "auth-int":
        # request-digest  = <"> < KD ( H(A1),     unq(nonce-value)
        #                              ":" nc-value
        #                              ":" unq(cnonce-value)
        #                              ":" unq(qop-value)
        #                              ":" H(A2)
        #                      ) <">
        request = "%s:%s:%s:%s:%s" % (
            params["nonce"],
            params["nc"],
            params["cnonce"],
            params["qop"],
            H_A2,
        )
    elif qop is None:
        # If the "qop" directive is not present (this construction is
        # for compatibility with RFC 2069):
        # request-digest  =
        #         <"> < KD ( H(A1), unq(nonce-value) ":" H(A2) ) > <">
        request = "%s:%s" % (params["nonce"], H_A2)

    return KD(H_A1, request)

def _checkDigestResponse(auth_map, password, method="GET", A1=None, **kwargs):
    """This function is used to verify the response given by the client when
    he tries to authenticate.
    Optional arguments:
     entity_body - when 'qop' is set to 'auth-int' you MUST provide the
                   raw data you are going to send to the client (usually the
                   HTML page.
     request_uri - the uri from the request line compared with the 'uri'
                   directive of the authorization map. They must represent
                   the same resource (unused at this time).
    """

    if auth_map['realm'] != kwargs.get('realm', None):
        return False

    response = _computeDigestResponse(auth_map, password, method, A1, **kwargs)

    return response == auth_map["response"]

def _checkBasicResponse (auth_map, password, method='GET', encrypt=None, **kwargs):
    # Note that the Basic response doesn't provide the realm value so we cannot
    # test it
    try:
        return encrypt(auth_map["password"], auth_map["username"]) == password
    except TypeError:
        return encrypt(auth_map["password"]) == password

AUTH_RESPONSES = {
    "basic": _checkBasicResponse,
    "digest": _checkDigestResponse,
}

def checkResponse (auth_map, password, method="GET", encrypt=None, **kwargs):
    """'checkResponse' compares the auth_map with the password and optionally
    other arguments that each implementation might need.
    
    If the response is of type 'Basic' then the function has the following
    signature:
    
    checkBasicResponse (auth_map, password) -> bool
    
    If the response is of type 'Digest' then the function has the following
    signature:
    
    checkDigestResponse (auth_map, password, method = 'GET', A1 = None) -> bool
    
    The 'A1' argument is only used in MD5_SESS algorithm based responses.
    Check md5SessionKey() for more info.
    """
    global AUTH_RESPONSES
    checker = AUTH_RESPONSES[auth_map["auth_scheme"]]
    return checker (auth_map, password, method=method, encrypt=encrypt, **kwargs)
 




########NEW FILE########
__FILENAME__ = httputil
"""HTTP library functions."""

# This module contains functions for building an HTTP application
# framework: any one, not just one whose name starts with "Ch". ;) If you
# reference any modules from some popular framework inside *this* module,
# FuManChu will personally hang you up by your thumbs and submit you
# to a public caning.

from binascii import b2a_base64
from BaseHTTPServer import BaseHTTPRequestHandler
response_codes = BaseHTTPRequestHandler.responses.copy()

# From http://www.cherrypy.org/ticket/361
response_codes[500] = ('Internal Server Error',
                      'The server encountered an unexpected condition '
                      'which prevented it from fulfilling the request.')
response_codes[503] = ('Service Unavailable',
                      'The server is currently unable to handle the '
                      'request due to a temporary overloading or '
                      'maintenance of the server.')

import re
import urllib

from rfc822 import formatdate as HTTPDate


def urljoin(*atoms):
    """Return the given path *atoms, joined into a single URL.
    
    This will correctly join a SCRIPT_NAME and PATH_INFO into the
    original URL, even if either atom is blank.
    """
    url = "/".join([x for x in atoms if x])
    while "//" in url:
        url = url.replace("//", "/")
    # Special-case the final url of "", and return "/" instead.
    return url or "/"

def protocol_from_http(protocol_str):
    """Return a protocol tuple from the given 'HTTP/x.y' string."""
    return int(protocol_str[5]), int(protocol_str[7])

def get_ranges(headervalue, content_length):
    """Return a list of (start, stop) indices from a Range header, or None.
    
    Each (start, stop) tuple will be composed of two ints, which are suitable
    for use in a slicing operation. That is, the header "Range: bytes=3-6",
    if applied against a Python string, is requesting resource[3:7]. This
    function will return the list [(3, 7)].
    
    If this function returns an empty list, you should return HTTP 416.
    """
    
    if not headervalue:
        return None
    
    result = []
    bytesunit, byteranges = headervalue.split("=", 1)
    for brange in byteranges.split(","):
        start, stop = [x.strip() for x in brange.split("-", 1)]
        if start:
            if not stop:
                stop = content_length - 1
            start, stop = int(start), int(stop)
            if start >= content_length:
                # From rfc 2616 sec 14.16:
                # "If the server receives a request (other than one
                # including an If-Range request-header field) with an
                # unsatisfiable Range request-header field (that is,
                # all of whose byte-range-spec values have a first-byte-pos
                # value greater than the current length of the selected
                # resource), it SHOULD return a response code of 416
                # (Requested range not satisfiable)."
                continue
            if stop < start:
                # From rfc 2616 sec 14.16:
                # "If the server ignores a byte-range-spec because it
                # is syntactically invalid, the server SHOULD treat
                # the request as if the invalid Range header field
                # did not exist. (Normally, this means return a 200
                # response containing the full entity)."
                return None
            result.append((start, stop + 1))
        else:
            if not stop:
                # See rfc quote above.
                return None
            # Negative subscript (last N bytes)
            result.append((content_length - int(stop), content_length))
    
    return result


class HeaderElement(object):
    """An element (with parameters) from an HTTP header's element list."""
    
    def __init__(self, value, params=None):
        self.value = value
        if params is None:
            params = {}
        self.params = params
    
    def __cmp__(self, other):
        return cmp(self.value, other.value)
    
    def __unicode__(self):
        p = [";%s=%s" % (k, v) for k, v in self.params.iteritems()]
        return u"%s%s" % (self.value, "".join(p))
    
    def __str__(self):
        return str(self.__unicode__())
    
    def parse(elementstr):
        """Transform 'token;key=val' to ('token', {'key': 'val'})."""
        # Split the element into a value and parameters. The 'value' may
        # be of the form, "token=token", but we don't split that here.
        atoms = [x.strip() for x in elementstr.split(";") if x.strip()]
        if not atoms:
            initial_value = ''
        else:
            initial_value = atoms.pop(0).strip()
        params = {}
        for atom in atoms:
            atom = [x.strip() for x in atom.split("=", 1) if x.strip()]
            key = atom.pop(0)
            if atom:
                val = atom[0]
            else:
                val = ""
            params[key] = val
        return initial_value, params
    parse = staticmethod(parse)
    
    def from_str(cls, elementstr):
        """Construct an instance from a string of the form 'token;key=val'."""
        ival, params = cls.parse(elementstr)
        return cls(ival, params)
    from_str = classmethod(from_str)


q_separator = re.compile(r'; *q *=')

class AcceptElement(HeaderElement):
    """An element (with parameters) from an Accept* header's element list.
    
    AcceptElement objects are comparable; the more-preferred object will be
    "less than" the less-preferred object. They are also therefore sortable;
    if you sort a list of AcceptElement objects, they will be listed in
    priority order; the most preferred value will be first. Yes, it should
    have been the other way around, but it's too late to fix now.
    """
    
    def from_str(cls, elementstr):
        qvalue = None
        # The first "q" parameter (if any) separates the initial
        # media-range parameter(s) (if any) from the accept-params.
        atoms = q_separator.split(elementstr, 1)
        media_range = atoms.pop(0).strip()
        if atoms:
            # The qvalue for an Accept header can have extensions. The other
            # headers cannot, but it's easier to parse them as if they did.
            qvalue = HeaderElement.from_str(atoms[0].strip())
        
        media_type, params = cls.parse(media_range)
        if qvalue is not None:
            params["q"] = qvalue
        return cls(media_type, params)
    from_str = classmethod(from_str)
    
    def qvalue(self):
        val = self.params.get("q", "1")
        if isinstance(val, HeaderElement):
            val = val.value
        return float(val)
    qvalue = property(qvalue, doc="The qvalue, or priority, of this value.")
    
    def __cmp__(self, other):
        diff = cmp(self.qvalue, other.qvalue)
        if diff == 0:
            diff = cmp(str(self), str(other))
        return diff


def header_elements(fieldname, fieldvalue):
    """Return a sorted HeaderElement list from a comma-separated header str."""
    if not fieldvalue:
        return []
    
    result = []
    for element in fieldvalue.split(","):
        if fieldname.startswith("Accept") or fieldname == 'TE':
            hv = AcceptElement.from_str(element)
        else:
            hv = HeaderElement.from_str(element)
        result.append(hv)
    result.sort()
    result.reverse()
    return result

def decode_TEXT(value):
    """Decode RFC-2047 TEXT (e.g. "=?utf-8?q?f=C3=BCr?=" -> u"f\xfcr")."""
    from email.Header import decode_header
    atoms = decode_header(value)
    decodedvalue = ""
    for atom, charset in atoms:
        if charset is not None:
            atom = atom.decode(charset)
        decodedvalue += atom
    return decodedvalue

def valid_status(status):
    """Return legal HTTP status Code, Reason-phrase and Message.
    
    The status arg must be an int, or a str that begins with an int.
    
    If status is an int, or a str and no reason-phrase is supplied,
    a default reason-phrase will be provided.
    """
    
    if not status:
        status = 200
    
    status = str(status)
    parts = status.split(" ", 1)
    if len(parts) == 1:
        # No reason supplied.
        code, = parts
        reason = None
    else:
        code, reason = parts
        reason = reason.strip()
    
    try:
        code = int(code)
    except ValueError:
        raise ValueError("Illegal response status from server "
                         "(%s is non-numeric)." % repr(code))
    
    if code < 100 or code > 599:
        raise ValueError("Illegal response status from server "
                         "(%s is out of range)." % repr(code))
    
    if code not in response_codes:
        # code is unknown but not illegal
        default_reason, message = "", ""
    else:
        default_reason, message = response_codes[code]
    
    if reason is None:
        reason = default_reason
    
    return code, reason, message


def _parse_qs(qs, keep_blank_values=0, strict_parsing=0, encoding='utf-8'):
    """Parse a query given as a string argument.

    Arguments:

    qs: URL-encoded query string to be parsed

    keep_blank_values: flag indicating whether blank values in
        URL encoded queries should be treated as blank strings.  A
        true value indicates that blanks should be retained as blank
        strings.  The default false value indicates that blank values
        are to be ignored and treated as if they were  not included.

    strict_parsing: flag indicating what to do with parsing errors. If
        false (the default), errors are silently ignored. If true,
        errors raise a ValueError exception.

    Returns a dict, as G-d intended.
    """
    pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]
    d = {}
    for name_value in pairs:
        if not name_value and not strict_parsing:
            continue
        nv = name_value.split('=', 1)
        if len(nv) != 2:
            if strict_parsing:
                raise ValueError("bad query field: %r" % (name_value,))
            # Handle case of a control-name with no equal sign
            if keep_blank_values:
                nv.append('')
            else:
                continue
        if len(nv[1]) or keep_blank_values:
            name = urllib.unquote(nv[0].replace('+', ' '))
            name = name.decode(encoding, 'strict')
            value = urllib.unquote(nv[1].replace('+', ' '))
            value = value.decode(encoding, 'strict')
            if name in d:
                if not isinstance(d[name], list):
                    d[name] = [d[name]]
                d[name].append(value)
            else:
                d[name] = value
    return d


image_map_pattern = re.compile(r"[0-9]+,[0-9]+")

def parse_query_string(query_string, keep_blank_values=True, encoding='utf-8'):
    """Build a params dictionary from a query_string.
    
    Duplicate key/value pairs in the provided query_string will be
    returned as {'key': [val1, val2, ...]}. Single key/values will
    be returned as strings: {'key': 'value'}.
    """
    if image_map_pattern.match(query_string):
        # Server-side image map. Map the coords to 'x' and 'y'
        # (like CGI::Request does).
        pm = query_string.split(",")
        pm = {'x': int(pm[0]), 'y': int(pm[1])}
    else:
        pm = _parse_qs(query_string, keep_blank_values, encoding=encoding)
    return pm


class CaseInsensitiveDict(dict):
    """A case-insensitive dict subclass.
    
    Each key is changed on entry to str(key).title().
    """
    
    def __getitem__(self, key):
        return dict.__getitem__(self, str(key).title())
    
    def __setitem__(self, key, value):
        dict.__setitem__(self, str(key).title(), value)
    
    def __delitem__(self, key):
        dict.__delitem__(self, str(key).title())
    
    def __contains__(self, key):
        return dict.__contains__(self, str(key).title())
    
    def get(self, key, default=None):
        return dict.get(self, str(key).title(), default)
    
    def has_key(self, key):
        return dict.has_key(self, str(key).title())
    
    def update(self, E):
        for k in E.keys():
            self[str(k).title()] = E[k]
    
    def fromkeys(cls, seq, value=None):
        newdict = cls()
        for k in seq:
            newdict[str(k).title()] = value
        return newdict
    fromkeys = classmethod(fromkeys)
    
    def setdefault(self, key, x=None):
        key = str(key).title()
        try:
            return self[key]
        except KeyError:
            self[key] = x
            return x
    
    def pop(self, key, default):
        return dict.pop(self, str(key).title(), default)


class HeaderMap(CaseInsensitiveDict):
    """A dict subclass for HTTP request and response headers.
    
    Each key is changed on entry to str(key).title(). This allows headers
    to be case-insensitive and avoid duplicates.
    
    Values are header values (decoded according to RFC 2047 if necessary).
    """
    
    protocol = (1, 1)
    
    def elements(self, key):
        """Return a sorted list of HeaderElements for the given header."""
        key = str(key).title()
        value = self.get(key)
        return header_elements(key, value)
    
    def values(self, key):
        """Return a sorted list of HeaderElement.value for the given header."""
        return [e.value for e in self.elements(key)]
    
    def output(self):
        """Transform self into a list of (name, value) tuples."""
        header_list = []
        for k, v in self.items():
            if isinstance(k, unicode):
                k = k.encode("ISO-8859-1")
            
            if not isinstance(v, basestring):
                v = str(v)
            
            if isinstance(v, unicode):
                v = self.encode(v)
            header_list.append((k, v))
        return header_list
    
    def encode(self, v):
        """Return the given header value, encoded for HTTP output."""
        # HTTP/1.0 says, "Words of *TEXT may contain octets 
        # from character sets other than US-ASCII." and 
        # "Recipients of header field TEXT containing octets 
        # outside the US-ASCII character set may assume that 
        # they represent ISO-8859-1 characters." 
        try:
            v = v.encode("ISO-8859-1")
        except UnicodeEncodeError:
            if self.protocol == (1, 1):
                # Encode RFC-2047 TEXT 
                # (e.g. u"\u8200" -> "=?utf-8?b?6IiA?="). 
                # We do our own here instead of using the email module
                # because we never want to fold lines--folding has
                # been deprecated by the HTTP working group.
                v = b2a_base64(v.encode('utf-8'))
                v = ('=?utf-8?b?' + v.strip('\n') + '?=')
            else:
                raise
        return v

class Host(object):
    """An internet address.
    
    name should be the client's host name. If not available (because no DNS
        lookup is performed), the IP address should be used instead.
    """
    
    ip = "0.0.0.0"
    port = 80
    name = "unknown.tld"
    
    def __init__(self, ip, port, name=None):
        self.ip = ip
        self.port = port
        if name is None:
            name = ip
        self.name = name
    
    def __repr__(self):
        return "httputil.Host(%r, %r, %r)" % (self.ip, self.port, self.name)

########NEW FILE########
__FILENAME__ = jsontools
import sys
import cherrypy

if sys.version_info >= (2, 6):
    # Python 2.6: simplejson is part of the standard library
    import json
else:
    try:
        import simplejson as json
    except ImportError:
        json = None

if json is None:
    def json_decode(s):
        raise ValueError('No JSON library is available')
    def json_encode(s):
        raise ValueError('No JSON library is available')
else:
    json_decode = json.JSONDecoder().decode
    json_encode = json.JSONEncoder().iterencode

def json_in(force=True, debug=False):
    request = cherrypy.serving.request
    def json_processor(entity):
        """Read application/json data into request.json."""
        if not entity.headers.get(u"Content-Length", u""):
            raise cherrypy.HTTPError(411)
        
        body = entity.fp.read()
        try:
            request.json = json_decode(body)
        except ValueError:
            raise cherrypy.HTTPError(400, 'Invalid JSON document')
    if force:
        request.body.processors.clear()
        request.body.default_proc = cherrypy.HTTPError(
            415, 'Expected an application/json content type')
    request.body.processors[u'application/json'] = json_processor

def json_out(debug=False):
    request = cherrypy.serving.request
    response = cherrypy.serving.response
    
    real_handler = request.handler
    def json_handler(*args, **kwargs):
        response.headers['Content-Type'] = 'application/json'
        value = real_handler(*args, **kwargs)
        return json_encode(value)
    request.handler = json_handler


########NEW FILE########
__FILENAME__ = profiler
"""Profiler tools for CherryPy.

CherryPy users
==============

You can profile any of your pages as follows:

    from cherrypy.lib import profiler
    
    class Root:
        p = profile.Profiler("/path/to/profile/dir")
        
        def index(self):
            self.p.run(self._index)
        index.exposed = True
        
        def _index(self):
            return "Hello, world!"
    
    cherrypy.tree.mount(Root())


You can also turn on profiling for all requests
using the make_app function as WSGI middleware.


CherryPy developers
===================

This module can be used whenever you make changes to CherryPy,
to get a quick sanity-check on overall CP performance. Use the
"--profile" flag when running the test suite. Then, use the serve()
function to browse the results in a web browser. If you run this
module from the command line, it will call serve() for you.

"""


# Make profiler output more readable by adding __init__ modules' parents.
def new_func_strip_path(func_name):
    filename, line, name = func_name
    if filename.endswith("__init__.py"):
        return os.path.basename(filename[:-12]) + filename[-12:], line, name
    return os.path.basename(filename), line, name

try:
    import profile
    import pstats
    pstats.func_strip_path = new_func_strip_path
except ImportError:
    profile = None
    pstats = None

import os, os.path
import sys
import warnings

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

_count = 0

class Profiler(object):
    
    def __init__(self, path=None):
        if not path:
            path = os.path.join(os.path.dirname(__file__), "profile")
        self.path = path
        if not os.path.exists(path):
            os.makedirs(path)
    
    def run(self, func, *args, **params):
        """Dump profile data into self.path."""
        global _count
        c = _count = _count + 1
        path = os.path.join(self.path, "cp_%04d.prof" % c)
        prof = profile.Profile()
        result = prof.runcall(func, *args, **params)
        prof.dump_stats(path)
        return result
    
    def statfiles(self):
        """statfiles() -> list of available profiles."""
        return [f for f in os.listdir(self.path)
                if f.startswith("cp_") and f.endswith(".prof")]
    
    def stats(self, filename, sortby='cumulative'):
        """stats(index) -> output of print_stats() for the given profile."""
        sio = StringIO()
        if sys.version_info >= (2, 5):
            s = pstats.Stats(os.path.join(self.path, filename), stream=sio)
            s.strip_dirs()
            s.sort_stats(sortby)
            s.print_stats()
        else:
            # pstats.Stats before Python 2.5 didn't take a 'stream' arg,
            # but just printed to stdout. So re-route stdout.
            s = pstats.Stats(os.path.join(self.path, filename))
            s.strip_dirs()
            s.sort_stats(sortby)
            oldout = sys.stdout
            try:
                sys.stdout = sio
                s.print_stats()
            finally:
                sys.stdout = oldout
        response = sio.getvalue()
        sio.close()
        return response
    
    def index(self):
        return """<html>
        <head><title>CherryPy profile data</title></head>
        <frameset cols='200, 1*'>
            <frame src='menu' />
            <frame name='main' src='' />
        </frameset>
        </html>
        """
    index.exposed = True
    
    def menu(self):
        yield "<h2>Profiling runs</h2>"
        yield "<p>Click on one of the runs below to see profiling data.</p>"
        runs = self.statfiles()
        runs.sort()
        for i in runs:
            yield "<a href='report?filename=%s' target='main'>%s</a><br />" % (i, i)
    menu.exposed = True
    
    def report(self, filename):
        import cherrypy
        cherrypy.response.headers['Content-Type'] = 'text/plain'
        return self.stats(filename)
    report.exposed = True


class ProfileAggregator(Profiler):
    
    def __init__(self, path=None):
        Profiler.__init__(self, path)
        global _count
        self.count = _count = _count + 1
        self.profiler = profile.Profile()
    
    def run(self, func, *args):
        path = os.path.join(self.path, "cp_%04d.prof" % self.count)
        result = self.profiler.runcall(func, *args)
        self.profiler.dump_stats(path)
        return result


class make_app:
    def __init__(self, nextapp, path=None, aggregate=False):
        """Make a WSGI middleware app which wraps 'nextapp' with profiling.
        
        nextapp: the WSGI application to wrap, usually an instance of
            cherrypy.Application.
        path: where to dump the profiling output.
        aggregate: if True, profile data for all HTTP requests will go in
            a single file. If False (the default), each HTTP request will
            dump its profile data into a separate file.
        """
        if profile is None or pstats is None:
            msg = ("Your installation of Python does not have a profile module. "
                   "If you're on Debian, try `sudo apt-get install python-profiler`. "
                   "See http://www.cherrypy.org/wiki/ProfilingOnDebian for details.")
            warnings.warn(msg)
        
        self.nextapp = nextapp
        self.aggregate = aggregate
        if aggregate:
            self.profiler = ProfileAggregator(path)
        else:
            self.profiler = Profiler(path)
    
    def __call__(self, environ, start_response):
        def gather():
            result = []
            for line in self.nextapp(environ, start_response):
                result.append(line)
            return result
        return self.profiler.run(gather)


def serve(path=None, port=8080):
    if profile is None or pstats is None:
        msg = ("Your installation of Python does not have a profile module. "
               "If you're on Debian, try `sudo apt-get install python-profiler`. "
               "See http://www.cherrypy.org/wiki/ProfilingOnDebian for details.")
        warnings.warn(msg)
    
    import cherrypy
    cherrypy.config.update({'server.socket_port': int(port),
                            'server.thread_pool': 10,
                            'environment': "production",
                            })
    cherrypy.quickstart(Profiler(path))


if __name__ == "__main__":
    serve(*tuple(sys.argv[1:]))


########NEW FILE########
__FILENAME__ = reprconf
"""Generic configuration system using unrepr.

Configuration data may be supplied as a Python dictionary, as a filename,
or as an open file object. When you supply a filename or file, Python's
builtin ConfigParser is used (with some extensions).

Namespaces
----------

Configuration keys are separated into namespaces by the first "." in the key.

The only key that cannot exist in a namespace is the "environment" entry.
This special entry 'imports' other config entries from a template stored in
the Config.environments dict.

You can define your own namespaces to be called when new config is merged
by adding a named handler to Config.namespaces. The name can be any string,
and the handler must be either a callable or a context manager.
"""

from ConfigParser import ConfigParser
try:
    set
except NameError:
    from sets import Set as set
import sys

def as_dict(config):
    """Return a dict from 'config' whether it is a dict, file, or filename."""
    if isinstance(config, basestring):
        config = Parser().dict_from_file(config)
    elif hasattr(config, 'read'):
        config = Parser().dict_from_file(config)
    return config


class NamespaceSet(dict):
    """A dict of config namespace names and handlers.
    
    Each config entry should begin with a namespace name; the corresponding
    namespace handler will be called once for each config entry in that
    namespace, and will be passed two arguments: the config key (with the
    namespace removed) and the config value.
    
    Namespace handlers may be any Python callable; they may also be
    Python 2.5-style 'context managers', in which case their __enter__
    method should return a callable to be used as the handler.
    See cherrypy.tools (the Toolbox class) for an example.
    """
    
    def __call__(self, config):
        """Iterate through config and pass it to each namespace handler.
        
        'config' should be a flat dict, where keys use dots to separate
        namespaces, and values are arbitrary.
        
        The first name in each config key is used to look up the corresponding
        namespace handler. For example, a config entry of {'tools.gzip.on': v}
        will call the 'tools' namespace handler with the args: ('gzip.on', v)
        """
        # Separate the given config into namespaces
        ns_confs = {}
        for k in config:
            if "." in k:
                ns, name = k.split(".", 1)
                bucket = ns_confs.setdefault(ns, {})
                bucket[name] = config[k]
        
        # I chose __enter__ and __exit__ so someday this could be
        # rewritten using Python 2.5's 'with' statement:
        # for ns, handler in self.iteritems():
        #     with handler as callable:
        #         for k, v in ns_confs.get(ns, {}).iteritems():
        #             callable(k, v)
        for ns, handler in self.items():
            exit = getattr(handler, "__exit__", None)
            if exit:
                callable = handler.__enter__()
                no_exc = True
                try:
                    try:
                        for k, v in ns_confs.get(ns, {}).items():
                            callable(k, v)
                    except:
                        # The exceptional case is handled here
                        no_exc = False
                        if exit is None:
                            raise
                        if not exit(*sys.exc_info()):
                            raise
                        # The exception is swallowed if exit() returns true
                finally:
                    # The normal and non-local-goto cases are handled here
                    if no_exc and exit:
                        exit(None, None, None)
            else:
                for k, v in ns_confs.get(ns, {}).items():
                    handler(k, v)
    
    def __repr__(self):
        return "%s.%s(%s)" % (self.__module__, self.__class__.__name__,
                              dict.__repr__(self))
    
    def __copy__(self):
        newobj = self.__class__()
        newobj.update(self)
        return newobj
    copy = __copy__


class Config(dict):
    """A dict-like set of configuration data, with defaults and namespaces.
    
    May take a file, filename, or dict.
    """
    
    defaults = {}
    environments = {}
    namespaces = NamespaceSet()
    
    def __init__(self, file=None, **kwargs):
        self.reset()
        if file is not None:
            self.update(file)
        if kwargs:
            self.update(kwargs)
    
    def reset(self):
        """Reset self to default values."""
        self.clear()
        dict.update(self, self.defaults)
    
    def update(self, config):
        """Update self from a dict, file or filename."""
        if isinstance(config, basestring):
            # Filename
            config = Parser().dict_from_file(config)
        elif hasattr(config, 'read'):
            # Open file object
            config = Parser().dict_from_file(config)
        else:
            config = config.copy()
        self._apply(config)
    
    def _apply(self, config):
        """Update self from a dict."""
        which_env = config.get('environment')
        if which_env:
            env = self.environments[which_env]
            for k in env:
                if k not in config:
                    config[k] = env[k]
        
        dict.update(self, config)
        self.namespaces(config)
    
    def __setitem__(self, k, v):
        dict.__setitem__(self, k, v)
        self.namespaces({k: v})


class Parser(ConfigParser):
    """Sub-class of ConfigParser that keeps the case of options and that raises
    an exception if the file cannot be read.
    """
    
    def optionxform(self, optionstr):
        return optionstr
    
    def read(self, filenames):
        if isinstance(filenames, basestring):
            filenames = [filenames]
        for filename in filenames:
            # try:
            #     fp = open(filename)
            # except IOError:
            #     continue
            fp = open(filename)
            try:
                self._read(fp, filename)
            finally:
                fp.close()
    
    def as_dict(self, raw=False, vars=None):
        """Convert an INI file to a dictionary"""
        # Load INI file into a dict
        result = {}
        for section in self.sections():
            if section not in result:
                result[section] = {}
            for option in self.options(section):
                value = self.get(section, option, raw, vars)
                try:
                    value = unrepr(value)
                except Exception, x:
                    msg = ("Config error in section: %r, option: %r, "
                           "value: %r. Config values must be valid Python." % 
                           (section, option, value))
                    raise ValueError(msg, x.__class__.__name__, x.args)
                result[section][option] = value
        return result
    
    def dict_from_file(self, file):
        if hasattr(file, 'read'):
            self.readfp(file)
        else:
            self.read(file)
        return self.as_dict()


# public domain "unrepr" implementation, found on the web and then improved.

class _Builder:
    
    def build(self, o):
        m = getattr(self, 'build_' + o.__class__.__name__, None)
        if m is None:
            raise TypeError("unrepr does not recognize %s" % 
                            repr(o.__class__.__name__))
        return m(o)
    
    def build_Subscript(self, o):
        expr, flags, subs = o.getChildren()
        expr = self.build(expr)
        subs = self.build(subs)
        return expr[subs]
    
    def build_CallFunc(self, o):
        children = map(self.build, o.getChildren())
        callee = children.pop(0)
        kwargs = children.pop() or {}
        starargs = children.pop() or ()
        args = tuple(children) + tuple(starargs)
        return callee(*args, **kwargs)
    
    def build_List(self, o):
        return map(self.build, o.getChildren())
    
    def build_Const(self, o):
        return o.value
    
    def build_Dict(self, o):
        d = {}
        i = iter(map(self.build, o.getChildren()))
        for el in i:
            d[el] = i.next()
        return d
    
    def build_Tuple(self, o):
        return tuple(self.build_List(o))
    
    def build_Name(self, o):
        name = o.name
        if name == 'None':
            return None
        if name == 'True':
            return True
        if name == 'False':
            return False
        
        # See if the Name is a package or module. If it is, import it.
        try:
            return modules(name)
        except ImportError:
            pass
        
        # See if the Name is in builtins.
        try:
            import __builtin__
            return getattr(__builtin__, name)
        except AttributeError:
            pass
        
        raise TypeError("unrepr could not resolve the name %s" % repr(name))
    
    def build_Add(self, o):
        left, right = map(self.build, o.getChildren())
        return left + right
    
    def build_Getattr(self, o):
        parent = self.build(o.expr)
        return getattr(parent, o.attrname)
    
    def build_NoneType(self, o):
        return None
    
    def build_UnarySub(self, o):
        return - self.build(o.getChildren()[0])
    
    def build_UnaryAdd(self, o):
        return self.build(o.getChildren()[0])


def _astnode(s):
    """Return a Python ast Node compiled from a string."""
    try:
        import compiler
    except ImportError:
        # Fallback to eval when compiler package is not available,
        # e.g. IronPython 1.0.
        return eval(s)
    
    p = compiler.parse("__tempvalue__ = " + s)
    return p.getChildren()[1].getChildren()[0].getChildren()[1]
    

def unrepr(s):
    """Return a Python object compiled from a string."""
    if not s:
        return s
    obj = _astnode(s)
    return _Builder().build(obj)


def modules(modulePath):
    """Load a module and retrieve a reference to that module."""
    try:
        mod = sys.modules[modulePath]
        if mod is None:
            raise KeyError()
    except KeyError:
        # The last [''] is important.
        mod = __import__(modulePath, globals(), locals(), [''])
    return mod

def attributes(full_attribute_name):
    """Load a module and retrieve an attribute of that module."""
    
    # Parse out the path, module, and attribute
    last_dot = full_attribute_name.rfind(".")
    attr_name = full_attribute_name[last_dot + 1:]
    mod_path = full_attribute_name[:last_dot]
    
    mod = modules(mod_path)
    # Let an AttributeError propagate outward.
    try:
        attr = getattr(mod, attr_name)
    except AttributeError:
        raise AttributeError("'%s' object has no attribute '%s'"
                             % (mod_path, attr_name))
    
    # Return a reference to the attribute.
    return attr



########NEW FILE########
__FILENAME__ = sessions
"""Session implementation for CherryPy.

We use cherrypy.request to store some convenient variables as
well as data about the session for the current request. Instead of
polluting cherrypy.request we use a Session object bound to
cherrypy.session to store these variables.
"""

import datetime
import os
try:
    import cPickle as pickle
except ImportError:
    import pickle
import random
try:
    # Python 2.5+
    from hashlib import sha1 as sha
except ImportError:
    from sha import new as sha
import time
import threading
import types
from warnings import warn

import cherrypy
from cherrypy.lib import httputil


missing = object()

class Session(object):
    """A CherryPy dict-like Session object (one per request)."""
    
    __metaclass__ = cherrypy._AttributeDocstrings
    
    _id = None
    id_observers = None
    id_observers__doc = "A list of callbacks to which to pass new id's."
    
    id__doc = "The current session ID."
    def _get_id(self):
        return self._id
    def _set_id(self, value):
        self._id = value
        for o in self.id_observers:
            o(value)
    id = property(_get_id, _set_id, doc=id__doc)
    
    timeout = 60
    timeout__doc = "Number of minutes after which to delete session data."
    
    locked = False
    locked__doc = """
    If True, this session instance has exclusive read/write access
    to session data."""
    
    loaded = False
    loaded__doc = """
    If True, data has been retrieved from storage. This should happen
    automatically on the first attempt to access session data."""
    
    clean_thread = None
    clean_thread__doc = "Class-level Monitor which calls self.clean_up."
    
    clean_freq = 5
    clean_freq__doc = "The poll rate for expired session cleanup in minutes."
    
    originalid = None
    originalid__doc = "The session id passed by the client. May be missing or unsafe."
    
    missing = False
    missing__doc = "True if the session requested by the client did not exist."
    
    regenerated = False
    regenerated__doc = """
    True if the application called session.regenerate(). This is not set by
    internal calls to regenerate the session id."""
    
    debug = False
    
    def __init__(self, id=None, **kwargs):
        self.id_observers = []
        self._data = {}
        
        for k, v in kwargs.items():
            setattr(self, k, v)
        
        self.originalid = id
        self.missing = False
        if id is None:
            if self.debug:
                cherrypy.log('No id given; making a new one', 'TOOLS.SESSIONS')
            self._regenerate()
        else:
            self.id = id
            if not self._exists():
                if self.debug:
                    cherrypy.log('Expired or malicious session %r; '
                                 'making a new one' % id, 'TOOLS.SESSIONS')
                # Expired or malicious session. Make a new one.
                # See http://www.cherrypy.org/ticket/709.
                self.id = None
                self.missing = True
                self._regenerate()
    
    def regenerate(self):
        """Replace the current session (with a new id)."""
        self.regenerated = True
        self._regenerate()
    
    def _regenerate(self):
        if self.id is not None:
            self.delete()
        
        old_session_was_locked = self.locked
        if old_session_was_locked:
            self.release_lock()
        
        self.id = None
        while self.id is None:
            self.id = self.generate_id()
            # Assert that the generated id is not already stored.
            if self._exists():
                self.id = None
        
        if old_session_was_locked:
            self.acquire_lock()
    
    def clean_up(self):
        """Clean up expired sessions."""
        pass
    
    try:
        os.urandom(20)
    except (AttributeError, NotImplementedError):
        # os.urandom not available until Python 2.4. Fall back to random.random.
        def generate_id(self):
            """Return a new session id."""
            return sha('%s' % random.random()).hexdigest()
    else:
        def generate_id(self):
            """Return a new session id."""
            return os.urandom(20).encode('hex')
    
    def save(self):
        """Save session data."""
        try:
            # If session data has never been loaded then it's never been
            #   accessed: no need to save it
            if self.loaded:
                t = datetime.timedelta(seconds=self.timeout * 60)
                expiration_time = datetime.datetime.now() + t
                if self.debug:
                    cherrypy.log('Saving with expiry %s' % expiration_time,
                                 'TOOLS.SESSIONS')
                self._save(expiration_time)
            
        finally:
            if self.locked:
                # Always release the lock if the user didn't release it
                self.release_lock()
    
    def load(self):
        """Copy stored session data into this session instance."""
        data = self._load()
        # data is either None or a tuple (session_data, expiration_time)
        if data is None or data[1] < datetime.datetime.now():
            if self.debug:
                cherrypy.log('Expired session, flushing data', 'TOOLS.SESSIONS')
            self._data = {}
        else:
            self._data = data[0]
        self.loaded = True
        
        # Stick the clean_thread in the class, not the instance.
        # The instances are created and destroyed per-request.
        cls = self.__class__
        if self.clean_freq and not cls.clean_thread:
            # clean_up is in instancemethod and not a classmethod,
            # so that tool config can be accessed inside the method.
            t = cherrypy.process.plugins.Monitor(
                cherrypy.engine, self.clean_up, self.clean_freq * 60,
                name='Session cleanup')
            t.subscribe()
            cls.clean_thread = t
            t.start()
    
    def delete(self):
        """Delete stored session data."""
        self._delete()
    
    def __getitem__(self, key):
        if not self.loaded: self.load()
        return self._data[key]
    
    def __setitem__(self, key, value):
        if not self.loaded: self.load()
        self._data[key] = value
    
    def __delitem__(self, key):
        if not self.loaded: self.load()
        del self._data[key]
    
    def pop(self, key, default=missing):
        """Remove the specified key and return the corresponding value.
        If key is not found, default is returned if given,
        otherwise KeyError is raised.
        """
        if not self.loaded: self.load()
        if default is missing:
            return self._data.pop(key)
        else:
            return self._data.pop(key, default)
    
    def __contains__(self, key):
        if not self.loaded: self.load()
        return key in self._data
    
    def has_key(self, key):
        """D.has_key(k) -> True if D has a key k, else False."""
        if not self.loaded: self.load()
        return key in self._data
    
    def get(self, key, default=None):
        """D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None."""
        if not self.loaded: self.load()
        return self._data.get(key, default)
    
    def update(self, d):
        """D.update(E) -> None.  Update D from E: for k in E: D[k] = E[k]."""
        if not self.loaded: self.load()
        self._data.update(d)
    
    def setdefault(self, key, default=None):
        """D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D."""
        if not self.loaded: self.load()
        return self._data.setdefault(key, default)
    
    def clear(self):
        """D.clear() -> None.  Remove all items from D."""
        if not self.loaded: self.load()
        self._data.clear()
    
    def keys(self):
        """D.keys() -> list of D's keys."""
        if not self.loaded: self.load()
        return self._data.keys()
    
    def items(self):
        """D.items() -> list of D's (key, value) pairs, as 2-tuples."""
        if not self.loaded: self.load()
        return self._data.items()
    
    def values(self):
        """D.values() -> list of D's values."""
        if not self.loaded: self.load()
        return self._data.values()


class RamSession(Session):
    
    # Class-level objects. Don't rebind these!
    cache = {}
    locks = {}
    
    def clean_up(self):
        """Clean up expired sessions."""
        now = datetime.datetime.now()
        for id, (data, expiration_time) in self.cache.items():
            if expiration_time <= now:
                try:
                    del self.cache[id]
                except KeyError:
                    pass
                try:
                    del self.locks[id]
                except KeyError:
                    pass
    
    def _exists(self):
        return self.id in self.cache
    
    def _load(self):
        return self.cache.get(self.id)
    
    def _save(self, expiration_time):
        self.cache[self.id] = (self._data, expiration_time)
    
    def _delete(self):
        self.cache.pop(self.id, None)
    
    def acquire_lock(self):
        """Acquire an exclusive lock on the currently-loaded session data."""
        self.locked = True
        self.locks.setdefault(self.id, threading.RLock()).acquire()
    
    def release_lock(self):
        """Release the lock on the currently-loaded session data."""
        self.locks[self.id].release()
        self.locked = False
    
    def __len__(self):
        """Return the number of active sessions."""
        return len(self.cache)


class FileSession(Session):
    """Implementation of the File backend for sessions
    
    storage_path: the folder where session data will be saved. Each session
        will be saved as pickle.dump(data, expiration_time) in its own file;
        the filename will be self.SESSION_PREFIX + self.id.
    """
    
    SESSION_PREFIX = 'session-'
    LOCK_SUFFIX = '.lock'
    pickle_protocol = pickle.HIGHEST_PROTOCOL
    
    def __init__(self, id=None, **kwargs):
        # The 'storage_path' arg is required for file-based sessions.
        kwargs['storage_path'] = os.path.abspath(kwargs['storage_path'])
        Session.__init__(self, id=id, **kwargs)
    
    def setup(cls, **kwargs):
        """Set up the storage system for file-based sessions.
        
        This should only be called once per process; this will be done
        automatically when using sessions.init (as the built-in Tool does).
        """
        # The 'storage_path' arg is required for file-based sessions.
        kwargs['storage_path'] = os.path.abspath(kwargs['storage_path'])
        
        for k, v in kwargs.items():
            setattr(cls, k, v)
        
        # Warn if any lock files exist at startup.
        lockfiles = [fname for fname in os.listdir(cls.storage_path)
                     if (fname.startswith(cls.SESSION_PREFIX)
                         and fname.endswith(cls.LOCK_SUFFIX))]
        if lockfiles:
            plural = ('', 's')[len(lockfiles) > 1]
            warn("%s session lockfile%s found at startup. If you are "
                 "only running one process, then you may need to "
                 "manually delete the lockfiles found at %r."
                 % (len(lockfiles), plural, cls.storage_path))
    setup = classmethod(setup)
    
    def _get_file_path(self):
        f = os.path.join(self.storage_path, self.SESSION_PREFIX + self.id)
        if not os.path.abspath(f).startswith(self.storage_path):
            raise cherrypy.HTTPError(400, "Invalid session id in cookie.")
        return f
    
    def _exists(self):
        path = self._get_file_path()
        return os.path.exists(path)
    
    def _load(self, path=None):
        if path is None:
            path = self._get_file_path()
        try:
            f = open(path, "rb")
            try:
                return pickle.load(f)
            finally:
                f.close()
        except (IOError, EOFError):
            return None
    
    def _save(self, expiration_time):
        f = open(self._get_file_path(), "wb")
        try:
            pickle.dump((self._data, expiration_time), f, self.pickle_protocol)
        finally:
            f.close()
    
    def _delete(self):
        try:
            os.unlink(self._get_file_path())
        except OSError:
            pass
    
    def acquire_lock(self, path=None):
        """Acquire an exclusive lock on the currently-loaded session data."""
        if path is None:
            path = self._get_file_path()
        path += self.LOCK_SUFFIX
        while True:
            try:
                lockfd = os.open(path, os.O_CREAT | os.O_WRONLY | os.O_EXCL)
            except OSError:
                time.sleep(0.1)
            else:
                os.close(lockfd) 
                break
        self.locked = True
    
    def release_lock(self, path=None):
        """Release the lock on the currently-loaded session data."""
        if path is None:
            path = self._get_file_path()
        os.unlink(path + self.LOCK_SUFFIX)
        self.locked = False
    
    def clean_up(self):
        """Clean up expired sessions."""
        now = datetime.datetime.now()
        # Iterate over all session files in self.storage_path
        for fname in os.listdir(self.storage_path):
            if (fname.startswith(self.SESSION_PREFIX)
                and not fname.endswith(self.LOCK_SUFFIX)):
                # We have a session file: lock and load it and check
                #   if it's expired. If it fails, nevermind.
                path = os.path.join(self.storage_path, fname)
                self.acquire_lock(path)
                try:
                    contents = self._load(path)
                    # _load returns None on IOError
                    if contents is not None:
                        data, expiration_time = contents
                        if expiration_time < now:
                            # Session expired: deleting it
                            os.unlink(path)
                finally:
                    self.release_lock(path)
    
    def __len__(self):
        """Return the number of active sessions."""
        return len([fname for fname in os.listdir(self.storage_path)
                    if (fname.startswith(self.SESSION_PREFIX)
                        and not fname.endswith(self.LOCK_SUFFIX))])


class PostgresqlSession(Session):
    """ Implementation of the PostgreSQL backend for sessions. It assumes
        a table like this:

            create table session (
                id varchar(40),
                data text,
                expiration_time timestamp
            )
    
    You must provide your own get_db function.
    """
    
    pickle_protocol = pickle.HIGHEST_PROTOCOL
    
    def __init__(self, id=None, **kwargs):
        Session.__init__(self, id, **kwargs)
        self.cursor = self.db.cursor()
    
    def setup(cls, **kwargs):
        """Set up the storage system for Postgres-based sessions.
        
        This should only be called once per process; this will be done
        automatically when using sessions.init (as the built-in Tool does).
        """
        for k, v in kwargs.items():
            setattr(cls, k, v)
        
        self.db = self.get_db()
    setup = classmethod(setup)
    
    def __del__(self):
        if self.cursor:
            self.cursor.close()
        self.db.commit()
    
    def _exists(self):
        # Select session data from table
        self.cursor.execute('select data, expiration_time from session '
                            'where id=%s', (self.id,))
        rows = self.cursor.fetchall()
        return bool(rows)
    
    def _load(self):
        # Select session data from table
        self.cursor.execute('select data, expiration_time from session '
                            'where id=%s', (self.id,))
        rows = self.cursor.fetchall()
        if not rows:
            return None
        
        pickled_data, expiration_time = rows[0]
        data = pickle.loads(pickled_data)
        return data, expiration_time
    
    def _save(self, expiration_time):
        pickled_data = pickle.dumps(self._data, self.pickle_protocol)
        self.cursor.execute('update session set data = %s, '
                            'expiration_time = %s where id = %s',
                            (pickled_data, expiration_time, self.id))
    
    def _delete(self):
        self.cursor.execute('delete from session where id=%s', (self.id,))
   
    def acquire_lock(self):
        """Acquire an exclusive lock on the currently-loaded session data."""
        # We use the "for update" clause to lock the row
        self.locked = True
        self.cursor.execute('select id from session where id=%s for update',
                            (self.id,))
    
    def release_lock(self):
        """Release the lock on the currently-loaded session data."""
        # We just close the cursor and that will remove the lock
        #   introduced by the "for update" clause
        self.cursor.close()
        self.locked = False
    
    def clean_up(self):
        """Clean up expired sessions."""
        self.cursor.execute('delete from session where expiration_time < %s',
                            (datetime.datetime.now(),))


class MemcachedSession(Session):
    
    # The most popular memcached client for Python isn't thread-safe.
    # Wrap all .get and .set operations in a single lock.
    mc_lock = threading.RLock()
    
    # This is a seperate set of locks per session id.
    locks = {}
    
    servers = ['127.0.0.1:11211']
    
    def setup(cls, **kwargs):
        """Set up the storage system for memcached-based sessions.
        
        This should only be called once per process; this will be done
        automatically when using sessions.init (as the built-in Tool does).
        """
        for k, v in kwargs.items():
            setattr(cls, k, v)
        
        import memcache
        cls.cache = memcache.Client(cls.servers)
    setup = classmethod(setup)
    
    def _exists(self):
        self.mc_lock.acquire()
        try:
            return bool(self.cache.get(self.id))
        finally:
            self.mc_lock.release()
    
    def _load(self):
        self.mc_lock.acquire()
        try:
            return self.cache.get(self.id)
        finally:
            self.mc_lock.release()
    
    def _save(self, expiration_time):
        # Send the expiration time as "Unix time" (seconds since 1/1/1970)
        td = int(time.mktime(expiration_time.timetuple()))
        self.mc_lock.acquire()
        try:
            if not self.cache.set(self.id, (self._data, expiration_time), td):
                raise AssertionError("Session data for id %r not set." % self.id)
        finally:
            self.mc_lock.release()
    
    def _delete(self):
        self.cache.delete(self.id)
    
    def acquire_lock(self):
        """Acquire an exclusive lock on the currently-loaded session data."""
        self.locked = True
        self.locks.setdefault(self.id, threading.RLock()).acquire()
    
    def release_lock(self):
        """Release the lock on the currently-loaded session data."""
        self.locks[self.id].release()
        self.locked = False
    
    def __len__(self):
        """Return the number of active sessions."""
        raise NotImplementedError


# Hook functions (for CherryPy tools)

def save():
    """Save any changed session data."""
    
    if not hasattr(cherrypy.serving, "session"):
        return
    request = cherrypy.serving.request
    response = cherrypy.serving.response
    
    # Guard against running twice
    if hasattr(request, "_sessionsaved"):
        return
    request._sessionsaved = True
    
    if response.stream:
        # If the body is being streamed, we have to save the data
        #   *after* the response has been written out
        request.hooks.attach('on_end_request', cherrypy.session.save)
    else:
        # If the body is not being streamed, we save the data now
        # (so we can release the lock).
        if isinstance(response.body, types.GeneratorType):
            response.collapse_body()
        cherrypy.session.save()
save.failsafe = True

def close():
    """Close the session object for this request."""
    sess = getattr(cherrypy.serving, "session", None)
    if getattr(sess, "locked", False):
        # If the session is still locked we release the lock
        sess.release_lock()
close.failsafe = True
close.priority = 90


def init(storage_type='ram', path=None, path_header=None, name='session_id',
         timeout=60, domain=None, secure=False, clean_freq=5,
         persistent=True, debug=False, **kwargs):
    """Initialize session object (using cookies).
    
    storage_type: one of 'ram', 'file', 'postgresql'. This will be used
        to look up the corresponding class in cherrypy.lib.sessions
        globals. For example, 'file' will use the FileSession class.
    path: the 'path' value to stick in the response cookie metadata.
    path_header: if 'path' is None (the default), then the response
        cookie 'path' will be pulled from request.headers[path_header].
    name: the name of the cookie.
    timeout: the expiration timeout (in minutes) for the stored session data.
        If 'persistent' is True (the default), this is also the timeout
        for the cookie.
    domain: the cookie domain.
    secure: if False (the default) the cookie 'secure' value will not
        be set. If True, the cookie 'secure' value will be set (to 1).
    clean_freq (minutes): the poll rate for expired session cleanup.
    persistent: if True (the default), the 'timeout' argument will be used
        to expire the cookie. If False, the cookie will not have an expiry,
        and the cookie will be a "session cookie" which expires when the
        browser is closed.
    
    Any additional kwargs will be bound to the new Session instance,
    and may be specific to the storage type. See the subclass of Session
    you're using for more information.
    """
    
    request = cherrypy.serving.request
    
    # Guard against running twice
    if hasattr(request, "_session_init_flag"):
        return
    request._session_init_flag = True
    
    # Check if request came with a session ID
    id = None
    if name in request.cookie:
        id = request.cookie[name].value
        if debug:
            cherrypy.log('ID obtained from request.cookie: %r' % id,
                         'TOOLS.SESSIONS')
    
    # Find the storage class and call setup (first time only).
    storage_class = storage_type.title() + 'Session'
    storage_class = globals()[storage_class]
    if not hasattr(cherrypy, "session"):
        if hasattr(storage_class, "setup"):
            storage_class.setup(**kwargs)
    
    # Create and attach a new Session instance to cherrypy.serving.
    # It will possess a reference to (and lock, and lazily load)
    # the requested session data.
    kwargs['timeout'] = timeout
    kwargs['clean_freq'] = clean_freq
    cherrypy.serving.session = sess = storage_class(id, **kwargs)
    sess.debug = debug
    def update_cookie(id):
        """Update the cookie every time the session id changes."""
        cherrypy.serving.response.cookie[name] = id
    sess.id_observers.append(update_cookie)
    
    # Create cherrypy.session which will proxy to cherrypy.serving.session
    if not hasattr(cherrypy, "session"):
        cherrypy.session = cherrypy._ThreadLocalProxy('session')
    
    if persistent:
        cookie_timeout = timeout
    else:
        # See http://support.microsoft.com/kb/223799/EN-US/
        # and http://support.mozilla.com/en-US/kb/Cookies
        cookie_timeout = None
    set_response_cookie(path=path, path_header=path_header, name=name,
                        timeout=cookie_timeout, domain=domain, secure=secure)


def set_response_cookie(path=None, path_header=None, name='session_id',
                        timeout=60, domain=None, secure=False):
    """Set a response cookie for the client.
    
    path: the 'path' value to stick in the response cookie metadata.
    path_header: if 'path' is None (the default), then the response
        cookie 'path' will be pulled from request.headers[path_header].
    name: the name of the cookie.
    timeout: the expiration timeout for the cookie. If 0 or other boolean
        False, no 'expires' param will be set, and the cookie will be a
        "session cookie" which expires when the browser is closed.
    domain: the cookie domain.
    secure: if False (the default) the cookie 'secure' value will not
        be set. If True, the cookie 'secure' value will be set (to 1).
    """
    # Set response cookie
    cookie = cherrypy.serving.response.cookie
    cookie[name] = cherrypy.serving.session.id
    cookie[name]['path'] = (path or cherrypy.serving.request.headers.get(path_header)
                            or '/')
    
    # We'd like to use the "max-age" param as indicated in
    # http://www.faqs.org/rfcs/rfc2109.html but IE doesn't
    # save it to disk and the session is lost if people close
    # the browser. So we have to use the old "expires" ... sigh ...
##    cookie[name]['max-age'] = timeout * 60
    if timeout:
        e = time.time() + (timeout * 60)
        cookie[name]['expires'] = httputil.HTTPDate(e)
    if domain is not None:
        cookie[name]['domain'] = domain
    if secure:
        cookie[name]['secure'] = 1


def expire():
    """Expire the current session cookie."""
    name = cherrypy.serving.request.config.get('tools.sessions.name', 'session_id')
    one_year = 60 * 60 * 24 * 365
    e = time.time() - one_year
    cherrypy.serving.response.cookie[name]['expires'] = httputil.HTTPDate(e)



########NEW FILE########
__FILENAME__ = static
import logging
import mimetypes
mimetypes.init()
mimetypes.types_map['.dwg'] = 'image/x-dwg'
mimetypes.types_map['.ico'] = 'image/x-icon'
mimetypes.types_map['.bz2'] = 'application/x-bzip2'
mimetypes.types_map['.gz'] = 'application/x-gzip'

import os
import re
import stat
import time
from urllib import unquote

import cherrypy
from cherrypy.lib import cptools, httputil, file_generator_limited


def serve_file(path, content_type=None, disposition=None, name=None, debug=False):
    """Set status, headers, and body in order to serve the given path.
    
    The Content-Type header will be set to the content_type arg, if provided.
    If not provided, the Content-Type will be guessed by the file extension
    of the 'path' argument.
    
    If disposition is not None, the Content-Disposition header will be set
    to "<disposition>; filename=<name>". If name is None, it will be set
    to the basename of path. If disposition is None, no Content-Disposition
    header will be written.
    """
    
    response = cherrypy.serving.response
    
    # If path is relative, users should fix it by making path absolute.
    # That is, CherryPy should not guess where the application root is.
    # It certainly should *not* use cwd (since CP may be invoked from a
    # variety of paths). If using tools.staticdir, you can make your relative
    # paths become absolute by supplying a value for "tools.staticdir.root".
    if not os.path.isabs(path):
        msg = "'%s' is not an absolute path." % path
        if debug:
            cherrypy.log(msg, 'TOOLS.STATICFILE')
        raise ValueError(msg)
    
    try:
        st = os.stat(path)
    except OSError:
        if debug:
            cherrypy.log('os.stat(%r) failed' % path, 'TOOLS.STATIC')
        raise cherrypy.NotFound()
    
    # Check if path is a directory.
    if stat.S_ISDIR(st.st_mode):
        # Let the caller deal with it as they like.
        if debug:
            cherrypy.log('%r is a directory' % path, 'TOOLS.STATIC')
        raise cherrypy.NotFound()
    
    # Set the Last-Modified response header, so that
    # modified-since validation code can work.
    response.headers['Last-Modified'] = httputil.HTTPDate(st.st_mtime)
    cptools.validate_since()
    
    if content_type is None:
        # Set content-type based on filename extension
        ext = ""
        i = path.rfind('.')
        if i != -1:
            ext = path[i:].lower()
        content_type = mimetypes.types_map.get(ext, None)
    if content_type is not None:
        response.headers['Content-Type'] = content_type
    if debug:
        cherrypy.log('Content-Type: %r' % content_type, 'TOOLS.STATIC')
    
    cd = None
    if disposition is not None:
        if name is None:
            name = os.path.basename(path)
        cd = '%s; filename="%s"' % (disposition, name)
        response.headers["Content-Disposition"] = cd
    if debug:
        cherrypy.log('Content-Disposition: %r' % cd, 'TOOLS.STATIC')
    
    # Set Content-Length and use an iterable (file object)
    #   this way CP won't load the whole file in memory
    content_length = st.st_size
    fileobj = open(path, 'rb')
    return _serve_fileobj(fileobj, content_type, content_length, debug=debug)

def serve_fileobj(fileobj, content_type=None, disposition=None, name=None,
                  debug=False):
    """Set status, headers, and body in order to serve the given file object.
    
    The Content-Type header will be set to the content_type arg, if provided.
    
    If disposition is not None, the Content-Disposition header will be set
    to "<disposition>; filename=<name>". If name is None, 'filename' will
    not be set. If disposition is None, no Content-Disposition header will
    be written.

    CAUTION: If the request contains a 'Range' header, one or more seek()s will
    be performed on the file object.  This may cause undesired behavior if
    the file object is not seekable.  It could also produce undesired results
    if the caller set the read position of the file object prior to calling
    serve_fileobj(), expecting that the data would be served starting from that
    position.
    """
    
    response = cherrypy.serving.response
    
    try:
        st = os.fstat(fileobj.fileno())
    except AttributeError:
        if debug:
            cherrypy.log('os has no fstat attribute', 'TOOLS.STATIC')
        content_length = None
    else:
        # Set the Last-Modified response header, so that
        # modified-since validation code can work.
        response.headers['Last-Modified'] = httputil.HTTPDate(st.st_mtime)
        cptools.validate_since()
        content_length = st.st_size
    
    if content_type is not None:
        response.headers['Content-Type'] = content_type
    if debug:
        cherrypy.log('Content-Type: %r' % content_type, 'TOOLS.STATIC')
    
    cd = None
    if disposition is not None:
        if name is None:
            cd = disposition
        else:
            cd = '%s; filename="%s"' % (disposition, name)
        response.headers["Content-Disposition"] = cd
    if debug:
        cherrypy.log('Content-Disposition: %r' % cd, 'TOOLS.STATIC')
    
    return _serve_fileobj(fileobj, content_type, content_length, debug=debug)

def _serve_fileobj(fileobj, content_type, content_length, debug=False):
    """Internal. Set response.body to the given file object, perhaps ranged."""
    response = cherrypy.serving.response
    
    # HTTP/1.0 didn't have Range/Accept-Ranges headers, or the 206 code
    request = cherrypy.serving.request
    if request.protocol >= (1, 1):
        response.headers["Accept-Ranges"] = "bytes"
        r = httputil.get_ranges(request.headers.get('Range'), content_length)
        if r == []:
            response.headers['Content-Range'] = "bytes */%s" % content_length
            message = "Invalid Range (first-byte-pos greater than Content-Length)"
            if debug:
                cherrypy.log(message, 'TOOLS.STATIC')
            raise cherrypy.HTTPError(416, message)
        
        if r:
            if len(r) == 1:
                # Return a single-part response.
                start, stop = r[0]
                if stop > content_length:
                    stop = content_length
                r_len = stop - start
                if debug:
                    cherrypy.log('Single part; start: %r, stop: %r' % (start, stop),
                                 'TOOLS.STATIC')
                response.status = "206 Partial Content"
                response.headers['Content-Range'] = (
                    "bytes %s-%s/%s" % (start, stop - 1, content_length))
                response.headers['Content-Length'] = r_len
                fileobj.seek(start)
                response.body = file_generator_limited(fileobj, r_len)
            else:
                # Return a multipart/byteranges response.
                response.status = "206 Partial Content"
                import mimetools
                boundary = mimetools.choose_boundary()
                ct = "multipart/byteranges; boundary=%s" % boundary
                response.headers['Content-Type'] = ct
                if "Content-Length" in response.headers:
                    # Delete Content-Length header so finalize() recalcs it.
                    del response.headers["Content-Length"]
                
                def file_ranges():
                    # Apache compatibility:
                    yield "\r\n"
                    
                    for start, stop in r:
                        if debug:
                            cherrypy.log('Multipart; start: %r, stop: %r' % (start, stop),
                                         'TOOLS.STATIC')
                        yield "--" + boundary
                        yield "\r\nContent-type: %s" % content_type
                        yield ("\r\nContent-range: bytes %s-%s/%s\r\n\r\n"
                               % (start, stop - 1, content_length))
                        fileobj.seek(start)
                        for chunk in file_generator_limited(fileobj, stop - start):
                            yield chunk
                        yield "\r\n"
                    # Final boundary
                    yield "--" + boundary + "--"
                    
                    # Apache compatibility:
                    yield "\r\n"
                response.body = file_ranges()
            return response.body
        else:
            if debug:
                cherrypy.log('No byteranges requested', 'TOOLS.STATIC')
    
    # Set Content-Length and use an iterable (file object)
    #   this way CP won't load the whole file in memory
    response.headers['Content-Length'] = content_length
    response.body = fileobj
    return response.body

def serve_download(path, name=None):
    """Serve 'path' as an application/x-download attachment."""
    # This is such a common idiom I felt it deserved its own wrapper.
    return serve_file(path, "application/x-download", "attachment", name)


def _attempt(filename, content_types, debug=False):
    if debug:
        cherrypy.log('Attempting %r (content_types %r)' % 
                     (filename, content_types), 'TOOLS.STATICDIR')
    try:
        # you can set the content types for a
        # complete directory per extension
        content_type = None
        if content_types:
            r, ext = os.path.splitext(filename)
            content_type = content_types.get(ext[1:], None)
        serve_file(filename, content_type=content_type, debug=debug)
        return True
    except cherrypy.NotFound:
        # If we didn't find the static file, continue handling the
        # request. We might find a dynamic handler instead.
        if debug:
            cherrypy.log('NotFound', 'TOOLS.STATICFILE')
        return False

def staticdir(section, dir, root="", match="", content_types=None, index="",
              debug=False):
    """Serve a static resource from the given (root +) dir.
    
    If 'match' is given, request.path_info will be searched for the given
    regular expression before attempting to serve static content.
    
    If content_types is given, it should be a Python dictionary of
    {file-extension: content-type} pairs, where 'file-extension' is
    a string (e.g. "gif") and 'content-type' is the value to write
    out in the Content-Type response header (e.g. "image/gif").
    
    If 'index' is provided, it should be the (relative) name of a file to
    serve for directory requests. For example, if the dir argument is
    '/home/me', the Request-URI is 'myapp', and the index arg is
    'index.html', the file '/home/me/myapp/index.html' will be sought.
    """
    request = cherrypy.serving.request
    if request.method not in ('GET', 'HEAD'):
        if debug:
            cherrypy.log('request.method not GET or HEAD', 'TOOLS.STATICDIR')
        return False
    
    if match and not re.search(match, request.path_info):
        if debug:
            cherrypy.log('request.path_info %r does not match pattern %r' % 
                         (request.path_info, match), 'TOOLS.STATICDIR')
        return False
    
    # Allow the use of '~' to refer to a user's home directory.
    dir = os.path.expanduser(dir)

    # If dir is relative, make absolute using "root".
    if not os.path.isabs(dir):
        if not root:
            msg = "Static dir requires an absolute dir (or root)."
            if debug:
                cherrypy.log(msg, 'TOOLS.STATICDIR')
            raise ValueError(msg)
        dir = os.path.join(root, dir)
    
    # Determine where we are in the object tree relative to 'section'
    # (where the static tool was defined).
    if section == 'global':
        section = "/"
    section = section.rstrip(r"\/")
    branch = request.path_info[len(section) + 1:]
    branch = unquote(branch.lstrip(r"\/"))
    
    # If branch is "", filename will end in a slash
    filename = os.path.join(dir, branch)
    if debug:
        cherrypy.log('Checking file %r to fulfill %r' % 
                     (filename, request.path_info), 'TOOLS.STATICDIR')
    
    # There's a chance that the branch pulled from the URL might
    # have ".." or similar uplevel attacks in it. Check that the final
    # filename is a child of dir.
    if not os.path.normpath(filename).startswith(os.path.normpath(dir)):
        raise cherrypy.HTTPError(403) # Forbidden
    
    handled = _attempt(filename, content_types)
    if not handled:
        # Check for an index file if a folder was requested.
        if index:
            handled = _attempt(os.path.join(filename, index), content_types)
            if handled:
                request.is_index = filename[-1] in (r"\/")
    return handled

def staticfile(filename, root=None, match="", content_types=None, debug=False):
    """Serve a static resource from the given (root +) filename.
    
    If 'match' is given, request.path_info will be searched for the given
    regular expression before attempting to serve static content.
    
    If content_types is given, it should be a Python dictionary of
    {file-extension: content-type} pairs, where 'file-extension' is
    a string (e.g. "gif") and 'content-type' is the value to write
    out in the Content-Type response header (e.g. "image/gif").
    """
    request = cherrypy.serving.request
    if request.method not in ('GET', 'HEAD'):
        if debug:
            cherrypy.log('request.method not GET or HEAD', 'TOOLS.STATICFILE')
        return False
    
    if match and not re.search(match, request.path_info):
        if debug:
            cherrypy.log('request.path_info %r does not match pattern %r' % 
                         (request.path_info, match), 'TOOLS.STATICFILE')
        return False
    
    # If filename is relative, make absolute using "root".
    if not os.path.isabs(filename):
        if not root:
            msg = "Static tool requires an absolute filename (got '%s')." % filename
            if debug:
                cherrypy.log(msg, 'TOOLS.STATICFILE')
            raise ValueError(msg)
        filename = os.path.join(root, filename)
    
    return _attempt(filename, content_types, debug=debug)

########NEW FILE########
__FILENAME__ = xmlrpc
import sys

import cherrypy


def process_body():
    """Return (params, method) from request body."""
    try:
        import xmlrpclib
        return xmlrpclib.loads(cherrypy.request.body.read())
    except Exception:
        return ('ERROR PARAMS',), 'ERRORMETHOD'


def patched_path(path):
    """Return 'path', doctored for RPC."""
    if not path.endswith('/'):
        path += '/'
    if path.startswith('/RPC2/'):
        # strip the first /rpc2
        path = path[5:]
    return path


def _set_response(body):
    # The XML-RPC spec (http://www.xmlrpc.com/spec) says:
    # "Unless there's a lower-level error, always return 200 OK."
    # Since Python's xmlrpclib interprets a non-200 response
    # as a "Protocol Error", we'll just return 200 every time.
    response = cherrypy.response
    response.status = '200 OK'
    response.body = body
    response.headers['Content-Type'] = 'text/xml'
    response.headers['Content-Length'] = len(body)


def respond(body, encoding='utf-8', allow_none=0):
    from xmlrpclib import Fault, dumps
    if not isinstance(body, Fault):
        body = (body,)
    _set_response(dumps(body, methodresponse=1,
                        encoding=encoding,
                        allow_none=allow_none))

def on_error(*args, **kwargs):
    body = str(sys.exc_info()[1])
    from xmlrpclib import Fault, dumps
    _set_response(dumps(Fault(1, body)))


########NEW FILE########
__FILENAME__ = plugins
"""Site services for use with a Web Site Process Bus."""

import os
import re
try:
    set
except NameError:
    from sets import Set as set
import signal as _signal
import sys
import time
import thread
import threading

# _module__file__base is used by Autoreload to make
# absolute any filenames retrieved from sys.modules which are not
# already absolute paths.  This is to work around Python's quirk
# of importing the startup script and using a relative filename
# for it in sys.modules.
#
# Autoreload examines sys.modules afresh every time it runs. If an application
# changes the current directory by executing os.chdir(), then the next time
# Autoreload runs, it will not be able to find any filenames which are
# not absolute paths, because the current directory is not the same as when the
# module was first imported.  Autoreload will then wrongly conclude the file has
# "changed", and initiate the shutdown/re-exec sequence.
# See ticket #917.
# For this workaround to have a decent probability of success, this module
# needs to be imported as early as possible, before the app has much chance
# to change the working directory.
_module__file__base = os.getcwd()


class SimplePlugin(object):
    """Plugin base class which auto-subscribes methods for known channels."""
    
    def __init__(self, bus):
        self.bus = bus
    
    def subscribe(self):
        """Register this object as a (multi-channel) listener on the bus."""
        for channel in self.bus.listeners:
            # Subscribe self.start, self.exit, etc. if present.
            method = getattr(self, channel, None)
            if method is not None:
                self.bus.subscribe(channel, method)
    
    def unsubscribe(self):
        """Unregister this object as a listener on the bus."""
        for channel in self.bus.listeners:
            # Unsubscribe self.start, self.exit, etc. if present.
            method = getattr(self, channel, None)
            if method is not None:
                self.bus.unsubscribe(channel, method)



class SignalHandler(object):
    """Register bus channels (and listeners) for system signals.
    
    By default, instantiating this object subscribes the following signals
    and listeners:
    
        TERM: bus.exit
        HUP : bus.restart
        USR1: bus.graceful
    """
    
    # Map from signal numbers to names
    signals = {}
    for k, v in vars(_signal).items():
        if k.startswith('SIG') and not k.startswith('SIG_'):
            signals[v] = k
    del k, v
    
    def __init__(self, bus):
        self.bus = bus
        # Set default handlers
        self.handlers = {'SIGTERM': self.bus.exit,
                         'SIGHUP': self.handle_SIGHUP,
                         'SIGUSR1': self.bus.graceful,
                         }
        
        self._previous_handlers = {}
    
    def subscribe(self):
        for sig, func in self.handlers.items():
            try:
                self.set_handler(sig, func)
            except ValueError:
                pass
    
    def unsubscribe(self):
        for signum, handler in self._previous_handlers.items():
            signame = self.signals[signum]
            
            if handler is None:
                self.bus.log("Restoring %s handler to SIG_DFL." % signame)
                handler = _signal.SIG_DFL
            else:
                self.bus.log("Restoring %s handler %r." % (signame, handler))
            
            try:
                our_handler = _signal.signal(signum, handler)
                if our_handler is None:
                    self.bus.log("Restored old %s handler %r, but our "
                                 "handler was not registered." % 
                                 (signame, handler), level=30)
            except ValueError:
                self.bus.log("Unable to restore %s handler %r." % 
                             (signame, handler), level=40, traceback=True)
    
    def set_handler(self, signal, listener=None):
        """Subscribe a handler for the given signal (number or name).
        
        If the optional 'listener' argument is provided, it will be
        subscribed as a listener for the given signal's channel.
        
        If the given signal name or number is not available on the current
        platform, ValueError is raised.
        """
        if isinstance(signal, basestring):
            signum = getattr(_signal, signal, None)
            if signum is None:
                raise ValueError("No such signal: %r" % signal)
            signame = signal
        else:
            try:
                signame = self.signals[signal]
            except KeyError:
                raise ValueError("No such signal: %r" % signal)
            signum = signal
        
        prev = _signal.signal(signum, self._handle_signal)
        self._previous_handlers[signum] = prev
        
        if listener is not None:
            self.bus.log("Listening for %s." % signame)
            self.bus.subscribe(signame, listener)
    
    def _handle_signal(self, signum=None, frame=None):
        """Python signal handler (self.set_handler subscribes it for you)."""
        signame = self.signals[signum]
        self.bus.log("Caught signal %s." % signame)
        self.bus.publish(signame)
    
    def handle_SIGHUP(self):
        if os.isatty(sys.stdin.fileno()):
            # not daemonized (may be foreground or background)
            self.bus.log("SIGHUP caught but not daemonized. Exiting.")
            self.bus.exit()
        else:
            self.bus.log("SIGHUP caught while daemonized. Restarting.")
            self.bus.restart()


try:
    import pwd, grp
except ImportError:
    pwd, grp = None, None


class DropPrivileges(SimplePlugin):
    """Drop privileges. uid/gid arguments not available on Windows.
    
    Special thanks to Gavin Baker: http://antonym.org/node/100.
    """
    
    def __init__(self, bus, umask=None, uid=None, gid=None):
        SimplePlugin.__init__(self, bus)
        self.finalized = False
        self.uid = uid
        self.gid = gid
        self.umask = umask
    
    def _get_uid(self):
        return self._uid
    def _set_uid(self, val):
        if val is not None:
            if pwd is None:
                self.bus.log("pwd module not available; ignoring uid.",
                             level=30)
                val = None
            elif isinstance(val, basestring):
                val = pwd.getpwnam(val)[2]
        self._uid = val
    uid = property(_get_uid, _set_uid, doc="The uid under which to run.")
    
    def _get_gid(self):
        return self._gid
    def _set_gid(self, val):
        if val is not None:
            if grp is None:
                self.bus.log("grp module not available; ignoring gid.",
                             level=30)
                val = None
            elif isinstance(val, basestring):
                val = grp.getgrnam(val)[2]
        self._gid = val
    gid = property(_get_gid, _set_gid, doc="The gid under which to run.")
    
    def _get_umask(self):
        return self._umask
    def _set_umask(self, val):
        if val is not None:
            try:
                os.umask
            except AttributeError:
                self.bus.log("umask function not available; ignoring umask.",
                             level=30)
                val = None
        self._umask = val
    umask = property(_get_umask, _set_umask, doc="The umask under which to run.")
    
    def start(self):
        # uid/gid
        def current_ids():
            """Return the current (uid, gid) if available."""
            name, group = None, None
            if pwd:
                name = pwd.getpwuid(os.getuid())[0]
            if grp:
                group = grp.getgrgid(os.getgid())[0]
            return name, group
        
        if self.finalized:
            if not (self.uid is None and self.gid is None):
                self.bus.log('Already running as uid: %r gid: %r' % 
                             current_ids())
        else:
            if self.uid is None and self.gid is None:
                if pwd or grp:
                    self.bus.log('uid/gid not set', level=30)
            else:
                self.bus.log('Started as uid: %r gid: %r' % current_ids())
                if self.gid is not None:
                    os.setgid(self.gid)
                if self.uid is not None:
                    os.setuid(self.uid)
                self.bus.log('Running as uid: %r gid: %r' % current_ids())
        
        # umask
        if self.finalized:
            if self.umask is not None:
                self.bus.log('umask already set to: %03o' % self.umask)
        else:
            if self.umask is None:
                self.bus.log('umask not set', level=30)
            else:
                old_umask = os.umask(self.umask)
                self.bus.log('umask old: %03o, new: %03o' % 
                             (old_umask, self.umask))
        
        self.finalized = True
    # This is slightly higher than the priority for server.start
    # in order to facilitate the most common use: starting on a low
    # port (which requires root) and then dropping to another user.
    start.priority = 77


class Daemonizer(SimplePlugin):
    """Daemonize the running script.
    
    Use this with a Web Site Process Bus via:
        
        Daemonizer(bus).subscribe()
    
    When this component finishes, the process is completely decoupled from
    the parent environment. Please note that when this component is used,
    the return code from the parent process will still be 0 if a startup
    error occurs in the forked children. Errors in the initial daemonizing
    process still return proper exit codes. Therefore, if you use this
    plugin to daemonize, don't use the return code as an accurate indicator
    of whether the process fully started. In fact, that return code only
    indicates if the process succesfully finished the first fork.
    """
    
    def __init__(self, bus, stdin='/dev/null', stdout='/dev/null',
                 stderr='/dev/null'):
        SimplePlugin.__init__(self, bus)
        self.stdin = stdin
        self.stdout = stdout
        self.stderr = stderr
        self.finalized = False
    
    def start(self):
        if self.finalized:
            self.bus.log('Already deamonized.')
        
        # forking has issues with threads:
        # http://www.opengroup.org/onlinepubs/000095399/functions/fork.html
        # "The general problem with making fork() work in a multi-threaded
        #  world is what to do with all of the threads..."
        # So we check for active threads:
        if threading.activeCount() != 1:
            self.bus.log('There are %r active threads. '
                         'Daemonizing now may cause strange failures.' % 
                         threading.enumerate(), level=30)
        
        # See http://www.erlenstar.demon.co.uk/unix/faq_2.html#SEC16
        # (or http://www.faqs.org/faqs/unix-faq/programmer/faq/ section 1.7)
        # and http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/66012
        
        # Finish up with the current stdout/stderr
        sys.stdout.flush()
        sys.stderr.flush()
        
        # Do first fork.
        try:
            pid = os.fork()
            if pid == 0:
                # This is the child process. Continue.
                pass
            else:
                # This is the first parent. Exit, now that we've forked.
                self.bus.log('Forking once.')
                os._exit(0)
        except OSError, exc:
            # Python raises OSError rather than returning negative numbers.
            sys.exit("%s: fork #1 failed: (%d) %s\n"
                     % (sys.argv[0], exc.errno, exc.strerror))
        
        os.setsid()
        
        # Do second fork
        try:
            pid = os.fork()
            if pid > 0:
                self.bus.log('Forking twice.')
                os._exit(0) # Exit second parent
        except OSError, exc:
            sys.exit("%s: fork #2 failed: (%d) %s\n"
                     % (sys.argv[0], exc.errno, exc.strerror))
        
        os.chdir("/")
        os.umask(0)
        
        si = open(self.stdin, "r")
        so = open(self.stdout, "a+")
        se = open(self.stderr, "a+")

        # os.dup2(fd, fd2) will close fd2 if necessary,
        # so we don't explicitly close stdin/out/err.
        # See http://docs.python.org/lib/os-fd-ops.html
        os.dup2(si.fileno(), sys.stdin.fileno())
        os.dup2(so.fileno(), sys.stdout.fileno())
        os.dup2(se.fileno(), sys.stderr.fileno())
        
        self.bus.log('Daemonized to PID: %s' % os.getpid())
        self.finalized = True
    start.priority = 65


class PIDFile(SimplePlugin):
    """Maintain a PID file via a WSPBus."""
    
    def __init__(self, bus, pidfile):
        SimplePlugin.__init__(self, bus)
        self.pidfile = pidfile
        self.finalized = False
    
    def start(self):
        pid = os.getpid()
        if self.finalized:
            self.bus.log('PID %r already written to %r.' % (pid, self.pidfile))
        else:
            open(self.pidfile, "wb").write(str(pid))
            self.bus.log('PID %r written to %r.' % (pid, self.pidfile))
            self.finalized = True
    start.priority = 70
    
    def exit(self):
        try:
            os.remove(self.pidfile)
            self.bus.log('PID file removed: %r.' % self.pidfile)
        except (KeyboardInterrupt, SystemExit):
            raise
        except:
            pass


class PerpetualTimer(threading._Timer):
    """A subclass of threading._Timer whose run() method repeats."""
    
    def run(self):
        while True:
            self.finished.wait(self.interval)
            if self.finished.isSet():
                return
            try:
                self.function(*self.args, **self.kwargs)
            except Exception, x:
                self.bus.log("Error in perpetual timer thread function %r." % 
                             self.function, level=40, traceback=True)
                # Quit on first error to avoid massive logs.
                raise


class Monitor(SimplePlugin):
    """WSPBus listener to periodically run a callback in its own thread.
    
    bus: a Web Site Process Bus object.
    callback: the function to call at intervals.
    frequency: the time in seconds between callback runs.
    """
    
    frequency = 60
    
    def __init__(self, bus, callback, frequency=60, name=None):
        SimplePlugin.__init__(self, bus)
        self.callback = callback
        self.frequency = frequency
        self.thread = None
        self.name = name
    
    def start(self):
        """Start our callback in its own perpetual timer thread."""
        if self.frequency > 0:
            threadname = self.name or self.__class__.__name__
            if self.thread is None:
                self.thread = PerpetualTimer(self.frequency, self.callback)
                self.thread.bus = self.bus
                self.thread.setName(threadname)
                self.thread.start()
                self.bus.log("Started monitor thread %r." % threadname)
            else:
                self.bus.log("Monitor thread %r already started." % threadname)
    start.priority = 70
    
    def stop(self):
        """Stop our callback's perpetual timer thread."""
        if self.thread is None:
            self.bus.log("No thread running for %s." % self.name or self.__class__.__name__)
        else:
            if self.thread is not threading.currentThread():
                name = self.thread.getName()
                self.thread.cancel()
                self.thread.join()
                self.bus.log("Stopped thread %r." % name)
            self.thread = None
    
    def graceful(self):
        """Stop the callback's perpetual timer thread and restart it."""
        self.stop()
        self.start()


class Autoreloader(Monitor):
    """Monitor which re-executes the process when files change."""
    
    frequency = 1
    match = '.*'
    
    def __init__(self, bus, frequency=1, match='.*'):
        self.mtimes = {}
        self.files = set()
        self.match = match
        Monitor.__init__(self, bus, self.run, frequency)
    
    def start(self):
        """Start our own perpetual timer thread for self.run."""
        if self.thread is None:
            self.mtimes = {}
        Monitor.start(self)
    start.priority = 70 
    
    def sysfiles(self):
        """Return a Set of filenames which the Autoreloader will monitor."""
        files = set()
        for k, m in sys.modules.items():
            if re.match(self.match, k):
                if hasattr(m, '__loader__') and hasattr(m.__loader__, 'archive'):
                    f = m.__loader__.archive
                else:
                    f = getattr(m, '__file__', None)
                    if f is not None and not os.path.isabs(f):
                        # ensure absolute paths so a os.chdir() in the app doesn't break me
                        f = os.path.normpath(os.path.join(_module__file__base, f))
                files.add(f)
        return files
    
    def run(self):
        """Reload the process if registered files have been modified."""
        for filename in self.sysfiles() | self.files:
            if filename:
                if filename.endswith('.pyc'):
                    filename = filename[:-1]
                
                oldtime = self.mtimes.get(filename, 0)
                if oldtime is None:
                    # Module with no .py file. Skip it.
                    continue
                
                try:
                    mtime = os.stat(filename).st_mtime
                except OSError:
                    # Either a module with no .py file, or it's been deleted.
                    mtime = None
                
                if filename not in self.mtimes:
                    # If a module has no .py file, this will be None.
                    self.mtimes[filename] = mtime
                else:
                    if mtime is None or mtime > oldtime:
                        # The file has been deleted or modified.
                        self.bus.log("Restarting because %s changed." % filename)
                        self.thread.cancel()
                        self.bus.log("Stopped thread %r." % self.thread.getName())
                        self.bus.restart()
                        return


class ThreadManager(SimplePlugin):
    """Manager for HTTP request threads.
    
    If you have control over thread creation and destruction, publish to
    the 'acquire_thread' and 'release_thread' channels (for each thread).
    This will register/unregister the current thread and publish to
    'start_thread' and 'stop_thread' listeners in the bus as needed.
    
    If threads are created and destroyed by code you do not control
    (e.g., Apache), then, at the beginning of every HTTP request,
    publish to 'acquire_thread' only. You should not publish to
    'release_thread' in this case, since you do not know whether
    the thread will be re-used or not. The bus will call
    'stop_thread' listeners for you when it stops.
    """
    
    def __init__(self, bus):
        self.threads = {}
        SimplePlugin.__init__(self, bus)
        self.bus.listeners.setdefault('acquire_thread', set())
        self.bus.listeners.setdefault('release_thread', set())
    
    def acquire_thread(self):
        """Run 'start_thread' listeners for the current thread.
        
        If the current thread has already been seen, any 'start_thread'
        listeners will not be run again.
        """
        thread_ident = thread.get_ident()
        if thread_ident not in self.threads:
            # We can't just use _get_ident as the thread ID
            # because some platforms reuse thread ID's.
            i = len(self.threads) + 1
            self.threads[thread_ident] = i
            self.bus.publish('start_thread', i)
    
    def release_thread(self):
        """Release the current thread and run 'stop_thread' listeners."""
        thread_ident = threading._get_ident()
        i = self.threads.pop(thread_ident, None)
        if i is not None:
            self.bus.publish('stop_thread', i)
    
    def stop(self):
        """Release all threads and run all 'stop_thread' listeners."""
        for thread_ident, i in self.threads.items():
            self.bus.publish('stop_thread', i)
        self.threads.clear()
    graceful = stop


########NEW FILE########
__FILENAME__ = servers
"""Adapt an HTTP server."""

import time


class ServerAdapter(object):
    """Adapter for an HTTP server.
    
    If you need to start more than one HTTP server (to serve on multiple
    ports, or protocols, etc.), you can manually register each one and then
    start them all with bus.start:
    
        s1 = ServerAdapter(bus, MyWSGIServer(host='0.0.0.0', port=80))
        s2 = ServerAdapter(bus, another.HTTPServer(host='127.0.0.1', SSL=True))
        s1.subscribe()
        s2.subscribe()
        bus.start()
    """
    
    def __init__(self, bus, httpserver=None, bind_addr=None):
        self.bus = bus
        self.httpserver = httpserver
        self.bind_addr = bind_addr
        self.interrupt = None
        self.running = False
    
    def subscribe(self):
        self.bus.subscribe('start', self.start)
        self.bus.subscribe('stop', self.stop)
    
    def unsubscribe(self):
        self.bus.unsubscribe('start', self.start)
        self.bus.unsubscribe('stop', self.stop)
    
    def start(self):
        """Start the HTTP server."""
        if self.bind_addr is None:
            on_what = "unknown interface (dynamic?)"
        elif isinstance(self.bind_addr, tuple):
            host, port = self.bind_addr
            on_what = "%s:%s" % (host, port)
        else:
            on_what = "socket file: %s" % self.bind_addr
        
        if self.running:
            self.bus.log("Already serving on %s" % on_what)
            return
        
        self.interrupt = None
        if not self.httpserver:
            raise ValueError("No HTTP server has been created.")
        
        # Start the httpserver in a new thread.
        if isinstance(self.bind_addr, tuple):
            wait_for_free_port(*self.bind_addr)
        
        import threading
        t = threading.Thread(target=self._start_http_thread)
        t.setName("HTTPServer " + t.getName())
        t.start()
        
        self.wait()
        self.running = True
        self.bus.log("Serving on %s" % on_what)
    start.priority = 75
    
    def _start_http_thread(self):
        """HTTP servers MUST be running in new threads, so that the
        main thread persists to receive KeyboardInterrupt's. If an
        exception is raised in the httpserver's thread then it's
        trapped here, and the bus (and therefore our httpserver)
        are shut down.
        """
        try:
            self.httpserver.start()
        except KeyboardInterrupt, exc:
            self.bus.log("<Ctrl-C> hit: shutting down HTTP server")
            self.interrupt = exc
            self.bus.exit()
        except SystemExit, exc:
            self.bus.log("SystemExit raised: shutting down HTTP server")
            self.interrupt = exc
            self.bus.exit()
            raise
        except:
            import sys
            self.interrupt = sys.exc_info()[1]
            self.bus.log("Error in HTTP server: shutting down",
                         traceback=True, level=40)
            self.bus.exit()
            raise
    
    def wait(self):
        """Wait until the HTTP server is ready to receive requests."""
        while not getattr(self.httpserver, "ready", False):
            if self.interrupt:
                raise self.interrupt
            time.sleep(.1)
        
        # Wait for port to be occupied
        if isinstance(self.bind_addr, tuple):
            host, port = self.bind_addr
            wait_for_occupied_port(host, port)
    
    def stop(self):
        """Stop the HTTP server."""
        if self.running:
            # stop() MUST block until the server is *truly* stopped.
            self.httpserver.stop()
            # Wait for the socket to be truly freed.
            if isinstance(self.bind_addr, tuple):
                wait_for_free_port(*self.bind_addr)
            self.running = False
            self.bus.log("HTTP Server %s shut down" % self.httpserver)
        else:
            self.bus.log("HTTP Server %s already shut down" % self.httpserver)
    stop.priority = 25
    
    def restart(self):
        """Restart the HTTP server."""
        self.stop()
        self.start()


class FlupFCGIServer(object):
    """Adapter for a flup.server.fcgi.WSGIServer."""
    
    def __init__(self, *args, **kwargs):
        if kwargs.get('bindAddress', None) is None:
            import socket
            if not hasattr(socket.socket, 'fromfd'):
                raise ValueError(
                    'Dynamic FCGI server not available on this platform. '
                    'You must use a static or external one by providing a '
                    'legal bindAddress.')
        self.args = args
        self.kwargs = kwargs
        self.ready = False
    
    def start(self):
        """Start the FCGI server."""
        # We have to instantiate the server class here because its __init__
        # starts a threadpool. If we do it too early, daemonize won't work.
        from flup.server.fcgi import WSGIServer
        self.fcgiserver = WSGIServer(*self.args, **self.kwargs)
        # TODO: report this bug upstream to flup.
        # If we don't set _oldSIGs on Windows, we get:
        #   File "C:\Python24\Lib\site-packages\flup\server\threadedserver.py",
        #   line 108, in run
        #     self._restoreSignalHandlers()
        #   File "C:\Python24\Lib\site-packages\flup\server\threadedserver.py",
        #   line 156, in _restoreSignalHandlers
        #     for signum,handler in self._oldSIGs:
        #   AttributeError: 'WSGIServer' object has no attribute '_oldSIGs'
        self.fcgiserver._installSignalHandlers = lambda: None
        self.fcgiserver._oldSIGs = []
        self.ready = True
        self.fcgiserver.run()
    
    def stop(self):
        """Stop the HTTP server."""
        # Forcibly stop the fcgi server main event loop.
        self.fcgiserver._keepGoing = False
        # Force all worker threads to die off.
        self.fcgiserver._threadPool.maxSpare = self.fcgiserver._threadPool._idleCount
        self.ready = False


class FlupSCGIServer(object):
    """Adapter for a flup.server.scgi.WSGIServer."""
    
    def __init__(self, *args, **kwargs):
        self.args = args
        self.kwargs = kwargs
        self.ready = False
    
    def start(self):
        """Start the SCGI server."""
        # We have to instantiate the server class here because its __init__
        # starts a threadpool. If we do it too early, daemonize won't work.
        from flup.server.scgi import WSGIServer
        self.scgiserver = WSGIServer(*self.args, **self.kwargs)
        # TODO: report this bug upstream to flup.
        # If we don't set _oldSIGs on Windows, we get:
        #   File "C:\Python24\Lib\site-packages\flup\server\threadedserver.py",
        #   line 108, in run
        #     self._restoreSignalHandlers()
        #   File "C:\Python24\Lib\site-packages\flup\server\threadedserver.py",
        #   line 156, in _restoreSignalHandlers
        #     for signum,handler in self._oldSIGs:
        #   AttributeError: 'WSGIServer' object has no attribute '_oldSIGs'
        self.scgiserver._installSignalHandlers = lambda: None
        self.scgiserver._oldSIGs = []
        self.ready = True
        self.scgiserver.run()
    
    def stop(self):
        """Stop the HTTP server."""
        self.ready = False
        # Forcibly stop the scgi server main event loop.
        self.scgiserver._keepGoing = False
        # Force all worker threads to die off.
        self.scgiserver._threadPool.maxSpare = 0


def client_host(server_host):
    """Return the host on which a client can connect to the given listener."""
    if server_host == '0.0.0.0':
        # 0.0.0.0 is INADDR_ANY, which should answer on localhost.
        return '127.0.0.1'
    if server_host == '::':
        # :: is IN6ADDR_ANY, which should answer on localhost.
        return '::1'
    return server_host

def check_port(host, port, timeout=1.0):
    """Raise an error if the given port is not free on the given host."""
    if not host:
        raise ValueError("Host values of '' or None are not allowed.")
    host = client_host(host)
    port = int(port)
    
    import socket
    
    # AF_INET or AF_INET6 socket
    # Get the correct address family for our host (allows IPv6 addresses)
    try:
        info = socket.getaddrinfo(host, port, socket.AF_UNSPEC,
                                  socket.SOCK_STREAM)
    except socket.gaierror:
        if ':' in host:
            info = [(socket.AF_INET6, socket.SOCK_STREAM, 0, "", (host, port, 0, 0))]
        else:
            info = [(socket.AF_INET, socket.SOCK_STREAM, 0, "", (host, port))]
    
    for res in info:
        af, socktype, proto, canonname, sa = res
        s = None
        try:
            s = socket.socket(af, socktype, proto)
            # See http://groups.google.com/group/cherrypy-users/
            #        browse_frm/thread/bbfe5eb39c904fe0
            s.settimeout(timeout)
            s.connect((host, port))
            s.close()
            raise IOError("Port %s is in use on %s; perhaps the previous "
                          "httpserver did not shut down properly." % 
                          (repr(port), repr(host)))
        except socket.error:
            if s:
                s.close()

def wait_for_free_port(host, port):
    """Wait for the specified port to become free (drop requests)."""
    if not host:
        raise ValueError("Host values of '' or None are not allowed.")
    
    for trial in range(50):
        try:
            # we are expecting a free port, so reduce the timeout
            check_port(host, port, timeout=0.1)
        except IOError:
            # Give the old server thread time to free the port.
            time.sleep(0.1)
        else:
            return
    
    raise IOError("Port %r not free on %r" % (port, host))

def wait_for_occupied_port(host, port):
    """Wait for the specified port to become active (receive requests)."""
    if not host:
        raise ValueError("Host values of '' or None are not allowed.")
    
    for trial in range(50):
        try:
            check_port(host, port)
        except IOError:
            return
        else:
            time.sleep(.1)
    
    raise IOError("Port %r not bound on %r" % (port, host))

########NEW FILE########
__FILENAME__ = win32
"""Windows service. Requires pywin32."""

import os
import win32api
import win32con
import win32event
import win32service
import win32serviceutil

from cherrypy.process import wspbus, plugins


class ConsoleCtrlHandler(plugins.SimplePlugin):
    """A WSPBus plugin for handling Win32 console events (like Ctrl-C)."""
    
    def __init__(self, bus):
        self.is_set = False
        plugins.SimplePlugin.__init__(self, bus)
    
    def start(self):
        if self.is_set:
            self.bus.log('Handler for console events already set.', level=40)
            return
        
        result = win32api.SetConsoleCtrlHandler(self.handle, 1)
        if result == 0:
            self.bus.log('Could not SetConsoleCtrlHandler (error %r)' % 
                         win32api.GetLastError(), level=40)
        else:
            self.bus.log('Set handler for console events.', level=40)
            self.is_set = True
    
    def stop(self):
        if not self.is_set:
            self.bus.log('Handler for console events already off.', level=40)
            return
        
        try:
            result = win32api.SetConsoleCtrlHandler(self.handle, 0)
        except ValueError:
            # "ValueError: The object has not been registered"
            result = 1
        
        if result == 0:
            self.bus.log('Could not remove SetConsoleCtrlHandler (error %r)' % 
                         win32api.GetLastError(), level=40)
        else:
            self.bus.log('Removed handler for console events.', level=40)
            self.is_set = False
    
    def handle(self, event):
        """Handle console control events (like Ctrl-C)."""
        if event in (win32con.CTRL_C_EVENT, win32con.CTRL_LOGOFF_EVENT,
                     win32con.CTRL_BREAK_EVENT, win32con.CTRL_SHUTDOWN_EVENT,
                     win32con.CTRL_CLOSE_EVENT):
            self.bus.log('Console event %s: shutting down bus' % event)
            
            # Remove self immediately so repeated Ctrl-C doesn't re-call it.
            try:
                self.stop()
            except ValueError:
                pass
            
            self.bus.exit()
            # 'First to return True stops the calls'
            return 1
        return 0


class Win32Bus(wspbus.Bus):
    """A Web Site Process Bus implementation for Win32.
    
    Instead of time.sleep, this bus blocks using native win32event objects.
    """
    
    def __init__(self):
        self.events = {}
        wspbus.Bus.__init__(self)
    
    def _get_state_event(self, state):
        """Return a win32event for the given state (creating it if needed)."""
        try:
            return self.events[state]
        except KeyError:
            event = win32event.CreateEvent(None, 0, 0,
                                           "WSPBus %s Event (pid=%r)" % 
                                           (state.name, os.getpid()))
            self.events[state] = event
            return event
    
    def _get_state(self):
        return self._state
    def _set_state(self, value):
        self._state = value
        event = self._get_state_event(value)
        win32event.PulseEvent(event)
    state = property(_get_state, _set_state)
    
    def wait(self, state, interval=0.1, channel=None):
        """Wait for the given state(s), KeyboardInterrupt or SystemExit.
        
        Since this class uses native win32event objects, the interval
        argument is ignored.
        """
        if isinstance(state, (tuple, list)):
            # Don't wait for an event that beat us to the punch ;)
            if self.state not in state:
                events = tuple([self._get_state_event(s) for s in state])
                win32event.WaitForMultipleObjects(events, 0, win32event.INFINITE)
        else:
            # Don't wait for an event that beat us to the punch ;)
            if self.state != state:
                event = self._get_state_event(state)
                win32event.WaitForSingleObject(event, win32event.INFINITE)


class _ControlCodes(dict):
    """Control codes used to "signal" a service via ControlService.
    
    User-defined control codes are in the range 128-255. We generally use
    the standard Python value for the Linux signal and add 128. Example:
    
        >>> signal.SIGUSR1
        10
        control_codes['graceful'] = 128 + 10
    """
    
    def key_for(self, obj):
        """For the given value, return its corresponding key."""
        for key, val in self.items():
            if val is obj:
                return key
        raise ValueError("The given object could not be found: %r" % obj)

control_codes = _ControlCodes({'graceful': 138})


def signal_child(service, command):
    if command == 'stop':
        win32serviceutil.StopService(service)
    elif command == 'restart':
        win32serviceutil.RestartService(service)
    else:
        win32serviceutil.ControlService(service, control_codes[command])


class PyWebService(win32serviceutil.ServiceFramework):
    """Python Web Service."""
    
    _svc_name_ = "Python Web Service"
    _svc_display_name_ = "Python Web Service"
    _svc_deps_ = None        # sequence of service names on which this depends
    _exe_name_ = "pywebsvc"
    _exe_args_ = None        # Default to no arguments
    
    # Only exists on Windows 2000 or later, ignored on windows NT
    _svc_description_ = "Python Web Service"
    
    def SvcDoRun(self):
        from cherrypy import process
        process.bus.start()
        process.bus.block()
    
    def SvcStop(self):
        from cherrypy import process
        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)
        process.bus.exit()
    
    def SvcOther(self, control):
        process.bus.publish(control_codes.key_for(control))


if __name__ == '__main__':
    win32serviceutil.HandleCommandLine(PyWebService)

########NEW FILE########
__FILENAME__ = wspbus
"""An implementation of the Web Site Process Bus.

This module is completely standalone, depending only on the stdlib.

Web Site Process Bus
--------------------

A Bus object is used to contain and manage site-wide behavior:
daemonization, HTTP server start/stop, process reload, signal handling,
drop privileges, PID file management, logging for all of these,
and many more.

In addition, a Bus object provides a place for each web framework
to register code that runs in response to site-wide events (like
process start and stop), or which controls or otherwise interacts with
the site-wide components mentioned above. For example, a framework which
uses file-based templates would add known template filenames to an
autoreload component.

Ideally, a Bus object will be flexible enough to be useful in a variety
of invocation scenarios:

 1. The deployer starts a site from the command line via a framework-
     neutral deployment script; applications from multiple frameworks
     are mixed in a single site. Command-line arguments and configuration
     files are used to define site-wide components such as the HTTP server,
     WSGI component graph, autoreload behavior, signal handling, etc.
 2. The deployer starts a site via some other process, such as Apache;
     applications from multiple frameworks are mixed in a single site.
     Autoreload and signal handling (from Python at least) are disabled.
 3. The deployer starts a site via a framework-specific mechanism;
     for example, when running tests, exploring tutorials, or deploying
     single applications from a single framework. The framework controls
     which site-wide components are enabled as it sees fit.

The Bus object in this package uses topic-based publish-subscribe
messaging to accomplish all this. A few topic channels are built in
('start', 'stop', 'exit', 'graceful', 'log', and 'main'). Frameworks and
site containers are free to define their own. If a message is sent to a
channel that has not been defined or has no listeners, there is no effect.

In general, there should only ever be a single Bus object per process.
Frameworks and site containers share a single Bus object by publishing
messages and subscribing listeners.

The Bus object works as a finite state machine which models the current
state of the process. Bus methods move it from one state to another;
those methods then publish to subscribed listeners on the channel for
the new state.

                        O
                        |
                        V
       STOPPING --> STOPPED --> EXITING -> X
          A   A         |
          |    \___     |
          |        \    |
          |         V   V
        STARTED <-- STARTING

"""

import atexit
import os
try:
    set
except NameError:
    from sets import Set as set
import sys
import threading
import time
import traceback as _traceback
import warnings

# Here I save the value of os.getcwd(), which, if I am imported early enough,
# will be the directory from which the startup script was run.  This is needed
# by _do_execv(), to change back to the original directory before execv()ing a
# new process.  This is a defense against the application having changed the
# current working directory (which could make sys.executable "not found" if
# sys.executable is a relative-path, and/or cause other problems).
_startup_cwd = os.getcwd()

class ChannelFailures(Exception):
    delimiter = '\n'
    
    def __init__(self, *args, **kwargs):
        # Don't use 'super' here; Exceptions are old-style in Py2.4
        # See http://www.cherrypy.org/ticket/959
        Exception.__init__(self, *args, **kwargs)
        self._exceptions = list()
    
    def handle_exception(self):
        self._exceptions.append(sys.exc_info())
    
    def get_instances(self):
        return [instance for cls, instance, traceback in self._exceptions]
    
    def __str__(self):
        exception_strings = map(repr, self.get_instances())
        return self.delimiter.join(exception_strings)
    
    def __nonzero__(self):
        return bool(self._exceptions)

# Use a flag to indicate the state of the bus.
class _StateEnum(object):
    class State(object):
        name = None
        def __repr__(self):
            return "states.%s" % self.name
    
    def __setattr__(self, key, value):
        if isinstance(value, self.State):
            value.name = key
        object.__setattr__(self, key, value)
states = _StateEnum()
states.STOPPED = states.State()
states.STARTING = states.State()
states.STARTED = states.State()
states.STOPPING = states.State()
states.EXITING = states.State()


class Bus(object):
    """Process state-machine and messenger for HTTP site deployment.
    
    All listeners for a given channel are guaranteed to be called even
    if others at the same channel fail. Each failure is logged, but
    execution proceeds on to the next listener. The only way to stop all
    processing from inside a listener is to raise SystemExit and stop the
    whole server.
    """
    
    states = states
    state = states.STOPPED
    execv = False
    
    def __init__(self):
        self.execv = False
        self.state = states.STOPPED
        self.listeners = dict(
            [(channel, set()) for channel
             in ('start', 'stop', 'exit', 'graceful', 'log', 'main')])
        self._priorities = {}
    
    def subscribe(self, channel, callback, priority=None):
        """Add the given callback at the given channel (if not present)."""
        if channel not in self.listeners:
            self.listeners[channel] = set()
        self.listeners[channel].add(callback)
        
        if priority is None:
            priority = getattr(callback, 'priority', 50)
        self._priorities[(channel, callback)] = priority
    
    def unsubscribe(self, channel, callback):
        """Discard the given callback (if present)."""
        listeners = self.listeners.get(channel)
        if listeners and callback in listeners:
            listeners.discard(callback)
            del self._priorities[(channel, callback)]
    
    def publish(self, channel, *args, **kwargs):
        """Return output of all subscribers for the given channel."""
        if channel not in self.listeners:
            return []
        
        exc = ChannelFailures()
        output = []
        
        items = [(self._priorities[(channel, listener)], listener)
                 for listener in self.listeners[channel]]
        items.sort()
        for priority, listener in items:
            try:
                output.append(listener(*args, **kwargs))
            except KeyboardInterrupt:
                raise
            except SystemExit, e:
                # If we have previous errors ensure the exit code is non-zero
                if exc and e.code == 0:
                    e.code = 1
                raise
            except:
                exc.handle_exception()
                if channel == 'log':
                    # Assume any further messages to 'log' will fail.
                    pass
                else:
                    self.log("Error in %r listener %r" % (channel, listener),
                             level=40, traceback=True)
        if exc:
            raise exc
        return output
    
    def _clean_exit(self):
        """An atexit handler which asserts the Bus is not running."""
        if self.state != states.EXITING:
            warnings.warn(
                "The main thread is exiting, but the Bus is in the %r state; "
                "shutting it down automatically now. You must either call "
                "bus.block() after start(), or call bus.exit() before the "
                "main thread exits." % self.state, RuntimeWarning)
            self.exit()
    
    def start(self):
        """Start all services."""
        atexit.register(self._clean_exit)
        
        self.state = states.STARTING
        self.log('Bus STARTING')
        try:
            self.publish('start')
            self.state = states.STARTED
            self.log('Bus STARTED')
        except (KeyboardInterrupt, SystemExit):
            raise
        except:
            self.log("Shutting down due to error in start listener:",
                     level=40, traceback=True)
            e_info = sys.exc_info()
            try:
                self.exit()
            except:
                # Any stop/exit errors will be logged inside publish().
                pass
            raise e_info[0], e_info[1], e_info[2]
    
    def exit(self):
        """Stop all services and prepare to exit the process."""
        exitstate = self.state
        try:
            self.stop()
            
            self.state = states.EXITING
            self.log('Bus EXITING')
            self.publish('exit')
            # This isn't strictly necessary, but it's better than seeing
            # "Waiting for child threads to terminate..." and then nothing.
            self.log('Bus EXITED')
        except:
            # This method is often called asynchronously (whether thread,
            # signal handler, console handler, or atexit handler), so we
            # can't just let exceptions propagate out unhandled.
            # Assume it's been logged and just die.
            os._exit(70) # EX_SOFTWARE
        
        if exitstate == states.STARTING:
            # exit() was called before start() finished, possibly due to
            # Ctrl-C because a start listener got stuck. In this case,
            # we could get stuck in a loop where Ctrl-C never exits the
            # process, so we just call os.exit here.
            os._exit(70) # EX_SOFTWARE
    
    def restart(self):
        """Restart the process (may close connections).
        
        This method does not restart the process from the calling thread;
        instead, it stops the bus and asks the main thread to call execv.
        """
        self.execv = True
        self.exit()
    
    def graceful(self):
        """Advise all services to reload."""
        self.log('Bus graceful')
        self.publish('graceful')
    
    def block(self, interval=0.1):
        """Wait for the EXITING state, KeyboardInterrupt or SystemExit.
        
        This function is intended to be called only by the main thread.
        After waiting for the EXITING state, it also waits for all threads
        to terminate, and then calls os.execv if self.execv is True. This
        design allows another thread to call bus.restart, yet have the main
        thread perform the actual execv call (required on some platforms).
        """
        try:
            self.wait(states.EXITING, interval=interval, channel='main')
        except (KeyboardInterrupt, IOError):
            # The time.sleep call might raise
            # "IOError: [Errno 4] Interrupted function call" on KBInt.
            self.log('Keyboard Interrupt: shutting down bus')
            self.exit()
        except SystemExit:
            self.log('SystemExit raised: shutting down bus')
            self.exit()
            raise
        
        # Waiting for ALL child threads to finish is necessary on OS X.
        # See http://www.cherrypy.org/ticket/581.
        # It's also good to let them all shut down before allowing
        # the main thread to call atexit handlers.
        # See http://www.cherrypy.org/ticket/751.
        self.log("Waiting for child threads to terminate...")
        for t in threading.enumerate():
            if t != threading.currentThread() and t.isAlive():
                # Note that any dummy (external) threads are always daemonic.
                if hasattr(threading.Thread, "daemon"):
                    # Python 2.6+
                    d = t.daemon
                else:
                    d = t.isDaemon()
                if not d:
                    t.join()
        
        if self.execv:
            self._do_execv()
    
    def wait(self, state, interval=0.1, channel=None):
        """Wait for the given state(s)."""
        if isinstance(state, (tuple, list)):
            states = state
        else:
            states = [state]
        
        def _wait():
            while self.state not in states:
                time.sleep(interval)
                self.publish(channel)
        
        # From http://psyco.sourceforge.net/psycoguide/bugs.html:
        # "The compiled machine code does not include the regular polling
        # done by Python, meaning that a KeyboardInterrupt will not be
        # detected before execution comes back to the regular Python
        # interpreter. Your program cannot be interrupted if caught
        # into an infinite Psyco-compiled loop."
        try:
            sys.modules['psyco'].cannotcompile(_wait)
        except (KeyError, AttributeError):
            pass
        
        _wait()
    
    def _do_execv(self):
        """Re-execute the current process.
        
        This must be called from the main thread, because certain platforms
        (OS X) don't allow execv to be called in a child thread very well.
        """
        args = sys.argv[:]
        self.log('Re-spawning %s' % ' '.join(args))
        args.insert(0, sys.executable)
        if sys.platform == 'win32':
            args = ['"%s"' % arg for arg in args]

        os.chdir(_startup_cwd)
        os.execv(sys.executable, args)
    
    def stop(self):
        """Stop all services."""
        self.state = states.STOPPING
        self.log('Bus STOPPING')
        self.publish('stop')
        self.state = states.STOPPED
        self.log('Bus STOPPED')
    
    def start_with_callback(self, func, args=None, kwargs=None):
        """Start 'func' in a new thread T, then start self (and return T)."""
        if args is None:
            args = ()
        if kwargs is None:
            kwargs = {}
        args = (func,) + args
        
        def _callback(func, *a, **kw):
            self.wait(states.STARTED)
            func(*a, **kw)
        t = threading.Thread(target=_callback, args=args, kwargs=kwargs)
        t.setName('Bus Callback ' + t.getName())
        t.start()
        
        self.start()
        
        return t
    
    def log(self, msg="", level=20, traceback=False):
        """Log the given message. Append the last traceback if requested."""
        if traceback:
            exc = sys.exc_info()
            msg += "\n" + "".join(_traceback.format_exception(*exc))
        self.publish('log', msg, level)

bus = Bus()

########NEW FILE########
__FILENAME__ = ssl_builtin
"""A library for integrating pyOpenSSL with CherryPy.

The ssl module must be importable for SSL functionality.

To use this module, set CherryPyWSGIServer.ssl_adapter to an instance of
BuiltinSSLAdapter.

    ssl_adapter.certificate: the filename of the server SSL certificate.
    ssl_adapter.private_key: the filename of the server's private key file.
"""

try:
    import ssl
except ImportError:
    ssl = None

from cherrypy import wsgiserver


class BuiltinSSLAdapter(wsgiserver.SSLAdapter):
    """A wrapper for integrating Python's builtin ssl module with CherryPy."""
    
    def __init__(self, certificate, private_key, certificate_chain=None):
        if ssl is None:
            raise ImportError("You must install the ssl module to use HTTPS.")
        self.certificate = certificate
        self.private_key = private_key
        self.certificate_chain = certificate_chain
    
    def bind(self, sock):
        """Wrap and return the given socket."""
        return sock
    
    def wrap(self, sock):
        """Wrap and return the given socket, plus WSGI environ entries."""
        try:
            s = ssl.wrap_socket(sock, do_handshake_on_connect=True,
                    server_side=True, certfile=self.certificate,
                    keyfile=self.private_key, ssl_version=ssl.PROTOCOL_SSLv23)
        except ssl.SSLError, e:
            if e.errno == ssl.SSL_ERROR_EOF:
                # This is almost certainly due to the cherrypy engine
                # 'pinging' the socket to assert it's connectable;
                # the 'ping' isn't SSL.
                return None, {}
            elif e.errno == ssl.SSL_ERROR_SSL:
                if e.args[1].endswith('http request'):
                    # The client is speaking HTTP to an HTTPS server.
                    raise wsgiserver.NoSSLError
            raise
        return s, self.get_environ(s)
    
    # TODO: fill this out more with mod ssl env
    def get_environ(self, sock):
        """Create WSGI environ entries to be merged into each request."""
        cipher = sock.cipher()
        ssl_environ = {
            "wsgi.url_scheme": "https",
            "HTTPS": "on",
            'SSL_PROTOCOL': cipher[1],
            'SSL_CIPHER': cipher[0]
##            SSL_VERSION_INTERFACE 	string 	The mod_ssl program version
##            SSL_VERSION_LIBRARY 	string 	The OpenSSL program version
            }
        return ssl_environ
    
    def makefile(self, sock, mode='r', bufsize= -1):
        return wsgiserver.CP_fileobject(sock, mode, bufsize)


########NEW FILE########
__FILENAME__ = ssl_pyopenssl
"""A library for integrating pyOpenSSL with CherryPy.

The OpenSSL module must be importable for SSL functionality.
You can obtain it from http://pyopenssl.sourceforge.net/

To use this module, set CherryPyWSGIServer.ssl_adapter to an instance of
SSLAdapter. There are two ways to use SSL:

Method One:
    ssl_adapter.context: an instance of SSL.Context.
    
    If this is not None, it is assumed to be an SSL.Context instance,
    and will be passed to SSL.Connection on bind(). The developer is
    responsible for forming a valid Context object. This approach is
    to be preferred for more flexibility, e.g. if the cert and key are
    streams instead of files, or need decryption, or SSL.SSLv3_METHOD
    is desired instead of the default SSL.SSLv23_METHOD, etc. Consult
    the pyOpenSSL documentation for complete options.

Method Two (shortcut):
    ssl_adapter.certificate: the filename of the server SSL certificate.
    ssl_adapter.private_key: the filename of the server's private key file.
    
    Both are None by default. If ssl_adapter.context is None, but .private_key
    and .certificate are both given and valid, they will be read, and the
    context will be automatically created from them.
    
    ssl_adapter.certificate_chain: (optional) the filename of CA's intermediate
        certificate bundle. This is needed for cheaper "chained root" SSL
        certificates, and should be left as None if not required.
"""

import socket
import threading
import time

from cherrypy import wsgiserver

try:
    from OpenSSL import SSL
    from OpenSSL import crypto
except ImportError:
    SSL = None


class SSL_fileobject(wsgiserver.CP_fileobject):
    """SSL file object attached to a socket object."""
    
    ssl_timeout = 3
    ssl_retry = .01
    
    def _safe_call(self, is_reader, call, *args, **kwargs):
        """Wrap the given call with SSL error-trapping.
        
        is_reader: if False EOF errors will be raised. If True, EOF errors
            will return "" (to emulate normal sockets).
        """
        start = time.time()
        while True:
            try:
                return call(*args, **kwargs)
            except SSL.WantReadError:
                # Sleep and try again. This is dangerous, because it means
                # the rest of the stack has no way of differentiating
                # between a "new handshake" error and "client dropped".
                # Note this isn't an endless loop: there's a timeout below.
                time.sleep(self.ssl_retry)
            except SSL.WantWriteError:
                time.sleep(self.ssl_retry)
            except SSL.SysCallError, e:
                if is_reader and e.args == (-1, 'Unexpected EOF'):
                    return ""
                
                errnum = e.args[0]
                if is_reader and errnum in wsgiserver.socket_errors_to_ignore:
                    return ""
                raise socket.error(errnum)
            except SSL.Error, e:
                if is_reader and e.args == (-1, 'Unexpected EOF'):
                    return ""
                
                thirdarg = None
                try:
                    thirdarg = e.args[0][0][2]
                except IndexError:
                    pass
                
                if thirdarg == 'http request':
                    # The client is talking HTTP to an HTTPS server.
                    raise wsgiserver.NoSSLError()
                
                raise wsgiserver.FatalSSLAlert(*e.args)
            except:
                raise
            
            if time.time() - start > self.ssl_timeout:
                raise socket.timeout("timed out")
    
    def recv(self, *args, **kwargs):
        buf = []
        r = super(SSL_fileobject, self).recv
        while True:
            data = self._safe_call(True, r, *args, **kwargs)
            buf.append(data)
            p = self._sock.pending()
            if not p:
                return "".join(buf)
    
    def sendall(self, *args, **kwargs):
        return self._safe_call(False, super(SSL_fileobject, self).sendall,
                               *args, **kwargs)

    def send(self, *args, **kwargs):
        return self._safe_call(False, super(SSL_fileobject, self).send,
                               *args, **kwargs)


class SSLConnection:
    """A thread-safe wrapper for an SSL.Connection.
    
    *args: the arguments to create the wrapped SSL.Connection(*args).
    """
    
    def __init__(self, *args):
        self._ssl_conn = SSL.Connection(*args)
        self._lock = threading.RLock()
    
    for f in ('get_context', 'pending', 'send', 'write', 'recv', 'read',
              'renegotiate', 'bind', 'listen', 'connect', 'accept',
              'setblocking', 'fileno', 'close', 'get_cipher_list',
              'getpeername', 'getsockname', 'getsockopt', 'setsockopt',
              'makefile', 'get_app_data', 'set_app_data', 'state_string',
              'sock_shutdown', 'get_peer_certificate', 'want_read',
              'want_write', 'set_connect_state', 'set_accept_state',
              'connect_ex', 'sendall', 'settimeout', 'gettimeout'):
        exec("""def %s(self, *args):
        self._lock.acquire()
        try:
            return self._ssl_conn.%s(*args)
        finally:
            self._lock.release()
""" % (f, f))
    
    def shutdown(self, *args):
        self._lock.acquire()
        try:
            # pyOpenSSL.socket.shutdown takes no args
            return self._ssl_conn.shutdown()
        finally:
            self._lock.release()


class pyOpenSSLAdapter(wsgiserver.SSLAdapter):
    """A wrapper for integrating pyOpenSSL with CherryPy."""
    
    def __init__(self, certificate, private_key, certificate_chain=None):
        if SSL is None:
            raise ImportError("You must install pyOpenSSL to use HTTPS.")
        
        self.context = None
        self.certificate = certificate
        self.private_key = private_key
        self.certificate_chain = certificate_chain
        self._environ = None
    
    def bind(self, sock):
        """Wrap and return the given socket."""
        if self.context is None:
            self.context = self.get_context()
        conn = SSLConnection(self.context, sock)
        self._environ = self.get_environ()
        return conn
    
    def wrap(self, sock):
        """Wrap and return the given socket, plus WSGI environ entries."""
        return sock, self._environ.copy()
    
    def get_context(self):
        """Return an SSL.Context from self attributes."""
        # See http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/442473
        c = SSL.Context(SSL.SSLv23_METHOD)
        c.use_privatekey_file(self.private_key)
        if self.certificate_chain:
            c.load_verify_locations(self.certificate_chain)
        c.use_certificate_file(self.certificate)
        return c
    
    def get_environ(self):
        """Return WSGI environ entries to be merged into each request."""
        ssl_environ = {
            "HTTPS": "on",
            # pyOpenSSL doesn't provide access to any of these AFAICT
##            'SSL_PROTOCOL': 'SSLv2',
##            SSL_CIPHER 	string 	The cipher specification name
##            SSL_VERSION_INTERFACE 	string 	The mod_ssl program version
##            SSL_VERSION_LIBRARY 	string 	The OpenSSL program version
            }
        
        if self.certificate:
            # Server certificate attributes
            cert = open(self.certificate, 'rb').read()
            cert = crypto.load_certificate(crypto.FILETYPE_PEM, cert)
            ssl_environ.update({
                'SSL_SERVER_M_VERSION': cert.get_version(),
                'SSL_SERVER_M_SERIAL': cert.get_serial_number(),
##                'SSL_SERVER_V_START': Validity of server's certificate (start time),
##                'SSL_SERVER_V_END': Validity of server's certificate (end time),
                })
            
            for prefix, dn in [("I", cert.get_issuer()),
                               ("S", cert.get_subject())]:
                # X509Name objects don't seem to have a way to get the
                # complete DN string. Use str() and slice it instead,
                # because str(dn) == "<X509Name object '/C=US/ST=...'>"
                dnstr = str(dn)[18:-2]
                
                wsgikey = 'SSL_SERVER_%s_DN' % prefix
                ssl_environ[wsgikey] = dnstr
                
                # The DN should be of the form: /k1=v1/k2=v2, but we must allow
                # for any value to contain slashes itself (in a URL).
                while dnstr:
                    pos = dnstr.rfind("=")
                    dnstr, value = dnstr[:pos], dnstr[pos + 1:]
                    pos = dnstr.rfind("/")
                    dnstr, key = dnstr[:pos], dnstr[pos + 1:]
                    if key and value:
                        wsgikey = 'SSL_SERVER_%s_DN_%s' % (prefix, key)
                        ssl_environ[wsgikey] = value
        
        return ssl_environ
    
    def makefile(self, sock, mode='r', bufsize= -1):
        if SSL and isinstance(sock, SSL.ConnectionType):
            timeout = sock.gettimeout()
            f = SSL_fileobject(sock, mode, bufsize)
            f.ssl_timeout = timeout
            return f
        else:
            return wsgiserver.CP_fileobject(sock, mode, bufsize)


########NEW FILE########
__FILENAME__ = _cpchecker
import os
import warnings

import cherrypy


class Checker(object):
    """A checker for CherryPy sites and their mounted applications.
    
    on: set this to False to turn off the checker completely.
    
    When this object is called at engine startup, it executes each
    of its own methods whose names start with "check_". If you wish
    to disable selected checks, simply add a line in your global
    config which sets the appropriate method to False:
    
    [global]
    checker.check_skipped_app_config = False
    
    You may also dynamically add or replace check_* methods in this way.
    """
    
    on = True
    
    def __init__(self):
        self._populate_known_types()
    
    def __call__(self):
        """Run all check_* methods."""
        if self.on:
            oldformatwarning = warnings.formatwarning
            warnings.formatwarning = self.formatwarning
            try:
                for name in dir(self):
                    if name.startswith("check_"):
                        method = getattr(self, name)
                        if method and callable(method):
                            method()
            finally:
                warnings.formatwarning = oldformatwarning
    
    def formatwarning(self, message, category, filename, lineno, line=None):
        """Function to format a warning."""
        return "CherryPy Checker:\n%s\n\n" % message
    
    # This value should be set inside _cpconfig.
    global_config_contained_paths = False
    
    def check_app_config_entries_dont_start_with_script_name(self):
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            if not app.config:
                continue
            if sn == '':
                continue
            sn_atoms = sn.strip("/").split("/")
            for key in app.config.keys():
                key_atoms = key.strip("/").split("/")
                if key_atoms[:len(sn_atoms)] == sn_atoms:
                    warnings.warn(
                        "The application mounted at %r has config " \
                        "entries that start with its script name: %r" % (sn, key))
    
    def check_site_config_entries_in_app_config(self):
        for sn, app in cherrypy.tree.apps.iteritems():
            if not isinstance(app, cherrypy.Application):
                continue
            
            msg = []
            for section, entries in app.config.iteritems():
                if section.startswith('/'):
                    for key, value in entries.iteritems():
                        for n in ("engine.", "server.", "tree.", "checker."):
                            if key.startswith(n):
                                msg.append("[%s] %s = %s" % (section, key, value))
            if msg:
                msg.insert(0,
                    "The application mounted at %r contains the following "
                    "config entries, which are only allowed in site-wide "
                    "config. Move them to a [global] section and pass them "
                    "to cherrypy.config.update() instead of tree.mount()." % sn)
                warnings.warn(os.linesep.join(msg))
    
    def check_skipped_app_config(self):
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            if not app.config:
                msg = "The Application mounted at %r has an empty config." % sn
                if self.global_config_contained_paths:
                    msg += (" It looks like the config you passed to "
                            "cherrypy.config.update() contains application-"
                            "specific sections. You must explicitly pass "
                            "application config via "
                            "cherrypy.tree.mount(..., config=app_config)")
                warnings.warn(msg)
                return
    
    def check_app_config_brackets(self):
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            if not app.config:
                continue
            for key in app.config.keys():
                if key.startswith("[") or key.endswith("]"):
                    warnings.warn(
                        "The application mounted at %r has config " \
                        "section names with extraneous brackets: %r. "
                        "Config *files* need brackets; config *dicts* "
                        "(e.g. passed to tree.mount) do not." % (sn, key))
    
    def check_static_paths(self):
        # Use the dummy Request object in the main thread.
        request = cherrypy.request
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            request.app = app
            for section in app.config:
                # get_resource will populate request.config
                request.get_resource(section + "/dummy.html")
                conf = request.config.get
                
                if conf("tools.staticdir.on", False):
                    msg = ""
                    root = conf("tools.staticdir.root")
                    dir = conf("tools.staticdir.dir")
                    if dir is None:
                        msg = "tools.staticdir.dir is not set."
                    else:
                        fulldir = ""
                        if os.path.isabs(dir):
                            fulldir = dir
                            if root:
                                msg = ("dir is an absolute path, even "
                                       "though a root is provided.")
                                testdir = os.path.join(root, dir[1:])
                                if os.path.exists(testdir):
                                    msg += ("\nIf you meant to serve the "
                                            "filesystem folder at %r, remove "
                                            "the leading slash from dir." % testdir)
                        else:
                            if not root:
                                msg = "dir is a relative path and no root provided."
                            else:
                                fulldir = os.path.join(root, dir)
                                if not os.path.isabs(fulldir):
                                    msg = "%r is not an absolute path." % fulldir
                        
                        if fulldir and not os.path.exists(fulldir):
                            if msg:
                                msg += "\n"
                            msg += ("%r (root + dir) is not an existing "
                                    "filesystem path." % fulldir)
                    
                    if msg:
                        warnings.warn("%s\nsection: [%s]\nroot: %r\ndir: %r"
                                      % (msg, section, root, dir))
    
    
    # -------------------------- Compatibility -------------------------- #
    
    obsolete = {
        'server.default_content_type': 'tools.response_headers.headers',
        'log_access_file': 'log.access_file',
        'log_config_options': None,
        'log_file': 'log.error_file',
        'log_file_not_found': None,
        'log_request_headers': 'tools.log_headers.on',
        'log_to_screen': 'log.screen',
        'show_tracebacks': 'request.show_tracebacks',
        'throw_errors': 'request.throw_errors',
        'profiler.on': ('cherrypy.tree.mount(profiler.make_app('
                        'cherrypy.Application(Root())))'),
        }
    
    deprecated = {}
    
    def _compat(self, config):
        """Process config and warn on each obsolete or deprecated entry."""
        for section, conf in config.items():
            if isinstance(conf, dict):
                for k, v in conf.items():
                    if k in self.obsolete:
                        warnings.warn("%r is obsolete. Use %r instead.\n"
                                      "section: [%s]" % 
                                      (k, self.obsolete[k], section))
                    elif k in self.deprecated:
                        warnings.warn("%r is deprecated. Use %r instead.\n"
                                      "section: [%s]" % 
                                      (k, self.deprecated[k], section))
            else:
                if section in self.obsolete:
                    warnings.warn("%r is obsolete. Use %r instead."
                                  % (section, self.obsolete[section]))
                elif section in self.deprecated:
                    warnings.warn("%r is deprecated. Use %r instead."
                                  % (section, self.deprecated[section]))
    
    def check_compatibility(self):
        """Process config and warn on each obsolete or deprecated entry."""
        self._compat(cherrypy.config)
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            self._compat(app.config)
    
    
    # ------------------------ Known Namespaces ------------------------ #
    
    extra_config_namespaces = []
    
    def _known_ns(self, app):
        ns = ["wsgi"]
        ns.extend(app.toolboxes.keys())
        ns.extend(app.namespaces.keys())
        ns.extend(app.request_class.namespaces.keys())
        ns.extend(cherrypy.config.namespaces.keys())
        ns += self.extra_config_namespaces
        
        for section, conf in app.config.items():
            is_path_section = section.startswith("/")
            if is_path_section and isinstance(conf, dict):
                for k, v in conf.items():
                    atoms = k.split(".")
                    if len(atoms) > 1:
                        if atoms[0] not in ns:
                            # Spit out a special warning if a known
                            # namespace is preceded by "cherrypy."
                            if (atoms[0] == "cherrypy" and atoms[1] in ns):
                                msg = ("The config entry %r is invalid; "
                                       "try %r instead.\nsection: [%s]"
                                       % (k, ".".join(atoms[1:]), section))
                            else:
                                msg = ("The config entry %r is invalid, because "
                                       "the %r config namespace is unknown.\n"
                                       "section: [%s]" % (k, atoms[0], section))
                            warnings.warn(msg)
                        elif atoms[0] == "tools":
                            if atoms[1] not in dir(cherrypy.tools):
                                msg = ("The config entry %r may be invalid, "
                                       "because the %r tool was not found.\n"
                                       "section: [%s]" % (k, atoms[1], section))
                                warnings.warn(msg)
    
    def check_config_namespaces(self):
        """Process config and warn on each unknown config namespace."""
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            self._known_ns(app)


    
    
    # -------------------------- Config Types -------------------------- #
    
    known_config_types = {}
    
    def _populate_known_types(self):
        import __builtin__ as builtins
        b = [x for x in vars(builtins).values()
             if type(x) is type(str)]
        
        def traverse(obj, namespace):
            for name in dir(obj):
                # Hack for 3.2's warning about body_params
                if name == 'body_params':
                    continue
                vtype = type(getattr(obj, name, None))
                if vtype in b:
                    self.known_config_types[namespace + "." + name] = vtype
        
        traverse(cherrypy.request, "request")
        traverse(cherrypy.response, "response")
        traverse(cherrypy.server, "server")
        traverse(cherrypy.engine, "engine")
        traverse(cherrypy.log, "log")
    
    def _known_types(self, config):
        msg = ("The config entry %r in section %r is of type %r, "
               "which does not match the expected type %r.")
        
        for section, conf in config.items():
            if isinstance(conf, dict):
                for k, v in conf.items():
                    if v is not None:
                        expected_type = self.known_config_types.get(k, None)
                        vtype = type(v)
                        if expected_type and vtype != expected_type:
                            warnings.warn(msg % (k, section, vtype.__name__,
                                                 expected_type.__name__))
            else:
                k, v = section, conf
                if v is not None:
                    expected_type = self.known_config_types.get(k, None)
                    vtype = type(v)
                    if expected_type and vtype != expected_type:
                        warnings.warn(msg % (k, section, vtype.__name__,
                                             expected_type.__name__))
    
    def check_config_types(self):
        """Assert that config values are of the same type as default values."""
        self._known_types(cherrypy.config)
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            self._known_types(app.config)
    
    
    # -------------------- Specific config warnings -------------------- #
    
    def check_localhost(self):
        """Warn if any socket_host is 'localhost'. See #711."""
        for k, v in cherrypy.config.items():
            if k == 'server.socket_host' and v == 'localhost':
                warnings.warn("The use of 'localhost' as a socket host can "
                    "cause problems on newer systems, since 'localhost' can "
                    "map to either an IPv4 or an IPv6 address. You should "
                    "use '127.0.0.1' or '[::1]' instead.")

########NEW FILE########
__FILENAME__ = _cpconfig
"""Configuration system for CherryPy.

Configuration in CherryPy is implemented via dictionaries. Keys are strings
which name the mapped value, which may be of any type.


Architecture
------------

CherryPy Requests are part of an Application, which runs in a global context,
and configuration data may apply to any of those three scopes:

    Global: configuration entries which apply everywhere are stored in
    cherrypy.config.
    
    Application: entries which apply to each mounted application are stored
    on the Application object itself, as 'app.config'. This is a two-level
    dict where each key is a path, or "relative URL" (for example, "/" or
    "/path/to/my/page"), and each value is a config dict. Usually, this
    data is provided in the call to tree.mount(root(), config=conf),
    although you may also use app.merge(conf).
    
    Request: each Request object possesses a single 'Request.config' dict.
    Early in the request process, this dict is populated by merging global
    config entries, Application entries (whose path equals or is a parent
    of Request.path_info), and any config acquired while looking up the
    page handler (see next).


Declaration
-----------

Configuration data may be supplied as a Python dictionary, as a filename,
or as an open file object. When you supply a filename or file, CherryPy
uses Python's builtin ConfigParser; you declare Application config by
writing each path as a section header:

    [/path/to/my/page]
    request.stream = True

To declare global configuration entries, place them in a [global] section.

You may also declare config entries directly on the classes and methods
(page handlers) that make up your CherryPy application via the '_cp_config'
attribute. For example:

    class Demo:
        _cp_config = {'tools.gzip.on': True}
        
        def index(self):
            return "Hello world"
        index.exposed = True
        index._cp_config = {'request.show_tracebacks': False}

Note, however, that this behavior is only guaranteed for the default
dispatcher. Other dispatchers may have different restrictions on where
you can attach _cp_config attributes.


Namespaces
----------

Configuration keys are separated into namespaces by the first "." in the key.
Current namespaces:

    engine:     Controls the 'application engine', including autoreload.
                These can only be declared in the global config.
    tree:       Grafts cherrypy.Application objects onto cherrypy.tree.
                These can only be declared in the global config.
    hooks:      Declares additional request-processing functions.
    log:        Configures the logging for each application.
                These can only be declared in the global or / config.
    request:    Adds attributes to each Request.
    response:   Adds attributes to each Response.
    server:     Controls the default HTTP server via cherrypy.server.
                These can only be declared in the global config.
    tools:      Runs and configures additional request-processing packages.
    wsgi:       Adds WSGI middleware to an Application's "pipeline".
                These can only be declared in the app's root config ("/").
    checker:    Controls the 'checker', which looks for common errors in
                app state (including config) when the engine starts.
                Global config only.

The only key that does not exist in a namespace is the "environment" entry.
This special entry 'imports' other config entries from a template stored in
cherrypy._cpconfig.environments[environment]. It only applies to the global
config, and only when you use cherrypy.config.update.

You can define your own namespaces to be called at the Global, Application,
or Request level, by adding a named handler to cherrypy.config.namespaces,
app.namespaces, or app.request_class.namespaces. The name can
be any string, and the handler must be either a callable or a (Python 2.5
style) context manager.
"""

try:
    set
except NameError:
    from sets import Set as set

import cherrypy
from cherrypy.lib import reprconf

# Deprecated in  CherryPy 3.2--remove in 3.3
NamespaceSet = reprconf.NamespaceSet

def merge(base, other):
    """Merge one app config (from a dict, file, or filename) into another.
    
    If the given config is a filename, it will be appended to
    the list of files to monitor for "autoreload" changes.
    """
    if isinstance(other, basestring):
        cherrypy.engine.autoreload.files.add(other)
    
    # Load other into base
    for section, value_map in reprconf.as_dict(other).items():
        if not isinstance(value_map, dict):
            raise ValueError(
                "Application config must include section headers, but the "
                "config you tried to merge doesn't have any sections. "
                "Wrap your config in another dict with paths as section "
                "headers, for example: {'/': config}.")
        base.setdefault(section, {}).update(value_map)


class Config(reprconf.Config):
    """The 'global' configuration data for the entire CherryPy process."""

    def update(self, config):
        """Update self from a dict, file or filename."""
        if isinstance(config, basestring):
            # Filename
            cherrypy.engine.autoreload.files.add(config)
        reprconf.Config.update(self, config)

    def _apply(self, config):
        """Update self from a dict."""
        if isinstance(config.get("global", None), dict):
            if len(config) > 1:
                cherrypy.checker.global_config_contained_paths = True
            config = config["global"]
        if 'tools.staticdir.dir' in config:
            config['tools.staticdir.section'] = "global"
        reprconf.Config._apply(self, config)
    
    def __call__(self, *args, **kwargs):
        """Decorator for page handlers to set _cp_config."""
        if args:
            raise TypeError(
                "The cherrypy.config decorator does not accept positional "
                "arguments; you must use keyword arguments.")
        def tool_decorator(f):
            if not hasattr(f, "_cp_config"):
                f._cp_config = {}
            for k, v in kwargs.items():
                f._cp_config[k] = v
            return f
        return tool_decorator


Config.environments = environments = {
    "staging": {
        'engine.autoreload_on': False,
        'checker.on': False,
        'tools.log_headers.on': False,
        'request.show_tracebacks': False,
        'request.show_mismatched_params': False,
        },
    "production": {
        'engine.autoreload_on': False,
        'checker.on': False,
        'tools.log_headers.on': False,
        'request.show_tracebacks': False,
        'request.show_mismatched_params': False,
        'log.screen': False,
        },
    "embedded": {
        # For use with CherryPy embedded in another deployment stack.
        'engine.autoreload_on': False,
        'checker.on': False,
        'tools.log_headers.on': False,
        'request.show_tracebacks': False,
        'request.show_mismatched_params': False,
        'log.screen': False,
        'engine.SIGHUP': None,
        'engine.SIGTERM': None,
        },
    "test_suite": {
        'engine.autoreload_on': False,
        'checker.on': False,
        'tools.log_headers.on': False,
        'request.show_tracebacks': True,
        'request.show_mismatched_params': True,
        'log.screen': False,
        },
    }


def _server_namespace_handler(k, v):
    """Config handler for the "server" namespace."""
    atoms = k.split(".", 1)
    if len(atoms) > 1:
        # Special-case config keys of the form 'server.servername.socket_port'
        # to configure additional HTTP servers.
        if not hasattr(cherrypy, "servers"):
            cherrypy.servers = {}
        
        servername, k = atoms
        if servername not in cherrypy.servers:
            from cherrypy import _cpserver
            cherrypy.servers[servername] = _cpserver.Server()
            # On by default, but 'on = False' can unsubscribe it (see below).
            cherrypy.servers[servername].subscribe()
        
        if k == 'on':
            if v:
                cherrypy.servers[servername].subscribe()
            else:
                cherrypy.servers[servername].unsubscribe()
        else:
            setattr(cherrypy.servers[servername], k, v)
    else:
        setattr(cherrypy.server, k, v)
Config.namespaces["server"] = _server_namespace_handler

def _engine_namespace_handler(k, v):
    """Backward compatibility handler for the "engine" namespace."""
    engine = cherrypy.engine
    if k == 'autoreload_on':
        if v:
            engine.autoreload.subscribe()
        else:
            engine.autoreload.unsubscribe()
    elif k == 'autoreload_frequency':
        engine.autoreload.frequency = v
    elif k == 'autoreload_match':
        engine.autoreload.match = v
    elif k == 'reload_files':
        engine.autoreload.files = set(v)
    elif k == 'deadlock_poll_freq':
        engine.timeout_monitor.frequency = v
    elif k == 'SIGHUP':
        engine.listeners['SIGHUP'] = set([v])
    elif k == 'SIGTERM':
        engine.listeners['SIGTERM'] = set([v])
    elif "." in k:
        plugin, attrname = k.split(".", 1)
        plugin = getattr(engine, plugin)
        if attrname == 'on':
            if v and hasattr(getattr(plugin, 'subscribe', None), '__call__'):
                plugin.subscribe()
                return
            elif (not v) and hasattr(getattr(plugin, 'unsubscribe', None), '__call__'):
                plugin.unsubscribe()
                return
        setattr(plugin, attrname, v)
    else:
        setattr(engine, k, v)
Config.namespaces["engine"] = _engine_namespace_handler


def _tree_namespace_handler(k, v):
    """Namespace handler for the 'tree' config namespace."""
    cherrypy.tree.graft(v, v.script_name)
    cherrypy.engine.log("Mounted: %s on %s" % (v, v.script_name or "/"))
Config.namespaces["tree"] = _tree_namespace_handler



########NEW FILE########
__FILENAME__ = _cpdispatch
"""CherryPy dispatchers.

A 'dispatcher' is the object which looks up the 'page handler' callable
and collects config for the current request based on the path_info, other
request attributes, and the application architecture. The core calls the
dispatcher as early as possible, passing it a 'path_info' argument.

The default dispatcher discovers the page handler by matching path_info
to a hierarchical arrangement of objects, starting at request.app.root.
"""

import cherrypy


class PageHandler(object):
    """Callable which sets response.body."""
    
    def __init__(self, callable, *args, **kwargs):
        self.callable = callable
        self.args = args
        self.kwargs = kwargs
    
    def __call__(self):
        try:
            return self.callable(*self.args, **self.kwargs)
        except TypeError, x:
            try:
                test_callable_spec(self.callable, self.args, self.kwargs)
            except cherrypy.HTTPError, error:
                raise error
            except:
                raise x
            raise


def test_callable_spec(callable, callable_args, callable_kwargs):
    """
    Inspect callable and test to see if the given args are suitable for it.

    When an error occurs during the handler's invoking stage there are 2
    erroneous cases:
    1.  Too many parameters passed to a function which doesn't define
        one of *args or **kwargs.
    2.  Too little parameters are passed to the function.

    There are 3 sources of parameters to a cherrypy handler.
    1.  query string parameters are passed as keyword parameters to the handler.
    2.  body parameters are also passed as keyword parameters.
    3.  when partial matching occurs, the final path atoms are passed as
        positional args.
    Both the query string and path atoms are part of the URI.  If they are
    incorrect, then a 404 Not Found should be raised. Conversely the body
    parameters are part of the request; if they are invalid a 400 Bad Request.
    """
    show_mismatched_params = getattr(
        cherrypy.serving.request, 'show_mismatched_params', False)
    try:
        (args, varargs, varkw, defaults) = inspect.getargspec(callable)
    except TypeError:
        if isinstance(callable, object) and hasattr(callable, '__call__'):
            (args, varargs, varkw, defaults) = inspect.getargspec(callable.__call__)
        else:
            # If it wasn't one of our own types, re-raise 
            # the original error
            raise

    if args and args[0] == 'self':
        args = args[1:]

    arg_usage = dict([(arg, 0,) for arg in args])
    vararg_usage = 0
    varkw_usage = 0
    extra_kwargs = set()

    for i, value in enumerate(callable_args):
        try:
            arg_usage[args[i]] += 1
        except IndexError:
            vararg_usage += 1

    for key in callable_kwargs.keys():
        try:
            arg_usage[key] += 1
        except KeyError:
            varkw_usage += 1
            extra_kwargs.add(key)

    # figure out which args have defaults.
    args_with_defaults = args[-len(defaults or []):]
    for i, val in enumerate(defaults or []):
        # Defaults take effect only when the arg hasn't been used yet.
        if arg_usage[args_with_defaults[i]] == 0:
            arg_usage[args_with_defaults[i]] += 1

    missing_args = []
    multiple_args = []
    for key, usage in arg_usage.items():
        if usage == 0:
            missing_args.append(key)
        elif usage > 1:
            multiple_args.append(key)

    if missing_args:
        # In the case where the method allows body arguments
        # there are 3 potential errors:
        # 1. not enough query string parameters -> 404
        # 2. not enough body parameters -> 400
        # 3. not enough path parts (partial matches) -> 404
        #
        # We can't actually tell which case it is, 
        # so I'm raising a 404 because that covers 2/3 of the
        # possibilities
        # 
        # In the case where the method does not allow body
        # arguments it's definitely a 404.
        message = None
        if show_mismatched_params:
            message = "Missing parameters: %s" % ",".join(missing_args)
        raise cherrypy.HTTPError(404, message=message)

    # the extra positional arguments come from the path - 404 Not Found
    if not varargs and vararg_usage > 0:
        raise cherrypy.HTTPError(404)

    body_params = cherrypy.serving.request.body.params or {}
    body_params = set(body_params.keys())
    qs_params = set(callable_kwargs.keys()) - body_params

    if multiple_args:
        if qs_params.intersection(set(multiple_args)):
            # If any of the multiple parameters came from the query string then
            # it's a 404 Not Found
            error = 404
        else:
            # Otherwise it's a 400 Bad Request
            error = 400

        message = None
        if show_mismatched_params:
            message = "Multiple values for parameters: "\
                    "%s" % ",".join(multiple_args)
        raise cherrypy.HTTPError(error, message=message)

    if not varkw and varkw_usage > 0:

        # If there were extra query string parameters, it's a 404 Not Found
        extra_qs_params = set(qs_params).intersection(extra_kwargs)
        if extra_qs_params:
            message = None
            if show_mismatched_params:
                message = "Unexpected query string "\
                        "parameters: %s" % ", ".join(extra_qs_params)
            raise cherrypy.HTTPError(404, message=message)

        # If there were any extra body parameters, it's a 400 Not Found
        extra_body_params = set(body_params).intersection(extra_kwargs)
        if extra_body_params:
            message = None
            if show_mismatched_params:
                message = "Unexpected body parameters: "\
                        "%s" % ", ".join(extra_body_params)
            raise cherrypy.HTTPError(400, message=message)


try:
    import inspect
except ImportError:
    test_callable_spec = lambda callable, args, kwargs: None



class LateParamPageHandler(PageHandler):
    """When passing cherrypy.request.params to the page handler, we do not
    want to capture that dict too early; we want to give tools like the
    decoding tool a chance to modify the params dict in-between the lookup
    of the handler and the actual calling of the handler. This subclass
    takes that into account, and allows request.params to be 'bound late'
    (it's more complicated than that, but that's the effect).
    """
    
    def _get_kwargs(self):
        kwargs = cherrypy.serving.request.params.copy()
        if self._kwargs:
            kwargs.update(self._kwargs)
        return kwargs
    
    def _set_kwargs(self, kwargs):
        self._kwargs = kwargs
    
    kwargs = property(_get_kwargs, _set_kwargs,
                      doc='page handler kwargs (with '
                      'cherrypy.request.params copied in)')


class Dispatcher(object):
    """CherryPy Dispatcher which walks a tree of objects to find a handler.
    
    The tree is rooted at cherrypy.request.app.root, and each hierarchical
    component in the path_info argument is matched to a corresponding nested
    attribute of the root object. Matching handlers must have an 'exposed'
    attribute which evaluates to True. The special method name "index"
    matches a URI which ends in a slash ("/"). The special method name
    "default" may match a portion of the path_info (but only when no longer
    substring of the path_info matches some other object).
    
    This is the default, built-in dispatcher for CherryPy.
    """
    __metaclass__ = cherrypy._AttributeDocstrings

    dispatch_method_name = '_cp_dispatch'
    dispatch_method_name__doc = """
    The name of the dispatch method that nodes may optionally implement
    to provide their own dynamic dispatch algorithm.
    """
    
    def __init__(self, dispatch_method_name=None):
        if dispatch_method_name:
            self.dispatch_method_name = dispatch_method_name

    def __call__(self, path_info):
        """Set handler and config for the current request."""
        request = cherrypy.serving.request
        func, vpath = self.find_handler(path_info)
        
        if func:
            # Decode any leftover %2F in the virtual_path atoms.
            vpath = [x.replace("%2F", "/") for x in vpath]
            request.handler = LateParamPageHandler(func, *vpath)
        else:
            request.handler = cherrypy.NotFound()
    
    def find_handler(self, path):
        """Return the appropriate page handler, plus any virtual path.
        
        This will return two objects. The first will be a callable,
        which can be used to generate page output. Any parameters from
        the query string or request body will be sent to that callable
        as keyword arguments.
        
        The callable is found by traversing the application's tree,
        starting from cherrypy.request.app.root, and matching path
        components to successive objects in the tree. For example, the
        URL "/path/to/handler" might return root.path.to.handler.
        
        The second object returned will be a list of names which are
        'virtual path' components: parts of the URL which are dynamic,
        and were not used when looking up the handler.
        These virtual path components are passed to the handler as
        positional arguments.
        """
        request = cherrypy.serving.request
        app = request.app
        root = app.root
        dispatch_name = self.dispatch_method_name
        
        # Get config for the root object/path.
        curpath = ""
        nodeconf = {}
        if hasattr(root, "_cp_config"):
            nodeconf.update(root._cp_config)
        if "/" in app.config:
            nodeconf.update(app.config["/"])
        object_trail = [['root', root, nodeconf, curpath]]
        
        node = root
        names = [x for x in path.strip('/').split('/') if x] + ['index']
        iternames = names[:]
        while iternames:
            name = iternames[0]
            # map to legal Python identifiers (replace '.' with '_')
            objname = name.replace('.', '_')
            
            nodeconf = {}
            subnode = getattr(node, objname, None)
            if subnode is None:
                dispatch = getattr(node, dispatch_name, None)
                if dispatch and callable(dispatch) and not \
                        getattr(dispatch, 'exposed', False):
                    subnode = dispatch(vpath=iternames)
            name = iternames.pop(0)
            node = subnode

            if node is not None:
                # Get _cp_config attached to this node.
                if hasattr(node, "_cp_config"):
                    nodeconf.update(node._cp_config)
            
            # Mix in values from app.config for this path.
            curpath = "/".join((curpath, name))
            if curpath in app.config:
                nodeconf.update(app.config[curpath])
            
            object_trail.append([name, node, nodeconf, curpath])
        
        def set_conf():
            """Collapse all object_trail config into cherrypy.request.config."""
            base = cherrypy.config.copy()
            # Note that we merge the config from each node
            # even if that node was None.
            for name, obj, conf, curpath in object_trail:
                base.update(conf)
                if 'tools.staticdir.dir' in conf:
                    base['tools.staticdir.section'] = curpath
            return base
        
        # Try successive objects (reverse order)
        num_candidates = len(object_trail) - 1
        for i in range(num_candidates, -1, -1):
            
            name, candidate, nodeconf, curpath = object_trail[i]
            if candidate is None:
                continue
            
            # Try a "default" method on the current leaf.
            if hasattr(candidate, "default"):
                defhandler = candidate.default
                if getattr(defhandler, 'exposed', False):
                    # Insert any extra _cp_config from the default handler.
                    conf = getattr(defhandler, "_cp_config", {})
                    object_trail.insert(i + 1, ["default", defhandler, conf, curpath])
                    request.config = set_conf()
                    # See http://www.cherrypy.org/ticket/613
                    request.is_index = path.endswith("/")
                    return defhandler, names[i:-1]
            
            # Uncomment the next line to restrict positional params to "default".
            # if i < num_candidates - 2: continue
            
            # Try the current leaf.
            if getattr(candidate, 'exposed', False):
                request.config = set_conf()
                if i == num_candidates:
                    # We found the extra ".index". Mark request so tools
                    # can redirect if path_info has no trailing slash.
                    request.is_index = True
                else:
                    # We're not at an 'index' handler. Mark request so tools
                    # can redirect if path_info has NO trailing slash.
                    # Note that this also includes handlers which take
                    # positional parameters (virtual paths).
                    request.is_index = False
                return candidate, names[i:-1]
        
        # We didn't find anything
        request.config = set_conf()
        return None, []


class MethodDispatcher(Dispatcher):
    """Additional dispatch based on cherrypy.request.method.upper().
    
    Methods named GET, POST, etc will be called on an exposed class.
    The method names must be all caps; the appropriate Allow header
    will be output showing all capitalized method names as allowable
    HTTP verbs.
    
    Note that the containing class must be exposed, not the methods.
    """
    
    def __call__(self, path_info):
        """Set handler and config for the current request."""
        request = cherrypy.serving.request
        resource, vpath = self.find_handler(path_info)
        
        if resource:
            # Set Allow header
            avail = [m for m in dir(resource) if m.isupper()]
            if "GET" in avail and "HEAD" not in avail:
                avail.append("HEAD")
            avail.sort()
            cherrypy.serving.response.headers['Allow'] = ", ".join(avail)
            
            # Find the subhandler
            meth = request.method.upper()
            func = getattr(resource, meth, None)
            if func is None and meth == "HEAD":
                func = getattr(resource, "GET", None)
            if func:
                # Grab any _cp_config on the subhandler.
                if hasattr(func, "_cp_config"):
                    request.config.update(func._cp_config)
                
                # Decode any leftover %2F in the virtual_path atoms.
                vpath = [x.replace("%2F", "/") for x in vpath]
                request.handler = LateParamPageHandler(func, *vpath)
            else:
                request.handler = cherrypy.HTTPError(405)
        else:
            request.handler = cherrypy.NotFound()


class RoutesDispatcher(object):
    """A Routes based dispatcher for CherryPy."""
    
    def __init__(self, full_result=False):
        """
        Routes dispatcher

        Set full_result to True if you wish the controller
        and the action to be passed on to the page handler
        parameters. By default they won't be.
        """
        import routes
        self.full_result = full_result
        self.controllers = {}
        self.mapper = routes.Mapper()
        self.mapper.controller_scan = self.controllers.keys
        
    def connect(self, name, route, controller, **kwargs):
        self.controllers[name] = controller
        self.mapper.connect(name, route, controller=name, **kwargs)
    
    def redirect(self, url):
        raise cherrypy.HTTPRedirect(url)
    
    def __call__(self, path_info):
        """Set handler and config for the current request."""
        func = self.find_handler(path_info)
        if func:
            cherrypy.serving.request.handler = LateParamPageHandler(func)
        else:
            cherrypy.serving.request.handler = cherrypy.NotFound()
    
    def find_handler(self, path_info):
        """Find the right page handler, and set request.config."""
        import routes
        
        request = cherrypy.serving.request
        
        config = routes.request_config()
        config.mapper = self.mapper
        if hasattr(request, 'wsgi_environ'):
            config.environ = request.wsgi_environ
        config.host = request.headers.get('Host', None)
        config.protocol = request.scheme
        config.redirect = self.redirect
        
        result = self.mapper.match(path_info)
        
        config.mapper_dict = result
        params = {}
        if result:
            params = result.copy()
        if not self.full_result:
            params.pop('controller', None)
            params.pop('action', None)
        request.params.update(params)
        
        # Get config for the root object/path.
        request.config = base = cherrypy.config.copy()
        curpath = ""
        
        def merge(nodeconf):
            if 'tools.staticdir.dir' in nodeconf:
                nodeconf['tools.staticdir.section'] = curpath or "/"
            base.update(nodeconf)
        
        app = request.app
        root = app.root
        if hasattr(root, "_cp_config"):
            merge(root._cp_config)
        if "/" in app.config:
            merge(app.config["/"])
        
        # Mix in values from app.config.
        atoms = [x for x in path_info.split("/") if x]
        if atoms:
            last = atoms.pop()
        else:
            last = None
        for atom in atoms:
            curpath = "/".join((curpath, atom))
            if curpath in app.config:
                merge(app.config[curpath])
        
        handler = None
        if result:
            controller = result.get('controller', None)
            controller = self.controllers.get(controller)
            if controller:
                # Get config from the controller.
                if hasattr(controller, "_cp_config"):
                    merge(controller._cp_config)
            
            action = result.get('action', None)
            if action is not None:
                handler = getattr(controller, action, None)
                # Get config from the handler 
                if hasattr(handler, "_cp_config"): 
                    merge(handler._cp_config)
                    
        # Do the last path atom here so it can
        # override the controller's _cp_config.
        if last:
            curpath = "/".join((curpath, last))
            if curpath in app.config:
                merge(app.config[curpath])
        
        return handler


def XMLRPCDispatcher(next_dispatcher=Dispatcher()):
    from cherrypy.lib import xmlrpc
    def xmlrpc_dispatch(path_info):
        path_info = xmlrpc.patched_path(path_info)
        return next_dispatcher(path_info)
    return xmlrpc_dispatch


def VirtualHost(next_dispatcher=Dispatcher(), use_x_forwarded_host=True, **domains):
    """Select a different handler based on the Host header.
    
    This can be useful when running multiple sites within one CP server.
    It allows several domains to point to different parts of a single
    website structure. For example:
    
        http://www.domain.example  ->  root
        http://www.domain2.example  ->  root/domain2/
        http://www.domain2.example:443  ->  root/secure
    
    can be accomplished via the following config:
    
        [/]
        request.dispatch = cherrypy.dispatch.VirtualHost(
            **{'www.domain2.example': '/domain2',
               'www.domain2.example:443': '/secure',
              })
    
    next_dispatcher: the next dispatcher object in the dispatch chain.
        The VirtualHost dispatcher adds a prefix to the URL and calls
        another dispatcher. Defaults to cherrypy.dispatch.Dispatcher().
    
    use_x_forwarded_host: if True (the default), any "X-Forwarded-Host"
        request header will be used instead of the "Host" header. This
        is commonly added by HTTP servers (such as Apache) when proxying.
    
    **domains: a dict of {host header value: virtual prefix} pairs.
        The incoming "Host" request header is looked up in this dict,
        and, if a match is found, the corresponding "virtual prefix"
        value will be prepended to the URL path before calling the
        next dispatcher. Note that you often need separate entries
        for "example.com" and "www.example.com". In addition, "Host"
        headers may contain the port number.
    """
    from cherrypy.lib import httputil
    def vhost_dispatch(path_info):
        request = cherrypy.serving.request
        header = request.headers.get
        
        domain = header('Host', '')
        if use_x_forwarded_host:
            domain = header("X-Forwarded-Host", domain)
        
        prefix = domains.get(domain, "")
        if prefix:
            path_info = httputil.urljoin(prefix, path_info)
        
        result = next_dispatcher(path_info)
        
        # Touch up staticdir config. See http://www.cherrypy.org/ticket/614.
        section = request.config.get('tools.staticdir.section')
        if section:
            section = section[len(prefix):]
            request.config['tools.staticdir.section'] = section
        
        return result
    return vhost_dispatch


########NEW FILE########
__FILENAME__ = _cperror
"""Error classes for CherryPy."""

from cgi import escape as _escape
from sys import exc_info as _exc_info
from traceback import format_exception as _format_exception
from urlparse import urljoin as _urljoin
from cherrypy.lib import httputil as _httputil


class CherryPyException(Exception):
    pass


class TimeoutError(CherryPyException):
    """Exception raised when Response.timed_out is detected."""
    pass


class InternalRedirect(CherryPyException):
    """Exception raised to switch to the handler for a different URL.
    
    Any request.params must be supplied in a query string.
    """
    
    def __init__(self, path, query_string=""):
        import cherrypy
        self.request = cherrypy.serving.request
        
        self.query_string = query_string
        if "?" in path:
            # Separate any params included in the path
            path, self.query_string = path.split("?", 1)
        
        # Note that urljoin will "do the right thing" whether url is:
        #  1. a URL relative to root (e.g. "/dummy")
        #  2. a URL relative to the current path
        # Note that any query string will be discarded.
        path = _urljoin(self.request.path_info, path)
        
        # Set a 'path' member attribute so that code which traps this
        # error can have access to it.
        self.path = path
        
        CherryPyException.__init__(self, path, self.query_string)


class HTTPRedirect(CherryPyException):
    """Exception raised when the request should be redirected.
    
    The new URL must be passed as the first argument to the Exception,
    e.g., HTTPRedirect(newUrl). Multiple URLs are allowed. If a URL is
    absolute, it will be used as-is. If it is relative, it is assumed
    to be relative to the current cherrypy.request.path_info.
    """
    
    def __init__(self, urls, status=None):
        import cherrypy
        request = cherrypy.serving.request
        
        if isinstance(urls, basestring):
            urls = [urls]
        
        abs_urls = []
        for url in urls:
            # Note that urljoin will "do the right thing" whether url is:
            #  1. a complete URL with host (e.g. "http://www.example.com/test")
            #  2. a URL relative to root (e.g. "/dummy")
            #  3. a URL relative to the current path
            # Note that any query string in cherrypy.request is discarded.
            url = _urljoin(cherrypy.url(), url)
            abs_urls.append(url)
        self.urls = abs_urls
        
        # RFC 2616 indicates a 301 response code fits our goal; however,
        # browser support for 301 is quite messy. Do 302/303 instead. See
        # http://www.alanflavell.org.uk/www/post-redirect.html
        if status is None:
            if request.protocol >= (1, 1):
                status = 303
            else:
                status = 302
        else:
            status = int(status)
            if status < 300 or status > 399:
                raise ValueError("status must be between 300 and 399.")
        
        self.status = status
        CherryPyException.__init__(self, abs_urls, status)
    
    def set_response(self):
        """Modify cherrypy.response status, headers, and body to represent self.
        
        CherryPy uses this internally, but you can also use it to create an
        HTTPRedirect object and set its output without *raising* the exception.
        """
        import cherrypy
        response = cherrypy.serving.response
        response.status = status = self.status
        
        if status in (300, 301, 302, 303, 307):
            response.headers['Content-Type'] = "text/html;charset=utf-8"
            # "The ... URI SHOULD be given by the Location field
            # in the response."
            response.headers['Location'] = self.urls[0]
            
            # "Unless the request method was HEAD, the entity of the response
            # SHOULD contain a short hypertext note with a hyperlink to the
            # new URI(s)."
            msg = {300: "This resource can be found at <a href='%s'>%s</a>.",
                   301: "This resource has permanently moved to <a href='%s'>%s</a>.",
                   302: "This resource resides temporarily at <a href='%s'>%s</a>.",
                   303: "This resource can be found at <a href='%s'>%s</a>.",
                   307: "This resource has moved temporarily to <a href='%s'>%s</a>.",
                   }[status]
            msgs = [msg % (u, u) for u in self.urls]
            response.body = "<br />\n".join(msgs)
            # Previous code may have set C-L, so we have to reset it
            # (allow finalize to set it).
            response.headers.pop('Content-Length', None)
        elif status == 304:
            # Not Modified.
            # "The response MUST include the following header fields:
            # Date, unless its omission is required by section 14.18.1"
            # The "Date" header should have been set in Response.__init__
            
            # "...the response SHOULD NOT include other entity-headers."
            for key in ('Allow', 'Content-Encoding', 'Content-Language',
                        'Content-Length', 'Content-Location', 'Content-MD5',
                        'Content-Range', 'Content-Type', 'Expires',
                        'Last-Modified'):
                if key in response.headers:
                    del response.headers[key]
            
            # "The 304 response MUST NOT contain a message-body."
            response.body = None
            # Previous code may have set C-L, so we have to reset it.
            response.headers.pop('Content-Length', None)
        elif status == 305:
            # Use Proxy.
            # self.urls[0] should be the URI of the proxy.
            response.headers['Location'] = self.urls[0]
            response.body = None
            # Previous code may have set C-L, so we have to reset it.
            response.headers.pop('Content-Length', None)
        else:
            raise ValueError("The %s status code is unknown." % status)
    
    def __call__(self):
        """Use this exception as a request.handler (raise self)."""
        raise self


def clean_headers(status):
    """Remove any headers which should not apply to an error response."""
    import cherrypy
    
    response = cherrypy.serving.response
    
    # Remove headers which applied to the original content,
    # but do not apply to the error page.
    respheaders = response.headers
    for key in ["Accept-Ranges", "Age", "ETag", "Location", "Retry-After",
                "Vary", "Content-Encoding", "Content-Length", "Expires",
                "Content-Location", "Content-MD5", "Last-Modified"]:
        if key in respheaders:
            del respheaders[key]
    
    if status != 416:
        # A server sending a response with status code 416 (Requested
        # range not satisfiable) SHOULD include a Content-Range field
        # with a byte-range-resp-spec of "*". The instance-length
        # specifies the current length of the selected resource.
        # A response with status code 206 (Partial Content) MUST NOT
        # include a Content-Range field with a byte-range- resp-spec of "*".
        if "Content-Range" in respheaders:
            del respheaders["Content-Range"]


class HTTPError(CherryPyException):
    """ Exception used to return an HTTP error code (4xx-5xx) to the client.
        This exception will automatically set the response status and body.
        
        A custom message (a long description to display in the browser)
        can be provided in place of the default.
    """
    
    def __init__(self, status=500, message=None):
        self.status = status
        try:
            self.code, self.reason, defaultmsg = _httputil.valid_status(status)
        except ValueError, x:
            raise self.__class__(500, x.args[0])
        
        if self.code < 400 or self.code > 599:
            raise ValueError("status must be between 400 and 599.")
        
        # See http://www.python.org/dev/peps/pep-0352/
        # self.message = message
        self._message = message or defaultmsg
        CherryPyException.__init__(self, status, message)
    
    def set_response(self):
        """Modify cherrypy.response status, headers, and body to represent self.
        
        CherryPy uses this internally, but you can also use it to create an
        HTTPError object and set its output without *raising* the exception.
        """
        import cherrypy
        
        response = cherrypy.serving.response
        
        clean_headers(self.code)
        
        # In all cases, finalize will be called after this method,
        # so don't bother cleaning up response values here.
        response.status = self.status
        tb = None
        if cherrypy.serving.request.show_tracebacks:
            tb = format_exc()
        response.headers['Content-Type'] = "text/html;charset=utf-8"
        response.headers.pop('Content-Length', None)
        
        content = self.get_error_page(self.status, traceback=tb,
                                      message=self._message)
        response.body = content
        
        _be_ie_unfriendly(self.code)
    
    def get_error_page(self, *args, **kwargs):
        return get_error_page(*args, **kwargs)
    
    def __call__(self):
        """Use this exception as a request.handler (raise self)."""
        raise self


class NotFound(HTTPError):
    """Exception raised when a URL could not be mapped to any handler (404)."""
    
    def __init__(self, path=None):
        if path is None:
            import cherrypy
            request = cherrypy.serving.request
            path = request.script_name + request.path_info
        self.args = (path,)
        HTTPError.__init__(self, 404, "The path '%s' was not found." % path)


_HTTPErrorTemplate = '''<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
    <title>%(status)s</title>
    <style type="text/css">
    #powered_by {
        margin-top: 20px;
        border-top: 2px solid black;
        font-style: italic;
    }

    #traceback {
        color: red;
    }
    </style>
</head>
    <body>
        <h2>%(status)s</h2>
        <p>%(message)s</p>
        <pre id="traceback">%(traceback)s</pre>
    <div id="powered_by">
    <span>Powered by <a href="http://www.cherrypy.org">CherryPy %(version)s</a></span>
    </div>
    </body>
</html>
'''

def get_error_page(status, **kwargs):
    """Return an HTML page, containing a pretty error response.
    
    status should be an int or a str.
    kwargs will be interpolated into the page template.
    """
    import cherrypy
    
    try:
        code, reason, message = _httputil.valid_status(status)
    except ValueError, x:
        raise cherrypy.HTTPError(500, x.args[0])
    
    # We can't use setdefault here, because some
    # callers send None for kwarg values.
    if kwargs.get('status') is None:
        kwargs['status'] = "%s %s" % (code, reason)
    if kwargs.get('message') is None:
        kwargs['message'] = message
    if kwargs.get('traceback') is None:
        kwargs['traceback'] = ''
    if kwargs.get('version') is None:
        kwargs['version'] = cherrypy.__version__
    
    for k, v in kwargs.iteritems():
        if v is None:
            kwargs[k] = ""
        else:
            kwargs[k] = _escape(kwargs[k])
    
    # Use a custom template or callable for the error page?
    pages = cherrypy.serving.request.error_page
    error_page = pages.get(code) or pages.get('default')
    if error_page:
        try:
            if callable(error_page):
                return error_page(**kwargs)
            else:
                return open(error_page, 'rb').read() % kwargs
        except:
            e = _format_exception(*_exc_info())[-1]
            m = kwargs['message']
            if m:
                m += "<br />"
            m += "In addition, the custom error page failed:\n<br />%s" % e
            kwargs['message'] = m
    
    return _HTTPErrorTemplate % kwargs


_ie_friendly_error_sizes = {
    400: 512, 403: 256, 404: 512, 405: 256,
    406: 512, 408: 512, 409: 512, 410: 256,
    500: 512, 501: 512, 505: 512,
    }


def _be_ie_unfriendly(status):
    import cherrypy
    response = cherrypy.serving.response
    
    # For some statuses, Internet Explorer 5+ shows "friendly error
    # messages" instead of our response.body if the body is smaller
    # than a given size. Fix this by returning a body over that size
    # (by adding whitespace).
    # See http://support.microsoft.com/kb/q218155/
    s = _ie_friendly_error_sizes.get(status, 0)
    if s:
        s += 1
        # Since we are issuing an HTTP error status, we assume that
        # the entity is short, and we should just collapse it.
        content = response.collapse_body()
        l = len(content)
        if l and l < s:
            # IN ADDITION: the response must be written to IE
            # in one chunk or it will still get replaced! Bah.
            content = content + (" " * (s - l))
        response.body = content
        response.headers[u'Content-Length'] = str(len(content))


def format_exc(exc=None):
    """Return exc (or sys.exc_info if None), formatted."""
    if exc is None:
        exc = _exc_info()
    if exc == (None, None, None):
        return ""
    import traceback
    return "".join(traceback.format_exception(*exc))

def bare_error(extrabody=None):
    """Produce status, headers, body for a critical error.
    
    Returns a triple without calling any other questionable functions,
    so it should be as error-free as possible. Call it from an HTTP server
    if you get errors outside of the request.
    
    If extrabody is None, a friendly but rather unhelpful error message
    is set in the body. If extrabody is a string, it will be appended
    as-is to the body.
    """
    
    # The whole point of this function is to be a last line-of-defense
    # in handling errors. That is, it must not raise any errors itself;
    # it cannot be allowed to fail. Therefore, don't add to it!
    # In particular, don't call any other CP functions.
    
    body = "Unrecoverable error in the server."
    if extrabody is not None:
        if not isinstance(extrabody, str): 
            extrabody = extrabody.encode('utf-8')
        body += "\n" + extrabody
    
    return ("500 Internal Server Error",
            [('Content-Type', 'text/plain'),
             ('Content-Length', str(len(body)))],
            [body])



########NEW FILE########
__FILENAME__ = _cplogging
"""CherryPy logging."""

import datetime
import logging
# Silence the no-handlers "warning" (stderr write!) in stdlib logging
logging.Logger.manager.emittedNoHandlerWarning = 1
logfmt = logging.Formatter("%(message)s")
import os
import sys

import cherrypy
from cherrypy import _cperror


class LogManager(object):
    
    appid = None
    error_log = None
    access_log = None
    access_log_format = \
        '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s"'
    
    def __init__(self, appid=None, logger_root="cherrypy"):
        self.logger_root = logger_root
        self.appid = appid
        if appid is None:
            self.error_log = logging.getLogger("%s.error" % logger_root)
            self.access_log = logging.getLogger("%s.access" % logger_root)
        else:
            self.error_log = logging.getLogger("%s.error.%s" % (logger_root, appid))
            self.access_log = logging.getLogger("%s.access.%s" % (logger_root, appid))
        self.error_log.setLevel(logging.INFO)
        self.access_log.setLevel(logging.INFO)
        cherrypy.engine.subscribe('graceful', self.reopen_files)
    
    def reopen_files(self):
        """Close and reopen all file handlers."""
        for log in (self.error_log, self.access_log):
            for h in log.handlers:
                if isinstance(h, logging.FileHandler):
                    h.acquire()
                    h.stream.close()
                    h.stream = open(h.baseFilename, h.mode)
                    h.release()
    
    def error(self, msg='', context='', severity=logging.INFO, traceback=False):
        """Write to the error log.
        
        This is not just for errors! Applications may call this at any time
        to log application-specific information.
        """
        if traceback:
            msg += _cperror.format_exc()
        self.error_log.log(severity, ' '.join((self.time(), context, msg)))
    
    def __call__(self, *args, **kwargs):
        """Write to the error log.
        
        This is not just for errors! Applications may call this at any time
        to log application-specific information.
        """
        return self.error(*args, **kwargs)
    
    def access(self):
        """Write to the access log (in Apache/NCSA Combined Log format).
        
        Like Apache started doing in 2.0.46, non-printable and other special
        characters in %r (and we expand that to all parts) are escaped using
        \\xhh sequences, where hh stands for the hexadecimal representation
        of the raw byte. Exceptions from this rule are " and \\, which are
        escaped by prepending a backslash, and all whitespace characters,
        which are written in their C-style notation (\\n, \\t, etc).
        """
        request = cherrypy.serving.request
        remote = request.remote
        response = cherrypy.serving.response
        outheaders = response.headers
        inheaders = request.headers
        if response.output_status is None:
            status = "-"
        else:
            status = response.output_status.split(" ", 1)[0]
        
        atoms = {'h': remote.name or remote.ip,
                 'l': '-',
                 'u': getattr(request, "login", None) or "-",
                 't': self.time(),
                 'r': request.request_line,
                 's': status,
                 'b': dict.get(outheaders, 'Content-Length', '') or "-",
                 'f': dict.get(inheaders, 'Referer', ''),
                 'a': dict.get(inheaders, 'User-Agent', ''),
                 }
        for k, v in atoms.items():
            if isinstance(v, unicode):
                v = v.encode('utf8')
            elif not isinstance(v, str):
                v = str(v)
            # Fortunately, repr(str) escapes unprintable chars, \n, \t, etc
            # and backslash for us. All we have to do is strip the quotes.
            v = repr(v)[1:-1]
            # Escape double-quote.
            atoms[k] = v.replace('"', '\\"')
        
        try:
            self.access_log.log(logging.INFO, self.access_log_format % atoms)
        except:
            self(traceback=True)
    
    def time(self):
        """Return now() in Apache Common Log Format (no timezone)."""
        now = datetime.datetime.now()
        monthnames = ['jan', 'feb', 'mar', 'apr', 'may', 'jun',
                      'jul', 'aug', 'sep', 'oct', 'nov', 'dec']
        month = monthnames[now.month - 1].capitalize()
        return ('[%02d/%s/%04d:%02d:%02d:%02d]' % 
                (now.day, month, now.year, now.hour, now.minute, now.second))
    
    def _get_builtin_handler(self, log, key):
        for h in log.handlers:
            if getattr(h, "_cpbuiltin", None) == key:
                return h
    
    
    # ------------------------- Screen handlers ------------------------- #
    
    def _set_screen_handler(self, log, enable, stream=None):
        h = self._get_builtin_handler(log, "screen")
        if enable:
            if not h:
                if stream is None:
                    stream = sys.stderr
                h = logging.StreamHandler(stream)
                h.setFormatter(logfmt)
                h._cpbuiltin = "screen"
                log.addHandler(h)
        elif h:
            log.handlers.remove(h)
    
    def _get_screen(self):
        h = self._get_builtin_handler
        has_h = h(self.error_log, "screen") or h(self.access_log, "screen")
        return bool(has_h)
    
    def _set_screen(self, newvalue):
        self._set_screen_handler(self.error_log, newvalue, stream=sys.stderr)
        self._set_screen_handler(self.access_log, newvalue, stream=sys.stdout)
    screen = property(_get_screen, _set_screen,
                      doc="If True, error and access will print to stderr.")
    
    
    # -------------------------- File handlers -------------------------- #
    
    def _add_builtin_file_handler(self, log, fname):
        h = logging.FileHandler(fname)
        h.setFormatter(logfmt)
        h._cpbuiltin = "file"
        log.addHandler(h)
    
    def _set_file_handler(self, log, filename):
        h = self._get_builtin_handler(log, "file")
        if filename:
            if h:
                if h.baseFilename != os.path.abspath(filename):
                    h.close()
                    log.handlers.remove(h)
                    self._add_builtin_file_handler(log, filename)
            else:
                self._add_builtin_file_handler(log, filename)
        else:
            if h:
                h.close()
                log.handlers.remove(h)
    
    def _get_error_file(self):
        h = self._get_builtin_handler(self.error_log, "file")
        if h:
            return h.baseFilename
        return ''
    def _set_error_file(self, newvalue):
        self._set_file_handler(self.error_log, newvalue)
    error_file = property(_get_error_file, _set_error_file,
                          doc="The filename for self.error_log.")
    
    def _get_access_file(self):
        h = self._get_builtin_handler(self.access_log, "file")
        if h:
            return h.baseFilename
        return ''
    def _set_access_file(self, newvalue):
        self._set_file_handler(self.access_log, newvalue)
    access_file = property(_get_access_file, _set_access_file,
                           doc="The filename for self.access_log.")
    
    
    # ------------------------- WSGI handlers ------------------------- #
    
    def _set_wsgi_handler(self, log, enable):
        h = self._get_builtin_handler(log, "wsgi")
        if enable:
            if not h:
                h = WSGIErrorHandler()
                h.setFormatter(logfmt)
                h._cpbuiltin = "wsgi"
                log.addHandler(h)
        elif h:
            log.handlers.remove(h)
    
    def _get_wsgi(self):
        return bool(self._get_builtin_handler(self.error_log, "wsgi"))
    
    def _set_wsgi(self, newvalue):
        self._set_wsgi_handler(self.error_log, newvalue)
    wsgi = property(_get_wsgi, _set_wsgi,
                      doc="If True, error messages will be sent to wsgi.errors.")


class WSGIErrorHandler(logging.Handler):
    "A handler class which writes logging records to environ['wsgi.errors']."
    
    def flush(self):
        """Flushes the stream."""
        try:
            stream = cherrypy.serving.request.wsgi_environ.get('wsgi.errors')
        except (AttributeError, KeyError):
            pass
        else:
            stream.flush()
    
    def emit(self, record):
        """Emit a record."""
        try:
            stream = cherrypy.serving.request.wsgi_environ.get('wsgi.errors')
        except (AttributeError, KeyError):
            pass
        else:
            try:
                msg = self.format(record)
                fs = "%s\n"
                import types
                if not hasattr(types, "UnicodeType"): #if no unicode support...
                    stream.write(fs % msg)
                else:
                    try:
                        stream.write(fs % msg)
                    except UnicodeError:
                        stream.write(fs % msg.encode("UTF-8"))
                self.flush()
            except:
                self.handleError(record)

########NEW FILE########
__FILENAME__ = _cpmodpy
"""Native adapter for serving CherryPy via mod_python

Basic usage:

##########################################
# Application in a module called myapp.py
##########################################

import cherrypy

class Root:
    @cherrypy.expose
    def index(self):
        return 'Hi there, Ho there, Hey there'


# We will use this method from the mod_python configuration
# as the entry point to our application
def setup_server():
    cherrypy.tree.mount(Root())
    cherrypy.config.update({'environment': 'production',
                            'log.screen': False,
                            'show_tracebacks': False})

##########################################
# mod_python settings for apache2
# This should reside in your httpd.conf
# or a file that will be loaded at
# apache startup
##########################################

# Start
DocumentRoot "/"
Listen 8080
LoadModule python_module /usr/lib/apache2/modules/mod_python.so

<Location "/">
	PythonPath "sys.path+['/path/to/my/application']" 
	SetHandler python-program
	PythonHandler cherrypy._cpmodpy::handler
	PythonOption cherrypy.setup myapp::setup_server
	PythonDebug On
</Location> 
# End

The actual path to your mod_python.so is dependent on your
environment. In this case we suppose a global mod_python
installation on a Linux distribution such as Ubuntu.

We do set the PythonPath configuration setting so that
your application can be found by from the user running
the apache2 instance. Of course if your application
resides in the global site-package this won't be needed.

Then restart apache2 and access http://127.0.0.1:8080
"""

import logging
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

import cherrypy
from cherrypy._cperror import format_exc, bare_error
from cherrypy.lib import httputil


# ------------------------------ Request-handling



def setup(req):
    from mod_python import apache
    
    # Run any setup functions defined by a "PythonOption cherrypy.setup" directive.
    options = req.get_options()
    if 'cherrypy.setup' in options:
        for function in options['cherrypy.setup'].split():
            atoms = function.split('::', 1)
            if len(atoms) == 1:
                mod = __import__(atoms[0], globals(), locals())
            else:
                modname, fname = atoms
                mod = __import__(modname, globals(), locals(), [fname])
                func = getattr(mod, fname)
                func()
    
    cherrypy.config.update({'log.screen': False,
                            "tools.ignore_headers.on": True,
                            "tools.ignore_headers.headers": ['Range'],
                            })
    
    engine = cherrypy.engine
    if hasattr(engine, "signal_handler"):
        engine.signal_handler.unsubscribe()
    if hasattr(engine, "console_control_handler"):
        engine.console_control_handler.unsubscribe()
    engine.autoreload.unsubscribe()
    cherrypy.server.unsubscribe()
    
    def _log(msg, level):
        newlevel = apache.APLOG_ERR
        if logging.DEBUG >= level:
            newlevel = apache.APLOG_DEBUG
        elif logging.INFO >= level:
            newlevel = apache.APLOG_INFO
        elif logging.WARNING >= level:
            newlevel = apache.APLOG_WARNING
        # On Windows, req.server is required or the msg will vanish. See
        # http://www.modpython.org/pipermail/mod_python/2003-October/014291.html.
        # Also, "When server is not specified...LogLevel does not apply..."
        apache.log_error(msg, newlevel, req.server)
    engine.subscribe('log', _log)
    
    engine.start()
    
    def cherrypy_cleanup(data):
        engine.exit()
    try:
        # apache.register_cleanup wasn't available until 3.1.4.
        apache.register_cleanup(cherrypy_cleanup)
    except AttributeError:
        req.server.register_cleanup(req, cherrypy_cleanup)


class _ReadOnlyRequest:
    expose = ('read', 'readline', 'readlines')
    def __init__(self, req):
        for method in self.expose:
            self.__dict__[method] = getattr(req, method)


recursive = False

_isSetUp = False
def handler(req):
    from mod_python import apache
    try:
        global _isSetUp
        if not _isSetUp:
            setup(req)
            _isSetUp = True
        
        # Obtain a Request object from CherryPy
        local = req.connection.local_addr
        local = httputil.Host(local[0], local[1], req.connection.local_host or "")
        remote = req.connection.remote_addr
        remote = httputil.Host(remote[0], remote[1], req.connection.remote_host or "")
        
        scheme = req.parsed_uri[0] or 'http'
        req.get_basic_auth_pw()
        
        try:
            # apache.mpm_query only became available in mod_python 3.1
            q = apache.mpm_query
            threaded = q(apache.AP_MPMQ_IS_THREADED)
            forked = q(apache.AP_MPMQ_IS_FORKED)
        except AttributeError:
            bad_value = ("You must provide a PythonOption '%s', "
                         "either 'on' or 'off', when running a version "
                         "of mod_python < 3.1")
            
            threaded = options.get('multithread', '').lower()
            if threaded == 'on':
                threaded = True
            elif threaded == 'off':
                threaded = False
            else:
                raise ValueError(bad_value % "multithread")
            
            forked = options.get('multiprocess', '').lower()
            if forked == 'on':
                forked = True
            elif forked == 'off':
                forked = False
            else:
                raise ValueError(bad_value % "multiprocess")
        
        sn = cherrypy.tree.script_name(req.uri or "/")
        if sn is None:
            send_response(req, '404 Not Found', [], '')
        else:
            app = cherrypy.tree.apps[sn]
            method = req.method
            path = req.uri
            qs = req.args or ""
            reqproto = req.protocol
            headers = req.headers_in.items()
            rfile = _ReadOnlyRequest(req)
            prev = None
            
            try:
                redirections = []
                while True:
                    request, response = app.get_serving(local, remote, scheme,
                                                        "HTTP/1.1")
                    request.login = req.user
                    request.multithread = bool(threaded)
                    request.multiprocess = bool(forked)
                    request.app = app
                    request.prev = prev
                    
                    # Run the CherryPy Request object and obtain the response
                    try:
                        request.run(method, path, qs, reqproto, headers, rfile)
                        break
                    except cherrypy.InternalRedirect, ir:
                        app.release_serving()
                        prev = request
                        
                        if not recursive:
                            if ir.path in redirections:
                                raise RuntimeError("InternalRedirector visited the "
                                                   "same URL twice: %r" % ir.path)
                            else:
                                # Add the *previous* path_info + qs to redirections.
                                if qs:
                                    qs = "?" + qs
                                redirections.append(sn + path + qs)
                        
                        # Munge environment and try again.
                        method = "GET"
                        path = ir.path
                        qs = ir.query_string
                        rfile = StringIO()
                
                send_response(req, response.status, response.header_list,
                              response.body, response.stream)
            finally:
                app.release_serving()
    except:
        tb = format_exc()
        cherrypy.log(tb, 'MOD_PYTHON', severity=logging.ERROR)
        s, h, b = bare_error()
        send_response(req, s, h, b)
    return apache.OK


def send_response(req, status, headers, body, stream=False):
    # Set response status
    req.status = int(status[:3])
    
    # Set response headers
    req.content_type = "text/plain"
    for header, value in headers:
        if header.lower() == 'content-type':
            req.content_type = value
            continue
        req.headers_out.add(header, value)
    
    if stream:
        # Flush now so the status and headers are sent immediately.
        req.flush()
    
    # Set response body
    if isinstance(body, basestring):
        req.write(body)
    else:
        for seg in body:
            req.write(seg)



# --------------- Startup tools for CherryPy + mod_python --------------- #


import os
import re


def read_process(cmd, args=""):
    pipein, pipeout = os.popen4("%s %s" % (cmd, args))
    try:
        firstline = pipeout.readline()
        if (re.search(r"(not recognized|No such file|not found)", firstline,
                      re.IGNORECASE)):
            raise IOError('%s must be on your system path.' % cmd)
        output = firstline + pipeout.read()
    finally:
        pipeout.close()
    return output


class ModPythonServer(object):
    
    template = """
# Apache2 server configuration file for running CherryPy with mod_python.

DocumentRoot "/"
Listen %(port)s
LoadModule python_module modules/mod_python.so

<Location %(loc)s>
    SetHandler python-program
    PythonHandler %(handler)s
    PythonDebug On
%(opts)s
</Location>
"""
    
    def __init__(self, loc="/", port=80, opts=None, apache_path="apache",
                 handler="cherrypy._cpmodpy::handler"):
        self.loc = loc
        self.port = port
        self.opts = opts
        self.apache_path = apache_path
        self.handler = handler
    
    def start(self):
        opts = "".join(["    PythonOption %s %s\n" % (k, v)
                        for k, v in self.opts])
        conf_data = self.template % {"port": self.port,
                                     "loc": self.loc,
                                     "opts": opts,
                                     "handler": self.handler,
                                     }
        
        mpconf = os.path.join(os.path.dirname(__file__), "cpmodpy.conf")
        f = open(mpconf, 'wb')
        try:
            f.write(conf_data)
        finally:
            f.close()
        
        response = read_process(self.apache_path, "-k start -f %s" % mpconf)
        self.ready = True
        return response
    
    def stop(self):
        os.popen("apache -k stop")
        self.ready = False


########NEW FILE########
__FILENAME__ = _cpnative_server
"""Native adapter for serving CherryPy via its builtin server."""

import logging
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

import cherrypy
from cherrypy._cperror import format_exc, bare_error
from cherrypy.lib import httputil
from cherrypy import wsgiserver


class NativeGateway(wsgiserver.Gateway):
    
    recursive = False
    
    def respond(self):
        req = self.req
        try:
            # Obtain a Request object from CherryPy
            local = req.server.bind_addr
            local = httputil.Host(local[0], local[1], "")
            remote = req.conn.remote_addr, req.conn.remote_port
            remote = httputil.Host(remote[0], remote[1], "")
            
            scheme = req.scheme
            sn = cherrypy.tree.script_name(req.uri or "/")
            if sn is None:
                self.send_response('404 Not Found', [], [''])
            else:
                app = cherrypy.tree.apps[sn]
                method = req.method
                path = req.path
                qs = req.qs or ""
                headers = req.inheaders.items()
                rfile = req.rfile
                prev = None
                
                try:
                    redirections = []
                    while True:
                        request, response = app.get_serving(
                            local, remote, scheme, "HTTP/1.1")
                        request.multithread = True
                        request.multiprocess = False
                        request.app = app
                        request.prev = prev
                        
                        # Run the CherryPy Request object and obtain the response
                        try:
                            request.run(method, path, qs, req.request_protocol, headers, rfile)
                            break
                        except cherrypy.InternalRedirect, ir:
                            app.release_serving()
                            prev = request
                            
                            if not self.recursive:
                                if ir.path in redirections:
                                    raise RuntimeError("InternalRedirector visited the "
                                                       "same URL twice: %r" % ir.path)
                                else:
                                    # Add the *previous* path_info + qs to redirections.
                                    if qs:
                                        qs = "?" + qs
                                    redirections.append(sn + path + qs)
                            
                            # Munge environment and try again.
                            method = "GET"
                            path = ir.path
                            qs = ir.query_string
                            rfile = StringIO()
                    
                    self.send_response(
                        response.output_status, response.header_list,
                        response.body)
                finally:
                    app.release_serving()
        except:
            tb = format_exc()
            #print tb
            cherrypy.log(tb, 'NATIVE_ADAPTER', severity=logging.ERROR)
            s, h, b = bare_error()
            self.send_response(s, h, b)
    
    def send_response(self, status, headers, body):
        req = self.req
        
        # Set response status
        req.status = str(status or "500 Server Error")
        
        # Set response headers
        for header, value in headers:
            req.outheaders.append((header, value))
        if (req.ready and not req.sent_headers):
            req.sent_headers = True
            req.send_headers()
        
        # Set response body
        for seg in body:
            req.write(seg)


class CPHTTPServer(wsgiserver.HTTPServer):
    """Wrapper for wsgiserver.HTTPServer.
    
    wsgiserver has been designed to not reference CherryPy in any way,
    so that it can be used in other frameworks and applications.
    Therefore, we wrap it here, so we can apply some attributes
    from config -> cherrypy.server -> HTTPServer.
    """
    
    def __init__(self, server_adapter=cherrypy.server):
        self.server_adapter = server_adapter
        
        server_name = (self.server_adapter.socket_host or
                       self.server_adapter.socket_file or
                       None)
        
        wsgiserver.HTTPServer.__init__(
            self, server_adapter.bind_addr, NativeGateway,
            minthreads=server_adapter.thread_pool,
            maxthreads=server_adapter.thread_pool_max,
            server_name=server_name)
        
        self.max_request_header_size = self.server_adapter.max_request_header_size or 0
        self.max_request_body_size = self.server_adapter.max_request_body_size or 0
        self.request_queue_size = self.server_adapter.socket_queue_size
        self.timeout = self.server_adapter.socket_timeout
        self.shutdown_timeout = self.server_adapter.shutdown_timeout
        self.protocol = self.server_adapter.protocol_version
        self.nodelay = self.server_adapter.nodelay
        
        ssl_module = self.server_adapter.ssl_module or 'pyopenssl'
        if self.server_adapter.ssl_context:
            adapter_class = wsgiserver.get_ssl_adapter_class(ssl_module)
            self.ssl_adapter = adapter_class(
                self.server_adapter.ssl_certificate,
                self.server_adapter.ssl_private_key,
                self.server_adapter.ssl_certificate_chain)
            self.ssl_adapter.context = self.server_adapter.ssl_context
        elif self.server_adapter.ssl_certificate:
            adapter_class = wsgiserver.get_ssl_adapter_class(ssl_module)
            self.ssl_adapter = adapter_class(
                self.server_adapter.ssl_certificate,
                self.server_adapter.ssl_private_key,
                self.server_adapter.ssl_certificate_chain)



########NEW FILE########
__FILENAME__ = _cpreqbody
"""Request body processing for CherryPy.

When an HTTP request includes an entity body, it is often desirable to
provide that information to applications in a form other than the raw bytes.
Different content types demand different approaches. Examples:

 * For a GIF file, we want the raw bytes in a stream.
 * An HTML form is better parsed into its component fields, and each text field
    decoded from bytes to unicode.
 * A JSON body should be deserialized into a Python dict or list.

When the request contains a Content-Type header, the media type is used as a
key to look up a value in the 'request.body.processors' dict. If the full media
type is not found, then the major type is tried; for example, if no processor
is found for the 'image/jpeg' type, then we look for a processor for the 'image'
types altogether. If neither the full type nor the major type has a matching
processor, then a default processor is used (self.default_proc). For most
types, this means no processing is done, and the body is left unread as a
raw byte stream. Processors are configurable in an 'on_start_resource' hook.

Some processors, especially those for the 'text' types, attempt to decode bytes
to unicode. If the Content-Type request header includes a 'charset' parameter,
this is used to decode the entity. Otherwise, one or more default charsets may
be attempted, although this decision is up to each processor. If a processor
successfully decodes an Entity or Part, it should set the 'charset' attribute
on the Entity or Part to the name of the successful charset, so that
applications can easily re-encode or transcode the value if they wish.

If the Content-Type of the request entity is of major type 'multipart', then
the above parsing process, and possibly a decoding process, is performed for
each part.

For both the full entity and multipart parts, a Content-Disposition header may
be used to fill .name and .filename attributes on the request.body or the Part.
"""

import re
import tempfile
from urllib import unquote_plus

import cherrypy
from cherrypy.lib import httputil


# -------------------------------- Processors -------------------------------- #

def process_urlencoded(entity):
    """Read application/x-www-form-urlencoded data into entity.params."""
    qs = entity.fp.read()
    for charset in entity.attempt_charsets:
        try:
            params = {}
            for aparam in qs.split('&'):
                for pair in aparam.split(';'):
                    if not pair:
                        continue
                    
                    atoms = pair.split('=', 1)
                    if len(atoms) == 1:
                        atoms.append('')
                    
                    key = unquote_plus(atoms[0]).decode(charset)
                    value = unquote_plus(atoms[1]).decode(charset)
                    
                    if key in params:
                        if not isinstance(params[key], list):
                            params[key] = [params[key]]
                        params[key].append(value)
                    else:
                        params[key] = value
        except UnicodeDecodeError:
            pass
        else:
            entity.charset = charset
            break
    else:
        raise cherrypy.HTTPError(
            400, "The request entity could not be decoded. The following "
            "charsets were attempted: %s" % repr(entity.attempt_charsets))
        
    # Now that all values have been successfully parsed and decoded,
    # apply them to the entity.params dict.
    for key, value in params.items():
        if key in entity.params:
            if not isinstance(entity.params[key], list):
                entity.params[key] = [entity.params[key]]
            entity.params[key].append(value)
        else:
            entity.params[key] = value


def process_multipart(entity):
    """Read all multipart parts into entity.parts."""
    ib = u""
    if u'boundary' in entity.content_type.params:
        # http://tools.ietf.org/html/rfc2046#section-5.1.1
        # "The grammar for parameters on the Content-type field is such that it
        # is often necessary to enclose the boundary parameter values in quotes
        # on the Content-type line"
        ib = entity.content_type.params['boundary'].strip(u'"')
    
    if not re.match(u"^[ -~]{0,200}[!-~]$", ib):
        raise ValueError(u'Invalid boundary in multipart form: %r' % (ib,))
    
    ib = (u'--' + ib).encode('ascii')
    
    # Find the first marker
    while True:
        b = entity.readline()
        if not b:
            return
        
        b = b.strip()
        if b == ib:
            break
    
    # Read all parts
    while True:
        part = entity.part_class.from_fp(entity.fp, ib)
        entity.parts.append(part)
        part.process()
        if part.fp.done:
            break

def process_multipart_form_data(entity):
    """Read all multipart/form-data parts into entity.parts or entity.params."""
    process_multipart(entity)
    
    kept_parts = []
    for part in entity.parts:
        if part.name is None:
            kept_parts.append(part)
        else:
            if part.filename is None:
                # It's a regular field
                entity.params[part.name] = part.fullvalue()
            else:
                # It's a file upload. Retain the whole part so consumer code
                # has access to its .file and .filename attributes.
                entity.params[part.name] = part
    
    entity.parts = kept_parts

def _old_process_multipart(entity):
    """The behavior of 3.2 and lower. Deprecated and will be changed in 3.3."""
    process_multipart(entity)
    
    params = entity.params
    
    for part in entity.parts:
        if part.name is None:
            key = u'parts'
        else:
            key = part.name
        
        if part.filename is None:
            # It's a regular field
            value = part.fullvalue()
        else:
            # It's a file upload. Retain the whole part so consumer code
            # has access to its .file and .filename attributes.
            value = part
        
        if key in params:
            if not isinstance(params[key], list):
                params[key] = [params[key]]
            params[key].append(value)
        else:
            params[key] = value



# --------------------------------- Entities --------------------------------- #


class Entity(object):
    """An HTTP request body, or MIME multipart body."""
    
    __metaclass__ = cherrypy._AttributeDocstrings
    
    params = None
    params__doc = u"""
    If the request Content-Type is 'application/x-www-form-urlencoded' or
    multipart, this will be a dict of the params pulled from the entity
    body; that is, it will be the portion of request.params that come
    from the message body (sometimes called "POST params", although they
    can be sent with various HTTP method verbs). This value is set between
    the 'before_request_body' and 'before_handler' hooks (assuming that
    process_request_body is True)."""
    
    default_content_type = u'application/x-www-form-urlencoded'
    # http://tools.ietf.org/html/rfc2046#section-4.1.2:
    # "The default character set, which must be assumed in the
    # absence of a charset parameter, is US-ASCII."
    # However, many browsers send data in utf-8 with no charset.
    attempt_charsets = [u'utf-8']
    processors = {u'application/x-www-form-urlencoded': process_urlencoded,
                  u'multipart/form-data': process_multipart_form_data,
                  u'multipart': process_multipart,
                  }
    
    def __init__(self, fp, headers, params=None, parts=None):
        # Make an instance-specific copy of the class processors
        # so Tools, etc. can replace them per-request.
        self.processors = self.processors.copy()
        
        self.fp = fp
        self.headers = headers
        
        if params is None:
            params = {}
        self.params = params
        
        if parts is None:
            parts = []
        self.parts = parts
        
        # Content-Type
        self.content_type = headers.elements(u'Content-Type')
        if self.content_type:
            self.content_type = self.content_type[0]
        else:
            self.content_type = httputil.HeaderElement.from_str(
                self.default_content_type)
        
        # Copy the class 'attempt_charsets', prepending any Content-Type charset
        dec = self.content_type.params.get(u"charset", None)
        if dec:
            dec = dec.decode('ISO-8859-1')
            self.attempt_charsets = [dec] + [c for c in self.attempt_charsets
                                             if c != dec]
        else:
            self.attempt_charsets = self.attempt_charsets[:]
        
        # Length
        self.length = None
        clen = headers.get(u'Content-Length', None)
        # If Transfer-Encoding is 'chunked', ignore any Content-Length.
        if clen is not None and 'chunked' not in headers.get(u'Transfer-Encoding', ''):
            try:
                self.length = int(clen)
            except ValueError:
                pass
        
        # Content-Disposition
        self.name = None
        self.filename = None
        disp = headers.elements(u'Content-Disposition')
        if disp:
            disp = disp[0]
            if 'name' in disp.params:
                self.name = disp.params['name']
                if self.name.startswith(u'"') and self.name.endswith(u'"'):
                    self.name = self.name[1:-1]
            if 'filename' in disp.params:
                self.filename = disp.params['filename']
                if self.filename.startswith(u'"') and self.filename.endswith(u'"'):
                    self.filename = self.filename[1:-1]
    
    # The 'type' attribute is deprecated in 3.2; remove it in 3.3.
    type = property(lambda self: self.content_type)
    
    def read(self, size=None, fp_out=None):
        return self.fp.read(size, fp_out)
    
    def readline(self, size=None):
        return self.fp.readline(size)
    
    def readlines(self, sizehint=None):
        return self.fp.readlines(sizehint)
    
    def __iter__(self):
        return self
    
    def next(self):
        line = self.readline()
        if not line:
            raise StopIteration
        return line
    
    def read_into_file(self, fp_out=None):
        """Read the request body into fp_out (or make_file() if None). Return fp_out."""
        if fp_out is None:
            fp_out = self.make_file()
        self.read(fp_out=fp_out)
        return fp_out
    
    def make_file(self):
        """Return a file into which the request body will be read.
        
        By default, this will return a TemporaryFile. Override as needed."""
        return tempfile.TemporaryFile()
    
    def fullvalue(self):
        """Return this entity as a string, whether stored in a file or not."""
        if self.file:
            # It was stored in a tempfile. Read it.
            self.file.seek(0)
            value = self.file.read()
            self.file.seek(0)
        else:
            value = self.value
        return value
    
    def process(self):
        """Execute the best-match processor for the given media type."""
        proc = None
        ct = self.content_type.value
        try:
            proc = self.processors[ct]
        except KeyError:
            toptype = ct.split(u'/', 1)[0]
            try:
                proc = self.processors[toptype]
            except KeyError:
                pass
        if proc is None:
            self.default_proc()
        else:
            proc(self)
    
    def default_proc(self):
        # Leave the fp alone for someone else to read. This works fine
        # for request.body, but the Part subclasses need to override this
        # so they can move on to the next part.
        pass


class Part(Entity):
    """A MIME part entity, part of a multipart entity."""
    
    default_content_type = u'text/plain'
    # "The default character set, which must be assumed in the absence of a
    # charset parameter, is US-ASCII."
    attempt_charsets = [u'us-ascii', u'utf-8']
    # This is the default in stdlib cgi. We may want to increase it.
    maxrambytes = 1000
    
    def __init__(self, fp, headers, boundary):
        Entity.__init__(self, fp, headers)
        self.boundary = boundary
        self.file = None
        self.value = None
    
    def from_fp(cls, fp, boundary):
        headers = cls.read_headers(fp)
        return cls(fp, headers, boundary)
    from_fp = classmethod(from_fp)
    
    def read_headers(cls, fp):
        headers = httputil.HeaderMap()
        while True:
            line = fp.readline()
            if not line:
                # No more data--illegal end of headers
                raise EOFError(u"Illegal end of headers.")
            
            if line == '\r\n':
                # Normal end of headers
                break
            if not line.endswith('\r\n'):
                raise ValueError(u"MIME requires CRLF terminators: %r" % line)
            
            if line[0] in ' \t':
                # It's a continuation line.
                v = line.strip().decode(u'ISO-8859-1')
            else:
                k, v = line.split(":", 1)
                k = k.strip().decode(u'ISO-8859-1')
                v = v.strip().decode(u'ISO-8859-1')
            
            existing = headers.get(k)
            if existing:
                v = u", ".join((existing, v))
            headers[k] = v
        
        return headers
    read_headers = classmethod(read_headers)
    
    def read_lines_to_boundary(self, fp_out=None):
        """Read bytes from self.fp and return or write them to a file.
        
        If the 'fp_out' argument is None (the default), all bytes read are
        returned in a single byte string.
        
        If the 'fp_out' argument is not None, it must be a file-like object that
        supports the 'write' method; all bytes read will be written to the fp,
        and that fp is returned.
        """
        endmarker = self.boundary + "--"
        delim = ""
        prev_lf = True
        lines = []
        seen = 0
        while True:
            line = self.fp.readline(1 << 16)
            if not line:
                raise EOFError(u"Illegal end of multipart body.")
            if line.startswith("--") and prev_lf:
                strippedline = line.strip()
                if strippedline == self.boundary:
                    break
                if strippedline == endmarker:
                    self.fp.finish()
                    break
            
            line = delim + line
            
            if line.endswith("\r\n"):
                delim = "\r\n"
                line = line[:-2]
                prev_lf = True
            elif line.endswith("\n"):
                delim = "\n"
                line = line[:-1]
                prev_lf = True
            else:
                delim = ""
                prev_lf = False
            
            if fp_out is None:
                lines.append(line)
                seen += len(line)
                if seen > self.maxrambytes:
                    fp_out = self.make_file()
                    for line in lines:
                        fp_out.write(line)
            else:
                fp_out.write(line)
        
        if fp_out is None:
            result = ''.join(lines)
            for charset in self.attempt_charsets:
                try:
                    result = result.decode(charset)
                except UnicodeDecodeError:
                    pass
                else:
                    self.charset = charset
                    return result
            else:
                raise cherrypy.HTTPError(
                    400, "The request entity could not be decoded. The following "
                    "charsets were attempted: %s" % repr(self.attempt_charsets))
        else:
            fp_out.seek(0)
            return fp_out
    
    def default_proc(self):
        if self.filename:
            # Always read into a file if a .filename was given.
            self.file = self.read_into_file()
        else:
            result = self.read_lines_to_boundary()
            if isinstance(result, basestring):
                self.value = result
            else:
                self.file = result
    
    def read_into_file(self, fp_out=None):
        """Read the request body into fp_out (or make_file() if None). Return fp_out."""
        if fp_out is None:
            fp_out = self.make_file()
        self.read_lines_to_boundary(fp_out=fp_out)
        return fp_out

Entity.part_class = Part


class Infinity(object):
    def __cmp__(self, other):
        return 1
    def __sub__(self, other):
        return self
inf = Infinity()


comma_separated_headers = ['Accept', 'Accept-Charset', 'Accept-Encoding',
    'Accept-Language', 'Accept-Ranges', 'Allow', 'Cache-Control', 'Connection',
    'Content-Encoding', 'Content-Language', 'Expect', 'If-Match',
    'If-None-Match', 'Pragma', 'Proxy-Authenticate', 'Te', 'Trailer',
    'Transfer-Encoding', 'Upgrade', 'Vary', 'Via', 'Warning', 'Www-Authenticate']


class SizedReader:
    
    def __init__(self, fp, length, maxbytes, bufsize=8192, has_trailers=False):
        # Wrap our fp in a buffer so peek() works
        self.fp = fp
        self.length = length
        self.maxbytes = maxbytes
        self.buffer = ''
        self.bufsize = bufsize
        self.bytes_read = 0
        self.done = False
        self.has_trailers = has_trailers
    
    def read(self, size=None, fp_out=None):
        """Read bytes from the request body and return or write them to a file.
        
        A number of bytes less than or equal to the 'size' argument are read
        off the socket. The actual number of bytes read are tracked in
        self.bytes_read. The number may be smaller than 'size' when 1) the
        client sends fewer bytes, 2) the 'Content-Length' request header
        specifies fewer bytes than requested, or 3) the number of bytes read
        exceeds self.maxbytes (in which case, 413 is raised).
        
        If the 'fp_out' argument is None (the default), all bytes read are
        returned in a single byte string.
        
        If the 'fp_out' argument is not None, it must be a file-like object that
        supports the 'write' method; all bytes read will be written to the fp,
        and None is returned.
        """
        
        if self.length is None:
            if size is None:
                remaining = inf
            else:
                remaining = size
        else:
            remaining = self.length - self.bytes_read
            if size and size < remaining:
                remaining = size
        if remaining == 0:
            self.finish()
            if fp_out is None:
                return ''
            else:
                return None
        
        chunks = []
        
        # Read bytes from the buffer.
        if self.buffer:
            if remaining is inf:
                data = self.buffer
                self.buffer = ''
            else:
                data = self.buffer[:remaining]
                self.buffer = self.buffer[remaining:]
            datalen = len(data)
            remaining -= datalen
            
            # Check lengths.
            self.bytes_read += datalen
            if self.maxbytes and self.bytes_read > self.maxbytes:
                raise cherrypy.HTTPError(413)
            
            # Store the data.
            if fp_out is None:
                chunks.append(data)
            else:
                fp_out.write(data)
        
        # Read bytes from the socket.
        while remaining > 0:
            chunksize = min(remaining, self.bufsize)
            try:
                data = self.fp.read(chunksize)
            except Exception, e:
                if e.__class__.__name__ == 'MaxSizeExceeded':
                    # Post data is too big
                    raise cherrypy.HTTPError(
                        413, "Maximum request length: %r" % e.args[1])
                else:
                    raise
            if not data:
                self.finish()
                break
            datalen = len(data)
            remaining -= datalen
            
            # Check lengths.
            self.bytes_read += datalen
            if self.maxbytes and self.bytes_read > self.maxbytes:
                raise cherrypy.HTTPError(413)
            
            # Store the data.
            if fp_out is None:
                chunks.append(data)
            else:
                fp_out.write(data)
        
        if fp_out is None:
            return ''.join(chunks)
    
    def readline(self, size=None):
        """Read a line from the request body and return it."""
        chunks = []
        while size is None or size > 0:
            chunksize = self.bufsize
            if size is not None and size < self.bufsize:
                chunksize = size
            data = self.read(chunksize)
            if not data:
                break
            pos = data.find('\n') + 1
            if pos:
                chunks.append(data[:pos])
                remainder = data[pos:]
                self.buffer += remainder
                self.bytes_read -= len(remainder)
                break
            else:
                chunks.append(data)
        return ''.join(chunks)
    
    def readlines(self, sizehint=None):
        """Read lines from the request body and return them."""
        if self.length is not None:
            if sizehint is None:
                sizehint = self.length - self.bytes_read
            else:
                sizehint = min(sizehint, self.length - self.bytes_read)
        
        lines = []
        seen = 0
        while True:
            line = self.readline()
            if not line:
                break
            lines.append(line)
            seen += len(line)
            if seen >= sizehint:
                break
        return lines
    
    def finish(self):
        self.done = True
        if self.has_trailers and hasattr(self.fp, 'read_trailer_lines'):
            self.trailers = {}
            
            try:
                for line in self.fp.read_trailer_lines():
                    if line[0] in ' \t':
                        # It's a continuation line.
                        v = line.strip()
                    else:
                        try:
                            k, v = line.split(":", 1)
                        except ValueError:
                            raise ValueError("Illegal header line.")
                        k = k.strip().title()
                        v = v.strip()
                    
                    if k in comma_separated_headers:
                        existing = self.trailers.get(envname)
                        if existing:
                            v = ", ".join((existing, v))
                    self.trailers[k] = v
            except Exception, e:
                if e.__class__.__name__ == 'MaxSizeExceeded':
                    # Post data is too big
                    raise cherrypy.HTTPError(
                        413, "Maximum request length: %r" % e.args[1])
                else:
                    raise


class RequestBody(Entity):
    
    # Don't parse the request body at all if the client didn't provide
    # a Content-Type header. See http://www.cherrypy.org/ticket/790
    default_content_type = u''
    
    bufsize = 8 * 1024
    maxbytes = None
    
    def __init__(self, fp, headers, params=None, request_params=None):
        Entity.__init__(self, fp, headers, params)
        
        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.7.1
        # When no explicit charset parameter is provided by the
        # sender, media subtypes of the "text" type are defined
        # to have a default charset value of "ISO-8859-1" when
        # received via HTTP.
        if self.content_type.value.startswith('text/'):
            for c in (u'ISO-8859-1', u'iso-8859-1', u'Latin-1', u'latin-1'):
                if c in self.attempt_charsets:
                    break
            else:
                self.attempt_charsets.append(u'ISO-8859-1')
        
        # Temporary fix while deprecating passing .parts as .params.
        self.processors[u'multipart'] = _old_process_multipart
        
        if request_params is None:
            request_params = {}
        self.request_params = request_params
    
    def process(self):
        """Include body params in request params."""
        # "The presence of a message-body in a request is signaled by the
        # inclusion of a Content-Length or Transfer-Encoding header field in
        # the request's message-headers."
        # It is possible to send a POST request with no body, for example;
        # however, app developers are responsible in that case to set
        # cherrypy.request.process_body to False so this method isn't called.
        h = cherrypy.serving.request.headers
        if u'Content-Length' not in h and u'Transfer-Encoding' not in h:
            raise cherrypy.HTTPError(411)
        
        self.fp = SizedReader(self.fp, self.length,
                              self.maxbytes, bufsize=self.bufsize,
                              has_trailers='Trailer' in h)
        super(RequestBody, self).process()
        
        # Body params should also be a part of the request_params
        # add them in here.
        request_params = self.request_params
        for key, value in self.params.items():
            # Python 2 only: keyword arguments must be byte strings (type 'str').
            if isinstance(key, unicode):
                key = key.encode('ISO-8859-1')
            
            if key in request_params:
                if not isinstance(request_params[key], list):
                    request_params[key] = [request_params[key]]
                request_params[key].append(value)
            else:
                request_params[key] = value


########NEW FILE########
__FILENAME__ = _cprequest

from Cookie import SimpleCookie, CookieError
import os
import sys
import time
import types
import warnings

import cherrypy
from cherrypy import _cpreqbody, _cpconfig
from cherrypy._cperror import format_exc, bare_error
from cherrypy.lib import httputil, file_generator


class Hook(object):
    """A callback and its metadata: failsafe, priority, and kwargs."""
    
    __metaclass__ = cherrypy._AttributeDocstrings
    
    callback = None
    callback__doc = """
    The bare callable that this Hook object is wrapping, which will
    be called when the Hook is called."""
    
    failsafe = False
    failsafe__doc = """
    If True, the callback is guaranteed to run even if other callbacks
    from the same call point raise exceptions."""
    
    priority = 50
    priority__doc = """
    Defines the order of execution for a list of Hooks. Priority numbers
    should be limited to the closed interval [0, 100], but values outside
    this range are acceptable, as are fractional values."""
    
    kwargs = {}
    kwargs__doc = """
    A set of keyword arguments that will be passed to the
    callable on each call."""
    
    def __init__(self, callback, failsafe=None, priority=None, **kwargs):
        self.callback = callback
        
        if failsafe is None:
            failsafe = getattr(callback, "failsafe", False)
        self.failsafe = failsafe
        
        if priority is None:
            priority = getattr(callback, "priority", 50)
        self.priority = priority
        
        self.kwargs = kwargs
    
    def __cmp__(self, other):
        return cmp(self.priority, other.priority)
    
    def __call__(self):
        """Run self.callback(**self.kwargs)."""
        return self.callback(**self.kwargs)
    
    def __repr__(self):
        cls = self.__class__
        return ("%s.%s(callback=%r, failsafe=%r, priority=%r, %s)"
                % (cls.__module__, cls.__name__, self.callback,
                   self.failsafe, self.priority,
                   ", ".join(['%s=%r' % (k, v)
                              for k, v in self.kwargs.items()])))


class HookMap(dict):
    """A map of call points to lists of callbacks (Hook objects)."""
    
    def __new__(cls, points=None):
        d = dict.__new__(cls)
        for p in points or []:
            d[p] = []
        return d
    
    def __init__(self, *a, **kw):
        pass
    
    def attach(self, point, callback, failsafe=None, priority=None, **kwargs):
        """Append a new Hook made from the supplied arguments."""
        self[point].append(Hook(callback, failsafe, priority, **kwargs))
    
    def run(self, point):
        """Execute all registered Hooks (callbacks) for the given point."""
        exc = None
        hooks = self[point]
        hooks.sort()
        for hook in hooks:
            # Some hooks are guaranteed to run even if others at
            # the same hookpoint fail. We will still log the failure,
            # but proceed on to the next hook. The only way
            # to stop all processing from one of these hooks is
            # to raise SystemExit and stop the whole server.
            if exc is None or hook.failsafe:
                try:
                    hook()
                except (KeyboardInterrupt, SystemExit):
                    raise
                except (cherrypy.HTTPError, cherrypy.HTTPRedirect,
                        cherrypy.InternalRedirect):
                    exc = sys.exc_info()[1]
                except:
                    exc = sys.exc_info()[1]
                    cherrypy.log(traceback=True, severity=40)
        if exc:
            raise
    
    def __copy__(self):
        newmap = self.__class__()
        # We can't just use 'update' because we want copies of the
        # mutable values (each is a list) as well.
        for k, v in self.items():
            newmap[k] = v[:]
        return newmap
    copy = __copy__
    
    def __repr__(self):
        cls = self.__class__
        return "%s.%s(points=%r)" % (cls.__module__, cls.__name__, self.keys())


# Config namespace handlers

def hooks_namespace(k, v):
    """Attach bare hooks declared in config."""
    # Use split again to allow multiple hooks for a single
    # hookpoint per path (e.g. "hooks.before_handler.1").
    # Little-known fact you only get from reading source ;)
    hookpoint = k.split(".", 1)[0]
    if isinstance(v, basestring):
        v = cherrypy.lib.attributes(v)
    if not isinstance(v, Hook):
        v = Hook(v)
    cherrypy.serving.request.hooks[hookpoint].append(v)

def request_namespace(k, v):
    """Attach request attributes declared in config."""
    # Provides config entries to set request.body attrs (like attempt_charsets).
    if k[:5] == 'body.':
        setattr(cherrypy.serving.request.body, k[5:], v)
    else:
        setattr(cherrypy.serving.request, k, v)

def response_namespace(k, v):
    """Attach response attributes declared in config."""
    # Provides config entries to set default response headers
    # http://cherrypy.org/ticket/889
    if k[:8] == 'headers.':
        cherrypy.serving.response.headers[k.split('.', 1)[1]] = v
    else:
        setattr(cherrypy.serving.response, k, v)

def error_page_namespace(k, v):
    """Attach error pages declared in config."""
    if k != 'default':
        k = int(k)
    cherrypy.serving.request.error_page[k] = v


hookpoints = ['on_start_resource', 'before_request_body',
              'before_handler', 'before_finalize',
              'on_end_resource', 'on_end_request',
              'before_error_response', 'after_error_response']


class Request(object):
    """An HTTP request.
    
    This object represents the metadata of an HTTP request message;
    that is, it contains attributes which describe the environment
    in which the request URL, headers, and body were sent (if you
    want tools to interpret the headers and body, those are elsewhere,
    mostly in Tools). This 'metadata' consists of socket data,
    transport characteristics, and the Request-Line. This object
    also contains data regarding the configuration in effect for
    the given URL, and the execution plan for generating a response.
    """
    
    __metaclass__ = cherrypy._AttributeDocstrings
    
    prev = None
    prev__doc = """
    The previous Request object (if any). This should be None
    unless we are processing an InternalRedirect."""
    
    # Conversation/connection attributes
    local = httputil.Host("127.0.0.1", 80)
    local__doc = \
        "An httputil.Host(ip, port, hostname) object for the server socket."
    
    remote = httputil.Host("127.0.0.1", 1111)
    remote__doc = \
        "An httputil.Host(ip, port, hostname) object for the client socket."
    
    scheme = "http"
    scheme__doc = """
    The protocol used between client and server. In most cases,
    this will be either 'http' or 'https'."""
    
    server_protocol = "HTTP/1.1"
    server_protocol__doc = """
    The HTTP version for which the HTTP server is at least
    conditionally compliant."""
    
    base = ""
    base__doc = """The (scheme://host) portion of the requested URL.
    In some cases (e.g. when proxying via mod_rewrite), this may contain
    path segments which cherrypy.url uses when constructing url's, but
    which otherwise are ignored by CherryPy. Regardless, this value
    MUST NOT end in a slash."""
    
    # Request-Line attributes
    request_line = ""
    request_line__doc = """
    The complete Request-Line received from the client. This is a
    single string consisting of the request method, URI, and protocol
    version (joined by spaces). Any final CRLF is removed."""
    
    method = "GET"
    method__doc = """
    Indicates the HTTP method to be performed on the resource identified
    by the Request-URI. Common methods include GET, HEAD, POST, PUT, and
    DELETE. CherryPy allows any extension method; however, various HTTP
    servers and gateways may restrict the set of allowable methods.
    CherryPy applications SHOULD restrict the set (on a per-URI basis)."""
    
    query_string = ""
    query_string__doc = """
    The query component of the Request-URI, a string of information to be
    interpreted by the resource. The query portion of a URI follows the
    path component, and is separated by a '?'. For example, the URI
    'http://www.cherrypy.org/wiki?a=3&b=4' has the query component,
    'a=3&b=4'."""
    
    query_string_encoding = 'utf8'
    query_string_encoding__doc = """
    The encoding expected for query string arguments after % HEX HEX decoding).
    If a query string is provided that cannot be decoded with this encoding,
    404 is raised (since technically it's a different URI). If you want
    arbitrary encodings to not error, set this to 'Latin-1'; you can then
    encode back to bytes and re-decode to whatever encoding you like later.
    """
    
    protocol = (1, 1)
    protocol__doc = """The HTTP protocol version corresponding to the set
        of features which should be allowed in the response. If BOTH
        the client's request message AND the server's level of HTTP
        compliance is HTTP/1.1, this attribute will be the tuple (1, 1).
        If either is 1.0, this attribute will be the tuple (1, 0).
        Lower HTTP protocol versions are not explicitly supported."""
    
    params = {}
    params__doc = """
    A dict which combines query string (GET) and request entity (POST)
    variables. This is populated in two stages: GET params are added
    before the 'on_start_resource' hook, and POST params are added
    between the 'before_request_body' and 'before_handler' hooks."""
    
    # Message attributes
    header_list = []
    header_list__doc = """
    A list of the HTTP request headers as (name, value) tuples.
    In general, you should use request.headers (a dict) instead."""
    
    headers = httputil.HeaderMap()
    headers__doc = """
    A dict-like object containing the request headers. Keys are header
    names (in Title-Case format); however, you may get and set them in
    a case-insensitive manner. That is, headers['Content-Type'] and
    headers['content-type'] refer to the same value. Values are header
    values (decoded according to RFC 2047 if necessary). See also:
    httputil.HeaderMap, httputil.HeaderElement."""
    
    cookie = SimpleCookie()
    cookie__doc = """See help(Cookie)."""
    
    body = None
    body__doc = """See help(cherrypy.request.body)"""
    
    rfile = None
    rfile__doc = """
    If the request included an entity (body), it will be available
    as a stream in this attribute. However, the rfile will normally
    be read for you between the 'before_request_body' hook and the
    'before_handler' hook, and the resulting string is placed into
    either request.params or the request.body attribute.
    
    You may disable the automatic consumption of the rfile by setting
    request.process_request_body to False, either in config for the desired
    path, or in an 'on_start_resource' or 'before_request_body' hook.
    
    WARNING: In almost every case, you should not attempt to read from the
    rfile stream after CherryPy's automatic mechanism has read it. If you
    turn off the automatic parsing of rfile, you should read exactly the
    number of bytes specified in request.headers['Content-Length'].
    Ignoring either of these warnings may result in a hung request thread
    or in corruption of the next (pipelined) request.
    """
    
    process_request_body = True
    process_request_body__doc = """
    If True, the rfile (if any) is automatically read and parsed,
    and the result placed into request.params or request.body."""
    
    methods_with_bodies = ("POST", "PUT")
    methods_with_bodies__doc = """
    A sequence of HTTP methods for which CherryPy will automatically
    attempt to read a body from the rfile."""
    
    body = None
    body__doc = """
    If the request Content-Type is 'application/x-www-form-urlencoded'
    or multipart, this will be None. Otherwise, this will contain the
    request entity body as an open file object (which you can .read());
    this value is set between the 'before_request_body' and 'before_handler'
    hooks (assuming that process_request_body is True)."""
    
    body_params = None
    body_params__doc = """
    If the request Content-Type is 'application/x-www-form-urlencoded' or
    multipart, this will be a dict of the params pulled from the entity
    body; that is, it will be the portion of request.params that come
    from the message body (sometimes called "POST params", although they
    can be sent with various HTTP method verbs). This value is set between
    the 'before_request_body' and 'before_handler' hooks (assuming that
    process_request_body is True)."""
    
    # Dispatch attributes
    dispatch = cherrypy.dispatch.Dispatcher()
    dispatch__doc = """
    The object which looks up the 'page handler' callable and collects
    config for the current request based on the path_info, other
    request attributes, and the application architecture. The core
    calls the dispatcher as early as possible, passing it a 'path_info'
    argument.
    
    The default dispatcher discovers the page handler by matching path_info
    to a hierarchical arrangement of objects, starting at request.app.root.
    See help(cherrypy.dispatch) for more information."""
    
    script_name = ""
    script_name__doc = """
    The 'mount point' of the application which is handling this request.
    
    This attribute MUST NOT end in a slash. If the script_name refers to
    the root of the URI, it MUST be an empty string (not "/").
    """
    
    path_info = "/"
    path_info__doc = """
    The 'relative path' portion of the Request-URI. This is relative
    to the script_name ('mount point') of the application which is
    handling this request."""

    login = None
    login__doc = """
    When authentication is used during the request processing this is
    set to 'False' if it failed and to the 'username' value if it succeeded.
    The default 'None' implies that no authentication happened."""
    
    # Note that cherrypy.url uses "if request.app:" to determine whether
    # the call is during a real HTTP request or not. So leave this None.
    app = None
    app__doc = \
        """The cherrypy.Application object which is handling this request."""
    
    handler = None
    handler__doc = """
    The function, method, or other callable which CherryPy will call to
    produce the response. The discovery of the handler and the arguments
    it will receive are determined by the request.dispatch object.
    By default, the handler is discovered by walking a tree of objects
    starting at request.app.root, and is then passed all HTTP params
    (from the query string and POST body) as keyword arguments."""
    
    toolmaps = {}
    toolmaps__doc = """
    A nested dict of all Toolboxes and Tools in effect for this request,
    of the form: {Toolbox.namespace: {Tool.name: config dict}}."""
    
    config = None
    config__doc = """
    A flat dict of all configuration entries which apply to the
    current request. These entries are collected from global config,
    application config (based on request.path_info), and from handler
    config (exactly how is governed by the request.dispatch object in
    effect for this request; by default, handler config can be attached
    anywhere in the tree between request.app.root and the final handler,
    and inherits downward)."""
    
    is_index = None
    is_index__doc = """
    This will be True if the current request is mapped to an 'index'
    resource handler (also, a 'default' handler if path_info ends with
    a slash). The value may be used to automatically redirect the
    user-agent to a 'more canonical' URL which either adds or removes
    the trailing slash. See cherrypy.tools.trailing_slash."""
    
    hooks = HookMap(hookpoints)
    hooks__doc = """
    A HookMap (dict-like object) of the form: {hookpoint: [hook, ...]}.
    Each key is a str naming the hook point, and each value is a list
    of hooks which will be called at that hook point during this request.
    The list of hooks is generally populated as early as possible (mostly
    from Tools specified in config), but may be extended at any time.
    See also: _cprequest.Hook, _cprequest.HookMap, and cherrypy.tools."""
    
    error_response = cherrypy.HTTPError(500).set_response
    error_response__doc = """
    The no-arg callable which will handle unexpected, untrapped errors
    during request processing. This is not used for expected exceptions
    (like NotFound, HTTPError, or HTTPRedirect) which are raised in
    response to expected conditions (those should be customized either
    via request.error_page or by overriding HTTPError.set_response).
    By default, error_response uses HTTPError(500) to return a generic
    error response to the user-agent."""
    
    error_page = {}
    error_page__doc = """
    A dict of {error code: response filename or callable} pairs.
    
    The error code must be an int representing a given HTTP error code,
    or the string 'default', which will be used if no matching entry
    is found for a given numeric code.
    
    If a filename is provided, the file should contain a Python string-
    formatting template, and can expect by default to receive format 
    values with the mapping keys %(status)s, %(message)s, %(traceback)s,
    and %(version)s. The set of format mappings can be extended by
    overriding HTTPError.set_response.
    
    If a callable is provided, it will be called by default with keyword
    arguments 'status', 'message', 'traceback', and 'version', as for a
    string-formatting template. The callable must return a string or iterable of
    strings which will be set to response.body. It may also override headers or
    perform any other processing.
    
    If no entry is given for an error code, and no 'default' entry exists,
    a default template will be used.
    """
    
    show_tracebacks = True
    show_tracebacks__doc = """
    If True, unexpected errors encountered during request processing will
    include a traceback in the response body."""

    show_mismatched_params = True
    show_mismatched_params__doc = """
    If True, mismatched parameters encountered during PageHandler invocation
    processing will be included in the response body."""
    
    throws = (KeyboardInterrupt, SystemExit, cherrypy.InternalRedirect)
    throws__doc = \
        """The sequence of exceptions which Request.run does not trap."""
    
    throw_errors = False
    throw_errors__doc = """
    If True, Request.run will not trap any errors (except HTTPRedirect and
    HTTPError, which are more properly called 'exceptions', not errors)."""
    
    closed = False
    closed__doc = """
    True once the close method has been called, False otherwise."""
    
    stage = None
    stage__doc = """
    A string containing the stage reached in the request-handling process.
    This is useful when debugging a live server with hung requests."""
    
    namespaces = _cpconfig.NamespaceSet(
        **{"hooks": hooks_namespace,
           "request": request_namespace,
           "response": response_namespace,
           "error_page": error_page_namespace,
           "tools": cherrypy.tools,
           })
    
    def __init__(self, local_host, remote_host, scheme="http",
                 server_protocol="HTTP/1.1"):
        """Populate a new Request object.
        
        local_host should be an httputil.Host object with the server info.
        remote_host should be an httputil.Host object with the client info.
        scheme should be a string, either "http" or "https".
        """
        self.local = local_host
        self.remote = remote_host
        self.scheme = scheme
        self.server_protocol = server_protocol
        
        self.closed = False
        
        # Put a *copy* of the class error_page into self.
        self.error_page = self.error_page.copy()
        
        # Put a *copy* of the class namespaces into self.
        self.namespaces = self.namespaces.copy()
        
        self.stage = None
    
    def close(self):
        """Run cleanup code. (Core)"""
        if not self.closed:
            self.closed = True
            self.stage = 'on_end_request'
            self.hooks.run('on_end_request')
            self.stage = 'close'
    
    def run(self, method, path, query_string, req_protocol, headers, rfile):
        """Process the Request. (Core)
        
        method, path, query_string, and req_protocol should be pulled directly
            from the Request-Line (e.g. "GET /path?key=val HTTP/1.0").
        path should be %XX-unquoted, but query_string should not be.
            They both MUST be byte strings, not unicode strings.
        headers should be a list of (name, value) tuples.
        rfile should be a file-like object containing the HTTP request entity.
        
        When run() is done, the returned object should have 3 attributes:
          status, e.g. "200 OK"
          header_list, a list of (name, value) tuples
          body, an iterable yielding strings
        
        Consumer code (HTTP servers) should then access these response
        attributes to build the outbound stream.
        
        """
        response = cherrypy.serving.response
        self.stage = 'run'
        try:
            self.error_response = cherrypy.HTTPError(500).set_response
            
            self.method = method
            path = path or "/"
            self.query_string = query_string or ''
            self.params = {}
            
            # Compare request and server HTTP protocol versions, in case our
            # server does not support the requested protocol. Limit our output
            # to min(req, server). We want the following output:
            #     request    server     actual written   supported response
            #     protocol   protocol  response protocol    feature set
            # a     1.0        1.0           1.0                1.0
            # b     1.0        1.1           1.1                1.0
            # c     1.1        1.0           1.0                1.0
            # d     1.1        1.1           1.1                1.1
            # Notice that, in (b), the response will be "HTTP/1.1" even though
            # the client only understands 1.0. RFC 2616 10.5.6 says we should
            # only return 505 if the _major_ version is different.
            rp = int(req_protocol[5]), int(req_protocol[7])
            sp = int(self.server_protocol[5]), int(self.server_protocol[7])
            self.protocol = min(rp, sp)
            response.headers.protocol = self.protocol
            
            # Rebuild first line of the request (e.g. "GET /path HTTP/1.0").
            url = path
            if query_string:
                url += '?' + query_string
            self.request_line = '%s %s %s' % (method, url, req_protocol)
            
            self.header_list = list(headers)
            self.headers = httputil.HeaderMap()
            
            self.rfile = rfile
            self.body = None
            
            self.cookie = SimpleCookie()
            self.handler = None
            
            # path_info should be the path from the
            # app root (script_name) to the handler.
            self.script_name = self.app.script_name
            self.path_info = pi = path[len(self.script_name):]
            
            self.stage = 'respond'
            self.respond(pi)
            
        except self.throws:
            raise
        except:
            if self.throw_errors:
                raise
            else:
                # Failure in setup, error handler or finalize. Bypass them.
                # Can't use handle_error because we may not have hooks yet.
                cherrypy.log(traceback=True, severity=40)
                if self.show_tracebacks:
                    body = format_exc()
                else:
                    body = ""
                r = bare_error(body)
                response.output_status, response.header_list, response.body = r
        
        if self.method == "HEAD":
            # HEAD requests MUST NOT return a message-body in the response.
            response.body = []
        
        try:
            cherrypy.log.access()
        except:
            cherrypy.log.error(traceback=True)
        
        if response.timed_out:
            raise cherrypy.TimeoutError()
        
        return response
    
    # Uncomment for stage debugging
    # stage = property(lambda self: self._stage, lambda self, v: print(v))
    
    def respond(self, path_info):
        """Generate a response for the resource at self.path_info. (Core)"""
        response = cherrypy.serving.response
        try:
            try:
                try:
                    if self.app is None:
                        raise cherrypy.NotFound()
                    
                    # Get the 'Host' header, so we can HTTPRedirect properly.
                    self.stage = 'process_headers'
                    self.process_headers()
                    
                    # Make a copy of the class hooks
                    self.hooks = self.__class__.hooks.copy()
                    self.toolmaps = {}
                    
                    self.stage = 'get_resource'
                    self.get_resource(path_info)
                    
                    self.body = _cpreqbody.RequestBody(
                        self.rfile, self.headers, request_params=self.params)
                    
                    self.namespaces(self.config)
                    
                    self.stage = 'on_start_resource'
                    self.hooks.run('on_start_resource')
                    
                    # Parse the querystring
                    self.stage = 'process_query_string'
                    self.process_query_string()
                    
                    # Process the body
                    if self.process_request_body:
                        if self.method not in self.methods_with_bodies:
                            self.process_request_body = False
                    self.stage = 'before_request_body'
                    self.hooks.run('before_request_body')
                    if self.process_request_body:
                        self.body.process()
                    
                    # Run the handler
                    self.stage = 'before_handler'
                    self.hooks.run('before_handler')
                    if self.handler:
                        self.stage = 'handler'
                        response.body = self.handler()
                    
                    # Finalize
                    self.stage = 'before_finalize'
                    self.hooks.run('before_finalize')
                    response.finalize()
                except (cherrypy.HTTPRedirect, cherrypy.HTTPError), inst:
                    inst.set_response()
                    self.stage = 'before_finalize (HTTPError)'
                    self.hooks.run('before_finalize')
                    response.finalize()
            finally:
                self.stage = 'on_end_resource'
                self.hooks.run('on_end_resource')
        except self.throws:
            raise
        except:
            if self.throw_errors:
                raise
            self.handle_error()
    
    def process_query_string(self):
        """Parse the query string into Python structures. (Core)"""
        try:
            p = httputil.parse_query_string(
                self.query_string, encoding=self.query_string_encoding)
        except UnicodeDecodeError:
            raise cherrypy.HTTPError(
                404, "The given query string could not be processed. Query "
                "strings for this resource must be encoded with %r." % 
                self.query_string_encoding)
        
        # Python 2 only: keyword arguments must be byte strings (type 'str').
        for key, value in p.items():
            if isinstance(key, unicode):
                del p[key]
                p[key.encode(self.query_string_encoding)] = value
        self.params.update(p)
    
    def process_headers(self):
        """Parse HTTP header data into Python structures. (Core)"""
        # Process the headers into self.headers
        headers = self.headers
        for name, value in self.header_list:
            # Call title() now (and use dict.__method__(headers))
            # so title doesn't have to be called twice.
            name = name.title()
            value = value.strip()
            
            # Warning: if there is more than one header entry for cookies (AFAIK,
            # only Konqueror does that), only the last one will remain in headers
            # (but they will be correctly stored in request.cookie).
            if "=?" in value:
                dict.__setitem__(headers, name, httputil.decode_TEXT(value))
            else:
                dict.__setitem__(headers, name, value)
            
            # Handle cookies differently because on Konqueror, multiple
            # cookies come on different lines with the same key
            if name == 'Cookie':
                try:
                    self.cookie.load(value)
                except CookieError:
                    msg = "Illegal cookie name %s" % value.split('=')[0]
                    raise cherrypy.HTTPError(400, msg)
        
        if not dict.__contains__(headers, 'Host'):
            # All Internet-based HTTP/1.1 servers MUST respond with a 400
            # (Bad Request) status code to any HTTP/1.1 request message
            # which lacks a Host header field.
            if self.protocol >= (1, 1):
                msg = "HTTP/1.1 requires a 'Host' request header."
                raise cherrypy.HTTPError(400, msg)
        host = dict.get(headers, 'Host')
        if not host:
            host = self.local.name or self.local.ip
        self.base = "%s://%s" % (self.scheme, host)
    
    def get_resource(self, path):
        """Call a dispatcher (which sets self.handler and .config). (Core)"""
        # First, see if there is a custom dispatch at this URI. Custom
        # dispatchers can only be specified in app.config, not in _cp_config
        # (since custom dispatchers may not even have an app.root).
        dispatch = self.app.find_config(path, "request.dispatch", self.dispatch)
        
        # dispatch() should set self.handler and self.config
        dispatch(path)
    
    def handle_error(self):
        """Handle the last unanticipated exception. (Core)"""
        try:
            self.hooks.run("before_error_response")
            if self.error_response:
                self.error_response()
            self.hooks.run("after_error_response")
            cherrypy.serving.response.finalize()
        except cherrypy.HTTPRedirect, inst:
            inst.set_response()
            cherrypy.serving.response.finalize()
    
    # ------------------------- Properties ------------------------- #
    
    def _get_body_params(self):
        warnings.warn(
                "body_params is deprecated in CherryPy 3.2, will be removed in "
                "CherryPy 3.3.",
                DeprecationWarning
            )
        return self.body.params
    body_params = property(_get_body_params,
                      doc="""
    If the request Content-Type is 'application/x-www-form-urlencoded' or
    multipart, this will be a dict of the params pulled from the entity
    body; that is, it will be the portion of request.params that come
    from the message body (sometimes called "POST params", although they
    can be sent with various HTTP method verbs). This value is set between
    the 'before_request_body' and 'before_handler' hooks (assuming that
    process_request_body is True).
    
    Deprecated in 3.2, will be removed for 3.3""")


class ResponseBody(object):
    """The body of the HTTP response (the response entity)."""
    
    def __get__(self, obj, objclass=None):
        if obj is None:
            # When calling on the class instead of an instance...
            return self
        else:
            return obj._body
    
    def __set__(self, obj, value):
        # Convert the given value to an iterable object.
        if isinstance(value, basestring):
            # strings get wrapped in a list because iterating over a single
            # item list is much faster than iterating over every character
            # in a long string.
            if value:
                value = [value]
            else:
                # [''] doesn't evaluate to False, so replace it with [].
                value = []
        elif isinstance(value, types.FileType):
            value = file_generator(value)
        elif value is None:
            value = []
        obj._body = value


class Response(object):
    """An HTTP Response, including status, headers, and body.
    
    Application developers should use Response.headers (a dict) to
    set or modify HTTP response headers. When the response is finalized,
    Response.headers is transformed into Response.header_list as
    (key, value) tuples.
    """
    
    __metaclass__ = cherrypy._AttributeDocstrings
    
    # Class attributes for dev-time introspection.
    status = ""
    status__doc = """The HTTP Status-Code and Reason-Phrase."""
    
    header_list = []
    header_list__doc = """
    A list of the HTTP response headers as (name, value) tuples.
    In general, you should use response.headers (a dict) instead."""
    
    headers = httputil.HeaderMap()
    headers__doc = """
    A dict-like object containing the response headers. Keys are header
    names (in Title-Case format); however, you may get and set them in
    a case-insensitive manner. That is, headers['Content-Type'] and
    headers['content-type'] refer to the same value. Values are header
    values (decoded according to RFC 2047 if necessary). See also:
    httputil.HeaderMap, httputil.HeaderElement."""
    
    cookie = SimpleCookie()
    cookie__doc = """See help(Cookie)."""
    
    body = ResponseBody()
    body__doc = """The body (entity) of the HTTP response."""
    
    time = None
    time__doc = """The value of time.time() when created. Use in HTTP dates."""
    
    timeout = 300
    timeout__doc = """Seconds after which the response will be aborted."""
    
    timed_out = False
    timed_out__doc = """
    Flag to indicate the response should be aborted, because it has
    exceeded its timeout."""
    
    stream = False
    stream__doc = """If False, buffer the response body."""
    
    def __init__(self):
        self.status = None
        self.header_list = None
        self._body = []
        self.time = time.time()
        
        self.headers = httputil.HeaderMap()
        # Since we know all our keys are titled strings, we can
        # bypass HeaderMap.update and get a big speed boost.
        dict.update(self.headers, {
            "Content-Type": 'text/html',
            "Server": "CherryPy/" + cherrypy.__version__,
            "Date": httputil.HTTPDate(self.time),
        })
        self.cookie = SimpleCookie()
    
    def collapse_body(self):
        """Collapse self.body to a single string; replace it and return it."""
        if isinstance(self.body, basestring):
            return self.body

        newbody = ''.join([chunk for chunk in self.body])
        self.body = newbody
        return newbody
    
    def finalize(self):
        """Transform headers (and cookies) into self.header_list. (Core)"""
        try:
            code, reason, _ = httputil.valid_status(self.status)
        except ValueError, x:
            raise cherrypy.HTTPError(500, x.args[0])
        
        headers = self.headers
        
        self.output_status = str(code) + " " + headers.encode(reason)
        
        if self.stream:
            # The upshot: wsgiserver will chunk the response if
            # you pop Content-Length (or set it explicitly to None).
            # Note that lib.static sets C-L to the file's st_size.
            if dict.get(headers, 'Content-Length') is None:
                dict.pop(headers, 'Content-Length', None)
        elif code < 200 or code in (204, 205, 304):
            # "All 1xx (informational), 204 (no content),
            # and 304 (not modified) responses MUST NOT
            # include a message-body."
            dict.pop(headers, 'Content-Length', None)
            self.body = ""
        else:
            # Responses which are not streamed should have a Content-Length,
            # but allow user code to set Content-Length if desired.
            if dict.get(headers, 'Content-Length') is None:
                content = self.collapse_body()
                dict.__setitem__(headers, 'Content-Length', len(content))
        
        # Transform our header dict into a list of tuples.
        self.header_list = h = headers.output()
        
        cookie = self.cookie.output()
        if cookie:
            for line in cookie.split("\n"):
                if line.endswith("\r"):
                    # Python 2.4 emits cookies joined by LF but 2.5+ by CRLF.
                    line = line[:-1]
                name, value = line.split(": ", 1)
                if isinstance(name, unicode):
                    name = name.encode("ISO-8859-1")
                if isinstance(value, unicode):
                    value = headers.encode(value)
                h.append((name, value))
    
    def check_timeout(self):
        """If now > self.time + self.timeout, set self.timed_out.
        
        This purposefully sets a flag, rather than raising an error,
        so that a monitor thread can interrupt the Response thread.
        """
        if time.time() > self.time + self.timeout:
            self.timed_out = True




########NEW FILE########
__FILENAME__ = _cpserver
"""Manage HTTP servers with CherryPy."""

import warnings

import cherrypy
from cherrypy.lib import attributes

# We import * because we want to export check_port
# et al as attributes of this module.
from cherrypy.process.servers import *


class Server(ServerAdapter):
    """An adapter for an HTTP server.
    
    You can set attributes (like socket_host and socket_port)
    on *this* object (which is probably cherrypy.server), and call
    quickstart. For example:
    
        cherrypy.server.socket_port = 80
        cherrypy.quickstart()
    """
    
    socket_port = 8080
    
    _socket_host = '127.0.0.1'
    def _get_socket_host(self):
        return self._socket_host
    def _set_socket_host(self, value):
        if value == '':
            raise ValueError("The empty string ('') is not an allowed value. "
                             "Use '0.0.0.0' instead to listen on all active "
                             "interfaces (INADDR_ANY).")
        self._socket_host = value
    socket_host = property(_get_socket_host, _set_socket_host,
        doc="""The hostname or IP address on which to listen for connections.
        
        Host values may be any IPv4 or IPv6 address, or any valid hostname.
        The string 'localhost' is a synonym for '127.0.0.1' (or '::1', if
        your hosts file prefers IPv6). The string '0.0.0.0' is a special
        IPv4 entry meaning "any active interface" (INADDR_ANY), and '::'
        is the similar IN6ADDR_ANY for IPv6. The empty string or None are
        not allowed.""")
    
    socket_file = None
    socket_queue_size = 5
    socket_timeout = 10
    shutdown_timeout = 5
    protocol_version = 'HTTP/1.1'
    reverse_dns = False
    thread_pool = 10
    thread_pool_max = -1
    max_request_header_size = 500 * 1024
    max_request_body_size = 100 * 1024 * 1024
    instance = None
    ssl_context = None
    ssl_certificate = None
    ssl_certificate_chain = None
    ssl_private_key = None
    ssl_module = 'pyopenssl'
    nodelay = True
    wsgi_version = (1, 1)
    
    def __init__(self):
        self.bus = cherrypy.engine
        self.httpserver = None
        self.interrupt = None
        self.running = False
    
    def httpserver_from_self(self, httpserver=None):
        """Return a (httpserver, bind_addr) pair based on self attributes."""
        if httpserver is None:
            httpserver = self.instance
        if httpserver is None:
            from cherrypy import _cpwsgi_server
            httpserver = _cpwsgi_server.CPWSGIServer(self)
        if isinstance(httpserver, basestring):
            # Is anyone using this? Can I add an arg?
            httpserver = attributes(httpserver)(self)
        return httpserver, self.bind_addr
    
    def start(self):
        """Start the HTTP server."""
        if not self.httpserver:
            self.httpserver, self.bind_addr = self.httpserver_from_self()
        ServerAdapter.start(self)
    start.priority = 75
    
    def _get_bind_addr(self):
        if self.socket_file:
            return self.socket_file
        if self.socket_host is None and self.socket_port is None:
            return None
        return (self.socket_host, self.socket_port)
    def _set_bind_addr(self, value):
        if value is None:
            self.socket_file = None
            self.socket_host = None
            self.socket_port = None
        elif isinstance(value, basestring):
            self.socket_file = value
            self.socket_host = None
            self.socket_port = None
        else:
            try:
                self.socket_host, self.socket_port = value
                self.socket_file = None
            except ValueError:
                raise ValueError("bind_addr must be a (host, port) tuple "
                                 "(for TCP sockets) or a string (for Unix "
                                 "domain sockets), not %r" % value)
    bind_addr = property(_get_bind_addr, _set_bind_addr)
    
    def base(self):
        """Return the base (scheme://host[:port] or sock file) for this server."""
        if self.socket_file:
            return self.socket_file
        
        host = self.socket_host
        if host in ('0.0.0.0', '::'):
            # 0.0.0.0 is INADDR_ANY and :: is IN6ADDR_ANY.
            # Look up the host name, which should be the
            # safest thing to spit out in a URL.
            import socket
            host = socket.gethostname()
        
        port = self.socket_port
        
        if self.ssl_certificate:
            scheme = "https"
            if port != 443:
                host += ":%s" % port
        else:
            scheme = "http"
            if port != 80:
                host += ":%s" % port
        
        return "%s://%s" % (scheme, host)


########NEW FILE########
__FILENAME__ = _cpthreadinglocal
# This is a backport of Python-2.4's threading.local() implementation

"""Thread-local objects

(Note that this module provides a Python version of thread
 threading.local class.  Depending on the version of Python you're
 using, there may be a faster one available.  You should always import
 the local class from threading.)

Thread-local objects support the management of thread-local data.
If you have data that you want to be local to a thread, simply create
a thread-local object and use its attributes:

  >>> mydata = local()
  >>> mydata.number = 42
  >>> mydata.number
  42

You can also access the local-object's dictionary:

  >>> mydata.__dict__
  {'number': 42}
  >>> mydata.__dict__.setdefault('widgets', [])
  []
  >>> mydata.widgets
  []

What's important about thread-local objects is that their data are
local to a thread. If we access the data in a different thread:

  >>> log = []
  >>> def f():
  ...     items = mydata.__dict__.items()
  ...     items.sort()
  ...     log.append(items)
  ...     mydata.number = 11
  ...     log.append(mydata.number)

  >>> import threading
  >>> thread = threading.Thread(target=f)
  >>> thread.start()
  >>> thread.join()
  >>> log
  [[], 11]

we get different data.  Furthermore, changes made in the other thread
don't affect data seen in this thread:

  >>> mydata.number
  42

Of course, values you get from a local object, including a __dict__
attribute, are for whatever thread was current at the time the
attribute was read.  For that reason, you generally don't want to save
these values across threads, as they apply only to the thread they
came from.

You can create custom local objects by subclassing the local class:

  >>> class MyLocal(local):
  ...     number = 2
  ...     initialized = False
  ...     def __init__(self, **kw):
  ...         if self.initialized:
  ...             raise SystemError('__init__ called too many times')
  ...         self.initialized = True
  ...         self.__dict__.update(kw)
  ...     def squared(self):
  ...         return self.number ** 2

This can be useful to support default values, methods and
initialization.  Note that if you define an __init__ method, it will be
called each time the local object is used in a separate thread.  This
is necessary to initialize each thread's dictionary.

Now if we create a local object:

  >>> mydata = MyLocal(color='red')

Now we have a default number:

  >>> mydata.number
  2

an initial color:

  >>> mydata.color
  'red'
  >>> del mydata.color

And a method that operates on the data:

  >>> mydata.squared()
  4

As before, we can access the data in a separate thread:

  >>> log = []
  >>> thread = threading.Thread(target=f)
  >>> thread.start()
  >>> thread.join()
  >>> log
  [[('color', 'red'), ('initialized', True)], 11]

without affecting this thread's data:

  >>> mydata.number
  2
  >>> mydata.color
  Traceback (most recent call last):
  ...
  AttributeError: 'MyLocal' object has no attribute 'color'

Note that subclasses can define slots, but they are not thread
local. They are shared across threads:

  >>> class MyLocal(local):
  ...     __slots__ = 'number'

  >>> mydata = MyLocal()
  >>> mydata.number = 42
  >>> mydata.color = 'red'

So, the separate thread:

  >>> thread = threading.Thread(target=f)
  >>> thread.start()
  >>> thread.join()

affects what we see:

  >>> mydata.number
  11

>>> del mydata
"""

# Threading import is at end

class _localbase(object):
    __slots__ = '_local__key', '_local__args', '_local__lock'

    def __new__(cls, *args, **kw):
        self = object.__new__(cls)
        key = 'thread.local.' + str(id(self))
        object.__setattr__(self, '_local__key', key)
        object.__setattr__(self, '_local__args', (args, kw))
        object.__setattr__(self, '_local__lock', RLock())

        if args or kw and (cls.__init__ is object.__init__):
            raise TypeError("Initialization arguments are not supported")

        # We need to create the thread dict in anticipation of
        # __init__ being called, to make sure we don't call it
        # again ourselves.
        dict = object.__getattribute__(self, '__dict__')
        currentThread().__dict__[key] = dict

        return self

def _patch(self):
    key = object.__getattribute__(self, '_local__key')
    d = currentThread().__dict__.get(key)
    if d is None:
        d = {}
        currentThread().__dict__[key] = d
        object.__setattr__(self, '__dict__', d)

        # we have a new instance dict, so call out __init__ if we have
        # one
        cls = type(self)
        if cls.__init__ is not object.__init__:
            args, kw = object.__getattribute__(self, '_local__args')
            cls.__init__(self, *args, **kw)
    else:
        object.__setattr__(self, '__dict__', d)

class local(_localbase):

    def __getattribute__(self, name):
        lock = object.__getattribute__(self, '_local__lock')
        lock.acquire()
        try:
            _patch(self)
            return object.__getattribute__(self, name)
        finally:
            lock.release()

    def __setattr__(self, name, value):
        lock = object.__getattribute__(self, '_local__lock')
        lock.acquire()
        try:
            _patch(self)
            return object.__setattr__(self, name, value)
        finally:
            lock.release()

    def __delattr__(self, name):
        lock = object.__getattribute__(self, '_local__lock')
        lock.acquire()
        try:
            _patch(self)
            return object.__delattr__(self, name)
        finally:
            lock.release()


    def __del__():
        threading_enumerate = enumerate
        __getattribute__ = object.__getattribute__

        def __del__(self):
            key = __getattribute__(self, '_local__key')

            try:
                threads = list(threading_enumerate())
            except:
                # if enumerate fails, as it seems to do during
                # shutdown, we'll skip cleanup under the assumption
                # that there is nothing to clean up
                return

            for thread in threads:
                try:
                    __dict__ = thread.__dict__
                except AttributeError:
                    # Thread is dying, rest in peace
                    continue

                if key in __dict__:
                    try:
                        del __dict__[key]
                    except KeyError:
                        pass # didn't have anything in this thread

        return __del__
    __del__ = __del__()

from threading import currentThread, enumerate, RLock

########NEW FILE########
__FILENAME__ = _cptools
"""CherryPy tools. A "tool" is any helper, adapted to CP.

Tools are usually designed to be used in a variety of ways (although some
may only offer one if they choose):
    
    Library calls:
        All tools are callables that can be used wherever needed.
        The arguments are straightforward and should be detailed within the
        docstring.
    
    Function decorators:
        All tools, when called, may be used as decorators which configure
        individual CherryPy page handlers (methods on the CherryPy tree).
        That is, "@tools.anytool()" should "turn on" the tool via the
        decorated function's _cp_config attribute.
    
    CherryPy config:
        If a tool exposes a "_setup" callable, it will be called
        once per Request (if the feature is "turned on" via config).

Tools may be implemented as any object with a namespace. The builtins
are generally either modules or instances of the tools.Tool class.
"""

import cherrypy
import warnings


def _getargs(func):
    """Return the names of all static arguments to the given function."""
    # Use this instead of importing inspect for less mem overhead.
    import types
    if isinstance(func, types.MethodType):
        func = func.im_func
    co = func.func_code
    return co.co_varnames[:co.co_argcount]


_attr_error = ("CherryPy Tools cannot be turned on directly. Instead, turn them "
               "on via config, or use them as decorators on your page handlers.")

class Tool(object):
    """A registered function for use with CherryPy request-processing hooks.
    
    help(tool.callable) should give you more information about this Tool.
    """
    
    namespace = "tools"
    
    def __init__(self, point, callable, name=None, priority=50):
        self._point = point
        self.callable = callable
        self._name = name
        self._priority = priority
        self.__doc__ = self.callable.__doc__
        self._setargs()
    
    def _get_on(self):
        raise AttributeError(_attr_error)
    def _set_on(self, value):
        raise AttributeError(_attr_error)
    on = property(_get_on, _set_on)
    
    def _setargs(self):
        """Copy func parameter names to obj attributes."""
        try:
            for arg in _getargs(self.callable):
                setattr(self, arg, None)
        except (TypeError, AttributeError):
            if hasattr(self.callable, "__call__"):
                for arg in _getargs(self.callable.__call__):
                    setattr(self, arg, None)
        # IronPython 1.0 raises NotImplementedError because
        # inspect.getargspec tries to access Python bytecode
        # in co_code attribute.
        except NotImplementedError:
            pass
        # IronPython 1B1 may raise IndexError in some cases,
        # but if we trap it here it doesn't prevent CP from working.
        except IndexError:
            pass
    
    def _merged_args(self, d=None):
        """Return a dict of configuration entries for this Tool."""
        if d:
            conf = d.copy()
        else:
            conf = {}
        
        tm = cherrypy.serving.request.toolmaps[self.namespace]
        if self._name in tm:
            conf.update(tm[self._name])
        
        if "on" in conf:
            del conf["on"]
        
        return conf
    
    def __call__(self, *args, **kwargs):
        """Compile-time decorator (turn on the tool in config).
        
        For example:
        
            @tools.proxy()
            def whats_my_base(self):
                return cherrypy.request.base
            whats_my_base.exposed = True
        """
        if args:
            raise TypeError("The %r Tool does not accept positional "
                            "arguments; you must use keyword arguments."
                            % self._name)
        def tool_decorator(f):
            if not hasattr(f, "_cp_config"):
                f._cp_config = {}
            subspace = self.namespace + "." + self._name + "."
            f._cp_config[subspace + "on"] = True
            for k, v in kwargs.items():
                f._cp_config[subspace + k] = v
            return f
        return tool_decorator
    
    def _setup(self):
        """Hook this tool into cherrypy.request.
        
        The standard CherryPy request object will automatically call this
        method when the tool is "turned on" in config.
        """
        conf = self._merged_args()
        p = conf.pop("priority", None)
        if p is None:
            p = getattr(self.callable, "priority", self._priority)
        cherrypy.serving.request.hooks.attach(self._point, self.callable,
                                              priority=p, **conf)


class HandlerTool(Tool):
    """Tool which is called 'before main', that may skip normal handlers.
    
    If the tool successfully handles the request (by setting response.body),
    if should return True. This will cause CherryPy to skip any 'normal' page
    handler. If the tool did not handle the request, it should return False
    to tell CherryPy to continue on and call the normal page handler. If the
    tool is declared AS a page handler (see the 'handler' method), returning
    False will raise NotFound.
    """
    
    def __init__(self, callable, name=None):
        Tool.__init__(self, 'before_handler', callable, name)
    
    def handler(self, *args, **kwargs):
        """Use this tool as a CherryPy page handler.
        
        For example:
            class Root:
                nav = tools.staticdir.handler(section="/nav", dir="nav",
                                              root=absDir)
        """
        def handle_func(*a, **kw):
            handled = self.callable(*args, **self._merged_args(kwargs))
            if not handled:
                raise cherrypy.NotFound()
            return cherrypy.serving.response.body
        handle_func.exposed = True
        return handle_func
    
    def _wrapper(self, **kwargs):
        if self.callable(**kwargs):
            cherrypy.serving.request.handler = None
    
    def _setup(self):
        """Hook this tool into cherrypy.request.
        
        The standard CherryPy request object will automatically call this
        method when the tool is "turned on" in config.
        """
        conf = self._merged_args()
        p = conf.pop("priority", None)
        if p is None:
            p = getattr(self.callable, "priority", self._priority)
        cherrypy.serving.request.hooks.attach(self._point, self._wrapper,
                                              priority=p, **conf)


class HandlerWrapperTool(Tool):
    """Tool which wraps request.handler in a provided wrapper function.
    
    The 'newhandler' arg must be a handler wrapper function that takes a
    'next_handler' argument, plus *args and **kwargs. Like all page handler
    functions, it must return an iterable for use as cherrypy.response.body.
    
    For example, to allow your 'inner' page handlers to return dicts
    which then get interpolated into a template:
    
        def interpolator(next_handler, *args, **kwargs):
            filename = cherrypy.request.config.get('template')
            cherrypy.response.template = env.get_template(filename)
            response_dict = next_handler(*args, **kwargs)
            return cherrypy.response.template.render(**response_dict)
        cherrypy.tools.jinja = HandlerWrapperTool(interpolator)
    """
    
    def __init__(self, newhandler, point='before_handler', name=None, priority=50):
        self.newhandler = newhandler
        self._point = point
        self._name = name
        self._priority = priority
    
    def callable(self, debug=False):
        innerfunc = cherrypy.serving.request.handler
        def wrap(*args, **kwargs):
            return self.newhandler(innerfunc, *args, **kwargs)
        cherrypy.serving.request.handler = wrap


class ErrorTool(Tool):
    """Tool which is used to replace the default request.error_response."""
    
    def __init__(self, callable, name=None):
        Tool.__init__(self, None, callable, name)
    
    def _wrapper(self):
        self.callable(**self._merged_args())
    
    def _setup(self):
        """Hook this tool into cherrypy.request.
        
        The standard CherryPy request object will automatically call this
        method when the tool is "turned on" in config.
        """
        cherrypy.serving.request.error_response = self._wrapper


#                              Builtin tools                              #

from cherrypy.lib import cptools, encoding, auth, static, jsontools
from cherrypy.lib import sessions as _sessions, xmlrpc as _xmlrpc
from cherrypy.lib import caching as _caching
from cherrypy.lib import auth_basic, auth_digest


class SessionTool(Tool):
    """Session Tool for CherryPy.
    
    sessions.locking:
        When 'implicit' (the default), the session will be locked for you,
            just before running the page handler.
        When 'early', the session will be locked before reading the request
            body. This is off by default for safety reasons; for example,
            a large upload would block the session, denying an AJAX
            progress meter (see http://www.cherrypy.org/ticket/630).
        When 'explicit' (or any other value), you need to call
            cherrypy.session.acquire_lock() yourself before using
            session data.
    """
    
    def __init__(self):
        # _sessions.init must be bound after headers are read
        Tool.__init__(self, 'before_request_body', _sessions.init)
    
    def _lock_session(self):
        cherrypy.serving.session.acquire_lock()
    
    def _setup(self):
        """Hook this tool into cherrypy.request.
        
        The standard CherryPy request object will automatically call this
        method when the tool is "turned on" in config.
        """
        hooks = cherrypy.serving.request.hooks
        
        conf = self._merged_args()
        
        p = conf.pop("priority", None)
        if p is None:
            p = getattr(self.callable, "priority", self._priority)
        
        hooks.attach(self._point, self.callable, priority=p, **conf)
        
        locking = conf.pop('locking', 'implicit')
        if locking == 'implicit':
            hooks.attach('before_handler', self._lock_session)
        elif locking == 'early':
            # Lock before the request body (but after _sessions.init runs!)
            hooks.attach('before_request_body', self._lock_session,
                         priority=60)
        else:
            # Don't lock
            pass
        
        hooks.attach('before_finalize', _sessions.save)
        hooks.attach('on_end_request', _sessions.close)
        
    def regenerate(self):
        """Drop the current session and make a new one (with a new id)."""
        sess = cherrypy.serving.session
        sess.regenerate()
        
        # Grab cookie-relevant tool args
        conf = dict([(k, v) for k, v in self._merged_args().items()
                     if k in ('path', 'path_header', 'name', 'timeout',
                              'domain', 'secure')])
        _sessions.set_response_cookie(**conf)




class XMLRPCController(object):
    """A Controller (page handler collection) for XML-RPC.
    
    To use it, have your controllers subclass this base class (it will
    turn on the tool for you).
    
    You can also supply the following optional config entries:
        
        tools.xmlrpc.encoding: 'utf-8'
        tools.xmlrpc.allow_none: 0
    
    XML-RPC is a rather discontinuous layer over HTTP; dispatching to the
    appropriate handler must first be performed according to the URL, and
    then a second dispatch step must take place according to the RPC method
    specified in the request body. It also allows a superfluous "/RPC2"
    prefix in the URL, supplies its own handler args in the body, and
    requires a 200 OK "Fault" response instead of 404 when the desired
    method is not found.
    
    Therefore, XML-RPC cannot be implemented for CherryPy via a Tool alone.
    This Controller acts as the dispatch target for the first half (based
    on the URL); it then reads the RPC method from the request body and
    does its own second dispatch step based on that method. It also reads
    body params, and returns a Fault on error.
    
    The XMLRPCDispatcher strips any /RPC2 prefix; if you aren't using /RPC2
    in your URL's, you can safely skip turning on the XMLRPCDispatcher.
    Otherwise, you need to use declare it in config:
        
        request.dispatch: cherrypy.dispatch.XMLRPCDispatcher()
    """
    
    # Note we're hard-coding this into the 'tools' namespace. We could do
    # a huge amount of work to make it relocatable, but the only reason why
    # would be if someone actually disabled the default_toolbox. Meh.
    _cp_config = {'tools.xmlrpc.on': True}
    
    def default(self, *vpath, **params):
        rpcparams, rpcmethod = _xmlrpc.process_body()
        
        subhandler = self
        for attr in str(rpcmethod).split('.'):
            subhandler = getattr(subhandler, attr, None)
         
        if subhandler and getattr(subhandler, "exposed", False):
            body = subhandler(*(vpath + rpcparams), **params)
        
        else:
            # http://www.cherrypy.org/ticket/533
            # if a method is not found, an xmlrpclib.Fault should be returned
            # raising an exception here will do that; see
            # cherrypy.lib.xmlrpc.on_error
            raise Exception('method "%s" is not supported' % attr)
        
        conf = cherrypy.serving.request.toolmaps['tools'].get("xmlrpc", {})
        _xmlrpc.respond(body,
                        conf.get('encoding', 'utf-8'),
                        conf.get('allow_none', 0))
        return cherrypy.serving.response.body
    default.exposed = True


class SessionAuthTool(HandlerTool):
    
    def _setargs(self):
        for name in dir(cptools.SessionAuth):
            if not name.startswith("__"):
                setattr(self, name, None)


class CachingTool(Tool):
    """Caching Tool for CherryPy."""
    
    def _wrapper(self, **kwargs):
        request = cherrypy.serving.request
        if _caching.get(**kwargs):
            request.handler = None
        else:
            if request.cacheable:
                # Note the devious technique here of adding hooks on the fly
                request.hooks.attach('before_finalize', _caching.tee_output,
                                     priority=90)
    _wrapper.priority = 20
    
    def _setup(self):
        """Hook caching into cherrypy.request."""
        conf = self._merged_args()
        
        p = conf.pop("priority", None)
        cherrypy.serving.request.hooks.attach('before_handler', self._wrapper,
                                              priority=p, **conf)



class Toolbox(object):
    """A collection of Tools.
    
    This object also functions as a config namespace handler for itself.
    Custom toolboxes should be added to each Application's toolboxes dict.
    """
    
    def __init__(self, namespace):
        self.namespace = namespace
    
    def __setattr__(self, name, value):
        # If the Tool._name is None, supply it from the attribute name.
        if isinstance(value, Tool):
            if value._name is None:
                value._name = name
            value.namespace = self.namespace
        object.__setattr__(self, name, value)
    
    def __enter__(self):
        """Populate request.toolmaps from tools specified in config."""
        cherrypy.serving.request.toolmaps[self.namespace] = map = {}
        def populate(k, v):
            toolname, arg = k.split(".", 1)
            bucket = map.setdefault(toolname, {})
            bucket[arg] = v
        return populate
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Run tool._setup() for each tool in our toolmap."""
        map = cherrypy.serving.request.toolmaps.get(self.namespace)
        if map:
            for name, settings in map.items():
                if settings.get("on", False):
                    tool = getattr(self, name)
                    tool._setup()


class DeprecatedTool(Tool):
    
    _name = None
    warnmsg = "This Tool is deprecated."
    
    def __init__(self, point, warnmsg=None):
        self.point = point
        if warnmsg is not None:
            self.warnmsg = warnmsg
    
    def __call__(self, *args, **kwargs):
        warnings.warn(self.warnmsg)
        def tool_decorator(f):
            return f
        return tool_decorator
    
    def _setup(self):
        warnings.warn(self.warnmsg)


default_toolbox = _d = Toolbox("tools")
_d.session_auth = SessionAuthTool(cptools.session_auth)
_d.proxy = Tool('before_request_body', cptools.proxy, priority=30)
_d.response_headers = Tool('on_start_resource', cptools.response_headers)
_d.log_tracebacks = Tool('before_error_response', cptools.log_traceback)
_d.log_headers = Tool('before_error_response', cptools.log_request_headers)
_d.log_hooks = Tool('on_end_request', cptools.log_hooks, priority=100)
_d.err_redirect = ErrorTool(cptools.redirect)
_d.etags = Tool('before_finalize', cptools.validate_etags, priority=75)
_d.decode = Tool('before_request_body', encoding.decode)
# the order of encoding, gzip, caching is important
_d.encode = Tool('before_handler', encoding.ResponseEncoder, priority=70)
_d.gzip = Tool('before_finalize', encoding.gzip, priority=80)
_d.staticdir = HandlerTool(static.staticdir)
_d.staticfile = HandlerTool(static.staticfile)
_d.sessions = SessionTool()
_d.xmlrpc = ErrorTool(_xmlrpc.on_error)
_d.caching = CachingTool('before_handler', _caching.get, 'caching')
_d.expires = Tool('before_finalize', _caching.expires)
_d.tidy = DeprecatedTool('before_finalize',
    "The tidy tool has been removed from the standard distribution of CherryPy. "
    "The most recent version can be found at http://tools.cherrypy.org/browser.")
_d.nsgmls = DeprecatedTool('before_finalize',
    "The nsgmls tool has been removed from the standard distribution of CherryPy. "
    "The most recent version can be found at http://tools.cherrypy.org/browser.")
_d.ignore_headers = Tool('before_request_body', cptools.ignore_headers)
_d.referer = Tool('before_request_body', cptools.referer)
_d.basic_auth = Tool('on_start_resource', auth.basic_auth)
_d.digest_auth = Tool('on_start_resource', auth.digest_auth)
_d.trailing_slash = Tool('before_handler', cptools.trailing_slash, priority=60)
_d.flatten = Tool('before_finalize', cptools.flatten)
_d.accept = Tool('on_start_resource', cptools.accept)
_d.redirect = Tool('on_start_resource', cptools.redirect)
_d.autovary = Tool('on_start_resource', cptools.autovary, priority=0)
_d.json_in = Tool('before_request_body', jsontools.json_in, priority=30)
_d.json_out = Tool('before_handler', jsontools.json_out, priority=30)
_d.auth_basic = Tool('before_handler', auth_basic.basic_auth, priority=1)
_d.auth_digest = Tool('before_handler', auth_digest.digest_auth, priority=1)

del _d, cptools, encoding, auth, static

########NEW FILE########
__FILENAME__ = _cptree
"""CherryPy Application and Tree objects."""

import os
import cherrypy
from cherrypy import _cpconfig, _cplogging, _cprequest, _cpwsgi, tools
from cherrypy.lib import httputil


class Application(object):
    """A CherryPy Application.
    
    Servers and gateways should not instantiate Request objects directly.
    Instead, they should ask an Application object for a request object.
    
    An instance of this class may also be used as a WSGI callable
    (WSGI application object) for itself.
    """
    
    __metaclass__ = cherrypy._AttributeDocstrings
    
    root = None
    root__doc = """
    The top-most container of page handlers for this app. Handlers should
    be arranged in a hierarchy of attributes, matching the expected URI
    hierarchy; the default dispatcher then searches this hierarchy for a
    matching handler. When using a dispatcher other than the default,
    this value may be None."""
    
    config = {}
    config__doc = """
    A dict of {path: pathconf} pairs, where 'pathconf' is itself a dict
    of {key: value} pairs."""
    
    namespaces = _cpconfig.NamespaceSet()
    toolboxes = {'tools': cherrypy.tools}
    
    log = None
    log__doc = """A LogManager instance. See _cplogging."""
    
    wsgiapp = None
    wsgiapp__doc = """A CPWSGIApp instance. See _cpwsgi."""
    
    request_class = _cprequest.Request
    response_class = _cprequest.Response
    
    relative_urls = False
    
    def __init__(self, root, script_name="", config=None):
        self.log = _cplogging.LogManager(id(self), cherrypy.log.logger_root)
        self.root = root
        self.script_name = script_name
        self.wsgiapp = _cpwsgi.CPWSGIApp(self)
        
        self.namespaces = self.namespaces.copy()
        self.namespaces["log"] = lambda k, v: setattr(self.log, k, v)
        self.namespaces["wsgi"] = self.wsgiapp.namespace_handler
        
        self.config = self.__class__.config.copy()
        if config:
            self.merge(config)
    
    def __repr__(self):
        return "%s.%s(%r, %r)" % (self.__module__, self.__class__.__name__,
                                  self.root, self.script_name)
    
    script_name__doc = """
    The URI "mount point" for this app. A mount point is that portion of
    the URI which is constant for all URIs that are serviced by this
    application; it does not include scheme, host, or proxy ("virtual host")
    portions of the URI.
    
    For example, if script_name is "/my/cool/app", then the URL
    "http://www.example.com/my/cool/app/page1" might be handled by a
    "page1" method on the root object.
    
    The value of script_name MUST NOT end in a slash. If the script_name
    refers to the root of the URI, it MUST be an empty string (not "/").
    
    If script_name is explicitly set to None, then the script_name will be
    provided for each call from request.wsgi_environ['SCRIPT_NAME'].
    """
    def _get_script_name(self):
        if self._script_name is None:
            # None signals that the script name should be pulled from WSGI environ.
            return cherrypy.serving.request.wsgi_environ['SCRIPT_NAME'].rstrip("/")
        return self._script_name
    def _set_script_name(self, value):
        if value:
            value = value.rstrip("/")
        self._script_name = value
    script_name = property(fget=_get_script_name, fset=_set_script_name,
                           doc=script_name__doc)
    
    def merge(self, config):
        """Merge the given config into self.config."""
        _cpconfig.merge(self.config, config)
        
        # Handle namespaces specified in config.
        self.namespaces(self.config.get("/", {}))
    
    def find_config(self, path, key, default=None):
        """Return the most-specific value for key along path, or default."""
        trail = path or "/"
        while trail:
            nodeconf = self.config.get(trail, {})
            
            if key in nodeconf:
                return nodeconf[key]
            
            lastslash = trail.rfind("/")
            if lastslash == -1:
                break
            elif lastslash == 0 and trail != "/":
                trail = "/"
            else:
                trail = trail[:lastslash]
        
        return default
    
    def get_serving(self, local, remote, scheme, sproto):
        """Create and return a Request and Response object."""
        req = self.request_class(local, remote, scheme, sproto)
        req.app = self
        
        for name, toolbox in self.toolboxes.items():
            req.namespaces[name] = toolbox
        
        resp = self.response_class()
        cherrypy.serving.load(req, resp)
        cherrypy.engine.timeout_monitor.acquire()
        cherrypy.engine.publish('acquire_thread')
        
        return req, resp
    
    def release_serving(self):
        """Release the current serving (request and response)."""
        req = cherrypy.serving.request
        
        cherrypy.engine.timeout_monitor.release()
        
        try:
            req.close()
        except:
            cherrypy.log(traceback=True, severity=40)
        
        cherrypy.serving.clear()
    
    def __call__(self, environ, start_response):
        return self.wsgiapp(environ, start_response)


class Tree(object):
    """A registry of CherryPy applications, mounted at diverse points.
    
    An instance of this class may also be used as a WSGI callable
    (WSGI application object), in which case it dispatches to all
    mounted apps.
    """
    
    apps = {}
    apps__doc = """
    A dict of the form {script name: application}, where "script name"
    is a string declaring the URI mount point (no trailing slash), and
    "application" is an instance of cherrypy.Application (or an arbitrary
    WSGI callable if you happen to be using a WSGI server)."""
    
    def __init__(self):
        self.apps = {}
    
    def mount(self, root, script_name="", config=None):
        """Mount a new app from a root object, script_name, and config.
        
        root: an instance of a "controller class" (a collection of page
            handler methods) which represents the root of the application.
            This may also be an Application instance, or None if using
            a dispatcher other than the default.
        script_name: a string containing the "mount point" of the application.
            This should start with a slash, and be the path portion of the
            URL at which to mount the given root. For example, if root.index()
            will handle requests to "http://www.example.com:8080/dept/app1/",
            then the script_name argument would be "/dept/app1".
            
            It MUST NOT end in a slash. If the script_name refers to the
            root of the URI, it MUST be an empty string (not "/").
        config: a file or dict containing application config.
        """
        if script_name is None:
            raise TypeError(
                "The 'script_name' argument may not be None. Application "
                "objects may, however, possess a script_name of None (in "
                "order to inpect the WSGI environ for SCRIPT_NAME upon each "
                "request). You cannot mount such Applications on this Tree; "
                "you must pass them to a WSGI server interface directly.")
        
        # Next line both 1) strips trailing slash and 2) maps "/" -> "".
        script_name = script_name.rstrip("/")
        
        if isinstance(root, Application):
            app = root
            if script_name != "" and script_name != app.script_name:
                raise ValueError("Cannot specify a different script name and "
                                 "pass an Application instance to cherrypy.mount")
            script_name = app.script_name
        else:
            app = Application(root, script_name)
            
            # If mounted at "", add favicon.ico
            if (script_name == "" and root is not None
                    and not hasattr(root, "favicon_ico")):
                favicon = os.path.join(os.getcwd(), os.path.dirname(__file__),
                                       "favicon.ico")
                root.favicon_ico = tools.staticfile.handler(favicon)
        
        if config:
            app.merge(config)
        
        self.apps[script_name] = app
        
        return app
    
    def graft(self, wsgi_callable, script_name=""):
        """Mount a wsgi callable at the given script_name."""
        # Next line both 1) strips trailing slash and 2) maps "/" -> "".
        script_name = script_name.rstrip("/")
        self.apps[script_name] = wsgi_callable
    
    def script_name(self, path=None):
        """The script_name of the app at the given path, or None.
        
        If path is None, cherrypy.request is used.
        """
        if path is None:
            try:
                request = cherrypy.serving.request
                path = httputil.urljoin(request.script_name,
                                        request.path_info)
            except AttributeError:
                return None
        
        while True:
            if path in self.apps:
                return path
            
            if path == "":
                return None
            
            # Move one node up the tree and try again.
            path = path[:path.rfind("/")]
    
    def __call__(self, environ, start_response):
        # If you're calling this, then you're probably setting SCRIPT_NAME
        # to '' (some WSGI servers always set SCRIPT_NAME to '').
        # Try to look up the app using the full path.
        env1x = environ
        if environ.get(u'wsgi.version') == (u'u', 0):
            env1x = _cpwsgi.downgrade_wsgi_ux_to_1x(environ)
        path = httputil.urljoin(env1x.get('SCRIPT_NAME', ''),
                                env1x.get('PATH_INFO', ''))
        sn = self.script_name(path or "/")
        if sn is None:
            start_response('404 Not Found', [])
            return []
        
        app = self.apps[sn]
        
        # Correct the SCRIPT_NAME and PATH_INFO environ entries.
        environ = environ.copy()
        if environ.get(u'wsgi.version') == (u'u', 0):
            # Python 2/WSGI u.0: all strings MUST be of type unicode
            enc = environ[u'wsgi.url_encoding']
            environ[u'SCRIPT_NAME'] = sn.decode(enc)
            environ[u'PATH_INFO'] = path[len(sn.rstrip("/")):].decode(enc)
        else:
            # Python 2/WSGI 1.x: all strings MUST be of type str
            environ['SCRIPT_NAME'] = sn
            environ['PATH_INFO'] = path[len(sn.rstrip("/")):]
        return app(environ, start_response)


########NEW FILE########
__FILENAME__ = _cpwsgi
"""WSGI interface (see PEP 333)."""

import sys as _sys

import cherrypy as _cherrypy
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO
from cherrypy import _cperror
from cherrypy.lib import httputil


def downgrade_wsgi_ux_to_1x(environ):
    """Return a new environ dict for WSGI 1.x from the given WSGI u.x environ."""
    env1x = {}
    
    url_encoding = environ[u'wsgi.url_encoding']
    for k, v in environ.items():
        if k in [u'PATH_INFO', u'SCRIPT_NAME', u'QUERY_STRING']:
            v = v.encode(url_encoding)
        elif isinstance(v, unicode):
            v = v.encode('ISO-8859-1')
        env1x[k.encode('ISO-8859-1')] = v
    
    return env1x


class VirtualHost(object):
    """Select a different WSGI application based on the Host header.
    
    This can be useful when running multiple sites within one CP server.
    It allows several domains to point to different applications. For example:
    
        root = Root()
        RootApp = cherrypy.Application(root)
        Domain2App = cherrypy.Application(root)
        SecureApp = cherrypy.Application(Secure())
        
        vhost = cherrypy._cpwsgi.VirtualHost(RootApp,
            domains={'www.domain2.example': Domain2App,
                     'www.domain2.example:443': SecureApp,
                     })
        
        cherrypy.tree.graft(vhost)
    
    default: required. The default WSGI application.
    
    use_x_forwarded_host: if True (the default), any "X-Forwarded-Host"
        request header will be used instead of the "Host" header. This
        is commonly added by HTTP servers (such as Apache) when proxying.
    
    domains: a dict of {host header value: application} pairs.
        The incoming "Host" request header is looked up in this dict,
        and, if a match is found, the corresponding WSGI application
        will be called instead of the default. Note that you often need
        separate entries for "example.com" and "www.example.com".
        In addition, "Host" headers may contain the port number.
    """
    
    def __init__(self, default, domains=None, use_x_forwarded_host=True):
        self.default = default
        self.domains = domains or {}
        self.use_x_forwarded_host = use_x_forwarded_host
    
    def __call__(self, environ, start_response):
        domain = environ.get('HTTP_HOST', '')
        if self.use_x_forwarded_host:
            domain = environ.get("HTTP_X_FORWARDED_HOST", domain)
        
        nextapp = self.domains.get(domain)
        if nextapp is None:
            nextapp = self.default
        return nextapp(environ, start_response)


class InternalRedirector(object):
    """WSGI middleware that handles raised cherrypy.InternalRedirect."""
    
    def __init__(self, nextapp, recursive=False):
        self.nextapp = nextapp
        self.recursive = recursive
    
    def __call__(self, environ, start_response):
        redirections = []
        while True:
            environ = environ.copy()
            try:
                return self.nextapp(environ, start_response)
            except _cherrypy.InternalRedirect, ir:
                sn = environ.get('SCRIPT_NAME', '')
                path = environ.get('PATH_INFO', '')
                qs = environ.get('QUERY_STRING', '')
                
                # Add the *previous* path_info + qs to redirections.
                old_uri = sn + path
                if qs:
                    old_uri += "?" + qs
                redirections.append(old_uri)
                
                if not self.recursive:
                    # Check to see if the new URI has been redirected to already
                    new_uri = sn + ir.path
                    if ir.query_string:
                        new_uri += "?" + ir.query_string
                    if new_uri in redirections:
                        ir.request.close()
                        raise RuntimeError("InternalRedirector visited the "
                                           "same URL twice: %r" % new_uri)
                
                # Munge the environment and try again.
                environ['REQUEST_METHOD'] = "GET"
                environ['PATH_INFO'] = ir.path
                environ['QUERY_STRING'] = ir.query_string
                environ['wsgi.input'] = StringIO()
                environ['CONTENT_LENGTH'] = "0"
                environ['cherrypy.previous_request'] = ir.request


class ExceptionTrapper(object):
    
    def __init__(self, nextapp, throws=(KeyboardInterrupt, SystemExit)):
        self.nextapp = nextapp
        self.throws = throws
    
    def __call__(self, environ, start_response):
        return _TrappedResponse(self.nextapp, environ, start_response, self.throws)


class _TrappedResponse(object):
    
    response = iter([])
    
    def __init__(self, nextapp, environ, start_response, throws):
        self.nextapp = nextapp
        self.environ = environ
        self.start_response = start_response
        self.throws = throws
        self.started_response = False
        self.response = self.trap(self.nextapp, self.environ, self.start_response)
        self.iter_response = iter(self.response)
    
    def __iter__(self):
        self.started_response = True
        return self
    
    def next(self):
        return self.trap(self.iter_response.next)
    
    def close(self):
        if hasattr(self.response, 'close'):
            self.response.close()
    
    def trap(self, func, *args, **kwargs):
        try:
            return func(*args, **kwargs)
        except self.throws:
            raise
        except StopIteration:
            raise
        except:
            tb = _cperror.format_exc()
            #print('trapped (started %s):' % self.started_response, tb)
            _cherrypy.log(tb, severity=40)
            if not _cherrypy.request.show_tracebacks:
                tb = ""
            s, h, b = _cperror.bare_error(tb)
            if self.started_response:
                # Empty our iterable (so future calls raise StopIteration)
                self.iter_response = iter([])
            else:
                self.iter_response = iter(b)
            
            try:
                self.start_response(s, h, _sys.exc_info())
            except:
                # "The application must not trap any exceptions raised by
                # start_response, if it called start_response with exc_info.
                # Instead, it should allow such exceptions to propagate
                # back to the server or gateway."
                # But we still log and call close() to clean up ourselves.
                _cherrypy.log(traceback=True, severity=40)
                raise
            
            if self.started_response:
                return "".join(b)
            else:
                return b


#                           WSGI-to-CP Adapter                           #


class AppResponse(object):
    """WSGI response iterable for CherryPy applications."""
    
    def __init__(self, environ, start_response, cpapp):
        if environ.get(u'wsgi.version') == (u'u', 0):
            environ = downgrade_wsgi_ux_to_1x(environ)
        self.environ = environ
        self.cpapp = cpapp
        try:
            self.run()
        except:
            self.close()
            raise
        r = _cherrypy.serving.response
        self.iter_response = iter(r.body)
        self.write = start_response(r.output_status, r.header_list)
    
    def __iter__(self):
        return self
    
    def next(self):
        return self.iter_response.next()
    
    def close(self):
        """Close and de-reference the current request and response. (Core)"""
        self.cpapp.release_serving()
    
    def run(self):
        """Create a Request object using environ."""
        env = self.environ.get
        
        local = httputil.Host('', int(env('SERVER_PORT', 80)),
                           env('SERVER_NAME', ''))
        remote = httputil.Host(env('REMOTE_ADDR', ''),
                               int(env('REMOTE_PORT', -1)),
                               env('REMOTE_HOST', ''))
        scheme = env('wsgi.url_scheme')
        sproto = env('ACTUAL_SERVER_PROTOCOL', "HTTP/1.1")
        request, resp = self.cpapp.get_serving(local, remote, scheme, sproto)
        
        # LOGON_USER is served by IIS, and is the name of the
        # user after having been mapped to a local account.
        # Both IIS and Apache set REMOTE_USER, when possible.
        request.login = env('LOGON_USER') or env('REMOTE_USER') or None
        request.multithread = self.environ['wsgi.multithread']
        request.multiprocess = self.environ['wsgi.multiprocess']
        request.wsgi_environ = self.environ
        request.prev = env('cherrypy.previous_request', None)
        
        meth = self.environ['REQUEST_METHOD']
        
        path = httputil.urljoin(self.environ.get('SCRIPT_NAME', ''),
                                self.environ.get('PATH_INFO', ''))
        qs = self.environ.get('QUERY_STRING', '')
        rproto = self.environ.get('SERVER_PROTOCOL')
        headers = self.translate_headers(self.environ)
        rfile = self.environ['wsgi.input']
        request.run(meth, path, qs, rproto, headers, rfile)
    
    headerNames = {'HTTP_CGI_AUTHORIZATION': 'Authorization',
                   'CONTENT_LENGTH': 'Content-Length',
                   'CONTENT_TYPE': 'Content-Type',
                   'REMOTE_HOST': 'Remote-Host',
                   'REMOTE_ADDR': 'Remote-Addr',
                   }
    
    def translate_headers(self, environ):
        """Translate CGI-environ header names to HTTP header names."""
        for cgiName in environ:
            # We assume all incoming header keys are uppercase already.
            if cgiName in self.headerNames:
                yield self.headerNames[cgiName], environ[cgiName]
            elif cgiName[:5] == "HTTP_":
                # Hackish attempt at recovering original header names.
                translatedHeader = cgiName[5:].replace("_", "-")
                yield translatedHeader, environ[cgiName]


class CPWSGIApp(object):
    """A WSGI application object for a CherryPy Application.
    
    pipeline: a list of (name, wsgiapp) pairs. Each 'wsgiapp' MUST be a
        constructor that takes an initial, positional 'nextapp' argument,
        plus optional keyword arguments, and returns a WSGI application
        (that takes environ and start_response arguments). The 'name' can
        be any you choose, and will correspond to keys in self.config.
    
    head: rather than nest all apps in the pipeline on each call, it's only
        done the first time, and the result is memoized into self.head. Set
        this to None again if you change self.pipeline after calling self.
    
    config: a dict whose keys match names listed in the pipeline. Each
        value is a further dict which will be passed to the corresponding
        named WSGI callable (from the pipeline) as keyword arguments.
    """
    
    pipeline = [('ExceptionTrapper', ExceptionTrapper),
                ('InternalRedirector', InternalRedirector),
                ]
    head = None
    config = {}
    
    response_class = AppResponse
    
    def __init__(self, cpapp, pipeline=None):
        self.cpapp = cpapp
        self.pipeline = self.pipeline[:]
        if pipeline:
            self.pipeline.extend(pipeline)
        self.config = self.config.copy()
    
    def tail(self, environ, start_response):
        """WSGI application callable for the actual CherryPy application.
        
        You probably shouldn't call this; call self.__call__ instead,
        so that any WSGI middleware in self.pipeline can run first.
        """
        return self.response_class(environ, start_response, self.cpapp)
    
    def __call__(self, environ, start_response):
        head = self.head
        if head is None:
            # Create and nest the WSGI apps in our pipeline (in reverse order).
            # Then memoize the result in self.head.
            head = self.tail
            for name, callable in self.pipeline[::-1]:
                conf = self.config.get(name, {})
                head = callable(head, **conf)
            self.head = head
        return head(environ, start_response)
    
    def namespace_handler(self, k, v):
        """Config handler for the 'wsgi' namespace."""
        if k == "pipeline":
            # Note this allows multiple 'wsgi.pipeline' config entries
            # (but each entry will be processed in a 'random' order).
            # It should also allow developers to set default middleware
            # in code (passed to self.__init__) that deployers can add to
            # (but not remove) via config.
            self.pipeline.extend(v)
        elif k == "response_class":
            self.response_class = v
        else:
            name, arg = k.split(".", 1)
            bucket = self.config.setdefault(name, {})
            bucket[arg] = v


########NEW FILE########
__FILENAME__ = _cpwsgi_server
"""WSGI server interface (see PEP 333). This adds some CP-specific bits to
the framework-agnostic wsgiserver package.
"""
import sys

import cherrypy
from cherrypy import wsgiserver


class CPHTTPRequest(wsgiserver.HTTPRequest):
    pass


class CPHTTPConnection(wsgiserver.HTTPConnection):
    pass


class CPWSGIServer(wsgiserver.CherryPyWSGIServer):
    """Wrapper for wsgiserver.CherryPyWSGIServer.
    
    wsgiserver has been designed to not reference CherryPy in any way,
    so that it can be used in other frameworks and applications. Therefore,
    we wrap it here, so we can set our own mount points from cherrypy.tree
    and apply some attributes from config -> cherrypy.server -> wsgiserver.
    """
    
    def __init__(self, server_adapter=cherrypy.server):
        self.server_adapter = server_adapter
        self.max_request_header_size = self.server_adapter.max_request_header_size or 0
        self.max_request_body_size = self.server_adapter.max_request_body_size or 0
        
        server_name = (self.server_adapter.socket_host or
                       self.server_adapter.socket_file or
                       None)
        
        self.wsgi_version = self.server_adapter.wsgi_version
        s = wsgiserver.CherryPyWSGIServer
        s.__init__(self, server_adapter.bind_addr, cherrypy.tree,
                   self.server_adapter.thread_pool,
                   server_name,
                   max=self.server_adapter.thread_pool_max,
                   request_queue_size=self.server_adapter.socket_queue_size,
                   timeout=self.server_adapter.socket_timeout,
                   shutdown_timeout=self.server_adapter.shutdown_timeout,
                   )
        self.protocol = self.server_adapter.protocol_version
        self.nodelay = self.server_adapter.nodelay
        
        ssl_module = self.server_adapter.ssl_module or 'pyopenssl'
        if self.server_adapter.ssl_context:
            adapter_class = wsgiserver.get_ssl_adapter_class(ssl_module)
            self.ssl_adapter = adapter_class(
                self.server_adapter.ssl_certificate,
                self.server_adapter.ssl_private_key,
                self.server_adapter.ssl_certificate_chain)
            self.ssl_adapter.context = self.server_adapter.ssl_context
        elif self.server_adapter.ssl_certificate:
            adapter_class = wsgiserver.get_ssl_adapter_class(ssl_module)
            self.ssl_adapter = adapter_class(
                self.server_adapter.ssl_certificate,
                self.server_adapter.ssl_private_key,
                self.server_adapter.ssl_certificate_chain)

########NEW FILE########
__FILENAME__ = certgen
# -*- coding: latin-1 -*-
#
# Copyright (C) Martin Sjgren and AB Strakt 2001, All rights reserved
# Copyright (C) Jean-Paul Calderone 2008, All rights reserved
# This file is licenced under the GNU LESSER GENERAL PUBLIC LICENSE Version 2.1 or later (aka LGPL v2.1)
# Please see LGPL2.1.txt for more information
"""
Certificate generation module.
"""

from OpenSSL import crypto
import time

TYPE_RSA = crypto.TYPE_RSA
TYPE_DSA = crypto.TYPE_DSA

serial = int(time.time())


def createKeyPair(type, bits):
    """
    Create a public/private key pair.

    Arguments: type - Key type, must be one of TYPE_RSA and TYPE_DSA
               bits - Number of bits to use in the key
    Returns:   The public/private key pair in a PKey object
    """
    pkey = crypto.PKey()
    pkey.generate_key(type, bits)
    return pkey

def createCertRequest(pkey, digest="md5", **name):
    """
    Create a certificate request.

    Arguments: pkey   - The key to associate with the request
               digest - Digestion method to use for signing, default is md5
               **name - The name of the subject of the request, possible
                        arguments are:
                          C     - Country name
                          ST    - State or province name
                          L     - Locality name
                          O     - Organization name
                          OU    - Organizational unit name
                          CN    - Common name
                          emailAddress - E-mail address
    Returns:   The certificate request in an X509Req object
    """
    req = crypto.X509Req()
    subj = req.get_subject()

    for (key,value) in name.items():
        setattr(subj, key, value)

    req.set_pubkey(pkey)
    req.sign(pkey, digest)
    return req

def createCertificate(req, (issuerCert, issuerKey), serial, (notBefore, notAfter), digest="md5"):
    """
    Generate a certificate given a certificate request.

    Arguments: req        - Certificate reqeust to use
               issuerCert - The certificate of the issuer
               issuerKey  - The private key of the issuer
               serial     - Serial number for the certificate
               notBefore  - Timestamp (relative to now) when the certificate
                            starts being valid
               notAfter   - Timestamp (relative to now) when the certificate
                            stops being valid
               digest     - Digest method to use for signing, default is md5
    Returns:   The signed certificate in an X509 object
    """
    cert = crypto.X509()
    cert.set_serial_number(serial)
    cert.gmtime_adj_notBefore(notBefore)
    cert.gmtime_adj_notAfter(notAfter)
    cert.set_issuer(issuerCert.get_subject())
    cert.set_subject(req.get_subject())
    cert.set_pubkey(req.get_pubkey())
    cert.sign(issuerKey, digest)
    return cert

########NEW FILE########
__FILENAME__ = configobj
# configobj.py
# A config file reader/writer that supports nested sections in config files.
# Copyright (C) 2005-2009 Michael Foord, Nicola Larosa
# E-mail: fuzzyman AT voidspace DOT org DOT uk
#         nico AT tekNico DOT net

# ConfigObj 4
# http://www.voidspace.org.uk/python/configobj.html

# Released subject to the BSD License
# Please see http://www.voidspace.org.uk/python/license.shtml

# Scripts maintained at http://www.voidspace.org.uk/python/index.shtml
# For information about bugfixes, updates and support, please join the
# ConfigObj mailing list:
# http://lists.sourceforge.net/lists/listinfo/configobj-develop
# Comments, suggestions and bug reports welcome.


from __future__ import generators

import sys
import os
import re

compiler = None
try:
    import compiler
except ImportError:
    # for IronPython
    pass


try:
    from codecs import BOM_UTF8, BOM_UTF16, BOM_UTF16_BE, BOM_UTF16_LE
except ImportError:
    # Python 2.2 does not have these
    # UTF-8
    BOM_UTF8 = '\xef\xbb\xbf'
    # UTF-16, little endian
    BOM_UTF16_LE = '\xff\xfe'
    # UTF-16, big endian
    BOM_UTF16_BE = '\xfe\xff'
    if sys.byteorder == 'little':
        # UTF-16, native endianness
        BOM_UTF16 = BOM_UTF16_LE
    else:
        # UTF-16, native endianness
        BOM_UTF16 = BOM_UTF16_BE

# A dictionary mapping BOM to
# the encoding to decode with, and what to set the
# encoding attribute to.
BOMS = {
    BOM_UTF8: ('utf_8', None),
    BOM_UTF16_BE: ('utf16_be', 'utf_16'),
    BOM_UTF16_LE: ('utf16_le', 'utf_16'),
    BOM_UTF16: ('utf_16', 'utf_16'),
    }
# All legal variants of the BOM codecs.
# TODO: the list of aliases is not meant to be exhaustive, is there a
#   better way ?
BOM_LIST = {
    'utf_16': 'utf_16',
    'u16': 'utf_16',
    'utf16': 'utf_16',
    'utf-16': 'utf_16',
    'utf16_be': 'utf16_be',
    'utf_16_be': 'utf16_be',
    'utf-16be': 'utf16_be',
    'utf16_le': 'utf16_le',
    'utf_16_le': 'utf16_le',
    'utf-16le': 'utf16_le',
    'utf_8': 'utf_8',
    'u8': 'utf_8',
    'utf': 'utf_8',
    'utf8': 'utf_8',
    'utf-8': 'utf_8',
    }

# Map of encodings to the BOM to write.
BOM_SET = {
    'utf_8': BOM_UTF8,
    'utf_16': BOM_UTF16,
    'utf16_be': BOM_UTF16_BE,
    'utf16_le': BOM_UTF16_LE,
    None: BOM_UTF8
    }


def match_utf8(encoding):
    return BOM_LIST.get(encoding.lower()) == 'utf_8'


# Quote strings used for writing values
squot = "'%s'"
dquot = '"%s"'
noquot = "%s"
wspace_plus = ' \r\n\v\t\'"'
tsquot = '"""%s"""'
tdquot = "'''%s'''"

try:
    enumerate
except NameError:
    def enumerate(obj):
        """enumerate for Python 2.2."""
        i = -1
        for item in obj:
            i += 1
            yield i, item

# Sentinel for use in getattr calls to replace hasattr
MISSING = object()

__version__ = '4.6.0'

__revision__ = '$Id: configobj.py 156 2006-01-31 14:57:08Z fuzzyman $'

__docformat__ = "restructuredtext en" 

__all__ = (
    '__version__',
    'DEFAULT_INDENT_TYPE',
    'DEFAULT_INTERPOLATION',
    'ConfigObjError',
    'NestingError',
    'ParseError',
    'DuplicateError',
    'ConfigspecError',
    'ConfigObj',
    'SimpleVal',
    'InterpolationError',
    'InterpolationLoopError',
    'MissingInterpolationOption',
    'RepeatSectionError',
    'ReloadError',
    'UnreprError',
    'UnknownType',
    '__docformat__',
    'flatten_errors',
)

DEFAULT_INTERPOLATION = 'configparser'
DEFAULT_INDENT_TYPE = '    '
MAX_INTERPOL_DEPTH = 10

OPTION_DEFAULTS = {
    'interpolation': True,
    'raise_errors': False,
    'list_values': True,
    'create_empty': False,
    'file_error': False,
    'configspec': None,
    'stringify': True,
    # option may be set to one of ('', ' ', '\t')
    'indent_type': None,
    'encoding': None,
    'default_encoding': None,
    'unrepr': False,
    'write_empty_values': False,
}



def getObj(s):
    s = "a=" + s
    if compiler is None:
        raise ImportError('compiler module not available')
    p = compiler.parse(s)
    return p.getChildren()[1].getChildren()[0].getChildren()[1]


class UnknownType(Exception):
    pass


class Builder(object):
    
    def build(self, o):
        m = getattr(self, 'build_' + o.__class__.__name__, None)
        if m is None:
            raise UnknownType(o.__class__.__name__)
        return m(o)
    
    def build_List(self, o):
        return map(self.build, o.getChildren())
    
    def build_Const(self, o):
        return o.value
    
    def build_Dict(self, o):
        d = {}
        i = iter(map(self.build, o.getChildren()))
        for el in i:
            d[el] = i.next()
        return d
    
    def build_Tuple(self, o):
        return tuple(self.build_List(o))
    
    def build_Name(self, o):
        if o.name == 'None':
            return None
        if o.name == 'True':
            return True
        if o.name == 'False':
            return False
        
        # An undefined Name
        raise UnknownType('Undefined Name')
    
    def build_Add(self, o):
        real, imag = map(self.build_Const, o.getChildren())
        try:
            real = float(real)
        except TypeError:
            raise UnknownType('Add')
        if not isinstance(imag, complex) or imag.real != 0.0:
            raise UnknownType('Add')
        return real + imag
    
    def build_Getattr(self, o):
        parent = self.build(o.expr)
        return getattr(parent, o.attrname)
    
    def build_UnarySub(self, o):
        return - self.build_Const(o.getChildren()[0])
    
    def build_UnaryAdd(self, o):
        return self.build_Const(o.getChildren()[0])


_builder = Builder()


def unrepr(s):
    if not s:
        return s
    return _builder.build(getObj(s))



class ConfigObjError(SyntaxError):
    """
    This is the base class for all errors that ConfigObj raises.
    It is a subclass of SyntaxError.
    """
    def __init__(self, message='', line_number=None, line=''):
        self.line = line
        self.line_number = line_number
        SyntaxError.__init__(self, message)


class NestingError(ConfigObjError):
    """
    This error indicates a level of nesting that doesn't match.
    """


class ParseError(ConfigObjError):
    """
    This error indicates that a line is badly written.
    It is neither a valid ``key = value`` line,
    nor a valid section marker line.
    """


class ReloadError(IOError):
    """
    A 'reload' operation failed.
    This exception is a subclass of ``IOError``.
    """
    def __init__(self):
        IOError.__init__(self, 'reload failed, filename is not set.')


class DuplicateError(ConfigObjError):
    """
    The keyword or section specified already exists.
    """


class ConfigspecError(ConfigObjError):
    """
    An error occured whilst parsing a configspec.
    """


class InterpolationError(ConfigObjError):
    """Base class for the two interpolation errors."""


class InterpolationLoopError(InterpolationError):
    """Maximum interpolation depth exceeded in string interpolation."""

    def __init__(self, option):
        InterpolationError.__init__(
            self,
            'interpolation loop detected in value "%s".' % option)


class RepeatSectionError(ConfigObjError):
    """
    This error indicates additional sections in a section with a
    ``__many__`` (repeated) section.
    """


class MissingInterpolationOption(InterpolationError):
    """A value specified for interpolation was missing."""

    def __init__(self, option):
        InterpolationError.__init__(
            self,
            'missing option "%s" in interpolation.' % option)


class UnreprError(ConfigObjError):
    """An error parsing in unrepr mode."""



class InterpolationEngine(object):
    """
    A helper class to help perform string interpolation.

    This class is an abstract base class; its descendants perform
    the actual work.
    """

    # compiled regexp to use in self.interpolate()
    _KEYCRE = re.compile(r"%\(([^)]*)\)s")

    def __init__(self, section):
        # the Section instance that "owns" this engine
        self.section = section


    def interpolate(self, key, value):
        def recursive_interpolate(key, value, section, backtrail):
            """The function that does the actual work.

            ``value``: the string we're trying to interpolate.
            ``section``: the section in which that string was found
            ``backtrail``: a dict to keep track of where we've been,
            to detect and prevent infinite recursion loops

            This is similar to a depth-first-search algorithm.
            """
            # Have we been here already?
            if backtrail.has_key((key, section.name)):
                # Yes - infinite loop detected
                raise InterpolationLoopError(key)
            # Place a marker on our backtrail so we won't come back here again
            backtrail[(key, section.name)] = 1

            # Now start the actual work
            match = self._KEYCRE.search(value)
            while match:
                # The actual parsing of the match is implementation-dependent,
                # so delegate to our helper function
                k, v, s = self._parse_match(match)
                if k is None:
                    # That's the signal that no further interpolation is needed
                    replacement = v
                else:
                    # Further interpolation may be needed to obtain final value
                    replacement = recursive_interpolate(k, v, s, backtrail)
                # Replace the matched string with its final value
                start, end = match.span()
                value = ''.join((value[:start], replacement, value[end:]))
                new_search_start = start + len(replacement)
                # Pick up the next interpolation key, if any, for next time
                # through the while loop
                match = self._KEYCRE.search(value, new_search_start)

            # Now safe to come back here again; remove marker from backtrail
            del backtrail[(key, section.name)]

            return value

        # Back in interpolate(), all we have to do is kick off the recursive
        # function with appropriate starting values
        value = recursive_interpolate(key, value, self.section, {})
        return value


    def _fetch(self, key):
        """Helper function to fetch values from owning section.

        Returns a 2-tuple: the value, and the section where it was found.
        """
        # switch off interpolation before we try and fetch anything !
        save_interp = self.section.main.interpolation
        self.section.main.interpolation = False

        # Start at section that "owns" this InterpolationEngine
        current_section = self.section
        while True:
            # try the current section first
            val = current_section.get(key)
            if val is not None:
                break
            # try "DEFAULT" next
            val = current_section.get('DEFAULT', {}).get(key)
            if val is not None:
                break
            # move up to parent and try again
            # top-level's parent is itself
            if current_section.parent is current_section:
                # reached top level, time to give up
                break
            current_section = current_section.parent

        # restore interpolation to previous value before returning
        self.section.main.interpolation = save_interp
        if val is None:
            raise MissingInterpolationOption(key)
        return val, current_section


    def _parse_match(self, match):
        """Implementation-dependent helper function.

        Will be passed a match object corresponding to the interpolation
        key we just found (e.g., "%(foo)s" or "$foo"). Should look up that
        key in the appropriate config file section (using the ``_fetch()``
        helper function) and return a 3-tuple: (key, value, section)

        ``key`` is the name of the key we're looking for
        ``value`` is the value found for that key
        ``section`` is a reference to the section where it was found

        ``key`` and ``section`` should be None if no further
        interpolation should be performed on the resulting value
        (e.g., if we interpolated "$$" and returned "$").
        """
        raise NotImplementedError()
    


class ConfigParserInterpolation(InterpolationEngine):
    """Behaves like ConfigParser."""
    _KEYCRE = re.compile(r"%\(([^)]*)\)s")

    def _parse_match(self, match):
        key = match.group(1)
        value, section = self._fetch(key)
        return key, value, section



class TemplateInterpolation(InterpolationEngine):
    """Behaves like string.Template."""
    _delimiter = '$'
    _KEYCRE = re.compile(r"""
        \$(?:
          (?P<escaped>\$)              |   # Two $ signs
          (?P<named>[_a-z][_a-z0-9]*)  |   # $name format
          {(?P<braced>[^}]*)}              # ${name} format
        )
        """, re.IGNORECASE | re.VERBOSE)

    def _parse_match(self, match):
        # Valid name (in or out of braces): fetch value from section
        key = match.group('named') or match.group('braced')
        if key is not None:
            value, section = self._fetch(key)
            return key, value, section
        # Escaped delimiter (e.g., $$): return single delimiter
        if match.group('escaped') is not None:
            # Return None for key and section to indicate it's time to stop
            return None, self._delimiter, None
        # Anything else: ignore completely, just return it unchanged
        return None, match.group(), None


interpolation_engines = {
    'configparser': ConfigParserInterpolation,
    'template': TemplateInterpolation,
}


def __newobj__(cls, *args):
    # Hack for pickle
    return cls.__new__(cls, *args) 

class Section(dict):
    """
    A dictionary-like object that represents a section in a config file.
    
    It does string interpolation if the 'interpolation' attribute
    of the 'main' object is set to True.
    
    Interpolation is tried first from this object, then from the 'DEFAULT'
    section of this object, next from the parent and its 'DEFAULT' section,
    and so on until the main object is reached.
    
    A Section will behave like an ordered dictionary - following the
    order of the ``scalars`` and ``sections`` attributes.
    You can use this to change the order of members.
    
    Iteration follows the order: scalars, then sections.
    """

    
    def __setstate__(self, state):
        dict.update(self, state[0])
        self.__dict__.update(state[1])

    def __reduce__(self):
        state = (dict(self), self.__dict__)
        return (__newobj__, (self.__class__,), state)
    
    
    def __init__(self, parent, depth, main, indict=None, name=None):
        """
        * parent is the section above
        * depth is the depth level of this section
        * main is the main ConfigObj
        * indict is a dictionary to initialise the section with
        """
        if indict is None:
            indict = {}
        dict.__init__(self)
        # used for nesting level *and* interpolation
        self.parent = parent
        # used for the interpolation attribute
        self.main = main
        # level of nesting depth of this Section
        self.depth = depth
        # purely for information
        self.name = name
        #
        self._initialise()
        # we do this explicitly so that __setitem__ is used properly
        # (rather than just passing to ``dict.__init__``)
        for entry, value in indict.iteritems():
            self[entry] = value
            
            
    def _initialise(self):
        # the sequence of scalar values in this Section
        self.scalars = []
        # the sequence of sections in this Section
        self.sections = []
        # for comments :-)
        self.comments = {}
        self.inline_comments = {}
        # the configspec
        self.configspec = None
        # for defaults
        self.defaults = []
        self.default_values = {}


    def _interpolate(self, key, value):
        try:
            # do we already have an interpolation engine?
            engine = self._interpolation_engine
        except AttributeError:
            # not yet: first time running _interpolate(), so pick the engine
            name = self.main.interpolation
            if name == True:  # note that "if name:" would be incorrect here
                # backwards-compatibility: interpolation=True means use default
                name = DEFAULT_INTERPOLATION
            name = name.lower()  # so that "Template", "template", etc. all work
            class_ = interpolation_engines.get(name, None)
            if class_ is None:
                # invalid value for self.main.interpolation
                self.main.interpolation = False
                return value
            else:
                # save reference to engine so we don't have to do this again
                engine = self._interpolation_engine = class_(self)
        # let the engine do the actual work
        return engine.interpolate(key, value)


    def __getitem__(self, key):
        """Fetch the item and do string interpolation."""
        val = dict.__getitem__(self, key)
        if self.main.interpolation and isinstance(val, basestring):
            return self._interpolate(key, val)
        return val


    def __setitem__(self, key, value, unrepr=False):
        """
        Correctly set a value.
        
        Making dictionary values Section instances.
        (We have to special case 'Section' instances - which are also dicts)
        
        Keys must be strings.
        Values need only be strings (or lists of strings) if
        ``main.stringify`` is set.
        
        ``unrepr`` must be set when setting a value to a dictionary, without
        creating a new sub-section.
        """
        if not isinstance(key, basestring):
            raise ValueError('The key "%s" is not a string.' % key)
        
        # add the comment
        if not self.comments.has_key(key):
            self.comments[key] = []
            self.inline_comments[key] = ''
        # remove the entry from defaults
        if key in self.defaults:
            self.defaults.remove(key)
        #
        if isinstance(value, Section):
            if not self.has_key(key):
                self.sections.append(key)
            dict.__setitem__(self, key, value)
        elif isinstance(value, dict) and not unrepr:
            # First create the new depth level,
            # then create the section
            if not self.has_key(key):
                self.sections.append(key)
            new_depth = self.depth + 1
            dict.__setitem__(
                self,
                key,
                Section(
                    self,
                    new_depth,
                    self.main,
                    indict=value,
                    name=key))
        else:
            if not self.has_key(key):
                self.scalars.append(key)
            if not self.main.stringify:
                if isinstance(value, basestring):
                    pass
                elif isinstance(value, (list, tuple)):
                    for entry in value:
                        if not isinstance(entry, basestring):
                            raise TypeError('Value is not a string "%s".' % entry)
                else:
                    raise TypeError('Value is not a string "%s".' % value)
            dict.__setitem__(self, key, value)


    def __delitem__(self, key):
        """Remove items from the sequence when deleting."""
        dict. __delitem__(self, key)
        if key in self.scalars:
            self.scalars.remove(key)
        else:
            self.sections.remove(key)
        del self.comments[key]
        del self.inline_comments[key]


    def get(self, key, default=None):
        """A version of ``get`` that doesn't bypass string interpolation."""
        try:
            return self[key]
        except KeyError:
            return default


    def update(self, indict):
        """
        A version of update that uses our ``__setitem__``.
        """
        for entry in indict:
            self[entry] = indict[entry]


    def pop(self, key, *args):
        """
        'D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
        If key is not found, d is returned if given, otherwise KeyError is raised'
        """
        val = dict.pop(self, key, *args)
        if key in self.scalars:
            del self.comments[key]
            del self.inline_comments[key]
            self.scalars.remove(key)
        elif key in self.sections:
            del self.comments[key]
            del self.inline_comments[key]
            self.sections.remove(key)
        if self.main.interpolation and isinstance(val, basestring):
            return self._interpolate(key, val)
        return val


    def popitem(self):
        """Pops the first (key,val)"""
        sequence = (self.scalars + self.sections)
        if not sequence:
            raise KeyError(": 'popitem(): dictionary is empty'")
        key = sequence[0]
        val = self[key]
        del self[key]
        return key, val


    def clear(self):
        """
        A version of clear that also affects scalars/sections
        Also clears comments and configspec.
        
        Leaves other attributes alone :
            depth/main/parent are not affected
        """
        dict.clear(self)
        self.scalars = []
        self.sections = []
        self.comments = {}
        self.inline_comments = {}
        self.configspec = None


    def setdefault(self, key, default=None):
        """A version of setdefault that sets sequence if appropriate."""
        try:
            return self[key]
        except KeyError:
            self[key] = default
            return self[key]


    def items(self):
        """D.items() -> list of D's (key, value) pairs, as 2-tuples"""
        return zip((self.scalars + self.sections), self.values())


    def keys(self):
        """D.keys() -> list of D's keys"""
        return (self.scalars + self.sections)


    def values(self):
        """D.values() -> list of D's values"""
        return [self[key] for key in (self.scalars + self.sections)]


    def iteritems(self):
        """D.iteritems() -> an iterator over the (key, value) items of D"""
        return iter(self.items())


    def iterkeys(self):
        """D.iterkeys() -> an iterator over the keys of D"""
        return iter((self.scalars + self.sections))

    __iter__ = iterkeys


    def itervalues(self):
        """D.itervalues() -> an iterator over the values of D"""
        return iter(self.values())


    def __repr__(self):
        """x.__repr__() <==> repr(x)"""
        return '{%s}' % ', '.join([('%s: %s' % (repr(key), repr(self[key])))
            for key in (self.scalars + self.sections)])

    __str__ = __repr__
    __str__.__doc__ = "x.__str__() <==> str(x)"


    # Extra methods - not in a normal dictionary

    def dict(self):
        """
        Return a deepcopy of self as a dictionary.
        
        All members that are ``Section`` instances are recursively turned to
        ordinary dictionaries - by calling their ``dict`` method.
        
        >>> n = a.dict()
        >>> n == a
        1
        >>> n is a
        0
        """
        newdict = {}
        for entry in self:
            this_entry = self[entry]
            if isinstance(this_entry, Section):
                this_entry = this_entry.dict()
            elif isinstance(this_entry, list):
                # create a copy rather than a reference
                this_entry = list(this_entry)
            elif isinstance(this_entry, tuple):
                # create a copy rather than a reference
                this_entry = tuple(this_entry)
            newdict[entry] = this_entry
        return newdict


    def merge(self, indict):
        """
        A recursive update - useful for merging config files.
        
        >>> a = '''[section1]
        ...     option1 = True
        ...     [[subsection]]
        ...     more_options = False
        ...     # end of file'''.splitlines()
        >>> b = '''# File is user.ini
        ...     [section1]
        ...     option1 = False
        ...     # end of file'''.splitlines()
        >>> c1 = ConfigObj(b)
        >>> c2 = ConfigObj(a)
        >>> c2.merge(c1)
        >>> c2
        ConfigObj({'section1': {'option1': 'False', 'subsection': {'more_options': 'False'}}})
        """
        for key, val in indict.items():
            if (key in self and isinstance(self[key], dict) and
                                isinstance(val, dict)):
                self[key].merge(val)
            else:   
                self[key] = val


    def rename(self, oldkey, newkey):
        """
        Change a keyname to another, without changing position in sequence.
        
        Implemented so that transformations can be made on keys,
        as well as on values. (used by encode and decode)
        
        Also renames comments.
        """
        if oldkey in self.scalars:
            the_list = self.scalars
        elif oldkey in self.sections:
            the_list = self.sections
        else:
            raise KeyError('Key "%s" not found.' % oldkey)
        pos = the_list.index(oldkey)
        #
        val = self[oldkey]
        dict.__delitem__(self, oldkey)
        dict.__setitem__(self, newkey, val)
        the_list.remove(oldkey)
        the_list.insert(pos, newkey)
        comm = self.comments[oldkey]
        inline_comment = self.inline_comments[oldkey]
        del self.comments[oldkey]
        del self.inline_comments[oldkey]
        self.comments[newkey] = comm
        self.inline_comments[newkey] = inline_comment


    def walk(self, function, raise_errors=True,
            call_on_sections=False, **keywargs):
        """
        Walk every member and call a function on the keyword and value.
        
        Return a dictionary of the return values
        
        If the function raises an exception, raise the errror
        unless ``raise_errors=False``, in which case set the return value to
        ``False``.
        
        Any unrecognised keyword arguments you pass to walk, will be pased on
        to the function you pass in.
        
        Note: if ``call_on_sections`` is ``True`` then - on encountering a
        subsection, *first* the function is called for the *whole* subsection,
        and then recurses into it's members. This means your function must be
        able to handle strings, dictionaries and lists. This allows you
        to change the key of subsections as well as for ordinary members. The
        return value when called on the whole subsection has to be discarded.
        
        See  the encode and decode methods for examples, including functions.
        
        .. admonition:: caution
        
            You can use ``walk`` to transform the names of members of a section
            but you mustn't add or delete members.
        
        >>> config = '''[XXXXsection]
        ... XXXXkey = XXXXvalue'''.splitlines()
        >>> cfg = ConfigObj(config)
        >>> cfg
        ConfigObj({'XXXXsection': {'XXXXkey': 'XXXXvalue'}})
        >>> def transform(section, key):
        ...     val = section[key]
        ...     newkey = key.replace('XXXX', 'CLIENT1')
        ...     section.rename(key, newkey)
        ...     if isinstance(val, (tuple, list, dict)):
        ...         pass
        ...     else:
        ...         val = val.replace('XXXX', 'CLIENT1')
        ...         section[newkey] = val
        >>> cfg.walk(transform, call_on_sections=True)
        {'CLIENT1section': {'CLIENT1key': None}}
        >>> cfg
        ConfigObj({'CLIENT1section': {'CLIENT1key': 'CLIENT1value'}})
        """
        out = {}
        # scalars first
        for i in range(len(self.scalars)):
            entry = self.scalars[i]
            try:
                val = function(self, entry, **keywargs)
                # bound again in case name has changed
                entry = self.scalars[i]
                out[entry] = val
            except Exception:
                if raise_errors:
                    raise
                else:
                    entry = self.scalars[i]
                    out[entry] = False
        # then sections
        for i in range(len(self.sections)):
            entry = self.sections[i]
            if call_on_sections:
                try:
                    function(self, entry, **keywargs)
                except Exception:
                    if raise_errors:
                        raise
                    else:
                        entry = self.sections[i]
                        out[entry] = False
                # bound again in case name has changed
                entry = self.sections[i]
            # previous result is discarded
            out[entry] = self[entry].walk(
                function,
                raise_errors=raise_errors,
                call_on_sections=call_on_sections,
                **keywargs)
        return out


    def as_bool(self, key):
        """
        Accepts a key as input. The corresponding value must be a string or
        the objects (``True`` or 1) or (``False`` or 0). We allow 0 and 1 to
        retain compatibility with Python 2.2.
        
        If the string is one of  ``True``, ``On``, ``Yes``, or ``1`` it returns 
        ``True``.
        
        If the string is one of  ``False``, ``Off``, ``No``, or ``0`` it returns 
        ``False``.
        
        ``as_bool`` is not case sensitive.
        
        Any other input will raise a ``ValueError``.
        
        >>> a = ConfigObj()
        >>> a['a'] = 'fish'
        >>> a.as_bool('a')
        Traceback (most recent call last):
        ValueError: Value "fish" is neither True nor False
        >>> a['b'] = 'True'
        >>> a.as_bool('b')
        1
        >>> a['b'] = 'off'
        >>> a.as_bool('b')
        0
        """
        val = self[key]
        if val == True:
            return True
        elif val == False:
            return False
        else:
            try:
                if not isinstance(val, basestring):
                    # TODO: Why do we raise a KeyError here?
                    raise KeyError()
                else:
                    return self.main._bools[val.lower()]
            except KeyError:
                raise ValueError('Value "%s" is neither True nor False' % val)


    def as_int(self, key):
        """
        A convenience method which coerces the specified value to an integer.
        
        If the value is an invalid literal for ``int``, a ``ValueError`` will
        be raised.
        
        >>> a = ConfigObj()
        >>> a['a'] = 'fish'
        >>> a.as_int('a')
        Traceback (most recent call last):
        ValueError: invalid literal for int() with base 10: 'fish'
        >>> a['b'] = '1'
        >>> a.as_int('b')
        1
        >>> a['b'] = '3.2'
        >>> a.as_int('b')
        Traceback (most recent call last):
        ValueError: invalid literal for int() with base 10: '3.2'
        """
        return int(self[key])


    def as_float(self, key):
        """
        A convenience method which coerces the specified value to a float.
        
        If the value is an invalid literal for ``float``, a ``ValueError`` will
        be raised.
        
        >>> a = ConfigObj()
        >>> a['a'] = 'fish'
        >>> a.as_float('a')
        Traceback (most recent call last):
        ValueError: invalid literal for float(): fish
        >>> a['b'] = '1'
        >>> a.as_float('b')
        1.0
        >>> a['b'] = '3.2'
        >>> a.as_float('b')
        3.2000000000000002
        """
        return float(self[key])
    
    
    def as_list(self, key):
        """
        A convenience method which fetches the specified value, guaranteeing
        that it is a list.
        
        >>> a = ConfigObj()
        >>> a['a'] = 1
        >>> a.as_list('a')
        [1]
        >>> a['a'] = (1,)
        >>> a.as_list('a')
        [1]
        >>> a['a'] = [1]
        >>> a.as_list('a')
        [1]
        """
        result = self[key]
        if isinstance(result, (tuple, list)):
            return list(result)
        return [result]
        

    def restore_default(self, key):
        """
        Restore (and return) default value for the specified key.
        
        This method will only work for a ConfigObj that was created
        with a configspec and has been validated.
        
        If there is no default value for this key, ``KeyError`` is raised.
        """
        default = self.default_values[key]
        dict.__setitem__(self, key, default)
        if key not in self.defaults:
            self.defaults.append(key)
        return default

    
    def restore_defaults(self):
        """
        Recursively restore default values to all members
        that have them.
        
        This method will only work for a ConfigObj that was created
        with a configspec and has been validated.
        
        It doesn't delete or modify entries without default values.
        """
        for key in self.default_values:
            self.restore_default(key)
            
        for section in self.sections:
            self[section].restore_defaults()


class ConfigObj(Section):
    """An object to read, create, and write config files."""

    _keyword = re.compile(r'''^ # line start
        (\s*)                   # indentation
        (                       # keyword
            (?:".*?")|          # double quotes
            (?:'.*?')|          # single quotes
            (?:[^'"=].*?)       # no quotes
        )
        \s*=\s*                 # divider
        (.*)                    # value (including list values and comments)
        $   # line end
        ''',
        re.VERBOSE)

    _sectionmarker = re.compile(r'''^
        (\s*)                     # 1: indentation
        ((?:\[\s*)+)              # 2: section marker open
        (                         # 3: section name open
            (?:"\s*\S.*?\s*")|    # at least one non-space with double quotes
            (?:'\s*\S.*?\s*')|    # at least one non-space with single quotes
            (?:[^'"\s].*?)        # at least one non-space unquoted
        )                         # section name close
        ((?:\s*\])+)              # 4: section marker close
        \s*(\#.*)?                # 5: optional comment
        $''',
        re.VERBOSE)

    # this regexp pulls list values out as a single string
    # or single values and comments
    # FIXME: this regex adds a '' to the end of comma terminated lists
    #   workaround in ``_handle_value``
    _valueexp = re.compile(r'''^
        (?:
            (?:
                (
                    (?:
                        (?:
                            (?:".*?")|              # double quotes
                            (?:'.*?')|              # single quotes
                            (?:[^'",\#][^,\#]*?)    # unquoted
                        )
                        \s*,\s*                     # comma
                    )*      # match all list items ending in a comma (if any)
                )
                (
                    (?:".*?")|                      # double quotes
                    (?:'.*?')|                      # single quotes
                    (?:[^'",\#\s][^,]*?)|           # unquoted
                    (?:(?<!,))                      # Empty value
                )?          # last item in a list - or string value
            )|
            (,)             # alternatively a single comma - empty list
        )
        \s*(\#.*)?          # optional comment
        $''',
        re.VERBOSE)

    # use findall to get the members of a list value
    _listvalueexp = re.compile(r'''
        (
            (?:".*?")|          # double quotes
            (?:'.*?')|          # single quotes
            (?:[^'",\#].*?)       # unquoted
        )
        \s*,\s*                 # comma
        ''',
        re.VERBOSE)

    # this regexp is used for the value
    # when lists are switched off
    _nolistvalue = re.compile(r'''^
        (
            (?:".*?")|          # double quotes
            (?:'.*?')|          # single quotes
            (?:[^'"\#].*?)|     # unquoted
            (?:)                # Empty value
        )
        \s*(\#.*)?              # optional comment
        $''',
        re.VERBOSE)

    # regexes for finding triple quoted values on one line
    _single_line_single = re.compile(r"^'''(.*?)'''\s*(#.*)?$")
    _single_line_double = re.compile(r'^"""(.*?)"""\s*(#.*)?$')
    _multi_line_single = re.compile(r"^(.*?)'''\s*(#.*)?$")
    _multi_line_double = re.compile(r'^(.*?)"""\s*(#.*)?$')

    _triple_quote = {
        "'''": (_single_line_single, _multi_line_single),
        '"""': (_single_line_double, _multi_line_double),
    }

    # Used by the ``istrue`` Section method
    _bools = {
        'yes': True, 'no': False,
        'on': True, 'off': False,
        '1': True, '0': False,
        'true': True, 'false': False,
        }


    def __init__(self, infile=None, options=None, _inspec=False, **kwargs):
        """
        Parse a config file or create a config file object.
        
        ``ConfigObj(infile=None, options=None, **kwargs)``
        """
        self._inspec = _inspec
        # init the superclass
        Section.__init__(self, self, 0, self)
        
        infile = infile or []
        options = dict(options or {})
            
        # keyword arguments take precedence over an options dictionary
        options.update(kwargs)
        if _inspec:
            options['list_values'] = False
        
        defaults = OPTION_DEFAULTS.copy()
        # TODO: check the values too.
        for entry in options:
            if entry not in defaults:
                raise TypeError('Unrecognised option "%s".' % entry)
        
        # Add any explicit options to the defaults
        defaults.update(options)
        self._initialise(defaults)
        configspec = defaults['configspec']
        self._original_configspec = configspec
        self._load(infile, configspec)
        
        
    def _load(self, infile, configspec):
        if isinstance(infile, basestring):
            self.filename = infile
            if os.path.isfile(infile):
                h = open(infile, 'rb')
                infile = h.read() or []
                h.close()
            elif self.file_error:
                # raise an error if the file doesn't exist
                raise IOError('Config file not found: "%s".' % self.filename)
            else:
                # file doesn't already exist
                if self.create_empty:
                    # this is a good test that the filename specified
                    # isn't impossible - like on a non-existent device
                    h = open(infile, 'w')
                    h.write('')
                    h.close()
                infile = []
                
        elif isinstance(infile, (list, tuple)):
            infile = list(infile)
            
        elif isinstance(infile, dict):
            # initialise self
            # the Section class handles creating subsections
            if isinstance(infile, ConfigObj):
                # get a copy of our ConfigObj
                infile = infile.dict()
                
            for entry in infile:
                self[entry] = infile[entry]
            del self._errors
            
            if configspec is not None:
                self._handle_configspec(configspec)
            else:
                self.configspec = None
            return
        
        elif getattr(infile, 'read', MISSING) is not MISSING:
            # This supports file like objects
            infile = infile.read() or []
            # needs splitting into lines - but needs doing *after* decoding
            # in case it's not an 8 bit encoding
        else:
            raise TypeError('infile must be a filename, file like object, or list of lines.')
        
        if infile:
            # don't do it for the empty ConfigObj
            infile = self._handle_bom(infile)
            # infile is now *always* a list
            #
            # Set the newlines attribute (first line ending it finds)
            # and strip trailing '\n' or '\r' from lines
            for line in infile:
                if (not line) or (line[-1] not in ('\r', '\n', '\r\n')):
                    continue
                for end in ('\r\n', '\n', '\r'):
                    if line.endswith(end):
                        self.newlines = end
                        break
                break

            infile = [line.rstrip('\r\n') for line in infile]
            
        self._parse(infile)
        # if we had any errors, now is the time to raise them
        if self._errors:
            info = "at line %s." % self._errors[0].line_number
            if len(self._errors) > 1:
                msg = "Parsing failed with several errors.\nFirst error %s" % info
                error = ConfigObjError(msg)
            else:
                error = self._errors[0]
            # set the errors attribute; it's a list of tuples:
            # (error_type, message, line_number)
            error.errors = self._errors
            # set the config attribute
            error.config = self
            raise error
        # delete private attributes
        del self._errors
        
        if configspec is None:
            self.configspec = None
        else:
            self._handle_configspec(configspec)
    
    
    def _initialise(self, options=None):
        if options is None:
            options = OPTION_DEFAULTS
            
        # initialise a few variables
        self.filename = None
        self._errors = []
        self.raise_errors = options['raise_errors']
        self.interpolation = options['interpolation']
        self.list_values = options['list_values']
        self.create_empty = options['create_empty']
        self.file_error = options['file_error']
        self.stringify = options['stringify']
        self.indent_type = options['indent_type']
        self.encoding = options['encoding']
        self.default_encoding = options['default_encoding']
        self.BOM = False
        self.newlines = None
        self.write_empty_values = options['write_empty_values']
        self.unrepr = options['unrepr']
        
        self.initial_comment = []
        self.final_comment = []
        self.configspec = None
        
        if self._inspec:
            self.list_values = False
        
        # Clear section attributes as well
        Section._initialise(self)
        
        
    def __repr__(self):
        return ('ConfigObj({%s})' % 
                ', '.join([('%s: %s' % (repr(key), repr(self[key]))) 
                for key in (self.scalars + self.sections)]))
    
    
    def _handle_bom(self, infile):
        """
        Handle any BOM, and decode if necessary.
        
        If an encoding is specified, that *must* be used - but the BOM should
        still be removed (and the BOM attribute set).
        
        (If the encoding is wrongly specified, then a BOM for an alternative
        encoding won't be discovered or removed.)
        
        If an encoding is not specified, UTF8 or UTF16 BOM will be detected and
        removed. The BOM attribute will be set. UTF16 will be decoded to
        unicode.
        
        NOTE: This method must not be called with an empty ``infile``.
        
        Specifying the *wrong* encoding is likely to cause a
        ``UnicodeDecodeError``.
        
        ``infile`` must always be returned as a list of lines, but may be
        passed in as a single string.
        """
        if ((self.encoding is not None) and
            (self.encoding.lower() not in BOM_LIST)):
            # No need to check for a BOM
            # the encoding specified doesn't have one
            # just decode
            return self._decode(infile, self.encoding)
        
        if isinstance(infile, (list, tuple)):
            line = infile[0]
        else:
            line = infile
        if self.encoding is not None:
            # encoding explicitly supplied
            # And it could have an associated BOM
            # TODO: if encoding is just UTF16 - we ought to check for both
            # TODO: big endian and little endian versions.
            enc = BOM_LIST[self.encoding.lower()]
            if enc == 'utf_16':
                # For UTF16 we try big endian and little endian
                for BOM, (encoding, final_encoding) in BOMS.items():
                    if not final_encoding:
                        # skip UTF8
                        continue
                    if infile.startswith(BOM):
                        ### BOM discovered
                        ##self.BOM = True
                        # Don't need to remove BOM
                        return self._decode(infile, encoding)
                    
                # If we get this far, will *probably* raise a DecodeError
                # As it doesn't appear to start with a BOM
                return self._decode(infile, self.encoding)
            
            # Must be UTF8
            BOM = BOM_SET[enc]
            if not line.startswith(BOM):
                return self._decode(infile, self.encoding)
            
            newline = line[len(BOM):]
            
            # BOM removed
            if isinstance(infile, (list, tuple)):
                infile[0] = newline
            else:
                infile = newline
            self.BOM = True
            return self._decode(infile, self.encoding)
        
        # No encoding specified - so we need to check for UTF8/UTF16
        for BOM, (encoding, final_encoding) in BOMS.items():
            if not line.startswith(BOM):
                continue
            else:
                # BOM discovered
                self.encoding = final_encoding
                if not final_encoding:
                    self.BOM = True
                    # UTF8
                    # remove BOM
                    newline = line[len(BOM):]
                    if isinstance(infile, (list, tuple)):
                        infile[0] = newline
                    else:
                        infile = newline
                    # UTF8 - don't decode
                    if isinstance(infile, basestring):
                        return infile.splitlines(True)
                    else:
                        return infile
                # UTF16 - have to decode
                return self._decode(infile, encoding)
            
        # No BOM discovered and no encoding specified, just return
        if isinstance(infile, basestring):
            # infile read from a file will be a single string
            return infile.splitlines(True)
        return infile


    def _a_to_u(self, aString):
        """Decode ASCII strings to unicode if a self.encoding is specified."""
        if self.encoding:
            return aString.decode('ascii')
        else:
            return aString


    def _decode(self, infile, encoding):
        """
        Decode infile to unicode. Using the specified encoding.
        
        if is a string, it also needs converting to a list.
        """
        if isinstance(infile, basestring):
            # can't be unicode
            # NOTE: Could raise a ``UnicodeDecodeError``
            return infile.decode(encoding).splitlines(True)
        for i, line in enumerate(infile):
            if not isinstance(line, unicode):
                # NOTE: The isinstance test here handles mixed lists of unicode/string
                # NOTE: But the decode will break on any non-string values
                # NOTE: Or could raise a ``UnicodeDecodeError``
                infile[i] = line.decode(encoding)
        return infile


    def _decode_element(self, line):
        """Decode element to unicode if necessary."""
        if not self.encoding:
            return line
        if isinstance(line, str) and self.default_encoding:
            return line.decode(self.default_encoding)
        return line


    def _str(self, value):
        """
        Used by ``stringify`` within validate, to turn non-string values
        into strings.
        """
        if not isinstance(value, basestring):
            return str(value)
        else:
            return value


    def _parse(self, infile):
        """Actually parse the config file."""
        temp_list_values = self.list_values
        if self.unrepr:
            self.list_values = False
            
        comment_list = []
        done_start = False
        this_section = self
        maxline = len(infile) - 1
        cur_index = -1
        reset_comment = False
        
        while cur_index < maxline:
            if reset_comment:
                comment_list = []
            cur_index += 1
            line = infile[cur_index]
            sline = line.strip()
            # do we have anything on the line ?
            if not sline or sline.startswith('#'):
                reset_comment = False
                comment_list.append(line)
                continue
            
            if not done_start:
                # preserve initial comment
                self.initial_comment = comment_list
                comment_list = []
                done_start = True
                
            reset_comment = True
            # first we check if it's a section marker
            mat = self._sectionmarker.match(line)
            if mat is not None:
                # is a section line
                (indent, sect_open, sect_name, sect_close, comment) = mat.groups()
                if indent and (self.indent_type is None):
                    self.indent_type = indent
                cur_depth = sect_open.count('[')
                if cur_depth != sect_close.count(']'):
                    self._handle_error("Cannot compute the section depth at line %s.",
                                       NestingError, infile, cur_index)
                    continue
                
                if cur_depth < this_section.depth:
                    # the new section is dropping back to a previous level
                    try:
                        parent = self._match_depth(this_section,
                                                   cur_depth).parent
                    except SyntaxError:
                        self._handle_error("Cannot compute nesting level at line %s.",
                                           NestingError, infile, cur_index)
                        continue
                elif cur_depth == this_section.depth:
                    # the new section is a sibling of the current section
                    parent = this_section.parent
                elif cur_depth == this_section.depth + 1:
                    # the new section is a child the current section
                    parent = this_section
                else:
                    self._handle_error("Section too nested at line %s.",
                                       NestingError, infile, cur_index)
                    
                sect_name = self._unquote(sect_name)
                if parent.has_key(sect_name):
                    self._handle_error('Duplicate section name at line %s.',
                                       DuplicateError, infile, cur_index)
                    continue
                
                # create the new section
                this_section = Section(
                    parent,
                    cur_depth,
                    self,
                    name=sect_name)
                parent[sect_name] = this_section
                parent.inline_comments[sect_name] = comment
                parent.comments[sect_name] = comment_list
                continue
            #
            # it's not a section marker,
            # so it should be a valid ``key = value`` line
            mat = self._keyword.match(line)
            if mat is None:
                # it neither matched as a keyword
                # or a section marker
                self._handle_error(
                    'Invalid line at line "%s".',
                    ParseError, infile, cur_index)
            else:
                # is a keyword value
                # value will include any inline comment
                (indent, key, value) = mat.groups()
                if indent and (self.indent_type is None):
                    self.indent_type = indent
                # check for a multiline value
                if value[:3] in ['"""', "'''"]:
                    try:
                        (value, comment, cur_index) = self._multiline(
                            value, infile, cur_index, maxline)
                    except SyntaxError:
                        self._handle_error(
                            'Parse error in value at line %s.',
                            ParseError, infile, cur_index)
                        continue
                    else:
                        if self.unrepr:
                            comment = ''
                            try:
                                value = unrepr(value)
                            except Exception, e:
                                if type(e) == UnknownType:
                                    msg = 'Unknown name or type in value at line %s.'
                                else:
                                    msg = 'Parse error in value at line %s.'
                                self._handle_error(msg, UnreprError, infile,
                                    cur_index)
                                continue
                else:
                    if self.unrepr:
                        comment = ''
                        try:
                            value = unrepr(value)
                        except Exception, e:
                            if isinstance(e, UnknownType):
                                msg = 'Unknown name or type in value at line %s.'
                            else:
                                msg = 'Parse error in value at line %s.'
                            self._handle_error(msg, UnreprError, infile,
                                cur_index)
                            continue
                    else:
                        # extract comment and lists
                        try:
                            (value, comment) = self._handle_value(value)
                        except SyntaxError:
                            self._handle_error(
                                'Parse error in value at line %s.',
                                ParseError, infile, cur_index)
                            continue
                #
                key = self._unquote(key)
                if this_section.has_key(key):
                    self._handle_error(
                        'Duplicate keyword name at line %s.',
                        DuplicateError, infile, cur_index)
                    continue
                # add the key.
                # we set unrepr because if we have got this far we will never
                # be creating a new section
                this_section.__setitem__(key, value, unrepr=True)
                this_section.inline_comments[key] = comment
                this_section.comments[key] = comment_list
                continue
        #
        if self.indent_type is None:
            # no indentation used, set the type accordingly
            self.indent_type = ''

        # preserve the final comment
        if not self and not self.initial_comment:
            self.initial_comment = comment_list
        elif not reset_comment:
            self.final_comment = comment_list
        self.list_values = temp_list_values


    def _match_depth(self, sect, depth):
        """
        Given a section and a depth level, walk back through the sections
        parents to see if the depth level matches a previous section.
        
        Return a reference to the right section,
        or raise a SyntaxError.
        """
        while depth < sect.depth:
            if sect is sect.parent:
                # we've reached the top level already
                raise SyntaxError()
            sect = sect.parent
        if sect.depth == depth:
            return sect
        # shouldn't get here
        raise SyntaxError()


    def _handle_error(self, text, ErrorClass, infile, cur_index):
        """
        Handle an error according to the error settings.
        
        Either raise the error or store it.
        The error will have occured at ``cur_index``
        """
        line = infile[cur_index]
        cur_index += 1
        message = text % cur_index
        error = ErrorClass(message, cur_index, line)
        if self.raise_errors:
            # raise the error - parsing stops here
            raise error
        # store the error
        # reraise when parsing has finished
        self._errors.append(error)


    def _unquote(self, value):
        """Return an unquoted version of a value"""
        if (value[0] == value[-1]) and (value[0] in ('"', "'")):
            value = value[1:-1]
        return value


    def _quote(self, value, multiline=True):
        """
        Return a safely quoted version of a value.
        
        Raise a ConfigObjError if the value cannot be safely quoted.
        If multiline is ``True`` (default) then use triple quotes
        if necessary.
        
        * Don't quote values that don't need it.
        * Recursively quote members of a list and return a comma joined list.
        * Multiline is ``False`` for lists.
        * Obey list syntax for empty and single member lists.
        
        If ``list_values=False`` then the value is only quoted if it contains
        a ``\\n`` (is multiline) or '#'.
        
        If ``write_empty_values`` is set, and the value is an empty string, it
        won't be quoted.
        """
        if multiline and self.write_empty_values and value == '':
            # Only if multiline is set, so that it is used for values not
            # keys, and not values that are part of a list
            return ''
        
        if multiline and isinstance(value, (list, tuple)):
            if not value:
                return ','
            elif len(value) == 1:
                return self._quote(value[0], multiline=False) + ','
            return ', '.join([self._quote(val, multiline=False)
                for val in value])
        if not isinstance(value, basestring):
            if self.stringify:
                value = str(value)
            else:
                raise TypeError('Value "%s" is not a string.' % value)

        if not value:
            return '""'
        
        no_lists_no_quotes = not self.list_values and '\n' not in value and '#' not in value
        need_triple = multiline and ((("'" in value) and ('"' in value)) or ('\n' in value))
        hash_triple_quote = multiline and not need_triple and ("'" in value) and ('"' in value) and ('#' in value)
        check_for_single = (no_lists_no_quotes or not need_triple) and not hash_triple_quote
        
        if check_for_single:
            if not self.list_values:
                # we don't quote if ``list_values=False``
                quot = noquot
            # for normal values either single or double quotes will do
            elif '\n' in value:
                # will only happen if multiline is off - e.g. '\n' in key
                raise ConfigObjError('Value "%s" cannot be safely quoted.' % value)
            elif ((value[0] not in wspace_plus) and
                    (value[-1] not in wspace_plus) and
                    (',' not in value)):
                quot = noquot
            else:
                quot = self._get_single_quote(value)
        else:
            # if value has '\n' or "'" *and* '"', it will need triple quotes
            quot = self._get_triple_quote(value)
        
        if quot == noquot and '#' in value and self.list_values:
            quot = self._get_single_quote(value)
                
        return quot % value
    
    
    def _get_single_quote(self, value):
        if ("'" in value) and ('"' in value):
            raise ConfigObjError('Value "%s" cannot be safely quoted.' % value)
        elif '"' in value:
            quot = squot
        else:
            quot = dquot
        return quot
    
    
    def _get_triple_quote(self, value):
        if (value.find('"""') != -1) and (value.find("'''") != -1):
            raise ConfigObjError('Value "%s" cannot be safely quoted.' % value)
        if value.find('"""') == -1:
            quot = tdquot
        else:
            quot = tsquot 
        return quot


    def _handle_value(self, value):
        """
        Given a value string, unquote, remove comment,
        handle lists. (including empty and single member lists)
        """
        if self._inspec:
            # Parsing a configspec so don't handle comments
            return (value, '')
        # do we look for lists in values ?
        if not self.list_values:
            mat = self._nolistvalue.match(value)
            if mat is None:
                raise SyntaxError()
            # NOTE: we don't unquote here
            return mat.groups()
        #
        mat = self._valueexp.match(value)
        if mat is None:
            # the value is badly constructed, probably badly quoted,
            # or an invalid list
            raise SyntaxError()
        (list_values, single, empty_list, comment) = mat.groups()
        if (list_values == '') and (single is None):
            # change this if you want to accept empty values
            raise SyntaxError()
        # NOTE: note there is no error handling from here if the regex
        # is wrong: then incorrect values will slip through
        if empty_list is not None:
            # the single comma - meaning an empty list
            return ([], comment)
        if single is not None:
            # handle empty values
            if list_values and not single:
                # FIXME: the '' is a workaround because our regex now matches
                #   '' at the end of a list if it has a trailing comma
                single = None
            else:
                single = single or '""'
                single = self._unquote(single)
        if list_values == '':
            # not a list value
            return (single, comment)
        the_list = self._listvalueexp.findall(list_values)
        the_list = [self._unquote(val) for val in the_list]
        if single is not None:
            the_list += [single]
        return (the_list, comment)


    def _multiline(self, value, infile, cur_index, maxline):
        """Extract the value, where we are in a multiline situation."""
        quot = value[:3]
        newvalue = value[3:]
        single_line = self._triple_quote[quot][0]
        multi_line = self._triple_quote[quot][1]
        mat = single_line.match(value)
        if mat is not None:
            retval = list(mat.groups())
            retval.append(cur_index)
            return retval
        elif newvalue.find(quot) != -1:
            # somehow the triple quote is missing
            raise SyntaxError()
        #
        while cur_index < maxline:
            cur_index += 1
            newvalue += '\n'
            line = infile[cur_index]
            if line.find(quot) == -1:
                newvalue += line
            else:
                # end of multiline, process it
                break
        else:
            # we've got to the end of the config, oops...
            raise SyntaxError()
        mat = multi_line.match(line)
        if mat is None:
            # a badly formed line
            raise SyntaxError()
        (value, comment) = mat.groups()
        return (newvalue + value, comment, cur_index)


    def _handle_configspec(self, configspec):
        """Parse the configspec."""
        # FIXME: Should we check that the configspec was created with the 
        #        correct settings ? (i.e. ``list_values=False``)
        if not isinstance(configspec, ConfigObj):
            try:
                configspec = ConfigObj(configspec,
                                       raise_errors=True,
                                       file_error=True,
                                       _inspec=True)
            except ConfigObjError, e:
                # FIXME: Should these errors have a reference
                #        to the already parsed ConfigObj ?
                raise ConfigspecError('Parsing configspec failed: %s' % e)
            except IOError, e:
                raise IOError('Reading configspec failed: %s' % e)
        
        self.configspec = configspec
            

        
    def _set_configspec(self, section, copy):
        """
        Called by validate. Handles setting the configspec on subsections
        including sections to be validated by __many__
        """
        configspec = section.configspec
        many = configspec.get('__many__')
        if isinstance(many, dict):
            for entry in section.sections:
                if entry not in configspec:
                    section[entry].configspec = many
                    
        for entry in configspec.sections:
            if entry == '__many__':
                continue
            if entry not in section:
                section[entry] = {}
                if copy:
                    # copy comments
                    section.comments[entry] = configspec.comments.get(entry, [])
                    section.inline_comments[entry] = configspec.inline_comments.get(entry, '')
                
            # Could be a scalar when we expect a section
            if isinstance(section[entry], Section):
                section[entry].configspec = configspec[entry]
                        

    def _write_line(self, indent_string, entry, this_entry, comment):
        """Write an individual line, for the write method"""
        # NOTE: the calls to self._quote here handles non-StringType values.
        if not self.unrepr:
            val = self._decode_element(self._quote(this_entry))
        else:
            val = repr(this_entry)
        return '%s%s%s%s%s' % (indent_string,
                               self._decode_element(self._quote(entry, multiline=False)),
                               self._a_to_u(' = '),
                               val,
                               self._decode_element(comment))


    def _write_marker(self, indent_string, depth, entry, comment):
        """Write a section marker line"""
        return '%s%s%s%s%s' % (indent_string,
                               self._a_to_u('[' * depth),
                               self._quote(self._decode_element(entry), multiline=False),
                               self._a_to_u(']' * depth),
                               self._decode_element(comment))


    def _handle_comment(self, comment):
        """Deal with a comment."""
        if not comment:
            return ''
        start = self.indent_type
        if not comment.startswith('#'):
            start += self._a_to_u(' # ')
        return (start + comment)


    # Public methods

    def write(self, outfile=None, section=None):
        """
        Write the current ConfigObj as a file
        
        tekNico: FIXME: use StringIO instead of real files
        
        >>> filename = a.filename
        >>> a.filename = 'test.ini'
        >>> a.write()
        >>> a.filename = filename
        >>> a == ConfigObj('test.ini', raise_errors=True)
        1
        """
        if self.indent_type is None:
            # this can be true if initialised from a dictionary
            self.indent_type = DEFAULT_INDENT_TYPE
            
        out = []
        cs = self._a_to_u('#')
        csp = self._a_to_u('# ')
        if section is None:
            int_val = self.interpolation
            self.interpolation = False
            section = self
            for line in self.initial_comment:
                line = self._decode_element(line)
                stripped_line = line.strip()
                if stripped_line and not stripped_line.startswith(cs):
                    line = csp + line
                out.append(line)
                
        indent_string = self.indent_type * section.depth
        for entry in (section.scalars + section.sections):
            if entry in section.defaults:
                # don't write out default values
                continue
            for comment_line in section.comments[entry]:
                comment_line = self._decode_element(comment_line.lstrip())
                if comment_line and not comment_line.startswith(cs):
                    comment_line = csp + comment_line
                out.append(indent_string + comment_line)
            this_entry = section[entry]
            comment = self._handle_comment(section.inline_comments[entry])
            
            if isinstance(this_entry, dict):
                # a section
                out.append(self._write_marker(
                    indent_string,
                    this_entry.depth,
                    entry,
                    comment))
                out.extend(self.write(section=this_entry))
            else:
                out.append(self._write_line(
                    indent_string,
                    entry,
                    this_entry,
                    comment))
                
        if section is self:
            for line in self.final_comment:
                line = self._decode_element(line)
                stripped_line = line.strip()
                if stripped_line and not stripped_line.startswith(cs):
                    line = csp + line
                out.append(line)
            self.interpolation = int_val
            
        if section is not self:
            return out
        
        if (self.filename is None) and (outfile is None):
            # output a list of lines
            # might need to encode
            # NOTE: This will *screw* UTF16, each line will start with the BOM
            if self.encoding:
                out = [l.encode(self.encoding) for l in out]
            if (self.BOM and ((self.encoding is None) or
                (BOM_LIST.get(self.encoding.lower()) == 'utf_8'))):
                # Add the UTF8 BOM
                if not out:
                    out.append('')
                out[0] = BOM_UTF8 + out[0]
            return out
        
        # Turn the list to a string, joined with correct newlines
        newline = self.newlines or os.linesep
        output = self._a_to_u(newline).join(out)
        if self.encoding:
            output = output.encode(self.encoding)
        if self.BOM and ((self.encoding is None) or match_utf8(self.encoding)):
            # Add the UTF8 BOM
            output = BOM_UTF8 + output
            
        if not output.endswith(newline):
            output += newline
        if outfile is not None:
            outfile.write(output)
        else:
            h = open(self.filename, 'wb')
            h.write(output)
            h.close()


    def validate(self, validator, preserve_errors=False, copy=False,
                 section=None):
        """
        Test the ConfigObj against a configspec.
        
        It uses the ``validator`` object from *validate.py*.
        
        To run ``validate`` on the current ConfigObj, call: ::
        
            test = config.validate(validator)
        
        (Normally having previously passed in the configspec when the ConfigObj
        was created - you can dynamically assign a dictionary of checks to the
        ``configspec`` attribute of a section though).
        
        It returns ``True`` if everything passes, or a dictionary of
        pass/fails (True/False). If every member of a subsection passes, it
        will just have the value ``True``. (It also returns ``False`` if all
        members fail).
        
        In addition, it converts the values from strings to their native
        types if their checks pass (and ``stringify`` is set).
        
        If ``preserve_errors`` is ``True`` (``False`` is default) then instead
        of a marking a fail with a ``False``, it will preserve the actual
        exception object. This can contain info about the reason for failure.
        For example the ``VdtValueTooSmallError`` indicates that the value
        supplied was too small. If a value (or section) is missing it will
        still be marked as ``False``.
        
        You must have the validate module to use ``preserve_errors=True``.
        
        You can then use the ``flatten_errors`` function to turn your nested
        results dictionary into a flattened list of failures - useful for
        displaying meaningful error messages.
        """
        if section is None:
            if self.configspec is None:
                raise ValueError('No configspec supplied.')
            if preserve_errors:
                # We do this once to remove a top level dependency on the validate module
                # Which makes importing configobj faster
                from validate import VdtMissingValue
                self._vdtMissingValue = VdtMissingValue
                
            section = self

            if copy:
                section.initial_comment = section.configspec.initial_comment
                section.final_comment = section.configspec.final_comment
                section.encoding = section.configspec.encoding
                section.BOM = section.configspec.BOM
                section.newlines = section.configspec.newlines
                section.indent_type = section.configspec.indent_type
            
        #
        configspec = section.configspec
        self._set_configspec(section, copy)
        
        def validate_entry(entry, spec, val, missing, ret_true, ret_false):
            try:
                check = validator.check(spec,
                                        val,
                                        missing=missing
                                        )
            except validator.baseErrorClass, e:
                if not preserve_errors or isinstance(e, self._vdtMissingValue):
                    out[entry] = False
                else:
                    # preserve the error
                    out[entry] = e
                    ret_false = False
                ret_true = False
            else:
                try: 
                    section.default_values.pop(entry, None)
                except AttributeError: 
                    # For Python 2.2 compatibility
                    try:
                        del section.default_values[entry]
                    except KeyError:
                        pass
                    
                try: 
                    section.default_values[entry] = validator.get_default_value(configspec[entry])
                except (KeyError, AttributeError):
                    # No default or validator has no 'get_default_value' (e.g. SimpleVal)
                    pass
                    
                ret_false = False
                out[entry] = True
                if self.stringify or missing:
                    # if we are doing type conversion
                    # or the value is a supplied default
                    if not self.stringify:
                        if isinstance(check, (list, tuple)):
                            # preserve lists
                            check = [self._str(item) for item in check]
                        elif missing and check is None:
                            # convert the None from a default to a ''
                            check = ''
                        else:
                            check = self._str(check)
                    if (check != val) or missing:
                        section[entry] = check
                if not copy and missing and entry not in section.defaults:
                    section.defaults.append(entry)
            return ret_true, ret_false
        
        #
        out = {}
        ret_true = True
        ret_false = True
        
        unvalidated = [k for k in section.scalars if k not in configspec]
        incorrect_sections = [k for k in configspec.sections if k in section.scalars]        
        incorrect_scalars = [k for k in configspec.scalars if k in section.sections]
        
        for entry in configspec.scalars:
            if entry in ('__many__', '___many___'):
                # reserved names
                continue
            
            if (not entry in section.scalars) or (entry in section.defaults):
                # missing entries
                # or entries from defaults
                missing = True
                val = None
                if copy and not entry in section.scalars:
                    # copy comments
                    section.comments[entry] = (
                        configspec.comments.get(entry, []))
                    section.inline_comments[entry] = (
                        configspec.inline_comments.get(entry, ''))
                #
            else:
                missing = False
                val = section[entry]
                
            ret_true, ret_false = validate_entry(entry, configspec[entry], val,
                                                 missing, ret_true, ret_false)
        
        many = None
        if '__many__' in configspec.scalars:
            many = configspec['__many__']
        elif '___many___' in configspec.scalars:
            many = configspec['___many___']
        
        if many is not None:
            for entry in unvalidated:
                val = section[entry]
                ret_true, ret_false = validate_entry(entry, many, val, False,
                                                     ret_true, ret_false)

        for entry in incorrect_scalars:
            ret_true = False
            if not preserve_errors:
                out[entry] = False
            else:
                ret_false = False
                msg = 'Value %r was provided as a section' % entry
                out[entry] = validator.baseErrorClass(msg)
        for entry in incorrect_sections:
            ret_true = False
            if not preserve_errors:
                out[entry] = False
            else:
                ret_false = False
                msg = 'Section %r was provided as a single value' % entry
                out[entry] = validator.baseErrorClass(msg)
                
        # Missing sections will have been created as empty ones when the
        # configspec was read.
        for entry in section.sections:
            # FIXME: this means DEFAULT is not copied in copy mode
            if section is self and entry == 'DEFAULT':
                continue
            if section[entry].configspec is None:
                continue
            if copy:
                section.comments[entry] = configspec.comments.get(entry, [])
                section.inline_comments[entry] = configspec.inline_comments.get(entry, '')
            check = self.validate(validator, preserve_errors=preserve_errors, copy=copy, section=section[entry])
            out[entry] = check
            if check == False:
                ret_true = False
            elif check == True:
                ret_false = False
            else:
                ret_true = False
                ret_false = False
        #
        if ret_true:
            return True
        elif ret_false:
            return False
        return out


    def reset(self):
        """Clear ConfigObj instance and restore to 'freshly created' state."""
        self.clear()
        self._initialise()
        # FIXME: Should be done by '_initialise', but ConfigObj constructor (and reload)
        #        requires an empty dictionary
        self.configspec = None
        # Just to be sure ;-)
        self._original_configspec = None
        
        
    def reload(self):
        """
        Reload a ConfigObj from file.
        
        This method raises a ``ReloadError`` if the ConfigObj doesn't have
        a filename attribute pointing to a file.
        """
        if not isinstance(self.filename, basestring):
            raise ReloadError()

        filename = self.filename
        current_options = {}
        for entry in OPTION_DEFAULTS:
            if entry == 'configspec':
                continue
            current_options[entry] = getattr(self, entry)
            
        configspec = self._original_configspec
        current_options['configspec'] = configspec
            
        self.clear()
        self._initialise(current_options)
        self._load(filename, configspec)
        


class SimpleVal(object):
    """
    A simple validator.
    Can be used to check that all members expected are present.
    
    To use it, provide a configspec with all your members in (the value given
    will be ignored). Pass an instance of ``SimpleVal`` to the ``validate``
    method of your ``ConfigObj``. ``validate`` will return ``True`` if all
    members are present, or a dictionary with True/False meaning
    present/missing. (Whole missing sections will be replaced with ``False``)
    """
    
    def __init__(self):
        self.baseErrorClass = ConfigObjError
    
    def check(self, check, member, missing=False):
        """A dummy check method, always returns the value unchanged."""
        if missing:
            raise self.baseErrorClass()
        return member


# Check / processing functions for options
def flatten_errors(cfg, res, levels=None, results=None):
    """
    An example function that will turn a nested dictionary of results
    (as returned by ``ConfigObj.validate``) into a flat list.
    
    ``cfg`` is the ConfigObj instance being checked, ``res`` is the results
    dictionary returned by ``validate``.
    
    (This is a recursive function, so you shouldn't use the ``levels`` or
    ``results`` arguments - they are used by the function.)
    
    Returns a list of keys that failed. Each member of the list is a tuple :
    
    ::
    
        ([list of sections...], key, result)
    
    If ``validate`` was called with ``preserve_errors=False`` (the default)
    then ``result`` will always be ``False``.

    *list of sections* is a flattened list of sections that the key was found
    in.
    
    If the section was missing (or a section was expected and a scalar provided
    - or vice-versa) then key will be ``None``.
    
    If the value (or section) was missing then ``result`` will be ``False``.
    
    If ``validate`` was called with ``preserve_errors=True`` and a value
    was present, but failed the check, then ``result`` will be the exception
    object returned. You can use this as a string that describes the failure.
    
    For example *The value "3" is of the wrong type*.
    
    >>> import validate
    >>> vtor = validate.Validator()
    >>> my_ini = '''
    ...     option1 = True
    ...     [section1]
    ...     option1 = True
    ...     [section2]
    ...     another_option = Probably
    ...     [section3]
    ...     another_option = True
    ...     [[section3b]]
    ...     value = 3
    ...     value2 = a
    ...     value3 = 11
    ...     '''
    >>> my_cfg = '''
    ...     option1 = boolean()
    ...     option2 = boolean()
    ...     option3 = boolean(default=Bad_value)
    ...     [section1]
    ...     option1 = boolean()
    ...     option2 = boolean()
    ...     option3 = boolean(default=Bad_value)
    ...     [section2]
    ...     another_option = boolean()
    ...     [section3]
    ...     another_option = boolean()
    ...     [[section3b]]
    ...     value = integer
    ...     value2 = integer
    ...     value3 = integer(0, 10)
    ...         [[[section3b-sub]]]
    ...         value = string
    ...     [section4]
    ...     another_option = boolean()
    ...     '''
    >>> cs = my_cfg.split('\\n')
    >>> ini = my_ini.split('\\n')
    >>> cfg = ConfigObj(ini, configspec=cs)
    >>> res = cfg.validate(vtor, preserve_errors=True)
    >>> errors = []
    >>> for entry in flatten_errors(cfg, res):
    ...     section_list, key, error = entry
    ...     section_list.insert(0, '[root]')
    ...     if key is not None:
    ...        section_list.append(key)
    ...     else:
    ...         section_list.append('[missing]')
    ...     section_string = ', '.join(section_list)
    ...     errors.append((section_string, ' = ', error))
    >>> errors.sort()
    >>> for entry in errors:
    ...     print entry[0], entry[1], (entry[2] or 0)
    [root], option2  =  0
    [root], option3  =  the value "Bad_value" is of the wrong type.
    [root], section1, option2  =  0
    [root], section1, option3  =  the value "Bad_value" is of the wrong type.
    [root], section2, another_option  =  the value "Probably" is of the wrong type.
    [root], section3, section3b, section3b-sub, [missing]  =  0
    [root], section3, section3b, value2  =  the value "a" is of the wrong type.
    [root], section3, section3b, value3  =  the value "11" is too big.
    [root], section4, [missing]  =  0
    """
    if levels is None:
        # first time called
        levels = []
        results = []
    if res is True:
        return results
    if res is False or isinstance(res, Exception):
        results.append((levels[:], None, res))
        if levels:
            levels.pop()
        return results
    for (key, val) in res.items():
        if val == True:
            continue
        if isinstance(cfg.get(key), dict):
            # Go down one level
            levels.append(key)
            flatten_errors(cfg[key], val, levels, results)
            continue
        results.append((levels[:], key, val))
    #
    # Go up one level
    if levels:
        levels.pop()
    #
    return results


"""*A programming language is a medium of expression.* - Paul Graham"""

########NEW FILE########
__FILENAME__ = easter
"""
Copyright (c) 2003-2007  Gustavo Niemeyer <gustavo@niemeyer.net>

This module offers extensions to the standard python 2.3+
datetime module.
"""
__author__ = "Gustavo Niemeyer <gustavo@niemeyer.net>"
__license__ = "PSF License"

import datetime

__all__ = ["easter", "EASTER_JULIAN", "EASTER_ORTHODOX", "EASTER_WESTERN"]

EASTER_JULIAN   = 1
EASTER_ORTHODOX = 2
EASTER_WESTERN  = 3

def easter(year, method=EASTER_WESTERN):
    """
    This method was ported from the work done by GM Arts,
    on top of the algorithm by Claus Tondering, which was
    based in part on the algorithm of Ouding (1940), as
    quoted in "Explanatory Supplement to the Astronomical
    Almanac", P.  Kenneth Seidelmann, editor.

    This algorithm implements three different easter
    calculation methods:
    
    1 - Original calculation in Julian calendar, valid in
        dates after 326 AD
    2 - Original method, with date converted to Gregorian
        calendar, valid in years 1583 to 4099
    3 - Revised method, in Gregorian calendar, valid in
        years 1583 to 4099 as well

    These methods are represented by the constants:

    EASTER_JULIAN   = 1
    EASTER_ORTHODOX = 2
    EASTER_WESTERN  = 3

    The default method is method 3.
    
    More about the algorithm may be found at:

    http://users.chariot.net.au/~gmarts/eastalg.htm

    and

    http://www.tondering.dk/claus/calendar.html

    """

    if not (1 <= method <= 3):
        raise ValueError, "invalid method"

    # g - Golden year - 1
    # c - Century
    # h - (23 - Epact) mod 30
    # i - Number of days from March 21 to Paschal Full Moon
    # j - Weekday for PFM (0=Sunday, etc)
    # p - Number of days from March 21 to Sunday on or before PFM
    #     (-6 to 28 methods 1 & 3, to 56 for method 2)
    # e - Extra days to add for method 2 (converting Julian
    #     date to Gregorian date)

    y = year
    g = y % 19
    e = 0
    if method < 3:
        # Old method
        i = (19*g+15)%30
        j = (y+y//4+i)%7
        if method == 2:
            # Extra dates to convert Julian to Gregorian date
            e = 10
            if y > 1600:
                e = e+y//100-16-(y//100-16)//4
    else:
        # New method
        c = y//100
        h = (c-c//4-(8*c+13)//25+19*g+15)%30
        i = h-(h//28)*(1-(h//28)*(29//(h+1))*((21-g)//11))
        j = (y+y//4+i+2-c+c//4)%7

    # p can be from -6 to 56 corresponding to dates 22 March to 23 May
    # (later dates apply to method 2, although 23 May never actually occurs)
    p = i-j+e
    d = 1+(p+27+(p+6)//40)%31
    m = 3+(p+26)//30
    return datetime.date(int(y),int(m),int(d))


########NEW FILE########
__FILENAME__ = parser
# -*- coding:iso-8859-1 -*-
"""
Copyright (c) 2003-2007  Gustavo Niemeyer <gustavo@niemeyer.net>

This module offers extensions to the standard python 2.3+
datetime module.
"""
__author__ = "Gustavo Niemeyer <gustavo@niemeyer.net>"
__license__ = "PSF License"

import datetime
import string
import time
import sys
import os

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

import relativedelta
import tz


__all__ = ["parse", "parserinfo"]


# Some pointers:
#
# http://www.cl.cam.ac.uk/~mgk25/iso-time.html
# http://www.iso.ch/iso/en/prods-services/popstds/datesandtime.html
# http://www.w3.org/TR/NOTE-datetime
# http://ringmaster.arc.nasa.gov/tools/time_formats.html
# http://search.cpan.org/author/MUIR/Time-modules-2003.0211/lib/Time/ParseDate.pm
# http://stein.cshl.org/jade/distrib/docs/java.text.SimpleDateFormat.html


class _timelex(object):

    def __init__(self, instream):
        if isinstance(instream, basestring):
            instream = StringIO(instream)
        self.instream = instream
        self.wordchars = ('abcdfeghijklmnopqrstuvwxyz'
                          'ABCDEFGHIJKLMNOPQRSTUVWXYZ_'
                          ''
                          '')
        self.numchars = '0123456789'
        self.whitespace = ' \t\r\n'
        self.charstack = []
        self.tokenstack = []
        self.eof = False

    def get_token(self):
        if self.tokenstack:
            return self.tokenstack.pop(0)
        seenletters = False
        token = None
        state = None
        wordchars = self.wordchars
        numchars = self.numchars
        whitespace = self.whitespace
        while not self.eof:
            if self.charstack:
                nextchar = self.charstack.pop(0)
            else:
                nextchar = self.instream.read(1)
                while nextchar == '\x00':
                    nextchar = self.instream.read(1)
            if not nextchar:
                self.eof = True
                break
            elif not state:
                token = nextchar
                if nextchar in wordchars:
                    state = 'a'
                elif nextchar in numchars:
                    state = '0'
                elif nextchar in whitespace:
                    token = ' '
                    break # emit token
                else:
                    break # emit token
            elif state == 'a':
                seenletters = True
                if nextchar in wordchars:
                    token += nextchar
                elif nextchar == '.':
                    token += nextchar
                    state = 'a.'
                else:
                    self.charstack.append(nextchar)
                    break # emit token
            elif state == '0':
                if nextchar in numchars:
                    token += nextchar
                elif nextchar == '.':
                    token += nextchar
                    state = '0.'
                else:
                    self.charstack.append(nextchar)
                    break # emit token
            elif state == 'a.':
                seenletters = True
                if nextchar == '.' or nextchar in wordchars:
                    token += nextchar
                elif nextchar in numchars and token[-1] == '.':
                    token += nextchar
                    state = '0.'
                else:
                    self.charstack.append(nextchar)
                    break # emit token
            elif state == '0.':
                if nextchar == '.' or nextchar in numchars:
                    token += nextchar
                elif nextchar in wordchars and token[-1] == '.':
                    token += nextchar
                    state = 'a.'
                else:
                    self.charstack.append(nextchar)
                    break # emit token
        if (state in ('a.', '0.') and
            (seenletters or token.count('.') > 1 or token[-1] == '.')):
            l = token.split('.')
            token = l[0]
            for tok in l[1:]:
                self.tokenstack.append('.')
                if tok:
                    self.tokenstack.append(tok)
        return token

    def __iter__(self):
        return self

    def next(self):
        token = self.get_token()
        if token is None:
            raise StopIteration
        return token

    def split(cls, s):
        return list(cls(s))
    split = classmethod(split)


class _resultbase(object):

    def __init__(self):
        for attr in self.__slots__:
            setattr(self, attr, None)

    def _repr(self, classname):
        l = []
        for attr in self.__slots__:
            value = getattr(self, attr)
            if value is not None:
                l.append("%s=%s" % (attr, `value`))
        return "%s(%s)" % (classname, ", ".join(l))

    def __repr__(self):
        return self._repr(self.__class__.__name__)


class parserinfo(object):

    # m from a.m/p.m, t from ISO T separator
    JUMP = [" ", ".", ",", ";", "-", "/", "'",
            "at", "on", "and", "ad", "m", "t", "of",
            "st", "nd", "rd", "th"] 

    WEEKDAYS = [("Mon", "Monday"),
                ("Tue", "Tuesday"),
                ("Wed", "Wednesday"),
                ("Thu", "Thursday"),
                ("Fri", "Friday"),
                ("Sat", "Saturday"),
                ("Sun", "Sunday")]
    MONTHS   = [("Jan", "January"),
                ("Feb", "February"),
                ("Mar", "March"),
                ("Apr", "April"),
                ("May", "May"),
                ("Jun", "June"),
                ("Jul", "July"),
                ("Aug", "August"),
                ("Sep", "September"),
                ("Oct", "October"),
                ("Nov", "November"),
                ("Dec", "December")]
    HMS = [("h", "hour", "hours"),
           ("m", "minute", "minutes"),
           ("s", "second", "seconds")]
    AMPM = [("am", "a"),
            ("pm", "p")]
    UTCZONE = ["UTC", "GMT", "Z"]
    PERTAIN = ["of"]
    TZOFFSET = {}

    def __init__(self, dayfirst=False, yearfirst=False):
        self._jump = self._convert(self.JUMP)
        self._weekdays = self._convert(self.WEEKDAYS)
        self._months = self._convert(self.MONTHS)
        self._hms = self._convert(self.HMS)
        self._ampm = self._convert(self.AMPM)
        self._utczone = self._convert(self.UTCZONE)
        self._pertain = self._convert(self.PERTAIN)

        self.dayfirst = dayfirst
        self.yearfirst = yearfirst

        self._year = time.localtime().tm_year
        self._century = self._year//100*100

    def _convert(self, lst):
        dct = {}
        for i in range(len(lst)):
            v = lst[i]
            if isinstance(v, tuple):
                for v in v:
                    dct[v.lower()] = i
            else:
                dct[v.lower()] = i
        return dct

    def jump(self, name):
        return name.lower() in self._jump

    def weekday(self, name):
        if len(name) >= 3:
            try:
                return self._weekdays[name.lower()]
            except KeyError:
                pass
        return None

    def month(self, name):
        if len(name) >= 3:
            try:
                return self._months[name.lower()]+1
            except KeyError:
                pass
        return None

    def hms(self, name):
        try:
            return self._hms[name.lower()]
        except KeyError:
            return None

    def ampm(self, name):
        try:
            return self._ampm[name.lower()]
        except KeyError:
            return None

    def pertain(self, name):
        return name.lower() in self._pertain

    def utczone(self, name):
        return name.lower() in self._utczone

    def tzoffset(self, name):
        if name in self._utczone:
            return 0
        return self.TZOFFSET.get(name)

    def convertyear(self, year):
        if year < 100:
            year += self._century
            if abs(year-self._year) >= 50:
                if year < self._year:
                    year += 100
                else:
                    year -= 100
        return year

    def validate(self, res):
        # move to info
        if res.year is not None:
            res.year = self.convertyear(res.year)
        if res.tzoffset == 0 and not res.tzname or res.tzname == 'Z':
            res.tzname = "UTC"
            res.tzoffset = 0
        elif res.tzoffset != 0 and res.tzname and self.utczone(res.tzname):
            res.tzoffset = 0
        return True


class parser(object):

    def __init__(self, info=None):
        self.info = info or parserinfo()

    def parse(self, timestr, default=None,
                    ignoretz=False, tzinfos=None,
                    **kwargs):
        if not default:
            default = datetime.datetime.now().replace(hour=0, minute=0,
                                                      second=0, microsecond=0)
        res = self._parse(timestr, **kwargs)
        if res is None:
            raise ValueError, "unknown string format"
        repl = {}
        for attr in ["year", "month", "day", "hour",
                     "minute", "second", "microsecond"]:
            value = getattr(res, attr)
            if value is not None:
                repl[attr] = value
        ret = default.replace(**repl)
        if res.weekday is not None and not res.day:
            ret = ret+relativedelta.relativedelta(weekday=res.weekday)
        if not ignoretz:
            if callable(tzinfos) or tzinfos and res.tzname in tzinfos:
                if callable(tzinfos):
                    tzdata = tzinfos(res.tzname, res.tzoffset)
                else:
                    tzdata = tzinfos.get(res.tzname)
                if isinstance(tzdata, datetime.tzinfo):
                    tzinfo = tzdata
                elif isinstance(tzdata, basestring):
                    tzinfo = tz.tzstr(tzdata)
                elif isinstance(tzdata, int):
                    tzinfo = tz.tzoffset(res.tzname, tzdata)
                else:
                    raise ValueError, "offset must be tzinfo subclass, " \
                                      "tz string, or int offset"
                ret = ret.replace(tzinfo=tzinfo)
            elif res.tzname and res.tzname in time.tzname:
                ret = ret.replace(tzinfo=tz.tzlocal())
            elif res.tzoffset == 0:
                ret = ret.replace(tzinfo=tz.tzutc())
            elif res.tzoffset:
                ret = ret.replace(tzinfo=tz.tzoffset(res.tzname, res.tzoffset))
        return ret

    class _result(_resultbase):
        __slots__ = ["year", "month", "day", "weekday",
                     "hour", "minute", "second", "microsecond",
                     "tzname", "tzoffset"]

    def _parse(self, timestr, dayfirst=None, yearfirst=None, fuzzy=False):
        info = self.info
        if dayfirst is None:
            dayfirst = info.dayfirst
        if yearfirst is None:
            yearfirst = info.yearfirst
        res = self._result()
        l = _timelex.split(timestr)
        try:

            # year/month/day list
            ymd = []

            # Index of the month string in ymd
            mstridx = -1

            len_l = len(l)
            i = 0
            while i < len_l:

                # Check if it's a number
                try:
                    value_repr = l[i]
                    value = float(value_repr)
                except ValueError:
                    value = None

                if value is not None:
                    # Token is a number
                    len_li = len(l[i])
                    i += 1
                    if (len(ymd) == 3 and len_li in (2, 4)
                        and (i >= len_l or (l[i] != ':' and
                                            info.hms(l[i]) is None))):
                        # 19990101T23[59]
                        s = l[i-1]
                        res.hour = int(s[:2])
                        if len_li == 4:
                            res.minute = int(s[2:])
                    elif len_li == 6 or (len_li > 6 and l[i-1].find('.') == 6):
                        # YYMMDD or HHMMSS[.ss]
                        s = l[i-1] 
                        if not ymd and l[i-1].find('.') == -1:
                            ymd.append(info.convertyear(int(s[:2])))
                            ymd.append(int(s[2:4]))
                            ymd.append(int(s[4:]))
                        else:
                            # 19990101T235959[.59]
                            res.hour = int(s[:2])
                            res.minute = int(s[2:4])
                            res.second, res.microsecond = _parsems(s[4:])
                    elif len_li == 8:
                        # YYYYMMDD
                        s = l[i-1]
                        ymd.append(int(s[:4]))
                        ymd.append(int(s[4:6]))
                        ymd.append(int(s[6:]))
                    elif len_li in (12, 14):
                        # YYYYMMDDhhmm[ss]
                        s = l[i-1]
                        ymd.append(int(s[:4]))
                        ymd.append(int(s[4:6]))
                        ymd.append(int(s[6:8]))
                        res.hour = int(s[8:10])
                        res.minute = int(s[10:12])
                        if len_li == 14:
                            res.second = int(s[12:])
                    elif ((i < len_l and info.hms(l[i]) is not None) or
                          (i+1 < len_l and l[i] == ' ' and
                           info.hms(l[i+1]) is not None)):
                        # HH[ ]h or MM[ ]m or SS[.ss][ ]s
                        if l[i] == ' ':
                            i += 1
                        idx = info.hms(l[i])
                        while True:
                            if idx == 0:
                                res.hour = int(value)
                                if value%1:
                                    res.minute = int(60*(value%1))
                            elif idx == 1:
                                res.minute = int(value)
                                if value%1:
                                    res.second = int(60*(value%1))
                            elif idx == 2:
                                res.second, res.microsecond = \
                                    _parsems(value_repr)
                            i += 1
                            if i >= len_l or idx == 2:
                                break
                            # 12h00
                            try:
                                value_repr = l[i]
                                value = float(value_repr)
                            except ValueError:
                                break
                            else:
                                i += 1
                                idx += 1
                                if i < len_l:
                                    newidx = info.hms(l[i])
                                    if newidx is not None:
                                        idx = newidx
                    elif i+1 < len_l and l[i] == ':':
                        # HH:MM[:SS[.ss]]
                        res.hour = int(value)
                        i += 1
                        value = float(l[i])
                        res.minute = int(value)
                        if value%1:
                            res.second = int(60*(value%1))
                        i += 1
                        if i < len_l and l[i] == ':':
                            res.second, res.microsecond = _parsems(l[i+1])
                            i += 2
                    elif i < len_l and l[i] in ('-', '/', '.'):
                        sep = l[i]
                        ymd.append(int(value))
                        i += 1
                        if i < len_l and not info.jump(l[i]):
                            try:
                                # 01-01[-01]
                                ymd.append(int(l[i]))
                            except ValueError:
                                # 01-Jan[-01]
                                value = info.month(l[i])
                                if value is not None:
                                    ymd.append(value)
                                    assert mstridx == -1
                                    mstridx = len(ymd)-1
                                else:
                                    return None
                            i += 1
                            if i < len_l and l[i] == sep:
                                # We have three members
                                i += 1
                                value = info.month(l[i])
                                if value is not None:
                                    ymd.append(value)
                                    mstridx = len(ymd)-1
                                    assert mstridx == -1
                                else:
                                    ymd.append(int(l[i]))
                                i += 1
                    elif i >= len_l or info.jump(l[i]):
                        if i+1 < len_l and info.ampm(l[i+1]) is not None:
                            # 12 am
                            res.hour = int(value)
                            if res.hour < 12 and info.ampm(l[i+1]) == 1:
                                res.hour += 12
                            elif res.hour == 12 and info.ampm(l[i+1]) == 0:
                                res.hour = 0
                            i += 1
                        else:
                            # Year, month or day
                            ymd.append(int(value))
                        i += 1
                    elif info.ampm(l[i]) is not None:
                        # 12am
                        res.hour = int(value)
                        if res.hour < 12 and info.ampm(l[i]) == 1:
                            res.hour += 12
                        elif res.hour == 12 and info.ampm(l[i]) == 0:
                            res.hour = 0
                        i += 1
                    elif not fuzzy:
                        return None
                    else:
                        i += 1
                    continue

                # Check weekday
                value = info.weekday(l[i])
                if value is not None:
                    res.weekday = value
                    i += 1
                    continue

                # Check month name
                value = info.month(l[i])
                if value is not None:
                    ymd.append(value)
                    assert mstridx == -1
                    mstridx = len(ymd)-1
                    i += 1
                    if i < len_l:
                        if l[i] in ('-', '/'):
                            # Jan-01[-99]
                            sep = l[i]
                            i += 1
                            ymd.append(int(l[i]))
                            i += 1
                            if i < len_l and l[i] == sep:
                                # Jan-01-99
                                i += 1
                                ymd.append(int(l[i]))
                                i += 1
                        elif (i+3 < len_l and l[i] == l[i+2] == ' '
                              and info.pertain(l[i+1])):
                            # Jan of 01
                            # In this case, 01 is clearly year
                            try:
                                value = int(l[i+3])
                            except ValueError:
                                # Wrong guess
                                pass
                            else:
                                # Convert it here to become unambiguous
                                ymd.append(info.convertyear(value))
                            i += 4
                    continue

                # Check am/pm
                value = info.ampm(l[i])
                if value is not None:
                    if value == 1 and res.hour < 12:
                        res.hour += 12
                    elif value == 0 and res.hour == 12:
                        res.hour = 0
                    i += 1
                    continue

                # Check for a timezone name
                if (res.hour is not None and len(l[i]) <= 5 and
                    res.tzname is None and res.tzoffset is None and
                    not [x for x in l[i] if x not in string.ascii_uppercase]):
                    res.tzname = l[i]
                    res.tzoffset = info.tzoffset(res.tzname)
                    i += 1

                    # Check for something like GMT+3, or BRST+3. Notice
                    # that it doesn't mean "I am 3 hours after GMT", but
                    # "my time +3 is GMT". If found, we reverse the
                    # logic so that timezone parsing code will get it
                    # right.
                    if i < len_l and l[i] in ('+', '-'):
                        l[i] = ('+', '-')[l[i] == '+']
                        res.tzoffset = None
                        if info.utczone(res.tzname):
                            # With something like GMT+3, the timezone
                            # is *not* GMT.
                            res.tzname = None

                    continue

                # Check for a numbered timezone
                if res.hour is not None and l[i] in ('+', '-'):
                    signal = (-1,1)[l[i] == '+']
                    i += 1
                    len_li = len(l[i])
                    if len_li == 4:
                        # -0300
                        res.tzoffset = int(l[i][:2])*3600+int(l[i][2:])*60
                    elif i+1 < len_l and l[i+1] == ':':
                        # -03:00
                        res.tzoffset = int(l[i])*3600+int(l[i+2])*60
                        i += 2
                    elif len_li <= 2:
                        # -[0]3
                        res.tzoffset = int(l[i][:2])*3600
                    else:
                        return None
                    i += 1
                    res.tzoffset *= signal

                    # Look for a timezone name between parenthesis
                    if (i+3 < len_l and
                        info.jump(l[i]) and l[i+1] == '(' and l[i+3] == ')' and
                        3 <= len(l[i+2]) <= 5 and
                        not [x for x in l[i+2]
                                if x not in string.ascii_uppercase]):
                        # -0300 (BRST)
                        res.tzname = l[i+2]
                        i += 4
                    continue

                # Check jumps
                if not (info.jump(l[i]) or fuzzy):
                    return None

                i += 1

            # Process year/month/day
            len_ymd = len(ymd)
            if len_ymd > 3:
                # More than three members!?
                return None
            elif len_ymd == 1 or (mstridx != -1 and len_ymd == 2):
                # One member, or two members with a month string
                if mstridx != -1:
                    res.month = ymd[mstridx]
                    del ymd[mstridx]
                if len_ymd > 1 or mstridx == -1:
                    if ymd[0] > 31:
                        res.year = ymd[0]
                    else:
                        res.day = ymd[0]
            elif len_ymd == 2:
                # Two members with numbers
                if ymd[0] > 31:
                    # 99-01
                    res.year, res.month = ymd
                elif ymd[1] > 31:
                    # 01-99
                    res.month, res.year = ymd
                elif dayfirst and ymd[1] <= 12:
                    # 13-01
                    res.day, res.month = ymd
                else:
                    # 01-13
                    res.month, res.day = ymd
            if len_ymd == 3:
                # Three members
                if mstridx == 0:
                    res.month, res.day, res.year = ymd
                elif mstridx == 1:
                    if ymd[0] > 31 or (yearfirst and ymd[2] <= 31):
                        # 99-Jan-01
                        res.year, res.month, res.day = ymd
                    else:
                        # 01-Jan-01
                        # Give precendence to day-first, since
                        # two-digit years is usually hand-written.
                        res.day, res.month, res.year = ymd
                elif mstridx == 2:
                    # WTF!?
                    if ymd[1] > 31:
                        # 01-99-Jan
                        res.day, res.year, res.month = ymd
                    else:
                        # 99-01-Jan
                        res.year, res.day, res.month = ymd
                else:
                    if ymd[0] > 31 or \
                       (yearfirst and ymd[1] <= 12 and ymd[2] <= 31):
                        # 99-01-01
                        res.year, res.month, res.day = ymd
                    elif ymd[0] > 12 or (dayfirst and ymd[1] <= 12):
                        # 13-01-01
                        res.day, res.month, res.year = ymd
                    else:
                        # 01-13-01
                        res.month, res.day, res.year = ymd

        except (IndexError, ValueError, AssertionError):
            return None

        if not info.validate(res):
            return None
        return res

DEFAULTPARSER = parser()
def parse(timestr, parserinfo=None, **kwargs):
    if parserinfo:
        return parser(parserinfo).parse(timestr, **kwargs)
    else:
        return DEFAULTPARSER.parse(timestr, **kwargs)


class _tzparser(object):

    class _result(_resultbase):

        __slots__ = ["stdabbr", "stdoffset", "dstabbr", "dstoffset",
                     "start", "end"]

        class _attr(_resultbase):
            __slots__ = ["month", "week", "weekday",
                         "yday", "jyday", "day", "time"]

        def __repr__(self):
            return self._repr("")

        def __init__(self):
            _resultbase.__init__(self)
            self.start = self._attr()
            self.end = self._attr()

    def parse(self, tzstr):
        res = self._result()
        l = _timelex.split(tzstr)
        try:

            len_l = len(l)

            i = 0
            while i < len_l:
                # BRST+3[BRDT[+2]]
                j = i
                while j < len_l and not [x for x in l[j]
                                            if x in "0123456789:,-+"]:
                    j += 1
                if j != i:
                    if not res.stdabbr:
                        offattr = "stdoffset"
                        res.stdabbr = "".join(l[i:j])
                    else:
                        offattr = "dstoffset"
                        res.dstabbr = "".join(l[i:j])
                    i = j
                    if (i < len_l and
                        (l[i] in ('+', '-') or l[i][0] in "0123456789")):
                        if l[i] in ('+', '-'):
                            # Yes, that's right.  See the TZ variable
                            # documentation.
                            signal = (1,-1)[l[i] == '+']
                            i += 1
                        else:
                            signal = -1
                        len_li = len(l[i])
                        if len_li == 4:
                            # -0300
                            setattr(res, offattr,
                                    (int(l[i][:2])*3600+int(l[i][2:])*60)*signal)
                        elif i+1 < len_l and l[i+1] == ':':
                            # -03:00
                            setattr(res, offattr,
                                    (int(l[i])*3600+int(l[i+2])*60)*signal)
                            i += 2
                        elif len_li <= 2:
                            # -[0]3
                            setattr(res, offattr,
                                    int(l[i][:2])*3600*signal)
                        else:
                            return None
                        i += 1
                    if res.dstabbr:
                        break
                else:
                    break

            if i < len_l:
                for j in range(i, len_l):
                    if l[j] == ';': l[j] = ','

                assert l[i] == ','

                i += 1

            if i >= len_l:
                pass
            elif (8 <= l.count(',') <= 9 and
                not [y for x in l[i:] if x != ','
                       for y in x if y not in "0123456789"]):
                # GMT0BST,3,0,30,3600,10,0,26,7200[,3600]
                for x in (res.start, res.end):
                    x.month = int(l[i])
                    i += 2
                    if l[i] == '-':
                        value = int(l[i+1])*-1
                        i += 1
                    else:
                        value = int(l[i])
                    i += 2
                    if value:
                        x.week = value
                        x.weekday = (int(l[i])-1)%7
                    else:
                        x.day = int(l[i])
                    i += 2
                    x.time = int(l[i])
                    i += 2
                if i < len_l:
                    if l[i] in ('-','+'):
                        signal = (-1,1)[l[i] == "+"]
                        i += 1
                    else:
                        signal = 1
                    res.dstoffset = (res.stdoffset+int(l[i]))*signal
            elif (l.count(',') == 2 and l[i:].count('/') <= 2 and
                  not [y for x in l[i:] if x not in (',','/','J','M',
                                                     '.','-',':')
                         for y in x if y not in "0123456789"]):
                for x in (res.start, res.end):
                    if l[i] == 'J':
                        # non-leap year day (1 based)
                        i += 1
                        x.jyday = int(l[i])
                    elif l[i] == 'M':
                        # month[-.]week[-.]weekday
                        i += 1
                        x.month = int(l[i])
                        i += 1
                        assert l[i] in ('-', '.')
                        i += 1
                        x.week = int(l[i])
                        if x.week == 5:
                            x.week = -1
                        i += 1
                        assert l[i] in ('-', '.')
                        i += 1
                        x.weekday = (int(l[i])-1)%7
                    else:
                        # year day (zero based)
                        x.yday = int(l[i])+1

                    i += 1

                    if i < len_l and l[i] == '/':
                        i += 1
                        # start time
                        len_li = len(l[i])
                        if len_li == 4:
                            # -0300
                            x.time = (int(l[i][:2])*3600+int(l[i][2:])*60)
                        elif i+1 < len_l and l[i+1] == ':':
                            # -03:00
                            x.time = int(l[i])*3600+int(l[i+2])*60
                            i += 2
                            if i+1 < len_l and l[i+1] == ':':
                                i += 2
                                x.time += int(l[i])
                        elif len_li <= 2:
                            # -[0]3
                            x.time = (int(l[i][:2])*3600)
                        else:
                            return None
                        i += 1

                    assert i == len_l or l[i] == ','

                    i += 1

                assert i >= len_l

        except (IndexError, ValueError, AssertionError):
            return None
        
        return res


DEFAULTTZPARSER = _tzparser()
def _parsetz(tzstr):
    return DEFAULTTZPARSER.parse(tzstr)


def _parsems(value):
    """Parse a I[.F] seconds value into (seconds, microseconds)."""
    if "." not in value:
        return int(value), 0
    else:
        i, f = value.split(".")
        return int(i), int(f.ljust(6, "0")[:6])


# vim:ts=4:sw=4:et

########NEW FILE########
__FILENAME__ = relativedelta
"""
Copyright (c) 2003-2010  Gustavo Niemeyer <gustavo@niemeyer.net>

This module offers extensions to the standard python 2.3+
datetime module.
"""
__author__ = "Gustavo Niemeyer <gustavo@niemeyer.net>"
__license__ = "PSF License"

import datetime
import calendar

__all__ = ["relativedelta", "MO", "TU", "WE", "TH", "FR", "SA", "SU"]

class weekday(object):
    __slots__ = ["weekday", "n"]

    def __init__(self, weekday, n=None):
        self.weekday = weekday
        self.n = n

    def __call__(self, n):
        if n == self.n:
            return self
        else:
            return self.__class__(self.weekday, n)

    def __eq__(self, other):
        try:
            if self.weekday != other.weekday or self.n != other.n:
                return False
        except AttributeError:
            return False
        return True

    def __repr__(self):
        s = ("MO", "TU", "WE", "TH", "FR", "SA", "SU")[self.weekday]
        if not self.n:
            return s
        else:
            return "%s(%+d)" % (s, self.n)

MO, TU, WE, TH, FR, SA, SU = weekdays = tuple([weekday(x) for x in range(7)])

class relativedelta:
    """
The relativedelta type is based on the specification of the excelent
work done by M.-A. Lemburg in his mx.DateTime extension. However,
notice that this type does *NOT* implement the same algorithm as
his work. Do *NOT* expect it to behave like mx.DateTime's counterpart.

There's two different ways to build a relativedelta instance. The
first one is passing it two date/datetime classes:

    relativedelta(datetime1, datetime2)

And the other way is to use the following keyword arguments:

    year, month, day, hour, minute, second, microsecond:
        Absolute information.

    years, months, weeks, days, hours, minutes, seconds, microseconds:
        Relative information, may be negative.

    weekday:
        One of the weekday instances (MO, TU, etc). These instances may
        receive a parameter N, specifying the Nth weekday, which could
        be positive or negative (like MO(+1) or MO(-2). Not specifying
        it is the same as specifying +1. You can also use an integer,
        where 0=MO.

    leapdays:
        Will add given days to the date found, if year is a leap
        year, and the date found is post 28 of february.

    yearday, nlyearday:
        Set the yearday or the non-leap year day (jump leap days).
        These are converted to day/month/leapdays information.

Here is the behavior of operations with relativedelta:

1) Calculate the absolute year, using the 'year' argument, or the
   original datetime year, if the argument is not present.

2) Add the relative 'years' argument to the absolute year.

3) Do steps 1 and 2 for month/months.

4) Calculate the absolute day, using the 'day' argument, or the
   original datetime day, if the argument is not present. Then,
   subtract from the day until it fits in the year and month
   found after their operations.

5) Add the relative 'days' argument to the absolute day. Notice
   that the 'weeks' argument is multiplied by 7 and added to
   'days'.

6) Do steps 1 and 2 for hour/hours, minute/minutes, second/seconds,
   microsecond/microseconds.

7) If the 'weekday' argument is present, calculate the weekday,
   with the given (wday, nth) tuple. wday is the index of the
   weekday (0-6, 0=Mon), and nth is the number of weeks to add
   forward or backward, depending on its signal. Notice that if
   the calculated date is already Monday, for example, using
   (0, 1) or (0, -1) won't change the day.
    """

    def __init__(self, dt1=None, dt2=None,
                 years=0, months=0, days=0, leapdays=0, weeks=0,
                 hours=0, minutes=0, seconds=0, microseconds=0,
                 year=None, month=None, day=None, weekday=None,
                 yearday=None, nlyearday=None,
                 hour=None, minute=None, second=None, microsecond=None):
        if dt1 and dt2:
            if not isinstance(dt1, datetime.date) or \
               not isinstance(dt2, datetime.date):
                raise TypeError, "relativedelta only diffs datetime/date"
            if type(dt1) is not type(dt2):
                if not isinstance(dt1, datetime.datetime):
                    dt1 = datetime.datetime.fromordinal(dt1.toordinal())
                elif not isinstance(dt2, datetime.datetime):
                    dt2 = datetime.datetime.fromordinal(dt2.toordinal())
            self.years = 0
            self.months = 0
            self.days = 0
            self.leapdays = 0
            self.hours = 0
            self.minutes = 0
            self.seconds = 0
            self.microseconds = 0
            self.year = None
            self.month = None
            self.day = None
            self.weekday = None
            self.hour = None
            self.minute = None
            self.second = None
            self.microsecond = None
            self._has_time = 0

            months = (dt1.year*12+dt1.month)-(dt2.year*12+dt2.month)
            self._set_months(months)
            dtm = self.__radd__(dt2)
            if dt1 < dt2:
                while dt1 > dtm:
                    months += 1
                    self._set_months(months)
                    dtm = self.__radd__(dt2)
            else:
                while dt1 < dtm:
                    months -= 1
                    self._set_months(months)
                    dtm = self.__radd__(dt2)
            delta = dt1 - dtm
            self.seconds = delta.seconds+delta.days*86400
            self.microseconds = delta.microseconds
        else:
            self.years = years
            self.months = months
            self.days = days+weeks*7
            self.leapdays = leapdays
            self.hours = hours
            self.minutes = minutes
            self.seconds = seconds
            self.microseconds = microseconds
            self.year = year
            self.month = month
            self.day = day
            self.hour = hour
            self.minute = minute
            self.second = second
            self.microsecond = microsecond

            if type(weekday) is int:
                self.weekday = weekdays[weekday]
            else:
                self.weekday = weekday

            yday = 0
            if nlyearday:
                yday = nlyearday
            elif yearday:
                yday = yearday
                if yearday > 59:
                    self.leapdays = -1
            if yday:
                ydayidx = [31,59,90,120,151,181,212,243,273,304,334,366]
                for idx, ydays in enumerate(ydayidx):
                    if yday <= ydays:
                        self.month = idx+1
                        if idx == 0:
                            self.day = yday
                        else:
                            self.day = yday-ydayidx[idx-1]
                        break
                else:
                    raise ValueError, "invalid year day (%d)" % yday

        self._fix()

    def _fix(self):
        if abs(self.microseconds) > 999999:
            s = self.microseconds//abs(self.microseconds)
            div, mod = divmod(self.microseconds*s, 1000000)
            self.microseconds = mod*s
            self.seconds += div*s
        if abs(self.seconds) > 59:
            s = self.seconds//abs(self.seconds)
            div, mod = divmod(self.seconds*s, 60)
            self.seconds = mod*s
            self.minutes += div*s
        if abs(self.minutes) > 59:
            s = self.minutes//abs(self.minutes)
            div, mod = divmod(self.minutes*s, 60)
            self.minutes = mod*s
            self.hours += div*s
        if abs(self.hours) > 23:
            s = self.hours//abs(self.hours)
            div, mod = divmod(self.hours*s, 24)
            self.hours = mod*s
            self.days += div*s
        if abs(self.months) > 11:
            s = self.months//abs(self.months)
            div, mod = divmod(self.months*s, 12)
            self.months = mod*s
            self.years += div*s
        if (self.hours or self.minutes or self.seconds or self.microseconds or
            self.hour is not None or self.minute is not None or
            self.second is not None or self.microsecond is not None):
            self._has_time = 1
        else:
            self._has_time = 0

    def _set_months(self, months):
        self.months = months
        if abs(self.months) > 11:
            s = self.months//abs(self.months)
            div, mod = divmod(self.months*s, 12)
            self.months = mod*s
            self.years = div*s
        else:
            self.years = 0

    def __radd__(self, other):
        if not isinstance(other, datetime.date):
            raise TypeError, "unsupported type for add operation"
        elif self._has_time and not isinstance(other, datetime.datetime):
            other = datetime.datetime.fromordinal(other.toordinal())
        year = (self.year or other.year)+self.years
        month = self.month or other.month
        if self.months:
            assert 1 <= abs(self.months) <= 12
            month += self.months
            if month > 12:
                year += 1
                month -= 12
            elif month < 1:
                year -= 1
                month += 12
        day = min(calendar.monthrange(year, month)[1],
                  self.day or other.day)
        repl = {"year": year, "month": month, "day": day}
        for attr in ["hour", "minute", "second", "microsecond"]:
            value = getattr(self, attr)
            if value is not None:
                repl[attr] = value
        days = self.days
        if self.leapdays and month > 2 and calendar.isleap(year):
            days += self.leapdays
        ret = (other.replace(**repl)
               + datetime.timedelta(days=days,
                                    hours=self.hours,
                                    minutes=self.minutes,
                                    seconds=self.seconds,
                                    microseconds=self.microseconds))
        if self.weekday:
            weekday, nth = self.weekday.weekday, self.weekday.n or 1
            jumpdays = (abs(nth)-1)*7
            if nth > 0:
                jumpdays += (7-ret.weekday()+weekday)%7
            else:
                jumpdays += (ret.weekday()-weekday)%7
                jumpdays *= -1
            ret += datetime.timedelta(days=jumpdays)
        return ret

    def __rsub__(self, other):
        return self.__neg__().__radd__(other)

    def __add__(self, other):
        if not isinstance(other, relativedelta):
            raise TypeError, "unsupported type for add operation"
        return relativedelta(years=other.years+self.years,
                             months=other.months+self.months,
                             days=other.days+self.days,
                             hours=other.hours+self.hours,
                             minutes=other.minutes+self.minutes,
                             seconds=other.seconds+self.seconds,
                             microseconds=other.microseconds+self.microseconds,
                             leapdays=other.leapdays or self.leapdays,
                             year=other.year or self.year,
                             month=other.month or self.month,
                             day=other.day or self.day,
                             weekday=other.weekday or self.weekday,
                             hour=other.hour or self.hour,
                             minute=other.minute or self.minute,
                             second=other.second or self.second,
                             microsecond=other.second or self.microsecond)

    def __sub__(self, other):
        if not isinstance(other, relativedelta):
            raise TypeError, "unsupported type for sub operation"
        return relativedelta(years=other.years-self.years,
                             months=other.months-self.months,
                             days=other.days-self.days,
                             hours=other.hours-self.hours,
                             minutes=other.minutes-self.minutes,
                             seconds=other.seconds-self.seconds,
                             microseconds=other.microseconds-self.microseconds,
                             leapdays=other.leapdays or self.leapdays,
                             year=other.year or self.year,
                             month=other.month or self.month,
                             day=other.day or self.day,
                             weekday=other.weekday or self.weekday,
                             hour=other.hour or self.hour,
                             minute=other.minute or self.minute,
                             second=other.second or self.second,
                             microsecond=other.second or self.microsecond)

    def __neg__(self):
        return relativedelta(years=-self.years,
                             months=-self.months,
                             days=-self.days,
                             hours=-self.hours,
                             minutes=-self.minutes,
                             seconds=-self.seconds,
                             microseconds=-self.microseconds,
                             leapdays=self.leapdays,
                             year=self.year,
                             month=self.month,
                             day=self.day,
                             weekday=self.weekday,
                             hour=self.hour,
                             minute=self.minute,
                             second=self.second,
                             microsecond=self.microsecond)

    def __nonzero__(self):
        return not (not self.years and
                    not self.months and
                    not self.days and
                    not self.hours and
                    not self.minutes and
                    not self.seconds and
                    not self.microseconds and
                    not self.leapdays and
                    self.year is None and
                    self.month is None and
                    self.day is None and
                    self.weekday is None and
                    self.hour is None and
                    self.minute is None and
                    self.second is None and
                    self.microsecond is None)

    def __mul__(self, other):
        f = float(other)
        return relativedelta(years=self.years*f,
                             months=self.months*f,
                             days=self.days*f,
                             hours=self.hours*f,
                             minutes=self.minutes*f,
                             seconds=self.seconds*f,
                             microseconds=self.microseconds*f,
                             leapdays=self.leapdays,
                             year=self.year,
                             month=self.month,
                             day=self.day,
                             weekday=self.weekday,
                             hour=self.hour,
                             minute=self.minute,
                             second=self.second,
                             microsecond=self.microsecond)

    def __eq__(self, other):
        if not isinstance(other, relativedelta):
            return False
        if self.weekday or other.weekday:
            if not self.weekday or not other.weekday:
                return False
            if self.weekday.weekday != other.weekday.weekday:
                return False
            n1, n2 = self.weekday.n, other.weekday.n
            if n1 != n2 and not ((not n1 or n1 == 1) and (not n2 or n2 == 1)):
                return False
        return (self.years == other.years and
                self.months == other.months and
                self.days == other.days and
                self.hours == other.hours and
                self.minutes == other.minutes and
                self.seconds == other.seconds and
                self.leapdays == other.leapdays and
                self.year == other.year and
                self.month == other.month and
                self.day == other.day and
                self.hour == other.hour and
                self.minute == other.minute and
                self.second == other.second and
                self.microsecond == other.microsecond)

    def __ne__(self, other):
        return not self.__eq__(other)

    def __div__(self, other):
        return self.__mul__(1/float(other))

    def __repr__(self):
        l = []
        for attr in ["years", "months", "days", "leapdays",
                     "hours", "minutes", "seconds", "microseconds"]:
            value = getattr(self, attr)
            if value:
                l.append("%s=%+d" % (attr, value))
        for attr in ["year", "month", "day", "weekday",
                     "hour", "minute", "second", "microsecond"]:
            value = getattr(self, attr)
            if value is not None:
                l.append("%s=%s" % (attr, `value`))
        return "%s(%s)" % (self.__class__.__name__, ", ".join(l))

# vim:ts=4:sw=4:et

########NEW FILE########
__FILENAME__ = rrule
"""
Copyright (c) 2003-2010  Gustavo Niemeyer <gustavo@niemeyer.net>

This module offers extensions to the standard python 2.3+
datetime module.
"""
__author__ = "Gustavo Niemeyer <gustavo@niemeyer.net>"
__license__ = "PSF License"

import itertools
import datetime
import calendar
import thread
import sys

__all__ = ["rrule", "rruleset", "rrulestr",
           "YEARLY", "MONTHLY", "WEEKLY", "DAILY",
           "HOURLY", "MINUTELY", "SECONDLY",
           "MO", "TU", "WE", "TH", "FR", "SA", "SU"]

# Every mask is 7 days longer to handle cross-year weekly periods.
M366MASK = tuple([1]*31+[2]*29+[3]*31+[4]*30+[5]*31+[6]*30+
                 [7]*31+[8]*31+[9]*30+[10]*31+[11]*30+[12]*31+[1]*7)
M365MASK = list(M366MASK)
M29, M30, M31 = range(1,30), range(1,31), range(1,32)
MDAY366MASK = tuple(M31+M29+M31+M30+M31+M30+M31+M31+M30+M31+M30+M31+M31[:7])
MDAY365MASK = list(MDAY366MASK)
M29, M30, M31 = range(-29,0), range(-30,0), range(-31,0)
NMDAY366MASK = tuple(M31+M29+M31+M30+M31+M30+M31+M31+M30+M31+M30+M31+M31[:7])
NMDAY365MASK = list(NMDAY366MASK)
M366RANGE = (0,31,60,91,121,152,182,213,244,274,305,335,366)
M365RANGE = (0,31,59,90,120,151,181,212,243,273,304,334,365)
WDAYMASK = [0,1,2,3,4,5,6]*55
del M29, M30, M31, M365MASK[59], MDAY365MASK[59], NMDAY365MASK[31]
MDAY365MASK = tuple(MDAY365MASK)
M365MASK = tuple(M365MASK)

(YEARLY,
 MONTHLY,
 WEEKLY,
 DAILY,
 HOURLY,
 MINUTELY,
 SECONDLY) = range(7)

# Imported on demand.
easter = None
parser = None

class weekday(object):
    __slots__ = ["weekday", "n"]

    def __init__(self, weekday, n=None):
        if n == 0:
            raise ValueError, "Can't create weekday with n == 0"
        self.weekday = weekday
        self.n = n

    def __call__(self, n):
        if n == self.n:
            return self
        else:
            return self.__class__(self.weekday, n)

    def __eq__(self, other):
        try:
            if self.weekday != other.weekday or self.n != other.n:
                return False
        except AttributeError:
            return False
        return True

    def __repr__(self):
        s = ("MO", "TU", "WE", "TH", "FR", "SA", "SU")[self.weekday]
        if not self.n:
            return s
        else:
            return "%s(%+d)" % (s, self.n)

MO, TU, WE, TH, FR, SA, SU = weekdays = tuple([weekday(x) for x in range(7)])

class rrulebase:
    def __init__(self, cache=False):
        if cache:
            self._cache = []
            self._cache_lock = thread.allocate_lock()
            self._cache_gen  = self._iter()
            self._cache_complete = False
        else:
            self._cache = None
            self._cache_complete = False
        self._len = None

    def __iter__(self):
        if self._cache_complete:
            return iter(self._cache)
        elif self._cache is None:
            return self._iter()
        else:
            return self._iter_cached()

    def _iter_cached(self):
        i = 0
        gen = self._cache_gen
        cache = self._cache
        acquire = self._cache_lock.acquire
        release = self._cache_lock.release
        while gen:
            if i == len(cache):
                acquire()
                if self._cache_complete:
                    break
                try:
                    for j in range(10):
                        cache.append(gen.next())
                except StopIteration:
                    self._cache_gen = gen = None
                    self._cache_complete = True
                    break
                release()
            yield cache[i]
            i += 1
        while i < self._len:
            yield cache[i]
            i += 1

    def __getitem__(self, item):
        if self._cache_complete:
            return self._cache[item]
        elif isinstance(item, slice):
            if item.step and item.step < 0:
                return list(iter(self))[item]
            else:
                return list(itertools.islice(self,
                                             item.start or 0,
                                             item.stop or sys.maxint,
                                             item.step or 1))
        elif item >= 0:
            gen = iter(self)
            try:
                for i in range(item+1):
                    res = gen.next()
            except StopIteration:
                raise IndexError
            return res
        else:
            return list(iter(self))[item]

    def __contains__(self, item):
        if self._cache_complete:
            return item in self._cache
        else:
            for i in self:
                if i == item:
                    return True
                elif i > item:
                    return False
        return False

    # __len__() introduces a large performance penality.
    def count(self):
        if self._len is None:
            for x in self: pass
        return self._len

    def before(self, dt, inc=False):
        if self._cache_complete:
            gen = self._cache
        else:
            gen = self
        last = None
        if inc:
            for i in gen:
                if i > dt:
                    break
                last = i
        else:
            for i in gen:
                if i >= dt:
                    break
                last = i
        return last

    def after(self, dt, inc=False):
        if self._cache_complete:
            gen = self._cache
        else:
            gen = self
        if inc:
            for i in gen:
                if i >= dt:
                    return i
        else:
            for i in gen:
                if i > dt:
                    return i
        return None

    def between(self, after, before, inc=False):
        if self._cache_complete:
            gen = self._cache
        else:
            gen = self
        started = False
        l = []
        if inc:
            for i in gen:
                if i > before:
                    break
                elif not started:
                    if i >= after:
                        started = True
                        l.append(i)
                else:
                    l.append(i)
        else:
            for i in gen:
                if i >= before:
                    break
                elif not started:
                    if i > after:
                        started = True
                        l.append(i)
                else:
                    l.append(i)
        return l

class rrule(rrulebase):
    def __init__(self, freq, dtstart=None,
                 interval=1, wkst=None, count=None, until=None, bysetpos=None,
                 bymonth=None, bymonthday=None, byyearday=None, byeaster=None,
                 byweekno=None, byweekday=None,
                 byhour=None, byminute=None, bysecond=None,
                 cache=False):
        rrulebase.__init__(self, cache)
        global easter
        if not dtstart:
            dtstart = datetime.datetime.now().replace(microsecond=0)
        elif not isinstance(dtstart, datetime.datetime):
            dtstart = datetime.datetime.fromordinal(dtstart.toordinal())
        else:
            dtstart = dtstart.replace(microsecond=0)
        self._dtstart = dtstart
        self._tzinfo = dtstart.tzinfo
        self._freq = freq
        self._interval = interval
        self._count = count
        if until and not isinstance(until, datetime.datetime):
            until = datetime.datetime.fromordinal(until.toordinal())
        self._until = until
        if wkst is None:
            self._wkst = calendar.firstweekday()
        elif type(wkst) is int:
            self._wkst = wkst
        else:
            self._wkst = wkst.weekday
        if bysetpos is None:
            self._bysetpos = None
        elif type(bysetpos) is int:
            if bysetpos == 0 or not (-366 <= bysetpos <= 366):
                raise ValueError("bysetpos must be between 1 and 366, "
                                 "or between -366 and -1")
            self._bysetpos = (bysetpos,)
        else:
            self._bysetpos = tuple(bysetpos)
            for pos in self._bysetpos:
                if pos == 0 or not (-366 <= pos <= 366):
                    raise ValueError("bysetpos must be between 1 and 366, "
                                     "or between -366 and -1")
        if not (byweekno or byyearday or bymonthday or
                byweekday is not None or byeaster is not None):
            if freq == YEARLY:
                if not bymonth:
                    bymonth = dtstart.month
                bymonthday = dtstart.day
            elif freq == MONTHLY:
                bymonthday = dtstart.day
            elif freq == WEEKLY:
                byweekday = dtstart.weekday()
        # bymonth
        if not bymonth:
            self._bymonth = None
        elif type(bymonth) is int:
            self._bymonth = (bymonth,)
        else:
            self._bymonth = tuple(bymonth)
        # byyearday
        if not byyearday:
            self._byyearday = None
        elif type(byyearday) is int:
            self._byyearday = (byyearday,)
        else:
            self._byyearday = tuple(byyearday)
        # byeaster
        if byeaster is not None:
            if not easter:
                from dateutil import easter
            if type(byeaster) is int:
                self._byeaster = (byeaster,)
            else:
                self._byeaster = tuple(byeaster)
        else:
            self._byeaster = None
        # bymonthay
        if not bymonthday:
            self._bymonthday = ()
            self._bynmonthday = ()
        elif type(bymonthday) is int:
            if bymonthday < 0:
                self._bynmonthday = (bymonthday,)
                self._bymonthday = ()
            else:
                self._bymonthday = (bymonthday,)
                self._bynmonthday = ()
        else:
            self._bymonthday = tuple([x for x in bymonthday if x > 0])
            self._bynmonthday = tuple([x for x in bymonthday if x < 0])
        # byweekno
        if byweekno is None:
            self._byweekno = None
        elif type(byweekno) is int:
            self._byweekno = (byweekno,)
        else:
            self._byweekno = tuple(byweekno)
        # byweekday / bynweekday
        if byweekday is None:
            self._byweekday = None
            self._bynweekday = None
        elif type(byweekday) is int:
            self._byweekday = (byweekday,)
            self._bynweekday = None
        elif hasattr(byweekday, "n"):
            if not byweekday.n or freq > MONTHLY:
                self._byweekday = (byweekday.weekday,)
                self._bynweekday = None
            else:
                self._bynweekday = ((byweekday.weekday, byweekday.n),)
                self._byweekday = None
        else:
            self._byweekday = []
            self._bynweekday = []
            for wday in byweekday:
                if type(wday) is int:
                    self._byweekday.append(wday)
                elif not wday.n or freq > MONTHLY:
                    self._byweekday.append(wday.weekday)
                else:
                    self._bynweekday.append((wday.weekday, wday.n))
            self._byweekday = tuple(self._byweekday)
            self._bynweekday = tuple(self._bynweekday)
            if not self._byweekday:
                self._byweekday = None
            elif not self._bynweekday:
                self._bynweekday = None
        # byhour
        if byhour is None:
            if freq < HOURLY:
                self._byhour = (dtstart.hour,)
            else:
                self._byhour = None
        elif type(byhour) is int:
            self._byhour = (byhour,)
        else:
            self._byhour = tuple(byhour)
        # byminute
        if byminute is None:
            if freq < MINUTELY:
                self._byminute = (dtstart.minute,)
            else:
                self._byminute = None
        elif type(byminute) is int:
            self._byminute = (byminute,)
        else:
            self._byminute = tuple(byminute)
        # bysecond
        if bysecond is None:
            if freq < SECONDLY:
                self._bysecond = (dtstart.second,)
            else:
                self._bysecond = None
        elif type(bysecond) is int:
            self._bysecond = (bysecond,)
        else:
            self._bysecond = tuple(bysecond)

        if self._freq >= HOURLY:
            self._timeset = None
        else:
            self._timeset = []
            for hour in self._byhour:
                for minute in self._byminute:
                    for second in self._bysecond:
                        self._timeset.append(
                                datetime.time(hour, minute, second,
                                                    tzinfo=self._tzinfo))
            self._timeset.sort()
            self._timeset = tuple(self._timeset)

    def _iter(self):
        year, month, day, hour, minute, second, weekday, yearday, _ = \
            self._dtstart.timetuple()

        # Some local variables to speed things up a bit
        freq = self._freq
        interval = self._interval
        wkst = self._wkst
        until = self._until
        bymonth = self._bymonth
        byweekno = self._byweekno
        byyearday = self._byyearday
        byweekday = self._byweekday
        byeaster = self._byeaster
        bymonthday = self._bymonthday
        bynmonthday = self._bynmonthday
        bysetpos = self._bysetpos
        byhour = self._byhour
        byminute = self._byminute
        bysecond = self._bysecond

        ii = _iterinfo(self)
        ii.rebuild(year, month)

        getdayset = {YEARLY:ii.ydayset,
                     MONTHLY:ii.mdayset,
                     WEEKLY:ii.wdayset,
                     DAILY:ii.ddayset,
                     HOURLY:ii.ddayset,
                     MINUTELY:ii.ddayset,
                     SECONDLY:ii.ddayset}[freq]
        
        if freq < HOURLY:
            timeset = self._timeset
        else:
            gettimeset = {HOURLY:ii.htimeset,
                          MINUTELY:ii.mtimeset,
                          SECONDLY:ii.stimeset}[freq]
            if ((freq >= HOURLY and
                 self._byhour and hour not in self._byhour) or
                (freq >= MINUTELY and
                 self._byminute and minute not in self._byminute) or
                (freq >= SECONDLY and
                 self._bysecond and second not in self._bysecond)):
                timeset = ()
            else:
                timeset = gettimeset(hour, minute, second)

        total = 0
        count = self._count
        while True:
            # Get dayset with the right frequency
            dayset, start, end = getdayset(year, month, day)

            # Do the "hard" work ;-)
            filtered = False
            for i in dayset[start:end]:
                if ((bymonth and ii.mmask[i] not in bymonth) or
                    (byweekno and not ii.wnomask[i]) or
                    (byweekday and ii.wdaymask[i] not in byweekday) or
                    (ii.nwdaymask and not ii.nwdaymask[i]) or
                    (byeaster and not ii.eastermask[i]) or
                    ((bymonthday or bynmonthday) and
                     ii.mdaymask[i] not in bymonthday and
                     ii.nmdaymask[i] not in bynmonthday) or
                    (byyearday and
                     ((i < ii.yearlen and i+1 not in byyearday
                                      and -ii.yearlen+i not in byyearday) or
                      (i >= ii.yearlen and i+1-ii.yearlen not in byyearday
                                       and -ii.nextyearlen+i-ii.yearlen
                                           not in byyearday)))):
                    dayset[i] = None
                    filtered = True

            # Output results
            if bysetpos and timeset:
                poslist = []
                for pos in bysetpos:
                    if pos < 0:
                        daypos, timepos = divmod(pos, len(timeset))
                    else:
                        daypos, timepos = divmod(pos-1, len(timeset))
                    try:
                        i = [x for x in dayset[start:end]
                                if x is not None][daypos]
                        time = timeset[timepos]
                    except IndexError:
                        pass
                    else:
                        date = datetime.date.fromordinal(ii.yearordinal+i)
                        res = datetime.datetime.combine(date, time)
                        if res not in poslist:
                            poslist.append(res)
                poslist.sort()
                for res in poslist:
                    if until and res > until:
                        self._len = total
                        return
                    elif res >= self._dtstart:
                        total += 1
                        yield res
                        if count:
                            count -= 1
                            if not count:
                                self._len = total
                                return
            else:
                for i in dayset[start:end]:
                    if i is not None:
                        date = datetime.date.fromordinal(ii.yearordinal+i)
                        for time in timeset:
                            res = datetime.datetime.combine(date, time)
                            if until and res > until:
                                self._len = total
                                return
                            elif res >= self._dtstart:
                                total += 1
                                yield res
                                if count:
                                    count -= 1
                                    if not count:
                                        self._len = total
                                        return

            # Handle frequency and interval
            fixday = False
            if freq == YEARLY:
                year += interval
                if year > datetime.MAXYEAR:
                    self._len = total
                    return
                ii.rebuild(year, month)
            elif freq == MONTHLY:
                month += interval
                if month > 12:
                    div, mod = divmod(month, 12)
                    month = mod
                    year += div
                    if month == 0:
                        month = 12
                        year -= 1
                    if year > datetime.MAXYEAR:
                        self._len = total
                        return
                ii.rebuild(year, month)
            elif freq == WEEKLY:
                if wkst > weekday:
                    day += -(weekday+1+(6-wkst))+self._interval*7
                else:
                    day += -(weekday-wkst)+self._interval*7
                weekday = wkst
                fixday = True
            elif freq == DAILY:
                day += interval
                fixday = True
            elif freq == HOURLY:
                if filtered:
                    # Jump to one iteration before next day
                    hour += ((23-hour)//interval)*interval
                while True:
                    hour += interval
                    div, mod = divmod(hour, 24)
                    if div:
                        hour = mod
                        day += div
                        fixday = True
                    if not byhour or hour in byhour:
                        break
                timeset = gettimeset(hour, minute, second)
            elif freq == MINUTELY:
                if filtered:
                    # Jump to one iteration before next day
                    minute += ((1439-(hour*60+minute))//interval)*interval
                while True:
                    minute += interval
                    div, mod = divmod(minute, 60)
                    if div:
                        minute = mod
                        hour += div
                        div, mod = divmod(hour, 24)
                        if div:
                            hour = mod
                            day += div
                            fixday = True
                            filtered = False
                    if ((not byhour or hour in byhour) and
                        (not byminute or minute in byminute)):
                        break
                timeset = gettimeset(hour, minute, second)
            elif freq == SECONDLY:
                if filtered:
                    # Jump to one iteration before next day
                    second += (((86399-(hour*3600+minute*60+second))
                                //interval)*interval)
                while True:
                    second += self._interval
                    div, mod = divmod(second, 60)
                    if div:
                        second = mod
                        minute += div
                        div, mod = divmod(minute, 60)
                        if div:
                            minute = mod
                            hour += div
                            div, mod = divmod(hour, 24)
                            if div:
                                hour = mod
                                day += div
                                fixday = True
                    if ((not byhour or hour in byhour) and
                        (not byminute or minute in byminute) and
                        (not bysecond or second in bysecond)):
                        break
                timeset = gettimeset(hour, minute, second)

            if fixday and day > 28:
                daysinmonth = calendar.monthrange(year, month)[1]
                if day > daysinmonth:
                    while day > daysinmonth:
                        day -= daysinmonth
                        month += 1
                        if month == 13:
                            month = 1
                            year += 1
                            if year > datetime.MAXYEAR:
                                self._len = total
                                return
                        daysinmonth = calendar.monthrange(year, month)[1]
                    ii.rebuild(year, month)

class _iterinfo(object):
    __slots__ = ["rrule", "lastyear", "lastmonth",
                 "yearlen", "nextyearlen", "yearordinal", "yearweekday",
                 "mmask", "mrange", "mdaymask", "nmdaymask",
                 "wdaymask", "wnomask", "nwdaymask", "eastermask"]

    def __init__(self, rrule):
        for attr in self.__slots__:
            setattr(self, attr, None)
        self.rrule = rrule

    def rebuild(self, year, month):
        # Every mask is 7 days longer to handle cross-year weekly periods.
        rr = self.rrule
        if year != self.lastyear:
            self.yearlen = 365+calendar.isleap(year)
            self.nextyearlen = 365+calendar.isleap(year+1)
            firstyday = datetime.date(year, 1, 1)
            self.yearordinal = firstyday.toordinal()
            self.yearweekday = firstyday.weekday()

            wday = datetime.date(year, 1, 1).weekday()
            if self.yearlen == 365:
                self.mmask = M365MASK
                self.mdaymask = MDAY365MASK
                self.nmdaymask = NMDAY365MASK
                self.wdaymask = WDAYMASK[wday:]
                self.mrange = M365RANGE
            else:
                self.mmask = M366MASK
                self.mdaymask = MDAY366MASK
                self.nmdaymask = NMDAY366MASK
                self.wdaymask = WDAYMASK[wday:]
                self.mrange = M366RANGE

            if not rr._byweekno:
                self.wnomask = None
            else:
                self.wnomask = [0]*(self.yearlen+7)
                #no1wkst = firstwkst = self.wdaymask.index(rr._wkst)
                no1wkst = firstwkst = (7-self.yearweekday+rr._wkst)%7
                if no1wkst >= 4:
                    no1wkst = 0
                    # Number of days in the year, plus the days we got
                    # from last year.
                    wyearlen = self.yearlen+(self.yearweekday-rr._wkst)%7
                else:
                    # Number of days in the year, minus the days we
                    # left in last year.
                    wyearlen = self.yearlen-no1wkst
                div, mod = divmod(wyearlen, 7)
                numweeks = div+mod//4
                for n in rr._byweekno:
                    if n < 0:
                        n += numweeks+1
                    if not (0 < n <= numweeks):
                        continue
                    if n > 1:
                        i = no1wkst+(n-1)*7
                        if no1wkst != firstwkst:
                            i -= 7-firstwkst
                    else:
                        i = no1wkst
                    for j in range(7):
                        self.wnomask[i] = 1
                        i += 1
                        if self.wdaymask[i] == rr._wkst:
                            break
                if 1 in rr._byweekno:
                    # Check week number 1 of next year as well
                    # TODO: Check -numweeks for next year.
                    i = no1wkst+numweeks*7
                    if no1wkst != firstwkst:
                        i -= 7-firstwkst
                    if i < self.yearlen:
                        # If week starts in next year, we
                        # don't care about it.
                        for j in range(7):
                            self.wnomask[i] = 1
                            i += 1
                            if self.wdaymask[i] == rr._wkst:
                                break
                if no1wkst:
                    # Check last week number of last year as
                    # well. If no1wkst is 0, either the year
                    # started on week start, or week number 1
                    # got days from last year, so there are no
                    # days from last year's last week number in
                    # this year.
                    if -1 not in rr._byweekno:
                        lyearweekday = datetime.date(year-1,1,1).weekday()
                        lno1wkst = (7-lyearweekday+rr._wkst)%7
                        lyearlen = 365+calendar.isleap(year-1)
                        if lno1wkst >= 4:
                            lno1wkst = 0
                            lnumweeks = 52+(lyearlen+
                                           (lyearweekday-rr._wkst)%7)%7//4
                        else:
                            lnumweeks = 52+(self.yearlen-no1wkst)%7//4
                    else:
                        lnumweeks = -1
                    if lnumweeks in rr._byweekno:
                        for i in range(no1wkst):
                            self.wnomask[i] = 1

        if (rr._bynweekday and
            (month != self.lastmonth or year != self.lastyear)):
            ranges = []
            if rr._freq == YEARLY:
                if rr._bymonth:
                    for month in rr._bymonth:
                        ranges.append(self.mrange[month-1:month+1])
                else:
                    ranges = [(0, self.yearlen)]
            elif rr._freq == MONTHLY:
                ranges = [self.mrange[month-1:month+1]]
            if ranges:
                # Weekly frequency won't get here, so we may not
                # care about cross-year weekly periods.
                self.nwdaymask = [0]*self.yearlen
                for first, last in ranges:
                    last -= 1
                    for wday, n in rr._bynweekday:
                        if n < 0:
                            i = last+(n+1)*7
                            i -= (self.wdaymask[i]-wday)%7
                        else:
                            i = first+(n-1)*7
                            i += (7-self.wdaymask[i]+wday)%7
                        if first <= i <= last:
                            self.nwdaymask[i] = 1

        if rr._byeaster:
            self.eastermask = [0]*(self.yearlen+7)
            eyday = easter.easter(year).toordinal()-self.yearordinal
            for offset in rr._byeaster:
                self.eastermask[eyday+offset] = 1

        self.lastyear = year
        self.lastmonth = month

    def ydayset(self, year, month, day):
        return range(self.yearlen), 0, self.yearlen

    def mdayset(self, year, month, day):
        set = [None]*self.yearlen
        start, end = self.mrange[month-1:month+1]
        for i in range(start, end):
            set[i] = i
        return set, start, end

    def wdayset(self, year, month, day):
        # We need to handle cross-year weeks here.
        set = [None]*(self.yearlen+7)
        i = datetime.date(year, month, day).toordinal()-self.yearordinal
        start = i
        for j in range(7):
            set[i] = i
            i += 1
            #if (not (0 <= i < self.yearlen) or
            #    self.wdaymask[i] == self.rrule._wkst):
            # This will cross the year boundary, if necessary.
            if self.wdaymask[i] == self.rrule._wkst:
                break
        return set, start, i

    def ddayset(self, year, month, day):
        set = [None]*self.yearlen
        i = datetime.date(year, month, day).toordinal()-self.yearordinal
        set[i] = i
        return set, i, i+1

    def htimeset(self, hour, minute, second):
        set = []
        rr = self.rrule
        for minute in rr._byminute:
            for second in rr._bysecond:
                set.append(datetime.time(hour, minute, second,
                                         tzinfo=rr._tzinfo))
        set.sort()
        return set

    def mtimeset(self, hour, minute, second):
        set = []
        rr = self.rrule
        for second in rr._bysecond:
            set.append(datetime.time(hour, minute, second, tzinfo=rr._tzinfo))
        set.sort()
        return set

    def stimeset(self, hour, minute, second):
        return (datetime.time(hour, minute, second,
                tzinfo=self.rrule._tzinfo),)


class rruleset(rrulebase):

    class _genitem:
        def __init__(self, genlist, gen):
            try:
                self.dt = gen()
                genlist.append(self)
            except StopIteration:
                pass
            self.genlist = genlist
            self.gen = gen

        def next(self):
            try:
                self.dt = self.gen()
            except StopIteration:
                self.genlist.remove(self)

        def __cmp__(self, other):
            return cmp(self.dt, other.dt)

    def __init__(self, cache=False):
        rrulebase.__init__(self, cache)
        self._rrule = []
        self._rdate = []
        self._exrule = []
        self._exdate = []

    def rrule(self, rrule):
        self._rrule.append(rrule)
    
    def rdate(self, rdate):
        self._rdate.append(rdate)

    def exrule(self, exrule):
        self._exrule.append(exrule)

    def exdate(self, exdate):
        self._exdate.append(exdate)

    def _iter(self):
        rlist = []
        self._rdate.sort()
        self._genitem(rlist, iter(self._rdate).next)
        for gen in [iter(x).next for x in self._rrule]:
            self._genitem(rlist, gen)
        rlist.sort()
        exlist = []
        self._exdate.sort()
        self._genitem(exlist, iter(self._exdate).next)
        for gen in [iter(x).next for x in self._exrule]:
            self._genitem(exlist, gen)
        exlist.sort()
        lastdt = None
        total = 0
        while rlist:
            ritem = rlist[0]
            if not lastdt or lastdt != ritem.dt:
                while exlist and exlist[0] < ritem:
                    exlist[0].next()
                    exlist.sort()
                if not exlist or ritem != exlist[0]:
                    total += 1
                    yield ritem.dt
                lastdt = ritem.dt
            ritem.next()
            rlist.sort()
        self._len = total

class _rrulestr:

    _freq_map = {"YEARLY": YEARLY,
                 "MONTHLY": MONTHLY,
                 "WEEKLY": WEEKLY,
                 "DAILY": DAILY,
                 "HOURLY": HOURLY,
                 "MINUTELY": MINUTELY,
                 "SECONDLY": SECONDLY}

    _weekday_map = {"MO":0,"TU":1,"WE":2,"TH":3,"FR":4,"SA":5,"SU":6}

    def _handle_int(self, rrkwargs, name, value, **kwargs):
        rrkwargs[name.lower()] = int(value)

    def _handle_int_list(self, rrkwargs, name, value, **kwargs):
        rrkwargs[name.lower()] = [int(x) for x in value.split(',')]

    _handle_INTERVAL   = _handle_int
    _handle_COUNT      = _handle_int
    _handle_BYSETPOS   = _handle_int_list
    _handle_BYMONTH    = _handle_int_list
    _handle_BYMONTHDAY = _handle_int_list
    _handle_BYYEARDAY  = _handle_int_list
    _handle_BYEASTER   = _handle_int_list
    _handle_BYWEEKNO   = _handle_int_list
    _handle_BYHOUR     = _handle_int_list
    _handle_BYMINUTE   = _handle_int_list
    _handle_BYSECOND   = _handle_int_list

    def _handle_FREQ(self, rrkwargs, name, value, **kwargs):
        rrkwargs["freq"] = self._freq_map[value]

    def _handle_UNTIL(self, rrkwargs, name, value, **kwargs):
        global parser
        if not parser:
            from dateutil import parser
        try:
            rrkwargs["until"] = parser.parse(value,
                                           ignoretz=kwargs.get("ignoretz"),
                                           tzinfos=kwargs.get("tzinfos"))
        except ValueError:
            raise ValueError, "invalid until date"

    def _handle_WKST(self, rrkwargs, name, value, **kwargs):
        rrkwargs["wkst"] = self._weekday_map[value]

    def _handle_BYWEEKDAY(self, rrkwargs, name, value, **kwarsg):
        l = []
        for wday in value.split(','):
            for i in range(len(wday)):
                if wday[i] not in '+-0123456789':
                    break
            n = wday[:i] or None
            w = wday[i:]
            if n: n = int(n)
            l.append(weekdays[self._weekday_map[w]](n))
        rrkwargs["byweekday"] = l

    _handle_BYDAY = _handle_BYWEEKDAY

    def _parse_rfc_rrule(self, line,
                         dtstart=None,
                         cache=False,
                         ignoretz=False,
                         tzinfos=None):
        if line.find(':') != -1:
            name, value = line.split(':')
            if name != "RRULE":
                raise ValueError, "unknown parameter name"
        else:
            value = line
        rrkwargs = {}
        for pair in value.split(';'):
            name, value = pair.split('=')
            name = name.upper()
            value = value.upper()
            try:
                getattr(self, "_handle_"+name)(rrkwargs, name, value,
                                               ignoretz=ignoretz,
                                               tzinfos=tzinfos)
            except AttributeError:
                raise ValueError, "unknown parameter '%s'" % name
            except (KeyError, ValueError):
                raise ValueError, "invalid '%s': %s" % (name, value)
        return rrule(dtstart=dtstart, cache=cache, **rrkwargs)

    def _parse_rfc(self, s,
                   dtstart=None,
                   cache=False,
                   unfold=False,
                   forceset=False,
                   compatible=False,
                   ignoretz=False,
                   tzinfos=None):
        global parser
        if compatible:
            forceset = True
            unfold = True
        s = s.upper()
        if not s.strip():
            raise ValueError, "empty string"
        if unfold:
            lines = s.splitlines()
            i = 0
            while i < len(lines):
                line = lines[i].rstrip()
                if not line:
                    del lines[i]
                elif i > 0 and line[0] == " ":
                    lines[i-1] += line[1:]
                    del lines[i]
                else:
                    i += 1
        else:
            lines = s.split()
        if (not forceset and len(lines) == 1 and
            (s.find(':') == -1 or s.startswith('RRULE:'))):
            return self._parse_rfc_rrule(lines[0], cache=cache,
                                         dtstart=dtstart, ignoretz=ignoretz,
                                         tzinfos=tzinfos)
        else:
            rrulevals = []
            rdatevals = []
            exrulevals = []
            exdatevals = []
            for line in lines:
                if not line:
                    continue
                if line.find(':') == -1:
                    name = "RRULE"
                    value = line
                else:
                    name, value = line.split(':', 1)
                parms = name.split(';')
                if not parms:
                    raise ValueError, "empty property name"
                name = parms[0]
                parms = parms[1:]
                if name == "RRULE":
                    for parm in parms:
                        raise ValueError, "unsupported RRULE parm: "+parm
                    rrulevals.append(value)
                elif name == "RDATE":
                    for parm in parms:
                        if parm != "VALUE=DATE-TIME":
                            raise ValueError, "unsupported RDATE parm: "+parm
                    rdatevals.append(value)
                elif name == "EXRULE":
                    for parm in parms:
                        raise ValueError, "unsupported EXRULE parm: "+parm
                    exrulevals.append(value)
                elif name == "EXDATE":
                    for parm in parms:
                        if parm != "VALUE=DATE-TIME":
                            raise ValueError, "unsupported RDATE parm: "+parm
                    exdatevals.append(value)
                elif name == "DTSTART":
                    for parm in parms:
                        raise ValueError, "unsupported DTSTART parm: "+parm
                    if not parser:
                        from dateutil import parser
                    dtstart = parser.parse(value, ignoretz=ignoretz,
                                           tzinfos=tzinfos)
                else:
                    raise ValueError, "unsupported property: "+name
            if (forceset or len(rrulevals) > 1 or
                rdatevals or exrulevals or exdatevals):
                if not parser and (rdatevals or exdatevals):
                    from dateutil import parser
                set = rruleset(cache=cache)
                for value in rrulevals:
                    set.rrule(self._parse_rfc_rrule(value, dtstart=dtstart,
                                                    ignoretz=ignoretz,
                                                    tzinfos=tzinfos))
                for value in rdatevals:
                    for datestr in value.split(','):
                        set.rdate(parser.parse(datestr,
                                               ignoretz=ignoretz,
                                               tzinfos=tzinfos))
                for value in exrulevals:
                    set.exrule(self._parse_rfc_rrule(value, dtstart=dtstart,
                                                     ignoretz=ignoretz,
                                                     tzinfos=tzinfos))
                for value in exdatevals:
                    for datestr in value.split(','):
                        set.exdate(parser.parse(datestr,
                                                ignoretz=ignoretz,
                                                tzinfos=tzinfos))
                if compatible and dtstart:
                    set.rdate(dtstart)
                return set
            else:
                return self._parse_rfc_rrule(rrulevals[0],
                                             dtstart=dtstart,
                                             cache=cache,
                                             ignoretz=ignoretz,
                                             tzinfos=tzinfos)

    def __call__(self, s, **kwargs):
        return self._parse_rfc(s, **kwargs)

rrulestr = _rrulestr()

# vim:ts=4:sw=4:et

########NEW FILE########
__FILENAME__ = tz
"""
Copyright (c) 2003-2007  Gustavo Niemeyer <gustavo@niemeyer.net>

This module offers extensions to the standard python 2.3+
datetime module.
"""
__author__ = "Gustavo Niemeyer <gustavo@niemeyer.net>"
__license__ = "PSF License"

import datetime
import struct
import time
import sys
import os

relativedelta = None
parser = None
rrule = None

__all__ = ["tzutc", "tzoffset", "tzlocal", "tzfile", "tzrange",
           "tzstr", "tzical", "tzwin", "tzwinlocal", "gettz"]

try:
    from dateutil.tzwin import tzwin, tzwinlocal
except (ImportError, OSError):
    tzwin, tzwinlocal = None, None

ZERO = datetime.timedelta(0)
EPOCHORDINAL = datetime.datetime.utcfromtimestamp(0).toordinal()

class tzutc(datetime.tzinfo):

    def utcoffset(self, dt):
        return ZERO
     
    def dst(self, dt):
        return ZERO

    def tzname(self, dt):
        return "UTC"

    def __eq__(self, other):
        return (isinstance(other, tzutc) or
                (isinstance(other, tzoffset) and other._offset == ZERO))

    def __ne__(self, other):
        return not self.__eq__(other)

    def __repr__(self):
        return "%s()" % self.__class__.__name__

    __reduce__ = object.__reduce__

class tzoffset(datetime.tzinfo):

    def __init__(self, name, offset):
        self._name = name
        self._offset = datetime.timedelta(seconds=offset)

    def utcoffset(self, dt):
        return self._offset

    def dst(self, dt):
        return ZERO

    def tzname(self, dt):
        return self._name

    def __eq__(self, other):
        return (isinstance(other, tzoffset) and
                self._offset == other._offset)

    def __ne__(self, other):
        return not self.__eq__(other)

    def __repr__(self):
        return "%s(%s, %s)" % (self.__class__.__name__,
                               `self._name`,
                               self._offset.days*86400+self._offset.seconds)

    __reduce__ = object.__reduce__

class tzlocal(datetime.tzinfo):

    _std_offset = datetime.timedelta(seconds=-time.timezone)
    if time.daylight:
        _dst_offset = datetime.timedelta(seconds=-time.altzone)
    else:
        _dst_offset = _std_offset

    def utcoffset(self, dt):
        if self._isdst(dt):
            return self._dst_offset
        else:
            return self._std_offset

    def dst(self, dt):
        if self._isdst(dt):
            return self._dst_offset-self._std_offset
        else:
            return ZERO

    def tzname(self, dt):
        return time.tzname[self._isdst(dt)]

    def _isdst(self, dt):
        # We can't use mktime here. It is unstable when deciding if
        # the hour near to a change is DST or not.
        # 
        # timestamp = time.mktime((dt.year, dt.month, dt.day, dt.hour,
        #                         dt.minute, dt.second, dt.weekday(), 0, -1))
        # return time.localtime(timestamp).tm_isdst
        #
        # The code above yields the following result:
        #
        #>>> import tz, datetime
        #>>> t = tz.tzlocal()
        #>>> datetime.datetime(2003,2,15,23,tzinfo=t).tzname()
        #'BRDT'
        #>>> datetime.datetime(2003,2,16,0,tzinfo=t).tzname()
        #'BRST'
        #>>> datetime.datetime(2003,2,15,23,tzinfo=t).tzname()
        #'BRST'
        #>>> datetime.datetime(2003,2,15,22,tzinfo=t).tzname()
        #'BRDT'
        #>>> datetime.datetime(2003,2,15,23,tzinfo=t).tzname()
        #'BRDT'
        #
        # Here is a more stable implementation:
        #
        timestamp = ((dt.toordinal() - EPOCHORDINAL) * 86400
                     + dt.hour * 3600
                     + dt.minute * 60
                     + dt.second)
        return time.localtime(timestamp+time.timezone).tm_isdst

    def __eq__(self, other):
        if not isinstance(other, tzlocal):
            return False
        return (self._std_offset == other._std_offset and
                self._dst_offset == other._dst_offset)
        return True

    def __ne__(self, other):
        return not self.__eq__(other)

    def __repr__(self):
        return "%s()" % self.__class__.__name__

    __reduce__ = object.__reduce__

class _ttinfo(object):
    __slots__ = ["offset", "delta", "isdst", "abbr", "isstd", "isgmt"]

    def __init__(self):
        for attr in self.__slots__:
            setattr(self, attr, None)

    def __repr__(self):
        l = []
        for attr in self.__slots__:
            value = getattr(self, attr)
            if value is not None:
                l.append("%s=%s" % (attr, `value`))
        return "%s(%s)" % (self.__class__.__name__, ", ".join(l))

    def __eq__(self, other):
        if not isinstance(other, _ttinfo):
            return False
        return (self.offset == other.offset and
                self.delta == other.delta and
                self.isdst == other.isdst and
                self.abbr == other.abbr and
                self.isstd == other.isstd and
                self.isgmt == other.isgmt)

    def __ne__(self, other):
        return not self.__eq__(other)

    def __getstate__(self):
        state = {}
        for name in self.__slots__:
            state[name] = getattr(self, name, None)
        return state

    def __setstate__(self, state):
        for name in self.__slots__:
            if name in state:
                setattr(self, name, state[name])

class tzfile(datetime.tzinfo):

    # http://www.twinsun.com/tz/tz-link.htm
    # ftp://elsie.nci.nih.gov/pub/tz*.tar.gz
    
    def __init__(self, fileobj):
        if isinstance(fileobj, basestring):
            self._filename = fileobj
            fileobj = open(fileobj)
        elif hasattr(fileobj, "name"):
            self._filename = fileobj.name
        else:
            self._filename = `fileobj`

        # From tzfile(5):
        #
        # The time zone information files used by tzset(3)
        # begin with the magic characters "TZif" to identify
        # them as time zone information files, followed by
        # sixteen bytes reserved for future use, followed by
        # six four-byte values of type long, written in a
        # ``standard'' byte order (the high-order  byte
        # of the value is written first).

        if fileobj.read(4) != "TZif":
            raise ValueError, "magic not found"

        fileobj.read(16)

        (
         # The number of UTC/local indicators stored in the file.
         ttisgmtcnt,

         # The number of standard/wall indicators stored in the file.
         ttisstdcnt,
         
         # The number of leap seconds for which data is
         # stored in the file.
         leapcnt,

         # The number of "transition times" for which data
         # is stored in the file.
         timecnt,

         # The number of "local time types" for which data
         # is stored in the file (must not be zero).
         typecnt,

         # The  number  of  characters  of "time zone
         # abbreviation strings" stored in the file.
         charcnt,

        ) = struct.unpack(">6l", fileobj.read(24))

        # The above header is followed by tzh_timecnt four-byte
        # values  of  type long,  sorted  in ascending order.
        # These values are written in ``standard'' byte order.
        # Each is used as a transition time (as  returned  by
        # time(2)) at which the rules for computing local time
        # change.

        if timecnt:
            self._trans_list = struct.unpack(">%dl" % timecnt,
                                             fileobj.read(timecnt*4))
        else:
            self._trans_list = []

        # Next come tzh_timecnt one-byte values of type unsigned
        # char; each one tells which of the different types of
        # ``local time'' types described in the file is associated
        # with the same-indexed transition time. These values
        # serve as indices into an array of ttinfo structures that
        # appears next in the file.
        
        if timecnt:
            self._trans_idx = struct.unpack(">%dB" % timecnt,
                                            fileobj.read(timecnt))
        else:
            self._trans_idx = []
        
        # Each ttinfo structure is written as a four-byte value
        # for tt_gmtoff  of  type long,  in  a  standard  byte
        # order, followed  by a one-byte value for tt_isdst
        # and a one-byte  value  for  tt_abbrind.   In  each
        # structure, tt_gmtoff  gives  the  number  of
        # seconds to be added to UTC, tt_isdst tells whether
        # tm_isdst should be set by  localtime(3),  and
        # tt_abbrind serves  as an index into the array of
        # time zone abbreviation characters that follow the
        # ttinfo structure(s) in the file.

        ttinfo = []

        for i in range(typecnt):
            ttinfo.append(struct.unpack(">lbb", fileobj.read(6)))

        abbr = fileobj.read(charcnt)

        # Then there are tzh_leapcnt pairs of four-byte
        # values, written in  standard byte  order;  the
        # first  value  of  each pair gives the time (as
        # returned by time(2)) at which a leap second
        # occurs;  the  second  gives the  total  number of
        # leap seconds to be applied after the given time.
        # The pairs of values are sorted in ascending order
        # by time.

        # Not used, for now
        if leapcnt:
            leap = struct.unpack(">%dl" % (leapcnt*2),
                                 fileobj.read(leapcnt*8))

        # Then there are tzh_ttisstdcnt standard/wall
        # indicators, each stored as a one-byte value;
        # they tell whether the transition times associated
        # with local time types were specified as standard
        # time or wall clock time, and are used when
        # a time zone file is used in handling POSIX-style
        # time zone environment variables.

        if ttisstdcnt:
            isstd = struct.unpack(">%db" % ttisstdcnt,
                                  fileobj.read(ttisstdcnt))

        # Finally, there are tzh_ttisgmtcnt UTC/local
        # indicators, each stored as a one-byte value;
        # they tell whether the transition times associated
        # with local time types were specified as UTC or
        # local time, and are used when a time zone file
        # is used in handling POSIX-style time zone envi-
        # ronment variables.

        if ttisgmtcnt:
            isgmt = struct.unpack(">%db" % ttisgmtcnt,
                                  fileobj.read(ttisgmtcnt))

        # ** Everything has been read **

        # Build ttinfo list
        self._ttinfo_list = []
        for i in range(typecnt):
            gmtoff, isdst, abbrind =  ttinfo[i]
            # Round to full-minutes if that's not the case. Python's
            # datetime doesn't accept sub-minute timezones. Check
            # http://python.org/sf/1447945 for some information.
            gmtoff = (gmtoff+30)//60*60
            tti = _ttinfo()
            tti.offset = gmtoff
            tti.delta = datetime.timedelta(seconds=gmtoff)
            tti.isdst = isdst
            tti.abbr = abbr[abbrind:abbr.find('\x00', abbrind)]
            tti.isstd = (ttisstdcnt > i and isstd[i] != 0)
            tti.isgmt = (ttisgmtcnt > i and isgmt[i] != 0)
            self._ttinfo_list.append(tti)

        # Replace ttinfo indexes for ttinfo objects.
        trans_idx = []
        for idx in self._trans_idx:
            trans_idx.append(self._ttinfo_list[idx])
        self._trans_idx = tuple(trans_idx)

        # Set standard, dst, and before ttinfos. before will be
        # used when a given time is before any transitions,
        # and will be set to the first non-dst ttinfo, or to
        # the first dst, if all of them are dst.
        self._ttinfo_std = None
        self._ttinfo_dst = None
        self._ttinfo_before = None
        if self._ttinfo_list:
            if not self._trans_list:
                self._ttinfo_std = self._ttinfo_first = self._ttinfo_list[0]
            else:
                for i in range(timecnt-1,-1,-1):
                    tti = self._trans_idx[i]
                    if not self._ttinfo_std and not tti.isdst:
                        self._ttinfo_std = tti
                    elif not self._ttinfo_dst and tti.isdst:
                        self._ttinfo_dst = tti
                    if self._ttinfo_std and self._ttinfo_dst:
                        break
                else:
                    if self._ttinfo_dst and not self._ttinfo_std:
                        self._ttinfo_std = self._ttinfo_dst

                for tti in self._ttinfo_list:
                    if not tti.isdst:
                        self._ttinfo_before = tti
                        break
                else:
                    self._ttinfo_before = self._ttinfo_list[0]

        # Now fix transition times to become relative to wall time.
        #
        # I'm not sure about this. In my tests, the tz source file
        # is setup to wall time, and in the binary file isstd and
        # isgmt are off, so it should be in wall time. OTOH, it's
        # always in gmt time. Let me know if you have comments
        # about this.
        laststdoffset = 0
        self._trans_list = list(self._trans_list)
        for i in range(len(self._trans_list)):
            tti = self._trans_idx[i]
            if not tti.isdst:
                # This is std time.
                self._trans_list[i] += tti.offset
                laststdoffset = tti.offset
            else:
                # This is dst time. Convert to std.
                self._trans_list[i] += laststdoffset
        self._trans_list = tuple(self._trans_list)

    def _find_ttinfo(self, dt, laststd=0):
        timestamp = ((dt.toordinal() - EPOCHORDINAL) * 86400
                     + dt.hour * 3600
                     + dt.minute * 60
                     + dt.second)
        idx = 0
        for trans in self._trans_list:
            if timestamp < trans:
                break
            idx += 1
        else:
            return self._ttinfo_std
        if idx == 0:
            return self._ttinfo_before
        if laststd:
            while idx > 0:
                tti = self._trans_idx[idx-1]
                if not tti.isdst:
                    return tti
                idx -= 1
            else:
                return self._ttinfo_std
        else:
            return self._trans_idx[idx-1]

    def utcoffset(self, dt):
        if not self._ttinfo_std:
            return ZERO
        return self._find_ttinfo(dt).delta

    def dst(self, dt):
        if not self._ttinfo_dst:
            return ZERO
        tti = self._find_ttinfo(dt)
        if not tti.isdst:
            return ZERO

        # The documentation says that utcoffset()-dst() must
        # be constant for every dt.
        return tti.delta-self._find_ttinfo(dt, laststd=1).delta

        # An alternative for that would be:
        #
        # return self._ttinfo_dst.offset-self._ttinfo_std.offset
        #
        # However, this class stores historical changes in the
        # dst offset, so I belive that this wouldn't be the right
        # way to implement this.
        
    def tzname(self, dt):
        if not self._ttinfo_std:
            return None
        return self._find_ttinfo(dt).abbr

    def __eq__(self, other):
        if not isinstance(other, tzfile):
            return False
        return (self._trans_list == other._trans_list and
                self._trans_idx == other._trans_idx and
                self._ttinfo_list == other._ttinfo_list)

    def __ne__(self, other):
        return not self.__eq__(other)


    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, `self._filename`)

    def __reduce__(self):
        if not os.path.isfile(self._filename):
            raise ValueError, "Unpickable %s class" % self.__class__.__name__
        return (self.__class__, (self._filename,))

class tzrange(datetime.tzinfo):

    def __init__(self, stdabbr, stdoffset=None,
                 dstabbr=None, dstoffset=None,
                 start=None, end=None):
        global relativedelta
        if not relativedelta:
            from dateutil import relativedelta
        self._std_abbr = stdabbr
        self._dst_abbr = dstabbr
        if stdoffset is not None:
            self._std_offset = datetime.timedelta(seconds=stdoffset)
        else:
            self._std_offset = ZERO
        if dstoffset is not None:
            self._dst_offset = datetime.timedelta(seconds=dstoffset)
        elif dstabbr and stdoffset is not None:
            self._dst_offset = self._std_offset+datetime.timedelta(hours=+1)
        else:
            self._dst_offset = ZERO
        if dstabbr and start is None:
            self._start_delta = relativedelta.relativedelta(
                    hours=+2, month=4, day=1, weekday=relativedelta.SU(+1))
        else:
            self._start_delta = start
        if dstabbr and end is None:
            self._end_delta = relativedelta.relativedelta(
                    hours=+1, month=10, day=31, weekday=relativedelta.SU(-1))
        else:
            self._end_delta = end

    def utcoffset(self, dt):
        if self._isdst(dt):
            return self._dst_offset
        else:
            return self._std_offset

    def dst(self, dt):
        if self._isdst(dt):
            return self._dst_offset-self._std_offset
        else:
            return ZERO

    def tzname(self, dt):
        if self._isdst(dt):
            return self._dst_abbr
        else:
            return self._std_abbr

    def _isdst(self, dt):
        if not self._start_delta:
            return False
        year = datetime.datetime(dt.year,1,1)
        start = year+self._start_delta
        end = year+self._end_delta
        dt = dt.replace(tzinfo=None)
        if start < end:
            return dt >= start and dt < end
        else:
            return dt >= start or dt < end

    def __eq__(self, other):
        if not isinstance(other, tzrange):
            return False
        return (self._std_abbr == other._std_abbr and
                self._dst_abbr == other._dst_abbr and
                self._std_offset == other._std_offset and
                self._dst_offset == other._dst_offset and
                self._start_delta == other._start_delta and
                self._end_delta == other._end_delta)

    def __ne__(self, other):
        return not self.__eq__(other)

    def __repr__(self):
        return "%s(...)" % self.__class__.__name__

    __reduce__ = object.__reduce__

class tzstr(tzrange):
    
    def __init__(self, s):
        global parser
        if not parser:
            from dateutil import parser
        self._s = s

        res = parser._parsetz(s)
        if res is None:
            raise ValueError, "unknown string format"

        # Here we break the compatibility with the TZ variable handling.
        # GMT-3 actually *means* the timezone -3.
        if res.stdabbr in ("GMT", "UTC"):
            res.stdoffset *= -1

        # We must initialize it first, since _delta() needs
        # _std_offset and _dst_offset set. Use False in start/end
        # to avoid building it two times.
        tzrange.__init__(self, res.stdabbr, res.stdoffset,
                         res.dstabbr, res.dstoffset,
                         start=False, end=False)

        if not res.dstabbr:
            self._start_delta = None
            self._end_delta = None
        else:
            self._start_delta = self._delta(res.start)
            if self._start_delta:
                self._end_delta = self._delta(res.end, isend=1)

    def _delta(self, x, isend=0):
        kwargs = {}
        if x.month is not None:
            kwargs["month"] = x.month
            if x.weekday is not None:
                kwargs["weekday"] = relativedelta.weekday(x.weekday, x.week)
                if x.week > 0:
                    kwargs["day"] = 1
                else:
                    kwargs["day"] = 31
            elif x.day:
                kwargs["day"] = x.day
        elif x.yday is not None:
            kwargs["yearday"] = x.yday
        elif x.jyday is not None:
            kwargs["nlyearday"] = x.jyday
        if not kwargs:
            # Default is to start on first sunday of april, and end
            # on last sunday of october.
            if not isend:
                kwargs["month"] = 4
                kwargs["day"] = 1
                kwargs["weekday"] = relativedelta.SU(+1)
            else:
                kwargs["month"] = 10
                kwargs["day"] = 31
                kwargs["weekday"] = relativedelta.SU(-1)
        if x.time is not None:
            kwargs["seconds"] = x.time
        else:
            # Default is 2AM.
            kwargs["seconds"] = 7200
        if isend:
            # Convert to standard time, to follow the documented way
            # of working with the extra hour. See the documentation
            # of the tzinfo class.
            delta = self._dst_offset-self._std_offset
            kwargs["seconds"] -= delta.seconds+delta.days*86400
        return relativedelta.relativedelta(**kwargs)

    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, `self._s`)

class _tzicalvtzcomp:
    def __init__(self, tzoffsetfrom, tzoffsetto, isdst,
                       tzname=None, rrule=None):
        self.tzoffsetfrom = datetime.timedelta(seconds=tzoffsetfrom)
        self.tzoffsetto = datetime.timedelta(seconds=tzoffsetto)
        self.tzoffsetdiff = self.tzoffsetto-self.tzoffsetfrom
        self.isdst = isdst
        self.tzname = tzname
        self.rrule = rrule

class _tzicalvtz(datetime.tzinfo):
    def __init__(self, tzid, comps=[]):
        self._tzid = tzid
        self._comps = comps
        self._cachedate = []
        self._cachecomp = []

    def _find_comp(self, dt):
        if len(self._comps) == 1:
            return self._comps[0]
        dt = dt.replace(tzinfo=None)
        try:
            return self._cachecomp[self._cachedate.index(dt)]
        except ValueError:
            pass
        lastcomp = None
        lastcompdt = None
        for comp in self._comps:
            if not comp.isdst:
                # Handle the extra hour in DST -> STD
                compdt = comp.rrule.before(dt-comp.tzoffsetdiff, inc=True)
            else:
                compdt = comp.rrule.before(dt, inc=True)
            if compdt and (not lastcompdt or lastcompdt < compdt):
                lastcompdt = compdt
                lastcomp = comp
        if not lastcomp:
            # RFC says nothing about what to do when a given
            # time is before the first onset date. We'll look for the
            # first standard component, or the first component, if
            # none is found.
            for comp in self._comps:
                if not comp.isdst:
                    lastcomp = comp
                    break
            else:
                lastcomp = comp[0]
        self._cachedate.insert(0, dt)
        self._cachecomp.insert(0, lastcomp)
        if len(self._cachedate) > 10:
            self._cachedate.pop()
            self._cachecomp.pop()
        return lastcomp

    def utcoffset(self, dt):
        return self._find_comp(dt).tzoffsetto

    def dst(self, dt):
        comp = self._find_comp(dt)
        if comp.isdst:
            return comp.tzoffsetdiff
        else:
            return ZERO

    def tzname(self, dt):
        return self._find_comp(dt).tzname

    def __repr__(self):
        return "<tzicalvtz %s>" % `self._tzid`

    __reduce__ = object.__reduce__

class tzical:
    def __init__(self, fileobj):
        global rrule
        if not rrule:
            from dateutil import rrule

        if isinstance(fileobj, basestring):
            self._s = fileobj
            fileobj = open(fileobj)
        elif hasattr(fileobj, "name"):
            self._s = fileobj.name
        else:
            self._s = `fileobj`

        self._vtz = {}

        self._parse_rfc(fileobj.read())

    def keys(self):
        return self._vtz.keys()

    def get(self, tzid=None):
        if tzid is None:
            keys = self._vtz.keys()
            if len(keys) == 0:
                raise ValueError, "no timezones defined"
            elif len(keys) > 1:
                raise ValueError, "more than one timezone available"
            tzid = keys[0]
        return self._vtz.get(tzid)

    def _parse_offset(self, s):
        s = s.strip()
        if not s:
            raise ValueError, "empty offset"
        if s[0] in ('+', '-'):
            signal = (-1,+1)[s[0]=='+']
            s = s[1:]
        else:
            signal = +1
        if len(s) == 4:
            return (int(s[:2])*3600+int(s[2:])*60)*signal
        elif len(s) == 6:
            return (int(s[:2])*3600+int(s[2:4])*60+int(s[4:]))*signal
        else:
            raise ValueError, "invalid offset: "+s

    def _parse_rfc(self, s):
        lines = s.splitlines()
        if not lines:
            raise ValueError, "empty string"

        # Unfold
        i = 0
        while i < len(lines):
            line = lines[i].rstrip()
            if not line:
                del lines[i]
            elif i > 0 and line[0] == " ":
                lines[i-1] += line[1:]
                del lines[i]
            else:
                i += 1

        tzid = None
        comps = []
        invtz = False
        comptype = None
        for line in lines:
            if not line:
                continue
            name, value = line.split(':', 1)
            parms = name.split(';')
            if not parms:
                raise ValueError, "empty property name"
            name = parms[0].upper()
            parms = parms[1:]
            if invtz:
                if name == "BEGIN":
                    if value in ("STANDARD", "DAYLIGHT"):
                        # Process component
                        pass
                    else:
                        raise ValueError, "unknown component: "+value
                    comptype = value
                    founddtstart = False
                    tzoffsetfrom = None
                    tzoffsetto = None
                    rrulelines = []
                    tzname = None
                elif name == "END":
                    if value == "VTIMEZONE":
                        if comptype:
                            raise ValueError, \
                                  "component not closed: "+comptype
                        if not tzid:
                            raise ValueError, \
                                  "mandatory TZID not found"
                        if not comps:
                            raise ValueError, \
                                  "at least one component is needed"
                        # Process vtimezone
                        self._vtz[tzid] = _tzicalvtz(tzid, comps)
                        invtz = False
                    elif value == comptype:
                        if not founddtstart:
                            raise ValueError, \
                                  "mandatory DTSTART not found"
                        if tzoffsetfrom is None:
                            raise ValueError, \
                                  "mandatory TZOFFSETFROM not found"
                        if tzoffsetto is None:
                            raise ValueError, \
                                  "mandatory TZOFFSETFROM not found"
                        # Process component
                        rr = None
                        if rrulelines:
                            rr = rrule.rrulestr("\n".join(rrulelines),
                                                compatible=True,
                                                ignoretz=True,
                                                cache=True)
                        comp = _tzicalvtzcomp(tzoffsetfrom, tzoffsetto,
                                              (comptype == "DAYLIGHT"),
                                              tzname, rr)
                        comps.append(comp)
                        comptype = None
                    else:
                        raise ValueError, \
                              "invalid component end: "+value
                elif comptype:
                    if name == "DTSTART":
                        rrulelines.append(line)
                        founddtstart = True
                    elif name in ("RRULE", "RDATE", "EXRULE", "EXDATE"):
                        rrulelines.append(line)
                    elif name == "TZOFFSETFROM":
                        if parms:
                            raise ValueError, \
                                  "unsupported %s parm: %s "%(name, parms[0])
                        tzoffsetfrom = self._parse_offset(value)
                    elif name == "TZOFFSETTO":
                        if parms:
                            raise ValueError, \
                                  "unsupported TZOFFSETTO parm: "+parms[0]
                        tzoffsetto = self._parse_offset(value)
                    elif name == "TZNAME":
                        if parms:
                            raise ValueError, \
                                  "unsupported TZNAME parm: "+parms[0]
                        tzname = value
                    elif name == "COMMENT":
                        pass
                    else:
                        raise ValueError, "unsupported property: "+name
                else:
                    if name == "TZID":
                        if parms:
                            raise ValueError, \
                                  "unsupported TZID parm: "+parms[0]
                        tzid = value
                    elif name in ("TZURL", "LAST-MODIFIED", "COMMENT"):
                        pass
                    else:
                        raise ValueError, "unsupported property: "+name
            elif name == "BEGIN" and value == "VTIMEZONE":
                tzid = None
                comps = []
                invtz = True

    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, `self._s`)

if sys.platform != "win32":
    TZFILES = ["/etc/localtime", "localtime"]
    TZPATHS = ["/usr/share/zoneinfo", "/usr/lib/zoneinfo", "/etc/zoneinfo"]
else:
    TZFILES = []
    TZPATHS = []

def gettz(name=None):
    tz = None
    if not name:
        try:
            name = os.environ["TZ"]
        except KeyError:
            pass
    if name is None or name == ":":
        for filepath in TZFILES:
            if not os.path.isabs(filepath):
                filename = filepath
                for path in TZPATHS:
                    filepath = os.path.join(path, filename)
                    if os.path.isfile(filepath):
                        break
                else:
                    continue
            if os.path.isfile(filepath):
                try:
                    tz = tzfile(filepath)
                    break
                except (IOError, OSError, ValueError):
                    pass
        else:
            tz = tzlocal()
    else:
        if name.startswith(":"):
            name = name[:-1]
        if os.path.isabs(name):
            if os.path.isfile(name):
                tz = tzfile(name)
            else:
                tz = None
        else:
            for path in TZPATHS:
                filepath = os.path.join(path, name)
                if not os.path.isfile(filepath):
                    filepath = filepath.replace(' ','_')
                    if not os.path.isfile(filepath):
                        continue
                try:
                    tz = tzfile(filepath)
                    break
                except (IOError, OSError, ValueError):
                    pass
            else:
                tz = None
                if tzwin:
                    try:
                        tz = tzwin(name)
                    except OSError:
                        pass
                if not tz:
                    from dateutil.zoneinfo import gettz
                    tz = gettz(name)
                if not tz:
                    for c in name:
                        # name must have at least one offset to be a tzstr
                        if c in "0123456789":
                            try:
                                tz = tzstr(name)
                            except ValueError:
                                pass
                            break
                    else:
                        if name in ("GMT", "UTC"):
                            tz = tzutc()
                        elif name in time.tzname:
                            tz = tzlocal()
    return tz

# vim:ts=4:sw=4:et

########NEW FILE########
__FILENAME__ = tzwin
# This code was originally contributed by Jeffrey Harris.
import datetime
import struct
import _winreg

__author__ = "Jeffrey Harris & Gustavo Niemeyer <gustavo@niemeyer.net>"

__all__ = ["tzwin", "tzwinlocal"]

ONEWEEK = datetime.timedelta(7)

TZKEYNAMENT = r"SOFTWARE\Microsoft\Windows NT\CurrentVersion\Time Zones"
TZKEYNAME9X = r"SOFTWARE\Microsoft\Windows\CurrentVersion\Time Zones"
TZLOCALKEYNAME = r"SYSTEM\CurrentControlSet\Control\TimeZoneInformation"

def _settzkeyname():
    global TZKEYNAME
    handle = _winreg.ConnectRegistry(None, _winreg.HKEY_LOCAL_MACHINE)
    try:
        _winreg.OpenKey(handle, TZKEYNAMENT).Close()
        TZKEYNAME = TZKEYNAMENT
    except WindowsError:
        TZKEYNAME = TZKEYNAME9X
    handle.Close()

_settzkeyname()

class tzwinbase(datetime.tzinfo):
    """tzinfo class based on win32's timezones available in the registry."""

    def utcoffset(self, dt):
        if self._isdst(dt):
            return datetime.timedelta(minutes=self._dstoffset)
        else:
            return datetime.timedelta(minutes=self._stdoffset)

    def dst(self, dt):
        if self._isdst(dt):
            minutes = self._dstoffset - self._stdoffset
            return datetime.timedelta(minutes=minutes)
        else:
            return datetime.timedelta(0)
        
    def tzname(self, dt):
        if self._isdst(dt):
            return self._dstname
        else:
            return self._stdname

    def list():
        """Return a list of all time zones known to the system."""
        handle = _winreg.ConnectRegistry(None, _winreg.HKEY_LOCAL_MACHINE)
        tzkey = _winreg.OpenKey(handle, TZKEYNAME)
        result = [_winreg.EnumKey(tzkey, i)
                  for i in range(_winreg.QueryInfoKey(tzkey)[0])]
        tzkey.Close()
        handle.Close()
        return result
    list = staticmethod(list)

    def display(self):
        return self._display
    
    def _isdst(self, dt):
        dston = picknthweekday(dt.year, self._dstmonth, self._dstdayofweek,
                               self._dsthour, self._dstminute,
                               self._dstweeknumber)
        dstoff = picknthweekday(dt.year, self._stdmonth, self._stddayofweek,
                                self._stdhour, self._stdminute,
                                self._stdweeknumber)
        if dston < dstoff:
            return dston <= dt.replace(tzinfo=None) < dstoff
        else:
            return not dstoff <= dt.replace(tzinfo=None) < dston


class tzwin(tzwinbase):

    def __init__(self, name):
        self._name = name

        handle = _winreg.ConnectRegistry(None, _winreg.HKEY_LOCAL_MACHINE)
        tzkey = _winreg.OpenKey(handle, "%s\%s" % (TZKEYNAME, name))
        keydict = valuestodict(tzkey)
        tzkey.Close()
        handle.Close()

        self._stdname = keydict["Std"].encode("iso-8859-1")
        self._dstname = keydict["Dlt"].encode("iso-8859-1")

        self._display = keydict["Display"]
        
        # See http://ww_winreg.jsiinc.com/SUBA/tip0300/rh0398.htm
        tup = struct.unpack("=3l16h", keydict["TZI"])
        self._stdoffset = -tup[0]-tup[1]         # Bias + StandardBias * -1
        self._dstoffset = self._stdoffset-tup[2] # + DaylightBias * -1
        
        (self._stdmonth,
         self._stddayofweek,  # Sunday = 0
         self._stdweeknumber, # Last = 5
         self._stdhour,
         self._stdminute) = tup[4:9]

        (self._dstmonth,
         self._dstdayofweek,  # Sunday = 0
         self._dstweeknumber, # Last = 5
         self._dsthour,
         self._dstminute) = tup[12:17]

    def __repr__(self):
        return "tzwin(%s)" % repr(self._name)

    def __reduce__(self):
        return (self.__class__, (self._name,))


class tzwinlocal(tzwinbase):
    
    def __init__(self):

        handle = _winreg.ConnectRegistry(None, _winreg.HKEY_LOCAL_MACHINE)

        tzlocalkey = _winreg.OpenKey(handle, TZLOCALKEYNAME)
        keydict = valuestodict(tzlocalkey)
        tzlocalkey.Close()

        self._stdname = keydict["StandardName"].encode("iso-8859-1")
        self._dstname = keydict["DaylightName"].encode("iso-8859-1")

        try:
            tzkey = _winreg.OpenKey(handle, "%s\%s"%(TZKEYNAME, self._stdname))
            _keydict = valuestodict(tzkey)
            self._display = _keydict["Display"]
            tzkey.Close()
        except OSError:
            self._display = None

        handle.Close()
        
        self._stdoffset = -keydict["Bias"]-keydict["StandardBias"]
        self._dstoffset = self._stdoffset-keydict["DaylightBias"]


        # See http://ww_winreg.jsiinc.com/SUBA/tip0300/rh0398.htm
        tup = struct.unpack("=8h", keydict["StandardStart"])

        (self._stdmonth,
         self._stddayofweek,  # Sunday = 0
         self._stdweeknumber, # Last = 5
         self._stdhour,
         self._stdminute) = tup[1:6]

        tup = struct.unpack("=8h", keydict["DaylightStart"])

        (self._dstmonth,
         self._dstdayofweek,  # Sunday = 0
         self._dstweeknumber, # Last = 5
         self._dsthour,
         self._dstminute) = tup[1:6]

    def __reduce__(self):
        return (self.__class__, ())

def picknthweekday(year, month, dayofweek, hour, minute, whichweek):
    """dayofweek == 0 means Sunday, whichweek 5 means last instance"""
    first = datetime.datetime(year, month, 1, hour, minute)
    weekdayone = first.replace(day=((dayofweek-first.isoweekday())%7+1))
    for n in xrange(whichweek):
        dt = weekdayone+(whichweek-n)*ONEWEEK
        if dt.month == month:
            return dt

def valuestodict(key):
    """Convert a registry key's values to a dictionary."""
    dict = {}
    size = _winreg.QueryInfoKey(key)[1]
    for i in range(size):
        data = _winreg.EnumValue(key, i)
        dict[data[0]] = data[1]
    return dict

########NEW FILE########
__FILENAME__ = gntp
import re
import hashlib
import time
import platform

__version__ = '0.1'

class BaseError(Exception):
	pass

class ParseError(BaseError):
	def gntp_error(self):
		error = GNTPError(errorcode=500,errordesc='Error parsing the message')
		return error.encode()

class AuthError(BaseError):
	def gntp_error(self):
		error = GNTPError(errorcode=400,errordesc='Error with authorization')
		return error.encode()

class UnsupportedError(BaseError):
	def gntp_error(self):
		error = GNTPError(errorcode=500,errordesc='Currently unsupported by gntp.py')
		return error.encode()

class _GNTPBase(object):
	def __init__(self,messagetype):
		self.info = {
			'version':'1.0',
			'messagetype':messagetype,
			'encryptionAlgorithmID':None
		}
		self.requiredHeaders = []
		self.headers = {}
	def add_origin_info(self):
		self.add_header('Origin-Machine-Name',platform.node())
		self.add_header('Origin-Software-Name','gntp.py')
		self.add_header('Origin-Software-Version',__version__)
		self.add_header('Origin-Platform-Name',platform.system())
		self.add_header('Origin-Platform-Version',platform.platform())
	def send(self):
		print self.encode()
	def __str__(self):
		return self.encode()
	def parse_info(self,data):
		'''
		Parse the first line of a GNTP message to get security and other info values
		@param data: GNTP Message
		@return: GNTP Message information in a dictionary
		'''
		#GNTP/<version> <messagetype> <encryptionAlgorithmID>[:<ivValue>][ <keyHashAlgorithmID>:<keyHash>.<salt>]
		match = re.match('GNTP/(?P<version>\d+\.\d+) (?P<messagetype>REGISTER|NOTIFY|SUBSCRIBE|\-OK|\-ERROR)'+
						' (?P<encryptionAlgorithmID>[A-Z0-9]+(:(?P<ivValue>[A-F0-9]+))?) ?'+
						'((?P<keyHashAlgorithmID>[A-Z0-9]+):(?P<keyHash>[A-F0-9]+).(?P<salt>[A-F0-9]+))?\r\n', data,re.IGNORECASE)
		
		if not match:
			raise ParseError('ERROR_PARSING_INFO_LINE')
		
		info = match.groupdict()
		if info['encryptionAlgorithmID'] == 'NONE':
			info['encryptionAlgorithmID'] = None
		
		return info
	def set_password(self,password,encryptAlgo='MD5'):
		'''
		Set a password for a GNTP Message
		@param password:  Null to clear password
		@param encryptAlgo: Currently only supports MD5
		@todo: Support other hash functions
		'''
		self.password = password
		if not password:
			self.info['encryptionAlgorithmID'] = None
			self.info['keyHashAlgorithm'] = None;
			return
		password = password.encode('utf8')
		seed = time.ctime()
		salt = hashlib.md5(seed).hexdigest()
		saltHash = hashlib.md5(seed).digest()
		keyBasis = password+saltHash
		key = hashlib.md5(keyBasis).digest()
		keyHash = hashlib.md5(key).hexdigest()
				
		self.info['keyHashAlgorithmID'] = encryptAlgo.upper()
		self.info['keyHash'] = keyHash.upper()
		self.info['salt'] = salt.upper()
	def _decode_hex(self,value):
		'''
		Helper function to decode hex string to `proper` hex string
		@param value: Value to decode
		@return: Hex string
		'''
		result = ''
		for i in range(0,len(value),2):
			tmp = int(value[i:i+2],16)
			result += chr(tmp)
		return result
	def _decode_binary(self,rawIdentifier,identifier):
		rawIdentifier += '\r\n\r\n'
		dataLength = int(identifier['Length'])
		pointerStart = self.raw.find(rawIdentifier)+len(rawIdentifier)
		pointerEnd = pointerStart + dataLength
		data = self.raw[pointerStart:pointerEnd]
		if not len(data) == dataLength:
			raise ParseError('INVALID_DATA_LENGTH Expected: %s Recieved %s'%(dataLength,len(data)))
		return data
	def validate_password(self,password):
		'''
		Validate GNTP Message against stored password
		'''
		self.password = password
		if password == None: raise Exception()
		keyHash = self.info.get('keyHash',None)
		if keyHash is None and self.password is None:
			return True
		if keyHash is None:
			raise AuthError('Invalid keyHash')
		if self.password is None:
			raise AuthError('Missing password')
		
		password = self.password.encode('utf8')
		saltHash = self._decode_hex(self.info['salt'])
		
		keyBasis = password+saltHash
		key = hashlib.md5(keyBasis).digest()
		keyHash = hashlib.md5(key).hexdigest()
		
		if not keyHash.upper() == self.info['keyHash'].upper():
			raise AuthError('Invalid Hash')
		return True
	def validate(self):
		'''
		Verify required headers
		'''
		for header in self.requiredHeaders:
			if not self.headers.get(header,False):
				raise ParseError('Missing Notification Header: '+header)
		
	def format_info(self):
		'''
		Generate info line for GNTP Message
		@return: Info line string
		'''
		info = u'GNTP/%s %s'%(
			self.info.get('version'),
			self.info.get('messagetype'),
		)
		if self.info.get('encryptionAlgorithmID',None):
			info += ' %s:%s'%(
				self.info.get('encryptionAlgorithmID'),
				self.info.get('ivValue'),
			)
		else:
			info+=' NONE'
		
		if self.info.get('keyHashAlgorithmID',None):
			info += ' %s:%s.%s'%(
				self.info.get('keyHashAlgorithmID'),
				self.info.get('keyHash'),
				self.info.get('salt')
			)			
		
		return info	
	def parse_dict(self,data):
		'''
		Helper function to parse blocks of GNTP headers into a dictionary
		@param data:
		@return: Dictionary of headers
		'''
		dict = {}
		for line in data.split('\r\n'):
			match = re.match('([\w-]+):(.+)', line)
			if not match: continue
			
			key = match.group(1).strip()
			val = match.group(2).strip()
			dict[key] = val
			#print key,'\t\t\t',val
		return dict
	def add_header(self,key,value):
		self.headers[key] = value
	def decode(self,data,password=None):
		'''
		Decode GNTP Message
		@param data:
		'''
		self.password = password
		self.raw = data
		parts = self.raw.split('\r\n\r\n')
		self.info = self.parse_info(data)
		self.headers = self.parse_dict(parts[0])
	def encode(self):
		'''
		Encode a GNTP Message
		@return: GNTP Message ready to be sent
		'''
		self.validate()
		SEP = u': '
		EOL = u'\r\n'
		
		message = self.format_info() + EOL
		#Headers
		for k,v in self.headers.iteritems():
			message += k.encode('utf8') + SEP + str(v).encode('utf8') + EOL
		
		message += EOL
		return message
class GNTPRegister(_GNTPBase):
	'''
	GNTP Registration Message
	'''
	def __init__(self,data=None,password=None):
		'''
		@param data: (Optional) See decode()
		@param password: (Optional) Password to use while encoding/decoding messages
		'''
		_GNTPBase.__init__(self,'REGISTER')
		self.notifications = []
		self.resources = {}
		
		self.requiredHeaders = [
			'Application-Name',
			'Notifications-Count'
		]
		self.requiredNotification = [
			'Notification-Name',
		]
		if data:
			self.decode(data,password)
		else:
			self.set_password(password)
			self.headers['Application-Name'] = 'pygntp'
			self.headers['Notifications-Count'] = 0
			self.add_origin_info()
	def validate(self):
		'''
		Validate required headers and validate notification headers
		'''
		for header in self.requiredHeaders:
			if not self.headers.get(header,False):
				raise ParseError('Missing Registration Header: '+header)
		for notice in self.notifications:
			for header in self.requiredNotification:
				if not notice.get(header,False):
					raise ParseError('Missing Notification Header: '+header)		
	def decode(self,data,password):
		'''
		Decode existing GNTP Registration message
		@param data: Message to decode.
		'''
		self.raw = data
		parts = self.raw.split('\r\n\r\n')
		self.info = self.parse_info(data)
		self.validate_password(password)
		self.headers = self.parse_dict(parts[0])
		
		for i,part in enumerate(parts):
			if i==0: continue  #Skip Header
			if part.strip()=='': continue
			notice = self.parse_dict(part)
			if notice.get('Notification-Name',False):
				self.notifications.append(notice)
			elif notice.get('Identifier',False):
				notice['Data'] = self._decode_binary(part,notice)
				#open('register.png','wblol').write(notice['Data'])
				self.resources[ notice.get('Identifier') ] = notice
		
	def add_notification(self,name,enabled=True):
		'''
		Add new Notification to Registration message
		@param name: Notification Name
		@param enabled: Default Notification to Enabled
		'''
		notice = {}
		notice['Notification-Name'] = name
		notice['Notification-Enabled'] = str(enabled)
			
		self.notifications.append(notice)
		self.headers['Notifications-Count'] = len(self.notifications)
	def encode(self):
		'''
		Encode a GNTP Registration Message
		@return: GNTP Registration Message ready to be sent
		'''
		self.validate()
		SEP = u': '
		EOL = u'\r\n'
		
		message = self.format_info() + EOL
		#Headers
		for k,v in self.headers.iteritems():
			message += k.encode('utf8') + SEP + str(v).encode('utf8') + EOL
		
		#Notifications
		if len(self.notifications)>0:
			for notice in self.notifications:
				message += EOL
				for k,v in notice.iteritems():
					message += k.encode('utf8') + SEP + str(v).encode('utf8') + EOL
		
		message += EOL
		return message

class GNTPNotice(_GNTPBase):
	'''
	GNTP Notification Message
	'''
	def __init__(self,data=None,app=None,name=None,title=None,password=None):
		'''
		
		@param data: (Optional) See decode()
		@param app: (Optional) Set Application-Name
		@param name: (Optional) Set Notification-Name
		@param title: (Optional) Set Notification Title
		@param password: (Optional) Password to use while encoding/decoding messages
		'''
		_GNTPBase.__init__(self,'NOTIFY')
		self.resources	= {}
		
		self.requiredHeaders = [
			'Application-Name',
			'Notification-Name',
			'Notification-Title'
		]
		if data:
			self.decode(data,password)
		else:
			self.set_password(password)
			if app:
				self.headers['Application-Name'] = app
			if name:
				self.headers['Notification-Name'] = name
			if title:
				self.headers['Notification-Title'] = title
			self.add_origin_info()
	def decode(self,data,password):
		'''
		Decode existing GNTP Notification message
		@param data: Message to decode.
		'''
		self.raw = data
		parts = self.raw.split('\r\n\r\n')
		self.info = self.parse_info(data)
		self.validate_password(password)
		self.headers = self.parse_dict(parts[0])
		
		for i,part in enumerate(parts):
			if i==0: continue  #Skip Header
			if part.strip()=='': continue
			notice = self.parse_dict(part)
			if notice.get('Identifier',False):
				notice['Data'] = self._decode_binary(part,notice)
				#open('notice.png','wblol').write(notice['Data'])
				self.resources[ notice.get('Identifier') ] = notice
	def encode(self):
		'''
		Encode a GNTP Notification Message
		@return: GNTP Notification Message ready to be sent
		'''
		self.validate()
		SEP = u': '
		EOL = u'\r\n'
		
		message = self.format_info() + EOL
		#Headers
		for k,v in self.headers.iteritems():
			message += k + SEP + unicode(v) + EOL
		
		message += EOL
		return message.encode('utf-8')

class GNTPSubscribe(_GNTPBase):
	def __init__(self,data=None,password=None):
		_GNTPBase.__init__(self, 'SUBSCRIBE')
		self.requiredHeaders = [
			'Subscriber-ID',
			'Subscriber-Name',
		]
		if data:
			self.decode(data,password)
		else:
			self.set_password(password)
			self.add_origin_info()

class GNTPOK(_GNTPBase):
	def __init__(self,data=None,action=None):
		'''
		@param data: (Optional) See _GNTPResponse.decode()
		@param action: (Optional) Set type of action the OK Response is for
		'''
		_GNTPBase.__init__(self,'-OK')
		self.requiredHeaders = ['Response-Action']
		if data:
			self.decode(data)
		if action:
			self.headers['Response-Action'] = action
			self.add_origin_info()

class GNTPError(_GNTPBase):
	def __init__(self,data=None,errorcode=None,errordesc=None):
		'''
		@param data: (Optional) See _GNTPResponse.decode()
		@param errorcode: (Optional) Error code
		@param errordesc: (Optional) Error Description
		'''
		_GNTPBase.__init__(self,'-ERROR')
		self.requiredHeaders = ['Error-Code','Error-Description']
		if data:
			self.decode(data)
		if errorcode:
			self.headers['Error-Code'] = errorcode
			self.headers['Error-Description'] = errordesc
			self.add_origin_info()

def parse_gntp(data,password=None,debug=False):
	'''
	Attempt to parse a message as a GNTP message
	@param data: Message to be parsed
	@param password: Optional password to be used to verify the message
	@param debug: Print out extra debugging information
	'''
	match = re.match('GNTP/(?P<version>\d+\.\d+) (?P<messagetype>REGISTER|NOTIFY|SUBSCRIBE|\-OK|\-ERROR)',data,re.IGNORECASE)
	if not match:
		if debug:
			print '----'
			print self.data
			print '----'
		raise ParseError('INVALID_GNTP_INFO')
	info = match.groupdict()
	if info['messagetype'] == 'REGISTER':
		return GNTPRegister(data,password=password)
	elif info['messagetype'] == 'NOTIFY':
		return GNTPNotice(data,password=password)
	elif info['messagetype'] == 'SUBSCRIBE':
		return GNTPSubscribe(data,password=password)
	elif info['messagetype'] == '-OK':
		return GNTPOK(data)
	elif info['messagetype'] == '-ERROR':
		return GNTPError(data)
	if debug: print info
	raise ParseError('INVALID_GNTP_MESSAGE')

########NEW FILE########
__FILENAME__ = gntp_bridge
from gntp import *
import urllib
import Growl

def register_send(self):
	'''
	Resend a GNTP Register message to Growl running on a local OSX Machine
	'''
	print 'Sending Local Registration'
	
	#Local growls only need a list of strings
	notifications=[]
	defaultNotifications = []
	for notice in self.notifications:
		notifications.append(notice['Notification-Name'])
		if notice.get('Notification-Enabled',True):
			defaultNotifications.append(notice['Notification-Name'])
	
	appIcon = get_resource(self,'Application-Icon')
	
	growl = Growl.GrowlNotifier(
		applicationName			= self.headers['Application-Name'],
		notifications			= notifications,
		defaultNotifications	= defaultNotifications,
		applicationIcon			= appIcon,
	)
	growl.register()
	return self.encode()
	
def notice_send(self):
	'''
	Resend a GNTP Notify message to Growl running on a local OSX Machine
	'''
	print 'Sending Local Notification'
	growl = Growl.GrowlNotifier(
		applicationName			= self.headers['Application-Name'],
		notifications			= [self.headers['Notification-Name']]
	)
	
	noticeIcon = get_resource(self,'Notification-Icon')
	
	growl.notify(
		noteType = self.headers['Notification-Name'],
		title = self.headers['Notification-Title'],
		description=self.headers.get('Notification-Text',''),
		icon=noticeIcon
	)
	return self.encode()

def get_resource(self,key):
	try:
		resource = self.headers.get(key,'')
		if resource.startswith('x-growl-resource://'):
			resource = resource.split('://')
			return self.resources.get(resource[1])['Data']
		elif resource.startswith('http'):
			resource = resource.replace(' ', '%20')
			icon = urllib.urlopen(resource)
			return icon.read()
		else:
			return None
	except Exception,e:
		print e
		return None

GNTPRegister.send = register_send
GNTPNotice.send = notice_send

########NEW FILE########
__FILENAME__ = benchmark
from lib.hachoir_core.tools import humanDurationNanosec
from lib.hachoir_core.i18n import _
from math import floor
from time import time

class BenchmarkError(Exception):
    """
    Error during benchmark, use str(err) to format it as string.
    """
    def __init__(self, message):
        Exception.__init__(self,
            "Benchmark internal error: %s" % message)

class BenchmarkStat:
    """
    Benchmark statistics. This class automatically computes minimum value,
    maximum value and sum of all values.

    Methods:
    - append(value): append a value
    - getMin(): minimum value
    - getMax(): maximum value
    - getSum(): sum of all values
    - __len__(): get number of elements
    - __nonzero__(): isn't empty?
    """
    def __init__(self):
        self._values = []

    def append(self, value):
        self._values.append(value)
        try:
            self._min = min(self._min, value)
            self._max = max(self._max, value)
            self._sum += value
        except AttributeError:
            self._min = value
            self._max = value
            self._sum = value

    def __len__(self):
        return len(self._values)

    def __nonzero__(self):
        return bool(self._values)

    def getMin(self):
        return self._min

    def getMax(self):
        return self._max

    def getSum(self):
        return self._sum

class Benchmark:
    def __init__(self, max_time=5.0,
    min_count=5, max_count=None, progress_time=1.0):
        """
        Constructor:
        - max_time: Maximum wanted duration of the whole benchmark
          (default: 5 seconds, minimum: 1 second).
        - min_count: Minimum number of function calls to get good statistics
          (defaut: 5, minimum: 1).
        - progress_time: Time between each "progress" message
          (default: 1 second, minimum: 250 ms).
        - max_count: Maximum number of function calls (default: no limit).
        - verbose: Is verbose? (default: False)
        - disable_gc: Disable garbage collector? (default: False)
        """
        self.max_time = max(max_time, 1.0)
        self.min_count = max(min_count, 1)
        self.max_count = max_count
        self.progress_time = max(progress_time, 0.25)
        self.verbose = False
        self.disable_gc = False

    def formatTime(self, value):
        """
        Format a time delta to string: use humanDurationNanosec()
        """
        return humanDurationNanosec(value * 1000000000)

    def displayStat(self, stat):
        """
        Display statistics to stdout:
        - best time (minimum)
        - average time (arithmetic average)
        - worst time (maximum)
        - total time (sum)

        Use arithmetic avertage instead of geometric average because
        geometric fails if any value is zero (returns zero) and also
        because floating point multiplication lose precision with many
        values.
        """
        average = stat.getSum() / len(stat)
        values = (stat.getMin(), average, stat.getMax(), stat.getSum())
        values = tuple(self.formatTime(value) for value in values)
        print _("Benchmark: best=%s  average=%s  worst=%s  total=%s") \
            % values

    def _runOnce(self, func, args, kw):
        before = time()
        func(*args, **kw)
        after = time()
        return after - before

    def _run(self, func, args, kw):
        """
        Call func(*args, **kw) as many times as needed to get
        good statistics. Algorithm:
        - call the function once
        - compute needed number of calls
        - and then call function N times

        To compute number of calls, parameters are:
        - time of first function call
        - minimum number of calls (min_count attribute)
        - maximum test time (max_time attribute)

        Notice: The function will approximate number of calls.
        """
        # First call of the benchmark
        stat = BenchmarkStat()
        diff = self._runOnce(func, args, kw)
        best = diff
        stat.append(diff)
        total_time = diff

        # Compute needed number of calls
        count = int(floor(self.max_time / diff))
        count = max(count, self.min_count)
        if self.max_count:
            count = min(count, self.max_count)

        # Not other call? Just exit
        if count == 1:
            return stat
        estimate = diff * count
        if self.verbose:
            print _("Run benchmark: %s calls (estimate: %s)") \
                % (count, self.formatTime(estimate))

        display_progress = self.verbose and (1.0 <= estimate)
        total_count = 1
        while total_count < count:
            # Run benchmark and display each result
            if display_progress:
                print _("Result %s/%s: %s  (best: %s)") % \
                    (total_count, count,
                    self.formatTime(diff), self.formatTime(best))
            part = count - total_count

            # Will takes more than one second?
            average = total_time / total_count
            if self.progress_time < part * average:
                part = max( int(self.progress_time / average), 1)
            for index in xrange(part):
                diff = self._runOnce(func, args, kw)
                stat.append(diff)
                total_time += diff
                best = min(diff, best)
            total_count += part
        if display_progress:
            print _("Result %s/%s: %s  (best: %s)") % \
                (count, count,
                self.formatTime(diff), self.formatTime(best))
        return stat

    def validateStat(self, stat):
        """
        Check statistics and raise a BenchmarkError if they are invalid.
        Example of tests: reject empty stat, reject stat with only nul values.
        """
        if not stat:
            raise BenchmarkError("empty statistics")
        if not stat.getSum():
            raise BenchmarkError("nul statistics")

    def run(self, func, *args, **kw):
        """
        Run function func(*args, **kw), validate statistics,
        and display the result on stdout.

        Disable garbage collector if asked too.
        """

        # Disable garbarge collector is needed and if it does exist
        # (Jython 2.2 don't have it for example)
        if self.disable_gc:
            try:
                import gc
            except ImportError:
                self.disable_gc = False
        if self.disable_gc:
            gc_enabled = gc.isenabled()
            gc.disable()
        else:
            gc_enabled = False

        # Run the benchmark
        stat = self._run(func, args, kw)
        if gc_enabled:
            gc.enable()

        # Validate and display stats
        self.validateStat(stat)
        self.displayStat(stat)


########NEW FILE########
__FILENAME__ = bits
"""
Utilities to convert integers and binary strings to binary (number), binary
string, number, hexadecimal, etc.
"""

from lib.hachoir_core.endian import BIG_ENDIAN, LITTLE_ENDIAN
from lib.hachoir_core.compatibility import reversed
from itertools import chain, repeat
from struct import calcsize, unpack, error as struct_error

def swap16(value):
    """
    Swap byte between big and little endian of a 16 bits integer.

    >>> "%x" % swap16(0x1234)
    '3412'
    """
    return (value & 0xFF) << 8 | (value >> 8)

def swap32(value):
    """
    Swap byte between big and little endian of a 32 bits integer.

    >>> "%x" % swap32(0x12345678)
    '78563412'
    """
    value = long(value)
    return ((value & 0x000000FFL) << 24) \
         | ((value & 0x0000FF00L) << 8) \
         | ((value & 0x00FF0000L) >> 8) \
         | ((value & 0xFF000000L) >> 24)

def bin2long(text, endian):
    """
    Convert binary number written in a string into an integer.
    Skip characters differents than "0" and "1".

    >>> bin2long("110", BIG_ENDIAN)
    6
    >>> bin2long("110", LITTLE_ENDIAN)
    3
    >>> bin2long("11 00", LITTLE_ENDIAN)
    3
    """
    assert endian in (LITTLE_ENDIAN, BIG_ENDIAN)
    bits = [ (ord(character)-ord("0")) \
        for character in text if character in "01" ]
    assert len(bits) != 0
    if endian is not BIG_ENDIAN:
        bits = reversed(bits)
    value = 0
    for bit in bits:
        value *= 2
        value += bit
    return value

def str2hex(value, prefix="", glue=u"", format="%02X"):
    r"""
    Convert binary string in hexadecimal (base 16).

    >>> str2hex("ABC")
    u'414243'
    >>> str2hex("\xF0\xAF", glue=" ")
    u'F0 AF'
    >>> str2hex("ABC", prefix="0x")
    u'0x414243'
    >>> str2hex("ABC", format=r"\x%02X")
    u'\\x41\\x42\\x43'
    """
    if isinstance(glue, str):
        glue = unicode(glue)
    if 0 < len(prefix):
        text = [prefix]
    else:
        text = []
    for character in value:
        text.append(format % ord(character))
    return glue.join(text)

def countBits(value):
    """
    Count number of bits needed to store a (positive) integer number.

    >>> countBits(0)
    1
    >>> countBits(1000)
    10
    >>> countBits(44100)
    16
    >>> countBits(18446744073709551615)
    64
    """
    assert 0 <= value
    count = 1
    bits = 1
    while (1 << bits) <= value:
        count  += bits
        value >>= bits
        bits <<= 1
    while 2 <= value:
        if bits != 1:
            bits >>= 1
        else:
            bits -= 1
        while (1 << bits) <= value:
            count  += bits
            value >>= bits
    return count

def byte2bin(number, classic_mode=True):
    """
    Convert a byte (integer in 0..255 range) to a binary string.
    If classic_mode is true (default value), reverse bits.

    >>> byte2bin(10)
    '00001010'
    >>> byte2bin(10, False)
    '01010000'
    """
    text = ""
    for i in range(0, 8):
        if classic_mode:
            mask = 1 << (7-i)
        else:
            mask = 1 << i
        if (number & mask) == mask:
            text += "1"
        else:
            text += "0"
    return text

def long2raw(value, endian, size=None):
    r"""
    Convert a number (positive and not nul) to a raw string.
    If size is given, add nul bytes to fill to size bytes.

    >>> long2raw(0x1219, BIG_ENDIAN)
    '\x12\x19'
    >>> long2raw(0x1219, BIG_ENDIAN, 4)   # 32 bits
    '\x00\x00\x12\x19'
    >>> long2raw(0x1219, LITTLE_ENDIAN, 4)   # 32 bits
    '\x19\x12\x00\x00'
    """
    assert (not size and 0 < value) or (0 <= value)
    assert endian in (LITTLE_ENDIAN, BIG_ENDIAN)
    text = []
    while (value != 0 or text == ""):
        byte = value % 256
        text.append( chr(byte) )
        value >>= 8
    if size:
        need = max(size - len(text), 0)
    else:
        need = 0
    if need:
        if endian is BIG_ENDIAN:
            text = chain(repeat("\0", need), reversed(text))
        else:
            text = chain(text, repeat("\0", need))
    else:
        if endian is BIG_ENDIAN:
            text = reversed(text)
    return "".join(text)

def long2bin(size, value, endian, classic_mode=False):
    """
    Convert a number into bits (in a string):
    - size: size in bits of the number
    - value: positive (or nul) number
    - endian: BIG_ENDIAN (most important bit first)
      or LITTLE_ENDIAN (least important bit first)
    - classic_mode (default: False): reverse each packet of 8 bits

    >>> long2bin(16, 1+4 + (1+8)*256, BIG_ENDIAN)
    '10100000 10010000'
    >>> long2bin(16, 1+4 + (1+8)*256, BIG_ENDIAN, True)
    '00000101 00001001'
    >>> long2bin(16, 1+4 + (1+8)*256, LITTLE_ENDIAN)
    '00001001 00000101'
    >>> long2bin(16, 1+4 + (1+8)*256, LITTLE_ENDIAN, True)
    '10010000 10100000'
    """
    text = ""
    assert endian in (LITTLE_ENDIAN, BIG_ENDIAN)
    assert 0 <= value
    for index in xrange(size):
        if (value & 1) == 1:
            text += "1"
        else:
            text += "0"
        value >>= 1
    if endian is LITTLE_ENDIAN:
        text = text[::-1]
    result = ""
    while len(text) != 0:
        if len(result) != 0:
            result += " "
        if classic_mode:
            result += text[7::-1]
        else:
            result += text[:8]
        text = text[8:]
    return result

def str2bin(value, classic_mode=True):
    r"""
    Convert binary string to binary numbers.
    If classic_mode  is true (default value), reverse bits.

    >>> str2bin("\x03\xFF")
    '00000011 11111111'
    >>> str2bin("\x03\xFF", False)
    '11000000 11111111'
    """
    text = ""
    for character in value:
        if text != "":
            text += " "
        byte = ord(character)
        text += byte2bin(byte, classic_mode)
    return text

def _createStructFormat():
    """
    Create a dictionnary (endian, size_byte) => struct format used
    by str2long() to convert raw data to positive integer.
    """
    format = {
        BIG_ENDIAN:    {},
        LITTLE_ENDIAN: {},
    }
    for struct_format in "BHILQ":
        try:
            size = calcsize(struct_format)
            format[BIG_ENDIAN][size] = '>%s' % struct_format
            format[LITTLE_ENDIAN][size] = '<%s' % struct_format
        except struct_error:
            pass
    return format
_struct_format = _createStructFormat()

def str2long(data, endian):
    r"""
    Convert a raw data (type 'str') into a long integer.

    >>> chr(str2long('*', BIG_ENDIAN))
    '*'
    >>> str2long("\x00\x01\x02\x03", BIG_ENDIAN) == 0x10203
    True
    >>> str2long("\x2a\x10", LITTLE_ENDIAN) == 0x102a
    True
    >>> str2long("\xff\x14\x2a\x10", BIG_ENDIAN) == 0xff142a10
    True
    >>> str2long("\x00\x01\x02\x03", LITTLE_ENDIAN) == 0x3020100
    True
    >>> str2long("\xff\x14\x2a\x10\xab\x00\xd9\x0e", BIG_ENDIAN) == 0xff142a10ab00d90e
    True
    >>> str2long("\xff\xff\xff\xff\xff\xff\xff\xff", BIG_ENDIAN) == (2**64-1)
    True
    """
    assert 1 <= len(data) <= 32   # arbitrary limit: 256 bits
    try:
        return unpack(_struct_format[endian][len(data)], data)[0]
    except KeyError:
        pass

    assert endian in (BIG_ENDIAN, LITTLE_ENDIAN)
    shift = 0
    value = 0
    if endian is BIG_ENDIAN:
        data = reversed(data)
    for character in data:
        byte = ord(character)
        value += (byte << shift)
        shift += 8
    return value


########NEW FILE########
__FILENAME__ = cmd_line
from optparse import OptionGroup
from lib.hachoir_core.log import log
from lib.hachoir_core.i18n import _, getTerminalCharset
from lib.hachoir_core.tools import makePrintable
import lib.hachoir_core.config as config

def getHachoirOptions(parser):
    """
    Create an option group (type optparse.OptionGroup) of Hachoir
    library options.
    """
    def setLogFilename(*args):
        log.setFilename(args[2])

    common = OptionGroup(parser, _("Hachoir library"), \
        "Configure Hachoir library")
    common.add_option("--verbose", help=_("Verbose mode"),
        default=False, action="store_true")
    common.add_option("--log", help=_("Write log in a file"),
        type="string", action="callback", callback=setLogFilename)
    common.add_option("--quiet", help=_("Quiet mode (don't display warning)"),
        default=False, action="store_true")
    common.add_option("--debug", help=_("Debug mode"),
        default=False, action="store_true")
    return common

def configureHachoir(option):
    # Configure Hachoir using "option" (value from optparse)
    if option.quiet:
      config.quiet = True
    if option.verbose:
      config.verbose = True
    if option.debug:
      config.debug = True

def unicodeFilename(filename, charset=None):
    if not charset:
        charset = getTerminalCharset()
    try:
        return unicode(filename, charset)
    except UnicodeDecodeError:
        return makePrintable(filename, charset, to_unicode=True)


########NEW FILE########
__FILENAME__ = compatibility
"""
Compatibility constants and functions. This module works on Python 1.5 to 2.5.

This module provides:
- True and False constants ;
- any() and all() function ;
- has_yield and has_slice values ;
- isinstance() with Python 2.3 behaviour ;
- reversed() and sorted() function.


True and False constants
========================

Truth constants: True is yes (one) and False is no (zero).

>>> int(True), int(False)     # int value
(1, 0)
>>> int(False | True)         # and binary operator
1
>>> int(True & False)         # or binary operator
0
>>> int(not(True) == False)   # not binary operator
1

Warning: on Python smaller than 2.3, True and False are aliases to
number 1 and 0. So "print True" will displays 1 and not True.


any() function
==============

any() returns True if at least one items is True, or False otherwise.

>>> any([False, True])
True
>>> any([True, True])
True
>>> any([False, False])
False


all() function
==============

all() returns True if all items are True, or False otherwise.
This function is just apply binary and operator (&) on all values.

>>> all([True, True])
True
>>> all([False, True])
False
>>> all([False, False])
False


has_yield boolean
=================

has_yield: boolean which indicatese if the interpreter supports yield keyword.
yield keyworkd is available since Python 2.0.


has_yield boolean
=================

has_slice: boolean which indicates if the interpreter supports slices with step
argument or not. slice with step is available since Python 2.3.


reversed() and sorted() function
================================

reversed() and sorted() function has been introduced in Python 2.4.
It's should returns a generator, but this module it may be a list.

>>> data = list("cab")
>>> list(sorted(data))
['a', 'b', 'c']
>>> list(reversed("abc"))
['c', 'b', 'a']
"""

import copy
import operator

# --- True and False constants from Python 2.0                ---
# --- Warning: for Python < 2.3, they are aliases for 1 and 0 ---
try:
    True = True
    False = False
except NameError:
    True = 1
    False = 0

# --- any() from Python 2.5 ---
try:
    from __builtin__ import any
except ImportError:
    def any(items):
        for item in items:
            if item:
                return True
        return False

# ---all() from Python 2.5 ---
try:
    from __builtin__ import all
except ImportError:
    def all(items):
        return reduce(operator.__and__, items)

# --- test if interpreter supports yield keyword ---
try:
    eval(compile("""
from __future__ import generators

def gen():
    yield 1
    yield 2

if list(gen()) != [1, 2]:
    raise KeyError("42")
""", "<string>", "exec"))
except (KeyError, SyntaxError):
    has_yield = False
else:
    has_yield = True

# --- test if interpreter supports slices (with step argument) ---
try:
    has_slice = eval('"abc"[::-1] == "cba"')
except (TypeError, SyntaxError):
    has_slice = False

# --- isinstance with isinstance Python 2.3 behaviour (arg 2 is a type) ---
try:
    if isinstance(1, int):
        from __builtin__ import isinstance
except TypeError:
    print "Redef isinstance"
    def isinstance20(a, typea):
        if type(typea) != type(type):
            raise TypeError("TypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types")
        return type(typea) != typea
    isinstance = isinstance20

# --- reversed() from Python 2.4 ---
try:
    from __builtin__ import reversed
except ImportError:
#    if hasYield() == "ok":
#        code = """
#def reversed(data):
#    for index in xrange(len(data)-1, -1, -1):
#        yield data[index];
#reversed"""
#        reversed = eval(compile(code, "<string>", "exec"))
    if has_slice:
        def reversed(data):
            if not isinstance(data, list):
                data = list(data)
            return data[::-1]
    else:
        def reversed(data):
            if not isinstance(data, list):
                data = list(data)
            reversed_data = []
            for index in xrange(len(data)-1, -1, -1):
                reversed_data.append(data[index])
            return reversed_data

# --- sorted() from Python 2.4 ---
try:
    from __builtin__ import sorted
except ImportError:
    def sorted(data):
        sorted_data = copy.copy(data)
        sorted_data.sort()
        return sorted

__all__ = ("True", "False",
    "any", "all", "has_yield", "has_slice",
    "isinstance", "reversed", "sorted")


########NEW FILE########
__FILENAME__ = config
"""
Configuration of Hachoir
"""

import os

# UI: display options
max_string_length = 40    # Max. length in characters of GenericString.display
max_byte_length = 14      # Max. length in bytes of RawBytes.display
max_bit_length = 256      # Max. length in bits of RawBits.display
unicode_stdout = True     # Replace stdout and stderr with Unicode compatible objects
                          # Disable it for readline or ipython

# Global options
debug = False             # Display many informations usefull to debug
verbose = False           # Display more informations
quiet = False             # Don't display warnings

# Use internationalization and localization (gettext)?
if os.name == "nt":
    # TODO: Remove this hack and make i18n works on Windows :-)
    use_i18n = False
else:
    use_i18n = True

# Parser global options
autofix = True            # Enable Autofix? see hachoir_core.field.GenericFieldSet
check_padding_pattern = True   # Check padding fields pattern?


########NEW FILE########
__FILENAME__ = dict
"""
Dictionnary classes which store values order.
"""

from lib.hachoir_core.error import HachoirError
from lib.hachoir_core.i18n import _

class UniqKeyError(HachoirError):
    """
    Error raised when a value is set whereas the key already exist in a
    dictionnary.
    """
    pass

class Dict(object):
    """
    This class works like classic Python dict() but has an important method:
    __iter__() which allow to iterate into the dictionnary _values_ (and not
    keys like Python's dict does).
    """
    def __init__(self, values=None):
        self._index = {}        # key => index
        self._key_list = []     # index => key
        self._value_list = []   # index => value
        if values:
            for key, value in values:
                self.append(key,value)

    def _getValues(self):
        return self._value_list
    values = property(_getValues)

    def index(self, key):
        """
        Search a value by its key and returns its index
        Returns None if the key doesn't exist.

        >>> d=Dict( (("two", "deux"), ("one", "un")) )
        >>> d.index("two")
        0
        >>> d.index("one")
        1
        >>> d.index("three") is None
        True
        """
        return self._index.get(key)

    def __getitem__(self, key):
        """
        Get item with specified key.
        To get a value by it's index, use mydict.values[index]

        >>> d=Dict( (("two", "deux"), ("one", "un")) )
        >>> d["one"]
        'un'
        """
        return self._value_list[self._index[key]]

    def __setitem__(self, key, value):
        self._value_list[self._index[key]] = value

    def append(self, key, value):
        """
        Append new value
        """
        if key in self._index:
            raise UniqKeyError(_("Key '%s' already exists") % key)
        self._index[key] = len(self._value_list)
        self._key_list.append(key)
        self._value_list.append(value)

    def __len__(self):
        return len(self._value_list)

    def __contains__(self, key):
        return key in self._index

    def __iter__(self):
        return iter(self._value_list)

    def iteritems(self):
        """
        Create a generator to iterate on: (key, value).

        >>> d=Dict( (("two", "deux"), ("one", "un")) )
        >>> for key, value in d.iteritems():
        ...    print "%r: %r" % (key, value)
        ...
        'two': 'deux'
        'one': 'un'
        """
        for index in xrange(len(self)):
            yield (self._key_list[index], self._value_list[index])

    def itervalues(self):
        """
        Create an iterator on values
        """
        return iter(self._value_list)

    def iterkeys(self):
        """
        Create an iterator on keys
        """
        return iter(self._key_list)

    def replace(self, oldkey, newkey, new_value):
        """
        Replace an existing value with another one

        >>> d=Dict( (("two", "deux"), ("one", "un")) )
        >>> d.replace("one", "three", 3)
        >>> d
        {'two': 'deux', 'three': 3}

        You can also use the classic form:

        >>> d['three'] = 4
        >>> d
        {'two': 'deux', 'three': 4}
        """
        index = self._index[oldkey]
        self._value_list[index] = new_value
        if oldkey != newkey:
            del self._index[oldkey]
            self._index[newkey] = index
            self._key_list[index] = newkey

    def __delitem__(self, index):
        """
        Delete item at position index. May raise IndexError.

        >>> d=Dict( ((6, 'six'), (9, 'neuf'), (4, 'quatre')) )
        >>> del d[1]
        >>> d
        {6: 'six', 4: 'quatre'}
        """
        if index < 0:
            index += len(self._value_list)
        if not (0 <= index < len(self._value_list)):
            raise IndexError(_("list assignment index out of range (%s/%s)")
                % (index, len(self._value_list)))
        del self._value_list[index]
        del self._key_list[index]

        # First loop which may alter self._index
        for key, item_index in self._index.iteritems():
            if item_index == index:
                del self._index[key]
                break

        # Second loop update indexes
        for key, item_index in self._index.iteritems():
            if index < item_index:
                self._index[key] -= 1

    def insert(self, index, key, value):
        """
        Insert an item at specified position index.

        >>> d=Dict( ((6, 'six'), (9, 'neuf'), (4, 'quatre')) )
        >>> d.insert(1, '40', 'quarante')
        >>> d
        {6: 'six', '40': 'quarante', 9: 'neuf', 4: 'quatre'}
        """
        if key in self:
            raise UniqKeyError(_("Insert error: key '%s' ready exists") % key)
        _index = index
        if index < 0:
            index += len(self._value_list)
        if not(0 <= index <= len(self._value_list)):
            raise IndexError(_("Insert error: index '%s' is invalid") % _index)
        for item_key, item_index in self._index.iteritems():
            if item_index >= index:
                self._index[item_key] += 1
        self._index[key] = index
        self._key_list.insert(index, key)
        self._value_list.insert(index, value)

    def __repr__(self):
        items = ( "%r: %r" % (key, value) for key, value in self.iteritems() )
        return "{%s}" % ", ".join(items)


########NEW FILE########
__FILENAME__ = endian
"""
Constant values about endian.
"""

from lib.hachoir_core.i18n import _

BIG_ENDIAN = "ABCD"
LITTLE_ENDIAN = "DCBA"
NETWORK_ENDIAN = BIG_ENDIAN

endian_name = {
    BIG_ENDIAN: _("Big endian"),
    LITTLE_ENDIAN: _("Little endian"),
}


########NEW FILE########
__FILENAME__ = error
"""
Functions to display an error (error, warning or information) message.
"""

from lib.hachoir_core.log import log
from lib.hachoir_core.tools import makePrintable
import sys, traceback

def getBacktrace(empty="Empty backtrace."):
    """
    Try to get backtrace as string.
    Returns "Error while trying to get backtrace" on failure.
    """
    try:
        info = sys.exc_info()
        trace = traceback.format_exception(*info)
        sys.exc_clear()
        if trace[0] != "None\n":
            return "".join(trace)
    except:
        # No i18n here (imagine if i18n function calls error...)
        return "Error while trying to get backtrace"
    return empty

class HachoirError(Exception):
    """
    Parent of all errors in Hachoir library
    """
    def __init__(self, message):
        message_bytes = makePrintable(message, "ASCII")
        Exception.__init__(self, message_bytes)
        self.text = message

    def __unicode__(self):
        return self.text

# Error classes which may be raised by Hachoir core
# FIXME: Add EnvironmentError (IOError or OSError) and AssertionError?
# FIXME: Remove ArithmeticError and RuntimeError?
HACHOIR_ERRORS = (HachoirError, LookupError, NameError, AttributeError,
    TypeError, ValueError, ArithmeticError, RuntimeError)

info    = log.info
warning = log.warning
error   = log.error

########NEW FILE########
__FILENAME__ = event_handler
class EventHandler(object):
    """
    Class to connect events to event handlers.
    """

    def __init__(self):
        self.handlers = {}

    def connect(self, event_name, handler):
        """
        Connect an event handler to an event. Append it to handlers list.
        """
        try:
            self.handlers[event_name].append(handler)
        except KeyError:
            self.handlers[event_name] = [handler]

    def raiseEvent(self, event_name, *args):
        """
        Raiser an event: call each handler for this event_name.
        """
        if event_name not in self.handlers:
            return
        for handler in self.handlers[event_name]:
            handler(*args)


########NEW FILE########
__FILENAME__ = basic_field_set
from lib.hachoir_core.field import Field, FieldError
from lib.hachoir_core.stream import InputStream
from lib.hachoir_core.endian import BIG_ENDIAN, LITTLE_ENDIAN
from lib.hachoir_core.event_handler import EventHandler

class ParserError(FieldError):
    """
    Error raised by a field set.

    @see: L{FieldError}
    """
    pass

class MatchError(FieldError):
    """
    Error raised by a field set when the stream content doesn't
    match to file format.

    @see: L{FieldError}
    """
    pass

class BasicFieldSet(Field):
    _event_handler = None
    is_field_set = True
    endian = None

    def __init__(self, parent, name, stream, description, size):
        # Sanity checks (preconditions)
        assert not parent or issubclass(parent.__class__, BasicFieldSet)
        assert issubclass(stream.__class__, InputStream)

        # Set field set size
        if size is None and self.static_size:
            assert isinstance(self.static_size, (int, long))
            size = self.static_size

        # Set Field attributes
        self._parent = parent
        self._name = name
        self._size = size
        self._description = description
        self.stream = stream
        self._field_array_count = {}

        # Set endian
        if not self.endian:
            assert parent and parent.endian
            self.endian = parent.endian

        if parent:
            # This field set is one of the root leafs
            self._address = parent.nextFieldAddress()
            self.root = parent.root
            assert id(self.stream) == id(parent.stream)
        else:
            # This field set is the root
            self._address = 0
            self.root = self
            self._global_event_handler = None

        # Sanity checks (post-conditions)
        assert self.endian in (BIG_ENDIAN, LITTLE_ENDIAN)
        if (self._size is not None) and (self._size <= 0):
            raise ParserError("Invalid parser '%s' size: %s" % (self.path, self._size))

    def reset(self):
        self._field_array_count = {}

    def createValue(self):
        return None

    def connectEvent(self, event_name, handler, local=True):
        assert event_name in (
            # Callback prototype: def f(field)
            # Called when new value is already set
            "field-value-changed",

            # Callback prototype: def f(field)
            # Called when field size is already set
            "field-resized",

            # A new field has been inserted in the field set
            # Callback prototype: def f(index, new_field)
            "field-inserted",

            # Callback prototype: def f(old_field, new_field)
            # Called when new field is already in field set
            "field-replaced",

            # Callback prototype: def f(field, new_value)
            # Called to ask to set new value
            "set-field-value"
        ), "Event name %r is invalid" % event_name
        if local:
            if self._event_handler is None:
                self._event_handler = EventHandler()
            self._event_handler.connect(event_name, handler)
        else:
            if self.root._global_event_handler is None:
                self.root._global_event_handler = EventHandler()
            self.root._global_event_handler.connect(event_name, handler)

    def raiseEvent(self, event_name, *args):
        # Transfer event to local listeners
        if self._event_handler is not None:
            self._event_handler.raiseEvent(event_name, *args)

        # Transfer event to global listeners
        if self.root._global_event_handler is not None:
            self.root._global_event_handler.raiseEvent(event_name, *args)

    def setUniqueFieldName(self, field):
        key = field._name[:-2]
        try:
            self._field_array_count[key] += 1
        except KeyError:
            self._field_array_count[key] = 0
        field._name = key + "[%u]" % self._field_array_count[key]

    def readFirstFields(self, number):
        """
        Read first number fields if they are not read yet.

        Returns number of new added fields.
        """
        number = number - self.current_length
        if 0 < number:
            return self.readMoreFields(number)
        else:
            return 0

    def createFields(self):
        raise NotImplementedError()
    def __iter__(self):
        raise NotImplementedError()
    def __len__(self):
        raise NotImplementedError()
    def getField(self, key, const=True):
        raise NotImplementedError()
    def nextFieldAddress(self):
        raise NotImplementedError()
    def getFieldIndex(self, field):
        raise NotImplementedError()
    def readMoreFields(self, number):
        raise NotImplementedError()


########NEW FILE########
__FILENAME__ = bit_field
"""
Bit sized classes:
- Bit: Single bit, value is False or True ;
- Bits: Integer with a size in bits ;
- RawBits: unknown content with a size in bits.
"""

from lib.hachoir_core.field import Field
from lib.hachoir_core.i18n import _
from lib.hachoir_core import config

class RawBits(Field):
    """
    Unknown content with a size in bits.
    """
    static_size = staticmethod(lambda *args, **kw: args[1])

    def __init__(self, parent, name, size, description=None):
        """
        Constructor: see L{Field.__init__} for parameter description
        """
        Field.__init__(self, parent, name, size, description)

    def hasValue(self):
        return True

    def createValue(self):
        return self._parent.stream.readBits(
            self.absolute_address, self._size, self._parent.endian)

    def createDisplay(self):
        if self._size < config.max_bit_length:
            return unicode(self.value)
        else:
            return _("<%s size=%u>" %
                (self.__class__.__name__, self._size))
    createRawDisplay = createDisplay

class Bits(RawBits):
    """
    Positive integer with a size in bits

    @see: L{Bit}
    @see: L{RawBits}
    """
    pass

class Bit(RawBits):
    """
    Single bit: value can be False or True, and size is exactly one bit.

    @see: L{Bits}
    """
    static_size = 1

    def __init__(self, parent, name, description=None):
        """
        Constructor: see L{Field.__init__} for parameter description
        """
        RawBits.__init__(self, parent, name, 1, description=description)

    def createValue(self):
        return 1 == self._parent.stream.readBits(
                self.absolute_address, 1, self._parent.endian)

    def createRawDisplay(self):
        return unicode(int(self.value))


########NEW FILE########
__FILENAME__ = byte_field
"""
Very basic field: raw content with a size in byte. Use this class for
unknown content.
"""

from lib.hachoir_core.field import Field, FieldError
from lib.hachoir_core.tools import makePrintable
from lib.hachoir_core.bits import str2hex
from lib.hachoir_core import config

MAX_LENGTH = (2**64)

class RawBytes(Field):
    """
    Byte vector of unknown content

    @see: L{Bytes}
    """
    static_size = staticmethod(lambda *args, **kw: args[1]*8)

    def __init__(self, parent, name, length, description="Raw data"):
        assert issubclass(parent.__class__, Field)
        if not(0 < length <= MAX_LENGTH):
            raise FieldError("Invalid RawBytes length (%s)!" % length)
        Field.__init__(self, parent, name, length*8, description)
        self._display = None

    def _createDisplay(self, human):
        max_bytes = config.max_byte_length
        if type(self._getValue) is type(lambda: None):
            display = self.value[:max_bytes]
        else:
            if self._display is None:
                address = self.absolute_address
                length = min(self._size / 8, max_bytes)
                self._display = self._parent.stream.readBytes(address, length)
            display = self._display
        truncated = (8 * len(display) < self._size)
        if human:
            if truncated:
                display += "(...)"
            return makePrintable(display, "latin-1", quote='"', to_unicode=True)
        else:
            display = str2hex(display, format=r"\x%02x")
            if truncated:
                return '"%s(...)"' % display
            else:
                return '"%s"' % display

    def createDisplay(self):
        return self._createDisplay(True)

    def createRawDisplay(self):
        return self._createDisplay(False)

    def hasValue(self):
        return True

    def createValue(self):
        assert (self._size % 8) == 0
        if self._display:
            self._display = None
        return self._parent.stream.readBytes(
            self.absolute_address, self._size / 8)

class Bytes(RawBytes):
    """
    Byte vector: can be used for magic number or GUID/UUID for example.

    @see: L{RawBytes}
    """
    pass


########NEW FILE########
__FILENAME__ = character
"""
Character field class: a 8-bit character
"""

from lib.hachoir_core.field import Bits
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.tools import makePrintable

class Character(Bits):
    """
    A 8-bit character using ASCII charset for display attribute.
    """
    static_size = 8

    def __init__(self, parent, name, description=None):
        Bits.__init__(self, parent, name, 8, description=description)

    def createValue(self):
        return chr(self._parent.stream.readBits(
            self.absolute_address, 8, BIG_ENDIAN))

    def createRawDisplay(self):
        return unicode(Bits.createValue(self))

    def createDisplay(self):
        return makePrintable(self.value, "ASCII", quote="'", to_unicode=True)


########NEW FILE########
__FILENAME__ = enum
def Enum(field, enum, key_func=None):
    """
    Enum is an adapter to another field: it will just change its display
    attribute. It uses a dictionnary to associate a value to another.

    key_func is an optional function with prototype "def func(key)->key"
    which is called to transform key.
    """
    display = field.createDisplay
    if key_func:
        def createDisplay():
            try:
                key = key_func(field.value)
                return enum[key]
            except LookupError:
                return display()
    else:
        def createDisplay():
            try:
                return enum[field.value]
            except LookupError:
                return display()
    field.createDisplay = createDisplay
    field.getEnum = lambda: enum
    return field


########NEW FILE########
__FILENAME__ = fake_array
import itertools
from lib.hachoir_core.field import MissingField

class FakeArray:
    """
    Simulate an array for GenericFieldSet.array(): fielset.array("item")[0] is
    equivalent to fielset.array("item[0]").

    It's possible to iterate over the items using::

        for element in fieldset.array("item"):
            ...

    And to get array size using len(fieldset.array("item")).
    """
    def __init__(self, fieldset, name):
        pos = name.rfind("/")
        if pos != -1:
            self.fieldset = fieldset[name[:pos]]
            self.name = name[pos+1:]
        else:
            self.fieldset = fieldset
            self.name = name
        self._format = "%s[%%u]" % self.name
        self._cache = {}
        self._known_size = False
        self._max_index = -1

    def __nonzero__(self):
        "Is the array empty or not?"
        if self._cache:
            return True
        else:
            return (0 in self)

    def __len__(self):
        "Number of fields in the array"
        total = self._max_index+1
        if not self._known_size:
            for index in itertools.count(total):
                try:
                    field = self[index]
                    total += 1
                except MissingField:
                    break
        return total

    def __contains__(self, index):
        try:
            field = self[index]
            return True
        except MissingField:
            return False

    def __getitem__(self, index):
        """
        Get a field of the array. Returns a field, or raise MissingField
        exception if the field doesn't exist.
        """
        try:
            value = self._cache[index]
        except KeyError:
            try:
                value = self.fieldset[self._format % index]
            except MissingField:
                self._known_size = True
                raise
            self._cache[index] = value
            self._max_index = max(index, self._max_index)
        return value

    def __iter__(self):
        """
        Iterate in the fields in their index order: field[0], field[1], ...
        """
        for index in itertools.count(0):
            try:
                yield self[index]
            except MissingField:
                raise StopIteration()


########NEW FILE########
__FILENAME__ = field
"""
Parent of all (field) classes in Hachoir: Field.
"""

from lib.hachoir_core.compatibility import reversed
from lib.hachoir_core.stream import InputFieldStream
from lib.hachoir_core.error import HachoirError, HACHOIR_ERRORS
from lib.hachoir_core.log import Logger
from lib.hachoir_core.i18n import _
from lib.hachoir_core.tools import makePrintable
from weakref import ref as weakref_ref

class FieldError(HachoirError):
    """
    Error raised by a L{Field}.

    @see: L{HachoirError}
    """
    pass

def joinPath(path, name):
    if path != "/":
        return "/".join((path, name))
    else:
        return "/%s" % name

class MissingField(KeyError, FieldError):
    def __init__(self, field, key):
        KeyError.__init__(self)
        self.field = field
        self.key = key

    def __str__(self):
        return 'Can\'t get field "%s" from %s' % (self.key, self.field.path)

    def __unicode__(self):
        return u'Can\'t get field "%s" from %s' % (self.key, self.field.path)

class Field(Logger):
    # static size can have two differents value: None (no static size), an
    # integer (number of bits), or a function which returns an integer.
    #
    # This function receives exactly the same arguments than the constructor
    # except the first one (one). Example of function:
    #    static_size = staticmethod(lambda *args, **kw: args[1])
    static_size = None

    # Indicate if this field contains other fields (is a field set) or not
    is_field_set = False

    def __init__(self, parent, name, size=None, description=None):
        """
        Set default class attributes, set right address if None address is
        given.

        @param parent: Parent field of this field
        @type parent: L{Field}|None
        @param name: Name of the field, have to be unique in parent. If it ends
            with "[]", end will be replaced with "[new_id]" (eg. "raw[]"
            becomes "raw[0]", next will be "raw[1]", and then "raw[2]", etc.)
        @type name: str
        @param size: Size of the field in bit (can be None, so it
            will be computed later)
        @type size: int|None
        @param address: Address in bit relative to the parent absolute address
        @type address: int|None
        @param description: Optional string description
        @type description: str|None
        """
        assert issubclass(parent.__class__, Field)
        assert (size is None) or (0 <= size)
        self._parent = parent
        self._name = name
        self._address = parent.nextFieldAddress()
        self._size = size
        self._description = description

    def _logger(self):
        return self.path

    def createDescription(self):
        return ""
    def _getDescription(self):
        if self._description is None:
            try:
                self._description = self.createDescription()
                if isinstance(self._description, str):
                    self._description = makePrintable(
                        self._description, "ISO-8859-1", to_unicode=True)
            except HACHOIR_ERRORS, err:
                self.error("Error getting description: " + unicode(err))
                self._description = ""
        return self._description
    description = property(_getDescription,
    doc="Description of the field (string)")

    def __str__(self):
        return self.display
    def __unicode__(self):
        return self.display
    def __repr__(self):
        return "<%s path=%r, address=%s, size=%s>" % (
            self.__class__.__name__, self.path, self._address, self._size)

    def hasValue(self):
        return self._getValue() is not None
    def createValue(self):
        raise NotImplementedError()
    def _getValue(self):
        try:
            value = self.createValue()
        except HACHOIR_ERRORS, err:
            self.error(_("Unable to create value: %s") % unicode(err))
            value = None
        self._getValue = lambda: value
        return value
    value = property(lambda self: self._getValue(), doc="Value of field")

    def _getParent(self):
        return self._parent
    parent = property(_getParent, doc="Parent of this field")

    def createDisplay(self):
        return unicode(self.value)
    def _getDisplay(self):
        if not hasattr(self, "_Field__display"):
            try:
                self.__display = self.createDisplay()
            except HACHOIR_ERRORS, err:
                self.error("Unable to create display: %s" % err)
                self.__display = u""
        return self.__display
    display = property(lambda self: self._getDisplay(),
    doc="Short (unicode) string which represents field content")

    def createRawDisplay(self):
        value = self.value
        if isinstance(value, str):
            return makePrintable(value, "ASCII", to_unicode=True)
        else:
            return unicode(value)
    def _getRawDisplay(self):
        if not hasattr(self, "_Field__raw_display"):
            try:
                self.__raw_display = self.createRawDisplay()
            except HACHOIR_ERRORS, err:
                self.error("Unable to create raw display: %s" % err)
                self.__raw_display = u""
        return self.__raw_display
    raw_display = property(lambda self: self._getRawDisplay(),
    doc="(Unicode) string which represents raw field content")

    def _getName(self):
        return self._name
    name = property(_getName,
    doc="Field name (unique in its parent field set list)")

    def _getIndex(self):
        if not self._parent:
            return None
        return self._parent.getFieldIndex(self)
    index = property(_getIndex)

    def _getPath(self):
        if not self._parent:
            return '/'
        names = []
        field = self
        while field:
            names.append(field._name)
            field = field._parent
        names[-1] = ''
        return '/'.join(reversed(names))
    path = property(_getPath,
    doc="Full path of the field starting at root field")

    def _getAddress(self):
        return self._address
    address = property(_getAddress,
    doc="Relative address in bit to parent address")

    def _getAbsoluteAddress(self):
        address = self._address
        current = self._parent
        while current:
            address += current._address
            current = current._parent
        return address
    absolute_address = property(_getAbsoluteAddress,
    doc="Absolute address (from stream beginning) in bit")

    def _getSize(self):
        return self._size
    size = property(_getSize, doc="Content size in bit")

    def _getField(self, name, const):
        if name.strip("."):
            return None
        field = self
        for index in xrange(1, len(name)):
            field = field._parent
            if field is None:
                break
        return field

    def getField(self, key, const=True):
        if key:
            if key[0] == "/":
                if self._parent:
                    current = self._parent.root
                else:
                    current = self
                if len(key) == 1:
                    return current
                key = key[1:]
            else:
                current = self
            for part in key.split("/"):
                field = current._getField(part, const)
                if field is None:
                    raise MissingField(current, part)
                current = field
            return current
        raise KeyError("Key must not be an empty string!")

    def __getitem__(self, key):
        return self.getField(key, False)

    def __contains__(self, key):
        try:
            return self.getField(key, False) is not None
        except FieldError:
            return False

    def _createInputStream(self, **args):
        assert self._parent
        return InputFieldStream(self, **args)
    def getSubIStream(self):
        if hasattr(self, "_sub_istream"):
            stream = self._sub_istream()
        else:
            stream = None
        if stream is None:
            stream = self._createInputStream()
            self._sub_istream = weakref_ref(stream)
        return stream
    def setSubIStream(self, createInputStream):
        cis = self._createInputStream
        self._createInputStream = lambda **args: createInputStream(cis, **args)

    def __nonzero__(self):
        """
        Method called by code like "if field: (...)".
        Always returns True
        """
        return True

    def getFieldType(self):
        return self.__class__.__name__


########NEW FILE########
__FILENAME__ = field_set
from lib.hachoir_core.field import BasicFieldSet, GenericFieldSet

class FieldSet(GenericFieldSet):
    def __init__(self, parent, name, *args, **kw):
        assert issubclass(parent.__class__, BasicFieldSet)
        GenericFieldSet.__init__(self, parent, name, parent.stream, *args, **kw)


########NEW FILE########
__FILENAME__ = float
from lib.hachoir_core.field import Bit, Bits, FieldSet
from lib.hachoir_core.endian import BIG_ENDIAN, LITTLE_ENDIAN
import struct

# Make sure that we use right struct types
assert struct.calcsize("f") == 4
assert struct.calcsize("d") == 8
assert struct.unpack("<d", "\x1f\x85\xebQ\xb8\x1e\t@")[0] == 3.14
assert struct.unpack(">d", "\xc0\0\0\0\0\0\0\0")[0] == -2.0

class FloatMantissa(Bits):
    def createValue(self):
        value = Bits.createValue(self)
        return 1 + float(value) / (2 ** self.size)

    def createRawDisplay(self):
        return unicode(Bits.createValue(self))

class FloatExponent(Bits):
    def __init__(self, parent, name, size):
        Bits.__init__(self, parent, name, size)
        self.bias = 2 ** (size-1) - 1

    def createValue(self):
        return Bits.createValue(self) - self.bias

    def createRawDisplay(self):
        return unicode(self.value + self.bias)

def floatFactory(name, format, mantissa_bits, exponent_bits, doc):
    size = 1 + mantissa_bits + exponent_bits

    class Float(FieldSet):
        static_size = size
        __doc__ = doc

        def __init__(self, parent, name, description=None):
            assert parent.endian in (BIG_ENDIAN, LITTLE_ENDIAN)
            FieldSet.__init__(self, parent, name, description, size)
            if format:
                if self._parent.endian == BIG_ENDIAN:
                    self.struct_format = ">"+format
                else:
                    self.struct_format = "<"+format
            else:
                self.struct_format = None

        def createValue(self):
            """
            Create float value: use struct.unpack() when it's possible
            (32 and 64-bit float) or compute it with :
               mantissa * (2.0 ** exponent)

            This computation may raise an OverflowError.
            """
            if self.struct_format:
                raw = self._parent.stream.readBytes(
                    self.absolute_address, self._size//8)
                try:
                    return struct.unpack(self.struct_format, raw)[0]
                except struct.error, err:
                    raise ValueError("[%s] conversion error: %s" %
                        (self.__class__.__name__, err))
            else:
                try:
                    value = self["mantissa"].value * (2.0 ** float(self["exponent"].value))
                    if self["negative"].value:
                        return -(value)
                    else:
                        return value
                except OverflowError:
                    raise ValueError("[%s] floating point overflow" %
                        self.__class__.__name__)

        def createFields(self):
            yield Bit(self, "negative")
            yield FloatExponent(self, "exponent", exponent_bits)
            if 64 <= mantissa_bits:
                yield Bit(self, "one")
                yield FloatMantissa(self, "mantissa", mantissa_bits-1)
            else:
                yield FloatMantissa(self, "mantissa", mantissa_bits)

    cls = Float
    cls.__name__ = name
    return cls

# 32-bit float (standart: IEEE 754/854)
Float32 = floatFactory("Float32", "f", 23, 8,
    "Floating point number: format IEEE 754 int 32 bit")

# 64-bit float (standart: IEEE 754/854)
Float64 = floatFactory("Float64", "d", 52, 11,
    "Floating point number: format IEEE 754 in 64 bit")

# 80-bit float (standart: IEEE 754/854)
Float80 = floatFactory("Float80", None, 64, 15,
    "Floating point number: format IEEE 754 in 80 bit")


########NEW FILE########
__FILENAME__ = generic_field_set
from lib.hachoir_core.field import (MissingField, BasicFieldSet, Field, ParserError,
    createRawField, createNullField, createPaddingField, FakeArray)
from lib.hachoir_core.dict import Dict, UniqKeyError
from lib.hachoir_core.error import HACHOIR_ERRORS
from lib.hachoir_core.tools import lowerBound
import lib.hachoir_core.config as config

class GenericFieldSet(BasicFieldSet):
    """
    Ordered list of fields. Use operator [] to access fields using their
    name (field names are unique in a field set, but not in the whole
    document).

    Class attributes:
    - endian: Bytes order (L{BIG_ENDIAN} or L{LITTLE_ENDIAN}). Optional if the
      field set has a parent ;
    - static_size: (optional) Size of FieldSet in bits. This attribute should
      be used in parser of constant size.

    Instance attributes/methods:
    - _fields: Ordered dictionnary of all fields, may be incomplete
      because feeded when a field is requested ;
    - stream: Input stream used to feed fields' value
    - root: The root of all field sets ;
    - __len__(): Number of fields, may need to create field set ;
    - __getitem__(): Get an field by it's name or it's path.

    And attributes inherited from Field class:
    - parent: Parent field (may be None if it's the root) ;
    - name: Field name (unique in parent field set) ;
    - value: The field set ;
    - address: Field address (in bits) relative to parent ;
    - description: A string describing the content (can be None) ;
    - size: Size of field set in bits, may need to create field set.

    Event handling:
    - "connectEvent": Connect an handler to an event ;
    - "raiseEvent": Raise an event.

    To implement a new field set, you need to:
    - create a class which inherite from FieldSet ;
    - write createFields() method using lines like:
         yield Class(self, "name", ...) ;
    - and maybe set endian and static_size class attributes.
    """

    _current_size = 0

    def __init__(self, parent, name, stream, description=None, size=None):
        """
        Constructor
        @param parent: Parent field set, None for root parser
        @param name: Name of the field, have to be unique in parent. If it ends
            with "[]", end will be replaced with "[new_id]" (eg. "raw[]"
            becomes "raw[0]", next will be "raw[1]", and then "raw[2]", etc.)
        @type name: str
        @param stream: Input stream from which data are read
        @type stream: L{InputStream}
        @param description: Optional string description
        @type description: str|None
        @param size: Size in bits. If it's None, size will be computed. You
            can also set size with class attribute static_size
        """
        BasicFieldSet.__init__(self, parent, name, stream, description, size)
        self._fields = Dict()
        self._field_generator = self.createFields()
        self._array_cache = {}
        self.__is_feeding = False

    def array(self, key):
        try:
            return self._array_cache[key]
        except KeyError:
            array = FakeArray(self, key)
            self._array_cache[key] = array
            return self._array_cache[key]

    def reset(self):
        """
        Reset a field set:
         * clear fields ;
         * restart field generator ;
         * set current size to zero ;
         * clear field array count.

        But keep: name, value, description and size.
        """
        BasicFieldSet.reset(self)
        self._fields = Dict()
        self._field_generator = self.createFields()
        self._current_size = 0
        self._array_cache = {}

    def __str__(self):
        return '<%s path=%s, current_size=%s, current length=%s>' % \
            (self.__class__.__name__, self.path, self._current_size, len(self._fields))

    def __len__(self):
        """
        Returns number of fields, may need to create all fields
        if it's not done yet.
        """
        if self._field_generator is not None:
            self._feedAll()
        return len(self._fields)

    def _getCurrentLength(self):
        return len(self._fields)
    current_length = property(_getCurrentLength)

    def _getSize(self):
        if self._size is None:
            self._feedAll()
        return self._size
    size = property(_getSize, doc="Size in bits, may create all fields to get size")

    def _getCurrentSize(self):
        assert not(self.done)
        return self._current_size
    current_size = property(_getCurrentSize)

    eof = property(lambda self: self._checkSize(self._current_size + 1, True) < 0)

    def _checkSize(self, size, strict):
        field = self
        while field._size is None:
            if not field._parent:
                assert self.stream.size is None
                if not strict:
                    return None
                if self.stream.sizeGe(size):
                    return 0
                break
            size += field._address
            field = field._parent
        return field._size - size

    autofix = property(lambda self: self.root.autofix)

    def _addField(self, field):
        """
        Add a field to the field set:
        * add it into _fields
        * update _current_size

        May raise a StopIteration() on error
        """
        if not issubclass(field.__class__, Field):
            raise ParserError("Field type (%s) is not a subclass of 'Field'!"
                % field.__class__.__name__)
        assert isinstance(field._name, str)
        if field._name.endswith("[]"):
            self.setUniqueFieldName(field)
        if config.debug:
            self.info("[+] DBG: _addField(%s)" % field.name)

        # required for the msoffice parser
        if field._address != self._current_size:
            self.warning("Fix address of %s to %s (was %s)" %
                (field.path, self._current_size, field._address))
            field._address = self._current_size

        ask_stop = False
        # Compute field size and check that there is enough place for it
        self.__is_feeding = True
        try:
            field_size = field.size
        except HACHOIR_ERRORS, err:
            if field.is_field_set and field.current_length and field.eof:
                self.warning("Error when getting size of '%s': %s" % (field.name, err))
                field._stopFeeding()
                ask_stop = True
            else:
                self.warning("Error when getting size of '%s': delete it" % field.name)
                self.__is_feeding = False
                raise
        self.__is_feeding = False

        # No more place?
        dsize = self._checkSize(field._address + field.size, False)
        if (dsize is not None and dsize < 0) or (field.is_field_set and field.size <= 0):
            if self.autofix and self._current_size:
                self._fixFieldSize(field, field.size + dsize)
            else:
                raise ParserError("Field %s is too large!" % field.path)

        self._current_size += field.size
        try:
            self._fields.append(field._name, field)
        except UniqKeyError, err:
            self.warning("Duplicate field name " + unicode(err))
            field._name += "[]"
            self.setUniqueFieldName(field)
            self._fields.append(field._name, field)
        if ask_stop:
            raise StopIteration()

    def _fixFieldSize(self, field, new_size):
        if new_size > 0:
            if field.is_field_set and 0 < field.size:
                field._truncate(new_size)
                return

            # Don't add the field <=> delete item
            if self._size is None:
                self._size = self._current_size + new_size
        self.warning("[Autofix] Delete '%s' (too large)" % field.path)
        raise StopIteration()

    def _getField(self, name, const):
        field = Field._getField(self, name, const)
        if field is None:
            if name in self._fields:
                field = self._fields[name]
            elif self._field_generator is not None and not const:
                field = self._feedUntil(name)
        return field

    def getField(self, key, const=True):
        if isinstance(key, (int, long)):
            if key < 0:
                raise KeyError("Key must be positive!")
            if not const:
                self.readFirstFields(key+1)
            if len(self._fields.values) <= key:
                raise MissingField(self, key)
            return self._fields.values[key]
        return Field.getField(self, key, const)

    def _truncate(self, size):
        assert size > 0
        if size < self._current_size:
            self._size = size
            while True:
                field = self._fields.values[-1]
                if field._address < size:
                    break
                del self._fields[-1]
            self._current_size = field._address
            size -= field._address
            if size < field._size:
                if field.is_field_set:
                    field._truncate(size)
                else:
                    del self._fields[-1]
                    field = createRawField(self, size, "raw[]")
                    self._fields.append(field._name, field)
            self._current_size = self._size
        else:
            assert size < self._size or self._size is None
            self._size = size
        if self._size == self._current_size:
            self._field_generator = None

    def _deleteField(self, index):
        field = self._fields.values[index]
        size = field.size
        self._current_size -= size
        del self._fields[index]
        return field

    def _fixLastField(self):
        """
        Try to fix last field when we know current field set size.
        Returns new added field if any, or None.
        """
        assert self._size is not None

        # Stop parser
        message = ["stop parser"]
        self._field_generator = None

        # If last field is too big, delete it
        while self._size < self._current_size:
            field = self._deleteField(len(self._fields)-1)
            message.append("delete field %s" % field.path)
        assert self._current_size <= self._size

        # If field size current is smaller: add a raw field
        size = self._size - self._current_size
        if size:
            field = createRawField(self, size, "raw[]")
            message.append("add padding")
            self._current_size += field.size
            self._fields.append(field._name, field)
        else:
            field = None
        message = ", ".join(message)
        self.warning("[Autofix] Fix parser error: " + message)
        assert self._current_size == self._size
        return field

    def _stopFeeding(self):
        new_field = None
        if self._size is None:
            if self._parent:
                self._size = self._current_size
        elif self._size != self._current_size:
            if self.autofix:
                new_field = self._fixLastField()
            else:
                raise ParserError("Invalid parser \"%s\" size!" % self.path)
        self._field_generator = None
        return new_field

    def _fixFeedError(self, exception):
        """
        Try to fix a feeding error. Returns False if error can't be fixed,
        otherwise returns new field if any, or None.
        """
        if self._size is None or not self.autofix:
            return False
        self.warning(unicode(exception))
        return self._fixLastField()

    def _feedUntil(self, field_name):
        """
        Return the field if it was found, None else
        """
        if self.__is_feeding \
        or (self._field_generator and self._field_generator.gi_running):
            self.warning("Unable to get %s (and generator is already running)"
                % field_name)
            return None
        try:
            while True:
                field = self._field_generator.next()
                self._addField(field)
                if field.name == field_name:
                    return field
        except HACHOIR_ERRORS, err:
            if self._fixFeedError(err) is False:
                raise
        except StopIteration:
            self._stopFeeding()
        return None

    def readMoreFields(self, number):
        """
        Read more number fields, or do nothing if parsing is done.

        Returns number of new added fields.
        """
        if self._field_generator is None:
            return 0
        oldlen = len(self._fields)
        try:
            for index in xrange(number):
                self._addField( self._field_generator.next() )
        except HACHOIR_ERRORS, err:
            if self._fixFeedError(err) is False:
                raise
        except StopIteration:
            self._stopFeeding()
        return len(self._fields) - oldlen

    def _feedAll(self):
        if self._field_generator is None:
            return
        try:
            while True:
                field = self._field_generator.next()
                self._addField(field)
        except HACHOIR_ERRORS, err:
            if self._fixFeedError(err) is False:
                raise
        except StopIteration:
            self._stopFeeding()

    def __iter__(self):
        """
        Create a generator to iterate on each field, may create new
        fields when needed
        """
        try:
            done = 0
            while True:
                if done == len(self._fields):
                    if self._field_generator is None:
                        break
                    self._addField( self._field_generator.next() )
                for field in self._fields.values[done:]:
                    yield field
                    done += 1
        except HACHOIR_ERRORS, err:
            field = self._fixFeedError(err)
            if isinstance(field, Field):
                yield field
            elif hasattr(field, '__iter__'):
                for f in field:
                    yield f
            elif field is False:
                raise
        except StopIteration:
            field = self._stopFeeding()
            if isinstance(field, Field):
                yield field
            elif hasattr(field, '__iter__'):
                for f in field:
                    yield f

    def _isDone(self):
        return (self._field_generator is None)
    done = property(_isDone, doc="Boolean to know if parsing is done or not")

    #
    # FieldSet_SeekUtility
    #
    def seekBit(self, address, name="padding[]",
    description=None, relative=True, null=False):
        """
        Create a field to seek to specified address,
        or None if it's not needed.

        May raise an (ParserError) exception if address is invalid.
        """
        if relative:
            nbits = address - self._current_size
        else:
            nbits = address - (self.absolute_address + self._current_size)
        if nbits < 0:
            raise ParserError("Seek error, unable to go back!")
        if 0 < nbits:
            if null:
                return createNullField(self, nbits, name, description)
            else:
                return createPaddingField(self, nbits, name, description)
        else:
            return None

    def seekByte(self, address, name="padding[]", description=None, relative=True, null=False):
        """
        Same as seekBit(), but with address in byte.
        """
        return self.seekBit(address * 8, name, description, relative, null=null)

    #
    # RandomAccessFieldSet
    #
    def replaceField(self, name, new_fields):
        # TODO: Check in self and not self.field
        # Problem is that "generator is already executing"
        if name not in self._fields:
            raise ParserError("Unable to replace %s: field doesn't exist!" % name)
        assert 1 <= len(new_fields)
        old_field = self[name]
        total_size = sum( (field.size for field in new_fields) )
        if old_field.size != total_size:
            raise ParserError("Unable to replace %s: "
                "new field(s) hasn't same size (%u bits instead of %u bits)!"
                % (name, total_size, old_field.size))
        field = new_fields[0]
        if field._name.endswith("[]"):
            self.setUniqueFieldName(field)
        field._address = old_field.address
        if field.name != name and field.name in self._fields:
            raise ParserError(
                "Unable to replace %s: name \"%s\" is already used!"
                % (name, field.name))
        self._fields.replace(name, field.name, field)
        self.raiseEvent("field-replaced", old_field, field)
        if 1 < len(new_fields):
            index = self._fields.index(new_fields[0].name)+1
            address = field.address + field.size
            for field in new_fields[1:]:
                if field._name.endswith("[]"):
                    self.setUniqueFieldName(field)
                field._address = address
                if field.name in self._fields:
                    raise ParserError(
                        "Unable to replace %s: name \"%s\" is already used!"
                        % (name, field.name))
                self._fields.insert(index, field.name, field)
                self.raiseEvent("field-inserted", index, field)
                index += 1
                address += field.size

    def getFieldByAddress(self, address, feed=True):
        """
        Only search in existing fields
        """
        if feed and self._field_generator is not None:
            self._feedAll()
        if address < self._current_size:
            i = lowerBound(self._fields.values, lambda x: x.address + x.size <= address)
            if i is not None:
                return self._fields.values[i]
        return None

    def writeFieldsIn(self, old_field, address, new_fields):
        """
        Can only write in existing fields (address < self._current_size)
        """

        # Check size
        total_size = sum( field.size for field in new_fields )
        if old_field.size < total_size:
            raise ParserError( \
                "Unable to write fields at address %s " \
                "(too big)!" % (address))

        # Need padding before?
        replace = []
        size = address - old_field.address
        assert 0 <= size
        if 0 < size:
            padding = createPaddingField(self, size)
            padding._address = old_field.address
            replace.append(padding)

        # Set fields address
        for field in new_fields:
            field._address = address
            address += field.size
            replace.append(field)

        # Need padding after?
        size = (old_field.address + old_field.size) - address
        assert 0 <= size
        if 0 < size:
            padding = createPaddingField(self, size)
            padding._address = address
            replace.append(padding)

        self.replaceField(old_field.name, replace)

    def nextFieldAddress(self):
        return self._current_size

    def getFieldIndex(self, field):
        return self._fields.index(field._name)


########NEW FILE########
__FILENAME__ = helper
from lib.hachoir_core.field import (FieldError,
    RawBits, RawBytes,
    PaddingBits, PaddingBytes,
    NullBits, NullBytes,
    GenericString, GenericInteger)
from lib.hachoir_core.stream import FileOutputStream

def createRawField(parent, size, name="raw[]", description=None):
    if size <= 0:
        raise FieldError("Unable to create raw field of %s bits" % size)
    if (size % 8) == 0:
        return RawBytes(parent, name, size/8, description)
    else:
        return RawBits(parent, name, size, description)

def createPaddingField(parent, nbits, name="padding[]", description=None):
    if nbits <= 0:
        raise FieldError("Unable to create padding of %s bits" % nbits)
    if (nbits % 8) == 0:
        return PaddingBytes(parent, name, nbits/8, description)
    else:
        return PaddingBits(parent, name, nbits, description)

def createNullField(parent, nbits, name="padding[]", description=None):
    if nbits <= 0:
        raise FieldError("Unable to create null padding of %s bits" % nbits)
    if (nbits % 8) == 0:
        return NullBytes(parent, name, nbits/8, description)
    else:
        return NullBits(parent, name, nbits, description)

def isString(field):
    return issubclass(field.__class__, GenericString)

def isInteger(field):
    return issubclass(field.__class__, GenericInteger)

def writeIntoFile(fieldset, filename):
    output = FileOutputStream(filename)
    fieldset.writeInto(output)

def createOrphanField(fieldset, address, field_cls, *args, **kw):
    """
    Create an orphan field at specified address:
      field_cls(fieldset, *args, **kw)

    The field uses the fieldset properties but it isn't added to the
    field set.
    """
    save_size = fieldset._current_size
    try:
        fieldset._current_size = address
        field = field_cls(fieldset, *args, **kw)
    finally:
        fieldset._current_size = save_size
    return field


########NEW FILE########
__FILENAME__ = integer
"""
Integer field classes:
- UInt8, UInt16, UInt24, UInt32, UInt64: unsigned integer of 8, 16, 32, 64 bits ;
- Int8, Int16, Int24, Int32, Int64: signed integer of 8, 16, 32, 64 bits.
"""

from lib.hachoir_core.field import Bits, FieldError

class GenericInteger(Bits):
    """
    Generic integer class used to generate other classes.
    """
    def __init__(self, parent, name, signed, size, description=None):
        if not (8 <= size <= 256):
            raise FieldError("Invalid integer size (%s): have to be in 8..256" % size)
        Bits.__init__(self, parent, name, size, description)
        self.signed = signed

    def createValue(self):
        return self._parent.stream.readInteger(
            self.absolute_address, self.signed, self._size, self._parent.endian)

def integerFactory(name, is_signed, size, doc):
    class Integer(GenericInteger):
        __doc__ = doc
        static_size = size
        def __init__(self, parent, name, description=None):
            GenericInteger.__init__(self, parent, name, is_signed, size, description)
    cls = Integer
    cls.__name__ = name
    return cls

UInt8 = integerFactory("UInt8", False, 8, "Unsigned integer of 8 bits")
UInt16 = integerFactory("UInt16", False, 16, "Unsigned integer of 16 bits")
UInt24 = integerFactory("UInt24", False, 24, "Unsigned integer of 24 bits")
UInt32 = integerFactory("UInt32", False, 32, "Unsigned integer of 32 bits")
UInt64 = integerFactory("UInt64", False, 64, "Unsigned integer of 64 bits")

Int8 = integerFactory("Int8", True, 8, "Signed integer of 8 bits")
Int16 = integerFactory("Int16", True, 16, "Signed integer of 16 bits")
Int24 = integerFactory("Int24", True, 24, "Signed integer of 24 bits")
Int32 = integerFactory("Int32", True, 32, "Signed integer of 32 bits")
Int64 = integerFactory("Int64", True, 64, "Signed integer of 64 bits")


########NEW FILE########
__FILENAME__ = link
from lib.hachoir_core.field import Field, FieldSet, ParserError, Bytes, MissingField
from lib.hachoir_core.stream import FragmentedStream


class Link(Field):
    def __init__(self, parent, name, *args, **kw):
        Field.__init__(self, parent, name, 0, *args, **kw)

    def hasValue(self):
        return True

    def createValue(self):
        return self._parent[self.display]

    def createDisplay(self):
        value = self.value
        if value is None:
            return "<%s>" % MissingField.__name__
        return value.path

    def _getField(self, name, const):
        target = self.value
        assert self != target
        return target._getField(name, const)


class Fragments:
    def __init__(self, first):
        self.first = first

    def __iter__(self):
        fragment = self.first
        while fragment is not None:
            data = fragment.getData()
            yield data and data.size
            fragment = fragment.next


class Fragment(FieldSet):
    _first = None

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._field_generator = self._createFields(self._field_generator)
        if self.__class__.createFields == Fragment.createFields:
            self._getData = lambda: self

    def getData(self):
        try:
            return self._getData()
        except MissingField, e:
            self.error(str(e))
        return None

    def setLinks(self, first, next=None):
        self._first = first or self
        self._next = next
        self._feedLinks = lambda: self
        return self

    def _feedLinks(self):
        while self._first is None and self.readMoreFields(1):
            pass
        if self._first is None:
            raise ParserError("first is None")
        return self
    first = property(lambda self: self._feedLinks()._first)

    def _getNext(self):
        next = self._feedLinks()._next
        if callable(next):
            self._next = next = next()
        return next
    next  = property(_getNext)

    def _createInputStream(self, **args):
        first = self.first
        if first is self and hasattr(first, "_getData"):
            return FragmentedStream(first, packets=Fragments(first), **args)
        return FieldSet._createInputStream(self, **args)

    def _createFields(self, field_generator):
        if self._first is None:
            for field in field_generator:
                if self._first is not None:
                    break
                yield field
            else:
                raise ParserError("Fragment.setLinks not called")
        else:
            field = None
        if self._first is not self:
            link = Link(self, "first", None)
            link._getValue = lambda: self._first
            yield link
        if self._next:
            link = Link(self, "next", None)
            link.createValue = self._getNext
            yield link
        if field:
            yield field
        for field in field_generator:
            yield field

    def createFields(self):
        if self._size is None:
            self._size = self._getSize()
        yield Bytes(self, "data", self._size/8)


########NEW FILE########
__FILENAME__ = new_seekable_field_set
from lib.hachoir_core.field import BasicFieldSet, GenericFieldSet, ParserError, createRawField
from lib.hachoir_core.error import HACHOIR_ERRORS

# getgaps(int, int, [listof (int, int)]) -> generator of (int, int)
# Gets all the gaps not covered by a block in `blocks` from `start` for `length` units.
def getgaps(start, length, blocks):
    '''
    Example:
    >>> list(getgaps(0, 20, [(15,3), (6,2), (6,2), (1,2), (2,3), (11,2), (9,5)]))
    [(0, 1), (5, 1), (8, 1), (14, 1), (18, 2)]
    '''
    # done this way to avoid mutating the original
    blocks = sorted(blocks, key=lambda b: b[0])
    end = start+length
    for s, l in blocks:
        if s > start:
            yield (start, s-start)
            start = s
        if s+l > start:
            start = s+l
    if start < end:
        yield (start, end-start)

class NewRootSeekableFieldSet(GenericFieldSet):
    def seekBit(self, address, relative=True):
        if not relative:
            address -= self.absolute_address
        if address < 0:
            raise ParserError("Seek below field set start (%s.%s)" % divmod(address, 8))
        self._current_size = address
        return None

    def seekByte(self, address, relative=True):
        return self.seekBit(address*8, relative)

    def _fixLastField(self):
        """
        Try to fix last field when we know current field set size.
        Returns new added field if any, or None.
        """
        assert self._size is not None

        # Stop parser
        message = ["stop parser"]
        self._field_generator = None

        # If last field is too big, delete it
        while self._size < self._current_size:
            field = self._deleteField(len(self._fields)-1)
            message.append("delete field %s" % field.path)
        assert self._current_size <= self._size

        blocks = [(x.absolute_address, x.size) for x in self._fields]
        fields = []
        for start, length in getgaps(self.absolute_address, self._size, blocks):
            self.seekBit(start, relative=False)
            field = createRawField(self, length, "unparsed[]")
            self.setUniqueFieldName(field)
            self._fields.append(field.name, field)
            fields.append(field)
            message.append("found unparsed segment: start %s, length %s" % (start, length))

        self.seekBit(self._size, relative=False)
        message = ", ".join(message)
        if fields:
            self.warning("[Autofix] Fix parser error: " + message)
        return fields

    def _stopFeeding(self):
        new_field = None
        if self._size is None:
            if self._parent:
                self._size = self._current_size

        new_field = self._fixLastField()
        self._field_generator = None
        return new_field

class NewSeekableFieldSet(NewRootSeekableFieldSet):
    def __init__(self, parent, name, description=None, size=None):
        assert issubclass(parent.__class__, BasicFieldSet)
        NewRootSeekableFieldSet.__init__(self, parent, name, parent.stream, description, size)

########NEW FILE########
__FILENAME__ = padding
from lib.hachoir_core.field import Bits, Bytes
from lib.hachoir_core.tools import makePrintable, humanFilesize
from lib.hachoir_core import config

class PaddingBits(Bits):
    """
    Padding bits used, for example, to align address (of next field).
    See also NullBits and PaddingBytes types.

    Arguments:
     * nbits: Size of the field in bits

    Optional arguments:
     * pattern (int): Content pattern, eg. 0 if all bits are set to 0
    """
    static_size = staticmethod(lambda *args, **kw: args[1])
    MAX_SIZE = 128

    def __init__(self, parent, name, nbits, description="Padding", pattern=None):
        Bits.__init__(self, parent, name, nbits, description)
        self.pattern = pattern
        self._display_pattern = self.checkPattern()

    def checkPattern(self):
        if not(config.check_padding_pattern):
            return False
        if self.pattern != 0:
            return False

        if self.MAX_SIZE < self._size:
            value = self._parent.stream.readBits(
                self.absolute_address, self.MAX_SIZE, self._parent.endian)
        else:
            value = self.value
        if value != 0:
            self.warning("padding contents doesn't look normal (invalid pattern)")
            return False
        if self.MAX_SIZE < self._size:
            self.info("only check first %u bits" % self.MAX_SIZE)
        return True

    def createDisplay(self):
        if self._display_pattern:
            return u"<padding pattern=%s>" % self.pattern
        else:
            return Bits.createDisplay(self)

class PaddingBytes(Bytes):
    """
    Padding bytes used, for example, to align address (of next field).
    See also NullBytes and PaddingBits types.

    Arguments:
     * nbytes: Size of the field in bytes

    Optional arguments:
     * pattern (str): Content pattern, eg. "\0" for nul bytes
    """

    static_size = staticmethod(lambda *args, **kw: args[1]*8)
    MAX_SIZE = 4096

    def __init__(self, parent, name, nbytes,
    description="Padding", pattern=None):
        """ pattern is None or repeated string """
        assert (pattern is None) or (isinstance(pattern, str))
        Bytes.__init__(self, parent, name, nbytes, description)
        self.pattern = pattern
        self._display_pattern = self.checkPattern()

    def checkPattern(self):
        if not(config.check_padding_pattern):
            return False
        if self.pattern is None:
            return False

        if self.MAX_SIZE < self._size/8:
            self.info("only check first %s of padding" % humanFilesize(self.MAX_SIZE))
            content = self._parent.stream.readBytes(
                self.absolute_address, self.MAX_SIZE)
        else:
            content = self.value
        index = 0
        pattern_len = len(self.pattern)
        while index < len(content):
            if content[index:index+pattern_len] != self.pattern:
                self.warning(
                    "padding contents doesn't look normal"
                    " (invalid pattern at byte %u)!"
                    % index)
                return False
            index += pattern_len
        return True

    def createDisplay(self):
        if self._display_pattern:
            return u"<padding pattern=%s>" % makePrintable(self.pattern, "ASCII", quote="'")
        else:
            return Bytes.createDisplay(self)

    def createRawDisplay(self):
        return Bytes.createDisplay(self)

class NullBits(PaddingBits):
    """
    Null padding bits used, for example, to align address (of next field).
    See also PaddingBits and NullBytes types.

    Arguments:
     * nbits: Size of the field in bits
    """

    def __init__(self, parent, name, nbits, description=None):
        PaddingBits.__init__(self, parent, name, nbits, description, pattern=0)

    def createDisplay(self):
        if self._display_pattern:
            return "<null>"
        else:
            return Bits.createDisplay(self)

class NullBytes(PaddingBytes):
    """
    Null padding bytes used, for example, to align address (of next field).
    See also PaddingBytes and NullBits types.

    Arguments:
     * nbytes: Size of the field in bytes
    """
    def __init__(self, parent, name, nbytes, description=None):
        PaddingBytes.__init__(self, parent, name, nbytes, description, pattern="\0")

    def createDisplay(self):
        if self._display_pattern:
            return "<null>"
        else:
            return Bytes.createDisplay(self)


########NEW FILE########
__FILENAME__ = parser
from lib.hachoir_core.endian import BIG_ENDIAN, LITTLE_ENDIAN
from lib.hachoir_core.field import GenericFieldSet
from lib.hachoir_core.log import Logger
import lib.hachoir_core.config as config

class Parser(GenericFieldSet):
    """
    A parser is the root of all other fields. It create first level of fields
    and have special attributes and methods:
    - endian: Byte order (L{BIG_ENDIAN} or L{LITTLE_ENDIAN}) of input data ;
    - stream: Data input stream (set in L{__init__()}) ;
    - size: Field set size will be size of input stream.
    """

    def __init__(self, stream, description=None):
        """
        Parser constructor

        @param stream: Data input stream (see L{InputStream})
        @param description: (optional) String description
        """
        # Check arguments
        assert hasattr(self, "endian") \
            and self.endian in (BIG_ENDIAN, LITTLE_ENDIAN)

        # Call parent constructor
        GenericFieldSet.__init__(self, None, "root", stream, description, stream.askSize(self))

    def _logger(self):
        return Logger._logger(self)

    def _setSize(self, size):
        self._truncate(size)
        self.raiseEvent("field-resized", self)
    size = property(lambda self: self._size, doc="Size in bits")

    path = property(lambda self: "/")

    # dummy definition to prevent hachoir-core from depending on hachoir-parser
    autofix = property(lambda self: config.autofix)

########NEW FILE########
__FILENAME__ = seekable_field_set
from lib.hachoir_core.field import Field, BasicFieldSet, FakeArray, MissingField, ParserError
from lib.hachoir_core.tools import makeUnicode
from lib.hachoir_core.error import HACHOIR_ERRORS
from itertools import repeat
import lib.hachoir_core.config as config

class RootSeekableFieldSet(BasicFieldSet):
    def __init__(self, parent, name, stream, description, size):
        BasicFieldSet.__init__(self, parent, name, stream, description, size)
        self._generator = self.createFields()
        self._offset = 0
        self._current_size = 0
        if size:
            self._current_max_size = size
        else:
            self._current_max_size = 0
        self._field_dict = {}
        self._field_array = []

    def _feedOne(self):
        assert self._generator
        field = self._generator.next()
        self._addField(field)
        return field

    def array(self, key):
        return FakeArray(self, key)

    def getFieldByAddress(self, address, feed=True):
        for field in self._field_array:
            if field.address <= address < field.address + field.size:
                return field
        for field in self._readFields():
            if field.address <= address < field.address + field.size:
                return field
        return None

    def _stopFeed(self):
        self._size = self._current_max_size
        self._generator = None
    done = property(lambda self: not bool(self._generator))

    def _getSize(self):
        if self._size is None:
            self._feedAll()
        return self._size
    size = property(_getSize)

    def _getField(self, key, const):
        field = Field._getField(self, key, const)
        if field is not None:
            return field
        if key in self._field_dict:
            return self._field_dict[key]
        if self._generator and not const:
            try:
                while True:
                    field = self._feedOne()
                    if field.name == key:
                        return field
            except StopIteration:
                self._stopFeed()
            except HACHOIR_ERRORS, err:
                self.error("Error: %s" % makeUnicode(err))
                self._stopFeed()
        return None

    def getField(self, key, const=True):
        if isinstance(key, (int, long)):
            if key < 0:
                raise KeyError("Key must be positive!")
            if not const:
                self.readFirstFields(key+1)
            if len(self._field_array) <= key:
                raise MissingField(self, key)
            return self._field_array[key]
        return Field.getField(self, key, const)

    def _addField(self, field):
        if field._name.endswith("[]"):
            self.setUniqueFieldName(field)
        if config.debug:
            self.info("[+] DBG: _addField(%s)" % field.name)

        if field._address != self._offset:
            self.warning("Set field %s address to %s (was %s)" % (
                field.path, self._offset//8, field._address//8))
            field._address = self._offset
        assert field.name not in self._field_dict

        self._checkFieldSize(field)

        self._field_dict[field.name] = field
        self._field_array.append(field)
        self._current_size += field.size
        self._offset += field.size
        self._current_max_size = max(self._current_max_size, field.address + field.size)

    def _checkAddress(self, address):
        if self._size is not None:
            max_addr = self._size
        else:
            # FIXME: Use parent size
            max_addr = self.stream.size
        return address < max_addr

    def _checkFieldSize(self, field):
        size = field.size
        addr = field.address
        if not self._checkAddress(addr+size-1):
            raise ParserError("Unable to add %s: field is too large" % field.name)

    def seekBit(self, address, relative=True):
        if not relative:
            address -= self.absolute_address
        if address < 0:
            raise ParserError("Seek below field set start (%s.%s)" % divmod(address, 8))
        if not self._checkAddress(address):
            raise ParserError("Seek above field set end (%s.%s)" % divmod(address, 8))
        self._offset = address
        return None

    def seekByte(self, address, relative=True):
        return self.seekBit(address*8, relative)

    def readMoreFields(self, number):
        return self._readMoreFields(xrange(number))

    def _feedAll(self):
        return self._readMoreFields(repeat(1))

    def _readFields(self):
        while True:
            added = self._readMoreFields(xrange(1))
            if not added:
                break
            yield self._field_array[-1]

    def _readMoreFields(self, index_generator):
        added = 0
        if self._generator:
            try:
                for index in index_generator:
                    self._feedOne()
                    added += 1
            except StopIteration:
                self._stopFeed()
            except HACHOIR_ERRORS, err:
                self.error("Error: %s" % makeUnicode(err))
                self._stopFeed()
        return added

    current_length = property(lambda self: len(self._field_array))
    current_size = property(lambda self: self._offset)

    def __iter__(self):
        for field in self._field_array:
            yield field
        if self._generator:
            try:
                while True:
                    yield self._feedOne()
            except StopIteration:
                self._stopFeed()
                raise StopIteration

    def __len__(self):
        if self._generator:
            self._feedAll()
        return len(self._field_array)

    def nextFieldAddress(self):
        return self._offset

    def getFieldIndex(self, field):
        return self._field_array.index(field)

class SeekableFieldSet(RootSeekableFieldSet):
    def __init__(self, parent, name, description=None, size=None):
        assert issubclass(parent.__class__, BasicFieldSet)
        RootSeekableFieldSet.__init__(self, parent, name, parent.stream, description, size)


########NEW FILE########
__FILENAME__ = static_field_set
from lib.hachoir_core.field import FieldSet, ParserError

class StaticFieldSet(FieldSet):
    """
    Static field set: format class attribute is a tuple of all fields
    in syntax like:
       format = (
          (TYPE1, ARG1, ARG2, ...),
          (TYPE2, ARG1, ARG2, ..., {KEY1=VALUE1, ...}),
          ...
       )

    Types with dynamic size are forbidden, eg. CString, PascalString8, etc.
    """
    format = None  # You have to redefine this class variable
    _class = None

    def __new__(cls, *args, **kw):
        assert cls.format is not None, "Class attribute 'format' is not set"
        if cls._class is not cls.__name__:
            cls._class = cls.__name__
            cls.static_size = cls._computeStaticSize()
        return object.__new__(cls)

    @staticmethod
    def _computeItemSize(item):
        item_class = item[0]
        if item_class.static_size is None:
            raise ParserError("Unable to get static size of field type: %s"
                % item_class.__name__)
        if callable(item_class.static_size):
            if isinstance(item[-1], dict):
                return item_class.static_size(*item[1:-1], **item[-1])
            else:
                return item_class.static_size(*item[1:])
        else:
            assert isinstance(item_class.static_size, (int, long))
            return item_class.static_size

    def createFields(self):
        for item in self.format:
            if isinstance(item[-1], dict):
                yield item[0](self, *item[1:-1], **item[-1])
            else:
                yield item[0](self, *item[1:])

    @classmethod
    def _computeStaticSize(cls, *args):
        return sum(cls._computeItemSize(item) for item in cls.format)

    # Initial value of static_size, it changes when first instance
    # is created (see __new__)
    static_size = _computeStaticSize


########NEW FILE########
__FILENAME__ = string_field
"""
String field classes:
- String: Fixed length string (no prefix/no suffix) ;
- CString: String which ends with nul byte ("\0") ;
- UnixLine: Unix line of text, string which ends with "\n" ;
- PascalString8, PascalString16, PascalString32: String prefixed with
  length written in a 8, 16, 32-bit integer (use parent endian).

Constructor has optional arguments:
- strip: value can be a string or True ;
- charset: if set, convert string to unicode using this charset (in "replace"
  mode which replace all buggy characters with ".").

Note: For PascalStringXX, prefixed value is the number of bytes and not
      of characters!
"""

from lib.hachoir_core.field import FieldError, Bytes
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN
from lib.hachoir_core.tools import alignValue, makePrintable
from lib.hachoir_core.i18n import guessBytesCharset, _
from lib.hachoir_core import config
from codecs import BOM_UTF16_LE, BOM_UTF16_BE, BOM_UTF32_LE, BOM_UTF32_BE

# Default charset used to convert byte string to Unicode
# This charset is used if no charset is specified or on conversion error
FALLBACK_CHARSET = "ISO-8859-1"

class GenericString(Bytes):
    """
    Generic string class.

    charset have to be in CHARSET_8BIT or in UTF_CHARSET.
    """

    VALID_FORMATS = ("C", "UnixLine",
        "fixed", "Pascal8", "Pascal16", "Pascal32")

    # 8-bit charsets
    CHARSET_8BIT = set((
        "ASCII",          # ANSI X3.4-1968
        "MacRoman",
        "CP037",          # EBCDIC 037
        "CP874",          # Thai
        "WINDOWS-1250",   # Central Europe
        "WINDOWS-1251",   # Cyrillic
        "WINDOWS-1252",   # Latin I
        "WINDOWS-1253",   # Greek
        "WINDOWS-1254",   # Turkish
        "WINDOWS-1255",   # Hebrew
        "WINDOWS-1256",   # Arabic
        "WINDOWS-1257",   # Baltic
        "WINDOWS-1258",   # Vietnam
        "ISO-8859-1",     # Latin-1
        "ISO-8859-2",     # Latin-2
        "ISO-8859-3",     # Latin-3
        "ISO-8859-4",     # Latin-4
        "ISO-8859-5",
        "ISO-8859-6",
        "ISO-8859-7",
        "ISO-8859-8",
        "ISO-8859-9",     # Latin-5
        "ISO-8859-10",    # Latin-6
        "ISO-8859-11",    # Thai
        "ISO-8859-13",    # Latin-7
        "ISO-8859-14",    # Latin-8
        "ISO-8859-15",    # Latin-9 or ("Latin-0")
        "ISO-8859-16",    # Latin-10
    ))

    # UTF-xx charset familly
    UTF_CHARSET = {
        "UTF-8": (8, None),
        "UTF-16-LE": (16, LITTLE_ENDIAN),
        "UTF-32LE": (32, LITTLE_ENDIAN),
        "UTF-16-BE": (16, BIG_ENDIAN),
        "UTF-32BE": (32, BIG_ENDIAN),
        "UTF-16": (16, "BOM"),
        "UTF-32": (32, "BOM"),
    }

    # UTF-xx BOM => charset with endian
    UTF_BOM = {
        16: {BOM_UTF16_LE: "UTF-16-LE", BOM_UTF16_BE: "UTF-16-BE"},
        32: {BOM_UTF32_LE: "UTF-32LE", BOM_UTF32_BE: "UTF-32BE"},
    }

    # Suffix format: value is suffix (string)
    SUFFIX_FORMAT = {
        "C": {
             8: {LITTLE_ENDIAN: "\0",       BIG_ENDIAN: "\0"},
            16: {LITTLE_ENDIAN: "\0\0",     BIG_ENDIAN: "\0\0"},
            32: {LITTLE_ENDIAN: "\0\0\0\0", BIG_ENDIAN: "\0\0\0\0"},
        },
        "UnixLine": {
             8: {LITTLE_ENDIAN: "\n",       BIG_ENDIAN: "\n"},
            16: {LITTLE_ENDIAN: "\n\0",     BIG_ENDIAN: "\0\n"},
            32: {LITTLE_ENDIAN: "\n\0\0\0", BIG_ENDIAN: "\0\0\0\n"},
        },

    }

    # Pascal format: value is the size of the prefix in bits
    PASCAL_FORMATS = {
        "Pascal8":  1,
        "Pascal16": 2,
        "Pascal32": 4
    }

    # Raw value: with prefix and suffix, not stripped,
    # and not converted to Unicode
    _raw_value = None

    def __init__(self, parent, name, format, description=None,
    strip=None, charset=None, nbytes=None, truncate=None):
        Bytes.__init__(self, parent, name, 1, description)

        # Is format valid?
        assert format in self.VALID_FORMATS

        # Store options
        self._format = format
        self._strip = strip
        self._truncate = truncate

        # Check charset and compute character size in bytes
        # (or None when it's not possible to guess character size)
        if not charset or charset in self.CHARSET_8BIT:
            self._character_size = 1   # one byte per character
        elif charset in self.UTF_CHARSET:
            self._character_size = None
        else:
            raise FieldError("Invalid charset for %s: \"%s\"" %
                (self.path, charset))
        self._charset = charset

        # It is a fixed string?
        if nbytes is not None:
            assert self._format == "fixed"
            # Arbitrary limits, just to catch some bugs...
            if not (1 <= nbytes <= 0xffff):
                raise FieldError("Invalid string size for %s: %s" %
                    (self.path, nbytes))
            self._content_size = nbytes   # content length in bytes
            self._size = nbytes * 8
            self._content_offset = 0
        else:
            # Format with a suffix: Find the end of the string
            if self._format in self.SUFFIX_FORMAT:
                self._content_offset = 0

                # Choose the suffix
                suffix = self.suffix_str

                # Find the suffix
                length = self._parent.stream.searchBytesLength(
                    suffix, False, self.absolute_address)
                if length is None:
                    raise FieldError("Unable to find end of string %s (format %s)!"
                        % (self.path, self._format))
                if 1 < len(suffix):
                    # Fix length for little endian bug with UTF-xx charset:
                    #   u"abc" -> "a\0b\0c\0\0\0" (UTF-16-LE)
                    #   search returns length=5, whereas real lenght is 6
                    length = alignValue(length, len(suffix))

                # Compute sizes
                self._content_size = length # in bytes
                self._size = (length + len(suffix)) * 8

            # Format with a prefix: Read prefixed length in bytes
            else:
                assert self._format in self.PASCAL_FORMATS

                # Get the prefix size
                prefix_size = self.PASCAL_FORMATS[self._format]
                self._content_offset = prefix_size

                # Read the prefix and compute sizes
                value = self._parent.stream.readBits(
                    self.absolute_address, prefix_size*8, self._parent.endian)
                self._content_size = value   # in bytes
                self._size = (prefix_size + value) * 8

        # For UTF-16 and UTF-32, choose the right charset using BOM
        if self._charset in self.UTF_CHARSET:
            # Charset requires a BOM?
            bomsize, endian  = self.UTF_CHARSET[self._charset]
            if endian == "BOM":
                # Read the BOM value
                nbytes = bomsize // 8
                bom = self._parent.stream.readBytes(self.absolute_address, nbytes)

                # Choose right charset using the BOM
                bom_endian = self.UTF_BOM[bomsize]
                if bom not in bom_endian:
                    raise FieldError("String %s has invalid BOM (%s)!"
                        % (self.path, repr(bom)))
                self._charset = bom_endian[bom]
                self._content_size -= nbytes
                self._content_offset += nbytes

        # Compute length in character if possible
        if self._character_size:
            self._length = self._content_size //  self._character_size
        else:
            self._length = None

    @staticmethod
    def staticSuffixStr(format, charset, endian):
        if format not in GenericString.SUFFIX_FORMAT:
            return ''
        suffix = GenericString.SUFFIX_FORMAT[format]
        if charset in GenericString.UTF_CHARSET:
            suffix_size = GenericString.UTF_CHARSET[charset][0]
            suffix = suffix[suffix_size]
        else:
            suffix = suffix[8]
        return suffix[endian]

    def _getSuffixStr(self):
        return self.staticSuffixStr(
            self._format, self._charset, self._parent.endian)
    suffix_str = property(_getSuffixStr)

    def _convertText(self, text):
        if not self._charset:
            # charset is still unknown: guess the charset
            self._charset = guessBytesCharset(text, default=FALLBACK_CHARSET)

        # Try to convert to Unicode
        try:
            return unicode(text, self._charset, "strict")
        except UnicodeDecodeError, err:
            pass

        #--- Conversion error ---

        # Fix truncated UTF-16 string like 'B\0e' (3 bytes)
        # => Add missing nul byte: 'B\0e\0' (4 bytes)
        if err.reason == "truncated data" \
        and err.end == len(text) \
        and self._charset == "UTF-16-LE":
            try:
                text = unicode(text+"\0", self._charset, "strict")
                self.warning("Fix truncated %s string: add missing nul byte" % self._charset)
                return text
            except UnicodeDecodeError, err:
                pass

        # On error, use FALLBACK_CHARSET
        self.warning(u"Unable to convert string to Unicode: %s" % err)
        return unicode(text, FALLBACK_CHARSET, "strict")

    def _guessCharset(self):
        addr = self.absolute_address + self._content_offset * 8
        bytes = self._parent.stream.readBytes(addr, self._content_size)
        return guessBytesCharset(bytes, default=FALLBACK_CHARSET)

    def createValue(self, human=True):
        # Compress data address (in bits) and size (in bytes)
        if human:
            addr = self.absolute_address + self._content_offset * 8
            size = self._content_size
        else:
            addr = self.absolute_address
            size = self._size // 8
        if size == 0:
            # Empty string
            return u""

        # Read bytes in data stream
        text = self._parent.stream.readBytes(addr, size)

        # Don't transform data?
        if not human:
            return text

        # Convert text to Unicode
        text = self._convertText(text)

        # Truncate
        if self._truncate:
            pos = text.find(self._truncate)
            if 0 <= pos:
                text = text[:pos]

        # Strip string if needed
        if self._strip:
            if isinstance(self._strip, (str, unicode)):
                text = text.strip(self._strip)
            else:
                text = text.strip()
        assert isinstance(text, unicode)
        return text

    def createDisplay(self, human=True):
        if not human:
            if self._raw_value is None:
                self._raw_value = GenericString.createValue(self, False)
            value = makePrintable(self._raw_value, "ASCII", to_unicode=True)
        elif self._charset:
            value = makePrintable(self.value, "ISO-8859-1", to_unicode=True)
        else:
            value = self.value
        if config.max_string_length < len(value):
            # Truncate string if needed
            value = "%s(...)" % value[:config.max_string_length]
        if not self._charset or not human:
            return makePrintable(value, "ASCII", quote='"', to_unicode=True)
        else:
            if value:
                return '"%s"' % value.replace('"', '\\"')
            else:
                return _("(empty)")

    def createRawDisplay(self):
        return GenericString.createDisplay(self, human=False)

    def _getLength(self):
        if self._length is None:
            self._length = len(self.value)
        return self._length
    length = property(_getLength, doc="String length in characters")

    def _getFormat(self):
        return self._format
    format = property(_getFormat, doc="String format (eg. 'C')")

    def _getCharset(self):
        if not self._charset:
            self._charset = self._guessCharset()
        return self._charset
    charset = property(_getCharset, doc="String charset (eg. 'ISO-8859-1')")

    def _getContentSize(self):
        return self._content_size
    content_size = property(_getContentSize, doc="Content size in bytes")

    def _getContentOffset(self):
        return self._content_offset
    content_offset = property(_getContentOffset, doc="Content offset in bytes")

    def getFieldType(self):
        info = self.charset
        if self._strip:
            if isinstance(self._strip, (str, unicode)):
                info += ",strip=%s" % makePrintable(self._strip, "ASCII", quote="'")
            else:
                info += ",strip=True"
        return "%s<%s>" % (Bytes.getFieldType(self), info)

def stringFactory(name, format, doc):
    class NewString(GenericString):
        __doc__ = doc
        def __init__(self, parent, name, description=None,
        strip=None, charset=None, truncate=None):
            GenericString.__init__(self, parent, name, format, description,
            strip=strip, charset=charset, truncate=truncate)
    cls = NewString
    cls.__name__ = name
    return cls

# String which ends with nul byte ("\0")
CString = stringFactory("CString", "C",
    r"""C string: string ending with nul byte.
See GenericString to get more information.""")

# Unix line of text: string which ends with "\n" (ASCII 0x0A)
UnixLine = stringFactory("UnixLine", "UnixLine",
    r"""Unix line: string ending with "\n" (ASCII code 10).
See GenericString to get more information.""")

# String prefixed with length written in a 8-bit integer
PascalString8 = stringFactory("PascalString8", "Pascal8",
    r"""Pascal string: string prefixed with 8-bit integer containing its length (endian depends on parent endian).
See GenericString to get more information.""")

# String prefixed with length written in a 16-bit integer (use parent endian)
PascalString16 = stringFactory("PascalString16", "Pascal16",
    r"""Pascal string: string prefixed with 16-bit integer containing its length (endian depends on parent endian).
See GenericString to get more information.""")

# String prefixed with length written in a 32-bit integer (use parent endian)
PascalString32 = stringFactory("PascalString32", "Pascal32",
    r"""Pascal string: string prefixed with 32-bit integer containing its length (endian depends on parent endian).
See GenericString to get more information.""")


class String(GenericString):
    """
    String with fixed size (size in bytes).
    See GenericString to get more information.
    """
    static_size = staticmethod(lambda *args, **kw: args[1]*8)

    def __init__(self, parent, name, nbytes, description=None,
    strip=None, charset=None, truncate=None):
        GenericString.__init__(self, parent, name, "fixed", description,
            strip=strip, charset=charset, nbytes=nbytes, truncate=truncate)
String.__name__ = "FixedString"


########NEW FILE########
__FILENAME__ = sub_file
from lib.hachoir_core.field import Bytes
from lib.hachoir_core.tools import makePrintable, humanFilesize
from lib.hachoir_core.stream import InputIOStream

class SubFile(Bytes):
    """
    File stored in another file
    """
    def __init__(self, parent, name, length, description=None,
    parser=None, filename=None, mime_type=None, parser_class=None):
        if filename:
            if not isinstance(filename, unicode):
                filename = makePrintable(filename, "ISO-8859-1")
            if not description:
                description = 'File "%s" (%s)' % (filename, humanFilesize(length))
        Bytes.__init__(self, parent, name, length, description)
        def createInputStream(cis, **args):
            tags = args.setdefault("tags",[])
            if parser_class:
                tags.append(( "class", parser_class ))
            if parser is not None:
                tags.append(( "id", parser.PARSER_TAGS["id"] ))
            if mime_type:
                tags.append(( "mime", mime_type ))
            if filename:
                tags.append(( "filename", filename ))
            return cis(**args)
        self.setSubIStream(createInputStream)

class CompressedStream:
    offset = 0

    def __init__(self, stream, decompressor):
        self.stream = stream
        self.decompressor = decompressor(stream)
        self._buffer = ''

    def read(self, size):
        d = self._buffer
        data = [ d[:size] ]
        size -= len(d)
        if size > 0:
            d = self.decompressor(size)
            data.append(d[:size])
            size -= len(d)
            while size > 0:
                n = 4096
                if self.stream.size:
                    n = min(self.stream.size - self.offset, n)
                    if not n:
                        break
                d = self.stream.read(self.offset, n)[1]
                self.offset += 8 * len(d)
                d = self.decompressor(size, d)
                data.append(d[:size])
                size -= len(d)
        self._buffer = d[size+len(d):]
        return ''.join(data)

def CompressedField(field, decompressor):
    def createInputStream(cis, source=None, **args):
        if field._parent:
            stream = cis(source=source)
            args.setdefault("tags", []).extend(stream.tags)
        else:
            stream = field.stream
        input = CompressedStream(stream, decompressor)
        if source is None:
            source = "Compressed source: '%s' (offset=%s)" % (stream.source, field.absolute_address)
        return InputIOStream(input, source=source, **args)
    field.setSubIStream(createInputStream)
    return field

########NEW FILE########
__FILENAME__ = timestamp
from lib.hachoir_core.tools import (humanDatetime, humanDuration,
    timestampUNIX, timestampMac32, timestampUUID60,
    timestampWin64, durationWin64)
from lib.hachoir_core.field import Bits, FieldSet
from datetime import datetime

class GenericTimestamp(Bits):
    def __init__(self, parent, name, size, description=None):
        Bits.__init__(self, parent, name, size, description)

    def createDisplay(self):
        return humanDatetime(self.value)

    def createRawDisplay(self):
        value = Bits.createValue(self)
        return unicode(value)

    def __nonzero__(self):
        return Bits.createValue(self) != 0

def timestampFactory(cls_name, handler, size):
    class Timestamp(GenericTimestamp):
        def __init__(self, parent, name, description=None):
            GenericTimestamp.__init__(self, parent, name, size, description)

        def createValue(self):
            value = Bits.createValue(self)
            return handler(value)
    cls = Timestamp
    cls.__name__ = cls_name
    return cls

TimestampUnix32 = timestampFactory("TimestampUnix32", timestampUNIX, 32)
TimestampUnix64 = timestampFactory("TimestampUnix64", timestampUNIX, 64)
TimestampMac32 = timestampFactory("TimestampUnix32", timestampMac32, 32)
TimestampUUID60 = timestampFactory("TimestampUUID60", timestampUUID60, 60)
TimestampWin64 = timestampFactory("TimestampWin64", timestampWin64, 64)

class TimeDateMSDOS32(FieldSet):
    """
    32-bit MS-DOS timestamp (16-bit time, 16-bit date)
    """
    static_size = 32

    def createFields(self):
        # TODO: Create type "MSDOS_Second" : value*2
        yield Bits(self, "second", 5, "Second/2")
        yield Bits(self, "minute", 6)
        yield Bits(self, "hour", 5)

        yield Bits(self, "day", 5)
        yield Bits(self, "month", 4)
        # TODO: Create type "MSDOS_Year" : value+1980
        yield Bits(self, "year", 7, "Number of year after 1980")

    def createValue(self):
        return datetime(
            1980+self["year"].value, self["month"].value, self["day"].value,
            self["hour"].value, self["minute"].value, 2*self["second"].value)

    def createDisplay(self):
        return humanDatetime(self.value)

class DateTimeMSDOS32(TimeDateMSDOS32):
    """
    32-bit MS-DOS timestamp (16-bit date, 16-bit time)
    """
    def createFields(self):
        yield Bits(self, "day", 5)
        yield Bits(self, "month", 4)
        yield Bits(self, "year", 7, "Number of year after 1980")
        yield Bits(self, "second", 5, "Second/2")
        yield Bits(self, "minute", 6)
        yield Bits(self, "hour", 5)

class TimedeltaWin64(GenericTimestamp):
    def __init__(self, parent, name, description=None):
        GenericTimestamp.__init__(self, parent, name, 64, description)

    def createDisplay(self):
        return humanDuration(self.value)

    def createValue(self):
        value = Bits.createValue(self)
        return durationWin64(value)


########NEW FILE########
__FILENAME__ = vector
from lib.hachoir_core.field import Field, FieldSet, ParserError

class GenericVector(FieldSet):
    def __init__(self, parent, name, nb_items, item_class, item_name="item", description=None):
        # Sanity checks
        assert issubclass(item_class, Field)
        assert isinstance(item_class.static_size, (int, long))
        if not(0 < nb_items):
            raise ParserError('Unable to create empty vector "%s" in %s' \
                % (name, parent.path))
        size = nb_items * item_class.static_size
        self.__nb_items = nb_items
        self._item_class = item_class
        self._item_name = item_name
        FieldSet.__init__(self, parent, name, description, size=size)

    def __len__(self):
        return self.__nb_items

    def createFields(self):
        name = self._item_name + "[]"
        parser = self._item_class
        for index in xrange(len(self)):
            yield parser(self, name)

class UserVector(GenericVector):
    """
    To implement:
    - item_name: name of a field without [] (eg. "color" becomes "color[0]"),
      default value is "item"
    - item_class: class of an item
    """
    item_class = None
    item_name = "item"

    def __init__(self, parent, name, nb_items, description=None):
        GenericVector.__init__(self, parent, name, nb_items, self.item_class, self.item_name, description)


########NEW FILE########
__FILENAME__ = i18n
# -*- coding: UTF-8 -*-
"""
Functions to manage internationalisation (i18n):
- initLocale(): setup locales and install Unicode compatible stdout and
  stderr ;
- getTerminalCharset(): guess terminal charset ;
- gettext(text) translate a string to current language. The function always
  returns Unicode string. You can also use the alias: _() ;
- ngettext(singular, plural, count): translate a sentence with singular and
  plural form. The function always returns Unicode string.

WARNING: Loading this module indirectly calls initLocale() which sets
         locale LC_ALL to ''. This is needed to get user preferred locale
         settings.
"""

import lib.hachoir_core.config as config
import lib.hachoir_core
import locale
from os import path
import sys
from codecs import BOM_UTF8, BOM_UTF16_LE, BOM_UTF16_BE

def _getTerminalCharset():
    """
    Function used by getTerminalCharset() to get terminal charset.

    @see getTerminalCharset()
    """
    # (1) Try locale.getpreferredencoding()
    try:
        charset = locale.getpreferredencoding()
        if charset:
            return charset
    except (locale.Error, AttributeError):
        pass

    # (2) Try locale.nl_langinfo(CODESET)
    try:
        charset = locale.nl_langinfo(locale.CODESET)
        if charset:
            return charset
    except (locale.Error, AttributeError):
        pass

    # (3) Try sys.stdout.encoding
    if hasattr(sys.stdout, "encoding") and sys.stdout.encoding:
        return sys.stdout.encoding

    # (4) Otherwise, returns "ASCII"
    return "ASCII"

def getTerminalCharset():
    """
    Guess terminal charset using differents tests:
    1. Try locale.getpreferredencoding()
    2. Try locale.nl_langinfo(CODESET)
    3. Try sys.stdout.encoding
    4. Otherwise, returns "ASCII"

    WARNING: Call initLocale() before calling this function.
    """
    try:
        return getTerminalCharset.value
    except AttributeError:
        getTerminalCharset.value = _getTerminalCharset()
        return getTerminalCharset.value

class UnicodeStdout(object):
    def __init__(self, old_device, charset):
        self.device = old_device
        self.charset = charset

    def flush(self):
        self.device.flush()

    def write(self, text):
        if isinstance(text, unicode):
            text = text.encode(self.charset, 'replace')
        self.device.write(text)

    def writelines(self, lines):
        for text in lines:
            self.write(text)

def initLocale():
    # Only initialize locale once
    if initLocale.is_done:
        return getTerminalCharset()
    initLocale.is_done = True

    # Setup locales
    try:
        locale.setlocale(locale.LC_ALL, "")
    except (locale.Error, IOError):
        pass

    # Get the terminal charset
    charset = getTerminalCharset()

    # UnicodeStdout conflicts with the readline module
    if config.unicode_stdout and ('readline' not in sys.modules):
        # Replace stdout and stderr by unicode objet supporting unicode string
        sys.stdout = UnicodeStdout(sys.stdout, charset)
        sys.stderr = UnicodeStdout(sys.stderr, charset)
    return charset
initLocale.is_done = False

def _dummy_gettext(text):
    return unicode(text)

def _dummy_ngettext(singular, plural, count):
    if 1 < abs(count) or not count:
        return unicode(plural)
    else:
        return unicode(singular)

def _initGettext():
    charset = initLocale()

    # Try to load gettext module
    if config.use_i18n:
        try:
            import gettext
            ok = True
        except ImportError:
            ok = False
    else:
        ok = False

    # gettext is not available or not needed: use dummy gettext functions
    if not ok:
        return (_dummy_gettext, _dummy_ngettext)

    # Gettext variables
    package = lib.hachoir_core.PACKAGE
    locale_dir = path.join(path.dirname(__file__), "..", "locale")

    # Initialize gettext module
    gettext.bindtextdomain(package, locale_dir)
    gettext.textdomain(package)
    translate = gettext.gettext
    ngettext = gettext.ngettext

    # TODO: translate_unicode lambda function really sucks!
    # => find native function to do that
    unicode_gettext = lambda text: \
        unicode(translate(text), charset)
    unicode_ngettext = lambda singular, plural, count: \
        unicode(ngettext(singular, plural, count), charset)
    return (unicode_gettext, unicode_ngettext)

UTF_BOMS = (
    (BOM_UTF8, "UTF-8"),
    (BOM_UTF16_LE, "UTF-16-LE"),
    (BOM_UTF16_BE, "UTF-16-BE"),
)

# Set of valid characters for specific charset
CHARSET_CHARACTERS = (
    # U+00E0: LATIN SMALL LETTER A WITH GRAVE
    (set(u"\xE0".encode("ISO-8859-1")), "ISO-8859-1"),
    (set(u"\xE0".encode("ISO-8859-15")), "ISO-8859-15"),
    (set(u"".encode("MacRoman")), "MacRoman"),
    (set(u"".encode("ISO-8859-7")), "ISO-8859-7"),
)

def guessBytesCharset(bytes, default=None):
    r"""
    >>> guessBytesCharset("abc")
    'ASCII'
    >>> guessBytesCharset("\xEF\xBB\xBFabc")
    'UTF-8'
    >>> guessBytesCharset("abc\xC3\xA9")
    'UTF-8'
    >>> guessBytesCharset("File written by Adobe Photoshop\xA8 4.0\0")
    'MacRoman'
    >>> guessBytesCharset("\xE9l\xE9phant")
    'ISO-8859-1'
    >>> guessBytesCharset("100 \xA4")
    'ISO-8859-15'
    >>> guessBytesCharset('Word \xb8\xea\xe4\xef\xf3\xe7 - Microsoft Outlook 97 - \xd1\xf5\xe8\xec\xdf\xf3\xe5\xe9\xf2 e-mail')
    'ISO-8859-7'
    """
    # Check for UTF BOM
    for bom_bytes, charset in UTF_BOMS:
        if bytes.startswith(bom_bytes):
            return charset

    # Pure ASCII?
    try:
        text = unicode(bytes, 'ASCII', 'strict')
        return 'ASCII'
    except UnicodeDecodeError:
        pass

    # Valid UTF-8?
    try:
        text = unicode(bytes, 'UTF-8', 'strict')
        return 'UTF-8'
    except UnicodeDecodeError:
        pass

    # Create a set of non-ASCII characters
    non_ascii_set = set( byte for byte in bytes if ord(byte) >= 128 )
    for characters, charset in CHARSET_CHARACTERS:
        if characters.issuperset(non_ascii_set):
            return charset
    return default

# Initialize _(), gettext() and ngettext() functions
gettext, ngettext = _initGettext()
_ = gettext


########NEW FILE########
__FILENAME__ = iso639
# -*- coding: utf-8 -*-
"""
ISO639-2 standart: the module only contains the dictionary ISO639_2
which maps a language code in three letters (eg. "fre") to a language
name in english (eg. "French").
"""

# ISO-639, the list comes from:
# http://www.loc.gov/standards/iso639-2/php/English_list.php
_ISO639 = (
    (u"Abkhazian", "abk", "ab"),
    (u"Achinese", "ace", None),
    (u"Acoli", "ach", None),
    (u"Adangme", "ada", None),
    (u"Adygei", "ady", None),
    (u"Adyghe", "ady", None),
    (u"Afar", "aar", "aa"),
    (u"Afrihili", "afh", None),
    (u"Afrikaans", "afr", "af"),
    (u"Afro-Asiatic (Other)", "afa", None),
    (u"Ainu", "ain", None),
    (u"Akan", "aka", "ak"),
    (u"Akkadian", "akk", None),
    (u"Albanian", "alb/sqi", "sq"),
    (u"Alemani", "gsw", None),
    (u"Aleut", "ale", None),
    (u"Algonquian languages", "alg", None),
    (u"Altaic (Other)", "tut", None),
    (u"Amharic", "amh", "am"),
    (u"Angika", "anp", None),
    (u"Apache languages", "apa", None),
    (u"Arabic", "ara", "ar"),
    (u"Aragonese", "arg", "an"),
    (u"Aramaic", "arc", None),
    (u"Arapaho", "arp", None),
    (u"Araucanian", "arn", None),
    (u"Arawak", "arw", None),
    (u"Armenian", "arm/hye", "hy"),
    (u"Aromanian", "rup", None),
    (u"Artificial (Other)", "art", None),
    (u"Arumanian", "rup", None),
    (u"Assamese", "asm", "as"),
    (u"Asturian", "ast", None),
    (u"Athapascan languages", "ath", None),
    (u"Australian languages", "aus", None),
    (u"Austronesian (Other)", "map", None),
    (u"Avaric", "ava", "av"),
    (u"Avestan", "ave", "ae"),
    (u"Awadhi", "awa", None),
    (u"Aymara", "aym", "ay"),
    (u"Azerbaijani", "aze", "az"),
    (u"Bable", "ast", None),
    (u"Balinese", "ban", None),
    (u"Baltic (Other)", "bat", None),
    (u"Baluchi", "bal", None),
    (u"Bambara", "bam", "bm"),
    (u"Bamileke languages", "bai", None),
    (u"Banda", "bad", None),
    (u"Bantu (Other)", "bnt", None),
    (u"Basa", "bas", None),
    (u"Bashkir", "bak", "ba"),
    (u"Basque", "baq/eus", "eu"),
    (u"Batak (Indonesia)", "btk", None),
    (u"Beja", "bej", None),
    (u"Belarusian", "bel", "be"),
    (u"Bemba", "bem", None),
    (u"Bengali", "ben", "bn"),
    (u"Berber (Other)", "ber", None),
    (u"Bhojpuri", "bho", None),
    (u"Bihari", "bih", "bh"),
    (u"Bikol", "bik", None),
    (u"Bilin", "byn", None),
    (u"Bini", "bin", None),
    (u"Bislama", "bis", "bi"),
    (u"Blin", "byn", None),
    (u"Bokml, Norwegian", "nob", "nb"),
    (u"Bosnian", "bos", "bs"),
    (u"Braj", "bra", None),
    (u"Breton", "bre", "br"),
    (u"Buginese", "bug", None),
    (u"Bulgarian", "bul", "bg"),
    (u"Buriat", "bua", None),
    (u"Burmese", "bur/mya", "my"),
    (u"Caddo", "cad", None),
    (u"Carib", "car", None),
    (u"Castilian", "spa", "es"),
    (u"Catalan", "cat", "ca"),
    (u"Caucasian (Other)", "cau", None),
    (u"Cebuano", "ceb", None),
    (u"Celtic (Other)", "cel", None),
    (u"Central American Indian (Other)", "cai", None),
    (u"Chagatai", "chg", None),
    (u"Chamic languages", "cmc", None),
    (u"Chamorro", "cha", "ch"),
    (u"Chechen", "che", "ce"),
    (u"Cherokee", "chr", None),
    (u"Chewa", "nya", "ny"),
    (u"Cheyenne", "chy", None),
    (u"Chibcha", "chb", None),
    (u"Chichewa", "nya", "ny"),
    (u"Chinese", "chi/zho", "zh"),
    (u"Chinook jargon", "chn", None),
    (u"Chipewyan", "chp", None),
    (u"Choctaw", "cho", None),
    (u"Chuang", "zha", "za"),
    (u"Church Slavic", "chu", "cu"),
    (u"Church Slavonic", "chu", "cu"),
    (u"Chuukese", "chk", None),
    (u"Chuvash", "chv", "cv"),
    (u"Classical Nepal Bhasa", "nwc", None),
    (u"Classical Newari", "nwc", None),
    (u"Coptic", "cop", None),
    (u"Cornish", "cor", "kw"),
    (u"Corsican", "cos", "co"),
    (u"Cree", "cre", "cr"),
    (u"Creek", "mus", None),
    (u"Creoles and pidgins (Other)", "crp", None),
    (u"Creoles and pidgins, English based (Other)", "cpe", None),
    (u"Creoles and pidgins, French-based (Other)", "cpf", None),
    (u"Creoles and pidgins, Portuguese-based (Other)", "cpp", None),
    (u"Crimean Tatar", "crh", None),
    (u"Crimean Turkish", "crh", None),
    (u"Croatian", "scr/hrv", "hr"),
    (u"Cushitic (Other)", "cus", None),
    (u"Czech", "cze/ces", "cs"),
    (u"Dakota", "dak", None),
    (u"Danish", "dan", "da"),
    (u"Dargwa", "dar", None),
    (u"Dayak", "day", None),
    (u"Delaware", "del", None),
    (u"Dhivehi", "div", "dv"),
    (u"Dimili", "zza", None),
    (u"Dimli", "zza", None),
    (u"Dinka", "din", None),
    (u"Divehi", "div", "dv"),
    (u"Dogri", "doi", None),
    (u"Dogrib", "dgr", None),
    (u"Dravidian (Other)", "dra", None),
    (u"Duala", "dua", None),
    (u"Dutch", "dut/nld", "nl"),
    (u"Dutch, Middle (ca.1050-1350)", "dum", None),
    (u"Dyula", "dyu", None),
    (u"Dzongkha", "dzo", "dz"),
    (u"Eastern Frisian", "frs", None),
    (u"Efik", "efi", None),
    (u"Egyptian (Ancient)", "egy", None),
    (u"Ekajuk", "eka", None),
    (u"Elamite", "elx", None),
    (u"English", "eng", "en"),
    (u"English, Middle (1100-1500)", "enm", None),
    (u"English, Old (ca.450-1100)", "ang", None),
    (u"Erzya", "myv", None),
    (u"Esperanto", "epo", "eo"),
    (u"Estonian", "est", "et"),
    (u"Ewe", "ewe", "ee"),
    (u"Ewondo", "ewo", None),
    (u"Fang", "fan", None),
    (u"Fanti", "fat", None),
    (u"Faroese", "fao", "fo"),
    (u"Fijian", "fij", "fj"),
    (u"Filipino", "fil", None),
    (u"Finnish", "fin", "fi"),
    (u"Finno-Ugrian (Other)", "fiu", None),
    (u"Flemish", "dut/nld", "nl"),
    (u"Fon", "fon", None),
    (u"French", "fre/fra", "fr"),
    (u"French, Middle (ca.1400-1600)", "frm", None),
    (u"French, Old (842-ca.1400)", "fro", None),
    (u"Friulian", "fur", None),
    (u"Fulah", "ful", "ff"),
    (u"Ga", "gaa", None),
    (u"Gaelic", "gla", "gd"),
    (u"Galician", "glg", "gl"),
    (u"Ganda", "lug", "lg"),
    (u"Gayo", "gay", None),
    (u"Gbaya", "gba", None),
    (u"Geez", "gez", None),
    (u"Georgian", "geo/kat", "ka"),
    (u"German", "ger/deu", "de"),
    (u"German, Low", "nds", None),
    (u"German, Middle High (ca.1050-1500)", "gmh", None),
    (u"German, Old High (ca.750-1050)", "goh", None),
    (u"Germanic (Other)", "gem", None),
    (u"Gikuyu", "kik", "ki"),
    (u"Gilbertese", "gil", None),
    (u"Gondi", "gon", None),
    (u"Gorontalo", "gor", None),
    (u"Gothic", "got", None),
    (u"Grebo", "grb", None),
    (u"Greek, Ancient (to 1453)", "grc", None),
    (u"Greek, Modern (1453-)", "gre/ell", "el"),
    (u"Greenlandic", "kal", "kl"),
    (u"Guarani", "grn", "gn"),
    (u"Gujarati", "guj", "gu"),
    (u"Gwichin", "gwi", None),
    (u"Haida", "hai", None),
    (u"Haitian", "hat", "ht"),
    (u"Haitian Creole", "hat", "ht"),
    (u"Hausa", "hau", "ha"),
    (u"Hawaiian", "haw", None),
    (u"Hebrew", "heb", "he"),
    (u"Herero", "her", "hz"),
    (u"Hiligaynon", "hil", None),
    (u"Himachali", "him", None),
    (u"Hindi", "hin", "hi"),
    (u"Hiri Motu", "hmo", "ho"),
    (u"Hittite", "hit", None),
    (u"Hmong", "hmn", None),
    (u"Hungarian", "hun", "hu"),
    (u"Hupa", "hup", None),
    (u"Iban", "iba", None),
    (u"Icelandic", "ice/isl", "is"),
    (u"Ido", "ido", "io"),
    (u"Igbo", "ibo", "ig"),
    (u"Ijo", "ijo", None),
    (u"Iloko", "ilo", None),
    (u"Inari Sami", "smn", None),
    (u"Indic (Other)", "inc", None),
    (u"Indo-European (Other)", "ine", None),
    (u"Indonesian", "ind", "id"),
    (u"Ingush", "inh", None),
    (u"Interlingua", "ina", "ia"),
    (u"Interlingue", "ile", "ie"),
    (u"Inuktitut", "iku", "iu"),
    (u"Inupiaq", "ipk", "ik"),
    (u"Iranian (Other)", "ira", None),
    (u"Irish", "gle", "ga"),
    (u"Irish, Middle (900-1200)", "mga", None),
    (u"Irish, Old (to 900)", "sga", None),
    (u"Iroquoian languages", "iro", None),
    (u"Italian", "ita", "it"),
    (u"Japanese", "jpn", "ja"),
    (u"Javanese", "jav", "jv"),
    (u"Judeo-Arabic", "jrb", None),
    (u"Judeo-Persian", "jpr", None),
    (u"Kabardian", "kbd", None),
    (u"Kabyle", "kab", None),
    (u"Kachin", "kac", None),
    (u"Kalaallisut", "kal", "kl"),
    (u"Kalmyk", "xal", None),
    (u"Kamba", "kam", None),
    (u"Kannada", "kan", "kn"),
    (u"Kanuri", "kau", "kr"),
    (u"Kara-Kalpak", "kaa", None),
    (u"Karachay-Balkar", "krc", None),
    (u"Karelian", "krl", None),
    (u"Karen", "kar", None),
    (u"Kashmiri", "kas", "ks"),
    (u"Kashubian", "csb", None),
    (u"Kawi", "kaw", None),
    (u"Kazakh", "kaz", "kk"),
    (u"Khasi", "kha", None),
    (u"Khmer", "khm", "km"),
    (u"Khoisan (Other)", "khi", None),
    (u"Khotanese", "kho", None),
    (u"Kikuyu", "kik", "ki"),
    (u"Kimbundu", "kmb", None),
    (u"Kinyarwanda", "kin", "rw"),
    (u"Kirdki", "zza", None),
    (u"Kirghiz", "kir", "ky"),
    (u"Kirmanjki", "zza", None),
    (u"Klingon", "tlh", None),
    (u"Komi", "kom", "kv"),
    (u"Kongo", "kon", "kg"),
    (u"Konkani", "kok", None),
    (u"Korean", "kor", "ko"),
    (u"Kosraean", "kos", None),
    (u"Kpelle", "kpe", None),
    (u"Kru", "kro", None),
    (u"Kuanyama", "kua", "kj"),
    (u"Kumyk", "kum", None),
    (u"Kurdish", "kur", "ku"),
    (u"Kurukh", "kru", None),
    (u"Kutenai", "kut", None),
    (u"Kwanyama", "kua", "kj"),
    (u"Ladino", "lad", None),
    (u"Lahnda", "lah", None),
    (u"Lamba", "lam", None),
    (u"Lao", "lao", "lo"),
    (u"Latin", "lat", "la"),
    (u"Latvian", "lav", "lv"),
    (u"Letzeburgesch", "ltz", "lb"),
    (u"Lezghian", "lez", None),
    (u"Limburgan", "lim", "li"),
    (u"Limburger", "lim", "li"),
    (u"Limburgish", "lim", "li"),
    (u"Lingala", "lin", "ln"),
    (u"Lithuanian", "lit", "lt"),
    (u"Lojban", "jbo", None),
    (u"Low German", "nds", None),
    (u"Low Saxon", "nds", None),
    (u"Lower Sorbian", "dsb", None),
    (u"Lozi", "loz", None),
    (u"Luba-Katanga", "lub", "lu"),
    (u"Luba-Lulua", "lua", None),
    (u"Luiseno", "lui", None),
    (u"Lule Sami", "smj", None),
    (u"Lunda", "lun", None),
    (u"Luo (Kenya and Tanzania)", "luo", None),
    (u"Lushai", "lus", None),
    (u"Luxembourgish", "ltz", "lb"),
    (u"Macedo-Romanian", "rup", None),
    (u"Macedonian", "mac/mkd", "mk"),
    (u"Madurese", "mad", None),
    (u"Magahi", "mag", None),
    (u"Maithili", "mai", None),
    (u"Makasar", "mak", None),
    (u"Malagasy", "mlg", "mg"),
    (u"Malay", "may/msa", "ms"),
    (u"Malayalam", "mal", "ml"),
    (u"Maldivian", "div", "dv"),
    (u"Maltese", "mlt", "mt"),
    (u"Manchu", "mnc", None),
    (u"Mandar", "mdr", None),
    (u"Mandingo", "man", None),
    (u"Manipuri", "mni", None),
    (u"Manobo languages", "mno", None),
    (u"Manx", "glv", "gv"),
    (u"Maori", "mao/mri", "mi"),
    (u"Marathi", "mar", "mr"),
    (u"Mari", "chm", None),
    (u"Marshallese", "mah", "mh"),
    (u"Marwari", "mwr", None),
    (u"Masai", "mas", None),
    (u"Mayan languages", "myn", None),
    (u"Mende", "men", None),
    (u"Mi'kmaq", "mic", None),
    (u"Micmac", "mic", None),
    (u"Minangkabau", "min", None),
    (u"Mirandese", "mwl", None),
    (u"Miscellaneous languages", "mis", None),
    (u"Mohawk", "moh", None),
    (u"Moksha", "mdf", None),
    (u"Moldavian", "mol", "mo"),
    (u"Mon-Khmer (Other)", "mkh", None),
    (u"Mongo", "lol", None),
    (u"Mongolian", "mon", "mn"),
    (u"Mossi", "mos", None),
    (u"Multiple languages", "mul", None),
    (u"Munda languages", "mun", None),
    (u"N'Ko", "nqo", None),
    (u"Nahuatl", "nah", None),
    (u"Nauru", "nau", "na"),
    (u"Navaho", "nav", "nv"),
    (u"Navajo", "nav", "nv"),
    (u"Ndebele, North", "nde", "nd"),
    (u"Ndebele, South", "nbl", "nr"),
    (u"Ndonga", "ndo", "ng"),
    (u"Neapolitan", "nap", None),
    (u"Nepal Bhasa", "new", None),
    (u"Nepali", "nep", "ne"),
    (u"Newari", "new", None),
    (u"Nias", "nia", None),
    (u"Niger-Kordofanian (Other)", "nic", None),
    (u"Nilo-Saharan (Other)", "ssa", None),
    (u"Niuean", "niu", None),
    (u"No linguistic content", "zxx", None),
    (u"Nogai", "nog", None),
    (u"Norse, Old", "non", None),
    (u"North American Indian", "nai", None),
    (u"North Ndebele", "nde", "nd"),
    (u"Northern Frisian", "frr", None),
    (u"Northern Sami", "sme", "se"),
    (u"Northern Sotho", "nso", None),
    (u"Norwegian", "nor", "no"),
    (u"Norwegian Bokml", "nob", "nb"),
    (u"Norwegian Nynorsk", "nno", "nn"),
    (u"Nubian languages", "nub", None),
    (u"Nyamwezi", "nym", None),
    (u"Nyanja", "nya", "ny"),
    (u"Nyankole", "nyn", None),
    (u"Nynorsk, Norwegian", "nno", "nn"),
    (u"Nyoro", "nyo", None),
    (u"Nzima", "nzi", None),
    (u"Occitan (post 1500)", "oci", "oc"),
    (u"Oirat", "xal", None),
    (u"Ojibwa", "oji", "oj"),
    (u"Old Bulgarian", "chu", "cu"),
    (u"Old Church Slavonic", "chu", "cu"),
    (u"Old Newari", "nwc", None),
    (u"Old Slavonic", "chu", "cu"),
    (u"Oriya", "ori", "or"),
    (u"Oromo", "orm", "om"),
    (u"Osage", "osa", None),
    (u"Ossetian", "oss", "os"),
    (u"Ossetic", "oss", "os"),
    (u"Otomian languages", "oto", None),
    (u"Pahlavi", "pal", None),
    (u"Palauan", "pau", None),
    (u"Pali", "pli", "pi"),
    (u"Pampanga", "pam", None),
    (u"Pangasinan", "pag", None),
    (u"Panjabi", "pan", "pa"),
    (u"Papiamento", "pap", None),
    (u"Papuan (Other)", "paa", None),
    (u"Pedi", "nso", None),
    (u"Persian", "per/fas", "fa"),
    (u"Persian, Old (ca.600-400 B.C.)", "peo", None),
    (u"Philippine (Other)", "phi", None),
    (u"Phoenician", "phn", None),
    (u"Pilipino", "fil", None),
    (u"Pohnpeian", "pon", None),
    (u"Polish", "pol", "pl"),
    (u"Portuguese", "por", "pt"),
    (u"Prakrit languages", "pra", None),
    (u"Provenal", "oci", "oc"),
    (u"Provenal, Old (to 1500)", "pro", None),
    (u"Punjabi", "pan", "pa"),
    (u"Pushto", "pus", "ps"),
    (u"Quechua", "que", "qu"),
    (u"Raeto-Romance", "roh", "rm"),
    (u"Rajasthani", "raj", None),
    (u"Rapanui", "rap", None),
    (u"Rarotongan", "rar", None),
    (u"Reserved for local use", "qaa/qtz", None),
    (u"Romance (Other)", "roa", None),
    (u"Romanian", "rum/ron", "ro"),
    (u"Romany", "rom", None),
    (u"Rundi", "run", "rn"),
    (u"Russian", "rus", "ru"),
    (u"Salishan languages", "sal", None),
    (u"Samaritan Aramaic", "sam", None),
    (u"Sami languages (Other)", "smi", None),
    (u"Samoan", "smo", "sm"),
    (u"Sandawe", "sad", None),
    (u"Sango", "sag", "sg"),
    (u"Sanskrit", "san", "sa"),
    (u"Santali", "sat", None),
    (u"Sardinian", "srd", "sc"),
    (u"Sasak", "sas", None),
    (u"Saxon, Low", "nds", None),
    (u"Scots", "sco", None),
    (u"Scottish Gaelic", "gla", "gd"),
    (u"Selkup", "sel", None),
    (u"Semitic (Other)", "sem", None),
    (u"Sepedi", "nso", None),
    (u"Serbian", "scc/srp", "sr"),
    (u"Serer", "srr", None),
    (u"Shan", "shn", None),
    (u"Shona", "sna", "sn"),
    (u"Sichuan Yi", "iii", "ii"),
    (u"Sicilian", "scn", None),
    (u"Sidamo", "sid", None),
    (u"Sign Languages", "sgn", None),
    (u"Siksika", "bla", None),
    (u"Sindhi", "snd", "sd"),
    (u"Sinhala", "sin", "si"),
    (u"Sinhalese", "sin", "si"),
    (u"Sino-Tibetan (Other)", "sit", None),
    (u"Siouan languages", "sio", None),
    (u"Skolt Sami", "sms", None),
    (u"Slave (Athapascan)", "den", None),
    (u"Slavic (Other)", "sla", None),
    (u"Slovak", "slo/slk", "sk"),
    (u"Slovenian", "slv", "sl"),
    (u"Sogdian", "sog", None),
    (u"Somali", "som", "so"),
    (u"Songhai", "son", None),
    (u"Soninke", "snk", None),
    (u"Sorbian languages", "wen", None),
    (u"Sotho, Northern", "nso", None),
    (u"Sotho, Southern", "sot", "st"),
    (u"South American Indian (Other)", "sai", None),
    (u"South Ndebele", "nbl", "nr"),
    (u"Southern Altai", "alt", None),
    (u"Southern Sami", "sma", None),
    (u"Spanish", "spa", "es"),
    (u"Sranan Togo", "srn", None),
    (u"Sukuma", "suk", None),
    (u"Sumerian", "sux", None),
    (u"Sundanese", "sun", "su"),
    (u"Susu", "sus", None),
    (u"Swahili", "swa", "sw"),
    (u"Swati", "ssw", "ss"),
    (u"Swedish", "swe", "sv"),
    (u"Swiss German", "gsw", None),
    (u"Syriac", "syr", None),
    (u"Tagalog", "tgl", "tl"),
    (u"Tahitian", "tah", "ty"),
    (u"Tai (Other)", "tai", None),
    (u"Tajik", "tgk", "tg"),
    (u"Tamashek", "tmh", None),
    (u"Tamil", "tam", "ta"),
    (u"Tatar", "tat", "tt"),
    (u"Telugu", "tel", "te"),
    (u"Tereno", "ter", None),
    (u"Tetum", "tet", None),
    (u"Thai", "tha", "th"),
    (u"Tibetan", "tib/bod", "bo"),
    (u"Tigre", "tig", None),
    (u"Tigrinya", "tir", "ti"),
    (u"Timne", "tem", None),
    (u"Tiv", "tiv", None),
    (u"tlhIngan-Hol", "tlh", None),
    (u"Tlingit", "tli", None),
    (u"Tok Pisin", "tpi", None),
    (u"Tokelau", "tkl", None),
    (u"Tonga (Nyasa)", "tog", None),
    (u"Tonga (Tonga Islands)", "ton", "to"),
    (u"Tsimshian", "tsi", None),
    (u"Tsonga", "tso", "ts"),
    (u"Tswana", "tsn", "tn"),
    (u"Tumbuka", "tum", None),
    (u"Tupi languages", "tup", None),
    (u"Turkish", "tur", "tr"),
    (u"Turkish, Ottoman (1500-1928)", "ota", None),
    (u"Turkmen", "tuk", "tk"),
    (u"Tuvalu", "tvl", None),
    (u"Tuvinian", "tyv", None),
    (u"Twi", "twi", "tw"),
    (u"Udmurt", "udm", None),
    (u"Ugaritic", "uga", None),
    (u"Uighur", "uig", "ug"),
    (u"Ukrainian", "ukr", "uk"),
    (u"Umbundu", "umb", None),
    (u"Undetermined", "und", None),
    (u"Upper Sorbian", "hsb", None),
    (u"Urdu", "urd", "ur"),
    (u"Uyghur", "uig", "ug"),
    (u"Uzbek", "uzb", "uz"),
    (u"Vai", "vai", None),
    (u"Valencian", "cat", "ca"),
    (u"Venda", "ven", "ve"),
    (u"Vietnamese", "vie", "vi"),
    (u"Volapk", "vol", "vo"),
    (u"Votic", "vot", None),
    (u"Wakashan languages", "wak", None),
    (u"Walamo", "wal", None),
    (u"Walloon", "wln", "wa"),
    (u"Waray", "war", None),
    (u"Washo", "was", None),
    (u"Welsh", "wel/cym", "cy"),
    (u"Western Frisian", "fry", "fy"),
    (u"Wolof", "wol", "wo"),
    (u"Xhosa", "xho", "xh"),
    (u"Yakut", "sah", None),
    (u"Yao", "yao", None),
    (u"Yapese", "yap", None),
    (u"Yiddish", "yid", "yi"),
    (u"Yoruba", "yor", "yo"),
    (u"Yupik languages", "ypk", None),
    (u"Zande", "znd", None),
    (u"Zapotec", "zap", None),
    (u"Zaza", "zza", None),
    (u"Zazaki", "zza", None),
    (u"Zenaga", "zen", None),
    (u"Zhuang", "zha", "za"),
    (u"Zulu", "zul", "zu"),
    (u"Zuni", "zun", None),
)

# Bibliographic ISO-639-2 form (eg. "fre" => "French")
ISO639_2 = {}
for line in _ISO639:
    for key in line[1].split("/"):
        ISO639_2[key] = line[0]
del _ISO639


########NEW FILE########
__FILENAME__ = language
from lib.hachoir_core.iso639 import ISO639_2

class Language:
    def __init__(self, code):
        code = str(code)
        if code not in ISO639_2:
            raise ValueError("Invalid language code: %r" % code)
        self.code = code

    def __cmp__(self, other):
        if other.__class__ != Language:
            return 1
        return cmp(self.code, other.code)

    def __unicode__(self):
       return ISO639_2[self.code]

    def __str__(self):
       return self.__unicode__()

    def __repr__(self):
        return "<Language '%s', code=%r>" % (unicode(self), self.code)


########NEW FILE########
__FILENAME__ = log
import os, sys, time
import lib.hachoir_core.config as config
from lib.hachoir_core.i18n import _

class Log:
    LOG_INFO   = 0
    LOG_WARN   = 1
    LOG_ERROR  = 2

    level_name = {
        LOG_WARN: "[warn]",
        LOG_ERROR: "[err!]",
        LOG_INFO: "[info]"
    }

    def __init__(self):
        self.__buffer = {}
        self.__file = None
        self.use_print = True
        self.use_buffer = False
        self.on_new_message = None # Prototype: def func(level, prefix, text, context)

    def shutdown(self):
        if self.__file:
            self._writeIntoFile(_("Stop Hachoir"))

    def setFilename(self, filename, append=True):
        """
        Use a file to store all messages. The
        UTF-8 encoding will be used. Write an informative
        message if the file can't be created.

        @param filename: C{L{string}}
        """

        # Look if file already exists or not
        filename = os.path.expanduser(filename)
        filename = os.path.realpath(filename)
        append = os.access(filename, os.F_OK)

        # Create log file (or open it in append mode, if it already exists)
        try:
            import codecs
            if append:
                self.__file = codecs.open(filename, "a", "utf-8")
            else:
                self.__file = codecs.open(filename, "w", "utf-8")
            self._writeIntoFile(_("Starting Hachoir"))
        except IOError, err:
            if err.errno == 2:
                self.__file = None
                self.info(_("[Log] setFilename(%s) fails: no such file") % filename)
            else:
                raise

    def _writeIntoFile(self, message):
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self.__file.write(u"%s - %s\n" % (timestamp, message))
        self.__file.flush()

    def newMessage(self, level, text, ctxt=None):
        """
        Write a new message : append it in the buffer,
        display it to the screen (if needed), and write
        it in the log file (if needed).

        @param level: Message level.
        @type level: C{int}
        @param text: Message content.
        @type text: C{str}
        @param ctxt: The caller instance.
        """

        if level < self.LOG_ERROR and config.quiet or \
           level <= self.LOG_INFO and not config.verbose:
            return
        if config.debug:
            from lib.hachoir_core.error import getBacktrace
            backtrace = getBacktrace(None)
            if backtrace:
                text += "\n\n" + backtrace

        _text = text
        if hasattr(ctxt, "_logger"):
            _ctxt = ctxt._logger()
            if _ctxt is not None:
                text = "[%s] %s" % (_ctxt, text)

        # Add message to log buffer
        if self.use_buffer:
            if not self.__buffer.has_key(level):
                self.__buffer[level] = [text]
            else:
                self.__buffer[level].append(text)

        # Add prefix
        prefix = self.level_name.get(level, "[info]")

        # Display on stdout (if used)
        if self.use_print:
            sys.stdout.flush()
            sys.stderr.write("%s %s\n" % (prefix, text))
            sys.stderr.flush()

        # Write into outfile (if used)
        if self.__file:
            self._writeIntoFile("%s %s" % (prefix, text))

        # Use callback (if used)
        if self.on_new_message:
            self.on_new_message (level, prefix, _text, ctxt)

    def info(self, text):
        """
        New informative message.
        @type text: C{str}
        """
        self.newMessage(Log.LOG_INFO, text)

    def warning(self, text):
        """
        New warning message.
        @type text: C{str}
        """
        self.newMessage(Log.LOG_WARN, text)

    def error(self, text):
        """
        New error message.
        @type text: C{str}
        """
        self.newMessage(Log.LOG_ERROR, text)

log = Log()

class Logger(object):
    def _logger(self):
        return "<%s>" % self.__class__.__name__
    def info(self, text):
        log.newMessage(Log.LOG_INFO, text, self)
    def warning(self, text):
        log.newMessage(Log.LOG_WARN, text, self)
    def error(self, text):
        log.newMessage(Log.LOG_ERROR, text, self)

########NEW FILE########
__FILENAME__ = memory
import gc

#---- Default implementation when resource is missing ----------------------
PAGE_SIZE = 4096

def getMemoryLimit():
    """
    Get current memory limit in bytes.

    Return None on error.
    """
    return None

def setMemoryLimit(max_mem):
    """
    Set memory limit in bytes.
    Use value 'None' to disable memory limit.

    Return True if limit is set, False on error.
    """
    return False

def getMemorySize():
    """
    Read currenet process memory size: size of available virtual memory.
    This value is NOT the real memory usage.

    This function only works on Linux (use /proc/self/statm file).
    """
    try:
        statm = open('/proc/self/statm').readline().split()
    except IOError:
        return None
    return int(statm[0]) * PAGE_SIZE

def clearCaches():
    """
    Try to clear all caches: call gc.collect() (Python garbage collector).
    """
    gc.collect()
    #import re; re.purge()

try:
#---- 'resource' implementation ---------------------------------------------
    from resource import getpagesize, getrlimit, setrlimit, RLIMIT_AS

    PAGE_SIZE = getpagesize()

    def getMemoryLimit():
        try:
            limit = getrlimit(RLIMIT_AS)[0]
            if 0 < limit:
                limit *= PAGE_SIZE
            return limit
        except ValueError:
            return None

    def setMemoryLimit(max_mem):
        if max_mem is None:
            max_mem = -1
        try:
            setrlimit(RLIMIT_AS, (max_mem, -1))
            return True
        except ValueError:
            return False
except ImportError:
    pass

def limitedMemory(limit, func, *args, **kw):
    """
    Limit memory grow when calling func(*args, **kw):
    restrict memory grow to 'limit' bytes.

    Use try/except MemoryError to catch the error.
    """
    # First step: clear cache to gain memory
    clearCaches()

    # Get total program size
    max_rss = getMemorySize()
    if max_rss is not None:
        # Get old limit and then set our new memory limit
        old_limit = getMemoryLimit()
        limit = max_rss + limit
        limited = setMemoryLimit(limit)
    else:
        limited = False

    try:
        # Call function
        return func(*args, **kw)
    finally:
        # and unset our memory limit
        if limited:
            setMemoryLimit(old_limit)

        # After calling the function: clear all caches
        clearCaches()


########NEW FILE########
__FILENAME__ = profiler
from hotshot import Profile
from hotshot.stats import load as loadStats
from os import unlink

def runProfiler(func, args=tuple(), kw={}, verbose=True, nb_func=25, sort_by=('cumulative', 'calls')):
    profile_filename = "/tmp/profiler"
    prof = Profile(profile_filename)
    try:
        if verbose:
            print "[+] Run profiler"
        result = prof.runcall(func, *args, **kw)
        prof.close()
        if verbose:
            print "[+] Stop profiler"
            print "[+] Process data..."
        stat = loadStats(profile_filename)
        if verbose:
            print "[+] Strip..."
        stat.strip_dirs()
        if verbose:
            print "[+] Sort data..."
        stat.sort_stats(*sort_by)
        if verbose:
            print
            print "[+] Display statistics"
            print
        stat.print_stats(nb_func)
        return result
    finally:
        unlink(profile_filename)


########NEW FILE########
__FILENAME__ = input
from lib.hachoir_core.endian import BIG_ENDIAN, LITTLE_ENDIAN
from lib.hachoir_core.error import info
from lib.hachoir_core.log import Logger
from lib.hachoir_core.bits import str2long
from lib.hachoir_core.i18n import getTerminalCharset
from lib.hachoir_core.tools import lowerBound
from lib.hachoir_core.i18n import _
from os import dup, fdopen
from errno import ESPIPE
from weakref import ref as weakref_ref
from lib.hachoir_core.stream import StreamError

class InputStreamError(StreamError):
    pass

class ReadStreamError(InputStreamError):
    def __init__(self, size, address, got=None):
        self.size = size
        self.address = address
        self.got = got
        if self.got is not None:
            msg = _("Can't read %u bits at address %u (got %u bits)") % (self.size, self.address, self.got)
        else:
            msg = _("Can't read %u bits at address %u") % (self.size, self.address)
        InputStreamError.__init__(self, msg)

class NullStreamError(InputStreamError):
    def __init__(self, source):
        self.source = source
        msg = _("Input size is nul (source='%s')!") % self.source
        InputStreamError.__init__(self, msg)

class FileFromInputStream:
    _offset = 0
    _from_end = False

    def __init__(self, stream):
        self.stream = stream
        self._setSize(stream.askSize(self))

    def _setSize(self, size):
        if size is None:
            self._size = size
        elif size % 8:
            raise InputStreamError("Invalid size")
        else:
            self._size = size // 8

    def tell(self):
        if self._from_end:
            while self._size is None:
                self.stream._feed(max(self.stream._current_size << 1, 1 << 16))
            self._from_end = False
            self._offset += self._size
        return self._offset

    def seek(self, pos, whence=0):
        if whence == 0:
            self._from_end = False
            self._offset = pos
        elif whence == 1:
            self._offset += pos
        elif whence == 2:
            self._from_end = True
            self._offset = pos
        else:
            raise ValueError("seek() second argument must be 0, 1 or 2")

    def read(self, size=None):
        def read(address, size):
            shift, data, missing = self.stream.read(8 * address, 8 * size)
            if shift:
                raise InputStreamError("TODO: handle non-byte-aligned data")
            return data
        if self._size or size is not None and not self._from_end:
            # We don't want self.tell() to read anything
            # and the size must be known if we read until the end.
            pos = self.tell()
            if size is None or None < self._size < pos + size:
                size = self._size - pos
            if size <= 0:
                return ''
            data = read(pos, size)
            self._offset += len(data)
            return data
        elif self._from_end:
            # TODO: not tested
            max_size = - self._offset
            if size is None or max_size < size:
                size = max_size
            if size <= 0:
                return ''
            data = '', ''
            self._offset = max(0, self.stream._current_size // 8 + self._offset)
            self._from_end = False
            bs = max(max_size, 1 << 16)
            while True:
                d = read(self._offset, bs)
                data = data[1], d
                self._offset += len(d)
                if self._size:
                    bs = self._size - self._offset
                    if not bs:
                        data = data[0] + data[1]
                        d = len(data) - max_size
                        return data[d:d+size]
        else:
            # TODO: not tested
            data = [ ]
            size = 1 << 16
            while True:
                d = read(self._offset, size)
                data.append(d)
                self._offset += len(d)
                if self._size:
                    size = self._size - self._offset
                    if not size:
                        return ''.join(data)


class InputStream(Logger):
    _set_size = None
    _current_size = 0

    def __init__(self, source=None, size=None, packets=None, **args):
        self.source = source
        self._size = size   # in bits
        if size == 0:
            raise NullStreamError(source)
        self.tags = tuple(args.get("tags", tuple()))
        self.packets = packets

    def askSize(self, client):
        if self._size != self._current_size:
            if self._set_size is None:
                self._set_size = []
            self._set_size.append(weakref_ref(client))
        return self._size

    def _setSize(self, size=None):
        assert self._size is None or self._current_size <= self._size
        if self._size != self._current_size:
            self._size = self._current_size
            if not self._size:
                raise NullStreamError(self.source)
            if self._set_size:
                for client in self._set_size:
                    client = client()
                    if client:
                        client._setSize(self._size)
                del self._set_size

    size = property(lambda self: self._size, doc="Size of the stream in bits")
    checked = property(lambda self: self._size == self._current_size)

    def sizeGe(self, size, const=False):
        return self._current_size >= size or \
            not (None < self._size < size or const or self._feed(size))

    def _feed(self, size):
        return self.read(size-1,1)[2]

    def read(self, address, size):
        """
        Read 'size' bits at position 'address' (in bits)
        from the beginning of the stream.
        """
        raise NotImplementedError

    def readBits(self, address, nbits, endian):
        assert endian in (BIG_ENDIAN, LITTLE_ENDIAN)

        shift, data, missing = self.read(address, nbits)
        if missing:
            raise ReadStreamError(nbits, address)
        value = str2long(data, endian)
        if endian is BIG_ENDIAN:
            value >>= len(data) * 8 - shift - nbits
        else:
            value >>= shift
        return value & (1 << nbits) - 1

    def readInteger(self, address, signed, nbits, endian):
        """ Read an integer number """
        value = self.readBits(address, nbits, endian)

        # Signe number. Example with nbits=8:
        # if 128 <= value: value -= 256
        if signed and (1 << (nbits-1)) <= value:
            value -= (1 << nbits)
        return value

    def readBytes(self, address, nb_bytes):
        shift, data, missing = self.read(address, 8 * nb_bytes)
        if shift:
            raise InputStreamError("TODO: handle non-byte-aligned data")
        if missing:
            raise ReadStreamError(8 * nb_bytes, address)
        return data

    def searchBytesLength(self, needle, include_needle,
    start_address=0, end_address=None):
        """
        If include_needle is True, add its length to the result.
        Returns None is needle can't be found.
        """

        pos = self.searchBytes(needle, start_address, end_address)
        if pos is None:
            return None
        length = (pos - start_address) // 8
        if include_needle:
            length += len(needle)
        return length

    def searchBytes(self, needle, start_address=0, end_address=None):
        """
        Search some bytes in [start_address;end_address[. Addresses must
        be aligned to byte. Returns the address of the bytes if found,
        None else.
        """
        if start_address % 8:
            raise InputStreamError("Unable to search bytes with address with bit granularity")
        length = len(needle)
        size = max(3 * length, 4096)
        buffer = ''

        if self._size and (end_address is None or self._size < end_address):
            end_address = self._size

        while True:
            if end_address is not None:
                todo = (end_address - start_address) >> 3
                if todo < size:
                    if todo <= 0:
                        return None
                    size = todo
            data = self.readBytes(start_address, size)
            if end_address is None and self._size:
                end_address = self._size
                size = (end_address - start_address) >> 3
                assert size > 0
                data = data[:size]
            start_address += 8 * size
            buffer = buffer[len(buffer) - length + 1:] + data
            found = buffer.find(needle)
            if found >= 0:
                return start_address + (found - len(buffer)) * 8

    def file(self):
        return FileFromInputStream(self)


class InputPipe(object):
    """
    InputPipe makes input streams seekable by caching a certain
    amount of data. The memory usage may be unlimited in worst cases.
    A function (set_size) is called when the size of the stream is known.

    InputPipe sees the input stream as an array of blocks of
    size = (2 ^ self.buffer_size) and self.buffers maps to this array.
    It also maintains a circular ordered list of non-discarded blocks,
    sorted by access time.

    Each element of self.buffers is an array of 3 elements:
     * self.buffers[i][0] is the data.
       len(self.buffers[i][0]) == 1 << self.buffer_size
       (except at the end: the length may be smaller)
     * self.buffers[i][1] is the index of a more recently used block
     * self.buffers[i][2] is the opposite of self.buffers[1],
       in order to have a double-linked list.
    For any discarded block, self.buffers[i] = None

    self.last is the index of the most recently accessed block.
    self.first is the first (= smallest index) non-discarded block.

    How InputPipe discards blocks:
     * Just before returning from the read method.
     * Only if there are more than self.buffer_nb_min blocks in memory.
     * While self.buffers[self.first] is that least recently used block.

    Property: There is no hole in self.buffers, except at the beginning.
    """
    buffer_nb_min = 256
    buffer_size = 16
    last = None
    size = None

    def __init__(self, input, set_size=None):
        self._input = input
        self.first = self.address = 0
        self.buffers = []
        self.set_size = set_size

    current_size = property(lambda self: len(self.buffers) << self.buffer_size)

    def _append(self, data):
        if self.last is None:
            self.last = next = prev = 0
        else:
            prev = self.last
            last = self.buffers[prev]
            next = last[1]
            self.last = self.buffers[next][2] = last[1] = len(self.buffers)
        self.buffers.append([ data, next, prev ])

    def _get(self, index):
        if index >= len(self.buffers):
            return ''
        buf = self.buffers[index]
        if buf is None:
            raise InputStreamError(_("Error: Buffers too small. Can't seek backward."))
        if self.last != index:
            next = buf[1]
            prev = buf[2]
            self.buffers[next][2] = prev
            self.buffers[prev][1] = next
            first = self.buffers[self.last][1]
            buf[1] = first
            buf[2] = self.last
            self.buffers[first][2] = index
            self.buffers[self.last][1] = index
            self.last = index
        return buf[0]

    def _flush(self):
        lim = len(self.buffers) - self.buffer_nb_min
        while self.first < lim:
            buf = self.buffers[self.first]
            if buf[2] != self.last:
                break
            info("Discarding buffer %u." % self.first)
            self.buffers[self.last][1] = buf[1]
            self.buffers[buf[1]][2] = self.last
            self.buffers[self.first] = None
            self.first += 1

    def seek(self, address):
        assert 0 <= address
        self.address = address

    def read(self, size):
        end = self.address + size
        for i in xrange(len(self.buffers), (end >> self.buffer_size) + 1):
            data = self._input.read(1 << self.buffer_size)
            if len(data) < 1 << self.buffer_size:
                self.size = (len(self.buffers) << self.buffer_size) + len(data)
                if self.set_size:
                    self.set_size(self.size)
                if data:
                    self._append(data)
                break
            self._append(data)
        block, offset = divmod(self.address, 1 << self.buffer_size)
        data = ''.join(self._get(index)
                for index in xrange(block, (end - 1 >> self.buffer_size) + 1)
            )[offset:offset+size]
        self._flush()
        self.address += len(data)
        return data

class InputIOStream(InputStream):
    def __init__(self, input, size=None, **args):
        if not hasattr(input, "seek"):
            if size is None:
                input = InputPipe(input, self._setSize)
            else:
                input = InputPipe(input)
        elif size is None:
            try:
                input.seek(0, 2)
                size = input.tell() * 8
            except IOError, err:
                if err.errno == ESPIPE:
                    input = InputPipe(input, self._setSize)
                else:
                    charset = getTerminalCharset()
                    errmsg = unicode(str(err), charset)
                    source = args.get("source", "<inputio:%r>" % input)
                    raise InputStreamError(_("Unable to get size of %s: %s") % (source, errmsg))
        self._input = input
        InputStream.__init__(self, size=size, **args)

    def __current_size(self):
        if self._size:
            return self._size
        if self._input.size:
            return 8 * self._input.size
        return 8 * self._input.current_size
    _current_size = property(__current_size)

    def read(self, address, size):
        assert size > 0
        _size = self._size
        address, shift = divmod(address, 8)
        self._input.seek(address)
        size = (size + shift + 7) >> 3
        data = self._input.read(size)
        got = len(data)
        missing = size != got
        if missing and _size == self._size:
            raise ReadStreamError(8 * size, 8 * address, 8 * got)
        return shift, data, missing

    def file(self):
        if hasattr(self._input, "fileno"):
            new_fd = dup(self._input.fileno())
            new_file = fdopen(new_fd, "r")
            new_file.seek(0)
            return new_file
        return InputStream.file(self)


class StringInputStream(InputStream):
    def __init__(self, data, source="<string>", **args):
        self.data = data
        InputStream.__init__(self, source=source, size=8*len(data), **args)
        self._current_size = self._size

    def read(self, address, size):
        address, shift = divmod(address, 8)
        size = (size + shift + 7) >> 3
        data = self.data[address:address+size]
        got = len(data)
        if got != size:
            raise ReadStreamError(8 * size, 8 * address, 8 * got)
        return shift, data, False


class InputSubStream(InputStream):
    def __init__(self, stream, offset, size=None, source=None, **args):
        if offset is None:
            offset = 0
        if size is None and stream.size is not None:
            size = stream.size - offset
        if None < size <= 0:
            raise ValueError("InputSubStream: offset is outside input stream")
        self.stream = stream
        self._offset = offset
        if source is None:
            source = "<substream input=%s offset=%s size=%s>" % (stream.source, offset, size)
        InputStream.__init__(self, source=source, size=size, **args)
        self.stream.askSize(self)

    _current_size = property(lambda self: min(self._size, max(0, self.stream._current_size - self._offset)))

    def read(self, address, size):
        return self.stream.read(self._offset + address, size)

def InputFieldStream(field, **args):
    if not field.parent:
        return field.stream
    stream = field.parent.stream
    args["size"] = field.size
    args.setdefault("source", stream.source + field.path)
    return InputSubStream(stream, field.absolute_address, **args)


class FragmentedStream(InputStream):
    def __init__(self, field, **args):
        self.stream = field.parent.stream
        data = field.getData()
        self.fragments = [ (0, data.absolute_address, data.size) ]
        self.next = field.next
        args.setdefault("source", "%s%s" % (self.stream.source, field.path))
        InputStream.__init__(self, **args)
        if not self.next:
            self._current_size = data.size
            self._setSize()

    def _feed(self, end):
        if self._current_size < end:
            if self.checked:
                raise ReadStreamError(end - self._size, self._size)
            a, fa, fs = self.fragments[-1]
            while self.stream.sizeGe(fa + min(fs, end - a)):
                a += fs
                f = self.next
                if a >= end:
                    self._current_size = end
                    if a == end and not f:
                        self._setSize()
                    return False
                if f:
                    self.next = f.next
                    f = f.getData()
                if not f:
                    self._current_size = a
                    self._setSize()
                    return True
                fa = f.absolute_address
                fs = f.size
                self.fragments += [ (a, fa, fs) ]
            self._current_size = a + max(0, self.stream.size - fa)
            self._setSize()
            return True
        return False

    def read(self, address, size):
        assert size > 0
        missing = self._feed(address + size)
        if missing:
            size = self._size - address
            if size <= 0:
                return 0, '', True
        d = []
        i = lowerBound(self.fragments, lambda x: x[0] <= address)
        a, fa, fs = self.fragments[i-1]
        a -= address
        fa -= a
        fs += a
        s = None
        while True:
            n = min(fs, size)
            u, v, w = self.stream.read(fa, n)
            assert not w
            if s is None:
                s = u
            else:
                assert not u
            d += [ v ]
            size -= n
            if not size:
                return s, ''.join(d), missing
            a, fa, fs = self.fragments[i]
            i += 1


class ConcatStream(InputStream):
    # TODO: concatene any number of any type of stream
    def __init__(self, streams, **args):
        if len(streams) > 2 or not streams[0].checked:
            raise NotImplementedError
        self.__size0 = streams[0].size
        size1 = streams[1].askSize(self)
        if size1 is not None:
            args["size"] = self.__size0 + size1
        self.__streams = streams
        InputStream.__init__(self, **args)

    _current_size = property(lambda self: self.__size0 + self.__streams[1]._current_size)

    def read(self, address, size):
        _size = self._size
        s = self.__size0 - address
        shift, data, missing = None, '', False
        if s > 0:
            s = min(size, s)
            shift, data, w = self.__streams[0].read(address, s)
            assert not w
            a, s = 0, size - s
        else:
            a, s = -s, size
        if s:
            u, v, missing = self.__streams[1].read(a, s)
            if missing and _size == self._size:
                raise ReadStreamError(s, a)
            if shift is None:
                shift = u
            else:
                assert not u
            data += v
        return shift, data, missing

########NEW FILE########
__FILENAME__ = input_helper
from lib.hachoir_core.i18n import getTerminalCharset, guessBytesCharset, _
from lib.hachoir_core.stream import InputIOStream, InputSubStream, InputStreamError

def FileInputStream(filename, real_filename=None, **args):
    """
    Create an input stream of a file. filename must be unicode.

    real_filename is an optional argument used to specify the real filename,
    its type can be 'str' or 'unicode'. Use real_filename when you are
    not able to convert filename to real unicode string (ie. you have to
    use unicode(name, 'replace') or unicode(name, 'ignore')).
    """
    assert isinstance(filename, unicode)
    if not real_filename:
        real_filename = filename
    try:
        inputio = open(real_filename, 'rb')
    except IOError, err:
        charset = getTerminalCharset()
        errmsg = unicode(str(err), charset)
        raise InputStreamError(_("Unable to open file %s: %s") % (filename, errmsg))
    source = "file:" + filename
    offset = args.pop("offset", 0)
    size = args.pop("size", None)
    if offset or size:
        if size:
            size = 8 * size
        stream = InputIOStream(inputio, source=source, **args)
        return InputSubStream(stream, 8 * offset, size, **args)
    else:
        args.setdefault("tags",[]).append(("filename", filename))
        return InputIOStream(inputio, source=source, **args)

def guessStreamCharset(stream, address, size, default=None):
    size = min(size, 1024*8)
    bytes = stream.readBytes(address, size//8)
    return guessBytesCharset(bytes, default)


########NEW FILE########
__FILENAME__ = output
from cStringIO import StringIO
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.bits import long2raw
from lib.hachoir_core.stream import StreamError
from errno import EBADF

MAX_READ_NBYTES = 2 ** 16

class OutputStreamError(StreamError):
    pass

class OutputStream(object):
    def __init__(self, output, filename=None):
        self._output = output
        self._filename = filename
        self._bit_pos = 0
        self._byte = 0

    def _getFilename(self):
        return self._filename
    filename = property(_getFilename)

    def writeBit(self, state, endian):
        if self._bit_pos == 7:
            self._bit_pos = 0
            if state:
                if endian is BIG_ENDIAN:
                    self._byte |= 1
                else:
                    self._byte |= 128
            self._output.write(chr(self._byte))
            self._byte = 0
        else:
            if state:
                if endian is BIG_ENDIAN:
                    self._byte |= (1 << self._bit_pos)
                else:
                    self._byte |= (1 << (7-self._bit_pos))
            self._bit_pos += 1

    def writeBits(self, count, value, endian):
        assert 0 <= value < 2**count

        # Feed bits to align to byte address
        if self._bit_pos != 0:
            n = 8 - self._bit_pos
            if n <= count:
                count -= n
                if endian is BIG_ENDIAN:
                    self._byte |= (value >> count)
                    value &= ((1 << count) - 1)
                else:
                    self._byte |= (value & ((1 << n)-1)) << self._bit_pos
                    value >>= n
                self._output.write(chr(self._byte))
                self._bit_pos = 0
                self._byte = 0
            else:
                if endian is BIG_ENDIAN:
                    self._byte |= (value << (8-self._bit_pos-count))
                else:
                    self._byte |= (value << self._bit_pos)
                self._bit_pos += count
                return

        # Write byte per byte
        while 8 <= count:
            count -= 8
            if endian is BIG_ENDIAN:
                byte = (value >> count)
                value &= ((1 << count) - 1)
            else:
                byte = (value & 0xFF)
                value >>= 8
            self._output.write(chr(byte))

        # Keep last bits
        assert 0 <= count < 8
        self._bit_pos = count
        if 0 < count:
            assert 0 <= value < 2**count
            if endian is BIG_ENDIAN:
                self._byte = value << (8-count)
            else:
                self._byte = value
        else:
            assert value == 0
            self._byte = 0

    def writeInteger(self, value, signed, size_byte, endian):
        if signed:
            value += 1 << (size_byte*8 - 1)
        raw = long2raw(value, endian, size_byte)
        self.writeBytes(raw)

    def copyBitsFrom(self, input, address, nb_bits, endian):
        if (nb_bits % 8) == 0:
            self.copyBytesFrom(input, address, nb_bits/8)
        else:
            # Arbitrary limit (because we should use a buffer, like copyBytesFrom(),
            # but with endianess problem
            assert nb_bits <= 128
            data = input.readBits(address, nb_bits, endian)
            self.writeBits(nb_bits, data, endian)

    def copyBytesFrom(self, input, address, nb_bytes):
        if (address % 8):
            raise OutputStreamError("Unable to copy bytes with address with bit granularity")
        buffer_size = 1 << 12   # 8192 (8 KB)
        while 0 < nb_bytes:
            # Compute buffer size
            if nb_bytes < buffer_size:
                buffer_size = nb_bytes

            # Read
            data = input.readBytes(address, buffer_size)

            # Write
            self.writeBytes(data)

            # Move address
            address += buffer_size*8
            nb_bytes -= buffer_size

    def writeBytes(self, bytes):
        if self._bit_pos != 0:
            raise NotImplementedError()
        self._output.write(bytes)

    def readBytes(self, address, nbytes):
        """
        Read bytes from the stream at specified address (in bits).
        Address have to be a multiple of 8.
        nbytes have to in 1..MAX_READ_NBYTES (64 KB).

        This method is only supported for StringOuputStream (not on
        FileOutputStream).

        Return read bytes as byte string.
        """
        assert (address % 8) == 0
        assert (1 <= nbytes <= MAX_READ_NBYTES)
        self._output.flush()
        oldpos = self._output.tell()
        try:
            self._output.seek(0)
            try:
                return self._output.read(nbytes)
            except IOError, err:
                if err[0] == EBADF:
                    raise OutputStreamError("Stream doesn't support read() operation")
        finally:
            self._output.seek(oldpos)

def StringOutputStream():
    """
    Create an output stream into a string.
    """
    data = StringIO()
    return OutputStream(data)

def FileOutputStream(filename, real_filename=None):
    """
    Create an output stream into file with given name.

    Filename have to be unicode, whereas (optional) real_filename can be str.
    """
    assert isinstance(filename, unicode)
    if not real_filename:
        real_filename = filename
    output = open(real_filename, 'wb')
    return OutputStream(output, filename=filename)


########NEW FILE########
__FILENAME__ = stream
from lib.hachoir_core.error import HachoirError

class StreamError(HachoirError):
    pass


########NEW FILE########
__FILENAME__ = text_handler
"""
Utilities used to convert a field to human classic reprentation of data.
"""

from lib.hachoir_core.tools import (
    humanDuration, humanFilesize, alignValue,
    durationWin64 as doDurationWin64,
    deprecated)
from types import FunctionType, MethodType
from lib.hachoir_core.field import Field

def textHandler(field, handler):
    assert isinstance(handler, (FunctionType, MethodType))
    assert issubclass(field.__class__, Field)
    field.createDisplay = lambda: handler(field)
    return field

def displayHandler(field, handler):
    assert isinstance(handler, (FunctionType, MethodType))
    assert issubclass(field.__class__, Field)
    field.createDisplay = lambda: handler(field.value)
    return field

@deprecated("Use TimedeltaWin64 field type")
def durationWin64(field):
    """
    Convert Windows 64-bit duration to string. The timestamp format is
    a 64-bit number: number of 100ns. See also timestampWin64().

    >>> durationWin64(type("", (), dict(value=2146280000, size=64)))
    u'3 min 34 sec 628 ms'
    >>> durationWin64(type("", (), dict(value=(1 << 64)-1, size=64)))
    u'58494 years 88 days 5 hours'
    """
    assert hasattr(field, "value") and hasattr(field, "size")
    assert field.size == 64
    delta = doDurationWin64(field.value)
    return humanDuration(delta)

def filesizeHandler(field):
    """
    Format field value using humanFilesize()
    """
    return displayHandler(field, humanFilesize)

def hexadecimal(field):
    """
    Convert an integer to hexadecimal in lower case. Returns unicode string.

    >>> hexadecimal(type("", (), dict(value=412, size=16)))
    u'0x019c'
    >>> hexadecimal(type("", (), dict(value=0, size=32)))
    u'0x00000000'
    """
    assert hasattr(field, "value") and hasattr(field, "size")
    size = field.size
    padding = alignValue(size, 4) // 4
    pattern = u"0x%%0%ux" % padding
    return pattern % field.value


########NEW FILE########
__FILENAME__ = timeout
"""
limitedTime(): set a timeout in seconds when calling a function,
raise a Timeout error if time exceed.
"""
from math import ceil

IMPLEMENTATION = None

class Timeout(RuntimeError):
    """
    Timeout error, inherits from RuntimeError
    """
    pass

def signalHandler(signum, frame):
    """
    Signal handler to catch timeout signal: raise Timeout exception.
    """
    raise Timeout("Timeout exceed!")

def limitedTime(second, func, *args, **kw):
    """
    Call func(*args, **kw) with a timeout of second seconds.
    """
    return func(*args, **kw)

def fixTimeout(second):
    """
    Fix timeout value: convert to integer with a minimum of 1 second
    """
    if isinstance(second, float):
        second = int(ceil(second))
    assert isinstance(second, (int, long))
    return max(second, 1)

if not IMPLEMENTATION:
    try:
        from signal import signal, alarm, SIGALRM

        # signal.alarm() implementation
        def limitedTime(second, func, *args, **kw):
            second = fixTimeout(second)
            old_alarm = signal(SIGALRM, signalHandler)
            try:
                alarm(second)
                return func(*args, **kw)
            finally:
                alarm(0)
                signal(SIGALRM, old_alarm)

        IMPLEMENTATION = "signal.alarm()"
    except ImportError:
        pass

if not IMPLEMENTATION:
    try:
        from signal import signal, SIGXCPU
        from resource import getrlimit, setrlimit, RLIMIT_CPU

        # resource.setrlimit(RLIMIT_CPU) implementation
        # "Bug": timeout is 'CPU' time so sleep() are not part of the timeout
        def limitedTime(second, func, *args, **kw):
            second = fixTimeout(second)
            old_alarm = signal(SIGXCPU, signalHandler)
            current = getrlimit(RLIMIT_CPU)
            try:
                setrlimit(RLIMIT_CPU, (second, current[1]))
                return func(*args, **kw)
            finally:
                setrlimit(RLIMIT_CPU, current)
                signal(SIGXCPU, old_alarm)

        IMPLEMENTATION = "resource.setrlimit(RLIMIT_CPU)"
    except ImportError:
        pass


########NEW FILE########
__FILENAME__ = tools
# -*- coding: utf-8 -*-

"""
Various utilities.
"""

from lib.hachoir_core.i18n import _, ngettext
import re
import stat
from datetime import datetime, timedelta, MAXYEAR
from warnings import warn

def deprecated(comment=None):
    """
    This is a decorator which can be used to mark functions
    as deprecated. It will result in a warning being emmitted
    when the function is used.

    Examples: ::

       @deprecated
       def oldfunc(): ...

       @deprecated("use newfunc()!")
       def oldfunc2(): ...

    Code from: http://code.activestate.com/recipes/391367/
    """
    def _deprecated(func):
        def newFunc(*args, **kwargs):
            message = "Call to deprecated function %s" % func.__name__
            if comment:
                message += ": " + comment
            warn(message, category=DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        newFunc.__name__ = func.__name__
        newFunc.__doc__ = func.__doc__
        newFunc.__dict__.update(func.__dict__)
        return newFunc
    return _deprecated

def paddingSize(value, align):
    """
    Compute size of a padding field.

    >>> paddingSize(31, 4)
    1
    >>> paddingSize(32, 4)
    0
    >>> paddingSize(33, 4)
    3

    Note: (value + paddingSize(value, align)) == alignValue(value, align)
    """
    if value % align != 0:
        return align - (value % align)
    else:
        return 0

def alignValue(value, align):
    """
    Align a value to next 'align' multiple.

    >>> alignValue(31, 4)
    32
    >>> alignValue(32, 4)
    32
    >>> alignValue(33, 4)
    36

    Note: alignValue(value, align) == (value + paddingSize(value, align))
    """

    if value % align != 0:
        return value + align - (value % align)
    else:
        return value

def timedelta2seconds(delta):
    """
    Convert a datetime.timedelta() objet to a number of second
    (floatting point number).

    >>> timedelta2seconds(timedelta(seconds=2, microseconds=40000))
    2.04
    >>> timedelta2seconds(timedelta(minutes=1, milliseconds=250))
    60.25
    """
    return delta.microseconds / 1000000.0 \
        + delta.seconds + delta.days * 60*60*24

def humanDurationNanosec(nsec):
    """
    Convert a duration in nanosecond to human natural representation.
    Returns an unicode string.

    >>> humanDurationNanosec(60417893)
    u'60.42 ms'
    """

    # Nano second
    if nsec < 1000:
        return u"%u nsec" % nsec

    # Micro seconds
    usec, nsec = divmod(nsec, 1000)
    if usec < 1000:
        return u"%.2f usec" % (usec+float(nsec)/1000)

    # Milli seconds
    msec, usec = divmod(usec, 1000)
    if msec < 1000:
        return u"%.2f ms" % (msec + float(usec)/1000)
    return humanDuration(msec)

def humanDuration(delta):
    """
    Convert a duration in millisecond to human natural representation.
    Returns an unicode string.

    >>> humanDuration(0)
    u'0 ms'
    >>> humanDuration(213)
    u'213 ms'
    >>> humanDuration(4213)
    u'4 sec 213 ms'
    >>> humanDuration(6402309)
    u'1 hour 46 min 42 sec'
    """
    if not isinstance(delta, timedelta):
        delta = timedelta(microseconds=delta*1000)

    # Milliseconds
    text = []
    if 1000 <= delta.microseconds:
        text.append(u"%u ms" % (delta.microseconds//1000))

    # Seconds
    minutes, seconds = divmod(delta.seconds, 60)
    hours, minutes = divmod(minutes, 60)
    if seconds:
        text.append(u"%u sec" % seconds)
    if minutes:
        text.append(u"%u min" % minutes)
    if hours:
        text.append(ngettext("%u hour", "%u hours", hours) % hours)

    # Days
    years, days = divmod(delta.days, 365)
    if days:
        text.append(ngettext("%u day", "%u days", days) % days)
    if years:
        text.append(ngettext("%u year", "%u years", years) % years)
    if 3 < len(text):
        text = text[-3:]
    elif not text:
        return u"0 ms"
    return u" ".join(reversed(text))

def humanFilesize(size):
    """
    Convert a file size in byte to human natural representation.
    It uses the values: 1 KB is 1024 bytes, 1 MB is 1024 KB, etc.
    The result is an unicode string.

    >>> humanFilesize(1)
    u'1 byte'
    >>> humanFilesize(790)
    u'790 bytes'
    >>> humanFilesize(256960)
    u'250.9 KB'
    """
    if size < 10000:
        return ngettext("%u byte", "%u bytes", size) % size
    units = [_("KB"), _("MB"), _("GB"), _("TB")]
    size = float(size)
    divisor = 1024
    for unit in units:
        size = size / divisor
        if size < divisor:
            return "%.1f %s" % (size, unit)
    return "%u %s" % (size, unit)

def humanBitSize(size):
    """
    Convert a size in bit to human classic representation.
    It uses the values: 1 Kbit is 1000 bits, 1 Mbit is 1000 Kbit, etc.
    The result is an unicode string.

    >>> humanBitSize(1)
    u'1 bit'
    >>> humanBitSize(790)
    u'790 bits'
    >>> humanBitSize(256960)
    u'257.0 Kbit'
    """
    divisor = 1000
    if size < divisor:
        return ngettext("%u bit", "%u bits", size) % size
    units = [u"Kbit", u"Mbit", u"Gbit", u"Tbit"]
    size = float(size)
    for unit in units:
        size = size / divisor
        if size < divisor:
            return "%.1f %s" % (size, unit)
    return u"%u %s" % (size, unit)

def humanBitRate(size):
    """
    Convert a bit rate to human classic representation. It uses humanBitSize()
    to convert size into human reprensation. The result is an unicode string.

    >>> humanBitRate(790)
    u'790 bits/sec'
    >>> humanBitRate(256960)
    u'257.0 Kbit/sec'
    """
    return "".join((humanBitSize(size), "/sec"))

def humanFrequency(hertz):
    """
    Convert a frequency in hertz to human classic representation.
    It uses the values: 1 KHz is 1000 Hz, 1 MHz is 1000 KMhz, etc.
    The result is an unicode string.

    >>> humanFrequency(790)
    u'790 Hz'
    >>> humanFrequency(629469)
    u'629.5 kHz'
    """
    divisor = 1000
    if hertz < divisor:
        return u"%u Hz" % hertz
    units = [u"kHz", u"MHz", u"GHz", u"THz"]
    hertz = float(hertz)
    for unit in units:
        hertz = hertz / divisor
        if hertz < divisor:
            return u"%.1f %s" % (hertz, unit)
    return u"%s %s" % (hertz, unit)

regex_control_code = re.compile(r"([\x00-\x1f\x7f])")
controlchars = tuple({
        # Don't use "\0", because "\0"+"0"+"1" = "\001" = "\1" (1 character)
        # Same rease to not use octal syntax ("\1")
        ord("\n"): r"\n",
        ord("\r"): r"\r",
        ord("\t"): r"\t",
        ord("\a"): r"\a",
        ord("\b"): r"\b",
    }.get(code, '\\x%02x' % code)
    for code in xrange(128)
)

def makePrintable(data, charset, quote=None, to_unicode=False, smart=True):
    r"""
    Prepare a string to make it printable in the specified charset.
    It escapes control characters. Characters with code bigger than 127
    are escaped if data type is 'str' or if charset is "ASCII".

    Examples with Unicode:
    >>> aged = unicode("g", "UTF-8")
    >>> repr(aged)  # text type is 'unicode'
    "u'\\xe2g\\xe9'"
    >>> makePrintable("abc\0", "UTF-8")
    'abc\\0'
    >>> makePrintable(aged, "latin1")
    '\xe2g\xe9'
    >>> makePrintable(aged, "latin1", quote='"')
    '"\xe2g\xe9"'

    Examples with string encoded in latin1:
    >>> aged_latin = unicode("g", "UTF-8").encode("latin1")
    >>> repr(aged_latin)  # text type is 'str'
    "'\\xe2g\\xe9'"
    >>> makePrintable(aged_latin, "latin1")
    '\\xe2g\\xe9'
    >>> makePrintable("", "latin1")
    ''
    >>> makePrintable("a", "latin1", quote='"')
    '"a"'
    >>> makePrintable("", "latin1", quote='"')
    '(empty)'
    >>> makePrintable("abc", "latin1", quote="'")
    "'abc'"

    Control codes:
    >>> makePrintable("\0\x03\x0a\x10 \x7f", "latin1")
    '\\0\\3\\n\\x10 \\x7f'

    Quote character may also be escaped (only ' and "):
    >>> print makePrintable("a\"b", "latin-1", quote='"')
    "a\"b"
    >>> print makePrintable("a\"b", "latin-1", quote="'")
    'a"b'
    >>> print makePrintable("a'b", "latin-1", quote="'")
    'a\'b'
    """

    if data:
        if not isinstance(data, unicode):
            data = unicode(data, "ISO-8859-1")
            charset = "ASCII"
        data = regex_control_code.sub(
            lambda regs: controlchars[ord(regs.group(1))], data)
        if quote:
            if quote in "\"'":
                data = data.replace(quote, '\\' + quote)
            data = ''.join((quote, data, quote))
    elif quote:
        data = "(empty)"
    data = data.encode(charset, "backslashreplace")
    if smart:
        # Replace \x00\x01 by \0\1
        data = re.sub(r"\\x0([0-7])(?=[^0-7]|$)", r"\\\1", data)
    if to_unicode:
        data = unicode(data, charset)
    return data

def makeUnicode(text):
    r"""
    Convert text to printable Unicode string. For byte string (type 'str'),
    use charset ISO-8859-1 for the conversion to Unicode

    >>> makeUnicode(u'abc\0d')
    u'abc\\0d'
    >>> makeUnicode('a\xe9')
    u'a\xe9'
    """
    if isinstance(text, str):
        text = unicode(text, "ISO-8859-1")
    elif not isinstance(text, unicode):
        text = unicode(text)
    text = regex_control_code.sub(
        lambda regs: controlchars[ord(regs.group(1))], text)
    text = re.sub(r"\\x0([0-7])(?=[^0-7]|$)", r"\\\1", text)
    return text

def binarySearch(seq, cmp_func):
    """
    Search a value in a sequence using binary search. Returns index of the
    value, or None if the value doesn't exist.

    'seq' have to be sorted in ascending order according to the
    comparaison function ;

    'cmp_func', prototype func(x), is the compare function:
    - Return strictly positive value if we have to search forward ;
    - Return strictly negative value if we have to search backward ;
    - Otherwise (zero) we got the value.

    >>> # Search number 5 (search forward)
    ... binarySearch([0, 4, 5, 10], lambda x: 5-x)
    2
    >>> # Backward search
    ... binarySearch([10, 5, 4, 0], lambda x: x-5)
    1
    """
    lower = 0
    upper = len(seq)
    while lower < upper:
        index = (lower + upper) >> 1
        diff = cmp_func(seq[index])
        if diff < 0:
            upper = index
        elif diff > 0:
            lower = index + 1
        else:
            return index
    return None

def lowerBound(seq, cmp_func):
    f = 0
    l = len(seq)
    while l > 0:
        h = l >> 1
        m = f + h
        if cmp_func(seq[m]):
            f = m
            f += 1
            l -= h + 1
        else:
            l = h
    return f

def humanUnixAttributes(mode):
    """
    Convert a Unix file attributes (or "file mode") to an unicode string.

    Original source code:
    http://cvs.savannah.gnu.org/viewcvs/coreutils/lib/filemode.c?root=coreutils

    >>> humanUnixAttributes(0644)
    u'-rw-r--r-- (644)'
    >>> humanUnixAttributes(02755)
    u'-rwxr-sr-x (2755)'
    """

    def ftypelet(mode):
        if stat.S_ISREG (mode) or not stat.S_IFMT(mode):
            return '-'
        if stat.S_ISBLK (mode): return 'b'
        if stat.S_ISCHR (mode): return 'c'
        if stat.S_ISDIR (mode): return 'd'
        if stat.S_ISFIFO(mode): return 'p'
        if stat.S_ISLNK (mode): return 'l'
        if stat.S_ISSOCK(mode): return 's'
        return '?'

    chars = [ ftypelet(mode), 'r', 'w', 'x', 'r', 'w', 'x', 'r', 'w', 'x' ]
    for i in xrange(1, 10):
        if not mode & 1 << 9 - i:
            chars[i] = '-'
    if mode & stat.S_ISUID:
        if chars[3] != 'x':
            chars[3] = 'S'
        else:
            chars[3] = 's'
    if mode & stat.S_ISGID:
        if chars[6] != 'x':
            chars[6] = 'S'
        else:
            chars[6] = 's'
    if mode & stat.S_ISVTX:
        if chars[9] != 'x':
            chars[9] = 'T'
        else:
            chars[9] = 't'
    return u"%s (%o)" % (''.join(chars), mode)

def createDict(data, index):
    """
    Create a new dictionnay from dictionnary key=>values:
    just keep value number 'index' from all values.

    >>> data={10: ("dix", 100, "a"), 20: ("vingt", 200, "b")}
    >>> createDict(data, 0)
    {10: 'dix', 20: 'vingt'}
    >>> createDict(data, 2)
    {10: 'a', 20: 'b'}
    """
    return dict( (key,values[index]) for key, values in data.iteritems() )

# Start of UNIX timestamp (Epoch): 1st January 1970 at 00:00
UNIX_TIMESTAMP_T0 = datetime(1970, 1, 1)

def timestampUNIX(value):
    """
    Convert an UNIX (32-bit) timestamp to datetime object. Timestamp value
    is the number of seconds since the 1st January 1970 at 00:00. Maximum
    value is 2147483647: 19 january 2038 at 03:14:07.

    May raise ValueError for invalid value: value have to be in 0..2147483647.

    >>> timestampUNIX(0)
    datetime.datetime(1970, 1, 1, 0, 0)
    >>> timestampUNIX(1154175644)
    datetime.datetime(2006, 7, 29, 12, 20, 44)
    >>> timestampUNIX(1154175644.37)
    datetime.datetime(2006, 7, 29, 12, 20, 44, 370000)
    >>> timestampUNIX(2147483647)
    datetime.datetime(2038, 1, 19, 3, 14, 7)
    """
    if not isinstance(value, (float, int, long)):
        raise TypeError("timestampUNIX(): an integer or float is required")
    if not(0 <= value <= 2147483647):
        raise ValueError("timestampUNIX(): value have to be in 0..2147483647")
    return UNIX_TIMESTAMP_T0 + timedelta(seconds=value)

# Start of Macintosh timestamp: 1st January 1904 at 00:00
MAC_TIMESTAMP_T0 = datetime(1904, 1, 1)

def timestampMac32(value):
    """
    Convert an Mac (32-bit) timestamp to string. The format is the number
    of seconds since the 1st January 1904 (to 2040). Returns unicode string.

    >>> timestampMac32(0)
    datetime.datetime(1904, 1, 1, 0, 0)
    >>> timestampMac32(2843043290)
    datetime.datetime(1994, 2, 2, 14, 14, 50)
    """
    if not isinstance(value, (float, int, long)):
        raise TypeError("an integer or float is required")
    if not(0 <= value <= 4294967295):
        return _("invalid Mac timestamp (%s)") % value
    return MAC_TIMESTAMP_T0 + timedelta(seconds=value)

def durationWin64(value):
    """
    Convert Windows 64-bit duration to string. The timestamp format is
    a 64-bit number: number of 100ns. See also timestampWin64().

    >>> str(durationWin64(1072580000))
    '0:01:47.258000'
    >>> str(durationWin64(2146280000))
    '0:03:34.628000'
    """
    if not isinstance(value, (float, int, long)):
        raise TypeError("an integer or float is required")
    if value < 0:
        raise ValueError("value have to be a positive or nul integer")
    return timedelta(microseconds=value/10)

# Start of 64-bit Windows timestamp: 1st January 1600 at 00:00
WIN64_TIMESTAMP_T0 = datetime(1601, 1, 1, 0, 0, 0)

def timestampWin64(value):
    """
    Convert Windows 64-bit timestamp to string. The timestamp format is
    a 64-bit number which represents number of 100ns since the
    1st January 1601 at 00:00. Result is an unicode string.
    See also durationWin64(). Maximum date is 28 may 60056.

    >>> timestampWin64(0)
    datetime.datetime(1601, 1, 1, 0, 0)
    >>> timestampWin64(127840491566710000)
    datetime.datetime(2006, 2, 10, 12, 45, 56, 671000)
    """
    try:
        return WIN64_TIMESTAMP_T0 + durationWin64(value)
    except OverflowError:
        raise ValueError(_("date newer than year %s (value=%s)") % (MAXYEAR, value))

# Start of 60-bit UUID timestamp: 15 October 1582 at 00:00
UUID60_TIMESTAMP_T0 = datetime(1582, 10, 15, 0, 0, 0)

def timestampUUID60(value):
    """
    Convert UUID 60-bit timestamp to string. The timestamp format is
    a 60-bit number which represents number of 100ns since the
    the 15 October 1582 at 00:00. Result is an unicode string.

    >>> timestampUUID60(0)
    datetime.datetime(1582, 10, 15, 0, 0)
    >>> timestampUUID60(130435676263032368)
    datetime.datetime(1996, 2, 14, 5, 13, 46, 303236)
    """
    if not isinstance(value, (float, int, long)):
        raise TypeError("an integer or float is required")
    if value < 0:
        raise ValueError("value have to be a positive or nul integer")
    try:
        return UUID60_TIMESTAMP_T0 + timedelta(microseconds=value/10)
    except OverflowError:
        raise ValueError(_("timestampUUID60() overflow (value=%s)") % value)

def humanDatetime(value, strip_microsecond=True):
    """
    Convert a timestamp to Unicode string: use ISO format with space separator.

    >>> humanDatetime( datetime(2006, 7, 29, 12, 20, 44) )
    u'2006-07-29 12:20:44'
    >>> humanDatetime( datetime(2003, 6, 30, 16, 0, 5, 370000) )
    u'2003-06-30 16:00:05'
    >>> humanDatetime( datetime(2003, 6, 30, 16, 0, 5, 370000), False )
    u'2003-06-30 16:00:05.370000'
    """
    text = unicode(value.isoformat())
    text = text.replace('T', ' ')
    if strip_microsecond and "." in text:
        text = text.split(".")[0]
    return text

NEWLINES_REGEX = re.compile("\n+")

def normalizeNewline(text):
    r"""
    Replace Windows and Mac newlines with Unix newlines.
    Replace multiple consecutive newlines with one newline.

    >>> normalizeNewline('a\r\nb')
    'a\nb'
    >>> normalizeNewline('a\r\rb')
    'a\nb'
    >>> normalizeNewline('a\n\nb')
    'a\nb'
    """
    text = text.replace("\r\n", "\n")
    text = text.replace("\r", "\n")
    return NEWLINES_REGEX.sub("\n", text)


########NEW FILE########
__FILENAME__ = version
PACKAGE = "hachoir-core"
VERSION = "1.3.3"
WEBSITE = 'http://bitbucket.org/haypo/hachoir/wiki/hachoir-core'
LICENSE = 'GNU GPL v2'


########NEW FILE########
__FILENAME__ = archive
from lib.hachoir_metadata.metadata_item import QUALITY_BEST, QUALITY_FASTEST
from lib.hachoir_metadata.safe import fault_tolerant, getValue
from lib.hachoir_metadata.metadata import (
    RootMetadata, Metadata, MultipleMetadata, registerExtractor)
from lib.hachoir_parser.archive import (Bzip2Parser, CabFile, GzipParser,
    TarFile, ZipFile, MarFile)
from lib.hachoir_core.tools import humanUnixAttributes
from lib.hachoir_core.i18n import _

def maxNbFile(meta):
    if meta.quality <= QUALITY_FASTEST:
        return 0
    if QUALITY_BEST <= meta.quality:
        return None
    return 1 + int(10 * meta.quality)

def computeCompressionRate(meta):
    """
    Compute compression rate, sizes have to be in byte.
    """
    if not meta.has("file_size") \
    or not meta.get("compr_size", 0):
        return
    file_size = meta.get("file_size")
    if not file_size:
        return
    meta.compr_rate = float(file_size) / meta.get("compr_size")

class Bzip2Metadata(RootMetadata):
    def extract(self, zip):
        if "file" in zip:
            self.compr_size = zip["file"].size/8

class GzipMetadata(RootMetadata):
    def extract(self, gzip):
        self.useHeader(gzip)
        computeCompressionRate(self)

    @fault_tolerant
    def useHeader(self, gzip):
        self.compression = gzip["compression"].display
        if gzip["mtime"]:
            self.last_modification = gzip["mtime"].value
        self.os = gzip["os"].display
        if gzip["has_filename"].value:
            self.filename = getValue(gzip, "filename")
        if gzip["has_comment"].value:
            self.comment = getValue(gzip, "comment")
        self.compr_size = gzip["file"].size/8
        self.file_size = gzip["size"].value

class ZipMetadata(MultipleMetadata):
    def extract(self, zip):
        max_nb = maxNbFile(self)
        for index, field in enumerate(zip.array("file")):
            if max_nb is not None and max_nb <= index:
                self.warning("ZIP archive contains many files, but only first %s files are processed" % max_nb)
                break
            self.processFile(field)

    @fault_tolerant
    def processFile(self, field):
        meta = Metadata(self)
        meta.filename = field["filename"].value
        meta.creation_date = field["last_mod"].value
        meta.compression = field["compression"].display
        if "data_desc" in field:
            meta.file_size = field["data_desc/file_uncompressed_size"].value
            if field["data_desc/file_compressed_size"].value:
                meta.compr_size = field["data_desc/file_compressed_size"].value
        else:
            meta.file_size = field["uncompressed_size"].value
            if field["compressed_size"].value:
                meta.compr_size = field["compressed_size"].value
        computeCompressionRate(meta)
        self.addGroup(field.name, meta, "File \"%s\"" % meta.get('filename'))

class TarMetadata(MultipleMetadata):
    def extract(self, tar):
        max_nb = maxNbFile(self)
        for index, field in enumerate(tar.array("file")):
            if max_nb is not None and max_nb <= index:
                self.warning("TAR archive contains many files, but only first %s files are processed" % max_nb)
                break
            meta = Metadata(self)
            self.extractFile(field, meta)
            if meta.has("filename"):
                title = _('File "%s"') % meta.getText('filename')
            else:
                title = _("File")
            self.addGroup(field.name, meta, title)

    @fault_tolerant
    def extractFile(self, field, meta):
        meta.filename = field["name"].value
        meta.file_attr = humanUnixAttributes(field.getOctal("mode"))
        meta.file_size = field.getOctal("size")
        try:
            if field.getOctal("mtime"):
                meta.last_modification = field.getDatetime()
        except ValueError:
            pass
        meta.file_type = field["type"].display
        meta.author = "%s (uid=%s), group %s (gid=%s)" %\
            (field["uname"].value, field.getOctal("uid"),
             field["gname"].value, field.getOctal("gid"))


class CabMetadata(MultipleMetadata):
    def extract(self, cab):
        if "folder[0]" in cab:
            self.useFolder(cab["folder[0]"])
        self.format_version = "Microsoft Cabinet version %s" % cab["cab_version"].display
        self.comment = "%s folders, %s files" % (
            cab["nb_folder"].value, cab["nb_files"].value)
        max_nb = maxNbFile(self)
        for index, field in enumerate(cab.array("file")):
            if max_nb is not None and max_nb <= index:
                self.warning("CAB archive contains many files, but only first %s files are processed" % max_nb)
                break
            self.useFile(field)

    @fault_tolerant
    def useFolder(self, folder):
        compr = folder["compr_method"].display
        if folder["compr_method"].value != 0:
            compr += " (level %u)" % folder["compr_level"].value
        self.compression = compr

    @fault_tolerant
    def useFile(self, field):
        meta = Metadata(self)
        meta.filename = field["filename"].value
        meta.file_size = field["filesize"].value
        meta.creation_date = field["timestamp"].value
        attr = field["attributes"].value
        if attr != "(none)":
            meta.file_attr = attr
        if meta.has("filename"):
            title = _("File \"%s\"") % meta.getText('filename')
        else:
            title = _("File")
        self.addGroup(field.name, meta, title)

class MarMetadata(MultipleMetadata):
    def extract(self, mar):
        self.comment = "Contains %s files" % mar["nb_file"].value
        self.format_version = "Microsoft Archive version %s" % mar["version"].value
        max_nb = maxNbFile(self)
        for index, field in enumerate(mar.array("file")):
            if max_nb is not None and max_nb <= index:
                self.warning("MAR archive contains many files, but only first %s files are processed" % max_nb)
                break
            meta = Metadata(self)
            meta.filename = field["filename"].value
            meta.compression = "None"
            meta.file_size = field["filesize"].value
            self.addGroup(field.name, meta, "File \"%s\"" % meta.getText('filename'))

registerExtractor(CabFile, CabMetadata)
registerExtractor(GzipParser, GzipMetadata)
registerExtractor(Bzip2Parser, Bzip2Metadata)
registerExtractor(TarFile, TarMetadata)
registerExtractor(ZipFile, ZipMetadata)
registerExtractor(MarFile, MarMetadata)


########NEW FILE########
__FILENAME__ = audio
from lib.hachoir_metadata.metadata import (registerExtractor,
    Metadata, RootMetadata, MultipleMetadata)
from lib.hachoir_parser.audio import AuFile, MpegAudioFile, RealAudioFile, AiffFile, FlacParser
from lib.hachoir_parser.container import OggFile, RealMediaFile
from lib.hachoir_core.i18n import _
from lib.hachoir_core.tools import makePrintable, timedelta2seconds, humanBitRate
from datetime import timedelta
from lib.hachoir_metadata.metadata_item import QUALITY_FAST, QUALITY_NORMAL, QUALITY_BEST
from lib.hachoir_metadata.safe import fault_tolerant, getValue

def computeComprRate(meta, size):
    if not meta.has("duration") \
    or not meta.has("sample_rate") \
    or not meta.has("bits_per_sample") \
    or not meta.has("nb_channel") \
    or not size:
        return
    orig_size = timedelta2seconds(meta.get("duration")) * meta.get('sample_rate') * meta.get('bits_per_sample') * meta.get('nb_channel')
    meta.compr_rate = float(orig_size) / size

def computeBitRate(meta):
    if not meta.has("bits_per_sample") \
    or not meta.has("nb_channel") \
    or not meta.has("sample_rate"):
        return
    meta.bit_rate = meta.get('bits_per_sample') * meta.get('nb_channel') * meta.get('sample_rate')

VORBIS_KEY_TO_ATTR = {
    "ARTIST": "artist",
    "ALBUM": "album",
    "TRACKNUMBER": "track_number",
    "TRACKTOTAL": "track_total",
    "ENCODER": "producer",
    "TITLE": "title",
    "LOCATION": "location",
    "DATE": "creation_date",
    "ORGANIZATION": "organization",
    "GENRE": "music_genre",
    "": "comment",
    "COMPOSER": "music_composer",
    "DESCRIPTION": "comment",
    "COMMENT": "comment",
    "WWW": "url",
    "WOAF": "url",
    "LICENSE": "copyright",
}

@fault_tolerant
def readVorbisComment(metadata, comment):
    metadata.producer = getValue(comment, "vendor")
    for item in comment.array("metadata"):
        if "=" in item.value:
            key, value = item.value.split("=", 1)
            key = key.upper()
            if key in VORBIS_KEY_TO_ATTR:
                key = VORBIS_KEY_TO_ATTR[key]
                setattr(metadata, key, value)
            elif value:
                metadata.warning("Skip Vorbis comment %s: %s" % (key, value))

class OggMetadata(MultipleMetadata):
    def extract(self, ogg):
        granule_quotient = None
        for index, page in enumerate(ogg.array("page")):
            if "segments" not in page:
                continue
            page = page["segments"]
            if "vorbis_hdr" in page:
                meta = Metadata(self)
                self.vorbisHeader(page["vorbis_hdr"], meta)
                self.addGroup("audio[]", meta, "Audio")
                if not granule_quotient and meta.has("sample_rate"):
                    granule_quotient = meta.get('sample_rate')
            if "theora_hdr" in page:
                meta = Metadata(self)
                self.theoraHeader(page["theora_hdr"], meta)
                self.addGroup("video[]", meta, "Video")
            if "video_hdr" in page:
                meta = Metadata(self)
                self.videoHeader(page["video_hdr"], meta)
                self.addGroup("video[]", meta, "Video")
                if not granule_quotient and meta.has("frame_rate"):
                    granule_quotient = meta.get('frame_rate')
            if "comment" in page:
                readVorbisComment(self, page["comment"])
            if 3 <= index:
                # Only process pages 0..3
                break

        # Compute duration
        if granule_quotient and QUALITY_NORMAL <= self.quality:
            page = ogg.createLastPage()
            if page and "abs_granule_pos" in page:
                try:
                    self.duration = timedelta(seconds=float(page["abs_granule_pos"].value) / granule_quotient)
                except OverflowError:
                    pass

    def videoHeader(self, header, meta):
        meta.compression = header["fourcc"].display
        meta.width = header["width"].value
        meta.height = header["height"].value
        meta.bits_per_pixel = header["bits_per_sample"].value
        if header["time_unit"].value:
            meta.frame_rate = 10000000.0 / header["time_unit"].value

    def theoraHeader(self, header, meta):
        meta.compression = "Theora"
        meta.format_version = "Theora version %u.%u (revision %u)" % (\
            header["version_major"].value,
            header["version_minor"].value,
            header["version_revision"].value)
        meta.width = header["frame_width"].value
        meta.height = header["frame_height"].value
        if header["fps_den"].value:
            meta.frame_rate = float(header["fps_num"].value) / header["fps_den"].value
        if header["aspect_ratio_den"].value:
            meta.aspect_ratio = float(header["aspect_ratio_num"].value) / header["aspect_ratio_den"].value
        meta.pixel_format = header["pixel_format"].display
        meta.comment = "Quality: %s" % header["quality"].value

    def vorbisHeader(self, header, meta):
        meta.compression = u"Vorbis"
        meta.sample_rate = header["audio_sample_rate"].value
        meta.nb_channel = header["audio_channels"].value
        meta.format_version = u"Vorbis version %s" % header["vorbis_version"].value
        meta.bit_rate = header["bitrate_nominal"].value

class AuMetadata(RootMetadata):
    def extract(self, audio):
        self.sample_rate = audio["sample_rate"].value
        self.nb_channel = audio["channels"].value
        self.compression = audio["codec"].display
        if "info" in audio:
            self.comment = audio["info"].value
        self.bits_per_sample = audio.getBitsPerSample()
        computeBitRate(self)
        if "audio_data" in audio:
            if self.has("bit_rate"):
                self.duration = timedelta(seconds=float(audio["audio_data"].size) / self.get('bit_rate'))
            computeComprRate(self, audio["audio_data"].size)

class RealAudioMetadata(RootMetadata):
    FOURCC_TO_BITRATE = {
        u"28_8": 15200, # 28.8 kbit/sec (audio bit rate: 15.2 kbit/s)
        u"14_4": 8000,  # 14.4 kbit/sec
        u"lpcJ": 8000,  # 14.4 kbit/sec
    }

    def extract(self, real):
        version = real["version"].value
        if "metadata" in real:
            self.useMetadata(real["metadata"])
        self.useRoot(real)
        self.format_version = "Real audio version %s" % version
        if version == 3:
            size = getValue(real, "data_size")
        elif "filesize" in real and "headersize" in real:
            size = (real["filesize"].value + 40) - (real["headersize"].value + 16)
        else:
            size = None
        if size:
            size *= 8
            if self.has("bit_rate"):
                sec = float(size) / self.get('bit_rate')
                self.duration = timedelta(seconds=sec)
            computeComprRate(self, size)

    @fault_tolerant
    def useMetadata(self, info):
        self.title = info["title"].value
        self.author = info["author"].value
        self.copyright = info["copyright"].value
        self.comment = info["comment"].value

    @fault_tolerant
    def useRoot(self, real):
        self.bits_per_sample = 16   # FIXME: Is that correct?
        if real["version"].value != 3:
            self.sample_rate = real["sample_rate"].value
            self.nb_channel = real["channels"].value
        else:
            self.sample_rate = 8000
            self.nb_channel = 1
        fourcc = getValue(real, "FourCC")
        if fourcc:
            self.compression = fourcc
            try:
                self.bit_rate = self.FOURCC_TO_BITRATE[fourcc]
            except LookupError:
                pass

class RealMediaMetadata(MultipleMetadata):
    KEY_TO_ATTR = {
        "generated by": "producer",
        "creation date": "creation_date",
        "modification date": "last_modification",
        "description": "comment",
    }

    def extract(self, media):
        if "file_prop" in media:
            self.useFileProp(media["file_prop"])
        if "content_desc" in media:
            self.useContentDesc(media["content_desc"])
        for index, stream in enumerate(media.array("stream_prop")):
            self.useStreamProp(stream, index)

    @fault_tolerant
    def useFileInfoProp(self, prop):
        key = prop["name"].value.lower()
        value = prop["value"].value
        if key in self.KEY_TO_ATTR:
            setattr(self, self.KEY_TO_ATTR[key], value)
        elif value:
            self.warning("Skip %s: %s" % (prop["name"].value, value))

    @fault_tolerant
    def useFileProp(self, prop):
        self.bit_rate = prop["avg_bit_rate"].value
        self.duration = timedelta(milliseconds=prop["duration"].value)

    @fault_tolerant
    def useContentDesc(self, content):
        self.title = content["title"].value
        self.author = content["author"].value
        self.copyright = content["copyright"].value
        self.comment = content["comment"].value

    @fault_tolerant
    def useStreamProp(self, stream, index):
        meta = Metadata(self)
        meta.comment = "Start: %s" % stream["stream_start"].value
        if getValue(stream, "mime_type") == "logical-fileinfo":
            for prop in stream.array("file_info/prop"):
                self.useFileInfoProp(prop)
        else:
            meta.bit_rate = stream["avg_bit_rate"].value
            meta.duration = timedelta(milliseconds=stream["duration"].value)
            meta.mime_type = getValue(stream, "mime_type")
        meta.title = getValue(stream, "desc")
        self.addGroup("stream[%u]" % index, meta, "Stream #%u" % (1+index))

class MpegAudioMetadata(RootMetadata):
    TAG_TO_KEY = {
        # ID3 version 2.2
        "TP1": "author",
        "COM": "comment",
        "TEN": "producer",
        "TRK": "track_number",
        "TAL": "album",
        "TT2": "title",
        "TYE": "creation_date",
        "TCO": "music_genre",

        # ID3 version 2.3+
        "TPE1": "author",
        "COMM": "comment",
        "TENC": "producer",
        "TRCK": "track_number",
        "TALB": "album",
        "TIT2": "title",
        "TYER": "creation_date",
        "WXXX": "url",
        "TCON": "music_genre",
        "TLAN": "language",
        "TCOP": "copyright",
        "TDAT": "creation_date",
        "TRDA": "creation_date",
        "TORY": "creation_date",
        "TIT1": "title",
    }

    def processID3v2(self, field):
        # Read value
        if "content" not in field:
            return
        content = field["content"]
        if "text" not in content:
            return
        if "title" in content and content["title"].value:
            value = "%s: %s" % (content["title"].value, content["text"].value)
        else:
            value = content["text"].value

        # Known tag?
        tag = field["tag"].value
        if tag not in self.TAG_TO_KEY:
            if tag:
                if isinstance(tag, str):
                    tag = makePrintable(tag, "ISO-8859-1", to_unicode=True)
                self.warning("Skip ID3v2 tag %s: %s" % (tag, value))
            return
        key = self.TAG_TO_KEY[tag]
        setattr(self, key, value)

    def readID3v2(self, id3):
        for field in id3:
            if field.is_field_set and "tag" in field:
                self.processID3v2(field)

    def extract(self, mp3):
        if "/frames/frame[0]" in mp3:
            frame = mp3["/frames/frame[0]"]
            self.nb_channel = (frame.getNbChannel(), frame["channel_mode"].display)
            self.format_version = u"MPEG version %s layer %s" % \
                (frame["version"].display, frame["layer"].display)
            self.sample_rate = frame.getSampleRate()
            self.bits_per_sample = 16
            if mp3["frames"].looksConstantBitRate():
                self.computeBitrate(frame)
            else:
                self.computeVariableBitrate(mp3)
        if "id3v1" in mp3:
            id3 = mp3["id3v1"]
            self.comment = id3["comment"].value
            self.author = id3["author"].value
            self.title = id3["song"].value
            self.album = id3["album"].value
            if id3["year"].value != "0":
                self.creation_date = id3["year"].value
            if "track_nb" in id3:
                self.track_number = id3["track_nb"].value
        if "id3v2" in mp3:
            self.readID3v2(mp3["id3v2"])
        if "frames" in mp3:
            computeComprRate(self, mp3["frames"].size)

    def computeBitrate(self, frame):
        bit_rate = frame.getBitRate() # may returns None on error
        if not bit_rate:
            return
        self.bit_rate = (bit_rate, _("%s (constant)") % humanBitRate(bit_rate))
        self.duration = timedelta(seconds=float(frame["/frames"].size) / bit_rate)

    def computeVariableBitrate(self, mp3):
        if self.quality <= QUALITY_FAST:
            return
        count = 0
        if QUALITY_BEST <= self.quality:
            self.warning("Process all MPEG audio frames to compute exact duration")
            max_count = None
        else:
            max_count = 500 * self.quality
        total_bit_rate = 0.0
        for index, frame in enumerate(mp3.array("frames/frame")):
            if index < 3:
                continue
            bit_rate = frame.getBitRate()
            if bit_rate:
                total_bit_rate += float(bit_rate)
                count += 1
                if max_count and max_count <= count:
                    break
        if not count:
            return
        bit_rate = total_bit_rate / count
        self.bit_rate = (bit_rate,
            _("%s (Variable bit rate)") % humanBitRate(bit_rate))
        duration = timedelta(seconds=float(mp3["frames"].size) / bit_rate)
        self.duration = duration

class AiffMetadata(RootMetadata):
    def extract(self, aiff):
        if "common" in aiff:
            self.useCommon(aiff["common"])
        computeBitRate(self)

    @fault_tolerant
    def useCommon(self, info):
        self.nb_channel = info["nb_channel"].value
        self.bits_per_sample = info["sample_size"].value
        self.sample_rate = getValue(info, "sample_rate")
        if self.has("sample_rate"):
            rate = self.get("sample_rate")
            if rate:
                sec = float(info["nb_sample"].value) / rate
                self.duration = timedelta(seconds=sec)
        if "codec" in info:
            self.compression = info["codec"].display

class FlacMetadata(RootMetadata):
    def extract(self, flac):
        if "metadata/stream_info/content" in flac:
            self.useStreamInfo(flac["metadata/stream_info/content"])
        if "metadata/comment/content" in flac:
            readVorbisComment(self, flac["metadata/comment/content"])

    @fault_tolerant
    def useStreamInfo(self, info):
        self.nb_channel = info["nb_channel"].value + 1
        self.bits_per_sample = info["bits_per_sample"].value + 1
        self.sample_rate = info["sample_hertz"].value
        sec = info["total_samples"].value
        if sec:
            sec = float(sec) / info["sample_hertz"].value
            self.duration = timedelta(seconds=sec)

registerExtractor(AuFile, AuMetadata)
registerExtractor(MpegAudioFile, MpegAudioMetadata)
registerExtractor(OggFile, OggMetadata)
registerExtractor(RealMediaFile, RealMediaMetadata)
registerExtractor(RealAudioFile, RealAudioMetadata)
registerExtractor(AiffFile, AiffMetadata)
registerExtractor(FlacParser, FlacMetadata)


########NEW FILE########
__FILENAME__ = config
MAX_STR_LENGTH = 300  # characters
RAW_OUTPUT = False

########NEW FILE########
__FILENAME__ = file_system
from lib.hachoir_metadata.metadata import RootMetadata, registerExtractor
from lib.hachoir_metadata.safe import fault_tolerant
from lib.hachoir_parser.file_system import ISO9660
from datetime import datetime

class ISO9660_Metadata(RootMetadata):
    def extract(self, iso):
        desc = iso['volume[0]/content']
        self.title = desc['volume_id'].value
        self.title = desc['vol_set_id'].value
        self.author = desc['publisher'].value
        self.author = desc['data_preparer'].value
        self.producer = desc['application'].value
        self.copyright = desc['copyright'].value
        self.readTimestamp('creation_date', desc['creation_ts'].value)
        self.readTimestamp('last_modification', desc['modification_ts'].value)

    @fault_tolerant
    def readTimestamp(self, key, value):
        if value.startswith("0000"):
            return
        value = datetime(
            int(value[0:4]), int(value[4:6]), int(value[6:8]),
            int(value[8:10]), int(value[10:12]), int(value[12:14]))
        setattr(self, key, value)

registerExtractor(ISO9660, ISO9660_Metadata)


########NEW FILE########
__FILENAME__ = filter
from lib.hachoir_metadata.timezone import UTC
from datetime import date, datetime

# Year in 1850..2030
MIN_YEAR = 1850
MAX_YEAR = 2030

class Filter:
    def __init__(self, valid_types, min=None, max=None):
        self.types = valid_types
        self.min = min
        self.max = max

    def __call__(self, value):
        if not isinstance(value, self.types):
            return True
        if self.min is not None and value < self.min:
            return False
        if self.max is not None and self.max < value:
            return False
        return True

class NumberFilter(Filter):
    def __init__(self, min=None, max=None):
        Filter.__init__(self, (int, long, float), min, max)

class DatetimeFilter(Filter):
    def __init__(self, min=None, max=None):
        Filter.__init__(self, (date, datetime),
            datetime(MIN_YEAR, 1, 1),
            datetime(MAX_YEAR, 12, 31))
        self.min_date = date(MIN_YEAR, 1, 1)
        self.max_date = date(MAX_YEAR, 12, 31)
        self.min_tz = datetime(MIN_YEAR, 1, 1, tzinfo=UTC)
        self.max_tz = datetime(MAX_YEAR, 12, 31, tzinfo=UTC)

    def __call__(self, value):
        """
        Use different min/max values depending on value type
        (datetime with timezone, datetime or date).
        """
        if not isinstance(value, self.types):
            return True
        if hasattr(value, "tzinfo") and value.tzinfo:
            return (self.min_tz <= value <= self.max_tz)
        elif isinstance(value, datetime):
            return (self.min <= value <= self.max)
        else:
            return (self.min_date <= value <= self.max_date)

DATETIME_FILTER = DatetimeFilter()


########NEW FILE########
__FILENAME__ = formatter
from lib.hachoir_core.i18n import _, ngettext

NB_CHANNEL_NAME = {1: _("mono"), 2: _("stereo")}

def humanAudioChannel(value):
    return NB_CHANNEL_NAME.get(value, unicode(value))

def humanFrameRate(value):
    if isinstance(value, (int, long, float)):
        return _("%.1f fps") % value
    else:
        return value

def humanComprRate(rate):
    return u"%.1fx" % rate

def humanAltitude(value):
    return ngettext("%.1f meter", "%.1f meters", value) % value

def humanPixelSize(value):
    return ngettext("%s pixel", "%s pixels", value) % value

def humanDPI(value):
    return u"%s DPI" % value


########NEW FILE########
__FILENAME__ = image
from lib.hachoir_metadata.metadata import (registerExtractor,
    Metadata, RootMetadata, MultipleMetadata)
from lib.hachoir_parser.image import (
    BmpFile, IcoFile, PcxFile, GifFile, PngFile, TiffFile,
    XcfFile, TargaFile, WMF_File, PsdFile)
from lib.hachoir_parser.image.png import getBitsPerPixel as pngBitsPerPixel
from lib.hachoir_parser.image.xcf import XcfProperty
from lib.hachoir_core.i18n import _
from lib.hachoir_metadata.safe import fault_tolerant

def computeComprRate(meta, compr_size):
    """
    Compute image compression rate. Skip size of color palette, focus on
    image pixels. Original size is width x height x bpp. Compressed size
    is an argument (in bits).

    Set "compr_data" with a string like "1.52x".
    """
    if not meta.has("width") \
    or not meta.has("height") \
    or not meta.has("bits_per_pixel"):
        return
    if not compr_size:
        return
    orig_size = meta.get('width') * meta.get('height') * meta.get('bits_per_pixel')
    meta.compr_rate = float(orig_size) / compr_size

class BmpMetadata(RootMetadata):
    def extract(self, image):
        if "header" not in image:
            return
        hdr = image["header"]
        self.width = hdr["width"].value
        self.height = hdr["height"].value
        bpp = hdr["bpp"].value
        if bpp:
            if bpp <= 8 and "used_colors" in hdr:
                self.nb_colors = hdr["used_colors"].value
            self.bits_per_pixel = bpp
        self.compression = hdr["compression"].display
        self.format_version = u"Microsoft Bitmap version %s" % hdr.getFormatVersion()

        self.width_dpi = hdr["horizontal_dpi"].value
        self.height_dpi = hdr["vertical_dpi"].value

        if "pixels" in image:
            computeComprRate(self, image["pixels"].size)

class TiffMetadata(RootMetadata):
    key_to_attr = {
        "img_width": "width",
        "img_height": "width",

        # TODO: Enable that (need link to value)
#        "description": "comment",
#        "doc_name": "title",
#        "orientation": "image_orientation",
    }
    def extract(self, tiff):
        if "ifd" in tiff:
            self.useIFD(tiff["ifd"])

    def useIFD(self, ifd):
        for field in ifd:
            try:
                attrname = self.key_to_attr[field.name]
            except KeyError:
                continue
            if "value" not in field:
                continue
            value = field["value"].value
            setattr(self, attrname, value)

class IcoMetadata(MultipleMetadata):
    color_to_bpp = {
        2: 1,
        16: 4,
        256: 8
    }

    def extract(self, icon):
        for index, header in enumerate(icon.array("icon_header")):
            image = Metadata(self)

            # Read size and colors from header
            image.width = header["width"].value
            image.height = header["height"].value
            bpp = header["bpp"].value
            nb_colors = header["nb_color"].value
            if nb_colors != 0:
                image.nb_colors = nb_colors
                if bpp == 0 and nb_colors in self.color_to_bpp:
                    bpp = self.color_to_bpp[nb_colors]
            elif bpp == 0:
                bpp = 8
            image.bits_per_pixel = bpp
            image.setHeader(_("Icon #%u (%sx%s)")
                % (1+index, image.get("width", "?"), image.get("height", "?")))

            # Read compression from data (if available)
            key = "icon_data[%u]/header/codec" % index
            if key in icon:
                image.compression = icon[key].display
            key = "icon_data[%u]/pixels" % index
            if key in icon:
                computeComprRate(image, icon[key].size)

            # Store new image
            self.addGroup("image[%u]" % index, image)

class PcxMetadata(RootMetadata):
    @fault_tolerant
    def extract(self, pcx):
        self.width = 1 + pcx["xmax"].value
        self.height = 1 + pcx["ymax"].value
        self.width_dpi = pcx["horiz_dpi"].value
        self.height_dpi = pcx["vert_dpi"].value
        self.bits_per_pixel = pcx["bpp"].value
        if 1 <= pcx["bpp"].value <= 8:
            self.nb_colors = 2 ** pcx["bpp"].value
        self.compression = _("Run-length encoding (RLE)")
        self.format_version = "PCX: %s" % pcx["version"].display
        if "image_data" in pcx:
            computeComprRate(self, pcx["image_data"].size)

class XcfMetadata(RootMetadata):
    # Map image type to bits/pixel
    TYPE_TO_BPP = {0: 24, 1: 8, 2: 8}

    def extract(self, xcf):
        self.width = xcf["width"].value
        self.height = xcf["height"].value
        try:
            self.bits_per_pixel = self.TYPE_TO_BPP[ xcf["type"].value ]
        except KeyError:
            pass
        self.format_version = xcf["type"].display
        self.readProperties(xcf)

    @fault_tolerant
    def processProperty(self, prop):
        type = prop["type"].value
        if type == XcfProperty.PROP_PARASITES:
            for field in prop["data"]:
                if "name" not in field or "data" not in field:
                    continue
                if field["name"].value == "gimp-comment":
                    self.comment = field["data"].value
        elif type == XcfProperty.PROP_COMPRESSION:
            self.compression = prop["data/compression"].display
        elif type == XcfProperty.PROP_RESOLUTION:
            self.width_dpi = int(prop["data/xres"].value)
            self.height_dpi = int(prop["data/yres"].value)

    def readProperties(self, xcf):
        for prop in xcf.array("property"):
            self.processProperty(prop)

class PngMetadata(RootMetadata):
    TEXT_TO_ATTR = {
        "software": "producer",
    }

    def extract(self, png):
        if "header" in png:
            self.useHeader(png["header"])
        if "time" in png:
            self.useTime(png["time"])
        if "physical" in png:
            self.usePhysical(png["physical"])
        for comment in png.array("text"):
            if "text" not in comment:
                continue
            keyword = comment["keyword"].value
            text = comment["text"].value
            try:
                key = self.TEXT_TO_ATTR[keyword.lower()]
                setattr(self, key, text)
            except KeyError:
                if keyword.lower() != "comment":
                    self.comment = "%s=%s" % (keyword, text)
                else:
                    self.comment = text
        compr_size = sum( data.size for data in png.array("data") )
        computeComprRate(self, compr_size)

    @fault_tolerant
    def useTime(self, field):
        self.creation_date = field.value

    @fault_tolerant
    def usePhysical(self, field):
        self.width_dpi = field["pixel_per_unit_x"].value
        self.height_dpi = field["pixel_per_unit_y"].value

    @fault_tolerant
    def useHeader(self, header):
        self.width = header["width"].value
        self.height = header["height"].value

        # Read number of colors and pixel format
        if "/palette/size" in header:
            nb_colors = header["/palette/size"].value // 3
        else:
            nb_colors = None
        if not header["has_palette"].value:
            if header["has_alpha"].value:
                self.pixel_format = _("RGBA")
            else:
                self.pixel_format = _("RGB")
        elif "/transparency" in header:
            self.pixel_format = _("Color index with transparency")
            if nb_colors:
                nb_colors -= 1
        else:
            self.pixel_format = _("Color index")
        self.bits_per_pixel = pngBitsPerPixel(header)
        if nb_colors:
            self.nb_colors = nb_colors

        # Read compression, timestamp, etc.
        self.compression = header["compression"].display

class GifMetadata(RootMetadata):
    def extract(self, gif):
        self.useScreen(gif["/screen"])
        if self.has("bits_per_pixel"):
            self.nb_colors = (1 << self.get('bits_per_pixel'))
        self.compression = _("LZW")
        self.format_version =  "GIF version %s" % gif["version"].value
        for comments in gif.array("comments"):
            for comment in gif.array(comments.name + "/comment"):
                self.comment = comment.value
        if "graphic_ctl/has_transp" in gif and gif["graphic_ctl/has_transp"].value:
            self.pixel_format = _("Color index with transparency")
        else:
            self.pixel_format = _("Color index")

    @fault_tolerant
    def useScreen(self, screen):
        self.width = screen["width"].value
        self.height = screen["height"].value
        self.bits_per_pixel = (1 + screen["bpp"].value)

class TargaMetadata(RootMetadata):
    def extract(self, tga):
        self.width = tga["width"].value
        self.height = tga["height"].value
        self.bits_per_pixel = tga["bpp"].value
        if tga["nb_color"].value:
            self.nb_colors = tga["nb_color"].value
        self.compression = tga["codec"].display
        if "pixels" in tga:
            computeComprRate(self, tga["pixels"].size)

class WmfMetadata(RootMetadata):
    def extract(self, wmf):
        if wmf.isAPM():
            if "amf_header/rect" in wmf:
                rect = wmf["amf_header/rect"]
                self.width = (rect["right"].value - rect["left"].value)
                self.height = (rect["bottom"].value - rect["top"].value)
            self.bits_per_pixel = 24
        elif wmf.isEMF():
            emf = wmf["emf_header"]
            if "description" in emf:
                desc = emf["description"].value
                if "\0" in desc:
                    self.producer, self.title = desc.split("\0", 1)
                else:
                    self.producer = desc
            if emf["nb_colors"].value:
                self.nb_colors = emf["nb_colors"].value
                self.bits_per_pixel = 8
            else:
                self.bits_per_pixel = 24
            self.width = emf["width_px"].value
            self.height = emf["height_px"].value

class PsdMetadata(RootMetadata):
    @fault_tolerant
    def extract(self, psd):
        self.width = psd["width"].value
        self.height = psd["height"].value
        self.bits_per_pixel = psd["depth"].value * psd["nb_channels"].value
        self.pixel_format = psd["color_mode"].display
        self.compression = psd["compression"].display

registerExtractor(IcoFile, IcoMetadata)
registerExtractor(GifFile, GifMetadata)
registerExtractor(XcfFile, XcfMetadata)
registerExtractor(TargaFile, TargaMetadata)
registerExtractor(PcxFile, PcxMetadata)
registerExtractor(BmpFile, BmpMetadata)
registerExtractor(PngFile, PngMetadata)
registerExtractor(TiffFile, TiffMetadata)
registerExtractor(WMF_File, WmfMetadata)
registerExtractor(PsdFile, PsdMetadata)


########NEW FILE########
__FILENAME__ = jpeg
from lib.hachoir_metadata.metadata import RootMetadata, registerExtractor
from lib.hachoir_metadata.image import computeComprRate
from lib.hachoir_parser.image.exif import ExifEntry
from lib.hachoir_parser.image.jpeg import (
    JpegFile, JpegChunk,
    QUALITY_HASH_COLOR, QUALITY_SUM_COLOR,
    QUALITY_HASH_GRAY, QUALITY_SUM_GRAY)
from lib.hachoir_core.field import MissingField
from lib.hachoir_core.i18n import _
from lib.hachoir_core.tools import makeUnicode
from lib.hachoir_metadata.safe import fault_tolerant
from datetime import datetime

def deg2float(degree, minute, second):
    return degree + (float(minute) + float(second) / 60.0) / 60.0

class JpegMetadata(RootMetadata):
    EXIF_KEY = {
        # Exif metadatas
        ExifEntry.TAG_CAMERA_MANUFACTURER: "camera_manufacturer",
        ExifEntry.TAG_CAMERA_MODEL: "camera_model",
        ExifEntry.TAG_ORIENTATION: "image_orientation",
        ExifEntry.TAG_EXPOSURE: "camera_exposure",
        ExifEntry.TAG_FOCAL: "camera_focal",
        ExifEntry.TAG_BRIGHTNESS: "camera_brightness",
        ExifEntry.TAG_APERTURE: "camera_aperture",

        # Generic metadatas
        ExifEntry.TAG_IMG_TITLE: "title",
        ExifEntry.TAG_SOFTWARE: "producer",
        ExifEntry.TAG_FILE_TIMESTAMP: "creation_date",
        ExifEntry.TAG_WIDTH: "width",
        ExifEntry.TAG_HEIGHT: "height",
        ExifEntry.TAG_USER_COMMENT: "comment",
    }

    IPTC_KEY = {
         80: "author",
         90: "city",
        101: "country",
        116: "copyright",
        120: "title",
        231: "comment",
    }

    orientation_name = {
        1: _('Horizontal (normal)'),
        2: _('Mirrored horizontal'),
        3: _('Rotated 180'),
        4: _('Mirrored vertical'),
        5: _('Mirrored horizontal then rotated 90 counter-clock-wise'),
        6: _('Rotated 90 clock-wise'),
        7: _('Mirrored horizontal then rotated 90 clock-wise'),
        8: _('Rotated 90 counter clock-wise'),
    }

    def extract(self, jpeg):
        if "start_frame/content" in jpeg:
            self.startOfFrame(jpeg["start_frame/content"])
        elif "start_scan/content/nr_components" in jpeg:
            self.bits_per_pixel = 8 * jpeg["start_scan/content/nr_components"].value
        if "app0/content" in jpeg:
            self.extractAPP0(jpeg["app0/content"])

        if "exif/content" in jpeg:
            for ifd in jpeg.array("exif/content/ifd"):
                for entry in ifd.array("entry"):
                    self.processIfdEntry(ifd, entry)
                self.readGPS(ifd)
        if "photoshop/content" in jpeg:
            psd = jpeg["photoshop/content"]
            if "version/content/reader_name" in psd:
                self.producer = psd["version/content/reader_name"].value
            if "iptc/content" in psd:
                self.parseIPTC(psd["iptc/content"])
        for field in jpeg.array("comment"):
            if "content/comment" in field:
                self.comment = field["content/comment"].value
        self.computeQuality(jpeg)
        if "data" in jpeg:
            computeComprRate(self, jpeg["data"].size)
        if not self.has("producer") and "photoshop" in jpeg:
            self.producer = u"Adobe Photoshop"
        if self.has("compression"):
            self.compression = "JPEG"

    @fault_tolerant
    def startOfFrame(self, sof):
        # Set compression method
        key = sof["../type"].value
        self.compression = "JPEG (%s)" % JpegChunk.START_OF_FRAME[key]

        # Read image size and bits/pixel
        self.width = sof["width"].value
        self.height = sof["height"].value
        nb_components = sof["nr_components"].value
        self.bits_per_pixel = 8 * nb_components
        if nb_components == 3:
            self.pixel_format = _("YCbCr")
        elif nb_components == 1:
            self.pixel_format = _("Grayscale")
            self.nb_colors = 256

    @fault_tolerant
    def computeQuality(self, jpeg):
        # This function is an adaption to Python of ImageMagick code
        # to compute JPEG quality using quantization tables

        # Read quantization tables
        qtlist = []
        for dqt in jpeg.array("quantization"):
            for qt in dqt.array("content/qt"):
                # TODO: Take care of qt["index"].value?
                qtlist.append(qt)
        if not qtlist:
            return

        # Compute sum of all coefficients
        sumcoeff = 0
        for qt in qtlist:
           coeff = qt.array("coeff")
           for index in xrange(64):
                sumcoeff += coeff[index].value

        # Choose the right quality table and compute hash value
        try:
            hashval= qtlist[0]["coeff[2]"].value +  qtlist[0]["coeff[53]"].value
            if 2 <= len(qtlist):
                hashval += qtlist[1]["coeff[0]"].value + qtlist[1]["coeff[63]"].value
                hashtable = QUALITY_HASH_COLOR
                sumtable = QUALITY_SUM_COLOR
            else:
                hashtable = QUALITY_HASH_GRAY
                sumtable = QUALITY_SUM_GRAY
        except (MissingField, IndexError):
            # A coefficient is missing, so don't compute JPEG quality
            return

        # Find the JPEG quality
        for index in xrange(100):
            if (hashval >= hashtable[index]) or (sumcoeff >= sumtable[index]):
                quality = "%s%%" % (index + 1)
                if (hashval > hashtable[index]) or (sumcoeff > sumtable[index]):
                    quality += " " + _("(approximate)")
                self.comment = "JPEG quality: %s" % quality
                return

    @fault_tolerant
    def extractAPP0(self, app0):
        self.format_version = u"JFIF %u.%02u" \
            % (app0["ver_maj"].value, app0["ver_min"].value)
        if "y_density" in app0:
            self.width_dpi = app0["x_density"].value
            self.height_dpi = app0["y_density"].value

    @fault_tolerant
    def processIfdEntry(self, ifd, entry):
        # Skip unknown tags
        tag = entry["tag"].value
        if tag not in self.EXIF_KEY:
            return
        key = self.EXIF_KEY[tag]
        if key in ("width", "height") and self.has(key):
            # EXIF "valid size" are sometimes not updated when the image is scaled
            # so we just ignore it
            return

        # Read value
        if "value" in entry:
            value = entry["value"].value
        else:
            value = ifd["value_%s" % entry.name].value

        # Convert value to string
        if tag == ExifEntry.TAG_ORIENTATION:
            value = self.orientation_name.get(value, value)
        elif tag == ExifEntry.TAG_EXPOSURE:
            if not value:
                return
            if isinstance(value, float):
                value = (value, u"1/%g" % (1/value))
        elif entry["type"].value in (ExifEntry.TYPE_RATIONAL, ExifEntry.TYPE_SIGNED_RATIONAL):
            value = (value, u"%.3g" % value)

        # Store information
        setattr(self, key, value)

    @fault_tolerant
    def readGPS(self, ifd):
        # Read latitude and longitude
        latitude_ref = None
        longitude_ref = None
        latitude = None
        longitude = None
        altitude_ref = 1
        altitude = None
        timestamp = None
        datestamp = None
        for entry in ifd.array("entry"):
            tag = entry["tag"].value
            if tag == ExifEntry.TAG_GPS_LATITUDE_REF:
                if entry["value"].value == "N":
                    latitude_ref = 1
                else:
                    latitude_ref = -1
            elif tag == ExifEntry.TAG_GPS_LONGITUDE_REF:
                if entry["value"].value == "E":
                    longitude_ref = 1
                else:
                    longitude_ref = -1
            elif tag == ExifEntry.TAG_GPS_ALTITUDE_REF:
                if entry["value"].value == 1:
                    altitude_ref = -1
                else:
                    altitude_ref = 1
            elif tag == ExifEntry.TAG_GPS_LATITUDE:
                latitude = [ifd["value_%s[%u]" % (entry.name, index)].value for index in xrange(3)]
            elif tag == ExifEntry.TAG_GPS_LONGITUDE:
                longitude = [ifd["value_%s[%u]" % (entry.name, index)].value for index in xrange(3)]
            elif tag == ExifEntry.TAG_GPS_ALTITUDE:
                altitude = ifd["value_%s" % entry.name].value
            elif tag == ExifEntry.TAG_GPS_DATESTAMP:
                datestamp = ifd["value_%s" % entry.name].value
            elif tag == ExifEntry.TAG_GPS_TIMESTAMP:
                items = [ifd["value_%s[%u]" % (entry.name, index)].value for index in xrange(3)]
                items = map(int, items)
                items = map(str, items)
                timestamp = ":".join(items)
        if latitude_ref and latitude:
            value = deg2float(*latitude)
            if latitude_ref < 0:
                value = -value
            self.latitude = value
        if longitude and longitude_ref:
            value = deg2float(*longitude)
            if longitude_ref < 0:
                value = -value
            self.longitude = value
        if altitude:
            value = altitude
            if altitude_ref < 0:
                value = -value
            self.altitude = value
        if datestamp:
            if timestamp:
                datestamp += " " + timestamp
            self.creation_date = datestamp

    def parseIPTC(self, iptc):
        datestr = hourstr = None
        for field in iptc:
            # Skip incomplete field
            if "tag" not in field or "content" not in field:
                continue

            # Get value
            value = field["content"].value
            if isinstance(value, (str, unicode)):
                value = value.replace("\r", " ")
                value = value.replace("\n", " ")

            # Skip unknown tag
            tag = field["tag"].value
            if tag == 55:
                datestr = value
                continue
            if tag == 60:
                hourstr = value
                continue
            if tag not in self.IPTC_KEY:
                if tag != 0:
                    self.warning("Skip IPTC key %s: %s" % (
                        field["tag"].display, makeUnicode(value)))
                continue
            setattr(self, self.IPTC_KEY[tag], value)
        if datestr and hourstr:
            try:
                year = int(datestr[0:4])
                month = int(datestr[4:6])
                day = int(datestr[6:8])
                hour = int(hourstr[0:2])
                min = int(hourstr[2:4])
                sec = int(hourstr[4:6])
                self.creation_date = datetime(year, month, day, hour, min, sec)
            except ValueError:
                pass

registerExtractor(JpegFile, JpegMetadata)


########NEW FILE########
__FILENAME__ = metadata
# -*- coding: utf-8 -*-
from lib.hachoir_core.compatibility import any, sorted
from lib.hachoir_core.endian import endian_name
from lib.hachoir_core.tools import makePrintable, makeUnicode
from lib.hachoir_core.dict import Dict
from lib.hachoir_core.error import error, HACHOIR_ERRORS
from lib.hachoir_core.i18n import _
from lib.hachoir_core.log import Logger
from lib.hachoir_metadata.metadata_item import (
    MIN_PRIORITY, MAX_PRIORITY, QUALITY_NORMAL)
from lib.hachoir_metadata.register import registerAllItems

extractors = {}

class Metadata(Logger):
    header = u"Metadata"

    def __init__(self, parent, quality=QUALITY_NORMAL):
        assert isinstance(self.header, unicode)

        # Limit to 0.0 .. 1.0
        if parent:
            quality = parent.quality
        else:
            quality = min(max(0.0, quality), 1.0)

        object.__init__(self)
        object.__setattr__(self, "_Metadata__data", {})
        object.__setattr__(self, "quality", quality)
        header = self.__class__.header
        object.__setattr__(self, "_Metadata__header", header)

        registerAllItems(self)

    def _logger(self):
        pass

    def __setattr__(self, key, value):
        """
        Add a new value to data with name 'key'. Skip duplicates.
        """
        # Invalid key?
        if key not in self.__data:
            raise KeyError(_("%s has no metadata '%s'") % (self.__class__.__name__, key))

        # Skip duplicates
        self.__data[key].add(value)

    def setHeader(self, text):
        object.__setattr__(self, "header", text)

    def getItems(self, key):
        try:
            return self.__data[key]
        except LookupError:
            raise ValueError("Metadata has no value '%s'" % key)

    def getItem(self, key, index):
        try:
            return self.getItems(key)[index]
        except (LookupError, ValueError):
            return None

    def has(self, key):
        return 1 <= len(self.getItems(key))

    def get(self, key, default=None, index=0):
        """
        Read first value of tag with name 'key'.

        >>> from datetime import timedelta
        >>> a = RootMetadata()
        >>> a.duration = timedelta(seconds=2300)
        >>> a.get('duration')
        datetime.timedelta(0, 2300)
        >>> a.get('author', u'Anonymous')
        u'Anonymous'
        """
        item = self.getItem(key, index)
        if item is None:
            if default is None:
                raise ValueError("Metadata has no value '%s' (index %s)" % (key, index))
            else:
                return default
        return item.value

    def getValues(self, key):
        try:
            data = self.__data[key]
        except LookupError:
            raise ValueError("Metadata has no value '%s'" % key)
        return [ item.value for item in data ]

    def getText(self, key, default=None, index=0):
        """
        Read first value, as unicode string, of tag with name 'key'.

        >>> from datetime import timedelta
        >>> a = RootMetadata()
        >>> a.duration = timedelta(seconds=2300)
        >>> a.getText('duration')
        u'38 min 20 sec'
        >>> a.getText('titre', u'Unknown')
        u'Unknown'
        """
        item = self.getItem(key, index)
        if item is not None:
            return item.text
        else:
            return default

    def register(self, data):
        assert data.key not in self.__data
        data.metadata = self
        self.__data[data.key] = data

    def __iter__(self):
        return self.__data.itervalues()

    def __str__(self):
        r"""
        Create a multi-line ASCII string (end of line is "\n") which
        represents all datas.

        >>> a = RootMetadata()
        >>> a.author = "haypo"
        >>> a.copyright = unicode(" Hachoir", "UTF-8")
        >>> print a
        Metadata:
        - Author: haypo
        - Copyright: \xa9 Hachoir

        @see __unicode__() and exportPlaintext()
        """
        text = self.exportPlaintext()
        return "\n".join( makePrintable(line, "ASCII") for line in text )

    def __unicode__(self):
        r"""
        Create a multi-line Unicode string (end of line is "\n") which
        represents all datas.

        >>> a = RootMetadata()
        >>> a.copyright = unicode(" Hachoir", "UTF-8")
        >>> print repr(unicode(a))
        u'Metadata:\n- Copyright: \xa9 Hachoir'

        @see __str__() and exportPlaintext()
        """
        return "\n".join(self.exportPlaintext())

    def exportPlaintext(self, priority=None, human=True, line_prefix=u"- ", title=None):
        r"""
        Convert metadata to multi-line Unicode string and skip datas
        with priority lower than specified priority.

        Default priority is Metadata.MAX_PRIORITY. If human flag is True, data
        key are translated to better human name (eg. "bit_rate" becomes
        "Bit rate") which may be translated using gettext.

        If priority is too small, metadata are empty and so None is returned.

        >>> print RootMetadata().exportPlaintext()
        None
        >>> meta = RootMetadata()
        >>> meta.copyright = unicode(" Hachoir", "UTF-8")
        >>> print repr(meta.exportPlaintext())
        [u'Metadata:', u'- Copyright: \xa9 Hachoir']

        @see __str__() and __unicode__()
        """
        if priority is not None:
            priority = max(priority, MIN_PRIORITY)
            priority = min(priority, MAX_PRIORITY)
        else:
            priority = MAX_PRIORITY
        if not title:
            title = self.header
        text = ["%s:" % title]
        for data in sorted(self):
            if priority < data.priority:
                break
            if not data.values:
                continue
            if human:
                title = data.description
            else:
                title = data.key
            for item in data.values:
                if human:
                    value = item.text
                else:
                    value = makeUnicode(item.value)
                text.append("%s%s: %s" % (line_prefix, title, value))
        if 1 < len(text):
            return text
        else:
            return None

    def __nonzero__(self):
        return any(item for item in self.__data.itervalues())

class RootMetadata(Metadata):
    def __init__(self, quality=QUALITY_NORMAL):
        Metadata.__init__(self, None, quality)

class MultipleMetadata(RootMetadata):
    header = _("Common")
    def __init__(self, quality=QUALITY_NORMAL):
        RootMetadata.__init__(self, quality)
        object.__setattr__(self, "_MultipleMetadata__groups", Dict())
        object.__setattr__(self, "_MultipleMetadata__key_counter", {})

    def __contains__(self, key):
        return key in self.__groups

    def __getitem__(self, key):
        return self.__groups[key]

    def iterGroups(self):
        return self.__groups.itervalues()

    def __nonzero__(self):
        if RootMetadata.__nonzero__(self):
            return True
        return any(bool(group) for group in self.__groups)

    def addGroup(self, key, metadata, header=None):
        """
        Add a new group (metadata of a sub-document).

        Returns False if the group is skipped, True if it has been added.
        """
        if not metadata:
            self.warning("Skip empty group %s" % key)
            return False
        if key.endswith("[]"):
            key = key[:-2]
            if key in self.__key_counter:
                self.__key_counter[key] += 1
            else:
                self.__key_counter[key] = 1
            key += "[%u]" % self.__key_counter[key]
        if header:
            metadata.setHeader(header)
        self.__groups.append(key, metadata)
        return True

    def exportPlaintext(self, priority=None, human=True, line_prefix=u"- "):
        common = Metadata.exportPlaintext(self, priority, human, line_prefix)
        if common:
            text = common
        else:
            text = []
        for key, metadata in self.__groups.iteritems():
            if not human:
                title = key
            else:
                title = None
            value = metadata.exportPlaintext(priority, human, line_prefix, title=title)
            if value:
                text.extend(value)
        if len(text):
            return text
        else:
            return None

def registerExtractor(parser, extractor):
    assert parser not in extractors
    assert issubclass(extractor, RootMetadata)
    extractors[parser] = extractor

def extractMetadata(parser, quality=QUALITY_NORMAL):
    """
    Create a Metadata class from a parser. Returns None if no metadata
    extractor does exist for the parser class.
    """
    try:
        extractor = extractors[parser.__class__]
    except KeyError:
        return None
    metadata = extractor(quality)
    try:
        metadata.extract(parser)
    except HACHOIR_ERRORS, err:
        error("Error during metadata extraction: %s" % unicode(err))
    if metadata:
        metadata.mime_type = parser.mime_type
        metadata.endian = endian_name[parser.endian]
    return metadata


########NEW FILE########
__FILENAME__ = metadata_item
from lib.hachoir_core.tools import makeUnicode, normalizeNewline
from lib.hachoir_core.error import HACHOIR_ERRORS
from lib.hachoir_metadata import config
from lib.hachoir_metadata.setter import normalizeString

MIN_PRIORITY = 100
MAX_PRIORITY = 999

QUALITY_FASTEST = 0.0
QUALITY_FAST = 0.25
QUALITY_NORMAL = 0.5
QUALITY_GOOD = 0.75
QUALITY_BEST = 1.0

class DataValue:
    def __init__(self, value, text):
        self.value = value
        self.text = text

class Data:
    def __init__(self, key, priority, description,
    text_handler=None, type=None, filter=None, conversion=None):
        """
        handler is only used if value is not string nor unicode, prototype:
           def handler(value) -> str/unicode
        """
        assert MIN_PRIORITY <= priority <= MAX_PRIORITY
        assert isinstance(description, unicode)
        self.metadata = None
        self.key = key
        self.description = description
        self.values = []
        if type and not isinstance(type, (tuple, list)):
            type = (type,)
        self.type = type
        self.text_handler = text_handler
        self.filter = filter
        self.priority = priority
        self.conversion = conversion

    def _createItem(self, value, text=None):
        if text is None:
            if isinstance(value, unicode):
                text = value
            elif self.text_handler:
                text = self.text_handler(value)
                assert isinstance(text, unicode)
            else:
                text = makeUnicode(value)
        return DataValue(value, text)

    def add(self, value):
        if isinstance(value, tuple):
            if len(value) != 2:
                raise ValueError("Data.add() only accept tuple of 2 elements: (value,text)")
            value, text = value
        else:
            text = None

        # Skip value 'None'
        if value is None:
            return

        if isinstance(value, (str, unicode)):
            value = normalizeString(value)
            if not value:
                return

        # Convert string to Unicode string using charset ISO-8859-1
        if self.conversion:
            try:
                new_value = self.conversion(self.metadata, self.key, value)
            except HACHOIR_ERRORS, err:
                self.metadata.warning("Error during conversion of %r value: %s" % (
                    self.key, err))
                return
            if new_value is None:
                dest_types = " or ".join(str(item.__name__) for item in self.type)
                self.metadata.warning("Unable to convert %s=%r (%s) to %s" % (
                    self.key, value, type(value).__name__, dest_types))
                return
            if isinstance(new_value, tuple):
                if text:
                    value = new_value[0]
                else:
                    value, text = new_value
            else:
                value = new_value
        elif isinstance(value, str):
            value = unicode(value, "ISO-8859-1")

        if self.type and not isinstance(value, self.type):
            dest_types = " or ".join(str(item.__name__) for item in self.type)
            self.metadata.warning("Key %r: value %r type (%s) is not %s" % (
                self.key, value, type(value).__name__, dest_types))
            return

        # Skip empty strings
        if isinstance(value, unicode):
            value = normalizeNewline(value)
            if config.MAX_STR_LENGTH \
            and config.MAX_STR_LENGTH < len(value):
                value = value[:config.MAX_STR_LENGTH] + "(...)"

        # Skip duplicates
        if value in self:
            return

        # Use filter
        if self.filter and not self.filter(value):
            self.metadata.warning("Skip value %s=%r (filter)" % (self.key, value))
            return

        # For string, if you have "verlongtext" and "verylo",
        # keep the longer value
        if isinstance(value, unicode):
            for index, item in enumerate(self.values):
                item = item.value
                if not isinstance(item, unicode):
                    continue
                if value.startswith(item):
                    # Find longer value, replace the old one
                    self.values[index] = self._createItem(value, text)
                    return
                if item.startswith(value):
                    # Find truncated value, skip it
                    return

        # Add new value
        self.values.append(self._createItem(value, text))

    def __len__(self):
        return len(self.values)

    def __getitem__(self, index):
        return self.values[index]

    def __contains__(self, value):
        for item in self.values:
            if value == item.value:
                return True
        return False

    def __cmp__(self, other):
        return cmp(self.priority, other.priority)


########NEW FILE########
__FILENAME__ = misc
from lib.hachoir_metadata.metadata import RootMetadata, registerExtractor
from lib.hachoir_metadata.safe import fault_tolerant
from lib.hachoir_parser.container import SwfFile
from lib.hachoir_parser.misc import TorrentFile, TrueTypeFontFile, OLE2_File, PcfFile
from lib.hachoir_core.field import isString
from lib.hachoir_core.error import warning
from lib.hachoir_parser import guessParser
from lib.hachoir_metadata.setter import normalizeString

class TorrentMetadata(RootMetadata):
    KEY_TO_ATTR = {
        u"announce": "url",
        u"comment": "comment",
        u"creation_date": "creation_date",
    }
    INFO_TO_ATTR = {
        u"length": "file_size",
        u"name": "filename",
    }

    def extract(self, torrent):
        for field in torrent[0]:
            self.processRoot(field)

    @fault_tolerant
    def processRoot(self, field):
        if field.name in self.KEY_TO_ATTR:
            key = self.KEY_TO_ATTR[field.name]
            value = field.value
            setattr(self, key, value)
        elif field.name == "info" and "value" in field:
            for field in field["value"]:
                self.processInfo(field)

    @fault_tolerant
    def processInfo(self, field):
        if field.name in self.INFO_TO_ATTR:
            key = self.INFO_TO_ATTR[field.name]
            value = field.value
            setattr(self, key, value)
        elif field.name == "piece_length":
            self.comment = "Piece length: %s" % field.display

class TTF_Metadata(RootMetadata):
    NAMEID_TO_ATTR = {
        0: "copyright",   # Copyright notice
        3: "title",       # Unique font identifier
        5: "version",     # Version string
        8: "author",      # Manufacturer name
        11: "url",        # URL Vendor
        14: "copyright",  # License info URL
    }

    def extract(self, ttf):
        if "header" in ttf:
            self.extractHeader(ttf["header"])
        if "names" in ttf:
            self.extractNames(ttf["names"])

    @fault_tolerant
    def extractHeader(self, header):
        self.creation_date = header["created"].value
        self.last_modification = header["modified"].value
        self.comment = u"Smallest readable size in pixels: %s pixels" % header["lowest"].value
        self.comment = u"Font direction: %s" % header["font_dir"].display

    @fault_tolerant
    def extractNames(self, names):
        offset = names["offset"].value
        for header in names.array("header"):
            key = header["nameID"].value
            foffset = offset + header["offset"].value
            field = names.getFieldByAddress(foffset*8)
            if not field or not isString(field):
                continue
            value = field.value
            if key not in self.NAMEID_TO_ATTR:
                continue
            key = self.NAMEID_TO_ATTR[key]
            if key == "version" and value.startswith(u"Version "):
                # "Version 1.2" => "1.2"
                value = value[8:]
            setattr(self, key, value)

class OLE2_Metadata(RootMetadata):
    SUMMARY_ID_TO_ATTR = {
         2: "title",     # Title
         3: "title",     # Subject
         4: "author",
         6: "comment",
         8: "author",    # Last saved by
        12: "creation_date",
        13: "last_modification",
        14: "nb_page",
        18: "producer",
    }
    IGNORE_SUMMARY = set((
        1, # Code page
    ))

    DOC_SUMMARY_ID_TO_ATTR = {
         3: "title",     # Subject
        14: "author",    # Manager
    }
    IGNORE_DOC_SUMMARY = set((
        1, # Code page
    ))

    def extract(self, ole2):
        self._extract(ole2)

    def _extract(self, fieldset, main_document=True):
        if main_document:
            # _feedAll() is needed to make sure that we get all root[*] fragments
            fieldset._feedAll()
            if "root[0]" in fieldset:
                self.useRoot(fieldset["root[0]"])
        doc_summary = self.getField(fieldset, main_document, "doc_summary[0]")
        if doc_summary:
            self.useSummary(doc_summary, True)
        word_doc = self.getField(fieldset, main_document, "word_doc[0]")
        if word_doc:
            self.useWordDocument(word_doc)
        summary = self.getField(fieldset, main_document, "summary[0]")
        if summary:
            self.useSummary(summary, False)

    @fault_tolerant
    def useRoot(self, root):
        stream = root.getSubIStream()
        ministream = guessParser(stream)
        if not ministream:
            warning("Unable to create the OLE2 mini stream parser!")
            return
        self._extract(ministream, main_document=False)

    def getField(self, fieldset, main_document, name):
        if name not in fieldset:
            return None
        # _feedAll() is needed to make sure that we get all fragments
        # eg. summary[0], summary[1], ..., summary[n]
        fieldset._feedAll()
        field = fieldset[name]
        if main_document:
            stream = field.getSubIStream()
            field = guessParser(stream)
            if not field:
                warning("Unable to create the OLE2 parser for %s!" % name)
                return None
        return field

    @fault_tolerant
    def useSummary(self, summary, is_doc_summary):
        if "os" in summary:
            self.os = summary["os"].display
        if "section[0]" not in summary:
            return
        summary = summary["section[0]"]
        for property in summary.array("property_index"):
            self.useProperty(summary, property, is_doc_summary)

    @fault_tolerant
    def useWordDocument(self, doc):
        self.comment = "Encrypted: %s" % doc["fEncrypted"].value

    @fault_tolerant
    def useProperty(self, summary, property, is_doc_summary):
        field = summary.getFieldByAddress(property["offset"].value*8)
        if not field \
        or "value" not in field:
            return
        field = field["value"]
        if not field.hasValue():
            return

        # Get value
        value = field.value
        if isinstance(value, (str, unicode)):
            value = normalizeString(value)
            if not value:
                return

        # Get property identifier
        prop_id = property["id"].value
        if is_doc_summary:
            id_to_attr = self.DOC_SUMMARY_ID_TO_ATTR
            ignore = self.IGNORE_DOC_SUMMARY
        else:
            id_to_attr = self.SUMMARY_ID_TO_ATTR
            ignore = self.IGNORE_SUMMARY
        if prop_id in ignore:
            return

        # Get Hachoir metadata key
        try:
            key = id_to_attr[prop_id]
            use_prefix = False
        except LookupError:
            key = "comment"
            use_prefix = True
        if use_prefix:
            prefix = property["id"].display
            if (prefix in ("TotalEditingTime", "LastPrinted")) \
            and (not field):
                # Ignore null time delta
                return
            value = "%s: %s" % (prefix, value)
        else:
            if (key == "last_modification") and (not field):
                # Ignore null timestamp
                return
        setattr(self, key, value)

class PcfMetadata(RootMetadata):
    PROP_TO_KEY = {
        'CHARSET_REGISTRY': 'charset',
        'COPYRIGHT': 'copyright',
        'WEIGHT_NAME': 'font_weight',
        'FOUNDRY': 'author',
        'FONT': 'title',
        '_XMBDFED_INFO': 'producer',
    }

    def extract(self, pcf):
        if "properties" in pcf:
            self.useProperties(pcf["properties"])

    def useProperties(self, properties):
        last = properties["total_str_length"]
        offset0 = last.address + last.size
        for index in properties.array("property"):
            # Search name and value
            value = properties.getFieldByAddress(offset0+index["value_offset"].value*8)
            if not value:
                continue
            value = value.value
            if not value:
                continue
            name = properties.getFieldByAddress(offset0+index["name_offset"].value*8)
            if not name:
                continue
            name = name.value
            if name not in self.PROP_TO_KEY:
                warning("Skip %s=%r" % (name, value))
                continue
            key = self.PROP_TO_KEY[name]
            setattr(self, key, value)

class SwfMetadata(RootMetadata):
    def extract(self, swf):
        self.height = swf["rect/ymax"].value # twips
        self.width = swf["rect/xmax"].value # twips
        self.format_version = "flash version %s" % swf["version"].value
        self.frame_rate = swf["frame_rate"].value
        self.comment = "Frame count: %s" % swf["frame_count"].value

registerExtractor(TorrentFile, TorrentMetadata)
registerExtractor(TrueTypeFontFile, TTF_Metadata)
registerExtractor(OLE2_File, OLE2_Metadata)
registerExtractor(PcfFile, PcfMetadata)
registerExtractor(SwfFile, SwfMetadata)


########NEW FILE########
__FILENAME__ = program
from lib.hachoir_metadata.metadata import RootMetadata, registerExtractor
from lib.hachoir_parser.program import ExeFile
from lib.hachoir_metadata.safe import fault_tolerant, getValue

class ExeMetadata(RootMetadata):
    KEY_TO_ATTR = {
        u"ProductName": "title",
        u"LegalCopyright": "copyright",
        u"LegalTrademarks": "copyright",
        u"LegalTrademarks1": "copyright",
        u"LegalTrademarks2": "copyright",
        u"CompanyName": "author",
        u"BuildDate": "creation_date",
        u"FileDescription": "title",
        u"ProductVersion": "version",
    }
    SKIP_KEY = set((u"InternalName", u"OriginalFilename", u"FileVersion", u"BuildVersion"))

    def extract(self, exe):
        if exe.isPE():
            self.extractPE(exe)
        elif exe.isNE():
            self.extractNE(exe)

    def extractNE(self, exe):
        if "ne_header" in exe:
            self.useNE_Header(exe["ne_header"])
        if "info" in exe:
            self.useNEInfo(exe["info"])

    @fault_tolerant
    def useNEInfo(self, info):
        for node in info.array("node"):
            if node["name"].value == "StringFileInfo":
                self.readVersionInfo(node["node[0]"])

    def extractPE(self, exe):
        # Read information from headers
        if "pe_header" in exe:
            self.usePE_Header(exe["pe_header"])
        if "pe_opt_header" in exe:
            self.usePE_OptHeader(exe["pe_opt_header"])

        # Use PE resource
        resource = exe.getResource()
        if resource and "version_info/node[0]" in resource:
            for node in resource.array("version_info/node[0]/node"):
                if getValue(node, "name") == "StringFileInfo" \
                and "node[0]" in node:
                    self.readVersionInfo(node["node[0]"])

    @fault_tolerant
    def useNE_Header(self, hdr):
        if hdr["is_dll"].value:
            self.format_version = u"New-style executable: Dynamic-link library (DLL)"
        elif hdr["is_win_app"].value:
            self.format_version = u"New-style executable: Windows 3.x application"
        else:
            self.format_version = u"New-style executable for Windows 3.x"

    @fault_tolerant
    def usePE_Header(self, hdr):
        self.creation_date = hdr["creation_date"].value
        self.comment = "CPU: %s" % hdr["cpu"].display
        if hdr["is_dll"].value:
            self.format_version = u"Portable Executable: Dynamic-link library (DLL)"
        else:
            self.format_version = u"Portable Executable: Windows application"

    @fault_tolerant
    def usePE_OptHeader(self, hdr):
        self.comment = "Subsystem: %s" % hdr["subsystem"].display

    def readVersionInfo(self, info):
        values = {}
        for node in info.array("node"):
            if "value" not in node or "name" not in node:
                continue
            value = node["value"].value.strip(" \0")
            if not value:
                continue
            key = node["name"].value
            values[key] = value

        if "ProductName" in values and "FileDescription" in values:
            # Make sure that FileDescription is set before ProductName
            # as title value
            self.title = values["FileDescription"]
            self.title = values["ProductName"]
            del values["FileDescription"]
            del values["ProductName"]

        for key, value in values.iteritems():
            if key in self.KEY_TO_ATTR:
                setattr(self, self.KEY_TO_ATTR[key], value)
            elif key not in self.SKIP_KEY:
                self.comment = "%s=%s" % (key, value)

registerExtractor(ExeFile, ExeMetadata)


########NEW FILE########
__FILENAME__ = dialog_ui
# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'hachoir_metadata/qt/dialog.ui'
#
# Created: Mon Jul 26 03:10:06 2010
#      by: PyQt4 UI code generator 4.7.3
#
# WARNING! All changes made in this file will be lost!

from PyQt4 import QtCore, QtGui

class Ui_Form(object):
    def setupUi(self, Form):
        Form.setObjectName("Form")
        Form.resize(441, 412)
        self.verticalLayout = QtGui.QVBoxLayout(Form)
        self.verticalLayout.setObjectName("verticalLayout")
        self.horizontalLayout_2 = QtGui.QHBoxLayout()
        self.horizontalLayout_2.setObjectName("horizontalLayout_2")
        self.open_button = QtGui.QPushButton(Form)
        self.open_button.setObjectName("open_button")
        self.horizontalLayout_2.addWidget(self.open_button)
        self.files_combo = QtGui.QComboBox(Form)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Fixed)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.files_combo.sizePolicy().hasHeightForWidth())
        self.files_combo.setSizePolicy(sizePolicy)
        self.files_combo.setObjectName("files_combo")
        self.horizontalLayout_2.addWidget(self.files_combo)
        self.verticalLayout.addLayout(self.horizontalLayout_2)
        self.metadata_table = QtGui.QTableWidget(Form)
        self.metadata_table.setAlternatingRowColors(True)
        self.metadata_table.setShowGrid(False)
        self.metadata_table.setRowCount(0)
        self.metadata_table.setColumnCount(0)
        self.metadata_table.setObjectName("metadata_table")
        self.metadata_table.setColumnCount(0)
        self.metadata_table.setRowCount(0)
        self.verticalLayout.addWidget(self.metadata_table)
        self.quit_button = QtGui.QPushButton(Form)
        self.quit_button.setObjectName("quit_button")
        self.verticalLayout.addWidget(self.quit_button)

        self.retranslateUi(Form)
        QtCore.QMetaObject.connectSlotsByName(Form)

    def retranslateUi(self, Form):
        Form.setWindowTitle(QtGui.QApplication.translate("Form", "hachoir-metadata", None, QtGui.QApplication.UnicodeUTF8))
        self.open_button.setText(QtGui.QApplication.translate("Form", "Open", None, QtGui.QApplication.UnicodeUTF8))
        self.quit_button.setText(QtGui.QApplication.translate("Form", "Quit", None, QtGui.QApplication.UnicodeUTF8))


########NEW FILE########
__FILENAME__ = register
from lib.hachoir_core.i18n import _
from lib.hachoir_core.tools import (
    humanDuration, humanBitRate,
    humanFrequency, humanBitSize, humanFilesize,
    humanDatetime)
from lib.hachoir_core.language import Language
from lib.hachoir_metadata.filter import Filter, NumberFilter, DATETIME_FILTER
from datetime import date, datetime, timedelta
from lib.hachoir_metadata.formatter import (
    humanAudioChannel, humanFrameRate, humanComprRate, humanAltitude,
    humanPixelSize, humanDPI)
from lib.hachoir_metadata.setter import (
    setDatetime, setTrackNumber, setTrackTotal, setLanguage)
from lib.hachoir_metadata.metadata_item import Data

MIN_SAMPLE_RATE = 1000              # 1 kHz
MAX_SAMPLE_RATE = 192000            # 192 kHz
MAX_NB_CHANNEL = 8                  # 8 channels
MAX_WIDTH = 20000                   # 20 000 pixels
MAX_BIT_RATE = 500 * 1024 * 1024    # 500 Mbit/s
MAX_HEIGHT = MAX_WIDTH
MAX_DPI_WIDTH = 10000
MAX_DPI_HEIGHT = MAX_DPI_WIDTH
MAX_NB_COLOR = 2 ** 24              # 16 million of color
MAX_BITS_PER_PIXEL = 256            # 256 bits/pixel
MAX_FRAME_RATE = 150                # 150 frame/sec
MAX_NB_PAGE = 20000
MAX_COMPR_RATE = 1000.0
MIN_COMPR_RATE = 0.001
MAX_TRACK = 999

DURATION_FILTER = Filter(timedelta,
    timedelta(milliseconds=1),
    timedelta(days=365))

def registerAllItems(meta):
    meta.register(Data("title", 100, _("Title"), type=unicode))
    meta.register(Data("artist", 101, _("Artist"), type=unicode))
    meta.register(Data("author", 102, _("Author"), type=unicode))
    meta.register(Data("music_composer", 103, _("Music composer"), type=unicode))

    meta.register(Data("album", 200, _("Album"), type=unicode))
    meta.register(Data("duration", 201, _("Duration"), # integer in milliseconde
        type=timedelta, text_handler=humanDuration, filter=DURATION_FILTER))
    meta.register(Data("nb_page", 202, _("Nb page"), filter=NumberFilter(1, MAX_NB_PAGE)))
    meta.register(Data("music_genre", 203, _("Music genre"), type=unicode))
    meta.register(Data("language", 204, _("Language"), conversion=setLanguage, type=Language))
    meta.register(Data("track_number", 205, _("Track number"), conversion=setTrackNumber,
        filter=NumberFilter(1, MAX_TRACK), type=(int, long)))
    meta.register(Data("track_total", 206, _("Track total"), conversion=setTrackTotal,
        filter=NumberFilter(1, MAX_TRACK), type=(int, long)))
    meta.register(Data("organization", 210, _("Organization"), type=unicode))
    meta.register(Data("version", 220, _("Version")))


    meta.register(Data("width", 301, _("Image width"), filter=NumberFilter(1, MAX_WIDTH), type=(int, long), text_handler=humanPixelSize))
    meta.register(Data("height", 302, _("Image height"), filter=NumberFilter(1, MAX_HEIGHT), type=(int, long), text_handler=humanPixelSize))
    meta.register(Data("nb_channel", 303, _("Channel"), text_handler=humanAudioChannel, filter=NumberFilter(1, MAX_NB_CHANNEL), type=(int, long)))
    meta.register(Data("sample_rate", 304, _("Sample rate"), text_handler=humanFrequency, filter=NumberFilter(MIN_SAMPLE_RATE, MAX_SAMPLE_RATE), type=(int, long, float)))
    meta.register(Data("bits_per_sample", 305, _("Bits/sample"), text_handler=humanBitSize, filter=NumberFilter(1, 64), type=(int, long)))
    meta.register(Data("image_orientation", 306, _("Image orientation")))
    meta.register(Data("nb_colors", 307, _("Number of colors"), filter=NumberFilter(1, MAX_NB_COLOR), type=(int, long)))
    meta.register(Data("bits_per_pixel", 308, _("Bits/pixel"), filter=NumberFilter(1, MAX_BITS_PER_PIXEL), type=(int, long)))
    meta.register(Data("filename", 309, _("File name"), type=unicode))
    meta.register(Data("file_size", 310, _("File size"), text_handler=humanFilesize, type=(int, long)))
    meta.register(Data("pixel_format", 311, _("Pixel format")))
    meta.register(Data("compr_size", 312, _("Compressed file size"), text_handler=humanFilesize, type=(int, long)))
    meta.register(Data("compr_rate", 313, _("Compression rate"), text_handler=humanComprRate, filter=NumberFilter(MIN_COMPR_RATE, MAX_COMPR_RATE), type=(int, long, float)))

    meta.register(Data("width_dpi", 320, _("Image DPI width"), filter=NumberFilter(1, MAX_DPI_WIDTH), type=(int, long), text_handler=humanDPI))
    meta.register(Data("height_dpi", 321, _("Image DPI height"), filter=NumberFilter(1, MAX_DPI_HEIGHT), type=(int, long), text_handler=humanDPI))

    meta.register(Data("file_attr", 400, _("File attributes")))
    meta.register(Data("file_type", 401, _("File type")))
    meta.register(Data("subtitle_author", 402, _("Subtitle author"), type=unicode))

    meta.register(Data("creation_date", 500, _("Creation date"), text_handler=humanDatetime,
        filter=DATETIME_FILTER, type=(datetime, date), conversion=setDatetime))
    meta.register(Data("last_modification", 501, _("Last modification"), text_handler=humanDatetime,
        filter=DATETIME_FILTER, type=(datetime, date), conversion=setDatetime))
    meta.register(Data("latitude", 510, _("Latitude"), type=float))
    meta.register(Data("longitude", 511, _("Longitude"), type=float))
    meta.register(Data("altitude", 511, _("Altitude"), type=float, text_handler=humanAltitude))
    meta.register(Data("location", 530, _("Location"), type=unicode))
    meta.register(Data("city", 531, _("City"), type=unicode))
    meta.register(Data("country", 532, _("Country"), type=unicode))
    meta.register(Data("charset", 540, _("Charset"), type=unicode))
    meta.register(Data("font_weight", 550, _("Font weight")))

    meta.register(Data("camera_aperture", 520, _("Camera aperture")))
    meta.register(Data("camera_focal", 521, _("Camera focal")))
    meta.register(Data("camera_exposure", 522, _("Camera exposure")))
    meta.register(Data("camera_brightness", 530, _("Camera brightness")))
    meta.register(Data("camera_model", 531, _("Camera model"), type=unicode))
    meta.register(Data("camera_manufacturer", 532, _("Camera manufacturer"), type=unicode))

    meta.register(Data("compression", 600, _("Compression")))
    meta.register(Data("copyright", 601, _("Copyright"), type=unicode))
    meta.register(Data("url", 602, _("URL"), type=unicode))
    meta.register(Data("frame_rate", 603, _("Frame rate"), text_handler=humanFrameRate,
        filter=NumberFilter(1, MAX_FRAME_RATE), type=(int, long, float)))
    meta.register(Data("bit_rate", 604, _("Bit rate"), text_handler=humanBitRate,
        filter=NumberFilter(1, MAX_BIT_RATE), type=(int, long, float)))
    meta.register(Data("aspect_ratio", 604, _("Aspect ratio"), type=(int, long, float)))

    meta.register(Data("os", 900, _("OS"), type=unicode))
    meta.register(Data("producer", 901, _("Producer"), type=unicode))
    meta.register(Data("comment", 902, _("Comment"), type=unicode))
    meta.register(Data("format_version", 950, _("Format version"), type=unicode))
    meta.register(Data("mime_type", 951, _("MIME type"), type=unicode))
    meta.register(Data("endian", 952, _("Endianness"), type=unicode))


########NEW FILE########
__FILENAME__ = riff
"""
Extract metadata from RIFF file format: AVI video and WAV sound.
"""

from lib.hachoir_metadata.metadata import Metadata, MultipleMetadata, registerExtractor
from lib.hachoir_metadata.safe import fault_tolerant, getValue
from lib.hachoir_parser.container.riff import RiffFile
from lib.hachoir_parser.video.fourcc import UNCOMPRESSED_AUDIO
from lib.hachoir_core.tools import humanFilesize, makeUnicode, timedelta2seconds
from lib.hachoir_core.i18n import _
from lib.hachoir_metadata.audio import computeComprRate as computeAudioComprRate
from datetime import timedelta

class RiffMetadata(MultipleMetadata):
    TAG_TO_KEY = {
        "INAM": "title",
        "IART": "artist",
        "ICMT": "comment",
        "ICOP": "copyright",
        "IENG": "author",    # (engineer)
        "ISFT": "producer",
        "ICRD": "creation_date",
        "IDIT": "creation_date",
    }

    def extract(self, riff):
        type = riff["type"].value
        if type == "WAVE":
            self.extractWAVE(riff)
            size = getValue(riff, "audio_data/size")
            if size:
                computeAudioComprRate(self, size*8)
        elif type == "AVI ":
            if "headers" in riff:
                self.extractAVI(riff["headers"])
                self.extractInfo(riff["headers"])
        elif type == "ACON":
            self.extractAnim(riff)
        if "info" in riff:
            self.extractInfo(riff["info"])

    def processChunk(self, chunk):
        if "text" not in chunk:
            return
        value = chunk["text"].value
        tag = chunk["tag"].value
        if tag not in self.TAG_TO_KEY:
            self.warning("Skip RIFF metadata %s: %s" % (tag, value))
            return
        key = self.TAG_TO_KEY[tag]
        setattr(self, key, value)

    @fault_tolerant
    def extractWAVE(self, wav):
        format = wav["format"]

        # Number of channel, bits/sample, sample rate
        self.nb_channel = format["nb_channel"].value
        self.bits_per_sample = format["bit_per_sample"].value
        self.sample_rate = format["sample_per_sec"].value

        self.compression = format["codec"].display
        if "nb_sample/nb_sample" in wav \
        and 0 < format["sample_per_sec"].value:
            self.duration = timedelta(seconds=float(wav["nb_sample/nb_sample"].value) / format["sample_per_sec"].value)
        if format["codec"].value in UNCOMPRESSED_AUDIO:
            # Codec with fixed bit rate
            self.bit_rate = format["nb_channel"].value * format["bit_per_sample"].value * format["sample_per_sec"].value
            if not self.has("duration") \
            and "audio_data/size" in wav \
            and self.has("bit_rate"):
                duration = float(wav["audio_data/size"].value)*8 / self.get('bit_rate')
                self.duration = timedelta(seconds=duration)

    def extractInfo(self, fieldset):
        for field in fieldset:
            if not field.is_field_set:
                continue
            if "tag" in field:
                if field["tag"].value == "LIST":
                    self.extractInfo(field)
                else:
                    self.processChunk(field)

    @fault_tolerant
    def extractAVIVideo(self, header, meta):
        meta.compression = "%s (fourcc:\"%s\")" \
            % (header["fourcc"].display, makeUnicode(header["fourcc"].value))
        if header["rate"].value and header["scale"].value:
            fps = float(header["rate"].value) / header["scale"].value
            meta.frame_rate = fps
            if 0 < fps:
                self.duration = meta.duration = timedelta(seconds=float(header["length"].value) / fps)

        if "../stream_fmt/width" in header:
            format = header["../stream_fmt"]
            meta.width = format["width"].value
            meta.height = format["height"].value
            meta.bits_per_pixel = format["depth"].value
        else:
            meta.width = header["right"].value - header["left"].value
            meta.height = header["bottom"].value - header["top"].value

    @fault_tolerant
    def extractAVIAudio(self, format, meta):
        meta.nb_channel = format["channel"].value
        meta.sample_rate = format["sample_rate"].value
        meta.bit_rate = format["bit_rate"].value * 8
        if format["bits_per_sample"].value:
            meta.bits_per_sample = format["bits_per_sample"].value
        if "../stream_hdr" in format:
            header = format["../stream_hdr"]
            if header["rate"].value and header["scale"].value:
                frame_rate = float(header["rate"].value) / header["scale"].value
                meta.duration = timedelta(seconds=float(header["length"].value) / frame_rate)
            if header["fourcc"].value != "":
                meta.compression = "%s (fourcc:\"%s\")" \
                    % (format["codec"].display, header["fourcc"].value)
        if not meta.has("compression"):
            meta.compression = format["codec"].display

        self.computeAudioComprRate(meta)

    @fault_tolerant
    def computeAudioComprRate(self, meta):
        uncompr = meta.get('bit_rate', 0)
        if not uncompr:
            return
        compr = meta.get('nb_channel') * meta.get('sample_rate') * meta.get('bits_per_sample', default=16)
        if not compr:
            return
        meta.compr_rate = float(compr) / uncompr

    @fault_tolerant
    def useAviHeader(self, header):
        microsec = header["microsec_per_frame"].value
        if microsec:
            self.frame_rate = 1000000.0 / microsec
            total_frame = getValue(header, "total_frame")
            if total_frame and not self.has("duration"):
                self.duration = timedelta(microseconds=total_frame * microsec)
        self.width = header["width"].value
        self.height = header["height"].value

    def extractAVI(self, headers):
        audio_index = 1
        for stream in headers.array("stream"):
            if "stream_hdr/stream_type" not in stream:
                continue
            stream_type = stream["stream_hdr/stream_type"].value
            if stream_type == "vids":
                if "stream_hdr" in stream:
                    meta = Metadata(self)
                    self.extractAVIVideo(stream["stream_hdr"], meta)
                    self.addGroup("video", meta, "Video stream")
            elif stream_type == "auds":
                if "stream_fmt" in stream:
                    meta = Metadata(self)
                    self.extractAVIAudio(stream["stream_fmt"], meta)
                    self.addGroup("audio[%u]" % audio_index, meta, "Audio stream")
                    audio_index += 1
        if "avi_hdr" in headers:
            self.useAviHeader(headers["avi_hdr"])

        # Compute global bit rate
        if self.has("duration") and "/movie/size" in headers:
            self.bit_rate = float(headers["/movie/size"].value) * 8 / timedelta2seconds(self.get('duration'))

        # Video has index?
        if "/index" in headers:
            self.comment = _("Has audio/video index (%s)") \
                % humanFilesize(headers["/index"].size/8)

    @fault_tolerant
    def extractAnim(self, riff):
        if "anim_rate/rate[0]" in riff:
            count = 0
            total = 0
            for rate in riff.array("anim_rate/rate"):
                count += 1
                if 100 < count:
                    break
                total += rate.value / 60.0
            if count and total:
                self.frame_rate = count / total
        if not self.has("frame_rate") and "anim_hdr/jiffie_rate" in riff:
            self.frame_rate = 60.0 / riff["anim_hdr/jiffie_rate"].value

registerExtractor(RiffFile, RiffMetadata)


########NEW FILE########
__FILENAME__ = safe
from lib.hachoir_core.error import HACHOIR_ERRORS, warning

def fault_tolerant(func, *args):
    def safe_func(*args, **kw):
        try:
            func(*args, **kw)
        except HACHOIR_ERRORS, err:
            warning("Error when calling function %s(): %s" % (
                func.__name__, err))
    return safe_func

def getFieldAttribute(fieldset, key, attrname):
    try:
        field = fieldset[key]
        if field.hasValue():
            return getattr(field, attrname)
    except HACHOIR_ERRORS, err:
        warning("Unable to get %s of field %s/%s: %s" % (
            attrname, fieldset.path, key, err))
    return None

def getValue(fieldset, key):
    return getFieldAttribute(fieldset, key, "value")

def getDisplay(fieldset, key):
    return getFieldAttribute(fieldset, key, "display")


########NEW FILE########
__FILENAME__ = setter
from datetime import date, datetime
import re
from lib.hachoir_core.language import Language
from locale import setlocale, LC_ALL
from time import strptime
from lib.hachoir_metadata.timezone import createTimezone
from lib.hachoir_metadata import config

NORMALIZE_REGEX = re.compile("[-/.: ]+")
YEAR_REGEX1 = re.compile("^([0-9]{4})$")

# Date regex: YYYY-MM-DD (US format)
DATE_REGEX1 = re.compile("^([0-9]{4})~([01][0-9])~([0-9]{2})$")

# Date regex: YYYY-MM-DD HH:MM:SS (US format)
DATETIME_REGEX1 = re.compile("^([0-9]{4})~([01][0-9])~([0-9]{2})~([0-9]{1,2})~([0-9]{2})~([0-9]{2})$")

# Datetime regex: "MM-DD-YYYY HH:MM:SS" (FR format)
DATETIME_REGEX2 = re.compile("^([01]?[0-9])~([0-9]{2})~([0-9]{4})~([0-9]{1,2})~([0-9]{2})~([0-9]{2})$")

# Timezone regex: "(...) +0200"
TIMEZONE_REGEX = re.compile("^(.*)~([+-][0-9]{2})00$")

# Timestmap: 'February 2007'
MONTH_YEAR = "%B~%Y"

# Timestmap: 'Sun Feb 24 15:51:09 2008'
RIFF_TIMESTAMP = "%a~%b~%d~%H~%M~%S~%Y"

# Timestmap: 'Thu, 19 Jul 2007 09:03:57'
ISO_TIMESTAMP = "%a,~%d~%b~%Y~%H~%M~%S"

def parseDatetime(value):
    """
    Year and date:
    >>> parseDatetime("2000")
    (datetime.date(2000, 1, 1), u'2000')
    >>> parseDatetime("2004-01-02")
    datetime.date(2004, 1, 2)

    Timestamp:
    >>> parseDatetime("2004-01-02 18:10:45")
    datetime.datetime(2004, 1, 2, 18, 10, 45)
    >>> parseDatetime("2004-01-02 18:10:45")
    datetime.datetime(2004, 1, 2, 18, 10, 45)

    Timestamp with timezone:
    >>> parseDatetime(u'Thu, 19 Jul 2007 09:03:57 +0000')
    datetime.datetime(2007, 7, 19, 9, 3, 57, tzinfo=<TimezoneUTC delta=0, name=u'UTC'>)
    >>> parseDatetime(u'Thu, 19 Jul 2007 09:03:57 +0200')
    datetime.datetime(2007, 7, 19, 9, 3, 57, tzinfo=<Timezone delta=2:00:00, name='+0200'>)
    """
    value = NORMALIZE_REGEX.sub("~", value.strip())
    regs = YEAR_REGEX1.match(value)
    if regs:
        try:
            year = int(regs.group(1))
            return (date(year, 1, 1), unicode(year))
        except ValueError:
            pass
    regs = DATE_REGEX1.match(value)
    if regs:
        try:
            year = int(regs.group(1))
            month = int(regs.group(2))
            day = int(regs.group(3))
            return date(year, month, day)
        except ValueError:
            pass
    regs = DATETIME_REGEX1.match(value)
    if regs:
        try:
            year = int(regs.group(1))
            month = int(regs.group(2))
            day = int(regs.group(3))
            hour = int(regs.group(4))
            min = int(regs.group(5))
            sec = int(regs.group(6))
            return datetime(year, month, day, hour, min, sec)
        except ValueError:
            pass
    regs = DATETIME_REGEX2.match(value)
    if regs:
        try:
            month = int(regs.group(1))
            day = int(regs.group(2))
            year = int(regs.group(3))
            hour = int(regs.group(4))
            min = int(regs.group(5))
            sec = int(regs.group(6))
            return datetime(year, month, day, hour, min, sec)
        except ValueError:
            pass
    current_locale = setlocale(LC_ALL, "C")
    try:
        match = TIMEZONE_REGEX.match(value)
        if match:
            without_timezone = match.group(1)
            delta = int(match.group(2))
            delta = createTimezone(delta)
        else:
            without_timezone = value
            delta = None
        try:
            timestamp = strptime(without_timezone, ISO_TIMESTAMP)
            arguments = list(timestamp[0:6]) + [0, delta]
            return datetime(*arguments)
        except ValueError:
            pass

        try:
            timestamp = strptime(without_timezone, RIFF_TIMESTAMP)
            arguments = list(timestamp[0:6]) + [0, delta]
            return datetime(*arguments)
        except ValueError:
            pass

        try:
            timestamp = strptime(value, MONTH_YEAR)
            arguments = list(timestamp[0:3])
            return date(*arguments)
        except ValueError:
            pass
    finally:
        setlocale(LC_ALL, current_locale)
    return None

def setDatetime(meta, key, value):
    if isinstance(value, (str, unicode)):
        return parseDatetime(value)
    elif isinstance(value, (date, datetime)):
        return value
    return None

def setLanguage(meta, key, value):
    """
    >>> setLanguage(None, None, "fre")
    <Language 'French', code='fre'>
    >>> setLanguage(None, None, u"ger")
    <Language 'German', code='ger'>
    """
    return Language(value)

def setTrackTotal(meta, key, total):
    """
    >>> setTrackTotal(None, None, "10")
    10
    """
    try:
        return int(total)
    except ValueError:
        meta.warning("Invalid track total: %r" % total)
        return None

def setTrackNumber(meta, key, number):
    if isinstance(number, (int, long)):
        return number
    if "/" in number:
        number, total = number.split("/", 1)
        meta.track_total = total
    try:
        return int(number)
    except ValueError:
        meta.warning("Invalid track number: %r" % number)
        return None

def normalizeString(text):
    if config.RAW_OUTPUT:
        return text
    return text.strip(" \t\v\n\r\0")


########NEW FILE########
__FILENAME__ = timezone
from datetime import tzinfo, timedelta

class TimezoneUTC(tzinfo):
    """UTC timezone"""
    ZERO = timedelta(0)

    def utcoffset(self, dt):
        return TimezoneUTC.ZERO

    def tzname(self, dt):
        return u"UTC"

    def dst(self, dt):
        return TimezoneUTC.ZERO

    def __repr__(self):
        return "<TimezoneUTC delta=0, name=u'UTC'>"

class Timezone(TimezoneUTC):
    """Fixed offset in hour from UTC."""
    def __init__(self, offset):
        self._offset = timedelta(minutes=offset*60)
        self._name = u"%+03u00" % offset

    def utcoffset(self, dt):
        return self._offset

    def tzname(self, dt):
        return self._name

    def __repr__(self):
        return "<Timezone delta=%s, name='%s'>" % (
            self._offset, self._name)

UTC = TimezoneUTC()

def createTimezone(offset):
    if offset:
        return Timezone(offset)
    else:
        return UTC


########NEW FILE########
__FILENAME__ = version
PACKAGE = "hachoir-metadata"
VERSION = "1.3.3"
WEBSITE = "http://bitbucket.org/haypo/hachoir/wiki/hachoir-metadata"
LICENSE = "GNU GPL v2"


########NEW FILE########
__FILENAME__ = video
from lib.hachoir_core.field import MissingField
from lib.hachoir_metadata.metadata import (registerExtractor,
    Metadata, RootMetadata, MultipleMetadata)
from lib.hachoir_metadata.metadata_item import QUALITY_GOOD
from lib.hachoir_metadata.safe import fault_tolerant
from lib.hachoir_parser.video import MovFile, AsfFile, FlvFile
from lib.hachoir_parser.video.asf import Descriptor as ASF_Descriptor
from lib.hachoir_parser.container import MkvFile
from lib.hachoir_parser.container.mkv import dateToDatetime
from lib.hachoir_core.i18n import _
from lib.hachoir_core.tools import makeUnicode, makePrintable, timedelta2seconds
from datetime import timedelta

class MkvMetadata(MultipleMetadata):
    tag_key = {
        "TITLE": "title",
        "URL": "url",
        "COPYRIGHT": "copyright",

        # TODO: use maybe another name?
        # Its value may be different than (...)/Info/DateUTC/date
        "DATE_RECORDED": "creation_date",

        # TODO: Extract subtitle metadata
        "SUBTITLE": "subtitle_author",
    }

    def extract(self, mkv):
        for segment in mkv.array("Segment"):
            self.processSegment(segment)

    def processSegment(self, segment):
        for field in segment:
            if field.name.startswith("Info["):
                self.processInfo(field)
            elif field.name.startswith("Tags["):
                for tag in field.array("Tag"):
                    self.processTag(tag)
            elif field.name.startswith("Tracks["):
                self.processTracks(field)
            elif field.name.startswith("Cluster["):
                if self.quality < QUALITY_GOOD:
                    return

    def processTracks(self, tracks):
        for entry in tracks.array("TrackEntry"):
            self.processTrack(entry)

    def processTrack(self, track):
        if "TrackType/enum" not in track:
            return
        if track["TrackType/enum"].display == "video":
            self.processVideo(track)
        elif track["TrackType/enum"].display == "audio":
            self.processAudio(track)
        elif track["TrackType/enum"].display == "subtitle":
            self.processSubtitle(track)

    def trackCommon(self, track, meta):
        if "Name/unicode" in track:
            meta.title = track["Name/unicode"].value
        if "Language/string" in track \
        and track["Language/string"].value not in ("mis", "und"):
            meta.language = track["Language/string"].value

    def processVideo(self, track):
        video = Metadata(self)
        self.trackCommon(track, video)
        try:
            video.compression = track["CodecID/string"].value
            if "Video" in track:
                video.width = track["Video/PixelWidth/unsigned"].value
                video.height = track["Video/PixelHeight/unsigned"].value
        except MissingField:
            pass
        self.addGroup("video[]", video, "Video stream")

    def getDouble(self, field, parent):
        float_key = '%s/float' % parent
        if float_key in field:
            return field[float_key].value
        double_key = '%s/double' % parent
        if double_key in field:
            return field[double_key].value
        return None

    def processAudio(self, track):
        audio = Metadata(self)
        self.trackCommon(track, audio)
        if "Audio" in track:
            frequency = self.getDouble(track, "Audio/SamplingFrequency")
            if frequency is not None:
                audio.sample_rate = frequency
            if "Audio/Channels/unsigned" in track:
                audio.nb_channel = track["Audio/Channels/unsigned"].value
            if "Audio/BitDepth/unsigned" in track:
                audio.bits_per_sample = track["Audio/BitDepth/unsigned"].value
        if "CodecID/string" in track:
            audio.compression = track["CodecID/string"].value
        self.addGroup("audio[]", audio, "Audio stream")

    def processSubtitle(self, track):
        sub = Metadata(self)
        self.trackCommon(track, sub)
        try:
            sub.compression = track["CodecID/string"].value
        except MissingField:
            pass
        self.addGroup("subtitle[]", sub, "Subtitle")

    def processTag(self, tag):
        for field in tag.array("SimpleTag"):
            self.processSimpleTag(field)

    def processSimpleTag(self, tag):
        if "TagName/unicode" not in tag \
        or "TagString/unicode" not in tag:
            return
        name = tag["TagName/unicode"].value
        if name not in self.tag_key:
            return
        key = self.tag_key[name]
        value = tag["TagString/unicode"].value
        setattr(self, key, value)

    def processInfo(self, info):
        if "TimecodeScale/unsigned" in info:
            duration = self.getDouble(info, "Duration")
            if duration is not None:
                try:
                    seconds = duration * info["TimecodeScale/unsigned"].value * 1e-9
                    self.duration = timedelta(seconds=seconds)
                except OverflowError:
                    # Catch OverflowError for timedelta (long int too large
                    # to be converted to an int)
                    pass
        if "DateUTC/date" in info:
            try:
                self.creation_date = dateToDatetime(info["DateUTC/date"].value)
            except OverflowError:
                pass
        if "WritingApp/unicode" in info:
            self.producer = info["WritingApp/unicode"].value
        if "MuxingApp/unicode" in info:
            self.producer = info["MuxingApp/unicode"].value
        if "Title/unicode" in info:
            self.title = info["Title/unicode"].value

class FlvMetadata(MultipleMetadata):
    def extract(self, flv):
        if "video[0]" in flv:
            meta = Metadata(self)
            self.extractVideo(flv["video[0]"], meta)
            self.addGroup("video", meta, "Video stream")
        if "audio[0]" in flv:
            meta = Metadata(self)
            self.extractAudio(flv["audio[0]"], meta)
            self.addGroup("audio", meta, "Audio stream")
        # TODO: Computer duration
        # One technic: use last video/audio chunk and use timestamp
        # But this is very slow
        self.format_version = flv.description

        if "metadata/entry[1]" in flv:
            self.extractAMF(flv["metadata/entry[1]"])
        if self.has('duration'):
            self.bit_rate = flv.size / timedelta2seconds(self.get('duration'))

    @fault_tolerant
    def extractAudio(self, audio, meta):
        if audio["codec"].display == "MP3" and "music_data" in audio:
            meta.compression = audio["music_data"].description
        else:
            meta.compression = audio["codec"].display
        meta.sample_rate = audio.getSampleRate()
        if audio["is_16bit"].value:
            meta.bits_per_sample = 16
        else:
            meta.bits_per_sample = 8
        if audio["is_stereo"].value:
            meta.nb_channel = 2
        else:
            meta.nb_channel = 1

    @fault_tolerant
    def extractVideo(self, video, meta):
        meta.compression = video["codec"].display

    def extractAMF(self, amf):
        for entry in amf.array("item"):
            self.useAmfEntry(entry)

    @fault_tolerant
    def useAmfEntry(self, entry):
        key = entry["key"].value
        if key == "duration":
            self.duration = timedelta(seconds=entry["value"].value)
        elif key == "creator":
            self.producer = entry["value"].value
        elif key == "audiosamplerate":
            self.sample_rate = entry["value"].value
        elif key == "framerate":
            self.frame_rate = entry["value"].value
        elif key == "metadatacreator":
            self.producer = entry["value"].value
        elif key == "metadatadate":
            self.creation_date = entry.value
        elif key == "width":
            self.width = int(entry["value"].value)
        elif key == "height":
            self.height = int(entry["value"].value)

class MovMetadata(RootMetadata):
    def extract(self, mov):
        for atom in mov:
            if "movie" in atom:
                self.processMovie(atom["movie"])

    @fault_tolerant
    def processMovieHeader(self, hdr):
        self.creation_date = hdr["creation_date"].value
        self.last_modification = hdr["lastmod_date"].value
        self.duration = timedelta(seconds=float(hdr["duration"].value) / hdr["time_scale"].value)
        self.comment = _("Play speed: %.1f%%") % (hdr["play_speed"].value*100)
        self.comment = _("User volume: %.1f%%") % (float(hdr["volume"].value)*100//255)

    @fault_tolerant
    def processTrackHeader(self, hdr):
        width = int(hdr["frame_size_width"].value)
        height = int(hdr["frame_size_height"].value)
        if width and height:
            self.width = width
            self.height = height

    def processTrack(self, atom):
        for field in atom:
            if "track_hdr" in field:
                self.processTrackHeader(field["track_hdr"])

    def processMovie(self, atom):
        for field in atom:
            if "track" in field:
                self.processTrack(field["track"])
            if "movie_hdr" in field:
                self.processMovieHeader(field["movie_hdr"])


class AsfMetadata(MultipleMetadata):
    EXT_DESC_TO_ATTR = {
        "Encoder": "producer",
        "ToolName": "producer",
        "AlbumTitle": "album",
        "Track": "track_number",
        "TrackNumber": "track_total",
        "Year": "creation_date",
        "AlbumArtist": "author",
    }
    SKIP_EXT_DESC = set((
        # Useless informations
        "WMFSDKNeeded", "WMFSDKVersion",
        "Buffer Average", "VBR Peak", "EncodingTime",
        "MediaPrimaryClassID", "UniqueFileIdentifier",
    ))

    def extract(self, asf):
        if "header/content" in asf:
            self.processHeader(asf["header/content"])

    def processHeader(self, header):
        compression = []
        is_vbr = None

        if "ext_desc/content" in header:
            # Extract all data from ext_desc
            data = {}
            for desc in header.array("ext_desc/content/descriptor"):
                self.useExtDescItem(desc, data)

            # Have ToolName and ToolVersion? If yes, group them to producer key
            if "ToolName" in data and "ToolVersion" in data:
                self.producer = "%s (version %s)" % (data["ToolName"], data["ToolVersion"])
                del data["ToolName"]
                del data["ToolVersion"]

            # "IsVBR" key
            if "IsVBR" in data:
                is_vbr = (data["IsVBR"] == 1)
                del data["IsVBR"]

            # Store data
            for key, value in data.iteritems():
                if key in self.EXT_DESC_TO_ATTR:
                    key = self.EXT_DESC_TO_ATTR[key]
                else:
                    if isinstance(key, str):
                        key = makePrintable(key, "ISO-8859-1", to_unicode=True)
                    value = "%s=%s" % (key, value)
                    key = "comment"
                setattr(self, key, value)

        if "file_prop/content" in header:
            self.useFileProp(header["file_prop/content"], is_vbr)

        if "codec_list/content" in header:
            for codec in header.array("codec_list/content/codec"):
                if "name" in codec:
                    text = codec["name"].value
                    if "desc" in codec and codec["desc"].value:
                        text = "%s (%s)" % (text, codec["desc"].value)
                    compression.append(text)

        audio_index = 1
        video_index = 1
        for index, stream_prop in enumerate(header.array("stream_prop")):
            if "content/audio_header" in stream_prop:
                meta = Metadata(self)
                self.streamProperty(header, index, meta)
                self.streamAudioHeader(stream_prop["content/audio_header"], meta)
                if self.addGroup("audio[%u]" % audio_index, meta, "Audio stream #%u" % audio_index):
                    audio_index += 1
            elif "content/video_header" in stream_prop:
                meta = Metadata(self)
                self.streamProperty(header, index, meta)
                self.streamVideoHeader(stream_prop["content/video_header"], meta)
                if self.addGroup("video[%u]" % video_index, meta, "Video stream #%u" % video_index):
                    video_index += 1

        if "metadata/content" in header:
            info = header["metadata/content"]
            try:
                self.title = info["title"].value
                self.author = info["author"].value
                self.copyright = info["copyright"].value
            except MissingField:
                pass

    @fault_tolerant
    def streamAudioHeader(self, audio, meta):
        if not meta.has("compression"):
            meta.compression = audio["twocc"].display
        meta.nb_channel = audio["channels"].value
        meta.sample_rate = audio["sample_rate"].value
        meta.bits_per_sample = audio["bits_per_sample"].value

    @fault_tolerant
    def streamVideoHeader(self, video, meta):
        meta.width = video["width"].value
        meta.height = video["height"].value
        if "bmp_info" in video:
            bmp_info = video["bmp_info"]
            if not meta.has("compression"):
                meta.compression = bmp_info["codec"].display
            meta.bits_per_pixel = bmp_info["bpp"].value

    @fault_tolerant
    def useExtDescItem(self, desc, data):
        if desc["type"].value == ASF_Descriptor.TYPE_BYTE_ARRAY:
            # Skip binary data
            return
        key = desc["name"].value
        if "/" in key:
            # Replace "WM/ToolName" with "ToolName"
            key = key.split("/", 1)[1]
        if key in self.SKIP_EXT_DESC:
            # Skip some keys
            return
        value = desc["value"].value
        if not value:
            return
        value = makeUnicode(value)
        data[key] = value

    @fault_tolerant
    def useFileProp(self, prop, is_vbr):
        self.creation_date = prop["creation_date"].value
        self.duration = prop["play_duration"].value
        if prop["seekable"].value:
            self.comment = u"Is seekable"
        value = prop["max_bitrate"].value
        text = prop["max_bitrate"].display
        if is_vbr is True:
            text = "VBR (%s max)" % text
        elif is_vbr is False:
            text = "%s (CBR)" % text
        else:
            text = "%s (max)" % text
        self.bit_rate = (value, text)

    def streamProperty(self, header, index, meta):
        key = "bit_rates/content/bit_rate[%u]/avg_bitrate" % index
        if key in header:
            meta.bit_rate = header[key].value

        # TODO: Use codec list
        # It doesn't work when the video uses /header/content/bitrate_mutex
        # since the codec list are shared between streams but... how is it
        # shared?
#        key = "codec_list/content/codec[%u]" % index
#        if key in header:
#            codec = header[key]
#            if "name" in codec:
#                text = codec["name"].value
#                if "desc" in codec and codec["desc"].value:
#                    meta.compression = "%s (%s)" % (text, codec["desc"].value)
#                else:
#                    meta.compression = text

registerExtractor(MovFile, MovMetadata)
registerExtractor(AsfFile, AsfMetadata)
registerExtractor(FlvFile, FlvMetadata)
registerExtractor(MkvFile, MkvMetadata)


########NEW FILE########
__FILENAME__ = ace
"""
ACE parser

From wotsit.org and the SDK header (bitflags)

Partial study of a new block type (5) I've called "new_recovery", as its
syntax is very close to the former one (of type 2).

Status: can only read totally file and header blocks.
Author: Christophe Gisquet <christophe.gisquet@free.fr>
Creation date: 19 january 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (StaticFieldSet, FieldSet,
    Bit, Bits, NullBits, RawBytes, Enum,
    UInt8, UInt16, UInt32,
    PascalString8, PascalString16, String,
    TimeDateMSDOS32)
from lib.hachoir_core.text_handler import textHandler, filesizeHandler, hexadecimal
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_parser.common.msdos import MSDOSFileAttr32

MAGIC = "**ACE**"

OS_MSDOS = 0
OS_WIN32 = 2
HOST_OS = {
    0: "MS-DOS",
    1: "OS/2",
    2: "Win32",
    3: "Unix",
    4: "MAC-OS",
    5: "Win NT",
    6: "Primos",
    7: "APPLE GS",
    8: "ATARI",
    9: "VAX VMS",
    10: "AMIGA",
    11: "NEXT",
}

COMPRESSION_TYPE = {
    0: "Store",
    1: "Lempel-Ziv 77",
    2: "ACE v2.0",
}

COMPRESSION_MODE = {
    0: "fastest",
    1: "fast",
    2: "normal",
    3: "good",
    4: "best",
}

# TODO: Computing the CRC16 would also prove useful
#def markerValidate(self):
#    return not self["extend"].value and self["signature"].value == MAGIC and \
#           self["host_os"].value<12

class MarkerFlags(StaticFieldSet):
    format = (
        (Bit, "extend", "Whether the header is extended"),
        (Bit, "has_comment", "Whether the archive has a comment"),
        (NullBits, "unused", 7, "Reserved bits"),
        (Bit, "sfx", "SFX"),
        (Bit, "limited_dict", "Junior SFX with 256K dictionary"),
        (Bit, "multi_volume", "Part of a set of ACE archives"),
        (Bit, "has_av_string", "This header holds an AV-string"),
        (Bit, "recovery_record", "Recovery record preset"),
        (Bit, "locked", "Archive is locked"),
        (Bit, "solid", "Archive uses solid compression")
    )

def markerFlags(self):
    yield MarkerFlags(self, "flags", "Marker flags")

def markerHeader(self):
    yield String(self, "signature", 7, "Signature")
    yield UInt8(self, "ver_extract", "Version needed to extract archive")
    yield UInt8(self, "ver_created", "Version used to create archive")
    yield Enum(UInt8(self, "host_os", "OS where the files were compressed"), HOST_OS)
    yield UInt8(self, "vol_num", "Volume number")
    yield TimeDateMSDOS32(self, "time", "Date and time (MS DOS format)")
    yield Bits(self, "reserved", 64, "Reserved size for future extensions")
    flags = self["flags"]
    if flags["has_av_string"].value:
        yield PascalString8(self, "av_string", "AV String")
    if flags["has_comment"].value:
        size = filesizeHandler(UInt16(self, "comment_size", "Comment size"))
        yield size
        if size.value > 0:
            yield RawBytes(self, "compressed_comment", size.value, \
                           "Compressed comment")

class FileFlags(StaticFieldSet):
    format = (
        (Bit, "extend", "Whether the header is extended"),
        (Bit, "has_comment", "Presence of file comment"),
        (Bits, "unused", 10, "Unused bit flags"),
        (Bit, "encrypted", "File encrypted with password"),
        (Bit, "previous", "File continued from previous volume"),
        (Bit, "next", "File continues on the next volume"),
        (Bit, "solid", "File compressed using previously archived files")
    )

def fileFlags(self):
    yield FileFlags(self, "flags", "File flags")

def fileHeader(self):
    yield filesizeHandler(UInt32(self, "compressed_size", "Size of the compressed file"))
    yield filesizeHandler(UInt32(self, "uncompressed_size", "Uncompressed file size"))
    yield TimeDateMSDOS32(self, "ftime", "Date and time (MS DOS format)")
    if self["/header/host_os"].value in (OS_MSDOS, OS_WIN32):
        yield MSDOSFileAttr32(self, "file_attr", "File attributes")
    else:
        yield textHandler(UInt32(self, "file_attr", "File attributes"), hexadecimal)
    yield textHandler(UInt32(self, "file_crc32", "CRC32 checksum over the compressed file)"), hexadecimal)
    yield Enum(UInt8(self, "compression_type", "Type of compression"), COMPRESSION_TYPE)
    yield Enum(UInt8(self, "compression_mode", "Quality of compression"), COMPRESSION_MODE)
    yield textHandler(UInt16(self, "parameters", "Compression parameters"), hexadecimal)
    yield textHandler(UInt16(self, "reserved", "Reserved data"), hexadecimal)
    # Filename
    yield PascalString16(self, "filename", "Filename")
    # Comment
    if self["flags/has_comment"].value:
        yield filesizeHandler(UInt16(self, "comment_size", "Size of the compressed comment"))
        if self["comment_size"].value > 0:
            yield RawBytes(self, "comment_data", self["comment_size"].value, "Comment data")

def fileBody(self):
    size = self["compressed_size"].value
    if size > 0:
        yield RawBytes(self, "compressed_data", size, "Compressed data")

def fileDesc(self):
    return "File entry: %s (%s)" % (self["filename"].value, self["compressed_size"].display)

def recoveryHeader(self):
    yield filesizeHandler(UInt32(self, "rec_blk_size", "Size of recovery data"))
    self.body_size = self["rec_blk_size"].size
    yield String(self, "signature", 7, "Signature, normally '**ACE**'")
    yield textHandler(UInt32(self, "relative_start",
         "Relative start (to this block) of the data this block is mode of"),
         hexadecimal)
    yield UInt32(self, "num_blocks", "Number of blocks the data is split into")
    yield UInt32(self, "size_blocks", "Size of these blocks")
    yield UInt16(self, "crc16_blocks", "CRC16 over recovery data")
    # size_blocks blocks of size size_blocks follow
    # The ultimate data is the xor data of all those blocks
    size = self["size_blocks"].value
    for index in xrange(self["num_blocks"].value):
        yield RawBytes(self, "data[]", size, "Recovery block %i" % index)
    yield RawBytes(self, "xor_data", size, "The XOR value of the above data blocks")

def recoveryDesc(self):
    return "Recovery block, size=%u" % self["body_size"].display

def newRecoveryHeader(self):
    """
    This header is described nowhere
    """
    if self["flags/extend"].value:
        yield filesizeHandler(UInt32(self, "body_size", "Size of the unknown body following"))
        self.body_size = self["body_size"].value
    yield textHandler(UInt32(self, "unknown[]", "Unknown field, probably 0"),
        hexadecimal)
    yield String(self, "signature", 7, "Signature, normally '**ACE**'")
    yield textHandler(UInt32(self, "relative_start",
        "Offset (=crc16's) of this block in the file"), hexadecimal)
    yield textHandler(UInt32(self, "unknown[]",
        "Unknown field, probably 0"), hexadecimal)

class BaseFlags(StaticFieldSet):
    format = (
        (Bit, "extend", "Whether the header is extended"),
        (NullBits, "unused", 15, "Unused bit flags")
    )

def parseFlags(self):
    yield BaseFlags(self, "flags", "Unknown flags")

def parseHeader(self):
    if self["flags/extend"].value:
        yield filesizeHandler(UInt32(self, "body_size", "Size of the unknown body following"))
        self.body_size = self["body_size"].value

def parseBody(self):
    if self.body_size > 0:
        yield RawBytes(self, "body_data", self.body_size, "Body data, unhandled")

class Block(FieldSet):
    TAG_INFO = {
        0: ("header", "Archiver header", markerFlags, markerHeader, None),
        1: ("file[]", fileDesc, fileFlags, fileHeader, fileBody),
        2: ("recovery[]", recoveryDesc, recoveryHeader, None, None),
        5: ("new_recovery[]", None, None, newRecoveryHeader, None)
    }

    def __init__(self, parent, name, description=None):
        FieldSet.__init__(self, parent, name, description)
        self.body_size = 0
        self.desc_func = None
        type = self["block_type"].value
        if type in self.TAG_INFO:
            self._name, desc, self.parseFlags, self.parseHeader, self.parseBody = self.TAG_INFO[type]
            if desc:
                if isinstance(desc, str):
                    self._description = desc
                else:
                    self.desc_func = desc
        else:
            self.warning("Processing as unknown block block of type %u" % type)
        if not self.parseFlags:
            self.parseFlags = parseFlags
        if not self.parseHeader:
            self.parseHeader = parseHeader
        if not self.parseBody:
            self.parseBody = parseBody

    def createFields(self):
        yield textHandler(UInt16(self, "crc16", "Archive CRC16 (from byte 4 on)"), hexadecimal)
        yield filesizeHandler(UInt16(self, "head_size", "Block size (from byte 4 on)"))
        yield UInt8(self, "block_type", "Block type")

        # Flags
        for flag in self.parseFlags(self):
            yield flag

        # Rest of the header
        for field in self.parseHeader(self):
            yield field
        size = self["head_size"].value - (self.current_size//8) + (2+2)
        if size > 0:
            yield RawBytes(self, "extra_data", size, "Extra header data, unhandled")

        # Body in itself
        for field in self.parseBody(self):
            yield field

    def createDescription(self):
        if self.desc_func:
            return self.desc_func(self)
        else:
            return "Block: %s" % self["type"].display

class AceFile(Parser):
    endian = LITTLE_ENDIAN
    PARSER_TAGS = {
        "id": "ace",
        "category": "archive",
        "file_ext": ("ace",),
        "mime": (u"application/x-ace-compressed",),
        "min_size": 50*8,
        "description": "ACE archive"
    }

    def validate(self):
        if self.stream.readBytes(7*8, len(MAGIC)) != MAGIC:
            return "Invalid magic"
        return True

    def createFields(self):
        while not self.eof:
            yield Block(self, "block[]")


########NEW FILE########
__FILENAME__ = ar
"""
GNU ar archive : archive file (.a) and Debian (.deb) archive.
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    String, RawBytes, UnixLine)
from lib.hachoir_core.endian import BIG_ENDIAN

class ArchiveFileEntry(FieldSet):
    def createFields(self):
        yield UnixLine(self, "header", "Header")
        info = self["header"].value.split()
        if len(info) != 7:
            raise ParserError("Invalid file entry header")
        size = int(info[5])
        if 0 < size:
            yield RawBytes(self, "content", size, "File data")

    def createDescription(self):
        return "File entry (%s)" % self["header"].value.split()[0]

class ArchiveFile(Parser):
    endian = BIG_ENDIAN
    MAGIC = '!<arch>\n'
    PARSER_TAGS = {
        "id": "unix_archive",
        "category": "archive",
        "file_ext": ("a", "deb"),
        "mime":
            (u"application/x-debian-package",
             u"application/x-archive",
             u"application/x-dpkg"),
        "min_size": (8 + 13)*8, # file signature + smallest file as possible
        "magic": ((MAGIC, 0),),
        "description": "Unix archive"
    }

    def validate(self):
        if self.stream.readBytes(0, len(self.MAGIC)) != self.MAGIC:
            return "Invalid magic string"
        return True

    def createFields(self):
        yield String(self, "id", 8, "Unix archive identifier (\"<!arch>\")", charset="ASCII")
        while not self.eof:
            data = self.stream.readBytes(self.current_size, 1)
            if data == "\n":
                yield RawBytes(self, "empty_line[]", 1, "Empty line")
            else:
                yield ArchiveFileEntry(self, "file[]", "File")


########NEW FILE########
__FILENAME__ = bzip2_parser
"""
BZIP2 archive file

Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (ParserError, String,
    Bytes, Character, UInt8, UInt32, CompressedField)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal

try:
    from bz2 import BZ2Decompressor

    class Bunzip2:
        def __init__(self, stream):
            self.bzip2 = BZ2Decompressor()

        def __call__(self, size, data=''):
            try:
                return self.bzip2.decompress(data)
            except EOFError:
                return ''

    has_deflate = True
except ImportError:
    has_deflate = False

class Bzip2Parser(Parser):
    PARSER_TAGS = {
        "id": "bzip2",
        "category": "archive",
        "file_ext": ("bz2",),
        "mime": (u"application/x-bzip2",),
        "min_size": 10*8,
        "magic": (('BZh', 0),),
        "description": "bzip2 archive"
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 3) != 'BZh':
            return "Wrong file signature"
        if not("1" <= self["blocksize"].value <= "9"):
            return "Wrong blocksize"
        return True

    def createFields(self):
        yield String(self, "id", 3, "Identifier (BZh)", charset="ASCII")
        yield Character(self, "blocksize", "Block size (KB of memory needed to uncompress)")

        yield UInt8(self, "blockheader", "Block header")
        if self["blockheader"].value == 0x17:
            yield String(self, "id2", 4, "Identifier2 (re8P)", charset="ASCII")
            yield UInt8(self, "id3", "Identifier3 (0x90)")
        elif self["blockheader"].value == 0x31:
            yield String(self, "id2", 5, "Identifier 2 (AY&SY)", charset="ASCII")
            if self["id2"].value != "AY&SY":
                raise ParserError("Invalid identifier 2 (AY&SY)!")
        else:
            raise ParserError("Invalid block header!")
        yield textHandler(UInt32(self, "crc32", "CRC32"), hexadecimal)

        if self._size is None: # TODO: is it possible to handle piped input?
            raise NotImplementedError

        size = (self._size - self.current_size)/8
        if size:
            for tag, filename in self.stream.tags:
                if tag == "filename" and filename.endswith(".bz2"):
                    filename = filename[:-4]
                    break
            else:
                filename = None
            data = Bytes(self, "file", size)
            if has_deflate:
                CompressedField(self, Bunzip2)
                def createInputStream(**args):
                    if filename:
                        args.setdefault("tags",[]).append(("filename", filename))
                    return self._createInputStream(**args)
                data._createInputStream = createInputStream
            yield data


########NEW FILE########
__FILENAME__ = cab
"""
Microsoft Cabinet (CAB) archive.

Author: Victor Stinner
Creation date: 31 january 2007
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, Enum,
    CString, String,
    UInt16, UInt32, Bit, Bits, PaddingBits, NullBits,
    DateTimeMSDOS32, RawBytes)
from lib.hachoir_parser.common.msdos import MSDOSFileAttr16
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler
from lib.hachoir_core.endian import LITTLE_ENDIAN

MAX_NB_FOLDER = 30

COMPRESSION_NONE = 0
COMPRESSION_NAME = {
    0: "Uncompressed",
    1: "Deflate",
    2: "Quantum",
    3: "LZX",
}

class Folder(FieldSet):
    def createFields(self):
        yield UInt32(self, "off_data", "Offset of data")
        yield UInt16(self, "cf_data")
        yield Enum(Bits(self, "compr_method", 4, "Compression method"), COMPRESSION_NAME)
        yield Bits(self, "compr_level", 5, "Compression level")
        yield PaddingBits(self, "padding", 7)

    def createDescription(self):
        text= "Folder: compression %s" % self["compr_method"].display
        if self["compr_method"].value != COMPRESSION_NONE:
            text += " (level %u)" % self["compr_level"].value
        return text

class File(FieldSet):
    def createFields(self):
        yield filesizeHandler(UInt32(self, "filesize", "Uncompressed file size"))
        yield UInt32(self, "offset", "File offset after decompression")
        yield UInt16(self, "iFolder", "file control id")
        yield DateTimeMSDOS32(self, "timestamp")
        yield MSDOSFileAttr16(self, "attributes")
        yield CString(self, "filename", charset="ASCII")

    def createDescription(self):
        return "File %s (%s)" % (
            self["filename"].display, self["filesize"].display)

class Reserved(FieldSet):
    def createFields(self):
        yield UInt32(self, "size")
        size = self["size"].value
        if size:
            yield RawBytes(self, "data", size)

class Flags(FieldSet):
    static_size = 16
    def createFields(self):
        yield Bit(self, "has_previous")
        yield Bit(self, "has_next")
        yield Bit(self, "has_reserved")
        yield NullBits(self, "padding", 13)

class CabFile(Parser):
    endian = LITTLE_ENDIAN
    MAGIC = "MSCF"
    PARSER_TAGS = {
        "id": "cab",
        "category": "archive",
        "file_ext": ("cab",),
        "mime": (u"application/vnd.ms-cab-compressed",),
        "magic": ((MAGIC, 0),),
        "min_size": 1*8, # header + file entry
        "description": "Microsoft Cabinet archive"
    }

    def validate(self):
        if self.stream.readBytes(0, 4) != self.MAGIC:
            return "Invalid magic"
        if self["cab_version"].value != 0x0103:
            return "Unknown version (%s)" % self["cab_version"].display
        if not (1 <= self["nb_folder"].value <= MAX_NB_FOLDER):
            return "Invalid number of folder (%s)" % self["nb_folder"].value
        return True

    def createFields(self):
        yield String(self, "magic", 4, "Magic (MSCF)", charset="ASCII")
        yield textHandler(UInt32(self, "hdr_checksum", "Header checksum (0 if not used)"), hexadecimal)
        yield filesizeHandler(UInt32(self, "filesize", "Cabinet file size"))
        yield textHandler(UInt32(self, "fld_checksum", "Folders checksum (0 if not used)"), hexadecimal)
        yield UInt32(self, "off_file", "Offset of first file")
        yield textHandler(UInt32(self, "files_checksum", "Files checksum (0 if not used)"), hexadecimal)
        yield textHandler(UInt16(self, "cab_version", "Cabinet version"), hexadecimal)
        yield UInt16(self, "nb_folder", "Number of folders")
        yield UInt16(self, "nb_files", "Number of files")
        yield Flags(self, "flags")
        yield UInt16(self, "setid")
        yield UInt16(self, "number", "Zero-based cabinet number")

        # --- TODO: Support flags
        if self["flags/has_reserved"].value:
            yield Reserved(self, "reserved")
        #(3) Previous cabinet name, if CAB_HEADER.flags & CAB_FLAG_HASPREV
        #(4) Previous disk name, if CAB_HEADER.flags & CAB_FLAG_HASPREV
        #(5) Next cabinet name, if CAB_HEADER.flags & CAB_FLAG_HASNEXT
        #(6) Next disk name, if CAB_HEADER.flags & CAB_FLAG_HASNEXT
        # ----

        for index in xrange(self["nb_folder"].value):
            yield Folder(self, "folder[]")
        for index in xrange(self["nb_files"].value):
            yield File(self, "file[]")

        end = self.seekBit(self.size, "endraw")
        if end:
            yield end

    def createContentSize(self):
        return self["filesize"].value * 8


########NEW FILE########
__FILENAME__ = gzip_parser
"""
GZIP archive parser.

Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (
    UInt8, UInt16, UInt32, Enum, TimestampUnix32,
    Bit, CString, SubFile,
    NullBits, Bytes, RawBytes)
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_parser.common.deflate import Deflate

class GzipParser(Parser):
    endian = LITTLE_ENDIAN
    PARSER_TAGS = {
        "id": "gzip",
        "category": "archive",
        "file_ext": ("gz",),
        "mime": (u"application/x-gzip",),
        "min_size": 18*8,
        #"magic": (('\x1F\x8B\x08', 0),),
        "magic_regex": (
            # (magic, compression=deflate, <flags>, <mtime>, )
            ('\x1F\x8B\x08.{5}[\0\2\4\6][\x00-\x0D]', 0),
        ),
        "description": u"gzip archive",
    }
    os_name = {
         0: u"FAT filesystem",
         1: u"Amiga",
         2: u"VMS (or OpenVMS)",
         3: u"Unix",
         4: u"VM/CMS",
         5: u"Atari TOS",
         6: u"HPFS filesystem (OS/2, NT)",
         7: u"Macintosh",
         8: u"Z-System",
         9: u"CP/M",
        10: u"TOPS-20",
        11: u"NTFS filesystem (NT)",
        12: u"QDOS",
        13: u"Acorn RISCOS",
    }
    COMPRESSION_NAME = {
        8: u"deflate",
    }

    def validate(self):
        if self["signature"].value != '\x1F\x8B':
            return "Invalid signature"
        if self["compression"].value not in self.COMPRESSION_NAME:
            return "Unknown compression method (%u)" % self["compression"].value
        if self["reserved[0]"].value != 0:
            return "Invalid reserved[0] value"
        if self["reserved[1]"].value != 0:
            return "Invalid reserved[1] value"
        if self["reserved[2]"].value != 0:
            return "Invalid reserved[2] value"
        return True

    def createFields(self):
        # Gzip header
        yield Bytes(self, "signature", 2, r"GZip file signature (\x1F\x8B)")
        yield Enum(UInt8(self, "compression", "Compression method"), self.COMPRESSION_NAME)

        # Flags
        yield Bit(self, "is_text", "File content is probably ASCII text")
        yield Bit(self, "has_crc16", "Header CRC16")
        yield Bit(self, "has_extra", "Extra informations (variable size)")
        yield Bit(self, "has_filename", "Contains filename?")
        yield Bit(self, "has_comment", "Contains comment?")
        yield NullBits(self, "reserved[]", 3)
        yield TimestampUnix32(self, "mtime", "Modification time")

        # Extra flags
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "slowest", "Compressor used maximum compression (slowest)")
        yield Bit(self, "fastest", "Compressor used the fastest compression")
        yield NullBits(self, "reserved[]", 5)
        yield Enum(UInt8(self, "os", "Operating system"), self.os_name)

        # Optional fields
        if self["has_extra"].value:
            yield UInt16(self, "extra_length", "Extra length")
            yield RawBytes(self, "extra", self["extra_length"].value, "Extra")
        if self["has_filename"].value:
            yield CString(self, "filename", "Filename", charset="ISO-8859-1")
        if self["has_comment"].value:
            yield CString(self, "comment", "Comment")
        if self["has_crc16"].value:
            yield textHandler(UInt16(self, "hdr_crc16", "CRC16 of the header"),
                hexadecimal)

        if self._size is None: # TODO: is it possible to handle piped input?
            raise NotImplementedError()

        # Read file
        size = (self._size - self.current_size) // 8 - 8  # -8: crc32+size
        if 0 < size:
            if self["has_filename"].value:
                filename = self["filename"].value
            else:
                for tag, filename in self.stream.tags:
                    if tag == "filename" and filename.endswith(".gz"):
                        filename = filename[:-3]
                        break
                else:
                    filename = None
            yield Deflate(SubFile(self, "file", size, filename=filename))

        # Footer
        yield textHandler(UInt32(self, "crc32",
            "Uncompressed data content CRC32"), hexadecimal)
        yield filesizeHandler(UInt32(self, "size", "Uncompressed size"))

    def createDescription(self):
        desc = u"gzip archive"
        info = []
        if "filename" in self:
            info.append('filename "%s"' % self["filename"].value)
        if "size" in self:
            info.append("was %s" % self["size"].display)
        if self["mtime"].value:
            info.append(self["mtime"].display)
        return "%s: %s" % (desc, ", ".join(info))


########NEW FILE########
__FILENAME__ = mar
"""
Microsoft Archive parser

Author: Victor Stinner
Creation date: 2007-03-04
"""

MAX_NB_FILE = 100000

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import FieldSet, String, UInt32, SubFile
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, filesizeHandler, hexadecimal

class FileIndex(FieldSet):
    static_size = 68*8

    def createFields(self):
        yield String(self, "filename", 56, truncate="\0", charset="ASCII")
        yield filesizeHandler(UInt32(self, "filesize"))
        yield textHandler(UInt32(self, "crc32"), hexadecimal)
        yield UInt32(self, "offset")

    def createDescription(self):
        return "File %s (%s) at %s" % (
            self["filename"].value, self["filesize"].display, self["offset"].value)

class MarFile(Parser):
    MAGIC = "MARC"
    PARSER_TAGS = {
        "id": "mar",
        "category": "archive",
        "file_ext": ("mar",),
        "min_size": 80*8,  # At least one file index
        "magic": ((MAGIC, 0),),
        "description": "Microsoft Archive",
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 4) != self.MAGIC:
            return "Invalid magic"
        if self["version"].value != 3:
            return "Invalid version"
        if not(1 <= self["nb_file"].value <= MAX_NB_FILE):
            return "Invalid number of file"
        return True

    def createFields(self):
        yield String(self, "magic", 4, "File signature (MARC)", charset="ASCII")
        yield UInt32(self, "version")
        yield UInt32(self, "nb_file")
        files = []
        for index in xrange(self["nb_file"].value):
            item = FileIndex(self, "file[]")
            yield item
            if item["filesize"].value:
                files.append(item)
        files.sort(key=lambda item: item["offset"].value)
        for index in files:
            padding = self.seekByte(index["offset"].value)
            if padding:
                yield padding
            size = index["filesize"].value
            desc = "File %s" % index["filename"].value
            yield SubFile(self, "data[]", size, desc, filename=index["filename"].value)


########NEW FILE########
__FILENAME__ = rar
"""
RAR parser

Status: can only read higher-level attructures
Author: Christophe Gisquet
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (StaticFieldSet, FieldSet,
    Bit, Bits, Enum,
    UInt8, UInt16, UInt32, UInt64,
    String, TimeDateMSDOS32,
    NullBytes, NullBits, RawBytes)
from lib.hachoir_core.text_handler import textHandler, filesizeHandler, hexadecimal
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_parser.common.msdos import MSDOSFileAttr32

MAX_FILESIZE = 1000 * 1024 * 1024

BLOCK_NAME = {
    0x72: "Marker",
    0x73: "Archive",
    0x74: "File",
    0x75: "Comment",
    0x76: "Extra info",
    0x77: "Subblock",
    0x78: "Recovery record",
    0x79: "Archive authenticity",
    0x7A: "New-format subblock",
    0x7B: "Archive end",
}

COMPRESSION_NAME = {
    0x30: "Storing",
    0x31: "Fastest compression",
    0x32: "Fast compression",
    0x33: "Normal compression",
    0x34: "Good compression",
    0x35: "Best compression"
}

OS_MSDOS = 0
OS_WIN32 = 2
OS_NAME = {
    0: "MS DOS",
    1: "OS/2",
    2: "Win32",
    3: "Unix",
}

DICTIONARY_SIZE = {
    0: "Dictionary size 64 Kb",
    1: "Dictionary size 128 Kb",
    2: "Dictionary size 256 Kb",
    3: "Dictionary size 512 Kb",
    4: "Dictionary size 1024 Kb",
    7: "File is a directory",
}

def formatRARVersion(field):
    """
    Decodes the RAR version stored on 1 byte
    """
    return "%u.%u" % divmod(field.value, 10)

def commonFlags(s):
    yield Bit(s, "has_added_size", "Additional field indicating additional size")
    yield Bit(s, "is_ignorable", "Old versions of RAR should ignore this block when copying data")

class ArchiveFlags(StaticFieldSet):
    format = (
        (Bit, "vol", "Archive volume"),
        (Bit, "has_comment", "Whether there is a comment"),
        (Bit, "is_locked", "Archive volume"),
        (Bit, "is_solid", "Whether files can be extracted separately"),
        (Bit, "new_numbering", "New numbering, or compressed comment"), # From unrar
        (Bit, "has_authenticity_information", "The integrity/authenticity of the archive can be checked"),
        (Bit, "is_protected", "The integrity/authenticity of the archive can be checked"),
        (Bit, "is_passworded", "Needs a password to be decrypted"),
        (Bit, "is_first_vol", "Whether it is the first volume"),
        (Bit, "is_encrypted", "Whether the encryption version is present"),
        (NullBits, "internal", 6, "Reserved for 'internal use'")
    )

def archiveFlags(s):
    yield ArchiveFlags(s, "flags", "Archiver block flags")

def archiveHeader(s):
    yield NullBytes(s, "reserved[]", 2, "Reserved word")
    yield NullBytes(s, "reserved[]", 4, "Reserved dword")

def commentHeader(s):
    yield filesizeHandler(UInt16(s, "total_size", "Comment header size + comment size"))
    yield filesizeHandler(UInt16(s, "uncompressed_size", "Uncompressed comment size"))
    yield UInt8(s, "required_version", "RAR version needed to extract comment")
    yield UInt8(s, "packing_method", "Comment packing method")
    yield UInt16(s, "comment_crc16", "Comment CRC")

def commentBody(s):
    size = s["total_size"].value - s.current_size
    if size > 0:
        yield RawBytes(s, "comment_data", size, "Compressed comment data")

def signatureHeader(s):
    yield TimeDateMSDOS32(s, "creation_time")
    yield filesizeHandler(UInt16(s, "arc_name_size"))
    yield filesizeHandler(UInt16(s, "user_name_size"))

def recoveryHeader(s):
    yield filesizeHandler(UInt32(s, "total_size"))
    yield textHandler(UInt8(s, "version"), hexadecimal)
    yield UInt16(s, "rec_sectors")
    yield UInt32(s, "total_blocks")
    yield RawBytes(s, "mark", 8)

def avInfoHeader(s):
    yield filesizeHandler(UInt16(s, "total_size", "Total block size"))
    yield UInt8(s, "version", "Version needed to decompress", handler=hexadecimal)
    yield UInt8(s, "method", "Compression method", handler=hexadecimal)
    yield UInt8(s, "av_version", "Version for AV", handler=hexadecimal)
    yield UInt32(s, "av_crc", "AV info CRC32", handler=hexadecimal)

def avInfoBody(s):
    size = s["total_size"].value - s.current_size
    if size > 0:
        yield RawBytes(s, "av_info_data", size, "AV info")

class FileFlags(FieldSet):
    static_size = 16
    def createFields(self):
        yield Bit(self, "continued_from", "File continued from previous volume")
        yield Bit(self, "continued_in", "File continued in next volume")
        yield Bit(self, "is_encrypted", "File encrypted with password")
        yield Bit(self, "has_comment", "File comment present")
        yield Bit(self, "is_solid", "Information from previous files is used (solid flag)")
        # The 3 following lines are what blocks more staticity
        yield Enum(Bits(self, "dictionary_size", 3, "Dictionary size"), DICTIONARY_SIZE)
        for bit in commonFlags(self):
            yield bit
        yield Bit(self, "is_large", "file64 operations needed")
        yield Bit(self, "is_unicode", "Filename also encoded using Unicode")
        yield Bit(self, "has_salt", "Has salt for encryption")
        yield Bit(self, "uses_file_version", "File versioning is used")
        yield Bit(self, "has_ext_time", "Extra time ??")
        yield Bit(self, "has_ext_flags", "Extra flag ??")

def fileFlags(s):
    yield FileFlags(s, "flags", "File block flags")

class ExtTime(FieldSet):
    def createFields(self):
        yield textHandler(UInt16(self, "time_flags", "Flags for extended time"), hexadecimal)
        flags = self["time_flags"].value
        for index in xrange(4):
            rmode = flags >> ((3-index)*4)
            if rmode & 8:
                if index:
                    yield TimeDateMSDOS32(self, "dos_time[]", "DOS Time")
                if rmode & 3:
                    yield RawBytes(self, "remainder[]", rmode & 3, "Time remainder")

def specialHeader(s, is_file):
    yield filesizeHandler(UInt32(s, "compressed_size", "Compressed size (bytes)"))
    yield filesizeHandler(UInt32(s, "uncompressed_size", "Uncompressed size (bytes)"))
    yield Enum(UInt8(s, "host_os", "Operating system used for archiving"), OS_NAME)
    yield textHandler(UInt32(s, "crc32", "File CRC32"), hexadecimal)
    yield TimeDateMSDOS32(s, "ftime", "Date and time (MS DOS format)")
    yield textHandler(UInt8(s, "version", "RAR version needed to extract file"), formatRARVersion)
    yield Enum(UInt8(s, "method", "Packing method"), COMPRESSION_NAME)
    yield filesizeHandler(UInt16(s, "filename_length", "File name size"))
    if s["host_os"].value in (OS_MSDOS, OS_WIN32):
        yield MSDOSFileAttr32(s, "file_attr", "File attributes")
    else:
        yield textHandler(UInt32(s, "file_attr", "File attributes"), hexadecimal)

    # Start additional field from unrar
    if s["flags/is_large"].value:
        yield filesizeHandler(UInt64(s, "large_size", "Extended 64bits filesize"))

    # End additional field
    size = s["filename_length"].value
    if size > 0:
        if s["flags/is_unicode"].value:
            charset = "UTF-8"
        else:
            charset = "ISO-8859-15"
        yield String(s, "filename", size, "Filename", charset=charset)
    # Start additional fields from unrar - file only
    if is_file:
        if s["flags/has_salt"].value:
            yield textHandler(UInt8(s, "salt", "Salt"), hexadecimal)
        if s["flags/has_ext_time"].value:
            yield ExtTime(s, "extra_time", "Extra time info")

def fileHeader(s):
    return specialHeader(s, True)

def fileBody(s):
    # File compressed data
    size = s["compressed_size"].value
    if s["flags/is_large"].value:
        size += s["large_size"].value
    if size > 0:
        yield RawBytes(s, "compressed_data", size, "File compressed data")

def fileDescription(s):
    return "File entry: %s (%s)" % \
           (s["filename"].display, s["compressed_size"].display)

def newSubHeader(s):
    return specialHeader(s, False)

class EndFlags(StaticFieldSet):
    format = (
        (Bit, "has_next_vol", "Whether there is another next volume"),
        (Bit, "has_data_crc", "Whether a CRC value is present"),
        (Bit, "rev_space"),
        (Bit, "has_vol_number", "Whether the volume number is present"),
        (Bits, "unused[]", 4),
        (Bit, "has_added_size", "Additional field indicating additional size"),
        (Bit, "is_ignorable", "Old versions of RAR should ignore this block when copying data"),
        (Bits, "unused[]", 6),
    )

def endFlags(s):
    yield EndFlags(s, "flags", "End block flags")

class BlockFlags(FieldSet):
    static_size = 16

    def createFields(self):
        yield textHandler(Bits(self, "unused[]", 8, "Unused flag bits"), hexadecimal)
        yield Bit(self, "has_added_size", "Additional field indicating additional size")
        yield Bit(self, "is_ignorable", "Old versions of RAR should ignore this block when copying data")
        yield Bits(self, "unused[]", 6)

class Block(FieldSet):
    BLOCK_INFO = {
        # None means 'use default function'
        0x72: ("marker", "Archive header", None, None, None),
        0x73: ("archive_start", "Archive info", archiveFlags, archiveHeader, None),
        0x74: ("file[]", fileDescription, fileFlags, fileHeader, fileBody),
        0x75: ("comment[]", "Stray comment", None, commentHeader, commentBody),
        0x76: ("av_info[]", "Extra information", None, avInfoHeader, avInfoBody),
        0x77: ("sub_block[]", "Stray subblock", None, newSubHeader, fileBody),
        0x78: ("recovery[]", "Recovery block", None, recoveryHeader, None),
        0x79: ("signature", "Signature block", None, signatureHeader, None),
        0x7A: ("new_sub_block[]", "Stray new-format subblock", fileFlags,
               newSubHeader, fileBody),
        0x7B: ("archive_end", "Archive end block", endFlags, None, None),
    }

    def __init__(self, parent, name):
        FieldSet.__init__(self, parent, name)
        t = self["block_type"].value
        if t in self.BLOCK_INFO:
            self._name, desc, parseFlags, parseHeader, parseBody = self.BLOCK_INFO[t]
            if callable(desc):
                self.createDescription = lambda: desc(self)
            elif desc:
                self._description = desc
            if parseFlags    : self.parseFlags     = lambda: parseFlags(self)
            if parseHeader   : self.parseHeader    = lambda: parseHeader(self)
            if parseBody     : self.parseBody      = lambda: parseBody(self)
        else:
            self.info("Processing as unknown block block of type %u" % type)

        self._size = 8*self["block_size"].value
        if t == 0x74 or t == 0x7A:
            self._size += 8*self["compressed_size"].value
            if "is_large" in self["flags"] and self["flags/is_large"].value:
                self._size += 8*self["large_size"].value
        elif "has_added_size" in self:
            self._size += 8*self["added_size"].value
        # TODO: check if any other member is needed here

    def createFields(self):
        yield textHandler(UInt16(self, "crc16", "Block CRC16"), hexadecimal)
        yield textHandler(UInt8(self, "block_type", "Block type"), hexadecimal)

        # Parse flags
        for field in self.parseFlags():
            yield field

        # Get block size
        yield filesizeHandler(UInt16(self, "block_size", "Block size"))

        # Parse remaining header
        for field in self.parseHeader():
            yield field

        # Finish header with stuff of unknow size
        size = self["block_size"].value - (self.current_size//8)
        if size > 0:
            yield RawBytes(self, "unknown", size, "Unknow data (UInt32 probably)")

        # Parse body
        for field in self.parseBody():
            yield field

    def createDescription(self):
        return "Block entry: %s" % self["type"].display

    def parseFlags(self):
        yield BlockFlags(self, "flags", "Block header flags")

    def parseHeader(self):
        if "has_added_size" in self["flags"] and \
           self["flags/has_added_size"].value:
            yield filesizeHandler(UInt32(self, "added_size",
                "Supplementary block size"))

    def parseBody(self):
        """
        Parse what is left of the block
        """
        size = self["block_size"].value - (self.current_size//8)
        if "has_added_size" in self["flags"] and self["flags/has_added_size"].value:
            size += self["added_size"].value
        if size > 0:
            yield RawBytes(self, "body", size, "Body data")

class RarFile(Parser):
    MAGIC = "Rar!\x1A\x07\x00"
    PARSER_TAGS = {
        "id": "rar",
        "category": "archive",
        "file_ext": ("rar",),
        "mime": (u"application/x-rar-compressed", ),
        "min_size": 7*8,
        "magic": ((MAGIC, 0),),
        "description": "Roshal archive (RAR)",
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        magic = self.MAGIC
        if self.stream.readBytes(0, len(magic)) != magic:
            return "Invalid magic"
        return True

    def createFields(self):
        while not self.eof:
            yield Block(self, "block[]")

    def createContentSize(self):
        start = 0
        end = MAX_FILESIZE * 8
        pos = self.stream.searchBytes("\xC4\x3D\x7B\x00\x40\x07\x00", start, end)
        if pos is not None:
            return pos + 7*8
        return None


########NEW FILE########
__FILENAME__ = rpm
"""
RPM archive parser.

Author: Victor Stinner, 1st December 2005.
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt16, UInt32, UInt64, Enum,
    NullBytes, Bytes, RawBytes, SubFile,
    Character, CString, String)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_parser.archive.gzip_parser import GzipParser
from lib.hachoir_parser.archive.bzip2_parser import Bzip2Parser

class ItemContent(FieldSet):
    format_type = {
        0: UInt8,
        1: Character,
        2: UInt8,
        3: UInt16,
        4: UInt32,
        5: UInt64,
        6: CString,
        7: RawBytes,
        8: CString,
        9: CString
    }

    def __init__(self, parent, name, item):
        FieldSet.__init__(self, parent, name, item.description)
        self.related_item = item
        self._name = "content_%s" % item.name

    def createFields(self):
        item = self.related_item
        type = item["type"].value

        cls = self.format_type[type]
        count = item["count"].value
        if cls is RawBytes: # or type == 8:
            if cls is RawBytes:
                args = (self, "value", count)
            else:
                args = (self, "value") # cls is CString
            count = 1
        else:
            if 1 < count:
                args = (self, "value[]")
            else:
                args = (self, "value")
        for index in xrange(count):
            yield cls(*args)

class Item(FieldSet):
    type_name = {
        0: "NULL",
        1: "CHAR",
        2: "INT8",
        3: "INT16",
        4: "INT32",
        5: "INT64",
        6: "CSTRING",
        7: "BIN",
        8: "CSTRING_ARRAY",
        9: "CSTRING?"
    }
    tag_name = {
        1000: "File size",
        1001: "(Broken) MD5 signature",
        1002: "PGP 2.6.3 signature",
        1003: "(Broken) MD5 signature",
        1004: "MD5 signature",
        1005: "GnuPG signature",
        1006: "PGP5 signature",
        1007: "Uncompressed payload size (bytes)",
        256+8: "Broken SHA1 header digest",
        256+9: "Broken SHA1 header digest",
        256+13: "Broken SHA1 header digest",
        256+11: "DSA header signature",
        256+12: "RSA header signature"
    }

    def __init__(self, parent, name, description=None, tag_name_dict=None):
        FieldSet.__init__(self, parent, name, description)
        if tag_name_dict is None:
            tag_name_dict = Item.tag_name
        self.tag_name_dict = tag_name_dict

    def createFields(self):
        yield Enum(UInt32(self, "tag", "Tag"), self.tag_name_dict)
        yield Enum(UInt32(self, "type", "Type"), Item.type_name)
        yield UInt32(self, "offset", "Offset")
        yield UInt32(self, "count", "Count")

    def createDescription(self):
        return "Item: %s (%s)" % (self["tag"].display, self["type"].display)

class ItemHeader(Item):
    tag_name = {
        61: "Current image",
        62: "Signatures",
        63: "Immutable",
        64: "Regions",
        100: "I18N string locales",
        1000: "Name",
        1001: "Version",
        1002: "Release",
        1003: "Epoch",
        1004: "Summary",
        1005: "Description",
        1006: "Build time",
        1007: "Build host",
        1008: "Install time",
        1009: "Size",
        1010: "Distribution",
        1011: "Vendor",
        1012: "Gif",
        1013: "Xpm",
        1014: "Licence",
        1015: "Packager",
        1016: "Group",
        1017: "Changelog",
        1018: "Source",
        1019: "Patch",
        1020: "Url",
        1021: "OS",
        1022: "Arch",
        1023: "Prein",
        1024: "Postin",
        1025: "Preun",
        1026: "Postun",
        1027: "Old filenames",
        1028: "File sizes",
        1029: "File states",
        1030: "File modes",
        1031: "File uids",
        1032: "File gids",
        1033: "File rdevs",
        1034: "File mtimes",
        1035: "File MD5s",
        1036: "File link to's",
        1037: "File flags",
        1038: "Root",
        1039: "File username",
        1040: "File groupname",
        1043: "Icon",
        1044: "Source rpm",
        1045: "File verify flags",
        1046: "Archive size",
        1047: "Provide name",
        1048: "Require flags",
        1049: "Require name",
        1050: "Require version",
        1051: "No source",
        1052: "No patch",
        1053: "Conflict flags",
        1054: "Conflict name",
        1055: "Conflict version",
        1056: "Default prefix",
        1057: "Build root",
        1058: "Install prefix",
        1059: "Exclude arch",
        1060: "Exclude OS",
        1061: "Exclusive arch",
        1062: "Exclusive OS",
        1064: "RPM version",
        1065: "Trigger scripts",
        1066: "Trigger name",
        1067: "Trigger version",
        1068: "Trigger flags",
        1069: "Trigger index",
        1079: "Verify script",
        #TODO: Finish the list (id 1070..1162 using rpm library source code)
    }

    def __init__(self, parent, name, description=None):
        Item.__init__(self, parent, name, description, self.tag_name)

def sortRpmItem(a,b):
    return int( a["offset"].value - b["offset"].value )

class PropertySet(FieldSet):
    def __init__(self, parent, name, *args):
        FieldSet.__init__(self, parent, name, *args)
        self._size = self["content_item[1]"].address + self["size"].value * 8

    def createFields(self):
        # Read chunk header
        yield Bytes(self, "signature", 3, r"Property signature (\x8E\xAD\xE8)")
        if self["signature"].value != "\x8E\xAD\xE8":
            raise ParserError("Invalid property signature")
        yield UInt8(self, "version", "Signature version")
        yield NullBytes(self, "reserved", 4, "Reserved")
        yield UInt32(self, "count", "Count")
        yield UInt32(self, "size", "Size")

        # Read item header
        items = []
        for i in range(0, self["count"].value):
            item = ItemHeader(self, "item[]")
            yield item
            items.append(item)

        # Sort items by their offset
        items.sort( sortRpmItem )

        # Read item content
        start = self.current_size/8
        for item in items:
            offset = item["offset"].value
            diff = offset - (self.current_size/8 - start)
            if 0 < diff:
                yield NullBytes(self, "padding[]", diff)
            yield ItemContent(self, "content[]", item)
        size = start + self["size"].value - self.current_size/8
        if 0 < size:
            yield NullBytes(self, "padding[]", size)

class RpmFile(Parser):
    PARSER_TAGS = {
        "id": "rpm",
        "category": "archive",
        "file_ext": ("rpm",),
        "mime": (u"application/x-rpm",),
        "min_size": (96 + 16 + 16)*8, # file header + checksum + content header
        "magic": (('\xED\xAB\xEE\xDB', 0),),
        "description": "RPM package"
    }
    TYPE_NAME = {
        0: "Binary",
        1: "Source"
    }
    endian = BIG_ENDIAN

    def validate(self):
        if self["signature"].value != '\xED\xAB\xEE\xDB':
            return "Invalid signature"
        if self["major_ver"].value != 3:
            return "Unknown major version (%u)" % self["major_ver"].value
        if self["type"].value not in self.TYPE_NAME:
            return "Invalid RPM type"
        return True

    def createFields(self):
        yield Bytes(self, "signature", 4, r"RPM file signature (\xED\xAB\xEE\xDB)")
        yield UInt8(self, "major_ver", "Major version")
        yield UInt8(self, "minor_ver", "Minor version")
        yield Enum(UInt16(self, "type", "RPM type"), RpmFile.TYPE_NAME)
        yield UInt16(self, "architecture", "Architecture")
        yield String(self, "name", 66, "Archive name", strip="\0", charset="ASCII")
        yield UInt16(self, "os", "OS")
        yield UInt16(self, "signature_type", "Type of signature")
        yield NullBytes(self, "reserved", 16, "Reserved")
        yield PropertySet(self, "checksum", "Checksum (signature)")
        yield PropertySet(self, "header", "Header")

        if self._size is None: # TODO: is it possible to handle piped input?
            raise NotImplementedError

        size = (self._size - self.current_size) // 8
        if size:
            if 3 <= size and self.stream.readBytes(self.current_size, 3) == "BZh":
                yield SubFile(self, "content", size, "bzip2 content", parser=Bzip2Parser)
            else:
                yield SubFile(self, "content", size, "gzip content", parser=GzipParser)


########NEW FILE########
__FILENAME__ = sevenzip
"""
7zip file parser

Informations:
- File 7zformat.txt of 7-zip SDK:
  http://www.7-zip.org/sdk.html

Author: Olivier SCHWAB
Creation date: 6 december 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (Field, FieldSet, ParserError,
    GenericVector,
    Enum, UInt8, UInt32, UInt64,
    Bytes, RawBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler

class SZUInt64(Field):
    """
    Variable length UInt64, where the first byte gives both the number of bytes
    needed and the upper byte value.
    """
    def __init__(self, parent, name, max_size=None, description=None):
        Field.__init__(self, parent, name, size=8, description=description)
        value = 0
        addr = self.absolute_address
        mask = 0x80
        firstByte = parent.stream.readBits(addr, 8, LITTLE_ENDIAN)
        for i in xrange(8):
            addr += 8
            if not (firstByte & mask):
                value += ((firstByte & (mask-1)) << (8*i))
                break
            value |= (parent.stream.readBits(addr, 8, LITTLE_ENDIAN) << (8*i))
            mask >>= 1
            self._size += 8
        self.createValue = lambda: value

ID_END, ID_HEADER, ID_ARCHIVE_PROPS, ID_ADD_STREAM_INFO, ID_MAIN_STREAM_INFO, \
ID_FILES_INFO, ID_PACK_INFO, ID_UNPACK_INFO, ID_SUBSTREAMS_INFO, ID_SIZE, \
ID_CRC, ID_FOLDER, ID_CODERS_UNPACK_SIZE, ID_NUM_UNPACK_STREAMS, \
ID_EMPTY_STREAM, ID_EMPTY_FILE, ID_ANTI, ID_NAME, ID_CREATION_TIME, \
ID_LAST_ACCESS_TIME, ID_LAST_WRITE_TIME, ID_WIN_ATTR, ID_COMMENT, \
ID_ENCODED_HEADER = xrange(24)

ID_INFO = {
    ID_END               : "End",
    ID_HEADER            : "Header embedding another one",
    ID_ARCHIVE_PROPS     : "Archive Properties",
    ID_ADD_STREAM_INFO   : "Additional Streams Info",
    ID_MAIN_STREAM_INFO  : "Main Streams Info",
    ID_FILES_INFO        : "Files Info",
    ID_PACK_INFO         : "Pack Info",
    ID_UNPACK_INFO       : "Unpack Info",
    ID_SUBSTREAMS_INFO   : "Substreams Info",
    ID_SIZE              : "Size",
    ID_CRC               : "CRC",
    ID_FOLDER            : "Folder",
    ID_CODERS_UNPACK_SIZE: "Coders Unpacked size",
    ID_NUM_UNPACK_STREAMS: "Number of Unpacked Streams",
    ID_EMPTY_STREAM      : "Empty Stream",
    ID_EMPTY_FILE        : "Empty File",
    ID_ANTI              : "Anti",
    ID_NAME              : "Name",
    ID_CREATION_TIME     : "Creation Time",
    ID_LAST_ACCESS_TIME  : "Last Access Time",
    ID_LAST_WRITE_TIME   : "Last Write Time",
    ID_WIN_ATTR          : "Win Attributes",
    ID_COMMENT           : "Comment",
    ID_ENCODED_HEADER    : "Header holding encoded data info",
}

class SkippedData(FieldSet):
    def createFields(self):
        yield Enum(UInt8(self, "id[]"), ID_INFO)
        size = SZUInt64(self, "size")
        yield size
        if size.value > 0:
            yield RawBytes(self, "data", size.value)

def waitForID(s, wait_id, wait_name="waited_id[]"):
    while not s.eof:
        addr = s.absolute_address+s.current_size
        uid = s.stream.readBits(addr, 8, LITTLE_ENDIAN)
        if uid == wait_id:
            yield Enum(UInt8(s, wait_name), ID_INFO)
            s.info("Found ID %s (%u)" % (ID_INFO[uid], uid))
            return
        s.info("Skipping ID %u!=%u" % (uid, wait_id))
        yield SkippedData(s, "skipped_id[]", "%u != %u" % (uid, wait_id))

class HashDigest(FieldSet):
    def __init__(self, parent, name, num_digests, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self.num_digests = num_digests
    def createFields(self):
        yield Enum(UInt8(self, "id"), ID_INFO)
        bytes = self.stream.readBytes(self.absolute_address, self.num_digests)
        if self.num_digests > 0:
            yield GenericVector(self, "defined[]", self.num_digests, UInt8, "bool")
            for index in xrange(self.num_digests):
                if bytes[index]:
                    yield textHandler(UInt32(self, "hash[]",
                        "Hash for digest %u" % index), hexadecimal)

class PackInfo(FieldSet):
    def createFields(self):
        yield Enum(UInt8(self, "id"), ID_INFO)
        # Very important, helps determine where the data is
        yield SZUInt64(self, "pack_pos", "Position of the packs")
        num = SZUInt64(self, "num_pack_streams")
        yield num
        num = num.value

        for field in waitForID(self, ID_SIZE, "size_marker"):
            yield field

        for size in xrange(num):
            yield SZUInt64(self, "pack_size[]")

        while not self.eof:
            addr = self.absolute_address+self.current_size
            uid = self.stream.readBits(addr, 8, LITTLE_ENDIAN)
            if uid == ID_END:
                yield Enum(UInt8(self, "end_marker"), ID_INFO)
                break
            elif uid == ID_CRC:
                yield HashDigest(self, "hash_digest", size)
            else:
                yield SkippedData(self, "skipped_data")

def lzmaParams(value):
    param = value.value
    remainder = param / 9
    # Literal coder context bits
    lc = param % 9
    # Position state bits
    pb = remainder / 5
    # Literal coder position bits
    lp = remainder % 5
    return "lc=%u pb=%u lp=%u" % (lc, lp, pb)

class CoderID(FieldSet):
    CODECS = {
        # Only 2 methods ... and what about PPMD ?
        "\0"    : "copy",
        "\3\1\1": "lzma",
    }
    def createFields(self):
        byte = UInt8(self, "id_size")
        yield byte
        byte = byte.value
        self.info("ID=%u" % byte)
        size = byte & 0xF
        if size > 0:
            name = self.stream.readBytes(self.absolute_address+self.current_size, size)
            if name in self.CODECS:
                name = self.CODECS[name]
                self.info("Codec is %s" % name)
            else:
                self.info("Undetermined codec %s" % name)
                name = "unknown"
            yield RawBytes(self, name, size)
            #yield textHandler(Bytes(self, "id", size), lambda: name)
        if byte & 0x10:
            yield SZUInt64(self, "num_stream_in")
            yield SZUInt64(self, "num_stream_out")
            self.info("Streams: IN=%u    OUT=%u" % \
                      (self["num_stream_in"].value, self["num_stream_out"].value))
        if byte & 0x20:
            size = SZUInt64(self, "properties_size[]")
            yield size
            if size.value == 5:
                #LzmaDecodeProperties@LZMAStateDecode.c
                yield textHandler(UInt8(self, "parameters"), lzmaParams)
                yield filesizeHandler(UInt32(self, "dictionary_size"))
            elif size.value > 0:
                yield RawBytes(self, "properties[]", size.value)

class CoderInfo(FieldSet):
    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self.in_streams = 1
        self.out_streams = 1
    def createFields(self):
        # The real ID
        addr = self.absolute_address + self.current_size
        b = self.parent.stream.readBits(addr, 8, LITTLE_ENDIAN)
        cid = CoderID(self, "coder_id")
        yield cid
        if b&0x10: # Work repeated, ...
            self.in_streams = cid["num_stream_in"].value
            self.out_streams = cid["num_stream_out"].value

        # Skip other IDs
        while b&0x80:
            addr = self.absolute_address + self.current_size
            b = self.parent.stream.readBits(addr, 8, LITTLE_ENDIAN)
            yield CoderID(self, "unused_codec_id[]")

class BindPairInfo(FieldSet):
    def createFields(self):
        # 64 bits values then cast to 32 in fact
        yield SZUInt64(self, "in_index")
        yield SZUInt64(self, "out_index")
        self.info("Indexes: IN=%u   OUT=%u" % \
                  (self["in_index"].value, self["out_index"].value))

class FolderItem(FieldSet):
    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self.in_streams = 0
        self.out_streams = 0

    def createFields(self):
        yield SZUInt64(self, "num_coders")
        num = self["num_coders"].value
        self.info("Folder: %u codecs" % num)

        # Coders info
        for index in xrange(num):
            ci = CoderInfo(self, "coder_info[]")
            yield ci
            self.in_streams += ci.in_streams
            self.out_streams += ci.out_streams

        # Bin pairs
        self.info("out streams: %u" % self.out_streams)
        for index in xrange(self.out_streams-1):
            yield BindPairInfo(self, "bind_pair[]")

        # Packed streams
        # @todo: Actually find mapping
        packed_streams = self.in_streams - self.out_streams + 1
        if packed_streams == 1:
            pass
        else:
            for index in xrange(packed_streams):
                yield SZUInt64(self, "pack_stream[]")


class UnpackInfo(FieldSet):
    def createFields(self):
        yield Enum(UInt8(self, "id"), ID_INFO)
        # Wait for synch
        for field in waitForID(self, ID_FOLDER, "folder_marker"):
            yield field
        yield SZUInt64(self, "num_folders")

        # Get generic info
        num = self["num_folders"].value
        self.info("%u folders" % num)
        yield UInt8(self, "is_external")

        # Read folder items
        for folder_index in xrange(num):
            yield FolderItem(self, "folder_item[]")

        # Get unpack sizes for each coder of each folder
        for field in waitForID(self, ID_CODERS_UNPACK_SIZE, "coders_unpsize_marker"):
            yield field
        for folder_index in xrange(num):
            folder_item = self["folder_item[%u]" % folder_index]
            for index in xrange(folder_item.out_streams):
                #yield UInt8(self, "unpack_size[]")
                yield SZUInt64(self, "unpack_size[]")

        # Extract digests
        while not self.eof:
            addr = self.absolute_address+self.current_size
            uid = self.stream.readBits(addr, 8, LITTLE_ENDIAN)
            if uid == ID_END:
                yield Enum(UInt8(self, "end_marker"), ID_INFO)
                break
            elif uid == ID_CRC:
                yield HashDigest(self, "hash_digest", num)
            else:
                yield SkippedData(self, "skip_data")

class SubStreamInfo(FieldSet):
    def createFields(self):
        yield Enum(UInt8(self, "id"), ID_INFO)
        raise ParserError("SubStreamInfo not implemented yet")

class EncodedHeader(FieldSet):
    def createFields(self):
        yield Enum(UInt8(self, "id"), ID_INFO)
        while not self.eof:
            addr = self.absolute_address+self.current_size
            uid = self.stream.readBits(addr, 8, LITTLE_ENDIAN)
            if uid == ID_END:
                yield Enum(UInt8(self, "end_marker"), ID_INFO)
                break
            elif uid == ID_PACK_INFO:
                yield PackInfo(self, "pack_info", ID_INFO[ID_PACK_INFO])
            elif uid == ID_UNPACK_INFO:
                yield UnpackInfo(self, "unpack_info", ID_INFO[ID_UNPACK_INFO])
            elif uid == ID_SUBSTREAMS_INFO:
                yield SubStreamInfo(self, "substreams_info", ID_INFO[ID_SUBSTREAMS_INFO])
            else:
                self.info("Unexpected ID (%i)" % uid)
                break

class IDHeader(FieldSet):
    def createFields(self):
        yield Enum(UInt8(self, "id"), ID_INFO)
        ParserError("IDHeader not implemented")

class NextHeader(FieldSet):
    def __init__(self, parent, name, desc="Next header"):
        FieldSet.__init__(self, parent, name, desc)
        self._size = 8*self["/signature/start_hdr/next_hdr_size"].value
    # Less work, as much interpretable information as the other
    # version... what an obnoxious format
    def createFields2(self):
        yield Enum(UInt8(self, "header_type"), ID_INFO)
        yield RawBytes(self, "header_data", self._size-1)
    def createFields(self):
        uid = self.stream.readBits(self.absolute_address, 8, LITTLE_ENDIAN)
        if uid == ID_HEADER:
            yield IDHeader(self, "header", ID_INFO[ID_HEADER])
        elif uid == ID_ENCODED_HEADER:
            yield EncodedHeader(self, "encoded_hdr", ID_INFO[ID_ENCODED_HEADER])
            # Game Over: this is usually encoded using LZMA, not copy
            # See SzReadAndDecodePackedStreams/SzDecode being called with the
            # data position from "/next_hdr/encoded_hdr/pack_info/pack_pos"
            # We should process further, yet we can't...
        else:
            ParserError("Unexpected ID %u" % uid)
        size = self._size - self.current_size
        if size > 0:
            yield RawBytes(self, "next_hdr_data", size//8, "Next header's data")

class Body(FieldSet):
    def __init__(self, parent, name, desc="Body data"):
        FieldSet.__init__(self, parent, name, desc)
        self._size = 8*self["/signature/start_hdr/next_hdr_offset"].value
    def createFields(self):
        if "encoded_hdr" in self["/next_hdr/"]:
            pack_size = sum([s.value for s in self.array("/next_hdr/encoded_hdr/pack_info/pack_size")])
            body_size = self["/next_hdr/encoded_hdr/pack_info/pack_pos"].value
            yield RawBytes(self, "compressed_data", body_size, "Compressed data")
            # Here we could check if copy method was used to "compress" it,
            # but this never happens, so just output "compressed file info"
            yield RawBytes(self, "compressed_file_info", pack_size,
                           "Compressed file information")
            size = (self._size//8) - pack_size - body_size
            if size > 0:
                yield RawBytes(self, "unknown_data", size)
        elif "header" in self["/next_hdr"]:
            yield RawBytes(self, "compressed_data", self._size//8, "Compressed data")

class StartHeader(FieldSet):
    static_size = 160
    def createFields(self):
        yield textHandler(UInt64(self, "next_hdr_offset",
            "Next header offset"), hexadecimal)
        yield UInt64(self, "next_hdr_size", "Next header size")
        yield textHandler(UInt32(self, "next_hdr_crc",
            "Next header CRC"), hexadecimal)

class SignatureHeader(FieldSet):
    static_size = 96 + StartHeader.static_size
    def createFields(self):
        yield Bytes(self, "signature", 6, "Signature Header")
        yield UInt8(self, "major_ver", "Archive major version")
        yield UInt8(self, "minor_ver", "Archive minor version")
        yield textHandler(UInt32(self, "start_hdr_crc",
            "Start header CRC"), hexadecimal)
        yield StartHeader(self, "start_hdr", "Start header")

class SevenZipParser(Parser):
    PARSER_TAGS = {
        "id": "7zip",
        "category": "archive",
        "file_ext": ("7z",),
        "mime": (u"application/x-7z-compressed",),
        "min_size": 32*8,
        "magic": (("7z\xbc\xaf\x27\x1c", 0),),
        "description": "Compressed archive in 7z format"
    }
    endian = LITTLE_ENDIAN

    def createFields(self):
        yield SignatureHeader(self, "signature", "Signature Header")
        yield Body(self, "body_data")
        yield NextHeader(self, "next_hdr")

    def validate(self):
        if self.stream.readBytes(0,6) != "7z\xbc\xaf'\x1c":
            return "Invalid signature"
        return True

    def createContentSize(self):
        size = self["/signature/start_hdr/next_hdr_offset"].value
        size += self["/signature/start_hdr/next_hdr_size"].value
        size += 12 # Signature size
        size += 20 # Start header size
        return size*8

########NEW FILE########
__FILENAME__ = tar
"""
Tar archive parser.

Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    Enum, UInt8, SubFile, String, NullBytes)
from lib.hachoir_core.tools import humanFilesize, paddingSize, timestampUNIX
from lib.hachoir_core.endian import BIG_ENDIAN
import re

class FileEntry(FieldSet):
    type_name = {
        # 48 is "0", 49 is "1", ...
         0: u"Normal disk file (old format)",
        48: u"Normal disk file",
        49: u"Link to previously dumped file",
        50: u"Symbolic link",
        51: u"Character special file",
        52: u"Block special file",
        53: u"Directory",
        54: u"FIFO special file",
        55: u"Contiguous file"
    }

    def getOctal(self, name):
        return self.octal2int(self[name].value)

    def getDatetime(self):
        """
        Create modification date as Unicode string, may raise ValueError.
        """
        timestamp = self.getOctal("mtime")
        return timestampUNIX(timestamp)

    def createFields(self):
        yield String(self, "name", 100, "Name", strip="\0", charset="ISO-8859-1")
        yield String(self, "mode", 8, "Mode", strip=" \0", charset="ASCII")
        yield String(self, "uid", 8, "User ID", strip=" \0", charset="ASCII")
        yield String(self, "gid", 8, "Group ID", strip=" \0", charset="ASCII")
        yield String(self, "size", 12, "Size", strip=" \0", charset="ASCII")
        yield String(self, "mtime", 12, "Modification time", strip=" \0", charset="ASCII")
        yield String(self, "check_sum", 8, "Check sum", strip=" \0", charset="ASCII")
        yield Enum(UInt8(self, "type", "Type"), self.type_name)
        yield String(self, "lname", 100, "Link name", strip=" \0", charset="ISO-8859-1")
        yield String(self, "magic", 8, "Magic", strip=" \0", charset="ASCII")
        yield String(self, "uname", 32, "User name", strip=" \0", charset="ISO-8859-1")
        yield String(self, "gname", 32, "Group name", strip=" \0", charset="ISO-8859-1")
        yield String(self, "devmajor", 8, "Dev major", strip=" \0", charset="ASCII")
        yield String(self, "devminor", 8, "Dev minor", strip=" \0", charset="ASCII")
        yield NullBytes(self, "padding", 167, "Padding (zero)")

        filesize = self.getOctal("size")
        if filesize:
            yield SubFile(self, "content", filesize, filename=self["name"].value)

        size = paddingSize(self.current_size//8, 512)
        if size:
            yield NullBytes(self, "padding_end", size, "Padding (512 align)")

    def convertOctal(self, chunk):
        return self.octal2int(chunk.value)

    def isEmpty(self):
        return self["name"].value == ""

    def octal2int(self, text):
        try:
            return int(text, 8)
        except ValueError:
            return 0

    def createDescription(self):
        if self.isEmpty():
            desc = "(terminator, empty header)"
        else:
            filename = self["name"].value
            filesize = humanFilesize(self.getOctal("size"))
            desc = "(%s: %s, %s)" % \
                (filename, self["type"].display, filesize)
        return "Tar File " + desc

class TarFile(Parser):
    endian = BIG_ENDIAN
    PARSER_TAGS = {
        "id": "tar",
        "category": "archive",
        "file_ext": ("tar",),
        "mime": (u"application/x-tar", u"application/x-gtar"),
        "min_size": 512*8,
        "magic": (("ustar  \0", 257*8),),
        "subfile": "skip",
        "description": "TAR archive",
    }
    _sign = re.compile("ustar *\0|[ \0]*$")

    def validate(self):
        if not self._sign.match(self.stream.readBytes(257*8, 8)):
            return "Invalid magic number"
        if self[0].name == "terminator":
            return "Don't contain any file"
        try:
            int(self["file[0]/uid"].value, 8)
            int(self["file[0]/gid"].value, 8)
            int(self["file[0]/size"].value, 8)
        except ValueError:
            return "Invalid file size"
        return True

    def createFields(self):
        while not self.eof:
            field = FileEntry(self, "file[]")
            if field.isEmpty():
                yield NullBytes(self, "terminator", 512)
                break
            yield field
        if self.current_size < self._size:
            yield self.seekBit(self._size, "end")

    def createContentSize(self):
        return self["terminator"].address + self["terminator"].size


########NEW FILE########
__FILENAME__ = zip
"""
Zip splitter.

Status: can read most important headers
Authors: Christophe Gisquet and Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    Bit, Bits, Enum,
    TimeDateMSDOS32, SubFile,
    UInt8, UInt16, UInt32, UInt64,
    String, PascalString16,
    RawBytes)
from lib.hachoir_core.text_handler import textHandler, filesizeHandler, hexadecimal
from lib.hachoir_core.error import HACHOIR_ERRORS
from lib.hachoir_core.tools import makeUnicode
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_parser.common.deflate import Deflate

MAX_FILESIZE = 1000 * 1024 * 1024

COMPRESSION_DEFLATE = 8
COMPRESSION_METHOD = {
     0: u"no compression",
     1: u"Shrunk",
     2: u"Reduced (factor 1)",
     3: u"Reduced (factor 2)",
     4: u"Reduced (factor 3)",
     5: u"Reduced (factor 4)",
     6: u"Imploded",
     7: u"Tokenizing",
     8: u"Deflate",
     9: u"Deflate64",
    10: u"PKWARE Imploding",
    11: u"Reserved by PKWARE",
    12: u"File is compressed using BZIP2 algorithm",
    13: u"Reserved by PKWARE",
    14: u"LZMA (EFS)",
    15: u"Reserved by PKWARE",
    16: u"Reserved by PKWARE",
    17: u"Reserved by PKWARE",
    18: u"File is compressed using IBM TERSE (new)",
    19: u"IBM LZ77 z Architecture (PFS)",
    98: u"PPMd version I, Rev 1",
}

def ZipRevision(field):
    return "%u.%u" % divmod(field.value, 10)

class ZipVersion(FieldSet):
    static_size = 16
    HOST_OS = {
         0: u"FAT file system (DOS, OS/2, NT)",
         1: u"Amiga",
         2: u"VMS (VAX or Alpha AXP)",
         3: u"Unix",
         4: u"VM/CMS",
         5: u"Atari",
         6: u"HPFS file system (OS/2, NT 3.x)",
         7: u"Macintosh",
         8: u"Z-System",
         9: u"CP/M",
        10: u"TOPS-20",
        11: u"NTFS file system (NT)",
        12: u"SMS/QDOS",
        13: u"Acorn RISC OS",
        14: u"VFAT file system (Win95, NT)",
        15: u"MVS",
        16: u"BeOS (BeBox or PowerMac)",
        17: u"Tandem",
    }
    def createFields(self):
        yield textHandler(UInt8(self, "zip_version", "ZIP version"), ZipRevision)
        yield Enum(UInt8(self, "host_os", "ZIP Host OS"), self.HOST_OS)

class ZipGeneralFlags(FieldSet):
    static_size = 16
    def createFields(self):
        # Need the compression info from the parent, and that is the byte following
        method = self.stream.readBits(self.absolute_address+16, 16, LITTLE_ENDIAN)

        yield Bits(self, "unused[]", 2, "Unused")
        yield Bit(self, "encrypted_central_dir", "Selected data values in the Local Header are masked")
        yield Bit(self, "incomplete", "Reserved by PKWARE for enhanced compression.")
        yield Bit(self, "uses_unicode", "Filename and comments are in UTF-8")
        yield Bits(self, "unused[]", 4, "Unused")
        yield Bit(self, "strong_encrypt", "Strong encryption (version >= 50)")
        yield Bit(self, "is_patched", "File is compressed with patched data?")
        yield Bit(self, "enhanced_deflate", "Reserved for use with method 8")
        yield Bit(self, "has_descriptor",
                  "Compressed data followed by descriptor?")
        if method == 6:
            yield Bit(self, "use_8k_sliding", "Use 8K sliding dictionary (instead of 4K)")
            yield Bit(self, "use_3shannon", "Use a 3 Shannon-Fano tree (instead of 2 Shannon-Fano)")
        elif method in (8, 9):
            NAME = {
                0: "Normal compression",
                1: "Maximum compression",
                2: "Fast compression",
                3: "Super Fast compression"
            }
            yield Enum(Bits(self, "method", 2), NAME)
        elif method == 14: #LZMA
            yield Bit(self, "lzma_eos", "LZMA stream is ended with a EndOfStream marker")
            yield Bit(self, "unused[]")
        else:
            yield Bits(self, "compression_info", 2)
        yield Bit(self, "is_encrypted", "File is encrypted?")

class ExtraField(FieldSet):
    EXTRA_FIELD_ID = {
        0x0007: "AV Info",
        0x0009: "OS/2 extended attributes (also Info-ZIP)",
        0x000a: "PKWARE Win95/WinNT FileTimes", # undocumented!
        0x000c: "PKWARE VAX/VMS (also Info-ZIP)",
        0x000d: "PKWARE Unix",
        0x000f: "Patch Descriptor",
        0x07c8: "Info-ZIP Macintosh (old, J. Lee)",
        0x2605: "ZipIt Macintosh (first version)",
        0x2705: "ZipIt Macintosh v 1.3.5 and newer (w/o full filename)",
        0x334d: "Info-ZIP Macintosh (new, D. Haase Mac3 field)",
        0x4341: "Acorn/SparkFS (David Pilling)",
        0x4453: "Windows NT security descriptor (binary ACL)",
        0x4704: "VM/CMS",
        0x470f: "MVS",
        0x4b46: "FWKCS MD5 (third party, see below)",
        0x4c41: "OS/2 access control list (text ACL)",
        0x4d49: "Info-ZIP VMS (VAX or Alpha)",
        0x5356: "AOS/VS (binary ACL)",
        0x5455: "extended timestamp",
        0x5855: "Info-ZIP Unix (original; also OS/2, NT, etc.)",
        0x6542: "BeOS (BeBox, PowerMac, etc.)",
        0x756e: "ASi Unix",
        0x7855: "Info-ZIP Unix (new)",
        0xfb4a: "SMS/QDOS",
    }
    def createFields(self):
        yield Enum(UInt16(self, "field_id", "Extra field ID"),
                   self.EXTRA_FIELD_ID)
        size = UInt16(self, "field_data_size", "Extra field data size")
        yield size
        if size.value > 0:
            yield RawBytes(self, "field_data", size, "Unknown field data")

def ZipStartCommonFields(self):
    yield ZipVersion(self, "version_needed", "Version needed")
    yield ZipGeneralFlags(self, "flags", "General purpose flag")
    yield Enum(UInt16(self, "compression", "Compression method"),
               COMPRESSION_METHOD)
    yield TimeDateMSDOS32(self, "last_mod", "Last modification file time")
    yield textHandler(UInt32(self, "crc32", "CRC-32"), hexadecimal)
    yield UInt32(self, "compressed_size", "Compressed size")
    yield UInt32(self, "uncompressed_size", "Uncompressed size")
    yield UInt16(self, "filename_length", "Filename length")
    yield UInt16(self, "extra_length", "Extra fields length")

def zipGetCharset(self):
    if self["flags/uses_unicode"].value:
        return "UTF-8"
    else:
        return "ISO-8859-15"

class ZipCentralDirectory(FieldSet):
    HEADER = 0x02014b50
    def createFields(self):
        yield ZipVersion(self, "version_made_by", "Version made by")
        for field in ZipStartCommonFields(self):
            yield field

        # Check unicode status
        charset = zipGetCharset(self)

        yield UInt16(self, "comment_length", "Comment length")
        yield UInt16(self, "disk_number_start", "Disk number start")
        yield UInt16(self, "internal_attr", "Internal file attributes")
        yield UInt32(self, "external_attr", "External file attributes")
        yield UInt32(self, "offset_header", "Relative offset of local header")
        yield String(self, "filename", self["filename_length"].value,
                     "Filename", charset=charset)
        if 0 < self["extra_length"].value:
            yield RawBytes(self, "extra", self["extra_length"].value,
                           "Extra fields")
        if 0 < self["comment_length"].value:
            yield String(self, "comment", self["comment_length"].value,
                         "Comment", charset=charset)

    def createDescription(self):
        return "Central directory: %s" % self["filename"].display

class Zip64EndCentralDirectory(FieldSet):
    HEADER = 0x06064b50
    def createFields(self):
        yield UInt64(self, "zip64_end_size",
                     "Size of zip64 end of central directory record")
        yield ZipVersion(self, "version_made_by", "Version made by")
        yield ZipVersion(self, "version_needed", "Version needed to extract")
        yield UInt32(self, "number_disk", "Number of this disk")
        yield UInt32(self, "number_disk2",
                     "Number of the disk with the start of the central directory")
        yield UInt64(self, "number_entries",
                     "Total number of entries in the central directory on this disk")
        yield UInt64(self, "number_entries2",
                     "Total number of entries in the central directory")
        yield UInt64(self, "size", "Size of the central directory")
        yield UInt64(self, "offset", "Offset of start of central directory")
        if 0 < self["zip64_end_size"].value:
            yield RawBytes(self, "data_sector", self["zip64_end_size"].value,
                           "zip64 extensible data sector")

class ZipEndCentralDirectory(FieldSet):
    HEADER = 0x06054b50
    def createFields(self):
        yield UInt16(self, "number_disk", "Number of this disk")
        yield UInt16(self, "number_disk2", "Number in the central dir")
        yield UInt16(self, "total_number_disk",
                     "Total number of entries in this disk")
        yield UInt16(self, "total_number_disk2",
                     "Total number of entries in the central dir")
        yield UInt32(self, "size", "Size of the central directory")
        yield UInt32(self, "offset", "Offset of start of central directory")
        yield PascalString16(self, "comment", "ZIP comment")

class ZipDataDescriptor(FieldSet):
    HEADER_STRING = "\x50\x4B\x07\x08"
    HEADER = 0x08074B50
    static_size = 96
    def createFields(self):
        yield textHandler(UInt32(self, "file_crc32",
            "Checksum (CRC32)"), hexadecimal)
        yield filesizeHandler(UInt32(self, "file_compressed_size",
            "Compressed size (bytes)"))
        yield filesizeHandler(UInt32(self, "file_uncompressed_size",
             "Uncompressed size (bytes)"))

class FileEntry(FieldSet):
    HEADER = 0x04034B50
    filename = None

    def data(self, size):
        compression = self["compression"].value
        if compression == 0:
            return SubFile(self, "data", size, filename=self.filename)
        compressed = SubFile(self, "compressed_data", size, filename=self.filename)
        if compression == COMPRESSION_DEFLATE:
            return Deflate(compressed)
        else:
            return compressed

    def resync(self):
        # Non-seekable output, search the next data descriptor
        size = self.stream.searchBytesLength(ZipDataDescriptor.HEADER_STRING, False,
                                            self.absolute_address+self.current_size)
        if size <= 0:
            raise ParserError("Couldn't resync to %s" %
                              ZipDataDescriptor.HEADER_STRING)
        yield self.data(size)
        yield textHandler(UInt32(self, "header[]", "Header"), hexadecimal)
        data_desc = ZipDataDescriptor(self, "data_desc", "Data descriptor")
        #self.info("Resynced!")
        yield data_desc
        # The above could be checked anytime, but we prefer trying parsing
        # than aborting
        if self["crc32"].value == 0 and \
            data_desc["file_compressed_size"].value != size:
            raise ParserError("Bad resync: position=>%i but data_desc=>%i" %
                              (size, data_desc["file_compressed_size"].value))

    def createFields(self):
        for field in ZipStartCommonFields(self):
            yield field
        length = self["filename_length"].value


        if length:
            filename = String(self, "filename", length, "Filename",
                              charset=zipGetCharset(self))
            yield filename
            self.filename = filename.value
        if self["extra_length"].value:
            yield RawBytes(self, "extra", self["extra_length"].value, "Extra")
        size = self["compressed_size"].value
        if size > 0:
            yield self.data(size)
        elif self["flags/incomplete"].value:
            for field in self.resync():
                yield field
        if self["flags/has_descriptor"].value:
            yield ZipDataDescriptor(self, "data_desc", "Data descriptor")

    def createDescription(self):
        return "File entry: %s (%s)" % \
            (self["filename"].value, self["compressed_size"].display)

    def validate(self):
        if self["compression"].value not in COMPRESSION_METHOD:
            return "Unknown compression method (%u)" % self["compression"].value
        return ""

class ZipSignature(FieldSet):
    HEADER = 0x05054B50
    def createFields(self):
        yield PascalString16(self, "signature", "Signature")

class Zip64EndCentralDirectoryLocator(FieldSet):
    HEADER = 0x07064b50
    def createFields(self):
        yield UInt32(self, "disk_number", \
                     "Number of the disk with the start of the zip64 end of central directory")
        yield UInt64(self, "relative_offset", \
                     "Relative offset of the zip64 end of central directory record")
        yield UInt32(self, "disk_total_number", "Total number of disks")


class ZipFile(Parser):
    endian = LITTLE_ENDIAN
    MIME_TYPES = {
        # Default ZIP archive
        u"application/zip": "zip",
        u"application/x-zip": "zip",

        # Java archive (JAR)
        u"application/x-jar": "jar",
        u"application/java-archive": "jar",

        # OpenOffice 1.0
        u"application/vnd.sun.xml.calc": "sxc",
        u"application/vnd.sun.xml.draw": "sxd",
        u"application/vnd.sun.xml.impress": "sxi",
        u"application/vnd.sun.xml.writer": "sxw",
        u"application/vnd.sun.xml.math": "sxm",

        # OpenOffice 1.0 (template)
        u"application/vnd.sun.xml.calc.template": "stc",
        u"application/vnd.sun.xml.draw.template": "std",
        u"application/vnd.sun.xml.impress.template": "sti",
        u"application/vnd.sun.xml.writer.template": "stw",
        u"application/vnd.sun.xml.writer.global": "sxg",

        # OpenDocument
        u"application/vnd.oasis.opendocument.chart": "odc",
        u"application/vnd.oasis.opendocument.image": "odi",
        u"application/vnd.oasis.opendocument.database": "odb",
        u"application/vnd.oasis.opendocument.formula": "odf",
        u"application/vnd.oasis.opendocument.graphics": "odg",
        u"application/vnd.oasis.opendocument.presentation": "odp",
        u"application/vnd.oasis.opendocument.spreadsheet": "ods",
        u"application/vnd.oasis.opendocument.text": "odt",
        u"application/vnd.oasis.opendocument.text-master": "odm",

        # OpenDocument (template)
        u"application/vnd.oasis.opendocument.graphics-template": "otg",
        u"application/vnd.oasis.opendocument.presentation-template": "otp",
        u"application/vnd.oasis.opendocument.spreadsheet-template": "ots",
        u"application/vnd.oasis.opendocument.text-template": "ott",
    }
    PARSER_TAGS = {
        "id": "zip",
        "category": "archive",
        "file_ext": tuple(MIME_TYPES.itervalues()),
        "mime": tuple(MIME_TYPES.iterkeys()),
        "magic": (("PK\3\4", 0),),
        "subfile": "skip",
        "min_size": (4 + 26)*8, # header + file entry
        "description": "ZIP archive"
    }

    def validate(self):
        if self["header[0]"].value != FileEntry.HEADER:
            return "Invalid magic"
        try:
            file0 = self["file[0]"]
        except HACHOIR_ERRORS, err:
            return "Unable to get file #0"
        err = file0.validate()
        if err:
            return "File #0: %s" % err
        return True

    def createFields(self):
        # File data
        self.signature = None
        self.central_directory = []
        while not self.eof:
            header = textHandler(UInt32(self, "header[]", "Header"), hexadecimal)
            yield header
            header = header.value
            if header == FileEntry.HEADER:
                yield FileEntry(self, "file[]")
            elif header == ZipDataDescriptor.HEADER:
                yield ZipDataDescriptor(self, "spanning[]")
            elif header == 0x30304b50:
                yield ZipDataDescriptor(self, "temporary_spanning[]")
            elif header == ZipCentralDirectory.HEADER:
                yield ZipCentralDirectory(self, "central_directory[]")
            elif header == ZipEndCentralDirectory.HEADER:
                yield ZipEndCentralDirectory(self, "end_central_directory", "End of central directory")
            elif header == Zip64EndCentralDirectory.HEADER:
                yield Zip64EndCentralDirectory(self, "end64_central_directory", "ZIP64 end of central directory")
            elif header == ZipSignature.HEADER:
                yield ZipSignature(self, "signature", "Signature")
            elif header == Zip64EndCentralDirectoryLocator.HEADER:
                yield Zip64EndCentralDirectoryLocator(self, "end_locator", "ZIP64 Enf of central directory locator")
            else:
                raise ParserError("Error, unknown ZIP header (0x%08X)." % header)

    def createMimeType(self):
        if self["file[0]/filename"].value == "mimetype":
            return makeUnicode(self["file[0]/data"].value)
        else:
            return u"application/zip"

    def createFilenameSuffix(self):
        if self["file[0]/filename"].value == "mimetype":
            mime = self["file[0]/compressed_data"].value
            if mime in self.MIME_TYPES:
                return "." + self.MIME_TYPES[mime]
        return ".zip"

    def createContentSize(self):
        start = 0
        end = MAX_FILESIZE * 8
        end = self.stream.searchBytes("PK\5\6", start, end)
        if end is not None:
            return end + 22*8
        return None


########NEW FILE########
__FILENAME__ = 8svx
"""
Audio Interchange File Format (AIFF) parser.

Author: Victor Stinner
Creation: 27 december 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt16, UInt32, Float80, TimestampMac32,
    RawBytes, NullBytes,
    String, Enum, PascalString32)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import filesizeHandler
from lib.hachoir_core.tools import alignValue
from lib.hachoir_parser.audio.id3 import ID3v2

CODEC_NAME = {
    'ACE2': u"ACE 2-to-1",
    'ACE8': u"ACE 8-to-3",
    'MAC3': u"MAC 3-to-1",
    'MAC6': u"MAC 6-to-1",
    'NONE': u"None",
    'sowt': u"Little-endian, no compression",
}

class Comment(FieldSet):
    def createFields(self):
        yield TimestampMac32(self, "timestamp")
        yield PascalString32(self, "text")

def parseText(self):
    yield String(self, "text", self["size"].value)

def parseID3(self):
    yield ID3v2(self, "id3v2", size=self["size"].value*8)

def parseComment(self):
    yield UInt16(self, "nb_comment")
    for index in xrange(self["nb_comment"].value):
        yield Comment(self, "comment[]")

def parseCommon(self):
    yield UInt16(self, "nb_channel")
    yield UInt32(self, "nb_sample")
    yield UInt16(self, "sample_size")
    yield Float80(self, "sample_rate")
    yield Enum(String(self, "codec", 4, strip="\0", charset="ASCII"), CODEC_NAME)

def parseVersion(self):
    yield TimestampMac32(self, "timestamp")

def parseSound(self):
    yield UInt32(self, "offset")
    yield UInt32(self, "block_size")
    size = (self.size - self.current_size) // 8
    if size:
        yield RawBytes(self, "data", size)

class Chunk(FieldSet):
    TAG_INFO = {
        'COMM': ('common', "Common chunk", parseCommon),
        'COMT': ('comment', "Comment", parseComment),
        'NAME': ('name', "Name", parseText),
        'AUTH': ('author', "Author", parseText),
        'FVER': ('version', "Version", parseVersion),
        'SSND': ('sound', "Sound data", parseSound),
        'ID3 ': ('id3', "ID3", parseID3),
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = (8 + alignValue(self["size"].value, 2)) * 8
        tag = self["type"].value
        if tag in self.TAG_INFO:
            self._name, self._description, self._parser = self.TAG_INFO[tag]
        else:
            self._parser = None

    def createFields(self):
        yield String(self, "type", 4, "Signature (FORM)", charset="ASCII")
        yield filesizeHandler(UInt32(self, "size"))
        size = self["size"].value
        if size:
            if self._parser:
                for field in self._parser(self):
                    yield field
                if size % 2:
                    yield NullBytes(self, "padding", 1)
            else:
                yield RawBytes(self, "data", size)

class HeightSVX(Parser):
    PARSER_TAGS = {
        "id": "8svx",
        "category": "audio",
        "file_ext": ("8svx",),
        "mime": (u"audio/x-aiff",),
        "min_size": 12*8,
        "description": "8SVX (audio) format"
    }
    endian = BIG_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 4) != "FORM":
            return "Invalid signature"
        if self.stream.readBytes(8*8, 4) != "8SVX":
            return "Invalid type"
        return True

    def createFields(self):
        yield String(self, "signature", 4, "Signature (FORM)", charset="ASCII")
        yield filesizeHandler(UInt32(self, "filesize"))
        yield String(self, "type", 4, "Form type (AIFF or AIFC)", charset="ASCII")
        while not self.eof:
            yield Chunk(self, "chunk[]")

    def createDescription(self):
        if self["type"].value == "AIFC":
            return "Audio Interchange File Format Compressed (AIFC)"
        else:
            return "Audio Interchange File Format (AIFF)"

    def createContentSize(self):
        return self["filesize"].value * 8


########NEW FILE########
__FILENAME__ = aiff
"""
Audio Interchange File Format (AIFF) parser.

Author: Victor Stinner
Creation: 27 december 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt16, UInt32, Float80, TimestampMac32,
    RawBytes, NullBytes,
    String, Enum, PascalString32)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import filesizeHandler
from lib.hachoir_core.tools import alignValue
from lib.hachoir_parser.audio.id3 import ID3v2

CODEC_NAME = {
    'ACE2': u"ACE 2-to-1",
    'ACE8': u"ACE 8-to-3",
    'MAC3': u"MAC 3-to-1",
    'MAC6': u"MAC 6-to-1",
    'NONE': u"None",
    'sowt': u"Little-endian, no compression",
}

class Comment(FieldSet):
    def createFields(self):
        yield TimestampMac32(self, "timestamp")
        yield PascalString32(self, "text")

def parseText(self):
    yield String(self, "text", self["size"].value)

def parseID3(self):
    yield ID3v2(self, "id3v2", size=self["size"].value*8)

def parseComment(self):
    yield UInt16(self, "nb_comment")
    for index in xrange(self["nb_comment"].value):
        yield Comment(self, "comment[]")

def parseCommon(self):
    yield UInt16(self, "nb_channel")
    yield UInt32(self, "nb_sample")
    yield UInt16(self, "sample_size")
    yield Float80(self, "sample_rate")
    yield Enum(String(self, "codec", 4, strip="\0", charset="ASCII"), CODEC_NAME)

def parseVersion(self):
    yield TimestampMac32(self, "timestamp")

def parseSound(self):
    yield UInt32(self, "offset")
    yield UInt32(self, "block_size")
    size = (self.size - self.current_size) // 8
    if size:
        yield RawBytes(self, "data", size)

class Chunk(FieldSet):
    TAG_INFO = {
        'COMM': ('common', "Common chunk", parseCommon),
        'COMT': ('comment', "Comment", parseComment),
        'NAME': ('name', "Name", parseText),
        'AUTH': ('author', "Author", parseText),
        'FVER': ('version', "Version", parseVersion),
        'SSND': ('sound', "Sound data", parseSound),
        'ID3 ': ('id3', "ID3", parseID3),
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = (8 + alignValue(self["size"].value, 2)) * 8
        tag = self["type"].value
        if tag in self.TAG_INFO:
            self._name, self._description, self._parser = self.TAG_INFO[tag]
        else:
            self._parser = None

    def createFields(self):
        yield String(self, "type", 4, "Signature (FORM)", charset="ASCII")
        yield filesizeHandler(UInt32(self, "size"))
        size = self["size"].value
        if size:
            if self._parser:
                for field in self._parser(self):
                    yield field
                if size % 2:
                    yield NullBytes(self, "padding", 1)
            else:
                yield RawBytes(self, "data", size)

class AiffFile(Parser):
    PARSER_TAGS = {
        "id": "aiff",
        "category": "audio",
        "file_ext": ("aif", "aiff", "aifc"),
        "mime": (u"audio/x-aiff",),
        "magic_regex": (("FORM.{4}AIF[CF]", 0),),
        "min_size": 12*8,
        "description": "Audio Interchange File Format (AIFF)"
    }
    endian = BIG_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 4) != "FORM":
            return "Invalid signature"
        if self.stream.readBytes(8*8, 4) not in ("AIFF", "AIFC"):
            return "Invalid type"
        return True

    def createFields(self):
        yield String(self, "signature", 4, "Signature (FORM)", charset="ASCII")
        yield filesizeHandler(UInt32(self, "filesize"))
        yield String(self, "type", 4, "Form type (AIFF or AIFC)", charset="ASCII")
        while not self.eof:
            yield Chunk(self, "chunk[]")

    def createDescription(self):
        if self["type"].value == "AIFC":
            return "Audio Interchange File Format Compressed (AIFC)"
        else:
            return "Audio Interchange File Format (AIFF)"

    def createContentSize(self):
        return self["filesize"].value * 8


########NEW FILE########
__FILENAME__ = au
"""
AU audio file parser

Author: Victor Stinner
Creation: 12 july 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import UInt32, Enum, String, RawBytes
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import displayHandler, filesizeHandler
from lib.hachoir_core.tools import createDict, humanFrequency

class AuFile(Parser):
    PARSER_TAGS = {
        "id": "sun_next_snd",
        "category": "audio",
        "file_ext": ("au", "snd"),
        "mime": (u"audio/basic",),
        "min_size": 24*8,
        "magic": ((".snd", 0),),
        "description": "Sun/NeXT audio"
    }
    endian = BIG_ENDIAN

    CODEC_INFO = {
        1: (8,    u"8-bit ISDN u-law"),
        2: (8,    u"8-bit linear PCM"),
        3: (16,   u"16-bit linear PCM"),
        4: (24,   u"24-bit linear PCM"),
        5: (32,   u"32-bit linear PCM"),
        6: (32,   u"32-bit IEEE floating point"),
        7: (64,   u"64-bit IEEE floating point"),
        8: (None, u"Fragmented sample data"),
        9: (None, u"DSP program"),
       10: (8,    u"8-bit fixed point"),
       11: (16,   u"16-bit fixed point"),
       12: (24,   u"24-bit fixed point"),
       13: (32,   u"32-bit fixed point"),
       18: (16,   u"16-bit linear with emphasis"),
       19: (16,   u"16-bit linear compressed"),
       20: (16,   u"16-bit linear with emphasis and compression"),
       21: (None, u"Music kit DSP commands"),
       23: (None, u"4-bit ISDN u-law compressed (CCITT G.721 ADPCM)"),
       24: (None, u"ITU-T G.722 ADPCM"),
       25: (None, u"ITU-T G.723 3-bit ADPCM"),
       26: (None, u"ITU-T G.723 5-bit ADPCM"),
       27: (8,    u"8-bit ISDN A-law"),
    }

    # Create bit rate and codec name dictionnaries
    BITS_PER_SAMPLE = createDict(CODEC_INFO, 0)
    CODEC_NAME = createDict(CODEC_INFO, 1)

    VALID_NB_CHANNEL = set((1,2))   # FIXME: 4, 5, 7, 8 channels are supported?

    def validate(self):
        if self.stream.readBytes(0, 4) != ".snd":
            return "Wrong file signature"
        if self["channels"].value not in self.VALID_NB_CHANNEL:
            return "Invalid number of channel"
        return True

    def getBitsPerSample(self):
        """
        Get bit rate (number of bit per sample per channel),
        may returns None if you unable to compute it.
        """
        return self.BITS_PER_SAMPLE.get(self["codec"].value)

    def createFields(self):
        yield String(self, "signature", 4, 'Format signature (".snd")', charset="ASCII")
        yield UInt32(self, "data_ofs", "Data offset")
        yield filesizeHandler(UInt32(self, "data_size", "Data size"))
        yield Enum(UInt32(self, "codec", "Audio codec"), self.CODEC_NAME)
        yield displayHandler(UInt32(self, "sample_rate", "Number of samples/second"), humanFrequency)
        yield UInt32(self, "channels", "Number of interleaved channels")

        size = self["data_ofs"].value - self.current_size // 8
        if 0 < size:
            yield String(self, "info", size, "Information", strip=" \0", charset="ISO-8859-1")

        size = min(self["data_size"].value, (self.size - self.current_size) // 8)
        yield RawBytes(self, "audio_data", size, "Audio data")

    def createContentSize(self):
        return (self["data_ofs"].value + self["data_size"].value) * 8


########NEW FILE########
__FILENAME__ = flac
"""
FLAC (audio) parser

Documentation:

 * http://flac.sourceforge.net/format.html

Author: Esteban Loiseau <baal AT tuxfamily.org>
Creation date: 2008-04-09
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import FieldSet, String, Bit, Bits, UInt16, UInt24, RawBytes, Enum, NullBytes
from lib.hachoir_core.stream import BIG_ENDIAN, LITTLE_ENDIAN
from lib.hachoir_core.tools import createDict
from lib.hachoir_parser.container.ogg import parseVorbisComment

class VorbisComment(FieldSet):
    endian = LITTLE_ENDIAN
    createFields = parseVorbisComment

class StreamInfo(FieldSet):
    static_size = 34*8
    def createFields(self):
        yield UInt16(self, "min_block_size", "The minimum block size (in samples) used in the stream")
        yield UInt16(self, "max_block_size", "The maximum block size (in samples) used in the stream")
        yield UInt24(self, "min_frame_size", "The minimum frame size (in bytes) used in the stream")
        yield UInt24(self, "max_frame_size", "The maximum frame size (in bytes) used in the stream")
        yield Bits(self, "sample_hertz", 20, "Sample rate in Hertz")
        yield Bits(self, "nb_channel", 3, "Number of channels minus one")
        yield Bits(self, "bits_per_sample", 5, "Bits per sample minus one")
        yield Bits(self, "total_samples", 36, "Total samples in stream")
        yield RawBytes(self, "md5sum", 16, "MD5 signature of the unencoded audio data")

class SeekPoint(FieldSet):
    def createFields(self):
        yield Bits(self, "sample_number", 64, "Sample number")
        yield Bits(self, "offset", 64, "Offset in bytes")
        yield Bits(self, "nb_sample", 16)

class SeekTable(FieldSet):
    def createFields(self):
        while not self.eof:
            yield SeekPoint(self, "point[]")

class MetadataBlock(FieldSet):
    "Metadata block field: http://flac.sourceforge.net/format.html#metadata_block"

    BLOCK_TYPES = {
        0: ("stream_info", u"Stream info", StreamInfo),
        1: ("padding[]", u"Padding", None),
        2: ("application[]", u"Application", None),
        3: ("seek_table", u"Seek table", SeekTable),
        4: ("comment", u"Vorbis comment", VorbisComment),
        5: ("cue_sheet[]", u"Cue sheet", None),
        6: ("picture[]", u"Picture", None),
    }
    BLOCK_TYPE_DESC = createDict(BLOCK_TYPES, 1)

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = 32 + self["metadata_length"].value * 8
        try:
            key = self["block_type"].value
            self._name, self._description, self.handler = self.BLOCK_TYPES[key]
        except KeyError:
            self.handler = None

    def createFields(self):
        yield Bit(self, "last_metadata_block", "True if this is the last metadata block")
        yield Enum(Bits(self, "block_type", 7, "Metadata block header type"), self.BLOCK_TYPE_DESC)
        yield UInt24(self, "metadata_length", "Length of following metadata in bytes (doesn't include this header)")

        block_type = self["block_type"].value
        size = self["metadata_length"].value
        if not size:
            return
        try:
            handler = self.BLOCK_TYPES[block_type][2]
        except KeyError:
            handler = None
        if handler:
            yield handler(self, "content", size=size*8)
        elif self["block_type"].value == 1:
            yield NullBytes(self, "padding", size)
        else:
            yield RawBytes(self, "rawdata", size)

class Metadata(FieldSet):
    def createFields(self):
        while not self.eof:
            field = MetadataBlock(self,"metadata_block[]")
            yield field
            if field["last_metadata_block"].value:
                break

class Frame(FieldSet):
    SAMPLE_RATES = {
        0: "get from STREAMINFO metadata block",
        1: "88.2kHz",
        2: "176.4kHz",
        3: "192kHz",
        4: "8kHz",
        5: "16kHz",
        6: "22.05kHz",
        7: "24kHz",
        8: "32kHz",
        9: "44.1kHz",
        10: "48kHz",
        11: "96kHz",
        12: "get 8 bit sample rate (in kHz) from end of header",
        13: "get 16 bit sample rate (in Hz) from end of header",
        14: "get 16 bit sample rate (in tens of Hz) from end of header",
    }

    def createFields(self):
        yield Bits(self, "sync", 14, "Sync code: 11111111111110")
        yield Bit(self, "reserved[]")
        yield Bit(self, "blocking_strategy")
        yield Bits(self, "block_size", 4)
        yield Enum(Bits(self, "sample_rate", 4), self.SAMPLE_RATES)
        yield Bits(self, "channel_assign", 4)
        yield Bits(self, "sample_size", 3)
        yield Bit(self, "reserved[]")
        # FIXME: Finish frame header parser

class Frames(FieldSet):
    def createFields(self):
        while not self.eof:
            yield Frame(self, "frame[]")
            # FIXME: Parse all frames
            return

class FlacParser(Parser):
    "Parse FLAC audio files: FLAC is a lossless audio codec"
    MAGIC = "fLaC\x00"
    PARSER_TAGS = {
        "id": "flac",
        "category": "audio",
        "file_ext": ("flac",),
        "mime": (u"audio/x-flac",),
        "magic": ((MAGIC, 0),),
        "min_size": 4*8,
        "description": "FLAC audio",
    }
    endian = BIG_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, len(self.MAGIC)) != self.MAGIC:
            return u"Invalid magic string"
        return True

    def createFields(self):
        yield String(self, "signature", 4,charset="ASCII", description="FLAC signature: fLaC string")
        yield Metadata(self,"metadata")
        yield Frames(self,"frames")


########NEW FILE########
__FILENAME__ = id3
"""
ID3 metadata parser, supported versions: 1.O, 2.2, 2.3 and 2.4

Informations: http://www.id3.org/

Author: Victor Stinner
"""

from lib.hachoir_core.field import (FieldSet, MatchError, ParserError,
    Enum, UInt8, UInt24, UInt32,
    CString, String, RawBytes,
    Bit, Bits, NullBytes, NullBits)
from lib.hachoir_core.text_handler import textHandler
from lib.hachoir_core.tools import humanDuration
from lib.hachoir_core.endian import NETWORK_ENDIAN

class ID3v1(FieldSet):
    static_size = 128 * 8
    GENRE_NAME = {
          0: u"Blues",
          1: u"Classic Rock",
          2: u"Country",
          3: u"Dance",
          4: u"Disco",
          5: u"Funk",
          6: u"Grunge",
          7: u"Hip-Hop",
          8: u"Jazz",
          9: u"Metal",
         10: u"New Age",
         11: u"Oldies",
         12: u"Other",
         13: u"Pop",
         14: u"R&B",
         15: u"Rap",
         16: u"Reggae",
         17: u"Rock",
         18: u"Techno",
         19: u"Industrial",
         20: u"Alternative",
         21: u"Ska",
         22: u"Death Metal",
         23: u"Pranks",
         24: u"Soundtrack",
         25: u"Euro-Techno",
         26: u"Ambient",
         27: u"Trip-Hop",
         28: u"Vocal",
         29: u"Jazz+Funk",
         30: u"Fusion",
         31: u"Trance",
         32: u"Classical",
         33: u"Instrumental",
         34: u"Acid",
         35: u"House",
         36: u"Game",
         37: u"Sound Clip",
         38: u"Gospel",
         39: u"Noise",
         40: u"AlternRock",
         41: u"Bass",
         42: u"Soul",
         43: u"Punk",
         44: u"Space",
         45: u"Meditative",
         46: u"Instrumental Pop",
         47: u"Instrumental Rock",
         48: u"Ethnic",
         49: u"Gothic",
         50: u"Darkwave",
         51: u"Techno-Industrial",
         52: u"Electronic",
         53: u"Pop-Folk",
         54: u"Eurodance",
         55: u"Dream",
         56: u"Southern Rock",
         57: u"Comedy",
         58: u"Cult",
         59: u"Gangsta",
         60: u"Top 40",
         61: u"Christian Rap",
         62: u"Pop/Funk",
         63: u"Jungle",
         64: u"Native American",
         65: u"Cabaret",
         66: u"New Wave",
         67: u"Psychadelic",
         68: u"Rave",
         69: u"Showtunes",
         70: u"Trailer",
         71: u"Lo-Fi",
         72: u"Tribal",
         73: u"Acid Punk",
         74: u"Acid Jazz",
         75: u"Polka",
         76: u"Retro",
         77: u"Musical",
         78: u"Rock & Roll",
         79: u"Hard Rock",
         # Following are winamp extentions
         80: u"Folk",
         81: u"Folk-Rock",
         82: u"National Folk",
         83: u"Swing",
         84: u"Fast Fusion",
         85: u"Bebob",
         86: u"Latin",
         87: u"Revival",
         88: u"Celtic",
         89: u"Bluegrass",
         90: u"Avantgarde",
         91: u"Gothic Rock",
         92: u"Progressive Rock",
         93: u"Psychedelic Rock",
         94: u"Symphonic Rock",
         95: u"Slow Rock",
         96: u"Big Band",
         97: u"Chorus",
         98: u"Easy Listening",
         99: u"Acoustic",
        100: u"Humour",
        101: u"Speech",
        102: u"Chanson",
        103: u"Opera",
        104: u"Chamber Music",
        105: u"Sonata",
        106: u"Symphony",
        107: u"Booty Bass",
        108: u"Primus",
        109: u"Porn Groove",
        110: u"Satire",
        111: u"Slow Jam",
        112: u"Club",
        113: u"Tango",
        114: u"Samba",
        115: u"Folklore",
        116: u"Ballad",
        117: u"Power Ballad",
        118: u"Rhythmic Soul",
        119: u"Freestyle",
        120: u"Duet",
        121: u"Punk Rock",
        122: u"Drum Solo",
        123: u"A capella",
        124: u"Euro-House",
        125: u"Dance Hall",
        126: u"Goa",
        127: u"Drum & Bass",
        128: u"Club-House",
        129: u"Hardcore",
        130: u"Terror",
        131: u"Indie",
        132: u"Britpop",
        133: u"Negerpunk",
        134: u"Polsk Punk",
        135: u"Beat",
        136: u"Christian Gangsta Rap",
        137: u"Heavy Metal",
        138: u"Black Metal",
        139: u"Crossover",
        140: u"Contemporary Christian",
        141: u"Christian Rock ",
        142: u"Merengue",
        143: u"Salsa",
        144: u"Trash Metal",
        145: u"Anime",
        146: u"JPop",
        147: u"Synthpop"
    }

    def createFields(self):
        yield String(self, "signature", 3, "IDv1 signature (\"TAG\")", charset="ASCII")
        if self["signature"].value != "TAG":
            raise MatchError("Stream doesn't look like ID3v1 (wrong signature)!")
        # TODO: Charset of below strings?
        yield String(self, "song", 30, "Song title", strip=" \0", charset="ISO-8859-1")
        yield String(self, "author", 30, "Author", strip=" \0", charset="ISO-8859-1")
        yield String(self, "album", 30, "Album title", strip=" \0", charset="ISO-8859-1")
        yield String(self, "year", 4, "Year", strip=" \0", charset="ISO-8859-1")

        # TODO: Write better algorithm to guess ID3v1 version
        version = self.getVersion()
        if version in ("v1.1", "v1.1b"):
            if version == "v1.1b":
                # ID3 v1.1b
                yield String(self, "comment", 29, "Comment", strip=" \0", charset="ISO-8859-1")
                yield UInt8(self, "track_nb", "Track number")
            else:
                # ID3 v1.1
                yield String(self, "comment", 30, "Comment", strip=" \0", charset="ISO-8859-1")
            yield Enum(UInt8(self, "genre", "Genre"), self.GENRE_NAME)
        else:
            # ID3 v1.0
            yield String(self, "comment", 31, "Comment", strip=" \0", charset="ISO-8859-1")

    def getVersion(self):
        addr = self.absolute_address + 126*8
        bytes = self.stream.readBytes(addr, 2)

        # last byte (127) is not space?
        if bytes[1] != ' ':
            # byte 126 is nul?
            if bytes[0] == 0x00:
                return "v1.1"
            else:
                return "v1.1b"
        else:
            return "1.0"

    def createDescription(self):
        version = self.getVersion()
        return "ID3 %s: author=%s, song=%s" % (
            version, self["author"].value, self["song"].value)

def getCharset(field):
    try:
        key = field.value
        return ID3_StringCharset.charset_name[key]
    except KeyError:
        raise ParserError("ID3v2: Invalid charset (%s)." % key)

class ID3_String(FieldSet):
    STRIP = " \0"
    def createFields(self):
        yield String(self, "text", self._size/8, "Text", charset="ISO-8859-1", strip=self.STRIP)

class ID3_StringCharset(ID3_String):
    STRIP = " \0"
    charset_desc = {
        0: "ISO-8859-1",
        1: "UTF-16 with BOM",
        2: "UTF-16 (big endian)",
        3: "UTF-8"
    }
    charset_name = {
        0: "ISO-8859-1",
        1: "UTF-16",
        2: "UTF-16-BE",
        3: "UTF-8"
    }
    def createFields(self):
        yield Enum(UInt8(self, "charset"), self.charset_desc)
        size = (self.size - self.current_size)/8
        if not size:
            return
        charset = getCharset(self["charset"])
        yield String(self, "text", size, "Text", charset=charset, strip=self.STRIP)

class ID3_GEOB(ID3_StringCharset):
    def createFields(self):
        yield Enum(UInt8(self, "charset"), self.charset_desc)
        charset = getCharset(self["charset"])
        yield CString(self, "mime", "MIME type", charset=charset)
        yield CString(self, "filename", "File name", charset=charset)
        yield CString(self, "description", "Content description", charset=charset)
        size = (self.size - self.current_size) // 8
        if not size:
            return
        yield String(self, "text", size, "Text", charset=charset)

class ID3_Comment(ID3_StringCharset):
    def createFields(self):
        yield Enum(UInt8(self, "charset"), self.charset_desc)
        yield String(self, "lang", 3, "Language", charset="ASCII")
        charset = getCharset(self["charset"])
        yield CString(self, "title", "Title", charset=charset, strip=self.STRIP)
        size = (self.size - self.current_size) // 8
        if not size:
            return
        yield String(self, "text", size, "Text", charset=charset, strip=self.STRIP)

class ID3_StringTitle(ID3_StringCharset):
    def createFields(self):
        yield Enum(UInt8(self, "charset"), self.charset_desc)
        if self.current_size == self.size:
            return
        charset = getCharset(self["charset"])
        yield CString(self, "title", "Title", charset=charset, strip=self.STRIP)
        size = (self.size - self.current_size)/8
        if not size:
            return
        yield String(self, "text", size, "Text", charset=charset, strip=self.STRIP)

class ID3_Private(FieldSet):
    def createFields(self):
        size = self._size/8
        # TODO: Strings charset?
        if self.stream.readBytes(self.absolute_address, 9) == "PeakValue":
            yield String(self, "text", 9, "Text")
            size -= 9
        yield String(self, "content", size, "Content")

class ID3_TrackLength(FieldSet):
    def createFields(self):
        yield NullBytes(self, "zero", 1)
        yield textHandler(String(self, "length", self._size/8 - 1,
            "Length in ms", charset="ASCII"), self.computeLength)

    def computeLength(self, field):
        try:
            ms = int(field.value)
            return humanDuration(ms)
        except:
            return field.value

class ID3_Picture23(FieldSet):
    pict_type_name = {
        0x00: "Other",
        0x01: "32x32 pixels 'file icon' (PNG only)",
        0x02: "Other file icon",
        0x03: "Cover (front)",
        0x04: "Cover (back)",
        0x05: "Leaflet page",
        0x06: "Media (e.g. lable side of CD)",
        0x07: "Lead artist/lead performer/soloist",
        0x08: "Artist/performer",
        0x09: "Conductor",
        0x0A: "Band/Orchestra",
        0x0B: "Composer",
        0x0C: "Lyricist/text writer",
        0x0D: "Recording Location",
        0x0E: "During recording",
        0x0F: "During performance",
        0x10: "Movie/video screen capture",
        0x11: "A bright coloured fish",
        0x12: "Illustration",
        0x13: "Band/artist logotype",
        0x14: "Publisher/Studio logotype"
    }
    def createFields(self):
        yield Enum(UInt8(self, "charset"), ID3_StringCharset.charset_desc)
        charset = getCharset(self["charset"])
        yield String(self, "img_fmt", 3, charset="ASCII")
        yield Enum(UInt8(self, "pict_type"), self.pict_type_name)
        yield CString(self, "text", "Text", charset=charset, strip=" \0")
        size = (self._size - self._current_size) / 8
        if size:
            yield RawBytes(self, "img_data", size)

class ID3_Picture24(FieldSet):
    def createFields(self):
        yield Enum(UInt8(self, "charset"), ID3_StringCharset.charset_desc)
        charset = getCharset(self["charset"])
        yield CString(self, "mime", "MIME type", charset=charset)
        yield Enum(UInt8(self, "pict_type"), ID3_Picture23.pict_type_name)
        yield CString(self, "description", charset=charset)
        size = (self._size - self._current_size) / 8
        if size:
            yield RawBytes(self, "img_data", size)

class ID3_Chunk(FieldSet):
    endian = NETWORK_ENDIAN
    tag22_name = {
        "TT2": "Track title",
        "TP1": "Artist",
        "TRK": "Track number",
        "COM": "Comment",
        "TCM": "Composer",
        "TAL": "Album",
        "TYE": "Year",
        "TEN": "Encoder",
        "TCO": "Content type",
        "PIC": "Picture"
    }
    tag23_name = {
        "COMM": "Comment",
        "GEOB": "Encapsulated object",
        "PRIV": "Private",
        "TPE1": "Artist",
        "TCOP": "Copyright",
        "TALB": "Album",
        "TENC": "Encoder",
        "TYER": "Year",
        "TSSE": "Encoder settings",
        "TCOM": "Composer",
        "TRCK": "Track number",
        "PCNT": "Play counter",
        "TCON": "Content type",
        "TLEN": "Track length",
        "TIT2": "Track title",
        "WXXX": "User defined URL"
    }
    handler = {
        "COMM": ID3_Comment,
        "COM": ID3_Comment,
        "GEOB": ID3_GEOB,
        "PIC": ID3_Picture23,
        "APIC": ID3_Picture24,
        "PRIV": ID3_Private,
        "TXXX": ID3_StringTitle,
        "WOAR": ID3_String,
        "WXXX": ID3_StringTitle,
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        if 3 <= self["../ver_major"].value:
            self._size = (10 + self["size"].value) * 8
        else:
            self._size = (self["size"].value + 6) * 8

    def createFields(self):
        if 3 <= self["../ver_major"].value:
            # ID3 v2.3 and 2.4
            yield Enum(String(self, "tag", 4, "Tag", charset="ASCII", strip="\0"), ID3_Chunk.tag23_name)
            if 4 <= self["../ver_major"].value:
                yield ID3_Size(self, "size")   # ID3 v2.4
            else:
                yield UInt32(self, "size")   # ID3 v2.3

            yield Bit(self, "tag_alter", "Tag alter preservation")
            yield Bit(self, "file_alter", "Tag alter preservation")
            yield Bit(self, "rd_only", "Read only?")
            yield NullBits(self, "padding[]", 5)

            yield Bit(self, "compressed", "Frame is compressed?")
            yield Bit(self, "encrypted", "Frame is encrypted?")
            yield Bit(self, "group", "Grouping identity")
            yield NullBits(self, "padding[]", 5)
            size = self["size"].value
            is_compressed = self["compressed"].value
        else:
            # ID3 v2.2
            yield Enum(String(self, "tag", 3, "Tag", charset="ASCII", strip="\0"), ID3_Chunk.tag22_name)
            yield UInt24(self, "size")
            size = self["size"].value - self.current_size/8 + 6
            is_compressed = False

        if size:
            cls = None
            if not(is_compressed):
                tag = self["tag"].value
                if tag in ID3_Chunk.handler:
                    cls = ID3_Chunk.handler[tag]
                elif tag[0] == "T":
                    cls = ID3_StringCharset
            if cls:
                yield cls(self, "content", "Content", size=size*8)
            else:
                yield RawBytes(self, "content", size, "Raw data content")

    def createDescription(self):
        if self["size"].value != 0:
            return "ID3 Chunk: %s" % self["tag"].display
        else:
            return "ID3 Chunk: (terminator)"

class ID3_Size(Bits):
    static_size = 32

    def __init__(self, parent, name, description=None):
        Bits.__init__(self, parent, name, 32, description)

    def createValue(self):
        data = self.parent.stream.readBytes(self.absolute_address, 4)
        # TODO: Check that bit #7 of each byte is nul: not(ord(data[i]) & 127)
        return reduce(lambda x, y: x*128 + y, (ord(item) for item in data ))

class ID3v2(FieldSet):
    endian = NETWORK_ENDIAN
    VALID_MAJOR_VERSIONS = (2, 3, 4)

    def __init__(self, parent, name, size=None):
        FieldSet.__init__(self, parent, name, size=size)
        if not self._size:
            self._size = (self["size"].value + 10) * 8

    def createDescription(self):
        return "ID3 v2.%s.%s" % \
            (self["ver_major"].value, self["ver_minor"].value)

    def createFields(self):
        # Signature + version
        yield String(self, "header", 3, "Header (ID3)", charset="ASCII")
        yield UInt8(self, "ver_major", "Version (major)")
        yield UInt8(self, "ver_minor", "Version (minor)")

        # Check format
        if self["header"].value != "ID3":
            raise MatchError("Signature error, should be \"ID3\".")
        if self["ver_major"].value not in self.VALID_MAJOR_VERSIONS \
        or self["ver_minor"].value != 0:
            raise MatchError(
                "Unknown ID3 metadata version (2.%u.%u)"
                % (self["ver_major"].value, self["ver_minor"].value))

        # Flags
        yield Bit(self, "unsync", "Unsynchronisation is used?")
        yield Bit(self, "ext", "Extended header is used?")
        yield Bit(self, "exp", "Experimental indicator")
        yield NullBits(self, "padding[]", 5)

        # Size
        yield ID3_Size(self, "size")

        # All tags
        while self.current_size < self._size:
            field = ID3_Chunk(self, "field[]")
            yield field
            if field["size"].value == 0:
                break

        # Search first byte of the MPEG file
        padding = self.seekBit(self._size)
        if padding:
            yield padding


########NEW FILE########
__FILENAME__ = itunesdb
"""
iPod iTunesDB parser.

Documentation:
- http://ipodlinux.org/ITunesDB

Author: Romain HERAULT
Creation date: 19 august 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt8, UInt16, UInt32, UInt64, TimestampMac32,
    String, Float32, NullBytes, Enum)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.tools import humanDuration
from lib.hachoir_core.text_handler import displayHandler, filesizeHandler

list_order={
        1 : "playlist order (manual sort order)",
        2 : "???",
        3 : "songtitle",
        4 : "album",
        5 : "artist",
        6 : "bitrate",
        7 : "genre",
        8 : "kind",
        9 : "date modified",
        10 : "track number",
        11 : "size",
        12 : "time",
        13 : "year",
        14 : "sample rate",
        15 : "comment",
        16 : "date added",
        17 : "equalizer",
        18 : "composer",
        19 : "???",
        20 : "play count",
        21 : "last played",
        22 : "disc number",
        23 : "my rating",
        24 : "release date",
        25 : "BPM",
        26 : "grouping",
        27 : "category",
        28 : "description",
        29 : "show",
        30 : "season",
        31 : "episode number"
    }

class DataObject(FieldSet):
    type_name={
        1:"Title",
        2:"Location",
        3:"Album",
        4:"Artist",
        5:"Genre",
        6:"Filetype",
        7:"EQ Setting",
        8:"Comment",
        9:"Category",
        12:"Composer",
        13:"Grouping",
        14:"Description text",
        15:"Podcast Enclosure URL",
        16:"Podcast RSS URL",
        17:"Chapter data",
        18:"Subtitle",
        19:"Show (for TV Shows only)",
        20:"Episode",
        21:"TV Network",
        50:"Smart Playlist Data",
        51:"Smart Playlist Rules",
        52:"Library Playlist Index",
        100:"Column info",
    }

    mhod52_sort_index_type_name={
        3:"Title",
        4:"Album, then Disk/Tracknumber, then Title",
        5:"Artist, then Album, then Disc/Tracknumber, then Title",
        7:"Genre, then Artist, then Album, then Disc/Tracknumber, then Title",
        8:"Composer, then Title"
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = self["entry_length"].value *8

    def createFields(self):
        yield String(self, "header_id", 4, "Data Object Header Markup (\"mhod\")", charset="ISO-8859-1")
        yield UInt32(self, "header_length", "Header Length")
        yield UInt32(self, "entry_length", "Entry Length")
        yield Enum(UInt32(self, "type", "type"),self.type_name)
        if(self["type"].value<15):
            yield UInt32(self, "unknown[]")
            yield UInt32(self, "unknown[]")
            yield UInt32(self, "position", "Position")
            yield UInt32(self, "length", "String Length in bytes")
            yield UInt32(self, "unknown[]")
            yield UInt32(self, "unknown[]")
            yield String(self, "string", self["length"].value, "String Data", charset="UTF-16-LE")
        elif (self["type"].value<17):
            yield UInt32(self, "unknown[]")
            yield UInt32(self, "unknown[]")
            yield String(self, "string", self._size/8-self["header_length"].value, "String Data", charset="UTF-8")
        elif (self["type"].value == 52):
            yield UInt32(self, "unknown[]", "unk1")
            yield UInt32(self, "unknown[]", "unk2")
            yield Enum(UInt32(self, "sort_index_type", "Sort Index Type"),self.mhod52_sort_index_type_name)
            yield UInt32(self, "entry_count", "Entry Count")
            indexes_size = self["entry_count"].value*4
            padding_offset = self["entry_length"].value - indexes_size
            padding = self.seekByte(padding_offset, "header padding")
            if padding:
                yield padding
            for i in xrange(self["entry_count"].value):
                yield UInt32(self, "index["+str(i)+"]", "Index of the "+str(i)+"nth mhit")
        else:
            padding = self.seekByte(self["header_length"].value, "header padding")
            if padding:
                yield padding
        padding = self.seekBit(self._size, "entry padding")
        if padding:
            yield padding

class TrackItem(FieldSet):
    x1_type_name={
        0:"AAC or CBR MP3",
        1:"VBR MP3"
    }
    x2_type_name={
        0:"AAC",
        1:"MP3"
    }
    media_type_name={
        0x00:"Audio/Video",
        0x01:"Audio",
        0x02:"Video",
        0x04:"Podcast",
        0x06:"Video Podcast",
        0x08:"Audiobook",
        0x20:"Music Video",
        0x40:"TV Show",
        0X60:"TV Show (Music lists)",
    }
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = self["entry_length"].value *8

    def createFields(self):
        yield String(self, "header_id", 4, "Track Item Header Markup (\"mhit\")", charset="ISO-8859-1")
        yield UInt32(self, "header_length", "Header Length")
        yield UInt32(self, "entry_length", "Entry Length")
        yield UInt32(self, "string_number", "Number of Strings")
        yield UInt32(self, "unique_id", "Unique ID")
        yield UInt32(self, "visible_tag", "Visible Tag")
        yield String(self, "file_type", 4, "File Type")
        yield Enum(UInt8(self, "x1_type", "Extended Type 1"),self.x1_type_name)
        yield Enum(UInt8(self, "x2_type", "Extended type 2"),self.x2_type_name)
        yield UInt8(self, "compilation_flag", "Compilation Flag")
        yield UInt8(self, "rating", "Rating")
        yield TimestampMac32(self, "added_date", "Date when the item was added")
        yield filesizeHandler(UInt32(self, "size", "Track size in bytes"))
        yield displayHandler(UInt32(self, "length", "Track length in milliseconds"), humanDuration)
        yield UInt32(self, "track_number", "Number of this track")
        yield UInt32(self, "total_track", "Total number of tracks")
        yield UInt32(self, "year", "Year of the track")
        yield UInt32(self, "bitrate", "Bitrate")
        yield UInt32(self, "samplerate", "Sample Rate")
        yield UInt32(self, "volume", "volume")
        yield UInt32(self, "start_time", "Start playing at, in milliseconds")
        yield UInt32(self, "stop_time", "Stop playing at,  in milliseconds")
        yield UInt32(self, "soundcheck", "SoundCheck preamp")
        yield UInt32(self, "playcount_1", "Play count of the track")
        yield UInt32(self, "playcount_2", "Play count of the track (identical to playcount_1)")
        yield UInt32(self, "last_played_time", "Time the song was last played")
        yield UInt32(self, "disc_number", "disc number in multi disc sets")
        yield UInt32(self, "total_discs", "Total number of discs in the disc set")
        yield UInt32(self, "userid", "User ID in the DRM scheme")
        yield TimestampMac32(self, "last_modified", "Time of the last modification of the track")
        yield UInt32(self, "bookmark_time", "Bookmark time for AudioBook")
        yield UInt64(self, "dbid", "Unique DataBase ID for the song (identical in mhit and in mhii)")
        yield UInt8(self, "checked", "song is checked")
        yield UInt8(self, "application_rating", "Last Rating before change")
        yield UInt16(self, "BPM", "BPM of the track")
        yield UInt16(self, "artwork_count", "number of artworks fo this item")
        yield UInt16(self, "unknown[]")
        yield UInt32(self, "artwork_size", "Total size of artworks in bytes")
        yield UInt32(self, "unknown[]")
        yield Float32(self, "sample_rate_2", "Sample Rate express in float")
        yield UInt32(self, "released_date", "Date of release in Music Store or in Podcast")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt8(self, "has_artwork", "0x01 for track with artwork, 0x02 otherwise")
        yield UInt8(self, "skip_wen_shuffling", "Skip that track when shuffling")
        yield UInt8(self, "remember_playback_position", "Remember playback position")
        yield UInt8(self, "flag4", "Flag 4")
        yield UInt64(self, "dbid2", "Unique DataBase ID for the song (identical as above)")
        yield UInt8(self, "lyrics_flag", "Lyrics Flag")
        yield UInt8(self, "movie_file_flag", "Movie File Flag")
        yield UInt8(self, "played_mark", "Track has been played")
        yield UInt8(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "sample_count", "Number of samples in the song (only for WAV and AAC files)")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield Enum(UInt32(self, "media_type", "Media Type for video iPod"),self.media_type_name)
        yield UInt32(self, "season_number", "Season Number")
        yield UInt32(self, "episode_number", "Episode Number")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        padding = self.seekByte(self["header_length"].value, "header padding")
        if padding:
            yield padding

        #while ((self.stream.readBytes(0, 4) == 'mhod') and  ((self.current_size/8) < self["entry_length"].value)):
        for i in xrange(self["string_number"].value):
            yield DataObject(self, "data[]")
        padding = self.seekBit(self._size, "entry padding")
        if padding:
            yield padding

class TrackList(FieldSet):
    def createFields(self):
        yield String(self, "header_id", 4, "Track List Header Markup (\"mhlt\")", charset="ISO-8859-1")
        yield UInt32(self, "header_length", "Header Length")
        yield UInt32(self, "track_number", "Number of Tracks")

        padding = self.seekByte(self["header_length"].value, "header padding")
        if padding:
            yield padding

        for i in xrange(self["track_number"].value):
            yield TrackItem(self, "track[]")

class PlaylistItem(FieldSet):
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = self["entry_length"].value *8

    def createFields(self):
        yield String(self, "header_id", 4, "Playlist Item Header Markup (\"mhip\")", charset="ISO-8859-1")
        yield UInt32(self, "header_length", "Header Length")
        yield UInt32(self, "entry_length", "Entry Length")
        yield UInt32(self, "data_object_child_count", "Number of Child Data Objects")
        yield UInt32(self, "podcast_grouping_flag", "Podcast Grouping Flag")
        yield UInt32(self, "group_id", "Group ID")
        yield UInt32(self, "track_id", "Track ID")
        yield TimestampMac32(self, "timestamp", "Song Timestamp")
        yield UInt32(self, "podcast_grouping_ref", "Podcast Grouping Reference")
        padding = self.seekByte(self["header_length"].value, "header padding")
        if padding:
            yield padding

        for i in xrange(self["data_object_child_count"].value):
            yield DataObject(self, "mhod[]")


class Playlist(FieldSet):
    is_master_pl_name={
        0:"Regular playlist",
        1:"Master playlist"
    }

    is_podcast_name={
        0:"Normal Playlist List",
        1:"Podcast Playlist List"
    }

    list_sort_order_name={
        1:"Manual Sort Order",
        2:"???",
        3:"Song Title",
        4:"Album",
        5:"Artist",
        6:"Bitrate",
        7:"Genre",
        8:"Kind",
        9:"Date Modified",
        10:"Track Number",
        11:"Size",
        12:"Time",
        13:"Year",
        14:"Sample Rate",
        15:"Comment",
        16:"Date Added",
        17:"Equalizer",
        18:"Composer",
        19:"???",
        20:"Play Count",
        21:"Last Played",
        22:"Disc Number",
        23:"My Rating",
        24:"Release Date",
        25:"BPM",
        26:"Grouping",
        27:"Category",
        28:"Description",
        29:"Show",
        30:"Season",
        31:"Episode Number"
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = self["entry_length"].value *8

    def createFields(self):
        yield String(self, "header_id", 4, "Playlist List Header Markup (\"mhyp\")", charset="ISO-8859-1")
        yield UInt32(self, "header_length", "Header Length")
        yield UInt32(self, "entry_length", "Entry Length")
        yield UInt32(self, "data_object_child_count", "Number of Child Data Objects")
        yield UInt32(self, "playlist_count", "Number of Playlist Items")
        yield Enum(UInt8(self, "type", "Normal or master playlist?"), self.is_master_pl_name)
        yield UInt8(self, "XXX1", "XXX1")
        yield UInt8(self, "XXX2", "XXX2")
        yield UInt8(self, "XXX3", "XXX3")
        yield TimestampMac32(self, "creation_date", "Date when the playlist was created")
        yield UInt64(self, "playlistid", "Persistent Playlist ID")
        yield UInt32(self, "unk3", "unk3")
        yield UInt16(self, "string_mhod_count", "Number of string MHODs for this playlist")
        yield Enum(UInt16(self, "is_podcast", "Playlist or Podcast List?"), self.is_podcast_name)
        yield Enum(UInt32(self, "sort_order", "Playlist Sort Order"), self.list_sort_order_name)

        padding = self.seekByte(self["header_length"].value, "entry padding")
        if padding:
            yield padding

        for i in xrange(self["data_object_child_count"].value):
            yield DataObject(self, "mhod[]")

        for i in xrange(self["playlist_count"].value):
            yield PlaylistItem(self, "playlist_item[]")



class PlaylistList(FieldSet):
    def createFields(self):
        yield String(self, "header_id", 4, "Playlist List Header Markup (\"mhlp\")", charset="ISO-8859-1")
        yield UInt32(self, "header_length", "Header Length")
        yield UInt32(self, "playlist_number", "Number of Playlists")

        padding = self.seekByte(self["header_length"].value, "header padding")
        if padding:
            yield padding

        for i in xrange(self["playlist_number"].value):
            yield Playlist(self, "playlist[]")

class DataSet(FieldSet):
    type_name={
        1:"Track List",
        2:"Play List",
        3:"Podcast List"
        }
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = self["entry_length"].value *8

    def createFields(self):
        yield String(self, "header_id", 4, "DataSet Header Markup (\"mhsd\")", charset="ISO-8859-1")
        yield UInt32(self, "header_length", "Header Length")
        yield UInt32(self, "entry_length", "Entry Length")
        yield Enum(UInt32(self, "type", "type"),self.type_name)
        padding = self.seekByte(self["header_length"].value, "header_raw")
        if padding:
            yield padding
        if self["type"].value == 1:
            yield TrackList(self, "tracklist[]")
        if self["type"].value == 2:
            yield PlaylistList(self, "playlist_list[]");
        if self["type"].value == 3:
            yield PlaylistList(self, "podcast_list[]");
        padding = self.seekBit(self._size, "entry padding")
        if padding:
            yield padding

class DataBase(FieldSet):
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = self["entry_length"].value *8

#    def createFields(self):

class ITunesDBFile(Parser):
    PARSER_TAGS = {
        "id": "itunesdb",
        "category": "audio",
        "min_size": 44*8,
        "magic": (('mhbd',0),),
        "description": "iPod iTunesDB file"
    }

    endian = LITTLE_ENDIAN

    def validate(self):
        return self.stream.readBytes(0, 4) == 'mhbd'

    def createFields(self):
        yield String(self, "header_id", 4, "DataBase Header Markup (\"mhbd\")", charset="ISO-8859-1")
        yield UInt32(self, "header_length", "Header Length")
        yield UInt32(self, "entry_length", "Entry Length")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "version_number", "Version Number")
        yield UInt32(self, "child_number", "Number of Children")
        yield UInt64(self, "id", "ID for this database")
        yield UInt32(self, "unknown[]")
        yield UInt64(self, "initial_dbid", "Initial DBID")
        size = self["header_length"].value-self.current_size/ 8
        if size>0:
            yield NullBytes(self, "padding", size)
        for i in xrange(self["child_number"].value):
            yield DataSet(self, "dataset[]")
        padding = self.seekByte(self["entry_length"].value, "entry padding")
        if padding:
            yield padding

    def createContentSize(self):
        return self["entry_length"].value * 8


########NEW FILE########
__FILENAME__ = midi
"""
Musical Instrument Digital Interface (MIDI) audio file parser.

Documentation:
 - Standard MIDI File Format, Dustin Caldwell (downloaded on wotsit.org)

Author: Victor Stinner
Creation: 27 december 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, Bits, ParserError,
    String, UInt32, UInt24, UInt16, UInt8, Enum, RawBytes)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.tools import createDict, humanDurationNanosec
from lib.hachoir_parser.common.tracker import NOTE_NAME

MAX_FILESIZE = 10 * 1024 * 1024

class Integer(Bits):
    def __init__(self, parent, name, description=None):
        Bits.__init__(self, parent, name, 8, description)
        stream = parent.stream
        addr = self.absolute_address
        value = 0
        while True:
            bits = stream.readBits(addr, 8, parent.endian)
            value = (value << 7) + (bits & 127)
            if not(bits & 128):
                break
            addr += 8
            self._size += 8
            if 32 < self._size:
                raise ParserError("Integer size is bigger than 32-bit")
        self.createValue = lambda: value

def parseNote(parser):
    yield Enum(UInt8(parser, "note", "Note number"), NOTE_NAME)
    yield UInt8(parser, "velocity")

def parseControl(parser):
    yield UInt8(parser, "control", "Controller number")
    yield UInt8(parser, "value", "New value")

def parsePatch(parser):
    yield UInt8(parser, "program", "New program number")

def parseChannel(parser):
    yield UInt8(parser, "channel", "Channel number")

def parsePitch(parser):
    yield UInt8(parser, "bottom", "(least sig) 7 bits of value")
    yield UInt8(parser, "top", "(most sig) 7 bits of value")

def parseText(parser, size):
    yield String(parser, "text", size)

def formatTempo(field):
    return humanDurationNanosec(field.value*1000)

def parseTempo(parser, size):
    yield textHandler(UInt24(parser, "microsec_quarter", "Microseconds per quarter note"), formatTempo)

def parseTimeSignature(parser, size):
    yield UInt8(parser, "numerator", "Numerator of time signature")
    yield UInt8(parser, "denominator", "denominator of time signature 2=quarter 3=eighth, etc.")
    yield UInt8(parser, "nb_tick", "Number of ticks in metronome click")
    yield UInt8(parser, "nb_32nd_note", "Number of 32nd notes to the quarter note")

class Command(FieldSet):
    COMMAND = {}
    for channel in xrange(16):
        COMMAND[0x80+channel] = ("Note off (channel %u)" % channel, parseNote)
        COMMAND[0x90+channel] = ("Note on (channel %u)" % channel, parseNote)
        COMMAND[0xA0+channel] = ("Key after-touch (channel %u)" % channel, parseNote)
        COMMAND[0xB0+channel] = ("Control change (channel %u)" % channel, parseControl)
        COMMAND[0xC0+channel] = ("Program (patch) change (channel %u)" % channel, parsePatch)
        COMMAND[0xD0+channel] = ("Channel after-touch (channel %u)" % channel, parseChannel)
        COMMAND[0xE0+channel] = ("Pitch wheel change (channel %u)" % channel, parsePitch)
    COMMAND_DESC = createDict(COMMAND, 0)
    COMMAND_PARSER = createDict(COMMAND, 1)

    META_COMMAND_TEXT = 1
    META_COMMAND_NAME = 3
    META_COMMAND = {
        0x00: ("Sets the track's sequence number", None),
        0x01: ("Text event", parseText),
        0x02: ("Copyright info", parseText),
        0x03: ("Sequence or Track name", parseText),
        0x04: ("Track instrument name", parseText),
        0x05: ("Lyric", parseText),
        0x06: ("Marker", parseText),
        0x07: ("Cue point", parseText),
        0x2F: ("End of the track", None),
        0x51: ("Set tempo", parseTempo),
        0x58: ("Time Signature", parseTimeSignature),
        0x59: ("Key signature", None),
        0x7F: ("Sequencer specific information", None),
    }
    META_COMMAND_DESC = createDict(META_COMMAND, 0)
    META_COMMAND_PARSER = createDict(META_COMMAND, 1)

    def createFields(self):
        yield Integer(self, "time", "Delta time in ticks")
        yield Enum(textHandler(UInt8(self, "command"), hexadecimal), self.COMMAND_DESC)
        command = self["command"].value
        if command == 0xFF:
            yield Enum(textHandler(UInt8(self, "meta_command"), hexadecimal), self.META_COMMAND_DESC)
            yield UInt8(self, "data_len")
            size = self["data_len"].value
            if size:
                command = self["meta_command"].value
                if command in self.META_COMMAND_PARSER:
                    parser = self.META_COMMAND_PARSER[command]
                else:
                    parser = None
                if parser:
                    for field in parser(self, size):
                        yield field
                else:
                    yield RawBytes(self, "data", size)
        else:
            if command not in self.COMMAND_PARSER:
                raise ParserError("Unknown command: %s" % self["command"].display)
            parser = self.COMMAND_PARSER[command]
            for field in parser(self):
                yield field

    def createDescription(self):
        if "meta_command" in self:
            return self["meta_command"].display
        else:
            return self["command"].display

class Track(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = (8 + self["size"].value) * 8

    def createFields(self):
        yield String(self, "marker", 4, "Track marker (MTrk)", charset="ASCII")
        yield UInt32(self, "size")
        if True:
            while not self.eof:
                yield Command(self, "command[]")
        else:
            size = self["size"].value
            if size:
                yield RawBytes(self, "raw", size)

    def createDescription(self):
        command = self["command[0]"]
        if "meta_command" in command \
        and command["meta_command"].value in (Command.META_COMMAND_TEXT, Command.META_COMMAND_NAME) \
        and "text" in command:
            return command["text"].value.strip("\r\n")
        else:
            return ""

class Header(FieldSet):
    static_size = 10*8
    FILE_FORMAT = {
        0: "Single track",
        1: "Multiple tracks, synchronous",
        2: "Multiple tracks, asynchronous",
    }

    def createFields(self):
        yield UInt32(self, "size")
        yield Enum(UInt16(self, "file_format"), self.FILE_FORMAT)
        yield UInt16(self, "nb_track")
        yield UInt16(self, "delta_time", "Delta-time ticks per quarter note")

    def createDescription(self):
        return "%s; %s tracks" % (
            self["file_format"].display, self["nb_track"].value)

class MidiFile(Parser):
    MAGIC = "MThd"
    PARSER_TAGS = {
        "id": "midi",
        "category": "audio",
        "file_ext": ["mid", "midi"],
        "mime": (u"audio/mime", ),
        "magic": ((MAGIC, 0),),
        "min_size": 64,
        "description": "MIDI audio"
    }
    endian = BIG_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 4) != self.MAGIC:
            return "Invalid signature"
        if self["header/size"].value != 6:
            return "Invalid header size"
        return True

    def createFields(self):
        yield String(self, "signature", 4, r"MIDI signature (MThd)", charset="ASCII")
        yield Header(self, "header")
        while not self.eof:
            yield Track(self, "track[]")

    def createDescription(self):
        return "MIDI audio: %s" % self["header"].description

    def createContentSize(self):
        count = self["/header/nb_track"].value - 1
        start = self["track[%u]" % count].absolute_address
        # Search "End of track" of last track
        end = self.stream.searchBytes("\xff\x2f\x00", start, MAX_FILESIZE*8)
        if end is not None:
            return end + 3*8
        return None


########NEW FILE########
__FILENAME__ = mod
"""
Parser of FastTrackerII Extended Module (XM) version 1.4

Documents:
- Modplug source code (file modplug/soundlib/Load_mod.cpp)
  http://sourceforge.net/projects/modplug
- Dumb source code (files include/dumb.h and src/it/readmod.c
  http://dumb.sf.net/
- Documents on "MOD" format on Wotsit
  http://www.wotsit.org

Compressed formats (i.e. starting with "PP20" or having "PACK" as type
are not handled. Also NoiseTracker's NST modules aren't handled, although
it might be possible: no file format and 15 samples

Author: Christophe GISQUET <christophe.gisquet@free.fr>
Creation: 18th February 2007
"""

from math import log10
from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    Bits, UInt16, UInt8,
    RawBytes, String, GenericVector)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler

# Old NoiseTracker 15-samples modules can have anything here.
MODULE_TYPE = {
    "M.K.": ("Noise/Pro-Tracker", 4),
    "M!K!": ("Noise/Pro-Tracker", 4),
    "M&K&": ("Noise/Pro-Tracker", 4),
    "RASP": ("StarTrekker", 4),
    "FLT4": ("StarTrekker", 4),
    "FLT8": ("StarTrekker", 8),
    "6CHN": ("FastTracker", 6),
    "8CHN": ("FastTracker", 8),
    "CD81": ("Octalyser", 8),
    "OCTA": ("Octalyser", 8),
    "FA04": ("Digital Tracker", 4),
    "FA06": ("Digital Tracker", 6),
    "FA08": ("Digital Tracker", 8),
}

def getFineTune(val):
    return ("0", "1", "2", "3", "4", "5", "6", "7", "8",
            "-8", "-7", "-6", "-5", "-4", "-3", "-2", "-1")[val.value]

def getVolume(val):
    return "%.1f dB" % (20.0*log10(val.value/64.0))

class SampleInfo(FieldSet):
    static_size = 30*8
    def createFields(self):
        yield String(self, "name", 22, strip='\0')
        yield UInt16(self, "sample_count")
        yield textHandler(UInt8(self, "fine_tune"), getFineTune)
        yield textHandler(UInt8(self, "volume"), getVolume)
        yield UInt16(self, "loop_start", "Loop start offset in samples")
        yield UInt16(self, "loop_len", "Loop length in samples")

    def createValue(self):
        return self["name"].value

class Header(FieldSet):
    static_size = 1084*8

    def createFields(self):
        yield String(self, "name", 20, strip='\0')
        yield GenericVector(self, "samples", 31, SampleInfo, "info")
        yield UInt8(self, "length")
        yield UInt8(self, "played_patterns_count")
        yield GenericVector(self, "patterns", 128, UInt8, "position")
        yield String(self, "type", 4)

    def getNumChannels(self):
        return MODULE_TYPE[self["type"].value][1]

class Note(FieldSet):
    static_size = 8*4
    def createFields(self):
        yield Bits(self, 4, "note_hi_nibble")
        yield Bits(self, 12, "period")
        yield Bits(self, 4, "note_low_nibble")
        yield Bits(self, 4, "effect")
        yield UInt8(self, "parameter")

class Row(FieldSet):
    def __init__(self, parent, name, channels, desc=None):
        FieldSet.__init__(self, parent, name, description=desc)
        self.channels = channels
        self._size = 8*self.channels*4

    def createFields(self):
        for index in xrange(self.channels):
            yield Note(self, "note[]")

class Pattern(FieldSet):
    def __init__(self, parent, name, channels, desc=None):
        FieldSet.__init__(self, parent, name, description=desc)
        self.channels = channels
        self._size = 64*8*self.channels*4

    def createFields(self):
        for index in xrange(64):
            yield Row(self, "row[]", self.channels)

class AmigaModule(Parser):
    PARSER_TAGS = {
        "id": "mod",
        "category": "audio",
        "file_ext": ("mod", "nst", "wow", "oct", "sd0" ),
        "mime": (u'audio/mod', u'audio/x-mod', u'audio/mod', u'audio/x-mod'),
        "min_size": 1084*8,
        "description": "Uncompressed amiga module"
    }
    endian = BIG_ENDIAN

    def validate(self):
        t = self.stream.readBytes(1080*8, 4)
        if t not in MODULE_TYPE:
            return "Invalid module type '%s'" % t
        self.createValue = lambda t: "%s module, %u channels" % MODULE_TYPE[t]
        return True

    def createFields(self):
        header = Header(self, "header")
        yield header
        channels = header.getNumChannels()

        # Number of patterns
        patterns = 0
        for index in xrange(128):
            patterns = max(patterns,
                           header["patterns/position[%u]" % index].value)
        patterns += 1

        # Yield patterns
        for index in xrange(patterns):
            yield Pattern(self, "pattern[]", channels)

        # Yield samples
        for index in xrange(31):
            count = header["samples/info[%u]/sample_count" % index].value
            if count:
                self.info("Yielding sample %u: %u samples" % (index, count))
                yield RawBytes(self, "sample_data[]", 2*count, \
                               "Sample %u" % index)


########NEW FILE########
__FILENAME__ = modplug
"""
Modplug metadata inserted into module files.

Doc:
- http://modplug.svn.sourceforge.net/viewvc/modplug/trunk/modplug/soundlib/

Author: Christophe GISQUET <christophe.gisquet@free.fr>
Creation: 10th February 2007
"""

from lib.hachoir_core.field import (FieldSet,
    UInt32, UInt16, UInt8, Int8, Float32,
    RawBytes, String, GenericVector, ParserError)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal

MAX_ENVPOINTS = 32

def parseComments(parser):
    size = parser["block_size"].value
    if size > 0:
        yield String(parser, "comment", size)

class MidiOut(FieldSet):
    static_size = 9*32*8
    def createFields(self):
        for name in ("start", "stop", "tick", "noteon", "noteoff",
                     "volume", "pan", "banksel", "program"):
            yield String(self, name, 32, strip='\0')

class Command(FieldSet):
    static_size = 32*8
    def createFields(self):
        start = self.absolute_address
        size = self.stream.searchBytesLength("\0", False, start)
        if size > 0:
            self.info("Command: %s" % self.stream.readBytes(start, size))
            yield String(self, "command", size, strip='\0')
        yield RawBytes(self, "parameter", (self._size//8)-size)

class MidiSFXExt(FieldSet):
    static_size = 16*32*8
    def createFields(self):
        for index in xrange(16):
            yield Command(self, "command[]")

class MidiZXXExt(FieldSet):
    static_size = 128*32*8
    def createFields(self):
        for index in xrange(128):
            yield Command(self, "command[]")

def parseMidiConfig(parser):
    yield MidiOut(parser, "midi_out")
    yield MidiSFXExt(parser, "sfx_ext")
    yield MidiZXXExt(parser, "zxx_ext")

def parseChannelSettings(parser):
    size = parser["block_size"].value//4
    if size > 0:
        yield GenericVector(parser, "settings", size, UInt32, "mix_plugin")

def parseEQBands(parser):
    size = parser["block_size"].value//4
    if size > 0:
        yield GenericVector(parser, "gains", size, UInt32, "band")

class SoundMixPluginInfo(FieldSet):
    static_size = 128*8
    def createFields(self):
        yield textHandler(UInt32(self, "plugin_id1"), hexadecimal)
        yield textHandler(UInt32(self, "plugin_id2"), hexadecimal)
        yield UInt32(self, "input_routing")
        yield UInt32(self, "output_routing")
        yield GenericVector(self, "routing_info", 4, UInt32, "reserved")
        yield String(self, "name", 32, strip='\0')
        yield String(self, "dll_name", 64, desc="Original DLL name", strip='\0')

class ExtraData(FieldSet):
    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self._size = (4+self["size"].value)*8

    def createFields(self):
        yield UInt32(self, "size")
        size = self["size"].value
        if size:
            yield RawBytes(self, "data", size)

class XPlugData(FieldSet):
    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self._size = (4+self["size"].value)*8

    def createFields(self):
        yield UInt32(self, "size")
        while not self.eof:
            yield UInt32(self, "marker")
            if self["marker"].value == 'DWRT':
                yield Float32(self, "dry_ratio")
            elif self["marker"].value == 'PORG':
                yield UInt32(self, "default_program")

def parsePlugin(parser):
    yield SoundMixPluginInfo(parser, "info")

    # Check if VST setchunk present
    size = parser.stream.readBits(parser.absolute_address+parser.current_size, 32, LITTLE_ENDIAN)
    if 0 < size < parser.current_size + parser._size:
        yield ExtraData(parser, "extra_data")

    # Check if XPlugData is present
    size = parser.stream.readBits(parser.absolute_address+parser.current_size, 32, LITTLE_ENDIAN)
    if 0 < size < parser.current_size + parser._size:
        yield XPlugData(parser, "xplug_data")

# Format: "XXXX": (type, count, name)
EXTENSIONS = {
    # WriteInstrumentHeaderStruct@Sndfile.cpp
    "XTPM": {
         "..Fd": (UInt32, 1, "Flags"),
         "..OF": (UInt32, 1, "Fade out"),
         "..VG": (UInt32, 1, "Global Volume"),
         "...P": (UInt32, 1, "Panning"),
         "..EV": (UInt32, 1, "Volume Envelope"),
         "..EP": (UInt32, 1, "Panning Envelope"),
         ".EiP": (UInt32, 1, "Pitch Envelope"),
         ".SLV": (UInt8, 1, "Volume Loop Start"),
         ".ELV": (UInt8, 1, "Volume Loop End"),
         ".BSV": (UInt8, 1, "Volume Sustain Begin"),
         ".ESV": (UInt8, 1, "Volume Sustain End"),
         ".SLP": (UInt8, 1, "Panning Loop Start"),
         ".ELP": (UInt8, 1, "Panning Loop End"),
         ".BSP": (UInt8, 1, "Panning Substain Begin"),
         ".ESP": (UInt8, 1, "Padding Substain End"),
         "SLiP": (UInt8, 1, "Pitch Loop Start"),
         "ELiP": (UInt8, 1, "Pitch Loop End"),
         "BSiP": (UInt8, 1, "Pitch Substain Begin"),
         "ESiP": (UInt8, 1, "Pitch Substain End"),
         ".ANN": (UInt8, 1, "NNA"),
         ".TCD": (UInt8, 1, "DCT"),
         ".AND": (UInt8, 1, "DNA"),
         "..SP": (UInt8, 1, "Panning Swing"),
         "..SV": (UInt8, 1, "Volume Swing"),
         ".CFI": (UInt8, 1, "IFC"),
         ".RFI": (UInt8, 1, "IFR"),
         "..BM": (UInt32, 1, "Midi Bank"),
         "..PM": (UInt8, 1, "Midi Program"),
         "..CM": (UInt8, 1, "Midi Channel"),
         ".KDM": (UInt8, 1, "Midi Drum Key"),
         ".SPP": (Int8, 1, "PPS"),
         ".CPP": (UInt8, 1, "PPC"),
         ".[PV": (UInt32, MAX_ENVPOINTS, "Volume Points"),
         ".[PP": (UInt32, MAX_ENVPOINTS, "Panning Points"),
         "[PiP": (UInt32, MAX_ENVPOINTS, "Pitch Points"),
         ".[EV": (UInt8, MAX_ENVPOINTS, "Volume Enveloppe"),
         ".[EP": (UInt8, MAX_ENVPOINTS, "Panning Enveloppe"),
         "[EiP": (UInt8, MAX_ENVPOINTS, "Pitch Enveloppe"),
         ".[MN": (UInt8, 128, "Note Mapping"),
         "..[K": (UInt32, 128, "Keyboard"),
         "..[n": (String, 32, "Name"),
         ".[nf": (String, 12, "Filename"),
         ".PiM": (UInt8, 1, "MixPlug"),
         "..RV": (UInt16, 1, "Volume Ramping"),
         "...R": (UInt16, 1, "Resampling"),
         "..SC": (UInt8, 1, "Cut Swing"),
         "..SR": (UInt8, 1, "Res Swing"),
         "..MF": (UInt8, 1, "Filter Mode"),
    },

    # See after "CODE tag dictionary", same place, elements with [EXT]
    "STPM": {
         "...C": (UInt32, 1, "Channels"),
         ".VWC": (None, 0, "CreatedWith version"),
         ".VGD": (None, 0, "Default global volume"),
         "..TD": (None, 0, "Default tempo"),
         "HIBE": (None, 0, "Embedded instrument header"),
         "VWSL": (None, 0, "LastSavedWith version"),
         ".MMP": (None, 0, "Plugin Mix mode"),
         ".BPR": (None, 0, "Rows per beat"),
         ".MPR": (None, 0, "Rows per measure"),
         "@PES": (None, 0, "Chunk separator"),
         ".APS": (None, 0, "Song Pre-amplification"),
         "..MT": (None, 0, "Tempo mode"),
         "VTSV": (None, 0, "VSTi volume"),
    }
}

class MPField(FieldSet):
    def __init__(self, parent, name, ext, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self.ext = ext
        self.info(self.createDescription())
        self._size = (6+self["data_size"].value)*8

    def createFields(self):
        # Identify tag
        code = self.stream.readBytes(self.absolute_address, 4)
        if code in self.ext:
            cls, count, comment = self.ext[code]
        else:
            cls, count, comment = RawBytes, 1, "Unknown tag"

        # Header
        yield String(self, "code", 4, comment)
        yield UInt16(self, "data_size")

        # Data
        if not cls:
            size = self["data_size"].value
            if size > 0:
                yield RawBytes(self, "data", size)
        elif cls in (String, RawBytes):
            yield cls(self, "value", count)
        else:
            if count > 1:
                yield GenericVector(self, "values", count, cls, "item")
            else:
                yield cls(self, "value")

    def createDescription(self):
        return "Element '%s', size %i" % \
               (self["code"]._description, self["data_size"].value)

def parseFields(parser):
    # Determine field names
    ext = EXTENSIONS[parser["block_type"].value]
    if ext == None:
        raise ParserError("Unknown parent '%s'" % parser["block_type"].value)

    # Parse fields
    addr = parser.absolute_address + parser.current_size
    while not parser.eof and parser.stream.readBytes(addr, 4) in ext:
        field = MPField(parser, "field[]", ext)
        yield field
        addr += field._size

    # Abort on unknown codes
    parser.info("End of extension '%s' when finding '%s'" %
           (parser["block_type"].value, parser.stream.readBytes(addr, 4)))

class ModplugBlock(FieldSet):
    BLOCK_INFO = {
        "TEXT": ("comment", True, "Comment", parseComments),
        "MIDI": ("midi_config", True, "Midi configuration", parseMidiConfig),
        "XFHC": ("channel_settings", True, "Channel settings", parseChannelSettings),
        "XTPM": ("instrument_ext", False, "Instrument extensions", parseFields),
        "STPM": ("song_ext", False, "Song extensions", parseFields),
    }
    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self.parseBlock = parsePlugin

        t = self["block_type"].value
        self.has_size = False
        if t in self.BLOCK_INFO:
            self._name, self.has_size, desc, parseBlock = self.BLOCK_INFO[t]
            if callable(desc):
                self.createDescription = lambda: desc(self)
            if parseBlock:
                self.parseBlock = lambda: parseBlock(self)

        if self.has_size:
            self._size = 8*(self["block_size"].value + 8)

    def createFields(self):
        yield String(self, "block_type", 4)
        if self.has_size:
            yield UInt32(self, "block_size")

        if self.parseBlock:
            for field in self.parseBlock():
                yield field

        if self.has_size:
            size = self["block_size"].value - (self.current_size//8)
            if size > 0:
                yield RawBytes(self, "data", size, "Unknown data")

def ParseModplugMetadata(parser):
    while not parser.eof:
        block = ModplugBlock(parser, "block[]")
        yield block
        if block["block_type"].value == "STPM":
            break

    # More undocumented stuff: date ?
    size = (parser._size - parser.absolute_address - parser.current_size)//8
    if size > 0:
        yield RawBytes(parser, "info", size)


########NEW FILE########
__FILENAME__ = mpeg_audio
"""
MPEG audio file parser.

Creation: 12 decembre 2005
Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    MissingField, ParserError, createOrphanField,
    Bit, Bits, Enum,
    PaddingBits, PaddingBytes,
    RawBytes)
from lib.hachoir_parser.audio.id3 import ID3v1, ID3v2
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.tools import humanFrequency, humanBitSize
from lib.hachoir_core.bits import long2raw
from lib.hachoir_core.error import HACHOIR_ERRORS
from lib.hachoir_core.stream import InputStreamError

# Max MP3 filesize: 200 MB
MAX_FILESIZE = 200*1024*1024*8

class Frame(FieldSet):
    VERSION_NAME = { 0: "2.5", 2: "2", 3: "1" }
    MPEG_I = 3
    MPEG_II = 2
    MPEG_II_5 = 0

    LAYER_NAME = { 1: "III", 2: "II", 3: "I" }
    LAYER_I = 3
    LAYER_II = 2
    LAYER_III = 1

    # Bit rates (bit_rate * 1000 = bits/sec)
    # key 15 is always invalid
    BIT_RATES = {
        1: ( # MPEG1
            ( 0, 32,  64,  96, 128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448 ), # layer I
            ( 0, 32,  48,  56,  64,  80,  96, 112, 128, 160, 192, 224, 256, 320, 384 ), # layer II
            ( 0, 32,  40,  48,  56,  64,  80,  96, 112, 128, 160, 192, 224, 256, 320 ), # layer III
            # -   1    2    3    4    5    6    7    8    9   10   11   12   13   14 -
        ),
        2: ( # MPEG2 / MPEG2.5
            ( 0, 32,  48,  56,  64,  80,  96, 112, 128, 144, 160, 176, 192, 224, 256 ), # layer I
            ( 0,  8,  16,  24,  32,  40,  48,  56,  64,  80,  96, 112, 128, 144, 160 ), # layer II
            ( 0,  8,  16,  24,  32,  40,  48,  56,  64,  80,  96, 112, 128, 144, 160 ), # layer III
            # -   1    2    3    4    5    6    7    8    9   10   11   12   13   14 -
        )
    }
    SAMPLING_RATES = {
        3: {0: 44100, 1: 48000, 2: 32000},  # MPEG1
        2: {0: 22050, 1: 24000, 2: 16000},  # MPEG2
        0: {0: 11025, 1: 12000, 2: 8000}    # MPEG2.5
    }
    EMPHASIS_NAME = {0: "none", 1: "50/15 ms",  3: "CCIT J.17"}
    CHANNEL_MODE_NAME = {
        0: "Stereo",
        1: "Joint stereo",
        2: "Dual channel",
        3: "Single channel"
    }
    # Channel mode => number of channels
    NB_CHANNEL = {
        0: 2,
        1: 2,
        2: 2,
        3: 1,
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        if not self._size:
            frame_size = self.getFrameSize()
            if not frame_size:
                raise ParserError("MPEG audio: Invalid frame %s" % self.path)
            self._size = min(frame_size * 8, self.parent.size - self.address)

    def createFields(self):
        # Header
        yield PaddingBits(self, "sync", 11, "Synchronize bits (set to 1)", pattern=1)
        yield Enum(Bits(self, "version", 2, "MPEG audio version"), self.VERSION_NAME)
        yield Enum(Bits(self, "layer", 2, "MPEG audio layer"), self.LAYER_NAME)
        yield Bit(self, "crc16", "No CRC16 protection?")

        # Rates and padding
        yield Bits(self, "bit_rate", 4, "Bit rate")
        yield Bits(self, "sampling_rate", 2, "Sampling rate")
        yield Bit(self, "use_padding", "Stream field use padding?")
        yield Bit(self, "extension", "Extension")

        # Channel mode, mode extension, copyright, ...
        yield Enum(Bits(self, "channel_mode", 2, "Channel mode"), self.CHANNEL_MODE_NAME)
        yield Bits(self, "mode_ext", 2, "Mode extension")
        yield Bit(self, "copyright", "Is copyrighted?")
        yield Bit(self, "original", "Is original?")
        yield Enum(Bits(self, "emphasis", 2, "Emphasis"), self.EMPHASIS_NAME)

        size = (self.size - self.current_size) / 8
        if size:
            yield RawBytes(self, "data", size)

    def isValid(self):
        return (self["layer"].value != 0
            and self["sync"].value == 2047
            and self["version"].value != 1
            and self["sampling_rate"].value != 3
            and self["bit_rate"].value not in (0, 15)
            and self["emphasis"].value != 2)

    def getSampleRate(self):
        """
        Read sampling rate. Returns None on error.
        """
        version = self["version"].value
        rate = self["sampling_rate"].value
        try:
            return self.SAMPLING_RATES[version][rate]
        except (KeyError, IndexError):
            return None

    def getBitRate(self):
        """
        Read bit rate in bit/sec. Returns None on error.
        """
        layer = 3 - self["layer"].value
        bit_rate = self["bit_rate"].value
        if bit_rate in (0, 15):
            return None
        if self["version"].value == 3:
            dataset = self.BIT_RATES[1] # MPEG1
        else:
            dataset = self.BIT_RATES[2] # MPEG2 / MPEG2.5
        try:
            return dataset[layer][bit_rate] * 1000
        except (KeyError, IndexError):
            return None

    def getFrameSize(self):
        """
        Read frame size in bytes. Returns None on error.
        """
        frame_size = self.getBitRate()
        if not frame_size:
            return None
        sample_rate = self.getSampleRate()
        if not sample_rate:
            return None
        padding = int(self["use_padding"].value)

        if self["layer"].value == self.LAYER_III:
            if self["version"].value == self.MPEG_I:
                return (frame_size * 144) // sample_rate + padding
            else:
                return (frame_size * 72)  // sample_rate + padding
        elif self["layer"].value == self.LAYER_II:
            return (frame_size * 144) / sample_rate + padding
        else: # self.LAYER_I:
            frame_size = (frame_size * 12) / sample_rate
            return (frame_size + padding) * 4

    def getNbChannel(self):
        return self.NB_CHANNEL[ self["channel_mode"].value ]

    def createDescription(self):
        info = ["layer %s" % self["layer"].display]
        bit_rate = self.getBitRate()
        if bit_rate:
            info.append("%s/sec" % humanBitSize(bit_rate))
        sampling_rate = self.getSampleRate()
        if sampling_rate:
            info.append(humanFrequency(sampling_rate))
        return "MPEG-%s %s" % (self["version"].display, ", ".join(info))

def findSynchronizeBits(parser, start, max_size):
    """
    Find synchronisation bits (11 bits set to 1)

    Returns None on error, or number of bytes before the synchronization.
    """
    address0 = parser.absolute_address
    end = start + max_size
    size = 0
    while start < end:
        # Fast search: search 0xFF (first byte of sync frame field)
        length = parser.stream.searchBytesLength("\xff", False, start, end)
        if length is None:
            return None
        size += length
        start += length * 8

        # Strong validation of frame: create the frame
        # and call method isValid()
        try:
            frame = createOrphanField(parser, start-address0, Frame, "frame")
            valid = frame.isValid()
        except HACHOIR_ERRORS:
            valid = False
        if valid:
            return size

        # Invalid frame: continue
        start += 8
        size += 1
    return None

class Frames(FieldSet):
    # Padding bytes allowed before a frame
    MAX_PADDING = 256

    def synchronize(self):
        addr = self.absolute_address
        start = addr + self.current_size
        end = min(start + self.MAX_PADDING*8, addr + self.size)
        padding = findSynchronizeBits(self, start, end)
        if padding is None:
            raise ParserError("MPEG audio: Unable to find synchronization bits")
        if padding:
            return PaddingBytes(self, "padding[]", padding, "Padding before synchronization")
        else:
            return None

    def looksConstantBitRate(self, count=10):
        """
        Guess if frames are constant bit rate. If it returns False, you can
        be sure that frames are variable bit rate. Otherwise, it looks like
        constant bit rate (on first count fields).
        """
        check_keys = ("version", "layer", "bit_rate")
        last_field = None
        for index, field in enumerate(self.array("frame")):
            if last_field:
                for key in check_keys:
                    if field[key].value != last_field[key].value:
                        return False
            last_field = field
            if index == count:
                break
        return True

    def createFields(self):
        # Find synchronisation bytes
        padding = self.synchronize()
        if padding:
            yield padding

        while self.current_size < self.size:
            yield Frame(self, "frame[]")
#            padding = self.synchronize()
#            if padding:
#                yield padding

        # Read raw bytes at the end (if any)
        size = (self.size - self.current_size) / 8
        if size:
            yield RawBytes(self, "raw", size)

    def createDescription(self):
        if self.looksConstantBitRate():
            text = "(looks like) Constant bit rate (CBR)"
        else:
            text = "Variable bit rate (VBR)"
        return "Frames: %s" % text

def createMpegAudioMagic():

    # ID3v1 magic
    magics = [("TAG", 0)]

    # ID3v2 magics
    for ver_major in ID3v2.VALID_MAJOR_VERSIONS:
       magic = "ID3%c\x00" % ver_major
       magics.append( (magic,0) )

    # MPEG frame magic
    # TODO: Use longer magic: 32 bits instead of 16 bits
    SYNC_BITS = 2047
    for version in Frame.VERSION_NAME.iterkeys():
        for layer in Frame.LAYER_NAME.iterkeys():
            for crc16 in (0, 1):
                magic = (SYNC_BITS << 5) | (version << 3) | (layer << 1) | crc16
                magic = long2raw(magic, BIG_ENDIAN, 2)
                magics.append( (magic, 0) )
    return magics

class MpegAudioFile(Parser):
    PARSER_TAGS = {
        "id": "mpeg_audio",
        "category": "audio",
        "file_ext": ("mpa", "mp1", "mp2", "mp3"),
        "mime": (u"audio/mpeg",),
        "min_size": 4*8,
#        "magic": createMpegAudioMagic(),
        "description": "MPEG audio version 1, 2, 2.5",
        "subfile": "skip",
    }
    endian = BIG_ENDIAN

    def validate(self):
        if self[0].name in ("id3v2", "id3v1"):
            return True

        if not self.stream.checked: # TODO: is it possible to handle piped input?
            return False

        # Validate first 5 frames
        for index in xrange(5):
            try:
                frame = self["frames/frame[%u]" % index]
            except MissingField:
                # Require a least one valid frame
                if (1 <= index) \
                and self["frames"].done:
                    return True
                return "Unable to get frame #%u" % index
            except (InputStreamError, ParserError):
                return "Unable to create frame #%u" % index

            # Check first frame values
            if not frame.isValid():
                return "Frame #%u is invalid" % index

            # Check that all frames are similar
            if not index:
                frame0 = frame
            else:
                if frame0["channel_mode"].value != frame["channel_mode"].value:
                    return "Frame #%u channel mode is different" % index
        return True

    def createFields(self):
        # Read ID3v2 (if any)
        if self.stream.readBytes(0, 3) == "ID3":
            yield ID3v2(self, "id3v2")

        if self._size is None: # TODO: is it possible to handle piped input?
            raise NotImplementedError

        # Check if file is ending with ID3v1 or not and compute frames size
        frames_size = self.size - self.current_size
        addr = self.size - 128*8
        if 0 <= addr:
            has_id3 = (self.stream.readBytes(addr, 3) == "TAG")
            if has_id3:
                frames_size -= 128*8
        else:
            has_id3 = False

        # Read frames (if any)
        if frames_size:
            yield Frames(self, "frames", size=frames_size)

        # Read ID3v1 (if any)
        if has_id3:
            yield ID3v1(self, "id3v1")

    def createDescription(self):
        if "frames" in self:
            frame = self["frames/frame[0]"]
            return "%s, %s" % (frame.description, frame["channel_mode"].display)
        elif "id3v2" in self:
            return self["id3v2"].description
        elif "id3v1" in self:
            return self["id3v1"].description
        else:
            return "MPEG audio"

    def createContentSize(self):
        # Get "frames" field
        field = self[0]
        if field.name != "frames":
            try:
                field = self[1]
            except MissingField:
                # File only contains ID3v1 or ID3v2
                return field.size

            # Error: second field are not the frames"?
            if field.name != "frames":
                return None

        # Go to last frame
        frames = field
        frame = frames["frame[0]"]
        address0 = field.absolute_address
        size = address0 + frame.size
        while True:
            try:
                # Parse one MPEG audio frame
                frame = createOrphanField(frames, size - address0, Frame, "frame")

                # Check frame 32 bits header
                if not frame.isValid():
                    break
            except HACHOIR_ERRORS:
                break
            if MAX_FILESIZE < (size + frame.size):
                break
            size += frame.size

        # ID3v1 at the end?
        try:
            if self.stream.readBytes(size, 3) == "TAG":
                size += ID3v1.static_size
        except InputStreamError:
            pass
        return size


########NEW FILE########
__FILENAME__ = real_audio
"""
RealAudio (.ra) parser

Author: Mike Melanson
References:
  http://wiki.multimedia.cx/index.php?title=RealMedia
Samples:
  http://samples.mplayerhq.hu/real/RA/
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt8, UInt16, UInt32,
    Bytes, RawBytes, String,
    PascalString8)
from lib.hachoir_core.tools import humanFrequency
from lib.hachoir_core.text_handler import displayHandler
from lib.hachoir_core.endian import BIG_ENDIAN

class Metadata(FieldSet):
    def createFields(self):
        yield PascalString8(self, "title", charset="ISO-8859-1")
        yield PascalString8(self, "author", charset="ISO-8859-1")
        yield PascalString8(self, "copyright", charset="ISO-8859-1")
        yield PascalString8(self, "comment", charset="ISO-8859-1")

class RealAudioFile(Parser):
    MAGIC = ".ra\xFD"
    PARSER_TAGS = {
        "id": "real_audio",
        "category": "audio",
        "file_ext": ["ra"],
        "mime": (u"audio/x-realaudio", u"audio/x-pn-realaudio"),
        "min_size": 6*8,
        "magic": ((MAGIC, 0),),
        "description": u"Real audio (.ra)",
    }
    endian = BIG_ENDIAN

    def validate(self):
        if self["signature"].value != self.MAGIC:
            return "Invalid signature"
        if self["version"].value not in (3, 4):
            return "Unknown version"
        return True

    def createFields(self):
        yield Bytes(self, "signature", 4, r"RealAudio identifier ('.ra\xFD')")
        yield UInt16(self, "version", "Version")
        if self["version"].value == 3:
            yield UInt16(self, "header_size", "Header size")
            yield RawBytes(self, "Unknown1", 10)
            yield UInt32(self, "data_size", "Data size")
            yield Metadata(self, "metadata")
            yield UInt8(self, "Unknown2")
            yield PascalString8(self, "FourCC")
            audio_size = self["data_size"].value
        else: # version = 4
            yield UInt16(self, "reserved1", "Reserved, should be 0")
            yield String(self, "ra4sig", 4, "'.ra4' signature")
            yield UInt32(self, "filesize", "File size (minus 40 bytes)")
            yield UInt16(self, "version2", "Version 2 (always equal to version)")
            yield UInt32(self, "headersize", "Header size (minus 16)")
            yield UInt16(self, "codec_flavor", "Codec flavor")
            yield UInt32(self, "coded_frame_size", "Coded frame size")
            yield RawBytes(self, "unknown1", 12)
            yield UInt16(self, "subpacketh", "Subpacket h (?)")
            yield UInt16(self, "frame_size", "Frame size")
            yield UInt16(self, "sub_packet_size", "Subpacket size")
            yield UInt16(self, "unknown2", "Unknown")
            yield displayHandler(UInt16(self, "sample_rate", "Sample rate"), humanFrequency)
            yield UInt16(self, "unknown3", "Unknown")
            yield UInt16(self, "sample_size", "Sample size")
            yield UInt16(self, "channels", "Channels")
            yield PascalString8(self, "Interleaving ID String")
            yield PascalString8(self, "FourCC")
            yield RawBytes(self, "unknown4", 3)
            yield Metadata(self, "metadata")
            audio_size = (self["filesize"].value + 40) - (self["headersize"].value + 16)
        if 0 < audio_size:
            yield RawBytes(self, "audio_data", audio_size)

    def createDescription(self):
        if (self["version"].value == 3):
            return "RealAudio v3 file, '%s' codec" % self["FourCC"].value
        elif (self["version"].value == 4):
            return "RealAudio v4 file, '%s' codec, %s, %u channels" % (
                self["FourCC"].value, self["sample_rate"].display, self["channels"].value)
        else:
            return "Real audio"

########NEW FILE########
__FILENAME__ = s3m
"""
The ScreamTracker 3.0x module format description for .s3m files.

Documents:
- Search s3m on Wotsit
  http://www.wotsit.org/

Author: Christophe GISQUET <christophe.gisquet@free.fr>
Creation: 11th February 2007
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (StaticFieldSet, FieldSet, Field,
    Bit, Bits,
    UInt32, UInt16, UInt8, Enum,
    PaddingBytes, RawBytes, NullBytes,
    String, GenericVector, ParserError)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.tools import alignValue

class Chunk:
    def __init__(self, cls, name, offset, size, *args):
        # Todo: swap and have None=unknown instead of now: 0=unknown
        assert size != None and size>=0
        self.cls = cls
        self.name = name
        self.offset = offset
        self.size = size
        self.args = args

class ChunkIndexer:
    def __init__(self):
        self.chunks = [ ]

    # Check if a chunk fits
    def canHouse(self, chunk, index):
        if index > 1:
            if chunk.offset + chunk.size > self.chunks[index-1].offset:
                return False
        # We could test now that it fits in the memory
        return True

    # Farthest element is last
    def addChunk(self, new_chunk):
        index = 0
        # Find first chunk whose value is bigger
        while index < len(self.chunks):
            offset = self.chunks[index].offset
            if offset < new_chunk.offset:
                if not self.canHouse(new_chunk, index):
                    raise ParserError("Chunk '%s' doesn't fit!" % new_chunk.name)
                self.chunks.insert(index, new_chunk)
                return
            index += 1

        # Not found or empty
        # We could at least check that it fits in the memory
        self.chunks.append(new_chunk)

    def yieldChunks(self, obj):
        while len(self.chunks) > 0:
            chunk = self.chunks.pop()
            current_pos = obj.current_size//8

            # Check if padding needed
            size = chunk.offset - current_pos
            if size > 0:
                obj.info("Padding of %u bytes needed: curr=%u offset=%u" % \
                         (size, current_pos, chunk.offset))
                yield PaddingBytes(obj, "padding[]", size)
                current_pos = obj.current_size//8

            # Find resynch point if needed
            count = 0
            old_off = chunk.offset
            while chunk.offset < current_pos:
                count += 1
                chunk = self.chunks.pop()
                # Unfortunaly, we also pass the underlying chunks
                if chunk == None:
                    obj.info("Couldn't resynch: %u object skipped to reach %u" % \
                             (count, current_pos))
                    return

            # Resynch
            size = chunk.offset-current_pos
            if size > 0:
                obj.info("Skipped %u objects to resynch to %u; chunk offset: %u->%u" % \
                         (count, current_pos, old_off, chunk.offset))
                yield RawBytes(obj, "resynch[]", size)

            # Yield
            obj.info("Yielding element of size %u at offset %u" % \
                     (chunk.size, chunk.offset))
            field = chunk.cls(obj, chunk.name, chunk.size, *chunk.args)
            # Not tested, probably wrong:
            #if chunk.size: field.static_size = 8*chunk.size
            yield field

            if hasattr(field, "getSubChunks"):
                for sub_chunk in field.getSubChunks():
                    obj.info("Adding sub chunk: position=%u size=%u name='%s'" % \
                             (sub_chunk.offset, sub_chunk.size, sub_chunk.name))
                    self.addChunk(sub_chunk)

            # Let missing padding be done by next chunk

class S3MFlags(StaticFieldSet):
    format = (
        (Bit, "st2_vibrato", "Vibrato (File version 1/ScreamTrack 2)"),
        (Bit, "st2_tempo", "Tempo (File version 1/ScreamTrack 2)"),
        (Bit, "amiga_slides", "Amiga slides (File version 1/ScreamTrack 2)"),
        (Bit, "zero_vol_opt", "Automatically turn off looping notes whose volume is zero for >2 note rows"),
        (Bit, "amiga_limits", "Disallow notes beyond Amiga hardware specs"),
        (Bit, "sb_processing", "Enable filter/SFX with SoundBlaster"),
        (Bit, "vol_slide", "Volume slide also performed on first row"),
        (Bit, "extended", "Special custom data in file"),
        (Bits, "unused[]", 8)
    )

def parseChannelType(val):
    val = val.value
    if val<8:
        return "Left Sample Channel %u" % val
    if val<16:
        return "Right Sample Channel %u" % (val-8)
    if val<32:
        return "Adlib channel %u" % (val-16)
    return "Value %u unknown" % val

class ChannelSettings(FieldSet):
    static_size = 8
    def createFields(self):
        yield textHandler(Bits(self, "type", 7), parseChannelType)
        yield Bit(self, "enabled")

class ChannelPanning(FieldSet):
    static_size = 8
    def createFields(self):
        yield Bits(self, "default_position", 4, "Default pan position")
        yield Bit(self, "reserved[]")
        yield Bit(self, "use_default", "Bits 0:3 specify default position")
        yield Bits(self, "reserved[]", 2)

# Provide an automatic constructor
class SizeFieldSet(FieldSet):
    """
    Provide an automatic constructor for a sized field that can be aligned
    on byte positions according to ALIGN.

    Size is ignored if static_size is set. Real size is stored
    for convenience, but beware, it is not in bits, but in bytes.

    Field can be automatically padded, unless:
    - size is 0 (unknown, so padding doesn't make sense)
    - it shouldn't be aligned

    If it shouldn't be aligned, two solutions:
    - change _size to another value than the one found through aligment.
    - derive a class with ALIGN = 0.
    """
    ALIGN = 16
    def __init__(self, parent, name, size, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        if size:
            self.real_size = size
            if self.static_size == None:
                self.setCheckedSizes(size)

    def setCheckedSizes(self, size):
        # First set size so that end is aligned, if needed
        self.real_size = size
        size *= 8
        if self.ALIGN:
            size = alignValue(self.absolute_address+size, 8*self.ALIGN) \
                   - self.absolute_address

        if self._parent._size:
            if self._parent.current_size + size > self._parent._size:
                size = self._parent._size - self._parent.current_size

        self._size = size

    def createFields(self):
        for field in self.createUnpaddedFields():
            yield field
        size = (self._size - self.current_size)//8
        if size > 0:
            yield PaddingBytes(self, "padding", size)

class Header(SizeFieldSet):
    def createDescription(self):
        return "%s (%u patterns, %u instruments)" % \
               (self["title"].value, self["num_patterns"].value,
                self["num_instruments"].value)

    def createValue(self):
        return self["title"].value

    # Header fields may have to be padded - specify static_size
    # or modify _size in a derived class if never.
    def createUnpaddedFields(self):
        yield String(self, "title", 28, strip='\0')
        yield textHandler(UInt8(self, "marker[]"), hexadecimal)
        for field in self.getFileVersionField():
            yield field

        yield UInt16(self, "num_orders")
        yield UInt16(self, "num_instruments")
        yield UInt16(self, "num_patterns")

        for field in self.getFirstProperties():
            yield field
        yield String(self, "marker[]", 4)
        for field in self.getLastProperties():
            yield field

        yield GenericVector(self, "channel_settings", 32,
                            ChannelSettings, "channel")

        # Orders
        yield GenericVector(self, "orders", self.getNumOrders(), UInt8, "order")

        for field in self.getHeaderEndFields():
            yield field

class S3MHeader(Header):
    """
          0   1   2   3   4   5   6   7   8   9   A   B   C   D   E   F
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0000: | Song name, max 28 chars (end with NUL (0))                    |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0010: |                                               |1Ah|Typ| x | x |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0020: |OrdNum |InsNum |PatNum | Flags | Cwt/v | Ffi   |'S'|'C'|'R'|'M'|
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0030: |g.v|i.s|i.t|m.v|u.c|d.p| x | x | x | x | x | x | x | x |Special|
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0040: |Channel settings for 32 channels, 255=unused,+128=disabled     |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0050: |                                                               |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0060: |Orders; length=OrdNum (should be even)                         |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  xxx1: |Parapointers to instruments; length=InsNum*2                   |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  xxx2: |Parapointers to patterns; length=PatNum*2                      |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  xxx3: |Channel default pan positions                                  |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
        xxx1=70h+orders
        xxx2=70h+orders+instruments*2
        xxx3=70h+orders+instruments*2+patterns*2
    """
    def __init__(self, parent, name, size, desc=None):
        Header.__init__(self, parent, name, size, desc)

        # Overwrite real_size
        size = 0x60 + self["num_orders"].value + \
               2*(self["num_instruments"].value + self["num_patterns"].value)
        if self["panning_info"].value == 252:
            size += 32

        # Deduce size for SizeFieldSet
        self.setCheckedSizes(size)

    def getFileVersionField(self):
        yield UInt8(self, "type")
        yield RawBytes(self, "reserved[]", 2)

    def getFirstProperties(self):
        yield S3MFlags(self, "flags")
        yield UInt8(self, "creation_version_minor")
        yield Bits(self, "creation_version_major", 4)
        yield Bits(self, "creation_version_unknown", 4, "(=1)")
        yield UInt16(self, "format_version")

    def getLastProperties(self):
        yield UInt8(self, "glob_vol", "Global volume")
        yield UInt8(self, "init_speed", "Initial speed (command A)")
        yield UInt8(self, "init_tempo", "Initial tempo (command T)")
        yield Bits(self, "volume", 7)
        yield Bit(self, "stereo")
        yield UInt8(self, "click_removal", "Number of GUS channels to run to prevent clicks")
        yield UInt8(self, "panning_info")
        yield RawBytes(self, "reserved[]", 8)
        yield UInt16(self, "custom_data_parapointer",
                     "Parapointer to special custom data (not used by ST3.01)")

    def getNumOrders(self): return self["num_orders"].value

    def getHeaderEndFields(self):
        instr = self["num_instruments"].value
        patterns = self["num_patterns"].value
        # File pointers
        if instr > 0:
            yield GenericVector(self, "instr_pptr", instr, UInt16, "offset")
        if patterns > 0:
            yield GenericVector(self, "pattern_pptr", patterns, UInt16, "offset")

        # S3M 3.20 extension
        if self["creation_version_major"].value >= 3 \
        and self["creation_version_minor"].value >= 0x20 \
        and self["panning_info"].value == 252:
            yield GenericVector(self, "channel_panning", 32, ChannelPanning, "channel")

        # Padding required for 16B alignment
        size = self._size - self.current_size
        if size > 0:
            yield PaddingBytes(self, "padding", size//8)

    def getSubChunks(self):
        # Instruments -  no warranty that they are concatenated
        for index in xrange(self["num_instruments"].value):
            yield Chunk(S3MInstrument, "instrument[]",
                        16*self["instr_pptr/offset[%u]" % index].value,
                        S3MInstrument.static_size//8)

        # Patterns - size unknown but listed in their headers
        for index in xrange(self["num_patterns"].value):
            yield Chunk(S3MPattern, "pattern[]",
                        16*self["pattern_pptr/offset[%u]" % index].value, 0)

class PTMHeader(Header):
    # static_size should prime over _size, right?
    static_size = 8*608

    def getTrackerVersion(val):
        val = val.value
        return "ProTracker x%04X" % val

    def getFileVersionField(self):
        yield UInt16(self, "type")
        yield RawBytes(self, "reserved[]", 1)

    def getFirstProperties(self):
        yield UInt16(self, "channels")
        yield UInt16(self, "flags") # 0 => NullBytes
        yield UInt16(self, "reserved[]")

    def getLastProperties(self):
        yield RawBytes(self, "reserved[]", 16)

    def getNumOrders(self): return 256

    def getHeaderEndFields(self):
        yield GenericVector(self, "pattern_pptr", 128, UInt16, "offset")

    def getSubChunks(self):
        # It goes like this in the BS: patterns->instruments->instr. samples

        if self._parent._size:
            min_off = self.absolute_address+self._parent._size
        else:
            min_off = 99999999999

        # Instruments and minimal end position for last pattern
        count = self["num_instruments"].value
        addr = self.absolute_address
        for index in xrange(count):
            offset = (self.static_size+index*PTMInstrument.static_size)//8
            yield Chunk(PTMInstrument, "instrument[]", offset,
                        PTMInstrument.static_size//8)
            offset = self.stream.readBits(addr+8*(offset+18), 32, LITTLE_ENDIAN)
            min_off = min(min_off, offset)

        # Patterns
        count = self["num_patterns"].value
        prev_off = 16*self["pattern_pptr/offset[0]"].value
        for index in range(1, count):
            offset = 16*self["pattern_pptr/offset[%u]" % index].value
            yield Chunk(PTMPattern, "pattern[]", prev_off, offset-prev_off)
            prev_off = offset

        # Difficult to account for
        yield Chunk(PTMPattern, "pattern[]", prev_off, min_off-prev_off)

class SampleFlags(StaticFieldSet):
    format = (
        (Bit, "loop_on"),
        (Bit, "stereo", "Sample size will be 2*length"),
        (Bit, "16bits", "16b sample, Intel LO-HI byteorder"),
        (Bits, "unused", 5)
    )

class S3MUInt24(Field):
    static_size = 24
    def __init__(self, parent, name, desc=None):
        Field.__init__(self, parent, name, size=24, description=desc)
        addr = self.absolute_address
        val = parent.stream.readBits(addr, 8, LITTLE_ENDIAN) << 20
        val += parent.stream.readBits(addr+8, 16, LITTLE_ENDIAN) << 4
        self.createValue = lambda: val

class SampleData(SizeFieldSet):
    def createUnpaddedFields(self):
        yield RawBytes(self, "data", self.real_size)
class PTMSampleData(SampleData):
    ALIGN = 0

class Instrument(SizeFieldSet):
    static_size = 8*0x50

    def createDescription(self):
        info = [self["c4_speed"].display]
        if "flags/stereo" in self:
            if self["flags/stereo"].value:
                info.append("stereo")
            else:
                info.append("mono")
        info.append("%u bits" % self.getSampleBits())
        return ", ".join(info)

    # Structure knows its size and doesn't need padding anyway, so
    # overwrite base member: no need to go through it.
    def createFields(self):
        yield self.getType()
        yield String(self, "filename", 12, strip='\0')

        for field in self.getInstrumentFields():
            yield field

        yield String(self, "name", 28, strip='\0')
        yield String(self, "marker", 4, "Either 'SCRS' or '(empty)'", strip='\0')

    def createValue(self):
        return self["name"].value

class S3MInstrument(Instrument):
    """
    In fact a sample. Description follows:

          0   1   2   3   4   5   6   7   8   9   A   B   C   D   E   F
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0000: |[T]| Dos filename (12345678.ABC)                   |    MemSeg |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0010: |Length |HI:leng|LoopBeg|HI:LBeg|LoopEnd|HI:Lend|Vol| x |[P]|[F]|
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0020: |C2Spd  |HI:C2sp| x | x | x | x |Int:Gp |Int:512|Int:lastused   |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0030: | Sample name, 28 characters max... (incl. NUL)                 |
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0040: | ...sample name...                             |'S'|'C'|'R'|'S'|
        +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  xxxx: sampledata
    """
    MAGIC = "SCRS"
    PACKING = {0: "Unpacked", 1: "DP30ADPCM" }
    TYPE = {0: "Unknown", 1: "Sample", 2: "adlib melody", 3: "adlib drum2" }

    def getType(self):
        return Enum(UInt8(self, "type"), self.TYPE)

    def getSampleBits(self):
        return 8*(1+self["flags/16bits"].value)

    def getInstrumentFields(self):
        yield S3MUInt24(self, "sample_offset")
        yield UInt32(self, "sample_size")
        yield UInt32(self, "loop_begin")
        yield UInt32(self, "loop_end")
        yield UInt8(self, "volume")
        yield UInt8(self, "reserved[]")
        yield Enum(UInt8(self, "packing"), self.PACKING)
        yield SampleFlags(self, "flags")
        yield UInt32(self, "c4_speed", "Frequency for middle C note")
        yield UInt32(self, "reserved[]", 4)
        yield UInt16(self, "internal[]", "Sample address in GUS memory")
        yield UInt16(self, "internal[]", "Flags for SoundBlaster loop expansion")
        yield UInt32(self, "internal[]", "Last used position (SB)")

    def getSubChunks(self):
        size = self["sample_size"].value
        if self["flags/stereo"].value: size *= 2
        if self["flags/16bits"].value: size *= 2
        yield Chunk(SampleData, "sample_data[]",
                    self["sample_offset"].value, size)


class PTMType(FieldSet):
    TYPES = {0: "No sample", 1: "Regular", 2: "OPL2/OPL2 instrument", 3: "MIDI instrument" }
    static_size = 8
    def createFields(self):
        yield Bits(self, "unused", 2)
        yield Bit(self, "is_tonable")
        yield Bit(self, "16bits")
        yield Bit(self, "loop_bidir")
        yield Bit(self, "loop")
        yield Enum(Bits(self, "origin", 2), self.TYPES)

##class PTMType(StaticFieldSet):
##    format = (
##        (Bits, "unused", 2),
##        (Bit, "is_tonable"),
##        (Bit, "16bits"),
##        (Bit, "loop_bidir"),
##        (Bit, "loop"),
##        (Bits, "origin", 2),
##    )

class PTMInstrument(Instrument):
    MAGIC = "PTMI"
    ALIGN = 0

    def getType(self):
        return PTMType(self, "flags") # Hack to have more common code

    # PTM doesn't pretend to manage 16bits
    def getSampleBits(self):
        return 8

    def getInstrumentFields(self):
        yield UInt8(self, "volume")
        yield UInt16(self, "c4_speed")
        yield UInt16(self, "sample_segment")
        yield UInt32(self, "sample_offset")
        yield UInt32(self, "sample_size")
        yield UInt32(self, "loop_begin")
        yield UInt32(self, "loop_end")
        yield UInt32(self, "gus_begin")
        yield UInt32(self, "gus_loop_start")
        yield UInt32(self, "gus_loop_end")
        yield textHandler(UInt8(self, "gus_loop_flags"), hexadecimal)
        yield UInt8(self, "reserved[]") # Should be 0

    def getSubChunks(self):
        # Samples are NOT padded, and the size is already the correct one
        size = self["sample_size"].value
        if size:
            yield Chunk(PTMSampleData, "sample_data[]", self["sample_offset"].value, size)


class S3MNoteInfo(StaticFieldSet):
    """
0=end of row
&31=channel
&32=follows;  BYTE:note, BYTE:instrument
&64=follows;  BYTE:volume
&128=follows; BYTE:command, BYTE:info
    """
    format = (
        (Bits, "channel", 5),
        (Bit, "has_note"),
        (Bit, "has_volume"),
        (Bit, "has_effect")
    )

class PTMNoteInfo(StaticFieldSet):
    format = (
        (Bits, "channel", 5),
        (Bit, "has_note"),
        (Bit, "has_effect"),
        (Bit, "has_volume")
    )

class Note(FieldSet):
    def createFields(self):
        # Used by Row to check if end of Row
        info = self.NOTE_INFO(self, "info")
        yield info
        if info["has_note"].value:
            yield UInt8(self, "note")
            yield UInt8(self, "instrument")
        if info["has_volume"].value:
            yield UInt8(self, "volume")
        if info["has_effect"].value:
            yield UInt8(self, "effect")
            yield UInt8(self, "param")

class S3MNote(Note):
    NOTE_INFO = S3MNoteInfo
class PTMNote(Note):
    NOTE_INFO = PTMNoteInfo

class Row(FieldSet):
    def createFields(self):
        addr = self.absolute_address
        while True:
            # Check empty note
            byte = self.stream.readBits(addr, 8, self.endian)
            if not byte:
                yield NullBytes(self, "terminator", 1)
                return

            note = self.NOTE(self, "note[]")
            yield note
            addr += note.size

class S3MRow(Row):
    NOTE = S3MNote
class PTMRow(Row):
    NOTE = PTMNote

class Pattern(SizeFieldSet):
    def createUnpaddedFields(self):
        count = 0
        while count < 64 and not self.eof:
            yield self.ROW(self, "row[]")
            count += 1

class S3MPattern(Pattern):
    ROW = S3MRow
    def __init__(self, parent, name, size, desc=None):
        Pattern.__init__(self, parent, name, size, desc)

        # Get real_size from header
        addr = self.absolute_address
        size = self.stream.readBits(addr, 16, LITTLE_ENDIAN)
        self.setCheckedSizes(size)

class PTMPattern(Pattern):
    ROW = PTMRow

class Module(Parser):
    # MARKER / HEADER are defined in derived classes
    endian = LITTLE_ENDIAN

    def validate(self):
        marker = self.stream.readBits(0x1C*8, 8, LITTLE_ENDIAN)
        if marker != 0x1A:
            return "Invalid start marker %u" % marker
        marker = self.stream.readBytes(0x2C*8, 4)
        if marker != self.MARKER:
            return "Invalid marker %s!=%s" % (marker, self.MARKER)
        return True

    def createFields(self):
        # Index chunks
        indexer = ChunkIndexer()
        # Add header - at least 0x50 bytes
        indexer.addChunk(Chunk(self.HEADER, "header", 0, 0x50))
        for field in indexer.yieldChunks(self):
            yield field


class S3MModule(Module):
    PARSER_TAGS = {
        "id": "s3m",
        "category": "audio",
        "file_ext": ("s3m",),
        "mime": (u'audio/s3m', u'audio/x-s3m'),
        "min_size": 64*8,
        "description": "ScreamTracker3 module"
    }
    MARKER = "SCRM"
    HEADER = S3MHeader

##    def createContentSize(self):
##        hdr = Header(self, "header")
##        max_offset = hdr._size//8

##        instr_size = Instrument._size//8
##        for index in xrange(self["header/num_instruments"].value):
##            offset = 16*hdr["instr_pptr/offset[%u]" % index].value
##            max_offset = max(offset+instr_size, max_offset)
##            addr = self.absolute_address + 8*offset

class PTMModule(Module):
    PARSER_TAGS = {
        "id": "ptm",
        "category": "audio",
        "file_ext": ("ptm",),
        "min_size": 64*8,
        "description": "PolyTracker module (v1.17)"
    }
    MARKER = "PTMF"
    HEADER = PTMHeader

########NEW FILE########
__FILENAME__ = xm
"""
Parser of FastTrackerII Extended Module (XM) version 1.4

Documents:
- Modplug source code (file modplug/soundlib/Load_xm.cpp)
  http://sourceforge.net/projects/modplug
- Dumb source code (files include/dumb.h and src/it/readxm.c
  http://dumb.sf.net/
- Documents of "XM" format on Wotsit
  http://www.wotsit.org

Author: Christophe GISQUET <christophe.gisquet@free.fr>
Creation: 8th February 2007
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (StaticFieldSet, FieldSet,
    Bit, RawBits, Bits,
    UInt32, UInt16, UInt8, Int8, Enum,
    RawBytes, String, GenericVector)
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, filesizeHandler, hexadecimal
from lib.hachoir_parser.audio.modplug import ParseModplugMetadata
from lib.hachoir_parser.common.tracker import NOTE_NAME

def parseSigned(val):
    return "%i" % (val.value-128)

# From dumb
SEMITONE_BASE = 1.059463094359295309843105314939748495817
PITCH_BASE = 1.000225659305069791926712241547647863626

SAMPLE_LOOP_MODE = ("No loop", "Forward loop", "Ping-pong loop", "Undef")

class SampleType(FieldSet):
    static_size = 8
    def createFields(self):
        yield Bits(self, "unused[]", 4)
        yield Bit(self, "16bits")
        yield Bits(self, "unused[]", 1)
        yield Enum(Bits(self, "loop_mode", 2), SAMPLE_LOOP_MODE)

class SampleHeader(FieldSet):
    static_size = 40*8
    def createFields(self):
        yield UInt32(self, "length")
        yield UInt32(self, "loop_start")
        yield UInt32(self, "loop_end")
        yield UInt8(self, "volume")
        yield Int8(self, "fine_tune")
        yield SampleType(self, "type")
        yield UInt8(self, "panning")
        yield Int8(self, "relative_note")
        yield UInt8(self, "reserved")
        yield String(self, "name", 22, charset="ASCII", strip=' \0')

    def createValue(self):
        bytes = 1+self["type/16bits"].value
        C5_speed = int(16726.0*pow(SEMITONE_BASE, self["relative_note"].value)
                       *pow(PITCH_BASE, self["fine_tune"].value*2))
        return "%s, %ubits, %u samples, %uHz" % \
               (self["name"].display, 8*bytes, self["length"].value/bytes, C5_speed)

class StuffType(StaticFieldSet):
    format = (
        (Bits, "unused", 5),
        (Bit, "loop"),
        (Bit, "sustain"),
        (Bit, "on")
    )

class InstrumentSecondHeader(FieldSet):
    static_size = 234*8
    def createFields(self):
        yield UInt32(self, "sample_header_size")
        yield GenericVector(self, "notes", 96, UInt8, "sample")
        yield GenericVector(self, "volume_envelope", 24, UInt16, "point")
        yield GenericVector(self, "panning_envelope", 24, UInt16, "point")
        yield UInt8(self, "volume_points", r"Number of volume points")
        yield UInt8(self, "panning_points", r"Number of panning points")
        yield UInt8(self, "volume_sustain_point")
        yield UInt8(self, "volume_loop_start_point")
        yield UInt8(self, "volume_loop_end_point")
        yield UInt8(self, "panning_sustain_point")
        yield UInt8(self, "panning_loop_start_point")
        yield UInt8(self, "panning_loop_end_point")
        yield StuffType(self, "volume_type")
        yield StuffType(self, "panning_type")
        yield UInt8(self, "vibrato_type")
        yield UInt8(self, "vibrato_sweep")
        yield UInt8(self, "vibrato_depth")
        yield UInt8(self, "vibrato_rate")
        yield UInt16(self, "volume_fadeout")
        yield GenericVector(self, "reserved", 11, UInt16, "word")

def createInstrumentContentSize(s, addr):
    start = addr
    samples = s.stream.readBits(addr+27*8, 16, LITTLE_ENDIAN)
    # Seek to end of header (1st + 2nd part)
    addr += 8*s.stream.readBits(addr, 32, LITTLE_ENDIAN)

    sample_size = 0
    if samples:
        for index in xrange(samples):
            # Read the sample size from the header
            sample_size += s.stream.readBits(addr, 32, LITTLE_ENDIAN)
            # Seek to next sample header
            addr += SampleHeader.static_size

    return addr - start + 8*sample_size

class Instrument(FieldSet):
    def __init__(self, parent, name):
        FieldSet.__init__(self, parent, name)
        self._size = createInstrumentContentSize(self, self.absolute_address)
        self.info(self.createDescription())

    # Seems to fix things...
    def fixInstrumentHeader(self):
        size = self["size"].value - self.current_size//8
        if size:
            yield RawBytes(self, "unknown_data", size)

    def createFields(self):
        yield UInt32(self, "size")
        yield String(self, "name", 22, charset="ASCII", strip=" \0")
        # Doc says type is always 0, but I've found values of 24 and 96 for
        # the _same_ song here, just different download sources for the file
        yield UInt8(self, "type")
        yield UInt16(self, "samples")
        num = self["samples"].value
        self.info(self.createDescription())

        if num:
            yield InstrumentSecondHeader(self, "second_header")

            for field in self.fixInstrumentHeader():
                yield field

            # This part probably wrong
            sample_size = [ ]
            for index in xrange(num):
                sample = SampleHeader(self, "sample_header[]")
                yield sample
                sample_size.append(sample["length"].value)

            for size in sample_size:
                if size:
                    yield RawBytes(self, "sample_data[]", size, "Deltas")
        else:
            for field in self.fixInstrumentHeader():
                yield field

    def createDescription(self):
        return "Instrument '%s': %i samples, header %i bytes" % \
               (self["name"].value, self["samples"].value, self["size"].value)

VOLUME_NAME = (
    "Volume slide down", "Volume slide up", "Fine volume slide down",
    "Fine volume slide up", "Set vibrato speed", "Vibrato",
    "Set panning", "Panning slide left", "Panning slide right",
    "Tone porta", "Unhandled")

def parseVolume(val):
    val = val.value
    if 0x10<=val<=0x50:
        return "Volume %i" % val-16
    else:
        return VOLUME_NAME[val/16 - 6]

class RealBit(RawBits):
    static_size = 1

    def __init__(self, parent, name, description=None):
        RawBits.__init__(self, parent, name, 1, description=description)

    def createValue(self):
        return self._parent.stream.readBits(self.absolute_address, 1, BIG_ENDIAN)

class NoteInfo(StaticFieldSet):
    format = (
        (RawBits, "unused", 2),
        (RealBit, "has_parameter"),
        (RealBit, "has_type"),
        (RealBit, "has_volume"),
        (RealBit, "has_instrument"),
        (RealBit, "has_note")
    )

EFFECT_NAME = (
    "Arppegio", "Porta up", "Porta down", "Tone porta", "Vibrato",
    "Tone porta+Volume slide", "Vibrato+Volume slide", "Tremolo",
    "Set panning", "Sample offset", "Volume slide", "Position jump",
    "Set volume", "Pattern break", None, "Set tempo/BPM",
    "Set global volume", "Global volume slide", "Unused", "Unused",
    "Unused", "Set envelope position", "Unused", "Unused",
    "Panning slide", "Unused", "Multi retrig note", "Unused",
    "Tremor", "Unused", "Unused", "Unused", None)

EFFECT_E_NAME = (
    "Unknown", "Fine porta up", "Fine porta down",
    "Set gliss control", "Set vibrato control", "Set finetune",
    "Set loop begin/loop", "Set tremolo control", "Retrig note",
    "Fine volume slide up", "Fine volume slide down", "Note cut",
    "Note delay", "Pattern delay")

class Effect(RawBits):
    def __init__(self, parent, name):
        RawBits.__init__(self, parent, name, 8)

    def createValue(self):
        t = self.parent.stream.readBits(self.absolute_address, 8, LITTLE_ENDIAN)
        param = self.parent.stream.readBits(self.absolute_address+8, 8, LITTLE_ENDIAN)
        if t == 0x0E:
            return EFFECT_E_NAME[param>>4] + " %i" % (param&0x07)
        elif t == 0x21:
            return ("Extra fine porta up", "Extra fine porta down")[param>>4]
        else:
            return EFFECT_NAME[t]

class Note(FieldSet):
    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self.flags = self.stream.readBits(self.absolute_address, 8, LITTLE_ENDIAN)
        if self.flags&0x80:
            # TODO: optimize bitcounting with a table:
            # http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetTable
            self._size = 8
            if self.flags&0x01: self._size += 8
            if self.flags&0x02: self._size += 8
            if self.flags&0x04: self._size += 8
            if self.flags&0x08: self._size += 8
            if self.flags&0x10: self._size += 8
        else:
            self._size = 5*8

    def createFields(self):
        # This stupid shit gets the LSB, not the MSB...
        self.info("Note info: 0x%02X" %
                  self.stream.readBits(self.absolute_address, 8, LITTLE_ENDIAN))
        yield RealBit(self, "is_extended")
        if self["is_extended"].value:
            info = NoteInfo(self, "info")
            yield info
            if info["has_note"].value:
                yield Enum(UInt8(self, "note"), NOTE_NAME)
            if info["has_instrument"].value:
                yield UInt8(self, "instrument")
            if info["has_volume"].value:
                yield textHandler(UInt8(self, "volume"), parseVolume)
            if info["has_type"].value:
                yield Effect(self, "effect_type")
            if info["has_parameter"].value:
                yield textHandler(UInt8(self, "effect_parameter"), hexadecimal)
        else:
            yield Enum(Bits(self, "note", 7), NOTE_NAME)
            yield UInt8(self, "instrument")
            yield textHandler(UInt8(self, "volume"), parseVolume)
            yield Effect(self, "effect_type")
            yield textHandler(UInt8(self, "effect_parameter"), hexadecimal)

    def createDescription(self):
        if "info" in self:
            info = self["info"]
            desc = []
            if info["has_note"].value:
                desc.append(self["note"].display)
            if info["has_instrument"].value:
                desc.append("instrument %i" % self["instrument"].value)
            if info["has_volume"].value:
                desc.append(self["has_volume"].display)
            if info["has_type"].value:
                desc.append("effect %s" % self["effect_type"].value)
            if info["has_parameter"].value:
                desc.append("parameter %i" % self["effect_parameter"].value)
        else:
            desc = (self["note"].display, "instrument %i" % self["instrument"].value,
                self["has_volume"].display, "effect %s" % self["effect_type"].value,
                "parameter %i" % self["effect_parameter"].value)
        if desc:
            return "Note %s" % ", ".join(desc)
        else:
            return "Note"

class Row(FieldSet):
    def createFields(self):
        for index in xrange(self["/header/channels"].value):
            yield Note(self, "note[]")

def createPatternContentSize(s, addr):
    return 8*(s.stream.readBits(addr, 32, LITTLE_ENDIAN) +
              s.stream.readBits(addr+7*8, 16, LITTLE_ENDIAN))

class Pattern(FieldSet):
    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self._size = createPatternContentSize(self, self.absolute_address)

    def createFields(self):
        yield UInt32(self, "header_size", r"Header length (9)")
        yield UInt8(self, "packing_type", r"Packing type (always 0)")
        yield UInt16(self, "rows", r"Number of rows in pattern (1..256)")
        yield UInt16(self, "data_size", r"Packed patterndata size")
        rows = self["rows"].value
        self.info("Pattern: %i rows" % rows)
        for index in xrange(rows):
            yield Row(self, "row[]")

    def createDescription(self):
        return "Pattern with %i rows" % self["rows"].value

class Header(FieldSet):
    MAGIC = "Extended Module: "
    static_size = 336*8

    def createFields(self):
        yield String(self, "signature", 17, "XM signature", charset="ASCII")
        yield String(self, "title", 20, "XM title", charset="ASCII", strip=' ')
        yield UInt8(self, "marker", "Marker (0x1A)")
        yield String(self, "tracker_name", 20, "XM tracker name", charset="ASCII", strip=' ')
        yield UInt8(self, "format_minor")
        yield UInt8(self, "format_major")
        yield filesizeHandler(UInt32(self, "header_size", "Header size (276)"))
        yield UInt16(self, "song_length", "Length in patten order table")
        yield UInt16(self, "restart", "Restart position")
        yield UInt16(self, "channels", "Number of channels (2,4,6,8,10,...,32)")
        yield UInt16(self, "patterns", "Number of patterns (max 256)")
        yield UInt16(self, "instruments", "Number of instruments (max 128)")
        yield Bit(self, "amiga_ftable", "Amiga frequency table")
        yield Bit(self, "linear_ftable", "Linear frequency table")
        yield Bits(self, "unused", 14)
        yield UInt16(self, "tempo", "Default tempo")
        yield UInt16(self, "bpm", "Default BPM")
        yield GenericVector(self, "pattern_order", 256, UInt8, "order")

    def createDescription(self):
        return "'%s' by '%s'" % (
            self["title"].value, self["tracker_name"].value)

class XMModule(Parser):
    PARSER_TAGS = {
        "id": "fasttracker2",
        "category": "audio",
        "file_ext": ("xm",),
        "mime": (
            u'audio/xm', u'audio/x-xm',
            u'audio/module-xm', u'audio/mod', u'audio/x-mod'),
        "magic": ((Header.MAGIC, 0),),
        "min_size": Header.static_size +29*8, # Header + 1 empty instrument
        "description": "FastTracker2 module"
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        header = self.stream.readBytes(0, 17)
        if header != Header.MAGIC:
            return "Invalid signature '%s'" % header
        if self["/header/header_size"].value != 276:
            return "Unknown header size (%u)" % self["/header/header_size"].value
        return True

    def createFields(self):
        yield Header(self, "header")
        for index in xrange(self["/header/patterns"].value):
            yield Pattern(self, "pattern[]")
        for index in xrange(self["/header/instruments"].value):
            yield Instrument(self, "instrument[]")

        # Metadata added by ModPlug - can be discarded
        for field in ParseModplugMetadata(self):
            yield field

    def createContentSize(self):
        # Header size
        size = Header.static_size

        # Add patterns size
        for index in xrange(self["/header/patterns"].value):
            size += createPatternContentSize(self, size)

        # Add instruments size
        for index in xrange(self["/header/instruments"].value):
            size += createInstrumentContentSize(self, size)

        # Not reporting Modplug metadata
        return size

    def createDescription(self):
        return self["header"].description


########NEW FILE########
__FILENAME__ = deflate
from lib.hachoir_core.field import CompressedField

try:
    from zlib import decompressobj, MAX_WBITS

    class DeflateStream:
        def __init__(self, stream, wbits=None):
            if wbits:
                self.gzip = decompressobj(-MAX_WBITS)
            else:
                self.gzip = decompressobj()

        def __call__(self, size, data=None):
            if data is None:
                data = self.gzip.unconsumed_tail
            return self.gzip.decompress(data, size)

    class DeflateStreamWbits(DeflateStream):
        def __init__(self, stream):
            DeflateStream.__init__(self, stream, True)

    def Deflate(field, wbits=True):
        if wbits:
            CompressedField(field, DeflateStreamWbits)
        else:
            CompressedField(field, DeflateStream)
        return field
    has_deflate = True
except ImportError:
    def Deflate(field, wbits=True):
        return field
    has_deflate = False


########NEW FILE########
__FILENAME__ = msdos
"""
MS-DOS structures.

Documentation:
- File attributes:
  http://www.cs.colorado.edu/~main/cs1300/include/ddk/winddk.h
"""

from lib.hachoir_core.field import StaticFieldSet
from lib.hachoir_core.field import Bit, NullBits

_FIELDS = (
    (Bit, "read_only"),
    (Bit, "hidden"),
    (Bit, "system"),
    (NullBits, "reserved[]", 1),
    (Bit, "directory"),
    (Bit, "archive"),
    (Bit, "device"),
    (Bit, "normal"),
    (Bit, "temporary"),
    (Bit, "sparse_file"),
    (Bit, "reparse_file"),
    (Bit, "compressed"),
    (Bit, "offline"),
    (Bit, "dont_index_content"),
    (Bit, "encrypted"),
)

class MSDOSFileAttr16(StaticFieldSet):
    """
    MSDOS 16-bit file attributes
    """
    format = _FIELDS + ((NullBits, "reserved[]", 1),)

    _text_keys = (
        # Sort attributes by importance
        "directory", "read_only", "compressed",
        "hidden", "system",
        "normal", "device",
        "temporary", "archive")

    def createValue(self):
        mode = []
        for name in self._text_keys:
            if self[name].value:
                if 4 <= len(mode):
                    mode.append("...")
                    break
                else:
                    mode.append(name)
        if mode:
            return ", ".join(mode)
        else:
            return "(none)"

class MSDOSFileAttr32(MSDOSFileAttr16):
    """
    MSDOS 32-bit file attributes
    """
    format = _FIELDS + ((NullBits, "reserved[]", 17),)


########NEW FILE########
__FILENAME__ = tracker
"""
Shared code for tracker parser.
"""

NOTE_NAME = {}
NOTES = ("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "G#", "A", "A#", "B")
for octave in xrange(10):
    for index, note in enumerate(NOTES):
        NOTE_NAME[octave*12+index] = "%s (octave %s)" % (note, octave)


########NEW FILE########
__FILENAME__ = win32
from lib.hachoir_core.field import (FieldSet,
    UInt16, UInt32, Enum, String, Bytes, Bits, TimestampUUID60)
from lib.hachoir_parser.video.fourcc import video_fourcc_name
from lib.hachoir_core.bits import str2hex
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_parser.network.common import MAC48_Address

# Dictionary: Windows codepage => Python charset name
CODEPAGE_CHARSET = {
      874: "CP874",
#     932: Japanese Shift-JIS
#     936: Simplified Chinese GBK
#     949: Korean
#     950: Traditional Chinese Big5
     1250: "WINDOWS-1250",
     1251: "WINDOWS-1251",
     1252: "WINDOWS-1252",
     1253: "WINDOWS-1253",
     1254: "WINDOWS-1254",
     1255: "WINDOWS-1255",
     1256: "WINDOWS-1256",
     1257: "WINDOWS-1257",
     1258: "WINDOWS-1258",
    65001: "UTF-8",
}

class PascalStringWin32(FieldSet):
    def __init__(self, parent, name, description=None, strip=None, charset="UTF-16-LE"):
        FieldSet.__init__(self, parent, name, description)
        length = self["length"].value
        self._size = 32 + length * 16
        self.strip = strip
        self.charset = charset

    def createFields(self):
        yield UInt32(self, "length", "Length in widechar characters")
        size = self["length"].value
        if size:
            yield String(self, "text", size*2, charset=self.charset, strip=self.strip)

    def createValue(self):
        if "text" in self:
            return self["text"].value
        else:
            return None

class GUID(FieldSet):
    """
    Windows 128 bits Globally Unique Identifier (GUID)

    See RFC 4122
    """
    static_size = 128
    NULL = "00000000-0000-0000-0000-000000000000"
    FIELD_NAMES = {
        3: ("sha1_high", "sha1_low"),
        4: ("random_high", "random_low"),
        5: ("md5_high", "md5_low"),
    }
    VERSION_NAME = {
        1: "Timestamp & MAC-48",
        2: "DCE Security version",
        3: "Name SHA-1 hash",
        4: "Randomly generated",
        5: "Name MD5 hash",
    }
    VARIANT_NAME = {
        0: "NCS",
        2: "Leach-Salz",
       # 5: Microsoft Corporation?
        6: "Microsoft Corporation",
        7: "Reserved Future",
    }
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self.version = self.stream.readBits(self.absolute_address + 32 + 16 + 12, 4, self.endian)

    def createFields(self):
        if self.version == 1:
            yield TimestampUUID60(self, "time")
            yield Enum(Bits(self, "version", 4), self.VERSION_NAME)
            yield Enum(Bits(self, "variant", 3), self.VARIANT_NAME)
            yield textHandler(Bits(self, "clock", 13), hexadecimal)
#            yield textHandler(Bits(self, "clock", 16), hexadecimal)
            if self.version == 1:
                yield MAC48_Address(self, "mac", "IEEE 802 MAC address")
            else:
                yield Bytes(self, "node", 6)
        else:
            namea, nameb = self.FIELD_NAMES.get(
                self.version, ("data_a", "data_b"))
            yield textHandler(Bits(self, namea, 60), hexadecimal)
            yield Enum(Bits(self, "version", 4), self.VERSION_NAME)
            yield Enum(Bits(self, "variant", 3), self.VARIANT_NAME)
            yield textHandler(Bits(self, nameb, 61), hexadecimal)

    def createValue(self):
        addr = self.absolute_address
        a = self.stream.readBits (addr,      32, self.endian)
        b = self.stream.readBits (addr + 32, 16, self.endian)
        c = self.stream.readBits (addr + 48, 16, self.endian)
        d = self.stream.readBytes(addr + 64, 2)
        e = self.stream.readBytes(addr + 80, 6)
        return "%08X-%04X-%04X-%s-%s" % (a, b, c, str2hex(d), str2hex(e))

    def createDisplay(self):
        value = self.value
        if value == self.NULL:
            name = "Null GUID: "
        else:
            name = "GUID v%u (%s): " % (self.version, self["version"].display)
        return name + value

    def createRawDisplay(self):
        value = self.stream.readBytes(self.absolute_address, 16)
        return str2hex(value, format=r"\x%02x")

class BitmapInfoHeader(FieldSet):
    """ Win32 BITMAPINFOHEADER structure from GDI """
    static_size = 40*8

    COMPRESSION_NAME = {
        0: u"Uncompressed (RGB)",
        1: u"RLE (8 bits)",
        2: u"RLE (4 bits)",
        3: u"Bitfields",
        4: u"JPEG",
        5: u"PNG"
    }

    def __init__(self, parent, name, use_fourcc=False):
        FieldSet.__init__(self, parent, name)
        self._use_fourcc = use_fourcc

    def createFields(self):
        yield UInt32(self, "hdr_size", "Header size (in bytes) (=40)")
        yield UInt32(self, "width", "Width")
        yield UInt32(self, "height", "Height")
        yield UInt16(self, "nb_planes", "Color planes")
        yield UInt16(self, "bpp", "Bits/pixel")
        if self._use_fourcc:
            yield Enum(String(self, "codec", 4, charset="ASCII"), video_fourcc_name)
        else:
            yield Enum(UInt32(self, "codec", "Compression"), self.COMPRESSION_NAME)
        yield UInt32(self, "size", "Image size (in bytes)")
        yield UInt32(self, "xres", "X pixels per meter")
        yield UInt32(self, "yres", "Y pixels per meter")
        yield UInt32(self, "color_used", "Number of used colors")
        yield UInt32(self, "color_important", "Number of important colors")

    def createDescription(self):
        return "Bitmap info header: %ux%u pixels, %u bits/pixel" % \
            (self["width"].value, self["height"].value, self["bpp"].value)


########NEW FILE########
__FILENAME__ = win32_lang_id
"""
Windows 2000 - List of Locale IDs and Language Groups

Original data table:
http://www.microsoft.com/globaldev/reference/win2k/setup/lcid.mspx
"""

LANGUAGE_ID = {
    0x0436: u"Afrikaans",
    0x041c: u"Albanian",
    0x0401: u"Arabic Saudi Arabia",
    0x0801: u"Arabic Iraq",
    0x0c01: u"Arabic Egypt",
    0x1001: u"Arabic Libya",
    0x1401: u"Arabic Algeria",
    0x1801: u"Arabic Morocco",
    0x1c01: u"Arabic Tunisia",
    0x2001: u"Arabic Oman",
    0x2401: u"Arabic Yemen",
    0x2801: u"Arabic Syria",
    0x2c01: u"Arabic Jordan",
    0x3001: u"Arabic Lebanon",
    0x3401: u"Arabic Kuwait",
    0x3801: u"Arabic UAE",
    0x3c01: u"Arabic Bahrain",
    0x4001: u"Arabic Qatar",
    0x042b: u"Armenian",
    0x042c: u"Azeri Latin",
    0x082c: u"Azeri Cyrillic",
    0x042d: u"Basque",
    0x0423: u"Belarusian",
    0x0402: u"Bulgarian",
    0x0403: u"Catalan",
    0x0404: u"Chinese Taiwan",
    0x0804: u"Chinese PRC",
    0x0c04: u"Chinese Hong Kong",
    0x1004: u"Chinese Singapore",
    0x1404: u"Chinese Macau",
    0x041a: u"Croatian",
    0x0405: u"Czech",
    0x0406: u"Danish",
    0x0413: u"Dutch Standard",
    0x0813: u"Dutch Belgian",
    0x0409: u"English United States",
    0x0809: u"English United Kingdom",
    0x0c09: u"English Australian",
    0x1009: u"English Canadian",
    0x1409: u"English New Zealand",
    0x1809: u"English Irish",
    0x1c09: u"English South Africa",
    0x2009: u"English Jamaica",
    0x2409: u"English Caribbean",
    0x2809: u"English Belize",
    0x2c09: u"English Trinidad",
    0x3009: u"English Zimbabwe",
    0x3409: u"English Philippines",
    0x0425: u"Estonian",
    0x0438: u"Faeroese",
    0x0429: u"Farsi",
    0x040b: u"Finnish",
    0x040c: u"French Standard",
    0x080c: u"French Belgian",
    0x0c0c: u"French Canadian",
    0x100c: u"French Swiss",
    0x140c: u"French Luxembourg",
    0x180c: u"French Monaco",
    0x0437: u"Georgian",
    0x0407: u"German Standard",
    0x0807: u"German Swiss",
    0x0c07: u"German Austrian",
    0x1007: u"German Luxembourg",
    0x1407: u"German Liechtenstein",
    0x0408: u"Greek",
    0x040d: u"Hebrew",
    0x0439: u"Hindi",
    0x040e: u"Hungarian",
    0x040f: u"Icelandic",
    0x0421: u"Indonesian",
    0x0410: u"Italian Standard",
    0x0810: u"Italian Swiss",
    0x0411: u"Japanese",
    0x043f: u"Kazakh",
    0x0457: u"Konkani",
    0x0412: u"Korean",
    0x0426: u"Latvian",
    0x0427: u"Lithuanian",
    0x042f: u"Macedonian",
    0x043e: u"Malay Malaysia",
    0x083e: u"Malay Brunei Darussalam",
    0x044e: u"Marathi",
    0x0414: u"Norwegian Bokmal",
    0x0814: u"Norwegian Nynorsk",
    0x0415: u"Polish",
    0x0416: u"Portuguese Brazilian",
    0x0816: u"Portuguese Standard",
    0x0418: u"Romanian",
    0x0419: u"Russian",
    0x044f: u"Sanskrit",
    0x081a: u"Serbian Latin",
    0x0c1a: u"Serbian Cyrillic",
    0x041b: u"Slovak",
    0x0424: u"Slovenian",
    0x040a: u"Spanish Traditional Sort",
    0x080a: u"Spanish Mexican",
    0x0c0a: u"Spanish Modern Sort",
    0x100a: u"Spanish Guatemala",
    0x140a: u"Spanish Costa Rica",
    0x180a: u"Spanish Panama",
    0x1c0a: u"Spanish Dominican Republic",
    0x200a: u"Spanish Venezuela",
    0x240a: u"Spanish Colombia",
    0x280a: u"Spanish Peru",
    0x2c0a: u"Spanish Argentina",
    0x300a: u"Spanish Ecuador",
    0x340a: u"Spanish Chile",
    0x380a: u"Spanish Uruguay",
    0x3c0a: u"Spanish Paraguay",
    0x400a: u"Spanish Bolivia",
    0x440a: u"Spanish El Salvador",
    0x480a: u"Spanish Honduras",
    0x4c0a: u"Spanish Nicaragua",
    0x500a: u"Spanish Puerto Rico",
    0x0441: u"Swahili",
    0x041d: u"Swedish",
    0x081d: u"Swedish Finland",
    0x0449: u"Tamil",
    0x0444: u"Tatar",
    0x041e: u"Thai",
    0x041f: u"Turkish",
    0x0422: u"Ukrainian",
    0x0420: u"Urdu",
    0x0443: u"Uzbek Latin",
    0x0843: u"Uzbek Cyrillic",
    0x042a: u"Vietnamese",
}


########NEW FILE########
__FILENAME__ = action_script
"""
SWF (Macromedia/Adobe Flash) file parser.

Documentation:

 - Alexis' SWF Reference:
   http://www.m2osw.com/swf_alexref.html

Author: Sebastien Ponce
Creation date: 26 April 2008
"""

from lib.hachoir_core.field import (FieldSet, ParserError,
    Bit, Bits, UInt8, UInt32, Int16, UInt16, Float32, CString,
    RawBytes)
#from lib.hachoir_core.field import Field
from lib.hachoir_core.field.float import FloatExponent
from struct import unpack

class FlashFloat64(FieldSet):
    def createFields(self):
        yield Bits(self, "mantisa_high", 20)
        yield FloatExponent(self, "exponent", 11)
        yield Bit(self, "negative")
        yield Bits(self, "mantisa_low", 32)

    def createValue(self):
        # Manual computation:
        # mantisa = mantisa_high * 2^32 + mantisa_low
        # float = 2^exponent + (1 + mantisa / 2^52)
        # (and float is negative if negative=True)
        bytes = self.parent.stream.readBytes(
            self.absolute_address, self.size//8)
        # Mix bytes: xxxxyyyy <=> yyyyxxxx
        bytes = bytes[4:8] + bytes[0:4]
        return unpack('<d', bytes)[0]

TYPE_INFO = {
    0x00: (CString, "Cstring[]"),
    0x01: (Float32, "Float[]"),
    0x02: (None, "Null[]"),
    0x03: (None, "Undefined[]"),
    0x04: (UInt8, "Register[]"),
    0x05: (UInt8, "Boolean[]"),
    0x06: (FlashFloat64, "Double[]"),
    0x07: (UInt32, "Integer[]"),
    0x08: (UInt8, "Dictionnary_Lookup_Index[]"),
    0x09: (UInt16, "Large_Dictionnary_Lookup_Index[]"),
}

def parseBranch(parent, size):
    yield Int16(parent, "offset")

def parseDeclareFunction(parent, size):
    yield CString(parent, "name")
    argCount = UInt16(parent, "arg_count")
    yield argCount
    for i in range(argCount.value):
        yield CString(parent, "arg[]")
    yield UInt16(parent, "function_length")

def parseDeclareFunctionV7(parent, size):
    yield CString(parent, "name")
    argCount = UInt16(parent, "arg_count")
    yield argCount
    yield UInt8(parent, "reg_count")
    yield Bits(parent, "reserved", 7)
    yield Bit(parent, "preload_global")
    yield Bit(parent, "preload_parent")
    yield Bit(parent, "preload_root")
    yield Bit(parent, "suppress_super")
    yield Bit(parent, "preload_super")
    yield Bit(parent, "suppress_arguments")
    yield Bit(parent, "preload_arguments")
    yield Bit(parent, "suppress_this")
    yield Bit(parent, "preload_this")
    for i in range(argCount.value):
        yield UInt8(parent, "register[]")
        yield CString(parent, "arg[]")
    yield UInt16(parent, "function_length")

def parseTry(parent, size):
    yield Bits(parent, "reserved", 5)
    catchInReg = Bit(parent, "catch_in_register")
    yield catchInReg
    yield Bit(parent, "finally")
    yield Bit(parent, "catch")
    yield UInt8(parent, "try_size")
    yield UInt8(parent, "catch_size")
    yield UInt8(parent, "finally_size")
    if catchInReg.value:
        yield CString(parent, "name")
    else:
        yield UInt8(parent, "register")

def parsePushData(parent, size):
    while not parent.eof:
        codeobj = UInt8(parent, "data_type[]")
        yield codeobj
        code = codeobj.value
        if code not in TYPE_INFO:
            raise ParserError("Unknown type in Push_Data : " + hex(code))
        parser, name = TYPE_INFO[code]
        if parser:
            yield parser(parent, name)
#        else:
#            yield Field(parent, name, 0)

def parseSetTarget(parent, size):
    yield CString(parent, "target")

def parseWith(parent, size):
    yield UInt16(parent, "size")

def parseGetURL(parent, size):
    yield CString(parent, "url")
    yield CString(parent, "target")

def parseGetURL2(parent, size):
    yield UInt8(parent, "method")

def parseGotoExpression(parent, size):
    yield UInt8(parent, "play")

def parseGotoFrame(parent, size):
    yield UInt16(parent, "frame_no")

def parseGotoLabel(parent, size):
    yield CString(parent, "label")

def parseWaitForFrame(parent, size):
    yield UInt16(parent, "frame")
    yield UInt8(parent, "skip")

def parseWaitForFrameDyn(parent, size):
    yield UInt8(parent, "skip")

def parseDeclareDictionnary(parent, size):
    count = UInt16(parent, "count")
    yield count
    for i in range(count.value):
        yield CString(parent, "dictionnary[]")

def parseStoreRegister(parent, size):
    yield UInt8(parent, "register")

def parseStrictMode(parent, size):
    yield UInt8(parent, "strict")

class Instruction(FieldSet):
    ACTION_INFO = {
        0x00: ("end[]", "End", None),
        0x99: ("Branch_Always[]", "Branch Always", parseBranch),
        0x9D: ("Branch_If_True[]", "Branch If True", parseBranch),
        0x3D: ("Call_Function[]", "Call Function", None),
        0x52: ("Call_Method[]", "Call Method", None),
        0x9B: ("Declare_Function[]", "Declare Function", parseDeclareFunction),
        0x8E: ("Declare_Function_V7[]", "Declare Function (V7)", parseDeclareFunctionV7),
        0x3E: ("Return[]", "Return", None),
        0x2A: ("Throw[]", "Throw", None),
        0x8F: ("Try[]", "Try", parseTry),
        # Stack Control
        0x4C: ("Duplicate[]", "Duplicate", None),
        0x96: ("Push_Data[]", "Push Data", parsePushData),
        0x4D: ("Swap[]", "Swap", None),
        # Action Script Context
        0x8B: ("Set_Target[]", "Set Target", parseSetTarget),
        0x20: ("Set_Target_dynamic[]", "Set Target (dynamic)", None),
        0x94: ("With[]", "With", parseWith),
        # Movie Control
        0x9E: ("Call_Frame[]", "Call Frame", None),
        0x83: ("Get_URL[]", "Get URL", parseGetURL),
        0x9A: ("Get_URL2[]", "Get URL2", parseGetURL2),
        0x9F: ("Goto_Expression[]", "Goto Expression", parseGotoExpression),
        0x81: ("Goto_Frame[]", "Goto Frame", parseGotoFrame),
        0x8C: ("Goto_Label[]", "Goto Label", parseGotoLabel),
        0x04: ("Next_Frame[]", "Next Frame", None),
        0x06: ("Play[]", "Play", None),
        0x05: ("Previous_Frame[]", "Previous Frame", None),
        0x07: ("Stop[]", "Stop", None),
        0x08: ("Toggle_Quality[]", "Toggle Quality", None),
        0x8A: ("Wait_For_Frame[]", "Wait For Frame", parseWaitForFrame),
        0x8D: ("Wait_For_Frame_dynamic[]", "Wait For Frame (dynamic)", parseWaitForFrameDyn),
        # Sound
        0x09: ("Stop_Sound[]", "Stop Sound", None),
        # Arithmetic
        0x0A: ("Add[]", "Add", None),
        0x47: ("Add_typed[]", "Add (typed)", None),
        0x51: ("Decrement[]", "Decrement", None),
        0x0D: ("Divide[]", "Divide", None),
        0x50: ("Increment[]", "Increment", None),
        0x18: ("Integral_Part[]", "Integral Part", None),
        0x3F: ("Modulo[]", "Modulo", None),
        0x0C: ("Multiply[]", "Multiply", None),
        0x4A: ("Number[]", "Number", None),
        0x0B: ("Subtract[]", "Subtract", None),
        # Comparisons
        0x0E: ("Equal[]", "Equal", None),
        0x49: ("Equal_typed[]", "Equal (typed)", None),
        0x66: ("Strict_Equal[]", "Strict Equal", None),
        0x67: ("Greater_Than_typed[]", "Greater Than (typed)", None),
        0x0F: ("Less_Than[]", "Less Than", None),
        0x48: ("Less_Than_typed[]", "Less Than (typed)", None),
        0x13: ("String_Equal[]", "String Equal", None),
        0x68: ("String_Greater_Than[]", "String Greater Than", None),
        0x29: ("String_Less_Than[]", "String Less Than", None),
        # Logical and Bit Wise
        0x60: ("And[]", "And", None),
        0x10: ("Logical_And[]", "Logical And", None),
        0x12: ("Logical_Not[]", "Logical Not", None),
        0x11: ("Logical_Or[]", "Logical Or", None),
        0x61: ("Or[]", "Or", None),
        0x63: ("Shift_Left[]", "Shift Left", None),
        0x64: ("Shift_Right[]", "Shift Right", None),
        0x65: ("Shift_Right_Unsigned[]", "Shift Right Unsigned", None),
        0x62: ("Xor[]", "Xor", None),
        # Strings & Characters (See the String Object also)
        0x33: ("Chr[]", "Chr", None),
        0x37: ("Chr_multi-bytes[]", "Chr (multi-bytes)", None),
        0x21: ("Concatenate_Strings[]", "Concatenate Strings", None),
        0x32: ("Ord[]", "Ord", None),
        0x36: ("Ord_multi-bytes[]", "Ord (multi-bytes)", None),
        0x4B: ("String[]", "String", None),
        0x14: ("String_Length[]", "String Length", None),
        0x31: ("String_Length_multi-bytes[]", "String Length (multi-bytes)", None),
        0x15: ("SubString[]", "SubString", None),
        0x35: ("SubString_multi-bytes[]", "SubString (multi-bytes)", None),
        # Properties
        0x22: ("Get_Property[]", "Get Property", None),
        0x23: ("Set_Property[]", "Set Property", None),
        # Objects
        0x2B: ("Cast_Object[]", "Cast Object", None),
        0x42: ("Declare_Array[]", "Declare Array", None),
        0x88: ("Declare_Dictionary[]", "Declare Dictionary", parseDeclareDictionnary),
        0x43: ("Declare_Object[]", "Declare Object", None),
        0x3A: ("Delete[]", "Delete", None),
        0x3B: ("Delete_All[]", "Delete All", None),
        0x24: ("Duplicate_Sprite[]", "Duplicate Sprite", None),
        0x46: ("Enumerate[]", "Enumerate", None),
        0x55: ("Enumerate_Object[]", "Enumerate Object", None),
        0x69: ("Extends[]", "Extends", None),
        0x4E: ("Get_Member[]", "Get Member", None),
        0x45: ("Get_Target[]", "Get Target", None),
        0x2C: ("Implements[]", "Implements", None),
        0x54: ("Instance_Of[]", "Instance Of", None),
        0x40: ("New[]", "New", None),
        0x53: ("New_Method[]", "New Method", None),
        0x25: ("Remove_Sprite[]", "Remove Sprite", None),
        0x4F: ("Set_Member[]", "Set Member", None),
        0x44: ("Type_Of[]", "Type Of", None),
        # Variables
        0x41: ("Declare_Local_Variable[]", "Declare Local Variable", None),
        0x1C: ("Get_Variable[]", "Get Variable", None),
        0x3C: ("Set_Local_Variable[]", "Set Local Variable", None),
        0x1D: ("Set_Variable[]", "Set Variable", None),
        # Miscellaneous
        0x2D: ("FSCommand2[]", "FSCommand2", None),
        0x34: ("Get_Timer[]", "Get Timer", None),
        0x30: ("Random[]", "Random", None),
        0x27: ("Start_Drag[]", "Start Drag", None),
        0x28: ("Stop_Drag[]", "Stop Drag", None),
        0x87: ("Store_Register[]", "Store Register", parseStoreRegister),
        0x89: ("Strict_Mode[]", "Strict Mode", parseStrictMode),
        0x26: ("Trace[]", "Trace", None),
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        code = self["action_id"].value
        if code & 128:
            self._size = (3 + self["action_length"].value) * 8
        else:
            self._size = 8
        if code in self.ACTION_INFO:
            self._name, self._description, self.parser = self.ACTION_INFO[code]
        else:
            self.parser = None

    def createFields(self):
        yield Bits(self, "action_id", 8)
        if not (self["action_id"].value & 128):
            return
        yield UInt16(self, "action_length")
        size = self["action_length"].value
        if not size:
            return
        if self.parser:
            for field in self.parser(self, size):
                yield field
        else:
            yield RawBytes(self, "action_data", size)

    def createDescription(self):
        return self._description

    def __str__(self):
        r = str(self._description)
        for f in self:
            if f.name not in ("action_id", "action_length", "count") and not f.name.startswith("data_type") :
                r = r + "\n   " + str((self.address+f.address)/8) + " " + str(f.name) + "=" + str(f.value)
        return r

class ActionScript(FieldSet):
    def createFields(self):
        while not self.eof:
            yield Instruction(self, "instr[]")

    def __str__(self):
        r = ""
        for f in self:
            r = r + str(f.address/8) + " " + str(f) + "\n"
        return r

def parseActionScript(parent, size):
    yield ActionScript(parent, "action", size=size*8)


########NEW FILE########
__FILENAME__ = asn1
"""
Abstract Syntax Notation One (ASN.1) parser.

Technical informations:
* PER standard
  http://www.tu.int/ITU-T/studygroups/com17/languages/X.691-0207.pdf
* Python library
  http://pyasn1.sourceforge.net/
* Specification of Abstract Syntax Notation One (ASN.1)
  ISO/IEC 8824:1990 Information Technology
* Specification of Basic Encoding Rules (BER) for ASN.1
  ISO/IEC 8825:1990 Information Technology
* OpenSSL asn1parser, use command:
  openssl asn1parse -i -inform DER -in file.der
* ITU-U recommendations:
  http://www.itu.int/rec/T-REC-X/en
  (X.680, X.681, X.682, X.683, X.690, X.691, X.692, X.693, X.694)
* dumpasn1
  http://www.cs.auckland.ac.nz/~pgut001/dumpasn1.c

General information:
* Wikipedia (english) article
  http://en.wikipedia.org/wiki/Abstract_Syntax_Notation_One
* ASN.1 information site
  http://asn1.elibel.tm.fr/en/
* ASN.1 consortium
  http://www.asn1.org/

Encodings:
* Basic Encoding Rules (BER)
* Canonical Encoding Rules (CER) -- DER derivative that is not widely used
* Distinguished Encoding Rules (DER) -- used for encrypted applications
* XML Encoding Rules (XER)
* Packed Encoding Rules (PER) -- result in the fewest number of bytes
* Generic String Encoding Rules (GSER)
=> Are encodings compatibles? Which encodings are supported??

Author: Victor Stinner
Creation date: 24 september 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    FieldError, ParserError,
    Bit, Bits, Bytes, UInt8, GenericInteger, String,
    Field, Enum, RawBytes)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.tools import createDict, humanDatetime
from lib.hachoir_core.stream import InputStreamError
from lib.hachoir_core.text_handler import textHandler

# --- Field parser ---

class ASNInteger(Field):
    """
    Integer: two cases:
    - first byte in 0..127: it's the value
    - first byte in 128..255: byte & 127 is the number of bytes,
      next bytes are the value
    """
    def __init__(self, parent, name, description=None):
        Field.__init__(self, parent, name, 8, description)
        stream = self._parent.stream
        addr = self.absolute_address
        value = stream.readBits(addr, 8, BIG_ENDIAN)
        if 128 <= value:
            nbits = (value & 127) * 8
            if not nbits:
                raise ParserError("ASN.1: invalid ASN integer size (zero)")
            if 64 < nbits:
                # Arbitrary limit to catch errors
                raise ParserError("ASN.1: ASN integer is limited to 64 bits")
            self._size = 8 + nbits
            value = stream.readBits(addr+8, nbits, BIG_ENDIAN)
        self.createValue = lambda: value

class OID_Integer(Bits):
    def __init__(self, parent, name, description=None):
        Bits.__init__(self, parent, name, 8, description)
        stream = self._parent.stream
        addr = self.absolute_address
        size = 8
        value = 0
        byte = stream.readBits(addr, 8, BIG_ENDIAN)
        value = byte & 127
        while 128 <= byte:
            addr += 8
            size += 8
            if 64 < size:
                # Arbitrary limit to catch errors
                raise ParserError("ASN.1: Object identifier is limited 64 bits")
            byte = stream.readBits(addr, 8, BIG_ENDIAN)
            value = (value << 7) + (byte & 127)
        self._size = size
        self.createValue = lambda: value

def readSequence(self, content_size):
    while self.current_size < self.size:
        yield Object(self, "item[]")

def readSet(self, content_size):
    yield Object(self, "value", size=content_size*8)

def readASCIIString(self, content_size):
    yield String(self, "value", content_size, charset="ASCII")

def readUTF8String(self, content_size):
    yield String(self, "value", content_size, charset="UTF-8")

def readBMPString(self, content_size):
    yield String(self, "value", content_size, charset="UTF-16")

def readBitString(self, content_size):
    yield UInt8(self, "padding_size", description="Number of unused bits")
    if content_size > 1:
        yield Bytes(self, "value", content_size-1)

def readOctetString(self, content_size):
    yield Bytes(self, "value", content_size)

def formatObjectID(fieldset):
    text = [ fieldset["first"].display ]
    items = [ field for field in fieldset if field.name.startswith("item[") ]
    text.extend( str(field.value) for field in items )
    return ".".join(text)

def readObjectID(self, content_size):
    yield textHandler(UInt8(self, "first"), formatFirstObjectID)
    while self.current_size < self.size:
        yield OID_Integer(self, "item[]")

def readBoolean(self, content_size):
    if content_size != 1:
        raise ParserError("Overlong boolean: got %s bytes, expected 1 byte"%content_size)
    yield textHandler(UInt8(self, "value"), lambda field:str(bool(field.value)))

def readInteger(self, content_size):
    # Always signed?
    yield GenericInteger(self, "value", True, content_size*8)

# --- Format ---

def formatFirstObjectID(field):
    value = field.value
    return "%u.%u" % (value // 40, value % 40)

def formatValue(fieldset):
    return fieldset["value"].display

def formatUTCTime(fieldset):
    import datetime
    value = fieldset["value"].value
    year = int(value[0:2])
    if year < 50:
        year += 2000
    else:
        year += 1900
    month = int(value[2:4])
    day = int(value[4:6])
    hour = int(value[6:8])
    minute = int(value[8:10])
    if value[-1] == "Z":
        second = int(value[10:12])
        dt = datetime.datetime(year, month, day, hour, minute, second)
    else:
        # Skip timezone...
        dt = datetime.datetime(year, month, day, hour, minute)
    return humanDatetime(dt)

# --- Object parser ---

class Object(FieldSet):
    TYPE_INFO = {
        0: ("end[]", None, "End (reserved for BER, None)", None), # TODO: Write parser
        1: ("boolean[]", readBoolean, "Boolean", None),
        2: ("integer[]", readInteger, "Integer", None),
        3: ("bit_str[]", readBitString, "Bit string", None),
        4: ("octet_str[]", readOctetString, "Octet string", None),
        5: ("null[]", None, "NULL (empty, None)", None),
        6: ("obj_id[]", readObjectID, "Object identifier", formatObjectID),
        7: ("obj_desc[]", None, "Object descriptor", None), # TODO: Write parser
        8: ("external[]", None, "External, instance of", None), # TODO: Write parser # External?
        9: ("real[]", readASCIIString, "Real number", None), # TODO: Write parser
        10: ("enum[]", readInteger, "Enumerated", None),
        11: ("embedded[]", None, "Embedded PDV", None), # TODO: Write parser
        12: ("utf8_str[]", readUTF8String, "Printable string", None),
        13: ("rel_obj_id[]", None, "Relative object identifier", None), # TODO: Write parser
        14: ("time[]", None, "Time", None), # TODO: Write parser
      # 15: invalid??? sequence of???
        16: ("seq[]", readSequence, "Sequence", None),
        17: ("set[]", readSet, "Set", None),
        18: ("num_str[]", readASCIIString, "Numeric string", None),
        19: ("print_str[]", readASCIIString, "Printable string", formatValue),
        20: ("teletex_str[]", readASCIIString, "Teletex (T61, None) string", None),
        21: ("videotex_str[]", readASCIIString, "Videotex string", None),
        22: ("ia5_str[]", readASCIIString, "IA5 string", formatValue),
        23: ("utc_time[]", readASCIIString, "UTC time", formatUTCTime),
        24: ("general_time[]", readASCIIString, "Generalized time", None),
        25: ("graphic_str[]", readASCIIString, "Graphic string", None),
        26: ("visible_str[]", readASCIIString, "Visible (ISO64, None) string", None),
        27: ("general_str[]", readASCIIString, "General string", None),
        28: ("universal_str[]", readASCIIString, "Universal string", None),
        29: ("unrestricted_str[]", readASCIIString, "Unrestricted string", None),
        30: ("bmp_str[]", readBMPString, "BMP string", None),
      # 31: multiple octet tag number, TODO: not supported

      # Extended tag values:
      #   31: Date
      #   32: Time of day
      #   33: Date-time
      #   34: Duration
    }
    TYPE_DESC = createDict(TYPE_INFO, 2)

    CLASS_DESC = {0: "universal", 1: "application", 2: "context", 3: "private"}
    FORM_DESC = {False: "primitive", True: "constructed"}

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        key = self["type"].value & 31
        if self['class'].value == 0:
            # universal object
            if key in self.TYPE_INFO:
                self._name, self._handler, self._description, create_desc = self.TYPE_INFO[key]
                if create_desc:
                    self.createDescription = lambda: "%s: %s" % (self.TYPE_INFO[key][2], create_desc(self))
                    self._description = None
            elif key == 31:
                raise ParserError("ASN.1 Object: tag bigger than 30 are not supported")
            else:
                self._handler = None
        elif self['form'].value:
            # constructed: treat as sequence
            self._name = 'seq[]'
            self._handler = readSequence
            self._description = 'constructed object type %i' % key
        else:
            # primitive, context/private
            self._name = 'raw[]'
            self._handler = readASCIIString
            self._description = '%s object type %i' % (self['class'].display, key)
        field = self["size"]
        self._size = field.address + field.size + field.value*8

    def createFields(self):
        yield Enum(Bits(self, "class", 2), self.CLASS_DESC)
        yield Enum(Bit(self, "form"), self.FORM_DESC)
        if self['class'].value == 0:
            yield Enum(Bits(self, "type", 5), self.TYPE_DESC)
        else:
            yield Bits(self, "type", 5)
        yield ASNInteger(self, "size", "Size in bytes")
        size = self["size"].value
        if size:
            if self._handler:
                for field in self._handler(self, size):
                    yield field
            else:
                yield RawBytes(self, "raw", size)

class ASN1File(Parser):
    PARSER_TAGS = {
        "id": "asn1",
        "category": "container",
        "file_ext": ("der",),
        "min_size": 16,
        "description": "Abstract Syntax Notation One (ASN.1)"
    }
    endian = BIG_ENDIAN

    def validate(self):
        try:
            root = self[0]
        except (InputStreamError, FieldError):
            return "Unable to create root object"
        if root.size != self.size:
            return "Invalid root object size"
        return True

    def createFields(self):
        yield Object(self, "root")


########NEW FILE########
__FILENAME__ = mkv
#
# Matroska parser
# Author Julien Muchembled <jm AT jm10.no-ip.com>
# Created: 8 june 2006
#

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, Link,
    MissingField, ParserError,
    Enum as _Enum, String as _String,
    Float32, Float64,
    NullBits, Bits, Bit, RawBytes, Bytes,
    Int16, GenericInteger)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.iso639 import ISO639_2
from lib.hachoir_core.tools import humanDatetime
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_parser.container.ogg import XiphInt
from datetime import datetime, timedelta

class RawInt(GenericInteger):
    """
    Raw integer: have to be used in BIG_ENDIAN!
    """
    def __init__(self, parent, name, description=None):
        GenericInteger.__init__(self, parent, name, False, 8, description)
        i = GenericInteger.createValue(self)
        if i == 0:
            raise ParserError('Invalid integer length!')
        while i < 0x80:
            self._size += 8
            i <<= 1

class Unsigned(RawInt):
    def __init__(self, parent, name, description=None):
        RawInt.__init__(self, parent, name, description)

    def hasValue(self):
        return True
    def createValue(self):
        header = 1 << self._size / 8 * 7
        value = RawInt.createValue(self) - header
        if value + 1 == header:
            return None
        return value

class Signed(Unsigned):
    def createValue(self):
        header = 1 << self._size / 8 * 7 - 1
        value = RawInt.createValue(self) - 3 * header + 1
        if value == header:
            return None
        return value

def Enum(parent, enum):
    return _Enum(GenericInteger(parent, 'enum', False, parent['size'].value*8), enum)

def Bool(parent):
    return textHandler(GenericInteger(parent, 'bool', False, parent['size'].value*8),
        lambda chunk: str(chunk.value != 0))

def UInt(parent):
    return GenericInteger(parent, 'unsigned', False, parent['size'].value*8)

def SInt(parent):
    return GenericInteger(parent, 'signed', True, parent['size'].value*8)

def String(parent):
    return _String(parent, 'string', parent['size'].value, charset="ASCII")

def EnumString(parent, enum):
    return _Enum(String(parent), enum)

def Binary(parent):
    return RawBytes(parent, 'binary', parent['size'].value)

class AttachedFile(Bytes):
    def __init__(self, parent):
        Bytes.__init__(self, parent, 'file', parent['size'].value, None)
    def _getFilename(self):
        if not hasattr(self, "_filename"):
            try:
                self._filename = self["../../FileName/unicode"].value
            except MissingField:
                self._filename = None
        return self._filename
    def createDescription(self):
        filename = self._getFilename()
        if filename:
            return 'File "%s"' % filename
        return "('Filename' entry not found)"
    def _createInputStream(self, **args):
        tags = args.setdefault("tags",[])
        try:
            tags.append(("mime", self["../../FileMimeType/string"].value))
        except MissingField:
            pass
        filename = self._getFilename()
        if filename:
            tags.append(("filename", filename))
        return Bytes._createInputStream(self, **args)

def UTF8(parent):
    return _String(parent,'unicode', parent['size'].value, charset='UTF-8')

def Float(parent):
    size = parent['size'].value
    if size == 4:
        return Float32(parent, 'float')
    elif size == 8:
        return Float64(parent, 'double')
    else:
        return RawBytes(parent, 'INVALID_FLOAT', size)

TIMESTAMP_T0 = datetime(2001, 1, 1)

def dateToDatetime(value):
    return TIMESTAMP_T0 + timedelta(microseconds=value//1000)

def dateToString(field):
    return humanDatetime(dateToDatetime(field.value))

def Date(parent):
    return textHandler(GenericInteger(parent, 'date', True, parent['size'].value*8),
        dateToString)

def SeekID(parent):
    return textHandler(GenericInteger(parent, 'binary', False, parent['size'].value*8),
        lambda chunk: segment.get(chunk.value, (hexadecimal(chunk),))[0])

def CueClusterPosition(parent):
    class Cluster(Link):
        def createValue(self):
            parent = self.parent
            segment = parent['.....']
            pos = parent['unsigned'].value * 8 + segment[2].address
            return segment.getFieldByAddress(pos, feed=False)
    return Cluster(parent, 'cluster')

def CueTrackPositions(parent):
    class Block(Link):
        def createValue(self):
            parent = self.parent
            time = parent['../CueTime/unsigned'].value
            track = parent['CueTrack/unsigned'].value
            cluster = parent['CueClusterPosition/cluster'].value
            time -= cluster['Timecode/unsigned'].value
            for field in cluster:
                if field.name.startswith('BlockGroup['):
                    for path in 'Block/block', 'SimpleBlock':
                        try:
                            block = field[path]
                            if block['track'].value == track and \
                               block['timecode'].value == time:
                                return field
                        except MissingField:
                            pass
            parent.error('Cue point not found')
            return self
    return Block(parent, 'block')

class Lace(FieldSet):
    def __init__(self, parent, lacing, size):
        self.n_frames = parent['n_frames'].value
        self.createFields = ( self.parseXiph, self.parseFixed, self.parseEBML )[lacing]
        FieldSet.__init__(self, parent, 'Lace', size=size * 8)

    def parseXiph(self):
        for i in xrange(self.n_frames):
            yield XiphInt(self, 'size[]')
        for i in xrange(self.n_frames):
            yield RawBytes(self, 'frame[]', self['size['+str(i)+']'].value)
        yield RawBytes(self,'frame[]', (self._size - self.current_size) / 8)

    def parseEBML(self):
        yield Unsigned(self, 'size')
        for i in xrange(1, self.n_frames):
            yield Signed(self, 'dsize[]')
        size = self['size'].value
        yield RawBytes(self, 'frame[]', size)
        for i in xrange(self.n_frames-1):
            size += self['dsize['+str(i)+']'].value
            yield RawBytes(self, 'frame[]', size)
        yield RawBytes(self,'frame[]', (self._size - self.current_size) / 8)

    def parseFixed(self):
        n = self.n_frames + 1
        size = self._size / 8 / n
        for i in xrange(n):
            yield RawBytes(self, 'frame[]', size)

class Block(FieldSet):
    def __init__(self, parent):
        FieldSet.__init__(self, parent, 'block')
        self._size = 8 * parent['size'].value

    def lacing(self):
        return _Enum(Bits(self, 'lacing', 2), [ 'none', 'Xiph', 'fixed', 'EBML' ])

    def createFields(self):
        yield Unsigned(self, 'track')
        yield Int16(self, 'timecode')

        if self.parent._name == 'Block':
            yield NullBits(self, 'reserved[]', 4)
            yield Bit(self, 'invisible')
            yield self.lacing()
            yield NullBits(self, 'reserved[]', 1)
        elif self.parent._name == 'SimpleBlock[]':
            yield Bit(self, 'keyframe')
            yield NullBits(self, 'reserved', 3)
            yield Bit(self, 'invisible')
            yield self.lacing()
            yield Bit(self, 'discardable')
        else:
            yield NullBits(self, 'reserved', 8)
            return

        size = (self._size - self.current_size) / 8
        lacing = self['lacing'].value
        if lacing:
            yield textHandler(GenericInteger(self, 'n_frames', False, 8),
                lambda chunk: str(chunk.value+1))
            yield Lace(self, lacing - 1, size - 1)
        else:
            yield RawBytes(self,'frame', size)

ebml = {
    0x1A45DFA3: ('EBML[]', {
        0x4286: ('EBMLVersion',UInt),
        0x42F7: ('EBMLReadVersion',UInt),
        0x42F2: ('EBMLMaxIDLength',UInt),
        0x42F3: ('EBMLMaxSizeLength',UInt),
        0x4282: ('DocType',String),
        0x4287: ('DocTypeVersion',UInt),
        0x4285: ('DocTypeReadVersion',UInt)
        })
}

signature = {
    0x7E8A: ('SignatureAlgo', UInt),
    0x7E9A: ('SignatureHash', UInt),
    0x7EA5: ('SignaturePublicKey', Binary),
    0x7EB5: ('Signature', Binary),
    0x7E5B: ('SignatureElements', {
        0x7E7B: ('SignatureElementList[]', {
            0x6532: ('SignedElement[]', Binary)
            })
        })
}

chapter_atom = {
    0x73C4: ('ChapterUID', UInt),
    0x91:   ('ChapterTimeStart', UInt),
    0x92:   ('ChapterTimeEnd', UInt),
    0x98:   ('ChapterFlagHidden', Bool),
    0x4598: ('ChapterFlagEnabled', Bool),
    0x6E67: ('ChapterSegmentUID', Binary),
    0x6EBC: ('ChapterSegmentEditionUID', Binary),
    0x63C3: ('ChapterPhysicalEquiv', UInt),
    0x8F:   ('ChapterTrack', {
        0x89:   ('ChapterTrackNumber[]', UInt)
        }),
    0x80:   ('ChapterDisplay[]', {
        0x85:   ('ChapString', UTF8),
        0x437C: ('ChapLanguage[]', String),
        0x437E: ('ChapCountry[]', String)
        }),
    0x6944: ('ChapProcess[]', {
        0x6955: ('ChapProcessCodecID', UInt),
        0x450D: ('ChapProcessPrivate', Binary),
        0x6911: ('ChapProcessCommand[]', {
        0x6922: ('ChapProcessTime', UInt),
        0x6933: ('ChapProcessData', Binary)
        })
        })
}

simple_tag = {
    0x45A3: ('TagName', UTF8),
    0x447A: ('TagLanguage', String),
    0x44B4: ('TagDefault', Bool), # 0x4484
    0x4487: ('TagString', UTF8),
    0x4485: ('TagBinary', Binary)
}

segment_seek = {
    0x4DBB:     ('Seek[]', {
        0x53AB:     ('SeekID', SeekID),
        0x53AC:     ('SeekPosition', UInt)
        })
}

segment_info = {
    0x73A4:     ('SegmentUID', Binary),
    0x7384:     ('SegmentFilename', UTF8),
    0x3CB923:   ('PrevUID', Binary),
    0x3C83AB:   ('PrevFilename', UTF8),
    0x3EB923:   ('NextUID', Binary),
    0x3E83BB:   ('NextFilename', UTF8),
    0x4444:     ('SegmentFamily[]', Binary),
    0x6924:     ('ChapterTranslate[]', {
        0x69FC:     ('ChapterTranslateEditionUID[]', UInt),
        0x69BF:     ('ChapterTranslateCodec', UInt),
        0x69A5:     ('ChapterTranslateID', Binary)
        }),
    0x2AD7B1:   ('TimecodeScale', UInt),
    0x4489:     ('Duration', Float),
    0x4461:     ('DateUTC', Date),
    0x7BA9:     ('Title', UTF8),
    0x4D80:     ('MuxingApp', UTF8),
    0x5741:     ('WritingApp', UTF8)
}

segment_clusters = {
    0xE7:       ('Timecode', UInt),
    0x5854:     ('SilentTracks', {
        0x58D7:     ('SilentTrackNumber[]', UInt)
        }),
    0xA7:       ('Position', UInt),
    0xAB:       ('PrevSize', UInt),
    0xA0:       ('BlockGroup[]', {
        0xA1:       ('Block', Block),
        0xA2:       ('BlockVirtual[]', Block),
        0x75A1:     ('BlockAdditions', {
            0xA6:       ('BlockMore[]', {
                0xEE:       ('BlockAddID', UInt),
                0xA5:       ('BlockAdditional', Binary)
                })
            }),
        0x9B:       ('BlockDuration', UInt),
        0xFA:       ('ReferencePriority', UInt),
        0xFB:       ('ReferenceBlock[]', SInt),
        0xFD:       ('ReferenceVirtual', SInt),
        0xA4:       ('CodecState', Binary),
        0x8E:       ('Slices[]', {
            0xE8:       ('TimeSlice[]', {
                0xCC:       ('LaceNumber', UInt),
                0xCD:       ('FrameNumber', UInt),
                0xCB:       ('BlockAdditionID', UInt),
                0xCE:       ('Delay', UInt),
                0xCF:       ('Duration', UInt)
                })
            })
        }),
    0xA3:       ('SimpleBlock[]', Block)
}

tracks_video = {
    0x9A:       ('FlagInterlaced', Bool),
    0x53B8:     ('StereoMode', lambda parent: Enum(parent, \
        [ 'mono', 'right eye', 'left eye', 'both eyes' ])),
    0xB0:       ('PixelWidth', UInt),
    0xBA:       ('PixelHeight', UInt),
    0x54AA:     ('PixelCropBottom', UInt),
    0x54BB:     ('PixelCropTop', UInt),
    0x54CC:     ('PixelCropLeft', UInt),
    0x54DD:     ('PixelCropRight', UInt),
    0x54B0:     ('DisplayWidth', UInt),
    0x54BA:     ('DisplayHeight', UInt),
    0x54B2:     ('DisplayUnit', lambda parent: Enum(parent, \
        [ 'pixels', 'centimeters', 'inches' ])),
    0x54B3:     ('AspectRatioType', lambda parent: Enum(parent, \
        [ 'free resizing', 'keep aspect ratio', 'fixed' ])),
    0x2EB524:   ('ColourSpace', Binary),
    0x2FB523:   ('GammaValue', Float)
}

tracks_audio = {
    0xB5:       ('SamplingFrequency', Float),
    0x78B5:     ('OutputSamplingFrequency', Float),
    0x9F:       ('Channels', UInt),
    0x7D7B:     ('ChannelPositions', Binary),
    0x6264:     ('BitDepth', UInt)
}

tracks_content_encodings = {
    0x6240:     ('ContentEncoding[]', {
        0x5031:     ('ContentEncodingOrder', UInt),
        0x5032:     ('ContentEncodingScope', UInt),
        0x5033:     ('ContentEncodingType', UInt),
        0x5034:     ('ContentCompression', {
            0x4254:     ('ContentCompAlgo', UInt),
            0x4255:     ('ContentCompSettings', Binary)
            }),
        0x5035:     ('ContentEncryption', {
            0x47e1:     ('ContentEncAlgo', UInt),
            0x47e2:     ('ContentEncKeyID', Binary),
            0x47e3:     ('ContentSignature', Binary),
            0x47e4:     ('ContentSigKeyID', Binary),
            0x47e5:     ('ContentSigAlgo', UInt),
            0x47e6:     ('ContentSigHashAlgo', UInt),
            })
        })
}

segment_tracks = {
    0xAE:       ('TrackEntry[]', {
        0xD7:       ('TrackNumber', UInt),
        0x73C5:     ('TrackUID', UInt),
        0x83:       ('TrackType', lambda parent: Enum(parent, {
            0x01: 'video',
            0x02: 'audio',
            0x03: 'complex',
            0x10: 'logo',
            0x11: 'subtitle',
            0x12: 'buttons',
            0x20: 'control'
            })),
        0xB9:       ('FlagEnabled', Bool),
        0x88:       ('FlagDefault', Bool),
        0x55AA:     ('FlagForced[]', Bool),
        0x9C:       ('FlagLacing', Bool),
        0x6DE7:     ('MinCache', UInt),
        0x6DF8:     ('MaxCache', UInt),
        0x23E383:   ('DefaultDuration', UInt),
        0x23314F:   ('TrackTimecodeScale', Float),
        0x537F:     ('TrackOffset', SInt),
        0x55EE:     ('MaxBlockAdditionID', UInt),
        0x536E:     ('Name', UTF8),
        0x22B59C:   ('Language', lambda parent: EnumString(parent, ISO639_2)),
        0x86:       ('CodecID', String),
        0x63A2:     ('CodecPrivate', Binary),
        0x258688:   ('CodecName', UTF8),
        0x7446:     ('AttachmentLink', UInt),
        0x3A9697:   ('CodecSettings', UTF8),
        0x3B4040:   ('CodecInfoURL[]', String),
        0x26B240:   ('CodecDownloadURL[]', String),
        0xAA:       ('CodecDecodeAll', Bool),
        0x6FAB:     ('TrackOverlay[]', UInt),
        0x6624:     ('TrackTranslate[]', {
            0x66FC:     ('TrackTranslateEditionUID[]', UInt),
            0x66BF:     ('TrackTranslateCodec', UInt),
            0x66A5:     ('TrackTranslateTrackID', Binary)
            }),
        0xE0:       ('Video', tracks_video),
        0xE1:       ('Audio', tracks_audio),
        0x6d80:     ('ContentEncodings', tracks_content_encodings)
        })
}

segment_cues = {
    0xBB:       ('CuePoint[]', {
        0xB3:       ('CueTime', UInt),
        0xB7:       ('CueTrackPositions[]', CueTrackPositions, {
            0xF7:       ('CueTrack', UInt),
            0xF1:       ('CueClusterPosition', CueClusterPosition, UInt),
            0x5378:     ('CueBlockNumber', UInt),
            0xEA:       ('CueCodecState', UInt),
            0xDB:       ('CueReference[]', {
                0x96:       ('CueRefTime', UInt),
                0x97:       ('CueRefCluster', UInt),
                0x535F:     ('CueRefNumber', UInt),
                0xEB:       ('CueRefCodecState', UInt)
                })
            })
        })
}

segment_attachments = {
    0x61A7:     ('AttachedFile[]', {
        0x467E:     ('FileDescription', UTF8),
        0x466E:     ('FileName', UTF8),
        0x4660:     ('FileMimeType', String),
        0x465C:     ('FileData', AttachedFile),
        0x46AE:     ('FileUID', UInt),
        0x4675:     ('FileReferral', Binary)
        })
}

segment_chapters = {
    0x45B9:     ('EditionEntry[]', {
        0x45BC:     ('EditionUID', UInt),
        0x45BD:     ('EditionFlagHidden', Bool),
        0x45DB:     ('EditionFlagDefault', Bool),
        0x45DD:     ('EditionFlagOrdered', Bool),
        0xB6:       ('ChapterAtom[]', chapter_atom)
        })
}

segment_tags = {
    0x7373:     ('Tag[]', {
        0x63C0:     ('Targets', {
            0x68CA:     ('TargetTypeValue', UInt),
            0x63CA:     ('TargetType', String),
            0x63C5:     ('TrackUID[]', UInt),
            0x63C9:     ('EditionUID[]', UInt),
            0x63C4:     ('ChapterUID[]', UInt),
            0x63C6:     ('AttachmentUID[]', UInt)
            }),
        0x67C8:     ('SimpleTag[]', simple_tag)
        })
}

segment = {
    0x114D9B74: ('SeekHead[]', segment_seek),
    0x1549A966: ('Info[]', segment_info),
    0x1F43B675: ('Cluster[]', segment_clusters),
    0x1654AE6B: ('Tracks[]', segment_tracks),
    0x1C53BB6B: ('Cues', segment_cues),
    0x1941A469: ('Attachments', segment_attachments),
    0x1043A770: ('Chapters', segment_chapters),
    0x1254C367: ('Tags[]', segment_tags)
}

class EBML(FieldSet):
    def __init__(self, parent, ids):
        FieldSet.__init__(self, parent, "?[]")

        # Set name
        id = self['id'].value
        self.val = ids.get(id)
        if not self.val:
            if id == 0xBF:
                self.val = 'CRC-32[]', Binary
            elif id == 0xEC:
                self.val = 'Void[]', Binary
            elif id == 0x1B538667:
                self.val = 'SignatureSlot[]', signature
            else:
                self.val = 'Unknown[]', Binary
        self._name = self.val[0]

        # Compute size
        size = self['size']
        if size.value is not None:
            self._size = size.address + size.size + size.value * 8
        elif self._parent._parent:
            raise ParserError("Unknown length (only allowed for the last Level 0 element)")
        elif self._parent._size is not None:
            self._size = self._parent._size - self.address

    def createFields(self):
        yield RawInt(self, 'id')
        yield Unsigned(self, 'size')
        for val in self.val[1:]:
            if callable(val):
                yield val(self)
            else:
                while not self.eof:
                    yield EBML(self, val)

class MkvFile(Parser):
    EBML_SIGNATURE = 0x1A45DFA3
    PARSER_TAGS = {
        "id": "matroska",
        "category": "container",
        "file_ext": ("mka", "mkv", "webm"),
        "mime": (
            u"video/x-matroska",
            u"audio/x-matroska",
            u"video/webm",
            u"audio/webm"),
        "min_size": 5*8,
        "magic": (("\x1A\x45\xDF\xA3", 0),),
        "description": "Matroska multimedia container"
    }
    endian = BIG_ENDIAN

    def _getDoctype(self):
        return self[0]['DocType/string'].value

    def validate(self):
        if self.stream.readBits(0, 32, self.endian) != self.EBML_SIGNATURE:
            return False
        try:
            first = self[0]
        except ParserError:
            return False
        if None < self._size < first._size:
            return "First chunk size is invalid"
        if self._getDoctype() not in ('matroska', 'webm'):
            return "Stream isn't a matroska document."
        return True

    def createFields(self):
        hdr = EBML(self, ebml)
        yield hdr

        while not self.eof:
            yield EBML(self, { 0x18538067: ('Segment[]', segment) })

    def createContentSize(self):
        field = self["Segment[0]/size"]
        return field.absolute_address + field.value * 8 + field.size

    def createDescription(self):
        if self._getDoctype() == 'webm':
            return 'WebM video'
        else:
            return 'Matroska video'

    def createMimeType(self):
        if self._getDoctype() == 'webm':
            return u"video/webm"
        else:
            return u"video/x-matroska"


########NEW FILE########
__FILENAME__ = ogg
#
# Ogg parser
# Author Julien Muchembled <jm AT jm10.no-ip.com>
# Created: 10 june 2006
#

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (Field, FieldSet, createOrphanField,
    NullBits, Bit, Bits, Enum, Fragment, MissingField, ParserError,
    UInt8, UInt16, UInt24, UInt32, UInt64,
    RawBytes, String, PascalString32, NullBytes)
from lib.hachoir_core.stream import FragmentedStream, InputStreamError
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN
from lib.hachoir_core.tools import humanDurationNanosec
from lib.hachoir_core.text_handler import textHandler, hexadecimal

MAX_FILESIZE = 1000 * 1024 * 1024

class XiphInt(Field):
    """
    Positive integer with variable size. Values bigger than 254 are stored as
    (255, 255, ..., rest): value is the sum of all bytes.

    Example: 1000 is stored as (255, 255, 255, 235), total = 255*3+235 = 1000
    """
    def __init__(self, parent, name, max_size=None, description=None):
        Field.__init__(self, parent, name, size=0, description=description)
        value = 0
        addr = self.absolute_address
        while max_size is None or self._size < max_size:
            byte = parent.stream.readBits(addr, 8, LITTLE_ENDIAN)
            value += byte
            self._size += 8
            if byte != 0xff:
                break
            addr += 8
        self.createValue = lambda: value

class Lacing(FieldSet):
    def createFields(self):
        size = self.size
        while size:
            field = XiphInt(self, 'size[]', size)
            yield field
            size -= field.size

def parseVorbisComment(parent):
    yield PascalString32(parent, 'vendor', charset="UTF-8")
    yield UInt32(parent, 'count')
    for index in xrange(parent["count"].value):
        yield PascalString32(parent, 'metadata[]', charset="UTF-8")
    if parent.current_size != parent.size:
        yield UInt8(parent, "framing_flag")

PIXEL_FORMATS = {
    0: "4:2:0",
    2: "4:2:2",
    3: "4:4:4",
}

def formatTimeUnit(field):
    return humanDurationNanosec(field.value * 100)

def parseVideoHeader(parent):
    yield NullBytes(parent, "padding[]", 2)
    yield String(parent, "fourcc", 4)
    yield UInt32(parent, "size")
    yield textHandler(UInt64(parent, "time_unit", "Frame duration"), formatTimeUnit)
    yield UInt64(parent, "sample_per_unit")
    yield UInt32(parent, "default_len")
    yield UInt32(parent, "buffer_size")
    yield UInt16(parent, "bits_per_sample")
    yield NullBytes(parent, "padding[]", 2)
    yield UInt32(parent, "width")
    yield UInt32(parent, "height")
    yield NullBytes(parent, "padding[]", 4)

def parseTheoraHeader(parent):
    yield UInt8(parent, "version_major")
    yield UInt8(parent, "version_minor")
    yield UInt8(parent, "version_revision")
    yield UInt16(parent, "width", "Width*16 in pixel")
    yield UInt16(parent, "height", "Height*16 in pixel")

    yield UInt24(parent, "frame_width")
    yield UInt24(parent, "frame_height")
    yield UInt8(parent, "offset_x")
    yield UInt8(parent, "offset_y")

    yield UInt32(parent, "fps_num", "Frame per second numerator")
    yield UInt32(parent, "fps_den", "Frame per second denominator")
    yield UInt24(parent, "aspect_ratio_num", "Aspect ratio numerator")
    yield UInt24(parent, "aspect_ratio_den", "Aspect ratio denominator")

    yield UInt8(parent, "color_space")
    yield UInt24(parent, "target_bitrate")
    yield Bits(parent, "quality", 6)
    yield Bits(parent, "gp_shift", 5)
    yield Enum(Bits(parent, "pixel_format", 2), PIXEL_FORMATS)
    yield Bits(parent, "spare_config", 3)

def parseVorbisHeader(parent):
    yield UInt32(parent, "vorbis_version")
    yield UInt8(parent, "audio_channels")
    yield UInt32(parent, "audio_sample_rate")
    yield UInt32(parent, "bitrate_maximum")
    yield UInt32(parent, "bitrate_nominal")
    yield UInt32(parent, "bitrate_minimum")
    yield Bits(parent, "blocksize_0", 4)
    yield Bits(parent, "blocksize_1", 4)
    yield UInt8(parent, "framing_flag")

class Chunk(FieldSet):
    tag_info = {
        "vorbis": {
            3: ("comment", parseVorbisComment),
            1: ("vorbis_hdr", parseVorbisHeader),
        }, "theora": {
            128: ("theora_hdr", parseTheoraHeader),
            129: ("comment", parseVorbisComment),
        }, "video\0": {
            1: ("video_hdr", parseVideoHeader),
        },
    }
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        if 7*8 <= self.size:
            try:
                self._name, self.parser = self.tag_info[self["codec"].value][self["type"].value]
                if self._name == "theora_hdr":
                    self.endian = BIG_ENDIAN
            except KeyError:
                self.parser = None
        else:
            self.parser = None

    def createFields(self):
        if 7*8 <= self.size:
            yield UInt8(self, 'type')
            yield String(self, 'codec', 6)
        if self.parser:
            for field in self.parser(self):
                yield field
        else:
            size = (self.size - self.current_size) // 8
            if size:
                yield RawBytes(self, "raw", size)

class Packets:
    def __init__(self, first):
        self.first = first

    def __iter__(self):
        fragment = self.first
        size = None
        while fragment is not None:
            page = fragment.parent
            continued_packet = page["continued_packet"].value
            for segment_size in page.segment_size:
                if continued_packet:
                    size += segment_size
                    continued_packet = False
                else:
                    if size:
                        yield size * 8
                    size = segment_size
            fragment = fragment.next
        if size:
            yield size * 8

class Segments(Fragment):
    def __init__(self, parent, *args, **kw):
        Fragment.__init__(self, parent, *args, **kw)
        if parent['last_page'].value:
            next = None
        else:
            next = self.createNext
        self.setLinks(parent.parent.streams.setdefault(parent['serial'].value, self), next)

    def _createInputStream(self, **args):
        if self.first is self:
            return FragmentedStream(self, packets=Packets(self), tags=[("id","ogg_stream")], **args)
        return Fragment._createInputStream(self, **args)

    def _getData(self):
        return self

    def createNext(self):
        parent = self.parent
        index = parent.index
        parent = parent.parent
        first = self.first
        try:
            while True:
                index += 1
                next = parent[index][self.name]
                if next.first is first:
                    return next
        except MissingField:
            pass

    def createFields(self):
        for segment_size in self.parent.segment_size:
            if segment_size:
                yield Chunk(self, "chunk[]", size=segment_size*8)

class OggPage(FieldSet):
    MAGIC = "OggS"

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        size = 27
        self.lacing_size = self['lacing_size'].value
        if self.lacing_size:
            size += self.lacing_size
            lacing = self['lacing']
            self.segment_size = [ field.value for field in lacing ]
            size += sum(self.segment_size)
        self._size = size * 8

    def createFields(self):
        yield String(self, 'capture_pattern', 4, charset="ASCII")
        if self['capture_pattern'].value != self.MAGIC:
            self.warning('Invalid signature. An Ogg page must start with "%s".' % self.MAGIC)
        yield UInt8(self, 'stream_structure_version')
        yield Bit(self, 'continued_packet')
        yield Bit(self, 'first_page')
        yield Bit(self, 'last_page')
        yield NullBits(self, 'unused', 5)
        yield UInt64(self, 'abs_granule_pos')
        yield textHandler(UInt32(self, 'serial'), hexadecimal)
        yield UInt32(self, 'page')
        yield textHandler(UInt32(self, 'checksum'), hexadecimal)
        yield UInt8(self, 'lacing_size')
        if self.lacing_size:
            yield Lacing(self, "lacing", size=self.lacing_size*8)
            yield Segments(self, "segments", size=self._size-self._current_size)

    def validate(self):
        if self['capture_pattern'].value != self.MAGIC:
            return "Wrong signature"
        if self['stream_structure_version'].value != 0:
            return "Unknown structure version (%s)" % self['stream_structure_version'].value
        return ""

class OggFile(Parser):
    PARSER_TAGS = {
        "id": "ogg",
        "category": "container",
        "file_ext": ("ogg", "ogm"),
        "mime": (
            u"application/ogg", u"application/x-ogg",
            u"audio/ogg", u"audio/x-ogg",
            u"video/ogg", u"video/x-ogg",
            u"video/theora", u"video/x-theora",
         ),
        "magic": ((OggPage.MAGIC, 0),),
        "subfile": "skip",
        "min_size": 28*8,
        "description": "Ogg multimedia container"
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        magic = OggPage.MAGIC
        if self.stream.readBytes(0, len(magic)) != magic:
            return "Invalid magic string"
        # Validate first 3 pages
        for index in xrange(3):
            try:
                page = self[index]
            except MissingField:
                if self.done:
                    return True
                return "Unable to get page #%u" % index
            except (InputStreamError, ParserError):
                return "Unable to create page #%u" % index
            err = page.validate()
            if err:
                return "Invalid page #%s: %s" % (index, err)
        return True

    def createMimeType(self):
        if "theora_hdr" in self["page[0]/segments"]:
            return u"video/theora"
        elif "vorbis_hdr" in self["page[0]/segments"]:
            return u"audio/vorbis"
        else:
            return u"application/ogg"

    def createDescription(self):
        if "theora_hdr" in self["page[0]"]:
            return u"Ogg/Theora video"
        elif "vorbis_hdr" in self["page[0]"]:
            return u"Ogg/Vorbis audio"
        else:
            return u"Ogg multimedia container"

    def createFields(self):
        self.streams = {}
        while not self.eof:
            yield OggPage(self, "page[]")

    def createLastPage(self):
        start = self[0].size
        end = MAX_FILESIZE * 8
        if True:
            # FIXME: This doesn't work on all files (eg. some Ogg/Theora)
            offset = self.stream.searchBytes("OggS\0\5", start, end)
            if offset is None:
                offset = self.stream.searchBytes("OggS\0\4", start, end)
            if offset is None:
                return None
            return createOrphanField(self, offset, OggPage, "page")
        else:
            # Very slow version
            page = None
            while True:
                offset = self.stream.searchBytes("OggS\0", start, end)
                if offset is None:
                    break
                page = createOrphanField(self, offset, OggPage, "page")
                start += page.size
            return page

    def createContentSize(self):
        page = self.createLastPage()
        if page:
            return page.absolute_address + page.size
        else:
            return None


class OggStream(Parser):
    PARSER_TAGS = {
        "id": "ogg_stream",
        "category": "container",
        "subfile": "skip",
        "min_size": 7*8,
        "description": "Ogg logical stream"
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        return False

    def createFields(self):
        for size in self.stream.packets:
            yield RawBytes(self, "packet[]", size//8)

########NEW FILE########
__FILENAME__ = realmedia
"""
RealMedia (.rm) parser

Author: Mike Melanson
Creation date: 15 december 2006

References:
- http://wiki.multimedia.cx/index.php?title=RealMedia
- Appendix E: RealMedia File Format (RMFF) Reference
  https://common.helixcommunity.org/nonav/2003/HCS_SDK_r5/htmfiles/rmff.htm

Samples:
- http://samples.mplayerhq.hu/real/
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt16, UInt32, Bit, RawBits,
    RawBytes, String, PascalString8, PascalString16)
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.endian import BIG_ENDIAN

def parseHeader(self):
    yield UInt32(self, "filever", "File version")
    yield UInt32(self, "numheaders", "number of headers")

def parseFileProperties(self):
    yield UInt32(self, "max_bit_rate", "Maximum bit rate")
    yield UInt32(self, "avg_bit_rate", "Average bit rate")
    yield UInt32(self, "max_pkt_size", "Size of largest data packet")
    yield UInt32(self, "avg_pkt_size", "Size of average data packet")
    yield UInt32(self, "num_pkts", "Number of data packets")
    yield UInt32(self, "duration", "File duration in milliseconds")
    yield UInt32(self, "preroll", "Suggested preroll in milliseconds")
    yield textHandler(UInt32(self, "index_offset", "Absolute offset of first index chunk"), hexadecimal)
    yield textHandler(UInt32(self, "data_offset", "Absolute offset of first data chunk"), hexadecimal)
    yield UInt16(self, "stream_count", "Number of streams in the file")
    yield RawBits(self, "reserved", 13)
    yield Bit(self, "is_live", "Whether file is a live broadcast")
    yield Bit(self, "is_perfect_play", "Whether PerfectPlay can be used")
    yield Bit(self, "is_saveable", "Whether file can be saved")

def parseContentDescription(self):
    yield PascalString16(self, "title", charset="ISO-8859-1", strip=" \0")
    yield PascalString16(self, "author", charset="ISO-8859-1", strip=" \0")
    yield PascalString16(self, "copyright", charset="ISO-8859-1", strip=" \0")
    yield PascalString16(self, "comment", charset="ISO-8859-1", strip=" \0")


class NameValueProperty(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["size"].value * 8

    def createFields(self):
        yield UInt32(self, "size")
        yield UInt16(self, "obj_version")
        yield PascalString8(self, "name", charset="ASCII")
        yield UInt32(self, "type")
        yield PascalString16(self, "value", charset="ISO-8859-1", strip=" \0")

class LogicalFileInfo(FieldSet):
    def createFields(self):
        yield UInt32(self, "size")
        yield UInt16(self, "obj_version")
        yield UInt16(self, "nb_physical_stream")
        for index in xrange(self["nb_physical_stream"].value):
            yield UInt16(self, "physical_stream[]")
        for index in xrange(self["nb_physical_stream"].value):
            yield UInt16(self, "data_offset[]")
        yield UInt16(self, "nb_rule")
        for index in xrange(self["nb_rule"].value):
            yield UInt16(self, "rule[]")
        yield UInt16(self, "nb_prop")
        for index in xrange(self["nb_prop"].value):
            yield NameValueProperty(self, "prop[]")

def parseMediaPropertiesHeader(self):
    yield UInt16(self, "stream_number", "Stream number")
    yield UInt32(self, "max_bit_rate", "Maximum bit rate")
    yield UInt32(self, "avg_bit_rate", "Average bit rate")
    yield UInt32(self, "max_pkt_size", "Size of largest data packet")
    yield UInt32(self, "avg_pkt_size", "Size of average data packet")
    yield UInt32(self, "stream_start", "Stream start offset in milliseconds")
    yield UInt32(self, "preroll", "Preroll in milliseconds")
    yield UInt32(self, "duration", "Stream duration in milliseconds")
    yield PascalString8(self, "desc", "Stream description", charset="ISO-8859-1")
    yield PascalString8(self, "mime_type", "MIME type string", charset="ASCII")
    yield UInt32(self, "specific_size", "Size of type-specific data")
    size = self['specific_size'].value
    if size:
        if self["mime_type"].value == "logical-fileinfo":
            yield LogicalFileInfo(self, "file_info", size=size*8)
        else:
            yield RawBytes(self, "specific", size, "Type-specific data")

class Chunk(FieldSet):
    tag_info = {
        ".RMF": ("header", parseHeader),
        "PROP": ("file_prop", parseFileProperties),
        "CONT": ("content_desc", parseContentDescription),
        "MDPR": ("stream_prop[]", parseMediaPropertiesHeader),
        "DATA": ("data[]", None),
        "INDX": ("file_index[]", None)
    }

    def createValueFunc(self):
        return self.value_func(self)

    def __init__(self, parent, name, description=None):
        FieldSet.__init__(self, parent, name, description)
        self._size = (self["size"].value) * 8
        tag = self["tag"].value
        if tag in self.tag_info:
            self._name, self.parse_func = self.tag_info[tag]
        else:
            self._description = ""
            self.parse_func = None

    def createFields(self):
        yield String(self, "tag", 4, "Chunk FourCC", charset="ASCII")
        yield UInt32(self, "size", "Chunk Size")
        yield UInt16(self, "version", "Chunk Version")

        if self.parse_func:
            for field in self.parse_func(self):
                yield field
        else:
            size = (self.size - self.current_size) // 8
            if size:
                yield RawBytes(self, "raw", size)

    def createDescription(self):
        return "Chunk: %s" % self["tag"].display

class RealMediaFile(Parser):
    MAGIC = '.RMF\0\0\0\x12\0\1'    # (magic, size=18, version=1)
    PARSER_TAGS = {
        "id": "real_media",
        "category": "container",
        "file_ext": ("rm",),
        "mime": (
            u"video/x-pn-realvideo",
            u"audio/x-pn-realaudio",
            u"audio/x-pn-realaudio-plugin",
            u"audio/x-real-audio",
            u"application/vnd.rn-realmedia"),
        "min_size": len(MAGIC)*8, # just the identifier
        "magic": ((MAGIC, 0),),
        "description": u"RealMedia (rm) Container File",
    }
    endian = BIG_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 4) != '.RMF':
            return "Invalid magic"
        if self["header/size"].value != 18:
            return "Invalid header size"
        if self["header/version"].value not in (0, 1):
            return "Unknown file format version (%s)" % self["header/version"].value
        return True

    def createFields(self):
        while not self.eof:
            yield Chunk(self, "chunk")

    def createMimeType(self):
        for prop in self.array("stream_prop"):
            if prop["mime_type"].value == "video/x-pn-realvideo":
                return u"video/x-pn-realvideo"
        return u"audio/x-pn-realaudio"


########NEW FILE########
__FILENAME__ = riff
# -*- coding: UTF-8 -*-

"""
RIFF parser, able to parse:
   * AVI video container
   * WAV audio container
   * CDA file

Documents:
- libavformat source code from ffmpeg library
  http://ffmpeg.mplayerhq.hu/
- Video for Windows Programmer's Guide
  http://www.opennet.ru/docs/formats/avi.txt
- What is an animated cursor?
  http://www.gdgsoft.com/anituner/help/aniformat.htm

Authors:
   * Aurlien Jacobs
   * Mickal KENIKSSI
   * Victor Stinner
Changelog:
   * 2007-03-30: support ACON (animated icons)
   * 2006-08-08: merge AVI, WAV and CDA parsers into RIFF parser
   * 2006-08-03: creation of CDA parser by Mickal KENIKSSI
   * 2005-06-21: creation of WAV parser by Victor Stinner
   * 2005-06-08: creation of AVI parser by Victor Stinner and Aurlien Jacobs
Thanks to:
   * Wojtek Kaniewski (wojtekka AT logonet.com.pl) for its CDA file
     format information
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt16, UInt32, Enum,
    Bit, NullBits, NullBytes,
    RawBytes, String, PaddingBytes,
    SubFile)
from lib.hachoir_core.tools import alignValue, humanDuration
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import filesizeHandler, textHandler
from lib.hachoir_parser.video.fourcc import audio_codec_name, video_fourcc_name
from lib.hachoir_parser.image.ico import IcoFile
from datetime import timedelta

def parseText(self):
    yield String(self, "text", self["size"].value,
        strip=" \0", truncate="\0",
        charset="ISO-8859-1")

def parseRawFormat(self, size):
    yield RawBytes(self, "raw_format", size)

def parseVideoFormat(self, size):
    yield UInt32(self, "video_size", "Video format: Size")
    yield UInt32(self, "width", "Video format: Width")
    yield UInt32(self, "height", "Video format: Height")
    yield UInt16(self, "panes", "Video format: Panes")
    yield UInt16(self, "depth", "Video format: Depth")
    yield UInt32(self, "tag1", "Video format: Tag1")
    yield UInt32(self, "img_size", "Video format: Image size")
    yield UInt32(self, "xpels_meter", "Video format: XPelsPerMeter")
    yield UInt32(self, "ypels_meter", "Video format: YPelsPerMeter")
    yield UInt32(self, "clr_used", "Video format: ClrUsed")
    yield UInt32(self, "clr_important", "Video format: ClrImportant")

def parseAudioFormat(self, size):
    yield Enum(UInt16(self, "codec", "Audio format: Codec id"), audio_codec_name)
    yield UInt16(self, "channel", "Audio format: Channels")
    yield UInt32(self, "sample_rate", "Audio format: Sample rate")
    yield UInt32(self, "bit_rate", "Audio format: Bit rate")
    yield UInt16(self, "block_align", "Audio format: Block align")
    if size >= 16:
        yield UInt16(self, "bits_per_sample", "Audio format: Bits per sample")
    if size >= 18:
        yield UInt16(self, "ext_size", "Audio format: Size of extra information")
    if size >= 28: # and self["a_channel"].value > 2
        yield UInt16(self, "reserved", "Audio format: ")
        yield UInt32(self, "channel_mask", "Audio format: channels placement bitmask")
        yield UInt32(self, "subformat", "Audio format: Subformat id")

def parseAVIStreamFormat(self):
    size = self["size"].value
    strtype = self["../stream_hdr/stream_type"].value
    TYPE_HANDLER = {
        "vids": (parseVideoFormat, 40),
        "auds": (parseAudioFormat, 16)
    }
    handler = parseRawFormat
    if strtype in TYPE_HANDLER:
        info = TYPE_HANDLER[strtype]
        if info[1] <= size:
            handler = info[0]
    for field in handler(self, size):
        yield field

def parseAVIStreamHeader(self):
    if self["size"].value != 56:
        raise ParserError("Invalid stream header size")
    yield String(self, "stream_type", 4, "Stream type four character code", charset="ASCII")
    field = String(self, "fourcc", 4, "Stream four character code", strip=" \0", charset="ASCII")
    if self["stream_type"].value == "vids":
        yield Enum(field, video_fourcc_name, lambda text: text.upper())
    else:
        yield field
    yield UInt32(self, "flags", "Stream flags")
    yield UInt16(self, "priority", "Stream priority")
    yield String(self, "language", 2, "Stream language", charset="ASCII", strip="\0")
    yield UInt32(self, "init_frames", "InitialFrames")
    yield UInt32(self, "scale", "Time scale")
    yield UInt32(self, "rate", "Divide by scale to give frame rate")
    yield UInt32(self, "start", "Stream start time (unit: rate/scale)")
    yield UInt32(self, "length", "Stream length (unit: rate/scale)")
    yield UInt32(self, "buf_size", "Suggested buffer size")
    yield UInt32(self, "quality", "Stream quality")
    yield UInt32(self, "sample_size", "Size of samples")
    yield UInt16(self, "left", "Destination rectangle (left)")
    yield UInt16(self, "top", "Destination rectangle (top)")
    yield UInt16(self, "right", "Destination rectangle (right)")
    yield UInt16(self, "bottom", "Destination rectangle (bottom)")

class RedBook(FieldSet):
    """
    RedBook offset parser, used in CD audio (.cda) file
    """
    def createFields(self):
        yield UInt8(self, "frame")
        yield UInt8(self, "second")
        yield UInt8(self, "minute")
        yield PaddingBytes(self, "notused", 1)

def formatSerialNumber(field):
    """
    Format an disc serial number.
    Eg. 0x00085C48 => "0008-5C48"
    """
    sn = field.value
    return "%04X-%04X" % (sn >> 16, sn & 0xFFFF)

def parseCDDA(self):
    """
    HSG address format: number of 1/75 second

    HSG offset = (minute*60 + second)*75 + frame + 150 (from RB offset)
    HSG length = (minute*60 + second)*75 + frame (from RB length)
    """
    yield UInt16(self, "cda_version", "CD file version (currently 1)")
    yield UInt16(self, "track_no", "Number of track")
    yield textHandler(UInt32(self, "disc_serial", "Disc serial number"),
        formatSerialNumber)
    yield UInt32(self, "hsg_offset", "Track offset (HSG format)")
    yield UInt32(self, "hsg_length", "Track length (HSG format)")
    yield RedBook(self, "rb_offset", "Track offset (Red-book format)")
    yield RedBook(self, "rb_length", "Track length (Red-book format)")

def parseWAVFormat(self):
    size = self["size"].value
    if size not in (16, 18):
        self.warning("Format with size of %s bytes is not supported!" % size)
    yield Enum(UInt16(self, "codec", "Audio codec"), audio_codec_name)
    yield UInt16(self, "nb_channel", "Number of audio channel")
    yield UInt32(self, "sample_per_sec", "Sample per second")
    yield UInt32(self, "byte_per_sec", "Average byte per second")
    yield UInt16(self, "block_align", "Block align")
    yield UInt16(self, "bit_per_sample", "Bits per sample")

def parseWAVFact(self):
    yield UInt32(self, "nb_sample", "Number of samples in audio stream")

def parseAviHeader(self):
    yield UInt32(self, "microsec_per_frame", "Microsecond per frame")
    yield UInt32(self, "max_byte_per_sec", "Maximum byte per second")
    yield NullBytes(self, "reserved", 4)

    # Flags
    yield NullBits(self, "reserved[]", 4)
    yield Bit(self, "has_index")
    yield Bit(self, "must_use_index")
    yield NullBits(self, "reserved[]", 2)
    yield Bit(self, "is_interleaved")
    yield NullBits(self, "reserved[]", 2)
    yield Bit(self, "trust_cktype")
    yield NullBits(self, "reserved[]", 4)
    yield Bit(self, "was_capture_file")
    yield Bit(self, "is_copyrighted")
    yield NullBits(self, "reserved[]", 14)

    yield UInt32(self, "total_frame", "Total number of frames in the video")
    yield UInt32(self, "init_frame", "Initial frame (used in interleaved video)")
    yield UInt32(self, "nb_stream", "Number of streams")
    yield UInt32(self, "sug_buf_size", "Suggested buffer size")
    yield UInt32(self, "width", "Width in pixel")
    yield UInt32(self, "height", "Height in pixel")
    yield UInt32(self, "scale")
    yield UInt32(self, "rate")
    yield UInt32(self, "start")
    yield UInt32(self, "length")

def parseODML(self):
    yield UInt32(self, "total_frame", "Real number of frame of OpenDML video")
    padding = self["size"].value - 4
    if 0 < padding:
        yield NullBytes(self, "padding[]", padding)

class AVIIndexEntry(FieldSet):
    size = 16*8
    def createFields(self):
        yield String(self, "tag", 4, "Tag", charset="ASCII")
        yield UInt32(self, "flags")
        yield UInt32(self, "start", "Offset from start of movie data")
        yield UInt32(self, "length")

def parseIndex(self):
    while not self.eof:
        yield AVIIndexEntry(self, "index[]")

class Chunk(FieldSet):
    TAG_INFO = {
        # This dictionnary is edited by RiffFile.validate()

        "LIST": ("list[]", None, "Sub-field list"),
        "JUNK": ("junk[]", None, "Junk (padding)"),

        # Metadata
        "INAM": ("title", parseText, "Document title"),
        "IART": ("artist", parseText, "Artist"),
        "ICMT": ("comment", parseText, "Comment"),
        "ICOP": ("copyright", parseText, "Copyright"),
        "IENG": ("author", parseText, "Author"),
        "ICRD": ("creation_date", parseText, "Creation date"),
        "ISFT": ("producer", parseText, "Producer"),
        "IDIT": ("datetime", parseText, "Date time"),

        # TODO: Todo: see below
        # "strn": Stream description
        # TWOCC code, movie/field[]/tag.value[2:4]:
        #   "db": "Uncompressed video frame",
        #   "dc": "Compressed video frame",
        #   "wb": "Audio data",
        #   "pc": "Palette change"
    }

    subtag_info = {
        "INFO": ("info", "File informations"),
        "hdrl": ("headers", "Headers"),
        "strl": ("stream[]", "Stream header list"),
        "movi": ("movie", "Movie stream"),
        "odml": ("odml", "ODML"),
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = (8 + alignValue(self["size"].value, 2)) * 8
        tag = self["tag"].value
        if tag in self.TAG_INFO:
            self.tag_info = self.TAG_INFO[tag]
            if tag == "LIST":
                subtag = self["subtag"].value
                if subtag in self.subtag_info:
                    info = self.subtag_info[subtag]
                    self.tag_info = (info[0], None, info[1])
            self._name = self.tag_info[0]
            self._description = self.tag_info[2]
        else:
            self.tag_info = ("field[]", None, None)

    def createFields(self):
        yield String(self, "tag", 4, "Tag", charset="ASCII")
        yield filesizeHandler(UInt32(self, "size", "Size"))
        if not self["size"].value:
            return
        if self["tag"].value == "LIST":
            yield String(self, "subtag", 4, "Sub-tag", charset="ASCII")
            handler = self.tag_info[1]
            while 8 < (self.size - self.current_size)/8:
                field = self.__class__(self, "field[]")
                yield field
                if (field.size/8) % 2 != 0:
                    yield UInt8(self, "padding[]", "Padding")
        else:
            handler = self.tag_info[1]
            if handler:
                for field in handler(self):
                    yield field
            else:
                yield RawBytes(self, "raw_content", self["size"].value)
            padding = self.seekBit(self._size)
            if padding:
                yield padding

    def createDescription(self):
        tag = self["tag"].display
        return u"Chunk (tag %s)" % tag

class ChunkAVI(Chunk):
    TAG_INFO = Chunk.TAG_INFO.copy()
    TAG_INFO.update({
        "strh": ("stream_hdr", parseAVIStreamHeader, "Stream header"),
        "strf": ("stream_fmt", parseAVIStreamFormat, "Stream format"),
        "avih": ("avi_hdr", parseAviHeader, "AVI header"),
        "idx1": ("index", parseIndex, "Stream index"),
        "dmlh": ("odml_hdr", parseODML, "ODML header"),
    })

class ChunkCDDA(Chunk):
    TAG_INFO = Chunk.TAG_INFO.copy()
    TAG_INFO.update({
        'fmt ': ("cdda", parseCDDA, "CD audio informations"),
    })

class ChunkWAVE(Chunk):
    TAG_INFO = Chunk.TAG_INFO.copy()
    TAG_INFO.update({
        'fmt ': ("format", parseWAVFormat, "Audio format"),
        'fact': ("nb_sample", parseWAVFact, "Number of samples"),
        'data': ("audio_data", None, "Audio stream data"),
    })

def parseAnimationHeader(self):
    yield UInt32(self, "hdr_size", "Size of header (36 bytes)")
    if self["hdr_size"].value != 36:
        self.warning("Animation header with unknown size (%s)" % self["size"].value)
    yield UInt32(self, "nb_frame", "Number of unique Icons in this cursor")
    yield UInt32(self, "nb_step", "Number of Blits before the animation cycles")
    yield UInt32(self, "cx")
    yield UInt32(self, "cy")
    yield UInt32(self, "bit_count")
    yield UInt32(self, "planes")
    yield UInt32(self, "jiffie_rate", "Default Jiffies (1/60th of a second) if rate chunk not present")
    yield Bit(self, "is_icon")
    yield NullBits(self, "padding", 31)

def parseAnimationSequence(self):
    while not self.eof:
        yield UInt32(self, "icon[]")

def formatJiffie(field):
    sec = float(field.value) / 60
    return humanDuration(timedelta(seconds=sec))

def parseAnimationRate(self):
    while not self.eof:
        yield textHandler(UInt32(self, "rate[]"), formatJiffie)

def parseIcon(self):
    yield SubFile(self, "icon_file", self["size"].value, parser_class=IcoFile)

class ChunkACON(Chunk):
    TAG_INFO = Chunk.TAG_INFO.copy()
    TAG_INFO.update({
        'anih': ("anim_hdr", parseAnimationHeader, "Animation header"),
        'seq ': ("anim_seq", parseAnimationSequence, "Animation sequence"),
        'rate': ("anim_rate", parseAnimationRate, "Animation sequence"),
        'icon': ("icon[]", parseIcon, "Icon"),
    })

class RiffFile(Parser):
    PARSER_TAGS = {
        "id": "riff",
        "category": "container",
        "file_ext": ("avi", "cda", "wav", "ani"),
        "min_size": 16*8,
        "mime": (u"video/x-msvideo", u"audio/x-wav", u"audio/x-cda"),
        # FIXME: Use regex "RIFF.{4}(WAVE|CDDA|AVI )"
        "magic": (
            ("AVI LIST", 8*8),
            ("WAVEfmt ", 8*8),
            ("CDDAfmt ", 8*8),
            ("ACONanih", 8*8),
        ),
        "description": "Microsoft RIFF container"
    }
    VALID_TYPES = {
        "WAVE": (ChunkWAVE, u"audio/x-wav",     u"Microsoft WAVE audio", ".wav"),
        "CDDA": (ChunkCDDA, u"audio/x-cda",     u"Microsoft Windows audio CD file (cda)", ".cda"),
        "AVI ": (ChunkAVI,  u"video/x-msvideo", u"Microsoft AVI video", ".avi"),
        "ACON": (ChunkACON, u"image/x-ani",     u"Microsoft Windows animated cursor", ".ani"),
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 4) != "RIFF":
            return "Wrong signature"
        if self["type"].value not in self.VALID_TYPES:
            return "Unknown RIFF content type"
        return True

    def createFields(self):
        yield String(self, "signature", 4, "AVI header (RIFF)", charset="ASCII")
        yield filesizeHandler(UInt32(self, "filesize", "File size"))
        yield String(self, "type", 4, "Content type (\"AVI \", \"WAVE\", ...)", charset="ASCII")

        # Choose chunk type depending on file type
        try:
            chunk_cls = self.VALID_TYPES[self["type"].value][0]
        except KeyError:
            chunk_cls = Chunk

        # Parse all chunks up to filesize
        while self.current_size < self["filesize"].value*8+8:
            yield chunk_cls(self, "chunk[]")
        if not self.eof:
            yield RawBytes(self, "padding[]", (self.size-self.current_size)/8)

    def createMimeType(self):
        try:
            return self.VALID_TYPES[self["type"].value][1]
        except KeyError:
            return None

    def createDescription(self):
        tag = self["type"].value
        if tag == "AVI ":
            desc = u"Microsoft AVI video"
            if "headers/avi_hdr" in self:
                header = self["headers/avi_hdr"]
                desc += ": %ux%u pixels" % (header["width"].value, header["height"].value)
                microsec = header["microsec_per_frame"].value
                if microsec:
                    desc += ", %.1f fps" % (1000000.0 / microsec)
                    if "total_frame" in header and header["total_frame"].value:
                        delta = timedelta(seconds=float(header["total_frame"].value) * microsec)
                        desc += ", " + humanDuration(delta)
            return desc
        else:
            try:
                return self.VALID_TYPES[tag][2]
            except KeyError:
                return u"Microsoft RIFF container"

    def createContentSize(self):
        size = (self["filesize"].value + 8) * 8
        return min(size, self.stream.size)

    def createFilenameSuffix(self):
        try:
            return self.VALID_TYPES[self["type"].value][3]
        except KeyError:
            return ".riff"


########NEW FILE########
__FILENAME__ = swf
"""
SWF (Macromedia/Adobe Flash) file parser.

Documentation:

 - Alexis' SWF Reference:
   http://www.m2osw.com/swf_alexref.html
 - http://www.half-serious.com/swf/format/
 - http://www.anotherbigidea.com/javaswf/
 - http://www.gnu.org/software/gnash/

Author: Victor Stinner
Creation date: 29 october 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    Bit, Bits, UInt8, UInt32, UInt16, CString, Enum,
    Bytes, RawBytes, NullBits, String, SubFile)
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, filesizeHandler
from lib.hachoir_core.tools import paddingSize, humanFrequency
from lib.hachoir_parser.image.common import RGB
from lib.hachoir_parser.image.jpeg import JpegChunk, JpegFile
from lib.hachoir_core.stream import StringInputStream, ConcatStream
from lib.hachoir_parser.common.deflate import Deflate, has_deflate
from lib.hachoir_parser.container.action_script import parseActionScript
import math

# Maximum file size (50 MB)
MAX_FILE_SIZE = 50 * 1024 * 1024

TWIPS = 20

class RECT(FieldSet):
    endian = BIG_ENDIAN
    def createFields(self):
        yield Bits(self, "nbits", 5)
        nbits = self["nbits"].value
        if not nbits:
            raise ParserError("SWF parser: Invalid RECT field size (0)")
        yield Bits(self, "xmin", nbits, "X minimum in twips")
        yield Bits(self, "xmax", nbits, "X maximum in twips")
        yield Bits(self, "ymin", nbits, "Y minimum in twips")
        yield Bits(self, "ymax", nbits, "Y maximum in twips")
        size = paddingSize(self.current_size, 8)
        if size:
            yield NullBits(self, "padding", size)

    def getWidth(self):
        return math.ceil(float(self["xmax"].value) / TWIPS)
    def getHeight(self):
        return math.ceil(float(self["ymax"].value) / TWIPS)

    def createDescription(self):
        return "Rectangle: %ux%u" % (self.getWidth(), self.getHeight())

class FixedFloat16(FieldSet):
    def createFields(self):
        yield UInt8(self, "float_part")
        yield UInt8(self, "int_part")

    def createValue(self):
        return self["int_part"].value +  float(self["float_part"].value) / 256

def parseBackgroundColor(parent, size):
    yield RGB(parent, "color")

def bit2hertz(field):
    return humanFrequency(5512.5 * (2 ** field.value))

SOUND_CODEC_MP3 = 2
SOUND_CODEC = {
    0: "RAW",
    1: "ADPCM",
    SOUND_CODEC_MP3: "MP3",
    3: "Uncompressed",
    6: "Nellymoser",
}

class SoundEnvelope(FieldSet):
    def createFields(self):
        yield UInt8(self, "count")
        for index in xrange(self["count"].value):
            yield UInt32(self, "mark44[]")
            yield UInt16(self, "level0[]")
            yield UInt16(self, "level1[]")

def parseSoundBlock(parent, size):
    # TODO: Be able to get codec... Need to know last sound "def_sound[]" field
#    if not (...)sound_header:
#        raise ParserError("Sound block without header")
    if True: #sound_header == SOUND_CODEC_MP3:
        yield UInt16(parent, "samples")
        yield UInt16(parent, "left")
    size = (parent.size - parent.current_size) // 8
    if size:
        yield RawBytes(parent, "music_data", size)

def parseStartSound(parent, size):
    yield UInt16(parent, "sound_id")
    yield Bit(parent, "has_in_point")
    yield Bit(parent, "has_out_point")
    yield Bit(parent, "has_loops")
    yield Bit(parent, "has_envelope")
    yield Bit(parent, "no_multiple")
    yield Bit(parent, "stop_playback")
    yield NullBits(parent, "reserved", 2)

    if parent["has_in_point"].value:
        yield UInt32(parent, "in_point")
    if parent["has_out_point"].value:
        yield UInt32(parent, "out_point")
    if parent["has_loops"].value:
        yield UInt16(parent, "loop_count")
    if parent["has_envelope"].value:
        yield SoundEnvelope(parent, "envelope")

def parseDefineSound(parent, size):
    yield UInt16(parent, "sound_id")

    yield Bit(parent, "is_stereo")
    yield Bit(parent, "is_16bit")
    yield textHandler(Bits(parent, "rate", 2), bit2hertz)
    yield Enum(Bits(parent, "codec", 4), SOUND_CODEC)

    yield UInt32(parent, "sample_count")

    if parent["codec"].value == SOUND_CODEC_MP3:
        yield UInt16(parent, "len")

    size = (parent.size - parent.current_size) // 8
    if size:
        yield RawBytes(parent, "music_data", size)

def parseSoundHeader(parent, size):
    yield Bit(parent, "playback_is_stereo")
    yield Bit(parent, "playback_is_16bit")
    yield textHandler(Bits(parent, "playback_rate", 2), bit2hertz)
    yield NullBits(parent, "reserved", 4)

    yield Bit(parent, "sound_is_stereo")
    yield Bit(parent, "sound_is_16bit")
    yield textHandler(Bits(parent, "sound_rate", 2), bit2hertz)
    yield Enum(Bits(parent, "codec", 4), SOUND_CODEC)

    yield UInt16(parent, "sample_count")

    if parent["codec"].value == 2:
        yield UInt16(parent, "latency_seek")

class JpegHeader(FieldSet):
    endian = BIG_ENDIAN
    def createFields(self):
        count = 1
        while True:
            chunk = JpegChunk(self, "jpeg_chunk[]")
            yield chunk
            if 1 < count and chunk["type"].value in (JpegChunk.TAG_SOI, JpegChunk.TAG_EOI):
                break
            count += 1

def parseJpeg(parent, size):
    yield UInt16(parent, "char_id", "Character identifier")
    size -= 2

    code = parent["code"].value
    if code != Tag.TAG_BITS:
        if code == Tag.TAG_BITS_JPEG3:
            yield UInt32(parent, "alpha_offset", "Character identifier")
            size -= 4

        addr = parent.absolute_address + parent.current_size + 16
        if parent.stream.readBytes(addr, 2) in ("\xff\xdb", "\xff\xd8"):
            header = JpegHeader(parent, "jpeg_header")
            yield header
            hdr_size = header.size // 8
            size -= hdr_size
        else:
            hdr_size = 0

        if code == Tag.TAG_BITS_JPEG3:
            img_size = parent["alpha_offset"].value - hdr_size
        else:
            img_size = size
    else:
        img_size = size
    yield SubFile(parent, "image", img_size, "JPEG picture", parser=JpegFile)
    if code == Tag.TAG_BITS_JPEG3:
        size = (parent.size - parent.current_size) // 8
        yield RawBytes(parent, "alpha", size, "Image data")

def parseVideoFrame(parent, size):
    yield UInt16(parent, "stream_id")
    yield UInt16(parent, "frame_num")
    if 4 < size:
        yield RawBytes(parent, "video_data", size-4)

class Export(FieldSet):
    def createFields(self):
        yield UInt16(self, "object_id")
        yield CString(self, "name")

def parseExport(parent, size):
    yield UInt16(parent, "count")
    for index in xrange(parent["count"].value):
        yield Export(parent, "export[]")

class Tag(FieldSet):
    TAG_BITS = 6
    TAG_BITS_JPEG2 = 32
    TAG_BITS_JPEG3 = 35
    TAG_INFO = {
        # SWF version 1.0
         0: ("end[]", "End", None),
         1: ("show_frame[]", "Show frame", None),
         2: ("def_shape[]", "Define shape", None),
         3: ("free_char[]", "Free character", None),
         4: ("place_obj[]", "Place object", None),
         5: ("remove_obj[]", "Remove object", None),
         6: ("def_bits[]", "Define bits", parseJpeg),
         7: ("def_but[]", "Define button", None),
         8: ("jpg_table", "JPEG tables", None),
         9: ("bkgd_color[]", "Set background color", parseBackgroundColor),
        10: ("def_font[]", "Define font", None),
        11: ("def_text[]", "Define text", None),
        12: ("action[]", "Action script", parseActionScript),
        13: ("def_font_info[]", "Define font info", None),

        # SWF version 2.0
        14: ("def_sound[]", "Define sound", parseDefineSound),
        15: ("start_sound[]", "Start sound", parseStartSound),
        16: ("stop_sound[]", "Stop sound", None),
        17: ("def_but_sound[]", "Define button sound", None),
        18: ("sound_hdr", "Sound stream header", parseSoundHeader),
        19: ("sound_blk[]", "Sound stream block", parseSoundBlock),
        20: ("def_bits_lossless[]", "Define bits lossless", None),
        21: ("def_bits_jpeg2[]", "Define bits JPEG 2", parseJpeg),
        22: ("def_shape2[]", "Define shape 2", None),
        23: ("def_but_cxform[]", "Define button CXFORM", None),
        24: ("protect", "File is protected", None),

        # SWF version 3.0
        25: ("path_are_ps[]", "Paths are Postscript", None),
        26: ("place_obj2[]", "Place object 2", None),
        28: ("remove_obj2[]", "Remove object 2", None),
        29: ("sync_frame[]", "Synchronize frame", None),
        31: ("free_all[]", "Free all", None),
        32: ("def_shape3[]", "Define shape 3", None),
        33: ("def_text2[]", "Define text 2", None),
        34: ("def_but2[]", "Define button2", None),
        35: ("def_bits_jpeg3[]", "Define bits JPEG 3", parseJpeg),
        36: ("def_bits_lossless2[]", "Define bits lossless 2", None),
        39: ("def_sprite[]", "Define sprite", None),
        40: ("name_character[]", "Name character", None),
        41: ("serial_number", "Serial number", None),
        42: ("generator_text[]", "Generator text", None),
        43: ("frame_label[]", "Frame label", None),
        45: ("sound_hdr2[]", "Sound stream header2", parseSoundHeader),
        46: ("def_morph_shape[]", "Define morph shape", None),
        47: ("gen_frame[]", "Generate frame", None),
        48: ("def_font2[]", "Define font 2", None),
        49: ("tpl_command[]", "Template command", None),

        # SWF version 4.0
        37: ("def_text_field[]", "Define text field", None),
        38: ("def_quicktime_movie[]", "Define QuickTime movie", None),

        # SWF version 5.0
        50: ("def_cmd_obj[]", "Define command object", None),
        51: ("flash_generator", "Flash generator", None),
        52: ("gen_ext_font[]", "Gen external font", None),
        56: ("export[]", "Export", parseExport),
        57: ("import[]", "Import", None),
        58: ("ebnable_debug", "Enable debug", None),

        # SWF version 6.0
        59: ("do_init_action[]", "Do init action", None),
        60: ("video_str[]", "Video stream", None),
        61: ("video_frame[]", "Video frame", parseVideoFrame),
        62: ("def_font_info2[]", "Define font info 2", None),
        63: ("mx4[]", "MX4", None),
        64: ("enable_debug2", "Enable debugger 2", None),

        # SWF version 7.0
        65: ("script_limits[]", "Script limits", None),
        66: ("tab_index[]", "Set tab index", None),

        # SWF version 8.0
        69: ("file_attr[]", "File attributes", None),
        70: ("place_obj3[]", "Place object 3", None),
        71: ("import2[]", "Import a definition list from another movie", None),
        73: ("def_font_align[]", "Define font alignment zones", None),
        74: ("csm_txt_set[]", "CSM text settings", None),
        75: ("def_font3[]", "Define font text 3", None),
        77: ("metadata[]", "XML code describing the movie", None),
        78: ("def_scale_grid[]", "Define scaling factors", None),
        83: ("def_shape4[]", "Define shape 4", None),
        84: ("def_morph2[]", "Define a morphing shape 2", None),
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        size = self["length"].value
        if self[0].name == "length_ext":
            self._size = (6+size) * 8
        else:
            self._size = (2+size) * 8
        code = self["code"].value
        if code in self.TAG_INFO:
            self._name, self._description, self.parser = self.TAG_INFO[code]
        else:
            self.parser = None

    def createFields(self):
        if self.stream.readBits(self.absolute_address, 6, self.endian) == 63:
            yield Bits(self, "length_ext", 6)
            yield Bits(self, "code", 10)
            yield filesizeHandler(UInt32(self, "length"))
        else:
            yield filesizeHandler(Bits(self, "length", 6))
            yield Bits(self, "code", 10)
        size = self["length"].value
        if 0 < size:
            if self.parser:
                for field in self.parser(self, size):
                    yield field
            else:
                yield RawBytes(self, "data", size)

    def createDescription(self):
        return "Tag: %s (%s)" % (self["code"].display, self["length"].display)

class SwfFile(Parser):
    VALID_VERSIONS = set(xrange(1, 9+1))
    PARSER_TAGS = {
        "id": "swf",
        "category": "container",
        "file_ext": ["swf"],
        "mime": (u"application/x-shockwave-flash",),
        "min_size": 64,
        "description": u"Macromedia Flash data"
    }
    PARSER_TAGS["magic"] = []
    for version in VALID_VERSIONS:
        PARSER_TAGS["magic"].append(("FWS%c" % version, 0))
        PARSER_TAGS["magic"].append(("CWS%c" % version, 0))
    endian = LITTLE_ENDIAN
    SWF_SCALE_FACTOR = 1.0 / 20

    def validate(self):
        if self.stream.readBytes(0, 3) not in ("FWS", "CWS"):
            return "Wrong file signature"
        if self["version"].value not in self.VALID_VERSIONS:
            return "Unknown version"
        if MAX_FILE_SIZE < self["filesize"].value:
            return "File too big (%u)" % self["filesize"].value
        if self["signature"].value == "FWS":
            if self["rect/padding"].value != 0:
                return "Unknown rectangle padding value"
        return True

    def createFields(self):
        yield String(self, "signature", 3, "SWF format signature", charset="ASCII")
        yield UInt8(self, "version")
        yield filesizeHandler(UInt32(self, "filesize"))
        if self["signature"].value != "CWS":
            yield RECT(self, "rect")
            yield FixedFloat16(self, "frame_rate")
            yield UInt16(self, "frame_count")

            while not self.eof:
                yield Tag(self, "tag[]")
        else:
            size = (self.size - self.current_size) // 8
            if has_deflate:
                data = Deflate(Bytes(self, "compressed_data", size), False)
                def createInputStream(cis, source=None, **args):
                    stream = cis(source=source)
                    header = StringInputStream("FWS" + self.stream.readBytes(3*8, 5))
                    args.setdefault("tags",[]).append(("class", SwfFile))
                    return ConcatStream((header, stream), source=stream.source, **args)
                data.setSubIStream(createInputStream)
                yield data
            else:
                yield Bytes(self, "compressed_data", size)

    def createDescription(self):
        desc = ["version %u" % self["version"].value]
        if self["signature"].value == "CWS":
            desc.append("compressed")
        return u"Macromedia Flash data: %s" % (", ".join(desc))

    def createContentSize(self):
        if self["signature"].value == "FWS":
            return self["filesize"].value * 8
        else:
            # TODO: Size of compressed Flash?
            return None


########NEW FILE########
__FILENAME__ = ext2
"""
EXT2 (Linux) file system parser.

Author: Victor Stinner

Sources:
- EXT2FS source code
  http://ext2fsd.sourceforge.net/
- Analysis of the Ext2fs structure
  http://www.nondot.org/sabre/os/files/FileSystems/ext2fs/
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    Bit, Bits, UInt8, UInt16, UInt32,
    Enum, String, TimestampUnix32, RawBytes, NullBytes)
from lib.hachoir_core.tools import (alignValue,
    humanDuration, humanFilesize)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler
from itertools import izip

class DirectoryEntry(FieldSet):
    file_type = {
        1: "Regular",
        2: "Directory",
        3: "Char. dev.",
        4: "Block dev.",
        5: "Fifo",
        6: "Socket",
        7: "Symlink",
        8: "Max"
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["rec_len"].value * 8

    def createFields(self):
        yield UInt32(self, "inode", "Inode")
        yield UInt16(self, "rec_len", "Record length")
        yield UInt8(self, "name_len", "Name length")
        yield Enum(UInt8(self, "file_type", "File type"), self.file_type)
        yield String(self, "name", self["name_len"].value, "File name")
        size = (self._size - self.current_size)//8
        if size:
            yield NullBytes(self, "padding", size)

    def createDescription(self):
        name = self["name"].value.strip("\0")
        if name:
            return "Directory entry: %s" % name
        else:
            return "Directory entry (empty)"

class Inode(FieldSet):
    inode_type_name = {
        1: "list of bad blocks",
        2: "Root directory",
        3: "ACL inode",
        4: "ACL inode",
        5: "Boot loader",
        6: "Undelete directory",
        8: "EXT3 journal"
    }
    file_type = {
        1: "Fifo",
        2: "Character device",
        4: "Directory",
        6: "Block device",
        8: "Regular",
        10: "Symbolic link",
        12: "Socket",
    }
    file_type_letter = {
        1: "p",
        4: "d",
        2: "c",
        6: "b",
        10: "l",
        12: "s",
    }
    static_size = (68 + 15*4)*8

    def __init__(self, parent, name, index):
        FieldSet.__init__(self, parent, name, None)
        self.uniq_id = 1+index

    def createDescription(self):
        desc = "Inode %s: " % self.uniq_id
        size = self["size"].value
        if self["blocks"].value == 0:
            desc += "(unused)"
        elif 11 <= self.uniq_id:
            size = humanFilesize(size)
            desc += "file, size=%s, mode=%s" % (size, self.getMode())
        else:
            if self.uniq_id in self.inode_type_name:
                desc += self.inode_type_name[self.uniq_id]
                if self.uniq_id == 2:
                    desc += " (%s)" % self.getMode()
            else:
                desc += "special"
        return desc

    def getMode(self):
        names = (
            ("owner_read", "owner_write", "owner_exec"),
            ("group_read", "group_write", "group_exec"),
            ("other_read", "other_write", "other_exec"))
        letters = "rwx"
        mode = [ "-"  for index in xrange(10) ]
        index = 1
        for loop in xrange(3):
            for name, letter in izip(names[loop], letters):
                if self[name].value:
                    mode[index] = letter
                index += 1
        file_type = self["file_type"].value
        if file_type in self.file_type_letter:
            mode[0] = self.file_type_letter[file_type]
        return "".join(mode)

    def createFields(self):
        # File mode
        yield Bit(self, "other_exec")
        yield Bit(self, "other_write")
        yield Bit(self, "other_read")
        yield Bit(self, "group_exec")
        yield Bit(self, "group_write")
        yield Bit(self, "group_read")
        yield Bit(self, "owner_exec")
        yield Bit(self, "owner_write")
        yield Bit(self, "owner_read")
        yield Bit(self, "sticky")
        yield Bit(self, "setgid")
        yield Bit(self, "setuid")
        yield Enum(Bits(self, "file_type", 4), self.file_type)

        yield UInt16(self, "uid", "User ID")
        yield UInt32(self, "size", "File size (in bytes)")
        yield TimestampUnix32(self, "atime", "Last access time")
        yield TimestampUnix32(self, "ctime", "Creation time")
        yield TimestampUnix32(self, "mtime", "Last modification time")
        yield TimestampUnix32(self, "dtime", "Delete time")
        yield UInt16(self, "gid", "Group ID")
        yield UInt16(self, "links_count", "Links count")
        yield UInt32(self, "blocks", "Number of blocks")
        yield UInt32(self, "flags", "Flags")
        yield NullBytes(self, "reserved[]", 4, "Reserved")
        for index in xrange(15):
            yield UInt32(self, "block[]")
        yield UInt32(self, "version", "Version")
        yield UInt32(self, "file_acl", "File ACL")
        yield UInt32(self, "dir_acl", "Directory ACL")
        yield UInt32(self, "faddr", "Block where the fragment of the file resides")

        os = self["/superblock/creator_os"].value
        if os == SuperBlock.OS_LINUX:
            yield UInt8(self, "frag", "Number of fragments in the block")
            yield UInt8(self, "fsize", "Fragment size")
            yield UInt16(self, "padding", "Padding")
            yield UInt16(self, "uid_high", "High 16 bits of user ID")
            yield UInt16(self, "gid_high", "High 16 bits of group ID")
            yield NullBytes(self, "reserved[]", 4, "Reserved")
        elif os == SuperBlock.OS_HURD:
            yield UInt8(self, "frag", "Number of fragments in the block")
            yield UInt8(self, "fsize", "Fragment size")
            yield UInt16(self, "mode_high", "High 16 bits of mode")
            yield UInt16(self, "uid_high", "High 16 bits of user ID")
            yield UInt16(self, "gid_high", "High 16 bits of group ID")
            yield UInt32(self, "author", "Author ID (?)")
        else:
            yield RawBytes(self, "raw", 12, "Reserved")

class Bitmap(FieldSet):
    def __init__(self, parent, name, start, size, description, **kw):
        description = "%s: %s items" % (description, size)
        FieldSet.__init__(self, parent, name, description, size=size, **kw)
        self.start = 1+start

    def createFields(self):
        for index in xrange(self._size):
            yield Bit(self, "item[]", "Item %s" % (self.start+index))

BlockBitmap = Bitmap
InodeBitmap = Bitmap

class GroupDescriptor(FieldSet):
    static_size = 32*8

    def __init__(self, parent, name, index):
        FieldSet.__init__(self, parent, name)
        self.uniq_id = index

    def createDescription(self):
        blocks_per_group = self["/superblock/blocks_per_group"].value
        start = self.uniq_id * blocks_per_group
        end = start + blocks_per_group
        return "Group descriptor: blocks %s-%s" % (start, end)

    def createFields(self):
        yield UInt32(self, "block_bitmap", "Points to the blocks bitmap block")
        yield UInt32(self, "inode_bitmap", "Points to the inodes bitmap block")
        yield UInt32(self, "inode_table", "Points to the inodes table first block")
        yield UInt16(self, "free_blocks_count", "Number of free blocks")
        yield UInt16(self, "free_inodes_count", "Number of free inodes")
        yield UInt16(self, "used_dirs_count", "Number of inodes allocated to directories")
        yield UInt16(self, "padding", "Padding")
        yield NullBytes(self, "reserved", 12, "Reserved")

class SuperBlock(FieldSet):
    static_size = 433*8

    OS_LINUX = 0
    OS_HURD = 1
    os_name = {
        0: "Linux",
        1: "Hurd",
        2: "Masix",
        3: "FreeBSD",
        4: "Lites",
        5: "WinNT"
    }
    state_desc = {
        1: "Valid (Unmounted cleanly)",
        2: "Error (Errors detected)",
        4: "Orphan FS (Orphans being recovered)",
    }
    error_handling_desc = { 1: "Continue" }

    def __init__(self, parent, name):
        FieldSet.__init__(self, parent, name)
        self._group_count = None

    def createDescription(self):
        if self["feature_compat"].value & 4:
            fstype = "ext3"
        else:
            fstype = "ext2"
        return "Superblock: %s file system" % fstype

    def createFields(self):
        yield UInt32(self, "inodes_count", "Inodes count")
        yield UInt32(self, "blocks_count", "Blocks count")
        yield UInt32(self, "r_blocks_count", "Reserved blocks count")
        yield UInt32(self, "free_blocks_count", "Free blocks count")
        yield UInt32(self, "free_inodes_count", "Free inodes count")
        yield UInt32(self, "first_data_block", "First data block")
        yield UInt32(self, "log_block_size", "Block size")
        yield UInt32(self, "log_frag_size", "Fragment size")
        yield UInt32(self, "blocks_per_group", "Blocks per group")
        yield UInt32(self, "frags_per_group", "Fragments per group")
        yield UInt32(self, "inodes_per_group", "Inodes per group")
        yield TimestampUnix32(self, "mtime", "Mount time")
        yield TimestampUnix32(self, "wtime", "Write time")
        yield UInt16(self, "mnt_count", "Mount count")
        yield UInt16(self, "max_mnt_count", "Max mount count")
        yield String(self, "magic", 2, "Magic number (0x53EF)")
        yield Enum(UInt16(self, "state", "File system state"), self.state_desc)
        yield Enum(UInt16(self, "errors", "Behaviour when detecting errors"), self.error_handling_desc)
        yield UInt16(self, "minor_rev_level", "Minor revision level")
        yield TimestampUnix32(self, "last_check", "Time of last check")
        yield textHandler(UInt32(self, "check_interval", "Maximum time between checks"), self.postMaxTime)
        yield Enum(UInt32(self, "creator_os", "Creator OS"), self.os_name)
        yield UInt32(self, "rev_level", "Revision level")
        yield UInt16(self, "def_resuid", "Default uid for reserved blocks")
        yield UInt16(self, "def_resgid", "Default gid for reserved blocks")
        yield UInt32(self, "first_ino", "First non-reserved inode")
        yield UInt16(self, "inode_size", "Size of inode structure")
        yield UInt16(self, "block_group_nr", "Block group # of this superblock")
        yield UInt32(self, "feature_compat", "Compatible feature set")
        yield UInt32(self, "feature_incompat", "Incompatible feature set")
        yield UInt32(self, "feature_ro_compat", "Read-only compatible feature set")
        yield RawBytes(self, "uuid", 16, "128-bit uuid for volume")
        yield String(self, "volume_name", 16, "Volume name", strip="\0")
        yield String(self, "last_mounted", 64, "Directory where last mounted", strip="\0")
        yield UInt32(self, "compression", "For compression (algorithm usage bitmap)")
        yield UInt8(self, "prealloc_blocks", "Number of blocks to try to preallocate")
        yield UInt8(self, "prealloc_dir_blocks", "Number to preallocate for directories")
        yield UInt16(self, "padding", "Padding")
        yield String(self, "journal_uuid", 16, "uuid of journal superblock")
        yield UInt32(self, "journal_inum", "inode number of journal file")
        yield UInt32(self, "journal_dev", "device number of journal file")
        yield UInt32(self, "last_orphan", "start of list of inodes to delete")
        yield RawBytes(self, "reserved", 197, "Reserved")

    def _getGroupCount(self):
        if self._group_count is None:
            # Calculate number of groups
            blocks_per_group = self["blocks_per_group"].value
            self._group_count = (self["blocks_count"].value - self["first_data_block"].value + (blocks_per_group - 1)) / blocks_per_group
        return self._group_count
    group_count = property(_getGroupCount)

    def postMaxTime(self, chunk):
        return humanDuration(chunk.value * 1000)

class GroupDescriptors(FieldSet):
    def __init__(self, parent, name, count):
        FieldSet.__init__(self, parent, name)
        self.count = count

    def createDescription(self):
        return "Group descriptors: %s items" % self.count

    def createFields(self):
        for index in range(0, self.count):
            yield GroupDescriptor(self, "group[]", index)

class InodeTable(FieldSet):
    def __init__(self, parent, name, start, count):
        FieldSet.__init__(self, parent, name)
        self.start = start
        self.count = count
        self._size = self.count * self["/superblock/inode_size"].value * 8

    def createDescription(self):
        return "Group descriptors: %s items" % self.count

    def createFields(self):
        for index in range(self.start, self.start+self.count):
            yield Inode(self, "inode[]", index)

class Group(FieldSet):
    def __init__(self, parent, name, index):
        FieldSet.__init__(self, parent, name)
        self.uniq_id = index

    def createDescription(self):
        desc = "Group %s: %s" % (self.uniq_id, humanFilesize(self.size/8))
        if "superblock_copy" in self:
            desc += " (with superblock copy)"
        return desc

    def createFields(self):
        group = self["../group_desc/group[%u]" % self.uniq_id]
        superblock = self["/superblock"]
        block_size = self["/"].block_size

        # Read block bitmap
        addr = self.absolute_address + 56*8
        self.superblock_copy = (self.stream.readBytes(addr, 2) == "\x53\xEF")
        if self.superblock_copy:
            yield SuperBlock(self, "superblock_copy")

        # Compute number of block and inodes
        block_count = superblock["blocks_per_group"].value
        inode_count = superblock["inodes_per_group"].value
        block_index = self.uniq_id * block_count
        inode_index = self.uniq_id * inode_count
        if (block_count % 8) != 0:
            raise ParserError("Invalid block count")
        if (inode_count % 8) != 0:
            raise ParserError("Invalid inode count")
        block_count = min(block_count, superblock["blocks_count"].value - block_index)
        inode_count = min(inode_count, superblock["inodes_count"].value - inode_index)

        # Read block bitmap
        field = self.seekByte(group["block_bitmap"].value * block_size, relative=False, null=True)
        if field:
            yield field
        yield BlockBitmap(self, "block_bitmap", block_index, block_count, "Block bitmap")

        # Read inode bitmap
        field = self.seekByte(group["inode_bitmap"].value * block_size, relative=False)
        if field:
            yield field
        yield InodeBitmap(self, "inode_bitmap", inode_index, inode_count, "Inode bitmap")

        # Read inode table
        field = self.seekByte(alignValue(self.current_size//8, block_size))
        if field:
            yield field
        yield InodeTable(self, "inode_table", inode_index, inode_count)

        # Add padding if needed
        addr = min(self.parent.size / 8,
            (self.uniq_id+1) * superblock["blocks_per_group"].value * block_size)
        yield self.seekByte(addr, "data", relative=False)

class EXT2_FS(Parser):
    """
    Parse an EXT2 or EXT3 partition.

    Attributes:
       * block_size: Size of a block (in bytes)

    Fields:
       * superblock: Most important block, store most important informations
       * ...
    """
    PARSER_TAGS = {
        "id": "ext2",
        "category": "file_system",
        "description": "EXT2/EXT3 file system",
        "min_size": (1024*2)*8,
        "magic": (
            # (magic, state=valid)
            ("\x53\xEF\1\0", 1080*8),
            # (magic, state=error)
            ("\x53\xEF\2\0", 1080*8),
            # (magic, state=error)
            ("\x53\xEF\4\0", 1080*8),
        ),
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self.stream.readBytes((1024+56)*8, 2) != "\x53\xEF":
            return "Invalid magic number"
        if not(0 <= self["superblock/log_block_size"].value <= 2):
            return "Invalid (log) block size"
        if self["superblock/inode_size"].value != (68 + 15*4):
            return "Unsupported inode size"
        return True

    def createFields(self):
        # Skip something (what is stored here? MBR?)
        yield NullBytes(self, "padding[]", 1024)

        # Read superblock
        superblock = SuperBlock(self, "superblock")
        yield superblock
        if not(0 <= self["superblock/log_block_size"].value <= 2):
            raise ParserError("EXT2: Invalid (log) block size")
        self.block_size = 1024 << superblock["log_block_size"].value # in bytes

        # Read groups' descriptor
        field = self.seekByte(((1023 + superblock.size/8) / self.block_size + 1) * self.block_size, null=True)
        if field:
            yield field
        groups = GroupDescriptors(self, "group_desc", superblock.group_count)
        yield groups

        # Read groups
        address = groups["group[0]/block_bitmap"].value * self.block_size
        field = self.seekByte(address, null=True)
        if field:
            yield field
        for index in range(0, superblock.group_count):
            yield Group(self, "group[]", index)

    def getSuperblock(self):
        # FIXME: Use superblock copy if main superblock is invalid
        return self["superblock"]

    def createDescription(self):
        superblock = self.getSuperblock()
        block_size = 1024 << superblock["log_block_size"].value
        nb_block = superblock["blocks_count"].value
        total = nb_block * block_size
        used = (superblock["free_blocks_count"].value) * block_size
        desc = "EXT2/EXT3"
        if "group[0]/inode_table/inode[7]/blocks" in self:
            if 0 < self["group[0]/inode_table/inode[7]/blocks"].value:
                desc = "EXT3"
            else:
                desc = "EXT2"
        return desc + " file system: total=%s, used=%s, block=%s" % (
            humanFilesize(total), humanFilesize(used),
            humanFilesize(block_size))



########NEW FILE########
__FILENAME__ = fat
from lib.hachoir_core.compatibility import sorted
from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, StaticFieldSet,
    RawBytes, PaddingBytes, createPaddingField, Link, Fragment,
    Bit, Bits, UInt8, UInt16, UInt32,
    String, Bytes, NullBytes)
from lib.hachoir_core.field.integer import GenericInteger
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.error import error
from lib.hachoir_core.tools import humanFilesize, makePrintable
import datetime
import re

strip_index = re.compile(r'\[[^]]+]$')


class Boot(FieldSet):
    static_size = 512*8
    def createFields(self):
        yield Bytes(self, "jmp", 3, "Jump instruction (to skip over header on boot)")
        yield Bytes(self, "oem_name", 8, "OEM Name (padded with spaces)")
        yield UInt16(self, "sector_size", "Bytes per sector")
        yield UInt8 (self, "cluster_size", "Sectors per cluster")
        yield UInt16(self, "reserved_sectors", "Reserved sector count (including boot sector)")
        yield UInt8 (self, "fat_nb", "Number of file allocation tables")
        yield UInt16(self, "max_root", "Maximum number of root directory entries")
        yield UInt16(self, "sectors1", "Total sectors (if zero, use 'sectors2')")
        yield UInt8 (self, "media_desc", "Media descriptor")
        yield UInt16(self, "fat_size", "Sectors per FAT")
        yield UInt16(self, "track_size", "Sectors per track")
        yield UInt16(self, "head_nb", "Number of heads")
        yield UInt32(self, "hidden", "Hidden sectors")
        yield UInt32(self, "sectors2", "Total sectors (if greater than 65535)")
        if self.parent.version == 32:
            yield UInt32(self, "fat32_size", "Sectors per FAT")
            yield UInt16(self, "fat_flags", "FAT Flags")
            yield UInt16(self, "version", "Version")
            yield UInt32(self, "root_start", "Cluster number of root directory start")
            yield UInt16(self, "inf_sector", "Sector number of FS Information Sector")
            yield UInt16(self, "boot_copy", "Sector number of a copy of this boot sector")
            yield NullBytes(self, "reserved[]", 12, "Reserved")
        yield UInt8(self, "phys_drv", "Physical drive number")
        yield NullBytes(self, "reserved[]", 1, 'Reserved ("current head")')
        yield UInt8(self, "sign", "Signature")
        yield textHandler(UInt32(self, "serial", "ID (serial number)"), hexadecimal)
        yield String(self, "label", 11, "Volume Label", strip=' ', charset="ASCII")
        yield String(self, "fs_type", 8, "FAT file system type", strip=' ', charset="ASCII")
        yield Bytes(self, "code", 510-self.current_size/8, "Operating system boot code")
        yield Bytes(self, "trail_sig", 2, "Signature (0x55 0xAA)")


class FSInfo(StaticFieldSet):
    format = (
        (String, "lead_sig", 4, 'Signature ("RRaA")'),
        (NullBytes,  "reserved[]", 480),
        (String, "struct_sig", 4, 'Signature ("rrAa")'),
        (UInt32, "free_count", "Last known free cluster count on the volume"),
        (UInt32, "nxt_free",),
        (NullBytes,  "reserved[]", 12),
        (Bytes,  "trail_sig", 4, "Signature (0x00 0x00 0x55 0xAA)")
    )


class FAT(FieldSet):
    class FAT(FieldSet):
        def createFields(self):
            parent = self.parent
            version = parent.parent.version
            text_handler = parent.text_handler
            while self.current_size < self._size:
                yield textHandler(GenericInteger(self, 'entry[]', False, version), text_handler)
    def createFields(self):
        version = self.parent.version
        max_entry = 1 << min(28, version)
        def FatEntry(chunk):
            i = chunk.value
            j = (1 - i) % max_entry
            if j == 0:
                return "reserved cluster"
            elif j == 1:
                return "free cluster"
            elif j < 10:
                return "end of a chain"
            elif j == 10:
                return "bad cluster"
            elif j < 18:
                return "reserved value"
            else:
                return str(i)
        self.text_handler = FatEntry
        while self.current_size < self._size:
            yield FAT.FAT(self, 'group[]', size=min(1000*version,self._size-self.current_size))


class Date(FieldSet):
    def __init__(self, parent, name):
        FieldSet.__init__(self, parent, name, size={
            "create": 5,
            "access": 2,
            "modify": 4,
        }[name] * 8)

    def createFields(self):
        size = self.size / 8
        if size > 2:
            if size > 4:
                yield UInt8(self, "cs", "10ms units, values from 0 to 199")
            yield Bits(self, "2sec", 5, "seconds/2")
            yield Bits(self, "min", 6, "minutes")
            yield Bits(self, "hour", 5, "hours")
        yield Bits(self, "day", 5, "(1-31)")
        yield Bits(self, "month", 4, "(1-12)")
        yield Bits(self, "year", 7, "(0 = 1980, 127 = 2107)")

    def createDescription(self):
        date = [ self["year"].value, self["month"].value, self["day"].value ]
        size = self.size / 8
        if size > 2:
            mkdate = datetime.datetime
            cs = 200 * self["2sec"].value
            if size > 4:
                cs += self["cs"].value
            date += [ self["hour"].value, self["min"].value, cs / 100, cs % 100 * 10000 ]
        else:
            mkdate = datetime.date
        if date == [ 0 for i in date ]:
            date = None
        else:
            date[0] += 1980
            try:
                date = mkdate(*tuple(date))
            except ValueError:
                return "invalid"
        return str(date)


class InodeLink(Link):
    def __init__(self, parent, name, target=None):
        Link.__init__(self, parent, name)
        self.target = target
        self.first = None

    def _getTargetPath(self):
        if not self.target:
            parent = self.parent
            self.target = strip_index.sub(r"\\", parent.parent._name) + parent.getFilename().rstrip("/")
        return self.target

    def createValue(self):
        field = InodeGen(self["/"], self.parent, self._getTargetPath())(self)
        if field:
            self._display = field.path
            return Link.createValue(self)

    def createDisplay(self):
        return "/%s[0]" % self._getTargetPath()


class FileEntry(FieldSet):
    static_size = 32*8
    process = False
    LFN = False

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self.status = self.stream.readBits(self.absolute_address, 8, LITTLE_ENDIAN)
        if self.status in (0, 0xE5):
            return

        magic = self.stream.readBits(self.absolute_address+11*8, 8, LITTLE_ENDIAN)
        if magic & 0x3F == 0x0F:
            self.LFN = True
        elif self.getFilename() not in (".", ".."):
            self.process = True

    def getFilename(self):
        name = self["name"].value
        if isinstance(name, str):
            name = makePrintable(name, "ASCII", to_unicode=True)
        ext = self["ext"].value
        if ext:
            name += "." + ext
        if name[0] == 5:
            name = "\xE5" + name[1:]
        if not self.LFN and self["directory"].value:
            name += "/"
        return name

    def createDescription(self):
        if self.status == 0:
            return "Free entry"
        elif self.status == 0xE5:
            return "Deleted file"
        elif self.LFN:
            name = "".join( field.value for field in self.array("name") )
            try:
                name = name[:name.index('\0')]
            except ValueError:
                pass
            seq_no = self["seq_no"].value
            return "Long filename part: '%s' [%u]" % (name, seq_no)
        else:
            return "File: '%s'" % self.getFilename()

    def getCluster(self):
        cluster = self["cluster_lo"].value
        if self.parent.parent.version > 16:
            cluster += self["cluster_hi"].value << 16
        return cluster

    def createFields(self):
        if not self.LFN:
            yield String(self, "name", 8, "DOS file name (padded with spaces)",
                strip=' ', charset="ASCII")
            yield String(self, "ext", 3, "DOS file extension (padded with spaces)",
                strip=' ', charset="ASCII")
            yield Bit(self, "read_only")
            yield Bit(self, "hidden")
            yield Bit(self, "system")
            yield Bit(self, "volume_label")
            yield Bit(self, "directory")
            yield Bit(self, "archive")
            yield Bit(self, "device")
            yield Bit(self, "unused")
            yield RawBytes(self, "reserved", 1, "Something about the case")
            yield Date(self, "create")
            yield Date(self, "access")
            if self.parent.parent.version > 16:
                yield UInt16(self, "cluster_hi")
            else:
                yield UInt16(self, "ea_index")
            yield Date(self, "modify")
            yield UInt16(self, "cluster_lo")
            size = UInt32(self, "size")
            yield size
            if self.process:
                del self.process
                target_size = size.value
                if self["directory"].value:
                    if target_size:
                        size.error("(FAT) value must be zero")
                        target_size = 0
                elif not target_size:
                    return
                self.target_size = 8 * target_size
                yield InodeLink(self, "data")
        else:
            yield UInt8(self, "seq_no", "Sequence Number")
            yield String(self, "name[]", 10, "(5 UTF-16 characters)",
                charset="UTF-16-LE")
            yield UInt8(self, "magic", "Magic number (15)")
            yield NullBytes(self, "reserved", 1, "(always 0)")
            yield UInt8(self, "checksum", "Checksum of DOS file name")
            yield String(self, "name[]", 12, "(6 UTF-16 characters)",
                charset="UTF-16-LE")
            yield UInt16(self, "first_cluster", "(always 0)")
            yield String(self, "name[]",  4, "(2 UTF-16 characters)",
                charset="UTF-16-LE")

class Directory(Fragment):
    def createFields(self):
        while self.current_size < self._size:
            yield FileEntry(self, "entry[]")

class File(Fragment):
    def _getData(self):
        return self["data"]
    def createFields(self):
        yield Bytes(self, "data", self.datasize/8)
        padding = self._size - self.current_size
        if padding:
            yield createPaddingField(self, padding)

class InodeGen:
    def __init__(self, root, entry, path):
        self.root = root
        self.cluster = root.clusters(entry.getCluster)
        self.path = path
        self.filesize = entry.target_size
        self.done = 0
        def createInputStream(cis, **args):
            args["size"] = self.filesize
            args.setdefault("tags",[]).append(("filename", entry.getFilename()))
            return cis(**args)
        self.createInputStream = createInputStream

    def __call__(self, prev):
        name = self.path + "[]"
        address, size, last = self.cluster.next()
        if self.filesize:
            if self.done >= self.filesize:
                error("(FAT) bad metadata for " + self.path)
                return
            field = File(self.root, name, size=size)
            if prev.first is None:
                field._description = 'File size: %s' % humanFilesize(self.filesize//8)
                field.setSubIStream(self.createInputStream)
            field.datasize = min(self.filesize - self.done, size)
            self.done += field.datasize
        else:
            field = Directory(self.root, name, size=size)
        padding = self.root.getFieldByAddress(address, feed=False)
        if not isinstance(padding, (PaddingBytes, RawBytes)):
            error("(FAT) address %u doesn't point to a padding field" % address)
            return
        if last:
            next = None
        else:
            next = lambda: self(field)
        field.setLinks(prev.first, next)
        self.root.writeFieldsIn(padding, address, (field,))
        return field


class FAT_FS(Parser):
    endian = LITTLE_ENDIAN
    PARSER_TAGS = {
        "category": "file_system",
        "min_size": 512*8,
        "file_ext": ("",),
    }

    def _validate(self, type_offset):
        if self.stream.readBytes(type_offset*8, 8) != ("FAT%-5u" % self.version):
            return "Invalid FAT%u signature" % self.version
        if self.stream.readBytes(510*8, 2) != "\x55\xAA":
            return "Invalid BIOS signature"
        return True

    def clusters(self, cluster_func):
        max_entry = (1 << min(28, self.version)) - 16
        cluster = cluster_func()
        if 1 < cluster < max_entry:
            clus_nb = 1
            next = cluster
            while True:
                next = self.fat[next/1000][next%1000].value
                if not 1 < next < max_entry:
                    break
                if cluster + clus_nb == next:
                    clus_nb += 1
                else:
                    yield self.data_start + cluster * self.cluster_size, clus_nb * self.cluster_size, False
                    cluster = next
                    clus_nb = 1
            yield self.data_start + cluster * self.cluster_size, clus_nb * self.cluster_size, True

    def createFields(self):
        # Read boot seector
        boot = Boot(self, "boot", "Boot sector")
        yield boot
        self.sector_size = boot["sector_size"].value

        if self.version == 32:
            for field in sorted((
                (boot["inf_sector"].value, lambda: FSInfo(self, "fsinfo")),
                (boot["boot_copy"].value, lambda: Boot(self, "bkboot", "Copy of the boot sector")),
            )):
                if field[0]:
                    padding = self.seekByte(field[0] * self.sector_size)
                    if padding:
                        yield padding
                    yield field[1]()
        padding = self.seekByte(boot["reserved_sectors"].value * self.sector_size)
        if padding:
            yield padding

        # Read the two FAT
        fat_size = boot["fat_size"].value
        if fat_size == 0:
            fat_size = boot["fat32_size"].value
        fat_size *= self.sector_size * 8
        for i in xrange(boot["fat_nb"].value):
            yield FAT(self, "fat[]", "File Allocation Table", size=fat_size)

        # Read inode table (Directory)
        self.cluster_size = boot["cluster_size"].value * self.sector_size * 8
        self.fat = self["fat[0]"]
        if "root_start" in boot:
            self.target_size = 0
            self.getCluster = lambda: boot["root_start"].value
            yield InodeLink(self, "root", "root")
        else:
            yield Directory(self, "root[]", size=boot["max_root"].value * 32 * 8)
        self.data_start = self.current_size - 2 * self.cluster_size
        sectors = boot["sectors1"].value
        if not sectors:
            sectors = boot["sectors2"].value

        # Create one big padding field for the end
        size = sectors * self.sector_size
        if self._size:
            size = min(size, self.size//8)
        padding = self.seekByte(size)
        if padding:
            yield padding


class FAT12(FAT_FS):
    PARSER_TAGS = {
        "id": "fat12",
        "description": "FAT12 filesystem",
        "magic": (("FAT12   ", 54*8),),
    }
    version = 12

    def validate(self):
        return FAT_FS._validate(self, 54)


class FAT16(FAT_FS):
    PARSER_TAGS = {
        "id": "fat16",
        "description": "FAT16 filesystem",
        "magic": (("FAT16   ", 54*8),),
    }
    version = 16

    def validate(self):
        return FAT_FS._validate(self, 54)


class FAT32(FAT_FS):
    PARSER_TAGS = {
        "id": "fat32",
        "description": "FAT32 filesystem",
        "magic": (("FAT32   ", 82*8),),
    }
    version = 32

    def validate(self):
        return FAT_FS._validate(self, 82)

########NEW FILE########
__FILENAME__ = iso9660
"""
ISO 9660 (cdrom) file system parser.

Documents:
- Standard ECMA-119 (december 1987)
  http://www.nondot.org/sabre/os/files/FileSystems/iso9660.pdf

Author: Victor Stinner
Creation: 11 july 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt32, UInt64, Enum,
    NullBytes, RawBytes, String)
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN

class PrimaryVolumeDescriptor(FieldSet):
    static_size = 2041*8
    def createFields(self):
        yield NullBytes(self, "unused[]", 1)
        yield String(self, "system_id", 32, "System identifier", strip=" ")
        yield String(self, "volume_id", 32, "Volume identifier", strip=" ")
        yield NullBytes(self, "unused[]", 8)
        yield UInt64(self, "space_size", "Volume space size")
        yield NullBytes(self, "unused[]", 32)
        yield UInt32(self, "set_size", "Volume set size")
        yield UInt32(self, "seq_num", "Sequence number")
        yield UInt32(self, "block_size", "Block size")
        yield UInt64(self, "path_table_size", "Path table size")
        yield UInt32(self, "occu_lpath", "Location of Occurrence of Type L Path Table")
        yield UInt32(self, "opt_lpath", "Location of Optional of Type L Path Table")
        yield UInt32(self, "occu_mpath", "Location of Occurrence of Type M Path Table")
        yield UInt32(self, "opt_mpath", "Location of Optional of Type M Path Table")
        yield RawBytes(self, "root", 34, "Directory Record for Root Directory")
        yield String(self, "vol_set_id", 128, "Volume set identifier", strip=" ")
        yield String(self, "publisher", 128, "Publisher identifier", strip=" ")
        yield String(self, "data_preparer", 128, "Data preparer identifier", strip=" ")
        yield String(self, "application", 128, "Application identifier", strip=" ")
        yield String(self, "copyright", 37, "Copyright file identifier", strip=" ")
        yield String(self, "abstract", 37, "Abstract file identifier", strip=" ")
        yield String(self, "biographic", 37, "Biographic file identifier", strip=" ")
        yield String(self, "creation_ts", 17, "Creation date and time", strip=" ")
        yield String(self, "modification_ts", 17, "Modification date and time", strip=" ")
        yield String(self, "expiration_ts", 17, "Expiration date and time", strip=" ")
        yield String(self, "effective_ts", 17, "Effective date and time", strip=" ")
        yield UInt8(self, "struct_ver", "Structure version")
        yield NullBytes(self, "unused[]", 1)
        yield String(self, "app_use", 512, "Application use", strip=" \0")
        yield NullBytes(self, "unused[]", 653)

class BootRecord(FieldSet):
    static_size = 2041*8
    def createFields(self):
        yield String(self, "sys_id", 31, "Boot system identifier", strip="\0")
        yield String(self, "boot_id", 31, "Boot identifier", strip="\0")
        yield RawBytes(self, "system_use", 1979, "Boot system use")

class Terminator(FieldSet):
    static_size = 2041*8
    def createFields(self):
        yield NullBytes(self, "null", 2041)

class Volume(FieldSet):
    endian = BIG_ENDIAN
    TERMINATOR = 255
    type_name = {
        0: "Boot Record",
        1: "Primary Volume Descriptor",
        2: "Supplementary Volume Descriptor",
        3: "Volume Partition Descriptor",
        TERMINATOR: "Volume Descriptor Set Terminator",
    }
    static_size = 2048 * 8
    content_handler = {
        0: BootRecord,
        1: PrimaryVolumeDescriptor,
        TERMINATOR: Terminator,
    }

    def createFields(self):
        yield Enum(UInt8(self, "type", "Volume descriptor type"), self.type_name)
        yield RawBytes(self, "signature", 5, "ISO 9960 signature (CD001)")
        if self["signature"].value != "CD001":
            raise ParserError("Invalid ISO 9960 volume signature")
        yield UInt8(self, "version", "Volume descriptor version")
        cls = self.content_handler.get(self["type"].value, None)
        if cls:
            yield cls(self, "content")
        else:
            yield RawBytes(self, "raw_content", 2048-7)

class ISO9660(Parser):
    endian = LITTLE_ENDIAN
    MAGIC = "\x01CD001"
    NULL_BYTES = 0x8000
    PARSER_TAGS = {
        "id": "iso9660",
        "category": "file_system",
        "description": "ISO 9660 file system",
        "min_size": (NULL_BYTES + 6)*8,
        "magic": ((MAGIC, NULL_BYTES*8),),
    }

    def validate(self):
        if self.stream.readBytes(self.NULL_BYTES*8, len(self.MAGIC)) != self.MAGIC:
            return "Invalid signature"
        return True

    def createFields(self):
        yield self.seekByte(self.NULL_BYTES, null=True)

        while True:
            volume = Volume(self, "volume[]")
            yield volume
            if volume["type"].value == Volume.TERMINATOR:
                break

        if self.current_size < self._size:
            yield self.seekBit(self._size, "end")


########NEW FILE########
__FILENAME__ = linux_swap
"""
Linux swap file.

Documentation: Linux kernel source code, files:
 - mm/swapfile.c
 - include/linux/swap.h

Author: Victor Stinner
Creation date: 25 december 2006 (christmas ;-))
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (ParserError, GenericVector,
    UInt32, String,
    Bytes, NullBytes, RawBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.tools import humanFilesize
from lib.hachoir_core.bits import str2hex

PAGE_SIZE = 4096

# Definition of MAX_SWAP_BADPAGES in Linux kernel:
#  (__swapoffset(magic.magic) - __swapoffset(info.badpages)) / sizeof(int)
MAX_SWAP_BADPAGES = ((PAGE_SIZE - 10) - 1536) // 4

class Page(RawBytes):
    static_size = PAGE_SIZE*8
    def __init__(self, parent, name):
        RawBytes.__init__(self, parent, name, PAGE_SIZE)

class UUID(Bytes):
    static_size = 16*8
    def __init__(self, parent, name):
        Bytes.__init__(self, parent, name, 16)
    def createDisplay(self):
        text = str2hex(self.value, format=r"%02x")
        return "%s-%s-%s-%s-%s" % (
            text[:8], text[8:12], text[12:16], text[16:20], text[20:])

class LinuxSwapFile(Parser):
    PARSER_TAGS = {
        "id": "linux_swap",
        "file_ext": ("",),
        "category": "file_system",
        "min_size": PAGE_SIZE*8,
        "description": "Linux swap file",
        "magic": (
            ("SWAP-SPACE", (PAGE_SIZE-10)*8),
            ("SWAPSPACE2", (PAGE_SIZE-10)*8),
            ("S1SUSPEND\0", (PAGE_SIZE-10)*8),
        ),
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        magic = self.stream.readBytes((PAGE_SIZE-10)*8, 10)
        if magic not in ("SWAP-SPACE", "SWAPSPACE2", "S1SUSPEND\0"):
            return "Unknown magic string"
        if MAX_SWAP_BADPAGES < self["nb_badpage"].value:
            return "Invalid number of bad page (%u)" % self["nb_badpage"].value
        return True

    def getPageCount(self):
        """
        Number of pages which can really be used for swapping:
        number of page minus bad pages minus one page (used for the header)
        """
        # -1 because first page is used for the header
        return self["last_page"].value - self["nb_badpage"].value - 1

    def createDescription(self):
        if self["magic"].value == "S1SUSPEND\0":
            text = "Suspend swap file version 1"
        elif self["magic"].value == "SWAPSPACE2":
            text = "Linux swap file version 2"
        else:
            text = "Linux swap file version 1"
        nb_page = self.getPageCount()
        return "%s, page size: %s, %s pages" % (
            text, humanFilesize(PAGE_SIZE), nb_page)

    def createFields(self):
        # First kilobyte: boot sectors
        yield RawBytes(self, "boot", 1024, "Space for disklabel etc.")

        # Header
        yield UInt32(self, "version")
        yield UInt32(self, "last_page")
        yield UInt32(self, "nb_badpage")
        yield UUID(self, "sws_uuid")
        yield UUID(self, "sws_volume")
        yield NullBytes(self, "reserved", 117*4)

        # Read bad pages (if any)
        count = self["nb_badpage"].value
        if count:
            if MAX_SWAP_BADPAGES < count:
                raise ParserError("Invalid number of bad page (%u)" % count)
            yield GenericVector(self, "badpages", count, UInt32, "badpage")

        # Read magic
        padding = self.seekByte(PAGE_SIZE - 10, "padding", null=True)
        if padding:
            yield padding
        yield String(self, "magic", 10, charset="ASCII")

        # Read all pages
        yield GenericVector(self, "pages", self["last_page"].value, Page, "page")

        # Padding at the end
        padding = self.seekBit(self.size, "end_padding", null=True)
        if padding:
            yield padding


########NEW FILE########
__FILENAME__ = mbr
"""
Master Boot Record.


"""

# cfdisk uses the following algorithm to compute the geometry:
# 0. Use the values given by the user.
# 1. Try to guess the geometry from the partition table:
#    if all the used partitions end at the same head H and the
#    same sector S, then there are (H+1) heads and S sectors/cylinder.
# 2. Ask the system (ioctl/HDIO_GETGEO).
# 3. 255 heads and 63 sectors/cylinder.

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    Enum, Bits, UInt8, UInt16, UInt32,
    RawBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.tools import humanFilesize
from lib.hachoir_core.text_handler import textHandler, hexadecimal

BLOCK_SIZE = 512  # bytes

class CylinderNumber(Bits):
    def __init__(self, parent, name, description=None):
        Bits.__init__(self, parent, name, 10, description)

    def createValue(self):
        i = self.parent.stream.readInteger(
            self.absolute_address, False, self._size, self.parent.endian)
        return i >> 2 | i % 4 << 8

class PartitionHeader(FieldSet):
    static_size = 16*8

    # taken from the source of cfdisk:
    # sed -n 's/.*{\(.*\), N_(\(.*\))}.*/        \1: \2,/p' i386_sys_types.c
    system_name = {
        0x00: "Empty",
        0x01: "FAT12",
        0x02: "XENIX root",
        0x03: "XENIX usr",
        0x04: "FAT16 <32M",
        0x05: "Extended",
        0x06: "FAT16",
        0x07: "HPFS/NTFS",
        0x08: "AIX",
        0x09: "AIX bootable",
        0x0a: "OS/2 Boot Manager",
        0x0b: "W95 FAT32",
        0x0c: "W95 FAT32 (LBA)",
        0x0e: "W95 FAT16 (LBA)",
        0x0f: "W95 Ext'd (LBA)",
        0x10: "OPUS",
        0x11: "Hidden FAT12",
        0x12: "Compaq diagnostics",
        0x14: "Hidden FAT16 <32M",
        0x16: "Hidden FAT16",
        0x17: "Hidden HPFS/NTFS",
        0x18: "AST SmartSleep",
        0x1b: "Hidden W95 FAT32",
        0x1c: "Hidden W95 FAT32 (LBA)",
        0x1e: "Hidden W95 FAT16 (LBA)",
        0x24: "NEC DOS",
        0x39: "Plan 9",
        0x3c: "PartitionMagic recovery",
        0x40: "Venix 80286",
        0x41: "PPC PReP Boot",
        0x42: "SFS",
        0x4d: "QNX4.x",
        0x4e: "QNX4.x 2nd part",
        0x4f: "QNX4.x 3rd part",
        0x50: "OnTrack DM",
        0x51: "OnTrack DM6 Aux1",
        0x52: "CP/M",
        0x53: "OnTrack DM6 Aux3",
        0x54: "OnTrackDM6",
        0x55: "EZ-Drive",
        0x56: "Golden Bow",
        0x5c: "Priam Edisk",
        0x61: "SpeedStor",
        0x63: "GNU HURD or SysV",
        0x64: "Novell Netware 286",
        0x65: "Novell Netware 386",
        0x70: "DiskSecure Multi-Boot",
        0x75: "PC/IX",
        0x80: "Old Minix",
        0x81: "Minix / old Linux",
        0x82: "Linux swap / Solaris",
        0x83: "Linux (ext2/ext3)",
        0x84: "OS/2 hidden C: drive",
        0x85: "Linux extended",
        0x86: "NTFS volume set",
        0x87: "NTFS volume set",
        0x88: "Linux plaintext",
        0x8e: "Linux LVM",
        0x93: "Amoeba",
        0x94: "Amoeba BBT",
        0x9f: "BSD/OS",
        0xa0: "IBM Thinkpad hibernation",
        0xa5: "FreeBSD",
        0xa6: "OpenBSD",
        0xa7: "NeXTSTEP",
        0xa8: "Darwin UFS",
        0xa9: "NetBSD",
        0xab: "Darwin boot",
        0xb7: "BSDI fs",
        0xb8: "BSDI swap",
        0xbb: "Boot Wizard hidden",
        0xbe: "Solaris boot",
        0xbf: "Solaris",
        0xc1: "DRDOS/sec (FAT-12)",
        0xc4: "DRDOS/sec (FAT-16 < 32M)",
        0xc6: "DRDOS/sec (FAT-16)",
        0xc7: "Syrinx",
        0xda: "Non-FS data",
        0xdb: "CP/M / CTOS / ...",
        0xde: "Dell Utility",
        0xdf: "BootIt",
        0xe1: "DOS access",
        0xe3: "DOS R/O",
        0xe4: "SpeedStor",
        0xeb: "BeOS fs",
        0xee: "EFI GPT",
        0xef: "EFI (FAT-12/16/32)",
        0xf0: "Linux/PA-RISC boot",
        0xf1: "SpeedStor",
        0xf4: "SpeedStor",
        0xf2: "DOS secondary",
        0xfd: "Linux raid autodetect",
        0xfe: "LANstep",
        0xff: "BBT"
    }

    def createFields(self):
        yield UInt8(self, "bootable", "Bootable flag (true if equals to 0x80)")
        if self["bootable"].value not in (0x00, 0x80):
            self.warning("Stream doesn't look like master boot record (partition bootable error)!")
        yield UInt8(self, "start_head", "Starting head number of the partition")
        yield Bits(self, "start_sector", 6, "Starting sector number of the partition")
        yield CylinderNumber(self, "start_cylinder", "Starting cylinder number of the partition")
        yield Enum(UInt8(self, "system", "System indicator"), self.system_name)
        yield UInt8(self, "end_head", "Ending head number of the partition")
        yield Bits(self, "end_sector", 6, "Ending sector number of the partition")
        yield CylinderNumber(self, "end_cylinder", "Ending cylinder number of the partition")
        yield UInt32(self, "LBA", "LBA (number of sectors before this partition)")
        yield UInt32(self, "size", "Size (block count)")

    def isUsed(self):
        return self["system"].value != 0

    def createDescription(self):
        desc = "Partition header: "
        if self.isUsed():
            system = self["system"].display
            size = self["size"].value * BLOCK_SIZE
            desc += "%s, %s" % (system, humanFilesize(size))
        else:
            desc += "(unused)"
        return desc


class MasterBootRecord(FieldSet):
    static_size = 512*8

    def createFields(self):
        yield RawBytes(self, "program", 446, "Boot program (Intel x86 machine code)")
        yield PartitionHeader(self, "header[0]")
        yield PartitionHeader(self, "header[1]")
        yield PartitionHeader(self, "header[2]")
        yield PartitionHeader(self, "header[3]")
        yield textHandler(UInt16(self, "signature", "Signature (0xAA55)"), hexadecimal)

    def _getPartitions(self):
        return ( self[index] for index in xrange(1,5) )
    headers = property(_getPartitions)


class Partition(FieldSet):
    def createFields(self):
        mbr = MasterBootRecord(self, "mbr")
        yield mbr

        # No error if we only want to analyse a backup of a mbr
        if self.eof:
            return

        for start, index, header in sorted((hdr["LBA"].value, index, hdr)
                for index, hdr in enumerate(mbr.headers) if hdr.isUsed()):
            # Seek to the beginning of the partition
            padding = self.seekByte(start * BLOCK_SIZE, "padding[]")
            if padding:
                yield padding

            # Content of the partition
            name = "partition[%u]" % index
            size = BLOCK_SIZE * header["size"].value
            desc = header["system"].display
            if header["system"].value == 5:
                yield Partition(self, name, desc, size * 8)
            else:
                yield RawBytes(self, name, size, desc)

        # Padding at the end
        if self.current_size < self._size:
            yield self.seekBit(self._size, "end")


class MSDos_HardDrive(Parser, Partition):
    endian = LITTLE_ENDIAN
    MAGIC = "\x55\xAA"
    PARSER_TAGS = {
        "id": "msdos_harddrive",
        "category": "file_system",
        "description": "MS-DOS hard drive with Master Boot Record (MBR)",
        "min_size": 512*8,
        "file_ext": ("",),
#        "magic": ((MAGIC, 510*8),),
    }

    def validate(self):
        if self.stream.readBytes(510*8, 2) != self.MAGIC:
            return "Invalid signature"
        used = False
        for hdr in self["mbr"].headers:
            if hdr["bootable"].value not in (0x00, 0x80):
                return "Wrong boot flag"
            used |= hdr.isUsed()
        return used or "No partition found"

########NEW FILE########
__FILENAME__ = ntfs
"""
New Technology File System (NTFS) file system parser.

Sources:
- The NTFS documentation
  http://www.linux-ntfs.org/
- NTFS-3G driver
  http://www.ntfs-3g.org/

Creation date: 3rd january 2007
Author: Victor Stinner
"""

SECTOR_SIZE = 512

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, Enum,
    UInt8, UInt16, UInt32, UInt64, TimestampWin64,
    String, Bytes, Bit,
    NullBits, NullBytes, PaddingBytes, RawBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler
from lib.hachoir_core.tools import humanFilesize, createDict
from lib.hachoir_parser.common.msdos import MSDOSFileAttr32

class BiosParameterBlock(FieldSet):
    """
    BIOS parameter block (bpb) structure
    """
    static_size = 25 * 8
    MEDIA_TYPE = {0xf8: "Hard disk"}

    def createFields(self):
        yield UInt16(self, "bytes_per_sector", "Size of a sector in bytes")
        yield UInt8(self, "sectors_per_cluster", "Size of a cluster in sectors")
        yield NullBytes(self, "reserved_sectors", 2)
        yield NullBytes(self, "fats", 1)
        yield NullBytes(self, "root_entries", 2)
        yield NullBytes(self, "sectors", 2)
        yield Enum(UInt8(self, "media_type"), self.MEDIA_TYPE)
        yield NullBytes(self, "sectors_per_fat", 2)
        yield UInt16(self, "sectors_per_track")
        yield UInt16(self, "heads")
        yield UInt32(self, "hidden_sectors")
        yield NullBytes(self, "large_sectors", 4)

    def validate(self):
        if self["bytes_per_sector"].value not in (256, 512, 1024, 2048, 4096):
            return "Invalid sector size (%u bytes)" % \
                self["bytes_per_sector"].value
        if self["sectors_per_cluster"].value not in (1, 2, 4, 8, 16, 32, 64, 128):
            return "Invalid cluster size (%u sectors)" % \
                self["sectors_per_cluster"].value
        return ""

class MasterBootRecord(FieldSet):
    static_size = 512*8
    def createFields(self):
        yield Bytes(self, "jump", 3, "Intel x86 jump instruction")
        yield String(self, "name", 8)
        yield BiosParameterBlock(self, "bios", "BIOS parameters")

        yield textHandler(UInt8(self, "physical_drive", "(0x80)"), hexadecimal)
        yield NullBytes(self, "current_head", 1)
        yield textHandler(UInt8(self, "ext_boot_sig", "Extended boot signature (0x80)"), hexadecimal)
        yield NullBytes(self, "unused", 1)

        yield UInt64(self, "nb_sectors")
        yield UInt64(self, "mft_cluster", "Cluster location of MFT data")
        yield UInt64(self, "mftmirr_cluster", "Cluster location of copy of MFT")
        yield UInt8(self, "cluster_per_mft", "MFT record size in clusters")
        yield NullBytes(self, "reserved[]", 3)
        yield UInt8(self, "cluster_per_index", "Index block size in clusters")
        yield NullBytes(self, "reserved[]", 3)
        yield textHandler(UInt64(self, "serial_number"), hexadecimal)
        yield textHandler(UInt32(self, "checksum", "Boot sector checksum"), hexadecimal)
        yield Bytes(self, "boot_code", 426)
        yield Bytes(self, "mbr_magic", 2, r"Master boot record magic number (\x55\xAA)")

    def createDescription(self):
        size = self["nb_sectors"].value * self["bios/bytes_per_sector"].value
        return "NTFS Master Boot Record (%s)" % humanFilesize(size)

class MFT_Flags(FieldSet):
    static_size = 16
    def createFields(self):
        yield Bit(self, "in_use")
        yield Bit(self, "is_directory")
        yield NullBits(self, "padding", 14)

class Attribute(FieldSet):
    # --- Common code ---
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["size"].value * 8
        type = self["type"].value
        if type in self.ATTR_INFO:
            self._name = self.ATTR_INFO[type][0]
            self._parser = self.ATTR_INFO[type][2]

    def createFields(self):
        yield Enum(textHandler(UInt32(self, "type"), hexadecimal), self.ATTR_NAME)
        yield UInt32(self, "size")
        yield UInt8(self, "non_resident", "Non-resident flag")
        yield UInt8(self, "name_length", "Name length in bytes")
        yield UInt16(self, "name_offset", "Name offset")
        yield UInt16(self, "flags")
        yield textHandler(UInt16(self, "attribute_id"), hexadecimal)
        yield UInt32(self, "length_attr", "Length of the Attribute")
        yield UInt16(self, "offset_attr", "Offset of the Attribute")
        yield UInt8(self, "indexed_flag")
        yield NullBytes(self, "padding", 1)
        if self._parser:
            for field in self._parser(self):
                yield field
        else:
            size = self["length_attr"].value
            if size:
                yield RawBytes(self, "data", size)
        size = (self.size - self.current_size) // 8
        if size:
            yield PaddingBytes(self, "end_padding", size)

    def createDescription(self):
        return "Attribute %s" % self["type"].display
    FILENAME_NAMESPACE = {
        0: "POSIX",
        1: "Win32",
        2: "DOS",
        3: "Win32 & DOS",
    }

    # --- Parser specific to a type ---
    def parseStandardInfo(self):
        yield TimestampWin64(self, "ctime", "File Creation")
        yield TimestampWin64(self, "atime", "File Altered")
        yield TimestampWin64(self, "mtime", "MFT Changed")
        yield TimestampWin64(self, "rtime", "File Read")
        yield MSDOSFileAttr32(self, "file_attr", "DOS File Permissions")
        yield UInt32(self, "max_version", "Maximum Number of Versions")
        yield UInt32(self, "version", "Version Number")
        yield UInt32(self, "class_id")
        yield UInt32(self, "owner_id")
        yield UInt32(self, "security_id")
        yield filesizeHandler(UInt64(self, "quota_charged", "Quota Charged"))
        yield UInt64(self, "usn", "Update Sequence Number (USN)")

    def parseFilename(self):
        yield UInt64(self, "ref", "File reference to the parent directory")
        yield TimestampWin64(self, "ctime", "File Creation")
        yield TimestampWin64(self, "atime", "File Altered")
        yield TimestampWin64(self, "mtime", "MFT Changed")
        yield TimestampWin64(self, "rtime", "File Read")
        yield filesizeHandler(UInt64(self, "alloc_size", "Allocated size of the file"))
        yield filesizeHandler(UInt64(self, "real_size", "Real size of the file"))
        yield UInt32(self, "file_flags")
        yield UInt32(self, "file_flags2", "Used by EAs and Reparse")
        yield UInt8(self, "filename_length", "Filename length in characters")
        yield Enum(UInt8(self, "filename_namespace"), self.FILENAME_NAMESPACE)
        size = self["filename_length"].value * 2
        if size:
            yield String(self, "filename", size, charset="UTF-16-LE")

    def parseData(self):
        size = (self.size - self.current_size) // 8
        if size:
            yield Bytes(self, "data", size)

    def parseBitmap(self):
        size = (self.size - self.current_size)
        for index in xrange(size):
            yield Bit(self, "bit[]")

    # --- Type information ---
    ATTR_INFO = {
         0x10: ('standard_info', 'STANDARD_INFORMATION ', parseStandardInfo),
         0x20: ('attr_list', 'ATTRIBUTE_LIST ', None),
         0x30: ('filename', 'FILE_NAME ', parseFilename),
         0x40: ('vol_ver', 'VOLUME_VERSION', None),
         0x40: ('obj_id', 'OBJECT_ID ', None),
         0x50: ('security', 'SECURITY_DESCRIPTOR ', None),
         0x60: ('vol_name', 'VOLUME_NAME ', None),
         0x70: ('vol_info', 'VOLUME_INFORMATION ', None),
         0x80: ('data', 'DATA ', parseData),
         0x90: ('index_root', 'INDEX_ROOT ', None),
         0xA0: ('index_alloc', 'INDEX_ALLOCATION ', None),
         0xB0: ('bitmap', 'BITMAP ', parseBitmap),
         0xC0: ('sym_link', 'SYMBOLIC_LINK', None),
         0xC0: ('reparse', 'REPARSE_POINT ', None),
         0xD0: ('ea_info', 'EA_INFORMATION ', None),
         0xE0: ('ea', 'EA ', None),
         0xF0: ('prop_set', 'PROPERTY_SET', None),
        0x100: ('log_util', 'LOGGED_UTILITY_STREAM', None),
    }
    ATTR_NAME = createDict(ATTR_INFO, 1)

class File(FieldSet):
#    static_size = 48*8
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["bytes_allocated"].value * 8

    def createFields(self):
        yield Bytes(self, "signature", 4, "Usually the magic is 'FILE'")
        yield UInt16(self, "usa_ofs", "Update Sequence Array offset")
        yield UInt16(self, "usa_count", "Update Sequence Array count")
        yield UInt64(self, "lsn", "$LogFile sequence number for this record")
        yield UInt16(self, "sequence_number", "Number of times this mft record has been reused")
        yield UInt16(self, "link_count", "Number of hard links")
        yield UInt16(self, "attrs_offset", "Byte offset to the first attribute")
        yield MFT_Flags(self, "flags")
        yield UInt32(self, "bytes_in_use", "Number of bytes used in this record")
        yield UInt32(self, "bytes_allocated", "Number of bytes allocated for this record")
        yield UInt64(self, "base_mft_record")
        yield UInt16(self, "next_attr_instance")

        # The below fields are specific to NTFS 3.1+ (Windows XP and above)
        yield NullBytes(self, "reserved", 2)
        yield UInt32(self, "mft_record_number", "Number of this mft record")

        padding = self.seekByte(self["attrs_offset"].value, relative=True)
        if padding:
            yield padding

        while not self.eof:
            addr = self.absolute_address + self.current_size
            if self.stream.readBytes(addr, 4) == "\xFF\xFF\xFF\xFF":
                yield Bytes(self, "attr_end_marker", 8)
                break
            yield Attribute(self, "attr[]")

        size = self["bytes_in_use"].value - self.current_size//8
        if size:
            yield RawBytes(self, "end_rawdata", size)

        size = (self.size - self.current_size) // 8
        if size:
            yield RawBytes(self, "end_padding", size, "Unused but allocated bytes")

    def createDescription(self):
        text = "File"
        if "filename/filename" in self:
            text += ' "%s"' % self["filename/filename"].value
        if "filename/real_size" in self:
            text += ' (%s)' % self["filename/real_size"].display
        if "standard_info/file_attr" in self:
            text += ', %s' % self["standard_info/file_attr"].display
        return text

class NTFS(Parser):
    MAGIC = "\xEB\x52\x90NTFS    "
    PARSER_TAGS = {
        "id": "ntfs",
        "category": "file_system",
        "description": "NTFS file system",
        "min_size": 1024*8,
        "magic": ((MAGIC, 0),),
    }
    endian = LITTLE_ENDIAN
    _cluster_size = None

    def validate(self):
        if self.stream.readBytes(0, len(self.MAGIC)) != self.MAGIC:
            return "Invalid magic string"
        err = self["mbr/bios"].validate()
        if err:
            return err
        return True

    def createFields(self):
        yield MasterBootRecord(self, "mbr")

        bios = self["mbr/bios"]
        cluster_size = bios["sectors_per_cluster"].value * bios["bytes_per_sector"].value
        offset = self["mbr/mft_cluster"].value * cluster_size
        padding = self.seekByte(offset, relative=False)
        if padding:
            yield padding
        for index in xrange(1000):
            yield File(self, "file[]")

        size = (self.size - self.current_size) // 8
        if size:
            yield RawBytes(self, "end", size)


########NEW FILE########
__FILENAME__ = reiser_fs
"""
ReiserFS file system version 3 parser (version 1, 2 and 4 are not supported).

Author: Frederic Weisbecker
Creation date: 8 december 2006

Sources:
 - http://p-nand-q.com/download/rfstool/reiserfs_docs.html
 - http://homes.cerias.purdue.edu/~florian/reiser/reiserfs.php
 - file://usr/src/linux-2.6.16.19/include/linux/reiserfs_fs.h

NOTES:

The most part of the description of the structures, their fields and their
comments decribed here comes from the file include/linux/reiserfs_fs.h
- written by Hans reiser - located in the Linux kernel 2.6.16.19 and from
the Reiserfs explanations in
http://p-nand-q.com/download/rfstool/reiserfs_docs.html written by Gerson
Kurz.
"""


from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, Enum,
    UInt16, UInt32, String, RawBytes, NullBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN

class Journal_params(FieldSet):
    static_size = 32*8

    def createFields(self):
        yield UInt32(self, "1st_block", "Journal 1st block number")
        yield UInt32(self, "dev", "Journal device number")
        yield UInt32(self, "size", "Size of the journal")
        yield UInt32(self, "trans_max", "Max number of blocks in a transaction")
        #TODO: Must be explained: it was sb_journal_block_count
        yield UInt32(self, "magic", "Random value made on fs creation.")
        yield UInt32(self, "max_batch", "Max number of blocks to batch into a trans")
        yield UInt32(self, "max_commit_age", "In seconds, how old can an async commit be")
        yield UInt32(self, "max_trans_age", "In seconds, how old can a transaction be")


    def createDescription(self):
        return "Parameters of the journal"

class SuperBlock(FieldSet):
    static_size = 204*8

    UMOUNT_STATE = { 1: "unmounted", 2: "not unmounted" }
    HASH_FUNCTIONS = {
        0: "UNSET_HASH",
        1: "TEA_HASH",
        2: "YURA_HASH",
        3: "R5_HASH"
    }

    def createFields(self):
        #TODO: This structure is normally divided in two parts:
        # _reiserfs_super_block_v1
        # _reiserfs_super_block
        # It will be divided later to easily support older version of the first part
        yield UInt32(self, "block_count", "Number of blocks")
        yield UInt32(self, "free_blocks", "Number of free blocks")
        yield UInt32(self, "root_block", "Root block number")
        yield Journal_params(self, "Journal parameters")
        yield UInt16(self, "blocksize", "Size of a block")
        yield UInt16(self, "oid_maxsize", "Max size of object id array")
        yield UInt16(self, "oid_cursize", "Current size of object id array")
        yield Enum(UInt16(self, "umount_state", "Filesystem umounted or not"), self.UMOUNT_STATE)
        yield String(self, "magic", 10, "Magic string", strip="\0")
        #TODO: change the type of s_fs_state in Enum to have more details about this fsck state
        yield UInt16(self, "fs_state", "Rebuilding phase of fsck ")
        yield Enum(UInt32(self, "hash_function", "Hash function to sort names in a directory"), self.HASH_FUNCTIONS)
        yield UInt16(self, "tree_height", "Height of disk tree")
        yield UInt16(self, "bmap_nr", "Amount of bitmap blocks needed to address each block of file system")
        #TODO: find a good description for this field
        yield UInt16(self, "version", "Field only reliable on filesystem with non-standard journal")
        yield UInt16(self, "reserved_for_journal", "Size in blocks of journal area on main device")
        #TODO: same as above
        yield UInt32(self, "inode_generation", "No description")
        #TODO: same as above and should be an enum field
        yield UInt32(self, "flags", "No description")
        #TODO: Create a special Type to format this id
        yield RawBytes(self, "uuid", 16, "Filesystem unique identifier")
        yield String(self, "label", 16, "Filesystem volume label", strip="\0")
        yield NullBytes(self, "unused", 88)

    def createDescription(self):
        return "Superblock: ReiserFs Filesystem"

class REISER_FS(Parser):
    PARSER_TAGS = {
        "id": "reiserfs",
        "category": "file_system",
        # 130 blocks before the journal +
        # Minimal size of journal (513 blocks) +
        # 1 block for the rest
        # And The Minimal size of a block is 512 bytes
        "min_size": (130+513+1) * (512*8),
        "description": "ReiserFS file system"
    }
    endian = LITTLE_ENDIAN

    # Offsets (in bytes) of important information
    SUPERBLOCK_OFFSET = 64*1024
    MAGIC_OFFSET = SUPERBLOCK_OFFSET + 52

    def validate(self):
        # Let's look at the magic field in the superblock
        magic = self.stream.readBytes(self.MAGIC_OFFSET*8, 9).rstrip("\0")
        if magic == "ReIsEr3Fs":
            return True
        if magic in ("ReIsEr2Fs", "ReIsErFs"):
            return "Unsupported version of ReiserFs"
        return "Invalid magic string"

    def createFields(self):
        yield NullBytes(self, "padding[]", self.SUPERBLOCK_OFFSET)
        yield SuperBlock(self, "superblock")


########NEW FILE########
__FILENAME__ = blp
"""
Blizzard BLP Image File Parser

Author: Robert Xiao
Creation date: July 10 2007

- BLP1 File Format
  http://magos.thejefffiles.com/War3ModelEditor/MagosBlpFormat.txt
- BLP2 File Format (Wikipedia)
  http://en.wikipedia.org/wiki/.BLP
- S3TC (DXT1, 3, 5) Formats
  http://en.wikipedia.org/wiki/S3_Texture_Compression
"""

from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.field import String, UInt32, UInt8, Enum, FieldSet, RawBytes, GenericVector, Bit, Bits
from lib.hachoir_parser.parser import Parser
from lib.hachoir_parser.image.common import PaletteRGBA
from lib.hachoir_core.tools import alignValue

class PaletteIndex(UInt8):
    def createDescription(self):
        return "Palette index %i (%s)" % (self.value, self["/palette/color[%i]" % self.value].description)

class Generic2DArray(FieldSet):
    def __init__(self, parent, name, width, height, item_class, row_name="row", item_name="item", *args, **kwargs):
        FieldSet.__init__(self, parent, name, *args, **kwargs)
        self.width = width
        self.height = height
        self.item_class = item_class
        self.row_name = row_name
        self.item_name = item_name

    def createFields(self):
        for i in xrange(self.height):
            yield GenericVector(self, self.row_name+"[]", self.width, self.item_class, self.item_name)

class BLP1File(Parser):
    MAGIC = "BLP1"
    PARSER_TAGS = {
        "id": "blp1",
        "category": "game",
        "file_ext": ("blp",),
        "mime": (u"application/x-blp",), # TODO: real mime type???
        "magic": ((MAGIC, 0),),
        "min_size": 7*32,   # 7 DWORDs start, incl. magic
        "description": "Blizzard Image Format, version 1",
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 4) != "BLP1":
            return "Invalid magic"
        return True

    def createFields(self):
        yield String(self, "magic", 4, "Signature (BLP1)")
        yield Enum(UInt32(self, "compression"), {
            0:"JPEG Compression",
            1:"Uncompressed"})
        yield UInt32(self, "flags")
        yield UInt32(self, "width")
        yield UInt32(self, "height")
        yield Enum(UInt32(self, "type"), {
            3:"Uncompressed Index List + Alpha List",
            4:"Uncompressed Index List + Alpha List",
            5:"Uncompressed Index List"})
        yield UInt32(self, "subtype")
        for i in xrange(16):
            yield UInt32(self, "mipmap_offset[]")
        for i in xrange(16):
            yield UInt32(self, "mipmap_size[]")

        compression = self["compression"].value
        image_type = self["type"].value
        width = self["width"].value
        height = self["height"].value

        if compression == 0: # JPEG Compression
            yield UInt32(self, "jpeg_header_len")
            yield RawBytes(self, "jpeg_header", self["jpeg_header_len"].value, "Shared JPEG Header")
        else:
            yield PaletteRGBA(self, "palette", 256)

        offsets = self.array("mipmap_offset")
        sizes = self.array("mipmap_size")
        for i in xrange(16):
            if not offsets[i].value or not sizes[i].value:
                continue
            padding = self.seekByte(offsets[i].value)
            if padding:
                yield padding
            if compression == 0:
                yield RawBytes(self, "mipmap[%i]" % i, sizes[i].value, "JPEG data, append to header to recover complete image")
            elif compression == 1:
                yield Generic2DArray(self, "mipmap_indexes[%i]" % i, width, height, PaletteIndex, "row", "index", "Indexes into the palette")
                if image_type in (3, 4):
                    yield Generic2DArray(self, "mipmap_alphas[%i]" % i, width, height, UInt8, "row", "alpha", "Alpha values")
            width /= 2
            height /= 2

def interp_avg(data_low, data_high, n):
    """Interpolated averages. For example,

    >>> list(interp_avg(1, 10, 3))
    [4, 7]
    """
    if isinstance(data_low, (int, long)):
        for i in range(1, n):
            yield (data_low * (n-i) + data_high * i) / n
    else: # iterable
        pairs = zip(data_low, data_high)
        pair_iters = [interp_avg(x, y, n) for x, y in pairs]
        for i in range(1, n):
            yield [iter.next() for iter in pair_iters]

def color_name(data, bits):
    """Color names in #RRGGBB format, given the number of bits for each component."""
    ret = ["#"]
    for i in range(3):
        ret.append("%02X" % (data[i] << (8-bits[i])))
    return ''.join(ret)

class DXT1(FieldSet):
    static_size = 64
    def __init__(self, parent, name, dxt2_mode=False, *args, **kwargs):
        """with dxt2_mode on, this field will always use the four color model"""
        FieldSet.__init__(self, parent, name, *args, **kwargs)
        self.dxt2_mode = dxt2_mode
    def createFields(self):
        values = [[], []]
        for i in (0, 1):
            yield Bits(self, "blue[]", 5)
            yield Bits(self, "green[]", 6)
            yield Bits(self, "red[]", 5)
            values[i] = [self["red[%i]" % i].value,
                         self["green[%i]" % i].value,
                         self["blue[%i]" % i].value]
        if values[0] > values[1] or self.dxt2_mode:
            values += interp_avg(values[0], values[1], 3)
        else:
            values += interp_avg(values[0], values[1], 2)
            values.append(None) # transparent
        for i in xrange(16):
            pixel = Bits(self, "pixel[%i][%i]" % divmod(i, 4), 2)
            color = values[pixel.value]
            if color is None:
                pixel._description = "Transparent"
            else:
                pixel._description = "RGB color: %s" % color_name(color, [5, 6, 5])
            yield pixel

class DXT3Alpha(FieldSet):
    static_size = 64
    def createFields(self):
        for i in xrange(16):
            yield Bits(self, "alpha[%i][%i]" % divmod(i, 4), 4)

class DXT3(FieldSet):
    static_size = 128
    def createFields(self):
        yield DXT3Alpha(self, "alpha", "Alpha Channel Data")
        yield DXT1(self, "color", True, "Color Channel Data")

class DXT5Alpha(FieldSet):
    static_size = 64
    def createFields(self):
        values = []
        yield UInt8(self, "alpha_val[0]", "First alpha value")
        values.append(self["alpha_val[0]"].value)
        yield UInt8(self, "alpha_val[1]", "Second alpha value")
        values.append(self["alpha_val[1]"].value)
        if values[0] > values[1]:
            values += interp_avg(values[0], values[1], 7)
        else:
            values += interp_avg(values[0], values[1], 5)
            values += [0, 255]
        for i in xrange(16):
            pixel = Bits(self, "alpha[%i][%i]" % divmod(i, 4), 3)
            alpha = values[pixel.value]
            pixel._description = "Alpha value: %i" % alpha
            yield pixel

class DXT5(FieldSet):
    static_size = 128
    def createFields(self):
        yield DXT5Alpha(self, "alpha", "Alpha Channel Data")
        yield DXT1(self, "color", True, "Color Channel Data")

class BLP2File(Parser):
    MAGIC = "BLP2"
    PARSER_TAGS = {
        "id": "blp2",
        "category": "game",
        "file_ext": ("blp",),
        "mime": (u"application/x-blp",),
        "magic": ((MAGIC, 0),),
        "min_size": 5*32,   # 5 DWORDs start, incl. magic
        "description": "Blizzard Image Format, version 2",
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 4) != "BLP2":
            return "Invalid magic"
        return True

    def createFields(self):
        yield String(self, "magic", 4, "Signature (BLP2)")
        yield Enum(UInt32(self, "compression", "Compression type"), {
            0:"JPEG Compressed",
            1:"Uncompressed or DXT/S3TC compressed"})
        yield Enum(UInt8(self, "encoding", "Encoding type"), {
            1:"Raw",
            2:"DXT/S3TC Texture Compression (a.k.a. DirectX)"})
        yield UInt8(self, "alpha_depth", "Alpha channel depth, in bits (0 = no alpha)")
        yield Enum(UInt8(self, "alpha_encoding", "Encoding used for alpha channel"), {
            0:"DXT1 alpha (0 or 1 bit alpha)",
            1:"DXT3 alpha (4 bit alpha)",
            7:"DXT5 alpha (8 bit interpolated alpha)"})
        yield Enum(UInt8(self, "has_mips", "Are mip levels present?"), {
            0:"No mip levels",
            1:"Mip levels present; number of levels determined by image size"})
        yield UInt32(self, "width", "Base image width")
        yield UInt32(self, "height", "Base image height")
        for i in xrange(16):
            yield UInt32(self, "mipmap_offset[]")
        for i in xrange(16):
            yield UInt32(self, "mipmap_size[]")
        yield PaletteRGBA(self, "palette", 256)

        compression = self["compression"].value
        encoding = self["encoding"].value
        alpha_depth = self["alpha_depth"].value
        alpha_encoding = self["alpha_encoding"].value
        width = self["width"].value
        height = self["height"].value

        if compression == 0: # JPEG Compression
            yield UInt32(self, "jpeg_header_len")
            yield RawBytes(self, "jpeg_header", self["jpeg_header_len"].value, "Shared JPEG Header")

        offsets = self.array("mipmap_offset")
        sizes = self.array("mipmap_size")
        for i in xrange(16):
            if not offsets[i].value or not sizes[i].value:
                continue
            padding = self.seekByte(offsets[i].value)
            if padding:
                yield padding
            if compression == 0:
                yield RawBytes(self, "mipmap[%i]" % i, sizes[i].value, "JPEG data, append to header to recover complete image")
            elif compression == 1 and encoding == 1:
                yield Generic2DArray(self, "mipmap_indexes[%i]" % i, height, width, PaletteIndex, "row", "index", "Indexes into the palette")
                if alpha_depth == 1:
                    yield GenericVector(self, "mipmap_alphas[%i]" % i, height, width, Bit, "row", "is_opaque", "Alpha values")
                elif alpha_depth == 8:
                    yield GenericVector(self, "mipmap_alphas[%i]" % i, height, width, UInt8, "row", "alpha", "Alpha values")
            elif compression == 1 and encoding == 2:
                block_height = alignValue(height, 4) // 4
                block_width = alignValue(width, 4) // 4
                if alpha_depth in [0, 1] and alpha_encoding == 0:
                    yield Generic2DArray(self, "mipmap[%i]" % i, block_height, block_width, DXT1, "row", "block", "DXT1-compressed image blocks")
                elif alpha_depth == 8 and alpha_encoding == 1:
                    yield Generic2DArray(self, "mipmap[%i]" % i, block_height, block_width, DXT3, "row", "block", "DXT3-compressed image blocks")
                elif alpha_depth == 8 and alpha_encoding == 7:
                    yield Generic2DArray(self, "mipmap[%i]" % i, block_height, block_width, DXT5, "row", "block", "DXT5-compressed image blocks")
            width /= 2
            height /= 2

########NEW FILE########
__FILENAME__ = laf
# -*- coding: utf-8 -*-

"""
LucasArts Font parser.

Author: Cyril Zorin
Creation date: 1 January 2007
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
        UInt8, UInt16, UInt32, GenericVector)
from lib.hachoir_core.endian import LITTLE_ENDIAN

class CharData(FieldSet):
  def __init__(self, chars, *args):
    FieldSet.__init__(self, *args)
    self.chars = chars

  def createFields(self):
    for char in self.chars:
      yield CharBitmap(char, self, "char_bitmap[]")

class CharBitmap(FieldSet):
  def __init__(self, char, *args):
    FieldSet.__init__(self, *args)
    self.char = char

  def createFields(self):
    width = self.char["width_pixels"].value
    for line in xrange(self.char["height_pixels"].value):
      yield GenericVector(self, "line[]", width,
                          UInt8, "pixel")

class CharInfo(FieldSet):
  static_size = 16 * 8

  def createFields(self):
    yield UInt32(self, "data_offset")
    yield UInt8(self, "logical_width")
    yield UInt8(self, "unknown[]")
    yield UInt8(self, "unknown[]")
    yield UInt8(self, "unknown[]")
    yield UInt32(self, "width_pixels")
    yield UInt32(self, "height_pixels")

class LafFile(Parser):
  PARSER_TAGS = {
    "id": "lucasarts_font",
    "category": "game",
    "file_ext" : ("laf",),
    "min_size" : 32*8,
    "description" : "LucasArts Font"
    }

  endian = LITTLE_ENDIAN

  def validate(self):
    if self["num_chars"].value != 256:
        return "Invalid number of characters (%u)" % self["num_chars"].value
    if self["first_char_code"].value != 0:
        return "Invalid of code of first character code (%u)" % self["first_char_code"].value
    if self["last_char_code"].value != 255:
        return "Invalid of code of last character code (%u)" % self["last_char_code"].value
    if self["char_codes/char[0]"].value != 0:
        return "Invalid character code #0 (%u)" % self["char_codes/char[0]"].value
    if self["chars/char[0]/data_offset"].value != 0:
        return "Invalid character #0 offset"
    return True

  def createFields(self):
    yield UInt32(self, "num_chars")
    yield UInt32(self, "raw_font_data_size")
    yield UInt32(self, "max_char_width")
    yield UInt32(self, "min_char_width")
    yield UInt32(self, "unknown[]", 4)
    yield UInt32(self, "unknown[]", 4)
    yield UInt32(self, "first_char_code")
    yield UInt32(self, "last_char_code")

    yield GenericVector(self, "char_codes", self["num_chars"].value,
            UInt16, "char")

    yield GenericVector(self, "chars", self["num_chars"].value,
            CharInfo, "char")

    # character data. we make an effort to provide
    # something more meaningful than "RawBytes:
    # character bitmap data"
    yield CharData(self["chars"], self, "char_data")

    # read to the end
    if self.current_size < self._size:
      yield self.seekBit(self._size, "unknown[]")

########NEW FILE########
__FILENAME__ = spider_man_video
"""
Parser for an obscure FMV file format: bin files from the game
"The Amazing Spider-Man vs. The Kingpin" (Sega CD)

Author: Mike Melanson
Creation date: 2006-09-30
File samples: http://samples.mplayerhq.hu/game-formats/spiderman-segacd-bin/
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import FieldSet, UInt32, String, RawBytes
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal

class Chunk(FieldSet):
    tag_info = {
        "CONF" : ("conf[]", None, "Configuration header"),
        "AUDI" : ("audio[]", None, "Audio chunk"),
        "SYNC" : ("sync[]", None, "Start of video frame data"),
        "IVRA" : ("ivra[]", None, "Vector codebook (?)"),
        "VRAM" : ("video[]", None, "Video RAM tile pattern"),
        "CRAM" : ("color[]", None, "Color RAM (palette)"),
        "CEND" : ("video_end[]", None, "End of video data"),
        "MEND" : ("end_file", None, "End of file"),
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["length"].value * 8
        fourcc = self["fourcc"].value
        if fourcc in self.tag_info:
            self._name, self._parser, self._description = self.tag_info[fourcc]
        else:
            self._parser = None
            self._description = "Unknown chunk: fourcc %s" % self["fourcc"].display

    def createFields(self):
        yield String(self, "fourcc", 4, "FourCC", charset="ASCII")
        yield textHandler(UInt32(self, "length", "length"), hexadecimal)
        size = self["length"].value - 8
        if 0 < size:
            if self._parser:
                for field in self._parser(self, size):
                    yield field
            else:
                yield RawBytes(self, "data", size)

class SpiderManVideoFile(Parser):
    PARSER_TAGS = {
        "id": "spiderman_video",
        "category": "game",
        "file_ext": ("bin",),
        "min_size": 8*8,
        "description": "The Amazing Spider-Man vs. The Kingpin (Sega CD) FMV video"
    }

    endian = BIG_ENDIAN

    def validate(self):
        return (self.stream.readBytes(0, 4) == 'CONF')

    def createFields(self):
        while not self.eof:
            yield Chunk(self, "chunk[]")


########NEW FILE########
__FILENAME__ = zsnes
"""
ZSNES Save State Parser (v143 only currently)

Author: Jason Gorski
Creation date: 2006-09-15
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, StaticFieldSet,
    UInt8, UInt16, UInt32,
    String, PaddingBytes, Bytes, RawBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN

class ZSTHeader(StaticFieldSet):
    format = (
        (String, "zs_mesg", 26, "File header", {"charset": "ASCII"}),
        (UInt8, "zs_mesglen", "File header string len"),
        (UInt8, "zs_version", "Version minor #"),
        (UInt8, "curcyc", "cycles left in scanline"),
        (UInt16, "curypos", "current y position"),
        (UInt8, "cacheud", "update cache every ? frames"),
        (UInt8, "ccud", "current cache increment"),
        (UInt8, "intrset", "interrupt set"),
        (UInt8, "cycpl", "cycles per scanline"),
        (UInt8, "cycphb", "cycles per hblank"),
        (UInt8, "spcon", "SPC Enable (1=enabled)"),
        (UInt16, "stackand", "value to and stack to keep it from going to the wrong area"),
        (UInt16, "stackor", "value to or stack to keep it from going to the wrong area"),
    )

class ZSTcpu(StaticFieldSet):
    format = (
        (UInt16, "xat"),
        (UInt8, "xdbt"),
        (UInt8, "xpbt"),
        (UInt16, "xst"),
        (UInt16, "xdt"),
        (UInt16, "xxt"),
        (UInt16, "xyt"),
        (UInt8, "xp"),
        (UInt8, "xe"),
        (UInt16, "xpc"),
        (UInt8, "xirqb", "which bank the irqs start at"),
        (UInt8, "debugger", "Start with debugger (1: yes, 0: no)"),
        (UInt32, "Curtable" "Current table address"),
        (UInt8, "curnmi", "if in NMI (1=yes)"),
        (UInt32, "cycpbl", "percentage left of CPU/SPC to run (3.58 = 175)"),
        (UInt32, "cycpblt", "percentage of CPU/SPC to run"),
    )

class ZSTppu(FieldSet):
    static_size = 3019*8
    def createFields(self):
        yield UInt8(self, "sndrot", "rotates to use A,X or Y for sound skip")
        yield UInt8(self, "sndrot2", "rotates a random value for sound skip")
        yield UInt8(self, "INTEnab", "enables NMI(7)/VIRQ(5)/HIRQ(4)/JOY(0)")
        yield UInt8(self, "NMIEnab", "controlled in e65816 loop. Sets to 81h")
        yield UInt16(self, "VIRQLoc", "VIRQ Y location")
        yield UInt8(self, "vidbright", "screen brightness 0..15")
        yield UInt8(self, "previdbr", "previous screen brightness")
        yield UInt8(self, "forceblnk", "force blanking on/off ($80=on)")
        yield UInt32(self, "objptr", "pointer to object data in VRAM")
        yield UInt32(self, "objptrn", "pointer2 to object data in VRAM")
        yield UInt8(self, "objsize1", "1=8dot, 4=16dot, 16=32dot, 64=64dot")
        yield UInt8(self, "objsize2", "large object size")
        yield UInt8(self, "objmovs1", "number of bytes to move/paragraph")
        yield UInt16(self, "objadds1", "number of bytes to add/paragraph")
        yield UInt8(self, "objmovs2", "number of bytes to move/paragraph")
        yield UInt16(self, "objadds2", "number of bytes to add/paragraph")
        yield UInt16(self, "oamaddrt", "oam address")
        yield UInt16(self, "oamaddrs", "oam address at beginning of vblank")
        yield UInt8(self, "objhipr", "highest priority object #")
        yield UInt8(self, "bgmode", "graphics mode 0..7")
        yield UInt8(self, "bg3highst", "is 1 if background 3 has the highest priority")
        yield UInt8(self, "bgtilesz", "0=8x8, 1=16x16 bit0=bg1, bit1=bg2, etc.")
        yield UInt8(self, "mosaicon", "mosaic on, bit 0=bg1, bit1=bg2, etc.")
        yield UInt8(self, "mosaicsz", "mosaic size in pixels")
        yield UInt16(self, "bg1ptr", "pointer to background1")
        yield UInt16(self, "bg2ptr", "pointer to background2")
        yield UInt16(self, "bg3ptr", "pointer to background3")
        yield UInt16(self, "bg4ptr", "pointer to background4")
        yield UInt16(self, "bg1ptrb", "pointer to background1")
        yield UInt16(self, "bg2ptrb", "pointer to background2")
        yield UInt16(self, "bg3ptrb", "pointer to background3")
        yield UInt16(self, "bg4ptrb", "pointer to background4")
        yield UInt16(self, "bg1ptrc", "pointer to background1")
        yield UInt16(self, "bg2ptrc", "pointer to background2")
        yield UInt16(self, "bg3ptrc", "pointer to background3")
        yield UInt16(self, "bg4ptrc", "pointer to background4")
        yield UInt16(self, "bg1ptrd", "pointer to background1")
        yield UInt16(self, "bg2ptrd", "pointer to background2")
        yield UInt16(self, "bg3ptrd", "pointer to background3")
        yield UInt16(self, "bg4ptrd", "pointer to background4")
        yield UInt8(self, "bg1scsize", "bg #1 screen size (0=1x1,1=1x2,2=2x1,3=2x2)")
        yield UInt8(self, "bg2scsize", "bg #2 screen size (0=1x1,1=1x2,2=2x1,3=2x2)")
        yield UInt8(self, "bg3scsize", "bg #3 screen size (0=1x1,1=1x2,2=2x1,3=2x2)")
        yield UInt8(self, "bg4scsize", "bg #4 screen size (0=1x1,1=1x2,2=2x1,3=2x2)")
        yield UInt16(self, "bg1objptr", "pointer to tiles in background1")
        yield UInt16(self, "bg2objptr", "pointer to tiles in background2")
        yield UInt16(self, "bg3objptr", "pointer to tiles in background3")
        yield UInt16(self, "bg4objptr", "pointer to tiles in background4")
        yield UInt16(self, "bg1scrolx", "background 1 x position")
        yield UInt16(self, "bg2scrolx", "background 2 x position")
        yield UInt16(self, "bg3scrolx", "background 3 x position")
        yield UInt16(self, "bg4scrolx", "background 4 x position")
        yield UInt16(self, "bg1sx", "Temporary Variable for Debugging purposes")
        yield UInt16(self, "bg1scroly", "background 1 y position")
        yield UInt16(self, "bg2scroly", "background 2 y position")
        yield UInt16(self, "bg3scroly", "background 3 y position")
        yield UInt16(self, "bg4scroly", "background 4 y position")
        yield UInt16(self, "addrincr", "vram increment (2,64,128,256)")
        yield UInt8(self, "vramincr", "0 = increment at 2118/2138, 1 = 2119,213A")
        yield UInt8(self, "vramread", "0 = address set, 1 = already read once")
        yield UInt32(self, "vramaddr", "vram address")

        yield UInt16(self, "cgaddr", "cg (palette)")
        yield UInt8(self, "cgmod", "if cgram is modified or not")
        yield UInt16(self, "scrnon", "main & sub screen on")
        yield UInt8(self, "scrndist", "which background is disabled")
        yield UInt16(self, "resolutn", "screen resolution")
        yield UInt8(self, "multa", "multiplier A")
        yield UInt16(self, "diva", "divisor C")
        yield UInt16(self, "divres", "quotent of divc/divb")
        yield UInt16(self, "multres", "result of multa * multb/remainder of divc/divb")
        yield UInt16(self, "latchx", "latched x value")
        yield UInt16(self, "latchy", "latched y value")
        yield UInt8(self, "latchxr", "low or high byte read for x value")
        yield UInt8(self, "latchyr", "low or high byte read for y value")
        yield UInt8(self, "frskipper", "used to control frame skipping")
        yield UInt8(self, "winl1", "window 1 left position")
        yield UInt8(self, "winr1", "window 1 right position")
        yield UInt8(self, "winl2", "window 2 left position")
        yield UInt8(self, "winr2", "window 2 right position")
        yield UInt8(self, "winbg1en", "Win1 on (IN/OUT) or Win2 on (IN/OUT) on BG1")
        yield UInt8(self, "winbg2en", "Win1 on (IN/OUT) or Win2 on (IN/OUT) on BG2")
        yield UInt8(self, "winbg3en", "Win1 on (IN/OUT) or Win2 on (IN/OUT) on BG3")
        yield UInt8(self, "winbg4en", "Win1 on (IN/OUT) or Win2 on (IN/OUT) on BG4")
        yield UInt8(self, "winobjen", "Win1 on (IN/OUT) or Win2 on (IN/OUT) on sprites")
        yield UInt8(self, "wincolen", "Win1 on (IN/OUT) or Win2 on (IN/OUT) on backarea")
        yield UInt8(self, "winlogica", "Window logic type for BG1 to 4")
        yield UInt8(self, "winlogicb", "Window logic type for Sprites and Backarea")
        yield UInt8(self, "winenabm", "Window logic enable for main screen")
        yield UInt8(self, "winenabs", "Window logic enable for sub sceen")
        yield UInt8(self, "mode7set", "mode 7 settings")
        yield UInt16(self, "mode7A", "A value for Mode 7")
        yield UInt16(self, "mode7B", "B value for Mode 7")
        yield UInt16(self, "mode7C", "C value for Mode 7")
        yield UInt16(self, "mode7D", "D value for Mode 7")
        yield UInt16(self, "mode7X0", "Center X for Mode 7")
        yield UInt16(self, "mode7Y0", "Center Y for Mode 7")
        yield UInt8(self, "JoyAPos", "Old-Style Joystick Read Position for Joy 1 & 3")
        yield UInt8(self, "JoyBPos", "Old-Style Joystick Read Position for Joy 2 & 4")
        yield UInt32(self, "compmult", "Complement Multiplication for Mode 7")
        yield UInt8(self, "joyalt", "temporary joystick alternation")
        yield UInt32(self, "wramrwadr", "continuous read/write to wram address")
        yield RawBytes(self, "dmadata", 129, "dma data (written from ports 43xx)")
        yield UInt8(self, "irqon", "if IRQ has been called (80h) or not (0)")
        yield UInt8(self, "nexthdma", "HDMA data to execute once vblank ends")
        yield UInt8(self, "curhdma", "Currently executed hdma")
        yield RawBytes(self, "hdmadata", 152, "4 dword register addresses, # bytes to transfer/line, address increment (word)")
        yield UInt8(self, "hdmatype", "if first time executing hdma or not")
        yield UInt8(self, "coladdr", "red value of color to add")
        yield UInt8(self, "coladdg", "green value of color to add")
        yield UInt8(self, "coladdb", "blue value of color to add")
        yield UInt8(self, "colnull", "keep this 0 (when accessing colors by dword)")
        yield UInt8(self, "scaddset", "screen/fixed color addition settings")
        yield UInt8(self, "scaddtype", "which screen to add/sub")
        yield UInt8(self, "Voice0Disabl2", "Disable Voice 0")
        yield UInt8(self, "Voice1Disabl2", "Disable Voice 1")
        yield UInt8(self, "Voice2Disabl2", "Disable Voice 2")
        yield UInt8(self, "Voice3Disabl2", "Disable Voice 3")
        yield UInt8(self, "Voice4Disabl2", "Disable Voice 4")
        yield UInt8(self, "Voice5Disabl2", "Disable Voice 5")
        yield UInt8(self, "Voice6Disabl2", "Disable Voice 6")
        yield UInt8(self, "Voice7Disabl2", "Disable Voice 7")
        yield RawBytes(self, "oamram", 1024, "OAMRAM (544 bytes)")
        yield RawBytes(self, "cgram", 512, "CGRAM")
        yield RawBytes(self, "pcgram", 512, "Previous CGRAM")
        yield UInt8(self, "vraminctype")
        yield UInt8(self, "vramincby8on", "if increment by 8 is on")
        yield UInt8(self, "vramincby8left", "how many left")
        yield UInt8(self, "vramincby8totl", "how many in total (32,64,128)")
        yield UInt8(self, "vramincby8rowl", "how many left in that row (start at 8)")
        yield UInt16(self, "vramincby8ptri", "increment by how many when rowl = 0")
        yield UInt8(self, "nexthprior")
        yield UInt8(self, "doirqnext")
        yield UInt16(self, "vramincby8var")
        yield UInt8(self, "screstype")
        yield UInt8(self, "extlatch")
        yield UInt8(self, "cfield")
        yield UInt8(self, "interlval")
        yield UInt16(self, "HIRQLoc HIRQ X")

        # NEWer ZST format
        yield UInt8(self, "KeyOnStA")
        yield UInt8(self, "KeyOnStB")
        yield UInt8(self, "SDD1BankA")
        yield UInt8(self, "SDD1BankB")
        yield UInt8(self, "SDD1BankC")
        yield UInt8(self, "SDD1BankD")
        yield UInt8(self, "vramread2")
        yield UInt8(self, "nosprincr")
        yield UInt16(self, "poamaddrs")
        yield UInt8(self, "ioportval")
        yield UInt8(self, "iohvlatch")
        yield UInt8(self, "ppustatus")

        yield PaddingBytes(self, "tempdat", 477, "Reserved/Unused")

class ZSNESFile(Parser):
    PARSER_TAGS = {
        "id": "zsnes",
        "category": "game",
        "description": "ZSNES Save State File (only version 143)",
        "min_size": 3091*8,
        "file_ext": ("zst", "zs1", "zs2", "zs3", "zs4", "zs5", "zs6",
            "zs7", "zs8", "zs9")
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        temp = self.stream.readBytes(0,28)
        if temp[0:26] != "ZSNES Save State File V143":
            return "Wrong header"
        if ord(temp[27:28]) != 143: # extra...
            return "Wrong save version %d <> 143" % temp[27:1]
        return True

    def seek(self, offset):
        padding = self.seekByte(offset, relative=False)
        if padding is not None:
            yield padding

    def createFields(self):
        yield ZSTHeader(self, "header", "ZST header") # Offset: 0
        yield ZSTcpu(self, "cpu", "ZST cpu registers") # 41
        yield ZSTppu(self, "ppu", "ZST CPU registers") # 72
        yield RawBytes(self, "wram7E", 65536) # 3091
        yield RawBytes(self, "wram7F", 65536) # 68627
        yield RawBytes(self, "vram", 65536) # 134163

        # TODO: Interpret extra on-cart chip data found at/beyond... 199699

        # TODO: Interpret Thumbnail/Screenshot data found at 275291
        # 64*56*2(16bit colors) = 7168
        padding = self.seekByte(275291, relative=False)
        if padding is not None:
            yield padding
        yield Bytes(self, "thumbnail", 7168, "Thumbnail of playing game in some sort of raw 64x56x16-bit RGB mode?")


########NEW FILE########
__FILENAME__ = guess
"""
Parser list managment:
- createParser() find the best parser for a file.
"""

import os
from lib.hachoir_core.error import warning, info, HACHOIR_ERRORS
from lib.hachoir_parser import ValidateError, HachoirParserList
from lib.hachoir_core.stream import FileInputStream
from lib.hachoir_core.i18n import _


class QueryParser(object):
    fallback = None
    other = None

    def __init__(self, tags):
        self.validate = True
        self.use_fallback = False
        self.parser_args = None
        self.db = HachoirParserList.getInstance()
        self.parsers = set(self.db)
        parsers = []
        for tag in tags:
            if not self.parsers:
                break
            parsers += self._getByTag(tag)
            if self.fallback is None:
                self.fallback = len(parsers) == 1
        if self.parsers:
            other = len(parsers)
            parsers += list(self.parsers)
            self.other = parsers[other]
        self.parsers = parsers

    def __iter__(self):
        return iter(self.parsers)

    def translate(self, name, value):
        if name == "filename":
            filename = os.path.basename(value).split(".")
            if len(filename) <= 1:
                value = ""
            else:
                value = filename[-1].lower()
            name = "file_ext"
        return name, value

    def _getByTag(self, tag):
        if tag is None:
            self.parsers.clear()
            return []
        elif callable(tag):
            parsers = [ parser for parser in self.parsers if tag(parser) ]
            for parser in parsers:
                self.parsers.remove(parser)
        elif tag[0] == "class":
            self.validate = False
            return [ tag[1] ]
        elif tag[0] == "args":
            self.parser_args = tag[1]
            return []
        else:
            tag = self.translate(*tag)
            parsers = []
            if tag is not None:
                key = tag[0]
                byname = self.db.bytag.get(key,{})
                if tag[1] is None:
                    values = byname.itervalues()
                else:
                    values = byname.get(tag[1],()),
                if key == "id" and values:
                    self.validate = False
                for value in values:
                    for parser in value:
                        if parser in self.parsers:
                            parsers.append(parser)
                            self.parsers.remove(parser)
        return parsers

    def parse(self, stream, fallback=True):
        fb = None
        warn = warning
        for parser in self.parsers:
            try:
                parser_obj = parser(stream, validate=self.validate)
                if self.parser_args:
                    for key, value in self.parser_args.iteritems():
                        setattr(parser_obj, key, value)
                return parser_obj
            except ValidateError, err:
                res = unicode(err)
                if fallback and self.fallback:
                    fb = parser
            except HACHOIR_ERRORS, err:
                res = unicode(err)
            if warn:
                if parser == self.other:
                    warn = info
                warn(_("Skip parser '%s': %s") % (parser.__name__, res))
            fallback = False
        if self.use_fallback and fb:
            warning(_("Force use of parser '%s'") % fb.__name__)
            return fb(stream)


def guessParser(stream):
    return QueryParser(stream.tags).parse(stream)


def createParser(filename, real_filename=None, tags=None):
    """
    Create a parser from a file or returns None on error.

    Options:
    - filename (unicode): Input file name ;
    - real_filename (str|unicode): Real file name.
    """
    if not tags:
        tags = []
    stream = FileInputStream(filename, real_filename, tags=tags)
    return guessParser(stream)

########NEW FILE########
__FILENAME__ = bmp
"""
Microsoft Bitmap picture parser.
- file extension: ".bmp"

Author: Victor Stinner
Creation: 16 december 2005
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt8, UInt16, UInt32, Bits,
    String, RawBytes, Enum,
    PaddingBytes, NullBytes, createPaddingField)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_parser.image.common import RGB, PaletteRGBA
from lib.hachoir_core.tools import alignValue

class Pixel4bit(Bits):
    static_size = 4
    def __init__(self, parent, name):
        Bits.__init__(self, parent, name, 4)

class ImageLine(FieldSet):
    def __init__(self, parent, name, width, pixel_class):
        FieldSet.__init__(self, parent, name)
        self._pixel = pixel_class
        self._width = width
        self._size = alignValue(self._width * self._pixel.static_size, 32)

    def createFields(self):
        for x in xrange(self._width):
            yield self._pixel(self, "pixel[]")
        size = self.size - self.current_size
        if size:
            yield createPaddingField(self, size)

class ImagePixels(FieldSet):
    def __init__(self, parent, name, width, height, pixel_class, size=None):
        FieldSet.__init__(self, parent, name, size=size)
        self._width = width
        self._height = height
        self._pixel = pixel_class

    def createFields(self):
        for y in xrange(self._height-1, -1, -1):
            yield ImageLine(self, "line[%u]" % y, self._width, self._pixel)
        size = (self.size - self.current_size) // 8
        if size:
            yield NullBytes(self, "padding", size)

class CIEXYZ(FieldSet):
    def createFields(self):
        yield UInt32(self, "x")
        yield UInt32(self, "y")
        yield UInt32(self, "z")

class BmpHeader(FieldSet):
    color_space_name = {
        1: "Business (Saturation)",
        2: "Graphics (Relative)",
        4: "Images (Perceptual)",
        8: "Absolute colormetric (Absolute)",
    }

    def getFormatVersion(self):
        if "gamma_blue" in self:
            return 4
        if "important_color" in self:
            return 3
        return 2

    def createFields(self):
        # Version 2 (12 bytes)
        yield UInt32(self, "header_size", "Header size")
        yield UInt32(self, "width", "Width (pixels)")
        yield UInt32(self, "height", "Height (pixels)")
        yield UInt16(self, "nb_plan", "Number of plan (=1)")
        yield UInt16(self, "bpp", "Bits per pixel") # may be zero for PNG/JPEG picture

        # Version 3 (40 bytes)
        if self["header_size"].value < 40:
            return
        yield Enum(UInt32(self, "compression", "Compression method"), BmpFile.COMPRESSION_NAME)
        yield UInt32(self, "image_size", "Image size (bytes)")
        yield UInt32(self, "horizontal_dpi", "Horizontal DPI")
        yield UInt32(self, "vertical_dpi", "Vertical DPI")
        yield UInt32(self, "used_colors", "Number of color used")
        yield UInt32(self, "important_color", "Number of import colors")

        # Version 4 (108 bytes)
        if self["header_size"].value < 108:
            return
        yield textHandler(UInt32(self, "red_mask"), hexadecimal)
        yield textHandler(UInt32(self, "green_mask"), hexadecimal)
        yield textHandler(UInt32(self, "blue_mask"), hexadecimal)
        yield textHandler(UInt32(self, "alpha_mask"), hexadecimal)
        yield Enum(UInt32(self, "color_space"), self.color_space_name)
        yield CIEXYZ(self, "red_primary")
        yield CIEXYZ(self, "green_primary")
        yield CIEXYZ(self, "blue_primary")
        yield UInt32(self, "gamma_red")
        yield UInt32(self, "gamma_green")
        yield UInt32(self, "gamma_blue")

def parseImageData(parent, name, size, header):
    if ("compression" not in header) or (header["compression"].value in (0, 3)):
        width = header["width"].value
        height = header["height"].value
        bpp = header["bpp"].value
        if bpp == 32:
            cls = UInt32
        elif bpp == 24:
            cls = RGB
        elif bpp == 8:
            cls = UInt8
        elif bpp == 4:
            cls = Pixel4bit
        else:
            cls = None
        if cls:
            return ImagePixels(parent, name, width, height, cls, size=size*8)
    return RawBytes(parent, name, size)

class BmpFile(Parser):
    PARSER_TAGS = {
        "id": "bmp",
        "category": "image",
        "file_ext": ("bmp",),
        "mime": (u"image/x-ms-bmp", u"image/x-bmp"),
        "min_size": 30*8,
#        "magic": (("BM", 0),),
        "magic_regex": ((
            # "BM", <filesize>, <reserved>, header_size=(12|40|108)
            "BM.{4}.{8}[\x0C\x28\x6C]\0{3}",
        0),),
        "description": "Microsoft bitmap (BMP) picture"
    }
    endian = LITTLE_ENDIAN

    COMPRESSION_NAME = {
        0: u"Uncompressed",
        1: u"RLE 8-bit",
        2: u"RLE 4-bit",
        3: u"Bitfields",
        4: u"JPEG",
        5: u"PNG",
    }

    def validate(self):
        if self.stream.readBytes(0, 2) != 'BM':
            return "Wrong file signature"
        if self["header/header_size"].value not in (12, 40, 108):
            return "Unknown header size (%s)" % self["header_size"].value
        if self["header/nb_plan"].value != 1:
            return "Invalid number of planes"
        return True

    def createFields(self):
        yield String(self, "signature", 2, "Header (\"BM\")", charset="ASCII")
        yield UInt32(self, "file_size", "File size (bytes)")
        yield PaddingBytes(self, "reserved", 4, "Reserved")
        yield UInt32(self, "data_start", "Data start position")
        yield BmpHeader(self, "header")

        # Compute number of color
        header = self["header"]
        bpp = header["bpp"].value
        if 0 < bpp <= 8:
            if "used_colors" in header and header["used_colors"].value:
                nb_color = header["used_colors"].value
            else:
                nb_color = (1 << bpp)
        else:
            nb_color = 0

        # Color palette (if any)
        if nb_color:
            yield PaletteRGBA(self, "palette", nb_color)

        # Seek to data start
        field = self.seekByte(self["data_start"].value)
        if field:
            yield field

        # Image pixels
        size = min(self["file_size"].value-self["data_start"].value, (self.size - self.current_size)//8)
        yield parseImageData(self, "pixels", size, header)

    def createDescription(self):
        return u"Microsoft Bitmap version %s" % self["header"].getFormatVersion()

    def createContentSize(self):
        return self["file_size"].value * 8


########NEW FILE########
__FILENAME__ = common
from lib.hachoir_core.field import FieldSet, UserVector, UInt8

class RGB(FieldSet):
    color_name = {
        (  0,   0,   0): "Black",
        (255,   0,   0): "Red",
        (  0, 255,   0): "Green",
        (  0,   0, 255): "Blue",
        (255, 255, 255): "White",
    }
    static_size = 24

    def createFields(self):
        yield UInt8(self, "red", "Red")
        yield UInt8(self, "green", "Green")
        yield UInt8(self, "blue", "Blue")

    def createDescription(self):
        rgb = self["red"].value, self["green"].value, self["blue"].value
        name = self.color_name.get(rgb)
        if not name:
            name = "#%02X%02X%02X" % rgb
        return "RGB color: " + name

class RGBA(RGB):
    static_size = 32

    def createFields(self):
        yield UInt8(self, "red", "Red")
        yield UInt8(self, "green", "Green")
        yield UInt8(self, "blue", "Blue")
        yield UInt8(self, "alpha", "Alpha")

    def createDescription(self):
        description = RGB.createDescription(self)
        opacity = self["alpha"].value*100/255
        return "%s (opacity: %s%%)" % (description, opacity)

class PaletteRGB(UserVector):
    item_class = RGB
    item_name = "color"
    def createDescription(self):
        return "Palette of %u RGB colors" % len(self)

class PaletteRGBA(PaletteRGB):
    item_class = RGBA
    def createDescription(self):
        return "Palette of %u RGBA colors" % len(self)


########NEW FILE########
__FILENAME__ = exif
"""
EXIF metadata parser (can be found in a JPEG picture for example)

Author: Victor Stinner
"""

from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt16, UInt32,
    Int32, Enum, String,
    Bytes, SubFile,
    NullBytes, createPaddingField)
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN, NETWORK_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.tools import createDict

MAX_COUNT = 1000

def rationalFactory(class_name, size, field_class):
    class Rational(FieldSet):
        static_size = size

        def createFields(self):
            yield field_class(self, "numerator")
            yield field_class(self, "denominator")

        def createValue(self):
            return float(self["numerator"].value) / self["denominator"].value
    cls = Rational
    cls.__name__ = class_name
    return cls

RationalInt32 = rationalFactory("RationalInt32", 64, Int32)
RationalUInt32 = rationalFactory("RationalUInt32", 64, UInt32)

class BasicIFDEntry(FieldSet):
    TYPE_BYTE = 0
    TYPE_UNDEFINED = 7
    TYPE_RATIONAL = 5
    TYPE_SIGNED_RATIONAL = 10
    TYPE_INFO = {
         1: (UInt8, "BYTE (8 bits)"),
         2: (String, "ASCII (8 bits)"),
         3: (UInt16, "SHORT (16 bits)"),
         4: (UInt32, "LONG (32 bits)"),
         5: (RationalUInt32, "RATIONAL (2x LONG, 64 bits)"),
         7: (Bytes, "UNDEFINED (8 bits)"),
         9: (Int32, "SIGNED LONG (32 bits)"),
        10: (RationalInt32, "SRATIONAL (2x SIGNED LONGs, 64 bits)"),
    }
    ENTRY_FORMAT = createDict(TYPE_INFO, 0)
    TYPE_NAME = createDict(TYPE_INFO, 1)

    def createFields(self):
        yield Enum(textHandler(UInt16(self, "tag", "Tag"), hexadecimal), self.TAG_NAME)
        yield Enum(textHandler(UInt16(self, "type", "Type"), hexadecimal), self.TYPE_NAME)
        yield UInt32(self, "count", "Count")
        if self["type"].value not in (self.TYPE_BYTE, self.TYPE_UNDEFINED) \
        and  MAX_COUNT < self["count"].value:
            raise ParserError("EXIF: Invalid count value (%s)" % self["count"].value)
        value_size, array_size = self.getSizes()

        # Get offset/value
        if not value_size:
            yield NullBytes(self, "padding", 4)
        elif value_size <= 32:
            if 1 < array_size:
                name = "value[]"
            else:
                name = "value"
            kw = {}
            cls = self.value_cls
            if cls is String:
                args = (self, name, value_size/8, "Value")
                kw["strip"] = " \0"
                kw["charset"] = "ISO-8859-1"
            elif cls is Bytes:
                args = (self, name, value_size/8, "Value")
            else:
                args = (self, name, "Value")
            for index in xrange(array_size):
                yield cls(*args, **kw)

            size = array_size * value_size
            if size < 32:
                yield NullBytes(self, "padding", (32-size)//8)
        else:
            yield UInt32(self, "offset", "Value offset")

    def getSizes(self):
        """
        Returns (value_size, array_size): value_size in bits and
        array_size in number of items.
        """
        # Create format
        self.value_cls = self.ENTRY_FORMAT.get(self["type"].value, Bytes)

        # Set size
        count = self["count"].value
        if self.value_cls in (String, Bytes):
            return 8 * count, 1
        else:
            return self.value_cls.static_size * count, count

class ExifEntry(BasicIFDEntry):
    OFFSET_JPEG_SOI = 0x0201
    EXIF_IFD_POINTER = 0x8769

    TAG_WIDTH = 0xA002
    TAG_HEIGHT = 0xA003

    TAG_GPS_LATITUDE_REF = 0x0001
    TAG_GPS_LATITUDE = 0x0002
    TAG_GPS_LONGITUDE_REF = 0x0003
    TAG_GPS_LONGITUDE = 0x0004
    TAG_GPS_ALTITUDE_REF = 0x0005
    TAG_GPS_ALTITUDE = 0x0006
    TAG_GPS_TIMESTAMP = 0x0007
    TAG_GPS_DATESTAMP = 0x001d

    TAG_IMG_TITLE = 0x010e
    TAG_FILE_TIMESTAMP = 0x0132
    TAG_SOFTWARE = 0x0131
    TAG_CAMERA_MODEL = 0x0110
    TAG_CAMERA_MANUFACTURER = 0x010f
    TAG_ORIENTATION = 0x0112
    TAG_EXPOSURE = 0x829A
    TAG_FOCAL = 0x829D
    TAG_BRIGHTNESS = 0x9203
    TAG_APERTURE = 0x9205
    TAG_USER_COMMENT = 0x9286

    TAG_NAME = {
        # GPS
        0x0000: "GPS version ID",
        0x0001: "GPS latitude ref",
        0x0002: "GPS latitude",
        0x0003: "GPS longitude ref",
        0x0004: "GPS longitude",
        0x0005: "GPS altitude ref",
        0x0006: "GPS altitude",
        0x0007: "GPS timestamp",
        0x0008: "GPS satellites",
        0x0009: "GPS status",
        0x000a: "GPS measure mode",
        0x000b: "GPS DOP",
        0x000c: "GPS speed ref",
        0x000d: "GPS speed",
        0x000e: "GPS track ref",
        0x000f: "GPS track",
        0x0010: "GPS img direction ref",
        0x0011: "GPS img direction",
        0x0012: "GPS map datum",
        0x0013: "GPS dest latitude ref",
        0x0014: "GPS dest latitude",
        0x0015: "GPS dest longitude ref",
        0x0016: "GPS dest longitude",
        0x0017: "GPS dest bearing ref",
        0x0018: "GPS dest bearing",
        0x0019: "GPS dest distance ref",
        0x001a: "GPS dest distance",
        0x001b: "GPS processing method",
        0x001c: "GPS area information",
        0x001d: "GPS datestamp",
        0x001e: "GPS differential",

        0x0100: "Image width",
        0x0101: "Image height",
        0x0102: "Number of bits per component",
        0x0103: "Compression scheme",
        0x0106: "Pixel composition",
        TAG_ORIENTATION: "Orientation of image",
        0x0115: "Number of components",
        0x011C: "Image data arrangement",
        0x0212: "Subsampling ratio Y to C",
        0x0213: "Y and C positioning",
        0x011A: "Image resolution width direction",
        0x011B: "Image resolution in height direction",
        0x0128: "Unit of X and Y resolution",

        0x0111: "Image data location",
        0x0116: "Number of rows per strip",
        0x0117: "Bytes per compressed strip",
        0x0201: "Offset to JPEG SOI",
        0x0202: "Bytes of JPEG data",

        0x012D: "Transfer function",
        0x013E: "White point chromaticity",
        0x013F: "Chromaticities of primaries",
        0x0211: "Color space transformation matrix coefficients",
        0x0214: "Pair of blank and white reference values",

        TAG_FILE_TIMESTAMP: "File change date and time",
        TAG_IMG_TITLE: "Image title",
        TAG_CAMERA_MANUFACTURER: "Camera (Image input equipment) manufacturer",
        TAG_CAMERA_MODEL: "Camera (Input input equipment) model",
        TAG_SOFTWARE: "Software",
        0x013B: "File change date and time",
        0x8298: "Copyright holder",
        0x8769: "Exif IFD Pointer",

        TAG_EXPOSURE: "Exposure time",
        TAG_FOCAL: "F number",
        0x8822: "Exposure program",
        0x8824: "Spectral sensitivity",
        0x8827: "ISO speed rating",
        0x8828: "Optoelectric conversion factor OECF",
        0x9201: "Shutter speed",
        0x9202: "Aperture",
        TAG_BRIGHTNESS: "Brightness",
        0x9204: "Exposure bias",
        TAG_APERTURE: "Maximum lens aperture",
        0x9206: "Subject distance",
        0x9207: "Metering mode",
        0x9208: "Light source",
        0x9209: "Flash",
        0x920A: "Lens focal length",
        0x9214: "Subject area",
        0xA20B: "Flash energy",
        0xA20C: "Spatial frequency response",
        0xA20E: "Focal plane X resolution",
        0xA20F: "Focal plane Y resolution",
        0xA210: "Focal plane resolution unit",
        0xA214: "Subject location",
        0xA215: "Exposure index",
        0xA217: "Sensing method",
        0xA300: "File source",
        0xA301: "Scene type",
        0xA302: "CFA pattern",
        0xA401: "Custom image processing",
        0xA402: "Exposure mode",
        0xA403: "White balance",
        0xA404: "Digital zoom ratio",
        0xA405: "Focal length in 35 mm film",
        0xA406: "Scene capture type",
        0xA407: "Gain control",
        0xA408: "Contrast",

        0x9000: "Exif version",
        0xA000: "Supported Flashpix version",
        0xA001: "Color space information",
        0x9101: "Meaning of each component",
        0x9102: "Image compression mode",
        TAG_WIDTH: "Valid image width",
        TAG_HEIGHT: "Valid image height",
        0x927C: "Manufacturer notes",
        TAG_USER_COMMENT: "User comments",
        0xA004: "Related audio file",
        0x9003: "Date and time of original data generation",
        0x9004: "Date and time of digital data generation",
        0x9290: "DateTime subseconds",
        0x9291: "DateTimeOriginal subseconds",
        0x9292: "DateTimeDigitized subseconds",
        0xA420: "Unique image ID",
        0xA005: "Interoperability IFD Pointer"
    }

    def createDescription(self):
        return "Entry: %s" % self["tag"].display

def sortExifEntry(a,b):
    return int( a["offset"].value - b["offset"].value )

class ExifIFD(FieldSet):
    def seek(self, offset):
        """
        Seek to byte address relative to parent address.
        """
        padding = offset - (self.address + self.current_size)/8
        if 0 < padding:
            return createPaddingField(self, padding*8)
        else:
            return None

    def createFields(self):
        offset_diff = 6
        yield UInt16(self, "count", "Number of entries")
        entries = []
        next_chunk_offset = None
        count = self["count"].value
        if not count:
            return
        while count:
            addr = self.absolute_address + self.current_size
            next = self.stream.readBits(addr, 32, NETWORK_ENDIAN)
            if next in (0, 0xF0000000):
                break
            entry = ExifEntry(self, "entry[]")
            yield entry
            if entry["tag"].value in (ExifEntry.EXIF_IFD_POINTER, ExifEntry.OFFSET_JPEG_SOI):
                next_chunk_offset = entry["value"].value + offset_diff
            if 32 < entry.getSizes()[0]:
                entries.append(entry)
            count -= 1
        yield UInt32(self, "next", "Next IFD offset")
        try:
            entries.sort( sortExifEntry )
        except TypeError:
            raise ParserError("Unable to sort entries!")
        value_index = 0
        for entry in entries:
            padding = self.seek(entry["offset"].value + offset_diff)
            if padding is not None:
                yield padding

            value_size, array_size = entry.getSizes()
            if not array_size:
                continue
            cls = entry.value_cls
            if 1 < array_size:
                name = "value_%s[]" % entry.name
            else:
                name = "value_%s" % entry.name
            desc = "Value of \"%s\"" % entry["tag"].display
            if cls is String:
                for index in xrange(array_size):
                    yield cls(self, name, value_size/8, desc, strip=" \0", charset="ISO-8859-1")
            elif cls is Bytes:
                for index in xrange(array_size):
                    yield cls(self, name, value_size/8, desc)
            else:
                for index in xrange(array_size):
                    yield cls(self, name, desc)
            value_index += 1
        if next_chunk_offset is not None:
            padding = self.seek(next_chunk_offset)
            if padding is not None:
                yield padding

    def createDescription(self):
        return "Exif IFD (id %s)" % self["id"].value

class Exif(FieldSet):
    def createFields(self):
        # Headers
        yield String(self, "header", 6, "Header (Exif\\0\\0)", charset="ASCII")
        if self["header"].value != "Exif\0\0":
            raise ParserError("Invalid EXIF signature!")
        yield String(self, "byte_order", 2, "Byte order", charset="ASCII")
        if self["byte_order"].value not in ("II", "MM"):
            raise ParserError("Invalid endian!")
        if self["byte_order"].value == "II":
           self.endian = LITTLE_ENDIAN
        else:
           self.endian = BIG_ENDIAN
        yield UInt16(self, "version", "TIFF version number")
        yield UInt32(self, "img_dir_ofs", "Next image directory offset")
        while not self.eof:
            addr = self.absolute_address + self.current_size
            tag = self.stream.readBits(addr, 16, NETWORK_ENDIAN)
            if tag == 0xFFD8:
                size = (self._size - self.current_size) // 8
                yield SubFile(self, "thumbnail", size, "Thumbnail (JPEG file)", mime_type="image/jpeg")
                break
            elif tag == 0xFFFF:
                break
            yield ExifIFD(self, "ifd[]", "IFD")
        padding = self.seekBit(self._size)
        if padding is not None:
            yield padding



########NEW FILE########
__FILENAME__ = gif
"""
GIF picture parser.

Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    Enum, UInt8, UInt16,
    Bit, Bits, NullBytes,
    String, PascalString8, Character,
    NullBits, RawBytes)
from lib.hachoir_parser.image.common import PaletteRGB
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.tools import humanDuration
from lib.hachoir_core.text_handler import textHandler, displayHandler, hexadecimal

# Maximum image dimension (in pixel)
MAX_WIDTH = 6000
MAX_HEIGHT = MAX_WIDTH
MAX_FILE_SIZE = 100 * 1024 * 1024

class Image(FieldSet):
    def createFields(self):
        yield UInt16(self, "left", "Left")
        yield UInt16(self, "top", "Top")
        yield UInt16(self, "width", "Width")
        yield UInt16(self, "height", "Height")

        yield Bits(self, "bpp", 3, "Bits / pixel minus one")
        yield NullBits(self, "nul", 2)
        yield Bit(self, "sorted", "Sorted??")
        yield Bit(self, "interlaced", "Interlaced?")
        yield Bit(self, "has_local_map", "Use local color map?")

        if self["has_local_map"].value:
            nb_color = 1 << (1 + self["bpp"].value)
            yield PaletteRGB(self, "local_map", nb_color, "Local color map")

        yield UInt8(self, "code_size", "LZW Minimum Code Size")
        while True:
            blen = UInt8(self, "block_len[]", "Block Length")
            yield blen
            if blen.value != 0:
                yield RawBytes(self, "data[]", blen.value, "Image Data")
            else:
                break

    def createDescription(self):
        return "Image: %ux%u pixels at (%u,%u)" % (
            self["width"].value, self["height"].value,
            self["left"].value, self["top"].value)

DISPOSAL_METHOD = {
    0: "No disposal specified",
    1: "Do not dispose",
    2: "Restore to background color",
    3: "Restore to previous",
}

NETSCAPE_CODE = {
    1: "Loop count",
}

def parseApplicationExtension(parent):
    yield PascalString8(parent, "app_name", "Application name")
    yield UInt8(parent, "size")
    size = parent["size"].value
    if parent["app_name"].value == "NETSCAPE2.0" and size == 3:
        yield Enum(UInt8(parent, "netscape_code"), NETSCAPE_CODE)
        if parent["netscape_code"].value == 1:
            yield UInt16(parent, "loop_count")
        else:
            yield RawBytes(parent, "raw", 2)
    else:
        yield RawBytes(parent, "raw", size)
    yield NullBytes(parent, "terminator", 1, "Terminator (0)")

def parseGraphicControl(parent):
    yield UInt8(parent, "size", "Block size (4)")

    yield Bit(parent, "has_transp", "Has transparency")
    yield Bit(parent, "user_input", "User input")
    yield Enum(Bits(parent, "disposal_method", 3), DISPOSAL_METHOD)
    yield NullBits(parent, "reserved[]", 3)

    if parent["size"].value != 4:
        raise ParserError("Invalid graphic control size")
    yield displayHandler(UInt16(parent, "delay", "Delay time in millisecond"), humanDuration)
    yield UInt8(parent, "transp", "Transparent color index")
    yield NullBytes(parent, "terminator", 1, "Terminator (0)")

def parseComments(parent):
    while True:
        field = PascalString8(parent, "comment[]", strip=" \0\r\n\t")
        yield field
        if field.length == 0:
            break

def parseTextExtension(parent):
    yield UInt8(parent, "block_size", "Block Size")
    yield UInt16(parent, "left", "Text Grid Left")
    yield UInt16(parent, "top", "Text Grid Top")
    yield UInt16(parent, "width", "Text Grid Width")
    yield UInt16(parent, "height", "Text Grid Height")
    yield UInt8(parent, "cell_width", "Character Cell Width")
    yield UInt8(parent, "cell_height", "Character Cell Height")
    yield UInt8(parent, "fg_color", "Foreground Color Index")
    yield UInt8(parent, "bg_color", "Background Color Index")
    while True:
        field = PascalString8(parent, "comment[]", strip=" \0\r\n\t")
        yield field
        if field.length == 0:
            break

def defaultExtensionParser(parent):
    while True:
        size = UInt8(parent, "size[]", "Size (in bytes)")
        yield size
        if 0 < size.value:
            yield RawBytes(parent, "content[]", size.value)
        else:
            break

class Extension(FieldSet):
    ext_code = {
        0xf9: ("graphic_ctl[]", parseGraphicControl, "Graphic control"),
        0xfe: ("comments[]", parseComments, "Comments"),
        0xff: ("app_ext[]", parseApplicationExtension, "Application extension"),
        0x01: ("text_ext[]", parseTextExtension, "Plain text extension")
    }
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        code = self["code"].value
        if code in self.ext_code:
            self._name, self.parser, self._description = self.ext_code[code]
        else:
            self.parser = defaultExtensionParser

    def createFields(self):
        yield textHandler(UInt8(self, "code", "Extension code"), hexadecimal)
        for field in self.parser(self):
            yield field

    def createDescription(self):
        return "Extension: function %s" % self["func"].display

class ScreenDescriptor(FieldSet):
    def createFields(self):
        yield UInt16(self, "width", "Width")
        yield UInt16(self, "height", "Height")
        yield Bits(self, "bpp", 3, "Bits per pixel minus one")
        yield Bit(self, "reserved", "(reserved)")
        yield Bits(self, "color_res", 3, "Color resolution minus one")
        yield Bit(self, "global_map", "Has global map?")
        yield UInt8(self, "background", "Background color")
        yield UInt8(self, "pixel_aspect_ratio", "Pixel Aspect Ratio")

    def createDescription(self):
        colors = 1 << (self["bpp"].value+1)
        return "Screen descriptor: %ux%u pixels %u colors" \
            % (self["width"].value, self["height"].value, colors)

class GifFile(Parser):
    endian = LITTLE_ENDIAN
    separator_name = {
        "!": "Extension",
        ",": "Image",
        ";": "Terminator"
    }
    PARSER_TAGS = {
        "id": "gif",
        "category": "image",
        "file_ext": ("gif",),
        "mime": (u"image/gif",),
        "min_size": (6 + 7 + 1 + 9)*8,   # signature + screen + separator + image
        "magic": (("GIF87a", 0), ("GIF89a", 0)),
        "description": "GIF picture"
    }

    def validate(self):
        if self.stream.readBytes(0, 6) not in ("GIF87a", "GIF89a"):
            return "Wrong header"
        if self["screen/width"].value == 0 or self["screen/height"].value == 0:
            return "Invalid image size"
        if MAX_WIDTH < self["screen/width"].value:
            return "Image width too big (%u)" % self["screen/width"].value
        if MAX_HEIGHT < self["screen/height"].value:
            return "Image height too big (%u)" % self["screen/height"].value
        return True

    def createFields(self):
        # Header
        yield String(self, "magic", 3, "File magic code", charset="ASCII")
        yield String(self, "version", 3, "GIF version", charset="ASCII")

        yield ScreenDescriptor(self, "screen")
        if self["screen/global_map"].value:
            bpp = (self["screen/bpp"].value+1)
            yield PaletteRGB(self, "color_map", 1 << bpp, "Color map")
            self.color_map = self["color_map"]
        else:
            self.color_map = None

        self.images = []
        while True:
            code = Enum(Character(self, "separator[]", "Separator code"), self.separator_name)
            yield code
            code = code.value
            if code == "!":
                yield Extension(self, "extensions[]")
            elif code == ",":
                yield Image(self, "image[]")
            elif code == ";":
                # GIF Terminator
                break
            else:
                raise ParserError("Wrong GIF image separator: 0x%02X" % ord(code))

    def createContentSize(self):
        field = self["image[0]"]
        start = field.absolute_address + field.size
        end = start + MAX_FILE_SIZE*8
        pos = self.stream.searchBytes("\0;", start, end)
        if pos:
            return pos + 16
        return None

########NEW FILE########
__FILENAME__ = ico
"""
Microsoft Windows icon and cursor file format parser.

Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt16, UInt32, Enum, RawBytes)
from lib.hachoir_parser.image.common import PaletteRGBA
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_parser.common.win32 import BitmapInfoHeader

class IconHeader(FieldSet):
    def createFields(self):
        yield UInt8(self, "width", "Width")
        yield UInt8(self, "height", "Height")
        yield UInt8(self, "nb_color", "Number of colors")
        yield UInt8(self, "reserved", "(reserved)")
        yield UInt16(self, "planes", "Color planes (=1)")
        yield UInt16(self, "bpp", "Bits per pixel")
        yield UInt32(self, "size", "Content size in bytes")
        yield UInt32(self, "offset", "Data offset")

    def createDescription(self):
        return "Icon: %ux%u pixels, %u bits/pixel" % \
            (self["width"].value, self["height"].value, self["bpp"].value)

    def isValid(self):
        if self["nb_color"].value == 0:
            if self["bpp"].value in (8, 24, 32) and self["planes"].value == 1:
                return True
            if self["planes"].value == 4 and self["bpp"].value == 0:
                return True
        elif self["nb_color"].value == 16:
            if self["bpp"].value in (4, 16) and self["planes"].value == 1:
                return True
        else:
            return False
        if self["bpp"].value == 0 and self["planes"].value == 0:
            return True
        return False

class IconData(FieldSet):
    def __init__(self, parent, name, header):
        FieldSet.__init__(self, parent, name, "Icon data")
        self.header = header

    def createFields(self):
        yield BitmapInfoHeader(self, "header")

        # Read palette if needed
        nb_color = self.header["nb_color"].value
        if self.header["bpp"].value == 8:
            nb_color = 256
        if nb_color != 0:
            yield PaletteRGBA(self, "palette", nb_color)

        # Read pixels
        size = self.header["size"].value - self.current_size/8
        yield RawBytes(self, "pixels", size, "Image pixels")

class IcoFile(Parser):
    endian = LITTLE_ENDIAN
    PARSER_TAGS = {
        "id": "ico",
        "category": "image",
        "file_ext": ("ico", "cur"),
        "mime": (u"image/x-ico",),
        "min_size": (22 + 40)*8,
#        "magic": (
#            ("\0\0\1\0", 0), # Icon
#            ("\0\0\2\0", 0), # Cursor
#        ),
        "magic_regex": ((
            # signature=0, type=(1|2), count in 1..20,
            "\0\0[\1\2]\0[\x01-\x14]."
            # size=(16x16|32x32|48x48|64x64),
            "(\x10\x10|\x20\x20|\x30\x30|\x40\x40)"
            # nb_color=0 or 16; nb_plane=(0|1|4), bpp=(0|8|24|32)
            "[\x00\x10]\0[\0\1\4][\0\x08\x18\x20]\0",
        0),),
        "description": "Microsoft Windows icon or cursor",
    }
    TYPE_NAME = {
        1: "icon",
        2: "cursor"
    }

    def validate(self):
        # Check signature and type
        if self["signature"].value != 0:
            return "Wrong file signature"
        if self["type"].value not in self.TYPE_NAME:
            return "Unknown picture type"

        # Check all icon headers
        index = -1
        for field in self:
            if field.name.startswith("icon_header"):
                index += 1
                if not field.isValid():
                    return "Invalid header #%u" % index
            elif 0 <= index:
                break
        return True

    def createFields(self):
        yield UInt16(self, "signature", "Signature (0x0000)")
        yield Enum(UInt16(self, "type", "Resource type"), self.TYPE_NAME)
        yield UInt16(self, "nb_items", "Number of items")
        items = []
        for index in xrange(self["nb_items"].value):
            item = IconHeader(self, "icon_header[]")
            yield item
            items.append(item)
        for header in items:
            if header["offset"].value*8 != self.current_size:
                raise ParserError("Icon: Problem with icon data offset.")
            yield IconData(self, "icon_data[]", header)

    def createDescription(self):
        desc = "Microsoft Windows %s" % self["type"].display
        size = []
        for header in self.array("icon_header"):
            size.append("%ux%ux%u" % (header["width"].value,
                header["height"].value, header["bpp"].value))
        if size:
            return "%s: %s" % (desc, ", ".join(size))
        else:
            return desc

    def createContentSize(self):
        count = self["nb_items"].value
        if not count:
            return None
        field = self["icon_data[%u]" % (count-1)]
        return field.absolute_address + field.size


########NEW FILE########
__FILENAME__ = iptc
"""
IPTC metadata parser (can be found in a JPEG picture for example)

Sources:
- Image-MetaData Perl module:
  http://www.annocpan.org/~BETTELLI/Image-MetaData-JPEG-0.15/...
  ...lib/Image/MetaData/JPEG/TagLists.pod
- IPTC tag name and description:
  http://peccatte.karefil.com/software/IPTCTableau.pdf

Author: Victor Stinner
"""

from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt16, String, RawBytes, NullBytes)
from lib.hachoir_core.text_handler import textHandler, hexadecimal

def IPTC_String(parent, name, desc=None):
    # Charset may be utf-8, ISO-8859-1, or ...
    return String(parent, name, parent["size"].value, desc,
        strip=" ")

dataset1 = {
}
dataset2 = {
      0: ("record_version", "Record version (2 for JPEG)", UInt16),
      5: ("obj_name", "Object name", None),
      7: ("edit_stat", "Edit status", None),
     10: ("urgency", "Urgency", UInt8),
     15: ("category[]", "Category", None),
     22: ("fixture", "Fixture identifier", IPTC_String),
     25: ("keyword[]", "Keywords", IPTC_String),
     30: ("release_date", "Release date", IPTC_String),
     35: ("release_time", "Release time", IPTC_String),
     40: ("instruction", "Special instructions", IPTC_String),
     55: ("date_created", "Date created", IPTC_String),
     60: ("time_created", "Time created (ISO 8601)", IPTC_String),
     65: ("originating_prog", "Originating program", IPTC_String),
     70: ("prog_ver", "Program version", IPTC_String),
     80: ("author", "By-line (Author)", IPTC_String),
     85: ("author_job", "By-line (Author precision)", IPTC_String),
     90: ("city", "City", IPTC_String),
     95: ("state", "Province / State", IPTC_String),
    100: ("country_code", "Country / Primary location code", IPTC_String),
    101: ("country_name", "Country / Primary location name", IPTC_String),
    103: ("trans_ref", "Original transmission reference", IPTC_String),
    105: ("headline", "Headline", IPTC_String),
    110: ("credit", "Credit", IPTC_String),
    115: ("source", "Source", IPTC_String),
    116: ("copyright", "Copyright notice", IPTC_String),
    120: ("caption", "Caption/Abstract", IPTC_String),
    122: ("writer", "Writer/editor", IPTC_String),
    231: ("history[]", "Document history (timestamp)", IPTC_String)
}
datasets = {1: dataset1, 2: dataset2}

class IPTC_Size(FieldSet):
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        value = 0
        for field in self:
            value <<= 15
            value  += (field.value & 0x7fff)
        self.createValue = lambda: value

    def createFields(self):
        while True:
            field = UInt16(self, "value[]")
            yield field
            if field.value < 0x8000:
                break

class IPTC_Chunk(FieldSet):
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        number = self["dataset_nb"].value
        self.dataset_info = None
        if number in datasets:
            tag = self["tag"].value
            if tag in datasets[number]:
                self.dataset_info = datasets[number][tag]
                self._name = self.dataset_info[0]
                self._description = self.dataset_info[1]
        size_chunk = self["size"]
        self._size = 3*8 + size_chunk.size + size_chunk.value*8

    def createFields(self):
        yield textHandler(UInt8(self, "signature", "IPTC signature (0x1c)"), hexadecimal)
        if self["signature"].value != 0x1C:
            raise ParserError("Wrong IPTC signature")
        yield textHandler(UInt8(self, "dataset_nb", "Dataset number"), hexadecimal)
        yield UInt8(self, "tag", "Tag")
        yield IPTC_Size(self, "size", "Content size")

        size = self["size"].value
        if 0 < size:
            if self.dataset_info:
                cls = self.dataset_info[2]
            else:
                cls = None
            if cls:
                yield cls(self, "content")
            else:
                yield RawBytes(self, "content", size)

class IPTC(FieldSet):
    def createFields(self):
        while 5 <= (self._size - self.current_size)/8:
            yield IPTC_Chunk(self, "chunk[]")
        size = (self._size - self.current_size) / 8
        if 0 < size:
            yield NullBytes(self, "padding", size)


########NEW FILE########
__FILENAME__ = jpeg
"""
JPEG picture parser.

Information:

- APP14 documents
  http://partners.adobe.com/public/developer/en/ps/sdk/5116.DCT_Filter.pdf
  http://java.sun.com/j2se/1.5.0/docs/api/javax/imageio/metadata/doc-files/jpeg_metadata.html#color
- APP12:
  http://search.cpan.org/~exiftool/Image-ExifTool/lib/Image/ExifTool/TagNames.pod

Author: Victor Stinner
"""

from lib.hachoir_core.error import HachoirError
from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt16, Enum,
    Bit, Bits, NullBits, NullBytes,
    String, RawBytes)
from lib.hachoir_parser.image.common import PaletteRGB
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_parser.image.exif import Exif
from lib.hachoir_parser.image.photoshop_metadata import PhotoshopMetadata

MAX_FILESIZE = 100 * 1024 * 1024

# The four tables (hash/sum for color/grayscale JPEG) comes
# from ImageMagick project
QUALITY_HASH_COLOR = (
    1020, 1015,  932,  848,  780,  735,  702,  679,  660,  645,
     632,  623,  613,  607,  600,  594,  589,  585,  581,  571,
     555,  542,  529,  514,  494,  474,  457,  439,  424,  410,
     397,  386,  373,  364,  351,  341,  334,  324,  317,  309,
     299,  294,  287,  279,  274,  267,  262,  257,  251,  247,
     243,  237,  232,  227,  222,  217,  213,  207,  202,  198,
     192,  188,  183,  177,  173,  168,  163,  157,  153,  148,
     143,  139,  132,  128,  125,  119,  115,  108,  104,   99,
      94,   90,   84,   79,   74,   70,   64,   59,   55,   49,
      45,   40,   34,   30,   25,   20,   15,   11,    6,    4,
       0)

QUALITY_SUM_COLOR = (
    32640,32635,32266,31495,30665,29804,29146,28599,28104,27670,
    27225,26725,26210,25716,25240,24789,24373,23946,23572,22846,
    21801,20842,19949,19121,18386,17651,16998,16349,15800,15247,
    14783,14321,13859,13535,13081,12702,12423,12056,11779,11513,
    11135,10955,10676,10392,10208, 9928, 9747, 9564, 9369, 9193,
     9017, 8822, 8639, 8458, 8270, 8084, 7896, 7710, 7527, 7347,
     7156, 6977, 6788, 6607, 6422, 6236, 6054, 5867, 5684, 5495,
     5305, 5128, 4945, 4751, 4638, 4442, 4248, 4065, 3888, 3698,
     3509, 3326, 3139, 2957, 2775, 2586, 2405, 2216, 2037, 1846,
     1666, 1483, 1297, 1109,  927,  735,  554,  375,  201,  128,
        0)

QUALITY_HASH_GRAY = (
    510,  505,  422,  380,  355,  338,  326,  318,  311,  305,
    300,  297,  293,  291,  288,  286,  284,  283,  281,  280,
    279,  278,  277,  273,  262,  251,  243,  233,  225,  218,
    211,  205,  198,  193,  186,  181,  177,  172,  168,  164,
    158,  156,  152,  148,  145,  142,  139,  136,  133,  131,
    129,  126,  123,  120,  118,  115,  113,  110,  107,  105,
    102,  100,   97,   94,   92,   89,   87,   83,   81,   79,
     76,   74,   70,   68,   66,   63,   61,   57,   55,   52,
     50,   48,   44,   42,   39,   37,   34,   31,   29,   26,
     24,   21,   18,   16,   13,   11,    8,    6,    3,    2,
      0)

QUALITY_SUM_GRAY = (
    16320,16315,15946,15277,14655,14073,13623,13230,12859,12560,
    12240,11861,11456,11081,10714,10360,10027, 9679, 9368, 9056,
     8680, 8331, 7995, 7668, 7376, 7084, 6823, 6562, 6345, 6125,
     5939, 5756, 5571, 5421, 5240, 5086, 4976, 4829, 4719, 4616,
     4463, 4393, 4280, 4166, 4092, 3980, 3909, 3835, 3755, 3688,
     3621, 3541, 3467, 3396, 3323, 3247, 3170, 3096, 3021, 2952,
     2874, 2804, 2727, 2657, 2583, 2509, 2437, 2362, 2290, 2211,
     2136, 2068, 1996, 1915, 1858, 1773, 1692, 1620, 1552, 1477,
     1398, 1326, 1251, 1179, 1109, 1031,  961,  884,  814,  736,
      667,  592,  518,  441,  369,  292,  221,  151,   86,   64,
        0)

JPEG_NATURAL_ORDER = (
     0,  1,  8, 16,  9,  2,  3, 10,
    17, 24, 32, 25, 18, 11,  4,  5,
    12, 19, 26, 33, 40, 48, 41, 34,
    27, 20, 13,  6,  7, 14, 21, 28,
    35, 42, 49, 56, 57, 50, 43, 36,
    29, 22, 15, 23, 30, 37, 44, 51,
    58, 59, 52, 45, 38, 31, 39, 46,
    53, 60, 61, 54, 47, 55, 62, 63)

class JpegChunkApp0(FieldSet):
    UNIT_NAME = {
        0: "pixels",
        1: "dots per inch",
        2: "dots per cm",
    }

    def createFields(self):
        yield String(self, "jfif", 5, "JFIF string", charset="ASCII")
        if self["jfif"].value != "JFIF\0":
            raise ParserError(
                "Stream doesn't look like JPEG chunk (wrong JFIF signature)")
        yield UInt8(self, "ver_maj", "Major version")
        yield UInt8(self, "ver_min", "Minor version")
        yield Enum(UInt8(self, "units", "Units"), self.UNIT_NAME)
        if self["units"].value == 0:
            yield UInt16(self, "aspect_x", "Aspect ratio (X)")
            yield UInt16(self, "aspect_y", "Aspect ratio (Y)")
        else:
            yield UInt16(self, "x_density", "X density")
            yield UInt16(self, "y_density", "Y density")
        yield UInt8(self, "thumb_w", "Thumbnail width")
        yield UInt8(self, "thumb_h", "Thumbnail height")
        thumb_size = self["thumb_w"].value * self["thumb_h"].value
        if thumb_size != 0:
            yield PaletteRGB(self, "thumb_palette", 256)
            yield RawBytes(self, "thumb_data", thumb_size, "Thumbnail data")

class Ducky(FieldSet):
    BLOCK_TYPE = {
        0: "end",
        1: "Quality",
        2: "Comment",
        3: "Copyright",
    }
    def createFields(self):
        yield Enum(UInt16(self, "type"), self.BLOCK_TYPE)
        if self["type"].value == 0:
            return
        yield UInt16(self, "size")
        size = self["size"].value
        if size:
            yield RawBytes(self, "data", size)

class APP12(FieldSet):
    """
    The JPEG APP12 "Picture Info" segment was used by some older cameras, and
    contains ASCII-based meta information.
    """
    def createFields(self):
        yield String(self, "ducky", 5, '"Ducky" string', charset="ASCII")
        while not self.eof:
            yield Ducky(self, "item[]")

class StartOfFrame(FieldSet):
    def createFields(self):
        yield UInt8(self, "precision")

        yield UInt16(self, "height")
        yield UInt16(self, "width")
        yield UInt8(self, "nr_components")

        for index in range(self["nr_components"].value):
            yield UInt8(self, "component_id[]")
            yield UInt8(self, "high[]")
            yield UInt8(self, "low[]")

class Comment(FieldSet):
    def createFields(self):
        yield String(self, "comment", self.size//8, strip="\0")

class AdobeChunk(FieldSet):
    COLORSPACE_TRANSFORMATION = {
        1: "YCbCr (converted from RGB)",
        2: "YCCK (converted from CMYK)",
    }
    def createFields(self):
        if self.stream.readBytes(self.absolute_address, 5) != "Adobe":
            yield RawBytes(self, "raw", self.size//8, "Raw data")
            return
        yield String(self, "adobe", 5, "\"Adobe\" string", charset="ASCII")
        yield UInt16(self, "version", "DCT encoder version")
        yield Enum(Bit(self, "flag00"),
            {False: "Chop down or subsampling", True: "Blend"})
        yield NullBits(self, "flags0_reserved", 15)
        yield NullBytes(self, "flags1", 2)
        yield Enum(UInt8(self, "color_transform", "Colorspace transformation code"), self.COLORSPACE_TRANSFORMATION)

class StartOfScan(FieldSet):
    def createFields(self):
        yield UInt8(self, "nr_components")

        for index in range(self["nr_components"].value):
            comp_id = UInt8(self, "component_id[]")
            yield comp_id
            if not(1 <= comp_id.value <= self["nr_components"].value):
               raise ParserError("JPEG error: Invalid component-id")
            yield UInt8(self, "value[]")
        yield RawBytes(self, "raw", 3) # TODO: What's this???

class RestartInterval(FieldSet):
    def createFields(self):
        yield UInt16(self, "interval", "Restart interval")

class QuantizationTable(FieldSet):
    def createFields(self):
        # Code based on function get_dqt() (jdmarker.c from libjpeg62)
        yield Bits(self, "is_16bit", 4)
        yield Bits(self, "index", 4)
        if self["index"].value >= 4:
            raise ParserError("Invalid quantification index (%s)" % self["index"].value)
        if self["is_16bit"].value:
            coeff_type = UInt16
        else:
            coeff_type = UInt8
        for index in xrange(64):
            natural = JPEG_NATURAL_ORDER[index]
            yield coeff_type(self, "coeff[%u]" % natural)

    def createDescription(self):
        return "Quantification table #%u" % self["index"].value

class DefineQuantizationTable(FieldSet):
    def createFields(self):
        while self.current_size < self.size:
            yield QuantizationTable(self, "qt[]")

class JpegChunk(FieldSet):
    TAG_SOI = 0xD8
    TAG_EOI = 0xD9
    TAG_SOS = 0xDA
    TAG_DQT = 0xDB
    TAG_DRI = 0xDD
    TAG_INFO = {
        0xC4: ("huffman[]", "Define Huffman Table (DHT)", None),
        0xD8: ("start_image", "Start of image (SOI)", None),
        0xD9: ("end_image", "End of image (EOI)", None),
        0xDA: ("start_scan", "Start Of Scan (SOS)", StartOfScan),
        0xDB: ("quantization[]", "Define Quantization Table (DQT)", DefineQuantizationTable),
        0xDC: ("nb_line", "Define number of Lines (DNL)", None),
        0xDD: ("restart_interval", "Define Restart Interval (DRI)", RestartInterval),
        0xE0: ("app0", "APP0", JpegChunkApp0),
        0xE1: ("exif", "Exif metadata", Exif),
        0xE2: ("icc", "ICC profile", None),
        0xEC: ("app12", "APP12", APP12),
        0xED: ("photoshop", "Photoshop", PhotoshopMetadata),
        0xEE: ("adobe", "Image encoding information for DCT filters (Adobe)", AdobeChunk),
        0xFE: ("comment[]", "Comment", Comment),
    }
    START_OF_FRAME = {
        0xC0: u"Baseline",
        0xC1: u"Extended sequential",
        0xC2: u"Progressive",
        0xC3: u"Lossless",
        0xC5: u"Differential sequential",
        0xC6: u"Differential progressive",
        0xC7: u"Differential lossless",
        0xC9: u"Extended sequential, arithmetic coding",
        0xCA: u"Progressive, arithmetic coding",
        0xCB: u"Lossless, arithmetic coding",
        0xCD: u"Differential sequential, arithmetic coding",
        0xCE: u"Differential progressive, arithmetic coding",
        0xCF: u"Differential lossless, arithmetic coding",
    }
    for key, text in START_OF_FRAME.iteritems():
        TAG_INFO[key] = ("start_frame", "Start of frame (%s)" % text.lower(), StartOfFrame)

    def __init__(self, parent, name, description=None):
        FieldSet.__init__(self, parent, name, description)
        tag = self["type"].value
        if tag == 0xE1:
            # Hack for Adobe extension: XAP metadata (as XML)
            bytes = self.stream.readBytes(self.absolute_address + 32, 6)
            if bytes == "Exif\0\0":
                self._name = "exif"
                self._description = "EXIF"
                self._parser = Exif
            else:
                self._parser = None
        elif tag in self.TAG_INFO:
            self._name, self._description, self._parser = self.TAG_INFO[tag]
        else:
            self._parser = None

    def createFields(self):
        yield textHandler(UInt8(self, "header", "Header"), hexadecimal)
        if self["header"].value != 0xFF:
            raise ParserError("JPEG: Invalid chunk header!")
        yield textHandler(UInt8(self, "type", "Type"), hexadecimal)
        tag = self["type"].value
        if tag in (self.TAG_SOI, self.TAG_EOI):
            return
        yield UInt16(self, "size", "Size")
        size = (self["size"].value - 2)
        if 0 < size:
            if self._parser:
                yield self._parser(self, "content", "Chunk content", size=size*8)
            else:
                yield RawBytes(self, "data", size, "Data")

    def createDescription(self):
        return "Chunk: %s" % self["type"].display

class JpegFile(Parser):
    endian = BIG_ENDIAN
    PARSER_TAGS = {
        "id": "jpeg",
        "category": "image",
        "file_ext": ("jpg", "jpeg"),
        "mime": (u"image/jpeg",),
        "magic": (
            ("\xFF\xD8\xFF\xE0", 0),   # (Start Of Image, APP0)
            ("\xFF\xD8\xFF\xE1", 0),   # (Start Of Image, EXIF)
            ("\xFF\xD8\xFF\xEE", 0),   # (Start Of Image, Adobe)
        ),
        "min_size": 22*8,
        "description": "JPEG picture",
        "subfile": "skip",
    }

    def validate(self):
        if self.stream.readBytes(0, 2) != "\xFF\xD8":
            return "Invalid file signature"
        try:
            for index, field in enumerate(self):
                chunk_type = field["type"].value
                if chunk_type not in JpegChunk.TAG_INFO:
                    return "Unknown chunk type: 0x%02X (chunk #%s)" % (chunk_type, index)
                if index == 2:
                    # Only check 3 fields
                    break
        except HachoirError:
            return "Unable to parse at least three chunks"
        return True

    def createFields(self):
        while not self.eof:
            chunk = JpegChunk(self, "chunk[]")
            yield chunk
            if chunk["type"].value == JpegChunk.TAG_SOS:
                # TODO: Read JPEG image data...
                break

        # TODO: is it possible to handle piped input?
        if self._size is None:
            raise NotImplementedError

        has_end = False
        size = (self._size - self.current_size) // 8
        if size:
            if 2 < size \
            and self.stream.readBytes(self._size - 16, 2) == "\xff\xd9":
                has_end = True
                size -= 2
            yield RawBytes(self, "data", size, "JPEG data")
        if has_end:
            yield JpegChunk(self, "chunk[]")

    def createDescription(self):
        desc = "JPEG picture"
        if "sof/content" in self:
            header = self["sof/content"]
            desc += ": %ux%u pixels" % (header["width"].value, header["height"].value)
        return desc

    def createContentSize(self):
        if "end" in self:
            return self["end"].absolute_address + self["end"].size
        if "data" not in self:
            return None
        start = self["data"].absolute_address
        end = self.stream.searchBytes("\xff\xd9", start, MAX_FILESIZE*8)
        if end is not None:
            return end + 16
        return None


########NEW FILE########
__FILENAME__ = pcx
"""
PCX picture filter.
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (
    UInt8, UInt16,
    PaddingBytes, RawBytes,
    Enum)
from lib.hachoir_parser.image.common import PaletteRGB
from lib.hachoir_core.endian import LITTLE_ENDIAN

class PcxFile(Parser):
    endian = LITTLE_ENDIAN
    PARSER_TAGS = {
        "id": "pcx",
        "category": "image",
        "file_ext": ("pcx",),
        "mime": (u"image/x-pcx",),
        "min_size": 128*8,
        "description": "PC Paintbrush (PCX) picture"
    }
    compression_name = { 1: "Run-length encoding (RLE)" }
    version_name = {
        0: u"Version 2.5 of PC Paintbrush",
        2: u"Version 2.8 with palette information",
        3: u"Version 2.8 without palette information",
        4: u"PC Paintbrush for Windows",
        5: u"Version 3.0 (or greater) of PC Paintbrush"
    }

    def validate(self):
        if self["id"].value != 10:
            return "Wrong signature"
        if self["version"].value not in self.version_name:
            return "Unknown format version"
        if self["bpp"].value not in (1, 2, 4, 8, 24, 32):
            return "Unknown bits/pixel"
        if self["reserved[0]"].value != "\0":
            return "Invalid reserved value"
        return True

    def createFields(self):
        yield UInt8(self, "id", "PCX identifier (10)")
        yield Enum(UInt8(self, "version", "PCX version"), self.version_name)
        yield Enum(UInt8(self, "compression", "Compression method"), self.compression_name)
        yield UInt8(self, "bpp", "Bits / pixel")
        yield UInt16(self, "xmin", "Minimum X")
        yield UInt16(self, "ymin", "Minimum Y")
        yield UInt16(self, "xmax", "Width minus one") # value + 1
        yield UInt16(self, "ymax", "Height minus one") # value + 1
        yield UInt16(self, "horiz_dpi", "Horizontal DPI")
        yield UInt16(self, "vert_dpi", "Vertical DPI")
        yield PaletteRGB(self, "palette_4bits", 16, "Palette (4 bits)")
        yield PaddingBytes(self, "reserved[]", 1)
        yield UInt8(self, "nb_color_plan", "Number of color plans")
        yield UInt16(self, "bytes_per_line", "Bytes per line")
        yield UInt16(self, "color_mode", "Color mode")
        yield PaddingBytes(self, "reserved[]", 58)

        if self._size is None: # TODO: is it possible to handle piped input?
            raise NotImplementedError

        nb_colors = 256
        size = (self._size - self.current_size)/8
        has_palette = self["bpp"].value == 8
        if has_palette:
            size -= nb_colors*3
        yield RawBytes(self, "image_data", size, "Image data")

        if has_palette:
            yield PaletteRGB(self, "palette_8bits", nb_colors, "Palette (8 bit)")


########NEW FILE########
__FILENAME__ = photoshop_metadata
from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt16, UInt32,
    String, CString, PascalString8,
    NullBytes, RawBytes)
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.tools import alignValue, createDict
from lib.hachoir_parser.image.iptc import IPTC
from lib.hachoir_parser.common.win32 import PascalStringWin32

class Version(FieldSet):
    def createFields(self):
        yield UInt32(self, "version")
        yield UInt8(self, "has_realm")
        yield PascalStringWin32(self, "writer_name", charset="UTF-16-BE")
        yield PascalStringWin32(self, "reader_name", charset="UTF-16-BE")
        yield UInt32(self, "file_version")
        size = (self.size - self.current_size) // 8
        if size:
            yield NullBytes(self, "padding", size)

class Photoshop8BIM(FieldSet):
    TAG_INFO = {
        0x03ed: ("res_info", None, "Resolution information"),
        0x03f3: ("print_flag", None, "Print flags: labels, crop marks, colour bars, etc."),
        0x03f5: ("col_half_info", None, "Colour half-toning information"),
        0x03f8: ("color_trans_func", None, "Colour transfer function"),
        0x0404: ("iptc", IPTC, "IPTC/NAA"),
        0x0406: ("jpeg_qual", None, "JPEG quality"),
        0x0408: ("grid_guide", None, "Grid guides informations"),
        0x040a: ("copyright_flag", None, "Copyright flag"),
        0x040c: ("thumb_res2", None, "Thumbnail resource (2)"),
        0x040d: ("glob_angle", None, "Global lighting angle for effects"),
        0x0411: ("icc_tagged", None, "ICC untagged (1 means intentionally untagged)"),
        0x0414: ("base_layer_id", None, "Base value for new layers ID's"),
        0x0419: ("glob_altitude", None, "Global altitude"),
        0x041a: ("slices", None, "Slices"),
        0x041e: ("url_list", None, "Unicode URL's"),
        0x0421: ("version", Version, "Version information"),
        0x2710: ("print_flag2", None, "Print flags (2)"),
    }
    TAG_NAME = createDict(TAG_INFO, 0)
    CONTENT_HANDLER = createDict(TAG_INFO, 1)
    TAG_DESC = createDict(TAG_INFO, 2)

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        try:
            self._name, self.handler, self._description = self.TAG_INFO[self["tag"].value]
        except KeyError:
            self.handler = None
        size = self["size"]
        self._size = size.address + size.size + alignValue(size.value, 2) * 8

    def createFields(self):
        yield String(self, "signature", 4, "8BIM signature", charset="ASCII")
        if self["signature"].value != "8BIM":
            raise ParserError("Stream doesn't look like 8BIM item (wrong signature)!")
        yield textHandler(UInt16(self, "tag"), hexadecimal)
        if self.stream.readBytes(self.absolute_address + self.current_size, 4) != "\0\0\0\0":
            yield PascalString8(self, "name")
            size = 2 + (self["name"].size // 8) % 2
            yield NullBytes(self, "name_padding", size)
        else:
            yield String(self, "name", 4, strip="\0")
        yield UInt16(self, "size")
        size = alignValue(self["size"].value, 2)
        if not size:
            return
        if self.handler:
            yield self.handler(self, "content", size=size*8)
        else:
            yield RawBytes(self, "content", size)

class PhotoshopMetadata(FieldSet):
    def createFields(self):
        yield CString(self, "signature", "Photoshop version")
        if self["signature"].value == "Photoshop 3.0":
            while not self.eof:
                yield Photoshop8BIM(self, "item[]")
        else:
            size = (self._size - self.current_size) / 8
            yield RawBytes(self, "rawdata", size)


########NEW FILE########
__FILENAME__ = png
"""
PNG picture file parser.

Documents:
- RFC 2083
  http://www.faqs.org/rfcs/rfc2083.html

Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, Fragment,
    ParserError, MissingField,
    UInt8, UInt16, UInt32,
    String, CString,
    Bytes, RawBytes,
    Bit, NullBits,
    Enum, CompressedField)
from lib.hachoir_parser.image.common import RGB
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.endian import NETWORK_ENDIAN
from lib.hachoir_core.tools import humanFilesize
from datetime import datetime

MAX_FILESIZE = 500 * 1024 * 1024

try:
    from zlib import decompressobj

    class Gunzip:
        def __init__(self, stream):
            self.gzip = decompressobj()

        def __call__(self, size, data=None):
            if data is None:
                data = self.gzip.unconsumed_tail
            return self.gzip.decompress(data, size)

    has_deflate = True
except ImportError:
    has_deflate = False

UNIT_NAME = {1: "Meter"}
COMPRESSION_NAME = {
    0: u"deflate" # with 32K sliding window
}
MAX_CHUNK_SIZE = 500 * 1024 # Maximum chunk size (500 KB)

def headerParse(parent):
    yield UInt32(parent, "width", "Width (pixels)")
    yield UInt32(parent, "height", "Height (pixels)")
    yield UInt8(parent, "bit_depth", "Bit depth")
    yield NullBits(parent, "reserved", 5)
    yield Bit(parent, "has_alpha", "Has alpha channel?")
    yield Bit(parent, "color", "Color used?")
    yield Bit(parent, "has_palette", "Has a color palette?")
    yield Enum(UInt8(parent, "compression", "Compression method"), COMPRESSION_NAME)
    yield UInt8(parent, "filter", "Filter method")
    yield UInt8(parent, "interlace", "Interlace method")

def headerDescription(parent):
    return "Header: %ux%u pixels and %u bits/pixel" % \
        (parent["width"].value, parent["height"].value, getBitsPerPixel(parent))

def paletteParse(parent):
    size = parent["size"].value
    if (size % 3) != 0:
        raise ParserError("Palette have invalid size (%s), should be 3*n!" % size)
    nb_colors = size // 3
    for index in xrange(nb_colors):
        yield RGB(parent, "color[]")

def paletteDescription(parent):
    return "Palette: %u colors" % (parent["size"].value // 3)

def gammaParse(parent):
    yield UInt32(parent, "gamma", "Gamma (x100,000)")
def gammaValue(parent):
    return float(parent["gamma"].value) / 100000
def gammaDescription(parent):
    return "Gamma: %.3f" % parent.value

def textParse(parent):
    yield CString(parent, "keyword", "Keyword", charset="ISO-8859-1")
    length = parent["size"].value - parent["keyword"].size/8
    if length:
        yield String(parent, "text", length, "Text", charset="ISO-8859-1")

def textDescription(parent):
    if "text" in parent:
        return u'Text: %s' % parent["text"].display
    else:
        return u'Text'

def timestampParse(parent):
    yield UInt16(parent, "year", "Year")
    yield UInt8(parent, "month", "Month")
    yield UInt8(parent, "day", "Day")
    yield UInt8(parent, "hour", "Hour")
    yield UInt8(parent, "minute", "Minute")
    yield UInt8(parent, "second", "Second")

def timestampValue(parent):
    value = datetime(
        parent["year"].value, parent["month"].value, parent["day"].value,
        parent["hour"].value, parent["minute"].value, parent["second"].value)
    return value

def physicalParse(parent):
    yield UInt32(parent, "pixel_per_unit_x", "Pixel per unit, X axis")
    yield UInt32(parent, "pixel_per_unit_y", "Pixel per unit, Y axis")
    yield Enum(UInt8(parent, "unit", "Unit type"), UNIT_NAME)

def physicalDescription(parent):
    x = parent["pixel_per_unit_x"].value
    y = parent["pixel_per_unit_y"].value
    desc = "Physical: %ux%u pixels" % (x,y)
    if parent["unit"].value == 1:
        desc += " per meter"
    return desc

def parseBackgroundColor(parent):
    yield UInt16(parent, "red")
    yield UInt16(parent, "green")
    yield UInt16(parent, "blue")

def backgroundColorDesc(parent):
    rgb = parent["red"].value, parent["green"].value, parent["blue"].value
    name = RGB.color_name.get(rgb)
    if not name:
        name = "#%02X%02X%02X" % rgb
    return "Background color: %s" % name


class ImageData(Fragment):
    def __init__(self, parent, name="compressed_data"):
        Fragment.__init__(self, parent, name, None, 8*parent["size"].value)
        data = parent.name.split('[')
        data, next = "../%s[%%u]" % data[0], int(data[1][:-1]) + 1
        first = parent.getField(data % 0)
        if first is parent:
            first = None
            if has_deflate:
                CompressedField(self, Gunzip)
        else:
            first = first[name]
        try:
            _next = parent[data % next]
            next = lambda: _next[name]
        except MissingField:
            next = None
        self.setLinks(first, next)

def parseTransparency(parent):
    for i in range(parent["size"].value):
        yield UInt8(parent, "alpha_value[]", "Alpha value for palette entry %i"%i)

def getBitsPerPixel(header):
    nr_component = 1
    if header["has_alpha"].value:
        nr_component += 1
    if header["color"].value and not header["has_palette"].value:
        nr_component += 2
    return nr_component * header["bit_depth"].value

class Chunk(FieldSet):
    TAG_INFO = {
        "tIME": ("time", timestampParse, "Timestamp", timestampValue),
        "pHYs": ("physical", physicalParse, physicalDescription, None),
        "IHDR": ("header", headerParse, headerDescription, None),
        "PLTE": ("palette", paletteParse, paletteDescription, None),
        "gAMA": ("gamma", gammaParse, gammaDescription, gammaValue),
        "tEXt": ("text[]", textParse, textDescription, None),
        "tRNS": ("transparency", parseTransparency, "Transparency Info", None),

        "bKGD": ("background", parseBackgroundColor, backgroundColorDesc, None),
        "IDAT": ("data[]", lambda parent: (ImageData(parent),), "Image data", None),
        "iTXt": ("utf8_text[]", None, "International text (encoded in UTF-8)", None),
        "zTXt": ("comp_text[]", None, "Compressed text", None),
        "IEND": ("end", None, "End", None)
    }

    def createValueFunc(self):
        return self.value_func(self)

    def __init__(self, parent, name, description=None):
        FieldSet.__init__(self, parent, name, description)
        self._size = (self["size"].value + 3*4) * 8
        if MAX_CHUNK_SIZE < (self._size//8):
            raise ParserError("PNG: Chunk is too big (%s)"
                % humanFilesize(self._size//8))
        tag = self["tag"].value
        self.desc_func = None
        self.value_func = None
        if tag in self.TAG_INFO:
            self._name, self.parse_func, desc, value_func = self.TAG_INFO[tag]
            if value_func:
                self.value_func = value_func
                self.createValue = self.createValueFunc
            if desc:
                if isinstance(desc, str):
                    self._description = desc
                else:
                    self.desc_func = desc
        else:
            self._description = ""
            self.parse_func = None

    def createFields(self):
        yield UInt32(self, "size", "Size")
        yield String(self, "tag", 4, "Tag", charset="ASCII")

        size = self["size"].value
        if size != 0:
            if self.parse_func:
                for field in self.parse_func(self):
                    yield field
            else:
                yield RawBytes(self, "content", size, "Data")
        yield textHandler(UInt32(self, "crc32", "CRC32"), hexadecimal)

    def createDescription(self):
        if self.desc_func:
            return self.desc_func(self)
        else:
            return "Chunk: %s" % self["tag"].display

class PngFile(Parser):
    PARSER_TAGS = {
        "id": "png",
        "category": "image",
        "file_ext": ("png",),
        "mime": (u"image/png", u"image/x-png"),
        "min_size": 8*8, # just the identifier
        "magic": [('\x89PNG\r\n\x1A\n', 0)],
        "description": "Portable Network Graphics (PNG) picture"
    }
    endian = NETWORK_ENDIAN

    def validate(self):
        if self["id"].value != '\x89PNG\r\n\x1A\n':
            return "Invalid signature"
        if self[1].name != "header":
            return "First chunk is not header"
        return True

    def createFields(self):
        yield Bytes(self, "id", 8, r"PNG identifier ('\x89PNG\r\n\x1A\n')")
        while not self.eof:
            yield Chunk(self, "chunk[]")

    def createDescription(self):
        header = self["header"]
        desc = "PNG picture: %ux%ux%u" % (
            header["width"].value, header["height"].value, getBitsPerPixel(header))
        if header["has_alpha"].value:
            desc += " (alpha layer)"
        return desc

    def createContentSize(self):
        field = self["header"]
        start = field.absolute_address + field.size
        end = MAX_FILESIZE * 8
        pos = self.stream.searchBytes("\0\0\0\0IEND\xae\x42\x60\x82", start, end)
        if pos is not None:
            return pos + 12*8
        return None


########NEW FILE########
__FILENAME__ = psd
"""
Photoshop parser (.psd file).

Creation date: 8 january 2006
Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt16, UInt32, String, NullBytes, Enum, RawBytes)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_parser.image.photoshop_metadata import Photoshop8BIM

class Config(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = (4 + self["size"].value) * 8

    def createFields(self):
        yield UInt32(self, "size")
        while not self.eof:
            yield Photoshop8BIM(self, "item[]")

class PsdFile(Parser):
    endian = BIG_ENDIAN
    PARSER_TAGS = {
        "id": "psd",
        "category": "image",
        "file_ext": ("psd",),
        "mime": (u"image/psd", u"image/photoshop", u"image/x-photoshop"),
        "min_size": 4*8,
        "magic": (("8BPS\0\1",0),),
        "description": "Photoshop (PSD) picture",
    }
    COLOR_MODE = {
        0: u"Bitmap",
        1: u"Grayscale",
        2: u"Indexed",
        3: u"RGB color",
        4: u"CMYK color",
        7: u"Multichannel",
        8: u"Duotone",
        9: u"Lab Color",
    }
    COMPRESSION_NAME = {
        0: "Raw data",
        1: "RLE",
    }

    def validate(self):
        if self.stream.readBytes(0, 4) != "8BPS":
            return "Invalid signature"
        return True

    def createFields(self):
        yield String(self, "signature", 4, "PSD signature (8BPS)", charset="ASCII")
        yield UInt16(self, "version")
        yield NullBytes(self, "reserved[]", 6)
        yield UInt16(self, "nb_channels")
        yield UInt32(self, "width")
        yield UInt32(self, "height")
        yield UInt16(self, "depth")
        yield Enum(UInt16(self, "color_mode"), self.COLOR_MODE)

        # Mode data
        yield UInt32(self, "mode_data_size")
        size = self["mode_data_size"].value
        if size:
            yield RawBytes(self, "mode_data", size)

        # Resources
        yield Config(self, "config")

        # Reserved
        yield UInt32(self, "reserved_data_size")
        size = self["reserved_data_size"].value
        if size:
            yield RawBytes(self, "reserved_data", size)

        yield Enum(UInt16(self, "compression"), self.COMPRESSION_NAME)

        size = (self.size - self.current_size) // 8
        if size:
            yield RawBytes(self, "end", size)


########NEW FILE########
__FILENAME__ = tga
"""
Truevision Targa Graphic (TGA) picture parser.

Author: Victor Stinner
Creation: 18 december 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import FieldSet, UInt8, UInt16, Enum, RawBytes
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_parser.image.common import PaletteRGB

class Line(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["/width"].value * self["/bpp"].value

    def createFields(self):
        for x in xrange(self["/width"].value):
            yield UInt8(self, "pixel[]")

class Pixels(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["/width"].value * self["/height"].value * self["/bpp"].value

    def createFields(self):
        if self["/options"].value == 0:
            RANGE = xrange(self["/height"].value-1,-1,-1)
        else:
            RANGE = xrange(self["/height"].value)
        for y in RANGE:
            yield Line(self, "line[%u]" % y)

class TargaFile(Parser):
    PARSER_TAGS = {
        "id": "targa",
        "category": "image",
        "file_ext": ("tga",),
        "mime": (u"image/targa", u"image/tga", u"image/x-tga"),
        "min_size": 18*8,
        "description": u"Truevision Targa Graphic (TGA)"
    }
    CODEC_NAME = {
         1: u"8-bit uncompressed",
         2: u"24-bit uncompressed",
         9: u"8-bit RLE",
        10: u"24-bit RLE",
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self["version"].value != 1:
            return "Unknown version"
        if self["codec"].value not in self.CODEC_NAME:
            return "Unknown codec"
        if self["x_min"].value != 0 or self["y_min"].value != 0:
            return "(x_min, y_min) is not (0,0)"
        if self["bpp"].value not in (8, 24):
            return "Unknown bits/pixel value"
        return True

    def createFields(self):
        yield UInt8(self, "hdr_size", "Header size in bytes")
        yield UInt8(self, "version", "Targa version (always one)")
        yield Enum(UInt8(self, "codec", "Pixels encoding"), self.CODEC_NAME)
        yield UInt16(self, "palette_ofs", "Palette absolute file offset")
        yield UInt16(self, "nb_color", "Number of color")
        yield UInt8(self, "color_map_size", "Color map entry size")
        yield UInt16(self, "x_min")
        yield UInt16(self, "y_min")
        yield UInt16(self, "width")
        yield UInt16(self, "height")
        yield UInt8(self, "bpp", "Bits per pixel")
        yield UInt8(self, "options", "Options (0: vertical mirror)")
        if self["bpp"].value == 8:
            yield PaletteRGB(self, "palette", 256)
        if self["codec"].value == 1:
            yield Pixels(self, "pixels")
        else:
            size = (self.size - self.current_size) // 8
            if size:
                yield RawBytes(self, "raw_pixels", size)



########NEW FILE########
__FILENAME__ = tiff
"""
TIFF image parser.

Authors: Victor Stinner and Sebastien Ponce
Creation date: 30 september 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, SeekableFieldSet, ParserError, RootSeekableFieldSet,
    UInt16, UInt32, Bytes, String)
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN
from lib.hachoir_parser.image.exif import BasicIFDEntry
from lib.hachoir_core.tools import createDict

MAX_COUNT = 250

class IFDEntry(BasicIFDEntry):
    static_size = 12*8

    TAG_INFO = {
        254: ("new_subfile_type", "New subfile type"),
        255: ("subfile_type", "Subfile type"),
        256: ("img_width", "Image width in pixels"),
        257: ("img_height", "Image height in pixels"),
        258: ("bits_per_sample", "Bits per sample"),
        259: ("compression", "Compression method"),
        262: ("photo_interpret", "Photometric interpretation"),
        263: ("thres", "Thresholding"),
        264: ("cell_width", "Cellule width"),
        265: ("cell_height", "Cellule height"),
        266: ("fill_order", "Fill order"),
        269: ("doc_name", "Document name"),
        270: ("description", "Image description"),
        271: ("make", "Make"),
        272: ("model", "Model"),
        273: ("strip_ofs", "Strip offsets"),
        274: ("orientation", "Orientation"),
        277: ("sample_pixel", "Samples per pixel"),
        278: ("row_per_strip", "Rows per strip"),
        279: ("strip_byte", "Strip byte counts"),
        280: ("min_sample_value", "Min sample value"),
        281: ("max_sample_value", "Max sample value"),
        282: ("xres", "X resolution"),
        283: ("yres", "Y resolution"),
        284: ("planar_conf", "Planar configuration"),
        285: ("page_name", "Page name"),
        286: ("xpos", "X position"),
        287: ("ypos", "Y position"),
        288: ("free_ofs", "Free offsets"),
        289: ("free_byte", "Free byte counts"),
        290: ("gray_resp_unit", "Gray response unit"),
        291: ("gray_resp_curve", "Gray response curve"),
        292: ("group3_opt", "Group 3 options"),
        293: ("group4_opt", "Group 4 options"),
        296: ("res_unit", "Resolution unit"),
        297: ("page_nb", "Page number"),
        301: ("color_respt_curve", "Color response curves"),
        305: ("software", "Software"),
        306: ("date_time", "Date time"),
        315: ("artist", "Artist"),
        316: ("host_computer", "Host computer"),
        317: ("predicator", "Predicator"),
        318: ("white_pt", "White point"),
        319: ("prim_chomat", "Primary chromaticities"),
        320: ("color_map", "Color map"),
        321: ("half_tone_hints", "Halftone Hints"),
        322: ("tile_width", "TileWidth"),
        323: ("tile_length", "TileLength"),
        324: ("tile_offsets", "TileOffsets"),
        325: ("tile_byte_counts", "TileByteCounts"),
        332: ("ink_set", "InkSet"),
        333: ("ink_names", "InkNames"),
        334: ("number_of_inks", "NumberOfInks"),
        336: ("dot_range", "DotRange"),
        337: ("target_printer", "TargetPrinter"),
        338: ("extra_samples", "ExtraSamples"),
        339: ("sample_format", "SampleFormat"),
        340: ("smin_sample_value", "SMinSampleValue"),
        341: ("smax_sample_value", "SMaxSampleValue"),
        342: ("transfer_range", "TransferRange"),
        512: ("jpeg_proc", "JPEGProc"),
        513: ("jpeg_interchange_format", "JPEGInterchangeFormat"),
        514: ("jpeg_interchange_format_length", "JPEGInterchangeFormatLength"),
        515: ("jpeg_restart_interval", "JPEGRestartInterval"),
        517: ("jpeg_lossless_predictors", "JPEGLosslessPredictors"),
        518: ("jpeg_point_transforms", "JPEGPointTransforms"),
        519: ("jpeg_qtables", "JPEGQTables"),
        520: ("jpeg_dctables", "JPEGDCTables"),
        521: ("jpeg_actables", "JPEGACTables"),
        529: ("ycbcr_coefficients", "YCbCrCoefficients"),
        530: ("ycbcr_subsampling", "YCbCrSubSampling"),
        531: ("ycbcr_positioning", "YCbCrPositioning"),
        532: ("reference_blackwhite", "ReferenceBlackWhite"),
        33432: ("copyright", "Copyright"),
        0x8769: ("ifd_pointer", "Pointer to next IFD entry"),
    }
    TAG_NAME = createDict(TAG_INFO, 0)

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        tag = self["tag"].value
        if tag in self.TAG_INFO:
            self._name, self._description = self.TAG_INFO[tag]
        else:
            self._parser = None

class IFD(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = 16 + self["count"].value * IFDEntry.static_size
        self._has_offset = False

    def createFields(self):
        yield UInt16(self, "count")
        if MAX_COUNT < self["count"].value:
            raise ParserError("TIFF IFD: Invalid count (%s)"
                % self["count"].value)
        for index in xrange(self["count"].value):
            yield IFDEntry(self, "entry[]")

class ImageFile(SeekableFieldSet):
    def __init__(self, parent, name, description, ifd):
        SeekableFieldSet.__init__(self, parent, name, description, None)
        self._has_offset = False
        self._ifd = ifd

    def createFields(self):
        datas = {}
        for entry in self._ifd:
            if type(entry) != IFDEntry:
                continue
            for c in entry:
                if c.name != "offset":
                    continue
                self.seekByte(c.value, False)
                desc = "data of ifd entry " + entry.name,
                entryType = BasicIFDEntry.ENTRY_FORMAT[entry["type"].value]
                count = entry["count"].value
                if entryType == String:
                    yield String(self, entry.name, count, desc, "\0", "ISO-8859-1")
                else:    
                    d = Data(self, entry.name, desc, entryType, count)
                    datas[d.name] = d
                    yield d
                break
        # image data
        if "strip_ofs" in datas and "strip_byte" in datas:
            for i in xrange(datas["strip_byte"]._count):
                self.seekByte(datas["strip_ofs"]["value["+str(i)+"]"].value, False)
                yield Bytes(self, "strip[]", datas["strip_byte"]["value["+str(i)+"]"].value)

class Data(FieldSet):

    def __init__(self, parent, name, desc, type, count):
        size = type.static_size * count
        FieldSet.__init__(self, parent, name, desc, size)
        self._count = count
        self._type = type

    def createFields(self):
        for i in xrange(self._count):
            yield self._type(self, "value[]")

class TiffFile(RootSeekableFieldSet, Parser):
    PARSER_TAGS = {
        "id": "tiff",
        "category": "image",
        "file_ext": ("tif", "tiff"),
        "mime": (u"image/tiff",),
        "min_size": 8*8,
# TODO: Re-enable magic
        "magic": (("II\x2A\0", 0), ("MM\0\x2A", 0)),
        "description": "TIFF picture"
    }

    # Correct endian is set in constructor
    endian = LITTLE_ENDIAN

    def __init__(self, stream, **args):
        RootSeekableFieldSet.__init__(self, None, "root", stream, None, stream.askSize(self))
        if self.stream.readBytes(0, 2) == "MM":
            self.endian = BIG_ENDIAN
        Parser.__init__(self, stream, **args)

    def validate(self):
        endian = self.stream.readBytes(0, 2)
        if endian not in ("MM", "II"):
            return "Invalid endian (%r)" % endian
        if self["version"].value != 42:
            return "Unknown TIFF version"
        return True

    def createFields(self):
        yield String(self, "endian", 2, 'Endian ("II" or "MM")', charset="ASCII")
        yield UInt16(self, "version", "TIFF version number")
        offset = UInt32(self, "img_dir_ofs[]", "Next image directory offset (in bytes from the beginning)")
        yield offset
        ifds = []
        while True:
            if offset.value == 0:
                break

            self.seekByte(offset.value, relative=False)
            ifd = IFD(self, "ifd[]", "Image File Directory", None)
            ifds.append(ifd)
            yield ifd
            offset = UInt32(self, "img_dir_ofs[]", "Next image directory offset (in bytes from the beginning)")
            yield offset
        for ifd in ifds:
            image = ImageFile(self, "image[]", "Image File", ifd)
            yield image

########NEW FILE########
__FILENAME__ = wmf
"""
Hachoir parser of Microsoft Windows Metafile (WMF) file format.

Documentation:
 - Microsoft Windows Metafile; also known as: WMF,
   Enhanced Metafile, EMF, APM
   http://wvware.sourceforge.net/caolan/ora-wmf.html
 - libwmf source code:
     - include/libwmf/defs.h: enums
     - src/player/meta.h: arguments parsers
 - libemf source code

Author: Victor Stinner
Creation date: 26 december 2006
"""

MAX_FILESIZE = 50 * 1024 * 1024

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, StaticFieldSet, Enum,
    MissingField, ParserError,
    UInt32, Int32, UInt16, Int16, UInt8, NullBytes, RawBytes, String)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.tools import createDict
from lib.hachoir_parser.image.common import RGBA

POLYFILL_MODE = {1: "Alternate", 2: "Winding"}

BRUSH_STYLE = {
    0: u"Solid",
    1: u"Null",
    2: u"Hollow",
    3: u"Pattern",
    4: u"Indexed",
    5: u"DIB pattern",
    6: u"DIB pattern point",
    7: u"Pattern 8x8",
    8: u"DIB pattern 8x8",
}

HATCH_STYLE = {
    0: u"Horizontal",      # -----
    1: u"Vertical",        # |||||
    2: u"FDIAGONAL",       # \\\\\
    3: u"BDIAGONAL",       # /////
    4: u"Cross",           # +++++
    5: u"Diagonal cross",  # xxxxx
}

PEN_STYLE = {
    0: u"Solid",
    1: u"Dash",          # -------
    2: u"Dot",           # .......
    3: u"Dash dot",      # _._._._
    4: u"Dash dot dot",  # _.._.._
    5: u"Null",
    6: u"Inside frame",
    7: u"User style",
    8: u"Alternate",
}

# Binary raster operations
ROP2_DESC = {
     1: u"Black (0)",
     2: u"Not merge pen (DPon)",
     3: u"Mask not pen (DPna)",
     4: u"Not copy pen (PN)",
     5: u"Mask pen not (PDna)",
     6: u"Not (Dn)",
     7: u"Xor pen (DPx)",
     8: u"Not mask pen (DPan)",
     9: u"Mask pen (DPa)",
    10: u"Not xor pen (DPxn)",
    11: u"No operation (D)",
    12: u"Merge not pen (DPno)",
    13: u"Copy pen (P)",
    14: u"Merge pen not (PDno)",
    15: u"Merge pen (DPo)",
    16: u"White (1)",
}

def parseXY(parser):
    yield Int16(parser, "x")
    yield Int16(parser, "y")

def parseCreateBrushIndirect(parser):
    yield Enum(UInt16(parser, "brush_style"), BRUSH_STYLE)
    yield RGBA(parser, "color")
    yield Enum(UInt16(parser, "brush_hatch"), HATCH_STYLE)

def parsePenIndirect(parser):
    yield Enum(UInt16(parser, "pen_style"), PEN_STYLE)
    yield UInt16(parser, "pen_width")
    yield UInt16(parser, "pen_height")
    yield RGBA(parser, "color")

def parsePolyFillMode(parser):
    yield Enum(UInt16(parser, "operation"), POLYFILL_MODE)

def parseROP2(parser):
    yield Enum(UInt16(parser, "operation"), ROP2_DESC)

def parseObjectID(parser):
    yield UInt16(parser, "object_id")

class Point(FieldSet):
    static_size = 32
    def createFields(self):
        yield Int16(self, "x")
        yield Int16(self, "y")
    def createDescription(self):
        return "Point (%s, %s)" % (self["x"].value, self["y"].value)

def parsePolygon(parser):
    yield UInt16(parser, "count")
    for index in xrange(parser["count"].value):
        yield Point(parser, "point[]")

META = {
    0x0000: ("EOF", u"End of file", None),
    0x001E: ("SAVEDC", u"Save device context", None),
    0x0035: ("REALIZEPALETTE", u"Realize palette", None),
    0x0037: ("SETPALENTRIES", u"Set palette entries", None),
    0x00f7: ("CREATEPALETTE", u"Create palette", None),
    0x0102: ("SETBKMODE", u"Set background mode", None),
    0x0103: ("SETMAPMODE", u"Set mapping mode", None),
    0x0104: ("SETROP2", u"Set foreground mix mode", parseROP2),
    0x0106: ("SETPOLYFILLMODE", u"Set polygon fill mode", parsePolyFillMode),
    0x0107: ("SETSTRETCHBLTMODE", u"Set bitmap streching mode", None),
    0x0108: ("SETTEXTCHAREXTRA", u"Set text character extra", None),
    0x0127: ("RESTOREDC", u"Restore device context", None),
    0x012A: ("INVERTREGION", u"Invert region", None),
    0x012B: ("PAINTREGION", u"Paint region", None),
    0x012C: ("SELECTCLIPREGION", u"Select clipping region", None),
    0x012D: ("SELECTOBJECT", u"Select object", parseObjectID),
    0x012E: ("SETTEXTALIGN", u"Set text alignment", None),
    0x0142: ("CREATEDIBPATTERNBRUSH", u"Create DIB brush with specified pattern", None),
    0x01f0: ("DELETEOBJECT", u"Delete object", parseObjectID),
    0x0201: ("SETBKCOLOR", u"Set background color", None),
    0x0209: ("SETTEXTCOLOR", u"Set text color", None),
    0x020A: ("SETTEXTJUSTIFICATION", u"Set text justification", None),
    0x020B: ("SETWINDOWORG", u"Set window origin", parseXY),
    0x020C: ("SETWINDOWEXT", u"Set window extends", parseXY),
    0x020D: ("SETVIEWPORTORG", u"Set view port origin", None),
    0x020E: ("SETVIEWPORTEXT", u"Set view port extends", None),
    0x020F: ("OFFSETWINDOWORG", u"Offset window origin", None),
    0x0211: ("OFFSETVIEWPORTORG", u"Offset view port origin", None),
    0x0213: ("LINETO", u"Draw a line to", None),
    0x0214: ("MOVETO", u"Move to", None),
    0x0220: ("OFFSETCLIPRGN", u"Offset clipping rectangle", None),
    0x0228: ("FILLREGION", u"Fill region", None),
    0x0231: ("SETMAPPERFLAGS", u"Set mapper flags", None),
    0x0234: ("SELECTPALETTE", u"Select palette", None),
    0x02FB: ("CREATEFONTINDIRECT", u"Create font indirect", None),
    0x02FA: ("CREATEPENINDIRECT", u"Create pen indirect", parsePenIndirect),
    0x02FC: ("CREATEBRUSHINDIRECT", u"Create brush indirect", parseCreateBrushIndirect),
    0x0324: ("POLYGON", u"Draw a polygon", parsePolygon),
    0x0325: ("POLYLINE", u"Draw a polyline", None),
    0x0410: ("SCALEWINDOWEXT", u"Scale window extends", None),
    0x0412: ("SCALEVIEWPORTEXT", u"Scale view port extends", None),
    0x0415: ("EXCLUDECLIPRECT", u"Exclude clipping rectangle", None),
    0x0416: ("INTERSECTCLIPRECT", u"Intersect clipping rectangle", None),
    0x0418: ("ELLIPSE", u"Draw an ellipse", None),
    0x0419: ("FLOODFILL", u"Flood fill", None),
    0x041B: ("RECTANGLE", u"Draw a rectangle", None),
    0x041F: ("SETPIXEL", u"Set pixel", None),
    0x0429: ("FRAMEREGION", u"Fram region", None),
    0x0521: ("TEXTOUT", u"Draw text", None),
    0x0538: ("POLYPOLYGON", u"Draw multiple polygons", None),
    0x0548: ("EXTFLOODFILL", u"Extend flood fill", None),
    0x061C: ("ROUNDRECT", u"Draw a rounded rectangle", None),
    0x061D: ("PATBLT", u"Pattern blitting", None),
    0x0626: ("ESCAPE", u"Escape", None),
    0x06FF: ("CREATEREGION", u"Create region", None),
    0x0817: ("ARC", u"Draw an arc", None),
    0x081A: ("PIE", u"Draw a pie", None),
    0x0830: ("CHORD", u"Draw a chord", None),
    0x0940: ("DIBBITBLT", u"DIB bit blitting", None),
    0x0a32: ("EXTTEXTOUT", u"Draw text (extra)", None),
    0x0b41: ("DIBSTRETCHBLT", u"DIB stretch blitting", None),
    0x0d33: ("SETDIBTODEV", u"Set DIB to device", None),
    0x0f43: ("STRETCHDIB", u"Stretch DIB", None),
}
META_NAME = createDict(META, 0)
META_DESC = createDict(META, 1)

#----------------------------------------------------------------------------
# EMF constants

# EMF mapping modes
EMF_MAPPING_MODE = {
    1: "TEXT",
    2: "LOMETRIC",
    3: "HIMETRIC",
    4: "LOENGLISH",
    5: "HIENGLISH",
    6: "TWIPS",
    7: "ISOTROPIC",
    8: "ANISOTROPIC",
}

#----------------------------------------------------------------------------
# EMF parser

def parseEmfMappingMode(parser):
    yield Enum(Int32(parser, "mapping_mode"), EMF_MAPPING_MODE)

def parseXY32(parser):
    yield Int32(parser, "x")
    yield Int32(parser, "y")

def parseObjectID32(parser):
    yield textHandler(UInt32(parser, "object_id"), hexadecimal)

def parseBrushIndirect(parser):
    yield UInt32(parser, "ihBrush")
    yield UInt32(parser, "style")
    yield RGBA(parser, "color")
    yield Int32(parser, "hatch")

class Point16(FieldSet):
    static_size = 32
    def createFields(self):
        yield Int16(self, "x")
        yield Int16(self, "y")
    def createDescription(self):
        return "Point16: (%i,%i)" % (self["x"].value, self["y"].value)

def parsePoint16array(parser):
    yield RECT32(parser, "bounds")
    yield UInt32(parser, "count")
    for index in xrange(parser["count"].value):
        yield Point16(parser, "point[]")

def parseGDIComment(parser):
    yield UInt32(parser, "data_size")
    size = parser["data_size"].value
    if size:
        yield RawBytes(parser, "data", size)

def parseICMMode(parser):
    yield UInt32(parser, "icm_mode")

def parseExtCreatePen(parser):
    yield UInt32(parser, "ihPen")
    yield UInt32(parser, "offBmi")
    yield UInt32(parser, "cbBmi")
    yield UInt32(parser, "offBits")
    yield UInt32(parser, "cbBits")
    yield UInt32(parser, "pen_style")
    yield UInt32(parser, "width")
    yield UInt32(parser, "brush_style")
    yield RGBA(parser, "color")
    yield UInt32(parser, "hatch")
    yield UInt32(parser, "nb_style")
    for index in xrange(parser["nb_style"].value):
        yield UInt32(parser, "style")

EMF_META = {
    1: ("HEADER", u"Header", None),
    2: ("POLYBEZIER", u"Draw poly bezier", None),
    3: ("POLYGON", u"Draw polygon", None),
    4: ("POLYLINE", u"Draw polyline", None),
    5: ("POLYBEZIERTO", u"Draw poly bezier to", None),
    6: ("POLYLINETO", u"Draw poly line to", None),
    7: ("POLYPOLYLINE", u"Draw poly polyline", None),
    8: ("POLYPOLYGON", u"Draw poly polygon", None),
    9: ("SETWINDOWEXTEX", u"Set window extend EX", parseXY32),
    10: ("SETWINDOWORGEX", u"Set window origin EX", parseXY32),
    11: ("SETVIEWPORTEXTEX", u"Set viewport extend EX", parseXY32),
    12: ("SETVIEWPORTORGEX", u"Set viewport origin EX", parseXY32),
    13: ("SETBRUSHORGEX", u"Set brush org EX", None),
    14: ("EOF", u"End of file", None),
    15: ("SETPIXELV", u"Set pixel V", None),
    16: ("SETMAPPERFLAGS", u"Set mapper flags", None),
    17: ("SETMAPMODE", u"Set mapping mode", parseEmfMappingMode),
    18: ("SETBKMODE", u"Set background mode", None),
    19: ("SETPOLYFILLMODE", u"Set polyfill mode", None),
    20: ("SETROP2", u"Set ROP2", None),
    21: ("SETSTRETCHBLTMODE", u"Set stretching blitting mode", None),
    22: ("SETTEXTALIGN", u"Set text align", None),
    23: ("SETCOLORADJUSTMENT", u"Set color adjustment", None),
    24: ("SETTEXTCOLOR", u"Set text color", None),
    25: ("SETBKCOLOR", u"Set background color", None),
    26: ("OFFSETCLIPRGN", u"Offset clipping region", None),
    27: ("MOVETOEX", u"Move to EX", parseXY32),
    28: ("SETMETARGN", u"Set meta region", None),
    29: ("EXCLUDECLIPRECT", u"Exclude clipping rectangle", None),
    30: ("INTERSECTCLIPRECT", u"Intersect clipping rectangle", None),
    31: ("SCALEVIEWPORTEXTEX", u"Scale viewport extend EX", None),
    32: ("SCALEWINDOWEXTEX", u"Scale window extend EX", None),
    33: ("SAVEDC", u"Save device context", None),
    34: ("RESTOREDC", u"Restore device context", None),
    35: ("SETWORLDTRANSFORM", u"Set world transform", None),
    36: ("MODIFYWORLDTRANSFORM", u"Modify world transform", None),
    37: ("SELECTOBJECT", u"Select object", parseObjectID32),
    38: ("CREATEPEN", u"Create pen", None),
    39: ("CREATEBRUSHINDIRECT", u"Create brush indirect", parseBrushIndirect),
    40: ("DELETEOBJECT", u"Delete object", parseObjectID32),
    41: ("ANGLEARC", u"Draw angle arc", None),
    42: ("ELLIPSE", u"Draw ellipse", None),
    43: ("RECTANGLE", u"Draw rectangle", None),
    44: ("ROUNDRECT", u"Draw rounded rectangle", None),
    45: ("ARC", u"Draw arc", None),
    46: ("CHORD", u"Draw chord", None),
    47: ("PIE", u"Draw pie", None),
    48: ("SELECTPALETTE", u"Select palette", None),
    49: ("CREATEPALETTE", u"Create palette", None),
    50: ("SETPALETTEENTRIES", u"Set palette entries", None),
    51: ("RESIZEPALETTE", u"Resize palette", None),
    52: ("REALIZEPALETTE", u"Realize palette", None),
    53: ("EXTFLOODFILL", u"EXT flood fill", None),
    54: ("LINETO", u"Draw line to", parseXY32),
    55: ("ARCTO", u"Draw arc to", None),
    56: ("POLYDRAW", u"Draw poly draw", None),
    57: ("SETARCDIRECTION", u"Set arc direction", None),
    58: ("SETMITERLIMIT", u"Set miter limit", None),
    59: ("BEGINPATH", u"Begin path", None),
    60: ("ENDPATH", u"End path", None),
    61: ("CLOSEFIGURE", u"Close figure", None),
    62: ("FILLPATH", u"Fill path", None),
    63: ("STROKEANDFILLPATH", u"Stroke and fill path", None),
    64: ("STROKEPATH", u"Stroke path", None),
    65: ("FLATTENPATH", u"Flatten path", None),
    66: ("WIDENPATH", u"Widen path", None),
    67: ("SELECTCLIPPATH", u"Select clipping path", None),
    68: ("ABORTPATH", u"Arbort path", None),
    70: ("GDICOMMENT", u"GDI comment", parseGDIComment),
    71: ("FILLRGN", u"Fill region", None),
    72: ("FRAMERGN", u"Frame region", None),
    73: ("INVERTRGN", u"Invert region", None),
    74: ("PAINTRGN", u"Paint region", None),
    75: ("EXTSELECTCLIPRGN", u"EXT select clipping region", None),
    76: ("BITBLT", u"Bit blitting", None),
    77: ("STRETCHBLT", u"Stretch blitting", None),
    78: ("MASKBLT", u"Mask blitting", None),
    79: ("PLGBLT", u"PLG blitting", None),
    80: ("SETDIBITSTODEVICE", u"Set DIB bits to device", None),
    81: ("STRETCHDIBITS", u"Stretch DIB bits", None),
    82: ("EXTCREATEFONTINDIRECTW", u"EXT create font indirect W", None),
    83: ("EXTTEXTOUTA", u"EXT text out A", None),
    84: ("EXTTEXTOUTW", u"EXT text out W", None),
    85: ("POLYBEZIER16", u"Draw poly bezier (16-bit)", None),
    86: ("POLYGON16", u"Draw polygon (16-bit)", parsePoint16array),
    87: ("POLYLINE16", u"Draw polyline (16-bit)", parsePoint16array),
    88: ("POLYBEZIERTO16", u"Draw poly bezier to (16-bit)", parsePoint16array),
    89: ("POLYLINETO16", u"Draw polyline to (16-bit)", parsePoint16array),
    90: ("POLYPOLYLINE16", u"Draw poly polyline (16-bit)", None),
    91: ("POLYPOLYGON16", u"Draw poly polygon (16-bit)", parsePoint16array),
    92: ("POLYDRAW16", u"Draw poly draw (16-bit)", None),
    93: ("CREATEMONOBRUSH", u"Create monobrush", None),
    94: ("CREATEDIBPATTERNBRUSHPT", u"Create DIB pattern brush PT", None),
    95: ("EXTCREATEPEN", u"EXT create pen", parseExtCreatePen),
    96: ("POLYTEXTOUTA", u"Poly text out A", None),
    97: ("POLYTEXTOUTW", u"Poly text out W", None),
    98: ("SETICMMODE", u"Set ICM mode", parseICMMode),
    99: ("CREATECOLORSPACE", u"Create color space", None),
    100: ("SETCOLORSPACE", u"Set color space", None),
    101: ("DELETECOLORSPACE", u"Delete color space", None),
    102: ("GLSRECORD", u"GLS record", None),
    103: ("GLSBOUNDEDRECORD", u"GLS bound ED record", None),
    104: ("PIXELFORMAT", u"Pixel format", None),
}
EMF_META_NAME = createDict(EMF_META, 0)
EMF_META_DESC = createDict(EMF_META, 1)

class Function(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        if self.root.isEMF():
            self._size = self["size"].value * 8
        else:
            self._size = self["size"].value * 16

    def createFields(self):
        if self.root.isEMF():
            yield Enum(UInt32(self, "function"), EMF_META_NAME)
            yield UInt32(self, "size")
            try:
                parser = EMF_META[self["function"].value][2]
            except KeyError:
                parser = None
        else:
            yield UInt32(self, "size")
            yield Enum(UInt16(self, "function"), META_NAME)
            try:
                parser = META[self["function"].value][2]
            except KeyError:
                parser = None
        if parser:
            for field in parser(self):
                yield field
        else:
            size = (self.size - self.current_size) // 8
            if size:
                yield RawBytes(self, "data", size)

    def isValid(self):
        func = self["function"]
        return func.value in func.getEnum()

    def createDescription(self):
        if self.root.isEMF():
            return EMF_META_DESC[self["function"].value]
        try:
            return META_DESC[self["function"].value]
        except KeyError:
            return "Function %s" % self["function"].display

class RECT16(StaticFieldSet):
    format = (
        (Int16, "left"),
        (Int16, "top"),
        (Int16, "right"),
        (Int16, "bottom"),
    )
    def createDescription(self):
        return "%s: %ux%u at (%u,%u)" % (
            self.__class__.__name__,
            self["right"].value-self["left"].value,
            self["bottom"].value-self["top"].value,
            self["left"].value,
            self["top"].value)

class RECT32(RECT16):
    format = (
        (Int32, "left"),
        (Int32, "top"),
        (Int32, "right"),
        (Int32, "bottom"),
    )

class PlaceableHeader(FieldSet):
    """
    Header of Placeable Metafile (file extension .APM),
    created by Aldus Corporation
    """
    MAGIC = "\xD7\xCD\xC6\x9A\0\0"   # (magic, handle=0x0000)

    def createFields(self):
        yield textHandler(UInt32(self, "signature", "Placeable Metafiles signature (0x9AC6CDD7)"), hexadecimal)
        yield UInt16(self, "handle")
        yield RECT16(self, "rect")
        yield UInt16(self, "inch")
        yield NullBytes(self, "reserved", 4)
        yield textHandler(UInt16(self, "checksum"), hexadecimal)

class EMF_Header(FieldSet):
    MAGIC = "\x20\x45\x4D\x46\0\0"   # (magic, min_ver=0x0000)
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["size"].value * 8

    def createFields(self):
        LONG = Int32
        yield UInt32(self, "type", "Record type (always 1)")
        yield UInt32(self, "size", "Size of the header in bytes")
        yield RECT32(self, "Bounds", "Inclusive bounds")
        yield RECT32(self, "Frame", "Inclusive picture frame")
        yield textHandler(UInt32(self, "signature", "Signature ID (always 0x464D4520)"), hexadecimal)
        yield UInt16(self, "min_ver", "Minor version")
        yield UInt16(self, "maj_ver", "Major version")
        yield UInt32(self, "file_size", "Size of the file in bytes")
        yield UInt32(self, "NumOfRecords", "Number of records in the metafile")
        yield UInt16(self, "NumOfHandles", "Number of handles in the handle table")
        yield NullBytes(self, "reserved", 2)
        yield UInt32(self, "desc_size", "Size of description in 16-bit words")
        yield UInt32(self, "desc_ofst", "Offset of description string in metafile")
        yield UInt32(self, "nb_colors", "Number of color palette entries")
        yield LONG(self, "width_px", "Width of reference device in pixels")
        yield LONG(self, "height_px", "Height of reference device in pixels")
        yield LONG(self, "width_mm", "Width of reference device in millimeters")
        yield LONG(self, "height_mm", "Height of reference device in millimeters")

        # Read description (if any)
        offset = self["desc_ofst"].value
        current = (self.absolute_address + self.current_size) // 8
        size = self["desc_size"].value * 2
        if offset == current and size:
            yield String(self, "description", size, charset="UTF-16-LE", strip="\0 ")

        # Read padding (if any)
        size = self["size"].value - self.current_size//8
        if size:
            yield RawBytes(self, "padding", size)

class WMF_File(Parser):
    PARSER_TAGS = {
        "id": "wmf",
        "category": "image",
        "file_ext": ("wmf", "apm", "emf"),
        "mime": (
            u"image/wmf", u"image/x-wmf", u"image/x-win-metafile",
            u"application/x-msmetafile", u"application/wmf", u"application/x-wmf",
            u"image/x-emf"),
        "magic": (
            (PlaceableHeader.MAGIC, 0),
            (EMF_Header.MAGIC, 40*8),
            # WMF: file_type=memory, header size=9, version=3.0
            ("\0\0\x09\0\0\3", 0),
            # WMF: file_type=disk, header size=9, version=3.0
            ("\1\0\x09\0\0\3", 0),
        ),
        "min_size": 40*8,
        "description": u"Microsoft Windows Metafile (WMF)",
    }
    endian = LITTLE_ENDIAN
    FILE_TYPE = {0: "memory", 1: "disk"}

    def validate(self):
        if self.isEMF():
            # Check EMF header
            emf = self["emf_header"]
            if emf["signature"].value != 0x464D4520:
                return "Invalid signature"
            if emf["type"].value != 1:
                return "Invalid record type"
            if emf["reserved"].value != "\0\0":
                return "Invalid reserved"
        else:
            # Check AMF header
            if self.isAPM():
                amf = self["amf_header"]
                if amf["handle"].value != 0:
                    return "Invalid handle"
                if amf["reserved"].value != "\0\0\0\0":
                    return "Invalid reserved"

            # Check common header
            if self["file_type"].value not in (0, 1):
                return "Invalid file type"
            if self["header_size"].value != 9:
                return "Invalid header size"
            if self["nb_params"].value != 0:
                return "Invalid number of parameters"

        # Check first functions
        for index in xrange(5):
            try:
                func = self["func[%u]" % index]
            except MissingField:
                if self.done:
                    return True
                return "Unable to get function #%u" % index
            except ParserError:
                return "Unable to create function #%u" % index

            # Check first frame values
            if not func.isValid():
                return "Function #%u is invalid" % index
        return True

    def createFields(self):
        if self.isEMF():
            yield EMF_Header(self, "emf_header")
        else:
            if self.isAPM():
                yield PlaceableHeader(self, "amf_header")
            yield Enum(UInt16(self, "file_type"), self.FILE_TYPE)
            yield UInt16(self, "header_size", "Size of header in 16-bit words (always 9)")
            yield UInt8(self, "win_ver_min", "Minor version of Microsoft Windows")
            yield UInt8(self, "win_ver_maj", "Major version of Microsoft Windows")
            yield UInt32(self, "file_size", "Total size of the metafile in 16-bit words")
            yield UInt16(self, "nb_obj", "Number of objects in the file")
            yield UInt32(self, "max_record_size", "The size of largest record in 16-bit words")
            yield UInt16(self, "nb_params", "Not Used (always 0)")

        while not(self.eof):
            yield Function(self, "func[]")

    def isEMF(self):
        """File is in EMF format?"""
        if 1 <= self.current_length:
            return self[0].name == "emf_header"
        if self.size < 44*8:
            return False
        magic = EMF_Header.MAGIC
        return self.stream.readBytes(40*8, len(magic)) == magic

    def isAPM(self):
        """File is in Aldus Placeable Metafiles format?"""
        if 1 <= self.current_length:
            return self[0].name == "amf_header"
        else:
            magic = PlaceableHeader.MAGIC
            return (self.stream.readBytes(0, len(magic)) == magic)

    def createDescription(self):
        if self.isEMF():
            return u"Microsoft Enhanced Metafile (EMF) picture"
        elif self.isAPM():
            return u"Aldus Placeable Metafile (APM) picture"
        else:
            return u"Microsoft Windows Metafile (WMF) picture"

    def createMimeType(self):
        if self.isEMF():
            return u"image/x-emf"
        else:
            return u"image/wmf"

    def createContentSize(self):
        if self.isEMF():
            return None
        start = self["func[0]"].absolute_address
        end = self.stream.searchBytes("\3\0\0\0\0\0", start, MAX_FILESIZE * 8)
        if end is not None:
            return end + 6*8
        return None


########NEW FILE########
__FILENAME__ = xcf
"""
Gimp image parser (XCF file, ".xcf" extension).

You can find informations about XCF file in Gimp source code. URL to read
CVS online:
  http://cvs.gnome.org/viewcvs/gimp/app/xcf/
  \--> files xcf-read.c and xcf-load.c

Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (StaticFieldSet, FieldSet, ParserError,
    UInt8, UInt32, Enum, Float32, String, PascalString32, RawBytes)
from lib.hachoir_parser.image.common import RGBA
from lib.hachoir_core.endian import NETWORK_ENDIAN

class XcfCompression(FieldSet):
    static_size = 8
    COMPRESSION_NAME = {
        0: u"None",
        1: u"RLE",
        2: u"Zlib",
        3: u"Fractal"
    }

    def createFields(self):
        yield Enum(UInt8(self, "compression",  "Compression method"), self.COMPRESSION_NAME)

class XcfResolution(StaticFieldSet):
    format = (
        (Float32, "xres", "X resolution in DPI"),
        (Float32, "yres", "Y resolution in DPI")
    )

class XcfTattoo(StaticFieldSet):
    format = ((UInt32, "tattoo", "Tattoo"),)

class LayerOffsets(StaticFieldSet):
    format = (
        (UInt32, "ofst_x", "Offset X"),
        (UInt32, "ofst_y", "Offset Y")
    )

class LayerMode(FieldSet):
    static_size = 32
    MODE_NAME = {
         0: u"Normal",
         1: u"Dissolve",
         2: u"Behind",
         3: u"Multiply",
         4: u"Screen",
         5: u"Overlay",
         6: u"Difference",
         7: u"Addition",
         8: u"Subtract",
         9: u"Darken only",
        10: u"Lighten only",
        11: u"Hue",
        12: u"Saturation",
        13: u"Color",
        14: u"Value",
        15: u"Divide",
        16: u"Dodge",
        17: u"Burn",
        18: u"Hard light",
        19: u"Soft light",
        20: u"Grain extract",
        21: u"Grain merge",
        22: u"Color erase"
    }

    def createFields(self):
        yield Enum(UInt32(self, "mode", "Layer mode"), self.MODE_NAME)

class GimpBoolean(UInt32):
    def __init__(self, parent, name):
        UInt32.__init__(self, parent, name)

    def createValue(self):
        return 1 == UInt32.createValue(self)

class XcfUnit(StaticFieldSet):
    format = ((UInt32, "unit", "Unit"),)

class XcfParasiteEntry(FieldSet):
    def createFields(self):
        yield PascalString32(self, "name", "Name", strip="\0", charset="UTF-8")
        yield UInt32(self, "flags", "Flags")
        yield PascalString32(self, "data", "Data", strip=" \0", charset="UTF-8")

class XcfLevel(FieldSet):
    def createFields(self):
        yield UInt32(self, "width", "Width in pixel")
        yield UInt32(self, "height", "Height in pixel")
        yield UInt32(self, "offset", "Offset")
        offset = self["offset"].value
        if offset == 0:
            return
        data_offsets = []
        while (self.absolute_address + self.current_size)/8 < offset:
            chunk = UInt32(self, "data_offset[]", "Data offset")
            yield chunk
            if chunk.value == 0:
                break
            data_offsets.append(chunk)
        if (self.absolute_address + self.current_size)/8 != offset:
            raise ParserError("Problem with level offset.")
        previous = offset
        for chunk in data_offsets:
            data_offset = chunk.value
            size = data_offset - previous
            yield RawBytes(self, "data[]", size, "Data content of %s" % chunk.name)
            previous = data_offset

class XcfHierarchy(FieldSet):
    def createFields(self):
        yield UInt32(self, "width", "Width")
        yield UInt32(self, "height", "Height")
        yield UInt32(self, "bpp", "Bits/pixel")

        offsets = []
        while True:
            chunk = UInt32(self, "offset[]", "Level offset")
            yield chunk
            if chunk.value == 0:
                break
            offsets.append(chunk.value)
        for offset in offsets:
            padding = self.seekByte(offset, relative=False)
            if padding is not None:
                yield padding
            yield XcfLevel(self, "level[]", "Level")
#        yield XcfChannel(self, "channel[]", "Channel"))

class XcfChannel(FieldSet):
    def createFields(self):
        yield UInt32(self, "width", "Channel width")
        yield UInt32(self, "height", "Channel height")
        yield PascalString32(self, "name", "Channel name", strip="\0", charset="UTF-8")
        for field in readProperties(self):
            yield field
        yield UInt32(self, "hierarchy_ofs", "Hierarchy offset")
        yield XcfHierarchy(self, "hierarchy", "Hierarchy")

    def createDescription(self):
         return 'Channel "%s"' % self["name"].value

class XcfLayer(FieldSet):
    def createFields(self):
        yield UInt32(self, "width", "Layer width in pixels")
        yield UInt32(self, "height", "Layer height in pixels")
        yield Enum(UInt32(self, "type", "Layer type"), XcfFile.IMAGE_TYPE_NAME)
        yield PascalString32(self, "name", "Layer name", strip="\0", charset="UTF-8")
        for prop in readProperties(self):
            yield prop

        # --
        # TODO: Hack for Gimp 1.2 files
        # --

        yield UInt32(self, "hierarchy_ofs", "Hierarchy offset")
        yield UInt32(self, "mask_ofs", "Layer mask offset")
        padding = self.seekByte(self["hierarchy_ofs"].value, relative=False)
        if padding is not None:
            yield padding
        yield XcfHierarchy(self, "hierarchy", "Hierarchy")
        # TODO: Read layer mask if needed: self["mask_ofs"].value != 0

    def createDescription(self):
        return 'Layer "%s"' % self["name"].value

class XcfParasites(FieldSet):
    def createFields(self):
        size = self["../size"].value * 8
        while self.current_size < size:
            yield XcfParasiteEntry(self, "parasite[]", "Parasite")

class XcfProperty(FieldSet):
    PROP_COMPRESSION = 17
    PROP_RESOLUTION = 19
    PROP_PARASITES = 21
    TYPE_NAME = {
         0: u"End",
         1: u"Colormap",
         2: u"Active layer",
         3: u"Active channel",
         4: u"Selection",
         5: u"Floating selection",
         6: u"Opacity",
         7: u"Mode",
         8: u"Visible",
         9: u"Linked",
        10: u"Lock alpha",
        11: u"Apply mask",
        12: u"Edit mask",
        13: u"Show mask",
        14: u"Show masked",
        15: u"Offsets",
        16: u"Color",
        17: u"Compression",
        18: u"Guides",
        19: u"Resolution",
        20: u"Tattoo",
        21: u"Parasites",
        22: u"Unit",
        23: u"Paths",
        24: u"User unit",
        25: u"Vectors",
        26: u"Text layer flags",
    }

    handler = {
         6: RGBA,
         7: LayerMode,
         8: GimpBoolean,
         9: GimpBoolean,
        10: GimpBoolean,
        11: GimpBoolean,
        12: GimpBoolean,
        13: GimpBoolean,
        15: LayerOffsets,
        17: XcfCompression,
        19: XcfResolution,
        20: XcfTattoo,
        21: XcfParasites,
        22: XcfUnit
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = (8 + self["size"].value) * 8

    def createFields(self):
        yield Enum(UInt32(self, "type",  "Property type"), self.TYPE_NAME)
        yield UInt32(self, "size", "Property size")

        size = self["size"].value
        if 0 < size:
            cls = self.handler.get(self["type"].value, None)
            if cls:
                yield cls(self, "data", size=size*8)
            else:
                yield RawBytes(self, "data", size, "Data")

    def createDescription(self):
        return "Property: %s" % self["type"].display

def readProperties(parser):
    while True:
        prop = XcfProperty(parser, "property[]")
        yield prop
        if prop["type"].value == 0:
            return

class XcfFile(Parser):
    PARSER_TAGS = {
        "id": "xcf",
        "category": "image",
        "file_ext": ("xcf",),
        "mime": (u"image/x-xcf", u"application/x-gimp-image"),
        "min_size": (26 + 8 + 4 + 4)*8, # header+empty property+layer offset+channel offset
        "magic": (
            ('gimp xcf file\0', 0),
            ('gimp xcf v002\0', 0),
        ),
        "description": "Gimp (XCF) picture"
    }
    endian = NETWORK_ENDIAN
    IMAGE_TYPE_NAME = {
        0: u"RGB",
        1: u"Gray",
        2: u"Indexed"
    }

    def validate(self):
        if self.stream.readBytes(0, 14) not in ('gimp xcf file\0', 'gimp xcf v002\0'):
            return "Wrong signature"
        return True

    def createFields(self):
        # Read signature
        yield String(self, "signature", 14,  "Gimp picture signature (ends with nul byte)", charset="ASCII")

        # Read image general informations (width, height, type)
        yield UInt32(self, "width", "Image width")
        yield UInt32(self, "height", "Image height")
        yield Enum(UInt32(self, "type", "Image type"), self.IMAGE_TYPE_NAME)
        for prop in readProperties(self):
            yield prop

        # Read layer offsets
        layer_offsets = []
        while True:
            chunk = UInt32(self, "layer_offset[]", "Layer offset")
            yield chunk
            if chunk.value == 0:
                break
            layer_offsets.append(chunk.value)

        # Read channel offsets
        channel_offsets = []
        while True:
            chunk = UInt32(self, "channel_offset[]", "Channel offset")
            yield chunk
            if chunk.value == 0:
                break
            channel_offsets.append(chunk.value)

        # Read layers
        for index, offset in enumerate(layer_offsets):
            if index+1 < len(layer_offsets):
                size = (layer_offsets[index+1] - offset) * 8
            else:
                size = None
            padding = self.seekByte(offset, relative=False)
            if padding:
                yield padding
            yield XcfLayer(self, "layer[]", size=size)

        # Read channels
        for index, offset in enumerate(channel_offsets):
            if index+1 < len(channel_offsets):
                size = (channel_offsets[index+1] - offset) * 8
            else:
                size = None
            padding = self.seekByte(offset, relative=False)
            if padding is not None:
                yield padding
            yield XcfChannel(self, "channel[]", "Channel", size=size)


########NEW FILE########
__FILENAME__ = bplist
"""
Apple/NeXT Binary Property List (BPLIST) parser.

Also includes a .createXML() function which produces an XML representation of the object.
Note that it will discard unknown objects, nulls and fill values, but should work for most files.

Documents:
- CFBinaryPList.c
  http://src.gnu-darwin.org/DarwinSourceArchive/expanded/CF/CF-299/Parsing.subproj/CFBinaryPList.c
- ForFoundationOnly.h (for structure formats)
  http://src.gnu-darwin.org/DarwinSourceArchive/expanded/CF/CF-299/Base.subproj/ForFoundationOnly.h
- XML <-> BPList converter
  http://scw.us/iPhone/plutil/plutil.pl
Author: Robert Xiao
Created: 2008-09-21
"""

from lib.hachoir_parser import HachoirParser
from lib.hachoir_core.field import (RootSeekableFieldSet, FieldSet, Enum,
Bits, GenericInteger, Float32, Float64, UInt8, UInt64, Bytes, NullBytes, RawBytes, String)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import displayHandler
from lib.hachoir_core.tools import humanDatetime
from datetime import datetime, timedelta

class BPListTrailer(FieldSet):
    def createFields(self):
        yield NullBytes(self, "unused", 6)
        yield UInt8(self, "offsetIntSize", "Size (in bytes) of offsets in the offset table")
        yield UInt8(self, "objectRefSize", "Size (in bytes) of object numbers in object references")
        yield UInt64(self, "numObjects", "Number of objects in this file")
        yield UInt64(self, "topObject", "Top-level object reference")
        yield UInt64(self, "offsetTableOffset", "File offset to the offset table")

    def createDescription(self):
        return "Binary PList trailer"

class BPListOffsetTable(FieldSet):
    def createFields(self):
        size = self["../trailer/offsetIntSize"].value*8
        for i in range(self["../trailer/numObjects"].value):
            yield Bits(self, "offset[]", size)

class BPListSize(FieldSet):
    def createFields(self):
        yield Bits(self, "size", 4)
        if self['size'].value == 0xF:
            yield BPListObject(self, "fullsize")

    def createValue(self):
        if 'fullsize' in self:
            return self['fullsize'].value
        else:
            return self['size'].value

class BPListObjectRef(GenericInteger):
    def __init__(self, parent, name, description=None):
        size = parent['/trailer/objectRefSize'].value*8
        GenericInteger.__init__(self, parent, name, False, size, description)

    def getRef(self):
        return self.parent['/object[' + str(self.value) + ']']

    def createDisplay(self):
        return self.getRef().display

    def createXML(self, prefix=''):
        return self.getRef().createXML(prefix)

class BPListArray(FieldSet):
    def __init__(self, parent, name, size, description=None):
        FieldSet.__init__(self, parent, name, description=description)
        self.numels = size

    def createFields(self):
        for i in range(self.numels):
            yield BPListObjectRef(self, "ref[]")

    def createValue(self):
        return self.array('ref')

    def createDisplay(self):
        return '[' + ', '.join([x.display for x in self.value]) + ']'

    def createXML(self,prefix=''):
        return prefix + '<array>\n' + ''.join([x.createXML(prefix + '\t' ) + '\n' for x in self.value]) + prefix + '</array>'

class BPListDict(FieldSet):
    def __init__(self, parent, name, size, description=None):
        FieldSet.__init__(self, parent, name, description=description)
        self.numels = size

    def createFields(self):
        for i in range(self.numels):
            yield BPListObjectRef(self, "keyref[]")
        for i in range(self.numels):
            yield BPListObjectRef(self, "valref[]")

    def createValue(self):
        return zip(self.array('keyref'),self.array('valref'))

    def createDisplay(self):
        return '{' + ', '.join(['%s: %s'%(k.display,v.display) for k,v in self.value]) + '}'

    def createXML(self, prefix=''):
        return prefix + '<dict>\n' + ''.join(['%s\t<key>%s</key>\n%s\n'%(prefix,k.getRef().value.encode('utf-8'),v.createXML(prefix + '\t')) for k,v in self.value]) + prefix + '</dict>'

class BPListObject(FieldSet):
    def createFields(self):
        yield Enum(Bits(self, "marker_type", 4),
                    {0: "Simple",
                     1: "Int",
                     2: "Real",
                     3: "Date",
                     4: "Data",
                     5: "ASCII String",
                     6: "UTF-16-BE String",
                     8: "UID",
                     10: "Array",
                     13: "Dict",})
        markertype = self['marker_type'].value
        if markertype == 0:
            # Simple (Null)
            yield Enum(Bits(self, "value", 4),
                        {0: "Null",
                         8: "False",
                         9: "True",
                         15: "Fill Byte",})
            if self['value'].display == "False":
                self.xml=lambda prefix:prefix + "<false/>"
            elif self['value'].display == "True":
                self.xml=lambda prefix:prefix + "<true/>"
            else:
                self.xml=lambda prefix:prefix + ""

        elif markertype == 1:
            # Int
            yield Bits(self, "size", 4, "log2 of number of bytes")
            size=self['size'].value
            # 8-bit (size=0), 16-bit (size=1) and 32-bit (size=2) numbers are unsigned
            # 64-bit (size=3) numbers are signed
            yield GenericInteger(self, "value", (size>=3), (2**size)*8)
            self.xml=lambda prefix:prefix + "<integer>%s</integer>"%self['value'].value

        elif markertype == 2:
            # Real
            yield Bits(self, "size", 4, "log2 of number of bytes")
            if self['size'].value == 2: # 2**2 = 4 byte float
                yield Float32(self, "value")
            elif self['size'].value == 3: # 2**3 = 8 byte float
                yield Float64(self, "value")
            else:
                # FIXME: What is the format of the real?
                yield Bits(self, "value", (2**self['size'].value)*8)
            self.xml=lambda prefix:prefix + "<real>%s</real>"%self['value'].value

        elif markertype == 3:
            # Date
            yield Bits(self, "extra", 4, "Extra value, should be 3")
            cvt_time=lambda v:datetime(2001,1,1) + timedelta(seconds=v)
            yield displayHandler(Float64(self, "value"),lambda x:humanDatetime(cvt_time(x)))
            self.xml=lambda prefix:prefix + "<date>%s</date>"%(cvt_time(self['value'].value).isoformat())

        elif markertype == 4:
            # Data
            yield BPListSize(self, "size")
            if self['size'].value:
                yield Bytes(self, "value", self['size'].value)
                self.xml=lambda prefix:prefix + "<data>\n%s\n%s</data>"%(self['value'].value.encode('base64').strip(),prefix)
            else:
                self.xml=lambda prefix:prefix + '<data></data>'

        elif markertype == 5:
            # ASCII String
            yield BPListSize(self, "size")
            if self['size'].value:
                yield String(self, "value", self['size'].value, charset="ASCII")
                self.xml=lambda prefix:prefix + "<string>%s</string>"%(self['value'].value.encode('iso-8859-1'))
            else:
                self.xml=lambda prefix:prefix + '<string></string>'

        elif markertype == 6:
            # UTF-16-BE String
            yield BPListSize(self, "size")
            if self['size'].value:
                yield String(self, "value", self['size'].value*2, charset="UTF-16-BE")
                self.xml=lambda prefix:prefix + "<string>%s</string>"%(self['value'].value.encode('utf-8'))
            else:
                self.xml=lambda prefix:prefix + '<string></string>'

        elif markertype == 8:
            # UID
            yield Bits(self, "size", 4, "Number of bytes minus 1")
            yield GenericInteger(self, "value", False, (self['size'].value + 1)*8)
            self.xml=lambda prefix:prefix + "" # no equivalent?

        elif markertype == 10:
            # Array
            yield BPListSize(self, "size")
            size = self['size'].value
            if size:
                yield BPListArray(self, "value", size)
                self.xml=lambda prefix:self['value'].createXML(prefix)

        elif markertype == 13:
            # Dict
            yield BPListSize(self, "size")
            yield BPListDict(self, "value", self['size'].value)
            self.xml=lambda prefix:self['value'].createXML(prefix)

        else:
            yield Bits(self, "value", 4)
            self.xml=lambda prefix:''

    def createValue(self):
        if 'value' in self:
            return self['value'].value
        elif self['marker_type'].value in [4,5,6]:
            return u''
        else:
            return None

    def createDisplay(self):
        if 'value' in self:
            return unicode(self['value'].display)
        elif self['marker_type'].value in [4,5,6]:
            return u''
        else:
            return None

    def createXML(self, prefix=''):
        if 'value' in self:
            try:
                return self.xml(prefix)
            except AttributeError:
                return ''
        return ''

    def getFieldType(self):
        return '%s<%s>'%(FieldSet.getFieldType(self), self['marker_type'].display)

class BPList(HachoirParser, RootSeekableFieldSet):
    endian = BIG_ENDIAN
    MAGIC = "bplist00"
    PARSER_TAGS = {
        "id": "bplist",
        "category": "misc",
        "file_ext": ("plist",),
        "magic": ((MAGIC, 0),),
        "min_size": 8 + 32, # bplist00 + 32-byte trailer
        "description": "Apple/NeXT Binary Property List",
    }

    def __init__(self, stream, **args):
        RootSeekableFieldSet.__init__(self, None, "root", stream, None, stream.askSize(self))
        HachoirParser.__init__(self, stream, **args)

    def validate(self):
        if self.stream.readBytes(0, len(self.MAGIC)) != self.MAGIC:
            return "Invalid magic"
        return True

    def createFields(self):
        yield Bytes(self, "magic", 8, "File magic (bplist00)")
        if self.size:
            self.seekByte(self.size//8-32, True)
        else:
            # FIXME: UNTESTED
            while True:
                try:
                    self.seekByte(1024)
                except:
                    break
            self.seekByte(self.size//8-32)
        yield BPListTrailer(self, "trailer")
        self.seekByte(self['trailer/offsetTableOffset'].value)
        yield BPListOffsetTable(self, "offset_table")
        for i in self.array("offset_table/offset"):
            if self.current_size > i.value*8:
                self.seekByte(i.value)
            elif self.current_size < i.value*8:
                # try to detect files with gaps or unparsed content
                yield RawBytes(self, "padding[]", i.value-self.current_size//8)
            yield BPListObject(self, "object[]")

    def createXML(self, prefix=''):
        return '''<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple Computer//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
''' + self['/object[' + str(self['/trailer/topObject'].value) + ']'].createXML(prefix) + '''
</plist>'''


########NEW FILE########
__FILENAME__ = chm
"""
InfoTech Storage Format (ITSF) parser, used by Microsoft's HTML Help (.chm)

Document:
- Microsoft's HTML Help (.chm) format
  http://www.wotsit.org (search "chm")
- chmlib library
  http://www.jedrea.com/chmlib/

Author: Victor Stinner
Creation date: 2007-03-04
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (Field, FieldSet, ParserError,
    Int32, UInt32, UInt64,
    RawBytes, PaddingBytes,
    Enum, String)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_parser.common.win32 import GUID
from lib.hachoir_parser.common.win32_lang_id import LANGUAGE_ID
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler

class CWord(Field):
    """
    Compressed double-word
    """
    def __init__(self, parent, name, description=None):
        Field.__init__(self, parent, name, 8, description)

        endian = self._parent.endian
        stream = self._parent.stream
        addr = self.absolute_address

        value = 0
        byte = stream.readBits(addr, 8, endian)
        while byte & 0x80:
            value <<= 7
            value += (byte & 0x7f)
            self._size += 8
            if 64 < self._size:
                raise ParserError("CHM: CWord is limited to 64 bits")
            addr += 8
            byte = stream.readBits(addr, 8, endian)
        value += byte
        self.createValue = lambda: value

class Filesize_Header(FieldSet):
    def createFields(self):
        yield textHandler(UInt32(self, "unknown[]", "0x01FE"), hexadecimal)
        yield textHandler(UInt32(self, "unknown[]", "0x0"), hexadecimal)
        yield filesizeHandler(UInt64(self, "file_size"))
        yield textHandler(UInt32(self, "unknown[]", "0x0"), hexadecimal)
        yield textHandler(UInt32(self, "unknown[]", "0x0"), hexadecimal)

class ITSP(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["size"].value * 8

    def createFields(self):
        yield String(self, "magic", 4, "ITSP", charset="ASCII")
        yield UInt32(self, "version", "Version (=1)")
        yield filesizeHandler(UInt32(self, "size", "Length (in bytes) of the directory header (84)"))
        yield UInt32(self, "unknown[]", "(=10)")
        yield filesizeHandler(UInt32(self, "block_size", "Directory block size"))
        yield UInt32(self, "density", "Density of quickref section, usually 2")
        yield UInt32(self, "index_depth", "Depth of the index tree")
        yield Int32(self, "nb_dir", "Chunk number of root index chunk")
        yield UInt32(self, "first_pmgl", "Chunk number of first PMGL (listing) chunk")
        yield UInt32(self, "last_pmgl", "Chunk number of last PMGL (listing) chunk")
        yield Int32(self, "unknown[]", "-1")
        yield UInt32(self, "nb_dir_chunk", "Number of directory chunks (total)")
        yield Enum(UInt32(self, "lang_id", "Windows language ID"), LANGUAGE_ID)
        yield GUID(self, "system_uuid", "{5D02926A-212E-11D0-9DF9-00A0C922E6EC}")
        yield filesizeHandler(UInt32(self, "size2", "Same value than size"))
        yield Int32(self, "unknown[]", "-1")
        yield Int32(self, "unknown[]", "-1")
        yield Int32(self, "unknown[]", "-1")

class ITSF(FieldSet):
    def createFields(self):
        yield String(self, "magic", 4, "ITSF", charset="ASCII")
        yield UInt32(self, "version")
        yield UInt32(self, "header_size", "Total header length (in bytes)")
        yield UInt32(self, "one")
        yield UInt32(self, "last_modified")
        yield Enum(UInt32(self, "lang_id", "Windows Language ID"), LANGUAGE_ID)
        yield GUID(self, "dir_uuid", "{7C01FD10-7BAA-11D0-9E0C-00A0-C922-E6EC}")
        yield GUID(self, "stream_uuid", "{7C01FD11-7BAA-11D0-9E0C-00A0-C922-E6EC}")
        yield UInt64(self, "filesize_offset")
        yield filesizeHandler(UInt64(self, "filesize_len"))
        yield UInt64(self, "dir_offset")
        yield filesizeHandler(UInt64(self, "dir_len"))
        if 3 <= self["version"].value:
            yield UInt64(self, "data_offset")

class PMGL_Entry(FieldSet):
    def createFields(self):
        yield CWord(self, "name_len")
        yield String(self, "name", self["name_len"].value, charset="UTF-8")
        yield CWord(self, "space")
        yield CWord(self, "start")
        yield filesizeHandler(CWord(self, "length"))

    def createDescription(self):
        return "%s (%s)" % (self["name"].value, self["length"].display)

class PMGL(FieldSet):
    def createFields(self):
        # Header
        yield String(self, "magic", 4, "PMGL", charset="ASCII")
        yield filesizeHandler(Int32(self, "free_space",
            "Length of free space and/or quickref area at end of directory chunk"))
        yield Int32(self, "unknown")
        yield Int32(self, "previous", "Chunk number of previous listing chunk")
        yield Int32(self, "next", "Chunk number of previous listing chunk")

        # Entries
        stop = self.size - self["free_space"].value * 8
        while self.current_size < stop:
            yield PMGL_Entry(self, "entry[]")

        # Padding
        padding = (self.size - self.current_size) // 8
        if padding:
            yield PaddingBytes(self, "padding", padding)

class PMGI_Entry(FieldSet):
    def createFields(self):
        yield CWord(self, "name_len")
        yield String(self, "name", self["name_len"].value, charset="UTF-8")
        yield CWord(self, "page")

    def createDescription(self):
        return "%s (page #%u)" % (self["name"].value, self["page"].value)

class PMGI(FieldSet):
    def createFields(self):
        yield String(self, "magic", 4, "PMGI", charset="ASCII")
        yield filesizeHandler(UInt32(self, "free_space",
            "Length of free space and/or quickref area at end of directory chunk"))

        stop = self.size - self["free_space"].value * 8
        while self.current_size < stop:
            yield PMGI_Entry(self, "entry[]")

        padding = (self.size - self.current_size) // 8
        if padding:
            yield PaddingBytes(self, "padding", padding)

class Directory(FieldSet):
    def createFields(self):
        yield ITSP(self, "itsp")
        block_size = self["itsp/block_size"].value * 8

        nb_dir = self["itsp/nb_dir"].value

        if nb_dir < 0:
            nb_dir = 1
        for index in xrange(nb_dir):
            yield PMGL(self, "pmgl[]", size=block_size)

        if self.current_size < self.size:
            yield PMGI(self, "pmgi", size=block_size)

class ChmFile(Parser):
    PARSER_TAGS = {
        "id": "chm",
        "category": "misc",
        "file_ext": ("chm",),
        "min_size": 4*8,
        "magic": (("ITSF\3\0\0\0", 0),),
        "description": "Microsoft's HTML Help (.chm)",
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 4) != "ITSF":
            return "Invalid magic"
        if self["itsf/version"].value != 3:
            return "Invalid version"
        return True

    def createFields(self):
        yield ITSF(self, "itsf")
        yield Filesize_Header(self, "file_size", size=self["itsf/filesize_len"].value*8)

        padding = self.seekByte(self["itsf/dir_offset"].value)
        if padding:
            yield padding
        yield Directory(self, "dir", size=self["itsf/dir_len"].value*8)

        size = (self.size - self.current_size) // 8
        if size:
            yield RawBytes(self, "raw_end", size)

    def createContentSize(self):
        return self["file_size/file_size"].value * 8


########NEW FILE########
__FILENAME__ = common
from lib.hachoir_core.field import StaticFieldSet, Float32

class Vertex(StaticFieldSet):
    format = ((Float32, "x"), (Float32, "y"), (Float32, "z"))

    def createValue(self):
        return (self["x"].value, self["y"].value, self["z"].value)

class MapUV(StaticFieldSet):
    format = ((Float32, "u"), (Float32, "v"))

    def createValue(self):
        return (self["u"].value, self["v"].value)

########NEW FILE########
__FILENAME__ = file_3do
# -*- coding: utf-8 -*-

"""
3do model parser.

Author: Cyril Zorin
Creation date: 28 september 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt32, Int32, String, Float32,
    RawBytes, PaddingBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN
from lib.hachoir_parser.misc.common import Vertex, MapUV

class Vector(FieldSet):
    def __init__(self, parent, name,
    count, type, ename, edesc=None, description=None):
        FieldSet.__init__(self, parent, name, description)
        self.count = count
        self.type = type
        self.ename = ename+"[]"
        self.edesc = edesc
        try:
            item_size = self.type.static_size(self.ename, self.edesc)
        except TypeError:
            item_size = self.type.static_size
        if item_size:
            self._size = item_size * self.count

    def createFields(self):
        for index in xrange(self.count):
            yield self.type(self, self.ename, self.edesc)

class Face(FieldSet):
    def createFields(self):
        yield UInt32(self, "id")
        yield UInt32(self, "type")
        yield UInt32(self, "geometry_mode")
        yield UInt32(self, "lighting_mode")
        yield UInt32(self, "texture_mode")
        yield UInt32(self, "nvertices")
        yield Float32(self, "unknown[]", "unknown")
        yield UInt32(self, "has_texture", "Has texture?")
        yield UInt32(self, "has_material", "Has material?")
        yield Vertex(self, "unknown[]")
        yield Float32(self, "extra_light")
        yield Vertex(self, "unknown[]")
        yield Vertex(self, "normal")
        if self["nvertices"].value:
            yield Vector(self, "vertex_indices",
                self["nvertices"].value, UInt32, "vertex")
        if self["has_texture"].value:
            yield Vector(self, "texture_vertex_indices",
                self["nvertices"].value, UInt32, "texture_vertex")
        if self["has_material"].value:
            yield UInt32(self, "material_index", "material index")

    def createDescription(self):
        return "Face: id=%s" % self["id"].value

class Mesh(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)

    def createFields(self):
        yield String(self, "name", 32, strip="\0")
        yield UInt32(self, "id")
        yield UInt32(self, "geometry_mode")
        yield UInt32(self, "lighting_mode")
        yield UInt32(self, "texture_mode")
        yield UInt32(self, "nmesh_vertices")
        yield UInt32(self, "ntexture_vertices")
        yield UInt32(self, "nfaces")

        nb_vert = self["nmesh_vertices"].value
        if nb_vert:
            yield Vector(self, "vertices",
                nb_vert, Vertex, "vertex")
        if self["ntexture_vertices"].value:
            yield Vector(self, "texture vertices",
                self["ntexture_vertices"].value, MapUV, "texture_vertex")
        if nb_vert:
            yield Vector(self, "light vertices",
                nb_vert, Float32, "extra_light")
            yield Vector(self, "unknown[]",
                nb_vert, Float32, "unknown")
        if self["nfaces"].value:
            yield Vector(self, "faces", self["nfaces"].value, Face, "face")
        if nb_vert:
            yield Vector(self, "vertex normals",
                nb_vert, Vertex, "normal")

        yield UInt32(self, "has_shadow")
        yield Float32(self, "unknown[]")
        yield Float32(self, "radius")
        yield Vertex(self, "unknown[]")
        yield Vertex(self, "unknown[]")

    def createDescription(self):
        return 'Mesh "%s" (id %s)' % (self["name"].value, self["id"].value)

class Geoset(FieldSet):
    def createFields(self):
        yield UInt32(self, "count")
        for index in xrange(self["count"].value):
            yield Mesh(self, "mesh[]")

    def createDescription(self):
        return "Set of %s meshes" % self["count"].value

class Node(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        size = (188-4)*8
        if self["parent_offset"].value != 0:
            size += 32
        if self["first_child_offset"].value != 0:
            size += 32
        if self["next_sibling_offset"].value != 0:
            size += 32
        self._size = size

    def createFields(self):
        yield String(self, "name", 32, strip="\0")
        yield PaddingBytes(self, "unknown[]", 32, pattern="\xCC")
        yield UInt32(self, "flags")
        yield UInt32(self, "id")
        yield UInt32(self, "type")
        yield Int32(self, "mesh_id")
        yield UInt32(self, "depth")
        yield Int32(self, "parent_offset")
        yield UInt32(self, "nchildren")
        yield UInt32(self, "first_child_offset")
        yield UInt32(self, "next_sibling_offset")
        yield Vertex(self, "pivot")
        yield Vertex(self, "position")
        yield Float32(self, "pitch")
        yield Float32(self, "yaw")
        yield Float32(self, "roll")
        for index in xrange(4):
            yield Vertex(self, "unknown_vertex[]")
        if self["parent_offset"].value != 0:
            yield UInt32(self, "parent_id")
        if self["first_child_offset"].value != 0:
            yield UInt32(self, "first_child_id")
        if self["next_sibling_offset"].value != 0:
            yield UInt32(self, "next_sibling_id")

    def createDescription(self):
        return 'Node "%s"' % self["name"].value

class Nodes(FieldSet):
    def createFields(self):
        yield UInt32(self, "count")
        for index in xrange(self["count"].value):
            yield Node(self, "node[]")

    def createDescription(self):
        return 'Nodes (%s)' % self["count"].value

class Materials(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        count = self["count"]
        self._size = count.size + count.value * (32*8)

    def createFields(self):
        yield UInt32(self, "count")
        for index in xrange(self["count"].value):
            yield String(self, "filename[]", 32, "Material file name", strip="\0")

    def createDescription(self):
        return 'Material file names (%s)' % self["count"].value

class File3do(Parser):
    PARSER_TAGS = {
        "id": "3do",
        "category": "misc",
        "file_ext": ("3do",),
        "mime": (u"image/x-3do",),
        "min_size": 8*4,
        "description": "renderdroid 3d model."
    }

    endian = LITTLE_ENDIAN

    def validate(self):
        signature = self.stream.readBytes(0, 4)
        return signature in ('LDOM', 'MODL') # lazy endian-safe hack =D

    def createFields(self):
        # Read file signature, and fix endian if needed
        yield String(self, "file_sig", 4, "File signature", charset="ASCII")
        if self["file_sig"].value == "MODL":
            self.endian = BIG_ENDIAN

        # Read file content
        yield Materials(self, "materials")
        yield String(self, "model_name", 32, "model file name", strip="\0")
        yield RawBytes(self, "unknown[]", 4)
        yield UInt32(self, "ngeosets")
        for index in xrange(self["ngeosets"].value):
            yield Geoset(self, "geoset[]")
        yield RawBytes(self, "unknown[]", 4)
        yield Nodes(self, "nodes")
        yield Float32(self, "model_radius")
        yield Vertex(self, "insertion_offset")

        # Read the end of the file
        if self.current_size < self._size:
            yield self.seekBit(self._size, "end")


########NEW FILE########
__FILENAME__ = file_3ds
"""
3D Studio Max file (.3ds) parser.
Author: Victor Stinner
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (StaticFieldSet, FieldSet,
    UInt16, UInt32, RawBytes, Enum, CString)
from lib.hachoir_parser.image.common import RGB
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_parser.misc.common import Vertex, MapUV

def readObject(parent):
    yield CString(parent, "name", "Object name")
    size = parent["size"].value * 8
    while parent.current_size < size:
        yield Chunk(parent, "chunk[]")

def readTextureFilename(parent):
    yield CString(parent, "filename", "Texture filename")

def readVersion(parent):
    yield UInt32(parent, "version", "3DS file format version")

def readMaterialName(parent):
    yield CString(parent, "name", "Material name")

class Polygon(StaticFieldSet):
    format = (
        (UInt16, "a", "Vertex A"),
        (UInt16, "b", "Vertex B"),
        (UInt16, "c", "Vertex C"),
        (UInt16, "flags", "Flags"))

def readMapList(parent):
    yield UInt16(parent, "count", "Map count")
    for index in xrange(parent["count"].value):
        yield MapUV(parent, "map_uv[]", "Mapping UV")

def readColor(parent):
    yield RGB(parent, "color")

def readVertexList(parent):
    yield UInt16(parent, "count", "Vertex count")
    for index in range(0, parent["count"].value):
        yield Vertex(parent, "vertex[]", "Vertex")

def readPolygonList(parent):
    count = UInt16(parent, "count", "Vertex count")
    yield count
    for i in range(0, count.value):
        yield Polygon(parent, "polygon[]")
    size = parent["size"].value * 8
    while parent.current_size < size:
        yield Chunk(parent, "chunk[]")

class Chunk(FieldSet):
    # List of chunk type name
    type_name = {
        0x0011: "Color",
        0x4D4D: "Main chunk",
        0x0002: "File version",
        0x3D3D: "Materials and objects",
        0x4000: "Object",
        0x4100: "Mesh (triangular)",
        0x4110: "Vertices list",
        0x4120: "Polygon (faces) list",
        0x4140: "Map UV list",
        0x4130: "Object material",
        0xAFFF: "New material",
        0xA000: "Material name",
        0xA010: "Material ambient",
        0xA020: "Material diffuse",
        0xA030: "Texture specular",
        0xA200: "Texture",
        0xA300: "Texture filename",

        # Key frames
        0xB000: "Keyframes",
        0xB002: "Object node tag",
        0xB006: "Light target node tag",
        0xB007: "Spot light node tag",
        0xB00A: "Keyframes header",
        0xB009: "Keyframe current time",
        0xB030: "Node identifier",
        0xB010: "Node header",
        0x7001: "Viewport layout"
    }

    chunk_id_by_type = {
        0x4d4d: "main",
        0x0002: "version",
        0x3d3d: "obj_mat",
        0xb000: "keyframes",
        0xafff: "material[]",
        0x4000: "object[]",
        0x4110: "vertices_list",
        0x4120: "polygon_list",
        0x4140: "mapuv_list",
        0x4100: "mesh"
    }

    # List of chunks which contains other chunks
    sub_chunks = \
        (0x4D4D, 0x4100, 0x3D3D, 0xAFFF, 0xA200,
         0xB002, 0xB006, 0xB007,
         0xA010, 0xA030, 0xA020, 0xB000)

    # List of chunk type handlers
    handlers = {
        0xA000: readMaterialName,
        0x4000: readObject,
        0xA300: readTextureFilename,
        0x0011: readColor,
        0x0002: readVersion,
        0x4110: readVertexList,
        0x4120: readPolygonList,
        0x4140: readMapList
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)

        # Set description
        self._description = "Chunk: %s" % self["type"].display

        # Set name based on type field
        type = self["type"].value
        if type in Chunk.chunk_id_by_type:
            self._name = Chunk.chunk_id_by_type[type]
        else:
            self._name = "chunk_%04x" % type

        # Guess chunk size
        self._size = self["size"].value * 8

    def createFields(self):
        yield Enum(textHandler(UInt16(self, "type", "Chunk type"), hexadecimal), Chunk.type_name)
        yield UInt32(self, "size", "Chunk size (in bytes)")
        content_size = self["size"].value - 6
        if content_size == 0:
            return
        type = self["type"].value
        if type in Chunk.sub_chunks:
            while self.current_size < self.size:
                yield Chunk(self, "chunk[]")
        else:
            if type in Chunk.handlers:
                fields = Chunk.handlers[type] (self)
                for field in fields:
                    yield field
            else:
                yield RawBytes(self, "data", content_size)

class File3ds(Parser):
    endian = LITTLE_ENDIAN
    PARSER_TAGS = {
        "id": "3ds",
        "category": "misc",
        "file_ext": ("3ds",),
        "mime": (u"image/x-3ds",),
        "min_size": 16*8,
        "description": "3D Studio Max model"
    }

    def validate(self):
        if self.stream.readBytes(0, 2) != "MM":
            return "Wrong signature"
        if self["main/version/version"].value not in (2, 3):
            return "Unknown format version"
        return True

    def createFields(self):
        while not self.eof:
            yield Chunk(self, "chunk[]")


########NEW FILE########
__FILENAME__ = gnome_keyring
"""
Gnome keyring parser.

Sources:
 - Gnome Keyring source code,
   function generate_file() in keyrings/gkr-keyring.c,

Author: Victor Stinner
Creation date: 2008-04-09
"""

from lib.hachoir_core.tools import paddingSize
from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    Bit, NullBits, NullBytes,
    UInt8, UInt32, String, RawBytes, Enum,
    TimestampUnix64, CompressedField,
    SubFile)
from lib.hachoir_core.endian import BIG_ENDIAN

try:
    import hashlib
    def sha256(data):
        hash = hashlib.new('sha256')
        hash.update(data)
        return hash.digest()
except ImportError:
    def sha256(data):
        raise ImportError("hashlib module is missing")

try:
    from Crypto.Cipher import AES
    class DeflateStream:
        def __init__(self, stream):
            hash_iterations = 1234
            password = "x" * 8
            salt = "\0" * 8
            key, iv = generate_key(password, salt, hash_iterations)
            self.cipher = AES.new(key, AES.MODE_CBC, iv)

        def __call__(self, size, data=None):
            if data is None:
                return ''
            return self.cipher.decrypt(data)

    def Deflate(field):
        CompressedField(field, DeflateStream)
        return field
except ImportError:
    def Deflate(field):
        return field

class KeyringString(FieldSet):
    def createFields(self):
        yield UInt32(self, "length")
        length = self["length"].value
        if length == 0xffffffff:
            return
        yield String(self, "text", length, charset="UTF-8")

    def createValue(self):
        if "text" in self:
            return self["text"].value
        else:
            return u''

    def createDescription(self):
        if "text" in self:
            return self["text"].value
        else:
            return u"(empty string)"

class Attribute(FieldSet):
    def createFields(self):
        yield KeyringString(self, "name")
        yield UInt32(self, "type")
        type = self["type"].value
        if type == 0:
            yield KeyringString(self, "value")
        elif type == 1:
            yield UInt32(self, "value")
        else:
            raise TypeError("Unknown attribute type (%s)" % type)

    def createDescription(self):
        return 'Attribute "%s"' % self["name"].value

class ACL(FieldSet):
    def createFields(self):
        yield UInt32(self, "types_allowed")
        yield KeyringString(self, "display_name")
        yield KeyringString(self, "pathname")
        yield KeyringString(self, "reserved[]")
        yield UInt32(self, "reserved[]")

class Item(FieldSet):
    def createFields(self):
        yield UInt32(self, "id")
        yield UInt32(self, "type")
        yield UInt32(self, "attr_count")
        for index in xrange(self["attr_count"].value):
            yield Attribute(self, "attr[]")

    def createDescription(self):
        return "Item #%s: %s attributes" % (self["id"].value, self["attr_count"].value)

class Items(FieldSet):
    def createFields(self):
        yield UInt32(self, "count")
        for index in xrange(self["count"].value):
            yield Item(self, "item[]")

class EncryptedItem(FieldSet):
    def createFields(self):
        yield KeyringString(self, "display_name")
        yield KeyringString(self, "secret")
        yield TimestampUnix64(self, "mtime")
        yield TimestampUnix64(self, "ctime")
        yield KeyringString(self, "reserved[]")
        for index in xrange(4):
            yield UInt32(self, "reserved[]")
        yield UInt32(self, "attr_count")
        for index in xrange(self["attr_count"].value):
            yield Attribute(self, "attr[]")
        yield UInt32(self, "acl_count")
        for index in xrange(self["acl_count"].value):
            yield ACL(self, "acl[]")
#        size = 8 # paddingSize((self.stream.size - self.current_size) // 8, 16)
#        if size:
#            yield NullBytes(self, "hash_padding", size, "16 bytes alignment")

class EncryptedData(Parser):
    PARSER_TAGS = {
        "id": "gnomeencryptedkeyring",
        "min_size": 16*8,
        "description": u"Gnome encrypted keyring",
    }
    endian = BIG_ENDIAN
    def validate(self):
        return True

    def createFields(self):
        yield RawBytes(self, "md5", 16)
        while True:
            size = (self.size - self.current_size) // 8
            if size < 77:
                break
            yield EncryptedItem(self, "item[]")
        size = paddingSize(self.current_size // 8, 16)
        if size:
            yield NullBytes(self, "padding_align", size)

class GnomeKeyring(Parser):
    MAGIC = "GnomeKeyring\n\r\0\n"
    PARSER_TAGS = {
        "id": "gnomekeyring",
        "category": "misc",
        "magic": ((MAGIC, 0),),
        "min_size": 47*8,
        "description": u"Gnome keyring",
    }
    CRYPTO_NAMES = {
        0: u"AEL",
    }
    HASH_NAMES = {
        0: u"MD5",
    }

    endian = BIG_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, len(self.MAGIC)) != self.MAGIC:
            return u"Invalid magic string"
        return True

    def createFields(self):
        yield String(self, "magic", len(self.MAGIC), 'Magic string (%r)' % self.MAGIC, charset="ASCII")
        yield UInt8(self, "major_version")
        yield UInt8(self, "minor_version")
        yield Enum(UInt8(self, "crypto"), self.CRYPTO_NAMES)
        yield Enum(UInt8(self, "hash"), self.HASH_NAMES)
        yield KeyringString(self, "keyring_name")
        yield TimestampUnix64(self, "mtime")
        yield TimestampUnix64(self, "ctime")
        yield Bit(self, "lock_on_idle")
        yield NullBits(self, "reserved[]", 31, "Reserved for future flags")
        yield UInt32(self, "lock_timeout")
        yield UInt32(self, "hash_iterations")
        yield RawBytes(self, "salt", 8)
        yield NullBytes(self, "reserved[]", 16)
        yield Items(self, "items")
        yield UInt32(self, "encrypted_size")
        yield Deflate(SubFile(self, "encrypted", self["encrypted_size"].value, "AES128 CBC", parser_class=EncryptedData))

def generate_key(password, salt, hash_iterations):
    sha = sha256(password+salt)
    for index in xrange(hash_iterations-1):
        sha = sha256(sha)
    return sha[:16], sha[16:]


########NEW FILE########
__FILENAME__ = hlp
"""
Microsoft Windows Help (HLP) parser for Hachoir project.

Documents:
- Windows Help File Format / Annotation File Format / SHG and MRB File Format
  written by M. Winterhoff (100326.2776@compuserve.com)
  found on http://www.wotsit.org/

Author: Victor Stinner
Creation date: 2007-09-03
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    Bits, Int32, UInt16, UInt32,
    NullBytes, RawBytes, PaddingBytes, String)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import (textHandler, hexadecimal,
    displayHandler, humanFilesize)

class FileEntry(FieldSet):
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = self["res_space"].value * 8

    def createFields(self):
        yield displayHandler(UInt32(self, "res_space", "Reserved space"), humanFilesize)
        yield displayHandler(UInt32(self, "used_space", "Used space"), humanFilesize)
        yield Bits(self, "file_flags", 8, "(=4)")

        yield textHandler(UInt16(self, "magic"), hexadecimal)
        yield Bits(self, "flags", 16)
        yield displayHandler(UInt16(self, "page_size", "Page size in bytes"), humanFilesize)
        yield String(self, "structure", 16, strip="\0", charset="ASCII")
        yield NullBytes(self, "zero", 2)
        yield UInt16(self, "nb_page_splits", "Number of page splits B+ tree has suffered")
        yield UInt16(self, "root_page", "Page number of B+ tree root page")
        yield PaddingBytes(self, "one", 2, pattern="\xFF")
        yield UInt16(self, "nb_page", "Number of B+ tree pages")
        yield UInt16(self, "nb_level", "Number of levels of B+ tree")
        yield UInt16(self, "nb_entry", "Number of entries in B+ tree")

        size = (self.size - self.current_size)//8
        if size:
            yield PaddingBytes(self, "reserved_space", size)

class HlpFile(Parser):
    PARSER_TAGS = {
        "id": "hlp",
        "category": "misc",
        "file_ext": ("hlp",),
        "min_size": 32,
        "description": "Microsoft Windows Help (HLP)",
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self["magic"].value != 0x00035F3F:
            return "Invalid magic"
        if self["filesize"].value != self.stream.size//8:
            return "Invalid magic"
        return True

    def createFields(self):
        yield textHandler(UInt32(self, "magic"), hexadecimal)
        yield UInt32(self, "dir_start", "Directory start")
        yield Int32(self, "first_free_block", "First free block")
        yield UInt32(self, "filesize", "File size in bytes")

        yield self.seekByte(self["dir_start"].value)
        yield FileEntry(self, "file[]")

        size = (self.size - self.current_size)//8
        if size:
            yield RawBytes(self, "end", size)


########NEW FILE########
__FILENAME__ = lnk
"""
Windows Shortcut (.lnk) parser.

Documents:
- The Windows Shortcut File Format (document version 1.0)
  Reverse-engineered by Jesse Hager
  http://www.i2s-lab.com/Papers/The_Windows_Shortcut_File_Format.pdf
- Wine source code:
  http://source.winehq.org/source/include/shlobj.h (SHELL_LINK_DATA_FLAGS enum)
  http://source.winehq.org/source/dlls/shell32/pidl.h
- Microsoft:
  http://msdn2.microsoft.com/en-us/library/ms538128.aspx

Author: Robert Xiao, Victor Stinner

Changes:
  2007-06-27 - Robert Xiao
    * Fixes to FileLocationInfo to correctly handle Unicode paths
  2007-06-13 - Robert Xiao
    * ItemID, FileLocationInfo and ExtraInfo structs, correct Unicode string handling
  2007-03-15 - Victor Stinner
    * Creation of the parser
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    CString, String,
    UInt32, UInt16, UInt8,
    Bit, Bits, PaddingBits,
    TimestampWin64, DateTimeMSDOS32,
    NullBytes, PaddingBytes, RawBytes, Enum)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_parser.common.win32 import GUID
from lib.hachoir_parser.common.msdos import MSDOSFileAttr16, MSDOSFileAttr32
from lib.hachoir_core.text_handler import filesizeHandler

from lib.hachoir_core.tools import paddingSize

class ItemIdList(FieldSet):
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = (self["size"].value+2) * 8

    def createFields(self):
        yield UInt16(self, "size", "Size of item ID list")
        while True:
            item = ItemId(self, "itemid[]")
            yield item
            if not item["length"].value:
                break

class ItemId(FieldSet):
    ITEM_TYPE = {
        0x1F: "GUID",
        0x23: "Drive",
        0x25: "Drive",
        0x29: "Drive",
        0x2E: "GUID",
        0x2F: "Drive",
        0x30: "Dir/File",
        0x31: "Directory",
        0x32: "File",
        0x34: "File [Unicode Name]",
        0x41: "Workgroup",
        0x42: "Computer",
        0x46: "Net Provider",
        0x47: "Whole Network",
        0x61: "MSITStore",
        0x70: "Printer/RAS Connection",
        0xB1: "History/Favorite",
        0xC3: "Network Share",
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        if self["length"].value:
            self._size = self["length"].value * 8
        else:
            self._size = 16

    def createFields(self):
        yield UInt16(self, "length", "Length of Item ID Entry")
        if not self["length"].value:
            return

        yield Enum(UInt8(self, "type"),self.ITEM_TYPE)
        entrytype=self["type"].value
        if entrytype in (0x1F, 0x2E, 0x70):
            # GUID
            yield RawBytes(self, "dummy", 1, "should be 0x50")
            yield GUID(self, "guid")

        elif entrytype in (0x23, 0x25, 0x29, 0x2F):
            # Drive
            yield String(self, "drive", self["length"].value-3, strip="\0")

        elif entrytype in (0x30, 0x31, 0x32):
            yield RawBytes(self, "dummy", 1, "should be 0x00")
            yield UInt32(self, "size", "size of file; 0 for folders")
            yield DateTimeMSDOS32(self, "date_time", "File/folder date and time")
            yield MSDOSFileAttr16(self, "attribs", "File/folder attributes")
            yield CString(self, "name", "File/folder name")
            if self.root.hasUnicodeNames():
                # Align to 2-bytes
                n = paddingSize(self.current_size//8, 2)
                if n:
                    yield PaddingBytes(self, "pad", n)

                yield UInt16(self, "length_w", "Length of wide struct member")
                yield RawBytes(self, "unknown[]", 6)
                yield DateTimeMSDOS32(self, "creation_date_time", "File/folder creation date and time")
                yield DateTimeMSDOS32(self, "access_date_time", "File/folder last access date and time")
                yield RawBytes(self, "unknown[]", 4)
                yield CString(self, "unicode_name", "File/folder name", charset="UTF-16-LE")
                yield RawBytes(self, "unknown[]", 2)
            else:
                yield CString(self, "name_short", "File/folder short name")

        elif entrytype in (0x41, 0x42, 0x46):
            yield RawBytes(self, "unknown[]", 2)
            yield CString(self, "name")
            yield CString(self, "protocol")
            yield RawBytes(self, "unknown[]", 2)

        elif entrytype == 0x47:
            # Whole Network
            yield RawBytes(self, "unknown[]", 2)
            yield CString(self, "name")

        elif entrytype == 0xC3:
            # Network Share
            yield RawBytes(self, "unknown[]", 2)
            yield CString(self, "name")
            yield CString(self, "protocol")
            yield CString(self, "description")
            yield RawBytes(self, "unknown[]", 2)

        else:
            yield RawBytes(self, "raw", self["length"].value-3)

    def createDescription(self):
        if self["length"].value:
            return "Item ID Entry: "+self.ITEM_TYPE.get(self["type"].value,"Unknown")
        else:
            return "End of Item ID List"

def formatVolumeSerial(field):
    val = field.value
    return '%04X-%04X'%(val>>16, val&0xFFFF)

class LocalVolumeTable(FieldSet):
    VOLUME_TYPE={
        1: "No root directory",
        2: "Removable (Floppy, Zip, etc.)",
        3: "Fixed (Hard disk)",
        4: "Remote (Network drive)",
        5: "CD-ROM",
        6: "Ram drive",
    }

    def createFields(self):
        yield UInt32(self, "length", "Length of this structure")
        yield Enum(UInt32(self, "volume_type", "Volume Type"),self.VOLUME_TYPE)
        yield textHandler(UInt32(self, "volume_serial", "Volume Serial Number"), formatVolumeSerial)

        yield UInt32(self, "label_offset", "Offset to volume label")
        padding = self.seekByte(self["label_offset"].value)
        if padding:
            yield padding
        yield CString(self, "drive")

    def hasValue(self):
        return bool(self["drive"].value)

    def createValue(self):
        return self["drive"].value

class NetworkVolumeTable(FieldSet):
    def createFields(self):
        yield UInt32(self, "length", "Length of this structure")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "share_name_offset", "Offset to share name")
        yield UInt32(self, "unknown[]")
        yield UInt32(self, "unknown[]")
        padding = self.seekByte(self["share_name_offset"].value)
        if padding:
            yield padding
        yield CString(self, "share_name")

    def createValue(self):
        return self["share_name"].value

class FileLocationInfo(FieldSet):
    def createFields(self):
        yield UInt32(self, "length", "Length of this structure")
        if not self["length"].value:
            return

        yield UInt32(self, "first_offset_pos", "Position of first offset")
        has_unicode_paths = (self["first_offset_pos"].value == 0x24)
        yield Bit(self, "on_local_volume")
        yield Bit(self, "on_network_volume")
        yield PaddingBits(self, "reserved[]", 30)
        yield UInt32(self, "local_info_offset", "Offset to local volume table; only meaningful if on_local_volume = 1")
        yield UInt32(self, "local_pathname_offset", "Offset to local base pathname; only meaningful if on_local_volume = 1")
        yield UInt32(self, "remote_info_offset", "Offset to network volume table; only meaningful if on_network_volume = 1")
        yield UInt32(self, "pathname_offset", "Offset of remaining pathname")
        if has_unicode_paths:
            yield UInt32(self, "local_pathname_unicode_offset", "Offset to Unicode version of local base pathname; only meaningful if on_local_volume = 1")
            yield UInt32(self, "pathname_unicode_offset", "Offset to Unicode version of remaining pathname")
        if self["on_local_volume"].value:
            padding = self.seekByte(self["local_info_offset"].value)
            if padding:
                yield padding
            yield LocalVolumeTable(self, "local_volume_table", "Local Volume Table")

            padding = self.seekByte(self["local_pathname_offset"].value)
            if padding:
                yield padding
            yield CString(self, "local_base_pathname", "Local Base Pathname")
            if has_unicode_paths:
                padding = self.seekByte(self["local_pathname_unicode_offset"].value)
                if padding:
                    yield padding
                yield CString(self, "local_base_pathname_unicode", "Local Base Pathname in Unicode", charset="UTF-16-LE")

        if self["on_network_volume"].value:
            padding = self.seekByte(self["remote_info_offset"].value)
            if padding:
                yield padding
            yield NetworkVolumeTable(self, "network_volume_table")

        padding = self.seekByte(self["pathname_offset"].value)
        if padding:
            yield padding
        yield CString(self, "final_pathname", "Final component of the pathname")

        if has_unicode_paths:
            padding = self.seekByte(self["pathname_unicode_offset"].value)
            if padding:
                yield padding
            yield CString(self, "final_pathname_unicode", "Final component of the pathname in Unicode", charset="UTF-16-LE")

        padding=self.seekByte(self["length"].value)
        if padding:
            yield padding

class LnkString(FieldSet):
    def createFields(self):
        yield UInt16(self, "length", "Length of this string")
        if self.root.hasUnicodeNames():
            yield String(self, "data", self["length"].value*2, charset="UTF-16-LE")
        else:
            yield String(self, "data", self["length"].value, charset="ASCII")

    def createValue(self):
        return self["data"].value

class ColorRef(FieldSet):
    ''' COLORREF struct, 0x00bbggrr '''
    static_size=32
    def createFields(self):
        yield UInt8(self, "red", "Red")
        yield UInt8(self, "green", "Green")
        yield UInt8(self, "blue", "Blue")
        yield PaddingBytes(self, "pad", 1, "Padding (must be 0)")
    def createDescription(self):
        rgb = self["red"].value, self["green"].value, self["blue"].value
        return "RGB Color: #%02X%02X%02X" % rgb

class ColorTableIndex(Bits):
    def __init__(self, parent, name, size, description=None):
        Bits.__init__(self, parent, name, size, None)
        self.desc=description
    def createDescription(self):
        assert hasattr(self, 'parent') and hasattr(self, 'value')
        return "%s: %s"%(self.desc,
                         self.parent["color[%i]"%self.value].description)

class ExtraInfo(FieldSet):
    INFO_TYPE={
        0xA0000001: "Link Target Information", # EXP_SZ_LINK_SIG
        0xA0000002: "Console Window Properties", # NT_CONSOLE_PROPS_SIG
        0xA0000003: "Hostname and Other Stuff",
        0xA0000004: "Console Codepage Information", # NT_FE_CONSOLE_PROPS_SIG
        0xA0000005: "Special Folder Info", # EXP_SPECIAL_FOLDER_SIG
        0xA0000006: "DarwinID (Windows Installer ID) Information", # EXP_DARWIN_ID_SIG
        0xA0000007: "Custom Icon Details", # EXP_LOGO3_ID_SIG or EXP_SZ_ICON_SIG
    }
    SPECIAL_FOLDER = {
         0: "DESKTOP",
         1: "INTERNET",
         2: "PROGRAMS",
         3: "CONTROLS",
         4: "PRINTERS",
         5: "PERSONAL",
         6: "FAVORITES",
         7: "STARTUP",
         8: "RECENT",
         9: "SENDTO",
        10: "BITBUCKET",
        11: "STARTMENU",
        16: "DESKTOPDIRECTORY",
        17: "DRIVES",
        18: "NETWORK",
        19: "NETHOOD",
        20: "FONTS",
        21: "TEMPLATES",
        22: "COMMON_STARTMENU",
        23: "COMMON_PROGRAMS",
        24: "COMMON_STARTUP",
        25: "COMMON_DESKTOPDIRECTORY",
        26: "APPDATA",
        27: "PRINTHOOD",
        28: "LOCAL_APPDATA",
        29: "ALTSTARTUP",
        30: "COMMON_ALTSTARTUP",
        31: "COMMON_FAVORITES",
        32: "INTERNET_CACHE",
        33: "COOKIES",
        34: "HISTORY",
        35: "COMMON_APPDATA",
        36: "WINDOWS",
        37: "SYSTEM",
        38: "PROGRAM_FILES",
        39: "MYPICTURES",
        40: "PROFILE",
        41: "SYSTEMX86",
        42: "PROGRAM_FILESX86",
        43: "PROGRAM_FILES_COMMON",
        44: "PROGRAM_FILES_COMMONX86",
        45: "COMMON_TEMPLATES",
        46: "COMMON_DOCUMENTS",
        47: "COMMON_ADMINTOOLS",
        48: "ADMINTOOLS",
        49: "CONNECTIONS",
        53: "COMMON_MUSIC",
        54: "COMMON_PICTURES",
        55: "COMMON_VIDEO",
        56: "RESOURCES",
        57: "RESOURCES_LOCALIZED",
        58: "COMMON_OEM_LINKS",
        59: "CDBURN_AREA",
        61: "COMPUTERSNEARME",
    }
    BOOL_ENUM = {
        0: "False",
        1: "True",
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        if self["length"].value:
            self._size = self["length"].value * 8
        else:
            self._size = 32

    def createFields(self):
        yield UInt32(self, "length", "Length of this structure")
        if not self["length"].value:
            return

        yield Enum(textHandler(UInt32(self, "signature", "Signature determining the function of this structure"),hexadecimal),self.INFO_TYPE)

        if self["signature"].value == 0xA0000003:
            # Hostname and Other Stuff
            yield UInt32(self, "remaining_length")
            yield UInt32(self, "unknown[]")
            yield String(self, "hostname", 16, "Computer hostname on which shortcut was last modified", strip="\0")
            yield RawBytes(self, "unknown[]", 32)
            yield RawBytes(self, "unknown[]", 32)

        elif self["signature"].value == 0xA0000005:
            # Special Folder Info
            yield Enum(UInt32(self, "special_folder_id", "ID of the special folder"),self.SPECIAL_FOLDER)
            yield UInt32(self, "offset", "Offset to Item ID entry")

        elif self["signature"].value in (0xA0000001, 0xA0000006, 0xA0000007):
            if self["signature"].value == 0xA0000001: # Link Target Information
                object_name="target"
            elif self["signature"].value == 0xA0000006: # DarwinID (Windows Installer ID) Information
                object_name="darwinID"
            else: # Custom Icon Details
                object_name="icon_path"
            yield CString(self, object_name, "Data (ASCII format)", charset="ASCII")
            remaining = self["length"].value - self.current_size/8 - 260*2 # 260*2 = size of next part
            if remaining:
                yield RawBytes(self, "slack_space[]", remaining, "Data beyond end of string")
            yield CString(self, object_name+'_unicode', "Data (Unicode format)", charset="UTF-16-LE", truncate="\0")
            remaining = self["length"].value - self.current_size/8
            if remaining:
                yield RawBytes(self, "slack_space[]", remaining, "Data beyond end of string")

        elif self["signature"].value == 0xA0000002:
            # Console Window Properties
            yield ColorTableIndex(self, "color_text", 4, "Screen text color index")
            yield ColorTableIndex(self, "color_bg", 4, "Screen background color index")
            yield NullBytes(self, "reserved[]", 1)
            yield ColorTableIndex(self, "color_popup_text", 4, "Pop-up text color index")
            yield ColorTableIndex(self, "color_popup_bg", 4, "Pop-up background color index")
            yield NullBytes(self, "reserved[]", 1)
            yield UInt16(self, "buffer_width", "Screen buffer width (character cells)")
            yield UInt16(self, "buffer_height", "Screen buffer height (character cells)")
            yield UInt16(self, "window_width", "Window width (character cells)")
            yield UInt16(self, "window_height", "Window height (character cells)")
            yield UInt16(self, "position_left", "Window distance from left edge (screen coords)")
            yield UInt16(self, "position_top", "Window distance from top edge (screen coords)")
            yield UInt32(self, "font_number")
            yield UInt32(self, "input_buffer_size")
            yield UInt16(self, "font_width", "Font width in pixels; 0 for a non-raster font")
            yield UInt16(self, "font_height", "Font height in pixels; equal to the font size for non-raster fonts")
            yield UInt32(self, "font_family")
            yield UInt32(self, "font_weight")
            yield String(self, "font_name_unicode", 64, "Font Name (Unicode format)", charset="UTF-16-LE", truncate="\0")
            yield UInt32(self, "cursor_size", "Relative size of cursor (% of character size)")
            yield Enum(UInt32(self, "full_screen", "Run console in full screen?"), self.BOOL_ENUM)
            yield Enum(UInt32(self, "quick_edit", "Console uses quick-edit feature (using mouse to cut & paste)?"), self.BOOL_ENUM)
            yield Enum(UInt32(self, "insert_mode", "Console uses insertion mode?"), self.BOOL_ENUM)
            yield Enum(UInt32(self, "auto_position", "System automatically positions window?"), self.BOOL_ENUM)
            yield UInt32(self, "history_size", "Size of the history buffer (in lines)")
            yield UInt32(self, "history_count", "Number of history buffers (each process gets one up to this limit)")
            yield Enum(UInt32(self, "history_no_dup", "Automatically eliminate duplicate lines in the history buffer?"), self.BOOL_ENUM)
            for index in xrange(16):
                yield ColorRef(self, "color[]")

        elif self["signature"].value == 0xA0000004:
            # Console Codepage Information
            yield UInt32(self, "codepage", "Console's code page")

        else:
            yield RawBytes(self, "raw", self["length"].value-self.current_size/8)

    def createDescription(self):
        if self["length"].value:
            return "Extra Info Entry: "+self["signature"].display
        else:
            return "End of Extra Info"

HOT_KEYS = {
    0x00: u'None',
    0x13: u'Pause',
    0x14: u'Caps Lock',
    0x21: u'Page Up',
    0x22: u'Page Down',
    0x23: u'End',
    0x24: u'Home',
    0x25: u'Left',
    0x26: u'Up',
    0x27: u'Right',
    0x28: u'Down',
    0x2d: u'Insert',
    0x2e: u'Delete',
    0x6a: u'Num *',
    0x6b: u'Num +',
    0x6d: u'Num -',
    0x6e: u'Num .',
    0x6f: u'Num /',
    0x90: u'Num Lock',
    0x91: u'Scroll Lock',
    0xba: u';',
    0xbb: u'=',
    0xbc: u',',
    0xbd: u'-',
    0xbe: u'.',
    0xbf: u'/',
    0xc0: u'`',
    0xdb: u'[',
    0xdc: u'\\',
    0xdd: u']',
    0xde: u"'",
}

def text_hot_key(field):
    assert hasattr(field, "value")
    val=field.value
    if 0x30 <= val <= 0x39:
        return unichr(val)
    elif 0x41 <= val <= 0x5A:
        return unichr(val)
    elif 0x60 <= val <= 0x69:
        return u'Numpad %c' % unichr(val-0x30)
    elif 0x70 <= val <= 0x87:
        return 'F%i'%(val-0x6F)
    elif val in HOT_KEYS:
        return HOT_KEYS[val]
    return str(val)

class LnkFile(Parser):
    MAGIC = "\x4C\0\0\0\x01\x14\x02\x00\x00\x00\x00\x00\xc0\x00\x00\x00\x00\x00\x00\x46"
    PARSER_TAGS = {
        "id": "lnk",
        "category": "misc",
        "file_ext": ("lnk",),
        "mime": (u"application/x-ms-shortcut",),
        "magic": ((MAGIC, 0),),
        "min_size": len(MAGIC)*8,   # signature + guid = 20 bytes
        "description": "Windows Shortcut (.lnk)",
    }
    endian = LITTLE_ENDIAN

    SHOW_WINDOW_STATE = {
         0: u"Hide",
         1: u"Show Normal",
         2: u"Show Minimized",
         3: u"Show Maximized",
         4: u"Show Normal, not activated",
         5: u"Show",
         6: u"Minimize",
         7: u"Show Minimized, not activated",
         8: u"Show, not activated",
         9: u"Restore",
        10: u"Show Default",
    }

    def validate(self):
        if self["signature"].value != 0x0000004C:
            return "Invalid signature"
        if self["guid"].value != "00021401-0000-0000-C000-000000000046":
            return "Invalid GUID"
        return True

    def hasUnicodeNames(self):
        return self["has_unicode_names"].value

    def createFields(self):
        yield UInt32(self, "signature", "Shortcut signature (0x0000004C)")
        yield GUID(self, "guid", "Shortcut GUID (00021401-0000-0000-C000-000000000046)")

        yield Bit(self, "has_shell_id", "Is the Item ID List present?")
        yield Bit(self, "target_is_file", "Is a file or a directory?")
        yield Bit(self, "has_description", "Is the Description field present?")
        yield Bit(self, "has_rel_path", "Is the relative path to the target available?")
        yield Bit(self, "has_working_dir", "Is there a working directory?")
        yield Bit(self, "has_cmd_line_args", "Are there any command line arguments?")
        yield Bit(self, "has_custom_icon", "Is there a custom icon?")
        yield Bit(self, "has_unicode_names", "Are Unicode names used?")
        yield Bit(self, "force_no_linkinfo")
        yield Bit(self, "has_exp_sz")
        yield Bit(self, "run_in_separate")
        yield Bit(self, "has_logo3id", "Is LOGO3 ID info present?")
        yield Bit(self, "has_darwinid", "Is the DarwinID info present?")
        yield Bit(self, "runas_user", "Is the target run as another user?")
        yield Bit(self, "has_exp_icon_sz", "Is custom icon information available?")
        yield Bit(self, "no_pidl_alias")
        yield Bit(self, "force_unc_name")
        yield Bit(self, "run_with_shim_layer")
        yield PaddingBits(self, "reserved[]", 14, "Flag bits reserved for future use")

        yield MSDOSFileAttr32(self, "target_attr")

        yield TimestampWin64(self, "creation_time")
        yield TimestampWin64(self, "modification_time")
        yield TimestampWin64(self, "last_access_time")
        yield filesizeHandler(UInt32(self, "target_filesize"))
        yield UInt32(self, "icon_number")
        yield Enum(UInt32(self, "show_window"), self.SHOW_WINDOW_STATE)
        yield textHandler(UInt8(self, "hot_key", "Hot key used for quick access"),text_hot_key)
        yield Bit(self, "hot_key_shift", "Hot key: is Shift used?")
        yield Bit(self, "hot_key_ctrl", "Hot key: is Ctrl used?")
        yield Bit(self, "hot_key_alt", "Hot key: is Alt used?")
        yield PaddingBits(self, "hot_key_reserved", 21, "Hot key: (reserved)")
        yield NullBytes(self, "reserved[]", 8)

        if self["has_shell_id"].value:
            yield ItemIdList(self, "item_idlist", "Item ID List")
        if self["target_is_file"].value:
            yield FileLocationInfo(self, "file_location_info", "File Location Info")
        if self["has_description"].value:
            yield LnkString(self, "description")
        if self["has_rel_path"].value:
            yield LnkString(self, "relative_path", "Relative path to target")
        if self["has_working_dir"].value:
            yield LnkString(self, "working_dir", "Working directory (dir to start target in)")
        if self["has_cmd_line_args"].value:
            yield LnkString(self, "cmd_line_args", "Command Line Arguments")
        if self["has_custom_icon"].value:
            yield LnkString(self, "custom_icon", "Custom Icon Path")

        while not self.eof:
            yield ExtraInfo(self, "extra_info[]")


########NEW FILE########
__FILENAME__ = msoffice
"""
Parsers for the different streams and fragments found in an OLE2 file.

Documents:
 - goffice source code

Author: Robert Xiao, Victor Stinner
Creation: 2006-04-23
"""

from lib.hachoir_parser import HachoirParser
from lib.hachoir_core.field import FieldSet, RootSeekableFieldSet, RawBytes
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.stream import StringInputStream
from lib.hachoir_parser.misc.msoffice_summary import SummaryFieldSet, CompObj
from lib.hachoir_parser.misc.word_doc import WordDocumentFieldSet

PROPERTY_NAME = {
    u"\5DocumentSummaryInformation": "doc_summary",
    u"\5SummaryInformation": "summary",
    u"WordDocument": "word_doc",
}

class OfficeRootEntry(HachoirParser, RootSeekableFieldSet):
    PARSER_TAGS = {
        "description": "Microsoft Office document subfragments",
    }
    endian = LITTLE_ENDIAN

    def __init__(self, stream, **args):
        RootSeekableFieldSet.__init__(self, None, "root", stream, None, stream.askSize(self))
        HachoirParser.__init__(self, stream, **args)

    def validate(self):
        return True

    def createFields(self):
        for index, property in enumerate(self.ole2.properties):
            if index == 0:
                continue
            try:
                name = PROPERTY_NAME[property["name"].value]
            except LookupError:
                name = property.name+"content"
            for field in self.parseProperty(index, property, name):
                yield field

    def parseProperty(self, property_index, property, name_prefix):
        ole2 = self.ole2
        if not property["size"].value:
            return
        if property["size"].value >= ole2["header/threshold"].value:
            return
        name = "%s[]" % name_prefix
        first = None
        previous = None
        size = 0
        start = property["start"].value
        chain = ole2.getChain(start, True)
        blocksize = ole2.ss_size
        desc_format = "Small blocks %s..%s (%s)"
        while True:
            try:
                block = chain.next()
                contiguous = False
                if not first:
                    first = block
                    contiguous = True
                if previous and block == (previous+1):
                    contiguous = True
                if contiguous:
                    previous = block
                    size += blocksize
                    continue
            except StopIteration:
                block = None
            self.seekSBlock(first)
            desc = desc_format % (first, previous, previous-first+1)
            size = min(size, property["size"].value*8)
            if name_prefix in ("summary", "doc_summary"):
                yield SummaryFieldSet(self, name, desc, size=size)
            elif name_prefix == "word_doc":
                yield WordDocumentFieldSet(self, name, desc, size=size)
            elif property_index == 1:
                yield CompObj(self, "comp_obj", desc, size=size)
            else:
                yield RawBytes(self, name, size//8, desc)
            if block is None:
                break
            first = block
            previous = block
            size = ole2.sector_size

    def seekSBlock(self, block):
        self.seekBit(block * self.ole2.ss_size)

class FragmentGroup:
    def __init__(self, parser):
        self.items = []
        self.parser = parser

    def add(self, item):
        self.items.append(item)

    def createInputStream(self):
        # FIXME: Use lazy stream creation
        data = []
        for item in self.items:
            data.append( item["rawdata"].value )
        data = "".join(data)

        # FIXME: Use smarter code to send arguments
        args = {"ole2": self.items[0].root}
        tags = {"class": self.parser, "args": args}
        tags = tags.iteritems()
        return StringInputStream(data, "<fragment group>", tags=tags)

class CustomFragment(FieldSet):
    def __init__(self, parent, name, size, parser, description=None, group=None):
        FieldSet.__init__(self, parent, name, description, size=size)
        if not group:
            group = FragmentGroup(parser)
        self.group = group
        self.group.add(self)

    def createFields(self):
        yield RawBytes(self, "rawdata", self.size//8)

    def _createInputStream(self, **args):
        return self.group.createInputStream()


########NEW FILE########
__FILENAME__ = msoffice_summary
"""
Microsoft Document summaries structures.

Documents
---------

 - Apache POI (HPSF Internals):
   http://poi.apache.org/hpsf/internals.html
"""
from lib.hachoir_parser import HachoirParser
from lib.hachoir_core.field import (FieldSet, ParserError,
    RootSeekableFieldSet, SeekableFieldSet,
    Bit, Bits, NullBits,
    UInt8, UInt16, UInt32, TimestampWin64, TimedeltaWin64, Enum,
    Bytes, RawBytes, NullBytes, String,
    Int8, Int32, Float32, Float64, PascalString32)
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler
from lib.hachoir_core.tools import createDict
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN
from lib.hachoir_parser.common.win32 import GUID, PascalStringWin32, CODEPAGE_CHARSET
from lib.hachoir_parser.image.bmp import BmpHeader, parseImageData

MAX_SECTION_COUNT = 100

OS_MAC = 1
OS_NAME = {
    0: "Windows 16-bit",
    1: "Macintosh",
    2: "Windows 32-bit",
}

class OSConfig:
    def __init__(self, big_endian):
        if big_endian:
            self.charset = "MacRoman"
            self.utf16 = "UTF-16-BE"
        else:
            # FIXME: Don't guess the charset, use ISO-8859-1 or UTF-8
            #self.charset = "ISO-8859-1"
            self.charset = None
            self.utf16 = "UTF-16-LE"

class PropertyIndex(FieldSet):
    TAG_CODEPAGE = 1

    COMMON_PROPERTY = {
        0: "Dictionary",
        1: "CodePage",
        0x80000000: "LOCALE_SYSTEM_DEFAULT",
        0x80000003: "CASE_SENSITIVE",
    }

    DOCUMENT_PROPERTY = {
         2: "Category",
         3: "PresentationFormat",
         4: "NumBytes",
         5: "NumLines",
         6: "NumParagraphs",
         7: "NumSlides",
         8: "NumNotes",
         9: "NumHiddenSlides",
        10: "NumMMClips",
        11: "Scale",
        12: "HeadingPairs",
        13: "DocumentParts",
        14: "Manager",
        15: "Company",
        16: "LinksDirty",
        17: "DocSumInfo_17",
        18: "DocSumInfo_18",
        19: "DocSumInfo_19",
        20: "DocSumInfo_20",
        21: "DocSumInfo_21",
        22: "DocSumInfo_22",
        23: "DocSumInfo_23",
    }
    DOCUMENT_PROPERTY.update(COMMON_PROPERTY)

    COMPONENT_PROPERTY = {
         2: "Title",
         3: "Subject",
         4: "Author",
         5: "Keywords",
         6: "Comments",
         7: "Template",
         8: "LastSavedBy",
         9: "RevisionNumber",
        10: "TotalEditingTime",
        11: "LastPrinted",
        12: "CreateTime",
        13: "LastSavedTime",
        14: "NumPages",
        15: "NumWords",
        16: "NumCharacters",
        17: "Thumbnail",
        18: "AppName",
        19: "Security",
    }
    COMPONENT_PROPERTY.update(COMMON_PROPERTY)

    def createFields(self):
        if self["../.."].name.startswith("doc_summary"):
            enum = self.DOCUMENT_PROPERTY
        else:
            enum = self.COMPONENT_PROPERTY
        yield Enum(UInt32(self, "id"), enum)
        yield UInt32(self, "offset")

    def createDescription(self):
        return "Property: %s" % self["id"].display

class Bool(Int8):
    def createValue(self):
        value = Int8.createValue(self)
        return (value == -1)

class Thumbnail(FieldSet):
    """
    Thumbnail.

    Documents:
    - See Jakarta POI
      http://jakarta.apache.org/poi/hpsf/thumbnails.html
      http://www.penguin-soft.com/penguin/developer/poi/
          org/apache/poi/hpsf/Thumbnail.html#CF_BITMAP
    - How To Extract Thumbnail Images
      http://sparks.discreet.com/knowledgebase/public/
          solutions/ExtractThumbnailImg.htm
    """
    FORMAT_CLIPBOARD = -1
    FORMAT_NAME = {
        -1: "Windows clipboard",
        -2: "Macintosh clipboard",
        -3: "GUID that contains format identifier",
         0: "No data",
         2: "Bitmap",
         3: "Windows metafile format",
         8: "Device Independent Bitmap (DIB)",
        14: "Enhanced Windows metafile",
    }

    DIB_BMP = 8
    DIB_FORMAT = {
        2: "Bitmap Obsolete (old BMP)",
        3: "Windows metafile format (WMF)",
        8: "Device Independent Bitmap (BMP)",
       14: "Enhanced Windows metafile (EMF)",
    }
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["size"].value * 8

    def createFields(self):
        yield filesizeHandler(UInt32(self, "size"))
        yield Enum(Int32(self, "format"), self.FORMAT_NAME)
        if self["format"].value == self.FORMAT_CLIPBOARD:
            yield Enum(UInt32(self, "dib_format"), self.DIB_FORMAT)
            if self["dib_format"].value == self.DIB_BMP:
                yield BmpHeader(self, "bmp_header")
                size = (self.size - self.current_size) // 8
                yield parseImageData(self, "pixels", size, self["bmp_header"])
                return
        size = (self.size - self.current_size) // 8
        if size:
            yield RawBytes(self, "data", size)

class PropertyContent(FieldSet):
    TYPE_LPSTR = 30
    TYPE_INFO = {
        0: ("EMPTY", None),
        1: ("NULL", None),
        2: ("UInt16", UInt16),
        3: ("UInt32", UInt32),
        4: ("Float32", Float32),
        5: ("Float64", Float64),
        6: ("CY", None),
        7: ("DATE", None),
        8: ("BSTR", None),
        9: ("DISPATCH", None),
        10: ("ERROR", None),
        11: ("BOOL", Bool),
        12: ("VARIANT", None),
        13: ("UNKNOWN", None),
        14: ("DECIMAL", None),
        16: ("I1", None),
        17: ("UI1", None),
        18: ("UI2", None),
        19: ("UI4", None),
        20: ("I8", None),
        21: ("UI8", None),
        22: ("INT", None),
        23: ("UINT", None),
        24: ("VOID", None),
        25: ("HRESULT", None),
        26: ("PTR", None),
        27: ("SAFEARRAY", None),
        28: ("CARRAY", None),
        29: ("USERDEFINED", None),
        30: ("LPSTR", PascalString32),
        31: ("LPWSTR", PascalString32),
        64: ("FILETIME", TimestampWin64),
        65: ("BLOB", None),
        66: ("STREAM", None),
        67: ("STORAGE", None),
        68: ("STREAMED_OBJECT", None),
        69: ("STORED_OBJECT", None),
        70: ("BLOB_OBJECT", None),
        71: ("THUMBNAIL", Thumbnail),
        72: ("CLSID", None),
        0x1000: ("Vector", None),
    }
    TYPE_NAME = createDict(TYPE_INFO, 0)

    def createFields(self):
        self.osconfig = self.parent.osconfig
        if True:
            yield Enum(Bits(self, "type", 12), self.TYPE_NAME)
            yield Bit(self, "is_vector")
            yield NullBits(self, "padding", 32-12-1)
        else:
            yield Enum(Bits(self, "type", 32), self.TYPE_NAME)
        tag =  self["type"].value
        kw = {}
        try:
            handler = self.TYPE_INFO[tag][1]
            if handler == PascalString32:
                osconfig = self.osconfig
                if tag == self.TYPE_LPSTR:
                    kw["charset"] = osconfig.charset
                else:
                    kw["charset"] = osconfig.utf16
            elif handler == TimestampWin64:
                if self.description == "TotalEditingTime":
                    handler = TimedeltaWin64
        except LookupError:
            handler = None
        if not handler:
            raise ParserError("OLE2: Unable to parse property of type %s" \
                % self["type"].display)
        if self["is_vector"].value:
            yield UInt32(self, "count")
            for index in xrange(self["count"].value):
                yield handler(self, "item[]", **kw)
        else:
            yield handler(self, "value", **kw)
            self.createValue = lambda: self["value"].value
PropertyContent.TYPE_INFO[12] = ("VARIANT", PropertyContent)

class SummarySection(SeekableFieldSet):
    def __init__(self, *args):
        SeekableFieldSet.__init__(self, *args)
        self._size = self["size"].value * 8

    def createFields(self):
        self.osconfig = self.parent.osconfig
        yield UInt32(self, "size")
        yield UInt32(self, "property_count")
        for index in xrange(self["property_count"].value):
            yield PropertyIndex(self, "property_index[]")
        for index in xrange(self["property_count"].value):
            findex = self["property_index[%u]" % index]
            self.seekByte(findex["offset"].value)
            field = PropertyContent(self, "property[]", findex["id"].display)
            yield field
            if not self.osconfig.charset \
            and findex['id'].value == PropertyIndex.TAG_CODEPAGE:
                codepage = field['value'].value
                if codepage in CODEPAGE_CHARSET:
                    self.osconfig.charset = CODEPAGE_CHARSET[codepage]
                else:
                    self.warning("Unknown codepage: %r" % codepage)

class SummaryIndex(FieldSet):
    static_size = 20*8
    def createFields(self):
        yield String(self, "name", 16)
        yield UInt32(self, "offset")

class BaseSummary:
    endian = LITTLE_ENDIAN

    def __init__(self):
        if self["endian"].value == "\xFF\xFE":
            self.endian = BIG_ENDIAN
        elif self["endian"].value == "\xFE\xFF":
            self.endian = LITTLE_ENDIAN
        else:
            raise ParserError("OLE2: Invalid endian value")
        self.osconfig = OSConfig(self["os_type"].value == OS_MAC)

    def createFields(self):
        yield Bytes(self, "endian", 2, "Endian (0xFF 0xFE for Intel)")
        yield UInt16(self, "format", "Format (0)")
        yield UInt8(self, "os_version")
        yield UInt8(self, "os_revision")
        yield Enum(UInt16(self, "os_type"), OS_NAME)
        yield GUID(self, "format_id")
        yield UInt32(self, "section_count")
        if MAX_SECTION_COUNT < self["section_count"].value:
            raise ParserError("OLE2: Too much sections (%s)" % self["section_count"].value)

        section_indexes = []
        for index in xrange(self["section_count"].value):
            section_index = SummaryIndex(self, "section_index[]")
            yield section_index
            section_indexes.append(section_index)

        for section_index in section_indexes:
            self.seekByte(section_index["offset"].value)
            yield SummarySection(self, "section[]")

        size = (self.size - self.current_size) // 8
        if 0 < size:
            yield NullBytes(self, "end_padding", size)

class SummaryParser(BaseSummary, HachoirParser, RootSeekableFieldSet):
    PARSER_TAGS = {
        "description": "Microsoft Office summary",
    }

    def __init__(self, stream, **kw):
        RootSeekableFieldSet.__init__(self, None, "root", stream, None, stream.askSize(self))
        HachoirParser.__init__(self, stream, **kw)
        BaseSummary.__init__(self)

    def validate(self):
        return True

class SummaryFieldSet(BaseSummary, FieldSet):
    def __init__(self, parent, name, description=None, size=None):
        FieldSet.__init__(self, parent, name, description=description, size=size)
        BaseSummary.__init__(self)

class CompObj(FieldSet):
    OS_VERSION = {
        0x0a03: "Windows 3.1",
    }
    def createFields(self):
        # Header
        yield UInt16(self, "version", "Version (=1)")
        yield textHandler(UInt16(self, "endian", "Endian (0xFF 0xFE for Intel)"), hexadecimal)
        yield UInt8(self, "os_version")
        yield UInt8(self, "os_revision")
        yield Enum(UInt16(self, "os_type"), OS_NAME)
        yield Int32(self, "unused", "(=-1)")
        yield GUID(self, "clsid")

        # User type
        yield PascalString32(self, "user_type", strip="\0")

        # Clipboard format
        if self["os_type"].value == OS_MAC:
            yield Int32(self, "unused[]", "(=-2)")
            yield String(self, "clipboard_format", 4)
        else:
            yield PascalString32(self, "clipboard_format", strip="\0")
        if self.current_size == self.size:
            return

        #-- OLE 2.01 ---

        # Program ID
        yield PascalString32(self, "prog_id", strip="\0")

        if self["os_type"].value != OS_MAC:
            # Magic number
            yield textHandler(UInt32(self, "magic", "Magic number (0x71B239F4)"), hexadecimal)

            # Unicode version
            yield PascalStringWin32(self, "user_type_unicode", strip="\0")
            yield PascalStringWin32(self, "clipboard_format_unicode", strip="\0")
            yield PascalStringWin32(self, "prog_id_unicode", strip="\0")

        size = (self.size - self.current_size) // 8
        if size:
            yield NullBytes(self, "end_padding", size)


########NEW FILE########
__FILENAME__ = ole2
"""
Microsoft Office documents parser.

Informations:
* wordole.c of AntiWord program (v0.35)
  Copyright (C) 1998-2003 A.J. van Os
  Released under GNU GPL
  http://www.winfield.demon.nl/
* File gsf-infile-msole.c of libgsf library (v1.14.0)
  Copyright (C) 2002-2004 Jody Goldberg (jody@gnome.org)
  Released under GNU LGPL 2.1
  http://freshmeat.net/projects/libgsf/
* PDF from AAF Association
  Copyright (C) 2004 AAF Association
  Copyright (C) 1991-2003 Microsoft Corporation
  http://www.aafassociation.org/html/specs/aafcontainerspec-v1.0.1.pdf

Author: Victor Stinner
Creation: 2006-04-23
"""

from lib.hachoir_parser import HachoirParser
from lib.hachoir_core.field import (
    FieldSet, ParserError, SeekableFieldSet, RootSeekableFieldSet,
    UInt8, UInt16, UInt32, UInt64, TimestampWin64, Enum,
    Bytes, RawBytes, NullBytes, String)
from lib.hachoir_core.text_handler import filesizeHandler
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_parser.common.win32 import GUID
from lib.hachoir_parser.misc.msoffice import CustomFragment, OfficeRootEntry, PROPERTY_NAME
from lib.hachoir_parser.misc.word_doc import WordDocumentParser
from lib.hachoir_parser.misc.msoffice_summary import SummaryParser

MIN_BIG_BLOCK_LOG2 = 6   # 512 bytes
MAX_BIG_BLOCK_LOG2 = 14  # 64 kB

# Number of items in DIFAT
NB_DIFAT = 109

class SECT(UInt32):
    UNUSED       = 0xFFFFFFFF   # -1
    END_OF_CHAIN = 0xFFFFFFFE   # -2
    BFAT_SECTOR  = 0xFFFFFFFD   # -3
    DIFAT_SECTOR = 0xFFFFFFFC   # -4
    SPECIALS = set((END_OF_CHAIN, UNUSED, BFAT_SECTOR, DIFAT_SECTOR))

    special_value_name = {
        UNUSED: "unused",
        END_OF_CHAIN: "end of a chain",
        BFAT_SECTOR: "BFAT sector (in a FAT)",
        DIFAT_SECTOR: "DIFAT sector (in a FAT)",
    }

    def __init__(self, parent, name, description=None):
        UInt32.__init__(self, parent, name, description)

    def createDisplay(self):
        val = self.value
        return SECT.special_value_name.get(val, str(val))

class Property(FieldSet):
    TYPE_ROOT = 5
    TYPE_NAME = {
        1: "storage",
        2: "stream",
        3: "ILockBytes",
        4: "IPropertyStorage",
        5: "root"
    }
    DECORATOR_NAME = {
        0: "red",
        1: "black",
    }
    static_size = 128 * 8

    def createFields(self):
        bytes = self.stream.readBytes(self.absolute_address, 4)
        if bytes == "\0R\0\0":
            charset = "UTF-16-BE"
        else:
            charset = "UTF-16-LE"
        yield String(self, "name", 64, charset=charset, truncate="\0")
        yield UInt16(self, "namelen", "Length of the name")
        yield Enum(UInt8(self, "type", "Property type"), self.TYPE_NAME)
        yield Enum(UInt8(self, "decorator", "Decorator"), self.DECORATOR_NAME)
        yield SECT(self, "left")
        yield SECT(self, "right")
        yield SECT(self, "child", "Child node (valid for storage and root types)")
        yield GUID(self, "clsid", "CLSID of this storage (valid for storage and root types)")
        yield NullBytes(self, "flags", 4, "User flags")
        yield TimestampWin64(self, "creation", "Creation timestamp(valid for storage and root types)")
        yield TimestampWin64(self, "lastmod", "Modify timestamp (valid for storage and root types)")
        yield SECT(self, "start", "Starting SECT of the stream (valid for stream and root types)")
        if self["/header/bb_shift"].value == 9:
            yield filesizeHandler(UInt32(self, "size", "Size in bytes (valid for stream and root types)"))
            yield NullBytes(self, "padding", 4)
        else:
            yield filesizeHandler(UInt64(self, "size", "Size in bytes (valid for stream and root types)"))

    def createDescription(self):
        name = self["name"].display
        size = self["size"].display
        return "Property: %s (%s)" % (name, size)

class DIFat(SeekableFieldSet):
    def __init__(self, parent, name, db_start, db_count, description=None):
        SeekableFieldSet.__init__(self, parent, name, description)
        self.start=db_start
        self.count=db_count

    def createFields(self):
        for index in xrange(NB_DIFAT):
            yield SECT(self, "index[%u]" % index)

        for index in xrange(self.count):
            # this is relative to real DIFAT start
            self.seekBit(NB_DIFAT * SECT.static_size+self.parent.sector_size*(self.start+index))
            for sect_index in xrange(NB_DIFAT*(index+1),NB_DIFAT*(index+2)):
                yield SECT(self, "index[%u]" % sect_index)

class Header(FieldSet):
    static_size = 68 * 8
    def createFields(self):
        yield GUID(self, "clsid", "16 bytes GUID used by some apps")
        yield UInt16(self, "ver_min", "Minor version")
        yield UInt16(self, "ver_maj", "Minor version")
        yield Bytes(self, "endian", 2, "Endian (0xFFFE for Intel)")
        yield UInt16(self, "bb_shift", "Log, base 2, of the big block size")
        yield UInt16(self, "sb_shift", "Log, base 2, of the small block size")
        yield NullBytes(self, "reserved[]", 6, "(reserved)")
        yield UInt32(self, "csectdir", "Number of SECTs in directory chain for 4 KB sectors (version 4)")
        yield UInt32(self, "bb_count", "Number of Big Block Depot blocks")
        yield SECT(self, "bb_start", "Root start block")
        yield NullBytes(self, "transaction", 4, "Signature used for transactions (must be zero)")
        yield UInt32(self, "threshold", "Maximum size for a mini stream (typically 4096 bytes)")
        yield SECT(self, "sb_start", "Small Block Depot start block")
        yield UInt32(self, "sb_count")
        yield SECT(self, "db_start", "First block of DIFAT")
        yield UInt32(self, "db_count", "Number of SECTs in DIFAT")

# Header (ole_id, header, difat) size in bytes
HEADER_SIZE = 64 + Header.static_size + NB_DIFAT * SECT.static_size

class SectFat(FieldSet):
    def __init__(self, parent, name, start, count, description=None):
        FieldSet.__init__(self, parent, name, description, size=count*32)
        self.count = count
        self.start = start

    def createFields(self):
        for i in xrange(self.start, self.start + self.count):
            yield SECT(self, "index[%u]" % i)

class OLE2_File(HachoirParser, RootSeekableFieldSet):
    PARSER_TAGS = {
        "id": "ole2",
        "category": "misc",
        "file_ext": (
            "doc", "dot",                # Microsoft Word
            "ppt", "ppz", "pps", "pot",  # Microsoft Powerpoint
            "xls", "xla",                # Microsoft Excel
            "msi",                       # Windows installer
        ),
        "mime": (
            u"application/msword",
            u"application/msexcel",
            u"application/mspowerpoint",
        ),
        "min_size": 512*8,
        "description": "Microsoft Office document",
        "magic": (("\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1", 0),),
    }
    endian = LITTLE_ENDIAN

    def __init__(self, stream, **args):
        RootSeekableFieldSet.__init__(self, None, "root", stream, None, stream.askSize(self))
        HachoirParser.__init__(self, stream, **args)

    def validate(self):
        if self["ole_id"].value != "\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1":
            return "Invalid magic"
        if self["header/ver_maj"].value not in (3, 4):
            return "Unknown major version (%s)" % self["header/ver_maj"].value
        if self["header/endian"].value not in ("\xFF\xFE", "\xFE\xFF"):
            return "Unknown endian (%s)" % self["header/endian"].raw_display
        if not(MIN_BIG_BLOCK_LOG2 <= self["header/bb_shift"].value <= MAX_BIG_BLOCK_LOG2):
            return "Invalid (log 2 of) big block size (%s)" % self["header/bb_shift"].value
        if self["header/bb_shift"].value < self["header/sb_shift"].value:
            return "Small block size (log2=%s) is bigger than big block size (log2=%s)!" \
                % (self["header/sb_shift"].value, self["header/bb_shift"].value)
        return True

    def createFields(self):
        # Signature
        yield Bytes(self, "ole_id", 8, "OLE object signature")

        header = Header(self, "header")
        yield header

        # Configure values
        self.sector_size = (8 << header["bb_shift"].value)
        self.fat_count = header["bb_count"].value
        self.items_per_bbfat = self.sector_size / SECT.static_size
        self.ss_size = (8 << header["sb_shift"].value)
        self.items_per_ssfat = self.items_per_bbfat

        # Read DIFAT (one level of indirection)
        yield DIFat(self, "difat",  header["db_start"].value, header["db_count"].value, "Double Indirection FAT")

        # Read FAT (one level of indirection)
        for field in self.readBFAT():
            yield field

        # Read SFAT
        for field in self.readSFAT():
            yield field

        # Read properties
        chain = self.getChain(self["header/bb_start"].value)
        prop_per_sector = self.sector_size // Property.static_size
        self.properties = []
        for block in chain:
            self.seekBlock(block)
            for index in xrange(prop_per_sector):
                property = Property(self, "property[]")
                yield property
                self.properties.append(property)

        # Parse first property
        for index, property in enumerate(self.properties):
            if index == 0:
                name = "root"
            else:
                try:
                    name = PROPERTY_NAME[property["name"].value]
                except LookupError:
                    name = property.name+"content"
            for field in self.parseProperty(property, name):
                yield field

    def parseProperty(self, property, name_prefix):
        if not property["size"].value:
            return
        if property.name != "property[0]" \
        and (property["size"].value < self["header/threshold"].value):
            # Field is stored in the ministream, skip it
            return
        name = "%s[]" % name_prefix
        first = None
        previous = None
        size = 0
        fragment_group = None
        chain = self.getChain(property["start"].value)
        while True:
            try:
                block = chain.next()
                contiguous = False
                if not first:
                    first = block
                    contiguous = True
                if previous and block == (previous+1):
                    contiguous = True
                if contiguous:
                    previous = block
                    size += self.sector_size
                    continue
            except StopIteration:
                block = None
            if first is None:
                break
            self.seekBlock(first)
            desc = "Big blocks %s..%s (%s)" % (first, previous, previous-first+1)
            desc += " of %s bytes" % (self.sector_size // 8)
            if name_prefix in set(("root", "summary", "doc_summary", "word_doc")):
                if name_prefix == "root":
                    parser = OfficeRootEntry
                elif name_prefix == "word_doc":
                    parser = WordDocumentParser
                else:
                    parser = SummaryParser
                field = CustomFragment(self, name, size, parser, desc, fragment_group)
                yield field
                if not fragment_group:
                    fragment_group = field.group
            else:
                yield RawBytes(self, name, size//8, desc)
            if block is None:
                break
            first = block
            previous = block
            size = self.sector_size

    def getChain(self, start, use_sfat=False):
        if use_sfat:
            fat = self.ss_fat
            items_per_fat = self.items_per_ssfat
            err_prefix = "SFAT chain"
        else:
            fat = self.bb_fat
            items_per_fat = self.items_per_bbfat
            err_prefix = "BFAT chain"
        block = start
        block_set = set()
        previous = block
        while block != SECT.END_OF_CHAIN:
            if block in SECT.SPECIALS:
                raise ParserError("%s: Invalid block index (0x%08x), previous=%s" % (err_prefix, block, previous))
            if block in block_set:
                raise ParserError("%s: Found a loop (%s=>%s)" % (err_prefix, previous, block))
            block_set.add(block)
            yield block
            previous = block
            index = block // items_per_fat
            try:
                block = fat[index]["index[%u]" % block].value
            except LookupError:
                break

    def readBFAT(self):
        self.bb_fat = []
        start = 0
        count = self.items_per_bbfat
        for index, block in enumerate(self.array("difat/index")):
            block = block.value
            if block == SECT.UNUSED:
                break

            desc = "FAT %u/%u at block %u" % \
                (1+index, self["header/bb_count"].value, block)

            self.seekBlock(block)
            field = SectFat(self, "bbfat[]", start, count, desc)
            yield field
            self.bb_fat.append(field)

            start += count

    def readSFAT(self):
        chain = self.getChain(self["header/sb_start"].value)
        start = 0
        self.ss_fat = []
        count = self.items_per_ssfat
        for index, block in enumerate(chain):
            self.seekBlock(block)
            field = SectFat(self, "sfat[]", \
                start, count, \
                "SFAT %u/%u at block %u" % \
                (1+index, self["header/sb_count"].value, block))
            yield field
            self.ss_fat.append(field)
            start += count

    def createContentSize(self):
        max_block = 0
        for fat in self.array("bbfat"):
            for entry in fat:
                block = entry.value
                if block not in SECT.SPECIALS:
                    max_block = max(block, max_block)
        if max_block in SECT.SPECIALS:
            return None
        else:
            return HEADER_SIZE + (max_block+1) * self.sector_size

    def seekBlock(self, block):
        self.seekBit(HEADER_SIZE + block * self.sector_size)


########NEW FILE########
__FILENAME__ = pcf
"""
X11 Portable Compiled Font (pcf) parser.

Documents:
- Format for X11 pcf bitmap font files
  http://fontforge.sourceforge.net/pcf-format.html
  (file is based on the X11 sources)

Author: Victor Stinner
Creation date: 2007-03-20
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, Enum,
    UInt8, UInt32, Bytes, RawBytes, NullBytes,
    Bit, Bits, PaddingBits, CString)
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler
from lib.hachoir_core.tools import paddingSize

class TOC(FieldSet):
    TYPE_NAME = {
        0x00000001: "Properties",
        0x00000002: "Accelerators",
        0x00000004: "Metrics",
        0x00000008: "Bitmaps",
        0x00000010: "Ink metrics",
        0x00000020: "BDF encodings",
        0x00000040: "SWidths",
        0x00000080: "Glyph names",
        0x00000100: "BDF accelerators",
    }

    FORMAT_NAME = {
        0x00000000: "Default",
        0x00000200: "Ink bounds",
        0x00000100: "Accelerator W ink bounds",
#        0x00000200: "Compressed metrics",
    }

    def createFields(self):
        yield Enum(UInt32(self, "type"), self.TYPE_NAME)
        yield UInt32(self, "format")
        yield filesizeHandler(UInt32(self, "size"))
        yield UInt32(self, "offset")

    def createDescription(self):
        return "%s at %s (%s)" % (
            self["type"].display, self["offset"].value, self["size"].display)

class PropertiesFormat(FieldSet):
    static_size = 32
    endian = LITTLE_ENDIAN
    def createFields(self):
        yield Bits(self, "reserved[]", 2)
        yield Bit(self, "byte_big_endian")
        yield Bit(self, "bit_big_endian")
        yield Bits(self, "scan_unit", 2)
        yield textHandler(PaddingBits(self, "reserved[]", 26), hexadecimal)

class Property(FieldSet):
    def createFields(self):
        yield UInt32(self, "name_offset")
        yield UInt8(self, "is_string")
        yield UInt32(self, "value_offset")

    def createDescription(self):
        # FIXME: Use link or any better way to read name value
        name = self["../name[%s]" % (self.index-2)].value
        return "Property %s" % name

class GlyphNames(FieldSet):
    def __init__(self, parent, name, toc, description, size=None):
        FieldSet.__init__(self, parent, name, description, size=size)
        self.toc = toc
        if self["format/byte_big_endian"].value:
            self.endian = BIG_ENDIAN
        else:
            self.endian = LITTLE_ENDIAN

    def createFields(self):
        yield PropertiesFormat(self, "format")
        yield UInt32(self, "count")
        offsets = []
        for index in xrange(self["count"].value):
            offset = UInt32(self, "offset[]")
            yield offset
            offsets.append(offset.value)
        yield UInt32(self, "total_str_length")
        offsets.sort()
        offset0 = self.current_size // 8
        for offset in offsets:
            padding = self.seekByte(offset0+offset)
            if padding:
                yield padding
            yield CString(self, "name[]")
        padding = (self.size - self.current_size) // 8
        if padding:
            yield NullBytes(self, "end_padding", padding)

class Properties(GlyphNames):
    def createFields(self):
        yield PropertiesFormat(self, "format")
        yield UInt32(self, "nb_prop")
        properties = []
        for index in xrange(self["nb_prop"].value):
            property = Property(self, "property[]")
            yield property
            properties.append(property)
        padding = paddingSize(self.current_size//8, 4)
        if padding:
            yield NullBytes(self, "padding", padding)
        yield UInt32(self, "total_str_length")
        properties.sort(key=lambda entry: entry["name_offset"].value)
        offset0 = self.current_size // 8
        for property in properties:
            padding = self.seekByte(offset0+property["name_offset"].value)
            if padding:
                yield padding
            yield CString(self, "name[]", "Name of %s" % property.name)
            if property["is_string"].value:
                yield CString(self, "value[]", "Value of %s" % property.name)
        padding = (self.size - self.current_size) // 8
        if padding:
            yield NullBytes(self, "end_padding", padding)

class PcfFile(Parser):
    MAGIC = "\1fcp"
    PARSER_TAGS = {
        "id": "pcf",
        "category": "misc",
        "file_ext": ("pcf",),
        "magic": ((MAGIC, 0),),
        "min_size": 32, # FIXME
        "description": "X11 Portable Compiled Font (pcf)",
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        if self["signature"].value != self.MAGIC:
            return "Invalid signature"
        return True

    def createFields(self):
        yield Bytes(self, "signature", 4, r'File signature ("\1pcf")')
        yield UInt32(self, "nb_toc")
        entries = []
        for index in xrange(self["nb_toc"].value):
            entry = TOC(self, "toc[]")
            yield entry
            entries.append(entry)
        entries.sort(key=lambda entry: entry["offset"].value)
        for entry in entries:
            size = entry["size"].value
            padding = self.seekByte(entry["offset"].value)
            if padding:
                yield padding
            maxsize = (self.size-self.current_size)//8
            if maxsize < size:
                self.warning("Truncate content of %s to %s bytes (was %s)" % (entry.path, maxsize, size))
                size = maxsize
            if not size:
                continue
            if entry["type"].value == 1:
                yield Properties(self, "properties", entry, "Properties", size=size*8)
            elif entry["type"].value == 128:
                yield GlyphNames(self, "glyph_names", entry, "Glyph names", size=size*8)
            else:
                yield RawBytes(self, "data[]", size, "Content of %s" % entry.path)


########NEW FILE########
__FILENAME__ = pdf
"""
Adobe Portable Document Format (PDF) parser.

Author: Christophe Gisquet <christophe.gisquet@free.fr>
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (
    Field, FieldSet,
    ParserError,
    GenericVector,
    UInt8, UInt16, UInt32,
    String,
    RawBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal

MAGIC = "%PDF-"
ENDMAGIC = "%%EOF"

def getLineEnd(s, pos=None):
    if pos == None:
        pos = (s.absolute_address+s.current_size)//8
    end = s.stream.searchBytesLength("\x0D", False, 8*pos)
    other_end = s.stream.searchBytesLength("\x0A", False, 8*pos)
    if end == None or (other_end != None and other_end < end):
        return other_end
    return end

# TODO: rewrite to account for all possible terminations: ' ', '/', '\0XD'
#       But this probably requires changing *ALL* of the places they are used,
#       as ' ' is swallowed but not the others
def getElementEnd(s, limit=' ', offset=0):
    addr = s.absolute_address+s.current_size
    addr += 8*offset
    pos = s.stream.searchBytesLength(limit, True, addr)
    if pos == None:
        #s.info("Can't find '%s' starting at %u" % (limit, addr))
        return None
    return pos

class PDFNumber(Field):
    LIMITS = ['[', '/', '\x0D', ']']
    """
    sprintf("%i") or sprinf("%.?f")
    """
    def __init__(self, parent, name, desc=None):
        Field.__init__(self, parent, name, description=desc)
        # Get size
        size = getElementEnd(parent)
        for limit in self.LIMITS:
            other_size = getElementEnd(parent, limit)
            if other_size != None:
                other_size -= 1
                if size == None or other_size < size:
                    size = other_size

        self._size = 8*size

        # Get value
        val = parent.stream.readBytes(self.absolute_address, size)
        self.info("Number: size=%u value='%s'" % (size, val))
        if val.find('.') != -1:
            self.createValue = lambda: float(val)
        else:
            self.createValue = lambda: int(val)

class PDFString(Field):
    """
    A string of the shape:
    ( This string \
      uses 3 lines \
      with the CR(LF) inhibited )
    """
    def __init__(self, parent, name, desc=None):
        Field.__init__(self, parent, name, description=desc)
        val = ""
        count = 1
        off = 1
        while not parent.eof:
            char = parent.stream.readBytes(self.absolute_address+8*off, 1)
            # Non-ASCII
            if not char.isalpha() or char == '\\':
                off += 1
                continue
            if char == '(':
                count += 1
            if char == ')':
                count -= 1
            # Parenthesis block = 0 => end of string
            if count == 0:
                off += 1
                break

            # Add it to the string
            val += char

        self._size = 8*off
        self.createValue = lambda: val

class PDFName(Field):
    LIMITS = ['[', '/', '<', ']']
    """
    String starting with '/', where characters may be written using their
    ASCII code (exemple: '#20' would be ' '
    ' ', ']' and '\0' are supposed not to be part of the name
    """
    def __init__(self, parent, name, desc=None):
        Field.__init__(self, parent, name, description=desc)
        if parent.stream.readBytes(self.absolute_address, 1) != '/':
            raise ParserError("Unknown PDFName '%s'" %
                              parent.stream.readBytes(self.absolute_address, 10))
        size = getElementEnd(parent, offset=1)
        #other_size = getElementEnd(parent, '[')-1
        #if size == None or (other_size != None and other_size < size):
        #    size = other_size
        for limit in self.LIMITS:
            other_size = getElementEnd(parent, limit, 1)
            if other_size != None:
                other_size -= 1
                if size == None or other_size < size:
                    #self.info("New size: %u" % other_size)
                    size = other_size

        self._size = 8*(size+1)
        # Value should be without the initial '/' and final ' '
        self.createValue = lambda: parent.stream.readBytes(self.absolute_address+8, size).strip(' ')

class PDFID(Field):
    """
    Not described as an object, but let's do as it was.
    This ID has the shape <hexadecimal ASCII string>
    """
    def __init__(self, parent, name, desc=None):
        Field.__init__(self, parent, name, description=desc)
        self._size = 8*getElementEnd(parent, '>')
        self.createValue = lambda: parent.stream.readBytes(self.absolute_address+8, (self._size//8)-1)

class NotABool(Exception): pass
class PDFBool(Field):
    """
    "true" or "false" string standing for the boolean value
    """
    def __init__(self, parent, name, desc=None):
        Field.__init__(self, parent, name, description=desc)
        if parent.stream.readBytes(self.absolute_address, 4) == "true":
            self._size = 4
            self.createValue = lambda: True
        elif parent.stream.readBytes(self.absolute_address, 5) == "false":
            self._size = 5
            self.createValue = lambda: False
        raise NotABool

class LineEnd(FieldSet):
    """
    Made of 0x0A, 0x0D (we may include several line ends)
    """
    def createFields(self):
        while not self.eof:
            addr = self.absolute_address+self.current_size
            char = self.stream.readBytes(addr, 1)
            if char == '\x0A':
                yield UInt8(self, "lf", "Line feed")
            elif char == '\x0D':
                yield UInt8(self, "cr", "Line feed")
            else:
                self.info("Line ends at %u/%u, len %u" %
                          (addr, self.stream._size, self.current_size))
                break

class PDFDictionaryPair(FieldSet):
    def createFields(self):
        yield PDFName(self, "name", getElementEnd(self))
        for field in parsePDFType(self):
            yield field

class PDFDictionary(FieldSet):
    def createFields(self):
        yield String(self, "dict_start", 2)
        while not self.eof:
            addr = self.absolute_address+self.current_size
            if self.stream.readBytes(addr, 2) != '>>':
                for field in parsePDFType(self):
                    yield field
            else:
                break
        yield String(self, "dict_end", 2)

class PDFArray(FieldSet):
    """
    Array of possibly non-homogeneous elements, starting with '[' and ending
    with ']'
    """
    def createFields(self):
        yield String(self, "array_start", 1)
        while self.stream.readBytes(self.absolute_address+self.current_size, 1) != ']':
            for field in parsePDFType(self):
                yield field
        yield String(self, "array_end", 1)

def parsePDFType(s):
    addr = s.absolute_address+s.current_size
    char = s.stream.readBytes(addr, 1)
    if char == '/':
        yield PDFName(s, "type[]", getElementEnd(s))
    elif char == '<':
        if s.stream.readBytes(addr+8, 1) == '<':
            yield PDFDictionary(s, "dict[]")
        else:
            yield PDFID(s, "id[]")
    elif char == '(':
        yield PDFString(s, "string[]")
    elif char == '[':
        yield PDFArray(s, "array[]")
    else:
        # First parse size
        size = getElementEnd(s)
        for limit in ['/', '>', '<']:
            other_size = getElementEnd(s, limit)
            if other_size != None:
                other_size -= 1
                if size == None or (other_size>0 and other_size < size):
                    size = other_size

        # Get element
        name = s.stream.readBytes(addr, size)
        char = s.stream.readBytes(addr+8*size+8, 1)
        if name.count(' ') > 1 and char == '<':
            # Probably a catalog
            yield Catalog(s, "catalog[]")
        elif name[0] in ('.','-','+', '0', '1', '2', '3', \
                         '4', '5', '6', '7', '8', '9'):
            s.info("Not a catalog: %u spaces and end='%s'" % (name.count(' '), char))
            yield PDFNumber(s, "integer[]")
        else:
            s.info("Trying to parse '%s': %u bytes" % \
                   (s.stream.readBytes(s.absolute_address+s.current_size, 4), size))
            yield String(s, "unknown[]", size)

class Header(FieldSet):
    def createFields(self):
        yield String(self, "marker", 5, MAGIC)
        length = getLineEnd(self, 4)
        if length != None:
            #self.info("Found at position %08X" % len)
            yield String(self, "version", length-1)
            yield LineEnd(self, "line_end")
        else:
            self.warning("Can't determine version!")
    def createDescription(self):
        return "PDF version %s" % self["version"].display

class Body(FieldSet):
    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        pos = self.stream.searchBytesLength(CrossReferenceTable.MAGIC, False)
        if pos == None:
            raise ParserError("Can't find xref starting at %u" %
                              (self.absolute_address//8))
        self._size = 8*pos-self.absolute_address

    def createFields(self):
        while self.stream.readBytes(self.absolute_address+self.current_size, 1) == '%':
            size = getLineEnd(self, 4)
            if size == 2:
                yield textHandler(UInt16(self, "crc32"), hexadecimal)
            elif size == 4:
                yield textHandler(UInt32(self, "crc32"), hexadecimal)
            elif self.stream.readBytes(self.absolute_address+self.current_size, size).isalpha():
                yield String(self, "comment[]", size)
            else:
                RawBytes(self, "unknown_data[]", size)
            yield LineEnd(self, "line_end[]")

        #abs_offset = self.current_size//8
        # TODO: yield objects that read offsets and deduce size from
        # "/cross_ref_table/sub_section[]/entries/item[]"
        offsets = []
        for subsection in self.array("/cross_ref_table/sub_section"):
            for obj in subsection.array("entries/item"):
                if "byte_offset" in obj:
                    # Could be inserted already sorted
                    offsets.append(obj["byte_offset"].value)

        offsets.append(self["/cross_ref_table"].absolute_address//8)
        offsets.sort()
        for index in xrange(len(offsets)-1):
            yield Catalog(self, "object[]", size=offsets[index+1]-offsets[index])

class Entry(FieldSet):
    static_size = 20*8
    def createFields(self):
        typ = self.stream.readBytes(self.absolute_address+17*8, 1)
        if typ == 'n':
            yield PDFNumber(self, "byte_offset")
        elif typ == 'f':
            yield PDFNumber(self, "next_free_object_number")
        else:
            yield PDFNumber(self, "unknown_string")
        yield PDFNumber(self, "generation_number")
        yield UInt8(self, "type")
        yield LineEnd(self, "line_end")
    def createDescription(self):
        if self["type"].value == 'n':
            return "In-use entry at offset %u" % int(self["byte_offset"].value)
        elif self["type"].value == 'f':
            return "Free entry before in-use object %u" % \
                   int(self["next_free_object_number"].value)
        else:
            return "unknown %s" % self["unknown_string"].value

class SubSection(FieldSet):
    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, desc)
        self.info("Got entry count: '%s'" % self["entry_count"].value)
        self._size = self.current_size + 8*20*int(self["entry_count"].value) \
                     + self["line_end"].size

    def createFields(self):
        yield PDFNumber(self, "start_number",
                        "Object number of first entry in subsection")
        self.info("start_number = %i" % self["start_number"].value)

        yield PDFNumber(self, "entry_count", "Number of entries in subsection")
        self.info("entry_count = %i" % self["entry_count"].value)
        yield LineEnd(self, "line_end")
        yield GenericVector(self, "entries", int(self["entry_count"].value),
                            Entry)
        #yield LineEnd(self, "line_end[]")
    def createDescription(self):
        return "Subsection with %s elements, starting at %s" % \
               (self["entry_count"].value, self["start_number"])

class CrossReferenceTable(FieldSet):
    MAGIC = "xref"

    def __init__(self, parent, name, desc=None):
        FieldSet.__init__(self, parent, name, description=desc)
        pos = self.stream.searchBytesLength(Trailer.MAGIC, False)
        if pos == None:
            raise ParserError("Can't find '%s' starting at %u" \
                              (Trailer.MAGIC, self.absolute_address//8))
        self._size = 8*pos-self.absolute_address

    def createFields(self):
        yield RawBytes(self, "marker", len(self.MAGIC))
        yield LineEnd(self, "line_end[]")
        while not self.eof:
            yield SubSection(self, "sub_section[]")

class Catalog(FieldSet):
    END_NAME = ['<', '/', '[']
    def __init__(self, parent, name, size=None, desc=None):
        FieldSet.__init__(self, parent, name, description=desc)
        if size != None:
            self._size = 8*size
        # object catalogs are ended with "obj"
        elif self["object"].value == "obj":
            size = self.stream.searchBytesLength("endobj", False)
            if size != None:
                self._size = 8*(size+2)
    def createFields(self):
        yield PDFNumber(self, "index")
        yield PDFNumber(self, "unknown[]")
        length = getElementEnd(self)
        for limit in self.END_NAME:
            new_length = getElementEnd(self, limit)-len(limit)
            if length == None or (new_length != None and new_length < length):
                length = new_length
        yield String(self, "object", length, strip=' ')
        if self.stream.readBytes(self.absolute_address+self.current_size, 2) == '<<':
            yield PDFDictionary(self, "key_list")
        # End of catalog: this one has "endobj"
        if self["object"].value == "obj":
            yield LineEnd(self, "line_end[]")
            yield String(self, "end_object", len("endobj"))
            yield LineEnd(self, "line_end[]")

class Trailer(FieldSet):
    MAGIC = "trailer"
    def createFields(self):
        yield RawBytes(self, "marker", len(self.MAGIC))
        yield LineEnd(self, "line_end[]")
        yield String(self, "start_attribute_marker", 2)
        addr = self.absolute_address + self.current_size
        while self.stream.readBytes(addr, 2) != '>>':
            t = PDFName(self, "type[]")
            yield t
            name = t.value
            self.info("Parsing PDFName '%s'" % name)
            if name == "Size":
                yield PDFNumber(self, "size", "Entries in the file cross-reference section")
            elif name == "Prev":
                yield PDFNumber(self, "offset")
            elif name == "Root":
                yield Catalog(self, "object_catalog")
            elif name == "Info":
                yield Catalog(self, "info")
            elif name == "ID":
                yield PDFArray(self, "id")
            elif name == "Encrypt":
                yield PDFDictionary(self, "decrypt")
            else:
                raise ParserError("Don't know trailer type '%s'" % name)
            addr = self.absolute_address + self.current_size
        yield String(self, "end_attribute_marker", 2)
        yield LineEnd(self, "line_end[]")
        yield String(self, "start_xref", 9)
        yield LineEnd(self, "line_end[]")
        yield PDFNumber(self, "cross_ref_table_start_address")
        yield LineEnd(self, "line_end[]")
        yield String(self, "end_marker", len(ENDMAGIC))
        yield LineEnd(self, "line_end[]")

class PDFDocument(Parser):
    endian = LITTLE_ENDIAN
    PARSER_TAGS = {
        "id": "pdf",
        "category": "misc",
        "file_ext": ("pdf",),
        "mime": (u"application/pdf",),
        "min_size": (5+4)*8,
        "magic": ((MAGIC, 5),),
        "description": "Portable Document Format (PDF) document"
    }

    def validate(self):
        if self.stream.readBytes(0, len(MAGIC)) != MAGIC:
            return "Invalid magic string"
        return True

    # Size is not always determined by position of "%%EOF":
    # - updated documents have several of those
    # - PDF files should be parsed from *end*
    # => TODO: find when a document has been updated

    def createFields(self):
        yield Header(self, "header")
        yield Body(self, "body")
        yield CrossReferenceTable(self, "cross_ref_table")
        yield Trailer(self, "trailer")


########NEW FILE########
__FILENAME__ = pifv
"""
EFI Platform Initialization Firmware Volume parser.

Author: Alexandre Boeglin
Creation date: 08 jul 2007
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt8, UInt16, UInt24, UInt32, UInt64, Enum,
    CString, String, PaddingBytes, RawBytes, NullBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.tools import paddingSize, humanFilesize
from lib.hachoir_parser.common.win32 import GUID

EFI_SECTION_COMPRESSION = 0x1
EFI_SECTION_GUID_DEFINED = 0x2
EFI_SECTION_PE32 = 0x10
EFI_SECTION_PIC = 0x11
EFI_SECTION_TE = 0x12
EFI_SECTION_DXE_DEPEX = 0x13
EFI_SECTION_VERSION = 0x14
EFI_SECTION_USER_INTERFACE = 0x15
EFI_SECTION_COMPATIBILITY16 = 0x16
EFI_SECTION_FIRMWARE_VOLUME_IMAGE = 0x17
EFI_SECTION_FREEFORM_SUBTYPE_GUID = 0x18
EFI_SECTION_RAW = 0x19
EFI_SECTION_PEI_DEPEX = 0x1b

EFI_SECTION_TYPE = {
    EFI_SECTION_COMPRESSION: "Encapsulation section where other sections" \
        + " are compressed",
    EFI_SECTION_GUID_DEFINED: "Encapsulation section where other sections" \
        + " have format defined by a GUID",
    EFI_SECTION_PE32: "PE32+ Executable image",
    EFI_SECTION_PIC: "Position-Independent Code",
    EFI_SECTION_TE: "Terse Executable image",
    EFI_SECTION_DXE_DEPEX: "DXE Dependency Expression",
    EFI_SECTION_VERSION: "Version, Text and Numeric",
    EFI_SECTION_USER_INTERFACE: "User-Friendly name of the driver",
    EFI_SECTION_COMPATIBILITY16: "DOS-style 16-bit EXE",
    EFI_SECTION_FIRMWARE_VOLUME_IMAGE: "PI Firmware Volume image",
    EFI_SECTION_FREEFORM_SUBTYPE_GUID: "Raw data with GUID in header to" \
        + " define format",
    EFI_SECTION_RAW: "Raw data",
    EFI_SECTION_PEI_DEPEX: "PEI Dependency Expression",
}

EFI_FV_FILETYPE_RAW = 0x1
EFI_FV_FILETYPE_FREEFORM = 0x2
EFI_FV_FILETYPE_SECURITY_CORE = 0x3
EFI_FV_FILETYPE_PEI_CORE = 0x4
EFI_FV_FILETYPE_DXE_CORE = 0x5
EFI_FV_FILETYPE_PEIM = 0x6
EFI_FV_FILETYPE_DRIVER = 0x7
EFI_FV_FILETYPE_COMBINED_PEIM_DRIVER = 0x8
EFI_FV_FILETYPE_APPLICATION = 0x9
EFI_FV_FILETYPE_FIRMWARE_VOLUME_IMAGE = 0xb
EFI_FV_FILETYPE_FFS_PAD = 0xf0

EFI_FV_FILETYPE = {
    EFI_FV_FILETYPE_RAW: "Binary data",
    EFI_FV_FILETYPE_FREEFORM: "Sectioned data",
    EFI_FV_FILETYPE_SECURITY_CORE: "Platform core code used during the SEC" \
        + " phase",
    EFI_FV_FILETYPE_PEI_CORE: "PEI Foundation",
    EFI_FV_FILETYPE_DXE_CORE: "DXE Foundation",
    EFI_FV_FILETYPE_PEIM: "PEI module (PEIM)",
    EFI_FV_FILETYPE_DRIVER: "DXE driver",
    EFI_FV_FILETYPE_COMBINED_PEIM_DRIVER: "Combined PEIM/DXE driver",
    EFI_FV_FILETYPE_APPLICATION: "Application",
    EFI_FV_FILETYPE_FIRMWARE_VOLUME_IMAGE: "Firmware volume image",
    EFI_FV_FILETYPE_FFS_PAD: "Pad File For FFS",
}
for x in xrange(0xc0, 0xe0):
    EFI_FV_FILETYPE[x] = "OEM File"
for x in xrange(0xe0, 0xf0):
    EFI_FV_FILETYPE[x] = "Debug/Test File"
for x in xrange(0xf1, 0x100):
    EFI_FV_FILETYPE[x] = "Firmware File System Specific File"


class BlockMap(FieldSet):
    static_size = 8*8
    def createFields(self):
        yield UInt32(self, "num_blocks")
        yield UInt32(self, "len")

    def createDescription(self):
        return "%d blocks of %s" % (
            self["num_blocks"].value, humanFilesize(self["len"].value))


class FileSection(FieldSet):
    COMPRESSION_TYPE = {
        0: 'Not Compressed',
        1: 'Standard Compression',
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = self["size"].value * 8
        section_type = self["type"].value
        if section_type in (EFI_SECTION_DXE_DEPEX, EFI_SECTION_PEI_DEPEX):
            # These sections can sometimes be longer than what their size
            # claims! It's so nice to have so detailled specs and not follow
            # them ...
            if self.stream.readBytes(self.absolute_address +
                self._size, 1) == '\0':
                self._size = self._size + 16

    def createFields(self):
        # Header
        yield UInt24(self, "size")
        yield Enum(UInt8(self, "type"), EFI_SECTION_TYPE)
        section_type = self["type"].value

        if section_type == EFI_SECTION_COMPRESSION:
            yield UInt32(self, "uncomp_len")
            yield Enum(UInt8(self, "comp_type"), self.COMPRESSION_TYPE)
        elif section_type == EFI_SECTION_FREEFORM_SUBTYPE_GUID:
            yield GUID(self, "sub_type_guid")
        elif section_type == EFI_SECTION_GUID_DEFINED:
            yield GUID(self, "section_definition_guid")
            yield UInt16(self, "data_offset")
            yield UInt16(self, "attributes")
        elif section_type == EFI_SECTION_USER_INTERFACE:
            yield CString(self, "file_name", charset="UTF-16-LE")
        elif section_type == EFI_SECTION_VERSION:
            yield UInt16(self, "build_number")
            yield CString(self, "version", charset="UTF-16-LE")

        # Content
        content_size = (self.size - self.current_size) // 8
        if content_size == 0:
            return

        if section_type == EFI_SECTION_COMPRESSION:
            compression_type = self["comp_type"].value
            if compression_type == 1:
                while not self.eof:
                    yield RawBytes(self, "compressed_content", content_size)
            else:
                while not self.eof:
                    yield FileSection(self, "section[]")
        elif section_type == EFI_SECTION_FIRMWARE_VOLUME_IMAGE:
            yield FirmwareVolume(self, "firmware_volume")
        else:
            yield RawBytes(self, "content", content_size,
                EFI_SECTION_TYPE.get(self["type"].value,
                "Unknown Section Type"))

    def createDescription(self):
        return EFI_SECTION_TYPE.get(self["type"].value,
            "Unknown Section Type")


class File(FieldSet):
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = self["size"].value * 8

    def createFields(self):
        # Header
        yield GUID(self, "name")
        yield UInt16(self, "integrity_check")
        yield Enum(UInt8(self, "type"), EFI_FV_FILETYPE)
        yield UInt8(self, "attributes")
        yield UInt24(self, "size")
        yield UInt8(self, "state")

        # Content
        while not self.eof:
            yield FileSection(self, "section[]")

    def createDescription(self):
        return "%s: %s containing %d section(s)" % (
            self["name"].value,
            self["type"].display,
            len(self.array("section")))


class FirmwareVolume(FieldSet):
    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        if not self._size:
            self._size = self["volume_len"].value * 8

    def createFields(self):
        # Header
        yield NullBytes(self, "zero_vector", 16)
        yield GUID(self, "fs_guid")
        yield UInt64(self, "volume_len")
        yield String(self, "signature", 4)
        yield UInt32(self, "attributes")
        yield UInt16(self, "header_len")
        yield UInt16(self, "checksum")
        yield UInt16(self, "ext_header_offset")
        yield UInt8(self, "reserved")
        yield UInt8(self, "revision")
        while True:
            bm = BlockMap(self, "block_map[]")
            yield bm
            if bm['num_blocks'].value == 0 and bm['len'].value == 0:
                break
        # TODO must handle extended header

        # Content
        while not self.eof:
            padding = paddingSize(self.current_size // 8, 8)
            if padding:
                yield PaddingBytes(self, "padding[]", padding)
            yield File(self, "file[]")

    def createDescription(self):
        return "Firmware Volume containing %d file(s)" % len(self.array("file"))


class PIFVFile(Parser):
    endian = LITTLE_ENDIAN
    MAGIC = '_FVH'
    PARSER_TAGS = {
        "id": "pifv",
        "category": "program",
        "file_ext": ("bin", ""),
        "min_size": 64*8, # smallest possible header
        "magic_regex": (("\0{16}.{24}%s" % MAGIC, 0), ),
        "description": "EFI Platform Initialization Firmware Volume",
    }

    def validate(self):
        if self.stream.readBytes(40*8, 4) != self.MAGIC:
            return "Invalid magic number"
        if self.stream.readBytes(0, 16) != "\0"*16:
            return "Invalid zero vector"
        return True

    def createFields(self):
        while not self.eof:
            yield FirmwareVolume(self, "firmware_volume[]")


########NEW FILE########
__FILENAME__ = torrent
"""
.torrent metainfo file parser

http://wiki.theory.org/BitTorrentSpecification#Metainfo_File_Structure

Status: To statufy
Author: Christophe Gisquet <christophe.gisquet@free.fr>
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    String, RawBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.tools import makePrintable, timestampUNIX, humanFilesize

# Maximum number of bytes for string length
MAX_STRING_LENGTH = 6   # length in 0..999999

# Maximum number of bytes for integer value
MAX_INTEGER_SIZE = 21    # 21 decimal digits (or "-" sign and 20 digits)

class Integer(FieldSet):
    # i<integer encoded in base ten ASCII>e
    def createFields(self):
        yield String(self, "start", 1, "Integer start delimiter (i)", charset="ASCII")

        # Find integer end
        addr = self.absolute_address+self.current_size
        len = self.stream.searchBytesLength('e', False, addr, addr+(MAX_INTEGER_SIZE+1)*8)
        if len is None:
            raise ParserError("Torrent: Unable to find integer end delimiter (e)!")
        if not len:
            raise ParserError("Torrent: error, empty integer!")

        yield String(self, "value", len, "Integer value", charset="ASCII")
        yield String(self, "end", 1, "Integer end delimiter")

    def createValue(self):
        """Read integer value (may raise ValueError)"""
        return int(self["value"].value)

class TorrentString(FieldSet):
    # <string length encoded in base ten ASCII>:<string data>

    def createFields(self):
        addr = self.absolute_address
        len = self.stream.searchBytesLength(':', False, addr, addr+(MAX_STRING_LENGTH+1)*8)
        if len is None:
            raise ParserError("Torrent: unable to find string separator (':')")
        if not len:
            raise ParserError("Torrent: error: no string length!")
        val = String(self, "length", len, "String length")
        yield val
        try:
            len = int(val.value)
        except ValueError:
            len = -1
        if len < 0:
            raise ParserError("Invalid string length (%s)" % makePrintable(val.value, "ASCII", to_unicode=True))
        yield String(self, "separator", 1, "String length/value separator")
        if not len:
            self.info("Empty string: len=%i" % len)
            return
        if len<512:
            yield String(self, "value", len, "String value", charset="ISO-8859-1")
        else:
            # Probably raw data
            yield RawBytes(self, "value", len, "Raw data")

    def createValue(self):
        if "value" in self:
            field = self["value"]
            if field.__class__ != RawBytes:
                return field.value
            else:
                return None
        else:
            return None

class Dictionary(FieldSet):
    # d<bencoded string><bencoded element>e
    def createFields(self):
        yield String(self, "start", 1, "Dictionary start delimiter (d)", charset="ASCII")
        while self.stream.readBytes(self.absolute_address+self.current_size, 1) != "e":
            yield DictionaryItem(self, "item[]")
        yield String(self, "end", 1, "Dictionary end delimiter")

class List(FieldSet):
    # l<bencoded values>e
    def createFields(self):
        yield String(self, "start", 1, "List start delimiter")
        while self.stream.readBytes(self.absolute_address+self.current_size, 1) != "e":
            yield Entry(self, "item[]")
        yield String(self, "end", 1, "List end delimiter")

class DictionaryItem(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)

        # TODO: Remove this because it's not lazy?
        key = self["key"]
        if not key.hasValue():
            return
        key = key.value
        self._name = str(key).replace(" ", "_")

    def createDisplay(self):
        if not self["value"].hasValue():
            return None
        if self._name in ("length", "piece_length"):
            return humanFilesize(self.value)
        return FieldSet.createDisplay(self)

    def createValue(self):
        if not self["value"].hasValue():
            return None
        if self._name == "creation_date":
            return self.createTimestampValue()
        else:
            return self["value"].value

    def createFields(self):
        yield Entry(self, "key")
        yield Entry(self, "value")

    def createTimestampValue(self):
        return timestampUNIX(self["value"].value)

# Map first chunk byte => type
TAGS = {'d': Dictionary, 'i': Integer, 'l': List}
for index in xrange(1, 9+1):
    TAGS[str(index)] = TorrentString

# Create an entry
def Entry(parent, name):
    addr = parent.absolute_address + parent.current_size
    tag = parent.stream.readBytes(addr, 1)
    if tag not in TAGS:
        raise ParserError("Torrent: Entry of type %r not handled" % type)
    cls = TAGS[tag]
    return cls(parent, name)

class TorrentFile(Parser):
    endian = LITTLE_ENDIAN
    MAGIC = "d8:announce"
    PARSER_TAGS = {
        "id": "torrent",
        "category": "misc",
        "file_ext": ("torrent",),
        "min_size": 50*8,
        "mime": (u"application/x-bittorrent",),
        "magic": ((MAGIC, 0),),
        "description": "Torrent metainfo file"
    }

    def validate(self):
        if self.stream.readBytes(0, len(self.MAGIC)) != self.MAGIC:
            return "Invalid magic"
        return True

    def createFields(self):
        yield Dictionary(self, "root", size=self.size)


########NEW FILE########
__FILENAME__ = ttf
"""
TrueType Font parser.

Documents:
 - "An Introduction to TrueType Fonts: A look inside the TTF format"
   written by "NRSI: Computers & Writing Systems"
   http://scripts.sil.org/cms/scripts/page.php?site_id=nrsi&item_id=IWS-Chapter08

Author: Victor Stinner
Creation date: 2007-02-08
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt16, UInt32, Bit, Bits,
    PaddingBits, NullBytes,
    String, RawBytes, Bytes, Enum,
    TimestampMac32)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler

MAX_NAME_COUNT = 300
MIN_NB_TABLE = 3
MAX_NB_TABLE = 30

DIRECTION_NAME = {
    0: u"Mixed directional",
    1: u"Left to right",
    2: u"Left to right + neutrals",
   -1: u"Right to left",
   -2: u"Right to left + neutrals",
}

NAMEID_NAME = {
     0: u"Copyright notice",
     1: u"Font family name",
     2: u"Font subfamily name",
     3: u"Unique font identifier",
     4: u"Full font name",
     5: u"Version string",
     6: u"Postscript name",
     7: u"Trademark",
     8: u"Manufacturer name",
     9: u"Designer",
    10: u"Description",
    11: u"URL Vendor",
    12: u"URL Designer",
    13: u"License Description",
    14: u"License info URL",
    16: u"Preferred Family",
    17: u"Preferred Subfamily",
    18: u"Compatible Full",
    19: u"Sample text",
    20: u"PostScript CID findfont name",
}

PLATFORM_NAME = {
    0: "Unicode",
    1: "Macintosh",
    2: "ISO",
    3: "Microsoft",
    4: "Custom",
}

CHARSET_MAP = {
    # (platform, encoding) => charset
    0: {3: "UTF-16-BE"},
    1: {0: "MacRoman"},
    3: {1: "UTF-16-BE"},
}

class TableHeader(FieldSet):
    def createFields(self):
        yield String(self, "tag", 4)
        yield textHandler(UInt32(self, "checksum"), hexadecimal)
        yield UInt32(self, "offset")
        yield filesizeHandler(UInt32(self, "size"))

    def createDescription(self):
         return "Table entry: %s (%s)" % (self["tag"].display, self["size"].display)

class NameHeader(FieldSet):
    def createFields(self):
        yield Enum(UInt16(self, "platformID"), PLATFORM_NAME)
        yield UInt16(self, "encodingID")
        yield UInt16(self, "languageID")
        yield Enum(UInt16(self, "nameID"), NAMEID_NAME)
        yield UInt16(self, "length")
        yield UInt16(self, "offset")

    def getCharset(self):
        platform = self["platformID"].value
        encoding = self["encodingID"].value
        try:
            return CHARSET_MAP[platform][encoding]
        except KeyError:
            self.warning("TTF: Unknown charset (%s,%s)" % (platform, encoding))
            return "ISO-8859-1"

    def createDescription(self):
        platform = self["platformID"].display
        name = self["nameID"].display
        return "Name record: %s (%s)" % (name, platform)

def parseFontHeader(self):
    yield UInt16(self, "maj_ver", "Major version")
    yield UInt16(self, "min_ver", "Minor version")
    yield UInt16(self, "font_maj_ver", "Font major version")
    yield UInt16(self, "font_min_ver", "Font minor version")
    yield textHandler(UInt32(self, "checksum"), hexadecimal)
    yield Bytes(self, "magic", 4, r"Magic string (\x5F\x0F\x3C\xF5)")
    if self["magic"].value != "\x5F\x0F\x3C\xF5":
        raise ParserError("TTF: invalid magic of font header")

    # Flags
    yield Bit(self, "y0", "Baseline at y=0")
    yield Bit(self, "x0", "Left sidebearing point at x=0")
    yield Bit(self, "instr_point", "Instructions may depend on point size")
    yield Bit(self, "ppem", "Force PPEM to integer values for all")
    yield Bit(self, "instr_width", "Instructions may alter advance width")
    yield Bit(self, "vertical", "e laid out vertically?")
    yield PaddingBits(self, "reserved[]", 1)
    yield Bit(self, "linguistic", "Requires layout for correct linguistic rendering?")
    yield Bit(self, "gx", "Metamorphosis effects?")
    yield Bit(self, "strong", "Contains strong right-to-left glyphs?")
    yield Bit(self, "indic", "contains Indic-style rearrangement effects?")
    yield Bit(self, "lossless", "Data is lossless (Agfa MicroType compression)")
    yield Bit(self, "converted", "Font converted (produce compatible metrics)")
    yield Bit(self, "cleartype", "Optimised for ClearType")
    yield Bits(self, "adobe", 2, "(used by Adobe)")

    yield UInt16(self, "unit_per_em", "Units per em")
    if not(16 <= self["unit_per_em"].value <= 16384):
        raise ParserError("TTF: Invalid unit/em value")
    yield UInt32(self, "created_high")
    yield TimestampMac32(self, "created")
    yield UInt32(self, "modified_high")
    yield TimestampMac32(self, "modified")
    yield UInt16(self, "xmin")
    yield UInt16(self, "ymin")
    yield UInt16(self, "xmax")
    yield UInt16(self, "ymax")

    # Mac style
    yield Bit(self, "bold")
    yield Bit(self, "italic")
    yield Bit(self, "underline")
    yield Bit(self, "outline")
    yield Bit(self, "shadow")
    yield Bit(self, "condensed", "(narrow)")
    yield Bit(self, "expanded")
    yield PaddingBits(self, "reserved[]", 9)

    yield UInt16(self, "lowest", "Smallest readable size in pixels")
    yield Enum(UInt16(self, "font_dir", "Font direction hint"), DIRECTION_NAME)
    yield Enum(UInt16(self, "ofst_format"), {0: "short offsets", 1: "long"})
    yield UInt16(self, "glyph_format", "(=0)")

def parseNames(self):
    # Read header
    yield UInt16(self, "format")
    if self["format"].value != 0:
        raise ParserError("TTF (names): Invalid format (%u)" % self["format"].value)
    yield UInt16(self, "count")
    yield UInt16(self, "offset")
    if MAX_NAME_COUNT < self["count"].value:
        raise ParserError("Invalid number of names (%s)"
            % self["count"].value)

    # Read name index
    entries = []
    for index in xrange(self["count"].value):
        entry = NameHeader(self, "header[]")
        yield entry
        entries.append(entry)

    # Sort names by their offset
    entries.sort(key=lambda field: field["offset"].value)

    # Read name value
    last = None
    for entry in entries:
        # Skip duplicates values
        new = (entry["offset"].value, entry["length"].value)
        if last and last == new:
            self.warning("Skip duplicate %s %s" % (entry.name, new))
            continue
        last = (entry["offset"].value, entry["length"].value)

        # Skip negative offset
        offset = entry["offset"].value + self["offset"].value
        if offset < self.current_size//8:
            self.warning("Skip value %s (negative offset)" % entry.name)
            continue

        # Add padding if any
        padding = self.seekByte(offset, relative=True, null=True)
        if padding:
            yield padding

        # Read value
        size = entry["length"].value
        if size:
            yield String(self, "value[]", size, entry.description, charset=entry.getCharset())

    padding = (self.size - self.current_size) // 8
    if padding:
        yield NullBytes(self, "padding_end", padding)

class Table(FieldSet):
    TAG_INFO = {
        "head": ("header", "Font header", parseFontHeader),
        "name": ("names", "Names", parseNames),
    }

    def __init__(self, parent, name, table, **kw):
        FieldSet.__init__(self, parent, name, **kw)
        self.table = table
        tag = table["tag"].value
        if tag in self.TAG_INFO:
            self._name, self._description, self.parser = self.TAG_INFO[tag]
        else:
            self.parser = None

    def createFields(self):
        if self.parser:
            for field in self.parser(self):
                yield field
        else:
            yield RawBytes(self, "content", self.size//8)

    def createDescription(self):
        return "Table %s (%s)" % (self.table["tag"].value, self.table.path)

class TrueTypeFontFile(Parser):
    endian = BIG_ENDIAN
    PARSER_TAGS = {
        "id": "ttf",
        "category": "misc",
        "file_ext": ("ttf",),
        "min_size": 10*8, # FIXME
        "description": "TrueType font",
    }

    def validate(self):
        if self["maj_ver"].value != 1:
            return "Invalid major version (%u)" % self["maj_ver"].value
        if self["min_ver"].value != 0:
            return "Invalid minor version (%u)" % self["min_ver"].value
        if not (MIN_NB_TABLE <= self["nb_table"].value <= MAX_NB_TABLE):
            return "Invalid number of table (%u)" % self["nb_table"].value
        return True

    def createFields(self):
        yield UInt16(self, "maj_ver", "Major version")
        yield UInt16(self, "min_ver", "Minor version")
        yield UInt16(self, "nb_table")
        yield UInt16(self, "search_range")
        yield UInt16(self, "entry_selector")
        yield UInt16(self, "range_shift")
        tables = []
        for index in xrange(self["nb_table"].value):
            table = TableHeader(self, "table_hdr[]")
            yield table
            tables.append(table)
        tables.sort(key=lambda field: field["offset"].value)
        for table in tables:
            padding = self.seekByte(table["offset"].value, null=True)
            if padding:
                yield padding
            size = table["size"].value
            if size:
                yield Table(self, "table[]", table, size=size*8)
        padding = self.seekBit(self.size, null=True)
        if padding:
            yield padding


########NEW FILE########
__FILENAME__ = word_doc
"""
Documents:

* libwx source code: see fib.c source code
* "Microsoft Word 97 Binary File Format"
   http://bio.gsi.de/DOCS/AIX/wword8.html

   Microsoft Word 97 (aka Version 8) for Windows and Macintosh. From the Office
   book, found in the Microsoft Office Development section in the MSDN Online
   Library. HTMLified June 1998. Revised Aug 1 1998, added missing Definitions
   section. Revised Dec 21 1998, added missing Document Properties (section).
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    Bit, Bits,
    UInt8, Int16, UInt16, UInt32, Int32,
    NullBytes, RawBytes, PascalString16,
    DateTimeMSDOS32)
from lib.hachoir_core.endian import LITTLE_ENDIAN

TIMESTAMP = DateTimeMSDOS32

class BaseWordDocument:
    def createFields(self):
        yield UInt16(self, "wIdent", 2)
        yield UInt16(self, "nFib")
        yield UInt16(self, "nProduct")
        yield UInt16(self, "lid")
        yield Int16(self, "pnNext")

        yield Bit(self, "fDot")
        yield Bit(self, "fGlsy")
        yield Bit(self, "fComplex")
        yield Bit(self, "fHasPic")
        yield Bits(self, "cQuickSaves", 4)
        yield Bit(self, "fEncrypted")
        yield Bit(self, "fWhichTblStm")
        yield Bit(self, "fReadOnlyRecommanded")
        yield Bit(self, "fWriteReservation")
        yield Bit(self, "fExtChar")
        yield Bit(self, "fLoadOverride")
        yield Bit(self, "fFarEeast")
        yield Bit(self, "fCrypto")

        yield UInt16(self, "nFibBack")
        yield UInt32(self, "lKey")
        yield UInt8(self, "envr")

        yield Bit(self, "fMac")
        yield Bit(self, "fEmptySpecial")
        yield Bit(self, "fLoadOverridePage")
        yield Bit(self, "fFutureSavedUndo")
        yield Bit(self, "fWord97Save")
        yield Bits(self, "fSpare0", 3)

        yield UInt16(self, "chse")
        yield UInt16(self, "chsTables")
        yield UInt32(self, "fcMin")
        yield UInt32(self, "fcMac")

        yield PascalString16(self, "file_creator", strip="\0")

        yield NullBytes(self, "reserved[]", 12)

        yield Int16(self, "lidFE")
        yield UInt16(self, "clw")
        yield Int32(self, "cbMac")
        yield UInt32(self, "lProductCreated")
        yield TIMESTAMP(self, "lProductRevised")

        yield UInt32(self, "ccpText")
        yield Int32(self, "ccpFtn")
        yield Int32(self, "ccpHdr")
        yield Int32(self, "ccpMcr")
        yield Int32(self, "ccpAtn")
        yield Int32(self, "ccpEdn")
        yield Int32(self, "ccpTxbx")
        yield Int32(self, "ccpHdrTxbx")
        yield Int32(self, "pnFbpChpFirst")
        yield Int32(self, "pnChpFirst")
        yield Int32(self, "cpnBteChp")
        yield Int32(self, "pnFbpPapFirst")
        yield Int32(self, "pnPapFirst")
        yield Int32(self, "cpnBtePap")
        yield Int32(self, "pnFbpLvcFirst")
        yield Int32(self, "pnLvcFirst")
        yield Int32(self, "cpnBteLvc")
        yield Int32(self, "fcIslandFirst")
        yield Int32(self, "fcIslandLim")
        yield UInt16(self, "cfclcb")
        yield Int32(self, "fcStshfOrig")
        yield UInt32(self, "lcbStshfOrig")
        yield Int32(self, "fcStshf")
        yield UInt32(self, "lcbStshf")

        yield Int32(self, "fcPlcffndRef")
        yield UInt32(self, "lcbPlcffndRef")
        yield Int32(self, "fcPlcffndTxt")
        yield UInt32(self, "lcbPlcffndTxt")
        yield Int32(self, "fcPlcfandRef")
        yield UInt32(self, "lcbPlcfandRef")
        yield Int32(self, "fcPlcfandTxt")
        yield UInt32(self, "lcbPlcfandTxt")
        yield Int32(self, "fcPlcfsed")
        yield UInt32(self, "lcbPlcfsed")
        yield Int32(self, "fcPlcpad")
        yield UInt32(self, "lcbPlcpad")
        yield Int32(self, "fcPlcfphe")
        yield UInt32(self, "lcbPlcfphe")
        yield Int32(self, "fcSttbfglsy")
        yield UInt32(self, "lcbSttbfglsy")
        yield Int32(self, "fcPlcfglsy")
        yield UInt32(self, "lcbPlcfglsy")
        yield Int32(self, "fcPlcfhdd")
        yield UInt32(self, "lcbPlcfhdd")
        yield Int32(self, "fcPlcfbteChpx")
        yield UInt32(self, "lcbPlcfbteChpx")
        yield Int32(self, "fcPlcfbtePapx")
        yield UInt32(self, "lcbPlcfbtePapx")
        yield Int32(self, "fcPlcfsea")
        yield UInt32(self, "lcbPlcfsea")
        yield Int32(self, "fcSttbfffn")
        yield UInt32(self, "lcbSttbfffn")
        yield Int32(self, "fcPlcffldMom")
        yield UInt32(self, "lcbPlcffldMom")
        yield Int32(self, "fcPlcffldHdr")
        yield UInt32(self, "lcbPlcffldHdr")
        yield Int32(self, "fcPlcffldFtn")
        yield UInt32(self, "lcbPlcffldFtn")
        yield Int32(self, "fcPlcffldAtn")
        yield UInt32(self, "lcbPlcffldAtn")
        yield Int32(self, "fcPlcffldMcr")
        yield UInt32(self, "lcbPlcffldMcr")
        yield Int32(self, "fcSttbfbkmk")
        yield UInt32(self, "lcbSttbfbkmk")
        yield Int32(self, "fcPlcfbkf")
        yield UInt32(self, "lcbPlcfbkf")
        yield Int32(self, "fcPlcfbkl")
        yield UInt32(self, "lcbPlcfbkl")
        yield Int32(self, "fcCmds")
        yield UInt32(self, "lcbCmds")
        yield Int32(self, "fcPlcmcr")
        yield UInt32(self, "lcbPlcmcr")
        yield Int32(self, "fcSttbfmcr")
        yield UInt32(self, "lcbSttbfmcr")
        yield Int32(self, "fcPrDrvr")
        yield UInt32(self, "lcbPrDrvr")
        yield Int32(self, "fcPrEnvPort")
        yield UInt32(self, "lcbPrEnvPort")
        yield Int32(self, "fcPrEnvLand")
        yield UInt32(self, "lcbPrEnvLand")
        yield Int32(self, "fcWss")
        yield UInt32(self, "lcbWss")
        yield Int32(self, "fcDop")
        yield UInt32(self, "lcbDop")
        yield Int32(self, "fcSttbfAssoc")
        yield UInt32(self, "lcbSttbfAssoc")
        yield Int32(self, "fcClx")
        yield UInt32(self, "lcbClx")
        yield Int32(self, "fcPlcfpgdFtn")
        yield UInt32(self, "lcbPlcfpgdFtn")
        yield Int32(self, "fcAutosaveSource")
        yield UInt32(self, "lcbAutosaveSource")
        yield Int32(self, "fcGrpXstAtnOwners")
        yield UInt32(self, "lcbGrpXstAtnOwners")
        yield Int32(self, "fcSttbfAtnbkmk")
        yield UInt32(self, "lcbSttbfAtnbkmk")
        yield Int32(self, "fcPlcdoaMom")
        yield UInt32(self, "lcbPlcdoaMom")
        yield Int32(self, "fcPlcdoaHdr")
        yield UInt32(self, "lcbPlcdoaHdr")
        yield Int32(self, "fcPlcspaMom")
        yield UInt32(self, "lcbPlcspaMom")
        yield Int32(self, "fcPlcspaHdr")
        yield UInt32(self, "lcbPlcspaHdr")
        yield Int32(self, "fcPlcfAtnbkf")
        yield UInt32(self, "lcbPlcfAtnbkf")
        yield Int32(self, "fcPlcfAtnbkl")
        yield UInt32(self, "lcbPlcfAtnbkl")
        yield Int32(self, "fcPms")
        yield UInt32(self, "lcbPms")
        yield Int32(self, "fcFormFldSttbs")
        yield UInt32(self, "lcbFormFldSttbs")
        yield Int32(self, "fcPlcfendRef")
        yield UInt32(self, "lcbPlcfendRef")
        yield Int32(self, "fcPlcfendTxt")
        yield UInt32(self, "lcbPlcfendTxt")
        yield Int32(self, "fcPlcffldEdn")
        yield UInt32(self, "lcbPlcffldEdn")
        yield Int32(self, "fcPlcfpgdEdn")
        yield UInt32(self, "lcbPlcfpgdEdn")
        yield Int32(self, "fcDggInfo")
        yield UInt32(self, "lcbDggInfo")
        yield Int32(self, "fcSttbfRMark")
        yield UInt32(self, "lcbSttbfRMark")
        yield Int32(self, "fcSttbCaption")
        yield UInt32(self, "lcbSttbCaption")
        yield Int32(self, "fcSttbAutoCaption")
        yield UInt32(self, "lcbSttbAutoCaption")
        yield Int32(self, "fcPlcfwkb")
        yield UInt32(self, "lcbPlcfwkb")
        yield Int32(self, "fcPlcfspl")
        yield UInt32(self, "lcbPlcfspl")
        yield Int32(self, "fcPlcftxbxTxt")
        yield UInt32(self, "lcbPlcftxbxTxt")
        yield Int32(self, "fcPlcffldTxbx")
        yield UInt32(self, "lcbPlcffldTxbx")
        yield Int32(self, "fcPlcfhdrtxbxTxt")
        yield UInt32(self, "lcbPlcfhdrtxbxTxt")
        yield Int32(self, "fcPlcffldHdrTxbx")
        yield UInt32(self, "lcbPlcffldHdrTxbx")
        yield Int32(self, "fcStwUser")
        yield UInt32(self, "lcbStwUser")
        yield Int32(self, "fcSttbttmbd")
        yield UInt32(self, "cbSttbttmbd")
        yield Int32(self, "fcUnused")
        yield UInt32(self, "lcbUnused")
        yield Int32(self, "fcPgdMother")
        yield UInt32(self, "lcbPgdMother")
        yield Int32(self, "fcBkdMother")
        yield UInt32(self, "lcbBkdMother")
        yield Int32(self, "fcPgdFtn")
        yield UInt32(self, "lcbPgdFtn")
        yield Int32(self, "fcBkdFtn")
        yield UInt32(self, "lcbBkdFtn")
        yield Int32(self, "fcPgdEdn")
        yield UInt32(self, "lcbPgdEdn")
        yield Int32(self, "fcBkdEdn")
        yield UInt32(self, "lcbBkdEdn")
        yield Int32(self, "fcSttbfIntlFld")
        yield UInt32(self, "lcbSttbfIntlFld")
        yield Int32(self, "fcRouteSlip")
        yield UInt32(self, "lcbRouteSlip")
        yield Int32(self, "fcSttbSavedBy")
        yield UInt32(self, "lcbSttbSavedBy")
        yield Int32(self, "fcSttbFnm")
        yield UInt32(self, "lcbSttbFnm")
        yield Int32(self, "fcPlcfLst")
        yield UInt32(self, "lcbPlcfLst")
        yield Int32(self, "fcPlfLfo")
        yield UInt32(self, "lcbPlfLfo")
        yield Int32(self, "fcPlcftxbxBkd")
        yield UInt32(self, "lcbPlcftxbxBkd")
        yield Int32(self, "fcPlcftxbxHdrBkd")
        yield UInt32(self, "lcbPlcftxbxHdrBkd")
        yield Int32(self, "fcDocUndo")
        yield UInt32(self, "lcbDocUndo")
        yield Int32(self, "fcRgbuse")
        yield UInt32(self, "lcbRgbuse")
        yield Int32(self, "fcUsp")
        yield UInt32(self, "lcbUsp")
        yield Int32(self, "fcUskf")
        yield UInt32(self, "lcbUskf")
        yield Int32(self, "fcPlcupcRgbuse")
        yield UInt32(self, "lcbPlcupcRgbuse")
        yield Int32(self, "fcPlcupcUsp")
        yield UInt32(self, "lcbPlcupcUsp")
        yield Int32(self, "fcSttbGlsyStyle")
        yield UInt32(self, "lcbSttbGlsyStyle")
        yield Int32(self, "fcPlgosl")
        yield UInt32(self, "lcbPlgosl")
        yield Int32(self, "fcPlcocx")
        yield UInt32(self, "lcbPlcocx")
        yield Int32(self, "fcPlcfbteLvc")
        yield UInt32(self, "lcbPlcfbteLvc")
        yield TIMESTAMP(self, "ftModified")
        yield Int32(self, "fcPlcflvc")
        yield UInt32(self, "lcbPlcflvc")
        yield Int32(self, "fcPlcasumy")
        yield UInt32(self, "lcbPlcasumy")
        yield Int32(self, "fcPlcfgram")
        yield UInt32(self, "lcbPlcfgram")
        yield Int32(self, "fcSttbListNames")
        yield UInt32(self, "lcbSttbListNames")
        yield Int32(self, "fcSttbfUssr")
        yield UInt32(self, "lcbSttbfUssr")

        tail = (self.size - self.current_size) // 8
        if tail:
            yield RawBytes(self, "tail", tail)

class WordDocumentFieldSet(BaseWordDocument, FieldSet):
    pass

class WordDocumentParser(BaseWordDocument, Parser):
    PARSER_TAGS = {
        "id": "word_document",
        "min_size": 8,
        "description": "Microsoft Office Word document",
    }
    endian = LITTLE_ENDIAN

    def __init__(self, stream, **kw):
        Parser.__init__(self, stream, **kw)

    def validate(self):
        return True


########NEW FILE########
__FILENAME__ = common
from lib.hachoir_core.field import FieldSet, Field, Bits
from lib.hachoir_core.bits import str2hex
from lib.hachoir_parser.network.ouid import REGISTERED_OUID
from lib.hachoir_core.endian import BIG_ENDIAN
from socket import gethostbyaddr, herror as socket_host_error

def ip2name(addr):
    if not ip2name.resolve:
        return addr
    try:
        if addr in ip2name.cache:
            return ip2name.cache[addr]
        # FIXME: Workaround Python bug
        # Need double try/except to catch the bug
        try:
            name = gethostbyaddr(addr)[0]
        except KeyboardInterrupt:
            raise
    except (socket_host_error, ValueError):
        name = addr
    except (socket_host_error, KeyboardInterrupt, ValueError):
        ip2name.resolve = False
        name = addr
    ip2name.cache[addr] = name
    return name
ip2name.cache = {}
ip2name.resolve = True

class IPv4_Address(Field):
    def __init__(self, parent, name, description=None):
        Field.__init__(self, parent, name, 32, description)

    def createValue(self):
        value = self._parent.stream.readBytes(self.absolute_address, 4)
        return ".".join(( "%u" % ord(byte) for byte in value ))

    def createDisplay(self):
        return ip2name(self.value)

class IPv6_Address(Field):
    def __init__(self, parent, name, description=None):
        Field.__init__(self, parent, name, 128, description)

    def createValue(self):
        value = self._parent.stream.readBits(self.absolute_address, 128, self.parent.endian)
        parts = []
        for index in xrange(8):
            part = "%04x" % (value & 0xffff)
            value >>= 16
            parts.append(part)
        return ':'.join(reversed(parts))

    def createDisplay(self):
        return self.value

class OrganizationallyUniqueIdentifier(Bits):
    """
    IEEE 24-bit Organizationally unique identifier
    """
    static_size = 24

    def __init__(self, parent, name, description=None):
        Bits.__init__(self, parent, name, 24, description=None)

    def createDisplay(self, human=True):
        if human:
            key = self.value
            if key in REGISTERED_OUID:
                return REGISTERED_OUID[key]
            else:
                return self.raw_display
        else:
            return self.raw_display

    def createRawDisplay(self):
        value = self.value
        a = value >> 16
        b = (value >> 8) & 0xFF
        c = value & 0xFF
        return "%02X-%02X-%02X" % (a, b, c)

class NIC24(Bits):
    static_size = 24

    def __init__(self, parent, name, description=None):
        Bits.__init__(self, parent, name, 24, description=None)

    def createDisplay(self):
        value = self.value
        a = value >> 16
        b = (value >> 8) & 0xFF
        c = value & 0xFF
        return "%02x:%02x:%02x" % (a, b, c)

    def createRawDisplay(self):
        return "0x%06X" % self.value

class MAC48_Address(FieldSet):
    """
    IEEE 802 48-bit MAC address
    """
    static_size = 48
    endian = BIG_ENDIAN

    def createFields(self):
        yield OrganizationallyUniqueIdentifier(self, "organization")
        yield NIC24(self, "nic")

    def hasValue(self):
        return True

    def createValue(self):
        bytes = self.stream.readBytes(self.absolute_address, 6)
        return str2hex(bytes, format="%02x:")[:-1]

    def createDisplay(self):
        return "%s [%s]" % (self["organization"].display, self["nic"].display)


########NEW FILE########
__FILENAME__ = ouid
# -*- coding: utf-8 -*-
"""
List of registered IEEE 24-bit Organizationally Unique IDentifiers.

Original data file:
http://standards.ieee.org/regauth/oui/oui.txt
"""

REGISTERED_OUID = {
   0x000000: u'XEROX CORPORATION',
   0x000001: u'XEROX CORPORATION',
   0x000002: u'XEROX CORPORATION',
   0x000003: u'XEROX CORPORATION',
   0x000004: u'XEROX CORPORATION',
   0x000005: u'XEROX CORPORATION',
   0x000006: u'XEROX CORPORATION',
   0x000007: u'XEROX CORPORATION',
   0x000008: u'XEROX CORPORATION',
   0x000009: u'XEROX CORPORATION',
   0x00000A: u'OMRON TATEISI ELECTRONICS CO.',
   0x00000B: u'MATRIX CORPORATION',
   0x00000C: u'CISCO SYSTEMS, INC.',
   0x00000D: u'FIBRONICS LTD.',
   0x00000E: u'FUJITSU LIMITED',
   0x00000F: u'NEXT, INC.',
   0x000010: u'SYTEK INC.',
   0x000011: u'NORMEREL SYSTEMES',
   0x000012: u'INFORMATION TECHNOLOGY LIMITED',
   0x000013: u'CAMEX',
   0x000014: u'NETRONIX',
   0x000015: u'DATAPOINT CORPORATION',
   0x000016: u'DU PONT PIXEL SYSTEMS.',
   0x000017: u'TEKELEC',
   0x000018: u'WEBSTER COMPUTER CORPORATION',
   0x000019: u'APPLIED DYNAMICS INTERNATIONAL',
   0x00001A: u'ADVANCED MICRO DEVICES',
   0x00001B: u'NOVELL INC.',
   0x00001C: u'BELL TECHNOLOGIES',
   0x00001D: u'CABLETRON SYSTEMS, INC.',
   0x00001E: u'TELSIST INDUSTRIA ELECTRONICA',
   0x00001F: u'Telco Systems, Inc.',
   0x000020: u'DATAINDUSTRIER DIAB AB',
   0x000021: u'SUREMAN COMP. & COMMUN. CORP.',
   0x000022: u'VISUAL TECHNOLOGY INC.',
   0x000023: u'ABB INDUSTRIAL SYSTEMS AB',
   0x000024: u'CONNECT AS',
   0x000025: u'RAMTEK CORP.',
   0x000026: u'SHA-KEN CO., LTD.',
   0x000027: u'JAPAN RADIO COMPANY',
   0x000028: u'PRODIGY SYSTEMS CORPORATION',
   0x000029: u'IMC NETWORKS CORP.',
   0x00002A: u'TRW - SEDD/INP',
   0x00002B: u'CRISP AUTOMATION, INC',
   0x00002C: u'AUTOTOTE LIMITED',
   0x00002D: u'CHROMATICS INC',
   0x00002E: u'SOCIETE EVIRA',
   0x00002F: u'TIMEPLEX INC.',
   0x000030: u'VG LABORATORY SYSTEMS LTD',
   0x000031: u'QPSX COMMUNICATIONS PTY LTD',
   0x000032: u'Marconi plc',
   0x000033: u'EGAN MACHINERY COMPANY',
   0x000034: u'NETWORK RESOURCES CORPORATION',
   0x000035: u'SPECTRAGRAPHICS CORPORATION',
   0x000036: u'ATARI CORPORATION',
   0x000037: u'OXFORD METRICS LIMITED',
   0x000038: u'CSS LABS',
   0x000039: u'TOSHIBA CORPORATION',
   0x00003A: u'CHYRON CORPORATION',
   0x00003B: u'i Controls, Inc.',
   0x00003C: u'AUSPEX SYSTEMS INC.',
   0x00003D: u'UNISYS',
   0x00003E: u'SIMPACT',
   0x00003F: u'SYNTREX, INC.',
   0x000040: u'APPLICON, INC.',
   0x000041: u'ICE CORPORATION',
   0x000042: u'METIER MANAGEMENT SYSTEMS LTD.',
   0x000043: u'MICRO TECHNOLOGY',
   0x000044: u'CASTELLE CORPORATION',
   0x000045: u'FORD AEROSPACE & COMM. CORP.',
   0x000046: u'OLIVETTI NORTH AMERICA',
   0x000047: u'NICOLET INSTRUMENTS CORP.',
   0x000048: u'SEIKO EPSON CORPORATION',
   0x000049: u'APRICOT COMPUTERS, LTD',
   0x00004A: u'ADC CODENOLL TECHNOLOGY CORP.',
   0x00004B: u'ICL DATA OY',
   0x00004C: u'NEC CORPORATION',
   0x00004D: u'DCI CORPORATION',
   0x00004E: u'AMPEX CORPORATION',
   0x00004F: u'LOGICRAFT, INC.',
   0x000050: u'RADISYS CORPORATION',
   0x000051: u'HOB ELECTRONIC GMBH & CO. KG',
   0x000052: u'Intrusion.com, Inc.',
   0x000053: u'COMPUCORP',
   0x000054: u'MODICON, INC.',
   0x000055: u'COMMISSARIAT A L`ENERGIE ATOM.',
   0x000056: u'DR. B. STRUCK',
   0x000057: u'SCITEX CORPORATION LTD.',
   0x000058: u'RACORE COMPUTER PRODUCTS INC.',
   0x000059: u'HELLIGE GMBH',
   0x00005A: u'SysKonnect GmbH',
   0x00005B: u'ELTEC ELEKTRONIK AG',
   0x00005C: u'TELEMATICS INTERNATIONAL INC.',
   0x00005D: u'CS TELECOM',
   0x00005E: u'USC INFORMATION SCIENCES INST',
   0x00005F: u'SUMITOMO ELECTRIC IND., LTD.',
   0x000060: u'KONTRON ELEKTRONIK GMBH',
   0x000061: u'GATEWAY COMMUNICATIONS',
   0x000062: u'BULL HN INFORMATION SYSTEMS',
   0x000063: u'BARCO CONTROL ROOMS GMBH',
   0x000064: u'YOKOGAWA DIGITAL COMPUTER CORP',
   0x000065: u'Network General Corporation',
   0x000066: u'TALARIS SYSTEMS, INC.',
   0x000067: u'SOFT * RITE, INC.',
   0x000068: u'ROSEMOUNT CONTROLS',
   0x000069: u'CONCORD COMMUNICATIONS INC',
   0x00006A: u'COMPUTER CONSOLES INC.',
   0x00006B: u'SILICON GRAPHICS INC./MIPS',
   0x00006C: u'PRIVATE',
   0x00006D: u'CRAY COMMUNICATIONS, LTD.',
   0x00006E: u'ARTISOFT, INC.',
   0x00006F: u'Madge Ltd.',
   0x000070: u'HCL LIMITED',
   0x000071: u'ADRA SYSTEMS INC.',
   0x000072: u'MINIWARE TECHNOLOGY',
   0x000073: u'SIECOR CORPORATION',
   0x000074: u'RICOH COMPANY LTD.',
   0x000075: u'Nortel Networks',
   0x000076: u'ABEKAS VIDEO SYSTEM',
   0x000077: u'INTERPHASE CORPORATION',
   0x000078: u'LABTAM LIMITED',
   0x000079: u'NETWORTH INCORPORATED',
   0x00007A: u'DANA COMPUTER INC.',
   0x00007B: u'RESEARCH MACHINES',
   0x00007C: u'AMPERE INCORPORATED',
   0x00007D: u'SUN MICROSYSTEMS, INC.',
   0x00007E: u'CLUSTRIX CORPORATION',
   0x00007F: u'LINOTYPE-HELL AG',
   0x000080: u'CRAY COMMUNICATIONS A/S',
   0x000081: u'BAY NETWORKS',
   0x000082: u'LECTRA SYSTEMES SA',
   0x000083: u'TADPOLE TECHNOLOGY PLC',
   0x000084: u'SUPERNET',
   0x000085: u'CANON INC.',
   0x000086: u'MEGAHERTZ CORPORATION',
   0x000087: u'HITACHI, LTD.',
   0x000088: u'COMPUTER NETWORK TECH. CORP.',
   0x000089: u'CAYMAN SYSTEMS INC.',
   0x00008A: u'DATAHOUSE INFORMATION SYSTEMS',
   0x00008B: u'INFOTRON',
   0x00008C: u'Alloy Computer Products (Australia) Pty Ltd',
   0x00008D: u'VERDIX CORPORATION',
   0x00008E: u'SOLBOURNE COMPUTER, INC.',
   0x00008F: u'RAYTHEON COMPANY',
   0x000090: u'MICROCOM',
   0x000091: u'ANRITSU CORPORATION',
   0x000092: u'COGENT DATA TECHNOLOGIES',
   0x000093: u'PROTEON INC.',
   0x000094: u'ASANTE TECHNOLOGIES',
   0x000095: u'SONY TEKTRONIX CORP.',
   0x000096: u'MARCONI ELECTRONICS LTD.',
   0x000097: u'EPOCH SYSTEMS',
   0x000098: u'CROSSCOMM CORPORATION',
   0x000099: u'MTX, INC.',
   0x00009A: u'RC COMPUTER A/S',
   0x00009B: u'INFORMATION INTERNATIONAL, INC',
   0x00009C: u'ROLM MIL-SPEC COMPUTERS',
   0x00009D: u'LOCUS COMPUTING CORPORATION',
   0x00009E: u'MARLI S.A.',
   0x00009F: u'AMERISTAR TECHNOLOGIES INC.',
   0x0000A0: u'SANYO Electric Co., Ltd.',
   0x0000A1: u'MARQUETTE ELECTRIC CO.',
   0x0000A2: u'BAY NETWORKS',
   0x0000A3: u'NETWORK APPLICATION TECHNOLOGY',
   0x0000A4: u'ACORN COMPUTERS LIMITED',
   0x0000A5: u'COMPATIBLE SYSTEMS CORP.',
   0x0000A6: u'NETWORK GENERAL CORPORATION',
   0x0000A7: u'NETWORK COMPUTING DEVICES INC.',
   0x0000A8: u'STRATUS COMPUTER INC.',
   0x0000A9: u'NETWORK SYSTEMS CORP.',
   0x0000AA: u'XEROX CORPORATION',
   0x0000AB: u'LOGIC MODELING CORPORATION',
   0x0000AC: u'CONWARE COMPUTER CONSULTING',
   0x0000AD: u'BRUKER INSTRUMENTS INC.',
   0x0000AE: u'DASSAULT ELECTRONIQUE',
   0x0000AF: u'NUCLEAR DATA INSTRUMENTATION',
   0x0000B0: u'RND-RAD NETWORK DEVICES',
   0x0000B1: u'ALPHA MICROSYSTEMS INC.',
   0x0000B2: u'TELEVIDEO SYSTEMS, INC.',
   0x0000B3: u'CIMLINC INCORPORATED',
   0x0000B4: u'EDIMAX COMPUTER COMPANY',
   0x0000B5: u'DATABILITY SOFTWARE SYS. INC.',
   0x0000B6: u'MICRO-MATIC RESEARCH',
   0x0000B7: u'DOVE COMPUTER CORPORATION',
   0x0000B8: u'SEIKOSHA CO., LTD.',
   0x0000B9: u'MCDONNELL DOUGLAS COMPUTER SYS',
   0x0000BA: u'SIIG, INC.',
   0x0000BB: u'TRI-DATA',
   0x0000BC: u'ALLEN-BRADLEY CO. INC.',
   0x0000BD: u'MITSUBISHI CABLE COMPANY',
   0x0000BE: u'THE NTI GROUP',
   0x0000BF: u'SYMMETRIC COMPUTER SYSTEMS',
   0x0000C0: u'WESTERN DIGITAL CORPORATION',
   0x0000C1: u'Madge Ltd.',
   0x0000C2: u'INFORMATION PRESENTATION TECH.',
   0x0000C3: u'HARRIS CORP COMPUTER SYS DIV',
   0x0000C4: u'WATERS DIV. OF MILLIPORE',
   0x0000C5: u'FARALLON COMPUTING/NETOPIA',
   0x0000C6: u'EON SYSTEMS',
   0x0000C7: u'ARIX CORPORATION',
   0x0000C8: u'ALTOS COMPUTER SYSTEMS',
   0x0000C9: u'EMULEX CORPORATION',
   0x0000CA: u'ARRIS International',
   0x0000CB: u'COMPU-SHACK ELECTRONIC GMBH',
   0x0000CC: u'DENSAN CO., LTD.',
   0x0000CD: u'Allied Telesyn Research Ltd.',
   0x0000CE: u'MEGADATA CORP.',
   0x0000CF: u'HAYES MICROCOMPUTER PRODUCTS',
   0x0000D0: u'DEVELCON ELECTRONICS LTD.',
   0x0000D1: u'ADAPTEC INCORPORATED',
   0x0000D2: u'SBE, INC.',
   0x0000D3: u'WANG LABORATORIES INC.',
   0x0000D4: u'PURE DATA LTD.',
   0x0000D5: u'MICROGNOSIS INTERNATIONAL',
   0x0000D6: u'PUNCH LINE HOLDING',
   0x0000D7: u'DARTMOUTH COLLEGE',
   0x0000D8: u'NOVELL, INC.',
   0x0000D9: u'NIPPON TELEGRAPH & TELEPHONE',
   0x0000DA: u'ATEX',
   0x0000DB: u'BRITISH TELECOMMUNICATIONS PLC',
   0x0000DC: u'HAYES MICROCOMPUTER PRODUCTS',
   0x0000DD: u'TCL INCORPORATED',
   0x0000DE: u'CETIA',
   0x0000DF: u'BELL & HOWELL PUB SYS DIV',
   0x0000E0: u'QUADRAM CORP.',
   0x0000E1: u'GRID SYSTEMS',
   0x0000E2: u'ACER TECHNOLOGIES CORP.',
   0x0000E3: u'INTEGRATED MICRO PRODUCTS LTD',
   0x0000E4: u'IN2 GROUPE INTERTECHNIQUE',
   0x0000E5: u'SIGMEX LTD.',
   0x0000E6: u'APTOR PRODUITS DE COMM INDUST',
   0x0000E7: u'STAR GATE TECHNOLOGIES',
   0x0000E8: u'ACCTON TECHNOLOGY CORP.',
   0x0000E9: u'ISICAD, INC.',
   0x0000EA: u'UPNOD AB',
   0x0000EB: u'MATSUSHITA COMM. IND. CO. LTD.',
   0x0000EC: u'MICROPROCESS',
   0x0000ED: u'APRIL',
   0x0000EE: u'NETWORK DESIGNERS, LTD.',
   0x0000EF: u'KTI',
   0x0000F0: u'SAMSUNG ELECTRONICS CO., LTD.',
   0x0000F1: u'MAGNA COMPUTER CORPORATION',
   0x0000F2: u'SPIDER COMMUNICATIONS',
   0x0000F3: u'GANDALF DATA LIMITED',
   0x0000F4: u'ALLIED TELESYN INTERNATIONAL',
   0x0000F5: u'DIAMOND SALES LIMITED',
   0x0000F6: u'APPLIED MICROSYSTEMS CORP.',
   0x0000F7: u'YOUTH KEEP ENTERPRISE CO LTD',
   0x0000F8: u'DIGITAL EQUIPMENT CORPORATION',
   0x0000F9: u'QUOTRON SYSTEMS INC.',
   0x0000FA: u'MICROSAGE COMPUTER SYSTEMS INC',
   0x0000FB: u'RECHNER ZUR KOMMUNIKATION',
   0x0000FC: u'MEIKO',
   0x0000FD: u'HIGH LEVEL HARDWARE',
   0x0000FE: u'ANNAPOLIS MICRO SYSTEMS',
   0x0000FF: u'CAMTEC ELECTRONICS LTD.',
   0x000100: u'EQUIP\'TRANS',
   0x000101: u'PRIVATE',
   0x000102: u'3COM CORPORATION',
   0x000103: u'3COM CORPORATION',
   0x000104: u'DVICO Co., Ltd.',
   0x000105: u'BECKHOFF GmbH',
   0x000106: u'Tews Datentechnik GmbH',
   0x000107: u'Leiser GmbH',
   0x000108: u'AVLAB Technology, Inc.',
   0x000109: u'Nagano Japan Radio Co., Ltd.',
   0x00010A: u'CIS TECHNOLOGY INC.',
   0x00010B: u'Space CyberLink, Inc.',
   0x00010C: u'System Talks Inc.',
   0x00010D: u'CORECO, INC.',
   0x00010E: u'Bri-Link Technologies Co., Ltd',
   0x00010F: u'McDATA Corporation',
   0x000110: u'Gotham Networks',
   0x000111: u'iDigm Inc.',
   0x000112: u'Shark Multimedia Inc.',
   0x000113: u'OLYMPUS CORPORATION',
   0x000114: u'KANDA TSUSHIN KOGYO CO., LTD.',
   0x000115: u'EXTRATECH CORPORATION',
   0x000116: u'Netspect Technologies, Inc.',
   0x000117: u'CANAL +',
   0x000118: u'EZ Digital Co., Ltd.',
   0x000119: u'RTUnet (Australia)',
   0x00011A: u'EEH DataLink GmbH',
   0x00011B: u'Unizone Technologies, Inc.',
   0x00011C: u'Universal Talkware Corporation',
   0x00011D: u'Centillium Communications',
   0x00011E: u'Precidia Technologies, Inc.',
   0x00011F: u'RC Networks, Inc.',
   0x000120: u'OSCILLOQUARTZ S.A.',
   0x000121: u'Watchguard Technologies, Inc.',
   0x000122: u'Trend Communications, Ltd.',
   0x000123: u'DIGITAL ELECTRONICS CORP.',
   0x000124: u'Acer Incorporated',
   0x000125: u'YAESU MUSEN CO., LTD.',
   0x000126: u'PAC Labs',
   0x000127: u'OPEN Networks Pty Ltd',
   0x000128: u'EnjoyWeb, Inc.',
   0x000129: u'DFI Inc.',
   0x00012A: u'Telematica Sistems Inteligente',
   0x00012B: u'TELENET Co., Ltd.',
   0x00012C: u'Aravox Technologies, Inc.',
   0x00012D: u'Komodo Technology',
   0x00012E: u'PC Partner Ltd.',
   0x00012F: u'Twinhead International Corp',
   0x000130: u'Extreme Networks',
   0x000131: u'Detection Systems, Inc.',
   0x000132: u'Dranetz - BMI',
   0x000133: u'KYOWA Electronic Instruments C',
   0x000134: u'SIG Positec Systems AG',
   0x000135: u'KDC Corp.',
   0x000136: u'CyberTAN Technology, Inc.',
   0x000137: u'IT Farm Corporation',
   0x000138: u'XAVi Technologies Corp.',
   0x000139: u'Point Multimedia Systems',
   0x00013A: u'SHELCAD COMMUNICATIONS, LTD.',
   0x00013B: u'BNA SYSTEMS',
   0x00013C: u'TIW SYSTEMS',
   0x00013D: u'RiscStation Ltd.',
   0x00013E: u'Ascom Tateco AB',
   0x00013F: u'Neighbor World Co., Ltd.',
   0x000140: u'Sendtek Corporation',
   0x000141: u'CABLE PRINT',
   0x000142: u'Cisco Systems, Inc.',
   0x000143: u'Cisco Systems, Inc.',
   0x000144: u'EMC Corporation',
   0x000145: u'WINSYSTEMS, INC.',
   0x000146: u'Tesco Controls, Inc.',
   0x000147: u'Zhone Technologies',
   0x000148: u'X-traWeb Inc.',
   0x000149: u'T.D.T. Transfer Data Test GmbH',
   0x00014A: u'Sony Corporation',
   0x00014B: u'Ennovate Networks, Inc.',
   0x00014C: u'Berkeley Process Control',
   0x00014D: u'Shin Kin Enterprises Co., Ltd',
   0x00014E: u'WIN Enterprises, Inc.',
   0x00014F: u'ADTRAN INC',
   0x000150: u'GILAT COMMUNICATIONS, LTD.',
   0x000151: u'Ensemble Communications',
   0x000152: u'CHROMATEK INC.',
   0x000153: u'ARCHTEK TELECOM CORPORATION',
   0x000154: u'G3M Corporation',
   0x000155: u'Promise Technology, Inc.',
   0x000156: u'FIREWIREDIRECT.COM, INC.',
   0x000157: u'SYSWAVE CO., LTD',
   0x000158: u'Electro Industries/Gauge Tech',
   0x000159: u'S1 Corporation',
   0x00015A: u'Digital Video Broadcasting',
   0x00015B: u'ITALTEL S.p.A/RF-UP-I',
   0x00015C: u'CADANT INC.',
   0x00015D: u'Sun Microsystems, Inc',
   0x00015E: u'BEST TECHNOLOGY CO., LTD.',
   0x00015F: u'DIGITAL DESIGN GmbH',
   0x000160: u'ELMEX Co., LTD.',
   0x000161: u'Meta Machine Technology',
   0x000162: u'Cygnet Technologies, Inc.',
   0x000163: u'Cisco Systems, Inc.',
   0x000164: u'Cisco Systems, Inc.',
   0x000165: u'AirSwitch Corporation',
   0x000166: u'TC GROUP A/S',
   0x000167: u'HIOKI E.E. CORPORATION',
   0x000168: u'VITANA CORPORATION',
   0x000169: u'Celestix Networks Pte Ltd.',
   0x00016A: u'ALITEC',
   0x00016B: u'LightChip, Inc.',
   0x00016C: u'FOXCONN',
   0x00016D: u'CarrierComm Inc.',
   0x00016E: u'Conklin Corporation',
   0x00016F: u'HAITAI ELECTRONICS CO., LTD.',
   0x000170: u'ESE Embedded System Engineer\'g',
   0x000171: u'Allied Data Technologies',
   0x000172: u'TechnoLand Co., LTD.',
   0x000173: u'AMCC',
   0x000174: u'CyberOptics Corporation',
   0x000175: u'Radiant Communications Corp.',
   0x000176: u'Orient Silver Enterprises',
   0x000177: u'EDSL',
   0x000178: u'MARGI Systems, Inc.',
   0x000179: u'WIRELESS TECHNOLOGY, INC.',
   0x00017A: u'Chengdu Maipu Electric Industrial Co., Ltd.',
   0x00017B: u'Heidelberger Druckmaschinen AG',
   0x00017C: u'AG-E GmbH',
   0x00017D: u'ThermoQuest',
   0x00017E: u'ADTEK System Science Co., Ltd.',
   0x00017F: u'Experience Music Project',
   0x000180: u'AOpen, Inc.',
   0x000181: u'Nortel Networks',
   0x000182: u'DICA TECHNOLOGIES AG',
   0x000183: u'ANITE TELECOMS',
   0x000184: u'SIEB & MEYER AG',
   0x000185: u'Aloka Co., Ltd.',
   0x000186: u'Uwe Disch',
   0x000187: u'i2SE GmbH',
   0x000188: u'LXCO Technologies ag',
   0x000189: u'Refraction Technology, Inc.',
   0x00018A: u'ROI COMPUTER AG',
   0x00018B: u'NetLinks Co., Ltd.',
   0x00018C: u'Mega Vision',
   0x00018D: u'AudeSi Technologies',
   0x00018E: u'Logitec Corporation',
   0x00018F: u'Kenetec, Inc.',
   0x000190: u'SMK-M',
   0x000191: u'SYRED Data Systems',
   0x000192: u'Texas Digital Systems',
   0x000193: u'Hanbyul Telecom Co., Ltd.',
   0x000194: u'Capital Equipment Corporation',
   0x000195: u'Sena Technologies, Inc.',
   0x000196: u'Cisco Systems, Inc.',
   0x000197: u'Cisco Systems, Inc.',
   0x000198: u'Darim Vision',
   0x000199: u'HeiSei Electronics',
   0x00019A: u'LEUNIG GmbH',
   0x00019B: u'Kyoto Microcomputer Co., Ltd.',
   0x00019C: u'JDS Uniphase Inc.',
   0x00019D: u'E-Control Systems, Inc.',
   0x00019E: u'ESS Technology, Inc.',
   0x00019F: u'Phonex Broadband',
   0x0001A0: u'Infinilink Corporation',
   0x0001A1: u'Mag-Tek, Inc.',
   0x0001A2: u'Logical Co., Ltd.',
   0x0001A3: u'GENESYS LOGIC, INC.',
   0x0001A4: u'Microlink Corporation',
   0x0001A5: u'Nextcomm, Inc.',
   0x0001A6: u'Scientific-Atlanta Arcodan A/S',
   0x0001A7: u'UNEX TECHNOLOGY CORPORATION',
   0x0001A8: u'Welltech Computer Co., Ltd.',
   0x0001A9: u'BMW AG',
   0x0001AA: u'Airspan Communications, Ltd.',
   0x0001AB: u'Main Street Networks',
   0x0001AC: u'Sitara Networks, Inc.',
   0x0001AD: u'Coach Master International  d.b.a. CMI Worldwide, Inc.',
   0x0001AE: u'Trex Enterprises',
   0x0001AF: u'Motorola Computer Group',
   0x0001B0: u'Fulltek Technology Co., Ltd.',
   0x0001B1: u'General Bandwidth',
   0x0001B2: u'Digital Processing Systems, Inc.',
   0x0001B3: u'Precision Electronic Manufacturing',
   0x0001B4: u'Wayport, Inc.',
   0x0001B5: u'Turin Networks, Inc.',
   0x0001B6: u'SAEJIN T&M Co., Ltd.',
   0x0001B7: u'Centos, Inc.',
   0x0001B8: u'Netsensity, Inc.',
   0x0001B9: u'SKF Condition Monitoring',
   0x0001BA: u'IC-Net, Inc.',
   0x0001BB: u'Frequentis',
   0x0001BC: u'Brains Corporation',
   0x0001BD: u'Peterson Electro-Musical Products, Inc.',
   0x0001BE: u'Gigalink Co., Ltd.',
   0x0001BF: u'Teleforce Co., Ltd.',
   0x0001C0: u'CompuLab, Ltd.',
   0x0001C1: u'Vitesse Semiconductor Corporation',
   0x0001C2: u'ARK Research Corp.',
   0x0001C3: u'Acromag, Inc.',
   0x0001C4: u'NeoWave, Inc.',
   0x0001C5: u'Simpler Networks',
   0x0001C6: u'Quarry Technologies',
   0x0001C7: u'Cisco Systems, Inc.',
   0x0001C8: u'THOMAS CONRAD CORP.',
   0x0001C8: u'CONRAD CORP.',
   0x0001C9: u'Cisco Systems, Inc.',
   0x0001CA: u'Geocast Network Systems, Inc.',
   0x0001CB: u'EVR',
   0x0001CC: u'Japan Total Design Communication Co., Ltd.',
   0x0001CD: u'ARtem',
   0x0001CE: u'Custom Micro Products, Ltd.',
   0x0001CF: u'Alpha Data Parallel Systems, Ltd.',
   0x0001D0: u'VitalPoint, Inc.',
   0x0001D1: u'CoNet Communications, Inc.',
   0x0001D2: u'MacPower Peripherals, Ltd.',
   0x0001D3: u'PAXCOMM, Inc.',
   0x0001D4: u'Leisure Time, Inc.',
   0x0001D5: u'HAEDONG INFO & COMM CO., LTD',
   0x0001D6: u'MAN Roland Druckmaschinen AG',
   0x0001D7: u'F5 Networks, Inc.',
   0x0001D8: u'Teltronics, Inc.',
   0x0001D9: u'Sigma, Inc.',
   0x0001DA: u'WINCOMM Corporation',
   0x0001DB: u'Freecom Technologies GmbH',
   0x0001DC: u'Activetelco',
   0x0001DD: u'Avail Networks',
   0x0001DE: u'Trango Systems, Inc.',
   0x0001DF: u'ISDN Communications, Ltd.',
   0x0001E0: u'Fast Systems, Inc.',
   0x0001E1: u'Kinpo Electronics, Inc.',
   0x0001E2: u'Ando Electric Corporation',
   0x0001E3: u'Siemens AG',
   0x0001E4: u'Sitera, Inc.',
   0x0001E5: u'Supernet, Inc.',
   0x0001E6: u'Hewlett-Packard Company',
   0x0001E7: u'Hewlett-Packard Company',
   0x0001E8: u'Force10 Networks, Inc.',
   0x0001E9: u'Litton Marine Systems B.V.',
   0x0001EA: u'Cirilium Corp.',
   0x0001EB: u'C-COM Corporation',
   0x0001EC: u'Ericsson Group',
   0x0001ED: u'SETA Corp.',
   0x0001EE: u'Comtrol Europe, Ltd.',
   0x0001EF: u'Camtel Technology Corp.',
   0x0001F0: u'Tridium, Inc.',
   0x0001F1: u'Innovative Concepts, Inc.',
   0x0001F2: u'Mark of the Unicorn, Inc.',
   0x0001F3: u'QPS, Inc.',
   0x0001F4: u'Enterasys Networks',
   0x0001F5: u'ERIM S.A.',
   0x0001F6: u'Association of Musical Electronics Industry',
   0x0001F7: u'Image Display Systems, Inc.',
   0x0001F8: u'Adherent Systems, Ltd.',
   0x0001F9: u'TeraGlobal Communications Corp.',
   0x0001FA: u'HOROSCAS',
   0x0001FB: u'DoTop Technology, Inc.',
   0x0001FC: u'Keyence Corporation',
   0x0001FD: u'Digital Voice Systems, Inc.',
   0x0001FE: u'DIGITAL EQUIPMENT CORPORATION',
   0x0001FF: u'Data Direct Networks, Inc.',
   0x000200: u'Net & Sys Co., Ltd.',
   0x000201: u'IFM Electronic gmbh',
   0x000202: u'Amino Communications, Ltd.',
   0x000203: u'Woonsang Telecom, Inc.',
   0x000204: u'Bodmann Industries Elektronik GmbH',
   0x000205: u'Hitachi Denshi, Ltd.',
   0x000206: u'Telital R&D Denmark A/S',
   0x000207: u'VisionGlobal Network Corp.',
   0x000208: u'Unify Networks, Inc.',
   0x000209: u'Shenzhen SED Information Technology Co., Ltd.',
   0x00020A: u'Gefran Spa',
   0x00020B: u'Native Networks, Inc.',
   0x00020C: u'Metro-Optix',
   0x00020D: u'Micronpc.com',
   0x00020E: u'Laurel Networks, Inc.',
   0x00020F: u'AATR',
   0x000210: u'Fenecom',
   0x000211: u'Nature Worldwide Technology Corp.',
   0x000212: u'SierraCom',
   0x000213: u'S.D.E.L.',
   0x000214: u'DTVRO',
   0x000215: u'Cotas Computer Technology A/B',
   0x000216: u'Cisco Systems, Inc.',
   0x000217: u'Cisco Systems, Inc.',
   0x000218: u'Advanced Scientific Corp',
   0x000219: u'Paralon Technologies',
   0x00021A: u'Zuma Networks',
   0x00021B: u'Kollmorgen-Servotronix',
   0x00021C: u'Network Elements, Inc.',
   0x00021D: u'Data General Communication Ltd.',
   0x00021E: u'SIMTEL S.R.L.',
   0x00021F: u'Aculab PLC',
   0x000220: u'Canon Aptex, Inc.',
   0x000221: u'DSP Application, Ltd.',
   0x000222: u'Chromisys, Inc.',
   0x000223: u'ClickTV',
   0x000224: u'C-COR',
   0x000225: u'Certus Technology, Inc.',
   0x000226: u'XESystems, Inc.',
   0x000227: u'ESD GmbH',
   0x000228: u'Necsom, Ltd.',
   0x000229: u'Adtec Corporation',
   0x00022A: u'Asound Electronic',
   0x00022B: u'SAXA, Inc.',
   0x00022C: u'ABB Bomem, Inc.',
   0x00022D: u'Agere Systems',
   0x00022E: u'TEAC Corp. R& D',
   0x00022F: u'P-Cube, Ltd.',
   0x000230: u'Intersoft Electronics',
   0x000231: u'Ingersoll-Rand',
   0x000232: u'Avision, Inc.',
   0x000233: u'Mantra Communications, Inc.',
   0x000234: u'Imperial Technology, Inc.',
   0x000235: u'Paragon Networks International',
   0x000236: u'INIT GmbH',
   0x000237: u'Cosmo Research Corp.',
   0x000238: u'Serome Technology, Inc.',
   0x000239: u'Visicom',
   0x00023A: u'ZSK Stickmaschinen GmbH',
   0x00023B: u'Redback Networks',
   0x00023C: u'Creative Technology, Ltd.',
   0x00023D: u'NuSpeed, Inc.',
   0x00023E: u'Selta Telematica S.p.a',
   0x00023F: u'Compal Electronics, Inc.',
   0x000240: u'Seedek Co., Ltd.',
   0x000241: u'Amer.com',
   0x000242: u'Videoframe Systems',
   0x000243: u'Raysis Co., Ltd.',
   0x000244: u'SURECOM Technology Co.',
   0x000245: u'Lampus Co, Ltd.',
   0x000246: u'All-Win Tech Co., Ltd.',
   0x000247: u'Great Dragon Information Technology (Group) Co., Ltd.',
   0x000248: u'Pilz GmbH & Co.',
   0x000249: u'Aviv Infocom Co, Ltd.',
   0x00024A: u'Cisco Systems, Inc.',
   0x00024B: u'Cisco Systems, Inc.',
   0x00024C: u'SiByte, Inc.',
   0x00024D: u'Mannesman Dematic Colby Pty. Ltd.',
   0x00024E: u'Datacard Group',
   0x00024F: u'IPM Datacom S.R.L.',
   0x000250: u'Geyser Networks, Inc.',
   0x000251: u'Soma Networks, Inc.',
   0x000252: u'Carrier Corporation',
   0x000253: u'Televideo, Inc.',
   0x000254: u'WorldGate',
   0x000255: u'IBM Corporation',
   0x000256: u'Alpha Processor, Inc.',
   0x000257: u'Microcom Corp.',
   0x000258: u'Flying Packets Communications',
   0x000259: u'Tsann Kuen China (Shanghai)Enterprise Co., Ltd. IT Group',
   0x00025A: u'Catena Networks',
   0x00025B: u'Cambridge Silicon Radio',
   0x00025C: u'SCI Systems (Kunshan) Co., Ltd.',
   0x00025D: u'Calix Networks',
   0x00025E: u'High Technology Ltd',
   0x00025F: u'Nortel Networks',
   0x000260: u'Accordion Networks, Inc.',
   0x000261: u'Tilgin AB',
   0x000262: u'Soyo Group Soyo Com Tech Co., Ltd',
   0x000263: u'UPS Manufacturing SRL',
   0x000264: u'AudioRamp.com',
   0x000265: u'Virditech Co. Ltd.',
   0x000266: u'Thermalogic Corporation',
   0x000267: u'NODE RUNNER, INC.',
   0x000268: u'Harris Government Communications',
   0x000269: u'Nadatel Co., Ltd',
   0x00026A: u'Cocess Telecom Co., Ltd.',
   0x00026B: u'BCM Computers Co., Ltd.',
   0x00026C: u'Philips CFT',
   0x00026D: u'Adept Telecom',
   0x00026E: u'NeGeN Access, Inc.',
   0x00026F: u'Senao International Co., Ltd.',
   0x000270: u'Crewave Co., Ltd.',
   0x000271: u'Vpacket Communications',
   0x000272: u'CC&C Technologies, Inc.',
   0x000273: u'Coriolis Networks',
   0x000274: u'Tommy Technologies Corp.',
   0x000275: u'SMART Technologies, Inc.',
   0x000276: u'Primax Electronics Ltd.',
   0x000277: u'Cash Systemes Industrie',
   0x000278: u'Samsung Electro-Mechanics Co., Ltd.',
   0x000279: u'Control Applications, Ltd.',
   0x00027A: u'IOI Technology Corporation',
   0x00027B: u'Amplify Net, Inc.',
   0x00027C: u'Trilithic, Inc.',
   0x00027D: u'Cisco Systems, Inc.',
   0x00027E: u'Cisco Systems, Inc.',
   0x00027F: u'ask-technologies.com',
   0x000280: u'Mu Net, Inc.',
   0x000281: u'Madge Ltd.',
   0x000282: u'ViaClix, Inc.',
   0x000283: u'Spectrum Controls, Inc.',
   0x000284: u'AREVA T&D',
   0x000285: u'Riverstone Networks',
   0x000286: u'Occam Networks',
   0x000287: u'Adapcom',
   0x000288: u'GLOBAL VILLAGE COMMUNICATION',
   0x000289: u'DNE Technologies',
   0x00028A: u'Ambit Microsystems Corporation',
   0x00028B: u'VDSL Systems OY',
   0x00028C: u'Micrel-Synergy Semiconductor',
   0x00028D: u'Movita Technologies, Inc.',
   0x00028E: u'Rapid 5 Networks, Inc.',
   0x00028F: u'Globetek, Inc.',
   0x000290: u'Woorigisool, Inc.',
   0x000291: u'Open Network Co., Ltd.',
   0x000292: u'Logic Innovations, Inc.',
   0x000293: u'Solid Data Systems',
   0x000294: u'Tokyo Sokushin Co., Ltd.',
   0x000295: u'IP.Access Limited',
   0x000296: u'Lectron Co,. Ltd.',
   0x000297: u'C-COR.net',
   0x000298: u'Broadframe Corporation',
   0x000299: u'Apex, Inc.',
   0x00029A: u'Storage Apps',
   0x00029B: u'Kreatel Communications AB',
   0x00029C: u'3COM',
   0x00029D: u'Merix Corp.',
   0x00029E: u'Information Equipment Co., Ltd.',
   0x00029F: u'L-3 Communication Aviation Recorders',
   0x0002A0: u'Flatstack Ltd.',
   0x0002A1: u'World Wide Packets',
   0x0002A2: u'Hilscher GmbH',
   0x0002A3: u'ABB Power Automation',
   0x0002A4: u'AddPac Technology Co., Ltd.',
   0x0002A5: u'Compaq Computer Corporation',
   0x0002A6: u'Effinet Systems Co., Ltd.',
   0x0002A7: u'Vivace Networks',
   0x0002A8: u'Air Link Technology',
   0x0002A9: u'RACOM, s.r.o.',
   0x0002AA: u'PLcom Co., Ltd.',
   0x0002AB: u'CTC Union Technologies Co., Ltd.',
   0x0002AC: u'3PAR data',
   0x0002AD: u'Pentax Corpotation',
   0x0002AE: u'Scannex Electronics Ltd.',
   0x0002AF: u'TeleCruz Technology, Inc.',
   0x0002B0: u'Hokubu Communication & Industrial Co., Ltd.',
   0x0002B1: u'Anritsu, Ltd.',
   0x0002B2: u'Cablevision',
   0x0002B3: u'Intel Corporation',
   0x0002B4: u'DAPHNE',
   0x0002B5: u'Avnet, Inc.',
   0x0002B6: u'Acrosser Technology Co., Ltd.',
   0x0002B7: u'Watanabe Electric Industry Co., Ltd.',
   0x0002B8: u'WHI KONSULT AB',
   0x0002B9: u'Cisco Systems, Inc.',
   0x0002BA: u'Cisco Systems, Inc.',
   0x0002BB: u'Continuous Computing',
   0x0002BC: u'LVL 7 Systems, Inc.',
   0x0002BD: u'Bionet Co., Ltd.',
   0x0002BE: u'Totsu Engineering, Inc.',
   0x0002BF: u'dotRocket, Inc.',
   0x0002C0: u'Bencent Tzeng Industry Co., Ltd.',
   0x0002C1: u'Innovative Electronic Designs, Inc.',
   0x0002C2: u'Net Vision Telecom',
   0x0002C3: u'Arelnet Ltd.',
   0x0002C4: u'Vector International BUBA',
   0x0002C5: u'Evertz Microsystems Ltd.',
   0x0002C6: u'Data Track Technology PLC',
   0x0002C7: u'ALPS ELECTRIC Co., Ltd.',
   0x0002C8: u'Technocom Communications Technology (pte) Ltd',
   0x0002C9: u'Mellanox Technologies',
   0x0002CA: u'EndPoints, Inc.',
   0x0002CB: u'TriState Ltd.',
   0x0002CC: u'M.C.C.I',
   0x0002CD: u'TeleDream, Inc.',
   0x0002CE: u'FoxJet, Inc.',
   0x0002CF: u'ZyGate Communications, Inc.',
   0x0002D0: u'Comdial Corporation',
   0x0002D1: u'Vivotek, Inc.',
   0x0002D2: u'Workstation AG',
   0x0002D3: u'NetBotz, Inc.',
   0x0002D4: u'PDA Peripherals, Inc.',
   0x0002D5: u'ACR',
   0x0002D6: u'NICE Systems',
   0x0002D7: u'EMPEG Ltd',
   0x0002D8: u'BRECIS Communications Corporation',
   0x0002D9: u'Reliable Controls',
   0x0002DA: u'ExiO Communications, Inc.',
   0x0002DB: u'NETSEC',
   0x0002DC: u'Fujitsu General Limited',
   0x0002DD: u'Bromax Communications, Ltd.',
   0x0002DE: u'Astrodesign, Inc.',
   0x0002DF: u'Net Com Systems, Inc.',
   0x0002E0: u'ETAS GmbH',
   0x0002E1: u'Integrated Network Corporation',
   0x0002E2: u'NDC Infared Engineering',
   0x0002E3: u'LITE-ON Communications, Inc.',
   0x0002E4: u'JC HYUN Systems, Inc.',
   0x0002E5: u'Timeware Ltd.',
   0x0002E6: u'Gould Instrument Systems, Inc.',
   0x0002E7: u'CAB GmbH & Co KG',
   0x0002E8: u'E.D.&A.',
   0x0002E9: u'CS Systemes De Securite - C3S',
   0x0002EA: u'Focus Enhancements',
   0x0002EB: u'Pico Communications',
   0x0002EC: u'Maschoff Design Engineering',
   0x0002ED: u'DXO Telecom Co., Ltd.',
   0x0002EE: u'Nokia Danmark A/S',
   0x0002EF: u'CCC Network Systems Group Ltd.',
   0x0002F0: u'AME Optimedia Technology Co., Ltd.',
   0x0002F1: u'Pinetron Co., Ltd.',
   0x0002F2: u'eDevice, Inc.',
   0x0002F3: u'Media Serve Co., Ltd.',
   0x0002F4: u'PCTEL, Inc.',
   0x0002F5: u'VIVE Synergies, Inc.',
   0x0002F6: u'Equipe Communications',
   0x0002F7: u'ARM',
   0x0002F8: u'SEAKR Engineering, Inc.',
   0x0002F9: u'Mimos Semiconductor SDN BHD',
   0x0002FA: u'DX Antenna Co., Ltd.',
   0x0002FB: u'Baumuller Aulugen-Systemtechnik GmbH',
   0x0002FC: u'Cisco Systems, Inc.',
   0x0002FD: u'Cisco Systems, Inc.',
   0x0002FE: u'Viditec, Inc.',
   0x0002FF: u'Handan BroadInfoCom',
   0x000300: u'NetContinuum, Inc.',
   0x000301: u'Avantas Networks Corporation',
   0x000302: u'Charles Industries, Ltd.',
   0x000303: u'JAMA Electronics Co., Ltd.',
   0x000304: u'Pacific Broadband Communications',
   0x000305: u'Smart Network Devices GmbH',
   0x000306: u'Fusion In Tech Co., Ltd.',
   0x000307: u'Secure Works, Inc.',
   0x000308: u'AM Communications, Inc.',
   0x000309: u'Texcel Technology PLC',
   0x00030A: u'Argus Technologies',
   0x00030B: u'Hunter Technology, Inc.',
   0x00030C: u'Telesoft Technologies Ltd.',
   0x00030D: u'Uniwill Computer Corp.',
   0x00030E: u'Core Communications Co., Ltd.',
   0x00030F: u'Digital China (Shanghai) Networks Ltd.',
   0x000310: u'Link Evolution Corp.',
   0x000311: u'Micro Technology Co., Ltd.',
   0x000312: u'TR-Systemtechnik GmbH',
   0x000313: u'Access Media SPA',
   0x000314: u'Teleware Network Systems',
   0x000315: u'Cidco Incorporated',
   0x000316: u'Nobell Communications, Inc.',
   0x000317: u'Merlin Systems, Inc.',
   0x000318: u'Cyras Systems, Inc.',
   0x000319: u'Infineon AG',
   0x00031A: u'Beijing Broad Telecom Ltd., China',
   0x00031B: u'Cellvision Systems, Inc.',
   0x00031C: u'Svenska Hardvarufabriken AB',
   0x00031D: u'Taiwan Commate Computer, Inc.',
   0x00031E: u'Optranet, Inc.',
   0x00031F: u'Condev Ltd.',
   0x000320: u'Xpeed, Inc.',
   0x000321: u'Reco Research Co., Ltd.',
   0x000322: u'IDIS Co., Ltd.',
   0x000323: u'Cornet Technology, Inc.',
   0x000324: u'SANYO Multimedia Tottori Co., Ltd.',
   0x000325: u'Arima Computer Corp.',
   0x000326: u'Iwasaki Information Systems Co., Ltd.',
   0x000327: u'ACT\'L',
   0x000328: u'Mace Group, Inc.',
   0x000329: u'F3, Inc.',
   0x00032A: u'UniData Communication Systems, Inc.',
   0x00032B: u'GAI Datenfunksysteme GmbH',
   0x00032C: u'ABB Industrie AG',
   0x00032D: u'IBASE Technology, Inc.',
   0x00032E: u'Scope Information Management, Ltd.',
   0x00032F: u'Global Sun Technology, Inc.',
   0x000330: u'Imagenics, Co., Ltd.',
   0x000331: u'Cisco Systems, Inc.',
   0x000332: u'Cisco Systems, Inc.',
   0x000333: u'Digitel Co., Ltd.',
   0x000334: u'Newport Electronics',
   0x000335: u'Mirae Technology',
   0x000336: u'Zetes Technologies',
   0x000337: u'Vaone, Inc.',
   0x000338: u'Oak Technology',
   0x000339: u'Eurologic Systems, Ltd.',
   0x00033A: u'Silicon Wave, Inc.',
   0x00033B: u'TAMI Tech Co., Ltd.',
   0x00033C: u'Daiden Co., Ltd.',
   0x00033D: u'ILSHin Lab',
   0x00033E: u'Tateyama System Laboratory Co., Ltd.',
   0x00033F: u'BigBand Networks, Ltd.',
   0x000340: u'Floware Wireless Systems, Ltd.',
   0x000341: u'Axon Digital Design',
   0x000342: u'Nortel Networks',
   0x000343: u'Martin Professional A/S',
   0x000344: u'Tietech.Co., Ltd.',
   0x000345: u'Routrek Networks Corporation',
   0x000346: u'Hitachi Kokusai Electric, Inc.',
   0x000347: u'Intel Corporation',
   0x000348: u'Norscan Instruments, Ltd.',
   0x000349: u'Vidicode Datacommunicatie B.V.',
   0x00034A: u'RIAS Corporation',
   0x00034B: u'Nortel Networks',
   0x00034C: u'Shanghai DigiVision Technology Co., Ltd.',
   0x00034D: u'Chiaro Networks, Ltd.',
   0x00034E: u'Pos Data Company, Ltd.',
   0x00034F: u'Sur-Gard Security',
   0x000350: u'BTICINO SPA',
   0x000351: u'Diebold, Inc.',
   0x000352: u'Colubris Networks',
   0x000353: u'Mitac, Inc.',
   0x000354: u'Fiber Logic Communications',
   0x000355: u'TeraBeam Internet Systems',
   0x000356: u'Wincor Nixdorf GmbH & Co KG',
   0x000357: u'Intervoice-Brite, Inc.',
   0x000358: u'Hanyang Digitech Co., Ltd.',
   0x000359: u'DigitalSis',
   0x00035A: u'Photron Limited',
   0x00035B: u'BridgeWave Communications',
   0x00035C: u'Saint Song Corp.',
   0x00035D: u'Bosung Hi-Net Co., Ltd.',
   0x00035E: u'Metropolitan Area Networks, Inc.',
   0x00035F: u'Prueftechnik Condition Monitoring GmbH & Co. KG',
   0x000360: u'PAC Interactive Technology, Inc.',
   0x000361: u'Widcomm, Inc.',
   0x000362: u'Vodtel Communications, Inc.',
   0x000363: u'Miraesys Co., Ltd.',
   0x000364: u'Scenix Semiconductor, Inc.',
   0x000365: u'Kira Information & Communications, Ltd.',
   0x000366: u'ASM Pacific Technology',
   0x000367: u'Jasmine Networks, Inc.',
   0x000368: u'Embedone Co., Ltd.',
   0x000369: u'Nippon Antenna Co., Ltd.',
   0x00036A: u'Mainnet, Ltd.',
   0x00036B: u'Cisco Systems, Inc.',
   0x00036C: u'Cisco Systems, Inc.',
   0x00036D: u'Runtop, Inc.',
   0x00036E: u'Nicon Systems (Pty) Limited',
   0x00036F: u'Telsey SPA',
   0x000370: u'NXTV, Inc.',
   0x000371: u'Acomz Networks Corp.',
   0x000372: u'ULAN',
   0x000373: u'Aselsan A.S',
   0x000374: u'Hunter Watertech',
   0x000375: u'NetMedia, Inc.',
   0x000376: u'Graphtec Technology, Inc.',
   0x000377: u'Gigabit Wireless',
   0x000378: u'HUMAX Co., Ltd.',
   0x000379: u'Proscend Communications, Inc.',
   0x00037A: u'Taiyo Yuden Co., Ltd.',
   0x00037B: u'IDEC IZUMI Corporation',
   0x00037C: u'Coax Media',
   0x00037D: u'Stellcom',
   0x00037E: u'PORTech Communications, Inc.',
   0x00037F: u'Atheros Communications, Inc.',
   0x000380: u'SSH Communications Security Corp.',
   0x000381: u'Ingenico International',
   0x000382: u'A-One Co., Ltd.',
   0x000383: u'Metera Networks, Inc.',
   0x000384: u'AETA',
   0x000385: u'Actelis Networks, Inc.',
   0x000386: u'Ho Net, Inc.',
   0x000387: u'Blaze Network Products',
   0x000388: u'Fastfame Technology Co., Ltd.',
   0x000389: u'Plantronics',
   0x00038A: u'America Online, Inc.',
   0x00038B: u'PLUS-ONE I&T, Inc.',
   0x00038C: u'Total Impact',
   0x00038D: u'PCS Revenue Control Systems, Inc.',
   0x00038E: u'Atoga Systems, Inc.',
   0x00038F: u'Weinschel Corporation',
   0x000390: u'Digital Video Communications, Inc.',
   0x000391: u'Advanced Digital Broadcast, Ltd.',
   0x000392: u'Hyundai Teletek Co., Ltd.',
   0x000393: u'Apple Computer, Inc.',
   0x000394: u'Connect One',
   0x000395: u'California Amplifier',
   0x000396: u'EZ Cast Co., Ltd.',
   0x000397: u'Watchfront Electronics',
   0x000398: u'WISI',
   0x000399: u'Dongju Informations & Communications Co., Ltd.',
   0x00039A: u'SiConnect',
   0x00039B: u'NetChip Technology, Inc.',
   0x00039C: u'OptiMight Communications, Inc.',
   0x00039D: u'BENQ CORPORATION',
   0x00039E: u'Tera System Co., Ltd.',
   0x00039F: u'Cisco Systems, Inc.',
   0x0003A0: u'Cisco Systems, Inc.',
   0x0003A1: u'HIPER Information & Communication, Inc.',
   0x0003A2: u'Catapult Communications',
   0x0003A3: u'MAVIX, Ltd.',
   0x0003A4: u'Data Storage and Information Management',
   0x0003A5: u'Medea Corporation',
   0x0003A6: u'Traxit Technology, Inc.',
   0x0003A7: u'Unixtar Technology, Inc.',
   0x0003A8: u'IDOT Computers, Inc.',
   0x0003A9: u'AXCENT Media AG',
   0x0003AA: u'Watlow',
   0x0003AB: u'Bridge Information Systems',
   0x0003AC: u'Fronius Schweissmaschinen',
   0x0003AD: u'Emerson Energy Systems AB',
   0x0003AE: u'Allied Advanced Manufacturing Pte, Ltd.',
   0x0003AF: u'Paragea Communications',
   0x0003B0: u'Xsense Technology Corp.',
   0x0003B1: u'Hospira Inc.',
   0x0003B2: u'Radware',
   0x0003B3: u'IA Link Systems Co., Ltd.',
   0x0003B4: u'Macrotek International Corp.',
   0x0003B5: u'Entra Technology Co.',
   0x0003B6: u'QSI Corporation',
   0x0003B7: u'ZACCESS Systems',
   0x0003B8: u'NetKit Solutions, LLC',
   0x0003B9: u'Hualong Telecom Co., Ltd.',
   0x0003BA: u'Sun Microsystems',
   0x0003BB: u'Signal Communications Limited',
   0x0003BC: u'COT GmbH',
   0x0003BD: u'OmniCluster Technologies, Inc.',
   0x0003BE: u'Netility',
   0x0003BF: u'Centerpoint Broadband Technologies, Inc.',
   0x0003C0: u'RFTNC Co., Ltd.',
   0x0003C1: u'Packet Dynamics Ltd',
   0x0003C2: u'Solphone K.K.',
   0x0003C3: u'Micronik Multimedia',
   0x0003C4: u'Tomra Systems ASA',
   0x0003C5: u'Mobotix AG',
   0x0003C6: u'ICUE Systems, Inc.',
   0x0003C7: u'hopf Elektronik GmbH',
   0x0003C8: u'CML Emergency Services',
   0x0003C9: u'TECOM Co., Ltd.',
   0x0003CA: u'MTS Systems Corp.',
   0x0003CB: u'Nippon Systems Development Co., Ltd.',
   0x0003CC: u'Momentum Computer, Inc.',
   0x0003CD: u'Clovertech, Inc.',
   0x0003CE: u'ETEN Technologies, Inc.',
   0x0003CF: u'Muxcom, Inc.',
   0x0003D0: u'KOANKEISO Co., Ltd.',
   0x0003D1: u'Takaya Corporation',
   0x0003D2: u'Crossbeam Systems, Inc.',
   0x0003D3: u'Internet Energy Systems, Inc.',
   0x0003D4: u'Alloptic, Inc.',
   0x0003D5: u'Advanced Communications Co., Ltd.',
   0x0003D6: u'RADVision, Ltd.',
   0x0003D7: u'NextNet Wireless, Inc.',
   0x0003D8: u'iMPath Networks, Inc.',
   0x0003D9: u'Secheron SA',
   0x0003DA: u'Takamisawa Cybernetics Co., Ltd.',
   0x0003DB: u'Apogee Electronics Corp.',
   0x0003DC: u'Lexar Media, Inc.',
   0x0003DD: u'Comark Corp.',
   0x0003DE: u'OTC Wireless',
   0x0003DF: u'Desana Systems',
   0x0003E0: u'RadioFrame Networks, Inc.',
   0x0003E1: u'Winmate Communication, Inc.',
   0x0003E2: u'Comspace Corporation',
   0x0003E3: u'Cisco Systems, Inc.',
   0x0003E4: u'Cisco Systems, Inc.',
   0x0003E5: u'Hermstedt SG',
   0x0003E6: u'Entone Technologies, Inc.',
   0x0003E7: u'Logostek Co. Ltd.',
   0x0003E8: u'Wavelength Digital Limited',
   0x0003E9: u'Akara Canada, Inc.',
   0x0003EA: u'Mega System Technologies, Inc.',
   0x0003EB: u'Atrica',
   0x0003EC: u'ICG Research, Inc.',
   0x0003ED: u'Shinkawa Electric Co., Ltd.',
   0x0003EE: u'MKNet Corporation',
   0x0003EF: u'Oneline AG',
   0x0003F0: u'Redfern Broadband Networks',
   0x0003F1: u'Cicada Semiconductor, Inc.',
   0x0003F2: u'Seneca Networks',
   0x0003F3: u'Dazzle Multimedia, Inc.',
   0x0003F4: u'NetBurner',
   0x0003F5: u'Chip2Chip',
   0x0003F6: u'Allegro Networks, Inc.',
   0x0003F7: u'Plast-Control GmbH',
   0x0003F8: u'SanCastle Technologies, Inc.',
   0x0003F9: u'Pleiades Communications, Inc.',
   0x0003FA: u'TiMetra Networks',
   0x0003FB: u'Toko Seiki Company, Ltd.',
   0x0003FC: u'Intertex Data AB',
   0x0003FD: u'Cisco Systems, Inc.',
   0x0003FE: u'Cisco Systems, Inc.',
   0x0003FF: u'Microsoft Corporation',
   0x000400: u'LEXMARK INTERNATIONAL, INC.',
   0x000401: u'Osaki Electric Co., Ltd.',
   0x000402: u'Nexsan Technologies, Ltd.',
   0x000403: u'Nexsi Corporation',
   0x000404: u'Makino Milling Machine Co., Ltd.',
   0x000405: u'ACN Technologies',
   0x000406: u'Fa. Metabox AG',
   0x000407: u'Topcon Positioning Systems, Inc.',
   0x000408: u'Sanko Electronics Co., Ltd.',
   0x000409: u'Cratos Networks',
   0x00040A: u'Sage Systems',
   0x00040B: u'3com Europe Ltd.',
   0x00040C: u'KANNO Work\'s Ltd.',
   0x00040D: u'Avaya, Inc.',
   0x00040E: u'AVM GmbH',
   0x00040F: u'Asus Network Technologies, Inc.',
   0x000410: u'Spinnaker Networks, Inc.',
   0x000411: u'Inkra Networks, Inc.',
   0x000412: u'WaveSmith Networks, Inc.',
   0x000413: u'SNOM Technology AG',
   0x000414: u'Umezawa Musen Denki Co., Ltd.',
   0x000415: u'Rasteme Systems Co., Ltd.',
   0x000416: u'Parks S/A Comunicacoes Digitais',
   0x000417: u'ELAU AG',
   0x000418: u'Teltronic S.A.U.',
   0x000419: u'Fibercycle Networks, Inc.',
   0x00041A: u'ines GmbH',
   0x00041B: u'Digital Interfaces Ltd.',
   0x00041C: u'ipDialog, Inc.',
   0x00041D: u'Corega of America',
   0x00041E: u'Shikoku Instrumentation Co., Ltd.',
   0x00041F: u'Sony Computer Entertainment, Inc.',
   0x000420: u'Slim Devices, Inc.',
   0x000421: u'Ocular Networks',
   0x000422: u'Gordon Kapes, Inc.',
   0x000423: u'Intel Corporation',
   0x000424: u'TMC s.r.l.',
   0x000425: u'Atmel Corporation',
   0x000426: u'Autosys',
   0x000427: u'Cisco Systems, Inc.',
   0x000428: u'Cisco Systems, Inc.',
   0x000429: u'Pixord Corporation',
   0x00042A: u'Wireless Networks, Inc.',
   0x00042B: u'IT Access Co., Ltd.',
   0x00042C: u'Minet, Inc.',
   0x00042D: u'Sarian Systems, Ltd.',
   0x00042E: u'Netous Technologies, Ltd.',
   0x00042F: u'International Communications Products, Inc.',
   0x000430: u'Netgem',
   0x000431: u'GlobalStreams, Inc.',
   0x000432: u'Voyetra Turtle Beach, Inc.',
   0x000433: u'Cyberboard A/S',
   0x000434: u'Accelent Systems, Inc.',
   0x000435: u'Comptek International, Inc.',
   0x000436: u'ELANsat Technologies, Inc.',
   0x000437: u'Powin Information Technology, Inc.',
   0x000438: u'Nortel Networks',
   0x000439: u'Rosco Entertainment Technology, Inc.',
   0x00043A: u'Intelligent Telecommunications, Inc.',
   0x00043B: u'Lava Computer Mfg., Inc.',
   0x00043C: u'SONOS Co., Ltd.',
   0x00043D: u'INDEL AG',
   0x00043E: u'Telencomm',
   0x00043F: u'Electronic Systems Technology, Inc.',
   0x000440: u'cyberPIXIE, Inc.',
   0x000441: u'Half Dome Systems, Inc.',
   0x000442: u'NACT',
   0x000443: u'Agilent Technologies, Inc.',
   0x000444: u'Western Multiplex Corporation',
   0x000445: u'LMS Skalar Instruments GmbH',
   0x000446: u'CYZENTECH Co., Ltd.',
   0x000447: u'Acrowave Systems Co., Ltd.',
   0x000448: u'Polaroid Professional Imaging',
   0x000449: u'Mapletree Networks',
   0x00044A: u'iPolicy Networks, Inc.',
   0x00044B: u'NVIDIA',
   0x00044C: u'JENOPTIK',
   0x00044D: u'Cisco Systems, Inc.',
   0x00044E: u'Cisco Systems, Inc.',
   0x00044F: u'Leukhardt Systemelektronik GmbH',
   0x000450: u'DMD Computers SRL',
   0x000451: u'Medrad, Inc.',
   0x000452: u'RocketLogix, Inc.',
   0x000453: u'YottaYotta, Inc.',
   0x000454: u'Quadriga UK',
   0x000455: u'ANTARA.net',
   0x000456: u'PipingHot Networks',
   0x000457: u'Universal Access Technology, Inc.',
   0x000458: u'Fusion X Co., Ltd.',
   0x000459: u'Veristar Corporation',
   0x00045A: u'The Linksys Group, Inc.',
   0x00045B: u'Techsan Electronics Co., Ltd.',
   0x00045C: u'Mobiwave Pte Ltd',
   0x00045D: u'BEKA Elektronik',
   0x00045E: u'PolyTrax Information Technology AG',
   0x00045F: u'Evalue Technology, Inc.',
   0x000460: u'Knilink Technology, Inc.',
   0x000461: u'EPOX Computer Co., Ltd.',
   0x000462: u'DAKOS Data & Communication Co., Ltd.',
   0x000463: u'Bosch Security Systems',
   0x000464: u'Fantasma Networks, Inc.',
   0x000465: u'i.s.t isdn-support technik GmbH',
   0x000466: u'ARMITEL Co.',
   0x000467: u'Wuhan Research Institute of MII',
   0x000468: u'Vivity, Inc.',
   0x000469: u'Innocom, Inc.',
   0x00046A: u'Navini Networks',
   0x00046B: u'Palm Wireless, Inc.',
   0x00046C: u'Cyber Technology Co., Ltd.',
   0x00046D: u'Cisco Systems, Inc.',
   0x00046E: u'Cisco Systems, Inc.',
   0x00046F: u'Digitel S/A Industria Eletronica',
   0x000470: u'ipUnplugged AB',
   0x000471: u'IPrad',
   0x000472: u'Telelynx, Inc.',
   0x000473: u'Photonex Corporation',
   0x000474: u'LEGRAND',
   0x000475: u'3 Com Corporation',
   0x000476: u'3 Com Corporation',
   0x000477: u'Scalant Systems, Inc.',
   0x000478: u'G. Star Technology Corporation',
   0x000479: u'Radius Co., Ltd.',
   0x00047A: u'AXXESSIT ASA',
   0x00047B: u'Schlumberger',
   0x00047C: u'Skidata AG',
   0x00047D: u'Pelco',
   0x00047E: u'Optelecom=NKF',
   0x00047F: u'Chr. Mayr GmbH & Co. KG',
   0x000480: u'Foundry Networks, Inc.',
   0x000481: u'Econolite Control Products, Inc.',
   0x000482: u'Medialogic Corp.',
   0x000483: u'Deltron Technology, Inc.',
   0x000484: u'Amann GmbH',
   0x000485: u'PicoLight',
   0x000486: u'ITTC, University of Kansas',
   0x000487: u'Cogency Semiconductor, Inc.',
   0x000488: u'Eurotherm Controls',
   0x000489: u'YAFO Networks, Inc.',
   0x00048A: u'Temia Vertriebs GmbH',
   0x00048B: u'Poscon Corporation',
   0x00048C: u'Nayna Networks, Inc.',
   0x00048D: u'Tone Commander Systems, Inc.',
   0x00048E: u'Ohm Tech Labs, Inc.',
   0x00048F: u'TD Systems Corp.',
   0x000490: u'Optical Access',
   0x000491: u'Technovision, Inc.',
   0x000492: u'Hive Internet, Ltd.',
   0x000493: u'Tsinghua Unisplendour Co., Ltd.',
   0x000494: u'Breezecom, Ltd.',
   0x000495: u'Tejas Networks',
   0x000496: u'Extreme Networks',
   0x000497: u'MacroSystem Digital Video AG',
   0x000498: u'Mahi Networks',
   0x000499: u'Chino Corporation',
   0x00049A: u'Cisco Systems, Inc.',
   0x00049B: u'Cisco Systems, Inc.',
   0x00049C: u'Surgient Networks, Inc.',
   0x00049D: u'Ipanema Technologies',
   0x00049E: u'Wirelink Co., Ltd.',
   0x00049F: u'Freescale Semiconductor',
   0x0004A0: u'Verity Instruments, Inc.',
   0x0004A1: u'Pathway Connectivity',
   0x0004A2: u'L.S.I. Japan Co., Ltd.',
   0x0004A3: u'Microchip Technology, Inc.',
   0x0004A4: u'NetEnabled, Inc.',
   0x0004A5: u'Barco Projection Systems NV',
   0x0004A6: u'SAF Tehnika Ltd.',
   0x0004A7: u'FabiaTech Corporation',
   0x0004A8: u'Broadmax Technologies, Inc.',
   0x0004A9: u'SandStream Technologies, Inc.',
   0x0004AA: u'Jetstream Communications',
   0x0004AB: u'Comverse Network Systems, Inc.',
   0x0004AC: u'IBM CORP.',
   0x0004AD: u'Malibu Networks',
   0x0004AE: u'Liquid Metronics',
   0x0004AF: u'Digital Fountain, Inc.',
   0x0004B0: u'ELESIGN Co., Ltd.',
   0x0004B1: u'Signal Technology, Inc.',
   0x0004B2: u'ESSEGI SRL',
   0x0004B3: u'Videotek, Inc.',
   0x0004B4: u'CIAC',
   0x0004B5: u'Equitrac Corporation',
   0x0004B6: u'Stratex Networks, Inc.',
   0x0004B7: u'AMB i.t. Holding',
   0x0004B8: u'Kumahira Co., Ltd.',
   0x0004B9: u'S.I. Soubou, Inc.',
   0x0004BA: u'KDD Media Will Corporation',
   0x0004BB: u'Bardac Corporation',
   0x0004BC: u'Giantec, Inc.',
   0x0004BD: u'Motorola BCS',
   0x0004BE: u'OptXCon, Inc.',
   0x0004BF: u'VersaLogic Corp.',
   0x0004C0: u'Cisco Systems, Inc.',
   0x0004C1: u'Cisco Systems, Inc.',
   0x0004C2: u'Magnipix, Inc.',
   0x0004C3: u'CASTOR Informatique',
   0x0004C4: u'Allen & Heath Limited',
   0x0004C5: u'ASE Technologies, USA',
   0x0004C6: u'Yamaha Motor Co., Ltd.',
   0x0004C7: u'NetMount',
   0x0004C8: u'LIBA Maschinenfabrik GmbH',
   0x0004C9: u'Micro Electron Co., Ltd.',
   0x0004CA: u'FreeMs Corp.',
   0x0004CB: u'Tdsoft Communication, Ltd.',
   0x0004CC: u'Peek Traffic B.V.',
   0x0004CD: u'Informedia Research Group',
   0x0004CE: u'Patria Ailon',
   0x0004CF: u'Seagate Technology',
   0x0004D0: u'Softlink s.r.o.',
   0x0004D1: u'Drew Technologies, Inc.',
   0x0004D2: u'Adcon Telemetry GmbH',
   0x0004D3: u'Toyokeiki Co., Ltd.',
   0x0004D4: u'Proview Electronics Co., Ltd.',
   0x0004D5: u'Hitachi Communication Systems, Inc.',
   0x0004D6: u'Takagi Industrial Co., Ltd.',
   0x0004D7: u'Omitec Instrumentation Ltd.',
   0x0004D8: u'IPWireless, Inc.',
   0x0004D9: u'Titan Electronics, Inc.',
   0x0004DA: u'Relax Technology, Inc.',
   0x0004DB: u'Tellus Group Corp.',
   0x0004DC: u'Nortel Networks',
   0x0004DD: u'Cisco Systems, Inc.',
   0x0004DE: u'Cisco Systems, Inc.',
   0x0004DF: u'Teracom Telematica Ltda.',
   0x0004E0: u'Procket Networks',
   0x0004E1: u'Infinior Microsystems',
   0x0004E2: u'SMC Networks, Inc.',
   0x0004E3: u'Accton Technology Corp.',
   0x0004E4: u'Daeryung Ind., Inc.',
   0x0004E5: u'Glonet Systems, Inc.',
   0x0004E6: u'Banyan Network Private Limited',
   0x0004E7: u'Lightpointe Communications, Inc',
   0x0004E8: u'IER, Inc.',
   0x0004E9: u'Infiniswitch Corporation',
   0x0004EA: u'Hewlett-Packard Company',
   0x0004EB: u'Paxonet Communications, Inc.',
   0x0004EC: u'Memobox SA',
   0x0004ED: u'Billion Electric Co., Ltd.',
   0x0004EE: u'Lincoln Electric Company',
   0x0004EF: u'Polestar Corp.',
   0x0004F0: u'International Computers, Ltd',
   0x0004F1: u'WhereNet',
   0x0004F2: u'Polycom',
   0x0004F3: u'FS FORTH-SYSTEME GmbH',
   0x0004F4: u'Infinite Electronics Inc.',
   0x0004F5: u'SnowShore Networks, Inc.',
   0x0004F6: u'Amphus',
   0x0004F7: u'Omega Band, Inc.',
   0x0004F8: u'QUALICABLE TV Industria E Com., Ltda',
   0x0004F9: u'Xtera Communications, Inc.',
   0x0004FA: u'NBS Technologies Inc.',
   0x0004FB: u'Commtech, Inc.',
   0x0004FC: u'Stratus Computer (DE), Inc.',
   0x0004FD: u'Japan Control Engineering Co., Ltd.',
   0x0004FE: u'Pelago Networks',
   0x0004FF: u'Acronet Co., Ltd.',
   0x000500: u'Cisco Systems, Inc.',
   0x000501: u'Cisco Systems, Inc.',
   0x000502: u'APPLE COMPUTER',
   0x000503: u'ICONAG',
   0x000504: u'Naray Information & Communication Enterprise',
   0x000505: u'Systems Integration Solutions, Inc.',
   0x000506: u'Reddo Networks AB',
   0x000507: u'Fine Appliance Corp.',
   0x000508: u'Inetcam, Inc.',
   0x000509: u'AVOC Nishimura Ltd.',
   0x00050A: u'ICS Spa',
   0x00050B: u'SICOM Systems, Inc.',
   0x00050C: u'Network Photonics, Inc.',
   0x00050D: u'Midstream Technologies, Inc.',
   0x00050E: u'3ware, Inc.',
   0x00050F: u'Tanaka S/S Ltd.',
   0x000510: u'Infinite Shanghai Communication Terminals Ltd.',
   0x000511: u'Complementary Technologies Ltd',
   0x000512: u'MeshNetworks, Inc.',
   0x000513: u'VTLinx Multimedia Systems, Inc.',
   0x000514: u'KDT Systems Co., Ltd.',
   0x000515: u'Nuark Co., Ltd.',
   0x000516: u'SMART Modular Technologies',
   0x000517: u'Shellcomm, Inc.',
   0x000518: u'Jupiters Technology',
   0x000519: u'Siemens Building Technologies AG,',
   0x00051A: u'3Com Europe Ltd.',
   0x00051B: u'Magic Control Technology Corporation',
   0x00051C: u'Xnet Technology Corp.',
   0x00051D: u'Airocon, Inc.',
   0x00051E: u'Brocade Communications Systems, Inc.',
   0x00051F: u'Taijin Media Co., Ltd.',
   0x000520: u'Smartronix, Inc.',
   0x000521: u'Control Microsystems',
   0x000522: u'LEA*D Corporation, Inc.',
   0x000523: u'AVL List GmbH',
   0x000524: u'BTL System (HK) Limited',
   0x000525: u'Puretek Industrial Co., Ltd.',
   0x000526: u'IPAS GmbH',
   0x000527: u'SJ Tek Co. Ltd',
   0x000528: u'New Focus, Inc.',
   0x000529: u'Shanghai Broadan Communication Technology Co., Ltd',
   0x00052A: u'Ikegami Tsushinki Co., Ltd.',
   0x00052B: u'HORIBA, Ltd.',
   0x00052C: u'Supreme Magic Corporation',
   0x00052D: u'Zoltrix International Limited',
   0x00052E: u'Cinta Networks',
   0x00052F: u'Leviton Voice and Data',
   0x000530: u'Andiamo Systems, Inc.',
   0x000531: u'Cisco Systems, Inc.',
   0x000532: u'Cisco Systems, Inc.',
   0x000533: u'Sanera Systems, Inc.',
   0x000534: u'Northstar Engineering Ltd.',
   0x000535: u'Chip PC Ltd.',
   0x000536: u'Danam Communications, Inc.',
   0x000537: u'Nets Technology Co., Ltd.',
   0x000538: u'Merilus, Inc.',
   0x000539: u'A Brand New World in Sweden AB',
   0x00053A: u'Willowglen Services Pte Ltd',
   0x00053B: u'Harbour Networks Ltd., Co. Beijing',
   0x00053C: u'Xircom',
   0x00053D: u'Agere Systems',
   0x00053E: u'KID Systeme GmbH',
   0x00053F: u'VisionTek, Inc.',
   0x000540: u'FAST Corporation',
   0x000541: u'Advanced Systems Co., Ltd.',
   0x000542: u'Otari, Inc.',
   0x000543: u'IQ Wireless GmbH',
   0x000544: u'Valley Technologies, Inc.',
   0x000545: u'Internet Photonics',
   0x000546: u'KDDI Network & Solultions Inc.',
   0x000547: u'Starent Networks',
   0x000548: u'Disco Corporation',
   0x000549: u'Salira Optical Network Systems',
   0x00054A: u'Ario Data Networks, Inc.',
   0x00054B: u'Micro Innovation AG',
   0x00054C: u'RF Innovations Pty Ltd',
   0x00054D: u'Brans Technologies, Inc.',
   0x00054E: u'Philips Components',
   0x00054F: u'PRIVATE',
   0x000550: u'Vcomms Limited',
   0x000551: u'F & S Elektronik Systeme GmbH',
   0x000552: u'Xycotec Computer GmbH',
   0x000553: u'DVC Company, Inc.',
   0x000554: u'Rangestar Wireless',
   0x000555: u'Japan Cash Machine Co., Ltd.',
   0x000556: u'360 Systems',
   0x000557: u'Agile TV Corporation',
   0x000558: u'Synchronous, Inc.',
   0x000559: u'Intracom S.A.',
   0x00055A: u'Power Dsine Ltd.',
   0x00055B: u'Charles Industries, Ltd.',
   0x00055C: u'Kowa Company, Ltd.',
   0x00055D: u'D-Link Systems, Inc.',
   0x00055E: u'Cisco Systems, Inc.',
   0x00055F: u'Cisco Systems, Inc.',
   0x000560: u'LEADER COMM.CO., LTD',
   0x000561: u'nac Image Technology, Inc.',
   0x000562: u'Digital View Limited',
   0x000563: u'J-Works, Inc.',
   0x000564: u'Tsinghua Bitway Co., Ltd.',
   0x000565: u'Tailyn Communication Company Ltd.',
   0x000566: u'Secui.com Corporation',
   0x000567: u'Etymonic Design, Inc.',
   0x000568: u'Piltofish Networks AB',
   0x000569: u'VMWARE, Inc.',
   0x00056A: u'Heuft Systemtechnik GmbH',
   0x00056B: u'C.P. Technology Co., Ltd.',
   0x00056C: u'Hung Chang Co., Ltd.',
   0x00056D: u'Pacific Corporation',
   0x00056E: u'National Enhance Technology, Inc.',
   0x00056F: u'Innomedia Technologies Pvt. Ltd.',
   0x000570: u'Baydel Ltd.',
   0x000571: u'Seiwa Electronics Co.',
   0x000572: u'Deonet Co., Ltd.',
   0x000573: u'Cisco Systems, Inc.',
   0x000574: u'Cisco Systems, Inc.',
   0x000575: u'CDS-Electronics BV',
   0x000576: u'NSM Technology Ltd.',
   0x000577: u'SM Information & Communication',
   0x000578: u'PRIVATE',
   0x000579: u'Universal Control Solution Corp.',
   0x00057A: u'Hatteras Networks',
   0x00057B: u'Chung Nam Electronic Co., Ltd.',
   0x00057C: u'RCO Security AB',
   0x00057D: u'Sun Communications, Inc.',
   0x00057E: u'Eckelmann Steuerungstechnik GmbH',
   0x00057F: u'Acqis Technology',
   0x000580: u'Fibrolan Ltd.',
   0x000581: u'Snell & Wilcox Ltd.',
   0x000582: u'ClearCube Technology',
   0x000583: u'ImageCom Limited',
   0x000584: u'AbsoluteValue Systems, Inc.',
   0x000585: u'Juniper Networks, Inc.',
   0x000586: u'Lucent Technologies',
   0x000587: u'Locus, Incorporated',
   0x000588: u'Sensoria Corp.',
   0x000589: u'National Datacomputer',
   0x00058A: u'Netcom Co., Ltd.',
   0x00058B: u'IPmental, Inc.',
   0x00058C: u'Opentech Inc.',
   0x00058D: u'Lynx Photonic Networks, Inc.',
   0x00058E: u'Flextronics International GmbH & Co. Nfg. KG',
   0x00058F: u'CLCsoft co.',
   0x000590: u'Swissvoice Ltd.',
   0x000591: u'Active Silicon Ltd.',
   0x000592: u'Pultek Corp.',
   0x000593: u'Grammar Engine Inc.',
   0x000594: u'IXXAT Automation GmbH',
   0x000595: u'Alesis Corporation',
   0x000596: u'Genotech Co., Ltd.',
   0x000597: u'Eagle Traffic Control Systems',
   0x000598: u'CRONOS S.r.l.',
   0x000599: u'DRS Test and Energy Management or DRS-TEM',
   0x00059A: u'Cisco Systems, Inc.',
   0x00059B: u'Cisco Systems, Inc.',
   0x00059C: u'Kleinknecht GmbH, Ing. Buero',
   0x00059D: u'Daniel Computing Systems, Inc.',
   0x00059E: u'Zinwell Corporation',
   0x00059F: u'Yotta Networks, Inc.',
   0x0005A0: u'MOBILINE Kft.',
   0x0005A1: u'Zenocom',
   0x0005A2: u'CELOX Networks',
   0x0005A3: u'QEI, Inc.',
   0x0005A4: u'Lucid Voice Ltd.',
   0x0005A5: u'KOTT',
   0x0005A6: u'Extron Electronics',
   0x0005A7: u'Hyperchip, Inc.',
   0x0005A8: u'WYLE ELECTRONICS',
   0x0005A9: u'Princeton Networks, Inc.',
   0x0005AA: u'Moore Industries International Inc.',
   0x0005AB: u'Cyber Fone, Inc.',
   0x0005AC: u'Northern Digital, Inc.',
   0x0005AD: u'Topspin Communications, Inc.',
   0x0005AE: u'Mediaport USA',
   0x0005AF: u'InnoScan Computing A/S',
   0x0005B0: u'Korea Computer Technology Co., Ltd.',
   0x0005B1: u'ASB Technology BV',
   0x0005B2: u'Medison Co., Ltd.',
   0x0005B3: u'Asahi-Engineering Co., Ltd.',
   0x0005B4: u'Aceex Corporation',
   0x0005B5: u'Broadcom Technologies',
   0x0005B6: u'INSYS Microelectronics GmbH',
   0x0005B7: u'Arbor Technology Corp.',
   0x0005B8: u'Electronic Design Associates, Inc.',
   0x0005B9: u'Airvana, Inc.',
   0x0005BA: u'Area Netwoeks, Inc.',
   0x0005BB: u'Myspace AB',
   0x0005BC: u'Resorsys Ltd.',
   0x0005BD: u'ROAX BV',
   0x0005BE: u'Kongsberg Seatex AS',
   0x0005BF: u'JustEzy Technology, Inc.',
   0x0005C0: u'Digital Network Alacarte Co., Ltd.',
   0x0005C1: u'A-Kyung Motion, Inc.',
   0x0005C2: u'Soronti, Inc.',
   0x0005C3: u'Pacific Instruments, Inc.',
   0x0005C4: u'Telect, Inc.',
   0x0005C5: u'Flaga HF',
   0x0005C6: u'Triz Communications',
   0x0005C7: u'I/F-COM A/S',
   0x0005C8: u'VERYTECH',
   0x0005C9: u'LG Innotek',
   0x0005CA: u'Hitron Technology, Inc.',
   0x0005CB: u'ROIS Technologies, Inc.',
   0x0005CC: u'Sumtel Communications, Inc.',
   0x0005CD: u'Denon, Ltd.',
   0x0005CE: u'Prolink Microsystems Corporation',
   0x0005CF: u'Thunder River Technologies, Inc.',
   0x0005D0: u'Solinet Systems',
   0x0005D1: u'Metavector Technologies',
   0x0005D2: u'DAP Technologies',
   0x0005D3: u'eProduction Solutions, Inc.',
   0x0005D4: u'FutureSmart Networks, Inc.',
   0x0005D5: u'Speedcom Wireless',
   0x0005D6: u'Titan Wireless',
   0x0005D7: u'Vista Imaging, Inc.',
   0x0005D8: u'Arescom, Inc.',
   0x0005D9: u'Techno Valley, Inc.',
   0x0005DA: u'Apex Automationstechnik',
   0x0005DB: u'Nentec GmbH',
   0x0005DC: u'Cisco Systems, Inc.',
   0x0005DD: u'Cisco Systems, Inc.',
   0x0005DE: u'Gi Fone Korea, Inc.',
   0x0005DF: u'Electronic Innovation, Inc.',
   0x0005E0: u'Empirix Corp.',
   0x0005E1: u'Trellis Photonics, Ltd.',
   0x0005E2: u'Creativ Network Technologies',
   0x0005E3: u'LightSand Communications, Inc.',
   0x0005E4: u'Red Lion Controls L.P.',
   0x0005E5: u'Renishaw PLC',
   0x0005E6: u'Egenera, Inc.',
   0x0005E7: u'Netrake Corp.',
   0x0005E8: u'TurboWave, Inc.',
   0x0005E9: u'Unicess Network, Inc.',
   0x0005EA: u'Rednix',
   0x0005EB: u'Blue Ridge Networks, Inc.',
   0x0005EC: u'Mosaic Systems Inc.',
   0x0005ED: u'Technikum Joanneum GmbH',
   0x0005EE: u'BEWATOR Group',
   0x0005EF: u'ADOIR Digital Technology',
   0x0005F0: u'SATEC',
   0x0005F1: u'Vrcom, Inc.',
   0x0005F2: u'Power R, Inc.',
   0x0005F3: u'Weboyn',
   0x0005F4: u'System Base Co., Ltd.',
   0x0005F5: u'OYO Geospace Corp.',
   0x0005F6: u'Young Chang Co. Ltd.',
   0x0005F7: u'Analog Devices, Inc.',
   0x0005F8: u'Real Time Access, Inc.',
   0x0005F9: u'TOA Corporation',
   0x0005FA: u'IPOptical, Inc.',
   0x0005FB: u'ShareGate, Inc.',
   0x0005FC: u'Schenck Pegasus Corp.',
   0x0005FD: u'PacketLight Networks Ltd.',
   0x0005FE: u'Traficon N.V.',
   0x0005FF: u'SNS Solutions, Inc.',
   0x000600: u'Toshiba Teli Corporation',
   0x000601: u'Otanikeiki Co., Ltd.',
   0x000602: u'Cirkitech Electronics Co.',
   0x000603: u'Baker Hughes Inc.',
   0x000604: u'@Track Communications, Inc.',
   0x000605: u'Inncom International, Inc.',
   0x000606: u'RapidWAN, Inc.',
   0x000607: u'Omni Directional Control Technology Inc.',
   0x000608: u'At-Sky SAS',
   0x000609: u'Crossport Systems',
   0x00060A: u'Blue2space',
   0x00060B: u'Paceline Systems Corporation',
   0x00060C: u'Melco Industries, Inc.',
   0x00060D: u'Wave7 Optics',
   0x00060E: u'IGYS Systems, Inc.',
   0x00060F: u'Narad Networks Inc',
   0x000610: u'Abeona Networks Inc',
   0x000611: u'Zeus Wireless, Inc.',
   0x000612: u'Accusys, Inc.',
   0x000613: u'Kawasaki Microelectronics Incorporated',
   0x000614: u'Prism Holdings',
   0x000615: u'Kimoto Electric Co., Ltd.',
   0x000616: u'Tel Net Co., Ltd.',
   0x000617: u'Redswitch Inc.',
   0x000618: u'DigiPower Manufacturing Inc.',
   0x000619: u'Connection Technology Systems',
   0x00061A: u'Zetari Inc.',
   0x00061B: u'Portable Systems, IBM Japan Co, Ltd',
   0x00061C: u'Hoshino Metal Industries, Ltd.',
   0x00061D: u'MIP Telecom, Inc.',
   0x00061E: u'Maxan Systems',
   0x00061F: u'Vision Components GmbH',
   0x000620: u'Serial System Ltd.',
   0x000621: u'Hinox, Co., Ltd.',
   0x000622: u'Chung Fu Chen Yeh Enterprise Corp.',
   0x000623: u'MGE UPS Systems France',
   0x000624: u'Gentner Communications Corp.',
   0x000625: u'The Linksys Group, Inc.',
   0x000626: u'MWE GmbH',
   0x000627: u'Uniwide Technologies, Inc.',
   0x000628: u'Cisco Systems, Inc.',
   0x000629: u'IBM CORPORATION',
   0x00062A: u'Cisco Systems, Inc.',
   0x00062B: u'INTRASERVER TECHNOLOGY',
   0x00062C: u'Network Robots, Inc.',
   0x00062D: u'TouchStar Technologies, L.L.C.',
   0x00062E: u'Aristos Logic Corp.',
   0x00062F: u'Pivotech Systems Inc.',
   0x000630: u'Adtranz Sweden',
   0x000631: u'Optical Solutions, Inc.',
   0x000632: u'Mesco Engineering GmbH',
   0x000633: u'Smiths Heimann Biometric Systems',
   0x000634: u'GTE Airfone Inc.',
   0x000635: u'PacketAir Networks, Inc.',
   0x000636: u'Jedai Broadband Networks',
   0x000637: u'Toptrend-Meta Information (ShenZhen) Inc.',
   0x000638: u'Sungjin C&C Co., Ltd.',
   0x000639: u'Newtec',
   0x00063A: u'Dura Micro, Inc.',
   0x00063B: u'Arcturus Networks, Inc.',
   0x00063C: u'NMI Electronics Ltd',
   0x00063D: u'Microwave Data Systems Inc.',
   0x00063E: u'Opthos Inc.',
   0x00063F: u'Everex Communications Inc.',
   0x000640: u'White Rock Networks',
   0x000641: u'ITCN',
   0x000642: u'Genetel Systems Inc.',
   0x000643: u'SONO Computer Co., Ltd.',
   0x000644: u'NEIX Inc.',
   0x000645: u'Meisei Electric Co. Ltd.',
   0x000646: u'ShenZhen XunBao Network Technology Co Ltd',
   0x000647: u'Etrali S.A.',
   0x000648: u'Seedsware, Inc.',
   0x000649: u'Quante',
   0x00064A: u'Honeywell Co., Ltd. (KOREA)',
   0x00064B: u'Alexon Co., Ltd.',
   0x00064C: u'Invicta Networks, Inc.',
   0x00064D: u'Sencore',
   0x00064E: u'Broad Net Technology Inc.',
   0x00064F: u'PRO-NETS Technology Corporation',
   0x000650: u'Tiburon Networks, Inc.',
   0x000651: u'Aspen Networks Inc.',
   0x000652: u'Cisco Systems, Inc.',
   0x000653: u'Cisco Systems, Inc.',
   0x000654: u'Maxxio Technologies',
   0x000655: u'Yipee, Inc.',
   0x000656: u'Tactel AB',
   0x000657: u'Market Central, Inc.',
   0x000658: u'Helmut Fischer GmbH & Co. KG',
   0x000659: u'EAL (Apeldoorn) B.V.',
   0x00065A: u'Strix Systems',
   0x00065B: u'Dell Computer Corp.',
   0x00065C: u'Malachite Technologies, Inc.',
   0x00065D: u'Heidelberg Web Systems',
   0x00065E: u'Photuris, Inc.',
   0x00065F: u'ECI Telecom - NGTS Ltd.',
   0x000660: u'NADEX Co., Ltd.',
   0x000661: u'NIA Home Technologies Corp.',
   0x000662: u'MBM Technology Ltd.',
   0x000663: u'Human Technology Co., Ltd.',
   0x000664: u'Fostex Corporation',
   0x000665: u'Sunny Giken, Inc.',
   0x000666: u'Roving Networks',
   0x000667: u'Tripp Lite',
   0x000668: u'Vicon Industries Inc.',
   0x000669: u'Datasound Laboratories Ltd',
   0x00066A: u'InfiniCon Systems, Inc.',
   0x00066B: u'Sysmex Corporation',
   0x00066C: u'Robinson Corporation',
   0x00066D: u'Compuprint S.P.A.',
   0x00066E: u'Delta Electronics, Inc.',
   0x00066F: u'Korea Data Systems',
   0x000670: u'Upponetti Oy',
   0x000671: u'Softing AG',
   0x000672: u'Netezza',
   0x000673: u'Optelecom-nkf',
   0x000674: u'Spectrum Control, Inc.',
   0x000675: u'Banderacom, Inc.',
   0x000676: u'Novra Technologies Inc.',
   0x000677: u'SICK AG',
   0x000678: u'Marantz Japan, Inc.',
   0x000679: u'Konami Corporation',
   0x00067A: u'JMP Systems',
   0x00067B: u'Toplink C&C Corporation',
   0x00067C: u'CISCO SYSTEMS, INC.',
   0x00067D: u'Takasago Ltd.',
   0x00067E: u'WinCom Systems, Inc.',
   0x00067F: u'Rearden Steel Technologies',
   0x000680: u'Card Access, Inc.',
   0x000681: u'Goepel Electronic GmbH',
   0x000682: u'Convedia',
   0x000683: u'Bravara Communications, Inc.',
   0x000684: u'Biacore AB',
   0x000685: u'NetNearU Corporation',
   0x000686: u'ZARDCOM Co., Ltd.',
   0x000687: u'Omnitron Systems Technology, Inc.',
   0x000688: u'Telways Communication Co., Ltd.',
   0x000689: u'yLez Technologies Pte Ltd',
   0x00068A: u'NeuronNet Co. Ltd. R&D Center',
   0x00068B: u'AirRunner Technologies, Inc.',
   0x00068C: u'3Com Corporation',
   0x00068D: u'SEPATON, Inc.',
   0x00068E: u'HID Corporation',
   0x00068F: u'Telemonitor, Inc.',
   0x000690: u'Euracom Communication GmbH',
   0x000691: u'PT Inovacao',
   0x000692: u'Intruvert Networks, Inc.',
   0x000693: u'Flexus Computer Technology, Inc.',
   0x000694: u'Mobillian Corporation',
   0x000695: u'Ensure Technologies, Inc.',
   0x000696: u'Advent Networks',
   0x000697: u'R & D Center',
   0x000698: u'egnite Software GmbH',
   0x000699: u'Vida Design Co.',
   0x00069A: u'e & Tel',
   0x00069B: u'AVT Audio Video Technologies GmbH',
   0x00069C: u'Transmode Systems AB',
   0x00069D: u'Petards Mobile Intelligence',
   0x00069E: u'UNIQA, Inc.',
   0x00069F: u'Kuokoa Networks',
   0x0006A0: u'Mx Imaging',
   0x0006A1: u'Celsian Technologies, Inc.',
   0x0006A2: u'Microtune, Inc.',
   0x0006A3: u'Bitran Corporation',
   0x0006A4: u'INNOWELL Corp.',
   0x0006A5: u'PINON Corp.',
   0x0006A6: u'Artistic Licence (UK) Ltd',
   0x0006A7: u'Primarion',
   0x0006A8: u'KC Technology, Inc.',
   0x0006A9: u'Universal Instruments Corp.',
   0x0006AA: u'Miltope Corporation',
   0x0006AB: u'W-Link Systems, Inc.',
   0x0006AC: u'Intersoft Co.',
   0x0006AD: u'KB Electronics Ltd.',
   0x0006AE: u'Himachal Futuristic Communications Ltd',
   0x0006AF: u'PRIVATE',
   0x0006B0: u'Comtech EF Data Corp.',
   0x0006B1: u'Sonicwall',
   0x0006B2: u'Linxtek Co.',
   0x0006B3: u'Diagraph Corporation',
   0x0006B4: u'Vorne Industries, Inc.',
   0x0006B5: u'Luminent, Inc.',
   0x0006B6: u'Nir-Or Israel Ltd.',
   0x0006B7: u'TELEM GmbH',
   0x0006B8: u'Bandspeed Pty Ltd',
   0x0006B9: u'A5TEK Corp.',
   0x0006BA: u'Westwave Communications',
   0x0006BB: u'ATI Technologies Inc.',
   0x0006BC: u'Macrolink, Inc.',
   0x0006BD: u'BNTECHNOLOGY Co., Ltd.',
   0x0006BE: u'Baumer Optronic GmbH',
   0x0006BF: u'Accella Technologies Co., Ltd.',
   0x0006C0: u'United Internetworks, Inc.',
   0x0006C1: u'CISCO SYSTEMS, INC.',
   0x0006C2: u'Smartmatic Corporation',
   0x0006C3: u'Schindler Elevators Ltd.',
   0x0006C4: u'Piolink Inc.',
   0x0006C5: u'INNOVI Technologies Limited',
   0x0006C6: u'lesswire AG',
   0x0006C7: u'RFNET Technologies Pte Ltd (S)',
   0x0006C8: u'Sumitomo Metal Micro Devices, Inc.',
   0x0006C9: u'Technical Marketing Research, Inc.',
   0x0006CA: u'American Computer & Digital Components, Inc. (ACDC)',
   0x0006CB: u'Jotron Electronics A/S',
   0x0006CC: u'JMI Electronics Co., Ltd.',
   0x0006CD: u'Kodak IL Ltd.',
   0x0006CE: u'DATENO',
   0x0006CF: u'Thales Avionics In-Flight Systems, LLC',
   0x0006D0: u'Elgar Electronics Corp.',
   0x0006D1: u'Tahoe Networks, Inc.',
   0x0006D2: u'Tundra Semiconductor Corp.',
   0x0006D3: u'Alpha Telecom, Inc. U.S.A.',
   0x0006D4: u'Interactive Objects, Inc.',
   0x0006D5: u'Diamond Systems Corp.',
   0x0006D6: u'Cisco Systems, Inc.',
   0x0006D7: u'Cisco Systems, Inc.',
   0x0006D8: u'Maple Optical Systems',
   0x0006D9: u'IPM-Net S.p.A.',
   0x0006DA: u'ITRAN Communications Ltd.',
   0x0006DB: u'ICHIPS Co., Ltd.',
   0x0006DC: u'Syabas Technology (Amquest)',
   0x0006DD: u'AT & T Laboratories - Cambridge Ltd',
   0x0006DE: u'Flash Technology',
   0x0006DF: u'AIDONIC Corporation',
   0x0006E0: u'MAT Co., Ltd.',
   0x0006E1: u'Techno Trade s.a',
   0x0006E2: u'Ceemax Technology Co., Ltd.',
   0x0006E3: u'Quantitative Imaging Corporation',
   0x0006E4: u'Citel Technologies Ltd.',
   0x0006E5: u'Fujian Newland Computer Ltd. Co.',
   0x0006E6: u'DongYang Telecom Co., Ltd.',
   0x0006E7: u'Bit Blitz Communications Inc.',
   0x0006E8: u'Optical Network Testing, Inc.',
   0x0006E9: u'Intime Corp.',
   0x0006EA: u'ELZET80 Mikrocomputer GmbH&Co. KG',
   0x0006EB: u'Global Data',
   0x0006EC: u'M/A COM Private Radio System Inc.',
   0x0006ED: u'Inara Networks',
   0x0006EE: u'Shenyang Neu-era Information & Technology Stock Co., Ltd',
   0x0006EF: u'Maxxan Systems, Inc.',
   0x0006F0: u'Digeo, Inc.',
   0x0006F1: u'Optillion',
   0x0006F2: u'Platys Communications',
   0x0006F3: u'AcceLight Networks',
   0x0006F4: u'Prime Electronics & Satellitics Inc.',
   0x0006F8: u'CPU Technology, Inc.',
   0x0006F9: u'Mitsui Zosen Systems Research Inc.',
   0x0006FA: u'IP SQUARE Co, Ltd.',
   0x0006FB: u'Hitachi Printing Solutions, Ltd.',
   0x0006FC: u'Fnet Co., Ltd.',
   0x0006FD: u'Comjet Information Systems Corp.',
   0x0006FE: u'Celion Networks, Inc.',
   0x0006FF: u'Sheba Systems Co., Ltd.',
   0x000700: u'Zettamedia Korea',
   0x000701: u'RACAL-DATACOM',
   0x000702: u'Varian Medical Systems',
   0x000703: u'CSEE Transport',
   0x000705: u'Endress & Hauser GmbH & Co',
   0x000706: u'Sanritz Corporation',
   0x000707: u'Interalia Inc.',
   0x000708: u'Bitrage Inc.',
   0x000709: u'Westerstrand Urfabrik AB',
   0x00070A: u'Unicom Automation Co., Ltd.',
   0x00070B: u'Octal, SA',
   0x00070C: u'SVA-Intrusion.com Co. Ltd.',
   0x00070D: u'Cisco Systems Inc.',
   0x00070E: u'Cisco Systems Inc.',
   0x00070F: u'Fujant, Inc.',
   0x000710: u'Adax, Inc.',
   0x000711: u'Acterna',
   0x000712: u'JAL Information Technology',
   0x000713: u'IP One, Inc.',
   0x000714: u'Brightcom',
   0x000715: u'General Research of Electronics, Inc.',
   0x000716: u'J & S Marine Ltd.',
   0x000717: u'Wieland Electric GmbH',
   0x000718: u'iCanTek Co., Ltd.',
   0x000719: u'Mobiis Co., Ltd.',
   0x00071A: u'Finedigital Inc.',
   0x00071B: u'Position Technology Inc.',
   0x00071C: u'AT&T Fixed Wireless Services',
   0x00071D: u'Satelsa Sistemas Y Aplicaciones De Telecomunicaciones, S.A.',
   0x00071E: u'Tri-M Engineering / Nupak Dev. Corp.',
   0x00071F: u'European Systems Integration',
   0x000720: u'Trutzschler GmbH & Co. KG',
   0x000721: u'Formac Elektronik GmbH',
   0x000722: u'Nielsen Media Research',
   0x000723: u'ELCON Systemtechnik GmbH',
   0x000724: u'Telemax Co., Ltd.',
   0x000725: u'Bematech International Corp.',
   0x000727: u'Zi Corporation (HK) Ltd.',
   0x000728: u'Neo Telecom',
   0x000729: u'Kistler Instrumente AG',
   0x00072A: u'Innovance Networks',
   0x00072B: u'Jung Myung Telecom Co., Ltd.',
   0x00072C: u'Fabricom',
   0x00072D: u'CNSystems',
   0x00072E: u'North Node AB',
   0x00072F: u'Intransa, Inc.',
   0x000730: u'Hutchison OPTEL Telecom Technology Co., Ltd.',
   0x000731: u'Spiricon, Inc.',
   0x000732: u'AAEON Technology Inc.',
   0x000733: u'DANCONTROL Engineering',
   0x000734: u'ONStor, Inc.',
   0x000735: u'Flarion Technologies, Inc.',
   0x000736: u'Data Video Technologies Co., Ltd.',
   0x000737: u'Soriya Co. Ltd.',
   0x000738: u'Young Technology Co., Ltd.',
   0x000739: u'Motion Media Technology Ltd.',
   0x00073A: u'Inventel Systemes',
   0x00073B: u'Tenovis GmbH & Co KG',
   0x00073C: u'Telecom Design',
   0x00073D: u'Nanjing Postel Telecommunications Co., Ltd.',
   0x00073E: u'China Great-Wall Computer Shenzhen Co., Ltd.',
   0x00073F: u'Woojyun Systec Co., Ltd.',
   0x000740: u'Melco Inc.',
   0x000741: u'Sierra Automated Systems',
   0x000742: u'Current Technologies',
   0x000743: u'Chelsio Communications',
   0x000744: u'Unico, Inc.',
   0x000745: u'Radlan Computer Communications Ltd.',
   0x000746: u'TURCK, Inc.',
   0x000747: u'Mecalc',
   0x000748: u'The Imaging Source Europe',
   0x000749: u'CENiX Inc.',
   0x00074A: u'Carl Valentin GmbH',
   0x00074B: u'Daihen Corporation',
   0x00074C: u'Beicom Inc.',
   0x00074D: u'Zebra Technologies Corp.',
   0x00074E: u'Naughty boy co., Ltd.',
   0x00074F: u'Cisco Systems, Inc.',
   0x000750: u'Cisco Systems, Inc.',
   0x000751: u'm.u.t. - GmbH',
   0x000752: u'Rhythm Watch Co., Ltd.',
   0x000753: u'Beijing Qxcomm Technology Co., Ltd.',
   0x000754: u'Xyterra Computing, Inc.',
   0x000755: u'Lafon SA',
   0x000756: u'Juyoung Telecom',
   0x000757: u'Topcall International AG',
   0x000758: u'Dragonwave',
   0x000759: u'Boris Manufacturing Corp.',
   0x00075A: u'Air Products and Chemicals, Inc.',
   0x00075B: u'Gibson Guitars',
   0x00075C: u'Eastman Kodak Company',
   0x00075D: u'Celleritas Inc.',
   0x00075E: u'Ametek Power Instruments',
   0x00075F: u'VCS Video Communication Systems AG',
   0x000760: u'TOMIS Information & Telecom Corp.',
   0x000761: u'Logitech SA',
   0x000762: u'Group Sense Limited',
   0x000763: u'Sunniwell Cyber Tech. Co., Ltd.',
   0x000764: u'YoungWoo Telecom Co. Ltd.',
   0x000765: u'Jade Quantum Technologies, Inc.',
   0x000766: u'Chou Chin Industrial Co., Ltd.',
   0x000767: u'Yuxing Electronics Company Limited',
   0x000768: u'Danfoss A/S',
   0x000769: u'Italiana Macchi SpA',
   0x00076A: u'NEXTEYE Co., Ltd.',
   0x00076B: u'Stralfors AB',
   0x00076C: u'Daehanet, Inc.',
   0x00076D: u'Flexlight Networks',
   0x00076E: u'Sinetica Corporation Limited',
   0x00076F: u'Synoptics Limited',
   0x000770: u'Locusnetworks Corporation',
   0x000771: u'Embedded System Corporation',
   0x000772: u'Alcatel Shanghai Bell Co., Ltd.',
   0x000773: u'Ascom Powerline Communications Ltd.',
   0x000774: u'GuangZhou Thinker Technology Co. Ltd.',
   0x000775: u'Valence Semiconductor, Inc.',
   0x000776: u'Federal APD',
   0x000777: u'Motah Ltd.',
   0x000778: u'GERSTEL GmbH & Co. KG',
   0x000779: u'Sungil Telecom Co., Ltd.',
   0x00077A: u'Infoware System Co., Ltd.',
   0x00077B: u'Millimetrix Broadband Networks',
   0x00077C: u'OnTime Networks',
   0x00077E: u'Elrest GmbH',
   0x00077F: u'J Communications Co., Ltd.',
   0x000780: u'Bluegiga Technologies OY',
   0x000781: u'Itron Inc.',
   0x000782: u'Nauticus Networks, Inc.',
   0x000783: u'SynCom Network, Inc.',
   0x000784: u'Cisco Systems Inc.',
   0x000785: u'Cisco Systems Inc.',
   0x000786: u'Wireless Networks Inc.',
   0x000787: u'Idea System Co., Ltd.',
   0x000788: u'Clipcomm, Inc.',
   0x000789: u'Eastel Systems Corporation',
   0x00078A: u'Mentor Data System Inc.',
   0x00078B: u'Wegener Communications, Inc.',
   0x00078C: u'Elektronikspecialisten i Borlange AB',
   0x00078D: u'NetEngines Ltd.',
   0x00078E: u'Garz & Friche GmbH',
   0x00078F: u'Emkay Innovative Products',
   0x000790: u'Tri-M Technologies (s) Limited',
   0x000791: u'International Data Communications, Inc.',
   0x000792: u'Suetron Electronic GmbH',
   0x000793: u'Shin Satellite Public Company Limited',
   0x000794: u'Simple Devices, Inc.',
   0x000795: u'Elitegroup Computer System Co. (ECS)',
   0x000796: u'LSI Systems, Inc.',
   0x000797: u'Netpower Co., Ltd.',
   0x000798: u'Selea SRL',
   0x000799: u'Tipping Point Technologies, Inc.',
   0x00079A: u'SmartSight Networks Inc.',
   0x00079B: u'Aurora Networks',
   0x00079C: u'Golden Electronics Technology Co., Ltd.',
   0x00079D: u'Musashi Co., Ltd.',
   0x00079E: u'Ilinx Co., Ltd.',
   0x00079F: u'Action Digital Inc.',
   0x0007A0: u'e-Watch Inc.',
   0x0007A1: u'VIASYS Healthcare GmbH',
   0x0007A2: u'Opteon Corporation',
   0x0007A3: u'Ositis Software, Inc.',
   0x0007A4: u'GN Netcom Ltd.',
   0x0007A5: u'Y.D.K Co. Ltd.',
   0x0007A6: u'Home Automation, Inc.',
   0x0007A7: u'A-Z Inc.',
   0x0007A8: u'Haier Group Technologies Ltd.',
   0x0007A9: u'Novasonics',
   0x0007AA: u'Quantum Data Inc.',
   0x0007AC: u'Eolring',
   0x0007AD: u'Pentacon GmbH Foto-und Feinwerktechnik',
   0x0007AE: u'Britestream Networks, Inc.',
   0x0007AF: u'N-Tron Corp.',
   0x0007B0: u'Office Details, Inc.',
   0x0007B1: u'Equator Technologies',
   0x0007B2: u'Transaccess S.A.',
   0x0007B3: u'Cisco Systems Inc.',
   0x0007B4: u'Cisco Systems Inc.',
   0x0007B5: u'Any One Wireless Ltd.',
   0x0007B6: u'Telecom Technology Ltd.',
   0x0007B7: u'Samurai Ind. Prods Eletronicos Ltda',
   0x0007B8: u'American Predator Corp.',
   0x0007B9: u'Ginganet Corporation',
   0x0007BA: u'UTStarcom, Inc.',
   0x0007BB: u'Candera Inc.',
   0x0007BC: u'Identix Inc.',
   0x0007BD: u'Radionet Ltd.',
   0x0007BE: u'DataLogic SpA',
   0x0007BF: u'Armillaire Technologies, Inc.',
   0x0007C0: u'NetZerver Inc.',
   0x0007C1: u'Overture Networks, Inc.',
   0x0007C2: u'Netsys Telecom',
   0x0007C3: u'Cirpack',
   0x0007C4: u'JEAN Co. Ltd.',
   0x0007C5: u'Gcom, Inc.',
   0x0007C6: u'VDS Vosskuhler GmbH',
   0x0007C7: u'Synectics Systems Limited',
   0x0007C8: u'Brain21, Inc.',
   0x0007C9: u'Technol Seven Co., Ltd.',
   0x0007CA: u'Creatix Polymedia Ges Fur Kommunikaitonssysteme',
   0x0007CB: u'Freebox SA',
   0x0007CC: u'Kaba Benzing GmbH',
   0x0007CD: u'NMTEL Co., Ltd.',
   0x0007CE: u'Cabletime Limited',
   0x0007CF: u'Anoto AB',
   0x0007D0: u'Automat Engenharia de Automaoa Ltda.',
   0x0007D1: u'Spectrum Signal Processing Inc.',
   0x0007D2: u'Logopak Systeme',
   0x0007D3: u'Stork Digital Imaging B.V.',
   0x0007D4: u'Zhejiang Yutong Network Communication Co Ltd.',
   0x0007D5: u'3e Technologies Int;., Inc.',
   0x0007D6: u'Commil Ltd.',
   0x0007D7: u'Caporis Networks AG',
   0x0007D8: u'Hitron Systems Inc.',
   0x0007D9: u'Splicecom',
   0x0007DA: u'Neuro Telecom Co., Ltd.',
   0x0007DB: u'Kirana Networks, Inc.',
   0x0007DC: u'Atek Co, Ltd.',
   0x0007DD: u'Cradle Technologies',
   0x0007DE: u'eCopilt AB',
   0x0007DF: u'Vbrick Systems Inc.',
   0x0007E0: u'Palm Inc.',
   0x0007E1: u'WIS Communications Co. Ltd.',
   0x0007E2: u'Bitworks, Inc.',
   0x0007E3: u'Navcom Technology, Inc.',
   0x0007E4: u'SoftRadio Co., Ltd.',
   0x0007E5: u'Coup Corporation',
   0x0007E6: u'edgeflow Canada Inc.',
   0x0007E7: u'FreeWave Technologies',
   0x0007E8: u'St. Bernard Software',
   0x0007E9: u'Intel Corporation',
   0x0007EA: u'Massana, Inc.',
   0x0007EB: u'Cisco Systems Inc.',
   0x0007EC: u'Cisco Systems Inc.',
   0x0007ED: u'Altera Corporation',
   0x0007EE: u'telco Informationssysteme GmbH',
   0x0007EF: u'Lockheed Martin Tactical Systems',
   0x0007F0: u'LogiSync Corporation',
   0x0007F1: u'TeraBurst Networks Inc.',
   0x0007F2: u'IOA Corporation',
   0x0007F3: u'Thinkengine Networks',
   0x0007F4: u'Eletex Co., Ltd.',
   0x0007F5: u'Bridgeco Co AG',
   0x0007F6: u'Qqest Software Systems',
   0x0007F7: u'Galtronics',
   0x0007F8: u'ITDevices, Inc.',
   0x0007F9: u'Phonetics, Inc.',
   0x0007FA: u'ITT Co., Ltd.',
   0x0007FB: u'Giga Stream UMTS Technologies GmbH',
   0x0007FC: u'Adept Systems Inc.',
   0x0007FD: u'LANergy Ltd.',
   0x0007FE: u'Rigaku Corporation',
   0x0007FF: u'Gluon Networks',
   0x000800: u'MULTITECH SYSTEMS, INC.',
   0x000801: u'HighSpeed Surfing Inc.',
   0x000802: u'Compaq Computer Corporation',
   0x000803: u'Cos Tron',
   0x000804: u'ICA Inc.',
   0x000805: u'Techno-Holon Corporation',
   0x000806: u'Raonet Systems, Inc.',
   0x000807: u'Access Devices Limited',
   0x000808: u'PPT Vision, Inc.',
   0x000809: u'Systemonic AG',
   0x00080A: u'Espera-Werke GmbH',
   0x00080B: u'Birka BPA Informationssystem AB',
   0x00080C: u'VDA elettronica SrL',
   0x00080D: u'Toshiba',
   0x00080E: u'Motorola, BCS',
   0x00080F: u'Proximion Fiber Optics AB',
   0x000810: u'Key Technology, Inc.',
   0x000811: u'VOIX Corporation',
   0x000812: u'GM-2 Corporation',
   0x000813: u'Diskbank, Inc.',
   0x000814: u'TIL Technologies',
   0x000815: u'CATS Co., Ltd.',
   0x000816: u'Bluetags A/S',
   0x000817: u'EmergeCore Networks LLC',
   0x000818: u'Pixelworks, Inc.',
   0x000819: u'Banksys',
   0x00081A: u'Sanrad Intelligence Storage Communications (2000) Ltd.',
   0x00081B: u'Windigo Systems',
   0x00081C: u'@pos.com',
   0x00081D: u'Ipsil, Incorporated',
   0x00081E: u'Repeatit AB',
   0x00081F: u'Pou Yuen Tech Corp. Ltd.',
   0x000820: u'Cisco Systems Inc.',
   0x000821: u'Cisco Systems Inc.',
   0x000822: u'InPro Comm',
   0x000823: u'Texa Corp.',
   0x000824: u'Promatek Industries Ltd.',
   0x000825: u'Acme Packet',
   0x000826: u'Colorado Med Tech',
   0x000827: u'Pirelli Broadband Solutions',
   0x000828: u'Koei Engineering Ltd.',
   0x000829: u'Aval Nagasaki Corporation',
   0x00082A: u'Powerwallz Network Security',
   0x00082B: u'Wooksung Electronics, Inc.',
   0x00082C: u'Homag AG',
   0x00082D: u'Indus Teqsite Private Limited',
   0x00082E: u'Multitone Electronics PLC',
   0x00084E: u'DivergeNet, Inc.',
   0x00084F: u'Qualstar Corporation',
   0x000850: u'Arizona Instrument Corp.',
   0x000851: u'Canadian Bank Note Company, Ltd.',
   0x000852: u'Davolink Co. Inc.',
   0x000853: u'Schleicher GmbH & Co. Relaiswerke KG',
   0x000854: u'Netronix, Inc.',
   0x000855: u'NASA-Goddard Space Flight Center',
   0x000856: u'Gamatronic Electronic Industries Ltd.',
   0x000857: u'Polaris Networks, Inc.',
   0x000858: u'Novatechnology Inc.',
   0x000859: u'ShenZhen Unitone Electronics Co., Ltd.',
   0x00085A: u'IntiGate Inc.',
   0x00085B: u'Hanbit Electronics Co., Ltd.',
   0x00085C: u'Shanghai Dare Technologies Co. Ltd.',
   0x00085D: u'Aastra',
   0x00085E: u'PCO AG',
   0x00085F: u'Picanol N.V.',
   0x000860: u'LodgeNet Entertainment Corp.',
   0x000861: u'SoftEnergy Co., Ltd.',
   0x000862: u'NEC Eluminant Technologies, Inc.',
   0x000863: u'Entrisphere Inc.',
   0x000864: u'Fasy S.p.A.',
   0x000865: u'JASCOM CO., LTD',
   0x000866: u'DSX Access Systems, Inc.',
   0x000867: u'Uptime Devices',
   0x000868: u'PurOptix',
   0x000869: u'Command-e Technology Co.,Ltd.',
   0x00086A: u'Industrie Technik IPS GmbH',
   0x00086B: u'MIPSYS',
   0x00086C: u'Plasmon LMS',
   0x00086D: u'Missouri FreeNet',
   0x00086E: u'Hyglo AB',
   0x00086F: u'Resources Computer Network Ltd.',
   0x000870: u'Rasvia Systems, Inc.',
   0x000871: u'NORTHDATA Co., Ltd.',
   0x000872: u'Sorenson Technologies, Inc.',
   0x000873: u'DAP Design B.V.',
   0x000874: u'Dell Computer Corp.',
   0x000875: u'Acorp Electronics Corp.',
   0x000876: u'SDSystem',
   0x000877: u'Liebert HIROSS S.p.A.',
   0x000878: u'Benchmark Storage Innovations',
   0x000879: u'CEM Corporation',
   0x00087A: u'Wipotec GmbH',
   0x00087B: u'RTX Telecom A/S',
   0x00087C: u'Cisco Systems, Inc.',
   0x00087D: u'Cisco Systems Inc.',
   0x00087E: u'Bon Electro-Telecom Inc.',
   0x00087F: u'SPAUN electronic GmbH & Co. KG',
   0x000880: u'BroadTel Canada Communications inc.',
   0x000881: u'DIGITAL HANDS CO.,LTD.',
   0x000882: u'SIGMA CORPORATION',
   0x000883: u'Hewlett-Packard Company',
   0x000884: u'Index Braille AB',
   0x000885: u'EMS Dr. Thomas Wuensche',
   0x000886: u'Hansung Teliann, Inc.',
   0x000887: u'Maschinenfabrik Reinhausen GmbH',
   0x000888: u'OULLIM Information Technology Inc,.',
   0x000889: u'Echostar Technologies Corp',
   0x00088A: u'Minds@Work',
   0x00088B: u'Tropic Networks Inc.',
   0x00088C: u'Quanta Network Systems Inc.',
   0x00088D: u'Sigma-Links Inc.',
   0x00088E: u'Nihon Computer Co., Ltd.',
   0x00088F: u'ADVANCED DIGITAL TECHNOLOGY',
   0x000890: u'AVILINKS SA',
   0x000891: u'Lyan Inc.',
   0x000892: u'EM Solutions',
   0x000893: u'LE INFORMATION COMMUNICATION INC.',
   0x000894: u'InnoVISION Multimedia Ltd.',
   0x000895: u'DIRC Technologie GmbH & Co.KG',
   0x000896: u'Printronix, Inc.',
   0x000897: u'Quake Technologies',
   0x000898: u'Gigabit Optics Corporation',
   0x000899: u'Netbind, Inc.',
   0x00089A: u'Alcatel Microelectronics',
   0x00089B: u'ICP Electronics Inc.',
   0x00089C: u'Elecs Industry Co., Ltd.',
   0x00089D: u'UHD-Elektronik',
   0x00089E: u'Beijing Enter-Net co.LTD',
   0x00089F: u'EFM Networks',
   0x0008A0: u'Stotz Feinmesstechnik GmbH',
   0x0008A1: u'CNet Technology Inc.',
   0x0008A2: u'ADI Engineering, Inc.',
   0x0008A3: u'Cisco Systems',
   0x0008A4: u'Cisco Systems',
   0x0008A5: u'Peninsula Systems Inc.',
   0x0008A6: u'Multiware & Image Co., Ltd.',
   0x0008A7: u'iLogic Inc.',
   0x0008A8: u'Systec Co., Ltd.',
   0x0008A9: u'SangSang Technology, Inc.',
   0x0008AA: u'KARAM',
   0x0008AB: u'EnerLinx.com, Inc.',
   0x0008AC: u'PRIVATE',
   0x0008AD: u'Toyo-Linx Co., Ltd.',
   0x0008AE: u'PacketFront Sweden AB',
   0x0008AF: u'Novatec Corporation',
   0x0008B0: u'BKtel communications GmbH',
   0x0008B1: u'ProQuent Systems',
   0x0008B2: u'SHENZHEN COMPASS TECHNOLOGY DEVELOPMENT CO.,LTD',
   0x0008B3: u'Fastwel',
   0x0008B4: u'SYSPOL',
   0x0008B5: u'TAI GUEN ENTERPRISE CO., LTD',
   0x0008B6: u'RouteFree, Inc.',
   0x0008B7: u'HIT Incorporated',
   0x0008B8: u'E.F. Johnson',
   0x0008B9: u'KAON MEDIA Co., Ltd.',
   0x0008BA: u'Erskine Systems Ltd',
   0x0008BB: u'NetExcell',
   0x0008BC: u'Ilevo AB',
   0x0008BD: u'TEPG-US',
   0x0008BE: u'XENPAK MSA Group',
   0x0008BF: u'Aptus Elektronik AB',
   0x0008C0: u'ASA SYSTEMS',
   0x0008C1: u'Avistar Communications Corporation',
   0x0008C2: u'Cisco Systems',
   0x0008C3: u'Contex A/S',
   0x0008C4: u'Hikari Co.,Ltd.',
   0x0008C5: u'Liontech Co., Ltd.',
   0x0008C6: u'Philips Consumer Communications',
   0x0008C7: u'COMPAQ COMPUTER CORPORATION',
   0x0008C8: u'Soneticom, Inc.',
   0x0008C9: u'TechniSat Digital GmbH',
   0x0008CA: u'TwinHan Technology Co.,Ltd',
   0x0008CB: u'Zeta Broadband Inc.',
   0x0008CC: u'Remotec, Inc.',
   0x0008CD: u'With-Net Inc',
   0x0008CE: u'IPMobileNet Inc.',
   0x0008CF: u'Nippon Koei Power Systems Co., Ltd.',
   0x0008D0: u'Musashi Engineering Co., LTD.',
   0x0008D1: u'KAREL INC.',
   0x0008D2: u'ZOOM Networks Inc.',
   0x0008D3: u'Hercules Technologies S.A.',
   0x0008D4: u'IneoQuest Technologies, Inc',
   0x0008D5: u'Vanguard Managed Solutions',
   0x0008D6: u'HASSNET Inc.',
   0x0008D7: u'HOW CORPORATION',
   0x0008D8: u'Dowkey Microwave',
   0x0008D9: u'Mitadenshi Co.,LTD',
   0x0008DA: u'SofaWare Technologies Ltd.',
   0x0008DB: u'Corrigent Systems',
   0x0008DC: u'Wiznet',
   0x0008DD: u'Telena Communications, Inc.',
   0x0008DE: u'3UP Systems',
   0x0008DF: u'Alistel Inc.',
   0x0008E0: u'ATO Technology Ltd.',
   0x0008E1: u'Barix AG',
   0x0008E2: u'Cisco Systems',
   0x0008E3: u'Cisco Systems',
   0x0008E4: u'Envenergy Inc',
   0x0008E5: u'IDK Corporation',
   0x0008E6: u'Littlefeet',
   0x0008E7: u'SHI ControlSystems,Ltd.',
   0x0008E8: u'Excel Master Ltd.',
   0x0008E9: u'NextGig',
   0x0008EA: u'Motion Control Engineering, Inc',
   0x0008EB: u'ROMWin Co.,Ltd.',
   0x0008EC: u'Zonu, Inc.',
   0x0008ED: u'ST&T Instrument Corp.',
   0x0008EE: u'Logic Product Development',
   0x0008EF: u'DIBAL,S.A.',
   0x0008F0: u'Next Generation Systems, Inc.',
   0x0008F1: u'Voltaire',
   0x0008F2: u'C&S Technology',
   0x0008F3: u'WANY',
   0x0008F4: u'Bluetake Technology Co., Ltd.',
   0x0008F5: u'YESTECHNOLOGY Co.,Ltd.',
   0x0008F6: u'SUMITOMO ELECTRIC HIGHTECHS.co.,ltd.',
   0x0008F7: u'Hitachi Ltd, Semiconductor &amp; Integrated Circuits Gr',
   0x0008F8: u'Guardall Ltd',
   0x0008F9: u'Padcom, Inc.',
   0x0008FA: u'Karl E.Brinkmann GmbH',
   0x0008FB: u'SonoSite, Inc.',
   0x0008FC: u'Gigaphoton Inc.',
   0x0008FD: u'BlueKorea Co., Ltd.',
   0x0008FE: u'UNIK C&C Co.,Ltd.',
   0x0008FF: u'Trilogy Communications Ltd',
   0x000900: u'TMT',
   0x000901: u'Shenzhen Shixuntong Information & Technoligy Co',
   0x000902: u'Redline Communications Inc.',
   0x000903: u'Panasas, Inc',
   0x000904: u'MONDIAL electronic',
   0x000905: u'iTEC Technologies Ltd.',
   0x000906: u'Esteem Networks',
   0x000907: u'Chrysalis Development',
   0x000908: u'VTech Technology Corp.',
   0x000909: u'Telenor Connect A/S',
   0x00090A: u'SnedFar Technology Co., Ltd.',
   0x00090B: u'MTL  Instruments PLC',
   0x00090C: u'Mayekawa Mfg. Co. Ltd.',
   0x00090D: u'LEADER ELECTRONICS CORP.',
   0x00090E: u'Helix Technology Inc.',
   0x00090F: u'Fortinet Inc.',
   0x000910: u'Simple Access Inc.',
   0x000911: u'Cisco Systems',
   0x000912: u'Cisco Systems',
   0x000913: u'SystemK Corporation',
   0x000914: u'COMPUTROLS INC.',
   0x000915: u'CAS Corp.',
   0x000916: u'Listman Home Technologies, Inc.',
   0x000917: u'WEM Technology Inc',
   0x000918: u'SAMSUNG TECHWIN CO.,LTD',
   0x000919: u'MDS Gateways',
   0x00091A: u'Macat Optics & Electronics Co., Ltd.',
   0x00091B: u'Digital Generation Inc.',
   0x00091C: u'CacheVision, Inc',
   0x00091D: u'Proteam Computer Corporation',
   0x00091E: u'Firstech Technology Corp.',
   0x00091F: u'A&amp;D Co., Ltd.',
   0x000920: u'EpoX COMPUTER CO.,LTD.',
   0x000921: u'Planmeca Oy',
   0x000922: u'Touchless Sensor Technology AG',
   0x000923: u'Heaman System Co., Ltd',
   0x000924: u'Telebau GmbH',
   0x000925: u'VSN Systemen BV',
   0x000926: u'YODA COMMUNICATIONS, INC.',
   0x000927: u'TOYOKEIKI CO.,LTD.',
   0x000928: u'Telecore Inc',
   0x000929: u'Sanyo Industries (UK) Limited',
   0x00092A: u'MYTECS Co.,Ltd.',
   0x00092B: u'iQstor Networks, Inc.',
   0x00092C: u'Hitpoint Inc.',
   0x00092D: u'High Tech Computer, Corp.',
   0x00092E: u'B&Tech System Inc.',
   0x00092F: u'Akom Technology Corporation',
   0x000930: u'AeroConcierge Inc.',
   0x000931: u'Future Internet, Inc.',
   0x000932: u'Omnilux',
   0x000933: u'OPTOVALLEY Co. Ltd.',
   0x000934: u'Dream-Multimedia-Tv GmbH',
   0x000935: u'Sandvine Incorporated',
   0x000936: u'Ipetronik GmbH & Co.KG',
   0x000937: u'Inventec Appliance Corp',
   0x000938: u'Allot Communications',
   0x000939: u'ShibaSoku Co.,Ltd.',
   0x00093A: u'Molex Fiber Optics',
   0x00093B: u'HYUNDAI NETWORKS INC.',
   0x00093C: u'Jacques Technologies P/L',
   0x00093D: u'Newisys,Inc.',
   0x00093E: u'C&I Technologies',
   0x00093F: u'Double-Win Enterpirse CO., LTD',
   0x000940: u'AGFEO GmbH & Co. KG',
   0x000941: u'Allied Telesis K.K.',
   0x000942: u'CRESCO, LTD.',
   0x000943: u'Cisco Systems',
   0x000944: u'Cisco Systems',
   0x000945: u'Palmmicro Communications Inc',
   0x000946: u'Cluster Labs GmbH',
   0x000947: u'Aztek, Inc.',
   0x000948: u'Vista Control Systems, Corp.',
   0x000949: u'Glyph Technologies Inc.',
   0x00094A: u'Homenet Communications',
   0x00094B: u'FillFactory NV',
   0x00094C: u'Communication Weaver Co.,Ltd.',
   0x00094D: u'Braintree Communications Pty Ltd',
   0x00094E: u'BARTECH SYSTEMS INTERNATIONAL, INC',
   0x00094F: u'elmegt GmbH & Co. KG',
   0x000950: u'Independent Storage Corporation',
   0x000951: u'Apogee Instruments, Inc',
   0x000952: u'Auerswald GmbH & Co. KG',
   0x000953: u'Linkage System Integration Co.Ltd.',
   0x000954: u'AMiT spol. s. r. o.',
   0x000955: u'Young Generation International Corp.',
   0x000956: u'Network Systems Group, Ltd. (NSG)',
   0x000957: u'Supercaller, Inc.',
   0x000958: u'INTELNET S.A.',
   0x000959: u'Sitecsoft',
   0x00095A: u'RACEWOOD TECHNOLOGY',
   0x00095B: u'Netgear, Inc.',
   0x00095C: u'Philips Medical Systems - Cardiac and Monitoring Systems (CM',
   0x00095D: u'Dialogue Technology Corp.',
   0x00095E: u'Masstech Group Inc.',
   0x00095F: u'Telebyte, Inc.',
   0x000960: u'YOZAN Inc.',
   0x000961: u'Switchgear and Instrumentation Ltd',
   0x000962: u'Filetrac AS',
   0x000963: u'Dominion Lasercom Inc.',
   0x000964: u'Hi-Techniques',
   0x000965: u'PRIVATE',
   0x000966: u'Thales Navigation',
   0x000967: u'Tachyon, Inc',
   0x000968: u'TECHNOVENTURE, INC.',
   0x000969: u'Meret Optical Communications',
   0x00096A: u'Cloverleaf Communications Inc.',
   0x00096B: u'IBM Corporation',
   0x00096C: u'Imedia Semiconductor Corp.',
   0x00096D: u'Powernet Technologies Corp.',
   0x00096E: u'GIANT ELECTRONICS LTD.',
   0x00096F: u'Beijing Zhongqing Elegant Tech. Corp.,Limited',
   0x000970: u'Vibration Research Corporation',
   0x000971: u'Time Management, Inc.',
   0x000972: u'Securebase,Inc',
   0x000973: u'Lenten Technology Co., Ltd.',
   0x000974: u'Innopia Technologies, Inc.',
   0x000975: u'fSONA Communications Corporation',
   0x000976: u'Datasoft ISDN Systems GmbH',
   0x000977: u'Brunner Elektronik AG',
   0x000978: u'AIJI System Co., Ltd.',
   0x000979: u'Advanced Television Systems Committee, Inc.',
   0x00097A: u'Louis Design Labs.',
   0x00097B: u'Cisco Systems',
   0x00097C: u'Cisco Systems',
   0x00097D: u'SecWell Networks Oy',
   0x00097E: u'IMI TECHNOLOGY CO., LTD',
   0x00097F: u'Vsecure 2000 LTD.',
   0x000980: u'Power Zenith Inc.',
   0x000981: u'Newport Networks',
   0x000982: u'Loewe Opta GmbH',
   0x000983: u'Gvision Incorporated',
   0x000984: u'MyCasa Network Inc.',
   0x000985: u'Auto Telecom Company',
   0x000986: u'Metalink LTD.',
   0x000987: u'NISHI NIPPON ELECTRIC WIRE & CABLE CO.,LTD.',
   0x000988: u'Nudian Electron Co., Ltd.',
   0x000989: u'VividLogic Inc.',
   0x00098A: u'EqualLogic Inc',
   0x00098B: u'Entropic Communications, Inc.',
   0x00098C: u'Option Wireless Sweden',
   0x00098D: u'Velocity Semiconductor',
   0x00098E: u'ipcas GmbH',
   0x00098F: u'Cetacean Networks',
   0x000990: u'ACKSYS Communications & systems',
   0x000991: u'GE Fanuc Automation Manufacturing, Inc.',
   0x000992: u'InterEpoch Technology,INC.',
   0x000993: u'Visteon Corporation',
   0x000994: u'Cronyx Engineering',
   0x000995: u'Castle Technology Ltd',
   0x000996: u'RDI',
   0x000997: u'Nortel Networks',
   0x000998: u'Capinfo Company Limited',
   0x000999: u'CP GEORGES RENAULT',
   0x00099A: u'ELMO COMPANY, LIMITED',
   0x00099B: u'Western Telematic Inc.',
   0x00099C: u'Naval Research Laboratory',
   0x00099D: u'Haliplex Communications',
   0x00099E: u'Testech, Inc.',
   0x00099F: u'VIDEX INC.',
   0x0009A0: u'Microtechno Corporation',
   0x0009A1: u'Telewise Communications, Inc.',
   0x0009A2: u'Interface Co., Ltd.',
   0x0009A3: u'Leadfly Techologies Corp. Ltd.',
   0x0009A4: u'HARTEC Corporation',
   0x0009A5: u'HANSUNG ELETRONIC INDUSTRIES DEVELOPMENT CO., LTD',
   0x0009A6: u'Ignis Optics, Inc.',
   0x0009A7: u'Bang & Olufsen A/S',
   0x0009A8: u'Eastmode Pte Ltd',
   0x0009A9: u'Ikanos Communications',
   0x0009AA: u'Data Comm for Business, Inc.',
   0x0009AB: u'Netcontrol Oy',
   0x0009AC: u'LANVOICE',
   0x0009AD: u'HYUNDAI SYSCOMM, INC.',
   0x0009AE: u'OKANO ELECTRIC CO.,LTD',
   0x0009AF: u'e-generis',
   0x0009B0: u'Onkyo Corporation',
   0x0009B1: u'Kanematsu Electronics, Ltd.',
   0x0009B2: u'L&F Inc.',
   0x0009B3: u'MCM Systems Ltd',
   0x0009B4: u'KISAN TELECOM CO., LTD.',
   0x0009B5: u'3J Tech. Co., Ltd.',
   0x0009B6: u'Cisco Systems',
   0x0009B7: u'Cisco Systems',
   0x0009B8: u'Entise Systems',
   0x0009B9: u'Action Imaging Solutions',
   0x0009BA: u'MAKU Informationstechik GmbH',
   0x0009BB: u'MathStar, Inc.',
   0x0009BC: u'Integrian, Inc.',
   0x0009BD: u'Epygi Technologies, Ltd.',
   0x0009BE: u'Mamiya-OP Co.,Ltd.',
   0x0009BF: u'Nintendo Co.,Ltd.',
   0x0009C0: u'6WIND',
   0x0009C1: u'PROCES-DATA A/S',
   0x0009C2: u'PRIVATE',
   0x0009C3: u'NETAS',
   0x0009C4: u'Medicore Co., Ltd',
   0x0009C5: u'KINGENE Technology Corporation',
   0x0009C6: u'Visionics Corporation',
   0x0009C7: u'Movistec',
   0x0009C8: u'SINAGAWA TSUSHIN KEISOU SERVICE',
   0x0009C9: u'BlueWINC Co., Ltd.',
   0x0009CA: u'iMaxNetworks(Shenzhen)Limited.',
   0x0009CB: u'HBrain',
   0x0009CC: u'Moog GmbH',
   0x0009CD: u'HUDSON SOFT CO.,LTD.',
   0x0009CE: u'SpaceBridge Semiconductor Corp.',
   0x0009CF: u'iAd GmbH',
   0x0009D0: u'Versatel Networks',
   0x0009D1: u'SERANOA NETWORKS INC',
   0x0009D2: u'Mai Logic Inc.',
   0x0009D3: u'Western DataCom Co., Inc.',
   0x0009D4: u'Transtech Networks',
   0x0009D5: u'Signal Communication, Inc.',
   0x0009D6: u'KNC One GmbH',
   0x0009D7: u'DC Security Products',
   0x0009D8: u'PRIVATE',
   0x0009D9: u'Neoscale Systems, Inc',
   0x0009DA: u'Control Module Inc.',
   0x0009DB: u'eSpace',
   0x0009DC: u'Galaxis Technology AG',
   0x0009DD: u'Mavin Technology Inc.',
   0x0009DE: u'Samjin Information & Communications Co., Ltd.',
   0x0009DF: u'Vestel Komunikasyon Sanayi ve Ticaret A.S.',
   0x0009E0: u'XEMICS S.A.',
   0x0009E1: u'Gemtek Technology Co., Ltd.',
   0x0009E2: u'Sinbon Electronics Co., Ltd.',
   0x0009E3: u'Angel Iglesias S.A.',
   0x0009E4: u'K Tech Infosystem Inc.',
   0x0009E5: u'Hottinger Baldwin Messtechnik GmbH',
   0x0009E6: u'Cyber Switching Inc.',
   0x0009E7: u'ADC Techonology',
   0x0009E8: u'Cisco Systems',
   0x0009E9: u'Cisco Systems',
   0x0009EA: u'YEM Inc.',
   0x0009EB: u'HuMANDATA LTD.',
   0x0009EC: u'Daktronics, Inc.',
   0x0009ED: u'CipherOptics',
   0x0009EE: u'MEIKYO ELECTRIC CO.,LTD',
   0x0009EF: u'Vocera Communications',
   0x0009F0: u'Shimizu Technology Inc.',
   0x0009F1: u'Yamaki Electric Corporation',
   0x0009F2: u'Cohu, Inc., Electronics Division',
   0x0009F3: u'WELL Communication Corp.',
   0x0009F4: u'Alcon Laboratories, Inc.',
   0x0009F5: u'Emerson Network Power Co.,Ltd',
   0x0009F6: u'Shenzhen Eastern Digital Tech Ltd.',
   0x0009F7: u'SED, a division of Calian',
   0x0009F8: u'UNIMO TECHNOLOGY CO., LTD.',
   0x0009F9: u'ART JAPAN CO., LTD.',
   0x0009FB: u'Philips Medizinsysteme Boeblingen GmbH',
   0x0009FC: u'IPFLEX Inc.',
   0x0009FD: u'Ubinetics Limited',
   0x0009FE: u'Daisy Technologies, Inc.',
   0x0009FF: u'X.net 2000 GmbH',
   0x000A00: u'Mediatek Corp.',
   0x000A01: u'SOHOware, Inc.',
   0x000A02: u'ANNSO CO., LTD.',
   0x000A03: u'ENDESA SERVICIOS, S.L.',
   0x000A04: u'3Com Europe Ltd',
   0x000A05: u'Widax Corp.',
   0x000A06: u'Teledex LLC',
   0x000A07: u'WebWayOne Ltd',
   0x000A08: u'ALPINE ELECTRONICS, INC.',
   0x000A09: u'TaraCom Integrated Products, Inc.',
   0x000A0A: u'SUNIX Co., Ltd.',
   0x000A0B: u'Sealevel Systems, Inc.',
   0x000A0C: u'Scientific Research Corporation',
   0x000A0D: u'MergeOptics GmbH',
   0x000A0E: u'Invivo Research Inc.',
   0x000A0F: u'Ilryung Telesys, Inc',
   0x000A10: u'FAST media integrations AG',
   0x000A11: u'ExPet Technologies, Inc',
   0x000A12: u'Azylex Technology, Inc',
   0x000A13: u'Silent Witness',
   0x000A14: u'TECO a.s.',
   0x000A15: u'Silicon Data, Inc',
   0x000A16: u'Lassen Research',
   0x000A17: u'NESTAR COMMUNICATIONS, INC',
   0x000A18: u'Vichel Inc.',
   0x000A19: u'Valere Power, Inc.',
   0x000A1A: u'Imerge Ltd',
   0x000A1B: u'Stream Labs',
   0x000A1C: u'Bridge Information Co., Ltd.',
   0x000A1D: u'Optical Communications Products Inc.',
   0x000A1E: u'Red-M Products Limited',
   0x000A1F: u'ART WARE Telecommunication Co., Ltd.',
   0x000A20: u'SVA Networks, Inc.',
   0x000A21: u'Integra Telecom Co. Ltd',
   0x000A22: u'Amperion Inc',
   0x000A23: u'Parama Networks Inc',
   0x000A24: u'Octave Communications',
   0x000A25: u'CERAGON NETWORKS',
   0x000A26: u'CEIA S.p.A.',
   0x000A27: u'Apple Computer, Inc.',
   0x000A28: u'Motorola',
   0x000A29: u'Pan Dacom Networking AG',
   0x000A2A: u'QSI Systems Inc.',
   0x000A2B: u'Etherstuff',
   0x000A2C: u'Active Tchnology Corporation',
   0x000A2D: u'PRIVATE',
   0x000A2E: u'MAPLE NETWORKS CO., LTD',
   0x000A2F: u'Artnix Inc.',
   0x000A30: u'Johnson Controls-ASG',
   0x000A31: u'HCV Wireless',
   0x000A32: u'Xsido Corporation',
   0x000A33: u'Emulex Corporation',
   0x000A34: u'Identicard Systems Incorporated',
   0x000A35: u'Xilinx',
   0x000A36: u'Synelec Telecom Multimedia',
   0x000A37: u'Procera Networks, Inc.',
   0x000A38: u'Netlock Technologies, Inc.',
   0x000A39: u'LoPA Information Technology',
   0x000A3A: u'J-THREE INTERNATIONAL Holding Co., Ltd.',
   0x000A3B: u'GCT Semiconductor, Inc',
   0x000A3C: u'Enerpoint Ltd.',
   0x000A3D: u'Elo Sistemas Eletronicos S.A.',
   0x000A3E: u'EADS Telecom',
   0x000A3F: u'Data East Corporation',
   0x000A40: u'Crown Audio',
   0x000A41: u'Cisco Systems',
   0x000A42: u'Cisco Systems',
   0x000A43: u'Chunghwa Telecom Co., Ltd.',
   0x000A44: u'Avery Dennison Deutschland GmbH',
   0x000A45: u'Audio-Technica Corp.',
   0x000A46: u'ARO Controls SAS',
   0x000A47: u'Allied Vision Technologies',
   0x000A48: u'Albatron Technology',
   0x000A49: u'Acopia Networks',
   0x000A4A: u'Targa Systems Ltd.',
   0x000A4B: u'DataPower Technology, Inc.',
   0x000A4C: u'Molecular Devices Corporation',
   0x000A4D: u'Noritz Corporation',
   0x000A4E: u'UNITEK Electronics INC.',
   0x000A4F: u'Brain Boxes Limited',
   0x000A50: u'REMOTEK CORPORATION',
   0x000A51: u'GyroSignal Technology Co., Ltd.',
   0x000A52: u'AsiaRF Ltd.',
   0x000A53: u'Intronics, Incorporated',
   0x000A54: u'Laguna Hills, Inc.',
   0x000A55: u'MARKEM Corporation',
   0x000A56: u'HITACHI Maxell Ltd.',
   0x000A57: u'Hewlett-Packard Company - Standards',
   0x000A58: u'Ingenieur-Buero Freyer & Siegel',
   0x000A59: u'HW server',
   0x000A5A: u'GreenNET Technologies Co.,Ltd.',
   0x000A5B: u'Power-One as',
   0x000A5C: u'Carel s.p.a.',
   0x000A5D: u'PUC Founder (MSC) Berhad',
   0x000A5E: u'3COM Corporation',
   0x000A5F: u'almedio inc.',
   0x000A60: u'Autostar Technology Pte Ltd',
   0x000A61: u'Cellinx Systems Inc.',
   0x000A62: u'Crinis Networks, Inc.',
   0x000A63: u'DHD GmbH',
   0x000A64: u'Eracom Technologies',
   0x000A65: u'GentechMedia.co.,ltd.',
   0x000A66: u'MITSUBISHI ELECTRIC SYSTEM & SERVICE CO.,LTD.',
   0x000A67: u'OngCorp',
   0x000A68: u'SolarFlare Communications, Inc.',
   0x000A69: u'SUNNY bell Technology Co., Ltd.',
   0x000A6A: u'SVM Microwaves s.r.o.',
   0x000A6B: u'Tadiran Telecom Business Systems LTD',
   0x000A6C: u'Walchem Corporation',
   0x000A6D: u'EKS Elektronikservice GmbH',
   0x000A6E: u'Broadcast Technology Limited',
   0x000A6F: u'ZyFLEX Technologies Inc',
   0x000A70: u'MPLS Forum',
   0x000A71: u'Avrio Technologies, Inc',
   0x000A72: u'SimpleTech, Inc.',
   0x000A73: u'Scientific Atlanta',
   0x000A74: u'Manticom Networks Inc.',
   0x000A75: u'Cat Electronics',
   0x000A76: u'Beida Jade Bird Huaguang Technology Co.,Ltd',
   0x000A77: u'Bluewire Technologies LLC',
   0x000A78: u'OLITEC',
   0x000A79: u'corega K.K.',
   0x000A7A: u'Kyoritsu Electric Co., Ltd.',
   0x000A7B: u'Cornelius Consult',
   0x000A7C: u'Tecton Ltd',
   0x000A7D: u'Valo, Inc.',
   0x000A7E: u'The Advantage Group',
   0x000A7F: u'Teradon Industries, Inc',
   0x000A80: u'Telkonet Inc.',
   0x000A81: u'TEIMA Audiotex S.L.',
   0x000A82: u'TATSUTA SYSTEM ELECTRONICS CO.,LTD.',
   0x000A83: u'SALTO SYSTEMS S.L.',
   0x000A84: u'Rainsun Enterprise Co., Ltd.',
   0x000A85: u'PLAT\'C2,Inc',
   0x000A86: u'Lenze',
   0x000A87: u'Integrated Micromachines Inc.',
   0x000A88: u'InCypher S.A.',
   0x000A89: u'Creval Systems, Inc.',
   0x000A8A: u'Cisco Systems',
   0x000A8B: u'Cisco Systems',
   0x000A8C: u'Guardware Systems Ltd.',
   0x000A8D: u'EUROTHERM LIMITED',
   0x000A8E: u'Invacom Ltd',
   0x000A8F: u'Aska International Inc.',
   0x000A90: u'Bayside Interactive, Inc.',
   0x000A91: u'HemoCue AB',
   0x000A92: u'Presonus Corporation',
   0x000A93: u'W2 Networks, Inc.',
   0x000A94: u'ShangHai cellink CO., LTD',
   0x000A95: u'Apple Computer, Inc.',
   0x000A96: u'MEWTEL TECHNOLOGY INC.',
   0x000A97: u'SONICblue, Inc.',
   0x000A98: u'M+F Gwinner GmbH & Co',
   0x000A99: u'Dataradio Inc.',
   0x000A9A: u'Aiptek International Inc',
   0x000A9B: u'Towa Meccs Corporation',
   0x000A9C: u'Server Technology, Inc.',
   0x000A9D: u'King Young Technology Co. Ltd.',
   0x000A9E: u'BroadWeb Corportation',
   0x000A9F: u'Pannaway Technologies, Inc.',
   0x000AA0: u'Cedar Point Communications',
   0x000AA1: u'V V S Limited',
   0x000AA2: u'SYSTEK INC.',
   0x000AA3: u'SHIMAFUJI ELECTRIC CO.,LTD.',
   0x000AA4: u'SHANGHAI SURVEILLANCE TECHNOLOGY CO,LTD',
   0x000AA5: u'MAXLINK INDUSTRIES LIMITED',
   0x000AA6: u'Hochiki Corporation',
   0x000AA7: u'FEI Company',
   0x000AA8: u'ePipe Pty. Ltd.',
   0x000AA9: u'Brooks Automation GmbH',
   0x000AAA: u'AltiGen Communications Inc.',
   0x000AAB: u'TOYOTA MACS, INC.',
   0x000AAC: u'TerraTec Electronic GmbH',
   0x000AAD: u'Stargames Corporation',
   0x000AAE: u'Rosemount Process Analytical',
   0x000AAF: u'Pipal Systems',
   0x000AB0: u'LOYTEC electronics GmbH',
   0x000AB1: u'GENETEC Corporation',
   0x000AB2: u'Fresnel Wireless Systems',
   0x000AB3: u'Fa. GIRA',
   0x000AB4: u'ETIC Telecommunications',
   0x000AB5: u'Digital Electronic Network',
   0x000AB6: u'COMPUNETIX, INC',
   0x000AB7: u'Cisco Systems',
   0x000AB8: u'Cisco Systems',
   0x000AB9: u'Astera Technologies Corp.',
   0x000ABA: u'Arcon Technology Limited',
   0x000ABB: u'Taiwan Secom Co,. Ltd',
   0x000ABC: u'Seabridge Ltd.',
   0x000ABD: u'Rupprecht & Patashnick Co.',
   0x000ABE: u'OPNET Technologies CO., LTD.',
   0x000ABF: u'HIROTA SS',
   0x000AC0: u'Fuyoh Video Industry CO., LTD.',
   0x000AC1: u'Futuretel',
   0x000AC2: u'FiberHome Telecommunication Technologies CO.,LTD',
   0x000AC3: u'eM Technics Co., Ltd.',
   0x000AC4: u'Daewoo Teletech Co., Ltd',
   0x000AC5: u'Color Kinetics',
   0x000AC6: u'Ceterus Networks, Inc.',
   0x000AC7: u'Unication Group',
   0x000AC8: u'ZPSYS CO.,LTD. (Planning&Management)',
   0x000AC9: u'Zambeel Inc',
   0x000ACA: u'YOKOYAMA SHOKAI CO.,Ltd.',
   0x000ACB: u'XPAK MSA Group',
   0x000ACC: u'Winnow Networks, Inc.',
   0x000ACD: u'Sunrich Technology Limited',
   0x000ACE: u'RADIANTECH, INC.',
   0x000ACF: u'PROVIDEO Multimedia Co. Ltd.',
   0x000AD0: u'Niigata Develoment Center,  F.I.T. Co., Ltd.',
   0x000AD1: u'MWS',
   0x000AD2: u'JEPICO Corporation',
   0x000AD3: u'INITECH Co., Ltd',
   0x000AD4: u'CoreBell Systems Inc.',
   0x000AD5: u'Brainchild Electronic Co., Ltd.',
   0x000AD6: u'BeamReach Networks',
   0x000AD7: u'Origin ELECTRIC CO.,LTD.',
   0x000AD8: u'IPCserv Technology Corp.',
   0x000AD9: u'Sony Ericsson Mobile Communications AB',
   0x000ADA: u'PRIVATE',
   0x000ADB: u'SkyPilot Network, Inc',
   0x000ADC: u'RuggedCom Inc.',
   0x000ADD: u'InSciTek Microsystems, Inc.',
   0x000ADE: u'Happy Communication Co., Ltd.',
   0x000ADF: u'Gennum Corporation',
   0x000AE0: u'Fujitsu Softek',
   0x000AE1: u'EG Technology',
   0x000AE2: u'Binatone Electronics International, Ltd',
   0x000AE3: u'YANG MEI TECHNOLOGY CO., LTD',
   0x000AE4: u'Wistron Corp.',
   0x000AE5: u'ScottCare Corporation',
   0x000AE6: u'Elitegroup Computer System Co. (ECS)',
   0x000AE7: u'ELIOP S.A.',
   0x000AE8: u'Cathay Roxus Information Technology Co. LTD',
   0x000AE9: u'AirVast Technology Inc.',
   0x000AEA: u'ADAM ELEKTRONIK LTD.STI.',
   0x000AEB: u'Shenzhen Tp-Link Technology Co; Ltd.',
   0x000AEC: u'Koatsu Gas Kogyo Co., Ltd.',
   0x000AED: u'HARTING Vending G.m.b.H. & CO KG',
   0x000AEE: u'GCD Hard- & Software GmbH',
   0x000AEF: u'OTRUM ASA',
   0x000AF0: u'SHIN-OH ELECTRONICS CO., LTD. R&D',
   0x000AF1: u'Clarity Design, Inc.',
   0x000AF2: u'NeoAxiom Corp.',
   0x000AF3: u'Cisco Systems',
   0x000AF4: u'Cisco Systems',
   0x000AF5: u'Airgo Networks, Inc.',
   0x000AF6: u'Computer Process Controls',
   0x000AF7: u'Broadcom Corp.',
   0x000AF8: u'American Telecare Inc.',
   0x000AF9: u'HiConnect, Inc.',
   0x000AFA: u'Traverse Technologies Australia',
   0x000AFB: u'Ambri Limited',
   0x000AFC: u'Core Tec Communications, LLC',
   0x000AFD: u'Viking Electronic Services',
   0x000AFE: u'NovaPal Ltd',
   0x000AFF: u'Kilchherr Elektronik AG',
   0x000B00: u'FUJIAN START COMPUTER EQUIPMENT CO.,LTD',
   0x000B01: u'DAIICHI ELECTRONICS CO., LTD.',
   0x000B02: u'Dallmeier electronic',
   0x000B03: u'Taekwang Industrial Co., Ltd',
   0x000B04: u'Volktek Corporation',
   0x000B05: u'Pacific Broadband Networks',
   0x000B06: u'Motorola BCS',
   0x000B07: u'Voxpath Networks',
   0x000B08: u'Pillar Data Systems',
   0x000B09: u'Ifoundry Systems Singapore',
   0x000B0A: u'dBm Optics',
   0x000B0B: u'Corrent Corporation',
   0x000B0C: u'Agile Systems Inc.',
   0x000B0D: u'Air2U, Inc.',
   0x000B0E: u'Trapeze Networks',
   0x000B0F: u'Nyquist Industrial Control BV',
   0x000B10: u'11wave Technonlogy Co.,Ltd',
   0x000B11: u'HIMEJI ABC TRADING CO.,LTD.',
   0x000B12: u'NURI Telecom Co., Ltd.',
   0x000B13: u'ZETRON INC',
   0x000B14: u'ViewSonic Corporation',
   0x000B15: u'Platypus Technology',
   0x000B16: u'Communication Machinery Corporation',
   0x000B17: u'MKS Instruments',
   0x000B18: u'PRIVATE',
   0x000B19: u'Vernier Networks, Inc.',
   0x000B1A: u'Teltone Corporation',
   0x000B1B: u'Systronix, Inc.',
   0x000B1C: u'SIBCO bv',
   0x000B1D: u'LayerZero Power Systems, Inc.',
   0x000B1E: u'KAPPA opto-electronics GmbH',
   0x000B1F: u'I CON Computer Co.',
   0x000B20: u'Hirata corporation',
   0x000B21: u'G-Star Communications Inc.',
   0x000B22: u'Environmental Systems and Services',
   0x000B23: u'Siemens Subscriber Networks',
   0x000B24: u'AirLogic',
   0x000B25: u'Aeluros',
   0x000B26: u'Wetek Corporation',
   0x000B27: u'Scion Corporation',
   0x000B28: u'Quatech Inc.',
   0x000B29: u'LG Industrial Systems Co.,Ltd.',
   0x000B2A: u'HOWTEL Co., Ltd.',
   0x000B2B: u'HOSTNET CORPORATION',
   0x000B2C: u'Eiki Industrial Co. Ltd.',
   0x000B2D: u'Danfoss Inc.',
   0x000B2E: u'Cal-Comp Electronics (Thailand) Public Company Limited Taipe',
   0x000B2F: u'bplan GmbH',
   0x000B30: u'Beijing Gongye Science & Technology Co.,Ltd',
   0x000B31: u'Yantai ZhiYang Scientific and technology industry CO., LTD',
   0x000B32: u'VORMETRIC, INC.',
   0x000B33: u'Vivato',
   0x000B34: u'ShangHai Broadband Technologies CO.LTD',
   0x000B35: u'Quad Bit System co., Ltd.',
   0x000B36: u'Productivity Systems, Inc.',
   0x000B37: u'MANUFACTURE DES MONTRES ROLEX SA',
   0x000B38: u'Knuerr AG',
   0x000B39: u'Keisoku Giken Co.,Ltd.',
   0x000B3A: u'QuStream Corporation',
   0x000B3B: u'devolo AG',
   0x000B3C: u'Cygnal Integrated Products, Inc.',
   0x000B3D: u'CONTAL OK Ltd.',
   0x000B3E: u'BittWare, Inc',
   0x000B3F: u'Anthology Solutions Inc.',
   0x000B40: u'OpNext Inc.',
   0x000B41: u'Ing. Buero Dr. Beutlhauser',
   0x000B42: u'commax Co., Ltd.',
   0x000B43: u'Microscan Systems, Inc.',
   0x000B44: u'Concord IDea Corp.',
   0x000B45: u'Cisco',
   0x000B46: u'Cisco',
   0x000B47: u'Advanced Energy',
   0x000B48: u'sofrel',
   0x000B49: u'RF-Link System Inc.',
   0x000B4A: u'Visimetrics (UK) Ltd',
   0x000B4B: u'VISIOWAVE SA',
   0x000B4C: u'Clarion (M) Sdn Bhd',
   0x000B4D: u'Emuzed',
   0x000B4E: u'VertexRSI Antenna Products Division',
   0x000B4F: u'Verifone, INC.',
   0x000B50: u'Oxygnet',
   0x000B51: u'Micetek International Inc.',
   0x000B52: u'JOYMAX ELECTRONICS CORP.',
   0x000B53: u'INITIUM Co., Ltd.',
   0x000B54: u'BiTMICRO Networks, Inc.',
   0x000B55: u'ADInstruments',
   0x000B56: u'Cybernetics',
   0x000B57: u'Silicon Laboratories',
   0x000B58: u'Astronautics C.A  LTD',
   0x000B59: u'ScriptPro, LLC',
   0x000B5A: u'HyperEdge',
   0x000B5B: u'Rincon Research Corporation',
   0x000B5C: u'Newtech Co.,Ltd',
   0x000B5D: u'FUJITSU LIMITED',
   0x000B5E: u'Audio Engineering Society Inc.',
   0x000B5F: u'Cisco Systems',
   0x000B60: u'Cisco Systems',
   0x000B61: u'Friedrich Ltze GmbH &Co.',
   0x000B62: u'Ingenieurbro Ingo Mohnen',
   0x000B63: u'Kaleidescape',
   0x000B64: u'Kieback & Peter GmbH & Co KG',
   0x000B65: u'Sy.A.C. srl',
   0x000B66: u'Teralink Communications',
   0x000B67: u'Topview Technology Corporation',
   0x000B68: u'Addvalue Communications Pte Ltd',
   0x000B69: u'Franke Finland Oy',
   0x000B6A: u'Asiarock Incorporation',
   0x000B6B: u'Wistron Neweb Corp.',
   0x000B6C: u'Sychip Inc.',
   0x000B6D: u'SOLECTRON JAPAN NAKANIIDA',
   0x000B6E: u'Neff Instrument Corp.',
   0x000B6F: u'Media Streaming Networks Inc',
   0x000B70: u'Load Technology, Inc.',
   0x000B71: u'Litchfield Communications Inc.',
   0x000B72: u'Lawo AG',
   0x000B73: u'Kodeos Communications',
   0x000B74: u'Kingwave Technology Co., Ltd.',
   0x000B75: u'Iosoft Ltd.',
   0x000B76: u'ET&T Co. Ltd.',
   0x000B77: u'Cogent Systems, Inc.',
   0x000B78: u'TAIFATECH INC.',
   0x000B79: u'X-COM, Inc.',
   0x000B7A: u'Wave Science Inc.',
   0x000B7B: u'Test-Um Inc.',
   0x000B7C: u'Telex Communications',
   0x000B7D: u'SOLOMON EXTREME INTERNATIONAL LTD.',
   0x000B7E: u'SAGINOMIYA Seisakusho Inc.',
   0x000B7F: u'OmniWerks',
   0x000B80: u'Lycium Networks',
   0x000B81: u'Kaparel Corporation',
   0x000B82: u'Grandstream Networks, Inc.',
   0x000B83: u'DATAWATT B.V.',
   0x000B84: u'BODET',
   0x000B85: u'Airespace, Inc.',
   0x000B86: u'Aruba Networks',
   0x000B87: u'American Reliance Inc.',
   0x000B88: u'Vidisco ltd.',
   0x000B89: u'Top Global Technology, Ltd.',
   0x000B8A: u'MITEQ Inc.',
   0x000B8B: u'KERAJET, S.A.',
   0x000B8C: u'flextronics israel',
   0x000B8D: u'Avvio Networks',
   0x000B8E: u'Ascent Corporation',
   0x000B8F: u'AKITA ELECTRONICS SYSTEMS CO.,LTD.',
   0x000B90: u'Covaro Networks, Inc.',
   0x000B91: u'Aglaia Gesellschaft fr Bildverarbeitung und Kommunikation m',
   0x000B92: u'Ascom Danmark A/S',
   0x000B93: u'Barmag Electronic',
   0x000B94: u'Digital Monitoring Products, Inc.',
   0x000B95: u'eBet Gaming Systems Pty Ltd',
   0x000B96: u'Innotrac Diagnostics Oy',
   0x000B97: u'Matsushita Electric Industrial Co.,Ltd.',
   0x000B98: u'NiceTechVision',
   0x000B99: u'SensAble Technologies, Inc.',
   0x000B9A: u'Shanghai Ulink Telecom Equipment Co. Ltd.',
   0x000B9B: u'Sirius System Co, Ltd.',
   0x000B9C: u'TriBeam Technologies, Inc.',
   0x000B9D: u'TwinMOS Technologies Inc.',
   0x000B9E: u'Yasing Technology Corp.',
   0x000B9F: u'Neue ELSA GmbH',
   0x000BA0: u'T&L Information Inc.',
   0x000BA1: u'SYSCOM Ltd.',
   0x000BA2: u'Sumitomo Electric Networks, Inc',
   0x000BA3: u'Siemens AG, I&S',
   0x000BA4: u'Shiron Satellite Communications Ltd. (1996)',
   0x000BA5: u'Quasar Cipta Mandiri, PT',
   0x000BA6: u'Miyakawa Electric Works Ltd.',
   0x000BA7: u'Maranti Networks',
   0x000BA8: u'HANBACK ELECTRONICS CO., LTD.',
   0x000BA9: u'CloudShield Technologies, Inc.',
   0x000BAA: u'Aiphone co.,Ltd',
   0x000BAB: u'Advantech Technology (CHINA) Co., Ltd.',
   0x000BAC: u'3Com Europe Ltd.',
   0x000BAD: u'PC-PoS Inc.',
   0x000BAE: u'Vitals System Inc.',
   0x000BAF: u'WOOJU COMMUNICATIONS Co,.Ltd',
   0x000BB0: u'Sysnet Telematica srl',
   0x000BB1: u'Super Star Technology Co., Ltd.',
   0x000BB2: u'SMALLBIG TECHNOLOGY',
   0x000BB3: u'RiT technologies Ltd.',
   0x000BB4: u'RDC Semiconductor Inc.,',
   0x000BB5: u'nStor Technologies, Inc.',
   0x000BB6: u'Mototech Inc.',
   0x000BB7: u'Micro Systems Co.,Ltd.',
   0x000BB8: u'Kihoku Electronic Co.',
   0x000BB9: u'Imsys AB',
   0x000BBA: u'Harmonic Broadband Access Networks',
   0x000BBB: u'Etin Systems Co., Ltd',
   0x000BBC: u'En Garde Systems, Inc.',
   0x000BBD: u'Connexionz Limited',
   0x000BBE: u'Cisco Systems',
   0x000BBF: u'Cisco Systems',
   0x000BC0: u'China IWNComm Co., Ltd.',
   0x000BC1: u'Bay Microsystems, Inc.',
   0x000BC2: u'Corinex Communication Corp.',
   0x000BC3: u'Multiplex, Inc.',
   0x000BC4: u'BIOTRONIK GmbH & Co',
   0x000BC5: u'SMC Networks, Inc.',
   0x000BC6: u'ISAC, Inc.',
   0x000BC7: u'ICET S.p.A.',
   0x000BC8: u'AirFlow Networks',
   0x000BC9: u'Electroline Equipment',
   0x000BCA: u'DATAVAN International Corporation',
   0x000BCB: u'Fagor Automation , S. Coop',
   0x000BCC: u'JUSAN, S.A.',
   0x000BCD: u'Compaq (HP)',
   0x000BCE: u'Free2move AB',
   0x000BCF: u'AGFA NDT INC.',
   0x000BD0: u'XiMeta Technology Americas Inc.',
   0x000BD1: u'Aeronix, Inc.',
   0x000BD2: u'Remopro Technology Inc.',
   0x000BD3: u'cd3o',
   0x000BD4: u'Beijing Wise Technology & Science Development Co.Ltd',
   0x000BD5: u'Nvergence, Inc.',
   0x000BD6: u'Paxton Access Ltd',
   0x000BD7: u'MBB Gelma GmbH',
   0x000BD8: u'Industrial Scientific Corp.',
   0x000BD9: u'General Hydrogen',
   0x000BDA: u'EyeCross Co.,Inc.',
   0x000BDB: u'Dell ESG PCBA Test',
   0x000BDC: u'AKCP',
   0x000BDD: u'TOHOKU RICOH Co., LTD.',
   0x000BDE: u'TELDIX GmbH',
   0x000BDF: u'Shenzhen RouterD Networks Limited',
   0x000BE0: u'SercoNet Ltd.',
   0x000BE1: u'Nokia NET Product Operations',
   0x000BE2: u'Lumenera Corporation',
   0x000BE3: u'Key Stream Co., Ltd.',
   0x000BE4: u'Hosiden Corporation',
   0x000BE5: u'HIMS Korea Co., Ltd.',
   0x000BE6: u'Datel Electronics',
   0x000BE7: u'COMFLUX TECHNOLOGY INC.',
   0x000BE8: u'AOIP',
   0x000BE9: u'Actel Corporation',
   0x000BEA: u'Zultys Technologies',
   0x000BEB: u'Systegra AG',
   0x000BEC: u'NIPPON ELECTRIC INSTRUMENT, INC.',
   0x000BED: u'ELM Inc.',
   0x000BEE: u'inc.jet, Incorporated',
   0x000BEF: u'Code Corporation',
   0x000BF0: u'MoTEX Products Co., Ltd.',
   0x000BF1: u'LAP Laser Applikations',
   0x000BF2: u'Chih-Kan Technology Co., Ltd.',
   0x000BF3: u'BAE SYSTEMS',
   0x000BF4: u'PRIVATE',
   0x000BF5: u'Shanghai Sibo Telecom Technology Co.,Ltd',
   0x000BF6: u'Nitgen Co., Ltd',
   0x000BF7: u'NIDEK CO.,LTD',
   0x000BF8: u'Infinera',
   0x000BF9: u'Gemstone communications, Inc.',
   0x000BFA: u'EXEMYS SRL',
   0x000BFB: u'D-NET International Corporation',
   0x000BFC: u'Cisco Systems',
   0x000BFD: u'Cisco Systems',
   0x000BFE: u'CASTEL Broadband Limited',
   0x000BFF: u'Berkeley Camera Engineering',
   0x000C00: u'BEB Industrie-Elektronik AG',
   0x000C01: u'Abatron AG',
   0x000C02: u'ABB Oy',
   0x000C03: u'HDMI Licensing, LLC',
   0x000C04: u'Tecnova',
   0x000C05: u'RPA Reserch Co., Ltd.',
   0x000C06: u'Nixvue Systems  Pte Ltd',
   0x000C07: u'Iftest AG',
   0x000C08: u'HUMEX Technologies Corp.',
   0x000C09: u'Hitachi IE Systems Co., Ltd',
   0x000C0A: u'Guangdong Province Electronic Technology Research Institute',
   0x000C0B: u'Broadbus Technologies',
   0x000C0C: u'APPRO TECHNOLOGY INC.',
   0x000C0D: u'Communications & Power Industries / Satcom Division',
   0x000C0E: u'XtremeSpectrum, Inc.',
   0x000C0F: u'Techno-One Co., Ltd',
   0x000C10: u'PNI Corporation',
   0x000C11: u'NIPPON DEMPA CO.,LTD.',
   0x000C12: u'Micro-Optronic-Messtechnik GmbH',
   0x000C13: u'MediaQ',
   0x000C14: u'Diagnostic Instruments, Inc.',
   0x000C15: u'CyberPower Systems, Inc.',
   0x000C16: u'Concorde Microsystems Inc.',
   0x000C17: u'AJA Video Systems Inc',
   0x000C18: u'Zenisu Keisoku Inc.',
   0x000C19: u'Telio Communications GmbH',
   0x000C1A: u'Quest Technical Solutions Inc.',
   0x000C1B: u'ORACOM Co, Ltd.',
   0x000C1C: u'MicroWeb Co., Ltd.',
   0x000C1D: u'Mettler & Fuchs AG',
   0x000C1E: u'Global Cache',
   0x000C1F: u'Glimmerglass Networks',
   0x000C20: u'Fi WIn, Inc.',
   0x000C21: u'Faculty of Science and Technology, Keio University',
   0x000C22: u'Double D Electronics Ltd',
   0x000C23: u'Beijing Lanchuan Tech. Co., Ltd.',
   0x000C24: u'ANATOR',
   0x000C25: u'Allied Telesyn Networks',
   0x000C26: u'Weintek Labs. Inc.',
   0x000C27: u'Sammy Corporation',
   0x000C28: u'RIFATRON',
   0x000C29: u'VMware, Inc.',
   0x000C2A: u'OCTTEL Communication Co., Ltd.',
   0x000C2B: u'ELIAS Technology, Inc.',
   0x000C2C: u'Enwiser Inc.',
   0x000C2D: u'FullWave Technology Co., Ltd.',
   0x000C2E: u'Openet information technology(shenzhen) Co., Ltd.',
   0x000C2F: u'SeorimTechnology Co.,Ltd.',
   0x000C30: u'Cisco',
   0x000C31: u'Cisco',
   0x000C32: u'Avionic Design Development GmbH',
   0x000C33: u'Compucase Enterprise Co. Ltd.',
   0x000C34: u'Vixen Co., Ltd.',
   0x000C35: u'KaVo Dental GmbH & Co. KG',
   0x000C36: u'SHARP TAKAYA ELECTRONICS INDUSTRY CO.,LTD.',
   0x000C37: u'Geomation, Inc.',
   0x000C38: u'TelcoBridges Inc.',
   0x000C39: u'Sentinel Wireless Inc.',
   0x000C3A: u'Oxance',
   0x000C3B: u'Orion Electric Co., Ltd.',
   0x000C3C: u'MediaChorus, Inc.',
   0x000C3D: u'Glsystech Co., Ltd.',
   0x000C3E: u'Crest Audio',
   0x000C3F: u'Cogent Defence & Security Networks,',
   0x000C40: u'Altech Controls',
   0x000C41: u'The Linksys Group, Inc.',
   0x000C42: u'Routerboard.com',
   0x000C43: u'Ralink Technology, Corp.',
   0x000C44: u'Automated Interfaces, Inc.',
   0x000C45: u'Animation Technologies Inc.',
   0x000C46: u'Allied Telesyn Inc.',
   0x000C47: u'SK Teletech(R&D Planning Team)',
   0x000C48: u'QoStek Corporation',
   0x000C49: u'Dangaard Telecom RTC Division A/S',
   0x000C4A: u'Cygnus Microsystems Private Limited',
   0x000C4B: u'Cheops Elektronik',
   0x000C4C: u'Arcor AG&Co.',
   0x000C4D: u'ACRA CONTROL',
   0x000C4E: u'Winbest Technology CO,LT',
   0x000C4F: u'UDTech Japan Corporation',
   0x000C50: u'Seagate Technology',
   0x000C51: u'Scientific Technologies Inc.',
   0x000C52: u'Roll Systems Inc.',
   0x000C53: u'PRIVATE',
   0x000C54: u'Pedestal Networks, Inc',
   0x000C55: u'Microlink Communications Inc.',
   0x000C56: u'Megatel Computer (1986) Corp.',
   0x000C57: u'MACKIE Engineering Services Belgium BVBA',
   0x000C58: u'M&S Systems',
   0x000C59: u'Indyme Electronics, Inc.',
   0x000C5A: u'IBSmm Industrieelektronik Multimedia',
   0x000C5B: u'HANWANG TECHNOLOGY CO.,LTD',
   0x000C5C: u'GTN Systems B.V.',
   0x000C5D: u'CHIC TECHNOLOGY (CHINA) CORP.',
   0x000C5E: u'Calypso Medical',
   0x000C5F: u'Avtec, Inc.',
   0x000C60: u'ACM Systems',
   0x000C61: u'AC Tech corporation DBA Advanced Digital',
   0x000C62: u'ABB Automation Technology Products AB, Control',
   0x000C63: u'Zenith Electronics Corporation',
   0x000C64: u'X2 MSA Group',
   0x000C65: u'Sunin Telecom',
   0x000C66: u'Pronto Networks Inc',
   0x000C67: u'OYO ELECTRIC CO.,LTD',
   0x000C68: u'SigmaTel, Inc.',
   0x000C69: u'National Radio Astronomy Observatory',
   0x000C6A: u'MBARI',
   0x000C6B: u'Kurz Industrie-Elektronik GmbH',
   0x000C6C: u'Elgato Systems LLC',
   0x000C6D: u'BOC Edwards',
   0x000C6E: u'ASUSTEK COMPUTER INC.',
   0x000C6F: u'Amtek system co.,LTD.',
   0x000C70: u'ACC GmbH',
   0x000C71: u'Wybron, Inc',
   0x000C72: u'Tempearl Industrial Co., Ltd.',
   0x000C73: u'TELSON ELECTRONICS CO., LTD',
   0x000C74: u'RIVERTEC CORPORATION',
   0x000C75: u'Oriental integrated electronics. LTD',
   0x000C76: u'MICRO-STAR INTERNATIONAL CO., LTD.',
   0x000C77: u'Life Racing Ltd',
   0x000C78: u'In-Tech Electronics Limited',
   0x000C79: u'Extel Communications P/L',
   0x000C7A: u'DaTARIUS Technologies GmbH',
   0x000C7B: u'ALPHA PROJECT Co.,Ltd.',
   0x000C7C: u'Internet Information Image Inc.',
   0x000C7D: u'TEIKOKU ELECTRIC MFG. CO., LTD',
   0x000C7E: u'Tellium Incorporated',
   0x000C7F: u'synertronixx GmbH',
   0x000C80: u'Opelcomm Inc.',
   0x000C81: u'Nulec Industries Pty Ltd',
   0x000C82: u'NETWORK TECHNOLOGIES INC',
   0x000C83: u'Logical Solutions',
   0x000C84: u'Eazix, Inc.',
   0x000C85: u'Cisco Systems',
   0x000C86: u'Cisco Systems',
   0x000C87: u'ATI',
   0x000C88: u'Apache Micro Peripherals, Inc.',
   0x000C89: u'AC Electric Vehicles, Ltd.',
   0x000C8A: u'Bose Corporation',
   0x000C8B: u'Connect Tech Inc',
   0x000C8C: u'KODICOM CO.,LTD.',
   0x000C8D: u'MATRIX VISION GmbH',
   0x000C8E: u'Mentor Engineering Inc',
   0x000C8F: u'Nergal s.r.l.',
   0x000C90: u'Octasic Inc.',
   0x000C91: u'Riverhead Networks Inc.',
   0x000C92: u'WolfVision Gmbh',
   0x000C93: u'Xeline Co., Ltd.',
   0x000C94: u'United Electronic Industries, Inc.',
   0x000C95: u'PrimeNet',
   0x000C96: u'OQO, Inc.',
   0x000C97: u'NV ADB TTV Technologies SA',
   0x000C98: u'LETEK Communications Inc.',
   0x000C99: u'HITEL LINK Co.,Ltd',
   0x000C9A: u'Hitech Electronics Corp.',
   0x000C9B: u'EE Solutions, Inc',
   0x000C9C: u'Chongho information & communications',
   0x000C9D: u'AirWalk Communications, Inc.',
   0x000C9E: u'MemoryLink Corp.',
   0x000C9F: u'NKE Corporation',
   0x000CA0: u'StorCase Technology, Inc.',
   0x000CA1: u'SIGMACOM Co., LTD.',
   0x000CA2: u'Scopus Network Technologies Ltd',
   0x000CA3: u'Rancho Technology, Inc.',
   0x000CA4: u'Prompttec Product Management GmbH',
   0x000CA5: u'Naman NZ LTd',
   0x000CA6: u'Mintera Corporation',
   0x000CA7: u'Metro (Suzhou) Technologies Co., Ltd.',
   0x000CA8: u'Garuda Networks Corporation',
   0x000CA9: u'Ebtron Inc.',
   0x000CAA: u'Cubic Transportation Systems Inc',
   0x000CAB: u'COMMEND International',
   0x000CAC: u'Citizen Watch Co., Ltd.',
   0x000CAD: u'BTU International',
   0x000CAE: u'Ailocom Oy',
   0x000CAF: u'TRI TERM CO.,LTD.',
   0x000CB0: u'Star Semiconductor Corporation',
   0x000CB1: u'Salland Engineering (Europe) BV',
   0x000CB2: u'safei Co., Ltd.',
   0x000CB3: u'ROUND Co.,Ltd.',
   0x000CB4: u'AutoCell Laboratories, Inc.',
   0x000CB5: u'Premier Technolgies, Inc',
   0x000CB6: u'NANJING SEU MOBILE & INTERNET TECHNOLOGY CO.,LTD',
   0x000CB7: u'Nanjing Huazhuo Electronics Co., Ltd.',
   0x000CB8: u'MEDION AG',
   0x000CB9: u'LEA',
   0x000CBA: u'Jamex',
   0x000CBB: u'ISKRAEMECO',
   0x000CBC: u'Iscutum',
   0x000CBD: u'Interface Masters, Inc',
   0x000CBE: u'PRIVATE',
   0x000CBF: u'Holy Stone Ent. Co., Ltd.',
   0x000CC0: u'Genera Oy',
   0x000CC1: u'Cooper Industries Inc.',
   0x000CC2: u'PRIVATE',
   0x000CC3: u'BeWAN systems',
   0x000CC4: u'Tiptel AG',
   0x000CC5: u'Nextlink Co., Ltd.',
   0x000CC6: u'Ka-Ro electronics GmbH',
   0x000CC7: u'Intelligent Computer Solutions Inc.',
   0x000CC8: u'Xytronix Research & Design, Inc.',
   0x000CC9: u'ILWOO DATA & TECHNOLOGY CO.,LTD',
   0x000CCA: u'Hitachi Global Storage Technologies',
   0x000CCB: u'Design Combus Ltd',
   0x000CCC: u'Aeroscout Ltd.',
   0x000CCD: u'IEC - TC57',
   0x000CCE: u'Cisco Systems',
   0x000CCF: u'Cisco Systems',
   0x000CD0: u'Symetrix',
   0x000CD1: u'SFOM Technology Corp.',
   0x000CD2: u'Schaffner EMV AG',
   0x000CD3: u'Prettl Elektronik Radeberg GmbH',
   0x000CD4: u'Positron Public Safety Systems inc.',
   0x000CD5: u'Passave Inc.',
   0x000CD6: u'PARTNER TECH',
   0x000CD7: u'Nallatech Ltd',
   0x000CD8: u'M. K. Juchheim GmbH & Co',
   0x000CD9: u'Itcare Co., Ltd',
   0x000CDA: u'FreeHand Systems, Inc.',
   0x000CDB: u'Foundry Networks',
   0x000CDC: u'BECS Technology, Inc',
   0x000CDD: u'AOS Technologies AG',
   0x000CDE: u'ABB STOTZ-KONTAKT GmbH',
   0x000CDF: u'PULNiX America, Inc',
   0x000CE0: u'Trek Diagnostics Inc.',
   0x000CE1: u'The Open Group',
   0x000CE2: u'Rolls-Royce',
   0x000CE3: u'Option International N.V.',
   0x000CE4: u'NeuroCom International, Inc.',
   0x000CE5: u'Motorola BCS',
   0x000CE6: u'Meru Networks Inc',
   0x000CE7: u'MediaTek Inc.',
   0x000CE8: u'GuangZhou AnJuBao Co., Ltd',
   0x000CE9: u'BLOOMBERG L.P.',
   0x000CEA: u'aphona Kommunikationssysteme',
   0x000CEB: u'CNMP Networks, Inc.',
   0x000CEC: u'Spectracom Corp.',
   0x000CED: u'Real Digital Media',
   0x000CEE: u'jp-embedded',
   0x000CEF: u'Open Networks Engineering Ltd',
   0x000CF0: u'M & N GmbH',
   0x000CF1: u'Intel Corporation',
   0x000CF2: u'GAMESA ELICA',
   0x000CF3: u'CALL IMAGE SA',
   0x000CF4: u'AKATSUKI ELECTRIC MFG.CO.,LTD.',
   0x000CF5: u'InfoExpress',
   0x000CF6: u'Sitecom Europe BV',
   0x000CF7: u'Nortel Networks',
   0x000CF8: u'Nortel Networks',
   0x000CF9: u'ITT Flygt AB',
   0x000CFA: u'Digital Systems Corp',
   0x000CFB: u'Korea Network Systems',
   0x000CFC: u'S2io Technologies Corp',
   0x000CFD: u'PRIVATE',
   0x000CFE: u'Grand Electronic Co., Ltd',
   0x000CFF: u'MRO-TEK LIMITED',
   0x000D00: u'Seaway Networks Inc.',
   0x000D01: u'P&E Microcomputer Systems, Inc.',
   0x000D02: u'NEC AccessTechnica,Ltd',
   0x000D03: u'Matrics, Inc.',
   0x000D04: u'Foxboro Eckardt Development GmbH',
   0x000D05: u'cybernet manufacturing inc.',
   0x000D06: u'Compulogic Limited',
   0x000D07: u'Calrec Audio Ltd',
   0x000D08: u'AboveCable, Inc.',
   0x000D09: u'Yuehua(Zhuhai) Electronic CO. LTD',
   0x000D0A: u'Projectiondesign as',
   0x000D0B: u'Buffalo Inc.',
   0x000D0C: u'MDI Security Systems',
   0x000D0D: u'ITSupported, LLC',
   0x000D0E: u'Inqnet Systems, Inc.',
   0x000D0F: u'Finlux Ltd',
   0x000D10: u'Embedtronics Oy',
   0x000D11: u'DENTSPLY - Gendex',
   0x000D12: u'AXELL Corporation',
   0x000D13: u'Wilhelm Rutenbeck GmbH&Co.',
   0x000D14: u'Vtech Innovation LP dba Advanced American Telephones',
   0x000D15: u'Voipac s.r.o.',
   0x000D16: u'UHS Systems Pty Ltd',
   0x000D17: u'Turbo Networks Co.Ltd',
   0x000D18: u'Sunitec Enterprise Co., Ltd.',
   0x000D19: u'ROBE Show lighting',
   0x000D1A: u'Mustek System Inc.',
   0x000D1B: u'Kyoto Electronics Manufacturing Co., Ltd.',
   0x000D1C: u'I2E TELECOM',
   0x000D1D: u'HIGH-TEK HARNESS ENT. CO., LTD.',
   0x000D1E: u'Control Techniques',
   0x000D1F: u'AV Digital',
   0x000D20: u'ASAHIKASEI TECHNOSYSTEM CO.,LTD.',
   0x000D21: u'WISCORE Inc.',
   0x000D22: u'Unitronics',
   0x000D23: u'Smart Solution, Inc',
   0x000D24: u'SENTEC E&E CO., LTD.',
   0x000D25: u'SANDEN CORPORATION',
   0x000D26: u'Primagraphics Limited',
   0x000D27: u'MICROPLEX Printware AG',
   0x000D28: u'Cisco',
   0x000D29: u'Cisco',
   0x000D2A: u'Scanmatic AS',
   0x000D2B: u'Racal Instruments',
   0x000D2C: u'Patapsco Designs Ltd',
   0x000D2D: u'NCT Deutschland GmbH',
   0x000D2E: u'Matsushita Avionics Systems Corporation',
   0x000D2F: u'AIN Comm.Tech.Co., LTD',
   0x000D30: u'IceFyre Semiconductor',
   0x000D31: u'Compellent Technologies, Inc.',
   0x000D32: u'DispenseSource, Inc.',
   0x000D33: u'Prediwave Corp.',
   0x000D34: u'Shell International Exploration and Production, Inc.',
   0x000D35: u'PAC International Ltd',
   0x000D36: u'Wu Han Routon Electronic Co., Ltd',
   0x000D37: u'WIPLUG',
   0x000D38: u'NISSIN INC.',
   0x000D39: u'Network Electronics',
   0x000D3A: u'Microsoft Corp.',
   0x000D3B: u'Microelectronics Technology Inc.',
   0x000D3C: u'i.Tech Dynamic Ltd',
   0x000D3D: u'Hammerhead Systems, Inc.',
   0x000D3E: u'APLUX Communications Ltd.',
   0x000D3F: u'VXI Technology',
   0x000D40: u'Verint Loronix Video Solutions',
   0x000D41: u'Siemens AG ICM MP UC RD IT KLF1',
   0x000D42: u'Newbest Development Limited',
   0x000D43: u'DRS Tactical Systems Inc.',
   0x000D44: u'PRIVATE',
   0x000D45: u'Tottori SANYO Electric Co., Ltd.',
   0x000D46: u'SSD Drives, Inc.',
   0x000D47: u'Collex',
   0x000D48: u'AEWIN Technologies Co., Ltd.',
   0x000D49: u'Triton Systems of Delaware, Inc.',
   0x000D4A: u'Steag ETA-Optik',
   0x000D4B: u'Roku, LLC',
   0x000D4C: u'Outline Electronics Ltd.',
   0x000D4D: u'Ninelanes',
   0x000D4E: u'NDR Co.,LTD.',
   0x000D4F: u'Kenwood Corporation',
   0x000D50: u'Galazar Networks',
   0x000D51: u'DIVR Systems, Inc.',
   0x000D52: u'Comart system',
   0x000D53: u'Beijing 5w Communication Corp.',
   0x000D54: u'3Com Europe Ltd',
   0x000D55: u'SANYCOM Technology Co.,Ltd',
   0x000D56: u'Dell PCBA Test',
   0x000D57: u'Fujitsu I-Network Systems Limited.',
   0x000D58: u'PRIVATE',
   0x000D59: u'Amity Systems, Inc.',
   0x000D5A: u'Tiesse SpA',
   0x000D5B: u'Smart Empire Investments Limited',
   0x000D5C: u'Robert Bosch GmbH, VT-ATMO',
   0x000D5D: u'Raritan Computer, Inc',
   0x000D5E: u'NEC CustomTechnica, Ltd.',
   0x000D5F: u'Minds Inc',
   0x000D60: u'IBM Corporation',
   0x000D61: u'Giga-Byte Technology Co., Ltd.',
   0x000D62: u'Funkwerk Dabendorf GmbH',
   0x000D63: u'DENT Instruments, Inc.',
   0x000D64: u'COMAG Handels AG',
   0x000D65: u'Cisco Systems',
   0x000D66: u'Cisco Systems',
   0x000D67: u'BelAir Networks Inc.',
   0x000D68: u'Vinci Systems, Inc.',
   0x000D69: u'TMT&D Corporation',
   0x000D6A: u'Redwood Technologies LTD',
   0x000D6B: u'Mita-Teknik A/S',
   0x000D6C: u'M-Audio',
   0x000D6D: u'K-Tech Devices Corp.',
   0x000D6E: u'K-Patents Oy',
   0x000D6F: u'Ember Corporation',
   0x000D70: u'Datamax Corporation',
   0x000D71: u'boca systems',
   0x000D72: u'2Wire, Inc',
   0x000D73: u'Technical Support, Inc.',
   0x000D74: u'Sand Network Systems, Inc.',
   0x000D75: u'Kobian Pte Ltd - Taiwan Branch',
   0x000D76: u'Hokuto Denshi Co,. Ltd.',
   0x000D77: u'FalconStor Software',
   0x000D78: u'Engineering & Security',
   0x000D79: u'Dynamic Solutions Co,.Ltd.',
   0x000D7A: u'DiGATTO Asia Pacific Pte Ltd',
   0x000D7B: u'Consensys Computers Inc.',
   0x000D7C: u'Codian Ltd',
   0x000D7D: u'Afco Systems',
   0x000D7E: u'Axiowave Networks, Inc.',
   0x000D7F: u'MIDAS  COMMUNICATION TECHNOLOGIES PTE LTD ( Foreign Branch)',
   0x000D80: u'Online Development Inc',
   0x000D81: u'Pepperl+Fuchs GmbH',
   0x000D82: u'PHS srl',
   0x000D83: u'Sanmina-SCI Hungary  Ltd.',
   0x000D84: u'Makus Inc.',
   0x000D85: u'Tapwave, Inc.',
   0x000D86: u'Huber + Suhner AG',
   0x000D87: u'Elitegroup Computer System Co. (ECS)',
   0x000D88: u'D-Link Corporation',
   0x000D89: u'Bils Technology Inc',
   0x000D8A: u'Winners Electronics Co., Ltd.',
   0x000D8B: u'T&D Corporation',
   0x000D8C: u'Shanghai Wedone Digital Ltd. CO.',
   0x000D8D: u'ProLinx Communication Gateways, Inc.',
   0x000D8E: u'Koden Electronics Co., Ltd.',
   0x000D8F: u'King Tsushin Kogyo Co., LTD.',
   0x000D90: u'Factum Electronics AB',
   0x000D91: u'Eclipse (HQ Espana) S.L.',
   0x000D92: u'Arima Communication Corporation',
   0x000D93: u'Apple Computer',
   0x000D94: u'AFAR Communications,Inc',
   0x000D95: u'Opti-cell, Inc.',
   0x000D96: u'Vtera Technology Inc.',
   0x000D97: u'Tropos Networks, Inc.',
   0x000D98: u'S.W.A.C. Schmitt-Walter Automation Consult GmbH',
   0x000D99: u'Orbital Sciences Corp.; Launch Systems Group',
   0x000D9A: u'INFOTEC LTD',
   0x000D9B: u'Heraeus Electro-Nite International N.V.',
   0x000D9C: u'Elan GmbH & Co KG',
   0x000D9D: u'Hewlett Packard',
   0x000D9E: u'TOKUDEN OHIZUMI SEISAKUSYO Co.,Ltd.',
   0x000D9F: u'RF Micro Devices',
   0x000DA0: u'NEDAP N.V.',
   0x000DA1: u'MIRAE ITS Co.,LTD.',
   0x000DA2: u'Infrant Technologies, Inc.',
   0x000DA3: u'Emerging Technologies Limited',
   0x000DA4: u'DOSCH & AMAND SYSTEMS AG',
   0x000DA5: u'Fabric7 Systems, Inc',
   0x000DA6: u'Universal Switching Corporation',
   0x000DA7: u'PRIVATE',
   0x000DA8: u'Teletronics Technology Corporation',
   0x000DA9: u'T.E.A.M. S.L.',
   0x000DAA: u'S.A.Tehnology co.,Ltd.',
   0x000DAB: u'Parker Hannifin GmbH Electromechanical Division Europe',
   0x000DAC: u'Japan CBM Corporation',
   0x000DAD: u'Dataprobe Inc',
   0x000DAE: u'SAMSUNG HEAVY INDUSTRIES CO., LTD.',
   0x000DAF: u'Plexus Corp (UK) Ltd',
   0x000DB0: u'Olym-tech Co.,Ltd.',
   0x000DB1: u'Japan Network Service Co., Ltd.',
   0x000DB2: u'Ammasso, Inc.',
   0x000DB3: u'SDO Communication Corperation',
   0x000DB4: u'NETASQ',
   0x000DB5: u'GLOBALSAT TECHNOLOGY CORPORATION',
   0x000DB6: u'Teknovus, Inc.',
   0x000DB7: u'SANKO ELECTRIC CO,.LTD',
   0x000DB8: u'SCHILLER AG',
   0x000DB9: u'PC Engines GmbH',
   0x000DBA: u'Oc Document Technologies GmbH',
   0x000DBB: u'Nippon Dentsu Co.,Ltd.',
   0x000DBC: u'Cisco Systems',
   0x000DBD: u'Cisco Systems',
   0x000DBE: u'Bel Fuse Europe Ltd.,UK',
   0x000DBF: u'TekTone Sound & Signal Mfg., Inc.',
   0x000DC0: u'Spagat AS',
   0x000DC1: u'SafeWeb Inc',
   0x000DC2: u'PRIVATE',
   0x000DC3: u'First Communication, Inc.',
   0x000DC4: u'Emcore Corporation',
   0x000DC5: u'EchoStar International Corporation',
   0x000DC6: u'DigiRose Technology Co., Ltd.',
   0x000DC7: u'COSMIC ENGINEERING INC.',
   0x000DC8: u'AirMagnet, Inc',
   0x000DC9: u'THALES Elektronik Systeme GmbH',
   0x000DCA: u'Tait Electronics',
   0x000DCB: u'Petcomkorea Co., Ltd.',
   0x000DCC: u'NEOSMART Corp.',
   0x000DCD: u'GROUPE TXCOM',
   0x000DCE: u'Dynavac Technology Pte Ltd',
   0x000DCF: u'Cidra Corp.',
   0x000DD0: u'TetraTec Instruments GmbH',
   0x000DD1: u'Stryker Corporation',
   0x000DD2: u'Simrad Optronics ASA',
   0x000DD3: u'SAMWOO Telecommunication Co.,Ltd.',
   0x000DD4: u'Revivio Inc.',
   0x000DD5: u'O\'RITE TECHNOLOGY CO.,LTD',
   0x000DD6: u'ITI    LTD',
   0x000DD7: u'Bright',
   0x000DD8: u'BBN',
   0x000DD9: u'Anton Paar GmbH',
   0x000DDA: u'ALLIED TELESIS K.K.',
   0x000DDB: u'AIRWAVE TECHNOLOGIES INC.',
   0x000DDC: u'VAC',
   0x000DDD: u'PROFLO TELRA ELEKTRONK SANAY VE TCARET A..',
   0x000DDE: u'Joyteck Co., Ltd.',
   0x000DDF: u'Japan Image & Network Inc.',
   0x000DE0: u'ICPDAS Co.,LTD',
   0x000DE1: u'Control Products, Inc.',
   0x000DE2: u'CMZ Sistemi Elettronici',
   0x000DE3: u'AT Sweden AB',
   0x000DE4: u'DIGINICS, Inc.',
   0x000DE5: u'Samsung Thales',
   0x000DE6: u'YOUNGBO ENGINEERING CO.,LTD',
   0x000DE7: u'Snap-on OEM Group',
   0x000DE8: u'Nasaco Electronics Pte. Ltd',
   0x000DE9: u'Napatech Aps',
   0x000DEA: u'Kingtel Telecommunication Corp.',
   0x000DEB: u'CompXs Limited',
   0x000DEC: u'Cisco Systems',
   0x000DED: u'Cisco Systems',
   0x000DEE: u'Andrew RF Power Amplifier Group',
   0x000DEF: u'Soc. Coop. Bilanciai',
   0x000DF0: u'QCOM TECHNOLOGY INC.',
   0x000DF1: u'IONIX INC.',
   0x000DF2: u'PRIVATE',
   0x000DF3: u'Asmax Solutions',
   0x000DF4: u'Watertek Co.',
   0x000DF5: u'Teletronics International Inc.',
   0x000DF6: u'Technology Thesaurus Corp.',
   0x000DF7: u'Space Dynamics Lab',
   0x000DF8: u'ORGA Kartensysteme GmbH',
   0x000DF9: u'NDS Limited',
   0x000DFA: u'Micro Control Systems Ltd.',
   0x000DFB: u'Komax AG',
   0x000DFC: u'ITFOR Inc. resarch and development',
   0x000DFD: u'Huges Hi-Tech Inc.,',
   0x000DFE: u'Hauppauge Computer Works, Inc.',
   0x000DFF: u'CHENMING MOLD INDUSTRY CORP.',
   0x000E00: u'Atrie',
   0x000E01: u'ASIP Technologies Inc.',
   0x000E02: u'Advantech AMT Inc.',
   0x000E03: u'Emulex',
   0x000E04: u'CMA/Microdialysis AB',
   0x000E05: u'WIRELESS MATRIX CORP.',
   0x000E06: u'Team Simoco Ltd',
   0x000E07: u'Sony Ericsson Mobile Communications AB',
   0x000E08: u'Sipura Technology, Inc.',
   0x000E09: u'Shenzhen Coship Software Co.,LTD.',
   0x000E0A: u'SAKUMA DESIGN OFFICE',
   0x000E0B: u'Netac Technology Co., Ltd.',
   0x000E0C: u'Intel Corporation',
   0x000E0D: u'HESCH Schrder GmbH',
   0x000E0E: u'ESA elettronica S.P.A.',
   0x000E0F: u'ERMME',
   0x000E10: u'PRIVATE',
   0x000E11: u'BDT Bro- und Datentechnik GmbH & Co. KG',
   0x000E12: u'Adaptive Micro Systems Inc.',
   0x000E13: u'Accu-Sort Systems inc.',
   0x000E14: u'Visionary Solutions, Inc.',
   0x000E15: u'Tadlys LTD',
   0x000E16: u'SouthWing',
   0x000E17: u'PRIVATE',
   0x000E18: u'MyA Technology',
   0x000E19: u'LogicaCMG Pty Ltd',
   0x000E1A: u'JPS Communications',
   0x000E1B: u'IAV GmbH',
   0x000E1C: u'Hach Company',
   0x000E1D: u'ARION Technology Inc.',
   0x000E1E: u'PRIVATE',
   0x000E1F: u'TCL Networks Equipment Co., Ltd.',
   0x000E20: u'PalmSource, Inc.',
   0x000E21: u'MTU Friedrichshafen GmbH',
   0x000E22: u'PRIVATE',
   0x000E23: u'Incipient, Inc.',
   0x000E24: u'Huwell Technology Inc.',
   0x000E25: u'Hannae Technology Co., Ltd',
   0x000E26: u'Gincom Technology Corp.',
   0x000E27: u'Crere Networks, Inc.',
   0x000E28: u'Dynamic Ratings P/L',
   0x000E29: u'Shester Communications Inc',
   0x000E2A: u'PRIVATE',
   0x000E2B: u'Safari Technologies',
   0x000E2C: u'Netcodec co.',
   0x000E2D: u'Hyundai Digital Technology Co.,Ltd.',
   0x000E2E: u'Edimax Technology Co., Ltd.',
   0x000E2F: u'Disetronic Medical Systems AG',
   0x000E30: u'AERAS Networks, Inc.',
   0x000E31: u'Olympus BioSystems GmbH',
   0x000E32: u'Kontron Medical',
   0x000E33: u'Shuko Electronics Co.,Ltd',
   0x000E34: u'NexGen City, LP',
   0x000E35: u'Intel Corp',
   0x000E36: u'HEINESYS, Inc.',
   0x000E37: u'Harms & Wende GmbH & Co.KG',
   0x000E38: u'Cisco Systems',
   0x000E39: u'Cisco Systems',
   0x000E3A: u'Cirrus Logic',
   0x000E3B: u'Hawking Technologies, Inc.',
   0x000E3C: u'TransAct Technoloiges Inc.',
   0x000E3D: u'Televic N.V.',
   0x000E3E: u'Sun Optronics Inc',
   0x000E3F: u'Soronti, Inc.',
   0x000E40: u'Nortel Networks',
   0x000E41: u'NIHON MECHATRONICS CO.,LTD.',
   0x000E42: u'Motic Incoporation Ltd.',
   0x000E43: u'G-Tek Electronics Sdn. Bhd.',
   0x000E44: u'Digital 5, Inc.',
   0x000E45: u'Beijing Newtry Electronic Technology Ltd',
   0x000E46: u'Niigata Seimitsu Co.,Ltd.',
   0x000E47: u'NCI System Co.,Ltd.',
   0x000E48: u'Lipman TransAction Solutions',
   0x000E49: u'Forsway Scandinavia AB',
   0x000E4A: u'Changchun Huayu WEBPAD Co.,LTD',
   0x000E4B: u'atrium c and i',
   0x000E4C: u'Bermai Inc.',
   0x000E4D: u'Numesa Inc.',
   0x000E4E: u'Waveplus Technology Co., Ltd.',
   0x000E4F: u'Trajet GmbH',
   0x000E50: u'Thomson Telecom Belgium',
   0x000E51: u'tecna elettronica srl',
   0x000E52: u'Optium Corporation',
   0x000E53: u'AV TECH CORPORATION',
   0x000E54: u'AlphaCell Wireless Ltd.',
   0x000E55: u'AUVITRAN',
   0x000E56: u'4G Systems GmbH',
   0x000E57: u'Iworld Networking, Inc.',
   0x000E58: u'Sonos, Inc.',
   0x000E59: u'SAGEM SA',
   0x000E5A: u'TELEFIELD inc.',
   0x000E5B: u'ParkerVision - Direct2Data',
   0x000E5C: u'Motorola BCS',
   0x000E5D: u'Triple Play Technologies A/S',
   0x000E5E: u'Beijing Raisecom Science & Technology Development Co.,Ltd',
   0x000E5F: u'activ-net GmbH & Co. KG',
   0x000E60: u'360SUN Digital Broadband Corporation',
   0x000E61: u'MICROTROL LIMITED',
   0x000E62: u'Nortel Networks',
   0x000E63: u'Lemke Diagnostics GmbH',
   0x000E64: u'Elphel, Inc',
   0x000E65: u'TransCore',
   0x000E66: u'Hitachi Advanced Digital, Inc.',
   0x000E67: u'Eltis Microelectronics Ltd.',
   0x000E68: u'E-TOP Network Technology Inc.',
   0x000E69: u'China Electric Power Research Institute',
   0x000E6A: u'3COM EUROPE LTD',
   0x000E6B: u'Janitza electronics GmbH',
   0x000E6C: u'Device Drivers Limited',
   0x000E6D: u'Murata Manufacturing Co., Ltd.',
   0x000E6E: u'MICRELEC  ELECTRONICS S.A',
   0x000E6F: u'IRIS Corporation Berhad',
   0x000E70: u'in2 Networks',
   0x000E71: u'Gemstar Technology Development Ltd.',
   0x000E72: u'CTS electronics',
   0x000E73: u'Tpack A/S',
   0x000E74: u'Solar Telecom. Tech',
   0x000E75: u'New York Air Brake Corp.',
   0x000E76: u'GEMSOC INNOVISION INC.',
   0x000E77: u'Decru, Inc.',
   0x000E78: u'Amtelco',
   0x000E79: u'Ample Communications Inc.',
   0x000E7A: u'GemWon Communications Co., Ltd.',
   0x000E7B: u'Toshiba',
   0x000E7C: u'Televes S.A.',
   0x000E7D: u'Electronics Line 3000 Ltd.',
   0x000E7E: u'Comprog Oy',
   0x000E7F: u'Hewlett Packard',
   0x000E80: u'Thomson Technology Inc',
   0x000E81: u'Devicescape Software, Inc.',
   0x000E82: u'Commtech Wireless',
   0x000E83: u'Cisco Systems',
   0x000E84: u'Cisco Systems',
   0x000E85: u'Catalyst Enterprises, Inc.',
   0x000E86: u'Alcatel North America',
   0x000E87: u'adp Gauselmann GmbH',
   0x000E88: u'VIDEOTRON CORP.',
   0x000E89: u'CLEMATIC',
   0x000E8A: u'Avara Technologies Pty. Ltd.',
   0x000E8B: u'Astarte Technology Co, Ltd.',
   0x000E8C: u'Siemens AG A&D ET',
   0x000E8D: u'Systems in Progress Holding GmbH',
   0x000E8E: u'SparkLAN Communications, Inc.',
   0x000E8F: u'Sercomm Corp.',
   0x000E90: u'PONICO CORP.',
   0x000E91: u'Northstar Technologies',
   0x000E92: u'Millinet Co., Ltd.',
   0x000E93: u'Milnio 3 Sistemas Electrnicos, Lda.',
   0x000E94: u'Maas International BV',
   0x000E95: u'Fujiya Denki Seisakusho Co.,Ltd.',
   0x000E96: u'Cubic Defense Applications, Inc.',
   0x000E97: u'Ultracker Technology CO., Inc',
   0x000E98: u'Vitec CC, INC.',
   0x000E99: u'Spectrum Digital, Inc',
   0x000E9A: u'BOE TECHNOLOGY GROUP CO.,LTD',
   0x000E9B: u'Ambit Microsystems Corporation',
   0x000E9C: u'Pemstar',
   0x000E9D: u'Video Networks Ltd',
   0x000E9E: u'Topfield Co., Ltd',
   0x000E9F: u'TEMIC SDS GmbH',
   0x000EA0: u'NetKlass Technology Inc.',
   0x000EA1: u'Formosa Teletek Corporation',
   0x000EA2: u'CyberGuard Corporation',
   0x000EA3: u'CNCR-IT CO.,LTD,HangZhou P.R.CHINA',
   0x000EA4: u'Certance Inc.',
   0x000EA5: u'BLIP Systems',
   0x000EA6: u'ASUSTEK COMPUTER INC.',
   0x000EA7: u'Endace Inc Ltd.',
   0x000EA8: u'United Technologists Europe Limited',
   0x000EA9: u'Shanghai Xun Shi Communications Equipment Ltd. Co.',
   0x000EAA: u'Scalent Systems, Inc.',
   0x000EAB: u'OctigaBay Systems Corporation',
   0x000EAC: u'MINTRON ENTERPRISE CO., LTD.',
   0x000EAD: u'Metanoia Technologies, Inc.',
   0x000EAE: u'GAWELL TECHNOLOGIES CORP.',
   0x000EAF: u'CASTEL',
   0x000EB0: u'Solutions Radio BV',
   0x000EB1: u'Newcotech,Ltd',
   0x000EB2: u'Micro-Research Finland Oy',
   0x000EB3: u'LeftHand Networks',
   0x000EB4: u'GUANGZHOU GAOKE COMMUNICATIONS TECHNOLOGY CO.LTD.',
   0x000EB5: u'Ecastle Electronics Co., Ltd.',
   0x000EB6: u'Riverbed Technology, Inc.',
   0x000EB7: u'Knovative, Inc.',
   0x000EB8: u'Iiga co.,Ltd',
   0x000EB9: u'HASHIMOTO Electronics Industry Co.,Ltd.',
   0x000EBA: u'HANMI SEMICONDUCTOR CO., LTD.',
   0x000EBB: u'Everbee Networks',
   0x000EBC: u'Cullmann GmbH',
   0x000EBD: u'Burdick, a Quinton Compny',
   0x000EBE: u'B&B Electronics Manufacturing Co.',
   0x000EBF: u'Remsdaq Limited',
   0x000EC0: u'Nortel Networks',
   0x000EC1: u'MYNAH Technologies',
   0x000EC2: u'Lowrance Electronics, Inc.',
   0x000EC3: u'Logic Controls, Inc.',
   0x000EC4: u'Iskra Transmission d.d.',
   0x000EC5: u'Digital Multitools Inc',
   0x000EC6: u'ASIX ELECTRONICS CORP.',
   0x000EC7: u'Motorola Korea',
   0x000EC8: u'Zoran Corporation',
   0x000EC9: u'YOKO Technology Corp.',
   0x000ECA: u'WTSS Inc',
   0x000ECB: u'VineSys Technology',
   0x000ECC: u'Tableau',
   0x000ECD: u'SKOV A/S',
   0x000ECE: u'S.I.T.T.I. S.p.A.',
   0x000ECF: u'PROFIBUS Nutzerorganisation e.V.',
   0x000ED0: u'Privaris, Inc.',
   0x000ED1: u'Osaka Micro Computer.',
   0x000ED2: u'Filtronic plc',
   0x000ED3: u'Epicenter, Inc.',
   0x000ED4: u'CRESITT INDUSTRIE',
   0x000ED5: u'COPAN Systems Inc.',
   0x000ED6: u'Cisco Systems',
   0x000ED7: u'Cisco Systems',
   0x000ED8: u'Aktino, Inc.',
   0x000ED9: u'Aksys, Ltd.',
   0x000EDA: u'C-TECH UNITED CORP.',
   0x000EDB: u'XiNCOM Corp.',
   0x000EDC: u'Tellion INC.',
   0x000EDD: u'SHURE INCORPORATED',
   0x000EDE: u'REMEC, Inc.',
   0x000EDF: u'PLX Technology',
   0x000EE0: u'Mcharge',
   0x000EE1: u'ExtremeSpeed Inc.',
   0x000EE2: u'Custom Engineering S.p.A.',
   0x000EE3: u'Chiyu Technology Co.,Ltd',
   0x000EE4: u'BOE TECHNOLOGY GROUP CO.,LTD',
   0x000EE5: u'bitWallet, Inc.',
   0x000EE6: u'Adimos Systems LTD',
   0x000EE7: u'AAC ELECTRONICS CORP.',
   0x000EE8: u'zioncom',
   0x000EE9: u'WayTech Development, Inc.',
   0x000EEA: u'Shadong Luneng Jicheng Electronics,Co.,Ltd',
   0x000EEB: u'Sandmartin(zhong shan)Electronics Co.,Ltd',
   0x000EEC: u'Orban',
   0x000EED: u'Nokia Danmark A/S',
   0x000EEE: u'Muco Industrie BV',
   0x000EEF: u'PRIVATE',
   0x000EF0: u'Festo AG & Co. KG',
   0x000EF1: u'EZQUEST INC.',
   0x000EF2: u'Infinico Corporation',
   0x000EF3: u'Smarthome',
   0x000EF4: u'Shenzhen Kasda Digital Technology Co.,Ltd',
   0x000EF5: u'iPAC Technology Co., Ltd.',
   0x000EF6: u'E-TEN Information Systems Co., Ltd.',
   0x000EF7: u'Vulcan Portals Inc',
   0x000EF8: u'SBC ASI',
   0x000EF9: u'REA Elektronik GmbH',
   0x000EFA: u'Optoway Technology Incorporation',
   0x000EFB: u'Macey Enterprises',
   0x000EFC: u'JTAG Technologies B.V.',
   0x000EFD: u'FUJI PHOTO OPTICAL CO., LTD.',
   0x000EFE: u'EndRun Technologies LLC',
   0x000EFF: u'Megasolution,Inc.',
   0x000F00: u'Legra Systems, Inc.',
   0x000F01: u'DIGITALKS INC',
   0x000F02: u'Digicube Technology Co., Ltd',
   0x000F03: u'COM&C CO., LTD',
   0x000F04: u'cim-usa inc',
   0x000F05: u'3B SYSTEM INC.',
   0x000F06: u'Nortel Networks',
   0x000F07: u'Mangrove Systems, Inc.',
   0x000F08: u'Indagon Oy',
   0x000F09: u'PRIVATE',
   0x000F0A: u'Clear Edge Networks',
   0x000F0B: u'Kentima Technologies AB',
   0x000F0C: u'SYNCHRONIC ENGINEERING',
   0x000F0D: u'Hunt Electronic Co., Ltd.',
   0x000F0E: u'WaveSplitter Technologies, Inc.',
   0x000F0F: u'Real ID Technology Co., Ltd.',
   0x000F10: u'RDM Corporation',
   0x000F11: u'Prodrive B.V.',
   0x000F12: u'Panasonic AVC Networks Germany GmbH',
   0x000F13: u'Nisca corporation',
   0x000F14: u'Mindray Co., Ltd.',
   0x000F15: u'Kjaerulff1 A/S',
   0x000F16: u'JAY HOW TECHNOLOGY CO.,',
   0x000F17: u'Insta Elektro GmbH',
   0x000F18: u'Industrial Control Systems',
   0x000F19: u'Guidant Corporation',
   0x000F1A: u'Gaming Support B.V.',
   0x000F1B: u'Ego Systems Inc.',
   0x000F1C: u'DigitAll World Co., Ltd',
   0x000F1D: u'Cosmo Techs Co., Ltd.',
   0x000F1E: u'Chengdu KT Electric Co.of High & New Technology',
   0x000F1F: u'WW PCBA Test',
   0x000F20: u'Hewlett Packard',
   0x000F21: u'Scientific Atlanta, Inc',
   0x000F22: u'Helius, Inc.',
   0x000F23: u'Cisco Systems',
   0x000F24: u'Cisco Systems',
   0x000F25: u'AimValley B.V.',
   0x000F26: u'WorldAccxx  LLC',
   0x000F27: u'TEAL Electronics, Inc.',
   0x000F28: u'Itronix Corporation',
   0x000F29: u'Augmentix Corporation',
   0x000F2A: u'Cableware Electronics',
   0x000F2B: u'GREENBELL SYSTEMS',
   0x000F2C: u'Uplogix, Inc.',
   0x000F2D: u'CHUNG-HSIN ELECTRIC & MACHINERY MFG.CORP.',
   0x000F2E: u'Megapower International Corp.',
   0x000F2F: u'W-LINX TECHNOLOGY CO., LTD.',
   0x000F30: u'Raza Microelectronics Inc',
   0x000F31: u'Prosilica',
   0x000F32: u'LuTong Electronic Technology Co.,Ltd',
   0x000F33: u'DUALi Inc.',
   0x000F34: u'Cisco Systems',
   0x000F35: u'Cisco Systems',
   0x000F36: u'Accurate Techhnologies, Inc.',
   0x000F37: u'Xambala Incorporated',
   0x000F38: u'Netstar',
   0x000F39: u'IRIS SENSORS',
   0x000F3A: u'HISHARP',
   0x000F3B: u'Fuji System Machines Co., Ltd.',
   0x000F3C: u'Endeleo Limited',
   0x000F3D: u'D-Link Corporation',
   0x000F3E: u'CardioNet, Inc',
   0x000F3F: u'Big Bear Networks',
   0x000F40: u'Optical Internetworking Forum',
   0x000F41: u'Zipher Ltd',
   0x000F42: u'Xalyo Systems',
   0x000F43: u'Wasabi Systems Inc.',
   0x000F44: u'Tivella Inc.',
   0x000F45: u'Stretch, Inc.',
   0x000F46: u'SINAR AG',
   0x000F47: u'ROBOX SPA',
   0x000F48: u'Polypix Inc.',
   0x000F49: u'Northover Solutions Limited',
   0x000F4A: u'Kyushu-kyohan co.,ltd',
   0x000F4B: u'Katana Technology',
   0x000F4C: u'Elextech INC',
   0x000F4D: u'Centrepoint Technologies Inc.',
   0x000F4E: u'Cellink',
   0x000F4F: u'Cadmus Technology Ltd',
   0x000F50: u'Baxall Limited',
   0x000F51: u'Azul Systems, Inc.',
   0x000F52: u'YORK Refrigeration, Marine & Controls',
   0x000F53: u'Solarflare Communications Inc',
   0x000F54: u'Entrelogic Corporation',
   0x000F55: u'Datawire Communication Networks Inc.',
   0x000F56: u'Continuum Photonics Inc',
   0x000F57: u'CABLELOGIC Co., Ltd.',
   0x000F58: u'Adder Technology Limited',
   0x000F59: u'Phonak Communications AG',
   0x000F5A: u'Peribit Networks',
   0x000F5B: u'Delta Information Systems, Inc.',
   0x000F5C: u'Day One Digital Media Limited',
   0x000F5D: u'42Networks AB',
   0x000F5E: u'Veo',
   0x000F5F: u'Nicety Technologies Inc. (NTS)',
   0x000F60: u'Lifetron Co.,Ltd',
   0x000F61: u'Kiwi Networks',
   0x000F62: u'Alcatel Bell Space N.V.',
   0x000F63: u'Obzerv Technologies',
   0x000F64: u'D&R Electronica Weesp BV',
   0x000F65: u'icube Corp.',
   0x000F66: u'Cisco-Linksys',
   0x000F67: u'West Instruments',
   0x000F68: u'Vavic Network Technology, Inc.',
   0x000F69: u'SEW Eurodrive GmbH & Co. KG',
   0x000F6A: u'Nortel Networks',
   0x000F6B: u'GateWare Communications GmbH',
   0x000F6C: u'ADDI-DATA GmbH',
   0x000F6D: u'Midas Engineering',
   0x000F6E: u'BBox',
   0x000F6F: u'FTA Communication Technologies',
   0x000F70: u'Wintec Industries, inc.',
   0x000F71: u'Sanmei Electronics Co.,Ltd',
   0x000F72: u'Sandburst',
   0x000F73: u'Rockwell Samsung Automation',
   0x000F74: u'Qamcom Technology AB',
   0x000F75: u'First Silicon Solutions',
   0x000F76: u'Digital Keystone, Inc.',
   0x000F77: u'DENTUM CO.,LTD',
   0x000F78: u'Datacap Systems Inc',
   0x000F79: u'Bluetooth Interest Group Inc.',
   0x000F7A: u'BeiJing NuQX Technology CO.,LTD',
   0x000F7B: u'Arce Sistemas, S.A.',
   0x000F7C: u'ACTi Corporation',
   0x000F7D: u'Xirrus',
   0x000F7E: u'Ablerex Electronics Co., LTD',
   0x000F7F: u'UBSTORAGE Co.,Ltd.',
   0x000F80: u'Trinity Security Systems,Inc.',
   0x000F81: u'Secure Info Imaging',
   0x000F82: u'Mortara Instrument, Inc.',
   0x000F83: u'Brainium Technologies Inc.',
   0x000F84: u'Astute Networks, Inc.',
   0x000F85: u'ADDO-Japan Corporation',
   0x000F86: u'Research In Motion Limited',
   0x000F87: u'Maxcess International',
   0x000F88: u'AMETEK, Inc.',
   0x000F89: u'Winnertec System Co., Ltd.',
   0x000F8A: u'WideView',
   0x000F8B: u'Orion MultiSystems Inc',
   0x000F8C: u'Gigawavetech Pte Ltd',
   0x000F8D: u'FAST TV-Server AG',
   0x000F8E: u'DONGYANG TELECOM CO.,LTD.',
   0x000F8F: u'Cisco Systems',
   0x000F90: u'Cisco Systems',
   0x000F91: u'Aerotelecom Co.,Ltd.',
   0x000F92: u'Microhard Systems Inc.',
   0x000F93: u'Landis+Gyr Ltd.',
   0x000F94: u'Genexis',
   0x000F95: u'ELECOM Co.,LTD Laneed Division',
   0x000F96: u'Critical Telecom Corp.',
   0x000F97: u'Avanex Corporation',
   0x000F98: u'Avamax Co. Ltd.',
   0x000F99: u'APAC opto Electronics Inc.',
   0x000F9A: u'Synchrony, Inc.',
   0x000F9B: u'Ross Video Limited',
   0x000F9C: u'Panduit Corp',
   0x000F9D: u'Newnham Research Ltd',
   0x000F9E: u'Murrelektronik GmbH',
   0x000F9F: u'Motorola BCS',
   0x000FA0: u'CANON KOREA BUSINESS SOLUTIONS INC.',
   0x000FA1: u'Gigabit Systems Inc.',
   0x000FA2: u'Digital Path Networks',
   0x000FA3: u'Alpha Networks Inc.',
   0x000FA4: u'Sprecher Automation GmbH',
   0x000FA5: u'SMP / BWA Technology GmbH',
   0x000FA6: u'S2 Security Corporation',
   0x000FA7: u'Raptor Networks Technology',
   0x000FA8: u'Photometrics, Inc.',
   0x000FA9: u'PC Fabrik',
   0x000FAA: u'Nexus Technologies',
   0x000FAB: u'Kyushu Electronics Systems Inc.',
   0x000FAC: u'IEEE 802.11',
   0x000FAD: u'FMN communications GmbH',
   0x000FAE: u'E2O Communications',
   0x000FAF: u'Dialog Inc.',
   0x000FB0: u'Compal Electronics,INC.',
   0x000FB1: u'Cognio Inc.',
   0x000FB2: u'Broadband Pacenet (India) Pvt. Ltd.',
   0x000FB3: u'Actiontec Electronics, Inc',
   0x000FB4: u'Timespace Technology',
   0x000FB5: u'NETGEAR Inc',
   0x000FB6: u'Europlex Technologies',
   0x000FB7: u'Cavium Networks',
   0x000FB8: u'CallURL Inc.',
   0x000FB9: u'Adaptive Instruments',
   0x000FBA: u'Tevebox AB',
   0x000FBB: u'Siemens Networks GmbH & Co. KG',
   0x000FBC: u'Onkey Technologies, Inc.',
   0x000FBD: u'MRV Communications (Networks) LTD',
   0x000FBE: u'e-w/you Inc.',
   0x000FBF: u'DGT Sp. z o.o.',
   0x000FC0: u'DELCOMp',
   0x000FC1: u'WAVE Corporation',
   0x000FC2: u'Uniwell Corporation',
   0x000FC3: u'PalmPalm Technology, Inc.',
   0x000FC4: u'NST co.,LTD.',
   0x000FC5: u'KeyMed Ltd',
   0x000FC6: u'Eurocom Industries A/S',
   0x000FC7: u'Dionica R&D Ltd.',
   0x000FC8: u'Chantry Networks',
   0x000FC9: u'Allnet GmbH',
   0x000FCA: u'A-JIN TECHLINE CO, LTD',
   0x000FCB: u'3COM EUROPE LTD',
   0x000FCC: u'Netopia, Inc.',
   0x000FCD: u'Nortel Networks',
   0x000FCE: u'Kikusui Electronics Corp.',
   0x000FCF: u'Datawind Research',
   0x000FD0: u'ASTRI',
   0x000FD1: u'Applied Wireless Identifications Group, Inc.',
   0x000FD2: u'EWA Technologies, Inc.',
   0x000FD3: u'Digium',
   0x000FD4: u'Soundcraft',
   0x000FD5: u'Schwechat - RISE',
   0x000FD6: u'Sarotech Co., Ltd',
   0x000FD7: u'Harman Music Group',
   0x000FD8: u'Force, Inc.',
   0x000FD9: u'FlexDSL Telecommunications AG',
   0x000FDA: u'YAZAKI CORPORATION',
   0x000FDB: u'Westell Technologies',
   0x000FDC: u'Ueda Japan  Radio Co., Ltd.',
   0x000FDD: u'SORDIN AB',
   0x000FDE: u'Sony Ericsson Mobile Communications AB',
   0x000FDF: u'SOLOMON Technology Corp.',
   0x000FE0: u'NComputing Co.,Ltd.',
   0x000FE1: u'ID DIGITAL CORPORATION',
   0x000FE2: u'Hangzhou Huawei-3Com Tech. Co., Ltd.',
   0x000FE3: u'Damm Cellular Systems A/S',
   0x000FE4: u'Pantech Co.,Ltd',
   0x000FE5: u'MERCURY SECURITY CORPORATION',
   0x000FE6: u'MBTech Systems, Inc.',
   0x000FE7: u'Lutron Electronics Co., Inc.',
   0x000FE8: u'Lobos, Inc.',
   0x000FE9: u'GW TECHNOLOGIES CO.,LTD.',
   0x000FEA: u'Giga-Byte Technology Co.,LTD.',
   0x000FEB: u'Cylon Controls',
   0x000FEC: u'Arkus Inc.',
   0x000FED: u'Anam Electronics Co., Ltd',
   0x000FEE: u'XTec, Incorporated',
   0x000FEF: u'Thales e-Transactions GmbH',
   0x000FF0: u'Sunray Enterprise',
   0x000FF1: u'nex-G Systems Pte.Ltd',
   0x000FF2: u'Loud Technologies Inc.',
   0x000FF3: u'Jung Myoung Communications&Technology',
   0x000FF4: u'Guntermann & Drunck GmbH',
   0x000FF5: u'GN&S company',
   0x000FF6: u'Darfon Electronics Corp.',
   0x000FF7: u'Cisco Systems',
   0x000FF8: u'Cisco  Systems',
   0x000FF9: u'Valcretec, Inc.',
   0x000FFA: u'Optinel Systems, Inc.',
   0x000FFB: u'Nippon Denso Industry Co., Ltd.',
   0x000FFC: u'Merit Li-Lin Ent.',
   0x000FFD: u'Glorytek Network Inc.',
   0x000FFE: u'G-PRO COMPUTER',
   0x000FFF: u'Control4',
   0x001000: u'CABLE TELEVISION LABORATORIES, INC.',
   0x001001: u'MCK COMMUNICATIONS',
   0x001002: u'ACTIA',
   0x001003: u'IMATRON, INC.',
   0x001004: u'THE BRANTLEY COILE COMPANY,INC',
   0x001005: u'UEC COMMERCIAL',
   0x001006: u'Thales Contact Solutions Ltd.',
   0x001007: u'CISCO SYSTEMS, INC.',
   0x001008: u'VIENNA SYSTEMS CORPORATION',
   0x001009: u'HORO QUARTZ',
   0x00100A: u'WILLIAMS COMMUNICATIONS GROUP',
   0x00100B: u'CISCO SYSTEMS, INC.',
   0x00100C: u'ITO CO., LTD.',
   0x00100D: u'CISCO SYSTEMS, INC.',
   0x00100E: u'MICRO LINEAR COPORATION',
   0x00100F: u'INDUSTRIAL CPU SYSTEMS',
   0x001010: u'INITIO CORPORATION',
   0x001011: u'CISCO SYSTEMS, INC.',
   0x001012: u'PROCESSOR SYSTEMS (I) PVT LTD',
   0x001013: u'Kontron',
   0x001014: u'CISCO SYSTEMS, INC.',
   0x001015: u'OOmon Inc.',
   0x001016: u'T.SQWARE',
   0x001017: u'MICOS GmbH',
   0x001018: u'BROADCOM CORPORATION',
   0x001019: u'SIRONA DENTAL SYSTEMS GmbH & Co. KG',
   0x00101A: u'PictureTel Corp.',
   0x00101B: u'CORNET TECHNOLOGY, INC.',
   0x00101C: u'OHM TECHNOLOGIES INTL, LLC',
   0x00101D: u'WINBOND ELECTRONICS CORP.',
   0x00101E: u'MATSUSHITA ELECTRONIC INSTRUMENTS CORP.',
   0x00101F: u'CISCO SYSTEMS, INC.',
   0x001020: u'WELCH ALLYN, DATA COLLECTION',
   0x001021: u'ENCANTO NETWORKS, INC.',
   0x001022: u'SatCom Media Corporation',
   0x001023: u'FLOWWISE NETWORKS, INC.',
   0x001024: u'NAGOYA ELECTRIC WORKS CO., LTD',
   0x001025: u'GRAYHILL INC.',
   0x001026: u'ACCELERATED NETWORKS, INC.',
   0x001027: u'L-3 COMMUNICATIONS EAST',
   0x001028: u'COMPUTER TECHNICA, INC.',
   0x001029: u'CISCO SYSTEMS, INC.',
   0x00102A: u'ZF MICROSYSTEMS, INC.',
   0x00102B: u'UMAX DATA SYSTEMS, INC.',
   0x00102C: u'Lasat Networks A/S',
   0x00102D: u'HITACHI SOFTWARE ENGINEERING',
   0x00102E: u'NETWORK SYSTEMS & TECHNOLOGIES PVT. LTD.',
   0x00102F: u'CISCO SYSTEMS, INC.',
   0x001030: u'EION Inc.',
   0x001031: u'OBJECTIVE COMMUNICATIONS, INC.',
   0x001032: u'ALTA TECHNOLOGY',
   0x001033: u'ACCESSLAN COMMUNICATIONS, INC.',
   0x001034: u'GNP Computers',
   0x001035: u'ELITEGROUP COMPUTER SYSTEMS CO., LTD',
   0x001036: u'INTER-TEL INTEGRATED SYSTEMS',
   0x001037: u'CYQ\'ve Technology Co., Ltd.',
   0x001038: u'MICRO RESEARCH INSTITUTE, INC.',
   0x001039: u'Vectron Systems AG',
   0x00103A: u'DIAMOND NETWORK TECH',
   0x00103B: u'HIPPI NETWORKING FORUM',
   0x00103C: u'IC ENSEMBLE, INC.',
   0x00103D: u'PHASECOM, LTD.',
   0x00103E: u'NETSCHOOLS CORPORATION',
   0x00103F: u'TOLLGRADE COMMUNICATIONS, INC.',
   0x001040: u'INTERMEC CORPORATION',
   0x001041: u'BRISTOL BABCOCK, INC.',
   0x001042: u'AlacriTech',
   0x001043: u'A2 CORPORATION',
   0x001044: u'InnoLabs Corporation',
   0x001045: u'Nortel Networks',
   0x001046: u'ALCORN MCBRIDE INC.',
   0x001047: u'ECHO ELETRIC CO. LTD.',
   0x001048: u'HTRC AUTOMATION, INC.',
   0x001049: u'SHORELINE TELEWORKS, INC.',
   0x00104A: u'THE PARVUC CORPORATION',
   0x00104B: u'3COM CORPORATION',
   0x00104C: u'COMPUTER ACCESS TECHNOLOGY',
   0x00104D: u'SURTEC INDUSTRIES, INC.',
   0x00104E: u'CEOLOGIC',
   0x00104F: u'STORAGE TECHNOLOGY CORPORATION',
   0x001050: u'RION CO., LTD.',
   0x001051: u'CMICRO CORPORATION',
   0x001052: u'METTLER-TOLEDO (ALBSTADT) GMBH',
   0x001053: u'COMPUTER TECHNOLOGY CORP.',
   0x001054: u'CISCO SYSTEMS, INC.',
   0x001055: u'FUJITSU MICROELECTRONICS, INC.',
   0x001056: u'SODICK CO., LTD.',
   0x001057: u'Rebel.com, Inc.',
   0x001058: u'ArrowPoint Communications',
   0x001059: u'DIABLO RESEARCH CO. LLC',
   0x00105A: u'3COM CORPORATION',
   0x00105B: u'NET INSIGHT AB',
   0x00105C: u'QUANTUM DESIGNS (H.K.) LTD.',
   0x00105D: u'Draeger Medical',
   0x00105E: u'HEKIMIAN LABORATORIES, INC.',
   0x00105F: u'IN-SNEC',
   0x001060: u'BILLIONTON SYSTEMS, INC.',
   0x001061: u'HOSTLINK CORP.',
   0x001062: u'NX SERVER, ILNC.',
   0x001063: u'STARGUIDE DIGITAL NETWORKS',
   0x001064: u'DNPG, LLC',
   0x001065: u'RADYNE CORPORATION',
   0x001066: u'ADVANCED CONTROL SYSTEMS, INC.',
   0x001067: u'REDBACK NETWORKS, INC.',
   0x001068: u'COMOS TELECOM',
   0x001069: u'HELIOSS COMMUNICATIONS, INC.',
   0x00106A: u'DIGITAL MICROWAVE CORPORATION',
   0x00106B: u'SONUS NETWORKS, INC.',
   0x00106C: u'INFRATEC PLUS GmbH',
   0x00106D: u'Axxcelera Broadband Wireless',
   0x00106E: u'TADIRAN COM. LTD.',
   0x00106F: u'TRENTON TECHNOLOGY INC.',
   0x001070: u'CARADON TREND LTD.',
   0x001071: u'ADVANET INC.',
   0x001072: u'GVN TECHNOLOGIES, INC.',
   0x001073: u'TECHNOBOX, INC.',
   0x001074: u'ATEN INTERNATIONAL CO., LTD.',
   0x001075: u'Maxtor Corporation',
   0x001076: u'EUREM GmbH',
   0x001077: u'SAF DRIVE SYSTEMS, LTD.',
   0x001078: u'NUERA COMMUNICATIONS, INC.',
   0x001079: u'CISCO SYSTEMS, INC.',
   0x00107A: u'AmbiCom, Inc.',
   0x00107B: u'CISCO SYSTEMS, INC.',
   0x00107C: u'P-COM, INC.',
   0x00107D: u'AURORA COMMUNICATIONS, LTD.',
   0x00107E: u'BACHMANN ELECTRONIC GmbH',
   0x00107F: u'CRESTRON ELECTRONICS, INC.',
   0x001080: u'METAWAVE COMMUNICATIONS',
   0x001081: u'DPS, INC.',
   0x001082: u'JNA TELECOMMUNICATIONS LIMITED',
   0x001083: u'HEWLETT-PACKARD COMPANY',
   0x001084: u'K-BOT COMMUNICATIONS',
   0x001085: u'POLARIS COMMUNICATIONS, INC.',
   0x001086: u'ATTO TECHNOLOGY, INC.',
   0x001087: u'Xstreamis PLC',
   0x001088: u'AMERICAN NETWORKS INC.',
   0x001089: u'WebSonic',
   0x00108A: u'TeraLogic, Inc.',
   0x00108B: u'LASERANIMATION SOLLINGER GmbH',
   0x00108C: u'FUJITSU TELECOMMUNICATIONS EUROPE, LTD.',
   0x00108D: u'JOHNSON CONTROLS, INC.',
   0x00108E: u'HUGH SYMONS CONCEPT Technologies Ltd.',
   0x00108F: u'RAPTOR SYSTEMS',
   0x001090: u'CIMETRICS, INC.',
   0x001091: u'NO WIRES NEEDED BV',
   0x001092: u'NETCORE INC.',
   0x001093: u'CMS COMPUTERS, LTD.',
   0x001094: u'Performance Analysis Broadband, Spirent plc',
   0x001095: u'Thomson Inc.',
   0x001096: u'TRACEWELL SYSTEMS, INC.',
   0x001097: u'WinNet Metropolitan Communications Systems, Inc.',
   0x001098: u'STARNET TECHNOLOGIES, INC.',
   0x001099: u'InnoMedia, Inc.',
   0x00109A: u'NETLINE',
   0x00109B: u'Emulex Corporation',
   0x00109C: u'M-SYSTEM CO., LTD.',
   0x00109D: u'CLARINET SYSTEMS, INC.',
   0x00109E: u'AWARE, INC.',
   0x00109F: u'PAVO, INC.',
   0x0010A0: u'INNOVEX TECHNOLOGIES, INC.',
   0x0010A1: u'KENDIN SEMICONDUCTOR, INC.',
   0x0010A2: u'TNS',
   0x0010A3: u'OMNITRONIX, INC.',
   0x0010A4: u'XIRCOM',
   0x0010A5: u'OXFORD INSTRUMENTS',
   0x0010A6: u'CISCO SYSTEMS, INC.',
   0x0010A7: u'UNEX TECHNOLOGY CORPORATION',
   0x0010A8: u'RELIANCE COMPUTER CORP.',
   0x0010A9: u'ADHOC TECHNOLOGIES',
   0x0010AA: u'MEDIA4, INC.',
   0x0010AB: u'KOITO INDUSTRIES, LTD.',
   0x0010AC: u'IMCI TECHNOLOGIES',
   0x0010AD: u'SOFTRONICS USB, INC.',
   0x0010AE: u'SHINKO ELECTRIC INDUSTRIES CO.',
   0x0010AF: u'TAC SYSTEMS, INC.',
   0x0010B0: u'MERIDIAN TECHNOLOGY CORP.',
   0x0010B1: u'FOR-A CO., LTD.',
   0x0010B2: u'COACTIVE AESTHETICS',
   0x0010B3: u'NOKIA MULTIMEDIA TERMINALS',
   0x0010B4: u'ATMOSPHERE NETWORKS',
   0x0010B5: u'ACCTON TECHNOLOGY CORPORATION',
   0x0010B6: u'ENTRATA COMMUNICATIONS CORP.',
   0x0010B7: u'COYOTE TECHNOLOGIES, LLC',
   0x0010B8: u'ISHIGAKI COMPUTER SYSTEM CO.',
   0x0010B9: u'MAXTOR CORP.',
   0x0010BA: u'MARTINHO-DAVIS SYSTEMS, INC.',
   0x0010BB: u'DATA & INFORMATION TECHNOLOGY',
   0x0010BC: u'Aastra Telecom',
   0x0010BD: u'THE TELECOMMUNICATION TECHNOLOGY COMMITTEE',
   0x0010BE: u'TELEXIS CORP.',
   0x0010BF: u'InterAir Wireless',
   0x0010C0: u'ARMA, INC.',
   0x0010C1: u'OI ELECTRIC CO., LTD.',
   0x0010C2: u'WILLNET, INC.',
   0x0010C3: u'CSI-CONTROL SYSTEMS',
   0x0010C4: u'MEDIA LINKS CO., LTD.',
   0x0010C5: u'PROTOCOL TECHNOLOGIES, INC.',
   0x0010C6: u'USI',
   0x0010C7: u'DATA TRANSMISSION NETWORK',
   0x0010C8: u'COMMUNICATIONS ELECTRONICS SECURITY GROUP',
   0x0010C9: u'MITSUBISHI ELECTRONICS LOGISTIC SUPPORT CO.',
   0x0010CA: u'INTEGRAL ACCESS',
   0x0010CB: u'FACIT K.K.',
   0x0010CC: u'CLP COMPUTER LOGISTIK PLANUNG GmbH',
   0x0010CD: u'INTERFACE CONCEPT',
   0x0010CE: u'VOLAMP, LTD.',
   0x0010CF: u'FIBERLANE COMMUNICATIONS',
   0x0010D0: u'WITCOM, LTD.',
   0x0010D1: u'Top Layer Networks, Inc.',
   0x0010D2: u'NITTO TSUSHINKI CO., LTD',
   0x0010D3: u'GRIPS ELECTRONIC GMBH',
   0x0010D4: u'STORAGE COMPUTER CORPORATION',
   0x0010D5: u'IMASDE CANARIAS, S.A.',
   0x0010D6: u'ITT - A/CD',
   0x0010D7: u'ARGOSY RESEARCH INC.',
   0x0010D8: u'CALISTA',
   0x0010D9: u'IBM JAPAN, FUJISAWA MT+D',
   0x0010DA: u'MOTION ENGINEERING, INC.',
   0x0010DB: u'Juniper Networks, Inc.',
   0x0010DC: u'MICRO-STAR INTERNATIONAL CO., LTD.',
   0x0010DD: u'ENABLE SEMICONDUCTOR, INC.',
   0x0010DE: u'INTERNATIONAL DATACASTING CORPORATION',
   0x0010DF: u'RISE COMPUTER INC.',
   0x0010E0: u'COBALT MICROSERVER, INC.',
   0x0010E1: u'S.I. TECH, INC.',
   0x0010E2: u'ArrayComm, Inc.',
   0x0010E3: u'COMPAQ COMPUTER CORPORATION',
   0x0010E4: u'NSI CORPORATION',
   0x0010E5: u'SOLECTRON TEXAS',
   0x0010E6: u'APPLIED INTELLIGENT SYSTEMS, INC.',
   0x0010E7: u'BreezeCom',
   0x0010E8: u'TELOCITY, INCORPORATED',
   0x0010E9: u'RAIDTEC LTD.',
   0x0010EA: u'ADEPT TECHNOLOGY',
   0x0010EB: u'SELSIUS SYSTEMS, INC.',
   0x0010EC: u'RPCG, LLC',
   0x0010ED: u'SUNDANCE TECHNOLOGY, INC.',
   0x0010EE: u'CTI PRODUCTS, INC.',
   0x0010EF: u'DBTEL INCORPORATED',
   0x0010F1: u'I-O CORPORATION',
   0x0010F2: u'ANTEC',
   0x0010F3: u'Nexcom International Co., Ltd.',
   0x0010F4: u'VERTICAL NETWORKS, INC.',
   0x0010F5: u'AMHERST SYSTEMS, INC.',
   0x0010F6: u'CISCO SYSTEMS, INC.',
   0x0010F7: u'IRIICHI TECHNOLOGIES Inc.',
   0x0010F8: u'TEXIO CORPORATION',
   0x0010F9: u'UNIQUE SYSTEMS, INC.',
   0x0010FA: u'ZAYANTE, INC.',
   0x0010FB: u'ZIDA TECHNOLOGIES LIMITED',
   0x0010FC: u'BROADBAND NETWORKS, INC.',
   0x0010FD: u'COCOM A/S',
   0x0010FE: u'DIGITAL EQUIPMENT CORPORATION',
   0x0010FF: u'CISCO SYSTEMS, INC.',
   0x001100: u'RAM Industries, LLC',
   0x001101: u'CET Technologies Pte Ltd',
   0x001102: u'Aurora Multimedia Corp.',
   0x001103: u'kawamura electric inc.',
   0x001104: u'TELEXY',
   0x001105: u'Sunplus Technology Co., Ltd.',
   0x001106: u'Siemens NV (Belgium)',
   0x001107: u'RGB Networks Inc.',
   0x001108: u'Orbital Data Corporation',
   0x001109: u'Micro-Star International',
   0x00110A: u'Hewlett Packard',
   0x00110B: u'Franklin Technology Systems',
   0x00110C: u'Atmark Techno, Inc.',
   0x00110D: u'SANBlaze Technology, Inc.',
   0x00110E: u'Tsurusaki Sealand Transportation Co. Ltd.',
   0x00110F: u'netplat,Inc.',
   0x001110: u'Maxanna Technology Co., Ltd.',
   0x001111: u'Intel Corporation',
   0x001112: u'Honeywell CMSS',
   0x001113: u'Fraunhofer FOKUS',
   0x001114: u'EverFocus Electronics Corp.',
   0x001115: u'EPIN Technologies, Inc.',
   0x001116: u'COTEAU VERT CO., LTD.',
   0x001117: u'CESNET',
   0x001118: u'BLX IC Design Corp., Ltd.',
   0x001119: u'Solteras, Inc.',
   0x00111A: u'Motorola BCS',
   0x00111B: u'Targa Systems Div L-3 Communications Canada',
   0x00111C: u'Pleora Technologies Inc.',
   0x00111D: u'Hectrix Limited',
   0x00111E: u'EPSG (Ethernet Powerlink Standardization Group)',
   0x00111F: u'Doremi Labs, Inc.',
   0x001120: u'Cisco Systems',
   0x001121: u'Cisco Systems',
   0x001122: u'CIMSYS Inc',
   0x001123: u'Appointech, Inc.',
   0x001124: u'Apple Computer',
   0x001125: u'IBM Corporation',
   0x001126: u'Venstar Inc.',
   0x001127: u'TASI, Inc',
   0x001128: u'Streamit',
   0x001129: u'Paradise Datacom Ltd.',
   0x00112A: u'Niko NV',
   0x00112B: u'NetModule',
   0x00112C: u'IZT GmbH',
   0x00112D: u'Guys Without Ties',
   0x00112E: u'CEICOM',
   0x00112F: u'ASUSTek Computer Inc.',
   0x001130: u'Allied Telesis (Hong Kong) Ltd.',
   0x001131: u'UNATECH. CO.,LTD',
   0x001132: u'Synology Incorporated',
   0x001133: u'Siemens Austria SIMEA',
   0x001134: u'MediaCell, Inc.',
   0x001135: u'Grandeye Ltd',
   0x001136: u'Goodrich Sensor Systems',
   0x001137: u'AICHI ELECTRIC CO., LTD.',
   0x001138: u'TAISHIN CO., LTD.',
   0x001139: u'STOEBER ANTRIEBSTECHNIK GmbH + Co. KG.',
   0x00113A: u'SHINBORAM',
   0x00113B: u'Micronet Communications Inc.',
   0x00113C: u'Micronas GmbH',
   0x00113D: u'KN SOLTEC CO.,LTD.',
   0x00113E: u'JL Corporation',
   0x00113F: u'Alcatel DI',
   0x001140: u'Nanometrics Inc.',
   0x001141: u'GoodMan Corporation',
   0x001142: u'e-SMARTCOM  INC.',
   0x001143: u'DELL INC.',
   0x001144: u'Assurance Technology Corp',
   0x001145: u'ValuePoint Networks',
   0x001146: u'Telecard-Pribor Ltd',
   0x001147: u'Secom-Industry co.LTD.',
   0x001148: u'Prolon Control Systems',
   0x001149: u'Proliphix LLC',
   0x00114A: u'KAYABA INDUSTRY Co,.Ltd.',
   0x00114B: u'Francotyp-Postalia AG & Co. KG',
   0x00114C: u'caffeina applied research ltd.',
   0x00114D: u'Atsumi Electric Co.,LTD.',
   0x00114E: u'690885 Ontario Inc.',
   0x00114F: u'US Digital Television, Inc',
   0x001150: u'Belkin Corporation',
   0x001151: u'Mykotronx',
   0x001152: u'Eidsvoll Electronics AS',
   0x001153: u'Trident Tek, Inc.',
   0x001154: u'Webpro Technologies Inc.',
   0x001155: u'Sevis Systems',
   0x001156: u'Pharos Systems NZ',
   0x001157: u'OF Networks Co., Ltd.',
   0x001158: u'Nortel Networks',
   0x001159: u'MATISSE NETWORKS INC',
   0x00115A: u'Ivoclar Vivadent AG',
   0x00115B: u'Elitegroup Computer System Co. (ECS)',
   0x00115C: u'Cisco',
   0x00115D: u'Cisco',
   0x00115E: u'ProMinent Dosiertechnik GmbH',
   0x00115F: u'Intellix Co., Ltd.',
   0x001160: u'ARTDIO Company Co., LTD',
   0x001161: u'NetStreams, LLC',
   0x001162: u'STAR MICRONICS CO.,LTD.',
   0x001163: u'SYSTEM SPA DEPT. ELECTRONICS',
   0x001164: u'ACARD Technology Corp.',
   0x001165: u'Znyx Networks',
   0x001166: u'Taelim Electronics Co., Ltd.',
   0x001167: u'Integrated System Solution Corp.',
   0x001168: u'HomeLogic LLC',
   0x001169: u'EMS Satcom',
   0x00116A: u'Domo Ltd',
   0x00116B: u'Digital Data Communications Asia Co.,Ltd',
   0x00116C: u'Nanwang Multimedia Inc.,Ltd',
   0x00116D: u'American Time and Signal',
   0x00116E: u'PePLink Ltd.',
   0x00116F: u'Netforyou Co., LTD.',
   0x001170: u'GSC SRL',
   0x001171: u'DEXTER Communications, Inc.',
   0x001172: u'COTRON CORPORATION',
   0x001173: u'Adtron Corporation',
   0x001174: u'Wibhu Technologies, Inc.',
   0x001175: u'PathScale, Inc.',
   0x001176: u'Intellambda Systems, Inc.',
   0x001177: u'COAXIAL NETWORKS, INC.',
   0x001178: u'Chiron Technology Ltd',
   0x001179: u'Singular Technology Co. Ltd.',
   0x00117A: u'Singim International Corp.',
   0x00117B: u'Bchi Labortechnik AG',
   0x00117C: u'e-zy.net',
   0x00117D: u'ZMD America, Inc.',
   0x00117E: u'Progeny Inc.',
   0x00117F: u'Neotune Information Technology Corporation,.LTD',
   0x001180: u'Motorola BCS',
   0x001181: u'InterEnergy Co.Ltd,',
   0x001182: u'IMI Norgren Ltd',
   0x001183: u'PSC Scanning, Inc',
   0x001184: u'Humo Laboratory,Ltd.',
   0x001185: u'Hewlett Packard',
   0x001186: u'Prime Systems, Inc.',
   0x001187: u'Category Solutions, Inc',
   0x001188: u'Enterasys',
   0x001189: u'Aerotech Inc',
   0x00118A: u'Viewtran Technology Limited',
   0x00118B: u'NetDevices Inc.',
   0x00118C: u'Missouri Department of Transportation',
   0x00118D: u'Hanchang System Corp.',
   0x00118E: u'Halytech Mace',
   0x00118F: u'EUTECH INSTRUMENTS PTE. LTD.',
   0x001190: u'Digital Design Corporation',
   0x001191: u'CTS-Clima Temperatur Systeme GmbH',
   0x001192: u'Cisco Systems',
   0x001193: u'Cisco Systems',
   0x001194: u'Chi Mei Communication Systems, Inc.',
   0x001195: u'D-Link Corporation',
   0x001196: u'Actuality Systems, Inc.',
   0x001197: u'Monitoring Technologies Limited',
   0x001198: u'Prism Media Products Limited',
   0x001199: u'2wcom GmbH',
   0x00119A: u'Alkeria srl',
   0x00119B: u'Telesynergy Research Inc.',
   0x00119C: u'EP&T Energy',
   0x00119D: u'Diginfo Technology Corporation',
   0x00119E: u'Solectron Brazil',
   0x00119F: u'Nokia Danmark A/S',
   0x0011A0: u'Vtech Engineering Canada Ltd',
   0x0011A1: u'VISION NETWARE CO.,LTD',
   0x0011A2: u'Manufacturing Technology Inc',
   0x0011A3: u'LanReady Technologies Inc.',
   0x0011A4: u'JStream Technologies Inc.',
   0x0011A5: u'Fortuna Electronic Corp.',
   0x0011A6: u'Sypixx Networks',
   0x0011A7: u'Infilco Degremont Inc.',
   0x0011A8: u'Quest Technologies',
   0x0011A9: u'MOIMSTONE Co., LTD',
   0x0011AA: u'Uniclass Technology, Co., LTD',
   0x0011AB: u'TRUSTABLE TECHNOLOGY CO.,LTD.',
   0x0011AC: u'Simtec Electronics',
   0x0011AD: u'Shanghai Ruijie Technology',
   0x0011AE: u'Motorola BCS',
   0x0011AF: u'Medialink-i,Inc',
   0x0011B0: u'Fortelink Inc.',
   0x0011B1: u'BlueExpert Technology Corp.',
   0x0011B2: u'2001 Technology Inc.',
   0x0011B3: u'YOSHIMIYA CO.,LTD.',
   0x0011B4: u'Westermo Teleindustri AB',
   0x0011B5: u'Shenzhen Powercom Co.,Ltd',
   0x0011B6: u'Open Systems International',
   0x0011B7: u'Melexis Nederland B.V.',
   0x0011B8: u'Liebherr - Elektronik GmbH',
   0x0011B9: u'Inner Range Pty. Ltd.',
   0x0011BA: u'Elexol Pty Ltd',
   0x0011BB: u'Cisco Systems',
   0x0011BC: u'Cisco Systems',
   0x0011BD: u'Bombardier Transportation',
   0x0011BE: u'AGP Telecom Co. Ltd',
   0x0011BF: u'AESYS S.p.A.',
   0x0011C0: u'Aday Technology Inc',
   0x0011C1: u'4P MOBILE DATA PROCESSING',
   0x0011C2: u'United Fiber Optic Communication',
   0x0011C3: u'Transceiving System Technology Corporation',
   0x0011C4: u'Terminales de Telecomunicacion Terrestre, S.L.',
   0x0011C5: u'TEN Technology',
   0x0011C6: u'Seagate Technology LLC',
   0x0011C7: u'RAYMARINE Group Ltd.',
   0x0011C8: u'Powercom Co., Ltd.',
   0x0011C9: u'MTT Corporation',
   0x0011CA: u'Long Range Systems, Inc.',
   0x0011CB: u'Jacobsons RKH AB',
   0x0011CC: u'Guangzhou Jinpeng Group Co.,Ltd.',
   0x0011CD: u'Axsun Technologies',
   0x0011CE: u'Ubisense Limited',
   0x0011CF: u'Thrane & Thrane A/S',
   0x0011D0: u'Tandberg Data ASA',
   0x0011D1: u'Soft Imaging System GmbH',
   0x0011D2: u'Perception Digital Ltd',
   0x0011D3: u'NextGenTel Holding ASA',
   0x0011D4: u'NetEnrich, Inc',
   0x0011D5: u'Hangzhou Sunyard System Engineering Co.,Ltd.',
   0x0011D6: u'HandEra, Inc.',
   0x0011D7: u'eWerks Inc',
   0x0011D8: u'ASUSTek Computer Inc.',
   0x0011D9: u'TiVo',
   0x0011DA: u'Vivaas Technology Inc.',
   0x0011DB: u'Land-Cellular Corporation',
   0x0011DC: u'Glunz & Jensen',
   0x0011DD: u'FROMUS TEC. Co., Ltd.',
   0x0011DE: u'EURILOGIC',
   0x0011DF: u'Arecont Systems',
   0x0011E0: u'U-MEDIA Communications, Inc.',
   0x0011E1: u'BEKO Electronics Co.',
   0x0011E2: u'Hua Jung Components Co., Ltd.',
   0x0011E3: u'Thomson, Inc.',
   0x0011E4: u'Danelec Electronics A/S',
   0x0011E5: u'KCodes Corporation',
   0x0011E6: u'Scientific Atlanta',
   0x0011E7: u'WORLDSAT - Texas de France',
   0x0011E8: u'Tixi.Com',
   0x0011E9: u'STARNEX CO., LTD.',
   0x0011EA: u'IWICS Inc.',
   0x0011EB: u'Innovative Integration',
   0x0011EC: u'AVIX INC.',
   0x0011ED: u'802 Global',
   0x0011EE: u'Estari, Inc.',
   0x0011EF: u'Conitec Datensysteme GmbH',
   0x0011F0: u'Wideful Limited',
   0x0011F1: u'QinetiQ Ltd',
   0x0011F2: u'Institute of Network Technologies',
   0x0011F3: u'Gavitec AG- mobile digit',
   0x0011F4: u'woori-net',
   0x0011F5: u'ASKEY COMPUTER CORP.',
   0x0011F6: u'Asia Pacific Microsystems , Inc.',
   0x0011F7: u'Shenzhen Forward Industry Co., Ltd',
   0x0011F8: u'AIRAYA Corp',
   0x0011F9: u'Nortel Networks',
   0x0011FA: u'Rane Corporation',
   0x0011FB: u'Heidelberg Engineering GmbH',
   0x0011FC: u'HARTING Electric Gmbh & Co.KG',
   0x0011FD: u'KORG INC.',
   0x0011FE: u'Keiyo System Research, Inc.',
   0x0011FF: u'Digitro Tecnologia Ltda',
   0x001200: u'Cisco',
   0x001201: u'Cisco',
   0x001202: u'Audio International Inc.',
   0x001203: u'Activ Networks',
   0x001204: u'u10 Networks, Inc.',
   0x001205: u'Terrasat Communications, Inc.',
   0x001206: u'iQuest (NZ) Ltd',
   0x001207: u'Head Strong International Limited',
   0x001208: u'Gantner Electronic GmbH',
   0x001209: u'Fastrax Ltd',
   0x00120A: u'Emerson Electric GmbH & Co. OHG',
   0x00120B: u'Chinasys Technologies Limited',
   0x00120C: u'CE-Infosys Pte Ltd',
   0x00120D: u'Advanced Telecommunication Technologies, Inc.',
   0x00120E: u'AboCom',
   0x00120F: u'IEEE 802.3',
   0x001210: u'WideRay Corp',
   0x001211: u'Protechna Herbst GmbH & Co. KG',
   0x001212: u'PLUS Vision Corporation',
   0x001213: u'Metrohm AG',
   0x001214: u'Koenig & Bauer AG',
   0x001215: u'iStor Networks, Inc.',
   0x001216: u'ICP Internet Communication Payment AG',
   0x001217: u'Cisco-Linksys, LLC',
   0x001218: u'ARUZE Corporation',
   0x001219: u'Ahead Communication Systems Inc',
   0x00121A: u'Techno Soft Systemnics Inc.',
   0x00121B: u'Sound Devices, LLC',
   0x00121C: u'PARROT S.A.',
   0x00121D: u'Netfabric Corporation',
   0x00121E: u'Juniper Networks, Inc.',
   0x00121F: u'Harding Intruments',
   0x001220: u'Cadco Systems',
   0x001221: u'B.Braun Melsungen AG',
   0x001222: u'Skardin (UK) Ltd',
   0x001223: u'Pixim',
   0x001224: u'NexQL Corporation',
   0x001225: u'Motorola BCS',
   0x001226: u'Japan Direx Corporation',
   0x001227: u'Franklin Electric Co., Inc.',
   0x001228: u'Data Ltd.',
   0x001229: u'BroadEasy Technologies Co.,Ltd',
   0x00122A: u'VTech Telecommunications Ltd.',
   0x00122B: u'Virbiage Pty Ltd',
   0x00122C: u'Soenen Controls N.V.',
   0x00122D: u'SiNett Corporation',
   0x00122E: u'Signal Technology - AISD',
   0x00122F: u'Sanei Electric Inc.',
   0x001230: u'Picaso Infocommunication CO., LTD.',
   0x001231: u'Motion Control Systems, Inc.',
   0x001232: u'LeWiz Communications Inc.',
   0x001233: u'JRC TOKKI Co.,Ltd.',
   0x001234: u'Camille Bauer',
   0x001235: u'Andrew Corporation',
   0x001236: u'ConSentry Networks',
   0x001237: u'Texas Instruments',
   0x001238: u'SetaBox Technology Co., Ltd.',
   0x001239: u'S Net Systems Inc.',
   0x00123A: u'Posystech Inc., Co.',
   0x00123B: u'KeRo Systems ApS',
   0x00123C: u'IP3 Networks, Inc.',
   0x00123D: u'GES',
   0x00123E: u'ERUNE technology Co., Ltd.',
   0x00123F: u'Dell Inc',
   0x001240: u'AMOI ELECTRONICS CO.,LTD',
   0x001241: u'a2i marketing center',
   0x001242: u'Millennial Net',
   0x001243: u'Cisco',
   0x001244: u'Cisco',
   0x001245: u'Zellweger Analytics, Inc.',
   0x001246: u'T.O.M TECHNOLOGY INC..',
   0x001247: u'Samsung Electronics Co., Ltd.',
   0x001248: u'Kashya Inc.',
   0x001249: u'Delta Elettronica S.p.A.',
   0x00124A: u'Dedicated Devices, Inc.',
   0x00124B: u'Chipcon AS',
   0x00124C: u'BBWM Corporation',
   0x00124D: u'Inducon BV',
   0x00124E: u'XAC AUTOMATION CORP.',
   0x00124F: u'Tyco Thermal Controls LLC.',
   0x001250: u'Tokyo Aircaft Instrument Co., Ltd.',
   0x001251: u'SILINK',
   0x001252: u'Citronix, LLC',
   0x001253: u'AudioDev AB',
   0x001254: u'Spectra Technologies Holdings Company Ltd',
   0x001255: u'NetEffect Incorporated',
   0x001256: u'LG INFORMATION & COMM.',
   0x001257: u'LeapComm Communication Technologies Inc.',
   0x001258: u'Activis Polska',
   0x001259: u'THERMO ELECTRON KARLSRUHE',
   0x00125A: u'Microsoft Corporation',
   0x00125B: u'KAIMEI ELECTRONI',
   0x00125C: u'Green Hills Software, Inc.',
   0x00125D: u'CyberNet Inc.',
   0x00125E: u'CAEN',
   0x00125F: u'AWIND Inc.',
   0x001260: u'Stanton Magnetics,inc.',
   0x001261: u'Adaptix, Inc',
   0x001262: u'Nokia Danmark A/S',
   0x001263: u'Data Voice Technologies GmbH',
   0x001264: u'daum electronic gmbh',
   0x001265: u'Enerdyne Technologies, Inc.',
   0x001266: u'PRIVATE',
   0x001267: u'Matsushita Electronic Components Co., Ltd.',
   0x001268: u'IPS d.o.o.',
   0x001269: u'Value Electronics',
   0x00126A: u'OPTOELECTRONICS Co., Ltd.',
   0x00126B: u'Ascalade Communications Limited',
   0x00126C: u'Visonic Ltd.',
   0x00126D: u'University of California, Berkeley',
   0x00126E: u'Seidel Elektronik GmbH Nfg.KG',
   0x00126F: u'Rayson Technology Co., Ltd.',
   0x001270: u'NGES Denro Systems',
   0x001271: u'Measurement Computing Corp',
   0x001272: u'Redux Communications Ltd.',
   0x001273: u'Stoke Inc',
   0x001274: u'NIT lab',
   0x001275: u'Moteiv Corporation',
   0x001276: u'Microsol Holdings Ltd.',
   0x001277: u'Korenix Technologies Co., Ltd.',
   0x001278: u'International Bar Code',
   0x001279: u'Hewlett Packard',
   0x00127A: u'Sanyu Industry Co.,Ltd.',
   0x00127B: u'VIA Networking Technologies, Inc.',
   0x00127C: u'SWEGON AB',
   0x00127D: u'MobileAria',
   0x00127E: u'Digital Lifestyles Group, Inc.',
   0x00127F: u'Cisco',
   0x001280: u'Cisco',
   0x001281: u'CIEFFE srl',
   0x001282: u'Qovia',
   0x001283: u'Nortel Networks',
   0x001284: u'Lab33 Srl',
   0x001285: u'Gizmondo Europe Ltd',
   0x001286: u'ENDEVCO CORP',
   0x001287: u'Digital Everywhere Unterhaltungselektronik GmbH',
   0x001288: u'2Wire, Inc',
   0x001289: u'Advance Sterilization Products',
   0x00128A: u'Motorola PCS',
   0x00128B: u'Sensory Networks Inc',
   0x00128C: u'Woodward Governor',
   0x00128D: u'STB Datenservice GmbH',
   0x00128E: u'Q-Free ASA',
   0x00128F: u'Montilio',
   0x001290: u'KYOWA Electric & Machinery Corp.',
   0x001291: u'KWS Computersysteme GmbH',
   0x001292: u'Griffin Technology',
   0x001293: u'GE Energy',
   0x001294: u'Eudyna Devices Inc.',
   0x001295: u'Aiware Inc.',
   0x001296: u'Addlogix',
   0x001297: u'O2Micro, Inc.',
   0x001298: u'MICO ELECTRIC(SHENZHEN) LIMITED',
   0x001299: u'Ktech Telecommunications Inc',
   0x00129A: u'IRT Electronics Pty Ltd',
   0x00129B: u'E2S Electronic Engineering Solutions, S.L.',
   0x00129C: u'Yulinet',
   0x00129D: u'FIRST INTERNATIONAL COMPUTER DO BRASIL LTDA',
   0x00129E: u'Surf Communications Inc.',
   0x00129F: u'RAE Systems, Inc.',
   0x0012A0: u'NeoMeridian Sdn Bhd',
   0x0012A1: u'BluePacket Communications Co., Ltd.',
   0x0012A2: u'VITA',
   0x0012A3: u'Trust International B.V.',
   0x0012A4: u'ThingMagic, LLC',
   0x0012A5: u'Stargen, Inc.',
   0x0012A6: u'Lake Technology Ltd',
   0x0012A7: u'ISR TECHNOLOGIES Inc',
   0x0012A8: u'intec GmbH',
   0x0012A9: u'3COM EUROPE LTD',
   0x0012AA: u'IEE, Inc.',
   0x0012AB: u'WiLife, Inc.',
   0x0012AC: u'ONTIMETEK INC.',
   0x0012AD: u'IDS GmbH',
   0x0012AE: u'HLS HARD-LINE Solutions Inc.',
   0x0012AF: u'ELPRO Technologies',
   0x0012B0: u'Efore Oyj   (Plc)',
   0x0012B1: u'Dai Nippon Printing Co., Ltd',
   0x0012B2: u'AVOLITES LTD.',
   0x0012B3: u'Advance Wireless Technology Corp.',
   0x0012B4: u'Work GmbH',
   0x0012B5: u'Vialta, Inc.',
   0x0012B6: u'Santa Barbara Infrared, Inc.',
   0x0012B7: u'PTW Freiburg',
   0x0012B8: u'G2 Microsystems',
   0x0012B9: u'Fusion Digital Technology',
   0x0012BA: u'FSI Systems, Inc.',
   0x0012BB: u'Telecommunications Industry Association TR-41 Committee',
   0x0012BC: u'Echolab LLC',
   0x0012BD: u'Avantec Manufacturing Limited',
   0x0012BE: u'Astek Corporation',
   0x0012BF: u'Arcadyan Technology Corporation',
   0x0012C0: u'HotLava Systems, Inc.',
   0x0012C1: u'Check Point Software Technologies',
   0x0012C2: u'Apex Electronics Factory',
   0x0012C3: u'WIT S.A.',
   0x0012C4: u'Viseon, Inc.',
   0x0012C5: u'V-Show Technology Co.Ltd',
   0x0012C6: u'TGC America, Inc',
   0x0012C7: u'SECURAY Technologies Ltd.Co.',
   0x0012C8: u'Perfect tech',
   0x0012C9: u'Motorola BCS',
   0x0012CA: u'Hansen Telecom',
   0x0012CB: u'CSS Inc.',
   0x0012CC: u'Bitatek CO., LTD',
   0x0012CD: u'ASEM SpA',
   0x0012CE: u'Advanced Cybernetics Group',
   0x0012CF: u'Accton Technology Corporation',
   0x0012D0: u'Gossen-Metrawatt-GmbH',
   0x0012D1: u'Texas Instruments Inc',
   0x0012D2: u'Texas Instruments',
   0x0012D3: u'Zetta Systems, Inc.',
   0x0012D4: u'Princeton Technology, Ltd',
   0x0012D5: u'Motion Reality Inc.',
   0x0012D6: u'Jiangsu Yitong High-Tech Co.,Ltd',
   0x0012D7: u'Invento Networks, Inc.',
   0x0012D8: u'International Games System Co., Ltd.',
   0x0012D9: u'Cisco Systems',
   0x0012DA: u'Cisco Systems',
   0x0012DB: u'ZIEHL industrie-elektronik GmbH + Co KG',
   0x0012DC: u'SunCorp Industrial Limited',
   0x0012DD: u'Shengqu Information Technology (Shanghai) Co., Ltd.',
   0x0012DE: u'Radio Components Sweden AB',
   0x0012DF: u'Novomatic AG',
   0x0012E0: u'Codan Limited',
   0x0012E1: u'Alliant Networks, Inc',
   0x0012E2: u'ALAXALA Networks Corporation',
   0x0012E3: u'Agat-RT, Ltd.',
   0x0012E4: u'ZIEHL industrie-electronik GmbH + Co KG',
   0x0012E5: u'Time America, Inc.',
   0x0012E6: u'SPECTEC COMPUTER CO., LTD.',
   0x0012E7: u'Projectek Networking Electronics Corp.',
   0x0012E8: u'Fraunhofer IMS',
   0x0012E9: u'Abbey Systems Ltd',
   0x0012EA: u'Trane',
   0x0012EB: u'R2DI, LLC',
   0x0012EC: u'Movacolor b.v.',
   0x0012ED: u'AVG Advanced Technologies',
   0x0012EE: u'Sony Ericsson Mobile Communications AB',
   0x0012EF: u'OneAccess SA',
   0x0012F0: u'Intel Corporate',
   0x0012F1: u'IFOTEC',
   0x0012F2: u'Foundry Networks',
   0x0012F3: u'connectBlue AB',
   0x0012F4: u'Belco International Co.,Ltd.',
   0x0012F5: u'Prolificx Ltd',
   0x0012F6: u'MDK CO.,LTD.',
   0x0012F7: u'Xiamen Xinglian Electronics Co., Ltd.',
   0x0012F8: u'WNI Resources, LLC',
   0x0012F9: u'URYU SEISAKU, LTD.',
   0x0012FA: u'THX LTD',
   0x0012FB: u'Samsung Electronics',
   0x0012FC: u'PLANET System Co.,LTD',
   0x0012FD: u'OPTIMUS IC S.A.',
   0x0012FE: u'Lenovo Mobile Communication Technology Ltd.',
   0x0012FF: u'Lely Industries N.V.',
   0x001300: u'IT-FACTORY, INC.',
   0x001301: u'IronGate S.L.',
   0x001302: u'Intel Corporate',
   0x001303: u'GateConnect Technologies GmbH',
   0x001304: u'Flaircomm Technologies Co. LTD',
   0x001305: u'Epicom, Inc.',
   0x001306: u'Always On Wireless',
   0x001307: u'Paravirtual Corporation',
   0x001308: u'Nuvera Fuel Cells',
   0x001309: u'Ocean Broadband Networks',
   0x00130A: u'Nortel',
   0x00130B: u'Mextal B.V.',
   0x00130C: u'HF System Corporation',
   0x00130D: u'GALILEO AVIONICA',
   0x00130E: u'Focusrite Audio Engineering Limited',
   0x00130F: u'EGEMEN Bilgisayar Muh San ve Tic LTD STI',
   0x001310: u'Cisco-Linksys, LLC',
   0x001311: u'ARRIS International',
   0x001312: u'Amedia Networks Inc.',
   0x001313: u'GuangZhou Post & Telecom Equipment ltd',
   0x001314: u'Asiamajor Inc.',
   0x001315: u'SONY Computer Entertainment inc,',
   0x001316: u'L-S-B GmbH',
   0x001317: u'GN Netcom as',
   0x001318: u'DGSTATION Co., Ltd.',
   0x001319: u'Cisco Systems',
   0x00131A: u'Cisco Systems',
   0x00131B: u'BeCell Innovations Corp.',
   0x00131C: u'LiteTouch, Inc.',
   0x00131D: u'Scanvaegt International A/S',
   0x00131E: u'Peiker acustic GmbH & Co. KG',
   0x00131F: u'NxtPhase T&D, Corp.',
   0x001320: u'Intel Corporate',
   0x001321: u'Hewlett Packard',
   0x001322: u'DAQ Electronics, Inc.',
   0x001323: u'Cap Co., Ltd.',
   0x001324: u'Schneider Electric Ultra Terminal',
   0x001325: u'ImmenStar Inc.',
   0x001326: u'ECM Systems Ltd',
   0x001327: u'Data Acquisitions limited',
   0x001328: u'Westech Korea Inc.,',
   0x001329: u'VSST Co., LTD',
   0x00132A: u'STROM telecom, s. r. o.',
   0x00132B: u'Phoenix Digital',
   0x00132C: u'MAZ Brandenburg GmbH',
   0x00132D: u'iWise Communications Pty Ltd',
   0x00132E: u'ITian Coporation',
   0x00132F: u'Interactek',
   0x001330: u'EURO PROTECTION SURVEILLANCE',
   0x001331: u'CellPoint Connect',
   0x001332: u'Beijing Topsec Network Security Technology Co., Ltd.',
   0x001333: u'Baud Technology Inc.',
   0x001334: u'Arkados, Inc.',
   0x001335: u'VS Industry Berhad',
   0x001336: u'Tianjin 712 Communication Broadcasting co., ltd.',
   0x001337: u'Orient Power Home Network Ltd.',
   0x001338: u'FRESENIUS-VIAL',
   0x001339: u'EL-ME AG',
   0x00133A: u'VadaTech Inc.',
   0x00133B: u'Speed Dragon Multimedia Limited',
   0x00133C: u'QUINTRON SYSTEMS INC.',
   0x00133D: u'Micro Memory LLC',
   0x00133E: u'MetaSwitch',
   0x00133F: u'Eppendorf Instrumente GmbH',
   0x001340: u'AD.EL s.r.l.',
   0x001341: u'Shandong New Beiyang Information Technology Co.,Ltd',
   0x001342: u'Vision Research, Inc.',
   0x001343: u'Matsushita Electronic Components (Europe) GmbH',
   0x001344: u'Fargo Electronics Inc.',
   0x001345: u'Eaton Corporation',
   0x001346: u'D-Link Corporation',
   0x001347: u'BlueTree Wireless Data Inc.',
   0x001348: u'Artila Electronics Co., Ltd.',
   0x001349: u'ZyXEL Communications Corporation',
   0x00134A: u'Engim, Inc.',
   0x00134B: u'ToGoldenNet Technology Inc.',
   0x00134C: u'YDT Technology International',
   0x00134D: u'IPC systems',
   0x00134E: u'Valox Systems, Inc.',
   0x00134F: u'Tranzeo Wireless Technologies Inc.',
   0x001350: u'Silver Spring Networks, Inc',
   0x001351: u'Niles Audio Corporation',
   0x001352: u'Naztec, Inc.',
   0x001353: u'HYDAC Filtertechnik GMBH',
   0x001354: u'Zcomax Technologies, Inc.',
   0x001355: u'TOMEN Cyber-business Solutions, Inc.',
   0x001356: u'target systemelectronic gmbh',
   0x001357: u'Soyal Technology Co., Ltd.',
   0x001358: u'Realm Systems, Inc.',
   0x001359: u'ProTelevision Technologies A/S',
   0x00135A: u'Project T&E Limited',
   0x00135B: u'PanelLink Cinema, LLC',
   0x00135C: u'OnSite Systems, Inc.',
   0x00135D: u'NTTPC Communications, Inc.',
   0x00135E: u'EAB/RWI/K',
   0x00135F: u'Cisco Systems',
   0x001360: u'Cisco Systems',
   0x001361: u'Biospace Co., Ltd.',
   0x001362: u'ShinHeung Precision Co., Ltd.',
   0x001363: u'Verascape, Inc.',
   0x001364: u'Paradigm Technology Inc..',
   0x001365: u'Nortel',
   0x001366: u'Neturity Technologies Inc.',
   0x001367: u'Narayon. Co., Ltd.',
   0x001368: u'Maersk Data Defence',
   0x001369: u'Honda Electron Co., LED.',
   0x00136A: u'Hach Ultra Analytics',
   0x00136B: u'E-TEC',
   0x00136C: u'PRIVATE',
   0x00136D: u'Tentaculus AB',
   0x00136E: u'Techmetro Corp.',
   0x00136F: u'PacketMotion, Inc.',
   0x001370: u'Nokia Danmark A/S',
   0x001371: u'Motorola CHS',
   0x001372: u'Dell Inc.',
   0x001373: u'BLwave Electronics Co., Ltd',
   0x001374: u'Attansic Technology Corp.',
   0x001375: u'American Security Products Co.',
   0x001376: u'Tabor Electronics Ltd.',
   0x001377: u'Samsung Electronics CO., LTD',
   0x001378: u'QSAN Technology, Inc.',
   0x001379: u'PONDER INFORMATION INDUSTRIES LTD.',
   0x00137A: u'Netvox Technology Co., Ltd.',
   0x00137B: u'Movon Corporation',
   0x00137C: u'Kaicom co., Ltd.',
   0x00137D: u'Dynalab, Inc.',
   0x00137E: u'CorEdge Networks, Inc.',
   0x00137F: u'Cisco Systems',
   0x001380: u'Cisco Systems',
   0x001381: u'CHIPS & Systems, Inc.',
   0x001382: u'Cetacea Networks Corporation',
   0x001383: u'Application Technologies and Engineering Research Laboratory',
   0x001384: u'Advanced Motion Controls',
   0x001385: u'Add-On Technology Co., LTD.',
   0x001386: u'ABB Inc./Totalflow',
   0x001387: u'27M Technologies AB',
   0x001388: u'WiMedia Alliance',
   0x001389: u'Redes de Telefona Mvil S.A.',
   0x00138A: u'QINGDAO GOERTEK ELECTRONICS CO.,LTD.',
   0x00138B: u'Phantom Technologies LLC',
   0x00138C: u'Kumyoung.Co.Ltd',
   0x00138D: u'Kinghold',
   0x00138E: u'FOAB Elektronik AB',
   0x00138F: u'Asiarock Incorporation',
   0x001390: u'Termtek Computer Co., Ltd',
   0x001391: u'OUEN CO.,LTD.',
   0x001392: u'Ruckus Wireless',
   0x001393: u'Panta Systems, Inc.',
   0x001394: u'Infohand Co.,Ltd',
   0x001395: u'congatec AG',
   0x001396: u'Acbel Polytech Inc.',
   0x001397: u'Xsigo Systems, Inc.',
   0x001398: u'TrafficSim Co.,Ltd',
   0x001399: u'STAC Corporation.',
   0x00139A: u'K-ubique ID Corp.',
   0x00139B: u'ioIMAGE Ltd.',
   0x00139C: u'Exavera Technologies, Inc.',
   0x00139D: u'Design of Systems on Silicon S.A.',
   0x00139E: u'Ciara Technologies Inc.',
   0x00139F: u'Electronics Design Services, Co., Ltd.',
   0x0013A0: u'ALGOSYSTEM Co., Ltd.',
   0x0013A1: u'Crow Electronic Engeneering',
   0x0013A2: u'MaxStream, Inc',
   0x0013A3: u'Siemens Com CPE Devices',
   0x0013A4: u'KeyEye Communications',
   0x0013A5: u'General Solutions, LTD.',
   0x0013A6: u'Extricom Ltd',
   0x0013A7: u'BATTELLE MEMORIAL INSTITUTE',
   0x0013A8: u'Tanisys Technology',
   0x0013A9: u'Sony Corporation',
   0x0013AA: u'ALS  & TEC Ltd.',
   0x0013AB: u'Telemotive AG',
   0x0013AC: u'Sunmyung Electronics Co., LTD',
   0x0013AD: u'Sendo Ltd',
   0x0013AE: u'Radiance Technologies',
   0x0013AF: u'NUMA Technology,Inc.',
   0x0013B0: u'Jablotron',
   0x0013B1: u'Intelligent Control Systems (Asia) Pte Ltd',
   0x0013B2: u'Carallon Limited',
   0x0013B3: u'Beijing Ecom Communications Technology Co., Ltd.',
   0x0013B4: u'Appear TV',
   0x0013B5: u'Wavesat',
   0x0013B6: u'Sling Media, Inc.',
   0x0013B7: u'Scantech ID',
   0x0013B8: u'RyCo Electronic Systems Limited',
   0x0013B9: u'BM SPA',
   0x0013BA: u'ReadyLinks Inc',
   0x0013BB: u'PRIVATE',
   0x0013BC: u'Artimi Ltd',
   0x0013BD: u'HYMATOM SA',
   0x0013BE: u'Virtual Conexions',
   0x0013BF: u'Media System Planning Corp.',
   0x0013C0: u'Trix Tecnologia Ltda.',
   0x0013C1: u'Asoka USA Corporation',
   0x0013C2: u'WACOM Co.,Ltd',
   0x0013C3: u'Cisco Systems',
   0x0013C4: u'Cisco Systems',
   0x0013C5: u'LIGHTRON FIBER-OPTIC DEVICES INC.',
   0x0013C6: u'OpenGear, Inc',
   0x0013C7: u'IONOS Co.,Ltd.',
   0x0013C8: u'PIRELLI BROADBAND SOLUTIONS S.P.A.',
   0x0013C9: u'Beyond Achieve Enterprises Ltd.',
   0x0013CA: u'X-Digital Systems, Inc.',
   0x0013CB: u'Zenitel Norway AS',
   0x0013CC: u'Tall Maple Systems',
   0x0013CD: u'MTI co. LTD',
   0x0013CE: u'Intel Corporate',
   0x0013CF: u'4Access Communications',
   0x0013D0: u'e-San Limited',
   0x0013D1: u'KIRK telecom A/S',
   0x0013D2: u'PAGE IBERICA, S.A.',
   0x0013D3: u'MICRO-STAR INTERNATIONAL CO., LTD.',
   0x0013D4: u'ASUSTek COMPUTER INC.',
   0x0013D5: u'WiNetworks LTD',
   0x0013D6: u'TII NETWORK TECHNOLOGIES, INC.',
   0x0013D7: u'SPIDCOM Technologies SA',
   0x0013D8: u'Princeton Instruments',
   0x0013D9: u'Matrix Product Development, Inc.',
   0x0013DA: u'Diskware Co., Ltd',
   0x0013DB: u'SHOEI Electric Co.,Ltd',
   0x0013DC: u'IBTEK INC.',
   0x0013DD: u'Abbott Diagnostics',
   0x0013DE: u'Adapt4',
   0x0013DF: u'Ryvor Corp.',
   0x0013E0: u'Murata Manufacturing Co., Ltd.',
   0x0013E1: u'Iprobe',
   0x0013E2: u'GeoVision Inc.',
   0x0013E3: u'CoVi Technologies, Inc.',
   0x0013E4: u'YANGJAE SYSTEMS CORP.',
   0x0013E5: u'TENOSYS, INC.',
   0x0013E6: u'Technolution',
   0x0013E7: u'Minelab Electronics Pty Limited',
   0x0013E8: u'Intel Corporate',
   0x0013E9: u'VeriWave, Inc.',
   0x0013EA: u'Kamstrup A/S',
   0x0013EB: u'Sysmaster Corporation',
   0x0013EC: u'Sunbay Software AG',
   0x0013ED: u'PSIA',
   0x0013EE: u'JBX Designs Inc.',
   0x0013EF: u'Kingjon Digital Technology Co.,Ltd',
   0x0013F0: u'Wavefront Semiconductor',
   0x0013F1: u'AMOD Technology Co., Ltd.',
   0x0013F2: u'Klas Ltd',
   0x0013F3: u'Giga-byte Communications Inc.',
   0x0013F4: u'Psitek (Pty) Ltd',
   0x0013F5: u'Akimbi Systems',
   0x0013F6: u'Cintech',
   0x0013F7: u'SMC Networks, Inc.',
   0x0013F8: u'Dex Security Solutions',
   0x0013F9: u'Cavera Systems',
   0x0013FA: u'LifeSize Communications, Inc',
   0x0013FB: u'RKC INSTRUMENT INC.',
   0x0013FC: u'SiCortex, Inc',
   0x0013FD: u'Nokia Danmark A/S',
   0x0013FE: u'GRANDTEC ELECTRONIC CORP.',
   0x0013FF: u'Dage-MTI of MC, Inc.',
   0x001400: u'MINERVA KOREA CO., LTD',
   0x001401: u'Rivertree Networks Corp.',
   0x001402: u'kk-electronic a/s',
   0x001403: u'Renasis, LLC',
   0x001404: u'Motorola CHS',
   0x001405: u'OpenIB, Inc.',
   0x001406: u'Go Networks',
   0x001407: u'Biosystems',
   0x001408: u'Eka Systems Inc.',
   0x001409: u'MAGNETI MARELLI   S.E. S.p.A.',
   0x00140A: u'WEPIO Co., Ltd.',
   0x00140B: u'FIRST INTERNATIONAL COMPUTER, INC.',
   0x00140C: u'GKB CCTV CO., LTD.',
   0x00140D: u'Nortel',
   0x00140E: u'Nortel',
   0x00140F: u'Federal State Unitary Enterprise Leningrad R&D Institute of',
   0x001410: u'Suzhou Keda Technology CO.,Ltd',
   0x001411: u'Deutschmann Automation GmbH & Co. KG',
   0x001412: u'S-TEC electronics AG',
   0x001413: u'Trebing & Himstedt Prozessautomation GmbH & Co. KG',
   0x001414: u'Jumpnode Systems LLC.',
   0x001415: u'Intec Automation Inc.',
   0x001416: u'Scosche Industries, Inc.',
   0x001417: u'RSE Informations Technologie GmbH',
   0x001418: u'C4Line',
   0x001419: u'SIDSA',
   0x00141A: u'DEICY CORPORATION',
   0x00141B: u'Cisco Systems',
   0x00141C: u'Cisco Systems',
   0x00141D: u'Lust Antriebstechnik GmbH',
   0x00141E: u'P.A. Semi, Inc.',
   0x00141F: u'SunKwang Electronics Co., Ltd',
   0x001420: u'G-Links networking company',
   0x001421: u'Total Wireless Technologies Pte. Ltd.',
   0x001422: u'Dell Inc.',
   0x001423: u'J-S Co. NEUROCOM',
   0x001424: u'Merry Electrics CO., LTD.',
   0x001425: u'Galactic Computing Corp.',
   0x001426: u'NL Technology',
   0x001427: u'JazzMutant',
   0x001428: u'Vocollect, Inc',
   0x001429: u'V Center Technologies Co., Ltd.',
   0x00142A: u'Elitegroup Computer System Co., Ltd',
   0x00142B: u'Edata Technologies Inc.',
   0x00142C: u'Koncept International, Inc.',
   0x00142D: u'Toradex AG',
   0x00142E: u'77 Elektronika Kft.',
   0x00142F: u'WildPackets',
   0x001430: u'ViPowER, Inc',
   0x001431: u'PDL Electronics Ltd',
   0x001432: u'Tarallax Wireless, Inc.',
   0x001433: u'Empower Technologies(Canada) Inc.',
   0x001434: u'Keri Systems, Inc',
   0x001435: u'CityCom Corp.',
   0x001436: u'Qwerty Elektronik AB',
   0x001437: u'GSTeletech Co.,Ltd.',
   0x001438: u'Hewlett Packard',
   0x001439: u'Blonder Tongue Laboratories, Inc.',
   0x00143A: u'RAYTALK INTERNATIONAL SRL',
   0x00143B: u'Sensovation AG',
   0x00143C: u'Oerlikon Contraves Inc.',
   0x00143D: u'Aevoe Inc.',
   0x00143E: u'AirLink Communications, Inc.',
   0x00143F: u'Hotway Technology Corporation',
   0x001440: u'ATOMIC Corporation',
   0x001441: u'Innovation Sound Technology Co., LTD.',
   0x001442: u'ATTO CORPORATION',
   0x001443: u'Consultronics Europe Ltd',
   0x001444: u'Grundfos Electronics',
   0x001445: u'Telefon-Gradnja d.o.o.',
   0x001446: u'KidMapper, Inc.',
   0x001447: u'BOAZ Inc.',
   0x001448: u'Inventec Multimedia & Telecom Corporation',
   0x001449: u'Sichuan Changhong Electric Ltd.',
   0x00144A: u'Taiwan Thick-Film Ind. Corp.',
   0x00144B: u'Hifn, Inc.',
   0x00144C: u'General Meters Corp.',
   0x00144D: u'Intelligent Systems',
   0x00144E: u'SRISA',
   0x00144F: u'Sun Microsystems, Inc.',
   0x001450: u'Heim Systems GmbH',
   0x001451: u'Apple Computer Inc.',
   0x001452: u'CALCULEX,INC.',
   0x001453: u'ADVANTECH TECHNOLOGIES CO.,LTD',
   0x001454: u'Symwave',
   0x001455: u'Coder Electronics Corporation',
   0x001456: u'Edge Products',
   0x001457: u'T-VIPS AS',
   0x001458: u'HS Automatic ApS',
   0x001459: u'Moram Co., Ltd.',
   0x00145A: u'Elektrobit AG',
   0x00145B: u'SeekerNet Inc.',
   0x00145C: u'Intronics B.V.',
   0x00145D: u'WJ Communications, Inc.',
   0x00145E: u'IBM',
   0x00145F: u'ADITEC CO. LTD',
   0x001460: u'Kyocera Wireless Corp.',
   0x001461: u'CORONA CORPORATION',
   0x001462: u'Digiwell Technology, inc',
   0x001463: u'IDCS N.V.',
   0x001464: u'Cryptosoft',
   0x001465: u'Novo Nordisk A/S',
   0x001466: u'Kleinhenz Elektronik GmbH',
   0x001467: u'ArrowSpan Inc.',
   0x001468: u'CelPlan International, Inc.',
   0x001469: u'Cisco Systems',
   0x00146A: u'Cisco Systems',
   0x00146B: u'Anagran, Inc.',
   0x00146C: u'Netgear Inc.',
   0x00146D: u'RF Technologies',
   0x00146E: u'H. Stoll GmbH & Co. KG',
   0x00146F: u'Kohler Co',
   0x001470: u'Prokom Software SA',
   0x001471: u'Eastern Asia Technology Limited',
   0x001472: u'China Broadband Wireless IP Standard Group',
   0x001473: u'Bookham Inc',
   0x001474: u'K40 Electronics',
   0x001475: u'Wiline Networks, Inc.',
   0x001476: u'MultiCom Industries Limited',
   0x001477: u'Nertec  Inc.',
   0x001478: u'ShenZhen TP-LINK Technologies Co., Ltd.',
   0x001479: u'NEC Magnus Communications,Ltd.',
   0x00147A: u'Eubus GmbH',
   0x00147B: u'Iteris, Inc.',
   0x00147C: u'3Com Europe Ltd',
   0x00147D: u'Aeon Digital International',
   0x00147E: u'PanGo Networks, Inc.',
   0x00147F: u'Thomson Telecom Belgium',
   0x001480: u'Hitachi-LG Data Storage Korea, Inc',
   0x001481: u'Multilink Inc',
   0x001482: u'GoBackTV, Inc',
   0x001483: u'eXS Inc.',
   0x001484: u'CERMATE TECHNOLOGIES INC',
   0x001485: u'Giga-Byte',
   0x001486: u'Echo Digital Audio Corporation',
   0x001487: u'American Technology Integrators',
   0x001488: u'Akorri Networks',
   0x001489: u'B15402100 - JANDEI, S.L.',
   0x00148A: u'Elin Ebg Traction Gmbh',
   0x00148B: u'Globo Electronic GmbH & Co. KG',
   0x00148C: u'Fortress Technologies',
   0x00148D: u'Cubic Defense Simulation Systems',
   0x00148E: u'Tele Power Inc.',
   0x00148F: u'Protronic (Far East) Ltd.',
   0x001490: u'ASP Corporation',
   0x001491: u'Daniels Electronics Ltd.',
   0x001492: u'Liteon, Mobile Media Solution SBU',
   0x001493: u'Systimax Solutions',
   0x001494: u'ESU AG',
   0x001495: u'2Wire, Inc.',
   0x001496: u'Phonic Corp.',
   0x001497: u'ZHIYUAN Eletronics co.,ltd.',
   0x001498: u'Viking Design Technology',
   0x001499: u'Helicomm Inc',
   0x00149A: u'Motorola Mobile Devices Business',
   0x00149B: u'Nokota Communications, LLC',
   0x00149C: u'HF Company',
   0x00149D: u'Sound ID Inc.',
   0x00149E: u'UbONE Co., Ltd',
   0x00149F: u'System and Chips, Inc.',
   0x0014A0: u'RFID Asset Track, Inc.',
   0x0014A1: u'Synchronous Communication Corp',
   0x0014A2: u'Core Micro Systems Inc.',
   0x0014A3: u'Vitelec BV',
   0x0014A4: u'Hon Hai Precision Ind. Co., Ltd.',
   0x0014A5: u'Gemtek Technology Co., Ltd.',
   0x0014A6: u'Teranetics, Inc.',
   0x0014A7: u'Nokia Danmark A/S',
   0x0014A8: u'Cisco Systems',
   0x0014A9: u'Cisco Systems',
   0x0014AA: u'Ashly Audio, Inc.',
   0x0014AB: u'Senhai Electronic Technology Co., Ltd.',
   0x0014AC: u'Bountiful WiFi',
   0x0014AD: u'Gassner Wiege- u. Metechnik GmbH',
   0x0014AE: u'Wizlogics Co., Ltd.',
   0x0014AF: u'Datasym Inc.',
   0x0014B0: u'Naeil Community',
   0x0014B1: u'Avitec AB',
   0x0014B2: u'mCubelogics Corporation',
   0x0014B3: u'CoreStar International Corp',
   0x0014B4: u'General Dynamics United Kingdom Ltd',
   0x0014B5: u'PRIVATE',
   0x0014B6: u'Enswer Technology Inc.',
   0x0014B7: u'AR Infotek Inc.',
   0x0014B8: u'Hill-Rom',
   0x0014B9: u'STEPMIND',
   0x0014BA: u'Carvers SA de CV',
   0x0014BB: u'Open Interface North America',
   0x0014BC: u'SYNECTIC TELECOM EXPORTS PVT. LTD.',
   0x0014BD: u'incNETWORKS, Inc',
   0x0014BE: u'Wink communication technology CO.LTD',
   0x0014BF: u'Cisco-Linksys LLC',
   0x0014C0: u'Symstream Technology Group Ltd',
   0x0014C1: u'U.S. Robotics Corporation',
   0x0014C2: u'Hewlett Packard',
   0x0014C3: u'Seagate Technology LLC',
   0x0014C4: u'Vitelcom Mobile Technology',
   0x0014C5: u'Alive Technologies Pty Ltd',
   0x0014C6: u'Quixant Ltd',
   0x0014C7: u'Nortel',
   0x0014C8: u'Contemporary Research Corp',
   0x0014C9: u'Silverback Systems, Inc.',
   0x0014CA: u'Key Radio Systems Limited',
   0x0014CB: u'LifeSync Corporation',
   0x0014CC: u'Zetec, Inc.',
   0x0014CD: u'DigitalZone Co., Ltd.',
   0x0014CE: u'NF CORPORATION',
   0x0014CF: u'Nextlink.to A/S',
   0x0014D0: u'BTI Photonics',
   0x0014D1: u'TRENDware International, Inc.',
   0x0014D2: u'KYUKI CORPORATION',
   0x0014D3: u'SEPSA',
   0x0014D4: u'K Technology Corporation',
   0x0014D5: u'Datang Telecom Technology CO. , LCD,Optical Communication Br',
   0x0014D6: u'Jeongmin Electronics Co.,Ltd.',
   0x0014D7: u'DataStor Technology Inc.',
   0x0014D8: u'bio-logic SA',
   0x0014D9: u'IP Fabrics, Inc.',
   0x0014DA: u'Huntleigh Healthcare',
   0x0014DB: u'Elma Trenew Electronic GmbH',
   0x0014DC: u'Communication System Design & Manufacturing (CSDM)',
   0x0014DD: u'Covergence Inc.',
   0x0014DE: u'Sage Instruments Inc.',
   0x0014DF: u'HI-P Tech Corporation',
   0x0014E0: u'LET\'S Corporation',
   0x0014E1: u'Data Display AG',
   0x0014E2: u'datacom systems inc.',
   0x0014E3: u'mm-lab GmbH',
   0x0014E4: u'Integral Technologies',
   0x0014E5: u'Alticast',
   0x0014E6: u'AIM Infrarotmodule GmbH',
   0x0014E7: u'Stolinx,. Inc',
   0x0014E8: u'Motorola CHS',
   0x0014E9: u'Nortech International',
   0x0014EA: u'S Digm Inc. (Safe Paradigm Inc.)',
   0x0014EB: u'AwarePoint Corporation',
   0x0014EC: u'Acro Telecom',
   0x0014ED: u'Airak, Inc.',
   0x0014EE: u'Western Digital Technologies, Inc.',
   0x0014EF: u'TZero Technologies, Inc.',
   0x0014F0: u'Business Security OL AB',
   0x0014F1: u'Cisco Systems',
   0x0014F2: u'Cisco Systems',
   0x0014F3: u'ViXS Systems Inc',
   0x0014F4: u'DekTec Digital Video B.V.',
   0x0014F5: u'OSI Security Devices',
   0x0014F6: u'Juniper Networks, Inc.',
   0x0014F7: u'Crevis',
   0x0014F8: u'Scientific Atlanta',
   0x0014F9: u'Vantage Controls',
   0x0014FA: u'AsGa S.A.',
   0x0014FB: u'Technical Solutions Inc.',
   0x0014FC: u'Extandon, Inc.',
   0x0014FD: u'Thecus Technology Corp.',
   0x0014FE: u'Artech Electronics',
   0x0014FF: u'Precise Automation, LLC',
   0x001500: u'Intel Corporate',
   0x001501: u'LexBox',
   0x001502: u'BETA tech',
   0x001503: u'PROFIcomms s.r.o.',
   0x001504: u'GAME PLUS CO., LTD.',
   0x001505: u'Actiontec Electronics, Inc',
   0x001506: u'BeamExpress, Inc',
   0x001507: u'Renaissance Learning Inc',
   0x001508: u'Global Target Enterprise Inc',
   0x001509: u'Plus Technology Co., Ltd',
   0x00150A: u'Sonoa Systems, Inc',
   0x00150B: u'SAGE INFOTECH LTD.',
   0x00150C: u'AVM GmbH',
   0x00150D: u'Hoana Medical, Inc.',
   0x00150E: u'OPENBRAIN TECHNOLOGIES CO., LTD.',
   0x00150F: u'mingjong',
   0x001510: u'Techsphere Co., Ltd',
   0x001511: u'Data Center Systems',
   0x001512: u'Zurich University of Applied Sciences',
   0x001513: u'EFS sas',
   0x001514: u'Hu Zhou NAVA Networks&Electronics Ltd.',
   0x001515: u'Leipold+Co.GmbH',
   0x001516: u'URIEL SYSTEMS INC.',
   0x001517: u'Intel Corporate',
   0x001518: u'Shenzhen 10MOONS Technology Development CO.,Ltd',
   0x001519: u'StoreAge Networking Technologies',
   0x00151A: u'Hunter Engineering Company',
   0x00151B: u'Isilon Systems Inc.',
   0x00151C: u'LENECO',
   0x00151D: u'M2I CORPORATION',
   0x00151E: u'Metaware Co., Ltd.',
   0x00151F: u'Multivision Intelligent Surveillance (Hong Kong) Ltd',
   0x001520: u'Radiocrafts AS',
   0x001521: u'Horoquartz',
   0x001522: u'Dea Security',
   0x001523: u'Meteor Communications Corporation',
   0x001524: u'Numatics, Inc.',
   0x001525: u'PTI Integrated Systems, Inc.',
   0x001526: u'Remote Technologies Inc',
   0x001527: u'Balboa Instruments',
   0x001528: u'Beacon Medical Products LLC d.b.a. BeaconMedaes',
   0x001529: u'N3 Corporation',
   0x00152A: u'Nokia GmbH',
   0x00152B: u'Cisco Systems',
   0x00152C: u'Cisco Systems',
   0x00152D: u'TenX Networks, LLC',
   0x00152E: u'PacketHop, Inc.',
   0x00152F: u'Motorola CHS',
   0x001530: u'Bus-Tech, Inc.',
   0x001531: u'KOCOM',
   0x001532: u'Consumer Technologies Group, LLC',
   0x001533: u'NADAM.CO.,LTD',
   0x001534: u'A BELTRNICA, Companhia de Comunicaes, Lda',
   0x001535: u'OTE Spa',
   0x001536: u'Powertech co.,Ltd',
   0x001537: u'Ventus Networks',
   0x001538: u'RFID, Inc.',
   0x001539: u'Technodrive SRL',
   0x00153A: u'Shenzhen Syscan Technology Co.,Ltd.',
   0x00153B: u'EMH Elektrizittszhler GmbH & CoKG',
   0x00153C: u'Kprotech Co., Ltd.',
   0x00153D: u'ELIM PRODUCT CO.',
   0x00153E: u'Q-Matic Sweden AB',
   0x00153F: u'Alcatel Alenia Space Italia',
   0x001540: u'Nortel',
   0x001541: u'StrataLight Communications, Inc.',
   0x001542: u'MICROHARD S.R.L.',
   0x001543: u'Aberdeen Test Center',
   0x001544: u'coM.s.a.t. AG',
   0x001545: u'SEECODE Co., Ltd.',
   0x001546: u'ITG Worldwide Sdn Bhd',
   0x001547: u'AiZen Solutions Inc.',
   0x001548: u'CUBE TECHNOLOGIES',
   0x001549: u'Dixtal Biomedica Ind. Com. Ltda',
   0x00154A: u'WANSHIH ELECTRONIC CO., LTD',
   0x00154B: u'Wonde Proud Technology Co., Ltd',
   0x00154C: u'Saunders Electronics',
   0x00154D: u'Netronome Systems, Inc.',
   0x00154E: u'Hirschmann Automation and Control GmbH',
   0x00154F: u'one RF Technology',
   0x001550: u'Nits Technology Inc',
   0x001551: u'RadioPulse Inc.',
   0x001552: u'Wi-Gear Inc.',
   0x001553: u'Cytyc Corporation',
   0x001554: u'Atalum Wireless S.A.',
   0x001555: u'DFM GmbH',
   0x001556: u'SAGEM SA',
   0x001557: u'Olivetti',
   0x001558: u'FOXCONN',
   0x001559: u'Securaplane Technologies, Inc.',
   0x00155A: u'DAINIPPON PHARMACEUTICAL CO., LTD.',
   0x00155B: u'Sampo Corporation',
   0x00155C: u'Dresser Wayne',
   0x00155D: u'Microsoft Corporation',
   0x00155E: u'Morgan Stanley',
   0x00155F: u'Ubiwave',
   0x001560: u'Hewlett Packard',
   0x001561: u'JJPlus Corporation',
   0x001562: u'Cisco Systems',
   0x001563: u'Cisco Systems',
   0x001564: u'BEHRINGER Spezielle Studiotechnik GmbH',
   0x001565: u'XIAMEN YEALINK NETWORK TECHNOLOGY CO.,LTD',
   0x001566: u'A-First Technology Co., Ltd.',
   0x001567: u'RADWIN Inc.',
   0x001568: u'Dilithium Networks',
   0x001569: u'PECO II, Inc.',
   0x00156A: u'DG2L Technologies Pvt. Ltd.',
   0x00156B: u'Perfisans Networks Corp.',
   0x00156C: u'SANE SYSTEM CO., LTD',
   0x00156D: u'Ubiquiti Networks',
   0x00156E: u'A. W. Communication Systems Ltd',
   0x00156F: u'Xiranet Communications GmbH',
   0x001570: u'Symbol Technologies',
   0x001571: u'Nolan Systems',
   0x001572: u'Red-Lemon',
   0x001573: u'NewSoft  Technology Corporation',
   0x001574: u'Horizon Semiconductors Ltd.',
   0x001575: u'Nevis Networks Inc.',
   0x001576: u'scil animal care company GmbH',
   0x001577: u'Allied Telesyn, Inc.',
   0x001578: u'Audio / Video Innovations',
   0x001579: u'Lunatone Industrielle Elektronik GmbH',
   0x00157A: u'Telefin S.p.A.',
   0x00157B: u'Leuze electronic GmbH + Co. KG',
   0x00157C: u'Dave Networks, Inc.',
   0x00157D: u'POSDATA CO., LTD.',
   0x00157E: u'HEYFRA ELECTRONIC gmbH',
   0x00157F: u'ChuanG International Holding CO.,LTD.',
   0x001580: u'U-WAY CORPORATION',
   0x001581: u'MAKUS Inc.',
   0x001582: u'TVonics Ltd',
   0x001583: u'IVT corporation',
   0x001584: u'Schenck Process GmbH',
   0x001585: u'Aonvision Technolopy Corp.',
   0x001586: u'Xiamen Overseas Chinese Electronic Co., Ltd.',
   0x001587: u'Takenaka Seisakusho Co.,Ltd',
   0x001588: u'Balda-Thong Fook Solutions Sdn. Bhd.',
   0x001589: u'D-MAX Technology Co.,Ltd',
   0x00158A: u'SURECOM Technology Corp.',
   0x00158B: u'Park Air Systems Ltd',
   0x00158C: u'Liab ApS',
   0x00158D: u'Jennic Ltd',
   0x00158E: u'Plustek.INC',
   0x00158F: u'NTT Advanced Technology Corporation',
   0x001590: u'Hectronic GmbH',
   0x001591: u'RLW Inc.',
   0x001592: u'Facom UK Ltd (Melksham)',
   0x001593: u'U4EA Technologies Inc.',
   0x001594: u'BIXOLON CO.,LTD',
   0x001595: u'Quester Tangent Corporation',
   0x001596: u'ARRIS International',
   0x001597: u'AETA AUDIO SYSTEMS',
   0x001598: u'Kolektor group',
   0x001599: u'Samsung Electronics Co., LTD',
   0x00159A: u'Motorola CHS',
   0x00159B: u'Nortel',
   0x00159C: u'B-KYUNG SYSTEM Co.,Ltd.',
   0x00159D: u'Minicom Advanced Systems ltd',
   0x00159E: u'Saitek plc',
   0x00159F: u'Terascala, Inc.',
   0x0015A0: u'Nokia Danmark A/S',
   0x0015A1: u'SINTERS SAS',
   0x0015A2: u'ARRIS International',
   0x0015A3: u'ARRIS International',
   0x0015A4: u'ARRIS International',
   0x0015A5: u'DCI Co., Ltd.',
   0x0015A6: u'Digital Electronics Products Ltd.',
   0x0015A7: u'Robatech AG',
   0x0015A8: u'Motorola Mobile Devices',
   0x0015A9: u'KWANG WOO I&C CO.,LTD',
   0x0015AA: u'Rextechnik International Co.,',
   0x0015AB: u'PRO CO SOUND INC',
   0x0015AC: u'Capelon AB',
   0x0015AD: u'Accedian Networks',
   0x0015AE: u'kyung il',
   0x0015AF: u'AzureWave Technologies, Inc.',
   0x0015B0: u'AUTOTELENET CO.,LTD',
   0x0015B1: u'Ambient Corporation',
   0x0015B2: u'Advanced Industrial Computer, Inc.',
   0x0015B3: u'Caretech AB',
   0x0015B4: u'Polymap  Wireless LLC',
   0x0015B5: u'CI Network Corp.',
   0x0015B6: u'ShinMaywa Industries, Ltd.',
   0x0015B7: u'Toshiba',
   0x0015B8: u'Tahoe',
   0x0015B9: u'Samsung Electronics Co., Ltd.',
   0x0015BA: u'iba AG',
   0x0015BB: u'SMA Technologie AG',
   0x0015BC: u'Develco',
   0x0015BD: u'Group 4 Technology Ltd',
   0x0015BE: u'Iqua Ltd.',
   0x0015BF: u'technicob',
   0x0015C0: u'DIGITAL TELEMEDIA CO.,LTD.',
   0x0015C1: u'SONY Computer Entertainment inc,',
   0x0015C2: u'3M Germany',
   0x0015C3: u'Ruf Telematik AG',
   0x0015C4: u'FLOVEL CO., LTD.',
   0x0015C5: u'Dell Inc',
   0x0015C6: u'Cisco Systems',
   0x0015C7: u'Cisco Systems',
   0x0015C8: u'FlexiPanel Ltd',
   0x0015C9: u'Gumstix, Inc',
   0x0015CA: u'TeraRecon, Inc.',
   0x0015CB: u'Surf Communication Solutions Ltd.',
   0x0015CC: u'TEPCO UQUEST, LTD.',
   0x0015CD: u'Exartech International Corp.',
   0x0015CE: u'ARRIS International',
   0x0015CF: u'ARRIS International',
   0x0015D0: u'ARRIS International',
   0x0015D1: u'ARRIS International',
   0x0015D2: u'Xantech Corporation',
   0x0015D3: u'Pantech&Curitel Communications, Inc.',
   0x0015D4: u'Emitor AB',
   0x0015D5: u'NICEVT',
   0x0015D6: u'OSLiNK Sp. z o.o.',
   0x0015D7: u'Reti Corporation',
   0x0015D8: u'Interlink Electronics',
   0x0015D9: u'PKC Electronics Oy',
   0x0015DA: u'IRITEL A.D.',
   0x0015DB: u'Canesta Inc.',
   0x0015DC: u'KT&C Co., Ltd.',
   0x0015DD: u'IP Control Systems Ltd.',
   0x0015DE: u'Nokia Danmark A/S',
   0x0015DF: u'Clivet S.p.A.',
   0x0015E0: u'Ericsson Mobile Platforms',
   0x0015E1: u'picoChip Designs Ltd',
   0x0015E2: u'Wissenschaftliche Geraetebau Dr. Ing. H. Knauer GmbH',
   0x0015E3: u'Dream Technologies Corporation',
   0x0015E4: u'Zimmer Elektromedizin',
   0x0015E5: u'Cheertek Inc.',
   0x0015E6: u'MOBILE TECHNIKA Inc.',
   0x0015E7: u'Quantec ProAudio',
   0x0015E8: u'Nortel',
   0x0015E9: u'D-Link Corporation',
   0x0015EA: u'Tellumat (Pty) Ltd',
   0x0015EB: u'ZTE CORPORATION',
   0x0015EC: u'Boca Devices LLC',
   0x0015ED: u'Fulcrum Microsystems, Inc.',
   0x0015EE: u'Omnex Control Systems',
   0x0015EF: u'NEC TOKIN Corporation',
   0x0015F0: u'EGO BV',
   0x0015F1: u'KYLINK Communications Corp.',
   0x0015F2: u'ASUSTek COMPUTER INC.',
   0x0015F3: u'PELTOR AB',
   0x0015F4: u'Eventide',
   0x0015F5: u'Sustainable Energy Systems',
   0x0015F6: u'SCIENCE AND ENGINEERING SERVICES, INC.',
   0x0015F7: u'Wintecronics Ltd.',
   0x0015F8: u'Kingtronics Industrial Co. Ltd.',
   0x0015F9: u'Cisco Systems',
   0x0015FA: u'Cisco Systems',
   0x0015FB: u'setex schermuly textile computer gmbh',
   0x0015FC: u'Startco Engineering Ltd.',
   0x0015FD: u'Complete Media Systems',
   0x0015FE: u'SCHILLING ROBOTICS LLC',
   0x0015FF: u'Novatel Wireless, Inc.',
   0x001600: u'CelleBrite Mobile Synchronization',
   0x001601: u'Buffalo Inc.',
   0x001602: u'CEYON TECHNOLOGY CO.,LTD.',
   0x001603: u'PRIVATE',
   0x001604: u'Sigpro',
   0x001605: u'YORKVILLE SOUND INC.',
   0x001606: u'Ideal Industries',
   0x001607: u'Curves International Inc.',
   0x001608: u'Sequans Communications',
   0x001609: u'Unitech electronics co., ltd.',
   0x00160A: u'SWEEX Europe BV',
   0x00160B: u'TVWorks LLC',
   0x00160C: u'LPL  DEVELOPMENT S.A. DE C.V',
   0x00160D: u'Be Here Corporation',
   0x00160E: u'Optica Technologies Inc.',
   0x00160F: u'BADGER METER INC',
   0x001610: u'Carina Technology',
   0x001611: u'Altecon Srl',
   0x001612: u'Otsuka Electronics Co., Ltd.',
   0x001613: u'LibreStream Technologies Inc.',
   0x001614: u'Picosecond Pulse Labs',
   0x001615: u'Nittan Company, Limited',
   0x001616: u'BROWAN COMMUNICATION INC.',
   0x001617: u'MSI',
   0x001618: u'HIVION Co., Ltd.',
   0x001619: u'La Factora de Comunicaciones Aplicadas,S.L.',
   0x00161A: u'Dametric AB',
   0x00161B: u'Micronet Corporation',
   0x00161C: u'e:cue',
   0x00161D: u'Innovative Wireless Technologies, Inc.',
   0x00161E: u'Woojinnet',
   0x00161F: u'SUNWAVETEC Co., Ltd.',
   0x001620: u'Sony Ericsson Mobile Communications AB',
   0x001621: u'Colorado Vnet',
   0x001622: u'BBH SYSTEMS GMBH',
   0x001623: u'Interval Media',
   0x001624: u'PRIVATE',
   0x001625: u'Impinj, Inc.',
   0x001626: u'Motorola CHS',
   0x001627: u'embedded-logic DESIGN AND MORE GmbH',
   0x001628: u'Ultra Electronics Manufacturing and Card Systems',
   0x001629: u'Nivus GmbH',
   0x00162A: u'Antik computers & communications s.r.o.',
   0x00162B: u'Togami Electric Mfg.co.,Ltd.',
   0x00162C: u'Xanboo',
   0x00162D: u'STNet Co., Ltd.',
   0x00162E: u'Space Shuttle Hi-Tech Co., Ltd.',
   0x00162F: u'Geutebrck GmbH',
   0x001630: u'Vativ Technologies',
   0x001631: u'Xteam',
   0x001632: u'SAMSUNG ELECTRONICS CO., LTD.',
   0x001633: u'Oxford Diagnostics Ltd.',
   0x001634: u'Mathtech, Inc.',
   0x001635: u'Hewlett Packard',
   0x001636: u'Quanta Computer Inc.',
   0x001637: u'Citel Srl',
   0x001638: u'TECOM Co., Ltd.',
   0x001639: u'UBIQUAM Co.,Ltd',
   0x00163A: u'YVES TECHNOLOGY CO., LTD.',
   0x00163B: u'VertexRSI/General Dynamics',
   0x00163C: u'Rebox B.V.',
   0x00163D: u'Tsinghua Tongfang Legend Silicon Tech. Co., Ltd.',
   0x00163E: u'Xensource, Inc.',
   0x00163F: u'CReTE SYSTEMS Inc.',
   0x001640: u'Asmobile Communication Inc.',
   0x001641: u'USI',
   0x001642: u'Pangolin',
   0x001643: u'Sunhillo Corproation',
   0x001644: u'LITE-ON Technology Corp.',
   0x001645: u'Power Distribution, Inc.',
   0x001646: u'Cisco Systems',
   0x001647: u'Cisco Systems',
   0x001648: u'SSD Company Limited',
   0x001649: u'SetOne GmbH',
   0x00164A: u'Vibration Technology Limited',
   0x00164B: u'Quorion Data Systems GmbH',
   0x00164C: u'PLANET INT Co., Ltd',
   0x00164D: u'Alcatel North America IP Division',
   0x00164E: u'Nokia Danmark A/S',
   0x00164F: u'World Ethnic Broadcastin Inc.',
   0x001650: u'EYAL MICROWAVE',
   0x001651: u'PRIVATE',
   0x001652: u'Hoatech Technologies, Inc.',
   0x001653: u'LEGO System A/S IE Electronics Division',
   0x001654: u'Flex-P Industries Sdn. Bhd.',
   0x001655: u'FUHO TECHNOLOGY Co., LTD',
   0x001656: u'Nintendo Co., Ltd.',
   0x001657: u'Aegate Ltd',
   0x001658: u'Fusiontech Technologies Inc.',
   0x001659: u'Z.M.P. RADWAG',
   0x00165A: u'Harman Specialty Group',
   0x00165B: u'Grip Audio',
   0x00165C: u'Trackflow Ltd',
   0x00165D: u'AirDefense, Inc.',
   0x00165E: u'Precision I/O',
   0x00165F: u'Fairmount Automation',
   0x001660: u'Nortel',
   0x001661: u'Novatium Solutions (P) Ltd',
   0x001662: u'Liyuh Technology Ltd.',
   0x001663: u'KBT Mobile',
   0x001664: u'Prod-El SpA',
   0x001665: u'Cellon France',
   0x001666: u'Quantier Communication Inc.',
   0x001667: u'A-TEC Subsystem INC.',
   0x001668: u'Eishin Electronics',
   0x001669: u'MRV Communication (Networks) LTD',
   0x00166A: u'TPS',
   0x00166B: u'Samsung Electronics',
   0x00166C: u'Samsung Electonics Digital Video System Division',
   0x00166D: u'Yulong Computer Telecommunication Scientific(shenzhen)Co.,Lt',
   0x00166E: u'Arbitron Inc.',
   0x00166F: u'Intel Corporation',
   0x001670: u'SKNET Corporation',
   0x001671: u'Symphox Information Co.',
   0x001672: u'Zenway enterprise ltd',
   0x001673: u'PRIVATE',
   0x001674: u'EuroCB (Phils.), Inc.',
   0x001675: u'Motorola MDb',
   0x001676: u'Intel Corporation',
   0x001677: u'Bihl+Wiedemann GmbH',
   0x001678: u'SHENZHEN BAOAN GAOKE ELECTRONICS CO., LTD',
   0x001679: u'eOn Communications',
   0x00167A: u'Skyworth Overseas Dvelopment Ltd.',
   0x00167B: u'Haver&Boecker',
   0x00167C: u'iRex Technologies BV',
   0x00167D: u'Sky-Line',
   0x00167E: u'DIBOSS.CO.,LTD',
   0x00167F: u'Bluebird Soft Inc.',
   0x001680: u'Bally Gaming + Systems',
   0x001681: u'Vector Informatik GmbH',
   0x001682: u'Pro Dex, Inc',
   0x001683: u'WEBIO International Co.,.Ltd.',
   0x001684: u'Donjin Co.,Ltd.',
   0x001685: u'FRWD Technologies Ltd.',
   0x001686: u'Karl Storz Imaging',
   0x001687: u'Chubb CSC-Vendor AP',
   0x001688: u'ServerEngines LLC',
   0x001689: u'Pilkor Electronics Co., Ltd',
   0x00168A: u'id-Confirm Inc',
   0x00168B: u'Paralan Corporation',
   0x00168C: u'DSL Partner AS',
   0x00168D: u'KORWIN CO., Ltd.',
   0x00168E: u'Vimicro corporation',
   0x00168F: u'GN Netcom as',
   0x001690: u'J-TEK INCORPORATION',
   0x001691: u'Moser-Baer AG',
   0x001692: u'Scientific-Atlanta, Inc.',
   0x001693: u'PowerLink Technology Inc.',
   0x001694: u'Sennheiser Communications A/S',
   0x001695: u'AVC Technology Limited',
   0x001696: u'QDI Technology (H.K.) Limited',
   0x001697: u'NEC Corporation',
   0x001698: u'T&A Mobile Phones SAS',
   0x001699: u'PRIVATE',
   0x00169A: u'Quadrics Ltd',
   0x00169B: u'Alstom Transport',
   0x00169C: u'Cisco Systems',
   0x00169D: u'Cisco Systems',
   0x00169E: u'TV One Ltd',
   0x00169F: u'Vimtron Electronics Co., Ltd.',
   0x0016A0: u'Auto-Maskin',
   0x0016A1: u'3Leaf Networks',
   0x0016A2: u'CentraLite Systems, Inc.',
   0x0016A3: u'TEAM ARTECHE, S.A.',
   0x0016A4: u'Ezurio Ltd',
   0x0016A5: u'Tandberg Storage ASA',
   0x0016A6: u'Dovado FZ-LLC',
   0x0016A7: u'AWETA G&P',
   0x0016A8: u'CWT CO., LTD.',
   0x0016A9: u'2EI',
   0x0016AA: u'Kei Communication Technology Inc.',
   0x0016AB: u'PBI-Dansensor A/S',
   0x0016AC: u'Toho Technology Corp.',
   0x0016AD: u'BT-Links Company Limited',
   0x0016AE: u'INVENTEL',
   0x0016AF: u'Shenzhen Union Networks Equipment Co.,Ltd.',
   0x0016B0: u'VK Corporation',
   0x0016B1: u'KBS',
   0x0016B2: u'DriveCam Inc',
   0x0016B3: u'Photonicbridges (China) Co., Ltd.',
   0x0016B4: u'PRIVATE',
   0x0016B5: u'Motorola CHS',
   0x0016B6: u'Cisco-Linksys',
   0x0016B7: u'Seoul Commtech',
   0x0016B8: u'Sony Ericsson Mobile Communications',
   0x0016B9: u'ProCurve Networking',
   0x0016BA: u'WEATHERNEWS INC.',
   0x0016BB: u'Law-Chain Computer Technology Co Ltd',
   0x0016BC: u'Nokia Danmark A/S',
   0x0016BD: u'ATI Industrial Automation',
   0x0016BE: u'INFRANET, Inc.',
   0x0016BF: u'PaloDEx Group Oy',
   0x0016C0: u'Semtech Corporation',
   0x0016C1: u'Eleksen Ltd',
   0x0016C2: u'Avtec Systems Inc',
   0x0016C3: u'BA Systems Inc',
   0x0016C4: u'SiRF Technology, Inc.',
   0x0016C5: u'Shenzhen Xing Feng Industry Co.,Ltd',
   0x0016C6: u'North Atlantic Industries',
   0x0016C7: u'Cisco Systems',
   0x0016C8: u'Cisco Systems',
   0x0016C9: u'NAT Seattle, Inc.',
   0x0016CA: u'Nortel',
   0x0016CB: u'Apple Computer',
   0x0016CC: u'Xcute Mobile Corp.',
   0x0016CD: u'HIJI HIGH-TECH CO., LTD.',
   0x0016CE: u'Hon Hai Precision Ind. Co., Ltd.',
   0x0016CF: u'Hon Hai Precision Ind. Co., Ltd.',
   0x0016D0: u'ATech elektronika d.o.o.',
   0x0016D1: u'ZAT a.s.',
   0x0016D2: u'Caspian',
   0x0016D3: u'Wistron Corporation',
   0x0016D4: u'Compal Communications, Inc.',
   0x0016D5: u'Synccom Co., Ltd',
   0x0016D6: u'TDA Tech Pty Ltd',
   0x0016D7: u'Sunways AG',
   0x0016D8: u'Senea AB',
   0x0016D9: u'NINGBO BIRD CO.,LTD.',
   0x0016DA: u'Futronic Technology Co. Ltd.',
   0x0016DB: u'Samsung Electronics Co., Ltd.',
   0x0016DC: u'ARCHOS',
   0x0016DD: u'Gigabeam Corporation',
   0x0016DE: u'FAST Inc',
   0x0016DF: u'Lundinova AB',
   0x0016E0: u'3Com Europe Ltd',
   0x0016E1: u'SiliconStor, Inc.',
   0x0016E2: u'American Fibertek, Inc.',
   0x0016E3: u'ASKEY COMPUTER CORP.',
   0x0016E4: u'VANGUARD SECURITY ENGINEERING CORP.',
   0x0016E5: u'FORDLEY DEVELOPMENT LIMITED',
   0x0016E6: u'GIGA-BYTE TECHNOLOGY CO.,LTD.',
   0x0016E7: u'Dynamix Promotions Limited',
   0x0016E8: u'Sigma Designs, Inc.',
   0x0016E9: u'Tiba Medical Inc',
   0x0016EA: u'Intel Corporation',
   0x0016EB: u'Intel Corporation',
   0x0016EC: u'Elitegroup Computer Systems Co., Ltd.',
   0x0016ED: u'Integrian, Inc.',
   0x0016EE: u'RoyalDigital Inc.',
   0x0016EF: u'Koko Fitness, Inc.',
   0x0016F0: u'Zermatt Systems, Inc',
   0x0016F1: u'OmniSense, LLC',
   0x0016F2: u'Dmobile System Co., Ltd.',
   0x0016F3: u'CAST Information Co., Ltd',
   0x0016F4: u'Eidicom Co., Ltd.',
   0x0016F5: u'Dalian Golden Hualu Digital Technology Co.,Ltd',
   0x0016F6: u'Video Products Group',
   0x0016F7: u'L-3 Communications, Electrodynamics, Inc.',
   0x0016F8: u'AVIQTECH TECHNOLOGY CO., LTD.',
   0x0016F9: u'CETRTA POT, d.o.o., Kranj',
   0x0016FA: u'ECI Telecom Ltd.',
   0x0016FB: u'SHENZHEN MTC CO.,LTD.',
   0x0016FC: u'TOHKEN CO.,LTD.',
   0x0016FD: u'Jaty Electronics',
   0x0016FE: u'Alps Electric Co., Ltd',
   0x0016FF: u'Wamin Optocomm Mfg Corp',
   0x001700: u'Motorola MDb',
   0x001701: u'KDE, Inc.',
   0x001702: u'Osung Midicom Co., Ltd',
   0x001703: u'MOSDAN Internation Co.,Ltd',
   0x001704: u'Shinco Electronics Group Co.,Ltd',
   0x001705: u'Methode Electronics',
   0x001706: u'Techfaith Wireless Communication Technology Limited.',
   0x001707: u'InGrid, Inc',
   0x001708: u'Hewlett Packard',
   0x001709: u'Exalt Communications',
   0x00170A: u'INEW DIGITAL COMPANY',
   0x00170B: u'Contela, Inc.',
   0x00170C: u'Benefon Oyj',
   0x00170D: u'Dust Networks Inc.',
   0x00170E: u'Cisco Systems',
   0x00170F: u'Cisco Systems',
   0x001710: u'Casa Systems Inc.',
   0x001711: u'GE Healthcare Bio-Sciences AB',
   0x001712: u'ISCO International',
   0x001713: u'Tiger NetCom',
   0x001714: u'BR Controls Nederland bv',
   0x001715: u'Qstik',
   0x001716: u'Qno Technology Inc.',
   0x001717: u'Leica Geosystems AG',
   0x001718: u'Vansco Electronics Oy',
   0x001719: u'AudioCodes USA, Inc',
   0x00171A: u'Winegard Company',
   0x00171B: u'Innovation Lab Corp.',
   0x00171C: u'NT MicroSystems, Inc.',
   0x00171D: u'DIGIT',
   0x00171E: u'Theo Benning GmbH & Co. KG',
   0x00171F: u'IMV Corporation',
   0x001720: u'Image Sensing Systems, Inc.',
   0x001721: u'FITRE S.p.A.',
   0x001722: u'Hanazeder Electronic GmbH',
   0x001723: u'Summit Data Communications',
   0x001724: u'Studer Professional Audio GmbH',
   0x001725: u'Liquid Computing',
   0x001726: u'm2c Electronic Technology Ltd.',
   0x001727: u'Thermo Ramsey Italia s.r.l.',
   0x001728: u'Selex Communications',
   0x001729: u'Ubicod Co.LTD',
   0x00172A: u'Proware Technology Corp.',
   0x00172B: u'Global Technologies Inc.',
   0x00172C: u'TAEJIN INFOTECH',
   0x00172D: u'Axcen Photonics Corporation',
   0x00172E: u'FXC Inc.',
   0x00172F: u'NeuLion Incorporated',
   0x001730: u'Automation Electronics',
   0x001731: u'ASUSTek COMPUTER INC.',
   0x001732: u'Science-Technical Center "RISSA"',
   0x001733: u'neuf cegetel',
   0x001734: u'LGC Wireless Inc.',
   0x001735: u'PRIVATE',
   0x001736: u'iiTron Inc.',
   0x001737: u'Industrie Dial Face S.p.A.',
   0x001738: u'XIV',
   0x001739: u'Bright Headphone Electronics Company',
   0x00173A: u'Edge Integration Systems Inc.',
   0x00173B: u'Arched Rock Corporation',
   0x00173C: u'Extreme Engineering Solutions',
   0x00173D: u'Neology',
   0x00173E: u'LeucotronEquipamentos Ltda.',
   0x00173F: u'Belkin Corporation',
   0x001740: u'Technologies Labtronix',
   0x001741: u'DEFIDEV',
   0x001742: u'FUJITSU LIMITED',
   0x001743: u'Deck Srl',
   0x001744: u'Araneo Ltd.',
   0x001745: u'INNOTZ CO., Ltd',
   0x001746: u'Freedom9 Inc.',
   0x001747: u'Trimble',
   0x001748: u'Neokoros Brasil Ltda',
   0x001749: u'HYUNDAE YONG-O-SA CO.,LTD',
   0x00174A: u'SOCOMEC',
   0x00174B: u'Nokia Danmark A/S',
   0x00174C: u'Millipore',
   0x00174D: u'DYNAMIC NETWORK FACTORY, INC.',
   0x00174E: u'Parama-tech Co.,Ltd.',
   0x00174F: u'iCatch Inc.',
   0x001750: u'GSI Group, MicroE Systems',
   0x001751: u'Online Corporation',
   0x001752: u'DAGS, Inc',
   0x001753: u'nFore Technology Inc.',
   0x001754: u'Arkino Corporation., Ltd',
   0x001755: u'GE Security',
   0x001756: u'Vinci Labs Oy',
   0x001757: u'RIX TECHNOLOGY LIMITED',
   0x001758: u'ThruVision Ltd',
   0x001759: u'Cisco Systems',
   0x00175A: u'Cisco Systems',
   0x00175B: u'ACS Solutions Switzerland Ltd.',
   0x00175C: u'SHARP CORPORATION',
   0x00175D: u'Dongseo system.',
   0x00175E: u'Anta Systems, Inc.',
   0x00175F: u'XENOLINK Communications Co., Ltd.',
   0x001760: u'Naito Densei Machida MFG.CO.,LTD',
   0x001761: u'ZKSoftware Inc.',
   0x001762: u'Solar Technology, Inc.',
   0x001763: u'Essentia S.p.A.',
   0x001764: u'ATMedia GmbH',
   0x001765: u'Nortel',
   0x001766: u'Accense Technology, Inc.',
   0x001767: u'Earforce AS',
   0x001768: u'Zinwave Ltd',
   0x001769: u'Cymphonix Corp',
   0x00176A: u'Avago Technologies',
   0x00176B: u'Kiyon, Inc.',
   0x00176C: u'Pivot3, Inc.',
   0x00176D: u'CORE CORPORATION',
   0x00176E: u'DUCATI SISTEMI',
   0x00176F: u'PAX Computer Technology(Shenzhen) Ltd.',
   0x001770: u'Arti Industrial Electronics Ltd.',
   0x001771: u'APD Communications Ltd',
   0x001772: u'ASTRO Strobel Kommunikationssysteme GmbH',
   0x001773: u'Laketune Technologies Co. Ltd',
   0x001774: u'Elesta GmbH',
   0x001775: u'TTE Germany GmbH',
   0x001776: u'Meso Scale Diagnostics, LLC',
   0x001777: u'Obsidian Research Corporation',
   0x001778: u'Central Music Co.',
   0x001779: u'QuickTel',
   0x00177A: u'ASSA ABLOY AB',
   0x00177B: u'Azalea Networks inc',
   0x00177C: u'D-Link India Ltd',
   0x00177D: u'IDT International Limited',
   0x00177E: u'Meshcom Technologies Inc.',
   0x00177F: u'Worldsmart Retech',
   0x001780: u'Applera Holding B.V. Singapore Operations',
   0x001781: u'Greystone Data System, Inc.',
   0x001782: u'LoBenn Inc.',
   0x001783: u'Texas Instruments',
   0x001784: u'Motorola Mobile Devices',
   0x001785: u'Sparr Electronics Ltd',
   0x001786: u'wisembed',
   0x001787: u'Brother, Brother & Sons ApS',
   0x001788: u'Philips Lighting BV',
   0x001789: u'Zenitron Corporation',
   0x00178A: u'DARTS TECHNOLOGIES CORP.',
   0x00178B: u'Teledyne Technologies Incorporated',
   0x00178C: u'Independent Witness, Inc',
   0x00178D: u'Checkpoint Systems, Inc.',
   0x00178E: u'Gunnebo Cash Automation AB',
   0x00178F: u'NINGBO YIDONG ELECTRONIC CO.,LTD.',
   0x001790: u'HYUNDAI DIGITECH Co, Ltd.',
   0x001791: u'LinTech GmbH',
   0x001792: u'Falcom Wireless Comunications Gmbh',
   0x001793: u'Tigi Corporation',
   0x001794: u'Cisco Systems',
   0x001795: u'Cisco Systems',
   0x001796: u'Rittmeyer AG',
   0x001797: u'Telsy Elettronica S.p.A.',
   0x001798: u'Azonic Technology Co., LTD',
   0x001799: u'SmarTire Systems Inc.',
   0x00179A: u'D-Link Corporation',
   0x00179B: u'Chant Sincere CO., LTD.',
   0x00179C: u'DEPRAG SCHULZ GMBH u. CO.',
   0x00179D: u'Kelman Limited',
   0x00179E: u'Sirit Inc',
   0x00179F: u'Apricorn',
   0x0017A0: u'RoboTech srl',
   0x0017A1: u'3soft inc.',
   0x0017A2: u'Camrivox Ltd.',
   0x0017A3: u'MIX s.r.l.',
   0x0017A4: u'Global Data Services',
   0x0017A5: u'TrendChip Technologies Corp.',
   0x0017A6: u'YOSIN ELECTRONICS CO., LTD.',
   0x0017A7: u'Mobile Computing Promotion Consortium',
   0x0017A8: u'EDM Corporation',
   0x0017A9: u'Sentivision',
   0x0017AA: u'elab-experience inc.',
   0x0017AB: u'Nintendo Co., Ltd.',
   0x0017AC: u'O\'Neil Product Development Inc.',
   0x0017AD: u'AceNet Corporation',
   0x0017AE: u'GAI-Tronics',
   0x0017AF: u'Enermet',
   0x0017B0: u'Nokia Danmark A/S',
   0x0017B1: u'ACIST Medical Systems, Inc.',
   0x0017B2: u'SK Telesys',
   0x0017B3: u'Aftek Infosys Limited',
   0x0017B4: u'Remote Security Systems, LLC',
   0x0017B5: u'Peerless Systems Corporation',
   0x0017B6: u'Aquantia',
   0x0017B7: u'Tonze Technology Co.',
   0x0017B8: u'NOVATRON CO., LTD.',
   0x0017B9: u'Gambro Lundia AB',
   0x0017BA: u'SEDO CO., LTD.',
   0x0017BB: u'Syrinx Industrial Electronics',
   0x0017BC: u'Touchtunes Music Corporation',
   0x0017BD: u'Tibetsystem',
   0x0017BE: u'Tratec Telecom B.V.',
   0x0017BF: u'Coherent Research Limited',
   0x0017C0: u'PureTech Systems, Inc.',
   0x0017C1: u'CM Precision Technology LTD.',
   0x0017C2: u'Pirelli Broadband Solutions',
   0x0017C3: u'KTF Technologies Inc.',
   0x0017C4: u'Quanta Microsystems, INC.',
   0x0017C5: u'SonicWALL',
   0x0017C6: u'Labcal Technologies',
   0x0017C7: u'MARA Systems Consulting AB',
   0x0017C8: u'Kyocera Mita Corporation',
   0x0017C9: u'Samsung Electronics Co., Ltd.',
   0x0017CA: u'BenQ Corporation',
   0x0017CB: u'Juniper Networks',
   0x0017CC: u'Alcatel USA Sourcing LP',
   0x0017CD: u'CEC Wireless R&D Ltd.',
   0x0017CE: u'MB International Telecom Labs srl',
   0x0017CF: u'iMCA-GmbH',
   0x0017D0: u'Opticom Communications, LLC',
   0x0017D1: u'Nortel',
   0x0017D2: u'THINLINX PTY LTD',
   0x0017D3: u'Etymotic Research, Inc.',
   0x0017D4: u'Monsoon Multimedia, Inc',
   0x0017D5: u'Samsung Electronics Co., Ltd.',
   0x0017D6: u'Bluechips Microhouse Co.,Ltd.',
   0x0017D7: u'Input/Output Inc.',
   0x0017D8: u'Magnum Semiconductor, Inc.',
   0x0017D9: u'AAI Corporation',
   0x0017DA: u'Spans Logic',
   0x0017DB: u'PRIVATE',
   0x0017DC: u'DAEMYUNG ZERO1',
   0x0017DD: u'Clipsal Australia',
   0x0017DE: u'Advantage Six Ltd',
   0x0017DF: u'Cisco Systems',
   0x0017E0: u'Cisco Systems',
   0x0017E1: u'DACOS Technologies Co., Ltd.',
   0x0017E2: u'Motorola Mobile Devices',
   0x0017E3: u'Texas Instruments',
   0x0017E4: u'Texas Instruments',
   0x0017E5: u'Texas Instruments',
   0x0017E6: u'Texas Instruments',
   0x0017E7: u'Texas Instruments',
   0x0017E8: u'Texas Instruments',
   0x0017E9: u'Texas Instruments',
   0x0017EA: u'Texas Instruments',
   0x0017EB: u'Texas Instruments',
   0x0017EC: u'Texas Instruments',
   0x0017ED: u'WooJooIT Ltd.',
   0x0017EE: u'Motorola CHS',
   0x0017EF: u'Blade Network Technologies, Inc.',
   0x0017F0: u'SZCOM Broadband Network Technology Co.,Ltd',
   0x0017F1: u'Renu Electronics Pvt Ltd',
   0x0017F2: u'Apple Computer',
   0x0017F3: u'M/A-COM Wireless Systems',
   0x0017F4: u'ZERON ALLIANCE',
   0x0017F5: u'NEOPTEK',
   0x0017F6: u'Pyramid Meriden Inc.',
   0x0017F7: u'CEM Solutions Pvt Ltd',
   0x0017F8: u'Motech Industries Inc.',
   0x0017F9: u'Forcom Sp. z o.o.',
   0x0017FA: u'Microsoft Corporation',
   0x0017FB: u'FA',
   0x0017FC: u'Suprema Inc.',
   0x0017FD: u'Amulet Hotkey',
   0x0017FE: u'TALOS SYSTEM INC.',
   0x0017FF: u'PLAYLINE Co.,Ltd.',
   0x001800: u'UNIGRAND LTD',
   0x001801: u'Actiontec Electronics, Inc',
   0x001802: u'Alpha Networks Inc.',
   0x001803: u'ArcSoft Shanghai Co. LTD',
   0x001804: u'E-TEK DIGITAL TECHNOLOGY LIMITED',
   0x001805: u'Beijing InHand Networking',
   0x001806: u'Hokkei Industries Co., Ltd.',
   0x001807: u'Fanstel Corp.',
   0x001808: u'SightLogix, Inc.',
   0x001809: u'CRESYN',
   0x00180A: u'Meraki Networks, Inc.',
   0x00180B: u'Brilliant Telecommunications',
   0x00180C: u'Optelian Access Networks Corporation',
   0x00180D: u'Terabytes Server Storage Tech Corp',
   0x00180E: u'Avega Systems',
   0x00180F: u'Nokia Danmark A/S',
   0x001810: u'IPTrade S.A.',
   0x001811: u'Neuros Technology International, LLC.',
   0x001812: u'Beijing Xinwei Telecom Technology Co., Ltd.',
   0x001813: u'Sony Ericsson Mobile Communications',
   0x001814: u'Mitutoyo Corporation',
   0x001815: u'GZ Technologies, Inc.',
   0x001816: u'Ubixon Co., Ltd.',
   0x001817: u'D. E. Shaw Research, LLC',
   0x001818: u'Cisco Systems',
   0x001819: u'Cisco Systems',
   0x00181A: u'AVerMedia Technologies Inc.',
   0x00181B: u'TaiJin Metal Co., Ltd.',
   0x00181C: u'Exterity Limited',
   0x00181D: u'ASIA ELECTRONICS CO.,LTD',
   0x00181E: u'GDX Technologies Ltd.',
   0x00181F: u'Palmmicro Communications',
   0x001820: u'w5networks',
   0x001821: u'SINDORICOH',
   0x001822: u'CEC TELECOM CO.,LTD.',
   0x001823: u'Delta Electronics, Inc.',
   0x001824: u'Kimaldi Electronics, S.L.',
   0x001825: u'Wavion LTD',
   0x001826: u'Cale Access AB',
   0x001827: u'NEC PHILIPS UNIFIED SOLUTIONS NEDERLAND BV',
   0x001828: u'e2v technologies (UK) ltd.',
   0x001829: u'Gatsometer',
   0x00182A: u'Taiwan Video & Monitor',
   0x00182B: u'Softier',
   0x00182C: u'Ascend Networks, Inc.',
   0x00182D: u'Artec Group O',
   0x00182E: u'Wireless Ventures USA',
   0x00182F: u'Texas Instruments',
   0x001830: u'Texas Instruments',
   0x001831: u'Texas Instruments',
   0x001832: u'Texas Instruments',
   0x001833: u'Texas Instruments',
   0x001834: u'Texas Instruments',
   0x001835: u'ITC',
   0x001836: u'Reliance Electric Limited',
   0x001837: u'Universal ABIT Co., Ltd.',
   0x001838: u'PanAccess Communications,Inc.',
   0x001839: u'Cisco-Linksys LLC',
   0x00183A: u'Westell Technologies',
   0x00183B: u'CENITS Co., Ltd.',
   0x00183C: u'Encore Software Limited',
   0x00183D: u'Vertex Link Corporation',
   0x00183E: u'Digilent, Inc',
   0x00183F: u'2Wire, Inc',
   0x001840: u'3 Phoenix, Inc.',
   0x001841: u'High Tech Computer Corp',
   0x001842: u'Nokia Danmark A/S',
   0x001843: u'Dawevision Ltd',
   0x001844: u'Heads Up Technologies, Inc.',
   0x001845: u'NPL Pulsar Ltd.',
   0x001846: u'Crypto S.A.',
   0x001847: u'AceNet Technology Inc.',
   0x001848: u'Vecima Networks Inc.',
   0x001849: u'Pigeon Point Systems',
   0x00184A: u'Catcher, Inc.',
   0x00184B: u'Las Vegas Gaming, Inc.',
   0x00184C: u'Bogen Communications',
   0x00184D: u'Netgear Inc.',
   0x00184E: u'Lianhe Technologies, Inc.',
   0x00184F: u'8 Ways Technology Corp.',
   0x001850: u'Secfone Kft',
   0x001851: u'SWsoft',
   0x001852: u'StorLink Semiconductors, Inc.',
   0x001853: u'Atera Networks LTD.',
   0x001854: u'Argard Co., Ltd',
   0x001855: u'Aeromaritime Systembau GmbH',
   0x001856: u'EyeFi, Inc',
   0x001857: u'Unilever R&D',
   0x001858: u'TagMaster AB',
   0x001859: u'Strawberry Linux Co.,Ltd.',
   0x00185A: u'uControl, Inc.',
   0x00185B: u'Network Chemistry, Inc',
   0x00185C: u'EDS Lab Pte Ltd',
   0x00185D: u'TAIGUEN TECHNOLOGY (SHEN-ZHEN) CO., LTD.',
   0x00185E: u'Nexterm Inc.',
   0x00185F: u'TAC Inc.',
   0x001860: u'SIM Technology Group Shanghai Simcom Ltd.,',
   0x001861: u'Ooma, Inc.',
   0x001862: u'Seagate Technology',
   0x001863: u'Veritech Electronics Limited',
   0x001864: u'Cybectec Inc.',
   0x001865: u'Bayer Diagnostics Sudbury Ltd',
   0x001866: u'Leutron Vision',
   0x001867: u'Evolution Robotics Retail',
   0x001868: u'Scientific Atlanta, A Cisco Company',
   0x001869: u'KINGJIM',
   0x00186A: u'Global Link Digital Technology Co,.LTD',
   0x00186B: u'Sambu Communics CO., LTD.',
   0x00186C: u'Neonode AB',
   0x00186D: u'Zhenjiang Sapphire Electronic Industry CO.',
   0x00186E: u'3COM Europe Ltd',
   0x00186F: u'Setha Industria Eletronica LTDA',
   0x001870: u'E28 Shanghai Limited',
   0x001871: u'Global Data Services',
   0x001872: u'Expertise Engineering',
   0x001873: u'Cisco Systems',
   0x001874: u'Cisco Systems',
   0x001875: u'AnaCise Testnology Pte Ltd',
   0x001876: u'WowWee Ltd.',
   0x001877: u'Amplex A/S',
   0x001878: u'Mackware GmbH',
   0x001879: u'dSys',
   0x00187A: u'Wiremold',
   0x00187B: u'4NSYS Co. Ltd.',
   0x00187C: u'INTERCROSS, LLC',
   0x00187D: u'Armorlink shanghai Co. Ltd',
   0x00187E: u'RGB Spectrum',
   0x00187F: u'ZODIANET',
   0x001880: u'Mobilygen',
   0x001881: u'Buyang Electronics Industrial Co., Ltd',
   0x001882: u'Huawei Technologies Co., Ltd.',
   0x001883: u'FORMOSA21 INC.',
   0x001884: u'FON',
   0x001885: u'Avigilon Corporation',
   0x001886: u'EL-TECH, INC.',
   0x001887: u'Metasystem SpA',
   0x001888: u'GOTIVE a.s.',
   0x001889: u'WinNet Solutions Limited',
   0x00188A: u'Infinova LLC',
   0x00188B: u'Dell',
   0x00188C: u'Mobile Action Technology Inc.',
   0x00188D: u'Nokia Danmark A/S',
   0x00188E: u'Ekahau, Inc.',
   0x00188F: u'Montgomery Technology, Inc.',
   0x001890: u'RadioCOM, s.r.o.',
   0x001891: u'Zhongshan General K-mate Electronics Co., Ltd',
   0x001892: u'ads-tec GmbH',
   0x001893: u'SHENZHEN PHOTON BROADBAND TECHNOLOGY CO.,LTD',
   0x001894: u'zimocom',
   0x001895: u'Hansun Technologies Inc.',
   0x001896: u'Great Well Electronic LTD',
   0x001897: u'JESS-LINK PRODUCTS Co., LTD',
   0x001898: u'KINGSTATE ELECTRONICS CORPORATION',
   0x001899: u'ShenZhen jieshun Science&Technology Industry CO,LTD.',
   0x00189A: u'HANA Micron Inc.',
   0x00189B: u'Thomson Inc.',
   0x00189C: u'Weldex Corporation',
   0x00189D: u'Navcast Inc.',
   0x00189E: u'OMNIKEY GmbH.',
   0x00189F: u'Lenntek Corporation',
   0x0018A0: u'Cierma Ascenseurs',
   0x0018A1: u'Tiqit Computers, Inc.',
   0x0018A2: u'XIP Technology AB',
   0x0018A3: u'ZIPPY TECHNOLOGY CORP.',
   0x0018A4: u'Motorola Mobile Devices',
   0x0018A5: u'ADigit Technologies Corp.',
   0x0018A6: u'Persistent Systems, LLC',
   0x0018A7: u'Yoggie Security Systems LTD.',
   0x0018A8: u'AnNeal Technology Inc.',
   0x0018A9: u'Ethernet Direct Corporation',
   0x0018AA: u'PRIVATE',
   0x0018AB: u'BEIJING LHWT MICROELECTRONICS INC.',
   0x0018AC: u'Shanghai Jiao Da HISYS Technology Co. Ltd.',
   0x0018AD: u'NIDEC SANKYO CORPORATION',
   0x0018AE: u'Tongwei Video Technology CO.,LTD',
   0x0018AF: u'Samsung Electronics Co., Ltd.',
   0x0018B0: u'Nortel',
   0x0018B1: u'Blade Network Technologies',
   0x0018B2: u'ADEUNIS RF',
   0x0018B3: u'TEC WizHome Co., Ltd.',
   0x0018B4: u'Dawon Media Inc.',
   0x0018B5: u'Magna Carta',
   0x0018B6: u'S3C, Inc.',
   0x0018B7: u'D3 LED, LLC',
   0x0018B8: u'New Voice International AG',
   0x0018B9: u'Cisco Systems',
   0x0018BA: u'Cisco Systems',
   0x0018BB: u'Eliwell Controls srl',
   0x0018BC: u'ZAO NVP Bolid',
   0x0018BD: u'SHENZHEN DVBWORLD TECHNOLOGY CO., LTD.',
   0x0018BE: u'ANSA Corporation',
   0x0018BF: u'Essence Technology Solution, Inc.',
   0x0018C0: u'Motorola CHS',
   0x0018C1: u'Almitec Informtica e Comrcio Ltda.',
   0x0018C2: u'Firetide, Inc',
   0x0018C3: u'C&S Microwave',
   0x0018C4: u'Raba Technologies LLC',
   0x0018C5: u'Nokia Danmark A/S',
   0x0018C6: u'OPW Fuel Management Systems',
   0x0018C7: u'Real Time Automation',
   0x0018C8: u'ISONAS Inc.',
   0x0018C9: u'EOps Technology Limited',
   0x0018CA: u'Viprinet GmbH',
   0x0018CB: u'Tecobest Technology Limited',
   0x0018CC: u'AXIOHM SAS',
   0x0018CD: u'Erae Electronics Industry Co., Ltd',
   0x0018CE: u'Dreamtech Co., Ltd',
   0x0018CF: u'Baldor Electric Company',
   0x0018D0: u'@ROAD Inc',
   0x0018D1: u'Siemens Home & Office Comm. Devices',
   0x0018D2: u'High-Gain Antennas LLC',
   0x0018D3: u'TEAMCAST',
   0x0018D4: u'Unified Display Interface SIG',
   0x0018D5: u'REIGNCOM',
   0x0018D6: u'Swirlnet A/S',
   0x0018D7: u'Javad Navigation Systems Inc.',
   0x0018D8: u'ARCH METER Corporation',
   0x0018D9: u'Santosha Internatonal, Inc',
   0x0018DA: u'AMBER wireless GmbH',
   0x0018DB: u'EPL Technology Ltd',
   0x0018DC: u'Prostar Co., Ltd.',
   0x0018DD: u'Silicondust Engineering Ltd',
   0x0018DE: u'Intel Corporation',
   0x0018DF: u'The Morey Corporation',
   0x0018E0: u'ANAVEO',
   0x0018E1: u'Verkerk Service Systemen',
   0x0018E2: u'Topdata Sistemas de Automacao Ltda',
   0x0018E3: u'Visualgate Systems, Inc.',
   0x0018E4: u'YIGUANG',
   0x0018E5: u'Adhoco AG',
   0x0018E6: u'Computer Hardware Design SIA',
   0x0018E7: u'Cameo Communications, INC.',
   0x0018E8: u'Hacetron Corporation',
   0x0018E9: u'Numata Corporation',
   0x0018EA: u'Alltec GmbH',
   0x0018EB: u'BroVis Wireless Networks',
   0x0018EC: u'Welding Technology Corporation',
   0x0018ED: u'ACCUTECH INTERNATIONAL CO., LTD.',
   0x0018EE: u'Videology Imaging Solutions, Inc.',
   0x0018EF: u'Escape Communications, Inc.',
   0x0018F0: u'JOYTOTO Co., Ltd.',
   0x0018F1: u'Chunichi Denshi Co.,LTD.',
   0x0018F2: u'Beijing Tianyu Communication Equipment Co., Ltd',
   0x0018F3: u'ASUSTek COMPUTER INC.',
   0x0018F4: u'EO TECHNICS Co., Ltd.',
   0x0018F5: u'Shenzhen Streaming Video Technology Company Limited',
   0x0018F6: u'Thomson Telecom Belgium',
   0x0018F7: u'Kameleon Technologies',
   0x0018F8: u'Cisco-Linksys LLC',
   0x0018F9: u'VVOND, Inc.',
   0x0018FA: u'Yushin Precision Equipment Co.,Ltd.',
   0x0018FB: u'Compro Technology',
   0x0018FC: u'Altec Electronic AG',
   0x0018FD: u'Optimal Technologies International Inc.',
   0x0018FE: u'Hewlett Packard',
   0x0018FF: u'PowerQuattro Co.',
   0x001900: u'Intelliverese - DBA Voicecom',
   0x001901: u'F1MEDIA',
   0x001902: u'Cambridge Consultants Ltd',
   0x001903: u'Bigfoot Networks Inc',
   0x001904: u'WB Electronics Sp. z o.o.',
   0x001905: u'SCHRACK Seconet AG',
   0x001906: u'Cisco Systems',
   0x001907: u'Cisco Systems',
   0x001908: u'Duaxes Corporation',
   0x001909: u'Devi A/S',
   0x00190A: u'HASWARE INC.',
   0x00190B: u'Southern Vision Systems, Inc.',
   0x00190C: u'Encore Electronics, Inc.',
   0x00190D: u'IEEE 1394c',
   0x00190E: u'Atech Technology Co., Ltd.',
   0x00190F: u'Advansus Corp.',
   0x001910: u'Knick Elektronische Messgeraete GmbH & Co. KG',
   0x001911: u'Just In Mobile Information Technologies (Shanghai) Co., Ltd.',
   0x001912: u'Welcat Inc',
   0x001913: u'Chuang-Yi Network Equipment Co.Ltd.',
   0x001914: u'Winix Co., Ltd',
   0x001915: u'TECOM Co., Ltd.',
   0x001916: u'PayTec AG',
   0x001917: u'Posiflex Inc.',
   0x001918: u'Interactive Wear AG',
   0x001919: u'ASTEL Inc.',
   0x00191A: u'IRLINK',
   0x00191B: u'Sputnik Engineering AG',
   0x00191C: u'Sensicast Systems',
   0x00191D: u'Nintendo Co.,Ltd.',
   0x00191E: u'Beyondwiz Co., Ltd.',
   0x00191F: u'Microlink communications Inc.',
   0x001920: u'KUME electric Co.,Ltd.',
   0x001921: u'Elitegroup Computer System Co.',
   0x001922: u'CM Comandos Lineares',
   0x001923: u'Phonex Korea Co., LTD.',
   0x001924: u'LBNL  Engineering',
   0x001925: u'Intelicis Corporation',
   0x001926: u'BitsGen Co., Ltd.',
   0x001927: u'ImCoSys Ltd',
   0x001928: u'Siemens AG, Transportation Systems',
   0x001929: u'2M2B Montadora de Maquinas Bahia Brasil LTDA',
   0x00192A: u'Antiope Associates',
   0x00192B: u'Hexagram, Inc.',
   0x00192C: u'Motorola Mobile Devices',
   0x00192D: u'Nokia Corporation',
   0x00192E: u'Spectral Instruments, Inc.',
   0x00192F: u'Cisco Systems',
   0x001930: u'Cisco Systems',
   0x001931: u'Balluff GmbH',
   0x001932: u'Gude Analog- und Digialsysteme GmbH',
   0x001933: u'Strix Systems, Inc.',
   0x001934: u'TRENDON TOUCH TECHNOLOGY CORP.',
   0x001935: u'Duerr Dental GmbH & Co. KG',
   0x001936: u'STERLITE OPTICAL TECHNOLOGIES LIMITED',
   0x001937: u'CommerceGuard AB',
   0x001938: u'UMB Communications Co., Ltd.',
   0x001939: u'Gigamips',
   0x00193A: u'OESOLUTIONS',
   0x00193B: u'Deliberant LLC',
   0x00193C: u'HighPoint Technologies Incorporated',
   0x00193D: u'GMC Guardian Mobility Corp.',
   0x00193E: u'PIRELLI BROADBAND SOLUTIONS',
   0x00193F: u'RDI technology(Shenzhen) Co.,LTD',
   0x001940: u'Rackable Systems',
   0x001941: u'Pitney Bowes, Inc',
   0x001942: u'ON SOFTWARE INTERNATIONAL LIMITED',
   0x001943: u'Belden',
   0x001944: u'Fossil Partners, L.P.',
   0x001945: u'Ten-Tec Inc.',
   0x001946: u'Cianet Industria e Comercio S/A',
   0x001947: u'Scientific Atlanta, A Cisco Company',
   0x001948: u'AireSpider Networks',
   0x001949: u'TENTEL  COMTECH CO., LTD.',
   0x00194A: u'TESTO AG',
   0x00194B: u'SAGEM COMMUNICATION',
   0x00194C: u'Fujian Stelcom information & Technology CO.,Ltd',
   0x00194D: u'Avago Technologies Sdn Bhd',
   0x00194E: u'Ultra Electronics - TCS (Tactical Communication Systems)',
   0x00194F: u'Nokia Danmark A/S',
   0x001950: u'Harman Multimedia',
   0x001951: u'NETCONS, s.r.o.',
   0x001952: u'ACOGITO Co., Ltd',
   0x001953: u'Chainleader Communications Corp.',
   0x001954: u'Leaf Corporation.',
   0x001955: u'Cisco Systems',
   0x001956: u'Cisco Systems',
   0x001957: u'Saafnet Canada Inc.',
   0x001958: u'Bluetooth SIG, Inc.',
   0x001959: u'Staccato Communications Inc.',
   0x00195A: u'Jenaer Antriebstechnik GmbH',
   0x00195B: u'D-Link Corporation',
   0x00195C: u'Innotech Corporation',
   0x00195D: u'ShenZhen XinHuaTong Opto Electronics Co.,Ltd',
   0x00195E: u'Motorola CHS',
   0x00195F: u'Valemount Networks Corporation',
   0x001960: u'DoCoMo Systems, Inc.',
   0x001961: u'Blaupunkt GmbH',
   0x001962: u'Commerciant, LP',
   0x001963: u'Sony Ericsson Mobile Communications AB',
   0x001964: u'Doorking Inc.',
   0x001965: u'YuHua TelTech (ShangHai) Co., Ltd.',
   0x001966: u'Asiarock Technology Limited',
   0x001967: u'TELDAT Sp.J.',
   0x001968: u'Digital Video Networks(Shanghai) CO. LTD.',
   0x001969: u'Nortel',
   0x00196A: u'MikroM GmbH',
   0x00196B: u'Danpex Corporation',
   0x00196C: u'ETROVISION TECHNOLOGY',
   0x00196D: u'Raybit Systems Korea, Inc',
   0x00196E: u'Metacom (Pty) Ltd.',
   0x00196F: u'SensoPart GmbH',
   0x001970: u'Z-Com, Inc.',
   0x001971: u'Guangzhou Unicomp Technology Co.,Ltd',
   0x001972: u'Plexus (Xiamen) Co.,ltd',
   0x001973: u'Zeugma Systems',
   0x001974: u'AboCom Systems, Inc.',
   0x001975: u'Beijing Huisen networks technology Inc',
   0x001976: u'Xipher Technologies, LLC',
   0x001977: u'Aerohive Networks, Inc.',
   0x001978: u'Datum Systems, Inc.',
   0x001979: u'Nokia Danmark A/S',
   0x00197A: u'MAZeT GmbH',
   0x00197B: u'Picotest Corp.',
   0x00197C: u'Riedel Communications GmbH',
   0x00197D: u'Hon Hai Precision Ind. Co., Ltd',
   0x00197E: u'Hon Hai Precision Ind. Co., Ltd',
   0x00197F: u'PLANTRONICS, INC.',
   0x001980: u'Gridpoint Systems',
   0x001981: u'Vivox Inc',
   0x001982: u'SmarDTV',
   0x001983: u'CCT R&D Limited',
   0x001984: u'ESTIC Corporation',
   0x001985: u'IT Watchdogs, Inc',
   0x001986: u'Cheng Hongjian',
   0x001987: u'Panasonic Mobile Communications Co., Ltd.',
   0x001988: u'Wi2Wi, Inc',
   0x001989: u'Sonitrol Corporation',
   0x00198A: u'Northrop Grumman Systems Corp.',
   0x00198B: u'Novera Optics Korea, Inc.',
   0x00198C: u'iXSea',
   0x00198D: u'Ocean Optics, Inc.',
   0x00198E: u'Oticon A/S',
   0x00198F: u'Alcatel Bell N.V.',
   0x001990: u'ELM DATA Co., Ltd.',
   0x001991: u'avinfo',
   0x001992: u'Bluesocket, Inc',
   0x001993: u'Changshu Switchgear MFG. Co.,Ltd. (Former Changshu Switchgea',
   0x001994: u'Jorjin technologies inc.',
   0x001995: u'Jurong Hi-Tech (Suzhou)Co.ltd',
   0x001996: u'TurboChef Technologies Inc.',
   0x001997: u'Soft Device Sdn Bhd',
   0x001998: u'SATO CORPORATION',
   0x001999: u'Fujitsu Siemens Computers',
   0x00199A: u'EDO-EVI',
   0x00199B: u'Diversified Technical Systems, Inc.',
   0x00199C: u'CTRING',
   0x00199D: u'V, Inc.',
   0x00199E: u'SHOWADENSHI ELECTRONICS,INC.',
   0x00199F: u'DKT A/S',
   0x0019A0: u'NIHON DATA SYSTENS, INC.',
   0x0019A1: u'LG INFORMATION & COMM.',
   0x0019A2: u'ORION TELE-EQUIPMENTS PVT LTD',
   0x0019A3: u'asteel electronique atlantique',
   0x0019A4: u'Austar Technology (hang zhou) Co.,Ltd',
   0x0019A5: u'RadarFind Corporation',
   0x0019A6: u'Motorola CHS',
   0x0019A7: u'ITU-T',
   0x0019A8: u'WiQuest Communications, Inc',
   0x0019A9: u'Cisco Systems',
   0x0019AA: u'Cisco Systems',
   0x0019AB: u'Raycom CO ., LTD',
   0x0019AC: u'GSP SYSTEMS Inc.',
   0x0019AD: u'BOBST SA',
   0x0019AE: u'Hopling Technologies b.v.',
   0x0019AF: u'Rigol Technologies, Inc.',
   0x0019B0: u'HanYang System',
   0x0019B1: u'Arrow7 Corporation',
   0x0019B2: u'XYnetsoft Co.,Ltd',
   0x0019B3: u'Stanford Research Systems',
   0x0019B4: u'VideoCast Ltd.',
   0x0019B5: u'Famar Fueguina S.A.',
   0x0019B6: u'Euro Emme s.r.l.',
   0x0019B7: u'Nokia Danmark A/S',
   0x0019B8: u'Boundary Devices',
   0x0019B9: u'Dell Inc.',
   0x0019BA: u'Paradox Security Systems Ltd',
   0x0019BB: u'Hewlett Packard',
   0x0019BC: u'ELECTRO CHANCE SRL',
   0x0019BD: u'New Media Life',
   0x0019BE: u'Altai Technologies Limited',
   0x0019BF: u'Citiway technology Co.,ltd',
   0x0019C0: u'Motorola Mobile Devices',
   0x0019C1: u'Alps Electric Co., Ltd',
   0x0019C2: u'Equustek Solutions, Inc.',
   0x0019C3: u'Qualitrol',
   0x0019C4: u'Infocrypt Inc.',
   0x0019C5: u'SONY Computer Entertainment inc,',
   0x0019C6: u'ZTE Corporation',
   0x0019C7: u'Cambridge Industries(Group) Co.,Ltd.',
   0x0019C8: u'AnyDATA Corporation',
   0x0019C9: u'S&C ELECTRIC COMPANY',
   0x0019CA: u'Broadata Communications, Inc',
   0x0019CB: u'ZyXEL Communications Corporation',
   0x0019CC: u'RCG (HK) Ltd',
   0x0019CD: u'Chengdu ethercom information technology Ltd.',
   0x0019CE: u'Progressive Gaming International',
   0x0019CF: u'SALICRU, S.A.',
   0x0019D0: u'Cathexis',
   0x0019D1: u'Intel Corporation',
   0x0019D2: u'Intel Corporation',
   0x0019D3: u'TRAK Microwave',
   0x0019D4: u'ICX Technologies',
   0x0019D5: u'IP Innovations, Inc.',
   0x0019D6: u'LS Cable Ltd.',
   0x0019D7: u'FORTUNETEK CO., LTD',
   0x0019D8: u'MAXFOR',
   0x0019D9: u'Zeutschel GmbH',
   0x0019DA: u'Welltrans O&E Technology Co. , Ltd.',
   0x0019DB: u'MICRO-STAR INTERNATIONAL CO., LTD.',
   0x0019DC: u'ENENSYS Technologies',
   0x0019DD: u'FEI-Zyfer, Inc.',
   0x0019DE: u'MOBITEK',
   0x0019DF: u'THOMSON APDG',
   0x0019E0: u'TP-LINK Technologies Co., Ltd.',
   0x0019E1: u'Nortel',
   0x0019E2: u'Juniper Networks',
   0x0019E3: u'Apple Computers',
   0x0019E4: u'2Wire, Inc',
   0x0019E5: u'Lynx Studio Technology, Inc.',
   0x0019E6: u'TOYO MEDIC CO.,LTD.',
   0x0019E7: u'Cisco Systems',
   0x0019E8: u'Cisco Systems',
   0x0019E9: u'S-Information Technolgy, Co., Ltd.',
   0x0019EA: u'TeraMage Technologies Co., Ltd.',
   0x0019EB: u'Pyronix Ltd',
   0x0019EC: u'Sagamore Systems, Inc.',
   0x0019ED: u'Axesstel Inc.',
   0x0019EE: u'CARLO GAVAZZI CONTROLS SPA-Controls Division',
   0x0019EF: u'SHENZHEN LINNKING ELECTRONICS CO.,LTD',
   0x0019F0: u'UNIONMAN TECHNOLOGY CO.,LTD',
   0x0019F1: u'Star Communication Network Technology Co.,Ltd',
   0x0019F2: u'Teradyne K.K.',
   0x0019F3: u'Telematrix, Inc',
   0x0019F4: u'Convergens Oy Ltd',
   0x0019F5: u'Imagination Technologies Ltd',
   0x0019F6: u'Acconet (PTE) Ltd',
   0x0019F7: u'Onset Computer Corporation',
   0x0019F8: u'Embedded Systems Design, Inc.',
   0x0019F9: u'Lambda',
   0x0019FA: u'Cable Vision Electronics CO., LTD.',
   0x0019FB: u'AMSTRAD PLC',
   0x0019FC: u'PT. Ufoakses Sukses Luarbiasa',
   0x0019FD: u'Nintendo Co., Ltd.',
   0x0019FE: u'SHENZHEN SEECOMM TECHNOLOGY CO.,LTD.',
   0x0019FF: u'Finnzymes',
   0x001A00: u'MATRIX INC.',
   0x001A01: u'Smiths Medical',
   0x001A02: u'SECURE CARE PRODUCTS, INC',
   0x001A03: u'Angel Electronics Co., Ltd.',
   0x001A04: u'Interay Solutions BV',
   0x001A05: u'OPTIBASE LTD',
   0x001A06: u'OpVista, Inc.',
   0x001A07: u'Arecont Vision',
   0x001A08: u'Dalman Technical Services',
   0x001A09: u'Wayfarer Transit Systems Ltd',
   0x001A0A: u'Adaptive Micro-Ware Inc.',
   0x001A0B: u'BONA TECHNOLOGY INC.',
   0x001A0C: u'Swe-Dish Satellite Systems AB',
   0x001A0D: u'HandHeld entertainment, Inc.',
   0x001A0E: u'Cheng Uei Precision Industry Co.,Ltd',
   0x001A0F: u'Sistemas Avanzados de Control, S.A.',
   0x001A10: u'LUCENT TRANS ELECTRONICS CO.,LTD',
   0x001A11: u'Google Inc.',
   0x001A12: u'PRIVATE',
   0x001A13: u'Wanlida Group Co., LTD',
   0x001A14: u'Xin Hua Control Engineering Co.,Ltd.',
   0x001A15: u'gemalto e-Payment',
   0x001A16: u'Nokia Danmark A/S',
   0x001A17: u'Teak Technologies, Inc.',
   0x001A18: u'Advanced Simulation Technology inc.',
   0x001A19: u'Computer Engineering Limited',
   0x001A1A: u'Gentex Corporation/Electro-Acoustic Products',
   0x001A1B: u'Motorola Mobile Devices',
   0x001A1C: u'GT&T Engineering Pte Ltd',
   0x001A1D: u'PChome Online Inc.',
   0x001A1E: u'Aruba Networks',
   0x001A1F: u'Coastal Environmental Systems',
   0x001A20: u'CMOTECH Co. Ltd.',
   0x001A21: u'Indac B.V.',
   0x001A22: u'eq-3 GmbH',
   0x001A23: u'Ice Qube, Inc',
   0x001A24: u'Galaxy Telecom Technologies Ltd',
   0x001A25: u'DELTA DORE',
   0x001A26: u'Deltanode Solutions AB',
   0x001A27: u'Ubistar',
   0x001A28: u'ASWT Co., LTD. Taiwan Branch H.K.',
   0x001A29: u'Techsonic Industries d/b/a Humminbird',
   0x001A2A: u'Arcadyan Technology Corporation',
   0x001A2B: u'Ayecom Technology Co., Ltd.',
   0x001A2C: u'SATEC Co.,LTD',
   0x001A2D: u'The Navvo Group',
   0x001A2E: u'Ziova Coporation',
   0x001A2F: u'Cisco Systems',
   0x001A30: u'Cisco Systems',
   0x001A31: u'SCAN COIN Industries AB',
   0x001A32: u'ACTIVA MULTIMEDIA',
   0x001A33: u'ASI Communications, Inc.',
   0x001A34: u'Konka Group Co., Ltd.',
   0x001A35: u'BARTEC GmbH',
   0x001A36: u'Actimon GmbH & Co. KG',
   0x001A37: u'Lear Corporation',
   0x001A38: u'SCI Technology',
   0x001A39: u'Merten GmbH&CoKG',
   0x001A3A: u'Dongahelecomm',
   0x001A3B: u'Doah Elecom Inc.',
   0x001A3C: u'Technowave Ltd.',
   0x001A3D: u'Ajin Vision Co.,Ltd',
   0x001A3E: u'Faster Technology LLC',
   0x001A3F: u'intelbras',
   0x001A40: u'A-FOUR TECH CO., LTD.',
   0x001A41: u'INOCOVA Co.,Ltd',
   0x001A42: u'Techcity Technology co., Ltd.',
   0x001A43: u'Logical Link Communications',
   0x001A44: u'JWTrading Co., Ltd',
   0x001A45: u'GN Netcom as',
   0x001A46: u'Digital Multimedia Technology Co., Ltd',
   0x001A47: u'Agami Systems, Inc.',
   0x001A48: u'Takacom Corporation',
   0x001A49: u'Micro Vision Co.,LTD',
   0x001A4A: u'Qumranet Inc.',
   0x001A4B: u'Hewlett Packard',
   0x001A4C: u'Crossbow Technology, Inc',
   0x001A4D: u'GIGABYTE TECHNOLOGY CO.,LTD.',
   0x001A4E: u'NTI AG / LinMot',
   0x001A4F: u'AVM GmbH',
   0x001A50: u'PheeNet Technology Corp.',
   0x001A51: u'Alfred Mann Foundation',
   0x001A52: u'Meshlinx Wireless Inc.',
   0x001A53: u'Zylaya',
   0x001A54: u'Hip Shing Electronics Ltd.',
   0x001A55: u'ACA-Digital Corporation',
   0x001A56: u'ViewTel Co,. Ltd.',
   0x001A57: u'Matrix Design Group, LLC',
   0x001A58: u'Celectronic GmbH',
   0x001A59: u'Ircona',
   0x001A5A: u'Korea Electric Power Data Network  (KDN) Co., Ltd',
   0x001A5B: u'NetCare Service Co., Ltd.',
   0x001A5C: u'Euchner GmbH+Co. KG',
   0x001A5D: u'Mobinnova Corp.',
   0x001A5E: u'Thincom Technology Co.,Ltd',
   0x001A5F: u'KitWorks.fi Ltd.',
   0x001A60: u'Wave Electronics Co.,Ltd.',
   0x001A61: u'PacStar Corp.',
   0x001A62: u'trusted data',
   0x001A63: u'Elster Electricity, LLC',
   0x001A64: u'IBM Corp.',
   0x001A65: u'Seluxit',
   0x001A66: u'Motorola CHS',
   0x001A67: u'Infinite QL Sdn Bhd',
   0x001A68: u'Weltec Enterprise Co., Ltd.',
   0x001A69: u'Wuhan Yangtze Optical Technology CO.,Ltd.',
   0x001A6A: u'Tranzas, Inc.',
   0x001A6B: u'USI',
   0x001A6C: u'Cisco Systems',
   0x001A6D: u'Cisco Systems',
   0x001A6E: u'Impro Technologies',
   0x001A6F: u'MI.TEL s.r.l.',
   0x001A70: u'Cisco-Linksys, LLC',
   0x001A71: u'Diostech Co., Ltd.',
   0x001A72: u'Mosart Semiconductor Corp.',
   0x001A73: u'Gemtek Technology Co., Ltd.',
   0x001A74: u'Procare International Co',
   0x001A75: u'Sony Ericsson Mobile Communications',
   0x001A76: u'SDT information Technology Co.,LTD.',
   0x001A77: u'Motorola Mobile Devices',
   0x001A78: u'ubtos',
   0x001A79: u'TELECOMUNICATION TECHNOLOGIES LTD.',
   0x001A7A: u'Lismore Instruments Limited',
   0x001A7B: u'Teleco, Inc.',
   0x001A7C: u'Hirschmann Automation and Control B.V.',
   0x001A7D: u'cyber-blue(HK)Ltd',
   0x001A7E: u'LN Srithai Comm Ltd.',
   0x001A7F: u'GCI Science&Technology Co.,Ltd.',
   0x001A80: u'Sony Corporation',
   0x001A81: u'Zelax',
   0x001A82: u'PROBA Building Automation Co.,LTD',
   0x001A83: u'Pegasus Technologies Inc.',
   0x001A84: u'V One Multimedia Pte Ltd',
   0x001A85: u'NV Michel Van de Wiele',
   0x001A86: u'AdvancedIO Systems Inc',
   0x001A87: u'Canhold International Limited',
   0x001A88: u'Venergy,Co,Ltd',
   0x001A89: u'Nokia Danmark A/S',
   0x001A8A: u'Samsung Electronics Co., Ltd.',
   0x001A8B: u'CHUNIL ELECTRIC IND., CO.',
   0x001A8C: u'Astaro AG',
   0x001A8D: u'AVECS Bergen GmbH',
   0x001A8E: u'3Way Networks Ltd',
   0x001A8F: u'Nortel',
   0x001A90: u'Trpico Sistemas e Telecomunicaes da Amaznia LTDA.',
   0x001A91: u'FusionDynamic Ltd.',
   0x001A92: u'ASUSTek COMPUTER INC.',
   0x001A93: u'ERCO Leuchten GmbH',
   0x001A94: u'Votronic GmbH',
   0x001A95: u'Hisense Mobile Communications Technoligy Co.,Ltd.',
   0x001A96: u'ECLER S.A.',
   0x001A97: u'fitivision technology Inc.',
   0x001A98: u'Asotel Communication Limited Taiwan Branch',
   0x001A99: u'Smarty (HZ) Information Electronics Co., Ltd',
   0x001A9A: u'Skyworth Digital technology(shenzhen)co.ltd.',
   0x001A9B: u'ADEC & Parter AG',
   0x001A9C: u'RightHand Technologies, Inc.',
   0x001A9D: u'Skipper Wireless, Inc.',
   0x001A9E: u'ICON Digital International Limited',
   0x001A9F: u'A-Link Europe Ltd',
   0x001AA0: u'Dell Inc',
   0x001AA1: u'Cisco Systems',
   0x001AA2: u'Cisco Systems',
   0x001AA3: u'DELORME',
   0x001AA4: u'Future University-Hakodate',
   0x001AA5: u'BRN Phoenix',
   0x001AA6: u'Telefunken Radio Communication Systems GmbH &CO.KG',
   0x001AA7: u'Torian Wireless',
   0x001AA8: u'Mamiya Digital Imaging Co., Ltd.',
   0x001AA9: u'FUJIAN STAR-NET COMMUNICATION CO.,LTD',
   0x001AAA: u'Analogic Corp.',
   0x001AAB: u'eWings s.r.l.',
   0x001AAC: u'Corelatus AB',
   0x001AAD: u'Motorola CHS',
   0x001AAE: u'Savant Systems LLC',
   0x001AAF: u'BLUSENS TECHNOLOGY',
   0x001AB0: u'Signal Networks Pvt. Ltd.,',
   0x001AB1: u'Asia Pacific Satellite Industries Co., Ltd.',
   0x001AB2: u'Cyber Solutions Inc.',
   0x001AB3: u'VISIONITE INC.',
   0x001AB4: u'FFEI Ltd.',
   0x001AB5: u'Home Network System',
   0x001AB6: u'Luminary Micro Inc',
   0x001AB7: u'Ethos Networks LTD.',
   0x001AB8: u'Anseri Corporation',
   0x001AB9: u'PMC',
   0x001ABA: u'Caton Overseas Limited',
   0x001ABB: u'Fontal Technology Incorporation',
   0x001ABC: u'U4EA Technologies Ltd',
   0x001ABD: u'Impatica Inc.',
   0x001ABE: u'COMPUTER HI-TECH INC.',
   0x001ABF: u'TRUMPF Laser Marking Systems AG',
   0x001AC0: u'JOYBIEN TECHNOLOGIES CO., LTD.',
   0x001AC1: u'3COM EUROPE',
   0x001AC2: u'YEC Co.,Ltd.',
   0x001AC3: u'Scientific-Atlanta, Inc',
   0x001AC4: u'2Wire, Inc',
   0x001AC5: u'BreakingPoint Systems, Inc.',
   0x001AC6: u'Micro Control Designs',
   0x001AC7: u'UNIPOINT',
   0x001AC8: u'ISL (Instrumentation Scientifique de Laboratoire)',
   0x001AC9: u'SUZUKEN CO.,LTD',
   0x001ACA: u'Tilera Corporation',
   0x001ACB: u'Autocom Products Ltd',
   0x001ACC: u'Celestial Semiconductor, Ltd',
   0x001ACD: u'Tidel Engineering LP',
   0x001ACE: u'YUPITERU INDUSTRIES CO., LTD.',
   0x001ACF: u'C.T. ELETTRONICA',
   0x001AD0: u'Siemens Schweiz AG',
   0x001AD1: u'FARGO CO., LTD.',
   0x001AD2: u'Eletronica Nitron Ltda',
   0x001AD3: u'Vamp Ltd.',
   0x001AD4: u'iPOX Technology Co., Ltd.',
   0x001AD5: u'KMC CHAIN INDUSTRIAL CO., LTD.',
   0x001AD6: u'JIAGNSU AETNA ELECTRIC CO.,LTD',
   0x001AD7: u'Christie Digital Systems, Inc.',
   0x001AD8: u'AlsterAero GmbH',
   0x001AD9: u'International Broadband Electric Communications, Inc.',
   0x001ADA: u'Biz-2-Me Inc.',
   0x001ADB: u'Motorola Mobile Devices',
   0x001ADC: u'Nokia Danmark A/S',
   0x001ADD: u'PePWave Ltd',
   0x001ADE: u'Motorola CHS',
   0x001ADF: u'Interactivetv Pty Limited',
   0x001AE0: u'Mythology Tech Express Inc.',
   0x001AE1: u'EDGE ACCESS INC',
   0x001AE2: u'Cisco Systems',
   0x001AE3: u'Cisco Systems',
   0x001AE4: u'Liposonix Inc,',
   0x001AE5: u'Mvox Technologies Inc.',
   0x001AE6: u'Atlanta Advanced Communications Holdings Limited',
   0x001AE7: u'Aztek Networks, Inc.',
   0x001AE8: u'Siemens Enterprise Communications GmbH & Co. KG',
   0x001AE9: u'Nintendo Co., Ltd.',
   0x001AEA: u'Radio Terminal Systems Pty Ltd',
   0x001AEB: u'Allied Telesis K.K.',
   0x001AEC: u'Keumbee Electronics Co.,Ltd.',
   0x001AED: u'INCOTEC GmbH',
   0x001AEE: u'Shenztech Ltd',
   0x001AEF: u'Loopcomm Technology, Inc.',
   0x001AF0: u'Alcatel - IPD',
   0x001AF1: u'Embedded Artists AB',
   0x001AF2: u'Dynavisions GmbH',
   0x001AF3: u'Samyoung Electronics',
   0x001AF4: u'Handreamnet',
   0x001AF5: u'PENTAONE. CO., LTD.',
   0x001AF6: u'Woven Systems, Inc.',
   0x001AF7: u'dataschalt e+a GmbH',
   0x001AF8: u'Copley Controls Corporation',
   0x001AF9: u'AeroVIronment (AV Inc)',
   0x001AFA: u'Welch Allyn, Inc.',
   0x001AFB: u'Joby Inc.',
   0x001AFC: u'ModusLink Corporation',
   0x001AFD: u'EVOLIS',
   0x001AFE: u'SOFACREAL',
   0x001AFF: u'Wizyoung Tech.',
   0x001B00: u'Neopost Technologies',
   0x001B01: u'Applied Radio Technologies',
   0x001B02: u'ED Co.Ltd',
   0x001B03: u'Action Technology (SZ) Co., Ltd',
   0x001B04: u'Affinity International S.p.a',
   0x001B05: u'Young Media Concepts GmbH',
   0x001B06: u'Ateliers R. LAUMONIER',
   0x001B07: u'Mendocino Software',
   0x001B08: u'Danfoss Drives A/S',
   0x001B09: u'Matrix Telecom Pvt. Ltd.',
   0x001B0A: u'Intelligent Distributed Controls Ltd',
   0x001B0B: u'Phidgets Inc.',
   0x001B0C: u'Cisco Systems',
   0x001B0D: u'Cisco Systems',
   0x001B0E: u'InoTec GmbH Organisationssysteme',
   0x001B0F: u'Petratec',
   0x001B10: u'ShenZhen Kang Hui Technology Co.,ltd',
   0x001B11: u'D-Link Corporation',
   0x001B12: u'Apprion',
   0x001B13: u'Icron Technologies Corporation',
   0x001B14: u'Carex Lighting Equipment Factory',
   0x001B15: u'Voxtel, Inc.',
   0x001B16: u'Celtro Ltd.',
   0x001B17: u'Palo Alto Networks',
   0x001B18: u'Tsuken Electric Ind. Co.,Ltd',
   0x001B19: u'IEEE 1588 Standard',
   0x001B1A: u'e-trees Japan, Inc.',
   0x001B1B: u'Siemens AG, A&D AS EWK PU1',
   0x001B1C: u'Coherent',
   0x001B1D: u'Phoenix International Co., Ltd',
   0x001B1E: u'HART Communication Foundation',
   0x001B1F: u'DELTA - Danish Electronics, Light & Acoustics',
   0x001B20: u'TPine Technology',
   0x001B21: u'Intel Corporate',
   0x001B22: u'Palit Microsystems ( H.K.) Ltd.',
   0x001B23: u'SimpleComTools',
   0x001B24: u'Quanta Computer Inc.',
   0x001B25: u'Nortel',
   0x001B26: u'RON-Telecom ZAO',
   0x001B27: u'Merlin CSI',
   0x001B28: u'POLYGON, JSC',
   0x001B29: u'Avantis.Co.,Ltd',
   0x001B2A: u'Cisco Systems',
   0x001B2B: u'Cisco Systems',
   0x001B2C: u'ATRON electronic GmbH',
   0x001B2D: u'PRIVATE',
   0x001B2E: u'Sinkyo Electron Inc',
   0x001B2F: u'NETGEAR Inc.',
   0x001B30: u'Solitech Inc.',
   0x001B31: u'Neural Image. Co. Ltd.',
   0x001B32: u'QLogic Corporation',
   0x001B33: u'Nokia Danmark A/S',
   0x001B34: u'Focus System Inc.',
   0x001B35: u'ChongQing JINOU Science & Technology Development CO.,Ltd',
   0x001B36: u'Tsubata Engineering Co.,Ltd. (Head Office)',
   0x001B37: u'Computec Oy',
   0x001B38: u'COMPAL ELECTRONICS TECHNOLOGIC CO., LTD.',
   0x001B39: u'Proxicast',
   0x001B3A: u'SIMS Corp.',
   0x001B3B: u'Yi-Qing CO., LTD',
   0x001B3C: u'Software Technologies Group,Inc.',
   0x001B3D: u'EuroTel Spa',
   0x001B3E: u'Curtis, Inc.',
   0x001B3F: u'ProCurve Networking by HP',
   0x001B40: u'Network Automation mxc AB',
   0x001B41: u'General Infinity Co.,Ltd.',
   0x001B42: u'Wise & Blue',
   0x001B43: u'Beijing DG Telecommunications equipment Co.,Ltd',
   0x001B44: u'SanDisk Corporation',
   0x001B45: u'ABB AS, Division Automation Products',
   0x001B46: u'Blueone Technology Co.,Ltd',
   0x001B47: u'Futarque A/S',
   0x001B48: u'Shenzhen Lantech Electronics Co., Ltd.',
   0x001B49: u'Roberts Radio limited',
   0x001B4A: u'W&W Communications, Inc.',
   0x001B4B: u'SANION Co., Ltd.',
   0x001B4C: u'Signtech',
   0x001B4D: u'Areca Technology Corporation',
   0x001B4E: u'Navman New Zealand',
   0x001B4F: u'Avaya Inc.',
   0x001B50: u'Nizhny Novgorod Factory named after M.Frunze, FSUE (NZiF)',
   0x001B51: u'Vector Technology Corp.',
   0x001B52: u'Motorola Mobile Devices',
   0x001B53: u'Cisco Systems',
   0x001B54: u'Cisco Systems',
   0x001B55: u'Hurco Automation Ltd.',
   0x001B56: u'Tehuti Networks Ltd.',
   0x001B57: u'SEMINDIA SYSTEMS PRIVATE LIMITED',
   0x001B58: u'PRIVATE',
   0x001B59: u'Sony Ericsson Mobile Communications AB',
   0x001B5A: u'Apollo Imaging Technologies, Inc.',
   0x001B5B: u'2Wire, Inc.',
   0x001B5C: u'Azuretec Co., Ltd.',
   0x001B5D: u'Vololink Pty Ltd',
   0x001B5E: u'BPL Limited',
   0x001B5F: u'Alien Technology',
   0x001B60: u'NAVIGON AG',
   0x001B61: u'Digital Acoustics, LLC',
   0x001B62: u'JHT Optoelectronics Co.,Ltd.',
   0x001B63: u'Apple Inc.',
   0x001B64: u'IsaacLandKorea',
   0x001B65: u'China Gridcom Co., Ltd',
   0x001B66: u'Sennheiser electronic GmbH & Co. KG',
   0x001B67: u'Ubiquisys Ltd',
   0x001B68: u'Modnnet Co., Ltd',
   0x001B69: u'Equaline Corporation',
   0x001B6A: u'Powerwave UK Ltd',
   0x001B6B: u'Swyx Solutions AG',
   0x001B6C: u'LookX Digital Media BV',
   0x001B6D: u'Midtronics, Inc.',
   0x001B6E: u'Anue Systems, Inc.',
   0x001B6F: u'Teletrak Ltd',
   0x001B70: u'IRI Ubiteq, INC.',
   0x001B71: u'Telular Corp.',
   0x001B72: u'Sicep s.p.a.',
   0x001B73: u'DTL Broadcast Ltd',
   0x001B74: u'MiraLink Corporation',
   0x001B75: u'Hypermedia Systems',
   0x001B76: u'Ripcode, Inc.',
   0x001B77: u'Intel Corporate',
   0x001B78: u'Hewlett Packard',
   0x001B79: u'FAIVELEY TRANSPORT',
   0x001B7A: u'Nintendo Co., Ltd.',
   0x001B7B: u'The Tintometer Ltd',
   0x001B7C: u'A & R Cambridge',
   0x001B7D: u'CXR Anderson Jacobson',
   0x001B7E: u'Beckmann GmbH',
   0x001B7F: u'TMN Technologies Telecomunicacoes Ltda',
   0x001B80: u'LORD Corporation',
   0x001B81: u'DATAQ Instruments, Inc.',
   0x001B82: u'Taiwan Semiconductor Co., Ltd.',
   0x001B83: u'Finsoft Ltd',
   0x001B84: u'Scan Engineering Telecom',
   0x001B85: u'MAN Diesel A/S',
   0x001B86: u'Bosch Access Systems GmbH',
   0x001B87: u'Deepsound Tech. Co., Ltd',
   0x001B88: u'Divinet Access Technologies Ltd',
   0x001B89: u'EMZA Visual Sense Ltd.',
   0x001B8A: u'2M Electronic A/S',
   0x001B8B: u'NEC AccessTechnica,Ltd.',
   0x001B8C: u'JMicron Technology Corp.',
   0x001B8D: u'Electronic Computer Systems, Inc.',
   0x001B8E: u'Hulu Sweden AB',
   0x001B8F: u'Cisco Systems',
   0x001B90: u'Cisco Systems',
   0x001B91: u'EFKON AG',
   0x001B92: u'l-acoustics',
   0x001B93: u'JC Decaux SA DNT',
   0x001B94: u'T.E.M.A. S.p.A.',
   0x001B95: u'VIDEO SYSTEMS SRL',
   0x001B96: u'Snif Labs, Inc.',
   0x001B97: u'Violin Technologies',
   0x001B98: u'Samsung Electronics Co., Ltd.',
   0x001B99: u'KS System GmbH',
   0x001B9A: u'Apollo Fire Detectors Ltd',
   0x001B9B: u'Hose-McCann Communications',
   0x001B9C: u'SATEL sp. z o.o.',
   0x001B9D: u'Novus Security Sp. z o.o.',
   0x001B9E: u'ASKEY  COMPUTER  CORP',
   0x001B9F: u'Calyptech Pty Ltd',
   0x001BA0: u'Awox',
   0x001BA1: u'mic AB',
   0x001BA2: u'IDS Imaging Development Systems GmbH',
   0x001BA3: u'Flexit Group GmbH',
   0x001BA4: u'S.A.E Afikim',
   0x001BA5: u'MyungMin Systems, Inc.',
   0x001BA6: u'intotech inc.',
   0x001BA7: u'Lorica Solutions',
   0x001BA8: u'UBI&MOBI,.Inc',
   0x001BA9: u'BROTHER INDUSTRIES, LTD. Printing & Solutions Company',
   0x001BAA: u'XenICs nv',
   0x001BAB: u'Telchemy, Incorporated',
   0x001BAC: u'Curtiss Wright Controls Embedded Computing',
   0x001BAD: u'iControl Incorporated',
   0x001BAE: u'Micro Control Systems, Inc',
   0x001BAF: u'Nokia Danmark A/S',
   0x001BB0: u'BHARAT ELECTRONICS',
   0x001BB1: u'Wistron Neweb Corp.',
   0x001BB2: u'Intellect International NV',
   0x001BB3: u'Condalo GmbH',
   0x001BB4: u'Airvod Limited',
   0x001BB5: u'Cherry GmbH',
   0x001BB6: u'Bird Electronic Corp.',
   0x001BB7: u'Alta Heights Technology Corp.',
   0x001BB8: u'BLUEWAY ELECTRONIC CO;LTD',
   0x001BB9: u'Elitegroup Computer System Co.',
   0x001C7C: u'PERQ SYSTEMS CORPORATION',
   0x002000: u'LEXMARK INTERNATIONAL, INC.',
   0x002001: u'DSP SOLUTIONS, INC.',
   0x002002: u'SERITECH ENTERPRISE CO., LTD.',
   0x002003: u'PIXEL POWER LTD.',
   0x002004: u'YAMATAKE-HONEYWELL CO., LTD.',
   0x002005: u'SIMPLE TECHNOLOGY',
   0x002006: u'GARRETT COMMUNICATIONS, INC.',
   0x002007: u'SFA, INC.',
   0x002008: u'CABLE & COMPUTER TECHNOLOGY',
   0x002009: u'PACKARD BELL ELEC., INC.',
   0x00200A: u'SOURCE-COMM CORP.',
   0x00200B: u'OCTAGON SYSTEMS CORP.',
   0x00200C: u'ADASTRA SYSTEMS CORP.',
   0x00200D: u'CARL ZEISS',
   0x00200E: u'SATELLITE TECHNOLOGY MGMT, INC',
   0x00200F: u'TANBAC CO., LTD.',
   0x002010: u'JEOL SYSTEM TECHNOLOGY CO. LTD',
   0x002011: u'CANOPUS CO., LTD.',
   0x002012: u'CAMTRONICS MEDICAL SYSTEMS',
   0x002013: u'DIVERSIFIED TECHNOLOGY, INC.',
   0x002014: u'GLOBAL VIEW CO., LTD.',
   0x002015: u'ACTIS COMPUTER SA',
   0x002016: u'SHOWA ELECTRIC WIRE & CABLE CO',
   0x002017: u'ORBOTECH',
   0x002018: u'CIS TECHNOLOGY INC.',
   0x002019: u'OHLER GmbH',
   0x00201A: u'MRV Communications, Inc.',
   0x00201B: u'NORTHERN TELECOM/NETWORK',
   0x00201C: u'EXCEL, INC.',
   0x00201D: u'KATANA PRODUCTS',
   0x00201E: u'NETQUEST CORPORATION',
   0x00201F: u'BEST POWER TECHNOLOGY, INC.',
   0x002020: u'MEGATRON COMPUTER INDUSTRIES PTY, LTD.',
   0x002021: u'ALGORITHMS SOFTWARE PVT. LTD.',
   0x002022: u'NMS Communications',
   0x002023: u'T.C. TECHNOLOGIES PTY. LTD',
   0x002024: u'PACIFIC COMMUNICATION SCIENCES',
   0x002025: u'CONTROL TECHNOLOGY, INC.',
   0x002026: u'AMKLY SYSTEMS, INC.',
   0x002027: u'MING FORTUNE INDUSTRY CO., LTD',
   0x002028: u'WEST EGG SYSTEMS, INC.',
   0x002029: u'TELEPROCESSING PRODUCTS, INC.',
   0x00202A: u'N.V. DZINE',
   0x00202B: u'ADVANCED TELECOMMUNICATIONS MODULES, LTD.',
   0x00202C: u'WELLTRONIX CO., LTD.',
   0x00202D: u'TAIYO CORPORATION',
   0x00202E: u'DAYSTAR DIGITAL',
   0x00202F: u'ZETA COMMUNICATIONS, LTD.',
   0x002030: u'ANALOG & DIGITAL SYSTEMS',
   0x002031: u'ERTEC GmbH',
   0x002032: u'ALCATEL TAISEL',
   0x002033: u'SYNAPSE TECHNOLOGIES, INC.',
   0x002034: u'ROTEC INDUSTRIEAUTOMATION GMBH',
   0x002035: u'IBM CORPORATION',
   0x002036: u'BMC SOFTWARE',
   0x002037: u'SEAGATE TECHNOLOGY',
   0x002038: u'VME MICROSYSTEMS INTERNATIONAL CORPORATION',
   0x002039: u'SCINETS',
   0x00203A: u'DIGITAL BI0METRICS INC.',
   0x00203B: u'WISDM LTD.',
   0x00203C: u'EUROTIME AB',
   0x00203D: u'NOVAR ELECTRONICS CORPORATION',
   0x00203E: u'LogiCan Technologies, Inc.',
   0x00203F: u'JUKI CORPORATION',
   0x002040: u'Motorola Broadband Communications Sector',
   0x002041: u'DATA NET',
   0x002042: u'DATAMETRICS CORP.',
   0x002043: u'NEURON COMPANY LIMITED',
   0x002044: u'GENITECH PTY LTD',
   0x002045: u'ION Networks, Inc.',
   0x002046: u'CIPRICO, INC.',
   0x002047: u'STEINBRECHER CORP.',
   0x002048: u'Marconi Communications',
   0x002049: u'COMTRON, INC.',
   0x00204A: u'PRONET GMBH',
   0x00204B: u'AUTOCOMPUTER CO., LTD.',
   0x00204C: u'MITRON COMPUTER PTE LTD.',
   0x00204D: u'INOVIS GMBH',
   0x00204E: u'NETWORK SECURITY SYSTEMS, INC.',
   0x00204F: u'DEUTSCHE AEROSPACE AG',
   0x002050: u'KOREA COMPUTER INC.',
   0x002051: u'Verilink Corporation',
   0x002052: u'RAGULA SYSTEMS',
   0x002053: u'HUNTSVILLE MICROSYSTEMS, INC.',
   0x002054: u'EASTERN RESEARCH, INC.',
   0x002055: u'ALTECH CO., LTD.',
   0x002056: u'NEOPRODUCTS',
   0x002057: u'TITZE DATENTECHNIK GmbH',
   0x002058: u'ALLIED SIGNAL INC.',
   0x002059: u'MIRO COMPUTER PRODUCTS AG',
   0x00205A: u'COMPUTER IDENTICS',
   0x00205B: u'Kentrox, LLC',
   0x00205C: u'InterNet Systems of Florida, Inc.',
   0x00205D: u'NANOMATIC OY',
   0x00205E: u'CASTLE ROCK, INC.',
   0x00205F: u'GAMMADATA COMPUTER GMBH',
   0x002060: u'ALCATEL ITALIA S.p.A.',
   0x002061: u'DYNATECH COMMUNICATIONS, INC.',
   0x002062: u'SCORPION LOGIC, LTD.',
   0x002063: u'WIPRO INFOTECH LTD.',
   0x002064: u'PROTEC MICROSYSTEMS, INC.',
   0x002065: u'SUPERNET NETWORKING INC.',
   0x002066: u'GENERAL MAGIC, INC.',
   0x002067: u'PRIVATE',
   0x002068: u'ISDYNE',
   0x002069: u'ISDN SYSTEMS CORPORATION',
   0x00206A: u'OSAKA COMPUTER CORP.',
   0x00206B: u'KONICA MINOLTA HOLDINGS, INC.',
   0x00206C: u'EVERGREEN TECHNOLOGY CORP.',
   0x00206D: u'DATA RACE, INC.',
   0x00206E: u'XACT, INC.',
   0x00206F: u'FLOWPOINT CORPORATION',
   0x002070: u'HYNET, LTD.',
   0x002071: u'IBR GMBH',
   0x002072: u'WORKLINK INNOVATIONS',
   0x002073: u'FUSION SYSTEMS CORPORATION',
   0x002074: u'SUNGWOON SYSTEMS',
   0x002075: u'MOTOROLA COMMUNICATION ISRAEL',
   0x002076: u'REUDO CORPORATION',
   0x002077: u'KARDIOS SYSTEMS CORP.',
   0x002078: u'RUNTOP, INC.',
   0x002079: u'MIKRON GMBH',
   0x00207A: u'WiSE Communications, Inc.',
   0x00207B: u'Intel Corporation',
   0x00207C: u'AUTEC GmbH',
   0x00207D: u'ADVANCED COMPUTER APPLICATIONS',
   0x00207E: u'FINECOM Co., Ltd.',
   0x00207F: u'KYOEI SANGYO CO., LTD.',
   0x002080: u'SYNERGY (UK) LTD.',
   0x002081: u'TITAN ELECTRONICS',
   0x002082: u'ONEAC CORPORATION',
   0x002083: u'PRESTICOM INCORPORATED',
   0x002084: u'OCE PRINTING SYSTEMS, GMBH',
   0x002085: u'EXIDE ELECTRONICS',
   0x002086: u'MICROTECH ELECTRONICS LIMITED',
   0x002087: u'MEMOTEC COMMUNICATIONS CORP.',
   0x002088: u'GLOBAL VILLAGE COMMUNICATION',
   0x002089: u'T3PLUS NETWORKING, INC.',
   0x00208A: u'SONIX COMMUNICATIONS, LTD.',
   0x00208B: u'LAPIS TECHNOLOGIES, INC.',
   0x00208C: u'GALAXY NETWORKS, INC.',
   0x00208D: u'CMD TECHNOLOGY',
   0x00208E: u'CHEVIN SOFTWARE ENG. LTD.',
   0x00208F: u'ECI TELECOM LTD.',
   0x002090: u'ADVANCED COMPRESSION TECHNOLOGY, INC.',
   0x002091: u'J125, NATIONAL SECURITY AGENCY',
   0x002092: u'CHESS ENGINEERING B.V.',
   0x002093: u'LANDINGS TECHNOLOGY CORP.',
   0x002094: u'CUBIX CORPORATION',
   0x002095: u'RIVA ELECTRONICS',
   0x002096: u'Invensys',
   0x002097: u'APPLIED SIGNAL TECHNOLOGY',
   0x002098: u'HECTRONIC AB',
   0x002099: u'BON ELECTRIC CO., LTD.',
   0x00209A: u'THE 3DO COMPANY',
   0x00209B: u'ERSAT ELECTRONIC GMBH',
   0x00209C: u'PRIMARY ACCESS CORP.',
   0x00209D: u'LIPPERT AUTOMATIONSTECHNIK',
   0x00209E: u'BROWN\'S OPERATING SYSTEM SERVICES, LTD.',
   0x00209F: u'MERCURY COMPUTER SYSTEMS, INC.',
   0x0020A0: u'OA LABORATORY CO., LTD.',
   0x0020A1: u'DOVATRON',
   0x0020A2: u'GALCOM NETWORKING LTD.',
   0x0020A3: u'DIVICOM INC.',
   0x0020A4: u'MULTIPOINT NETWORKS',
   0x0020A5: u'API ENGINEERING',
   0x0020A6: u'PROXIM, INC.',
   0x0020A7: u'PAIRGAIN TECHNOLOGIES, INC.',
   0x0020A8: u'SAST TECHNOLOGY CORP.',
   0x0020A9: u'WHITE HORSE INDUSTRIAL',
   0x0020AA: u'DIGIMEDIA VISION LTD.',
   0x0020AB: u'MICRO INDUSTRIES CORP.',
   0x0020AC: u'INTERFLEX DATENSYSTEME GMBH',
   0x0020AD: u'LINQ SYSTEMS',
   0x0020AE: u'ORNET DATA COMMUNICATION TECH.',
   0x0020AF: u'3COM CORPORATION',
   0x0020B0: u'GATEWAY DEVICES, INC.',
   0x0020B1: u'COMTECH RESEARCH INC.',
   0x0020B2: u'GKD Gesellschaft Fur Kommunikation Und Datentechnik',
   0x0020B3: u'SCLTEC COMMUNICATIONS SYSTEMS',
   0x0020B4: u'TERMA ELEKTRONIK AS',
   0x0020B5: u'YASKAWA ELECTRIC CORPORATION',
   0x0020B6: u'AGILE NETWORKS, INC.',
   0x0020B7: u'NAMAQUA COMPUTERWARE',
   0x0020B8: u'PRIME OPTION, INC.',
   0x0020B9: u'METRICOM, INC.',
   0x0020BA: u'CENTER FOR HIGH PERFORMANCE',
   0x0020BB: u'ZAX CORPORATION',
   0x0020BC: u'Long Reach Networks Pty Ltd',
   0x0020BD: u'NIOBRARA R & D CORPORATION',
   0x0020BE: u'LAN ACCESS CORP.',
   0x0020BF: u'AEHR TEST SYSTEMS',
   0x0020C0: u'PULSE ELECTRONICS, INC.',
   0x0020C1: u'SAXA, Inc.',
   0x0020C2: u'TEXAS MEMORY SYSTEMS, INC.',
   0x0020C3: u'COUNTER SOLUTIONS LTD.',
   0x0020C4: u'INET,INC.',
   0x0020C5: u'EAGLE TECHNOLOGY',
   0x0020C6: u'NECTEC',
   0x0020C7: u'AKAI Professional M.I. Corp.',
   0x0020C8: u'LARSCOM INCORPORATED',
   0x0020C9: u'VICTRON BV',
   0x0020CA: u'DIGITAL OCEAN',
   0x0020CB: u'PRETEC ELECTRONICS CORP.',
   0x0020CC: u'DIGITAL SERVICES, LTD.',
   0x0020CD: u'HYBRID NETWORKS, INC.',
   0x0020CE: u'LOGICAL DESIGN GROUP, INC.',
   0x0020CF: u'TEST & MEASUREMENT SYSTEMS INC',
   0x0020D0: u'VERSALYNX CORPORATION',
   0x0020D1: u'MICROCOMPUTER SYSTEMS (M) SDN.',
   0x0020D2: u'RAD DATA COMMUNICATIONS, LTD.',
   0x0020D3: u'OST (OUEST STANDARD TELEMATIQU',
   0x0020D4: u'CABLETRON - ZEITTNET INC.',
   0x0020D5: u'VIPA GMBH',
   0x0020D6: u'BREEZECOM',
   0x0020D7: u'JAPAN MINICOMPUTER SYSTEMS CO., Ltd.',
   0x0020D8: u'Nortel Networks',
   0x0020D9: u'PANASONIC TECHNOLOGIES, INC./MIECO-US',
   0x0020DA: u'Alcatel North America ESD',
   0x0020DB: u'XNET TECHNOLOGY, INC.',
   0x0020DC: u'DENSITRON TAIWAN LTD.',
   0x0020DD: u'Cybertec Pty Ltd',
   0x0020DE: u'JAPAN DIGITAL LABORAT\'Y CO.LTD',
   0x0020DF: u'KYOSAN ELECTRIC MFG. CO., LTD.',
   0x0020E0: u'Actiontec Electronics, Inc.',
   0x0020E1: u'ALAMAR ELECTRONICS',
   0x0020E2: u'INFORMATION RESOURCE ENGINEERING',
   0x0020E3: u'MCD KENCOM CORPORATION',
   0x0020E4: u'HSING TECH ENTERPRISE CO., LTD',
   0x0020E5: u'APEX DATA, INC.',
   0x0020E6: u'LIDKOPING MACHINE TOOLS AB',
   0x0020E7: u'B&W NUCLEAR SERVICE COMPANY',
   0x0020E8: u'DATATREK CORPORATION',
   0x0020E9: u'DANTEL',
   0x0020EA: u'EFFICIENT NETWORKS, INC.',
   0x0020EB: u'CINCINNATI MICROWAVE, INC.',
   0x0020EC: u'TECHWARE SYSTEMS CORP.',
   0x0020ED: u'GIGA-BYTE TECHNOLOGY CO., LTD.',
   0x0020EE: u'GTECH CORPORATION',
   0x0020EF: u'USC CORPORATION',
   0x0020F0: u'UNIVERSAL MICROELECTRONICS CO.',
   0x0020F1: u'ALTOS INDIA LIMITED',
   0x0020F2: u'SUN MICROSYSTEMS, INC.',
   0x0020F3: u'RAYNET CORPORATION',
   0x0020F4: u'SPECTRIX CORPORATION',
   0x0020F5: u'PANDATEL AG',
   0x0020F6: u'NET TEK  AND KARLNET, INC.',
   0x0020F7: u'CYBERDATA',
   0x0020F8: u'CARRERA COMPUTERS, INC.',
   0x0020F9: u'PARALINK NETWORKS, INC.',
   0x0020FA: u'GDE SYSTEMS, INC.',
   0x0020FB: u'OCTEL COMMUNICATIONS CORP.',
   0x0020FC: u'MATROX',
   0x0020FD: u'ITV TECHNOLOGIES, INC.',
   0x0020FE: u'TOPWARE INC. / GRAND COMPUTER',
   0x0020FF: u'SYMMETRICAL TECHNOLOGIES',
   0x002654: u'3Com Corporation',
   0x003000: u'ALLWELL TECHNOLOGY CORP.',
   0x003001: u'SMP',
   0x003002: u'Expand Networks',
   0x003003: u'Phasys Ltd.',
   0x003004: u'LEADTEK RESEARCH INC.',
   0x003005: u'Fujitsu Siemens Computers',
   0x003006: u'SUPERPOWER COMPUTER',
   0x003007: u'OPTI, INC.',
   0x003008: u'AVIO DIGITAL, INC.',
   0x003009: u'Tachion Networks, Inc.',
   0x00300A: u'AZTECH SYSTEMS LTD.',
   0x00300B: u'mPHASE Technologies, Inc.',
   0x00300C: u'CONGRUENCY, LTD.',
   0x00300D: u'MMC Technology, Inc.',
   0x00300E: u'Klotz Digital AG',
   0x00300F: u'IMT - Information Management T',
   0x003010: u'VISIONETICS INTERNATIONAL',
   0x003011: u'HMS FIELDBUS SYSTEMS AB',
   0x003012: u'DIGITAL ENGINEERING LTD.',
   0x003013: u'NEC Corporation',
   0x003014: u'DIVIO, INC.',
   0x003015: u'CP CLARE CORP.',
   0x003016: u'ISHIDA CO., LTD.',
   0x003017: u'BlueArc UK Ltd',
   0x003018: u'Jetway Information Co., Ltd.',
   0x003019: u'CISCO SYSTEMS, INC.',
   0x00301A: u'SMARTBRIDGES PTE. LTD.',
   0x00301B: u'SHUTTLE, INC.',
   0x00301C: u'ALTVATER AIRDATA SYSTEMS',
   0x00301D: u'SKYSTREAM, INC.',
   0x00301E: u'3COM Europe Ltd.',
   0x00301F: u'OPTICAL NETWORKS, INC.',
   0x003020: u'TSI, Inc..',
   0x003021: u'HSING TECH. ENTERPRISE CO.,LTD',
   0x003022: u'Fong Kai Industrial Co., Ltd.',
   0x003023: u'COGENT COMPUTER SYSTEMS, INC.',
   0x003024: u'CISCO SYSTEMS, INC.',
   0x003025: u'CHECKOUT COMPUTER SYSTEMS, LTD',
   0x003026: u'HeiTel Digital Video GmbH',
   0x003027: u'KERBANGO, INC.',
   0x003028: u'FASE Saldatura srl',
   0x003029: u'OPICOM',
   0x00302A: u'SOUTHERN INFORMATION',
   0x00302B: u'INALP NETWORKS, INC.',
   0x00302C: u'SYLANTRO SYSTEMS CORPORATION',
   0x00302D: u'QUANTUM BRIDGE COMMUNICATIONS',
   0x00302E: u'Hoft & Wessel AG',
   0x00302F: u'Smiths Industries',
   0x003030: u'HARMONIX CORPORATION',
   0x003031: u'LIGHTWAVE COMMUNICATIONS, INC.',
   0x003032: u'MagicRam, Inc.',
   0x003033: u'ORIENT TELECOM CO., LTD.',
   0x003034: u'SET ENGINEERING',
   0x003035: u'Corning Incorporated',
   0x003036: u'RMP ELEKTRONIKSYSTEME GMBH',
   0x003037: u'Packard Bell Nec Services',
   0x003038: u'XCP, INC.',
   0x003039: u'SOFTBOOK PRESS',
   0x00303A: u'MAATEL',
   0x00303B: u'PowerCom Technology',
   0x00303C: u'ONNTO CORP.',
   0x00303D: u'IVA CORPORATION',
   0x00303E: u'Radcom Ltd.',
   0x00303F: u'TurboComm Tech Inc.',
   0x003040: u'CISCO SYSTEMS, INC.',
   0x003041: u'SAEJIN T & M CO., LTD.',
   0x003042: u'DeTeWe-Deutsche Telephonwerke',
   0x003043: u'IDREAM TECHNOLOGIES, PTE. LTD.',
   0x003044: u'Portsmith LLC',
   0x003045: u'Village Networks, Inc. (VNI)',
   0x003046: u'Controlled Electronic Manageme',
   0x003047: u'NISSEI ELECTRIC CO., LTD.',
   0x003048: u'Supermicro Computer, Inc.',
   0x003049: u'BRYANT TECHNOLOGY, LTD.',
   0x00304A: u'Fraunhofer IPMS',
   0x00304B: u'ORBACOM SYSTEMS, INC.',
   0x00304C: u'APPIAN COMMUNICATIONS, INC.',
   0x00304D: u'ESI',
   0x00304E: u'BUSTEC PRODUCTION LTD.',
   0x00304F: u'PLANET Technology Corporation',
   0x003050: u'Versa Technology',
   0x003051: u'ORBIT AVIONIC & COMMUNICATION',
   0x003052: u'ELASTIC NETWORKS',
   0x003053: u'Basler AG',
   0x003054: u'CASTLENET TECHNOLOGY, INC.',
   0x003055: u'Hitachi Semiconductor America,',
   0x003056: u'Beck IPC GmbH',
   0x003057: u'QTelNet, Inc.',
   0x003058: u'API MOTION',
   0x003059: u'DIGITAL-LOGIC AG',
   0x00305A: u'TELGEN CORPORATION',
   0x00305B: u'MODULE DEPARTMENT',
   0x00305C: u'SMAR Laboratories Corp.',
   0x00305D: u'DIGITRA SYSTEMS, INC.',
   0x00305E: u'Abelko Innovation',
   0x00305F: u'IMACON APS',
   0x003060: u'Powerfile, Inc.',
   0x003061: u'MobyTEL',
   0x003062: u'PATH 1 NETWORK TECHNOL\'S INC.',
   0x003063: u'SANTERA SYSTEMS, INC.',
   0x003064: u'ADLINK TECHNOLOGY, INC.',
   0x003065: u'APPLE COMPUTER, INC.',
   0x003066: u'DIGITAL WIRELESS CORPORATION',
   0x003067: u'BIOSTAR MICROTECH INT\'L CORP.',
   0x003068: u'CYBERNETICS TECH. CO., LTD.',
   0x003069: u'IMPACCT TECHNOLOGY CORP.',
   0x00306A: u'PENTA MEDIA CO., LTD.',
   0x00306B: u'CMOS SYSTEMS, INC.',
   0x00306C: u'Hitex Holding GmbH',
   0x00306D: u'LUCENT TECHNOLOGIES',
   0x00306E: u'HEWLETT PACKARD',
   0x00306F: u'SEYEON TECH. CO., LTD.',
   0x003070: u'1Net Corporation',
   0x003071: u'Cisco Systems, Inc.',
   0x003072: u'INTELLIBYTE INC.',
   0x003073: u'International Microsystems, In',
   0x003074: u'EQUIINET LTD.',
   0x003075: u'ADTECH',
   0x003076: u'Akamba Corporation',
   0x003077: u'ONPREM NETWORKS',
   0x003078: u'Cisco Systems, Inc.',
   0x003079: u'CQOS, INC.',
   0x00307A: u'Advanced Technology & Systems',
   0x00307B: u'Cisco Systems, Inc.',
   0x00307C: u'ADID SA',
   0x00307D: u'GRE AMERICA, INC.',
   0x00307E: u'Redflex Communication Systems',
   0x00307F: u'IRLAN LTD.',
   0x003080: u'CISCO SYSTEMS, INC.',
   0x003081: u'ALTOS C&C',
   0x003082: u'TAIHAN ELECTRIC WIRE CO., LTD.',
   0x003083: u'Ivron Systems',
   0x003084: u'ALLIED TELESYN INTERNAIONAL',
   0x003085: u'CISCO SYSTEMS, INC.',
   0x003086: u'Transistor Devices, Inc.',
   0x003087: u'VEGA GRIESHABER KG',
   0x003088: u'Siara Systems, Inc.',
   0x003089: u'Spectrapoint Wireless, LLC',
   0x00308A: u'NICOTRA SISTEMI S.P.A',
   0x00308B: u'Brix Networks',
   0x00308C: u'ADVANCED DIGITAL INFORMATION',
   0x00308D: u'PINNACLE SYSTEMS, INC.',
   0x00308E: u'CROSS MATCH TECHNOLOGIES, INC.',
   0x00308F: u'MICRILOR, Inc.',
   0x003090: u'CYRA TECHNOLOGIES, INC.',
   0x003091: u'TAIWAN FIRST LINE ELEC. CORP.',
   0x003092: u'ModuNORM GmbH',
   0x003093: u'SONNET TECHNOLOGIES, INC.',
   0x003094: u'Cisco Systems, Inc.',
   0x003095: u'Procomp Informatics, Ltd.',
   0x003096: u'CISCO SYSTEMS, INC.',
   0x003097: u'EXOMATIC AB',
   0x003098: u'Global Converging Technologies',
   0x003099: u'BOENIG UND KALLENBACH OHG',
   0x00309A: u'ASTRO TERRA CORP.',
   0x00309B: u'Smartware',
   0x00309C: u'Timing Applications, Inc.',
   0x00309D: u'Nimble Microsystems, Inc.',
   0x00309E: u'WORKBIT CORPORATION.',
   0x00309F: u'AMBER NETWORKS',
   0x0030A0: u'TYCO SUBMARINE SYSTEMS, LTD.',
   0x0030A1: u'WEBGATE Inc.',
   0x0030A2: u'Lightner Engineering',
   0x0030A3: u'CISCO SYSTEMS, INC.',
   0x0030A4: u'Woodwind Communications System',
   0x0030A5: u'ACTIVE POWER',
   0x0030A6: u'VIANET TECHNOLOGIES, LTD.',
   0x0030A7: u'SCHWEITZER ENGINEERING',
   0x0030A8: u'OL\'E COMMUNICATIONS, INC.',
   0x0030A9: u'Netiverse, Inc.',
   0x0030AA: u'AXUS MICROSYSTEMS, INC.',
   0x0030AB: u'DELTA NETWORKS, INC.',
   0x0030AC: u'Systeme Lauer GmbH & Co., Ltd.',
   0x0030AD: u'SHANGHAI COMMUNICATION',
   0x0030AE: u'Times N System, Inc.',
   0x0030AF: u'Honeywell GmbH',
   0x0030B0: u'Convergenet Technologies',
   0x0030B1: u'aXess-pro networks GmbH',
   0x0030B2: u'L-3 Sonoma EO',
   0x0030B3: u'San Valley Systems, Inc.',
   0x0030B4: u'INTERSIL CORP.',
   0x0030B5: u'Tadiran Microwave Networks',
   0x0030B6: u'CISCO SYSTEMS, INC.',
   0x0030B7: u'Teletrol Systems, Inc.',
   0x0030B8: u'RiverDelta Networks',
   0x0030B9: u'ECTEL',
   0x0030BA: u'AC&T SYSTEM CO., LTD.',
   0x0030BB: u'CacheFlow, Inc.',
   0x0030BC: u'Optronic AG',
   0x0030BD: u'BELKIN COMPONENTS',
   0x0030BE: u'City-Net Technology, Inc.',
   0x0030BF: u'MULTIDATA GMBH',
   0x0030C0: u'Lara Technology, Inc.',
   0x0030C1: u'HEWLETT-PACKARD',
   0x0030C2: u'COMONE',
   0x0030C3: u'FLUECKIGER ELEKTRONIK AG',
   0x0030C4: u'Canon Imaging System Technologies, Inc.',
   0x0030C5: u'CADENCE DESIGN SYSTEMS',
   0x0030C6: u'CONTROL SOLUTIONS, INC.',
   0x0030C7: u'MACROMATE CORP.',
   0x0030C8: u'GAD LINE, LTD.',
   0x0030C9: u'LuxN, N',
   0x0030CA: u'Discovery Com',
   0x0030CB: u'OMNI FLOW COMPUTERS, INC.',
   0x0030CC: u'Tenor Networks, Inc.',
   0x0030CD: u'CONEXANT SYSTEMS, INC.',
   0x0030CE: u'Zaffire',
   0x0030CF: u'TWO TECHNOLOGIES, INC.',
   0x0030D0: u'Tellabs',
   0x0030D1: u'INOVA CORPORATION',
   0x0030D2: u'WIN TECHNOLOGIES, CO., LTD.',
   0x0030D3: u'Agilent Technologies',
   0x0030D4: u'AAE Systems, Inc',
   0x0030D5: u'DResearch GmbH',
   0x0030D6: u'MSC VERTRIEBS GMBH',
   0x0030D7: u'Innovative Systems, L.L.C.',
   0x0030D8: u'SITEK',
   0x0030D9: u'DATACORE SOFTWARE CORP.',
   0x0030DA: u'COMTREND CO.',
   0x0030DB: u'Mindready Solutions, Inc.',
   0x0030DC: u'RIGHTECH CORPORATION',
   0x0030DD: u'INDIGITA CORPORATION',
   0x0030DE: u'WAGO Kontakttechnik GmbH',
   0x0030DF: u'KB/TEL TELECOMUNICACIONES',
   0x0030E0: u'OXFORD SEMICONDUCTOR LTD.',
   0x0030E1: u'ACROTRON SYSTEMS, INC.',
   0x0030E2: u'GARNET SYSTEMS CO., LTD.',
   0x0030E3: u'SEDONA NETWORKS CORP.',
   0x0030E4: u'CHIYODA SYSTEM RIKEN',
   0x0030E5: u'Amper Datos S.A.',
   0x0030E6: u'Draeger Medical Systems, Inc.',
   0x0030E7: u'CNF MOBILE SOLUTIONS, INC.',
   0x0030E8: u'ENSIM CORP.',
   0x0030E9: u'GMA COMMUNICATION MANUFACT\'G',
   0x0030EA: u'TeraForce Technology Corporation',
   0x0030EB: u'TURBONET COMMUNICATIONS, INC.',
   0x0030EC: u'BORGARDT',
   0x0030ED: u'Expert Magnetics Corp.',
   0x0030EE: u'DSG Technology, Inc.',
   0x0030EF: u'NEON TECHNOLOGY, INC.',
   0x0030F0: u'Uniform Industrial Corp.',
   0x0030F1: u'Accton Technology Corp.',
   0x0030F2: u'CISCO SYSTEMS, INC.',
   0x0030F3: u'At Work Computers',
   0x0030F4: u'STARDOT TECHNOLOGIES',
   0x0030F5: u'Wild Lab. Ltd.',
   0x0030F6: u'SECURELOGIX CORPORATION',
   0x0030F7: u'RAMIX INC.',
   0x0030F8: u'Dynapro Systems, Inc.',
   0x0030F9: u'Sollae Systems Co., Ltd.',
   0x0030FA: u'TELICA, INC.',
   0x0030FB: u'AZS Technology AG',
   0x0030FC: u'Terawave Communications, Inc.',
   0x0030FD: u'INTEGRATED SYSTEMS DESIGN',
   0x0030FE: u'DSA GmbH',
   0x0030FF: u'DATAFAB SYSTEMS, INC.',
   0x004000: u'PCI COMPONENTES DA AMZONIA LTD',
   0x004001: u'ZYXEL COMMUNICATIONS, INC.',
   0x004002: u'PERLE SYSTEMS LIMITED',
   0x004003: u'Emerson Process Management Power & Water Solutions, Inc.',
   0x004004: u'ICM CO. LTD.',
   0x004005: u'ANI COMMUNICATIONS INC.',
   0x004006: u'SAMPO TECHNOLOGY CORPORATION',
   0x004007: u'TELMAT INFORMATIQUE',
   0x004008: u'A PLUS INFO CORPORATION',
   0x004009: u'TACHIBANA TECTRON CO., LTD.',
   0x00400A: u'PIVOTAL TECHNOLOGIES, INC.',
   0x00400B: u'CISCO SYSTEMS, INC.',
   0x00400C: u'GENERAL MICRO SYSTEMS, INC.',
   0x00400D: u'LANNET DATA COMMUNICATIONS,LTD',
   0x00400E: u'MEMOTEC COMMUNICATIONS, INC.',
   0x00400F: u'DATACOM TECHNOLOGIES',
   0x004010: u'SONIC SYSTEMS, INC.',
   0x004011: u'ANDOVER CONTROLS CORPORATION',
   0x004012: u'WINDATA, INC.',
   0x004013: u'NTT DATA COMM. SYSTEMS CORP.',
   0x004014: u'COMSOFT GMBH',
   0x004015: u'ASCOM INFRASYS AG',
   0x004016: u'HADAX ELECTRONICS, INC.',
   0x004017: u'Silex Technology America',
   0x004018: u'ADOBE SYSTEMS, INC.',
   0x004019: u'AEON SYSTEMS, INC.',
   0x00401A: u'FUJI ELECTRIC CO., LTD.',
   0x00401B: u'PRINTER SYSTEMS CORP.',
   0x00401C: u'AST RESEARCH, INC.',
   0x00401D: u'INVISIBLE SOFTWARE, INC.',
   0x00401E: u'ICC',
   0x00401F: u'COLORGRAPH LTD',
   0x004020: u'PINACL COMMUNICATION',
   0x004021: u'RASTER GRAPHICS',
   0x004022: u'KLEVER COMPUTERS, INC.',
   0x004023: u'LOGIC CORPORATION',
   0x004024: u'COMPAC INC.',
   0x004025: u'MOLECULAR DYNAMICS',
   0x004026: u'MELCO, INC.',
   0x004027: u'SMC MASSACHUSETTS, INC.',
   0x004028: u'NETCOMM LIMITED',
   0x004029: u'COMPEX',
   0x00402A: u'CANOGA-PERKINS',
   0x00402B: u'TRIGEM COMPUTER, INC.',
   0x00402C: u'ISIS DISTRIBUTED SYSTEMS, INC.',
   0x00402D: u'HARRIS ADACOM CORPORATION',
   0x00402E: u'PRECISION SOFTWARE, INC.',
   0x00402F: u'XLNT DESIGNS INC.',
   0x004030: u'GK COMPUTER',
   0x004031: u'KOKUSAI ELECTRIC CO., LTD',
   0x004032: u'DIGITAL COMMUNICATIONS',
   0x004033: u'ADDTRON TECHNOLOGY CO., LTD.',
   0x004034: u'BUSTEK CORPORATION',
   0x004035: u'OPCOM',
   0x004036: u'TRIBE COMPUTER WORKS, INC.',
   0x004037: u'SEA-ILAN, INC.',
   0x004038: u'TALENT ELECTRIC INCORPORATED',
   0x004039: u'OPTEC DAIICHI DENKO CO., LTD.',
   0x00403A: u'IMPACT TECHNOLOGIES',
   0x00403B: u'SYNERJET INTERNATIONAL CORP.',
   0x00403C: u'FORKS, INC.',
   0x00403D: u'TERADATA',
   0x00403E: u'RASTER OPS CORPORATION',
   0x00403F: u'SSANGYONG COMPUTER SYSTEMS',
   0x004040: u'RING ACCESS, INC.',
   0x004041: u'FUJIKURA LTD.',
   0x004042: u'N.A.T. GMBH',
   0x004043: u'NOKIA TELECOMMUNICATIONS',
   0x004044: u'QNIX COMPUTER CO., LTD.',
   0x004045: u'TWINHEAD CORPORATION',
   0x004046: u'UDC RESEARCH LIMITED',
   0x004047: u'WIND RIVER SYSTEMS',
   0x004048: u'SMD INFORMATICA S.A.',
   0x004049: u'TEGIMENTA AG',
   0x00404A: u'WEST AUSTRALIAN DEPARTMENT',
   0x00404B: u'MAPLE COMPUTER SYSTEMS',
   0x00404C: u'HYPERTEC PTY LTD.',
   0x00404D: u'TELECOMMUNICATIONS TECHNIQUES',
   0x00404E: u'FLUENT, INC.',
   0x00404F: u'SPACE & NAVAL WARFARE SYSTEMS',
   0x004050: u'IRONICS, INCORPORATED',
   0x004051: u'GRACILIS, INC.',
   0x004052: u'STAR TECHNOLOGIES, INC.',
   0x004053: u'AMPRO COMPUTERS',
   0x004054: u'CONNECTION MACHINES SERVICES',
   0x004055: u'METRONIX GMBH',
   0x004056: u'MCM JAPAN LTD.',
   0x004057: u'LOCKHEED - SANDERS',
   0x004058: u'KRONOS, INC.',
   0x004059: u'YOSHIDA KOGYO K. K.',
   0x00405A: u'GOLDSTAR INFORMATION & COMM.',
   0x00405B: u'FUNASSET LIMITED',
   0x00405C: u'FUTURE SYSTEMS, INC.',
   0x00405D: u'STAR-TEK, INC.',
   0x00405E: u'NORTH HILLS ISRAEL',
   0x00405F: u'AFE COMPUTERS LTD.',
   0x004060: u'COMENDEC LTD',
   0x004061: u'DATATECH ENTERPRISES CO., LTD.',
   0x004062: u'E-SYSTEMS, INC./GARLAND DIV.',
   0x004063: u'VIA TECHNOLOGIES, INC.',
   0x004064: u'KLA INSTRUMENTS CORPORATION',
   0x004065: u'GTE SPACENET',
   0x004066: u'HITACHI CABLE, LTD.',
   0x004067: u'OMNIBYTE CORPORATION',
   0x004068: u'EXTENDED SYSTEMS',
   0x004069: u'LEMCOM SYSTEMS, INC.',
   0x00406A: u'KENTEK INFORMATION SYSTEMS,INC',
   0x00406B: u'SYSGEN',
   0x00406C: u'COPERNIQUE',
   0x00406D: u'LANCO, INC.',
   0x00406E: u'COROLLARY, INC.',
   0x00406F: u'SYNC RESEARCH INC.',
   0x004070: u'INTERWARE CO., LTD.',
   0x004071: u'ATM COMPUTER GMBH',
   0x004072: u'Applied Innovation Inc.',
   0x004073: u'BASS ASSOCIATES',
   0x004074: u'CABLE AND WIRELESS',
   0x004075: u'M-TRADE (UK) LTD',
   0x004076: u'Sun Conversion Technologies',
   0x004077: u'MAXTON TECHNOLOGY CORPORATION',
   0x004078: u'WEARNES AUTOMATION PTE LTD',
   0x004079: u'JUKO MANUFACTURE COMPANY, LTD.',
   0x00407A: u'SOCIETE D\'EXPLOITATION DU CNIT',
   0x00407B: u'SCIENTIFIC ATLANTA',
   0x00407C: u'QUME CORPORATION',
   0x00407D: u'EXTENSION TECHNOLOGY CORP.',
   0x00407E: u'EVERGREEN SYSTEMS, INC.',
   0x00407F: u'FLIR Systems',
   0x004080: u'ATHENIX CORPORATION',
   0x004081: u'MANNESMANN SCANGRAPHIC GMBH',
   0x004082: u'LABORATORY EQUIPMENT CORP.',
   0x004083: u'TDA INDUSTRIA DE PRODUTOS',
   0x004084: u'HONEYWELL INC.',
   0x004085: u'SAAB INSTRUMENTS AB',
   0x004086: u'MICHELS & KLEBERHOFF COMPUTER',
   0x004087: u'UBITREX CORPORATION',
   0x004088: u'MOBIUS TECHNOLOGIES, INC.',
   0x004089: u'MEIDENSHA CORPORATION',
   0x00408A: u'TPS TELEPROCESSING SYS. GMBH',
   0x00408B: u'RAYLAN CORPORATION',
   0x00408C: u'AXIS COMMUNICATIONS AB',
   0x00408D: u'THE GOODYEAR TIRE & RUBBER CO.',
   0x00408E: u'DIGILOG, INC.',
   0x00408F: u'WM-DATA MINFO AB',
   0x004090: u'ANSEL COMMUNICATIONS',
   0x004091: u'PROCOMP INDUSTRIA ELETRONICA',
   0x004092: u'ASP COMPUTER PRODUCTS, INC.',
   0x004093: u'PAXDATA NETWORKS LTD.',
   0x004094: u'SHOGRAPHICS, INC.',
   0x004095: u'R.P.T. INTERGROUPS INT\'L LTD.',
   0x004096: u'Cisco Systems, Inc.',
   0x004097: u'DATEX DIVISION OF',
   0x004098: u'DRESSLER GMBH & CO.',
   0x004099: u'NEWGEN SYSTEMS CORP.',
   0x00409A: u'NETWORK EXPRESS, INC.',
   0x00409B: u'HAL COMPUTER SYSTEMS INC.',
   0x00409C: u'TRANSWARE',
   0x00409D: u'DIGIBOARD, INC.',
   0x00409E: u'CONCURRENT TECHNOLOGIES  LTD.',
   0x00409F: u'LANCAST/CASAT TECHNOLOGY, INC.',
   0x0040A0: u'GOLDSTAR CO., LTD.',
   0x0040A1: u'ERGO COMPUTING',
   0x0040A2: u'KINGSTAR TECHNOLOGY INC.',
   0x0040A3: u'MICROUNITY SYSTEMS ENGINEERING',
   0x0040A4: u'ROSE ELECTRONICS',
   0x0040A5: u'CLINICOMP INTL.',
   0x0040A6: u'Cray, Inc.',
   0x0040A7: u'ITAUTEC PHILCO S.A.',
   0x0040A8: u'IMF INTERNATIONAL LTD.',
   0x0040A9: u'DATACOM INC.',
   0x0040AA: u'VALMET AUTOMATION INC.',
   0x0040AB: u'ROLAND DG CORPORATION',
   0x0040AC: u'SUPER WORKSTATION, INC.',
   0x0040AD: u'SMA REGELSYSTEME GMBH',
   0x0040AE: u'DELTA CONTROLS, INC.',
   0x0040AF: u'DIGITAL PRODUCTS, INC.',
   0x0040B0: u'BYTEX CORPORATION, ENGINEERING',
   0x0040B1: u'CODONICS INC.',
   0x0040B2: u'SYSTEMFORSCHUNG',
   0x0040B3: u'PAR MICROSYSTEMS CORPORATION',
   0x0040B4: u'NEXTCOM K.K.',
   0x0040B5: u'VIDEO TECHNOLOGY COMPUTERS LTD',
   0x0040B6: u'COMPUTERM  CORPORATION',
   0x0040B7: u'STEALTH COMPUTER SYSTEMS',
   0x0040B8: u'IDEA ASSOCIATES',
   0x0040B9: u'MACQ ELECTRONIQUE SA',
   0x0040BA: u'ALLIANT COMPUTER SYSTEMS CORP.',
   0x0040BB: u'GOLDSTAR CABLE CO., LTD.',
   0x0040BC: u'ALGORITHMICS LTD.',
   0x0040BD: u'STARLIGHT NETWORKS, INC.',
   0x0040BE: u'BOEING DEFENSE & SPACE',
   0x0040BF: u'CHANNEL SYSTEMS INTERN\'L INC.',
   0x0040C0: u'VISTA CONTROLS CORPORATION',
   0x0040C1: u'BIZERBA-WERKE WILHEIM KRAUT',
   0x0040C2: u'APPLIED COMPUTING DEVICES',
   0x0040C3: u'FISCHER AND PORTER CO.',
   0x0040C4: u'KINKEI SYSTEM CORPORATION',
   0x0040C5: u'MICOM COMMUNICATIONS INC.',
   0x0040C6: u'FIBERNET RESEARCH, INC.',
   0x0040C7: u'RUBY TECH CORPORATION',
   0x0040C8: u'MILAN TECHNOLOGY CORPORATION',
   0x0040C9: u'NCUBE',
   0x0040CA: u'FIRST INTERNAT\'L COMPUTER, INC',
   0x0040CB: u'LANWAN TECHNOLOGIES',
   0x0040CC: u'SILCOM MANUF\'G TECHNOLOGY INC.',
   0x0040CD: u'TERA MICROSYSTEMS, INC.',
   0x0040CE: u'NET-SOURCE, INC.',
   0x0040CF: u'STRAWBERRY TREE, INC.',
   0x0040D0: u'MITAC INTERNATIONAL CORP.',
   0x0040D1: u'FUKUDA DENSHI CO., LTD.',
   0x0040D2: u'PAGINE CORPORATION',
   0x0040D3: u'KIMPSION INTERNATIONAL CORP.',
   0x0040D4: u'GAGE TALKER CORP.',
   0x0040D5: u'SARTORIUS AG',
   0x0040D6: u'LOCAMATION B.V.',
   0x0040D7: u'STUDIO GEN INC.',
   0x0040D8: u'OCEAN OFFICE AUTOMATION LTD.',
   0x0040D9: u'AMERICAN MEGATRENDS INC.',
   0x0040DA: u'TELSPEC LTD',
   0x0040DB: u'ADVANCED TECHNICAL SOLUTIONS',
   0x0040DC: u'TRITEC ELECTRONIC GMBH',
   0x0040DD: u'HONG TECHNOLOGIES',
   0x0040DE: u'ELETTRONICA SAN GIORGIO',
   0x0040DF: u'DIGALOG SYSTEMS, INC.',
   0x0040E0: u'ATOMWIDE LTD.',
   0x0040E1: u'MARNER INTERNATIONAL, INC.',
   0x0040E2: u'MESA RIDGE TECHNOLOGIES, INC.',
   0x0040E3: u'QUIN SYSTEMS LTD',
   0x0040E4: u'E-M TECHNOLOGY, INC.',
   0x0040E5: u'SYBUS CORPORATION',
   0x0040E6: u'C.A.E.N.',
   0x0040E7: u'ARNOS INSTRUMENTS & COMPUTER',
   0x0040E8: u'CHARLES RIVER DATA SYSTEMS,INC',
   0x0040E9: u'ACCORD SYSTEMS, INC.',
   0x0040EA: u'PLAIN TREE SYSTEMS INC',
   0x0040EB: u'MARTIN MARIETTA CORPORATION',
   0x0040EC: u'MIKASA SYSTEM ENGINEERING',
   0x0040ED: u'NETWORK CONTROLS INT\'NATL INC.',
   0x0040EE: u'OPTIMEM',
   0x0040EF: u'HYPERCOM, INC.',
   0x0040F0: u'MICRO SYSTEMS, INC.',
   0x0040F1: u'CHUO ELECTRONICS CO., LTD.',
   0x0040F2: u'JANICH & KLASS COMPUTERTECHNIK',
   0x0040F3: u'NETCOR',
   0x0040F4: u'CAMEO COMMUNICATIONS, INC.',
   0x0040F5: u'OEM ENGINES',
   0x0040F6: u'KATRON COMPUTERS INC.',
   0x0040F7: u'POLAROID MEDICAL IMAGING SYS.',
   0x0040F8: u'SYSTEMHAUS DISCOM',
   0x0040F9: u'COMBINET',
   0x0040FA: u'MICROBOARDS, INC.',
   0x0040FB: u'CASCADE COMMUNICATIONS CORP.',
   0x0040FC: u'IBR COMPUTER TECHNIK GMBH',
   0x0040FD: u'LXE',
   0x0040FE: u'SYMPLEX COMMUNICATIONS',
   0x0040FF: u'TELEBIT CORPORATION',
   0x004252: u'RLX Technologies',
   0x004501: u'Versus Technology, Inc.',
   0x005000: u'NEXO COMMUNICATIONS, INC.',
   0x005001: u'YAMASHITA SYSTEMS CORP.',
   0x005002: u'OMNISEC AG',
   0x005003: u'GRETAG MACBETH AG',
   0x005004: u'3COM CORPORATION',
   0x005006: u'TAC AB',
   0x005007: u'SIEMENS TELECOMMUNICATION SYSTEMS LIMITED',
   0x005008: u'TIVA MICROCOMPUTER CORP. (TMC)',
   0x005009: u'PHILIPS BROADBAND NETWORKS',
   0x00500A: u'IRIS TECHNOLOGIES, INC.',
   0x00500B: u'CISCO SYSTEMS, INC.',
   0x00500C: u'e-Tek Labs, Inc.',
   0x00500D: u'SATORI ELECTORIC CO., LTD.',
   0x00500E: u'CHROMATIS NETWORKS, INC.',
   0x00500F: u'CISCO SYSTEMS, INC.',
   0x005010: u'NovaNET Learning, Inc.',
   0x005012: u'CBL - GMBH',
   0x005013: u'Chaparral Network Storage',
   0x005014: u'CISCO SYSTEMS, INC.',
   0x005015: u'BRIGHT STAR ENGINEERING',
   0x005016: u'SST/WOODHEAD INDUSTRIES',
   0x005017: u'RSR S.R.L.',
   0x005018: u'AMIT, Inc.',
   0x005019: u'SPRING TIDE NETWORKS, INC.',
   0x00501A: u'UISIQN',
   0x00501B: u'ABL CANADA, INC.',
   0x00501C: u'JATOM SYSTEMS, INC.',
   0x00501E: u'Miranda Technologies, Inc.',
   0x00501F: u'MRG SYSTEMS, LTD.',
   0x005020: u'MEDIASTAR CO., LTD.',
   0x005021: u'EIS INTERNATIONAL, INC.',
   0x005022: u'ZONET TECHNOLOGY, INC.',
   0x005023: u'PG DESIGN ELECTRONICS, INC.',
   0x005024: u'NAVIC SYSTEMS, INC.',
   0x005026: u'COSYSTEMS, INC.',
   0x005027: u'GENICOM CORPORATION',
   0x005028: u'AVAL COMMUNICATIONS',
   0x005029: u'1394 PRINTER WORKING GROUP',
   0x00502A: u'CISCO SYSTEMS, INC.',
   0x00502B: u'GENRAD LTD.',
   0x00502C: u'SOYO COMPUTER, INC.',
   0x00502D: u'ACCEL, INC.',
   0x00502E: u'CAMBEX CORPORATION',
   0x00502F: u'TollBridge Technologies, Inc.',
   0x005030: u'FUTURE PLUS SYSTEMS',
   0x005031: u'AEROFLEX LABORATORIES, INC.',
   0x005032: u'PICAZO COMMUNICATIONS, INC.',
   0x005033: u'MAYAN NETWORKS',
   0x005036: u'NETCAM, LTD.',
   0x005037: u'KOGA ELECTRONICS CO.',
   0x005038: u'DAIN TELECOM CO., LTD.',
   0x005039: u'MARINER NETWORKS',
   0x00503A: u'DATONG ELECTRONICS LTD.',
   0x00503B: u'MEDIAFIRE CORPORATION',
   0x00503C: u'TSINGHUA NOVEL ELECTRONICS',
   0x00503E: u'CISCO SYSTEMS, INC.',
   0x00503F: u'ANCHOR GAMES',
   0x005040: u'Matsushita Electric Works, Ltd.',
   0x005041: u'Coretronic Corporation',
   0x005042: u'SCI MANUFACTURING SINGAPORE PTE, LTD.',
   0x005043: u'MARVELL SEMICONDUCTOR, INC.',
   0x005044: u'ASACA CORPORATION',
   0x005045: u'RIOWORKS SOLUTIONS, INC.',
   0x005046: u'MENICX INTERNATIONAL CO., LTD.',
   0x005047: u'PRIVATE',
   0x005048: u'INFOLIBRIA',
   0x005049: u'ELLACOYA NETWORKS, INC.',
   0x00504A: u'ELTECO A.S.',
   0x00504B: u'BARCONET N.V.',
   0x00504C: u'GALIL MOTION CONTROL, INC.',
   0x00504D: u'TOKYO ELECTRON DEVICE LTD.',
   0x00504E: u'SIERRA MONITOR CORP.',
   0x00504F: u'OLENCOM ELECTRONICS',
   0x005050: u'CISCO SYSTEMS, INC.',
   0x005051: u'IWATSU ELECTRIC CO., LTD.',
   0x005052: u'TIARA NETWORKS, INC.',
   0x005053: u'CISCO SYSTEMS, INC.',
   0x005054: u'CISCO SYSTEMS, INC.',
   0x005055: u'DOMS A/S',
   0x005056: u'VMWare, Inc.',
   0x005057: u'BROADBAND ACCESS SYSTEMS',
   0x005058: u'VegaStream Limted',
   0x005059: u'iBAHN',
   0x00505A: u'NETWORK ALCHEMY, INC.',
   0x00505B: u'KAWASAKI LSI U.S.A., INC.',
   0x00505C: u'TUNDO CORPORATION',
   0x00505E: u'DIGITEK MICROLOGIC S.A.',
   0x00505F: u'BRAND INNOVATORS',
   0x005060: u'TANDBERG TELECOM AS',
   0x005062: u'KOUWELL ELECTRONICS CORP.  **',
   0x005063: u'OY COMSEL SYSTEM AB',
   0x005064: u'CAE ELECTRONICS',
   0x005065: u'DENSEI-LAMBAD Co., Ltd.',
   0x005066: u'AtecoM GmbH advanced telecomunication modules',
   0x005067: u'AEROCOMM, INC.',
   0x005068: u'ELECTRONIC INDUSTRIES ASSOCIATION',
   0x005069: u'PixStream Incorporated',
   0x00506A: u'EDEVA, INC.',
   0x00506B: u'SPX-ATEG',
   0x00506C: u'G & L BEIJER ELECTRONICS AB',
   0x00506D: u'VIDEOJET SYSTEMS',
   0x00506E: u'CORDER ENGINEERING CORPORATION',
   0x00506F: u'G-CONNECT',
   0x005070: u'CHAINTECH COMPUTER CO., LTD.',
   0x005071: u'AIWA CO., LTD.',
   0x005072: u'CORVIS CORPORATION',
   0x005073: u'CISCO SYSTEMS, INC.',
   0x005074: u'ADVANCED HI-TECH CORP.',
   0x005075: u'KESTREL SOLUTIONS',
   0x005076: u'IBM',
   0x005077: u'PROLIFIC TECHNOLOGY, INC.',
   0x005078: u'MEGATON HOUSE, LTD.',
   0x005079: u'PRIVATE',
   0x00507A: u'XPEED, INC.',
   0x00507B: u'MERLOT COMMUNICATIONS',
   0x00507C: u'VIDEOCON AG',
   0x00507D: u'IFP',
   0x00507E: u'NEWER TECHNOLOGY',
   0x00507F: u'DrayTek Corp.',
   0x005080: u'CISCO SYSTEMS, INC.',
   0x005081: u'MURATA MACHINERY, LTD.',
   0x005082: u'FORESSON CORPORATION',
   0x005083: u'GILBARCO, INC.',
   0x005084: u'ATL PRODUCTS',
   0x005086: u'TELKOM SA, LTD.',
   0x005087: u'TERASAKI ELECTRIC CO., LTD.',
   0x005088: u'AMANO CORPORATION',
   0x005089: u'SAFETY MANAGEMENT SYSTEMS',
   0x00508B: u'COMPAQ COMPUTER CORPORATION',
   0x00508C: u'RSI SYSTEMS',
   0x00508D: u'ABIT COMPUTER CORPORATION',
   0x00508E: u'OPTIMATION, INC.',
   0x00508F: u'ASITA TECHNOLOGIES INT\'L LTD.',
   0x005090: u'DCTRI',
   0x005091: u'NETACCESS, INC.',
   0x005092: u'RIGAKU INDUSTRIAL CORPORATION',
   0x005093: u'BOEING',
   0x005094: u'PACE MICRO TECHNOLOGY PLC',
   0x005095: u'PERACOM NETWORKS',
   0x005096: u'SALIX TECHNOLOGIES, INC.',
   0x005097: u'MMC-EMBEDDED COMPUTERTECHNIK GmbH',
   0x005098: u'GLOBALOOP, LTD.',
   0x005099: u'3COM EUROPE, LTD.',
   0x00509A: u'TAG ELECTRONIC SYSTEMS',
   0x00509B: u'SWITCHCORE AB',
   0x00509C: u'BETA RESEARCH',
   0x00509D: u'THE INDUSTREE B.V.',
   0x00509E: u'Les Technologies SoftAcoustik Inc.',
   0x00509F: u'HORIZON COMPUTER',
   0x0050A0: u'DELTA COMPUTER SYSTEMS, INC.',
   0x0050A1: u'CARLO GAVAZZI, INC.',
   0x0050A2: u'CISCO SYSTEMS, INC.',
   0x0050A3: u'TransMedia Communications, Inc.',
   0x0050A4: u'IO TECH, INC.',
   0x0050A5: u'CAPITOL BUSINESS SYSTEMS, LTD.',
   0x0050A6: u'OPTRONICS',
   0x0050A7: u'CISCO SYSTEMS, INC.',
   0x0050A8: u'OpenCon Systems, Inc.',
   0x0050A9: u'MOLDAT WIRELESS TECHNOLGIES',
   0x0050AA: u'KONICA MINOLTA HOLDINGS, INC.',
   0x0050AB: u'NALTEC, INC.',
   0x0050AC: u'MAPLE COMPUTER CORPORATION',
   0x0050AD: u'CommUnique Wireless Corp.',
   0x0050AE: u'IWAKI ELECTRONICS CO., LTD.',
   0x0050AF: u'INTERGON, INC.',
   0x0050B0: u'TECHNOLOGY ATLANTA CORPORATION',
   0x0050B1: u'GIDDINGS & LEWIS',
   0x0050B2: u'BRODEL AUTOMATION',
   0x0050B3: u'VOICEBOARD CORPORATION',
   0x0050B4: u'SATCHWELL CONTROL SYSTEMS, LTD',
   0x0050B5: u'FICHET-BAUCHE',
   0x0050B6: u'GOOD WAY IND. CO., LTD.',
   0x0050B7: u'BOSER TECHNOLOGY CO., LTD.',
   0x0050B8: u'INOVA COMPUTERS GMBH & CO. KG',
   0x0050B9: u'XITRON TECHNOLOGIES, INC.',
   0x0050BA: u'D-LINK',
   0x0050BB: u'CMS TECHNOLOGIES',
   0x0050BC: u'HAMMER STORAGE SOLUTIONS',
   0x0050BD: u'CISCO SYSTEMS, INC.',
   0x0050BE: u'FAST MULTIMEDIA AG',
   0x0050BF: u'MOTOTECH INC.',
   0x0050C0: u'GATAN, INC.',
   0x0050C1: u'GEMFLEX NETWORKS, LTD.',
   0x0050C2: u'IEEE REGISTRATION AUTHORITY',
   0x0050C4: u'IMD',
   0x0050C5: u'ADS TECHNOLOGIES, INC.',
   0x0050C6: u'LOOP TELECOMMUNICATION INTERNATIONAL, INC.',
   0x0050C8: u'ADDONICS COMMUNICATIONS, INC.',
   0x0050C9: u'MASPRO DENKOH CORP.',
   0x0050CA: u'NET TO NET TECHNOLOGIES',
   0x0050CB: u'JETTER',
   0x0050CC: u'XYRATEX',
   0x0050CD: u'DIGIANSWER A/S',
   0x0050CE: u'LG INTERNATIONAL CORP.',
   0x0050CF: u'VANLINK COMMUNICATION TECHNOLOGY RESEARCH INSTITUTE',
   0x0050D0: u'MINERVA SYSTEMS',
   0x0050D1: u'CISCO SYSTEMS, INC.',
   0x0050D2: u'CMC Electronics Inc',
   0x0050D3: u'DIGITAL AUDIO PROCESSING PTY. LTD.',
   0x0050D4: u'JOOHONG INFORMATION &',
   0x0050D5: u'AD SYSTEMS CORP.',
   0x0050D6: u'ATLAS COPCO TOOLS AB',
   0x0050D7: u'TELSTRAT',
   0x0050D8: u'UNICORN COMPUTER CORP.',
   0x0050D9: u'ENGETRON-ENGENHARIA ELETRONICA IND. e COM. LTDA',
   0x0050DA: u'3COM CORPORATION',
   0x0050DB: u'CONTEMPORARY CONTROL',
   0x0050DC: u'TAS TELEFONBAU A. SCHWABE GMBH & CO. KG',
   0x0050DD: u'SERRA SOLDADURA, S.A.',
   0x0050DE: u'SIGNUM SYSTEMS CORP.',
   0x0050DF: u'AirFiber, Inc.',
   0x0050E1: u'NS TECH ELECTRONICS SDN BHD',
   0x0050E2: u'CISCO SYSTEMS, INC.',
   0x0050E3: u'Terayon Communications Systems',
   0x0050E4: u'APPLE COMPUTER, INC.',
   0x0050E6: u'HAKUSAN CORPORATION',
   0x0050E7: u'PARADISE INNOVATIONS (ASIA)',
   0x0050E8: u'NOMADIX INC.',
   0x0050EA: u'XEL COMMUNICATIONS, INC.',
   0x0050EB: u'ALPHA-TOP CORPORATION',
   0x0050EC: u'OLICOM A/S',
   0x0050ED: u'ANDA NETWORKS',
   0x0050EE: u'TEK DIGITEL CORPORATION',
   0x0050EF: u'SPE Systemhaus GmbH',
   0x0050F0: u'CISCO SYSTEMS, INC.',
   0x0050F1: u'LIBIT SIGNAL PROCESSING, LTD.',
   0x0050F2: u'MICROSOFT CORP.',
   0x0050F3: u'GLOBAL NET INFORMATION CO., Ltd.',
   0x0050F4: u'SIGMATEK GMBH & CO. KG',
   0x0050F6: u'PAN-INTERNATIONAL INDUSTRIAL CORP.',
   0x0050F7: u'VENTURE MANUFACTURING (SINGAPORE) LTD.',
   0x0050F8: u'ENTREGA TECHNOLOGIES, INC.',
   0x0050F9: u'SENSORMATIC ACD',
   0x0050FA: u'OXTEL, LTD.',
   0x0050FB: u'VSK ELECTRONICS',
   0x0050FC: u'EDIMAX TECHNOLOGY CO., LTD.',
   0x0050FD: u'VISIONCOMM CO., LTD.',
   0x0050FE: u'PCTVnet ASA',
   0x0050FF: u'HAKKO ELECTRONICS CO., LTD.',
   0x006000: u'XYCOM INC.',
   0x006001: u'InnoSys, Inc.',
   0x006002: u'SCREEN SUBTITLING SYSTEMS, LTD',
   0x006003: u'TERAOKA WEIGH SYSTEM PTE, LTD.',
   0x006004: u'COMPUTADORES MODULARES SA',
   0x006005: u'FEEDBACK DATA LTD.',
   0x006006: u'SOTEC CO., LTD',
   0x006007: u'ACRES GAMING, INC.',
   0x006008: u'3COM CORPORATION',
   0x006009: u'CISCO SYSTEMS, INC.',
   0x00600A: u'SORD COMPUTER CORPORATION',
   0x00600B: u'LOGWARE GmbH',
   0x00600C: u'APPLIED DATA SYSTEMS, INC.',
   0x00600D: u'Digital Logic GmbH',
   0x00600E: u'WAVENET INTERNATIONAL, INC.',
   0x00600F: u'WESTELL, INC.',
   0x006010: u'NETWORK MACHINES, INC.',
   0x006011: u'CRYSTAL SEMICONDUCTOR CORP.',
   0x006012: u'POWER COMPUTING CORPORATION',
   0x006013: u'NETSTAL MASCHINEN AG',
   0x006014: u'EDEC CO., LTD.',
   0x006015: u'NET2NET CORPORATION',
   0x006016: u'CLARIION',
   0x006017: u'TOKIMEC INC.',
   0x006018: u'STELLAR ONE CORPORATION',
   0x006019: u'Roche Diagnostics',
   0x00601A: u'KEITHLEY INSTRUMENTS',
   0x00601B: u'MESA ELECTRONICS',
   0x00601C: u'TELXON CORPORATION',
   0x00601D: u'LUCENT TECHNOLOGIES',
   0x00601E: u'SOFTLAB, INC.',
   0x00601F: u'STALLION TECHNOLOGIES',
   0x006020: u'PIVOTAL NETWORKING, INC.',
   0x006021: u'DSC CORPORATION',
   0x006022: u'VICOM SYSTEMS, INC.',
   0x006023: u'PERICOM SEMICONDUCTOR CORP.',
   0x006024: u'GRADIENT TECHNOLOGIES, INC.',
   0x006025: u'ACTIVE IMAGING PLC',
   0x006026: u'VIKING COMPONENTS, INC.',
   0x006027: u'Superior Modular Products',
   0x006028: u'MACROVISION CORPORATION',
   0x006029: u'CARY PERIPHERALS INC.',
   0x00602A: u'SYMICRON COMPUTER COMMUNICATIONS, LTD.',
   0x00602B: u'PEAK AUDIO',
   0x00602C: u'LINX Data Terminals, Inc.',
   0x00602D: u'ALERTON TECHNOLOGIES, INC.',
   0x00602E: u'CYCLADES CORPORATION',
   0x00602F: u'CISCO SYSTEMS, INC.',
   0x006030: u'VILLAGE TRONIC ENTWICKLUNG',
   0x006031: u'HRK SYSTEMS',
   0x006032: u'I-CUBE, INC.',
   0x006033: u'ACUITY IMAGING, INC.',
   0x006034: u'ROBERT BOSCH GmbH',
   0x006035: u'DALLAS SEMICONDUCTOR, INC.',
   0x006036: u'AUSTRIAN RESEARCH CENTER SEIBERSDORF',
   0x006037: u'NXP Semiconductors',
   0x006038: u'Nortel Networks',
   0x006039: u'SanCom Technology, Inc.',
   0x00603A: u'QUICK CONTROLS LTD.',
   0x00603B: u'AMTEC spa',
   0x00603C: u'HAGIWARA SYS-COM CO., LTD.',
   0x00603D: u'3CX',
   0x00603E: u'CISCO SYSTEMS, INC.',
   0x00603F: u'PATAPSCO DESIGNS',
   0x006040: u'NETRO CORP.',
   0x006041: u'Yokogawa Electric Corporation',
   0x006042: u'TKS (USA), INC.',
   0x006043: u'ComSoft Systems, Inc.',
   0x006044: u'LITTON/POLY-SCIENTIFIC',
   0x006045: u'PATHLIGHT TECHNOLOGIES',
   0x006046: u'VMETRO, INC.',
   0x006047: u'CISCO SYSTEMS, INC.',
   0x006048: u'EMC CORPORATION',
   0x006049: u'VINA TECHNOLOGIES',
   0x00604A: u'SAIC IDEAS GROUP',
   0x00604B: u'Safe-com GmbH & Co. KG',
   0x00604C: u'SAGEM SA',
   0x00604D: u'MMC NETWORKS, INC.',
   0x00604E: u'CYCLE COMPUTER CORPORATION, INC.',
   0x00604F: u'SUZUKI MFG. CO., LTD.',
   0x006050: u'INTERNIX INC.',
   0x006051: u'QUALITY SEMICONDUCTOR',
   0x006052: u'PERIPHERALS ENTERPRISE CO., Ltd.',
   0x006053: u'TOYODA MACHINE WORKS, LTD.',
   0x006054: u'CONTROLWARE GMBH',
   0x006055: u'CORNELL UNIVERSITY',
   0x006056: u'NETWORK TOOLS, INC.',
   0x006057: u'MURATA MANUFACTURING CO., LTD.',
   0x006058: u'COPPER MOUNTAIN COMMUNICATIONS, INC.',
   0x006059: u'TECHNICAL COMMUNICATIONS CORP.',
   0x00605A: u'CELCORE, INC.',
   0x00605B: u'IntraServer Technology, Inc.',
   0x00605C: u'CISCO SYSTEMS, INC.',
   0x00605D: u'SCANIVALVE CORP.',
   0x00605E: u'LIBERTY TECHNOLOGY NETWORKING',
   0x00605F: u'NIPPON UNISOFT CORPORATION',
   0x006060: u'DAWNING TECHNOLOGIES, INC.',
   0x006061: u'WHISTLE COMMUNICATIONS CORP.',
   0x006062: u'TELESYNC, INC.',
   0x006063: u'PSION DACOM PLC.',
   0x006064: u'NETCOMM LIMITED',
   0x006065: u'BERNECKER & RAINER INDUSTRIE-ELEKTRONIC GmbH',
   0x006066: u'LACROIX TECHNOLGIE',
   0x006067: u'ACER NETXUS INC.',
   0x006068: u'EICON TECHNOLOGY CORPORATION',
   0x006069: u'BROCADE COMMUNICATIONS SYSTEMS, Inc.',
   0x00606A: u'MITSUBISHI WIRELESS COMMUNICATIONS. INC.',
   0x00606B: u'Synclayer Inc.',
   0x00606C: u'ARESCOM',
   0x00606D: u'DIGITAL EQUIPMENT CORP.',
   0x00606E: u'DAVICOM SEMICONDUCTOR, INC.',
   0x00606F: u'CLARION CORPORATION OF AMERICA',
   0x006070: u'CISCO SYSTEMS, INC.',
   0x006071: u'MIDAS LAB, INC.',
   0x006072: u'VXL INSTRUMENTS, LIMITED',
   0x006073: u'REDCREEK COMMUNICATIONS, INC.',
   0x006074: u'QSC AUDIO PRODUCTS',
   0x006075: u'PENTEK, INC.',
   0x006076: u'SCHLUMBERGER TECHNOLOGIES RETAIL PETROLEUM SYSTEMS',
   0x006077: u'PRISA NETWORKS',
   0x006078: u'POWER MEASUREMENT LTD.',
   0x006079: u'Mainstream Data, Inc.',
   0x00607A: u'DVS GmbH',
   0x00607B: u'FORE SYSTEMS, INC.',
   0x00607C: u'WaveAccess, Ltd.',
   0x00607D: u'SENTIENT NETWORKS INC.',
   0x00607E: u'GIGALABS, INC.',
   0x00607F: u'AURORA TECHNOLOGIES, INC.',
   0x006080: u'MICROTRONIX DATACOM LTD.',
   0x006081: u'TV/COM INTERNATIONAL',
   0x006082: u'NOVALINK TECHNOLOGIES, INC.',
   0x006083: u'CISCO SYSTEMS, INC.',
   0x006084: u'DIGITAL VIDEO',
   0x006085: u'Storage Concepts',
   0x006086: u'LOGIC REPLACEMENT TECH. LTD.',
   0x006087: u'KANSAI ELECTRIC CO., LTD.',
   0x006088: u'WHITE MOUNTAIN DSP, INC.',
   0x006089: u'XATA',
   0x00608A: u'CITADEL COMPUTER',
   0x00608B: u'ConferTech International',
   0x00608C: u'3COM CORPORATION',
   0x00608D: u'UNIPULSE CORP.',
   0x00608E: u'HE ELECTRONICS, TECHNOLOGIE & SYSTEMTECHNIK GmbH',
   0x00608F: u'TEKRAM TECHNOLOGY CO., LTD.',
   0x006090: u'ABLE COMMUNICATIONS, INC.',
   0x006091: u'FIRST PACIFIC NETWORKS, INC.',
   0x006092: u'MICRO/SYS, INC.',
   0x006093: u'VARIAN',
   0x006094: u'IBM CORP.',
   0x006095: u'ACCU-TIME SYSTEMS, INC.',
   0x006096: u'T.S. MICROTECH INC.',
   0x006097: u'3COM CORPORATION',
   0x006098: u'HT COMMUNICATIONS',
   0x006099: u'SBE, Inc.',
   0x00609A: u'NJK TECHNO CO.',
   0x00609B: u'ASTRO-MED, INC.',
   0x00609C: u'Perkin-Elmer Incorporated',
   0x00609D: u'PMI FOOD EQUIPMENT GROUP',
   0x00609E: u'ASC X3 - INFORMATION TECHNOLOGY STANDARDS SECRETARIATS',
   0x00609F: u'PHAST CORPORATION',
   0x0060A0: u'SWITCHED NETWORK TECHNOLOGIES, INC.',
   0x0060A1: u'VPNet, Inc.',
   0x0060A2: u'NIHON UNISYS LIMITED CO.',
   0x0060A3: u'CONTINUUM TECHNOLOGY CORP.',
   0x0060A4: u'GRINAKER SYSTEM TECHNOLOGIES',
   0x0060A5: u'PERFORMANCE TELECOM CORP.',
   0x0060A6: u'PARTICLE MEASURING SYSTEMS',
   0x0060A7: u'MICROSENS GmbH & CO. KG',
   0x0060A8: u'TIDOMAT AB',
   0x0060A9: u'GESYTEC MbH',
   0x0060AA: u'INTELLIGENT DEVICES INC. (IDI)',
   0x0060AB: u'LARSCOM INCORPORATED',
   0x0060AC: u'RESILIENCE CORPORATION',
   0x0060AD: u'MegaChips Corporation',
   0x0060AE: u'TRIO INFORMATION SYSTEMS AB',
   0x0060AF: u'PACIFIC MICRO DATA, INC.',
   0x0060B0: u'HEWLETT-PACKARD CO.',
   0x0060B1: u'INPUT/OUTPUT, INC.',
   0x0060B2: u'PROCESS CONTROL CORP.',
   0x0060B3: u'Z-COM, INC.',
   0x0060B4: u'GLENAYRE R&D INC.',
   0x0060B5: u'KEBA GmbH',
   0x0060B6: u'LAND COMPUTER CO., LTD.',
   0x0060B7: u'CHANNELMATIC, INC.',
   0x0060B8: u'CORELIS INC.',
   0x0060B9: u'NITSUKO CORPORATION',
   0x0060BA: u'SAHARA NETWORKS, INC.',
   0x0060BB: u'CABLETRON - NETLINK, INC.',
   0x0060BC: u'KeunYoung Electronics & Communication Co., Ltd.',
   0x0060BD: u'HUBBELL-PULSECOM',
   0x0060BE: u'WEBTRONICS',
   0x0060BF: u'MACRAIGOR SYSTEMS, INC.',
   0x0060C0: u'NERA AS',
   0x0060C1: u'WaveSpan Corporation',
   0x0060C2: u'MPL AG',
   0x0060C3: u'NETVISION CORPORATION',
   0x0060C4: u'SOLITON SYSTEMS K.K.',
   0x0060C5: u'ANCOT CORP.',
   0x0060C6: u'DCS AG',
   0x0060C7: u'AMATI COMMUNICATIONS CORP.',
   0x0060C8: u'KUKA WELDING SYSTEMS & ROBOTS',
   0x0060C9: u'ControlNet, Inc.',
   0x0060CA: u'HARMONIC SYSTEMS INCORPORATED',
   0x0060CB: u'HITACHI ZOSEN CORPORATION',
   0x0060CC: u'EMTRAK, INCORPORATED',
   0x0060CD: u'VideoServer, Inc.',
   0x0060CE: u'ACCLAIM COMMUNICATIONS',
   0x0060CF: u'ALTEON NETWORKS, INC.',
   0x0060D0: u'SNMP RESEARCH INCORPORATED',
   0x0060D1: u'CASCADE COMMUNICATIONS',
   0x0060D2: u'LUCENT TECHNOLOGIES TAIWAN TELECOMMUNICATIONS CO., LTD.',
   0x0060D3: u'AT&T',
   0x0060D4: u'ELDAT COMMUNICATION LTD.',
   0x0060D5: u'MIYACHI TECHNOS CORP.',
   0x0060D6: u'NovAtel Wireless Technologies Ltd.',
   0x0060D7: u'ECOLE POLYTECHNIQUE FEDERALE DE LAUSANNE (EPFL)',
   0x0060D8: u'ELMIC SYSTEMS, INC.',
   0x0060D9: u'TRANSYS NETWORKS INC.',
   0x0060DA: u'JBM ELECTRONICS CO.',
   0x0060DB: u'NTP ELEKTRONIK A/S',
   0x0060DC: u'Toyo Network Systems Co, Ltd.',
   0x0060DD: u'MYRICOM, INC.',
   0x0060DE: u'KAYSER-THREDE GmbH',
   0x0060DF: u'CNT Corporation',
   0x0060E0: u'AXIOM TECHNOLOGY CO., LTD.',
   0x0060E1: u'ORCKIT COMMUNICATIONS LTD.',
   0x0060E2: u'QUEST ENGINEERING & DEVELOPMENT',
   0x0060E3: u'ARBIN INSTRUMENTS',
   0x0060E4: u'COMPUSERVE, INC.',
   0x0060E5: u'FUJI AUTOMATION CO., LTD.',
   0x0060E6: u'SHOMITI SYSTEMS INCORPORATED',
   0x0060E7: u'RANDATA',
   0x0060E8: u'HITACHI COMPUTER PRODUCTS (AMERICA), INC.',
   0x0060E9: u'ATOP TECHNOLOGIES, INC.',
   0x0060EA: u'StreamLogic',
   0x0060EB: u'FOURTHTRACK SYSTEMS',
   0x0060EC: u'HERMARY OPTO ELECTRONICS INC.',
   0x0060ED: u'RICARDO TEST AUTOMATION LTD.',
   0x0060EE: u'APOLLO',
   0x0060EF: u'FLYTECH TECHNOLOGY CO., LTD.',
   0x0060F0: u'JOHNSON & JOHNSON MEDICAL, INC',
   0x0060F1: u'EXP COMPUTER, INC.',
   0x0060F2: u'LASERGRAPHICS, INC.',
   0x0060F3: u'Performance Analysis Broadband, Spirent plc',
   0x0060F4: u'ADVANCED COMPUTER SOLUTIONS, Inc.',
   0x0060F5: u'ICON WEST, INC.',
   0x0060F6: u'NEXTEST COMMUNICATIONS PRODUCTS, INC.',
   0x0060F7: u'DATAFUSION SYSTEMS',
   0x0060F8: u'Loran International Technologies Inc.',
   0x0060F9: u'DIAMOND LANE COMMUNICATIONS',
   0x0060FA: u'EDUCATIONAL TECHNOLOGY RESOURCES, INC.',
   0x0060FB: u'PACKETEER, INC.',
   0x0060FC: u'CONSERVATION THROUGH INNOVATION LTD.',
   0x0060FD: u'NetICs, Inc.',
   0x0060FE: u'LYNX SYSTEM DEVELOPERS, INC.',
   0x0060FF: u'QuVis, Inc.',
   0x0070B0: u'M/A-COM INC. COMPANIES',
   0x0070B3: u'DATA RECALL LTD.',
   0x008000: u'MULTITECH SYSTEMS, INC.',
   0x008001: u'PERIPHONICS CORPORATION',
   0x008002: u'SATELCOM (UK) LTD',
   0x008003: u'HYTEC ELECTRONICS LTD.',
   0x008004: u'ANTLOW COMMUNICATIONS, LTD.',
   0x008005: u'CACTUS COMPUTER INC.',
   0x008006: u'COMPUADD CORPORATION',
   0x008007: u'DLOG NC-SYSTEME',
   0x008008: u'DYNATECH COMPUTER SYSTEMS',
   0x008009: u'JUPITER SYSTEMS, INC.',
   0x00800A: u'JAPAN COMPUTER CORP.',
   0x00800B: u'CSK CORPORATION',
   0x00800C: u'VIDECOM LIMITED',
   0x00800D: u'VOSSWINKEL F.U.',
   0x00800E: u'ATLANTIX CORPORATION',
   0x00800F: u'STANDARD MICROSYSTEMS',
   0x008010: u'COMMODORE INTERNATIONAL',
   0x008011: u'DIGITAL SYSTEMS INT\'L. INC.',
   0x008012: u'INTEGRATED MEASUREMENT SYSTEMS',
   0x008013: u'THOMAS-CONRAD CORPORATION',
   0x008014: u'ESPRIT SYSTEMS',
   0x008015: u'SEIKO SYSTEMS, INC.',
   0x008016: u'WANDEL AND GOLTERMANN',
   0x008017: u'PFU LIMITED',
   0x008018: u'KOBE STEEL, LTD.',
   0x008019: u'DAYNA COMMUNICATIONS, INC.',
   0x00801A: u'BELL ATLANTIC',
   0x00801B: u'KODIAK TECHNOLOGY',
   0x00801C: u'NEWPORT SYSTEMS SOLUTIONS',
   0x00801D: u'INTEGRATED INFERENCE MACHINES',
   0x00801E: u'XINETRON, INC.',
   0x00801F: u'KRUPP ATLAS ELECTRONIK GMBH',
   0x008020: u'NETWORK PRODUCTS',
   0x008021: u'Alcatel Canada Inc.',
   0x008022: u'SCAN-OPTICS',
   0x008023: u'INTEGRATED BUSINESS NETWORKS',
   0x008024: u'KALPANA, INC.',
   0x008025: u'STOLLMANN GMBH',
   0x008026: u'NETWORK PRODUCTS CORPORATION',
   0x008027: u'ADAPTIVE SYSTEMS, INC.',
   0x008028: u'TRADPOST (HK) LTD',
   0x008029: u'EAGLE TECHNOLOGY, INC.',
   0x00802A: u'TEST SYSTEMS & SIMULATIONS INC',
   0x00802B: u'INTEGRATED MARKETING CO',
   0x00802C: u'THE SAGE GROUP PLC',
   0x00802D: u'XYLOGICS INC',
   0x00802E: u'CASTLE ROCK COMPUTING',
   0x00802F: u'NATIONAL INSTRUMENTS CORP.',
   0x008030: u'NEXUS ELECTRONICS',
   0x008031: u'BASYS, CORP.',
   0x008032: u'ACCESS CO., LTD.',
   0x008033: u'FORMATION, INC.',
   0x008034: u'SMT GOUPIL',
   0x008035: u'TECHNOLOGY WORKS, INC.',
   0x008036: u'REFLEX MANUFACTURING SYSTEMS',
   0x008037: u'Ericsson Group',
   0x008038: u'DATA RESEARCH & APPLICATIONS',
   0x008039: u'ALCATEL STC AUSTRALIA',
   0x00803A: u'VARITYPER, INC.',
   0x00803B: u'APT COMMUNICATIONS, INC.',
   0x00803C: u'TVS ELECTRONICS LTD',
   0x00803D: u'SURIGIKEN CO.,  LTD.',
   0x00803E: u'SYNERNETICS',
   0x00803F: u'TATUNG COMPANY',
   0x008040: u'JOHN FLUKE MANUFACTURING CO.',
   0x008041: u'VEB KOMBINAT ROBOTRON',
   0x008042: u'FORCE COMPUTERS',
   0x008043: u'NETWORLD, INC.',
   0x008044: u'SYSTECH COMPUTER CORP.',
   0x008045: u'MATSUSHITA ELECTRIC IND. CO',
   0x008046: u'UNIVERSITY OF TORONTO',
   0x008047: u'IN-NET CORP.',
   0x008048: u'COMPEX INCORPORATED',
   0x008049: u'NISSIN ELECTRIC CO., LTD.',
   0x00804A: u'PRO-LOG',
   0x00804B: u'EAGLE TECHNOLOGIES PTY.LTD.',
   0x00804C: u'CONTEC CO., LTD.',
   0x00804D: u'CYCLONE MICROSYSTEMS, INC.',
   0x00804E: u'APEX COMPUTER COMPANY',
   0x00804F: u'DAIKIN INDUSTRIES, LTD.',
   0x008050: u'ZIATECH CORPORATION',
   0x008051: u'FIBERMUX',
   0x008052: u'TECHNICALLY ELITE CONCEPTS',
   0x008053: u'INTELLICOM, INC.',
   0x008054: u'FRONTIER TECHNOLOGIES CORP.',
   0x008055: u'FERMILAB',
   0x008056: u'SPHINX ELEKTRONIK GMBH',
   0x008057: u'ADSOFT, LTD.',
   0x008058: u'PRINTER SYSTEMS CORPORATION',
   0x008059: u'STANLEY ELECTRIC CO., LTD',
   0x00805A: u'TULIP COMPUTERS INTERNAT\'L B.V',
   0x00805B: u'CONDOR SYSTEMS, INC.',
   0x00805C: u'AGILIS CORPORATION',
   0x00805D: u'CANSTAR',
   0x00805E: u'LSI LOGIC CORPORATION',
   0x00805F: u'COMPAQ COMPUTER CORPORATION',
   0x008060: u'NETWORK INTERFACE CORPORATION',
   0x008061: u'LITTON SYSTEMS, INC.',
   0x008062: u'INTERFACE  CO.',
   0x008063: u'RICHARD HIRSCHMANN GMBH & CO.',
   0x008064: u'WYSE TECHNOLOGY',
   0x008065: u'CYBERGRAPHIC SYSTEMS PTY LTD.',
   0x008066: u'ARCOM CONTROL SYSTEMS, LTD.',
   0x008067: u'SQUARE D COMPANY',
   0x008068: u'YAMATECH SCIENTIFIC LTD.',
   0x008069: u'COMPUTONE SYSTEMS',
   0x00806A: u'ERI (EMPAC RESEARCH INC.)',
   0x00806B: u'SCHMID TELECOMMUNICATION',
   0x00806C: u'CEGELEC PROJECTS LTD',
   0x00806D: u'CENTURY SYSTEMS CORP.',
   0x00806E: u'NIPPON STEEL CORPORATION',
   0x00806F: u'ONELAN LTD.',
   0x008070: u'COMPUTADORAS MICRON',
   0x008071: u'SAI TECHNOLOGY',
   0x008072: u'MICROPLEX SYSTEMS LTD.',
   0x008073: u'DWB ASSOCIATES',
   0x008074: u'FISHER CONTROLS',
   0x008075: u'PARSYTEC GMBH',
   0x008076: u'MCNC',
   0x008077: u'BROTHER INDUSTRIES, LTD.',
   0x008078: u'PRACTICAL PERIPHERALS, INC.',
   0x008079: u'MICROBUS DESIGNS LTD.',
   0x00807A: u'AITECH SYSTEMS LTD.',
   0x00807B: u'ARTEL COMMUNICATIONS CORP.',
   0x00807C: u'FIBERCOM, INC.',
   0x00807D: u'EQUINOX SYSTEMS INC.',
   0x00807E: u'SOUTHERN PACIFIC LTD.',
   0x00807F: u'DY-4 INCORPORATED',
   0x008080: u'DATAMEDIA CORPORATION',
   0x008081: u'KENDALL SQUARE RESEARCH CORP.',
   0x008082: u'PEP MODULAR COMPUTERS GMBH',
   0x008083: u'AMDAHL',
   0x008084: u'THE CLOUD INC.',
   0x008085: u'H-THREE SYSTEMS CORPORATION',
   0x008086: u'COMPUTER GENERATION INC.',
   0x008087: u'OKI ELECTRIC INDUSTRY CO., LTD',
   0x008088: u'VICTOR COMPANY OF JAPAN, LTD.',
   0x008089: u'TECNETICS (PTY) LTD.',
   0x00808A: u'SUMMIT MICROSYSTEMS CORP.',
   0x00808B: u'DACOLL LIMITED',
   0x00808C: u'NetScout Systems, Inc.',
   0x00808D: u'WESTCOAST TECHNOLOGY B.V.',
   0x00808E: u'RADSTONE TECHNOLOGY',
   0x00808F: u'C. ITOH ELECTRONICS, INC.',
   0x008090: u'MICROTEK INTERNATIONAL, INC.',
   0x008091: u'TOKYO ELECTRIC CO.,LTD',
   0x008092: u'JAPAN COMPUTER INDUSTRY, INC.',
   0x008093: u'XYRON CORPORATION',
   0x008094: u'ALFA LAVAL AUTOMATION AB',
   0x008095: u'BASIC MERTON HANDELSGES.M.B.H.',
   0x008096: u'HUMAN DESIGNED SYSTEMS, INC.',
   0x008097: u'CENTRALP AUTOMATISMES',
   0x008098: u'TDK CORPORATION',
   0x008099: u'KLOCKNER MOELLER IPC',
   0x00809A: u'NOVUS NETWORKS LTD',
   0x00809B: u'JUSTSYSTEM CORPORATION',
   0x00809C: u'LUXCOM, INC.',
   0x00809D: u'Commscraft Ltd.',
   0x00809E: u'DATUS GMBH',
   0x00809F: u'ALCATEL BUSINESS SYSTEMS',
   0x0080A0: u'EDISA HEWLETT PACKARD S/A',
   0x0080A1: u'MICROTEST, INC.',
   0x0080A2: u'CREATIVE ELECTRONIC SYSTEMS',
   0x0080A3: u'LANTRONIX',
   0x0080A4: u'LIBERTY ELECTRONICS',
   0x0080A5: u'SPEED INTERNATIONAL',
   0x0080A6: u'REPUBLIC TECHNOLOGY, INC.',
   0x0080A7: u'MEASUREX CORP.',
   0x0080A8: u'VITACOM CORPORATION',
   0x0080A9: u'CLEARPOINT RESEARCH',
   0x0080AA: u'MAXPEED',
   0x0080AB: u'DUKANE NETWORK INTEGRATION',
   0x0080AC: u'IMLOGIX, DIVISION OF GENESYS',
   0x0080AD: u'CNET TECHNOLOGY, INC.',
   0x0080AE: u'HUGHES NETWORK SYSTEMS',
   0x0080AF: u'ALLUMER CO., LTD.',
   0x0080B0: u'ADVANCED INFORMATION',
   0x0080B1: u'SOFTCOM A/S',
   0x0080B2: u'NETWORK EQUIPMENT TECHNOLOGIES',
   0x0080B3: u'AVAL DATA CORPORATION',
   0x0080B4: u'SOPHIA SYSTEMS',
   0x0080B5: u'UNITED NETWORKS INC.',
   0x0080B6: u'THEMIS COMPUTER',
   0x0080B7: u'STELLAR COMPUTER',
   0x0080B8: u'BUG, INCORPORATED',
   0x0080B9: u'ARCHE TECHNOLIGIES INC.',
   0x0080BA: u'SPECIALIX (ASIA) PTE, LTD',
   0x0080BB: u'HUGHES LAN SYSTEMS',
   0x0080BC: u'HITACHI ENGINEERING CO., LTD',
   0x0080BD: u'THE FURUKAWA ELECTRIC CO., LTD',
   0x0080BE: u'ARIES RESEARCH',
   0x0080BF: u'TAKAOKA ELECTRIC MFG. CO. LTD.',
   0x0080C0: u'PENRIL DATACOMM',
   0x0080C1: u'LANEX CORPORATION',
   0x0080C2: u'IEEE 802.1 COMMITTEE',
   0x0080C3: u'BICC INFORMATION SYSTEMS & SVC',
   0x0080C4: u'DOCUMENT TECHNOLOGIES, INC.',
   0x0080C5: u'NOVELLCO DE MEXICO',
   0x0080C6: u'NATIONAL DATACOMM CORPORATION',
   0x0080C7: u'XIRCOM',
   0x0080C8: u'D-LINK SYSTEMS, INC.',
   0x0080C9: u'ALBERTA MICROELECTRONIC CENTRE',
   0x0080CA: u'NETCOM RESEARCH INCORPORATED',
   0x0080CB: u'FALCO DATA PRODUCTS',
   0x0080CC: u'MICROWAVE BYPASS SYSTEMS',
   0x0080CD: u'MICRONICS COMPUTER, INC.',
   0x0080CE: u'BROADCAST TELEVISION SYSTEMS',
   0x0080CF: u'EMBEDDED PERFORMANCE INC.',
   0x0080D0: u'COMPUTER PERIPHERALS, INC.',
   0x0080D1: u'KIMTRON CORPORATION',
   0x0080D2: u'SHINNIHONDENKO CO., LTD.',
   0x0080D3: u'SHIVA CORP.',
   0x0080D4: u'CHASE RESEARCH LTD.',
   0x0080D5: u'CADRE TECHNOLOGIES',
   0x0080D6: u'NUVOTECH, INC.',
   0x0080D7: u'Fantum Engineering',
   0x0080D8: u'NETWORK PERIPHERALS INC.',
   0x0080D9: u'EMK ELEKTRONIK',
   0x0080DA: u'BRUEL & KJAER',
   0x0080DB: u'GRAPHON CORPORATION',
   0x0080DC: u'PICKER INTERNATIONAL',
   0x0080DD: u'GMX INC/GIMIX',
   0x0080DE: u'GIPSI S.A.',
   0x0080DF: u'ADC CODENOLL TECHNOLOGY CORP.',
   0x0080E0: u'XTP SYSTEMS, INC.',
   0x0080E1: u'STMICROELECTRONICS',
   0x0080E2: u'T.D.I. CO., LTD.',
   0x0080E3: u'CORAL NETWORK CORPORATION',
   0x0080E4: u'NORTHWEST DIGITAL SYSTEMS, INC',
   0x0080E5: u'LSI Logic Corporation',
   0x0080E6: u'PEER NETWORKS, INC.',
   0x0080E7: u'LYNWOOD SCIENTIFIC DEV. LTD.',
   0x0080E8: u'CUMULUS CORPORATIION',
   0x0080E9: u'Madge Ltd.',
   0x0080EA: u'ADVA Optical Networking Ltd.',
   0x0080EB: u'COMPCONTROL B.V.',
   0x0080EC: u'SUPERCOMPUTING SOLUTIONS, INC.',
   0x0080ED: u'IQ TECHNOLOGIES, INC.',
   0x0080EE: u'THOMSON CSF',
   0x0080EF: u'RATIONAL',
   0x0080F0: u'Panasonic Communications Co., Ltd.',
   0x0080F1: u'OPUS SYSTEMS',
   0x0080F2: u'RAYCOM SYSTEMS INC',
   0x0080F3: u'SUN ELECTRONICS CORP.',
   0x0080F4: u'TELEMECANIQUE ELECTRIQUE',
   0x0080F5: u'QUANTEL LTD',
   0x0080F6: u'SYNERGY MICROSYSTEMS',
   0x0080F7: u'ZENITH ELECTRONICS',
   0x0080F8: u'MIZAR, INC.',
   0x0080F9: u'HEURIKON CORPORATION',
   0x0080FA: u'RWT GMBH',
   0x0080FB: u'BVM LIMITED',
   0x0080FC: u'AVATAR CORPORATION',
   0x0080FD: u'EXSCEED CORPRATION',
   0x0080FE: u'AZURE TECHNOLOGIES, INC.',
   0x0080FF: u'SOC. DE TELEINFORMATIQUE RTC',
   0x009000: u'DIAMOND MULTIMEDIA',
   0x009001: u'NISHIMU ELECTRONICS INDUSTRIES CO., LTD.',
   0x009002: u'ALLGON AB',
   0x009003: u'APLIO',
   0x009004: u'3COM EUROPE LTD.',
   0x009005: u'PROTECH SYSTEMS CO., LTD.',
   0x009006: u'HAMAMATSU PHOTONICS K.K.',
   0x009007: u'DOMEX TECHNOLOGY CORP.',
   0x009008: u'HanA Systems Inc.',
   0x009009: u'i Controls, Inc.',
   0x00900A: u'PROTON ELECTRONIC INDUSTRIAL CO., LTD.',
   0x00900B: u'LANNER ELECTRONICS, INC.',
   0x00900C: u'CISCO SYSTEMS, INC.',
   0x00900D: u'Overland Storage Inc.',
   0x00900E: u'HANDLINK TECHNOLOGIES, INC.',
   0x00900F: u'KAWASAKI HEAVY INDUSTRIES, LTD',
   0x009010: u'SIMULATION LABORATORIES, INC.',
   0x009011: u'WAVTrace, Inc.',
   0x009012: u'GLOBESPAN SEMICONDUCTOR, INC.',
   0x009013: u'SAMSAN CORP.',
   0x009014: u'ROTORK INSTRUMENTS, LTD.',
   0x009015: u'CENTIGRAM COMMUNICATIONS CORP.',
   0x009016: u'ZAC',
   0x009017: u'ZYPCOM, INC.',
   0x009018: u'ITO ELECTRIC INDUSTRY CO, LTD.',
   0x009019: u'HERMES ELECTRONICS CO., LTD.',
   0x00901A: u'UNISPHERE SOLUTIONS',
   0x00901B: u'DIGITAL CONTROLS',
   0x00901C: u'mps Software Gmbh',
   0x00901D: u'PEC (NZ) LTD.',
   0x00901E: u'SELESTA INGEGNE RIA S.P.A.',
   0x00901F: u'ADTEC PRODUCTIONS, INC.',
   0x009020: u'PHILIPS ANALYTICAL X-RAY B.V.',
   0x009021: u'CISCO SYSTEMS, INC.',
   0x009022: u'IVEX',
   0x009023: u'ZILOG INC.',
   0x009024: u'PIPELINKS, INC.',
   0x009025: u'VISION SYSTEMS LTD. PTY',
   0x009026: u'ADVANCED SWITCHING COMMUNICATIONS, INC.',
   0x009027: u'INTEL CORPORATION',
   0x009028: u'NIPPON SIGNAL CO., LTD.',
   0x009029: u'CRYPTO AG',
   0x00902A: u'COMMUNICATION DEVICES, INC.',
   0x00902B: u'CISCO SYSTEMS, INC.',
   0x00902C: u'DATA & CONTROL EQUIPMENT LTD.',
   0x00902D: u'DATA ELECTRONICS (AUST.) PTY, LTD.',
   0x00902E: u'NAMCO LIMITED',
   0x00902F: u'NETCORE SYSTEMS, INC.',
   0x009030: u'HONEYWELL-DATING',
   0x009031: u'MYSTICOM, LTD.',
   0x009032: u'PELCOMBE GROUP LTD.',
   0x009033: u'INNOVAPHONE AG',
   0x009034: u'IMAGIC, INC.',
   0x009035: u'ALPHA TELECOM, INC.',
   0x009036: u'ens, inc.',
   0x009037: u'ACUCOMM, INC.',
   0x009038: u'FOUNTAIN TECHNOLOGIES, INC.',
   0x009039: u'SHASTA NETWORKS',
   0x00903A: u'NIHON MEDIA TOOL INC.',
   0x00903B: u'TriEMS Research Lab, Inc.',
   0x00903C: u'ATLANTIC NETWORK SYSTEMS',
   0x00903D: u'BIOPAC SYSTEMS, INC.',
   0x00903E: u'N.V. PHILIPS INDUSTRIAL ACTIVITIES',
   0x00903F: u'AZTEC RADIOMEDIA',
   0x009040: u'Siemens Network Convergence LLC',
   0x009041: u'APPLIED DIGITAL ACCESS',
   0x009042: u'ECCS, Inc.',
   0x009043: u'NICHIBEI DENSHI CO., LTD.',
   0x009044: u'ASSURED DIGITAL, INC.',
   0x009045: u'Marconi Communications',
   0x009046: u'DEXDYNE, LTD.',
   0x009047: u'GIGA FAST E. LTD.',
   0x009048: u'ZEAL CORPORATION',
   0x009049: u'ENTRIDIA CORPORATION',
   0x00904A: u'CONCUR SYSTEM TECHNOLOGIES',
   0x00904B: u'GemTek Technology Co., Ltd.',
   0x00904C: u'EPIGRAM, INC.',
   0x00904D: u'SPEC S.A.',
   0x00904E: u'DELEM BV',
   0x00904F: u'ABB POWER T&D COMPANY, INC.',
   0x009050: u'TELESTE OY',
   0x009051: u'ULTIMATE TECHNOLOGY CORP.',
   0x009052: u'SELCOM ELETTRONICA S.R.L.',
   0x009053: u'DAEWOO ELECTRONICS CO., LTD.',
   0x009054: u'INNOVATIVE SEMICONDUCTORS, INC',
   0x009055: u'PARKER HANNIFIN CORPORATION COMPUMOTOR DIVISION',
   0x009056: u'TELESTREAM, INC.',
   0x009057: u'AANetcom, Inc.',
   0x009058: u'Ultra Electronics Ltd., Command and Control Systems',
   0x009059: u'TELECOM DEVICE K.K.',
   0x00905A: u'DEARBORN GROUP, INC.',
   0x00905B: u'RAYMOND AND LAE ENGINEERING',
   0x00905C: u'EDMI',
   0x00905D: u'NETCOM SICHERHEITSTECHNIK GmbH',
   0x00905E: u'RAULAND-BORG CORPORATION',
   0x00905F: u'CISCO SYSTEMS, INC.',
   0x009060: u'SYSTEM CREATE CORP.',
   0x009061: u'PACIFIC RESEARCH & ENGINEERING CORPORATION',
   0x009062: u'ICP VORTEX COMPUTERSYSTEME GmbH',
   0x009063: u'COHERENT COMMUNICATIONS SYSTEMS CORPORATION',
   0x009064: u'THOMSON BROADCAST SYSTEMS',
   0x009065: u'FINISAR CORPORATION',
   0x009066: u'Troika Networks, Inc.',
   0x009067: u'WalkAbout Computers, Inc.',
   0x009068: u'DVT CORP.',
   0x009069: u'JUNIPER NETWORKS, INC.',
   0x00906A: u'TURNSTONE SYSTEMS, INC.',
   0x00906B: u'APPLIED RESOURCES, INC.',
   0x00906C: u'Sartorius Hamburg GmbH',
   0x00906D: u'CISCO SYSTEMS, INC.',
   0x00906E: u'PRAXON, INC.',
   0x00906F: u'CISCO SYSTEMS, INC.',
   0x009070: u'NEO NETWORKS, INC.',
   0x009071: u'Applied Innovation Inc.',
   0x009072: u'SIMRAD AS',
   0x009073: u'GAIO TECHNOLOGY',
   0x009074: u'ARGON NETWORKS, INC.',
   0x009075: u'NEC DO BRASIL S.A.',
   0x009076: u'FMT AIRCRAFT GATE SUPPORT SYSTEMS AB',
   0x009077: u'ADVANCED FIBRE COMMUNICATIONS',
   0x009078: u'MER TELEMANAGEMENT SOLUTIONS, LTD.',
   0x009079: u'ClearOne, Inc.',
   0x00907A: u'SPECTRALINK CORP.',
   0x00907B: u'E-TECH, INC.',
   0x00907C: u'DIGITALCAST, INC.',
   0x00907D: u'Lake Communications',
   0x00907E: u'VETRONIX CORP.',
   0x00907F: u'WatchGuard Technologies, Inc.',
   0x009080: u'NOT LIMITED, INC.',
   0x009081: u'ALOHA NETWORKS, INC.',
   0x009082: u'FORCE INSTITUTE',
   0x009083: u'TURBO COMMUNICATION, INC.',
   0x009084: u'ATECH SYSTEM',
   0x009085: u'GOLDEN ENTERPRISES, INC.',
   0x009086: u'CISCO SYSTEMS, INC.',
   0x009087: u'ITIS',
   0x009088: u'BAXALL SECURITY LTD.',
   0x009089: u'SOFTCOM MICROSYSTEMS, INC.',
   0x00908A: u'BAYLY COMMUNICATIONS, INC.',
   0x00908B: u'PFU Systems, Inc.',
   0x00908C: u'ETREND ELECTRONICS, INC.',
   0x00908D: u'VICKERS ELECTRONICS SYSTEMS',
   0x00908E: u'Nortel Networks Broadband Access',
   0x00908F: u'AUDIO CODES LTD.',
   0x009090: u'I-BUS',
   0x009091: u'DigitalScape, Inc.',
   0x009092: u'CISCO SYSTEMS, INC.',
   0x009093: u'NANAO CORPORATION',
   0x009094: u'OSPREY TECHNOLOGIES, INC.',
   0x009095: u'UNIVERSAL AVIONICS',
   0x009096: u'ASKEY COMPUTER CORP.',
   0x009097: u'SYCAMORE NETWORKS',
   0x009098: u'SBC DESIGNS, INC.',
   0x009099: u'ALLIED TELESIS, K.K.',
   0x00909A: u'ONE WORLD SYSTEMS, INC.',
   0x00909B: u'MARKPOINT AB',
   0x00909C: u'Terayon Communications Systems',
   0x00909D: u'NovaTech Process Solutions, LLC',
   0x00909E: u'Critical IO, LLC',
   0x00909F: u'DIGI-DATA CORPORATION',
   0x0090A0: u'8X8 INC.',
   0x0090A1: u'FLYING PIG SYSTEMS, LTD.',
   0x0090A2: u'CYBERTAN TECHNOLOGY, INC.',
   0x0090A3: u'Corecess Inc.',
   0x0090A4: u'ALTIGA NETWORKS',
   0x0090A5: u'SPECTRA LOGIC',
   0x0090A6: u'CISCO SYSTEMS, INC.',
   0x0090A7: u'CLIENTEC CORPORATION',
   0x0090A8: u'NineTiles Networks, Ltd.',
   0x0090A9: u'WESTERN DIGITAL',
   0x0090AA: u'INDIGO ACTIVE VISION SYSTEMS LIMITED',
   0x0090AB: u'CISCO SYSTEMS, INC.',
   0x0090AC: u'OPTIVISION, INC.',
   0x0090AD: u'ASPECT ELECTRONICS, INC.',
   0x0090AE: u'ITALTEL S.p.A.',
   0x0090AF: u'J. MORITA MFG. CORP.',
   0x0090B0: u'VADEM',
   0x0090B1: u'CISCO SYSTEMS, INC.',
   0x0090B2: u'AVICI SYSTEMS INC.',
   0x0090B3: u'AGRANAT SYSTEMS',
   0x0090B4: u'WILLOWBROOK TECHNOLOGIES',
   0x0090B5: u'NIKON CORPORATION',
   0x0090B6: u'FIBEX SYSTEMS',
   0x0090B7: u'DIGITAL LIGHTWAVE, INC.',
   0x0090B8: u'ROHDE & SCHWARZ GMBH & CO. KG',
   0x0090B9: u'BERAN INSTRUMENTS LTD.',
   0x0090BA: u'VALID NETWORKS, INC.',
   0x0090BB: u'TAINET COMMUNICATION SYSTEM Corp.',
   0x0090BC: u'TELEMANN CO., LTD.',
   0x0090BD: u'OMNIA COMMUNICATIONS, INC.',
   0x0090BE: u'IBC/INTEGRATED BUSINESS COMPUTERS',
   0x0090BF: u'CISCO SYSTEMS, INC.',
   0x0090C0: u'K.J. LAW ENGINEERS, INC.',
   0x0090C1: u'Peco II, Inc.',
   0x0090C2: u'JK microsystems, Inc.',
   0x0090C3: u'TOPIC SEMICONDUCTOR CORP.',
   0x0090C4: u'JAVELIN SYSTEMS, INC.',
   0x0090C5: u'INTERNET MAGIC, INC.',
   0x0090C6: u'OPTIM SYSTEMS, INC.',
   0x0090C7: u'ICOM INC.',
   0x0090C8: u'WAVERIDER COMMUNICATIONS (CANADA) INC.',
   0x0090C9: u'DPAC Technologies',
   0x0090CA: u'ACCORD VIDEO TELECOMMUNICATIONS, LTD.',
   0x0090CB: u'Wireless OnLine, Inc.',
   0x0090CC: u'PLANET COMMUNICATIONS, INC.',
   0x0090CD: u'ENT-EMPRESA NACIONAL DE TELECOMMUNICACOES, S.A.',
   0x0090CE: u'TETRA GmbH',
   0x0090CF: u'NORTEL',
   0x0090D0: u'Thomson Telecom Belgium',
   0x0090D1: u'LEICHU ENTERPRISE CO., LTD.',
   0x0090D2: u'ARTEL VIDEO SYSTEMS',
   0x0090D3: u'GIESECKE & DEVRIENT GmbH',
   0x0090D4: u'BindView Development Corp.',
   0x0090D5: u'EUPHONIX, INC.',
   0x0090D6: u'CRYSTAL GROUP',
   0x0090D7: u'NetBoost Corp.',
   0x0090D8: u'WHITECROSS SYSTEMS',
   0x0090D9: u'CISCO SYSTEMS, INC.',
   0x0090DA: u'DYNARC, INC.',
   0x0090DB: u'NEXT LEVEL COMMUNICATIONS',
   0x0090DC: u'TECO INFORMATION SYSTEMS',
   0x0090DD: u'THE MIHARU COMMUNICATIONS CO., LTD.',
   0x0090DE: u'CARDKEY SYSTEMS, INC.',
   0x0090DF: u'MITSUBISHI CHEMICAL AMERICA, INC.',
   0x0090E0: u'SYSTRAN CORP.',
   0x0090E1: u'TELENA S.P.A.',
   0x0090E2: u'DISTRIBUTED PROCESSING TECHNOLOGY',
   0x0090E3: u'AVEX ELECTRONICS INC.',
   0x0090E4: u'NEC AMERICA, INC.',
   0x0090E5: u'TEKNEMA, INC.',
   0x0090E6: u'ACER LABORATORIES, INC.',
   0x0090E7: u'HORSCH ELEKTRONIK AG',
   0x0090E8: u'MOXA TECHNOLOGIES CORP., LTD.',
   0x0090E9: u'JANZ COMPUTER AG',
   0x0090EA: u'ALPHA TECHNOLOGIES, INC.',
   0x0090EB: u'SENTRY TELECOM SYSTEMS',
   0x0090EC: u'PYRESCOM',
   0x0090ED: u'CENTRAL SYSTEM RESEARCH CO., LTD.',
   0x0090EE: u'PERSONAL COMMUNICATIONS TECHNOLOGIES',
   0x0090EF: u'INTEGRIX, INC.',
   0x0090F0: u'Harmonic Video Systems Ltd.',
   0x0090F1: u'DOT HILL SYSTEMS CORPORATION',
   0x0090F2: u'CISCO SYSTEMS, INC.',
   0x0090F3: u'ASPECT COMMUNICATIONS',
   0x0090F4: u'LIGHTNING INSTRUMENTATION',
   0x0090F5: u'CLEVO CO.',
   0x0090F6: u'ESCALATE NETWORKS, INC.',
   0x0090F7: u'NBASE COMMUNICATIONS LTD.',
   0x0090F8: u'MEDIATRIX TELECOM',
   0x0090F9: u'LEITCH',
   0x0090FA: u'EMULEX Corp',
   0x0090FB: u'PORTWELL, INC.',
   0x0090FC: u'NETWORK COMPUTING DEVICES',
   0x0090FD: u'CopperCom, Inc.',
   0x0090FE: u'ELECOM CO., LTD.  (LANEED DIV.)',
   0x0090FF: u'TELLUS TECHNOLOGY INC.',
   0x0091D6: u'Crystal Group, Inc.',
   0x009D8E: u'CARDIAC RECORDERS, INC.',
   0x00A000: u'CENTILLION NETWORKS, INC.',
   0x00A001: u'DRS Signal Solutions',
   0x00A002: u'LEEDS & NORTHRUP AUSTRALIA PTY LTD',
   0x00A003: u'STAEFA CONTROL SYSTEM',
   0x00A004: u'NETPOWER, INC.',
   0x00A005: u'DANIEL INSTRUMENTS, LTD.',
   0x00A006: u'IMAGE DATA PROCESSING SYSTEM GROUP',
   0x00A007: u'APEXX TECHNOLOGY, INC.',
   0x00A008: u'NETCORP',
   0x00A009: u'WHITETREE NETWORK',
   0x00A00A: u'Airspan',
   0x00A00B: u'COMPUTEX CO., LTD.',
   0x00A00C: u'KINGMAX TECHNOLOGY, INC.',
   0x00A00D: u'THE PANDA PROJECT',
   0x00A00E: u'VISUAL NETWORKS, INC.',
   0x00A00F: u'Broadband Technologies',
   0x00A010: u'SYSLOGIC DATENTECHNIK AG',
   0x00A011: u'MUTOH INDUSTRIES LTD.',
   0x00A012: u'B.A.T.M. ADVANCED TECHNOLOGIES',
   0x00A013: u'TELTREND LTD.',
   0x00A014: u'CSIR',
   0x00A015: u'WYLE',
   0x00A016: u'MICROPOLIS CORP.',
   0x00A017: u'J B M CORPORATION',
   0x00A018: u'CREATIVE CONTROLLERS, INC.',
   0x00A019: u'NEBULA CONSULTANTS, INC.',
   0x00A01A: u'BINAR ELEKTRONIK AB',
   0x00A01B: u'PREMISYS COMMUNICATIONS, INC.',
   0x00A01C: u'NASCENT NETWORKS CORPORATION',
   0x00A01D: u'SIXNET',
   0x00A01E: u'EST CORPORATION',
   0x00A01F: u'TRICORD SYSTEMS, INC.',
   0x00A020: u'CITICORP/TTI',
   0x00A021: u'General Dynamics',
   0x00A022: u'CENTRE FOR DEVELOPMENT OF ADVANCED COMPUTING',
   0x00A023: u'APPLIED CREATIVE TECHNOLOGY, INC.',
   0x00A024: u'3COM CORPORATION',
   0x00A025: u'REDCOM LABS INC.',
   0x00A026: u'TELDAT, S.A.',
   0x00A027: u'FIREPOWER SYSTEMS, INC.',
   0x00A028: u'CONNER PERIPHERALS',
   0x00A029: u'COULTER CORPORATION',
   0x00A02A: u'TRANCELL SYSTEMS',
   0x00A02B: u'TRANSITIONS RESEARCH CORP.',
   0x00A02C: u'interWAVE Communications',
   0x00A02D: u'1394 Trade Association',
   0x00A02E: u'BRAND COMMUNICATIONS, LTD.',
   0x00A02F: u'PIRELLI CAVI',
   0x00A030: u'CAPTOR NV/SA',
   0x00A031: u'HAZELTINE CORPORATION, MS 1-17',
   0x00A032: u'GES SINGAPORE PTE. LTD.',
   0x00A033: u'imc MeBsysteme GmbH',
   0x00A034: u'AXEL',
   0x00A035: u'CYLINK CORPORATION',
   0x00A036: u'APPLIED NETWORK TECHNOLOGY',
   0x00A037: u'DATASCOPE CORPORATION',
   0x00A038: u'EMAIL ELECTRONICS',
   0x00A039: u'ROSS TECHNOLOGY, INC.',
   0x00A03A: u'KUBOTEK CORPORATION',
   0x00A03B: u'TOSHIN ELECTRIC CO., LTD.',
   0x00A03C: u'EG&G NUCLEAR INSTRUMENTS',
   0x00A03D: u'OPTO-22',
   0x00A03E: u'ATM FORUM',
   0x00A03F: u'COMPUTER SOCIETY MICROPROCESSOR & MICROPROCESSOR STANDARDS C',
   0x00A040: u'APPLE COMPUTER',
   0x00A041: u'INFICON',
   0x00A042: u'SPUR PRODUCTS CORP.',
   0x00A043: u'AMERICAN TECHNOLOGY LABS, INC.',
   0x00A044: u'NTT IT CO., LTD.',
   0x00A045: u'PHOENIX CONTACT GMBH & CO.',
   0x00A046: u'SCITEX CORP. LTD.',
   0x00A047: u'INTEGRATED FITNESS CORP.',
   0x00A048: u'QUESTECH, LTD.',
   0x00A049: u'DIGITECH INDUSTRIES, INC.',
   0x00A04A: u'NISSHIN ELECTRIC CO., LTD.',
   0x00A04B: u'TFL LAN INC.',
   0x00A04C: u'INNOVATIVE SYSTEMS & TECHNOLOGIES, INC.',
   0x00A04D: u'EDA INSTRUMENTS, INC.',
   0x00A04E: u'VOELKER TECHNOLOGIES, INC.',
   0x00A04F: u'AMERITEC CORP.',
   0x00A050: u'CYPRESS SEMICONDUCTOR',
   0x00A051: u'ANGIA COMMUNICATIONS. INC.',
   0x00A052: u'STANILITE ELECTRONICS PTY. LTD',
   0x00A053: u'COMPACT DEVICES, INC.',
   0x00A054: u'PRIVATE',
   0x00A055: u'Data Device Corporation',
   0x00A056: u'MICROPROSS',
   0x00A057: u'LANCOM Systems GmbH',
   0x00A058: u'GLORY, LTD.',
   0x00A059: u'HAMILTON HALLMARK',
   0x00A05A: u'KOFAX IMAGE PRODUCTS',
   0x00A05B: u'MARQUIP, INC.',
   0x00A05C: u'INVENTORY CONVERSION, INC./',
   0x00A05D: u'CS COMPUTER SYSTEME GmbH',
   0x00A05E: u'MYRIAD LOGIC INC.',
   0x00A05F: u'BTG ENGINEERING BV',
   0x00A060: u'ACER PERIPHERALS, INC.',
   0x00A061: u'PURITAN BENNETT',
   0x00A062: u'AES PRODATA',
   0x00A063: u'JRL SYSTEMS, INC.',
   0x00A064: u'KVB/ANALECT',
   0x00A065: u'Symantec Corporation',
   0x00A066: u'ISA CO., LTD.',
   0x00A067: u'NETWORK SERVICES GROUP',
   0x00A068: u'BHP LIMITED',
   0x00A069: u'Symmetricom, Inc.',
   0x00A06A: u'Verilink Corporation',
   0x00A06B: u'DMS DORSCH MIKROSYSTEM GMBH',
   0x00A06C: u'SHINDENGEN ELECTRIC MFG. CO., LTD.',
   0x00A06D: u'MANNESMANN TALLY CORPORATION',
   0x00A06E: u'AUSTRON, INC.',
   0x00A06F: u'THE APPCON GROUP, INC.',
   0x00A070: u'COASTCOM',
   0x00A071: u'VIDEO LOTTERY TECHNOLOGIES,INC',
   0x00A072: u'OVATION SYSTEMS LTD.',
   0x00A073: u'COM21, INC.',
   0x00A074: u'PERCEPTION TECHNOLOGY',
   0x00A075: u'MICRON TECHNOLOGY, INC.',
   0x00A076: u'CARDWARE LAB, INC.',
   0x00A077: u'FUJITSU NEXION, INC.',
   0x00A078: u'Marconi Communications',
   0x00A079: u'ALPS ELECTRIC (USA), INC.',
   0x00A07A: u'ADVANCED PERIPHERALS TECHNOLOGIES, INC.',
   0x00A07B: u'DAWN COMPUTER INCORPORATION',
   0x00A07C: u'TONYANG NYLON CO., LTD.',
   0x00A07D: u'SEEQ TECHNOLOGY, INC.',
   0x00A07E: u'AVID TECHNOLOGY, INC.',
   0x00A07F: u'GSM-SYNTEL, LTD.',
   0x00A080: u'SBE, Inc.',
   0x00A081: u'ALCATEL DATA NETWORKS',
   0x00A082: u'NKT ELEKTRONIK A/S',
   0x00A083: u'ASIMMPHONY TURKEY',
   0x00A084: u'DATAPLEX PTY. LTD.',
   0x00A085: u'PRIVATE',
   0x00A086: u'AMBER WAVE SYSTEMS, INC.',
   0x00A087: u'Zarlink Semiconductor Ltd.',
   0x00A088: u'ESSENTIAL COMMUNICATIONS',
   0x00A089: u'XPOINT TECHNOLOGIES, INC.',
   0x00A08A: u'BROOKTROUT TECHNOLOGY, INC.',
   0x00A08B: u'ASTON ELECTRONIC DESIGNS LTD.',
   0x00A08C: u'MultiMedia LANs, Inc.',
   0x00A08D: u'JACOMO CORPORATION',
   0x00A08E: u'Nokia Internet Communications',
   0x00A08F: u'DESKNET SYSTEMS, INC.',
   0x00A090: u'TimeStep Corporation',
   0x00A091: u'APPLICOM INTERNATIONAL',
   0x00A092: u'H. BOLLMANN MANUFACTURERS, LTD',
   0x00A093: u'B/E AEROSPACE, Inc.',
   0x00A094: u'COMSAT CORPORATION',
   0x00A095: u'ACACIA NETWORKS, INC.',
   0x00A096: u'MITUMI ELECTRIC CO., LTD.',
   0x00A097: u'JC INFORMATION SYSTEMS',
   0x00A098: u'NETWORK APPLIANCE CORP.',
   0x00A099: u'K-NET LTD.',
   0x00A09A: u'NIHON KOHDEN AMERICA',
   0x00A09B: u'QPSX COMMUNICATIONS, LTD.',
   0x00A09C: u'Xyplex, Inc.',
   0x00A09D: u'JOHNATHON FREEMAN TECHNOLOGIES',
   0x00A09E: u'ICTV',
   0x00A09F: u'COMMVISION CORP.',
   0x00A0A0: u'COMPACT DATA, LTD.',
   0x00A0A1: u'EPIC DATA INC.',
   0x00A0A2: u'DIGICOM S.P.A.',
   0x00A0A3: u'RELIABLE POWER METERS',
   0x00A0A4: u'MICROS SYSTEMS, INC.',
   0x00A0A5: u'TEKNOR MICROSYSTEME, INC.',
   0x00A0A6: u'M.I. SYSTEMS, K.K.',
   0x00A0A7: u'VORAX CORPORATION',
   0x00A0A8: u'RENEX CORPORATION',
   0x00A0A9: u'NAVTEL COMMUNICATIONS INC.',
   0x00A0AA: u'SPACELABS MEDICAL',
   0x00A0AB: u'NETCS INFORMATIONSTECHNIK GMBH',
   0x00A0AC: u'GILAT SATELLITE NETWORKS, LTD.',
   0x00A0AD: u'MARCONI SPA',
   0x00A0AE: u'NUCOM SYSTEMS, INC.',
   0x00A0AF: u'WMS INDUSTRIES',
   0x00A0B0: u'I-O DATA DEVICE, INC.',
   0x00A0B1: u'FIRST VIRTUAL CORPORATION',
   0x00A0B2: u'SHIMA SEIKI',
   0x00A0B3: u'ZYKRONIX',
   0x00A0B4: u'TEXAS MICROSYSTEMS, INC.',
   0x00A0B5: u'3H TECHNOLOGY',
   0x00A0B6: u'SANRITZ AUTOMATION CO., LTD.',
   0x00A0B7: u'CORDANT, INC.',
   0x00A0B8: u'SYMBIOS LOGIC INC.',
   0x00A0B9: u'EAGLE TECHNOLOGY, INC.',
   0x00A0BA: u'PATTON ELECTRONICS CO.',
   0x00A0BB: u'HILAN GMBH',
   0x00A0BC: u'VIASAT, INCORPORATED',
   0x00A0BD: u'I-TECH CORP.',
   0x00A0BE: u'INTEGRATED CIRCUIT SYSTEMS, INC. COMMUNICATIONS GROUP',
   0x00A0BF: u'WIRELESS DATA GROUP MOTOROLA',
   0x00A0C0: u'DIGITAL LINK CORP.',
   0x00A0C1: u'ORTIVUS MEDICAL AB',
   0x00A0C2: u'R.A. SYSTEMS CO., LTD.',
   0x00A0C3: u'UNICOMPUTER GMBH',
   0x00A0C4: u'CRISTIE ELECTRONICS LTD.',
   0x00A0C5: u'ZYXEL COMMUNICATION',
   0x00A0C6: u'QUALCOMM INCORPORATED',
   0x00A0C7: u'TADIRAN TELECOMMUNICATIONS',
   0x00A0C8: u'ADTRAN INC.',
   0x00A0C9: u'INTEL CORPORATION - HF1-06',
   0x00A0CA: u'FUJITSU DENSO LTD.',
   0x00A0CB: u'ARK TELECOMMUNICATIONS, INC.',
   0x00A0CC: u'LITE-ON COMMUNICATIONS, INC.',
   0x00A0CD: u'DR. JOHANNES HEIDENHAIN GmbH',
   0x00A0CE: u'ASTROCOM CORPORATION',
   0x00A0CF: u'SOTAS, INC.',
   0x00A0D0: u'TEN X TECHNOLOGY, INC.',
   0x00A0D1: u'INVENTEC CORPORATION',
   0x00A0D2: u'ALLIED TELESIS INTERNATIONAL CORPORATION',
   0x00A0D3: u'INSTEM COMPUTER SYSTEMS, LTD.',
   0x00A0D4: u'RADIOLAN,  INC.',
   0x00A0D5: u'SIERRA WIRELESS INC.',
   0x00A0D6: u'SBE, INC.',
   0x00A0D7: u'KASTEN CHASE APPLIED RESEARCH',
   0x00A0D8: u'SPECTRA - TEK',
   0x00A0D9: u'CONVEX COMPUTER CORPORATION',
   0x00A0DA: u'INTEGRATED SYSTEMS Technology, Inc.',
   0x00A0DB: u'FISHER & PAYKEL PRODUCTION',
   0x00A0DC: u'O.N. ELECTRONIC CO., LTD.',
   0x00A0DD: u'AZONIX CORPORATION',
   0x00A0DE: u'YAMAHA CORPORATION',
   0x00A0DF: u'STS TECHNOLOGIES, INC.',
   0x00A0E0: u'TENNYSON TECHNOLOGIES PTY LTD',
   0x00A0E1: u'WESTPORT RESEARCH ASSOCIATES, INC.',
   0x00A0E2: u'KEISOKU GIKEN CORP.',
   0x00A0E3: u'XKL SYSTEMS CORP.',
   0x00A0E4: u'OPTIQUEST',
   0x00A0E5: u'NHC COMMUNICATIONS',
   0x00A0E6: u'DIALOGIC CORPORATION',
   0x00A0E7: u'CENTRAL DATA CORPORATION',
   0x00A0E8: u'REUTERS HOLDINGS PLC',
   0x00A0E9: u'ELECTRONIC RETAILING SYSTEMS INTERNATIONAL',
   0x00A0EA: u'ETHERCOM CORP.',
   0x00A0EB: u'Encore Networks',
   0x00A0EC: u'TRANSMITTON LTD.',
   0x00A0ED: u'Brooks Automation, Inc.',
   0x00A0EE: u'NASHOBA NETWORKS',
   0x00A0EF: u'LUCIDATA LTD.',
   0x00A0F0: u'TORONTO MICROELECTRONICS INC.',
   0x00A0F1: u'MTI',
   0x00A0F2: u'INFOTEK COMMUNICATIONS, INC.',
   0x00A0F3: u'STAUBLI',
   0x00A0F4: u'GE',
   0x00A0F5: u'RADGUARD LTD.',
   0x00A0F6: u'AutoGas Systems Inc.',
   0x00A0F7: u'V.I COMPUTER CORP.',
   0x00A0F8: u'SYMBOL TECHNOLOGIES, INC.',
   0x00A0F9: u'BINTEC COMMUNICATIONS GMBH',
   0x00A0FA: u'Marconi Communication GmbH',
   0x00A0FB: u'TORAY ENGINEERING CO., LTD.',
   0x00A0FC: u'IMAGE SCIENCES, INC.',
   0x00A0FD: u'SCITEX DIGITAL PRINTING, INC.',
   0x00A0FE: u'BOSTON TECHNOLOGY, INC.',
   0x00A0FF: u'TELLABS OPERATIONS, INC.',
   0x00AA00: u'INTEL CORPORATION',
   0x00AA01: u'INTEL CORPORATION',
   0x00AA02: u'INTEL CORPORATION',
   0x00AA3C: u'OLIVETTI TELECOM SPA (OLTECO)',
   0x00B009: u'Grass Valley Group',
   0x00B017: u'InfoGear Technology Corp.',
   0x00B019: u'Casi-Rusco',
   0x00B01C: u'Westport Technologies',
   0x00B01E: u'Rantic Labs, Inc.',
   0x00B02A: u'ORSYS GmbH',
   0x00B02D: u'ViaGate Technologies, Inc.',
   0x00B03B: u'HiQ Networks',
   0x00B048: u'Marconi Communications Inc.',
   0x00B04A: u'Cisco Systems, Inc.',
   0x00B052: u'Intellon Corporation',
   0x00B064: u'Cisco Systems, Inc.',
   0x00B069: u'Honewell Oy',
   0x00B06D: u'Jones Futurex Inc.',
   0x00B080: u'Mannesmann Ipulsys B.V.',
   0x00B086: u'LocSoft Limited',
   0x00B08E: u'Cisco Systems, Inc.',
   0x00B091: u'Transmeta Corp.',
   0x00B094: u'Alaris, Inc.',
   0x00B09A: u'Morrow Technologies Corp.',
   0x00B09D: u'Point Grey Research Inc.',
   0x00B0AC: u'SIAE-Microelettronica S.p.A.',
   0x00B0AE: u'Symmetricom',
   0x00B0B3: u'Xstreamis PLC',
   0x00B0C2: u'Cisco Systems, Inc.',
   0x00B0C7: u'Tellabs Operations, Inc.',
   0x00B0CE: u'TECHNOLOGY RESCUE',
   0x00B0D0: u'Dell Computer Corp.',
   0x00B0DB: u'Nextcell, Inc.',
   0x00B0DF: u'Reliable Data Technology, Inc.',
   0x00B0E7: u'British Federal Ltd.',
   0x00B0EC: u'EACEM',
   0x00B0EE: u'Ajile Systems, Inc.',
   0x00B0F0: u'CALY NETWORKS',
   0x00B0F5: u'NetWorth Technologies, Inc.',
   0x00BAC0: u'Biometric Access Company',
   0x00BB01: u'OCTOTHORPE CORP.',
   0x00BBF0: u'UNGERMANN-BASS INC.',
   0x00C000: u'LANOPTICS, LTD.',
   0x00C001: u'DIATEK PATIENT MANAGMENT',
   0x00C002: u'SERCOMM CORPORATION',
   0x00C003: u'GLOBALNET COMMUNICATIONS',
   0x00C004: u'JAPAN BUSINESS COMPUTER CO.LTD',
   0x00C005: u'LIVINGSTON ENTERPRISES, INC.',
   0x00C006: u'NIPPON AVIONICS CO., LTD.',
   0x00C007: u'PINNACLE DATA SYSTEMS, INC.',
   0x00C008: u'SECO SRL',
   0x00C009: u'KT TECHNOLOGY (S) PTE LTD',
   0x00C00A: u'MICRO CRAFT',
   0x00C00B: u'NORCONTROL A.S.',
   0x00C00C: u'RELIA TECHNOLGIES',
   0x00C00D: u'ADVANCED LOGIC RESEARCH, INC.',
   0x00C00E: u'PSITECH, INC.',
   0x00C00F: u'QUANTUM SOFTWARE SYSTEMS LTD.',
   0x00C010: u'HIRAKAWA HEWTECH CORP.',
   0x00C011: u'INTERACTIVE COMPUTING DEVICES',
   0x00C012: u'NETSPAN CORPORATION',
   0x00C013: u'NETRIX',
   0x00C014: u'TELEMATICS CALABASAS INT\'L,INC',
   0x00C015: u'NEW MEDIA CORPORATION',
   0x00C016: u'ELECTRONIC THEATRE CONTROLS',
   0x00C017: u'FORTE NETWORKS',
   0x00C018: u'LANART CORPORATION',
   0x00C019: u'LEAP TECHNOLOGY, INC.',
   0x00C01A: u'COROMETRICS MEDICAL SYSTEMS',
   0x00C01B: u'SOCKET COMMUNICATIONS, INC.',
   0x00C01C: u'INTERLINK COMMUNICATIONS LTD.',
   0x00C01D: u'GRAND JUNCTION NETWORKS, INC.',
   0x00C01E: u'LA FRANCAISE DES JEUX',
   0x00C01F: u'S.E.R.C.E.L.',
   0x00C020: u'ARCO ELECTRONIC, CONTROL LTD.',
   0x00C021: u'NETEXPRESS',
   0x00C022: u'LASERMASTER TECHNOLOGIES, INC.',
   0x00C023: u'TUTANKHAMON ELECTRONICS',
   0x00C024: u'EDEN SISTEMAS DE COMPUTACAO SA',
   0x00C025: u'DATAPRODUCTS CORPORATION',
   0x00C026: u'LANS TECHNOLOGY CO., LTD.',
   0x00C027: u'CIPHER SYSTEMS, INC.',
   0x00C028: u'JASCO CORPORATION',
   0x00C029: u'Nexans Deutschland AG - ANS',
   0x00C02A: u'OHKURA ELECTRIC CO., LTD.',
   0x00C02B: u'GERLOFF GESELLSCHAFT FUR',
   0x00C02C: u'CENTRUM COMMUNICATIONS, INC.',
   0x00C02D: u'FUJI PHOTO FILM CO., LTD.',
   0x00C02E: u'NETWIZ',
   0x00C02F: u'OKUMA CORPORATION',
   0x00C030: u'INTEGRATED ENGINEERING B. V.',
   0x00C031: u'DESIGN RESEARCH SYSTEMS, INC.',
   0x00C032: u'I-CUBED LIMITED',
   0x00C033: u'TELEBIT COMMUNICATIONS APS',
   0x00C034: u'TRANSACTION NETWORK',
   0x00C035: u'QUINTAR COMPANY',
   0x00C036: u'RAYTECH ELECTRONIC CORP.',
   0x00C037: u'DYNATEM',
   0x00C038: u'RASTER IMAGE PROCESSING SYSTEM',
   0x00C039: u'Teridian Semiconductor Corporation',
   0x00C03A: u'MEN-MIKRO ELEKTRONIK GMBH',
   0x00C03B: u'MULTIACCESS COMPUTING CORP.',
   0x00C03C: u'TOWER TECH S.R.L.',
   0x00C03D: u'WIESEMANN & THEIS GMBH',
   0x00C03E: u'FA. GEBR. HELLER GMBH',
   0x00C03F: u'STORES AUTOMATED SYSTEMS, INC.',
   0x00C040: u'ECCI',
   0x00C041: u'DIGITAL TRANSMISSION SYSTEMS',
   0x00C042: u'DATALUX CORP.',
   0x00C043: u'STRATACOM',
   0x00C044: u'EMCOM CORPORATION',
   0x00C045: u'ISOLATION SYSTEMS, LTD.',
   0x00C046: u'KEMITRON LTD.',
   0x00C047: u'UNIMICRO SYSTEMS, INC.',
   0x00C048: u'BAY TECHNICAL ASSOCIATES',
   0x00C049: u'U.S. ROBOTICS, INC.',
   0x00C04A: u'GROUP 2000 AG',
   0x00C04B: u'CREATIVE MICROSYSTEMS',
   0x00C04C: u'DEPARTMENT OF FOREIGN AFFAIRS',
   0x00C04D: u'MITEC, INC.',
   0x00C04E: u'COMTROL CORPORATION',
   0x00C04F: u'DELL COMPUTER CORPORATION',
   0x00C050: u'TOYO DENKI SEIZO K.K.',
   0x00C051: u'ADVANCED INTEGRATION RESEARCH',
   0x00C052: u'BURR-BROWN',
   0x00C053: u'Concerto Software',
   0x00C054: u'NETWORK PERIPHERALS, LTD.',
   0x00C055: u'MODULAR COMPUTING TECHNOLOGIES',
   0x00C056: u'SOMELEC',
   0x00C057: u'MYCO ELECTRONICS',
   0x00C058: u'DATAEXPERT CORP.',
   0x00C059: u'NIPPON DENSO CO., LTD.',
   0x00C05A: u'SEMAPHORE COMMUNICATIONS CORP.',
   0x00C05B: u'NETWORKS NORTHWEST, INC.',
   0x00C05C: u'ELONEX PLC',
   0x00C05D: u'L&N TECHNOLOGIES',
   0x00C05E: u'VARI-LITE, INC.',
   0x00C05F: u'FINE-PAL COMPANY LIMITED',
   0x00C060: u'ID SCANDINAVIA AS',
   0x00C061: u'SOLECTEK CORPORATION',
   0x00C062: u'IMPULSE TECHNOLOGY',
   0x00C063: u'MORNING STAR TECHNOLOGIES, INC',
   0x00C064: u'GENERAL DATACOMM IND. INC.',
   0x00C065: u'SCOPE COMMUNICATIONS, INC.',
   0x00C066: u'DOCUPOINT, INC.',
   0x00C067: u'UNITED BARCODE INDUSTRIES',
   0x00C068: u'PHILIP DRAKE ELECTRONICS LTD.',
   0x00C069: u'Axxcelera Broadband Wireless',
   0x00C06A: u'ZAHNER-ELEKTRIK GMBH & CO. KG',
   0x00C06B: u'OSI PLUS CORPORATION',
   0x00C06C: u'SVEC COMPUTER CORP.',
   0x00C06D: u'BOCA RESEARCH, INC.',
   0x00C06E: u'HAFT TECHNOLOGY, INC.',
   0x00C06F: u'KOMATSU LTD.',
   0x00C070: u'SECTRA SECURE-TRANSMISSION AB',
   0x00C071: u'AREANEX COMMUNICATIONS, INC.',
   0x00C072: u'KNX LTD.',
   0x00C073: u'XEDIA CORPORATION',
   0x00C074: u'TOYODA AUTOMATIC LOOM',
   0x00C075: u'XANTE CORPORATION',
   0x00C076: u'I-DATA INTERNATIONAL A-S',
   0x00C077: u'DAEWOO TELECOM LTD.',
   0x00C078: u'COMPUTER SYSTEMS ENGINEERING',
   0x00C079: u'FONSYS CO.,LTD.',
   0x00C07A: u'PRIVA B.V.',
   0x00C07B: u'ASCEND COMMUNICATIONS, INC.',
   0x00C07C: u'HIGHTECH INFORMATION',
   0x00C07D: u'RISC DEVELOPMENTS LTD.',
   0x00C07E: u'KUBOTA CORPORATION ELECTRONIC',
   0x00C07F: u'NUPON COMPUTING CORP.',
   0x00C080: u'NETSTAR, INC.',
   0x00C081: u'METRODATA LTD.',
   0x00C082: u'MOORE PRODUCTS CO.',
   0x00C083: u'TRACE MOUNTAIN PRODUCTS, INC.',
   0x00C084: u'DATA LINK CORP. LTD.',
   0x00C085: u'ELECTRONICS FOR IMAGING, INC.',
   0x00C086: u'THE LYNK CORPORATION',
   0x00C087: u'UUNET TECHNOLOGIES, INC.',
   0x00C088: u'EKF ELEKTRONIK GMBH',
   0x00C089: u'TELINDUS DISTRIBUTION',
   0x00C08A: u'LAUTERBACH DATENTECHNIK GMBH',
   0x00C08B: u'RISQ MODULAR SYSTEMS, INC.',
   0x00C08C: u'PERFORMANCE TECHNOLOGIES, INC.',
   0x00C08D: u'TRONIX PRODUCT DEVELOPMENT',
   0x00C08E: u'NETWORK INFORMATION TECHNOLOGY',
   0x00C08F: u'Matsushita Electric Works, Ltd.',
   0x00C090: u'PRAIM S.R.L.',
   0x00C091: u'JABIL CIRCUIT, INC.',
   0x00C092: u'MENNEN MEDICAL INC.',
   0x00C093: u'ALTA RESEARCH CORP.',
   0x00C094: u'VMX INC.',
   0x00C095: u'ZNYX',
   0x00C096: u'TAMURA CORPORATION',
   0x00C097: u'ARCHIPEL SA',
   0x00C098: u'CHUNTEX ELECTRONIC CO., LTD.',
   0x00C099: u'YOSHIKI INDUSTRIAL CO.,LTD.',
   0x00C09A: u'PHOTONICS CORPORATION',
   0x00C09B: u'RELIANCE COMM/TEC, R-TEC',
   0x00C09C: u'TOA ELECTRONIC LTD.',
   0x00C09D: u'DISTRIBUTED SYSTEMS INT\'L, INC',
   0x00C09E: u'CACHE COMPUTERS, INC.',
   0x00C09F: u'QUANTA COMPUTER, INC.',
   0x00C0A0: u'ADVANCE MICRO RESEARCH, INC.',
   0x00C0A1: u'TOKYO DENSHI SEKEI CO.',
   0x00C0A2: u'INTERMEDIUM A/S',
   0x00C0A3: u'DUAL ENTERPRISES CORPORATION',
   0x00C0A4: u'UNIGRAF OY',
   0x00C0A5: u'DICKENS DATA SYSTEMS',
   0x00C0A6: u'EXICOM AUSTRALIA PTY. LTD',
   0x00C0A7: u'SEEL LTD.',
   0x00C0A8: u'GVC CORPORATION',
   0x00C0A9: u'BARRON MCCANN LTD.',
   0x00C0AA: u'SILICON VALLEY COMPUTER',
   0x00C0AB: u'Telco Systems, Inc.',
   0x00C0AC: u'GAMBIT COMPUTER COMMUNICATIONS',
   0x00C0AD: u'MARBEN COMMUNICATION SYSTEMS',
   0x00C0AE: u'TOWERCOM CO. INC. DBA PC HOUSE',
   0x00C0AF: u'TEKLOGIX INC.',
   0x00C0B0: u'GCC TECHNOLOGIES,INC.',
   0x00C0B1: u'GENIUS NET CO.',
   0x00C0B2: u'NORAND CORPORATION',
   0x00C0B3: u'COMSTAT DATACOMM CORPORATION',
   0x00C0B4: u'MYSON TECHNOLOGY, INC.',
   0x00C0B5: u'CORPORATE NETWORK SYSTEMS,INC.',
   0x00C0B6: u'Adaptec, Inc.',
   0x00C0B7: u'AMERICAN POWER CONVERSION CORP',
   0x00C0B8: u'FRASER\'S HILL LTD.',
   0x00C0B9: u'FUNK SOFTWARE, INC.',
   0x00C0BA: u'NETVANTAGE',
   0x00C0BB: u'FORVAL CREATIVE, INC.',
   0x00C0BC: u'TELECOM AUSTRALIA/CSSC',
   0x00C0BD: u'INEX TECHNOLOGIES, INC.',
   0x00C0BE: u'ALCATEL - SEL',
   0x00C0BF: u'TECHNOLOGY CONCEPTS, LTD.',
   0x00C0C0: u'SHORE MICROSYSTEMS, INC.',
   0x00C0C1: u'QUAD/GRAPHICS, INC.',
   0x00C0C2: u'INFINITE NETWORKS LTD.',
   0x00C0C3: u'ACUSON COMPUTED SONOGRAPHY',
   0x00C0C4: u'COMPUTER OPERATIONAL',
   0x00C0C5: u'SID INFORMATICA',
   0x00C0C6: u'PERSONAL MEDIA CORP.',
   0x00C0C7: u'SPARKTRUM MICROSYSTEMS, INC.',
   0x00C0C8: u'MICRO BYTE PTY. LTD.',
   0x00C0C9: u'ELSAG BAILEY PROCESS',
   0x00C0CA: u'ALFA, INC.',
   0x00C0CB: u'CONTROL TECHNOLOGY CORPORATION',
   0x00C0CC: u'TELESCIENCES CO SYSTEMS, INC.',
   0x00C0CD: u'COMELTA, S.A.',
   0x00C0CE: u'CEI SYSTEMS & ENGINEERING PTE',
   0x00C0CF: u'IMATRAN VOIMA OY',
   0x00C0D0: u'RATOC SYSTEM INC.',
   0x00C0D1: u'COMTREE TECHNOLOGY CORPORATION',
   0x00C0D2: u'SYNTELLECT, INC.',
   0x00C0D3: u'OLYMPUS IMAGE SYSTEMS, INC.',
   0x00C0D4: u'AXON NETWORKS, INC.',
   0x00C0D5: u'QUANCOM ELECTRONIC GMBH',
   0x00C0D6: u'J1 SYSTEMS, INC.',
   0x00C0D7: u'TAIWAN TRADING CENTER DBA',
   0x00C0D8: u'UNIVERSAL DATA SYSTEMS',
   0x00C0D9: u'QUINTE NETWORK CONFIDENTIALITY',
   0x00C0DA: u'NICE SYSTEMS LTD.',
   0x00C0DB: u'IPC CORPORATION (PTE) LTD.',
   0x00C0DC: u'EOS TECHNOLOGIES, INC.',
   0x00C0DD: u'QLogic Corporation',
   0x00C0DE: u'ZCOMM, INC.',
   0x00C0DF: u'KYE Systems Corp.',
   0x00C0E0: u'DSC COMMUNICATION CORP.',
   0x00C0E1: u'SONIC SOLUTIONS',
   0x00C0E2: u'CALCOMP, INC.',
   0x00C0E3: u'OSITECH COMMUNICATIONS, INC.',
   0x00C0E4: u'SIEMENS BUILDING',
   0x00C0E5: u'GESPAC, S.A.',
   0x00C0E6: u'Verilink Corporation',
   0x00C0E7: u'FIBERDATA AB',
   0x00C0E8: u'PLEXCOM, INC.',
   0x00C0E9: u'OAK SOLUTIONS, LTD.',
   0x00C0EA: u'ARRAY TECHNOLOGY LTD.',
   0x00C0EB: u'SEH COMPUTERTECHNIK GMBH',
   0x00C0EC: u'DAUPHIN TECHNOLOGY',
   0x00C0ED: u'US ARMY ELECTRONIC',
   0x00C0EE: u'KYOCERA CORPORATION',
   0x00C0EF: u'ABIT CORPORATION',
   0x00C0F0: u'KINGSTON TECHNOLOGY CORP.',
   0x00C0F1: u'SHINKO ELECTRIC CO., LTD.',
   0x00C0F2: u'TRANSITION NETWORKS',
   0x00C0F3: u'NETWORK COMMUNICATIONS CORP.',
   0x00C0F4: u'INTERLINK SYSTEM CO., LTD.',
   0x00C0F5: u'METACOMP, INC.',
   0x00C0F6: u'CELAN TECHNOLOGY INC.',
   0x00C0F7: u'ENGAGE COMMUNICATION, INC.',
   0x00C0F8: u'ABOUT COMPUTING INC.',
   0x00C0F9: u'Motorola Embedded Computing Group',
   0x00C0FA: u'CANARY COMMUNICATIONS, INC.',
   0x00C0FB: u'ADVANCED TECHNOLOGY LABS',
   0x00C0FC: u'ELASTIC REALITY, INC.',
   0x00C0FD: u'PROSUM',
   0x00C0FE: u'APTEC COMPUTER SYSTEMS, INC.',
   0x00C0FF: u'DOT HILL SYSTEMS CORPORATION',
   0x00CBBD: u'Cambridge Broadband Ltd.',
   0x00CF1C: u'COMMUNICATION MACHINERY CORP.',
   0x00D000: u'FERRAN SCIENTIFIC, INC.',
   0x00D001: u'VST TECHNOLOGIES, INC.',
   0x00D002: u'DITECH CORPORATION',
   0x00D003: u'COMDA ENTERPRISES CORP.',
   0x00D004: u'PENTACOM LTD.',
   0x00D005: u'ZHS ZEITMANAGEMENTSYSTEME',
   0x00D006: u'CISCO SYSTEMS, INC.',
   0x00D007: u'MIC ASSOCIATES, INC.',
   0x00D008: u'MACTELL CORPORATION',
   0x00D009: u'HSING TECH. ENTERPRISE CO. LTD',
   0x00D00A: u'LANACCESS TELECOM S.A.',
   0x00D00B: u'RHK TECHNOLOGY, INC.',
   0x00D00C: u'SNIJDER MICRO SYSTEMS',
   0x00D00D: u'MICROMERITICS INSTRUMENT',
   0x00D00E: u'PLURIS, INC.',
   0x00D00F: u'SPEECH DESIGN GMBH',
   0x00D010: u'CONVERGENT NETWORKS, INC.',
   0x00D011: u'PRISM VIDEO, INC.',
   0x00D012: u'GATEWORKS CORP.',
   0x00D013: u'PRIMEX AEROSPACE COMPANY',
   0x00D014: u'ROOT, INC.',
   0x00D015: u'UNIVEX MICROTECHNOLOGY CORP.',
   0x00D016: u'SCM MICROSYSTEMS, INC.',
   0x00D017: u'SYNTECH INFORMATION CO., LTD.',
   0x00D018: u'QWES. COM, INC.',
   0x00D019: u'DAINIPPON SCREEN CORPORATE',
   0x00D01A: u'URMET  TLC S.P.A.',
   0x00D01B: u'MIMAKI ENGINEERING CO., LTD.',
   0x00D01C: u'SBS TECHNOLOGIES,',
   0x00D01D: u'FURUNO ELECTRIC CO., LTD.',
   0x00D01E: u'PINGTEL CORP.',
   0x00D01F: u'CTAM PTY. LTD.',
   0x00D020: u'AIM SYSTEM, INC.',
   0x00D021: u'REGENT ELECTRONICS CORP.',
   0x00D022: u'INCREDIBLE TECHNOLOGIES, INC.',
   0x00D023: u'INFORTREND TECHNOLOGY, INC.',
   0x00D024: u'Cognex Corporation',
   0x00D025: u'XROSSTECH, INC.',
   0x00D026: u'HIRSCHMANN AUSTRIA GMBH',
   0x00D027: u'APPLIED AUTOMATION, INC.',
   0x00D028: u'OMNEON VIDEO NETWORKS',
   0x00D029: u'WAKEFERN FOOD CORPORATION',
   0x00D02A: u'Voxent Systems Ltd.',
   0x00D02B: u'JETCELL, INC.',
   0x00D02C: u'CAMPBELL SCIENTIFIC, INC.',
   0x00D02D: u'ADEMCO',
   0x00D02E: u'COMMUNICATION AUTOMATION CORP.',
   0x00D02F: u'VLSI TECHNOLOGY INC.',
   0x00D030: u'SAFETRAN SYSTEMS CORP.',
   0x00D031: u'INDUSTRIAL LOGIC CORPORATION',
   0x00D032: u'YANO ELECTRIC CO., LTD.',
   0x00D033: u'DALIAN DAXIAN NETWORK',
   0x00D034: u'ORMEC SYSTEMS CORP.',
   0x00D035: u'BEHAVIOR TECH. COMPUTER CORP.',
   0x00D036: u'TECHNOLOGY ATLANTA CORP.',
   0x00D037: u'PHILIPS-DVS-LO BDR',
   0x00D038: u'FIVEMERE, LTD.',
   0x00D039: u'UTILICOM, INC.',
   0x00D03A: u'ZONEWORX, INC.',
   0x00D03B: u'VISION PRODUCTS PTY. LTD.',
   0x00D03C: u'Vieo, Inc.',
   0x00D03D: u'GALILEO TECHNOLOGY, LTD.',
   0x00D03E: u'ROCKETCHIPS, INC.',
   0x00D03F: u'AMERICAN COMMUNICATION',
   0x00D040: u'SYSMATE CO., LTD.',
   0x00D041: u'AMIGO TECHNOLOGY CO., LTD.',
   0x00D042: u'MAHLO GMBH & CO. UG',
   0x00D043: u'ZONAL RETAIL DATA SYSTEMS',
   0x00D044: u'ALIDIAN NETWORKS, INC.',
   0x00D045: u'KVASER AB',
   0x00D046: u'DOLBY LABORATORIES, INC.',
   0x00D047: u'XN TECHNOLOGIES',
   0x00D048: u'ECTON, INC.',
   0x00D049: u'IMPRESSTEK CO., LTD.',
   0x00D04A: u'PRESENCE TECHNOLOGY GMBH',
   0x00D04B: u'LA CIE GROUP S.A.',
   0x00D04C: u'EUROTEL TELECOM LTD.',
   0x00D04D: u'DIV OF RESEARCH & STATISTICS',
   0x00D04E: u'LOGIBAG',
   0x00D04F: u'BITRONICS, INC.',
   0x00D050: u'ISKRATEL',
   0x00D051: u'O2 MICRO, INC.',
   0x00D052: u'ASCEND COMMUNICATIONS, INC.',
   0x00D053: u'CONNECTED SYSTEMS',
   0x00D054: u'SAS INSTITUTE INC.',
   0x00D055: u'KATHREIN-WERKE KG',
   0x00D056: u'SOMAT CORPORATION',
   0x00D057: u'ULTRAK, INC.',
   0x00D058: u'CISCO SYSTEMS, INC.',
   0x00D059: u'AMBIT MICROSYSTEMS CORP.',
   0x00D05A: u'SYMBIONICS, LTD.',
   0x00D05B: u'ACROLOOP MOTION CONTROL',
   0x00D05C: u'TECHNOTREND SYSTEMTECHNIK GMBH',
   0x00D05D: u'INTELLIWORXX, INC.',
   0x00D05E: u'STRATABEAM TECHNOLOGY, INC.',
   0x00D05F: u'VALCOM, INC.',
   0x00D060: u'PANASONIC EUROPEAN',
   0x00D061: u'TREMON ENTERPRISES CO., LTD.',
   0x00D062: u'DIGIGRAM',
   0x00D063: u'CISCO SYSTEMS, INC.',
   0x00D064: u'MULTITEL',
   0x00D065: u'TOKO ELECTRIC',
   0x00D066: u'WINTRISS ENGINEERING CORP.',
   0x00D067: u'CAMPIO COMMUNICATIONS',
   0x00D068: u'IWILL CORPORATION',
   0x00D069: u'TECHNOLOGIC SYSTEMS',
   0x00D06A: u'LINKUP SYSTEMS CORPORATION',
   0x00D06B: u'SR TELECOM INC.',
   0x00D06C: u'SHAREWAVE, INC.',
   0x00D06D: u'ACRISON, INC.',
   0x00D06E: u'TRENDVIEW RECORDERS LTD.',
   0x00D06F: u'KMC CONTROLS',
   0x00D070: u'LONG WELL ELECTRONICS CORP.',
   0x00D071: u'ECHELON CORP.',
   0x00D072: u'BROADLOGIC',
   0x00D073: u'ACN ADVANCED COMMUNICATIONS',
   0x00D074: u'TAQUA SYSTEMS, INC.',
   0x00D075: u'ALARIS MEDICAL SYSTEMS, INC.',
   0x00D076: u'Merrill Lynch & Co., Inc.',
   0x00D077: u'LUCENT TECHNOLOGIES',
   0x00D078: u'ELTEX OF SWEDEN AB',
   0x00D079: u'CISCO SYSTEMS, INC.',
   0x00D07A: u'AMAQUEST COMPUTER CORP.',
   0x00D07B: u'COMCAM INTERNATIONAL LTD.',
   0x00D07C: u'KOYO ELECTRONICS INC. CO.,LTD.',
   0x00D07D: u'COSINE COMMUNICATIONS',
   0x00D07E: u'KEYCORP LTD.',
   0x00D07F: u'STRATEGY & TECHNOLOGY, LIMITED',
   0x00D080: u'EXABYTE CORPORATION',
   0x00D081: u'REAL TIME DEVICES USA, INC.',
   0x00D082: u'IOWAVE INC.',
   0x00D083: u'INVERTEX, INC.',
   0x00D084: u'NEXCOMM SYSTEMS, INC.',
   0x00D085: u'OTIS ELEVATOR COMPANY',
   0x00D086: u'FOVEON, INC.',
   0x00D087: u'MICROFIRST INC.',
   0x00D088: u'Terayon Communications Systems',
   0x00D089: u'DYNACOLOR, INC.',
   0x00D08A: u'PHOTRON USA',
   0x00D08B: u'ADVA Limited',
   0x00D08C: u'GENOA TECHNOLOGY, INC.',
   0x00D08D: u'PHOENIX GROUP, INC.',
   0x00D08E: u'NVISION INC.',
   0x00D08F: u'ARDENT TECHNOLOGIES, INC.',
   0x00D090: u'CISCO SYSTEMS, INC.',
   0x00D091: u'SMARTSAN SYSTEMS, INC.',
   0x00D092: u'GLENAYRE WESTERN MULTIPLEX',
   0x00D093: u'TQ - COMPONENTS GMBH',
   0x00D094: u'TIMELINE VISTA, INC.',
   0x00D095: u'Alcatel North America ESD',
   0x00D096: u'3COM EUROPE LTD.',
   0x00D097: u'CISCO SYSTEMS, INC.',
   0x00D098: u'Photon Dynamics Canada Inc.',
   0x00D099: u'ELCARD OY',
   0x00D09A: u'FILANET CORPORATION',
   0x00D09B: u'SPECTEL LTD.',
   0x00D09C: u'KAPADIA COMMUNICATIONS',
   0x00D09D: u'VERIS INDUSTRIES',
   0x00D09E: u'2WIRE, INC.',
   0x00D09F: u'NOVTEK TEST SYSTEMS',
   0x00D0A0: u'MIPS DENMARK',
   0x00D0A1: u'OSKAR VIERLING GMBH + CO. KG',
   0x00D0A2: u'INTEGRATED DEVICE',
   0x00D0A3: u'VOCAL DATA, INC.',
   0x00D0A4: u'ALANTRO COMMUNICATIONS',
   0x00D0A5: u'AMERICAN ARIUM',
   0x00D0A6: u'LANBIRD TECHNOLOGY CO., LTD.',
   0x00D0A7: u'TOKYO SOKKI KENKYUJO CO., LTD.',
   0x00D0A8: u'NETWORK ENGINES, INC.',
   0x00D0A9: u'SHINANO KENSHI CO., LTD.',
   0x00D0AA: u'CHASE COMMUNICATIONS',
   0x00D0AB: u'DELTAKABEL TELECOM CV',
   0x00D0AC: u'GRAYSON WIRELESS',
   0x00D0AD: u'TL INDUSTRIES',
   0x00D0AE: u'ORESIS COMMUNICATIONS, INC.',
   0x00D0AF: u'CUTLER-HAMMER, INC.',
   0x00D0B0: u'BITSWITCH LTD.',
   0x00D0B1: u'OMEGA ELECTRONICS SA',
   0x00D0B2: u'XIOTECH CORPORATION',
   0x00D0B3: u'DRS FLIGHT SAFETY AND',
   0x00D0B4: u'KATSUJIMA CO., LTD.',
   0x00D0B5: u'IPricot formerly DotCom',
   0x00D0B6: u'CRESCENT NETWORKS, INC.',
   0x00D0B7: u'INTEL CORPORATION',
   0x00D0B8: u'Iomega Corporation',
   0x00D0B9: u'MICROTEK INTERNATIONAL, INC.',
   0x00D0BA: u'CISCO SYSTEMS, INC.',
   0x00D0BB: u'CISCO SYSTEMS, INC.',
   0x00D0BC: u'CISCO SYSTEMS, INC.',
   0x00D0BD: u'SICAN GMBH',
   0x00D0BE: u'EMUTEC INC.',
   0x00D0BF: u'PIVOTAL TECHNOLOGIES',
   0x00D0C0: u'CISCO SYSTEMS, INC.',
   0x00D0C1: u'HARMONIC DATA SYSTEMS, LTD.',
   0x00D0C2: u'BALTHAZAR TECHNOLOGY AB',
   0x00D0C3: u'VIVID TECHNOLOGY PTE, LTD.',
   0x00D0C4: u'TERATECH CORPORATION',
   0x00D0C5: u'COMPUTATIONAL SYSTEMS, INC.',
   0x00D0C6: u'THOMAS & BETTS CORP.',
   0x00D0C7: u'PATHWAY, INC.',
   0x00D0C8: u'I/O CONSULTING A/S',
   0x00D0C9: u'ADVANTECH CO., LTD.',
   0x00D0CA: u'INTRINSYC SOFTWARE INC.',
   0x00D0CB: u'DASAN CO., LTD.',
   0x00D0CC: u'TECHNOLOGIES LYRE INC.',
   0x00D0CD: u'ATAN TECHNOLOGY INC.',
   0x00D0CE: u'ASYST ELECTRONIC',
   0x00D0CF: u'MORETON BAY',
   0x00D0D0: u'ZHONGXING TELECOM LTD.',
   0x00D0D1: u'SIROCCO SYSTEMS, INC.',
   0x00D0D2: u'EPILOG CORPORATION',
   0x00D0D3: u'CISCO SYSTEMS, INC.',
   0x00D0D4: u'V-BITS, INC.',
   0x00D0D5: u'GRUNDIG AG',
   0x00D0D6: u'AETHRA TELECOMUNICAZIONI',
   0x00D0D7: u'B2C2, INC.',
   0x00D0D8: u'3Com Corporation',
   0x00D0D9: u'DEDICATED MICROCOMPUTERS',
   0x00D0DA: u'TAICOM DATA SYSTEMS CO., LTD.',
   0x00D0DB: u'MCQUAY INTERNATIONAL',
   0x00D0DC: u'MODULAR MINING SYSTEMS, INC.',
   0x00D0DD: u'SUNRISE TELECOM, INC.',
   0x00D0DE: u'PHILIPS MULTIMEDIA NETWORK',
   0x00D0DF: u'KUZUMI ELECTRONICS, INC.',
   0x00D0E0: u'DOOIN ELECTRONICS CO.',
   0x00D0E1: u'AVIONITEK ISRAEL INC.',
   0x00D0E2: u'MRT MICRO, INC.',
   0x00D0E3: u'ELE-CHEM ENGINEERING CO., LTD.',
   0x00D0E4: u'CISCO SYSTEMS, INC.',
   0x00D0E5: u'SOLIDUM SYSTEMS CORP.',
   0x00D0E6: u'IBOND INC.',
   0x00D0E7: u'VCON TELECOMMUNICATION LTD.',
   0x00D0E8: u'MAC SYSTEM CO., LTD.',
   0x00D0E9: u'ADVANTAGE CENTURY',
   0x00D0EA: u'NEXTONE COMMUNICATIONS, INC.',
   0x00D0EB: u'LIGHTERA NETWORKS, INC.',
   0x00D0EC: u'NAKAYO TELECOMMUNICATIONS, INC',
   0x00D0ED: u'XIOX',
   0x00D0EE: u'DICTAPHONE CORPORATION',
   0x00D0EF: u'IGT',
   0x00D0F0: u'CONVISION TECHNOLOGY GMBH',
   0x00D0F1: u'SEGA ENTERPRISES, LTD.',
   0x00D0F2: u'MONTEREY NETWORKS',
   0x00D0F3: u'SOLARI DI UDINE SPA',
   0x00D0F4: u'CARINTHIAN TECH INSTITUTE',
   0x00D0F5: u'ORANGE MICRO, INC.',
   0x00D0F6: u'Alcatel Canada',
   0x00D0F7: u'NEXT NETS CORPORATION',
   0x00D0F8: u'FUJIAN STAR TERMINAL',
   0x00D0F9: u'ACUTE COMMUNICATIONS CORP.',
   0x00D0FA: u'RACAL GUARDATA',
   0x00D0FB: u'TEK MICROSYSTEMS, INCORPORATED',
   0x00D0FC: u'GRANITE MICROSYSTEMS',
   0x00D0FD: u'OPTIMA TELE.COM, INC.',
   0x00D0FE: u'ASTRAL POINT',
   0x00D0FF: u'CISCO SYSTEMS, INC.',
   0x00DD00: u'UNGERMANN-BASS INC.',
   0x00DD01: u'UNGERMANN-BASS INC.',
   0x00DD02: u'UNGERMANN-BASS INC.',
   0x00DD03: u'UNGERMANN-BASS INC.',
   0x00DD04: u'UNGERMANN-BASS INC.',
   0x00DD05: u'UNGERMANN-BASS INC.',
   0x00DD06: u'UNGERMANN-BASS INC.',
   0x00DD07: u'UNGERMANN-BASS INC.',
   0x00DD08: u'UNGERMANN-BASS INC.',
   0x00DD09: u'UNGERMANN-BASS INC.',
   0x00DD0A: u'UNGERMANN-BASS INC.',
   0x00DD0B: u'UNGERMANN-BASS INC.',
   0x00DD0C: u'UNGERMANN-BASS INC.',
   0x00DD0D: u'UNGERMANN-BASS INC.',
   0x00DD0E: u'UNGERMANN-BASS INC.',
   0x00DD0F: u'UNGERMANN-BASS INC.',
   0x00E000: u'FUJITSU, LTD',
   0x00E001: u'STRAND LIGHTING LIMITED',
   0x00E002: u'CROSSROADS SYSTEMS, INC.',
   0x00E003: u'NOKIA WIRELESS BUSINESS COMMUN',
   0x00E004: u'PMC-SIERRA, INC.',
   0x00E005: u'TECHNICAL CORP.',
   0x00E006: u'SILICON INTEGRATED SYS. CORP.',
   0x00E007: u'NETWORK ALCHEMY LTD.',
   0x00E008: u'AMAZING CONTROLS! INC.',
   0x00E009: u'MARATHON TECHNOLOGIES CORP.',
   0x00E00A: u'DIBA, INC.',
   0x00E00B: u'ROOFTOP COMMUNICATIONS CORP.',
   0x00E00C: u'MOTOROLA',
   0x00E00D: u'RADIANT SYSTEMS',
   0x00E00E: u'AVALON IMAGING SYSTEMS, INC.',
   0x00E00F: u'SHANGHAI BAUD DATA',
   0x00E010: u'HESS SB-AUTOMATENBAU GmbH',
   0x00E011: u'UNIDEN SAN DIEGO R&D CENTER, INC.',
   0x00E012: u'PLUTO TECHNOLOGIES INTERNATIONAL INC.',
   0x00E013: u'EASTERN ELECTRONIC CO., LTD.',
   0x00E014: u'CISCO SYSTEMS, INC.',
   0x00E015: u'HEIWA CORPORATION',
   0x00E016: u'RAPID CITY COMMUNICATIONS',
   0x00E017: u'EXXACT GmbH',
   0x00E018: u'ASUSTEK COMPUTER INC.',
   0x00E019: u'ING. GIORDANO ELETTRONICA',
   0x00E01A: u'COMTEC SYSTEMS. CO., LTD.',
   0x00E01B: u'SPHERE COMMUNICATIONS, INC.',
   0x00E01C: u'MOBILITY ELECTRONICSY',
   0x00E01D: u'WebTV NETWORKS, INC.',
   0x00E01E: u'CISCO SYSTEMS, INC.',
   0x00E01F: u'AVIDIA Systems, Inc.',
   0x00E020: u'TECNOMEN OY',
   0x00E021: u'FREEGATE CORP.',
   0x00E022: u'Analog Devices Inc.',
   0x00E023: u'TELRAD',
   0x00E024: u'GADZOOX NETWORKS',
   0x00E025: u'dit CO., LTD.',
   0x00E026: u'Redlake MASD LLC',
   0x00E027: u'DUX, INC.',
   0x00E028: u'APTIX CORPORATION',
   0x00E029: u'STANDARD MICROSYSTEMS CORP.',
   0x00E02A: u'TANDBERG TELEVISION AS',
   0x00E02B: u'EXTREME NETWORKS',
   0x00E02C: u'AST COMPUTER',
   0x00E02D: u'InnoMediaLogic, Inc.',
   0x00E02E: u'SPC ELECTRONICS CORPORATION',
   0x00E02F: u'MCNS HOLDINGS, L.P.',
   0x00E030: u'MELITA INTERNATIONAL CORP.',
   0x00E031: u'HAGIWARA ELECTRIC CO., LTD.',
   0x00E032: u'MISYS FINANCIAL SYSTEMS, LTD.',
   0x00E033: u'E.E.P.D. GmbH',
   0x00E034: u'CISCO SYSTEMS, INC.',
   0x00E035: u'LOUGHBOROUGH SOUND IMAGES, PLC',
   0x00E036: u'PIONEER CORPORATION',
   0x00E037: u'CENTURY CORPORATION',
   0x00E038: u'PROXIMA CORPORATION',
   0x00E039: u'PARADYNE CORP.',
   0x00E03A: u'CABLETRON SYSTEMS, INC.',
   0x00E03B: u'PROMINET CORPORATION',
   0x00E03C: u'AdvanSys',
   0x00E03D: u'FOCON ELECTRONIC SYSTEMS A/S',
   0x00E03E: u'ALFATECH, INC.',
   0x00E03F: u'JATON CORPORATION',
   0x00E040: u'DeskStation Technology, Inc.',
   0x00E041: u'CSPI',
   0x00E042: u'Pacom Systems Ltd.',
   0x00E043: u'VitalCom',
   0x00E044: u'LSICS CORPORATION',
   0x00E045: u'TOUCHWAVE, INC.',
   0x00E046: u'BENTLY NEVADA CORP.',
   0x00E047: u'INFOCUS SYSTEMS',
   0x00E048: u'SDL COMMUNICATIONS, INC.',
   0x00E049: u'MICROWI ELECTRONIC GmbH',
   0x00E04A: u'ENHANCED MESSAGING SYSTEMS, INC',
   0x00E04B: u'JUMP INDUSTRIELLE COMPUTERTECHNIK GmbH',
   0x00E04C: u'REALTEK SEMICONDUCTOR CORP.',
   0x00E04D: u'INTERNET INITIATIVE JAPAN, INC',
   0x00E04E: u'SANYO DENKI CO., LTD.',
   0x00E04F: u'CISCO SYSTEMS, INC.',
   0x00E050: u'EXECUTONE INFORMATION SYSTEMS, INC.',
   0x00E051: u'TALX CORPORATION',
   0x00E052: u'FOUNDRY NETWORKS, INC.',
   0x00E053: u'CELLPORT LABS, INC.',
   0x00E054: u'KODAI HITEC CO., LTD.',
   0x00E055: u'INGENIERIA ELECTRONICA COMERCIAL INELCOM S.A.',
   0x00E056: u'HOLONTECH CORPORATION',
   0x00E057: u'HAN MICROTELECOM. CO., LTD.',
   0x00E058: u'PHASE ONE DENMARK A/S',
   0x00E059: u'CONTROLLED ENVIRONMENTS, LTD.',
   0x00E05A: u'GALEA NETWORK SECURITY',
   0x00E05B: u'WEST END SYSTEMS CORP.',
   0x00E05C: u'MATSUSHITA KOTOBUKI ELECTRONICS INDUSTRIES, LTD.',
   0x00E05D: u'UNITEC CO., LTD.',
   0x00E05E: u'JAPAN AVIATION ELECTRONICS INDUSTRY, LTD.',
   0x00E05F: u'e-Net, Inc.',
   0x00E060: u'SHERWOOD',
   0x00E061: u'EdgePoint Networks, Inc.',
   0x00E062: u'HOST ENGINEERING',
   0x00E063: u'CABLETRON - YAGO SYSTEMS, INC.',
   0x00E064: u'SAMSUNG ELECTRONICS',
   0x00E065: u'OPTICAL ACCESS INTERNATIONAL',
   0x00E066: u'ProMax Systems, Inc.',
   0x00E067: u'eac AUTOMATION-CONSULTING GmbH',
   0x00E068: u'MERRIMAC SYSTEMS INC.',
   0x00E069: u'JAYCOR',
   0x00E06A: u'KAPSCH AG',
   0x00E06B: u'W&G SPECIAL PRODUCTS',
   0x00E06C: u'AEP Systems International Ltd',
   0x00E06D: u'COMPUWARE CORPORATION',
   0x00E06E: u'FAR SYSTEMS S.p.A.',
   0x00E06F: u'Terayon Communications Systems',
   0x00E070: u'DH TECHNOLOGY',
   0x00E071: u'EPIS MICROCOMPUTER',
   0x00E072: u'LYNK',
   0x00E073: u'NATIONAL AMUSEMENT NETWORK, INC.',
   0x00E074: u'TIERNAN COMMUNICATIONS, INC.',
   0x00E075: u'Verilink Corporation',
   0x00E076: u'DEVELOPMENT CONCEPTS, INC.',
   0x00E077: u'WEBGEAR, INC.',
   0x00E078: u'BERKELEY NETWORKS',
   0x00E079: u'A.T.N.R.',
   0x00E07A: u'MIKRODIDAKT AB',
   0x00E07B: u'BAY NETWORKS',
   0x00E07C: u'METTLER-TOLEDO, INC.',
   0x00E07D: u'NETRONIX, INC.',
   0x00E07E: u'WALT DISNEY IMAGINEERING',
   0x00E07F: u'LOGISTISTEM s.r.l.',
   0x00E080: u'CONTROL RESOURCES CORPORATION',
   0x00E081: u'TYAN COMPUTER CORP.',
   0x00E082: u'ANERMA',
   0x00E083: u'JATO TECHNOLOGIES, INC.',
   0x00E084: u'COMPULITE R&D',
   0x00E085: u'GLOBAL MAINTECH, INC.',
   0x00E086: u'CYBEX COMPUTER PRODUCTS',
   0x00E087: u'LeCroy - Networking Productions Division',
   0x00E088: u'LTX CORPORATION',
   0x00E089: u'ION Networks, Inc.',
   0x00E08A: u'GEC AVERY, LTD.',
   0x00E08B: u'QLogic Corp.',
   0x00E08C: u'NEOPARADIGM LABS, INC.',
   0x00E08D: u'PRESSURE SYSTEMS, INC.',
   0x00E08E: u'UTSTARCOM',
   0x00E08F: u'CISCO SYSTEMS, INC.',
   0x00E090: u'BECKMAN LAB. AUTOMATION DIV.',
   0x00E091: u'LG ELECTRONICS, INC.',
   0x00E092: u'ADMTEK INCORPORATED',
   0x00E093: u'ACKFIN NETWORKS',
   0x00E094: u'OSAI SRL',
   0x00E095: u'ADVANCED-VISION TECHNOLGIES CORP.',
   0x00E096: u'SHIMADZU CORPORATION',
   0x00E097: u'CARRIER ACCESS CORPORATION',
   0x00E098: u'AboCom Systems, Inc.',
   0x00E099: u'SAMSON AG',
   0x00E09A: u'POSITRON INDUSTRIES, INC.',
   0x00E09B: u'ENGAGE NETWORKS, INC.',
   0x00E09C: u'MII',
   0x00E09D: u'SARNOFF CORPORATION',
   0x00E09E: u'QUANTUM CORPORATION',
   0x00E09F: u'PIXEL VISION',
   0x00E0A0: u'WILTRON CO.',
   0x00E0A1: u'HIMA PAUL HILDEBRANDT GmbH Co. KG',
   0x00E0A2: u'MICROSLATE INC.',
   0x00E0A3: u'CISCO SYSTEMS, INC.',
   0x00E0A4: u'ESAOTE S.p.A.',
   0x00E0A5: u'ComCore Semiconductor, Inc.',
   0x00E0A6: u'TELOGY NETWORKS, INC.',
   0x00E0A7: u'IPC INFORMATION SYSTEMS, INC.',
   0x00E0A8: u'SAT GmbH & Co.',
   0x00E0A9: u'FUNAI ELECTRIC CO., LTD.',
   0x00E0AA: u'ELECTROSONIC LTD.',
   0x00E0AB: u'DIMAT S.A.',
   0x00E0AC: u'MIDSCO, INC.',
   0x00E0AD: u'EES TECHNOLOGY, LTD.',
   0x00E0AE: u'XAQTI CORPORATION',
   0x00E0AF: u'GENERAL DYNAMICS INFORMATION SYSTEMS',
   0x00E0B0: u'CISCO SYSTEMS, INC.',
   0x00E0B1: u'Alcatel North America ESD',
   0x00E0B2: u'TELMAX COMMUNICATIONS CORP.',
   0x00E0B3: u'EtherWAN Systems, Inc.',
   0x00E0B4: u'TECHNO SCOPE CO., LTD.',
   0x00E0B5: u'ARDENT COMMUNICATIONS CORP.',
   0x00E0B6: u'Entrada Networks',
   0x00E0B7: u'PI GROUP, LTD.',
   0x00E0B8: u'GATEWAY 2000',
   0x00E0B9: u'BYAS SYSTEMS',
   0x00E0BA: u'BERGHOF AUTOMATIONSTECHNIK GmbH',
   0x00E0BB: u'NBX CORPORATION',
   0x00E0BC: u'SYMON COMMUNICATIONS, INC.',
   0x00E0BD: u'INTERFACE SYSTEMS, INC.',
   0x00E0BE: u'GENROCO INTERNATIONAL, INC.',
   0x00E0BF: u'TORRENT NETWORKING TECHNOLOGIES CORP.',
   0x00E0C0: u'SEIWA ELECTRIC MFG. CO., LTD.',
   0x00E0C1: u'MEMOREX TELEX JAPAN, LTD.',
   0x00E0C2: u'NECSY S.p.A.',
   0x00E0C3: u'SAKAI SYSTEM DEVELOPMENT CORP.',
   0x00E0C4: u'HORNER ELECTRIC, INC.',
   0x00E0C5: u'BCOM ELECTRONICS INC.',
   0x00E0C6: u'LINK2IT, L.L.C.',
   0x00E0C7: u'EUROTECH SRL',
   0x00E0C8: u'VIRTUAL ACCESS, LTD.',
   0x00E0C9: u'AutomatedLogic Corporation',
   0x00E0CA: u'BEST DATA PRODUCTS',
   0x00E0CB: u'RESON, INC.',
   0x00E0CC: u'HERO SYSTEMS, LTD.',
   0x00E0CD: u'SENSIS CORPORATION',
   0x00E0CE: u'ARN',
   0x00E0CF: u'INTEGRATED DEVICE TECHNOLOGY, INC.',
   0x00E0D0: u'NETSPEED, INC.',
   0x00E0D1: u'TELSIS LIMITED',
   0x00E0D2: u'VERSANET COMMUNICATIONS, INC.',
   0x00E0D3: u'DATENTECHNIK GmbH',
   0x00E0D4: u'EXCELLENT COMPUTER',
   0x00E0D5: u'ARCXEL TECHNOLOGIES, INC.',
   0x00E0D6: u'COMPUTER & COMMUNICATION RESEARCH LAB.',
   0x00E0D7: u'SUNSHINE ELECTRONICS, INC.',
   0x00E0D8: u'LANBit Computer, Inc.',
   0x00E0D9: u'TAZMO CO., LTD.',
   0x00E0DA: u'Alcatel North America ESD',
   0x00E0DB: u'ViaVideo Communications, Inc.',
   0x00E0DC: u'NEXWARE CORP.',
   0x00E0DD: u'ZENITH ELECTRONICS CORPORATION',
   0x00E0DE: u'DATAX NV',
   0x00E0DF: u'KE KOMMUNIKATIONS-ELECTRONIK',
   0x00E0E0: u'SI ELECTRONICS, LTD.',
   0x00E0E1: u'G2 NETWORKS, INC.',
   0x00E0E2: u'INNOVA CORP.',
   0x00E0E3: u'SK-ELEKTRONIK GmbH',
   0x00E0E4: u'FANUC ROBOTICS NORTH AMERICA, Inc.',
   0x00E0E5: u'CINCO NETWORKS, INC.',
   0x00E0E6: u'INCAA DATACOM B.V.',
   0x00E0E7: u'RAYTHEON E-SYSTEMS, INC.',
   0x00E0E8: u'GRETACODER Data Systems AG',
   0x00E0E9: u'DATA LABS, INC.',
   0x00E0EA: u'INNOVAT COMMUNICATIONS, INC.',
   0x00E0EB: u'DIGICOM SYSTEMS, INCORPORATED',
   0x00E0EC: u'CELESTICA INC.',
   0x00E0ED: u'SILICOM, LTD.',
   0x00E0EE: u'MAREL HF',
   0x00E0EF: u'DIONEX',
   0x00E0F0: u'ABLER TECHNOLOGY, INC.',
   0x00E0F1: u'THAT CORPORATION',
   0x00E0F2: u'ARLOTTO COMNET, INC.',
   0x00E0F3: u'WebSprint Communications, Inc.',
   0x00E0F4: u'INSIDE Technology A/S',
   0x00E0F5: u'TELES AG',
   0x00E0F6: u'DECISION EUROPE',
   0x00E0F7: u'CISCO SYSTEMS, INC.',
   0x00E0F8: u'DICNA CONTROL AB',
   0x00E0F9: u'CISCO SYSTEMS, INC.',
   0x00E0FA: u'TRL TECHNOLOGY, LTD.',
   0x00E0FB: u'LEIGHTRONIX, INC.',
   0x00E0FC: u'HUAWEI TECHNOLOGIES CO., LTD.',
   0x00E0FD: u'A-TREND TECHNOLOGY CO., LTD.',
   0x00E0FE: u'CISCO SYSTEMS, INC.',
   0x00E0FF: u'SECURITY DYNAMICS TECHNOLOGIES, Inc.',
   0x00E6D3: u'NIXDORF COMPUTER CORP.',
   0x020701: u'RACAL-DATACOM',
   0x021C7C: u'PERQ SYSTEMS CORPORATION',
   0x026086: u'LOGIC REPLACEMENT TECH. LTD.',
   0x02608C: u'3COM CORPORATION',
   0x027001: u'RACAL-DATACOM',
   0x0270B0: u'M/A-COM INC. COMPANIES',
   0x0270B3: u'DATA RECALL LTD',
   0x029D8E: u'CARDIAC RECORDERS INC.',
   0x02AA3C: u'OLIVETTI TELECOMM SPA (OLTECO)',
   0x02BB01: u'OCTOTHORPE CORP.',
   0x02C08C: u'3COM CORPORATION',
   0x02CF1C: u'COMMUNICATION MACHINERY CORP.',
   0x02E6D3: u'NIXDORF COMPUTER CORPORATION',
   0x040AE0: u'XMIT AG COMPUTER NETWORKS',
   0x04E0C4: u'TRIUMPH-ADLER AG',
   0x080001: u'COMPUTERVISION CORPORATION',
   0x080002: u'BRIDGE COMMUNICATIONS INC.',
   0x080003: u'ADVANCED COMPUTER COMM.',
   0x080004: u'CROMEMCO INCORPORATED',
   0x080005: u'SYMBOLICS INC.',
   0x080006: u'SIEMENS AG',
   0x080007: u'APPLE COMPUTER INC.',
   0x080008: u'BOLT BERANEK AND NEWMAN INC.',
   0x080009: u'HEWLETT PACKARD',
   0x08000A: u'NESTAR SYSTEMS INCORPORATED',
   0x08000B: u'UNISYS CORPORATION',
   0x08000C: u'MIKLYN DEVELOPMENT CO.',
   0x08000D: u'INTERNATIONAL COMPUTERS LTD.',
   0x08000E: u'NCR CORPORATION',
   0x08000F: u'MITEL CORPORATION',
   0x080011: u'TEKTRONIX INC.',
   0x080012: u'BELL ATLANTIC INTEGRATED SYST.',
   0x080013: u'EXXON',
   0x080014: u'EXCELAN',
   0x080015: u'STC BUSINESS SYSTEMS',
   0x080016: u'BARRISTER INFO SYS CORP',
   0x080017: u'NATIONAL SEMICONDUCTOR',
   0x080018: u'PIRELLI FOCOM NETWORKS',
   0x080019: u'GENERAL ELECTRIC CORPORATION',
   0x08001A: u'TIARA/ 10NET',
   0x08001B: u'DATA GENERAL',
   0x08001C: u'KDD-KOKUSAI DEBNSIN DENWA CO.',
   0x08001D: u'ABLE COMMUNICATIONS INC.',
   0x08001E: u'APOLLO COMPUTER INC.',
   0x08001F: u'SHARP CORPORATION',
   0x080020: u'SUN MICROSYSTEMS INC.',
   0x080021: u'3M COMPANY',
   0x080022: u'NBI INC.',
   0x080023: u'Panasonic Communications Co., Ltd.',
   0x080024: u'10NET COMMUNICATIONS/DCA',
   0x080025: u'CONTROL DATA',
   0x080026: u'NORSK DATA A.S.',
   0x080027: u'CADMUS COMPUTER SYSTEMS',
   0x080028: u'Texas Instruments',
   0x080029: u'MEGATEK CORPORATION',
   0x08002A: u'MOSAIC TECHNOLOGIES INC.',
   0x08002B: u'DIGITAL EQUIPMENT CORPORATION',
   0x08002C: u'BRITTON LEE INC.',
   0x08002D: u'LAN-TEC INC.',
   0x08002E: u'METAPHOR COMPUTER SYSTEMS',
   0x08002F: u'PRIME COMPUTER INC.',
   0x080030: u'NETWORK RESEARCH CORPORATION',
   0x080030: u'CERN',
   0x080030: u'ROYAL MELBOURNE INST OF TECH',
   0x080031: u'LITTLE MACHINES INC.',
   0x080032: u'TIGAN INCORPORATED',
   0x080033: u'BAUSCH & LOMB',
   0x080034: u'FILENET CORPORATION',
   0x080035: u'MICROFIVE CORPORATION',
   0x080036: u'INTERGRAPH CORPORATION',
   0x080037: u'FUJI-XEROX CO. LTD.',
   0x080038: u'CII HONEYWELL BULL',
   0x080039: u'SPIDER SYSTEMS LIMITED',
   0x08003A: u'ORCATECH INC.',
   0x08003B: u'TORUS SYSTEMS LIMITED',
   0x08003C: u'SCHLUMBERGER WELL SERVICES',
   0x08003D: u'CADNETIX CORPORATIONS',
   0x08003E: u'CODEX CORPORATION',
   0x08003F: u'FRED KOSCHARA ENTERPRISES',
   0x080040: u'FERRANTI COMPUTER SYS. LIMITED',
   0x080041: u'RACAL-MILGO INFORMATION SYS..',
   0x080042: u'JAPAN MACNICS CORP.',
   0x080043: u'PIXEL COMPUTER INC.',
   0x080044: u'DAVID SYSTEMS INC.',
   0x080045: u'CONCURRENT COMPUTER CORP.',
   0x080046: u'SONY CORPORATION LTD.',
   0x080047: u'SEQUENT COMPUTER SYSTEMS INC.',
   0x080048: u'EUROTHERM GAUGING SYSTEMS',
   0x080049: u'UNIVATION',
   0x08004A: u'BANYAN SYSTEMS INC.',
   0x08004B: u'PLANNING RESEARCH CORP.',
   0x08004C: u'HYDRA COMPUTER SYSTEMS INC.',
   0x08004D: u'CORVUS SYSTEMS INC.',
   0x08004E: u'3COM EUROPE LTD.',
   0x08004F: u'CYGNET SYSTEMS',
   0x080050: u'DAISY SYSTEMS CORP.',
   0x080051: u'EXPERDATA',
   0x080052: u'INSYSTEC',
   0x080053: u'MIDDLE EAST TECH. UNIVERSITY',
   0x080055: u'STANFORD TELECOMM. INC.',
   0x080056: u'STANFORD LINEAR ACCEL. CENTER',
   0x080057: u'EVANS & SUTHERLAND',
   0x080058: u'SYSTEMS CONCEPTS',
   0x080059: u'A/S MYCRON',
   0x08005A: u'IBM CORPORATION',
   0x08005B: u'VTA TECHNOLOGIES INC.',
   0x08005C: u'FOUR PHASE SYSTEMS',
   0x08005D: u'GOULD INC.',
   0x08005E: u'COUNTERPOINT COMPUTER INC.',
   0x08005F: u'SABER TECHNOLOGY CORP.',
   0x080060: u'INDUSTRIAL NETWORKING INC.',
   0x080061: u'JAROGATE LTD.',
   0x080062: u'GENERAL DYNAMICS',
   0x080063: u'PLESSEY',
   0x080064: u'AUTOPHON AG',
   0x080065: u'GENRAD INC.',
   0x080066: u'AGFA CORPORATION',
   0x080067: u'COMDESIGN',
   0x080068: u'RIDGE COMPUTERS',
   0x080069: u'SILICON GRAPHICS INC.',
   0x08006A: u'ATT BELL LABORATORIES',
   0x08006B: u'ACCEL TECHNOLOGIES INC.',
   0x08006C: u'SUNTEK TECHNOLOGY INT\'L',
   0x08006D: u'WHITECHAPEL COMPUTER WORKS',
   0x08006E: u'MASSCOMP',
   0x08006F: u'PHILIPS APELDOORN B.V.',
   0x080070: u'MITSUBISHI ELECTRIC CORP.',
   0x080071: u'MATRA (DSIE)',
   0x080072: u'XEROX CORP UNIV GRANT PROGRAM',
   0x080073: u'TECMAR INC.',
   0x080074: u'CASIO COMPUTER CO. LTD.',
   0x080075: u'DANSK DATA ELECTRONIK',
   0x080076: u'PC LAN TECHNOLOGIES',
   0x080077: u'TSL COMMUNICATIONS LTD.',
   0x080078: u'ACCELL CORPORATION',
   0x080079: u'THE DROID WORKS',
   0x08007A: u'INDATA',
   0x08007B: u'SANYO ELECTRIC CO. LTD.',
   0x08007C: u'VITALINK COMMUNICATIONS CORP.',
   0x08007E: u'AMALGAMATED WIRELESS(AUS) LTD',
   0x08007F: u'CARNEGIE-MELLON UNIVERSITY',
   0x080080: u'AES DATA INC.',
   0x080081: u'ASTECH INC.',
   0x080082: u'VERITAS SOFTWARE',
   0x080083: u'Seiko Instruments Inc.',
   0x080084: u'TOMEN ELECTRONICS CORP.',
   0x080085: u'ELXSI',
   0x080086: u'KONICA MINOLTA HOLDINGS, INC.',
   0x080087: u'XYPLEX',
   0x080088: u'MCDATA CORPORATION',
   0x080089: u'KINETICS',
   0x08008A: u'PERFORMANCE TECHNOLOGY',
   0x08008B: u'PYRAMID TECHNOLOGY CORP.',
   0x08008C: u'NETWORK RESEARCH CORPORATION',
   0x08008D: u'XYVISION INC.',
   0x08008E: u'TANDEM COMPUTERS',
   0x08008F: u'CHIPCOM CORPORATION',
   0x080090: u'SONOMA SYSTEMS',
   0x081443: u'UNIBRAIN S.A.',
   0x08BBCC: u'AK-NORD EDV VERTRIEBSGES. mbH',
   0x100000: u'PRIVATE',
   0x10005A: u'IBM CORPORATION',
   0x1000E8: u'NATIONAL SEMICONDUCTOR',
   0x1100AA: u'PRIVATE',
   0x800010: u'ATT BELL LABORATORIES',
   0xA06A00: u'Verilink Corporation',
   0xAA0000: u'DIGITAL EQUIPMENT CORPORATION',
   0xAA0001: u'DIGITAL EQUIPMENT CORPORATION',
   0xAA0002: u'DIGITAL EQUIPMENT CORPORATION',
   0xAA0003: u'DIGITAL EQUIPMENT CORPORATION',
   0xAA0004: u'DIGITAL EQUIPMENT CORPORATION',
   0xACDE48: u'PRIVATE',
}


########NEW FILE########
__FILENAME__ = tcpdump
"""
Tcpdump parser

Source:
 * libpcap source code (file savefile.c)
 * RFC 791 (IPv4)
 * RFC 792 (ICMP)
 * RFC 793 (TCP)
 * RFC 1122 (Requirements for Internet Hosts)

Author: Victor Stinner
Creation: 23 march 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    Enum, Bytes, NullBytes, RawBytes,
    UInt8, UInt16, UInt32, Int32, TimestampUnix32,
    Bit, Bits, NullBits)
from lib.hachoir_core.endian import NETWORK_ENDIAN, LITTLE_ENDIAN
from lib.hachoir_core.tools import humanDuration
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.tools import createDict
from lib.hachoir_parser.network.common import MAC48_Address, IPv4_Address, IPv6_Address

def diff(field):
    return humanDuration(field.value*1000)

class Layer(FieldSet):
    endian = NETWORK_ENDIAN
    def parseNext(self, parent):
        return None

class ARP(Layer):
    opcode_name = {
        1: "request",
        2: "reply"
    }
    endian = NETWORK_ENDIAN

    def createFields(self):
        yield UInt16(self, "hw_type")
        yield UInt16(self, "proto_type")
        yield UInt8(self, "hw_size")
        yield UInt8(self, "proto_size")
        yield Enum(UInt16(self, "opcode"), ARP.opcode_name)
        yield MAC48_Address(self, "src_mac")
        yield IPv4_Address(self, "src_ip")
        yield MAC48_Address(self, "dst_mac")
        yield IPv4_Address(self, "dst_ip")

    def createDescription(self):
        desc = "ARP: %s" % self["opcode"].display
        opcode = self["opcode"].value
        src_ip = self["src_ip"].display
        dst_ip = self["dst_ip"].display
        if opcode == 1:
            desc += ", %s ask %s" % (dst_ip, src_ip)
        elif opcode == 2:
            desc += " from %s" % src_ip
        return desc

class TCP_Option(FieldSet):
    NOP = 1
    MAX_SEGMENT = 2
    WINDOW_SCALE = 3
    SACK = 4
    TIMESTAMP = 8

    code_name = {
        NOP: "NOP",
        MAX_SEGMENT: "Max segment size",
        WINDOW_SCALE: "Window scale",
        SACK: "SACK permitted",
        TIMESTAMP: "Timestamp"
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        if self["code"].value != self.NOP:
            self._size = self["length"].value * 8
        else:
            self._size = 8

    def createFields(self):
        yield Enum(UInt8(self, "code", "Code"), self.code_name)
        code = self["code"].value
        if code == self.NOP:
            return
        yield UInt8(self, "length", "Option size in bytes")
        if code == self.MAX_SEGMENT:
            yield UInt16(self, "max_seg", "Maximum segment size")
        elif code == self.WINDOW_SCALE:
            yield UInt8(self, "win_scale", "Window scale")
        elif code == self.TIMESTAMP:
            yield UInt32(self, "ts_val", "Timestamp value")
            yield UInt32(self, "ts_ecr", "Timestamp echo reply")
        else:
            size = (self.size - self.current_size) // 8
            if size:
                yield RawBytes(self, "data", size)

    def createDescription(self):
        return "TCP option: %s" % self["code"].display

class TCP(Layer):
    port_name = {
        13: "daytime",
        20: "ftp data",
        21: "ftp",
        23: "telnet",
        25: "smtp",
        53: "dns",
        63: "dhcp/bootp",
        80: "HTTP",
        110: "pop3",
        119: "nntp",
        123: "ntp",
        139: "netbios session service",
        1863: "MSNMS",
        6667: "IRC"
    }

    def createFields(self):
        yield Enum(UInt16(self, "src"), self.port_name)
        yield Enum(UInt16(self, "dst"), self.port_name)
        yield UInt32(self, "seq_num")
        yield UInt32(self, "ack_num")

        yield Bits(self, "hdrlen", 6, "Header lenght")
        yield NullBits(self, "reserved", 2, "Reserved")

        yield Bit(self, "cgst", "Congestion Window Reduced")
        yield Bit(self, "ecn-echo", "ECN-echo")
        yield Bit(self, "urg", "Urgent")
        yield Bit(self, "ack", "Acknowledge")
        yield Bit(self, "psh", "Push mmode")
        yield Bit(self, "rst", "Reset connection")
        yield Bit(self, "syn", "Synchronize")
        yield Bit(self, "fin", "Stop the connection")

        yield UInt16(self, "winsize", "Windows size")
        yield textHandler(UInt16(self, "checksum"), hexadecimal)
        yield UInt16(self, "urgent")

        size = self["hdrlen"].value*8 - self.current_size
        while 0 < size:
            option = TCP_Option(self, "option[]")
            yield option
            size -= option.size

    def parseNext(self, parent):
        return None

    def createDescription(self):
        src = self["src"].value
        dst = self["dst"].value
        if src < 32768:
            src = self["src"].display
        else:
            src = None
        if dst < 32768:
            dst = self["dst"].display
        else:
            dst = None
        desc = "TCP"
        if src != None and dst != None:
            desc += " (%s->%s)" % (src, dst)
        elif src != None:
            desc += " (%s->)" % (src)
        elif dst != None:
            desc += " (->%s)" % (dst)

        # Get flags
        flags = []
        if self["syn"].value:
            flags.append("SYN")
        if self["ack"].value:
            flags.append("ACK")
        if self["fin"].value:
            flags.append("FIN")
        if self["rst"].value:
            flags.append("RST")
        if flags:
            desc += " [%s]" % (",".join(flags))
        return desc

class UDP(Layer):
    port_name = {
        12: "daytime",
        22: "ssh",
        53: "DNS",
        67: "dhcp/bootp",
        80: "http",
        110: "pop3",
        123: "ntp",
        137: "netbios name service",
        138: "netbios datagram service"
    }

    def createFields(self):
        yield Enum(UInt16(self, "src"), UDP.port_name)
        yield Enum(UInt16(self, "dst"), UDP.port_name)
        yield UInt16(self, "length")
        yield textHandler(UInt16(self, "checksum"), hexadecimal)

    def createDescription(self):
        return "UDP (%s->%s)" % (self["src"].display, self["dst"].display)

class ICMP(Layer):
    REJECT = 3
    PONG = 0
    PING = 8
    type_desc = {
        PONG: "Pong",
        REJECT: "Reject",
        PING: "Ping"
    }
    reject_reason = {
        0: "net unreachable",
        1: "host unreachable",
        2: "protocol unreachable",
        3: "port unreachable",
        4: "fragmentation needed and DF set",
        5: "source route failed",
        6: "Destination network unknown error",
        7: "Destination host unknown error",
        8: "Source host isolated error",
        9: "Destination network administratively prohibited",
        10: "Destination host administratively prohibited",
        11: "Unreachable network for Type Of Service",
        12: "Unreachable host for Type Of Service.",
        13: "Communication administratively prohibited",
        14: "Host precedence violation",
        15: "Precedence cutoff in effect"
    }

    def createFields(self):
        # Type
        yield Enum(UInt8(self, "type"), self.type_desc)
        type = self["type"].value

        # Code
        field = UInt8(self, "code")
        if type == 3:
            field = Enum(field, self.reject_reason)
        yield field

        # Options
        yield textHandler(UInt16(self, "checksum"), hexadecimal)
        if type in (self.PING, self.PONG): # and self["code"].value == 0:
            yield UInt16(self, "id")
            yield UInt16(self, "seq_num")
            # follow: ping data
        elif type == self.REJECT:
            yield NullBytes(self, "empty", 2)
            yield UInt16(self, "hop_mtu", "Next-Hop MTU")

    def createDescription(self):
        type = self["type"].value
        if type in (self.PING, self.PONG):
            return "%s (num=%s)" % (self["type"].display, self["seq_num"].value)
        else:
            return "ICMP (%s)" % self["type"].display

    def parseNext(self, parent):
        if self["type"].value == self.REJECT:
            return IPv4(parent, "rejected_ipv4")
        else:
            return None

class ICMPv6(Layer):
    ECHO_REQUEST = 128
    ECHO_REPLY = 129
    TYPE_DESC = {
        128: "Echo request",
        129: "Echo reply",
    }

    def createFields(self):
        yield Enum(UInt8(self, "type"), self.TYPE_DESC)
        yield UInt8(self, "code")
        yield textHandler(UInt16(self, "checksum"), hexadecimal)

        if self['type'].value in (self.ECHO_REQUEST, self.ECHO_REPLY):
            yield UInt16(self, "id")
            yield UInt16(self, "sequence")

    def createDescription(self):
        if self['type'].value in (self.ECHO_REQUEST, self.ECHO_REPLY):
            return "%s (num=%s)" % (self["type"].display, self["sequence"].value)
        else:
            return "ICMPv6 (%s)" % self["type"].display

class IP(Layer):
    PROTOCOL_INFO = {
         1: ("icmp", ICMP, "ICMP"),
        6: ("tcp",  TCP, "TCP"),
        17: ("udp",  UDP, "UDP"),
        58: ("icmpv6",  ICMPv6, "ICMPv6"),
        60: ("ipv6_opts", None, "IPv6 destination option"),
    }
    PROTOCOL_NAME = createDict(PROTOCOL_INFO, 2)

    def parseNext(self, parent):
        proto = self["protocol"].value
        if proto not in self.PROTOCOL_INFO:
            return None
        name, parser, desc = self.PROTOCOL_INFO[proto]
        if not parser:
            return None
        return parser(parent, name)

class IPv4(IP):
    precedence_name = {
        7: "Network Control",
        6: "Internetwork Control",
        5: "CRITIC/ECP",
        4: "Flash Override",
        3: "Flash",
        2: "Immediate",
        1: "Priority",
        0: "Routine",
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = self["hdr_size"].value * 32

    def createFields(self):
        yield Bits(self, "version", 4, "Version")
        yield Bits(self, "hdr_size", 4, "Header size divided by 5")

        # Type of service
        yield Enum(Bits(self, "precedence", 3, "Precedence"), self.precedence_name)
        yield Bit(self, "low_delay", "If set, low delay, else normal delay")
        yield Bit(self, "high_throu", "If set, high throughput, else normal throughput")
        yield Bit(self, "high_rel", "If set, high relibility, else normal")
        yield NullBits(self, "reserved[]", 2, "(reserved for future use)")

        yield UInt16(self, "length")
        yield UInt16(self, "id")

        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "df", "Don't fragment")
        yield Bit(self, "more_frag", "There are more fragments? if not set, it's the last one")
        yield Bits(self, "frag_ofst_lo", 5)
        yield UInt8(self, "frag_ofst_hi")
        yield UInt8(self, "ttl", "Type to live")
        yield Enum(UInt8(self, "protocol"), self.PROTOCOL_NAME)
        yield textHandler(UInt16(self, "checksum"), hexadecimal)
        yield IPv4_Address(self, "src")
        yield IPv4_Address(self, "dst")

        size = (self.size - self.current_size) // 8
        if size:
            yield RawBytes(self, "options", size)

    def createDescription(self):
        return "IPv4 (%s>%s)" % (self["src"].display, self["dst"].display)

class IPv6(IP):
    static_size = 40 * 8
    endian = NETWORK_ENDIAN

    def createFields(self):
        yield Bits(self, "version", 4, "Version (6)")
        yield Bits(self, "traffic", 8, "Traffic class")
        yield Bits(self, "flow", 20, "Flow label")
        yield Bits(self, "length", 16, "Payload length")
        yield Enum(Bits(self, "protocol", 8, "Next header"), self.PROTOCOL_NAME)
        yield Bits(self, "hop_limit", 8, "Hop limit")
        yield IPv6_Address(self, "src")
        yield IPv6_Address(self, "dst")

    def createDescription(self):
        return "IPv6 (%s>%s)" % (self["src"].display, self["dst"].display)

class Layer2(Layer):
    PROTO_INFO = {
        0x0800: ("ipv4", IPv4, "IPv4"),
        0x0806: ("arp",  ARP,  "ARP"),
        0x86dd: ("ipv6", IPv6, "IPv6"),
    }
    PROTO_DESC = createDict(PROTO_INFO, 2)

    def parseNext(self, parent):
        try:
            name, parser, desc = self.PROTO_INFO[ self["protocol"].value ]
            return parser(parent, name)
        except KeyError:
            return None

class Unicast(Layer2):
    packet_type_name = {
        0: "Unicast to us"
    }
    def createFields(self):
        yield Enum(UInt16(self, "packet_type"), self.packet_type_name)
        yield UInt16(self, "addr_type", "Link-layer address type")
        yield UInt16(self, "addr_length", "Link-layer address length")
        length = self["addr_length"].value
        length = 8   # FIXME: Should we use addr_length or not?
        if length:
            yield RawBytes(self, "source", length)
        yield Enum(UInt16(self, "protocol"), self.PROTO_DESC)

class Ethernet(Layer2):
    static_size = 14*8
    def createFields(self):
        yield MAC48_Address(self, "dst")
        yield MAC48_Address(self, "src")
        yield Enum(UInt16(self, "protocol"), self.PROTO_DESC)

    def createDescription(self):
        return "Ethernet: %s>%s (%s)" % \
            (self["src"].display, self["dst"].display, self["protocol"].display)

class Packet(FieldSet):
    endian = LITTLE_ENDIAN

    def __init__(self, parent, name, parser, first_name):
        FieldSet.__init__(self, parent, name)
        self._size = (16 + self["caplen"].value) * 8
        self._first_parser = parser
        self._first_name = first_name

    def createFields(self):
        yield TimestampUnix32(self, "ts_epoch", "Timestamp (Epoch)")
        yield UInt32(self, "ts_nanosec", "Timestamp (nano second)")
        yield UInt32(self, "caplen", "length of portion present")
        yield UInt32(self, "len", "length this packet (off wire)")

        # Read different layers
        field = self._first_parser(self, self._first_name)
        while field:
            yield field
            field = field.parseNext(self)

        # Read data if any
        size = (self.size - self.current_size) // 8
        if size:
            yield RawBytes(self, "data", size)

    def getTimestamp(self):
        nano_sec = float(self["ts_nanosec"].value) / 100
        from datetime import timedelta
        return self["ts_epoch"].value + timedelta(microseconds=nano_sec)

    def createDescription(self):
        t0 = self["/packet[0]"].getTimestamp()
#        ts = max(self.getTimestamp() - t0, t0)
        ts = self.getTimestamp() - t0
        #text = ["%1.6f: " % ts]
        text = ["%s: " % ts]
        if "icmp" in self:
            text.append(self["icmp"].description)
        elif "tcp" in self:
            text.append(self["tcp"].description)
        elif "udp" in self:
            text.append(self["udp"].description)
        elif "arp" in self:
            text.append(self["arp"].description)
        else:
            text.append("Packet")
        return "".join(text)

class TcpdumpFile(Parser):
    PARSER_TAGS = {
        "id": "tcpdump",
        "category": "misc",
        "min_size": 24*8,
        "description": "Tcpdump file (network)",
        "magic": (("\xd4\xc3\xb2\xa1", 0),),
    }
    endian = LITTLE_ENDIAN

    LINK_TYPE = {
          1: ("ethernet", Ethernet),
        113: ("unicast", Unicast),
    }
    LINK_TYPE_DESC = createDict(LINK_TYPE, 0)

    def validate(self):
        if self["id"].value != "\xd4\xc3\xb2\xa1":
            return "Wrong file signature"
        if self["link_type"].value not in self.LINK_TYPE:
            return "Unknown link type"
        return True

    def createFields(self):
        yield Bytes(self, "id", 4, "Tcpdump identifier")
        yield UInt16(self, "maj_ver", "Major version")
        yield UInt16(self, "min_ver", "Minor version")
        yield Int32(self, "this_zone", "GMT to local time zone correction")
        yield Int32(self, "sigfigs", "accuracy of timestamps")
        yield UInt32(self, "snap_len", "max length saved portion of each pkt")
        yield Enum(UInt32(self, "link_type", "data link type"), self.LINK_TYPE_DESC)
        link = self["link_type"].value
        if link not in self.LINK_TYPE:
            raise ParserError("Unknown link type: %s" % link)
        name, parser = self.LINK_TYPE[link]
        while self.current_size < self.size:
            yield Packet(self, "packet[]", parser, name)


########NEW FILE########
__FILENAME__ = parser
import lib.hachoir_core.config as config
from lib.hachoir_core.field import Parser as GenericParser
from lib.hachoir_core.error import HACHOIR_ERRORS, HachoirError, error
from lib.hachoir_core.tools import makeUnicode
from lib.hachoir_core.i18n import _
from inspect import getmro


class ValidateError(HachoirError):
    pass

class HachoirParser(object):
    """
    A parser is the root of all other fields. It create first level of fields
    and have special attributes and methods:
    - tags: dictionnary with keys:
      - "file_ext": classical file extensions (string or tuple of strings) ;
      - "mime": MIME type(s) (string or tuple of strings) ;
      - "description": String describing the parser.
    - endian: Byte order (L{BIG_ENDIAN} or L{LITTLE_ENDIAN}) of input data ;
    - stream: Data input stream (set in L{__init__()}).

    Default values:
    - size: Field set size will be size of input stream ;
    - mime_type: First MIME type of tags["mime"] (if it does exist,
      None otherwise).
    """

    _autofix = False

    def __init__(self, stream, **args):
        validate = args.pop("validate", False)
        self._mime_type = None
        while validate:
            nbits = self.getParserTags()["min_size"]
            if stream.sizeGe(nbits):
                res = self.validate()
                if res is True:
                    break
                res = makeUnicode(res)
            else:
                res = _("stream is smaller than %s.%s bytes" % divmod(nbits, 8))
            raise ValidateError(res or _("no reason given"))
        self._autofix = True

    #--- Methods that can be overridden -------------------------------------
    def createDescription(self):
        """
        Create an Unicode description
        """
        return self.PARSER_TAGS["description"]

    def createMimeType(self):
        """
        Create MIME type (string), eg. "image/png"

        If it returns None, "application/octet-stream" is used.
        """
        if "mime" in self.PARSER_TAGS:
            return self.PARSER_TAGS["mime"][0]
        return None

    def validate(self):
        """
        Check that the parser is able to parse the stream. Valid results:
        - True: stream looks valid ;
        - False: stream is invalid ;
        - str: string describing the error.
        """
        raise NotImplementedError()

    #--- Getter methods -----------------------------------------------------
    def _getDescription(self):
        if self._description is None:
            try:
                self._description = self.createDescription()
                if isinstance(self._description, str):
                    self._description = makeUnicode(self._description)
            except HACHOIR_ERRORS, err:
                error("Error getting description of %s: %s" \
                    % (self.path, unicode(err)))
                self._description = self.PARSER_TAGS["description"]
        return self._description
    description = property(_getDescription,
    doc="Description of the parser")

    def _getMimeType(self):
        if not self._mime_type:
            try:
                self._mime_type = self.createMimeType()
            except HACHOIR_ERRORS, err:
                self.error("Error when creating MIME type: %s" % unicode(err))
            if not self._mime_type \
            and self.createMimeType != Parser.createMimeType:
                self._mime_type = Parser.createMimeType(self)
            if not self._mime_type:
                self._mime_type = u"application/octet-stream"
        return self._mime_type
    mime_type = property(_getMimeType)

    def createContentSize(self):
        return None
    def _getContentSize(self):
        if not hasattr(self, "_content_size"):
            try:
                self._content_size = self.createContentSize()
            except HACHOIR_ERRORS, err:
                error("Unable to compute %s content size: %s" % (self.__class__.__name__, err))
                self._content_size = None
        return self._content_size
    content_size = property(_getContentSize)

    def createFilenameSuffix(self):
        """
        Create filename suffix: "." + first value of self.PARSER_TAGS["file_ext"],
        or None if self.PARSER_TAGS["file_ext"] doesn't exist.
        """
        file_ext = self.getParserTags().get("file_ext")
        if isinstance(file_ext, (tuple, list)):
            file_ext = file_ext[0]
        return file_ext and '.' + file_ext
    def _getFilenameSuffix(self):
        if not hasattr(self, "_filename_suffix"):
            self._filename_extension = self.createFilenameSuffix()
        return self._filename_extension
    filename_suffix = property(_getFilenameSuffix)

    @classmethod
    def getParserTags(cls):
        tags = {}
        for cls in reversed(getmro(cls)):
            if hasattr(cls, "PARSER_TAGS"):
                tags.update(cls.PARSER_TAGS)
        return tags

    @classmethod
    def print_(cls, out, verbose):
        tags = cls.getParserTags()
        print >>out, "- %s: %s" % (tags["id"], tags["description"])
        if verbose:
            if "mime" in tags:
                print >>out, "  MIME type: %s" % (", ".join(tags["mime"]))
            if "file_ext" in tags:
                file_ext = ", ".join(
                    ".%s" % file_ext for file_ext in tags["file_ext"])
                print >>out, "  File extension: %s" % file_ext

    autofix = property(lambda self: self._autofix and config.autofix)

class Parser(HachoirParser, GenericParser):
    def __init__(self, stream, **args):
        GenericParser.__init__(self, stream)
        HachoirParser.__init__(self, stream, **args)


########NEW FILE########
__FILENAME__ = parser_list
import re
import types
from lib.hachoir_core.error import error
from lib.hachoir_core.i18n import _
from lib.hachoir_parser import Parser, HachoirParser
import sys

### Parser list ################################################################

class ParserList(object):
    VALID_CATEGORY = ("archive", "audio", "container", "file_system",
        "game", "image", "misc", "program", "video")
    ID_REGEX = re.compile("^[a-z0-9][a-z0-9_]{2,}$")

    def __init__(self):
        self.parser_list = []
        self.bytag = { "id": {}, "category": {} }

    def translate(self, name, value):
        if name in ("magic",):
            return True
        elif name == "min_size":
            return - value < 0 or "Invalid minimum size (min_size)"
        elif name == "description":
            return isinstance(value, (str, unicode)) and bool(value) or "Invalid description"
        elif name == "category":
            if value not in self.VALID_CATEGORY:
                return "Invalid category: %r" % value
        elif name == "id":
            if type(value) is not str or not self.ID_REGEX.match(value):
                return "Invalid identifier: %r" % value
            parser = self.bytag[name].get(value)
            if parser:
                return "Duplicate parser id: %s already used by %s" % \
                    (value, parser[0].__name__)
        # TODO: lists should be forbidden
        if isinstance(value, list):
            value = tuple(value)
        elif not isinstance(value, tuple):
            value = value,
        return name, value

    def validParser(self, parser, tags):
        if "id" not in tags:
            return "No identifier"
        if "description" not in tags:
            return "No description"
        # TODO: Allow simple strings for file_ext/mime ?
        # (see also HachoirParser.createFilenameSuffix)
        file_ext = tags.get("file_ext", ())
        if not isinstance(file_ext, (tuple, list)):
            return "File extension is not a tuple or list"
        mimes = tags.get("mime", ())
        if not isinstance(mimes, tuple):
            return "MIME type is not a tuple"
        for mime in mimes:
            if not isinstance(mime, unicode):
                return "MIME type %r is not an unicode string" % mime

        return ""

    def add(self, parser):
        tags = parser.getParserTags()
        err = self.validParser(parser, tags)
        if err:
            error("Skip parser %s: %s" % (parser.__name__, err))
            return

        _tags = []
        for tag in tags.iteritems():
            tag = self.translate(*tag)
            if isinstance(tag, tuple):
                _tags.append(tag)
            elif tag is not True:
                error("[%s] %s" % (parser.__name__, tag))
                return

        self.parser_list.append(parser)

        for name, values in _tags:
            byname = self.bytag.setdefault(name,{})
            for value in values:
                byname.setdefault(value,[]).append(parser)

    def __iter__(self):
        return iter(self.parser_list)

    def print_(self, title=None, out=None, verbose=False, format="one-line"):
        """Display a list of parser with its title
         * out: output file
         * title : title of the list to display
         * format: "rest", "trac", "file-ext", "mime" or "one_line" (default)
        """
        if out is None:
            out = sys.stdout

        if format in ("file-ext", "mime"):
            # Create file extension set
            extensions = set()
            for parser in self:
                file_ext = parser.getParserTags().get(format, ())
                file_ext = list(file_ext)
                try:
                    file_ext.remove("")
                except ValueError:
                    pass
                extensions |= set(file_ext)

            # Remove empty extension
            extensions -= set(('',))

            # Convert to list and sort by ASCII order
            extensions = list(extensions)
            extensions.sort()

            # Print list
            text = ", ".join( str(item) for item in extensions )
            if format == "file-ext":
                print >>out, "File extensions: %s." % text
                print >>out
                print >>out, "Total: %s file extensions." % len(extensions)
            else:
                print >>out, "MIME types: %s." % text
                print >>out
                print >>out, "Total: %s MIME types." % len(extensions)
            return

        if format == "trac":
            print >>out, "== List of parsers =="
            print >>out
            print >>out, "Total: %s parsers" % len(self.parser_list)
            print >>out
        elif format == "one_line":
            if title:
                print >>out, title
            else:
                print >>out, _("List of Hachoir parsers.")
            print >>out

        # Create parser list sorted by module
        bycategory = self.bytag["category"]
        for category in sorted(bycategory.iterkeys()):
            if format == "one_line":
                parser_list = [ parser.PARSER_TAGS["id"] for parser in bycategory[category] ]
                parser_list.sort()
                print >>out, "- %s: %s" % (category.title(), ", ".join(parser_list))
            else:
                if format == "rest":
                    print >>out, category.replace("_", " ").title()
                    print >>out, "-" * len(category)
                    print >>out
                elif format == "trac":
                    print >>out, "=== %s ===" % category.replace("_", " ").title()
                    print >>out
                else:
                    print >>out, "[%s]" % category
                parser_list = sorted(bycategory[category],
                    key=lambda parser: parser.PARSER_TAGS["id"])
                if format == "rest":
                    for parser in parser_list:
                        tags = parser.getParserTags()
                        print >>out, "* %s: %s" % (tags["id"], tags["description"])
                elif format == "trac":
                    for parser in parser_list:
                        tags = parser.getParserTags()
                        desc = tags["description"]
                        desc = re.sub(r"([A-Z][a-z]+[A-Z][^ ]+)", r"!\1", desc)
                        print >>out, " * %s: %s" % (tags["id"], desc)
                else:
                    for parser in parser_list:
                        parser.print_(out, verbose)
                print >>out
        if format != "trac":
            print >>out, "Total: %s parsers" % len(self.parser_list)


class HachoirParserList(ParserList):
    _instance = None

    @classmethod
    def getInstance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self):
        ParserList.__init__(self)
        self._load()

    def _load(self):
        """
        Load all parsers from "hachoir.parser" module.

        Return the list of loaded parsers.
        """
        # Parser list is already loaded?
        if self.parser_list:
            return self.parser_list

        todo = []
        from lib import hachoir_parser
        module = hachoir_parser
        for attrname in dir(module):
            attr = getattr(module, attrname)
            if isinstance(attr, types.ModuleType):
                todo.append(attr)

        for module in todo:
            for name in dir(module):
                attr = getattr(module, name)
                if isinstance(attr, type) \
                and issubclass(attr, HachoirParser) \
                and attr not in (Parser, HachoirParser):
                    self.add(attr)
        assert 1 <= len(self.parser_list)
        return self.parser_list


########NEW FILE########
__FILENAME__ = elf
"""
ELF (Unix/BSD executable file format) parser.

Author: Victor Stinner
Creation date: 08 may 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt16, UInt32, Enum,
    String, Bytes)
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN

class ElfHeader(FieldSet):
    static_size = 52*8
    LITTLE_ENDIAN_ID = 1
    BIG_ENDIAN_ID = 2
    MACHINE_NAME = {
        1: u"AT&T WE 32100",
        2: u"SPARC",
        3: u"Intel 80386",
        4: u"Motorola 68000",
        5: u"Motorola 88000",
        7: u"Intel 80860",
        8: u"MIPS RS3000"
    }
    CLASS_NAME = {
        1: u"32 bits",
        2: u"64 bits"
    }
    TYPE_NAME = {
             0: u"No file type",
             1: u"Relocatable file",
             2: u"Executable file",
             3: u"Shared object file",
             4: u"Core file",
        0xFF00: u"Processor-specific (0xFF00)",
        0xFFFF: u"Processor-specific (0xFFFF)"
    }
    ENDIAN_NAME = {
        LITTLE_ENDIAN_ID: "Little endian",
        BIG_ENDIAN_ID: "Big endian",
    }

    def createFields(self):
        yield Bytes(self, "signature", 4, r'ELF signature ("\x7fELF")')
        yield Enum(UInt8(self, "class", "Class"), self.CLASS_NAME)
        yield Enum(UInt8(self, "endian", "Endian"), self.ENDIAN_NAME)
        yield UInt8(self, "file_version", "File version")
        yield String(self, "pad", 8, "Pad")
        yield UInt8(self, "nb_ident", "Size of ident[]")
        yield Enum(UInt16(self, "type", "File type"), self.TYPE_NAME)
        yield Enum(UInt16(self, "machine", "Machine type"), self.MACHINE_NAME)
        yield UInt32(self, "version", "ELF format version")
        yield UInt32(self, "entry", "Number of entries")
        yield UInt32(self, "phoff", "Program header offset")
        yield UInt32(self, "shoff", "Section header offset")
        yield UInt32(self, "flags", "Flags")
        yield UInt16(self, "ehsize", "Elf header size (this header)")
        yield UInt16(self, "phentsize", "Program header entry size")
        yield UInt16(self, "phnum", "Program header entry count")
        yield UInt16(self, "shentsize", "Section header entry size")
        yield UInt16(self, "shnum", "Section header entre count")
        yield UInt16(self, "shstrndx", "Section header strtab index")

    def isValid(self):
        if self["signature"].value != "\x7FELF":
            return "Wrong ELF signature"
        if self["class"].value not in self.CLASS_NAME:
            return "Unknown class"
        if self["endian"].value not in self.ENDIAN_NAME:
            return "Unknown endian (%s)" % self["endian"].value
        return ""

class SectionHeader32(FieldSet):
    static_size = 40*8
    TYPE_NAME = {
        8: "BSS"
    }

    def createFields(self):
        yield UInt32(self, "name", "Name")
        yield Enum(UInt32(self, "type", "Type"), self.TYPE_NAME)
        yield UInt32(self, "flags", "Flags")
        yield textHandler(UInt32(self, "VMA", "Virtual memory address"), hexadecimal)
        yield textHandler(UInt32(self, "LMA", "Logical memory address (in file)"), hexadecimal)
        yield textHandler(UInt32(self, "size", "Size"), hexadecimal)
        yield UInt32(self, "link", "Link")
        yield UInt32(self, "info", "Information")
        yield UInt32(self, "addr_align", "Address alignment")
        yield UInt32(self, "entry_size", "Entry size")

    def createDescription(self):
        return "Section header (name: %s, type: %s)" % \
            (self["name"].value, self["type"].display)

class ProgramHeader32(FieldSet):
    TYPE_NAME = {
        3: "Dynamic library"
    }
    static_size = 32*8

    def createFields(self):
        yield Enum(UInt16(self, "type", "Type"), ProgramHeader32.TYPE_NAME)
        yield UInt16(self, "flags", "Flags")
        yield UInt32(self, "offset", "Offset")
        yield textHandler(UInt32(self, "vaddr", "V. address"), hexadecimal)
        yield textHandler(UInt32(self, "paddr", "P. address"), hexadecimal)
        yield UInt32(self, "file_size", "File size")
        yield UInt32(self, "mem_size", "Memory size")
        yield UInt32(self, "align", "Alignment")
        yield UInt32(self, "xxx", "???")

    def createDescription(self):
        return "Program Header (%s)" % self["type"].display

def sortSection(a, b):
    return int(a["offset"] - b["offset"])

#class Sections(FieldSet):
#    def createFields?(self, stream, parent, sections):
#        for section in sections:
#            ofs = section["offset"]
#            size = section["file_size"]
#            if size != 0:
#                sub = stream.createSub(ofs, size)
#                #yield DeflateFilter(self, "section[]", sub, size, Section,  "Section"))
#                chunk = self.doRead("section[]", "Section", (Section,), {"stream": sub})
#            else:
#                chunk = self.doRead("section[]", "Section", (FormatChunk, "string[0]"))
#            chunk.description = "ELF section (in file: %s..%s)" % (ofs, ofs+size)

class ElfFile(Parser):
    PARSER_TAGS = {
        "id": "elf",
        "category": "program",
        "file_ext": ("so", ""),
        "min_size": ElfHeader.static_size,  # At least one program header
        "mime": (
            u"application/x-executable",
            u"application/x-object",
            u"application/x-sharedlib",
            u"application/x-executable-file",
            u"application/x-coredump"),
        "magic": (("\x7FELF", 0),),
        "description": "ELF Unix/BSD program/library"
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        err = self["header"].isValid()
        if err:
            return err
        return True

    def createFields(self):
        # Choose the right endian depending on endian specified in header
        if self.stream.readBits(5*8, 8, BIG_ENDIAN) == ElfHeader.BIG_ENDIAN_ID:
            self.endian = BIG_ENDIAN
        else:
            self.endian = LITTLE_ENDIAN

        # Parse header and program headers
        yield ElfHeader(self, "header", "Header")
        for index in xrange(self["header/phnum"].value):
            yield ProgramHeader32(self, "prg_header[]")

        if False:
            raise ParserError("TODO: Parse sections...")
            #sections = self.array("prg_header")
            #size = self["header/shoff"].value - self.current_size//8
            #chunk = self.doRead("data", "Data", (DeflateFilter, stream, size, Sections, sections))
            #chunk.description = "Sections (use an evil hack to manage share same data on differents parts)"
            #assert self.current_size//8 == self["header/shoff"].value
        else:
            raw = self.seekByte(self["header/shoff"].value, "raw[]", relative=False)
            if raw:
                yield raw

        for index in xrange(self["header/shnum"].value):
            yield SectionHeader32(self, "section_header[]")

    def createDescription(self):
        return "ELF Unix/BSD program/library: %s" % (
            self["header/class"].display)


########NEW FILE########
__FILENAME__ = exe
"""
Microsoft Windows Portable Executable (PE) file parser.

Informations:
- Microsoft Portable Executable and Common Object File Format Specification:
  http://www.microsoft.com/whdc/system/platform/firmware/PECOFF.mspx

Author: Victor Stinner
Creation date: 2006-08-13
"""

from lib.hachoir_parser import HachoirParser
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.field import (FieldSet, RootSeekableFieldSet,
    UInt16, UInt32, String,
    RawBytes, PaddingBytes)
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_parser.program.exe_ne import NE_Header
from lib.hachoir_parser.program.exe_pe import PE_Header, PE_OptHeader, SectionHeader
from lib.hachoir_parser.program.exe_res import PE_Resource, NE_VersionInfoNode

MAX_NB_SECTION = 50

class MSDosHeader(FieldSet):
    static_size = 64*8

    def createFields(self):
        yield String(self, "header", 2, "File header (MZ)", charset="ASCII")
        yield UInt16(self, "size_mod_512", "File size in bytes modulo 512")
        yield UInt16(self, "size_div_512", "File size in bytes divide by 512")
        yield UInt16(self, "reloc_entries", "Number of relocation entries")
        yield UInt16(self, "code_offset", "Offset to the code in the file (divided by 16)")
        yield UInt16(self, "needed_memory", "Memory needed to run (divided by 16)")
        yield UInt16(self, "max_memory", "Maximum memory needed to run (divided by 16)")
        yield textHandler(UInt32(self, "init_ss_sp", "Initial value of SP:SS registers"), hexadecimal)
        yield UInt16(self, "checksum", "Checksum")
        yield textHandler(UInt32(self, "init_cs_ip", "Initial value of CS:IP registers"), hexadecimal)
        yield UInt16(self, "reloc_offset", "Offset in file to relocation table")
        yield UInt16(self, "overlay_number", "Overlay number")
        yield PaddingBytes(self, "reserved[]", 8, "Reserved")
        yield UInt16(self, "oem_id", "OEM id")
        yield UInt16(self, "oem_info", "OEM info")
        yield PaddingBytes(self, "reserved[]", 20, "Reserved")
        yield UInt32(self, "next_offset", "Offset to next header (PE or NE)")

    def isValid(self):
        if 512 <= self["size_mod_512"].value:
            return "Invalid field 'size_mod_512' value"
        if self["code_offset"].value < 4:
            return "Invalid code offset"
        looks_pe = self["size_div_512"].value < 4
        if looks_pe:
            if self["checksum"].value != 0:
                return "Invalid value of checksum"
            if not (80 <= self["next_offset"].value <= 1024):
                return "Invalid value of next_offset"
        return ""

class ExeFile(HachoirParser, RootSeekableFieldSet):
    PARSER_TAGS = {
        "id": "exe",
        "category": "program",
        "file_ext": ("exe", "dll", "ocx"),
        "mime": (u"application/x-dosexec",),
        "min_size": 64*8,
        #"magic": (("MZ", 0),),
        "magic_regex": (("MZ.[\0\1].{4}[^\0\1\2\3]", 0),),
        "description": "Microsoft Windows Portable Executable"
    }
    endian = LITTLE_ENDIAN

    def __init__(self, stream, **args):
        RootSeekableFieldSet.__init__(self, None, "root", stream, None, stream.askSize(self))
        HachoirParser.__init__(self, stream, **args)

    def validate(self):
        if self.stream.readBytes(0, 2) != 'MZ':
            return "Wrong header"
        err = self["msdos"].isValid()
        if err:
            return "Invalid MSDOS header: "+err
        if self.isPE():
            if MAX_NB_SECTION < self["pe_header/nb_section"].value:
                return "Invalid number of section (%s)" \
                    % self["pe_header/nb_section"].value
        return True

    def createFields(self):
        yield MSDosHeader(self, "msdos", "MS-DOS program header")

        if self.isPE() or self.isNE():
            offset = self["msdos/next_offset"].value
            self.seekByte(offset, relative=False)

        if self.isPE():
            for field in self.parsePortableExecutable():
                yield field
        elif self.isNE():
            for field in self.parseNE_Executable():
                yield field
        else:
            offset = self["msdos/code_offset"].value * 16
            self.seekByte(offset, relative=False)

    def parseNE_Executable(self):
        yield NE_Header(self, "ne_header")

        # FIXME: Compute resource offset instead of using searchBytes()
        # Ugly hack to get find version info structure
        start = self.current_size
        addr = self.stream.searchBytes('VS_VERSION_INFO', start)
        if addr:
            self.seekBit(addr-32)
            yield NE_VersionInfoNode(self, "info")

    def parsePortableExecutable(self):
        # Read PE header
        yield PE_Header(self, "pe_header")

        # Read PE optional header
        size = self["pe_header/opt_hdr_size"].value
        rsrc_rva = None
        if size:
            yield PE_OptHeader(self, "pe_opt_header", size=size*8)
            if "pe_opt_header/resource/rva" in self:
                rsrc_rva = self["pe_opt_header/resource/rva"].value

        # Read section headers
        sections = []
        for index in xrange(self["pe_header/nb_section"].value):
            section = SectionHeader(self, "section_hdr[]")
            yield section
            if section["phys_size"].value:
                sections.append(section)

        # Read sections
        sections.sort(key=lambda field: field["phys_off"].value)
        for section in sections:
            self.seekByte(section["phys_off"].value)
            size = section["phys_size"].value
            if size:
                name = section.createSectionName()
                if rsrc_rva is not None and section["rva"].value == rsrc_rva:
                    yield PE_Resource(self, name, section, size=size*8)
                else:
                    yield RawBytes(self, name, size)

    def isPE(self):
        if not hasattr(self, "_is_pe"):
            self._is_pe = False
            offset = self["msdos/next_offset"].value * 8
            if 2*8 <= offset \
            and (offset+PE_Header.static_size) <= self.size \
            and self.stream.readBytes(offset, 4) == 'PE\0\0':
                self._is_pe = True
        return self._is_pe

    def isNE(self):
        if not hasattr(self, "_is_ne"):
            self._is_ne = False
            offset = self["msdos/next_offset"].value * 8
            if 64*8 <= offset \
            and (offset+NE_Header.static_size) <= self.size \
            and self.stream.readBytes(offset, 2) == 'NE':
                self._is_ne = True
        return self._is_ne

    def getResource(self):
        # MS-DOS program: no resource
        if not self.isPE():
            return None

        # Check if PE has resource or not
        if "pe_opt_header/resource/size" in self:
            if not self["pe_opt_header/resource/size"].value:
                return None
        if "section_rsrc" in self:
            return self["section_rsrc"]
        return None

    def createDescription(self):
        if self.isPE():
            if self["pe_header/is_dll"].value:
                text = u"Microsoft Windows DLL"
            else:
                text = u"Microsoft Windows Portable Executable"
            info = [self["pe_header/cpu"].display]
            if "pe_opt_header" in self:
                hdr = self["pe_opt_header"]
                info.append(hdr["subsystem"].display)
            if self["pe_header/is_stripped"].value:
                info.append(u"stripped")
            return u"%s: %s" % (text, ", ".join(info))
        elif self.isNE():
            return u"New-style Executable (NE) for Microsoft MS Windows 3.x"
        else:
            return u"MS-DOS executable"

    def createContentSize(self):
        if self.isPE():
            size = 0
            for index in xrange(self["pe_header/nb_section"].value):
                section = self["section_hdr[%u]" % index]
                section_size = section["phys_size"].value
                if not section_size:
                    continue
                section_size = (section_size + section["phys_off"].value) * 8
                if size:
                    size = max(size, section_size)
                else:
                    size = section_size
            if size:
                return size
            else:
                return None
        elif self.isNE():
            # TODO: Guess NE size
            return None
        else:
            size = self["msdos/size_mod_512"].value + (self["msdos/size_div_512"].value-1) * 512
            if size < 0:
                return None
        return size*8


########NEW FILE########
__FILENAME__ = exe_ne
from lib.hachoir_core.field import (FieldSet,
    Bit, UInt8, UInt16, UInt32, Bytes,
    PaddingBits, PaddingBytes, NullBits, NullBytes)
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler

class NE_Header(FieldSet):
    static_size = 64*8
    def createFields(self):
        yield Bytes(self, "signature", 2, "New executable signature (NE)")
        yield UInt8(self, "link_ver", "Linker version number")
        yield UInt8(self, "link_rev", "Linker revision number")
        yield UInt16(self, "entry_table_ofst", "Offset to the entry table")
        yield UInt16(self, "entry_table_size", "Length (in bytes) of the entry table")
        yield PaddingBytes(self, "reserved[]", 4)

        yield Bit(self, "is_dll", "Is a dynamic-link library (DLL)?")
        yield Bit(self, "is_win_app", "Is a Windows application?")
        yield PaddingBits(self, "reserved[]", 9)
        yield Bit(self, "first_seg_code", "First segment contains code that loads the application?")
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "link_error", "Load even if linker detects errors?")
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "is_lib", "Is a library module?")

        yield UInt16(self, "auto_data_seg", "Automatic data segment number")
        yield filesizeHandler(UInt16(self, "local_heap_size", "Initial size (in bytes) of the local heap"))
        yield filesizeHandler(UInt16(self, "stack_size", "Initial size (in bytes) of the stack"))
        yield textHandler(UInt32(self, "cs_ip", "Value of CS:IP"), hexadecimal)
        yield textHandler(UInt32(self, "ss_sp", "Value of SS:SP"), hexadecimal)

        yield UInt16(self, "nb_entry_seg_tab", "Number of entries in the segment table")
        yield UInt16(self, "nb_entry_modref_tab", "Number of entries in the module-reference table")
        yield filesizeHandler(UInt16(self, "size_nonres_name_tab", "Number of bytes in the nonresident-name table"))
        yield UInt16(self, "seg_tab_ofs", "Segment table offset")
        yield UInt16(self, "rsrc_ofs", "Resource offset")

        yield UInt16(self, "res_name_tab_ofs", "Resident-name table offset")
        yield UInt16(self, "mod_ref_tab_ofs", "Module-reference table offset")
        yield UInt16(self, "import_tab_ofs", "Imported-name table offset")

        yield UInt32(self, "non_res_name_tab_ofs", "Nonresident-name table offset")
        yield UInt16(self, "nb_mov_ent_pt", "Number of movable entry points")
        yield UInt16(self, "log2_sector_size", "Log2 of the segment sector size")
        yield UInt16(self, "nb_rsrc_seg", "Number of resource segments")

        yield Bit(self, "unknown_os_format", "Operating system format is unknown")
        yield PaddingBits(self, "reserved[]", 1)
        yield Bit(self, "os_windows", "Operating system is Microsoft Windows")
        yield NullBits(self, "reserved[]", 6)
        yield Bit(self, "is_win20_prot", "Is Windows 2.x application running in version 3.x protected mode")
        yield Bit(self, "is_win20_font", "Is Windows 2.x application supporting proportional fonts")
        yield Bit(self, "fast_load", "Contains a fast-load area?")
        yield NullBits(self, "reserved[]", 4)

        yield UInt16(self, "fastload_ofs", "Fast-load area offset (in sector)")
        yield UInt16(self, "fastload_size", "Fast-load area length (in sector)")

        yield NullBytes(self, "reserved[]", 2)
        yield textHandler(UInt16(self, "win_version", "Expected Windows version number"), hexadecimal)


########NEW FILE########
__FILENAME__ = exe_pe
from lib.hachoir_core.field import (FieldSet, ParserError,
    Bit, UInt8, UInt16, UInt32, TimestampUnix32,
    Bytes, String, Enum,
    PaddingBytes, PaddingBits, NullBytes, NullBits)
from lib.hachoir_core.text_handler import textHandler, hexadecimal, filesizeHandler
from lib.hachoir_core.error import HACHOIR_ERRORS

class SectionHeader(FieldSet):
    static_size = 40 * 8
    def createFields(self):
        yield String(self, "name", 8, charset="ASCII", strip="\0 ")
        yield filesizeHandler(UInt32(self, "mem_size", "Size in memory"))
        yield textHandler(UInt32(self, "rva", "RVA (location) in memory"), hexadecimal)
        yield filesizeHandler(UInt32(self, "phys_size", "Physical size (on disk)"))
        yield filesizeHandler(UInt32(self, "phys_off", "Physical location (on disk)"))
        yield PaddingBytes(self, "reserved", 12)

        # 0x0000000#
        yield NullBits(self, "reserved[]", 4)
        # 0x000000#0
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "has_code", "Contains code")
        yield Bit(self, "has_init_data", "Contains initialized data")
        yield Bit(self, "has_uninit_data", "Contains uninitialized data")
        # 0x00000#00
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "has_comment", "Contains comments?")
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "remove", "Contents will not become part of image")
        # 0x0000#000
        yield Bit(self, "has_comdata", "Contains comdat?")
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "no_defer_spec_exc", "Reset speculative exceptions handling bits in the TLB entries")
        yield Bit(self, "gp_rel", "Content can be accessed relative to GP")
        # 0x000#0000
        yield NullBits(self, "reserved[]", 4)
        # 0x00#00000
        yield NullBits(self, "reserved[]", 4)
        # 0x0#000000
        yield Bit(self, "ext_reloc", "Contains extended relocations?")
        yield Bit(self, "discarded", "Can be discarded?")
        yield Bit(self, "is_not_cached", "Is not cachable?")
        yield Bit(self, "is_not_paged", "Is not pageable?")
        # 0x#0000000
        yield Bit(self, "is_shareable", "Is shareable?")
        yield Bit(self, "is_executable", "Is executable?")
        yield Bit(self, "is_readable", "Is readable?")
        yield Bit(self, "is_writable", "Is writable?")

    def rva2file(self, rva):
        return self["phys_off"].value + (rva - self["rva"].value)

    def createDescription(self):
        rva = self["rva"].value
        size = self["mem_size"].value
        info = [
            "rva=0x%08x..0x%08x" % (rva, rva+size),
            "size=%s" % self["mem_size"].display,
        ]
        if self["is_executable"].value:
            info.append("exec")
        if self["is_readable"].value:
            info.append("read")
        if self["is_writable"].value:
            info.append("write")
        return 'Section "%s": %s' % (self["name"].value, ", ".join(info))

    def createSectionName(self):
        try:
            name = str(self["name"].value.strip("."))
            if name:
                return "section_%s" % name
        except HACHOIR_ERRORS, err:
            self.warning(unicode(err))
            return "section[]"

class DataDirectory(FieldSet):
    def createFields(self):
        yield textHandler(UInt32(self, "rva", "Virtual address"), hexadecimal)
        yield filesizeHandler(UInt32(self, "size"))

    def createDescription(self):
        if self["size"].value:
            return "Directory at %s (%s)" % (
                self["rva"].display, self["size"].display)
        else:
            return "(empty directory)"

class PE_Header(FieldSet):
    static_size = 24*8
    cpu_name = {
        0x0184: u"Alpha AXP",
        0x01c0: u"ARM",
        0x014C: u"Intel 80386",
        0x014D: u"Intel 80486",
        0x014E: u"Intel Pentium",
        0x0200: u"Intel IA64",
        0x0268: u"Motorola 68000",
        0x0266: u"MIPS",
        0x0284: u"Alpha AXP 64 bits",
        0x0366: u"MIPS with FPU",
        0x0466: u"MIPS16 with FPU",
        0x01f0: u"PowerPC little endian",
        0x0162: u"R3000",
        0x0166: u"MIPS little endian (R4000)",
        0x0168: u"R10000",
        0x01a2: u"Hitachi SH3",
        0x01a6: u"Hitachi SH4",
        0x0160: u"R3000 (MIPS), big endian",
        0x0162: u"R3000 (MIPS), little endian",
        0x0166: u"R4000 (MIPS), little endian",
        0x0168: u"R10000 (MIPS), little endian",
        0x0184: u"DEC Alpha AXP",
        0x01F0: u"IBM Power PC, little endian",
    }

    def createFields(self):
        yield Bytes(self, "header", 4, r"PE header signature (PE\0\0)")
        if self["header"].value != "PE\0\0":
            raise ParserError("Invalid PE header signature")
        yield Enum(UInt16(self, "cpu", "CPU type"), self.cpu_name)
        yield UInt16(self, "nb_section", "Number of sections")
        yield TimestampUnix32(self, "creation_date", "Creation date")
        yield UInt32(self, "ptr_to_sym", "Pointer to symbol table")
        yield UInt32(self, "nb_symbols", "Number of symbols")
        yield UInt16(self, "opt_hdr_size", "Optional header size")

        yield Bit(self, "reloc_stripped", "If true, don't contain base relocations.")
        yield Bit(self, "exec_image", "Executable image?")
        yield Bit(self, "line_nb_stripped", "COFF line numbers stripped?")
        yield Bit(self, "local_sym_stripped", "COFF symbol table entries stripped?")
        yield Bit(self, "aggr_ws", "Aggressively trim working set")
        yield Bit(self, "large_addr", "Application can handle addresses greater than 2 GB")
        yield NullBits(self, "reserved", 1)
        yield Bit(self, "reverse_lo", "Little endian: LSB precedes MSB in memory")
        yield Bit(self, "32bit", "Machine based on 32-bit-word architecture")
        yield Bit(self, "is_stripped", "Debugging information removed?")
        yield Bit(self, "swap", "If image is on removable media, copy and run from swap file")
        yield PaddingBits(self, "reserved2", 1)
        yield Bit(self, "is_system", "It's a system file")
        yield Bit(self, "is_dll", "It's a dynamic-link library (DLL)")
        yield Bit(self, "up", "File should be run only on a UP machine")
        yield Bit(self, "reverse_hi", "Big endian: MSB precedes LSB in memory")

class PE_OptHeader(FieldSet):
    SUBSYSTEM_NAME = {
         1: u"Native",
         2: u"Windows GUI",
         3: u"Windows CUI",
         5: u"OS/2 CUI",
         7: u"POSIX CUI",
         8: u"Native Windows",
         9: u"Windows CE GUI",
        10: u"EFI application",
        11: u"EFI boot service driver",
        12: u"EFI runtime driver",
        13: u"EFI ROM",
        14: u"XBOX",
        16: u"Windows boot application",
    }
    DIRECTORY_NAME = {
         0: "export",
         1: "import",
         2: "resource",
         3: "exception",
         4: "certificate",
         5: "relocation",
         6: "debug",
         7: "description",
         8: "global_ptr",
         9: "tls",   # Thread local storage
        10: "load_config",
        11: "bound_import",
        12: "import_address",
    }
    def createFields(self):
        yield UInt16(self, "signature", "PE optional header signature (0x010b)")
        # TODO: Support PE32+ (signature=0x020b)
        if self["signature"].value != 0x010b:
            raise ParserError("Invalid PE optional header signature")
        yield UInt8(self, "maj_lnk_ver", "Major linker version")
        yield UInt8(self, "min_lnk_ver", "Minor linker version")
        yield filesizeHandler(UInt32(self, "size_code", "Size of code"))
        yield filesizeHandler(UInt32(self, "size_init_data", "Size of initialized data"))
        yield filesizeHandler(UInt32(self, "size_uninit_data", "Size of uninitialized data"))
        yield textHandler(UInt32(self, "entry_point", "Address (RVA) of the code entry point"), hexadecimal)
        yield textHandler(UInt32(self, "base_code", "Base (RVA) of code"), hexadecimal)
        yield textHandler(UInt32(self, "base_data", "Base (RVA) of data"), hexadecimal)
        yield textHandler(UInt32(self, "image_base", "Image base (RVA)"), hexadecimal)
        yield filesizeHandler(UInt32(self, "sect_align", "Section alignment"))
        yield filesizeHandler(UInt32(self, "file_align", "File alignment"))
        yield UInt16(self, "maj_os_ver", "Major OS version")
        yield UInt16(self, "min_os_ver", "Minor OS version")
        yield UInt16(self, "maj_img_ver", "Major image version")
        yield UInt16(self, "min_img_ver", "Minor image version")
        yield UInt16(self, "maj_subsys_ver", "Major subsystem version")
        yield UInt16(self, "min_subsys_ver", "Minor subsystem version")
        yield NullBytes(self, "reserved", 4)
        yield filesizeHandler(UInt32(self, "size_img", "Size of image"))
        yield filesizeHandler(UInt32(self, "size_hdr", "Size of headers"))
        yield textHandler(UInt32(self, "checksum"), hexadecimal)
        yield Enum(UInt16(self, "subsystem"), self.SUBSYSTEM_NAME)
        yield UInt16(self, "dll_flags")
        yield filesizeHandler(UInt32(self, "size_stack_reserve"))
        yield filesizeHandler(UInt32(self, "size_stack_commit"))
        yield filesizeHandler(UInt32(self, "size_heap_reserve"))
        yield filesizeHandler(UInt32(self, "size_heap_commit"))
        yield UInt32(self, "loader_flags")
        yield UInt32(self, "nb_directory", "Number of RVA and sizes")
        for index in xrange(self["nb_directory"].value):
            try:
                name = self.DIRECTORY_NAME[index]
            except KeyError:
                name = "data_dir[%u]" % index
            yield DataDirectory(self, name)

    def createDescription(self):
        return "PE optional header: %s, entry point %s" % (
            self["subsystem"].display,
            self["entry_point"].display)


########NEW FILE########
__FILENAME__ = exe_res
"""
Parser for resource of Microsoft Windows Portable Executable (PE).

Documentation:
- Wine project
  VS_FIXEDFILEINFO structure, file include/winver.h

Author: Victor Stinner
Creation date: 2007-01-19
"""

from lib.hachoir_core.field import (FieldSet, ParserError, Enum,
    Bit, Bits, SeekableFieldSet,
    UInt16, UInt32, TimestampUnix32,
    RawBytes, PaddingBytes, NullBytes, NullBits,
    CString, String)
from lib.hachoir_core.text_handler import textHandler, filesizeHandler, hexadecimal
from lib.hachoir_core.tools import createDict, paddingSize, alignValue, makePrintable
from lib.hachoir_core.error import HACHOIR_ERRORS
from lib.hachoir_parser.common.win32 import BitmapInfoHeader

MAX_DEPTH = 5
MAX_INDEX_PER_HEADER = 300
MAX_NAME_PER_HEADER = MAX_INDEX_PER_HEADER

class Version(FieldSet):
    static_size = 32
    def createFields(self):
        yield textHandler(UInt16(self, "minor", "Minor version number"), hexadecimal)
        yield textHandler(UInt16(self, "major", "Major version number"), hexadecimal)
    def createValue(self):
        return self["major"].value + float(self["minor"].value) / 10000

MAJOR_OS_NAME = {
    1: "DOS",
    2: "OS/2 16-bit",
    3: "OS/2 32-bit",
    4: "Windows NT",
}

MINOR_OS_BASE = 0
MINOR_OS_NAME = {
    0: "Base",
    1: "Windows 16-bit",
    2: "Presentation Manager 16-bit",
    3: "Presentation Manager 32-bit",
    4: "Windows 32-bit",
}

FILETYPE_DRIVER = 3
FILETYPE_FONT = 4
FILETYPE_NAME = {
    1: "Application",
    2: "DLL",
    3: "Driver",
    4: "Font",
    5: "VXD",
    7: "Static library",
}

DRIVER_SUBTYPE_NAME = {
     1: "Printer",
     2: "Keyboard",
     3: "Language",
     4: "Display",
     5: "Mouse",
     6: "Network",
     7: "System",
     8: "Installable",
     9: "Sound",
    10: "Communications",
}

FONT_SUBTYPE_NAME = {
    1: "Raster",
    2: "Vector",
    3: "TrueType",
}

class VersionInfoBinary(FieldSet):
    def createFields(self):
        yield textHandler(UInt32(self, "magic", "File information magic (0xFEEF04BD)"), hexadecimal)
        if self["magic"].value != 0xFEEF04BD:
            raise ParserError("EXE resource: invalid file info magic")
        yield Version(self, "struct_ver", "Structure version (1.0)")
        yield Version(self, "file_ver_ms", "File version MS")
        yield Version(self, "file_ver_ls", "File version LS")
        yield Version(self, "product_ver_ms", "Product version MS")
        yield Version(self, "product_ver_ls", "Product version LS")
        yield textHandler(UInt32(self, "file_flags_mask"), hexadecimal)

        yield Bit(self, "debug")
        yield Bit(self, "prerelease")
        yield Bit(self, "patched")
        yield Bit(self, "private_build")
        yield Bit(self, "info_inferred")
        yield Bit(self, "special_build")
        yield NullBits(self, "reserved", 26)

        yield Enum(textHandler(UInt16(self, "file_os_major"), hexadecimal), MAJOR_OS_NAME)
        yield Enum(textHandler(UInt16(self, "file_os_minor"), hexadecimal), MINOR_OS_NAME)
        yield Enum(textHandler(UInt32(self, "file_type"), hexadecimal), FILETYPE_NAME)
        field = textHandler(UInt32(self, "file_subfile"), hexadecimal)
        if field.value == FILETYPE_DRIVER:
            field = Enum(field, DRIVER_SUBTYPE_NAME)
        elif field.value == FILETYPE_FONT:
            field = Enum(field, FONT_SUBTYPE_NAME)
        yield field
        yield TimestampUnix32(self, "date_ms")
        yield TimestampUnix32(self, "date_ls")

class VersionInfoNode(FieldSet):
    TYPE_STRING = 1
    TYPE_NAME = {
        0: "binary",
        1: "string",
    }

    def __init__(self, parent, name, is_32bit=True):
        FieldSet.__init__(self, parent, name)
        self._size = alignValue(self["size"].value, 4) * 8
        self.is_32bit = is_32bit

    def createFields(self):
        yield UInt16(self, "size", "Node size (in bytes)")
        yield UInt16(self, "data_size")
        yield Enum(UInt16(self, "type"), self.TYPE_NAME)
        yield CString(self, "name", charset="UTF-16-LE")

        size = paddingSize(self.current_size//8, 4)
        if size:
            yield NullBytes(self, "padding[]", size)
        size = self["data_size"].value
        if size:
            if self["type"].value == self.TYPE_STRING:
                if self.is_32bit:
                    size *= 2
                yield String(self, "value", size, charset="UTF-16-LE", truncate="\0")
            elif self["name"].value == "VS_VERSION_INFO":
                yield VersionInfoBinary(self, "value", size=size*8)
                if self["value/file_flags_mask"].value == 0:
                    self.is_32bit = False
            else:
                yield RawBytes(self, "value", size)
        while 12 <= (self.size - self.current_size) // 8:
            yield VersionInfoNode(self, "node[]", self.is_32bit)
        size = (self.size - self.current_size) // 8
        if size:
            yield NullBytes(self, "padding[]", size)


    def createDescription(self):
        text = "Version info node: %s" % self["name"].value
        if self["type"].value == self.TYPE_STRING and "value" in self:
            text += "=%s" % self["value"].value
        return text

def parseVersionInfo(parent):
    yield VersionInfoNode(parent, "node[]")

def parseIcon(parent):
    yield BitmapInfoHeader(parent, "bmp_header")
    size = (parent.size - parent.current_size) // 8
    if size:
        yield RawBytes(parent, "raw", size)

class WindowsString(FieldSet):
    def createFields(self):
        yield UInt16(self, "length", "Number of 16-bit characters")
        size = self["length"].value * 2
        if size:
            yield String(self, "text", size, charset="UTF-16-LE")

    def createValue(self):
        if "text" in self:
            return self["text"].value
        else:
            return u""

    def createDisplay(self):
        return makePrintable(self.value, "UTF-8", to_unicode=True, quote='"')

def parseStringTable(parent):
    while not parent.eof:
        yield WindowsString(parent, "string[]")

RESOURCE_TYPE = {
    1: ("cursor[]", "Cursor", None),
    2: ("bitmap[]", "Bitmap", None),
    3: ("icon[]", "Icon", parseIcon),
    4: ("menu[]", "Menu", None),
    5: ("dialog[]", "Dialog", None),
    6: ("string_table[]", "String table", parseStringTable),
    7: ("font_dir[]", "Font directory", None),
    8: ("font[]", "Font", None),
    9: ("accelerators[]", "Accelerators", None),
    10: ("raw_res[]", "Unformatted resource data", None),
    11: ("message_table[]", "Message table", None),
    12: ("group_cursor[]", "Group cursor", None),
    14: ("group_icon[]", "Group icon", None),
    16: ("version_info", "Version information", parseVersionInfo),
}

class Entry(FieldSet):
    static_size = 16*8

    def __init__(self, parent, name, inode=None):
        FieldSet.__init__(self, parent, name)
        self.inode = inode

    def createFields(self):
        yield textHandler(UInt32(self, "rva"), hexadecimal)
        yield filesizeHandler(UInt32(self, "size"))
        yield UInt32(self, "codepage")
        yield NullBytes(self, "reserved", 4)

    def createDescription(self):
        return "Entry #%u: offset=%s size=%s" % (
            self.inode["offset"].value, self["rva"].display, self["size"].display)

class NameOffset(FieldSet):
    def createFields(self):
        yield UInt32(self, "name")
        yield Bits(self, "offset", 31)
        yield Bit(self, "is_name")

class IndexOffset(FieldSet):
    TYPE_DESC = createDict(RESOURCE_TYPE, 1)

    def __init__(self, parent, name, res_type=None):
        FieldSet.__init__(self, parent, name)
        self.res_type = res_type

    def createFields(self):
        yield Enum(UInt32(self, "type"), self.TYPE_DESC)
        yield Bits(self, "offset", 31)
        yield Bit(self, "is_subdir")

    def createDescription(self):
        if self["is_subdir"].value:
            return "Sub-directory: %s at %s" % (self["type"].display, self["offset"].value)
        else:
            return "Index: ID %s at %s" % (self["type"].display, self["offset"].value)

class ResourceContent(FieldSet):
    def __init__(self, parent, name, entry, size=None):
        FieldSet.__init__(self, parent, name, size=entry["size"].value*8)
        self.entry = entry
        res_type = self.getResType()
        if res_type in RESOURCE_TYPE:
            self._name, description, self._parser = RESOURCE_TYPE[res_type]
        else:
            self._parser = None

    def getResID(self):
        return self.entry.inode["offset"].value

    def getResType(self):
        return self.entry.inode.res_type

    def createFields(self):
        if self._parser:
            for field in self._parser(self):
                yield field
        else:
            yield RawBytes(self, "content", self.size//8)

    def createDescription(self):
        return "Resource #%u content: type=%s" % (
            self.getResID(), self.getResType())

class Header(FieldSet):
    static_size = 16*8
    def createFields(self):
        yield NullBytes(self, "options", 4)
        yield TimestampUnix32(self, "creation_date")
        yield UInt16(self, "maj_ver", "Major version")
        yield UInt16(self, "min_ver", "Minor version")
        yield UInt16(self, "nb_name", "Number of named entries")
        yield UInt16(self, "nb_index", "Number of indexed entries")

    def createDescription(self):
        text = "Resource header"
        info = []
        if self["nb_name"].value:
            info.append("%u name" % self["nb_name"].value)
        if self["nb_index"].value:
            info.append("%u index" % self["nb_index"].value)
        if self["creation_date"].value:
            info.append(self["creation_date"].display)
        if info:
            return "%s: %s" % (text, ", ".join(info))
        else:
            return text

class Name(FieldSet):
    def createFields(self):
        yield UInt16(self, "length")
        size = min(self["length"].value, 255)
        if size:
            yield String(self, "name", size, charset="UTF-16LE")

class Directory(FieldSet):
    def __init__(self, parent, name, res_type=None):
        FieldSet.__init__(self, parent, name)
        nb_entries = self["header/nb_name"].value + self["header/nb_index"].value
        self._size = Header.static_size + nb_entries * 64
        self.res_type = res_type

    def createFields(self):
        yield Header(self, "header")

        if MAX_NAME_PER_HEADER < self["header/nb_name"].value:
            raise ParserError("EXE resource: invalid number of name (%s)"
                % self["header/nb_name"].value)
        if MAX_INDEX_PER_HEADER < self["header/nb_index"].value:
            raise ParserError("EXE resource: invalid number of index (%s)"
                % self["header/nb_index"].value)

        hdr = self["header"]
        for index in xrange(hdr["nb_name"].value):
            yield NameOffset(self, "name[]")
        for index in xrange(hdr["nb_index"].value):
            yield IndexOffset(self, "index[]", self.res_type)

    def createDescription(self):
        return self["header"].description

class PE_Resource(SeekableFieldSet):
    def __init__(self, parent, name, section, size):
        SeekableFieldSet.__init__(self, parent, name, size=size)
        self.section = section

    def parseSub(self, directory, name, depth):
        indexes = []
        for index in directory.array("index"):
            if index["is_subdir"].value:
                indexes.append(index)

        #indexes.sort(key=lambda index: index["offset"].value)
        for index in indexes:
            self.seekByte(index["offset"].value)
            if depth == 1:
                res_type = index["type"].value
            else:
                res_type = directory.res_type
            yield Directory(self, name, res_type)

    def createFields(self):
        # Parse directories
        depth = 0
        subdir = Directory(self, "root")
        yield subdir
        subdirs = [subdir]
        alldirs = [subdir]
        while subdirs:
            depth += 1
            if MAX_DEPTH < depth:
                self.error("EXE resource: depth too high (%s), stop parsing directories" % depth)
                break
            newsubdirs = []
            for index, subdir in enumerate(subdirs):
                name = "directory[%u][%u][]" % (depth, index)
                try:
                    for field in self.parseSub(subdir, name, depth):
                        if field.__class__ == Directory:
                            newsubdirs.append(field)
                        yield field
                except HACHOIR_ERRORS, err:
                    self.error("Unable to create directory %s: %s" % (name, err))
            subdirs = newsubdirs
            alldirs.extend(subdirs)

        # Create resource list
        resources = []
        for directory in alldirs:
            for index in directory.array("index"):
                if not index["is_subdir"].value:
                    resources.append(index)

        # Parse entries
        entries = []
        for resource in resources:
            offset = resource["offset"].value
            if offset is None:
                continue
            self.seekByte(offset)
            entry = Entry(self, "entry[]", inode=resource)
            yield entry
            entries.append(entry)
        entries.sort(key=lambda entry: entry["rva"].value)

        # Parse resource content
        for entry in entries:
            try:
                offset = self.section.rva2file(entry["rva"].value)
                padding = self.seekByte(offset, relative=False)
                if padding:
                    yield padding
                yield ResourceContent(self, "content[]", entry)
            except HACHOIR_ERRORS, err:
                self.warning("Error when parsing entry %s: %s" % (entry.path, err))

        size = (self.size - self.current_size) // 8
        if size:
            yield PaddingBytes(self, "padding_end", size)

class NE_VersionInfoNode(FieldSet):
    TYPE_STRING = 1
    TYPE_NAME = {
        0: "binary",
        1: "string",
    }

    def __init__(self, parent, name):
        FieldSet.__init__(self, parent, name)
        self._size = alignValue(self["size"].value, 4) * 8

    def createFields(self):
        yield UInt16(self, "size", "Node size (in bytes)")
        yield UInt16(self, "data_size")
        yield CString(self, "name", charset="ISO-8859-1")

        size = paddingSize(self.current_size//8, 4)
        if size:
            yield NullBytes(self, "padding[]", size)
        size = self["data_size"].value
        if size:
            if self["name"].value == "VS_VERSION_INFO":
                yield VersionInfoBinary(self, "value", size=size*8)
            else:
                yield String(self, "value", size, charset="ISO-8859-1")
        while 12 <= (self.size - self.current_size) // 8:
            yield NE_VersionInfoNode(self, "node[]")
        size = (self.size - self.current_size) // 8
        if size:
            yield NullBytes(self, "padding[]", size)


    def createDescription(self):
        text = "Version info node: %s" % self["name"].value
#        if self["type"].value == self.TYPE_STRING and "value" in self:
#            text += "=%s" % self["value"].value
        return text


########NEW FILE########
__FILENAME__ = java
"""
Compiled Java classes parser.

Author: Thomas de Grenier de Latour (TGL) <degrenier@easyconnect.fr>
Creation: 2006/11/01
Last-update: 2006/11/06

Introduction:
 * This parser is for compiled Java classes, aka .class files.  What is nice
   with this format is that it is well documented in the official Java VM specs.
 * Some fields, and most field sets, have dynamic sizes, and there is no offset
   to directly jump from an header to a given section, or anything like that.
   It means that accessing a field at the end of the file requires that you've
   already parsed almost the whole file.  That's not very efficient, but it's
   okay given the usual size of .class files (usually a few KB).
 * Most fields are just indexes of some "constant pool" entries, which holds
   most constant datas of the class.  And constant pool entries reference other
   constant pool entries, etc.  Hence, a raw display of this fields only shows
   integers and is not really understandable.  Because of that, this parser
   comes with two important custom field classes:
    - CPInfo are constant pool entries.  They have a type ("Utf8", "Methodref",
      etc.), and some contents fields depending on this type.  They also have a
      "__str__()" method, which returns a syntetic view of this contents.
    - CPIndex are constant pool indexes (UInt16).  It is possible to specify
      what type of CPInfo they are allowed to points to.  They also have a
      custom display method, usually printing something like "->  foo", where
      foo is the str() of their target CPInfo.

References:
 * The Java Virtual Machine Specification, 2nd edition, chapter 4, in HTML:
   http://java.sun.com/docs/books/vmspec/2nd-edition/html/ClassFile.doc.html
    => That's the spec i've been implementing so far. I think it is format
       version 46.0 (JDK 1.2).
 * The Java Virtual Machine Specification, 2nd edition, chapter 4, in PDF:
   http://java.sun.com/docs/books/vmspec/2nd-edition/ClassFileFormat.pdf
    => don't trust the URL, this PDF version is more recent than the HTML one.
       It highligths some recent additions to the format (i don't know the
       exact version though), which are not yet implemented in this parser.
 * The Java Virtual Machine Specification, chapter 4:
   http://java.sun.com/docs/books/vmspec/html/ClassFile.doc.html
    => describes an older format, probably version 45.3 (JDK 1.1).

TODO/FIXME:
 * Google for some existing free .class files parsers, to get more infos on
   the various formats differences, etc.
 * Write/compile some good tests cases.
 * Rework pretty-printing of CPIndex fields.  This str() thing sinks.
 * Add support of formats other than 46.0 (45.3 seems to already be ok, but
   there are things to add for later formats).
 * Make parsing robust: currently, the parser will die on asserts as soon as
   something seems wrong.  It should rather be tolerant, print errors/warnings,
   and try its best to continue.  Check how error-handling is done in other
   parsers.
 * Gettextize the whole thing.
 * Check whether Float32/64 are really the same as Java floats/double. PEP-0754
   says that handling of +/-infinity and NaN is very implementation-dependent.
   Also check how this values are displayed.
 * Make the parser edition-proof.  For instance, editing a constant-pool string
   should update the length field of it's entry, etc.  Sounds like a huge work.
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (
        ParserError, FieldSet, StaticFieldSet,
        Enum, RawBytes, PascalString16, Float32, Float64,
        Int8, UInt8, Int16, UInt16, Int32, UInt32, Int64,
        Bit, NullBits )
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.tools import paddingSize

###############################################################################
def parse_flags(flags, flags_dict, show_unknown_flags=True, separator=" "):
    """
    Parses an integer representing a set of flags.  The known flags are
    stored with their bit-mask in a dictionnary.  Returns a string.
    """
    flags_list = []
    mask = 0x01
    while mask <= flags:
        if flags & mask:
            if mask in flags_dict:
                flags_list.append(flags_dict[mask])
            elif show_unknown_flags:
                flags_list.append("???")
        mask = mask << 1
    return separator.join(flags_list)


###############################################################################
code_to_type_name = {
    'B': "byte",
    'C': "char",
    'D': "double",
    'F': "float",
    'I': "int",
    'J': "long",
    'S': "short",
    'Z': "boolean",
    'V': "void",
}

def eat_descriptor(descr):
    """
    Read head of a field/method descriptor.  Returns a pair of strings, where
    the first one is a human-readable string representation of the first found
    type, and the second one is the tail of the parameter.
    """
    array_dim = 0
    while descr[0] == '[':
        array_dim += 1
        descr = descr[1:]
    if (descr[0] == 'L'):
        try: end = descr.find(';')
        except: raise ParserError("Not a valid descriptor string: " + descr)
        type = descr[1:end]
        descr = descr[end:]
    else:
        global code_to_type_name
        try:
            type = code_to_type_name[descr[0]]
        except KeyError:
            raise ParserError("Not a valid descriptor string: %s" % descr)
    return (type.replace("/", ".") + array_dim * "[]", descr[1:])

def parse_field_descriptor(descr, name=None):
    """
    Parse a field descriptor (single type), and returns it as human-readable
    string representation.
    """
    assert descr
    (type, tail) = eat_descriptor(descr)
    assert not tail
    if name:
        return type + " " + name
    else:
        return type

def parse_method_descriptor(descr, name=None):
    """
    Parse a method descriptor (params type and return type), and returns it
    as human-readable string representation.
    """
    assert descr and (descr[0] == '(')
    descr = descr[1:]
    params_list = []
    while descr[0] != ')':
        (param, descr) = eat_descriptor(descr)
        params_list.append(param)
    (type, tail) = eat_descriptor(descr[1:])
    assert not tail
    params = ", ".join(params_list)
    if name:
        return "%s %s(%s)" % (type, name, params)
    else:
        return "%s (%s)" % (type, params)

def parse_any_descriptor(descr, name=None):
    """
    Parse either a field or method descriptor, and returns it as human-
    readable string representation.
    """
    assert descr
    if descr[0] == '(':
        return parse_method_descriptor(descr, name)
    else:
        return parse_field_descriptor(descr, name)


###############################################################################
class FieldArray(FieldSet):
    """
    Holds a fixed length array of fields which all have the same type.  This
    type may be variable-length.  Each field will be named "foo[x]" (with x
    starting at 0).
    """
    def __init__(self, parent, name, elements_class, length,
            **elements_extra_args):
        """Create a FieldArray of <length> fields of class <elements_class>,
        named "<name>[x]".  The **elements_extra_args will be passed to the
        constructor of each field when yielded."""
        FieldSet.__init__(self, parent, name)
        self.array_elements_class = elements_class
        self.array_length = length
        self.array_elements_extra_args = elements_extra_args

    def createFields(self):
        for i in range(0, self.array_length):
            yield self.array_elements_class(self, "%s[%d]" % (self.name, i),
                    **self.array_elements_extra_args)

class ConstantPool(FieldSet):
    """
    ConstantPool is similar to a FieldArray of CPInfo fields, but:
    - numbering starts at 1 instead of zero
    - some indexes are skipped (after Long or Double entries)
    """
    def __init__(self, parent, name, length):
        FieldSet.__init__(self, parent, name)
        self.constant_pool_length = length
    def createFields(self):
        i = 1
        while i < self.constant_pool_length:
            name = "%s[%d]" % (self.name, i)
            yield CPInfo(self, name)
            i += 1
            if self[name].constant_type in ("Long", "Double"):
                i += 1


###############################################################################
class CPIndex(UInt16):
    """
    Holds index of a constant pool entry.
    """
    def __init__(self, parent, name, description=None, target_types=None,
                target_text_handler=(lambda x: x), allow_zero=False):
        """
        Initialize a CPIndex.
        - target_type is the tuple of expected type for the target CPInfo
          (if None, then there will be no type check)
        - target_text_handler is a string transformation function used for
          pretty printing the target str() result
        - allow_zero states whether null index is allowed (sometimes, constant
          pool index is optionnal)
        """
        UInt16.__init__(self, parent, name, description)
        if isinstance(target_types, str):
            self.target_types = (target_types,)
        else:
            self.target_types = target_types
        self.allow_zero = allow_zero
        self.target_text_handler = target_text_handler
        self.getOriginalDisplay = lambda: self.value

    def createDisplay(self):
        cp_entry = self.get_cp_entry()
        if self.allow_zero and not cp_entry:
            return "ZERO"
        assert cp_entry
        return "-> " + self.target_text_handler(str(cp_entry))

    def get_cp_entry(self):
        """
        Returns the target CPInfo field.
        """
        assert self.value < self["/constant_pool_count"].value
        if self.allow_zero and not self.value: return None
        cp_entry = self["/constant_pool/constant_pool[%d]" % self.value]
        assert isinstance(cp_entry, CPInfo)
        if self.target_types:
            assert cp_entry.constant_type in self.target_types
        return cp_entry


###############################################################################
class JavaOpcode(FieldSet):
    OPSIZE = 0
    def __init__(self, parent, name, op, desc):
        FieldSet.__init__(self, parent, name)
        if self.OPSIZE != 0: self._size = self.OPSIZE*8
        self.op = op
        self.desc = desc
    def createDisplay(self):
        return self.op
    def createDescription(self):
        return self.desc
    def createValue(self):
        return self.createDisplay()

class OpcodeNoArgs(JavaOpcode):
    OPSIZE = 1
    def createFields(self):
        yield UInt8(self, "opcode")

class OpcodeCPIndex(JavaOpcode):
    OPSIZE = 3
    def createFields(self):
        yield UInt8(self, "opcode")
        yield CPIndex(self, "index")
    def createDisplay(self):
        return "%s(%i)"%(self.op, self["index"].value)
        
class OpcodeCPIndexShort(JavaOpcode):
    OPSIZE = 2
    def createFields(self):
        yield UInt8(self, "opcode")
        yield UInt8(self, "index")
    def createDisplay(self):
        return "%s(%i)"%(self.op, self["index"].value)

class OpcodeIndex(JavaOpcode):
    OPSIZE = 2
    def createFields(self):
        yield UInt8(self, "opcode")
        yield UInt8(self, "index")
    def createDisplay(self):
        return "%s(%i)"%(self.op, self["index"].value)

class OpcodeShortJump(JavaOpcode):
    OPSIZE = 3
    def createFields(self):
        yield UInt8(self, "opcode")
        yield Int16(self, "offset")
    def createDisplay(self):
        return "%s(%s)"%(self.op, self["offset"].value)

class OpcodeLongJump(JavaOpcode):
    OPSIZE = 5
    def createFields(self):
        yield UInt8(self, "opcode")
        yield Int32(self, "offset")
    def createDisplay(self):
        return "%s(%s)"%(self.op, self["offset"].value)

class OpcodeSpecial_bipush(JavaOpcode):
    OPSIZE = 2
    def createFields(self):
        yield UInt8(self, "opcode")
        yield Int8(self, "value")
    def createDisplay(self):
        return "%s(%s)"%(self.op, self["value"].value)

class OpcodeSpecial_sipush(JavaOpcode):
    OPSIZE = 3
    def createFields(self):
        yield UInt8(self, "opcode")
        yield Int16(self, "value")
    def createDisplay(self):
        return "%s(%s)"%(self.op, self["value"].value)

class OpcodeSpecial_iinc(JavaOpcode):
    OPSIZE = 3
    def createFields(self):
        yield UInt8(self, "opcode")
        yield UInt8(self, "index")
        yield Int8(self, "value")
    def createDisplay(self):
        return "%s(%i,%i)"%(self.op, self["index"].value, self["value"].value)

class OpcodeSpecial_wide(JavaOpcode):
    def createFields(self):
        yield UInt8(self, "opcode")
        new_op = UInt8(self, "new_opcode")
        yield new_op
        op = new_op._description = JavaBytecode.OPCODE_TABLE.get(new_op.value, ["reserved", None, "Reserved"])[0]
        yield UInt16(self, "index")
        if op == "iinc":
            yield Int16(self, "value")
            self.createDisplay = lambda self: "%s(%i,%i)"%(self.op, self["index"].value, self["value"].value)
        else:
            self.createDisplay = lambda self: "%s(%i)"%(self.op, self["index"].value)

class OpcodeSpecial_invokeinterface(JavaOpcode):
    OPSIZE = 5
    def createFields(self):
        yield UInt8(self, "opcode")
        yield CPIndex(self, "index")
        yield UInt8(self, "count")
        yield UInt8(self, "zero", "Must be zero.")
    def createDisplay(self):
        return "%s(%i,%i,%i)"%(self.op, self["index"].value, self["count"].value, self["zero"].value)

class OpcodeSpecial_newarray(JavaOpcode):
    OPSIZE = 2
    def createFields(self):
        yield UInt8(self, "opcode")
        yield Enum(UInt8(self, "atype"), {4: "boolean",
                                           5: "char",
                                           6: "float",
                                           7: "double",
                                           8: "byte",
                                           9: "short",
                                           10:"int",
                                           11:"long"})
    def createDisplay(self):
        return "%s(%s)"%(self.op, self["atype"].createDisplay())

class OpcodeSpecial_multianewarray(JavaOpcode):
    OPSIZE = 4
    def createFields(self):
        yield UInt8(self, "opcode")
        yield CPIndex(self, "index")
        yield UInt8(self, "dimensions")
    def createDisplay(self):
        return "%s(%i,%i)"%(self.op, self["index"].value, self["dimensions"].value)

class OpcodeSpecial_tableswitch(JavaOpcode):
    def createFields(self):
        yield UInt8(self, "opcode")
        pad = paddingSize(self.address+8, 32)
        if pad:
            yield NullBits(self, "padding", pad)
        yield Int32(self, "default")
        low = Int32(self, "low")
        yield low
        high = Int32(self, "high")
        yield high
        for i in range(high.value-low.value+1):
            yield Int32(self, "offset[]")
    def createDisplay(self):
        return "%s(%i,%i,%i,...)"%(self.op, self["default"].value, self["low"].value, self["high"].value)

class OpcodeSpecial_lookupswitch(JavaOpcode):
    def createFields(self):
        yield UInt8(self, "opcode")
        pad = paddingSize(self.address+8, 32)
        if pad:
            yield NullBits(self, "padding", pad)
        yield Int32(self, "default")
        n = Int32(self, "npairs")
        yield n
        for i in range(n.value):
            yield Int32(self, "match[]")
            yield Int32(self, "offset[]")
    def createDisplay(self):
        return "%s(%i,%i,...)"%(self.op, self["default"].value, self["npairs"].value)

class JavaBytecode(FieldSet):
    OPCODE_TABLE = {
0x00: ("nop", OpcodeNoArgs, "performs no operation. Stack: [No change]"),
0x01: ("aconst_null", OpcodeNoArgs, "pushes a 'null' reference onto the stack. Stack: -> null"),
0x02: ("iconst_m1", OpcodeNoArgs, "loads the int value -1 onto the stack. Stack: -> -1"),
0x03: ("iconst_0", OpcodeNoArgs, "loads the int value 0 onto the stack. Stack: -> 0"),
0x04: ("iconst_1", OpcodeNoArgs, "loads the int value 1 onto the stack. Stack: -> 1"),
0x05: ("iconst_2", OpcodeNoArgs, "loads the int value 2 onto the stack. Stack: -> 2"),
0x06: ("iconst_3", OpcodeNoArgs, "loads the int value 3 onto the stack. Stack: -> 3"),
0x07: ("iconst_4", OpcodeNoArgs, "loads the int value 4 onto the stack. Stack: -> 4"),
0x08: ("iconst_5", OpcodeNoArgs, "loads the int value 5 onto the stack. Stack: -> 5"),
0x09: ("lconst_0", OpcodeNoArgs, "pushes the long 0 onto the stack. Stack: -> 0L"),
0x0a: ("lconst_1", OpcodeNoArgs, "pushes the long 1 onto the stack. Stack: -> 1L"),
0x0b: ("fconst_0", OpcodeNoArgs, "pushes '0.0f' onto the stack. Stack: -> 0.0f"),
0x0c: ("fconst_1", OpcodeNoArgs, "pushes '1.0f' onto the stack. Stack: -> 1.0f"),
0x0d: ("fconst_2", OpcodeNoArgs, "pushes '2.0f' onto the stack. Stack: -> 2.0f"),
0x0e: ("dconst_0", OpcodeNoArgs, "pushes the constant '0.0' onto the stack. Stack: -> 0.0"),
0x0f: ("dconst_1", OpcodeNoArgs, "pushes the constant '1.0' onto the stack. Stack: -> 1.0"),
0x10: ("bipush", OpcodeSpecial_bipush, "pushes the signed 8-bit integer argument onto the stack. Stack: -> value"),
0x11: ("sipush", OpcodeSpecial_sipush, "pushes the signed 16-bit integer argument onto the stack. Stack: -> value"),
0x12: ("ldc", OpcodeCPIndexShort, "pushes a constant from a constant pool (String, int, float or class type) onto the stack. Stack: -> value"),
0x13: ("ldc_w", OpcodeCPIndex, "pushes a constant from a constant pool (String, int, float or class type) onto the stack. Stack: -> value"),
0x14: ("ldc2_w", OpcodeCPIndex, "pushes a constant from a constant pool (double or long) onto the stack. Stack: -> value"),
0x15: ("iload", OpcodeIndex, "loads an int 'value' from a local variable '#index'. Stack: -> value"),
0x16: ("lload", OpcodeIndex, "loads a long value from a local variable '#index'. Stack: -> value"),
0x17: ("fload", OpcodeIndex, "loads a float 'value' from a local variable '#index'. Stack: -> value"),
0x18: ("dload", OpcodeIndex, "loads a double 'value' from a local variable '#index'. Stack: -> value"),
0x19: ("aload", OpcodeIndex, "loads a reference onto the stack from a local variable '#index'. Stack: -> objectref"),
0x1a: ("iload_0", OpcodeNoArgs, "loads an int 'value' from variable 0. Stack: -> value"),
0x1b: ("iload_1", OpcodeNoArgs, "loads an int 'value' from variable 1. Stack: -> value"),
0x1c: ("iload_2", OpcodeNoArgs, "loads an int 'value' from variable 2. Stack: -> value"),
0x1d: ("iload_3", OpcodeNoArgs, "loads an int 'value' from variable 3. Stack: -> value"),
0x1e: ("lload_0", OpcodeNoArgs, "load a long value from a local variable 0. Stack: -> value"),
0x1f: ("lload_1", OpcodeNoArgs, "load a long value from a local variable 1. Stack: -> value"),
0x20: ("lload_2", OpcodeNoArgs, "load a long value from a local variable 2. Stack: -> value"),
0x21: ("lload_3", OpcodeNoArgs, "load a long value from a local variable 3. Stack: -> value"),
0x22: ("fload_0", OpcodeNoArgs, "loads a float 'value' from local variable 0. Stack: -> value"),
0x23: ("fload_1", OpcodeNoArgs, "loads a float 'value' from local variable 1. Stack: -> value"),
0x24: ("fload_2", OpcodeNoArgs, "loads a float 'value' from local variable 2. Stack: -> value"),
0x25: ("fload_3", OpcodeNoArgs, "loads a float 'value' from local variable 3. Stack: -> value"),
0x26: ("dload_0", OpcodeNoArgs, "loads a double from local variable 0. Stack: -> value"),
0x27: ("dload_1", OpcodeNoArgs, "loads a double from local variable 1. Stack: -> value"),
0x28: ("dload_2", OpcodeNoArgs, "loads a double from local variable 2. Stack: -> value"),
0x29: ("dload_3", OpcodeNoArgs, "loads a double from local variable 3. Stack: -> value"),
0x2a: ("aload_0", OpcodeNoArgs, "loads a reference onto the stack from local variable 0. Stack: -> objectref"),
0x2b: ("aload_1", OpcodeNoArgs, "loads a reference onto the stack from local variable 1. Stack: -> objectref"),
0x2c: ("aload_2", OpcodeNoArgs, "loads a reference onto the stack from local variable 2. Stack: -> objectref"),
0x2d: ("aload_3", OpcodeNoArgs, "loads a reference onto the stack from local variable 3. Stack: -> objectref"),
0x2e: ("iaload", OpcodeNoArgs, "loads an int from an array. Stack: arrayref, index -> value"),
0x2f: ("laload", OpcodeNoArgs, "load a long from an array. Stack: arrayref, index -> value"),
0x30: ("faload", OpcodeNoArgs, "loads a float from an array. Stack: arrayref, index -> value"),
0x31: ("daload", OpcodeNoArgs, "loads a double from an array. Stack: arrayref, index -> value"),
0x32: ("aaload", OpcodeNoArgs, "loads onto the stack a reference from an array. Stack: arrayref, index -> value"),
0x33: ("baload", OpcodeNoArgs, "loads a byte or Boolean value from an array. Stack: arrayref, index -> value"),
0x34: ("caload", OpcodeNoArgs, "loads a char from an array. Stack: arrayref, index -> value"),
0x35: ("saload", OpcodeNoArgs, "load short from array. Stack: arrayref, index -> value"),
0x36: ("istore", OpcodeIndex, "store int 'value' into variable '#index'. Stack: value ->"),
0x37: ("lstore", OpcodeIndex, "store a long 'value' in a local variable '#index'. Stack: value ->"),
0x38: ("fstore", OpcodeIndex, "stores a float 'value' into a local variable '#index'. Stack: value ->"),
0x39: ("dstore", OpcodeIndex, "stores a double 'value' into a local variable '#index'. Stack: value ->"),
0x3a: ("astore", OpcodeIndex, "stores a reference into a local variable '#index'. Stack: objectref ->"),
0x3b: ("istore_0", OpcodeNoArgs, "store int 'value' into variable 0. Stack: value ->"),
0x3c: ("istore_1", OpcodeNoArgs, "store int 'value' into variable 1. Stack: value ->"),
0x3d: ("istore_2", OpcodeNoArgs, "store int 'value' into variable 2. Stack: value ->"),
0x3e: ("istore_3", OpcodeNoArgs, "store int 'value' into variable 3. Stack: value ->"),
0x3f: ("lstore_0", OpcodeNoArgs, "store a long 'value' in a local variable 0. Stack: value ->"),
0x40: ("lstore_1", OpcodeNoArgs, "store a long 'value' in a local variable 1. Stack: value ->"),
0x41: ("lstore_2", OpcodeNoArgs, "store a long 'value' in a local variable 2. Stack: value ->"),
0x42: ("lstore_3", OpcodeNoArgs, "store a long 'value' in a local variable 3. Stack: value ->"),
0x43: ("fstore_0", OpcodeNoArgs, "stores a float 'value' into local variable 0. Stack: value ->"),
0x44: ("fstore_1", OpcodeNoArgs, "stores a float 'value' into local variable 1. Stack: value ->"),
0x45: ("fstore_2", OpcodeNoArgs, "stores a float 'value' into local variable 2. Stack: value ->"),
0x46: ("fstore_3", OpcodeNoArgs, "stores a float 'value' into local variable 3. Stack: value ->"),
0x47: ("dstore_0", OpcodeNoArgs, "stores a double into local variable 0. Stack: value ->"),
0x48: ("dstore_1", OpcodeNoArgs, "stores a double into local variable 1. Stack: value ->"),
0x49: ("dstore_2", OpcodeNoArgs, "stores a double into local variable 2. Stack: value ->"),
0x4a: ("dstore_3", OpcodeNoArgs, "stores a double into local variable 3. Stack: value ->"),
0x4b: ("astore_0", OpcodeNoArgs, "stores a reference into local variable 0. Stack: objectref ->"),
0x4c: ("astore_1", OpcodeNoArgs, "stores a reference into local variable 1. Stack: objectref ->"),
0x4d: ("astore_2", OpcodeNoArgs, "stores a reference into local variable 2. Stack: objectref ->"),
0x4e: ("astore_3", OpcodeNoArgs, "stores a reference into local variable 3. Stack: objectref ->"),
0x4f: ("iastore", OpcodeNoArgs, "stores an int into an array. Stack: arrayref, index, value ->"),
0x50: ("lastore", OpcodeNoArgs, "store a long to an array. Stack: arrayref, index, value ->"),
0x51: ("fastore", OpcodeNoArgs, "stores a float in an array. Stack: arreyref, index, value ->"),
0x52: ("dastore", OpcodeNoArgs, "stores a double into an array. Stack: arrayref, index, value ->"),
0x53: ("aastore", OpcodeNoArgs, "stores into a reference to an array. Stack: arrayref, index, value ->"),
0x54: ("bastore", OpcodeNoArgs, "stores a byte or Boolean value into an array. Stack: arrayref, index, value ->"),
0x55: ("castore", OpcodeNoArgs, "stores a char into an array. Stack: arrayref, index, value ->"),
0x56: ("sastore", OpcodeNoArgs, "store short to array. Stack: arrayref, index, value ->"),
0x57: ("pop", OpcodeNoArgs, "discards the top value on the stack. Stack: value ->"),
0x58: ("pop2", OpcodeNoArgs, "discards the top two values on the stack (or one value, if it is a double or long). Stack: {value2, value1} ->"),
0x59: ("dup", OpcodeNoArgs, "duplicates the value on top of the stack. Stack: value -> value, value"),
0x5a: ("dup_x1", OpcodeNoArgs, "inserts a copy of the top value into the stack two values from the top. Stack: value2, value1 -> value1, value2, value1"),
0x5b: ("dup_x2", OpcodeNoArgs, "inserts a copy of the top value into the stack two (if value2 is double or long it takes up the entry of value3, too) or three values (if value2 is neither double nor long) from the top. Stack: value3, value2, value1 -> value1, value3, value2, value1"),
0x5c: ("dup2", OpcodeNoArgs, "duplicate top two stack words (two values, if value1 is not double nor long; a single value, if value1 is double or long). Stack: {value2, value1} -> {value2, value1}, {value2, value1}"),
0x5d: ("dup2_x1", OpcodeNoArgs, "duplicate two words and insert beneath third word. Stack: value3, {value2, value1} -> {value2, value1}, value3, {value2, value1}"),
0x5e: ("dup2_x2", OpcodeNoArgs, "duplicate two words and insert beneath fourth word. Stack: {value4, value3}, {value2, value1} -> {value2, value1}, {value4, value3}, {value2, value1}"),
0x5f: ("swap", OpcodeNoArgs, "swaps two top words on the stack (note that value1 and value2 must not be double or long). Stack: value2, value1 -> value1, value2"),
0x60: ("iadd", OpcodeNoArgs, "adds two ints together. Stack: value1, value2 -> result"),
0x61: ("ladd", OpcodeNoArgs, "add two longs. Stack: value1, value2 -> result"),
0x62: ("fadd", OpcodeNoArgs, "adds two floats. Stack: value1, value2 -> result"),
0x63: ("dadd", OpcodeNoArgs, "adds two doubles. Stack: value1, value2 -> result"),
0x64: ("isub", OpcodeNoArgs, "int subtract. Stack: value1, value2 -> result"),
0x65: ("lsub", OpcodeNoArgs, "subtract two longs. Stack: value1, value2 -> result"),
0x66: ("fsub", OpcodeNoArgs, "subtracts two floats. Stack: value1, value2 -> result"),
0x67: ("dsub", OpcodeNoArgs, "subtracts a double from another. Stack: value1, value2 -> result"),
0x68: ("imul", OpcodeNoArgs, "multiply two integers. Stack: value1, value2 -> result"),
0x69: ("lmul", OpcodeNoArgs, "multiplies two longs. Stack: value1, value2 -> result"),
0x6a: ("fmul", OpcodeNoArgs, "multiplies two floats. Stack: value1, value2 -> result"),
0x6b: ("dmul", OpcodeNoArgs, "multiplies two doubles. Stack: value1, value2 -> result"),
0x6c: ("idiv", OpcodeNoArgs, "divides two integers. Stack: value1, value2 -> result"),
0x6d: ("ldiv", OpcodeNoArgs, "divide two longs. Stack: value1, value2 -> result"),
0x6e: ("fdiv", OpcodeNoArgs, "divides two floats. Stack: value1, value2 -> result"),
0x6f: ("ddiv", OpcodeNoArgs, "divides two doubles. Stack: value1, value2 -> result"),
0x70: ("irem", OpcodeNoArgs, "logical int remainder. Stack: value1, value2 -> result"),
0x71: ("lrem", OpcodeNoArgs, "remainder of division of two longs. Stack: value1, value2 -> result"),
0x72: ("frem", OpcodeNoArgs, "gets the remainder from a division between two floats. Stack: value1, value2 -> result"),
0x73: ("drem", OpcodeNoArgs, "gets the remainder from a division between two doubles. Stack: value1, value2 -> result"),
0x74: ("ineg", OpcodeNoArgs, "negate int. Stack: value -> result"),
0x75: ("lneg", OpcodeNoArgs, "negates a long. Stack: value -> result"),
0x76: ("fneg", OpcodeNoArgs, "negates a float. Stack: value -> result"),
0x77: ("dneg", OpcodeNoArgs, "negates a double. Stack: value -> result"),
0x78: ("ishl", OpcodeNoArgs, "int shift left. Stack: value1, value2 -> result"),
0x79: ("lshl", OpcodeNoArgs, "bitwise shift left of a long 'value1' by 'value2' positions. Stack: value1, value2 -> result"),
0x7a: ("ishr", OpcodeNoArgs, "int shift right. Stack: value1, value2 -> result"),
0x7b: ("lshr", OpcodeNoArgs, "bitwise shift right of a long 'value1' by 'value2' positions. Stack: value1, value2 -> result"),
0x7c: ("iushr", OpcodeNoArgs, "int shift right. Stack: value1, value2 -> result"),
0x7d: ("lushr", OpcodeNoArgs, "bitwise shift right of a long 'value1' by 'value2' positions, unsigned. Stack: value1, value2 -> result"),
0x7e: ("iand", OpcodeNoArgs, "performs a logical and on two integers. Stack: value1, value2 -> result"),
0x7f: ("land", OpcodeNoArgs, "bitwise and of two longs. Stack: value1, value2 -> result"),
0x80: ("ior", OpcodeNoArgs, "logical int or. Stack: value1, value2 -> result"),
0x81: ("lor", OpcodeNoArgs, "bitwise or of two longs. Stack: value1, value2 -> result"),
0x82: ("ixor", OpcodeNoArgs, "int xor. Stack: value1, value2 -> result"),
0x83: ("lxor", OpcodeNoArgs, "bitwise exclusive or of two longs. Stack: value1, value2 -> result"),
0x84: ("iinc", OpcodeSpecial_iinc, "increment local variable '#index' by signed byte 'const'. Stack: [No change]"),
0x85: ("i2l", OpcodeNoArgs, "converts an int into a long. Stack: value -> result"),
0x86: ("i2f", OpcodeNoArgs, "converts an int into a float. Stack: value -> result"),
0x87: ("i2d", OpcodeNoArgs, "converts an int into a double. Stack: value -> result"),
0x88: ("l2i", OpcodeNoArgs, "converts a long to an int. Stack: value -> result"),
0x89: ("l2f", OpcodeNoArgs, "converts a long to a float. Stack: value -> result"),
0x8a: ("l2d", OpcodeNoArgs, "converts a long to a double. Stack: value -> result"),
0x8b: ("f2i", OpcodeNoArgs, "converts a float to an int. Stack: value -> result"),
0x8c: ("f2l", OpcodeNoArgs, "converts a float to a long. Stack: value -> result"),
0x8d: ("f2d", OpcodeNoArgs, "converts a float to a double. Stack: value -> result"),
0x8e: ("d2i", OpcodeNoArgs, "converts a double to an int. Stack: value -> result"),
0x8f: ("d2l", OpcodeNoArgs, "converts a double to a long. Stack: value -> result"),
0x90: ("d2f", OpcodeNoArgs, "converts a double to a float. Stack: value -> result"),
0x91: ("i2b", OpcodeNoArgs, "converts an int into a byte. Stack: value -> result"),
0x92: ("i2c", OpcodeNoArgs, "converts an int into a character. Stack: value -> result"),
0x93: ("i2s", OpcodeNoArgs, "converts an int into a short. Stack: value -> result"),
0x94: ("lcmp", OpcodeNoArgs, "compares two longs values. Stack: value1, value2 -> result"),
0x95: ("fcmpl", OpcodeNoArgs, "compares two floats. Stack: value1, value2 -> result"),
0x96: ("fcmpg", OpcodeNoArgs, "compares two floats. Stack: value1, value2 -> result"),
0x97: ("dcmpl", OpcodeNoArgs, "compares two doubles. Stack: value1, value2 -> result"),
0x98: ("dcmpg", OpcodeNoArgs, "compares two doubles. Stack: value1, value2 -> result"),
0x99: ("ifeq", OpcodeShortJump, "if 'value' is 0, branch to the 16-bit instruction offset argument. Stack: value ->"),
0x9a: ("ifne", OpcodeShortJump, "if 'value' is not 0, branch to the 16-bit instruction offset argument. Stack: value ->"),
0x9c: ("ifge", OpcodeShortJump, "if 'value' is greater than or equal to 0, branch to the 16-bit instruction offset argument. Stack: value ->"),
0x9d: ("ifgt", OpcodeShortJump, "if 'value' is greater than 0, branch to the 16-bit instruction offset argument. Stack: value ->"),
0x9e: ("ifle", OpcodeShortJump, "if 'value' is less than or equal to 0, branch to the 16-bit instruction offset argument. Stack: value ->"),
0x9f: ("if_icmpeq", OpcodeShortJump, "if ints are equal, branch to the 16-bit instruction offset argument. Stack: value1, value2 ->"),
0xa0: ("if_icmpne", OpcodeShortJump, "if ints are not equal, branch to the 16-bit instruction offset argument. Stack: value1, value2 ->"),
0xa1: ("if_icmplt", OpcodeShortJump, "if 'value1' is less than 'value2', branch to the 16-bit instruction offset argument. Stack: value1, value2 ->"),
0xa2: ("if_icmpge", OpcodeShortJump, "if 'value1' is greater than or equal to 'value2', branch to the 16-bit instruction offset argument. Stack: value1, value2 ->"),
0xa3: ("if_icmpgt", OpcodeShortJump, "if 'value1' is greater than 'value2', branch to the 16-bit instruction offset argument. Stack: value1, value2 ->"),
0xa4: ("if_icmple", OpcodeShortJump, "if 'value1' is less than or equal to 'value2', branch to the 16-bit instruction offset argument. Stack: value1, value2 ->"),
0xa5: ("if_acmpeq", OpcodeShortJump, "if references are equal, branch to the 16-bit instruction offset argument. Stack: value1, value2 ->"),
0xa6: ("if_acmpne", OpcodeShortJump, "if references are not equal, branch to the 16-bit instruction offset argument. Stack: value1, value2 ->"),
0xa7: ("goto", OpcodeShortJump, "goes to the 16-bit instruction offset argument. Stack: [no change]"),
0xa8: ("jsr", OpcodeShortJump, "jump to subroutine at the 16-bit instruction offset argument and place the return address on the stack. Stack: -> address"),
0xa9: ("ret", OpcodeIndex, "continue execution from address taken from a local variable '#index'. Stack: [No change]"),
0xaa: ("tableswitch", OpcodeSpecial_tableswitch, "continue execution from an address in the table at offset 'index'. Stack: index ->"),
0xab: ("lookupswitch", OpcodeSpecial_lookupswitch, "a target address is looked up from a table using a key and execution continues from the instruction at that address. Stack: key ->"),
0xac: ("ireturn", OpcodeNoArgs, "returns an integer from a method. Stack: value -> [empty]"),
0xad: ("lreturn", OpcodeNoArgs, "returns a long value. Stack: value -> [empty]"),
0xae: ("freturn", OpcodeNoArgs, "returns a float. Stack: value -> [empty]"),
0xaf: ("dreturn", OpcodeNoArgs, "returns a double from a method. Stack: value -> [empty]"),
0xb0: ("areturn", OpcodeNoArgs, "returns a reference from a method. Stack: objectref -> [empty]"),
0xb1: ("return", OpcodeNoArgs, "return void from method. Stack: -> [empty]"),
0xb2: ("getstatic", OpcodeCPIndex, "gets a static field 'value' of a class, where the field is identified by field reference in the constant pool. Stack: -> value"),
0xb3: ("putstatic", OpcodeCPIndex, "set static field to 'value' in a class, where the field is identified by a field reference in constant pool. Stack: value ->"),
0xb4: ("getfield", OpcodeCPIndex, "gets a field 'value' of an object 'objectref', where the field is identified by field reference <argument> in the constant pool. Stack: objectref -> value"),
0xb5: ("putfield", OpcodeCPIndex, "set field to 'value' in an object 'objectref', where the field is identified by a field reference <argument> in constant pool. Stack: objectref, value ->"),
0xb6: ("invokevirtual", OpcodeCPIndex, "invoke virtual method on object 'objectref', where the method is identified by method reference <argument> in constant pool. Stack: objectref, [arg1, arg2, ...] ->"),
0xb7: ("invokespecial", OpcodeCPIndex, "invoke instance method on object 'objectref', where the method is identified by method reference <argument> in constant pool. Stack: objectref, [arg1, arg2, ...] ->"),
0xb8: ("invokestatic", OpcodeCPIndex, "invoke a static method, where the method is identified by method reference <argument> in the constant pool. Stack: [arg1, arg2, ...] ->"),
0xb9: ("invokeinterface", OpcodeSpecial_invokeinterface, "invokes an interface method on object 'objectref', where the interface method is identified by method reference <argument> in constant pool. Stack: objectref, [arg1, arg2, ...] ->"),
0xba: ("xxxunusedxxx", OpcodeNoArgs, "this opcode is reserved for historical reasons. Stack: "),
0xbb: ("new", OpcodeCPIndex, "creates new object of type identified by class reference <argument> in constant pool. Stack: -> objectref"),
0xbc: ("newarray", OpcodeSpecial_newarray, "creates new array with 'count' elements of primitive type given in the argument. Stack: count -> arrayref"),
0xbd: ("anewarray", OpcodeCPIndex, "creates a new array of references of length 'count' and component type identified by the class reference <argument> in the constant pool. Stack: count -> arrayref"),
0xbe: ("arraylength", OpcodeNoArgs, "gets the length of an array. Stack: arrayref -> length"),
0xbf: ("athrow", OpcodeNoArgs, "throws an error or exception (notice that the rest of the stack is cleared, leaving only a reference to the Throwable). Stack: objectref -> [empty], objectref"),
0xc0: ("checkcast", OpcodeCPIndex, "checks whether an 'objectref' is of a certain type, the class reference of which is in the constant pool. Stack: objectref -> objectref"),
0xc1: ("instanceof", OpcodeCPIndex, "determines if an object 'objectref' is of a given type, identified by class reference <argument> in constant pool. Stack: objectref -> result"),
0xc2: ("monitorenter", OpcodeNoArgs, "enter monitor for object (\"grab the lock\" - start of synchronized() section). Stack: objectref -> "),
0xc3: ("monitorexit", OpcodeNoArgs, "exit monitor for object (\"release the lock\" - end of synchronized() section). Stack: objectref -> "),
0xc4: ("wide", OpcodeSpecial_wide, "execute 'opcode', where 'opcode' is either iload, fload, aload, lload, dload, istore, fstore, astore, lstore, dstore, or ret, but assume the 'index' is 16 bit; or execute iinc, where the 'index' is 16 bits and the constant to increment by is a signed 16 bit short. Stack: [same as for corresponding instructions]"),
0xc5: ("multianewarray", OpcodeSpecial_multianewarray, "create a new array of 'dimensions' dimensions with elements of type identified by class reference in constant pool; the sizes of each dimension is identified by 'count1', ['count2', etc]. Stack: count1, [count2,...] -> arrayref"),
0xc6: ("ifnull", OpcodeShortJump, "if 'value' is null, branch to the 16-bit instruction offset argument. Stack: value ->"),
0xc7: ("ifnonnull", OpcodeShortJump, "if 'value' is not null, branch to the 16-bit instruction offset argument. Stack: value ->"),
0xc8: ("goto_w", OpcodeLongJump, "goes to another instruction at the 32-bit branch offset argument. Stack: [no change]"),
0xc9: ("jsr_w", OpcodeLongJump, "jump to subroutine at the 32-bit branch offset argument and place the return address on the stack. Stack: -> address"),
0xca: ("breakpoint", OpcodeNoArgs, "reserved for breakpoints in Java debuggers; should not appear in any class file."),
0xfe: ("impdep1", OpcodeNoArgs, "reserved for implementation-dependent operations within debuggers; should not appear in any class file."),
0xff: ("impdep2", OpcodeNoArgs, "reserved for implementation-dependent operations within debuggers; should not appear in any class file.")}
    def __init__(self, parent, name, length):
        FieldSet.__init__(self, parent, name)
        self._size = length*8
    def createFields(self):
        while self.current_size < self.size:
            bytecode = ord(self.parent.stream.readBytes(self.absolute_address+self.current_size, 1))
            op, cls, desc = self.OPCODE_TABLE.get(bytecode,["<reserved_opcode>", OpcodeNoArgs, "Reserved opcode."])
            yield cls(self, "bytecode[]", op, desc)

###############################################################################
class CPInfo(FieldSet):
    """
    Holds a constant pool entry.  Entries all have a type, and various contents
    fields depending on their type.
    """
    def createFields(self):
        yield Enum(UInt8(self, "tag"), self.root.CONSTANT_TYPES)
        if self["tag"].value not in self.root.CONSTANT_TYPES:
            raise ParserError("Java: unknown constant type (%s)" % self["tag"].value)
        self.constant_type = self.root.CONSTANT_TYPES[self["tag"].value]
        if self.constant_type == "Utf8":
            yield PascalString16(self, "bytes", charset="UTF-8")
        elif self.constant_type == "Integer":
            yield Int32(self, "bytes")
        elif self.constant_type == "Float":
            yield Float32(self, "bytes")
        elif self.constant_type == "Long":
            yield Int64(self, "bytes")
        elif self.constant_type == "Double":
            yield Float64(self, "bytes")
        elif self.constant_type == "Class":
            yield CPIndex(self, "name_index", "Class or interface name", target_types="Utf8")
        elif self.constant_type == "String":
            yield CPIndex(self, "string_index", target_types="Utf8")
        elif self.constant_type == "Fieldref":
            yield CPIndex(self, "class_index", "Field class or interface name", target_types="Class")
            yield CPIndex(self, "name_and_type_index", target_types="NameAndType")
        elif self.constant_type == "Methodref":
            yield CPIndex(self, "class_index", "Method class name", target_types="Class")
            yield CPIndex(self, "name_and_type_index", target_types="NameAndType")
        elif self.constant_type == "InterfaceMethodref":
            yield CPIndex(self, "class_index", "Method interface name", target_types="Class")
            yield CPIndex(self, "name_and_type_index", target_types="NameAndType")
        elif self.constant_type == "NameAndType":
            yield CPIndex(self, "name_index", target_types="Utf8")
            yield CPIndex(self, "descriptor_index", target_types="Utf8")
        else:
            raise ParserError("Not a valid constant pool element type: "
                    + self["tag"].value)

    def __str__(self):
        """
        Returns a human-readable string representation of the constant pool
        entry.  It is used for pretty-printing of the CPIndex fields pointing
        to it.
        """
        if self.constant_type == "Utf8":
            return self["bytes"].value
        elif self.constant_type in ("Integer", "Float", "Long", "Double"):
            return self["bytes"].display
        elif self.constant_type == "Class":
            class_name = str(self["name_index"].get_cp_entry())
            return class_name.replace("/",".")
        elif self.constant_type == "String":
            return str(self["string_index"].get_cp_entry())
        elif self.constant_type == "Fieldref":
            return "%s (from %s)" % (self["name_and_type_index"], self["class_index"])
        elif self.constant_type == "Methodref":
            return "%s (from %s)" % (self["name_and_type_index"], self["class_index"])
        elif self.constant_type == "InterfaceMethodref":
             return "%s (from %s)" % (self["name_and_type_index"], self["class_index"])
        elif self.constant_type == "NameAndType":
            return parse_any_descriptor(
                    str(self["descriptor_index"].get_cp_entry()),
                    name=str(self["name_index"].get_cp_entry()))
        else:
            # FIXME: Return "<error>" instead of raising an exception?
            raise ParserError("Not a valid constant pool element type: "
                    + self["tag"].value)


###############################################################################
# field_info {
#        u2 access_flags;
#        u2 name_index;
#        u2 descriptor_index;
#        u2 attributes_count;
#        attribute_info attributes[attributes_count];
# }
class FieldInfo(FieldSet):
    def createFields(self):
        # Access flags (16 bits)
        yield NullBits(self, "reserved[]", 8)
        yield Bit(self, "transient")
        yield Bit(self, "volatile")
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "final")
        yield Bit(self, "static")
        yield Bit(self, "protected")
        yield Bit(self, "private")
        yield Bit(self, "public")

        yield CPIndex(self, "name_index", "Field name", target_types="Utf8")
        yield CPIndex(self, "descriptor_index", "Field descriptor", target_types="Utf8",
                target_text_handler=parse_field_descriptor)
        yield UInt16(self, "attributes_count", "Number of field attributes")
        if self["attributes_count"].value > 0:
            yield FieldArray(self, "attributes", AttributeInfo,
                    self["attributes_count"].value)


###############################################################################
# method_info {
#        u2 access_flags;
#        u2 name_index;
#        u2 descriptor_index;
#        u2 attributes_count;
#        attribute_info attributes[attributes_count];
# }
class MethodInfo(FieldSet):
    def createFields(self):
        # Access flags (16 bits)
        yield NullBits(self, "reserved[]", 4)
        yield Bit(self, "strict")
        yield Bit(self, "abstract")
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "native")
        yield NullBits(self, "reserved[]", 2)
        yield Bit(self, "synchronized")
        yield Bit(self, "final")
        yield Bit(self, "static")
        yield Bit(self, "protected")
        yield Bit(self, "private")
        yield Bit(self, "public")

        yield CPIndex(self, "name_index", "Method name", target_types="Utf8")
        yield CPIndex(self, "descriptor_index", "Method descriptor",
                target_types="Utf8",
                target_text_handler=parse_method_descriptor)
        yield UInt16(self, "attributes_count", "Number of method attributes")
        if self["attributes_count"].value > 0:
            yield FieldArray(self, "attributes", AttributeInfo,
                    self["attributes_count"].value)


###############################################################################
# attribute_info {
#        u2 attribute_name_index;
#        u4 attribute_length;
#        u1 info[attribute_length];
# }
# [...]
class AttributeInfo(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        self._size = (self["attribute_length"].value + 6) * 8

    def createFields(self):
        yield CPIndex(self, "attribute_name_index", "Attribute name", target_types="Utf8")
        yield UInt32(self, "attribute_length", "Length of the attribute")
        attr_name = str(self["attribute_name_index"].get_cp_entry())

        # ConstantValue_attribute {
        #   u2 attribute_name_index;
        #   u4 attribute_length;
        #   u2 constantvalue_index;
        # }
        if attr_name == "ConstantValue":
            if self["attribute_length"].value != 2:
                    raise ParserError("Java: Invalid attribute %s length (%s)" \
                        % (self.path, self["attribute_length"].value))
            yield CPIndex(self, "constantvalue_index",
                    target_types=("Long","Float","Double","Integer","String"))

        # Code_attribute {
        #   u2 attribute_name_index;
        #   u4 attribute_length;
        #   u2 max_stack;
        #   u2 max_locals;
        #   u4 code_length;
        #   u1 code[code_length];
        #   u2 exception_table_length;
        #   {   u2 start_pc;
        #       u2 end_pc;
        #       u2  handler_pc;
        #       u2  catch_type;
        #   } exception_table[exception_table_length];
        #   u2 attributes_count;
        #   attribute_info attributes[attributes_count];
        # }
        elif attr_name == "Code":
            yield UInt16(self, "max_stack")
            yield UInt16(self, "max_locals")
            yield UInt32(self, "code_length")
            if self["code_length"].value > 0:
                yield JavaBytecode(self, "code", self["code_length"].value)
            yield UInt16(self, "exception_table_length")
            if self["exception_table_length"].value > 0:
                yield FieldArray(self, "exception_table", ExceptionTableEntry,
                        self["exception_table_length"].value)
            yield UInt16(self, "attributes_count")
            if self["attributes_count"].value > 0:
                yield FieldArray(self, "attributes", AttributeInfo,
                        self["attributes_count"].value)

        # Exceptions_attribute {
        #   u2 attribute_name_index;
        #   u4 attribute_length;
        #   u2 number_of_exceptions;
        #   u2 exception_index_table[number_of_exceptions];
        # }
        elif (attr_name == "Exceptions"):
            yield UInt16(self, "number_of_exceptions")
            yield FieldArray(self, "exception_index_table", CPIndex,
                    self["number_of_exceptions"].value, target_types="Class")
            assert self["attribute_length"].value == \
                2 + self["number_of_exceptions"].value * 2

        # InnerClasses_attribute {
        #   u2 attribute_name_index;
        #   u4 attribute_length;
        #   u2 number_of_classes;
        #   {   u2 inner_class_info_index;
        #       u2 outer_class_info_index;
        #       u2 inner_name_index;
        #       u2 inner_class_access_flags;
        #   } classes[number_of_classes];
        # }
        elif (attr_name == "InnerClasses"):
            yield UInt16(self, "number_of_classes")
            if self["number_of_classes"].value > 0:
                yield FieldArray(self, "classes", InnerClassesEntry,
                       self["number_of_classes"].value)
            assert self["attribute_length"].value == \
                2 + self["number_of_classes"].value * 8

        # Synthetic_attribute {
        #   u2 attribute_name_index;
        #   u4 attribute_length;
        # }
        elif (attr_name == "Synthetic"):
            assert self["attribute_length"].value == 0

        # SourceFile_attribute {
        #   u2 attribute_name_index;
        #   u4 attribute_length;
        #   u2 sourcefile_index;
        # }
        elif (attr_name == "SourceFile"):
            assert self["attribute_length"].value == 2
            yield CPIndex(self, "sourcefile_index", target_types="Utf8")

        # LineNumberTable_attribute {
        #   u2 attribute_name_index;
        #   u4 attribute_length;
        #   u2 line_number_table_length;
        #   {   u2 start_pc;
        #       u2 line_number;
        #   } line_number_table[line_number_table_length];
        # }
        elif (attr_name == "LineNumberTable"):
            yield UInt16(self, "line_number_table_length")
            if self["line_number_table_length"].value > 0:
                yield FieldArray(self, "line_number_table",
                        LineNumberTableEntry,
                        self["line_number_table_length"].value)
            assert self["attribute_length"].value == \
                    2 + self["line_number_table_length"].value * 4

        # LocalVariableTable_attribute {
        #   u2 attribute_name_index;
        #   u4 attribute_length;
        #   u2 local_variable_table_length;
        #   {   u2 start_pc;
        #       u2 length;
        #       u2 name_index;
        #       u2 descriptor_index;
        #       u2 index;
        #   } local_variable_table[local_variable_table_length];
        # }
        elif (attr_name == "LocalVariableTable"):
            yield UInt16(self, "local_variable_table_length")
            if self["local_variable_table_length"].value > 0:
                yield FieldArray(self, "local_variable_table",
                        LocalVariableTableEntry,
                        self["local_variable_table_length"].value)
            assert self["attribute_length"].value == \
                    2 + self["local_variable_table_length"].value * 10

        # Deprecated_attribute {
        #   u2 attribute_name_index;
        #   u4 attribute_length;
        # }
        elif (attr_name == "Deprecated"):
            assert self["attribute_length"].value == 0

        # Unkown attribute type.  They are allowed by the JVM specs, but we
        # can't say much about them...
        elif self["attribute_length"].value > 0:
            yield RawBytes(self, "info", self["attribute_length"].value)

class ExceptionTableEntry(FieldSet):
    static_size = 48 + CPIndex.static_size

    def createFields(self):
        yield textHandler(UInt16(self, "start_pc"), hexadecimal)
        yield textHandler(UInt16(self, "end_pc"), hexadecimal)
        yield textHandler(UInt16(self, "handler_pc"), hexadecimal)
        yield CPIndex(self, "catch_type", target_types="Class")

class InnerClassesEntry(StaticFieldSet):
    format = (
        (CPIndex, "inner_class_info_index",
                {"target_types": "Class", "allow_zero": True}),
        (CPIndex, "outer_class_info_index",
                {"target_types": "Class", "allow_zero": True}),
        (CPIndex, "inner_name_index",
                {"target_types": "Utf8", "allow_zero": True}),

        # Inner class access flags (16 bits)
        (NullBits, "reserved[]", 5),
        (Bit, "abstract"),
        (Bit, "interface"),
        (NullBits, "reserved[]", 3),
        (Bit, "super"),
        (Bit, "final"),
        (Bit, "static"),
        (Bit, "protected"),
        (Bit, "private"),
        (Bit, "public"),
    )

class LineNumberTableEntry(StaticFieldSet):
    format = (
        (UInt16, "start_pc"),
        (UInt16, "line_number")
    )

class LocalVariableTableEntry(StaticFieldSet):
    format = (
        (UInt16, "start_pc"),
        (UInt16, "length"),
        (CPIndex, "name_index", {"target_types": "Utf8"}),
        (CPIndex, "descriptor_index", {"target_types": "Utf8",
                "target_text_handler": parse_field_descriptor}),
        (UInt16, "index")
    )


###############################################################################
# ClassFile {
#        u4 magic;
#        u2 minor_version;
#        u2 major_version;
#        u2 constant_pool_count;
#        cp_info constant_pool[constant_pool_count-1];
#        u2 access_flags;
#        u2 this_class;
#        u2 super_class;
#        u2 interfaces_count;
#        u2 interfaces[interfaces_count];
#        u2 fields_count;
#        field_info fields[fields_count];
#        u2 methods_count;
#        method_info methods[methods_count];
#        u2 attributes_count;
#        attribute_info attributes[attributes_count];
# }
class JavaCompiledClassFile(Parser):
    """
    Root of the .class parser.
    """

    endian = BIG_ENDIAN

    PARSER_TAGS = {
        "id": "java_class",
        "category": "program",
        "file_ext": ("class",),
        "mime": (u"application/java-vm",),
        "min_size": (32 + 3*16),
        "description": "Compiled Java class"
    }

    MAGIC = 0xCAFEBABE
    KNOWN_VERSIONS = {
        "45.3": "JDK 1.1",
        "46.0": "JDK 1.2",
        "47.0": "JDK 1.3",
        "48.0": "JDK 1.4",
        "49.0": "JDK 1.5",
        "50.0": "JDK 1.6"
    }

    # Constants go here since they will probably depend on the detected format
    # version at some point.  Though, if they happen to be really backward
    # compatible, they may become module globals.
    CONSTANT_TYPES = {
         1: "Utf8",
         3: "Integer",
         4: "Float",
         5: "Long",
         6: "Double",
         7: "Class",
         8: "String",
         9: "Fieldref",
        10: "Methodref",
        11: "InterfaceMethodref",
        12: "NameAndType"
    }

    def validate(self):
        if self["magic"].value != self.MAGIC:
            return "Wrong magic signature!"
        version = "%d.%d" % (self["major_version"].value, self["minor_version"].value)
        if version not in self.KNOWN_VERSIONS:
            return "Unknown version (%s)" % version
        return True

    def createDescription(self):
        version = "%d.%d" % (self["major_version"].value, self["minor_version"].value)
        if version in self.KNOWN_VERSIONS:
            return "Compiled Java class, %s" % self.KNOWN_VERSIONS[version]
        else:
            return "Compiled Java class, version %s" % version

    def createFields(self):
        yield textHandler(UInt32(self, "magic", "Java compiled class signature"),
            hexadecimal)
        yield UInt16(self, "minor_version", "Class format minor version")
        yield UInt16(self, "major_version", "Class format major version")
        yield UInt16(self, "constant_pool_count", "Size of the constant pool")
        if self["constant_pool_count"].value > 1:
            #yield FieldArray(self, "constant_pool", CPInfo,
            #        (self["constant_pool_count"].value - 1), first_index=1)
            # Mmmh... can't use FieldArray actually, because ConstantPool
            # requires some specific hacks (skipping some indexes after Long
            # and Double entries).
            yield ConstantPool(self, "constant_pool",
                    (self["constant_pool_count"].value))

        # Inner class access flags (16 bits)
        yield NullBits(self, "reserved[]", 5)
        yield Bit(self, "abstract")
        yield Bit(self, "interface")
        yield NullBits(self, "reserved[]", 3)
        yield Bit(self, "super")
        yield Bit(self, "final")
        yield Bit(self, "static")
        yield Bit(self, "protected")
        yield Bit(self, "private")
        yield Bit(self, "public")

        yield CPIndex(self, "this_class", "Class name", target_types="Class")
        yield CPIndex(self, "super_class", "Super class name", target_types="Class")
        yield UInt16(self, "interfaces_count", "Number of implemented interfaces")
        if self["interfaces_count"].value > 0:
            yield FieldArray(self, "interfaces", CPIndex,
                    self["interfaces_count"].value, target_types="Class")
        yield UInt16(self, "fields_count", "Number of fields")
        if self["fields_count"].value > 0:
            yield FieldArray(self, "fields", FieldInfo,
                    self["fields_count"].value)
        yield UInt16(self, "methods_count", "Number of methods")
        if self["methods_count"].value > 0:
            yield FieldArray(self, "methods", MethodInfo,
                    self["methods_count"].value)
        yield UInt16(self, "attributes_count", "Number of attributes")
        if self["attributes_count"].value > 0:
            yield FieldArray(self, "attributes", AttributeInfo,
                    self["attributes_count"].value)

# vim: set expandtab tabstop=4 shiftwidth=4 autoindent smartindent:

########NEW FILE########
__FILENAME__ = prc
"""
PRC (Palm resource) parser.

Author: Sebastien Ponce
Creation date: 29 october 2008
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt16, UInt32, TimestampMac32,
    String, RawBytes)
from lib.hachoir_core.endian import BIG_ENDIAN

class PRCHeader(FieldSet):
    static_size = 78*8

    def createFields(self):
        yield String(self, "name", 32, "Name")
        yield UInt16(self, "flags", "Flags")
        yield UInt16(self, "version", "Version")
        yield TimestampMac32(self, "create_time", "Creation time")
        yield TimestampMac32(self, "mod_time", "Modification time")
        yield TimestampMac32(self, "backup_time", "Backup time")
        yield UInt32(self, "mod_num", "mod num")
        yield UInt32(self, "app_info", "app info")
        yield UInt32(self, "sort_info", "sort info")
        yield UInt32(self, "type", "type")
        yield UInt32(self, "id", "id")
        yield UInt32(self, "unique_id_seed", "unique_id_seed")
        yield UInt32(self, "next_record_list", "next_record_list")
        yield UInt16(self, "num_records", "num_records")

class ResourceHeader(FieldSet):
    static_size = 10*8

    def createFields(self):
        yield String(self, "name", 4, "Name of the resource")
        yield UInt16(self, "flags", "ID number of the resource")
        yield UInt32(self, "offset", "Pointer to the resource data")

    def createDescription(self):
        return "Resource Header (%s)" % self["name"]

class PRCFile(Parser):
    PARSER_TAGS = {
        "id": "prc",
        "category": "program",
        "file_ext": ("prc", ""),
        "min_size": ResourceHeader.static_size,  # At least one program header
        "mime": (
            u"application/x-pilot-prc",
            u"application/x-palmpilot"),
        "description": "Palm Resource File"
    }
    endian = BIG_ENDIAN

    def validate(self):
        # FIXME: Implement the validation function!
        return False

    def createFields(self):
        # Parse header and program headers
        yield PRCHeader(self, "header", "Header")
        lens = []
        firstOne = True
        poff = 0
        for index in xrange(self["header/num_records"].value):
            r = ResourceHeader(self, "res_header[]")
            if firstOne:
                firstOne = False
            else:
                lens.append(r["offset"].value - poff)
            poff = r["offset"].value
            yield r
        lens.append(self.size/8 - poff)
        yield UInt16(self, "placeholder", "Place holder bytes")
        for i in range(len(lens)):
            yield RawBytes(self, "res[]", lens[i], '"'+self["res_header["+str(i)+"]/name"].value+"\" Resource")

    def createDescription(self):
        return "Palm Resource file"


########NEW FILE########
__FILENAME__ = python
"""
Python compiled source code parser.

Informations:
- Python 2.4.2 source code:
  files Python/marshal.c and Python/import.c

Author: Victor Stinner
Creation: 25 march 2005
"""

DISASSEMBLE = False

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, UInt8,
    UInt16, Int32, UInt32, Int64, ParserError, Float64, Enum,
    Character, Bytes, RawBytes, PascalString8, TimestampUnix32)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.bits import long2raw
from lib.hachoir_core.text_handler import textHandler, hexadecimal
from lib.hachoir_core.i18n import ngettext
if DISASSEMBLE:
    from dis import dis

    def disassembleBytecode(field):
        bytecode = field.value
        dis(bytecode)

# --- String and string reference ---
def parseString(parent):
    yield UInt32(parent, "length", "Length")
    length = parent["length"].value
    if parent.name == "lnotab":
        bytecode_offset=0
        line_number=parent['../firstlineno'].value
        for i in range(0,length,2):
            bc_off_delta=UInt8(parent, 'bytecode_offset_delta[]')
            yield bc_off_delta
            bytecode_offset+=bc_off_delta.value
            bc_off_delta._description='Bytecode Offset %i'%bytecode_offset
            line_number_delta=UInt8(parent, 'line_number_delta[]')
            yield line_number_delta
            line_number+=line_number_delta.value
            line_number_delta._description='Line Number %i'%line_number
    elif 0 < length:
        yield RawBytes(parent, "text", length, "Content")
    if DISASSEMBLE and parent.name == "compiled_code":
        disassembleBytecode(parent["text"])

def parseStringRef(parent):
    yield textHandler(UInt32(parent, "ref"), hexadecimal)
def createStringRefDesc(parent):
    return "String ref: %s" % parent["ref"].display

# --- Integers ---
def parseInt32(parent):
    yield Int32(parent, "value")

def parseInt64(parent):
    yield Int64(parent, "value")

def parseLong(parent):
    yield Int32(parent, "digit_count")
    for index in xrange( abs(parent["digit_count"].value) ):
        yield UInt16(parent, "digit[]")


# --- Float and complex ---
def parseFloat(parent):
    yield PascalString8(parent, "value")
def parseBinaryFloat(parent):
    yield Float64(parent, "value")
def parseComplex(parent):
    yield PascalString8(parent, "real")
    yield PascalString8(parent, "complex")
def parseBinaryComplex(parent):
    yield Float64(parent, "real")
    yield Float64(parent, "complex")


# --- Tuple and list ---
def parseTuple(parent):
    yield Int32(parent, "count", "Item count")
    count = parent["count"].value
    if count < 0:
        raise ParserError("Invalid tuple/list count")
    for index in xrange(count):
        yield Object(parent, "item[]")

def createTupleDesc(parent):
    count = parent["count"].value
    items = ngettext("%s item", "%s items", count) % count
    return "%s: %s" % (parent.code_info[2], items)


# --- Dict ---
def parseDict(parent):
    """
    Format is: (key1, value1, key2, value2, ..., keyn, valuen, NULL)
    where each keyi and valuei is an object.
    """
    parent.count = 0
    while True:
        key = Object(parent, "key[]")
        yield key
        if key["bytecode"].value == "0":
            break
        yield Object(parent, "value[]")
        parent.count += 1

def createDictDesc(parent):
    return "Dict: %s" % (ngettext("%s key", "%s keys", parent.count) % parent.count)

# --- Code ---
def parseCode(parent):
    if 0x3000000 <= parent.root.getVersion():
        yield UInt32(parent, "arg_count", "Argument count")
        yield UInt32(parent, "kwonlyargcount", "Keyword only argument count")
        yield UInt32(parent, "nb_locals", "Number of local variables")
        yield UInt32(parent, "stack_size", "Stack size")
        yield UInt32(parent, "flags")
    elif 0x2030000 <= parent.root.getVersion():
        yield UInt32(parent, "arg_count", "Argument count")
        yield UInt32(parent, "nb_locals", "Number of local variables")
        yield UInt32(parent, "stack_size", "Stack size")
        yield UInt32(parent, "flags")
    else:
        yield UInt16(parent, "arg_count", "Argument count")
        yield UInt16(parent, "nb_locals", "Number of local variables")
        yield UInt16(parent, "stack_size", "Stack size")
        yield UInt16(parent, "flags")
    yield Object(parent, "compiled_code")
    yield Object(parent, "consts")
    yield Object(parent, "names")
    yield Object(parent, "varnames")
    if 0x2000000 <= parent.root.getVersion():
        yield Object(parent, "freevars")
        yield Object(parent, "cellvars")
    yield Object(parent, "filename")
    yield Object(parent, "name")
    if 0x2030000 <= parent.root.getVersion():
        yield UInt32(parent, "firstlineno", "First line number")
    else:
        yield UInt16(parent, "firstlineno", "First line number")
    yield Object(parent, "lnotab")

class Object(FieldSet):
    bytecode_info = {
        # Don't contains any data
        '0': ("null", None, "NULL", None),
        'N': ("none", None, "None", None),
        'F': ("false", None, "False", None),
        'T': ("true", None, "True", None),
        'S': ("stop_iter", None, "StopIter", None),
        '.': ("ellipsis", None, "ELLIPSIS", None),
        '?': ("unknown", None, "Unknown", None),

        'i': ("int32", parseInt32, "Int32", None),
        'I': ("int64", parseInt64, "Int64", None),
        'f': ("float", parseFloat, "Float", None),
        'g': ("bin_float", parseBinaryFloat, "Binary float", None),
        'x': ("complex", parseComplex, "Complex", None),
        'y': ("bin_complex", parseBinaryComplex, "Binary complex", None),
        'l': ("long", parseLong, "Long", None),
        's': ("string", parseString, "String", None),
        't': ("interned", parseString, "Interned", None),
        'u': ("unicode", parseString, "Unicode", None),
        'R': ("string_ref", parseStringRef, "String ref", createStringRefDesc),
        '(': ("tuple", parseTuple, "Tuple", createTupleDesc),
        '[': ("list", parseTuple, "List", createTupleDesc),
        '<': ("set", parseTuple, "Set", createTupleDesc),
        '>': ("frozenset", parseTuple, "Frozen set", createTupleDesc),
        '{': ("dict", parseDict, "Dict", createDictDesc),
        'c': ("code", parseCode, "Code", None),
    }

    def __init__(self, parent, name, **kw):
        FieldSet.__init__(self, parent, name, **kw)
        code = self["bytecode"].value
        if code not in self.bytecode_info:
            raise ParserError('Unknown bytecode: "%s"' % code)
        self.code_info = self.bytecode_info[code]
        if not name:
            self._name = self.code_info[0]
        if code == "l":
            self.createValue = self.createValueLong
        elif code in ("i", "I", "f", "g"):
            self.createValue = lambda: self["value"].value
        elif code == "T":
            self.createValue = lambda: True
        elif code == "F":
            self.createValue = lambda: False
        elif code in ("x", "y"):
            self.createValue = self.createValueComplex
        elif code in ("s", "t", "u"):
            self.createValue = self.createValueString
            self.createDisplay = self.createDisplayString
            if code == 't':
                if not hasattr(self.root,'string_table'):
                    self.root.string_table=[]
                self.root.string_table.append(self)
        elif code == 'R':
            if hasattr(self.root,'string_table'):
                self.createValue = self.createValueStringRef

    def createValueString(self):
        if "text" in self:
            return self["text"].value
        else:
            return ""

    def createDisplayString(self):
        if "text" in self:
            return self["text"].display
        else:
            return "(empty)"

    def createValueLong(self):
        is_negative = self["digit_count"].value < 0
        count = abs(self["digit_count"].value)
        total = 0
        for index in xrange(count-1, -1, -1):
            total <<= 15
            total += self["digit[%u]" % index].value
        if is_negative:
            total = -total
        return total

    def createValueStringRef(self):
        return self.root.string_table[self['ref'].value].value

    def createDisplayStringRef(self):
        return self.root.string_table[self['ref'].value].display

    def createValueComplex(self):
        return complex(
            float(self["real"].value),
            float(self["complex"].value))

    def createFields(self):
        yield Character(self, "bytecode", "Bytecode")
        parser = self.code_info[1]
        if parser:
            for field in parser(self):
                yield field

    def createDescription(self):
        create = self.code_info[3]
        if create:
            return create(self)
        else:
            return self.code_info[2]

class PythonCompiledFile(Parser):
    PARSER_TAGS = {
        "id": "python",
        "category": "program",
        "file_ext": ("pyc", "pyo"),
        "min_size": 9*8,
        "description": "Compiled Python script (.pyc/.pyo files)"
    }
    endian = LITTLE_ENDIAN

    # Dictionnary which associate the pyc signature (32-bit integer)
    # to a Python version string (eg. "m\xf2\r\n" => "Python 2.4b1").
    # This list comes from CPython source code, see "MAGIC"
    # and "pyc_magic" in file Python/import.c
    MAGIC = {
        # Python 1.x
        20121: ("1.5", 0x1050000),

        # Python 2.x
        50823: ("2.0", 0x2000000),
        60202: ("2.1", 0x2010000),
        60717: ("2.2", 0x2020000),
        62011: ("2.3a0", 0x2030000),
        62021: ("2.3a0", 0x2030000),
        62041: ("2.4a0", 0x2040000),
        62051: ("2.4a3", 0x2040000),
        62061: ("2.4b1", 0x2040000),
        62071: ("2.5a0", 0x2050000),
        62081: ("2.5a0 (ast-branch)", 0x2050000),
        62091: ("2.5a0 (with)", 0x2050000),
        62092: ("2.5a0 (WITH_CLEANUP opcode)", 0x2050000),
        62101: ("2.5b3", 0x2050000),
        62111: ("2.5b3", 0x2050000),
        62121: ("2.5c1", 0x2050000),
        62131: ("2.5c2", 0x2050000),

        # Python 3.x
        3000:  ("3.0 (3000)",  0x3000000),
        3010:  ("3.0 (3010)",  0x3000000),
        3020:  ("3.0 (3020)",  0x3000000),
        3030:  ("3.0 (3030)",  0x3000000),
        3040:  ("3.0 (3040)",  0x3000000),
        3050:  ("3.0 (3050)",  0x3000000),
        3060:  ("3.0 (3060)",  0x3000000),
        3070:  ("3.0 (3070)",  0x3000000),
        3080:  ("3.0 (3080)",  0x3000000),
        3090:  ("3.0 (3090)",  0x3000000),
        3100:  ("3.0 (3100)",  0x3000000),
        3102:  ("3.0 (3102)",  0x3000000),
        3110:  ("3.0a4",       0x3000000),
        3130:  ("3.0a5",       0x3000000),
        3131:  ("3.0a5 unicode",       0x3000000),
    }

    # Dictionnary which associate the pyc signature (4-byte long string)
    # to a Python version string (eg. "m\xf2\r\n" => "2.4b1")
    STR_MAGIC = dict( \
        (long2raw(magic | (ord('\r')<<16) | (ord('\n')<<24), LITTLE_ENDIAN), value[0]) \
        for magic, value in MAGIC.iteritems())

    def validate(self):
        signature = self.stream.readBits(0, 16, self.endian)
        if signature not in self.MAGIC:
            return "Unknown version (%s)" % signature
        if self.stream.readBytes(2*8, 2) != "\r\n":
            return r"Wrong signature (\r\n)"
        if self.stream.readBytes(8*8, 1) != 'c':
            return "First object bytecode is not code"
        return True

    def getVersion(self):
        if not hasattr(self, "version"):
            signature = self.stream.readBits(0, 16, self.endian)
            self.version = self.MAGIC[signature][1]
        return self.version

    def createFields(self):
        yield Enum(Bytes(self, "signature", 4, "Python file signature and version"), self.STR_MAGIC)
        yield TimestampUnix32(self, "timestamp", "Timestamp")
        yield Object(self, "content")


########NEW FILE########
__FILENAME__ = template
"""
====================== 8< ============================
This file is an Hachoir parser template. Make a copy
of it, and adapt it to your needs.

You have to replace all "TODO" with you code.
====================== 8< ============================

TODO parser.

Author: TODO TODO
Creation date: YYYY-mm-DD
"""

# TODO: Just keep what you need
from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (ParserError,
    UInt8, UInt16, UInt32, String, RawBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN, BIG_ENDIAN

class TODOFile(Parser):
    PARSER_TAGS = {
        "id": "TODO",
        "category": "TODO",    # "archive", "audio", "container", ...
        "file_ext": ("TODO",), # TODO: Example ("bmp",) to parse the file "image.bmp"
        "mime": (u"TODO"),      # TODO: Example: "image/png"
        "min_size": 0,         # TODO: Minimum file size (x bits, or x*8 in bytes)
        "description": "TODO", # TODO: Example: "A bitmap picture"
    }

#    TODO: Choose between little or big endian
#    endian = LITTLE_ENDIAN
#    endian = BIG_ENDIAN

    def validate(self):
        # TODO: Check that file looks like your format
        # Example: check first two bytes
        # return (self.stream.readBytes(0, 2) == 'BM')
        return False

    def createFields(self):
        # TODO: Write your parser using this model:
        # yield UInt8(self, "name1", "description1")
        # yield UInt16(self, "name2", "description2")
        # yield UInt32(self, "name3", "description3")
        # yield String(self, "name4", 1, "description4") # TODO: add ", charset="ASCII")"
        # yield String(self, "name5", 1, "description5", charset="ASCII")
        # yield String(self, "name6", 1, "description6", charset="ISO-8859-1")

        # Read rest of the file (if any)
        # TODO: You may remove this code
        if self.current_size < self._size:
            yield self.seekBit(self._size, "end")


########NEW FILE########
__FILENAME__ = version
__version__ = "1.3.4"
PACKAGE = "hachoir-parser"
WEBSITE = "http://bitbucket.org/haypo/hachoir/wiki/hachoir-parser"
LICENSE = 'GNU GPL v2'


########NEW FILE########
__FILENAME__ = amf
"""
AMF metadata (inside Flash video, FLV file) parser.

Documentation:

 - flashticle: Python project to read Flash (formats SWF, FLV and AMF)
   http://undefined.org/python/#flashticle

Author: Victor Stinner
Creation date: 4 november 2006
"""

from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt8, UInt16, UInt32, PascalString16, Float64)
from lib.hachoir_core.tools import timestampUNIX

def parseUTF8(parent):
    yield PascalString16(parent, "value", charset="UTF-8")

def parseDouble(parent):
    yield Float64(parent, "value")

def parseBool(parent):
    yield UInt8(parent, "value")

def parseArray(parent):
    yield UInt32(parent, "count")
    for index in xrange(parent["count"].value):
        yield AMFObject(parent, "item[]")

def parseObjectAttributes(parent):
    while True:
        item = Attribute(parent, "attr[]")
        yield item
        if item["key"].value == "":
            break

def parseMixedArray(parent):
    yield UInt32(parent, "count")
    for index in xrange(parent["count"].value + 1):
        item = Attribute(parent, "item[]")
        yield item
        if not item['key'].value:
            break

def parseDate(parent):
    yield Float64(parent, "timestamp_microsec")
    yield UInt16(parent, "timestamp_sec")

def parseNothing(parent):
    raise StopIteration()

class AMFObject(FieldSet):
    CODE_DATE = 11
    tag_info = {
        # http://osflash.org/amf/astypes
         0: (parseDouble, "Double"),
         1: (parseBool, "Boolean"),
         2: (parseUTF8, "UTF-8 string"),
         3: (parseObjectAttributes, "Object attributes"),
        #MOVIECLIP = '\x04',
        #NULL = '\x05',
        #UNDEFINED = '\x06',
        #REFERENCE = '\x07',
         8: (parseMixedArray, "Mixed array"),
         9: (parseNothing, "End of object"),
        10: (parseArray, "Array"),
        CODE_DATE: (parseDate, "Date"),
        #LONGUTF8 = '\x0c',
        #UNSUPPORTED = '\x0d',
        ## Server-to-client only
        #RECORDSET = '\x0e',
        #XML = '\x0f',
        #TYPEDOBJECT = '\x10',
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        code = self["type"].value
        try:
            self.parser, desc = self.tag_info[code]
            if code == self.CODE_DATE:
                self.createValue = self.createValueDate
        except KeyError:
            raise ParserError("AMF: Unable to parse type %s" % code)

    def createFields(self):
        yield UInt8(self, "type")
        for field in self.parser(self):
            yield field

    def createValueDate(self):
        value = (self["timestamp_microsec"].value * 0.001) \
            - (self["timestamp_sec"].value * 60)
        return timestampUNIX(value)

class Attribute(AMFObject):
    def __init__(self, *args):
        AMFObject.__init__(self, *args)
        self._description = None

    def createFields(self):
        yield PascalString16(self, "key", charset="UTF-8")
        yield UInt8(self, "type")
        for field in self.parser(self):
            yield field

    def createDescription(self):
        return 'Attribute "%s"' % self["key"].value


########NEW FILE########
__FILENAME__ = asf
"""
Advanced Streaming Format (ASF) parser, format used by Windows Media Video
(WMF) and Windows Media Audio (WMA).

Informations:
- http://www.microsoft.com/windows/windowsmedia/forpros/format/asfspec.aspx
- http://swpat.ffii.org/pikta/xrani/asf/index.fr.html

Author: Victor Stinner
Creation: 5 august 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError,
    UInt16, UInt32, UInt64,
    TimestampWin64, TimedeltaWin64,
    String, PascalString16, Enum,
    Bit, Bits, PaddingBits,
    PaddingBytes, NullBytes, RawBytes)
from lib.hachoir_core.endian import LITTLE_ENDIAN
from lib.hachoir_core.text_handler import (
    displayHandler, filesizeHandler)
from lib.hachoir_core.tools import humanBitRate
from itertools import izip
from lib.hachoir_parser.video.fourcc import audio_codec_name, video_fourcc_name
from lib.hachoir_parser.common.win32 import BitmapInfoHeader, GUID

MAX_HEADER_SIZE = 100 * 1024  # bytes

class AudioHeader(FieldSet):
    guid = "F8699E40-5B4D-11CF-A8FD-00805F5C442B"
    def createFields(self):
        yield Enum(UInt16(self, "twocc"), audio_codec_name)
        yield UInt16(self, "channels")
        yield UInt32(self, "sample_rate")
        yield UInt32(self, "bit_rate")
        yield UInt16(self, "block_align")
        yield UInt16(self, "bits_per_sample")
        yield UInt16(self, "codec_specific_size")
        size = self["codec_specific_size"].value
        if size:
            yield RawBytes(self, "codec_specific", size)

class BitrateMutualExclusion(FieldSet):
    guid = "D6E229DC-35DA-11D1-9034-00A0C90349BE"
    mutex_name = {
        "D6E22A00-35DA-11D1-9034-00A0C90349BE": "Language",
        "D6E22A01-35DA-11D1-9034-00A0C90349BE": "Bitrate",
        "D6E22A02-35DA-11D1-9034-00A0C90349BE": "Unknown",
    }

    def createFields(self):
        yield Enum(GUID(self, "exclusion_type"), self.mutex_name)
        yield UInt16(self, "nb_stream")
        for index in xrange(self["nb_stream"].value):
            yield UInt16(self, "stream[]")

class VideoHeader(FieldSet):
    guid = "BC19EFC0-5B4D-11CF-A8FD-00805F5C442B"
    def createFields(self):
        if False:
            yield UInt32(self, "width0")
            yield UInt32(self, "height0")
            yield PaddingBytes(self, "reserved[]", 7)
            yield UInt32(self, "width")
            yield UInt32(self, "height")
            yield PaddingBytes(self, "reserved[]", 2)
            yield UInt16(self, "depth")
            yield Enum(String(self, "codec", 4, charset="ASCII"), video_fourcc_name)
            yield NullBytes(self, "padding", 20)
        else:
            yield UInt32(self, "width")
            yield UInt32(self, "height")
            yield PaddingBytes(self, "reserved[]", 1)
            yield UInt16(self, "format_data_size")
            if self["format_data_size"].value < 40:
                raise ParserError("Unknown format data size")
            yield BitmapInfoHeader(self, "bmp_info", use_fourcc=True)

class FileProperty(FieldSet):
    guid = "8CABDCA1-A947-11CF-8EE4-00C00C205365"
    def createFields(self):
        yield GUID(self, "guid")
        yield filesizeHandler(UInt64(self, "file_size"))
        yield TimestampWin64(self, "creation_date")
        yield UInt64(self, "pckt_count")
        yield TimedeltaWin64(self, "play_duration")
        yield TimedeltaWin64(self, "send_duration")
        yield UInt64(self, "preroll")
        yield Bit(self, "broadcast", "Is broadcast?")
        yield Bit(self, "seekable", "Seekable stream?")
        yield PaddingBits(self, "reserved[]", 30)
        yield filesizeHandler(UInt32(self, "min_pckt_size"))
        yield filesizeHandler(UInt32(self, "max_pckt_size"))
        yield displayHandler(UInt32(self, "max_bitrate"), humanBitRate)

class HeaderExtension(FieldSet):
    guid = "5FBF03B5-A92E-11CF-8EE3-00C00C205365"
    def createFields(self):
        yield GUID(self, "reserved[]")
        yield UInt16(self, "reserved[]")
        yield UInt32(self, "size")
        if self["size"].value:
            yield RawBytes(self, "data", self["size"].value)

class Header(FieldSet):
    guid = "75B22630-668E-11CF-A6D9-00AA0062CE6C"
    def createFields(self):
        yield UInt32(self, "obj_count")
        yield PaddingBytes(self, "reserved[]", 2)
        for index in xrange(self["obj_count"].value):
            yield Object(self, "object[]")

class Metadata(FieldSet):
    guid = "75B22633-668E-11CF-A6D9-00AA0062CE6C"
    names = ("title", "author", "copyright", "xxx", "yyy")
    def createFields(self):
        for index in xrange(5):
            yield UInt16(self, "size[]")
        for name, size in izip(self.names, self.array("size")):
            if size.value:
                yield String(self, name, size.value, charset="UTF-16-LE", strip=" \0")

class Descriptor(FieldSet):
    """
    See ExtendedContentDescription class.
    """
    TYPE_BYTE_ARRAY = 1
    TYPE_NAME = {
        0: "Unicode",
        1: "Byte array",
        2: "BOOL (32 bits)",
        3: "DWORD (32 bits)",
        4: "QWORD (64 bits)",
        5: "WORD (16 bits)"
    }
    def createFields(self):
        yield PascalString16(self, "name", "Name", charset="UTF-16-LE", strip="\0")
        yield Enum(UInt16(self, "type"), self.TYPE_NAME)
        yield UInt16(self, "value_length")
        type = self["type"].value
        size = self["value_length"].value
        name = "value"
        if type == 0 and (size % 2) == 0:
            yield String(self, name, size, charset="UTF-16-LE", strip="\0")
        elif type in (2, 3):
            yield UInt32(self, name)
        elif type == 4:
            yield UInt64(self, name)
        else:
            yield RawBytes(self, name, size)

class ExtendedContentDescription(FieldSet):
    guid = "D2D0A440-E307-11D2-97F0-00A0C95EA850"
    def createFields(self):
        yield UInt16(self, "count")
        for index in xrange(self["count"].value):
            yield Descriptor(self, "descriptor[]")

class Codec(FieldSet):
    """
    See CodecList class.
    """
    type_name = {
        1: "video",
        2: "audio"
    }
    def createFields(self):
        yield Enum(UInt16(self, "type"), self.type_name)
        yield UInt16(self, "name_len", "Name length in character (byte=len*2)")
        if self["name_len"].value:
            yield String(self, "name", self["name_len"].value*2, "Name", charset="UTF-16-LE", strip=" \0")
        yield UInt16(self, "desc_len", "Description length in character (byte=len*2)")
        if self["desc_len"].value:
            yield String(self, "desc", self["desc_len"].value*2, "Description", charset="UTF-16-LE", strip=" \0")
        yield UInt16(self, "info_len")
        if self["info_len"].value:
            yield RawBytes(self, "info", self["info_len"].value)

class CodecList(FieldSet):
    guid = "86D15240-311D-11D0-A3A4-00A0C90348F6"

    def createFields(self):
        yield GUID(self, "reserved[]")
        yield UInt32(self, "count")
        for index in xrange(self["count"].value):
            yield Codec(self, "codec[]")

class SimpleIndexEntry(FieldSet):
    """
    See SimpleIndex class.
    """
    def createFields(self):
        yield UInt32(self, "pckt_number")
        yield UInt16(self, "pckt_count")

class SimpleIndex(FieldSet):
    guid = "33000890-E5B1-11CF-89F4-00A0C90349CB"

    def createFields(self):
        yield GUID(self, "file_id")
        yield TimedeltaWin64(self, "entry_interval")
        yield UInt32(self, "max_pckt_count")
        yield UInt32(self, "entry_count")
        for index in xrange(self["entry_count"].value):
            yield SimpleIndexEntry(self, "entry[]")

class BitRate(FieldSet):
    """
    See BitRateList class.
    """
    def createFields(self):
        yield Bits(self, "stream_index", 7)
        yield PaddingBits(self, "reserved", 9)
        yield displayHandler(UInt32(self, "avg_bitrate"), humanBitRate)

class BitRateList(FieldSet):
    guid = "7BF875CE-468D-11D1-8D82-006097C9A2B2"

    def createFields(self):
        yield UInt16(self, "count")
        for index in xrange(self["count"].value):
            yield BitRate(self, "bit_rate[]")

class Data(FieldSet):
    guid = "75B22636-668E-11CF-A6D9-00AA0062CE6C"

    def createFields(self):
        yield GUID(self, "file_id")
        yield UInt64(self, "packet_count")
        yield PaddingBytes(self, "reserved", 2)
        size = (self.size - self.current_size) / 8
        yield RawBytes(self, "data", size)

class StreamProperty(FieldSet):
    guid = "B7DC0791-A9B7-11CF-8EE6-00C00C205365"
    def createFields(self):
        yield GUID(self, "type")
        yield GUID(self, "error_correction")
        yield UInt64(self, "time_offset")
        yield UInt32(self, "data_len")
        yield UInt32(self, "error_correct_len")
        yield Bits(self, "stream_index", 7)
        yield Bits(self, "reserved[]", 8)
        yield Bit(self, "encrypted", "Content is encrypted?")
        yield UInt32(self, "reserved[]")
        size = self["data_len"].value
        if size:
            tag = self["type"].value
            if tag in Object.TAG_INFO:
                name, parser = Object.TAG_INFO[tag][0:2]
                yield parser(self, name, size=size*8)
            else:
                yield RawBytes(self, "data", size)
        size = self["error_correct_len"].value
        if size:
            yield RawBytes(self, "error_correct", size)

class Object(FieldSet):
    # This list is converted to a dictionnary later where the key is the GUID
    TAG_INFO = (
        ("header", Header, "Header object"),
        ("file_prop", FileProperty, "File property"),
        ("header_ext", HeaderExtension, "Header extension"),
        ("codec_list", CodecList, "Codec list"),
        ("simple_index", SimpleIndex, "Simple index"),
        ("data", Data, "Data object"),
        ("stream_prop[]", StreamProperty, "Stream properties"),
        ("bit_rates", BitRateList, "Bit rate list"),
        ("ext_desc", ExtendedContentDescription, "Extended content description"),
        ("metadata", Metadata, "Metadata"),
        ("video_header", VideoHeader, "Video"),
        ("audio_header", AudioHeader, "Audio"),
        ("bitrate_mutex", BitrateMutualExclusion, "Bitrate mutual exclusion"),
    )

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)

        tag = self["guid"].value
        if tag not in self.TAG_INFO:
            self.handler = None
            return
        info = self.TAG_INFO[tag]
        self._name = info[0]
        self.handler = info[1]

    def createFields(self):
        yield GUID(self, "guid")
        yield filesizeHandler(UInt64(self, "size"))

        size = self["size"].value - self.current_size/8
        if 0 < size:
            if self.handler:
                yield self.handler(self, "content", size=size*8)
            else:
                yield RawBytes(self, "content", size)

tag_info_list = Object.TAG_INFO
Object.TAG_INFO = dict( (parser[1].guid, parser) for parser in tag_info_list )

class AsfFile(Parser):
    MAGIC = "\x30\x26\xB2\x75\x8E\x66\xCF\x11\xA6\xD9\x00\xAA\x00\x62\xCE\x6C"
    PARSER_TAGS = {
        "id": "asf",
        "category": "video",
        "file_ext": ("wmv", "wma", "asf"),
        "mime": (u"video/x-ms-asf", u"video/x-ms-wmv", u"audio/x-ms-wma"),
        "min_size": 24*8,
        "description": "Advanced Streaming Format (ASF), used for WMV (video) and WMA (audio)",
        "magic": ((MAGIC, 0),),
    }
    FILE_TYPE = {
        "video/x-ms-wmv": (".wmv", u"Window Media Video (wmv)"),
        "video/x-ms-asf": (".asf", u"ASF container"),
        "audio/x-ms-wma": (".wma", u"Window Media Audio (wma)"),
    }
    endian = LITTLE_ENDIAN

    def validate(self):
        magic = self.MAGIC
        if self.stream.readBytes(0, len(magic)) != magic:
            return "Invalid magic"
        header = self[0]
        if not(30 <= header["size"].value  <= MAX_HEADER_SIZE):
            return "Invalid header size (%u)" % header["size"].value
        return True

    def createMimeType(self):
        audio = False
        for prop in self.array("header/content/stream_prop"):
            guid = prop["content/type"].value
            if guid == VideoHeader.guid:
                return u"video/x-ms-wmv"
            if guid == AudioHeader.guid:
                audio = True
        if audio:
            return u"audio/x-ms-wma"
        else:
            return u"video/x-ms-asf"

    def createFields(self):
        while not self.eof:
            yield Object(self, "object[]")

    def createDescription(self):
        return self.FILE_TYPE[self.mime_type][1]

    def createFilenameSuffix(self):
        return self.FILE_TYPE[self.mime_type][0]

    def createContentSize(self):
        if self[0].name != "header":
            return None
        return self["header/content/file_prop/content/file_size"].value * 8


########NEW FILE########
__FILENAME__ = flv
"""
FLV video parser.

Documentation:

 - FLV File format: http://osflash.org/flv
 - libavformat from ffmpeg project
 - flashticle: Python project to read Flash (SWF and FLV with AMF metadata)
   http://undefined.org/python/#flashticle

Author: Victor Stinner
Creation date: 4 november 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet,
    UInt8, UInt24, UInt32, NullBits, NullBytes,
    Bit, Bits, String, RawBytes, Enum)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_parser.audio.mpeg_audio import Frame
from lib.hachoir_parser.video.amf import AMFObject
from lib.hachoir_core.tools import createDict

SAMPLING_RATE = {
    0: ( 5512, "5.5 kHz"),
    1: (11025, "11 kHz"),
    2: (22050, "22.1 kHz"),
    3: (44100, "44.1 kHz"),
}
SAMPLING_RATE_VALUE = createDict(SAMPLING_RATE, 0)
SAMPLING_RATE_TEXT = createDict(SAMPLING_RATE, 1)

AUDIO_CODEC_MP3 = 2
AUDIO_CODEC_NAME = {
    0: u"Uncompressed",
    1: u"ADPCM",
    2: u"MP3",
    5: u"Nellymoser 8kHz mono",
    6: u"Nellymoser",
}

VIDEO_CODEC_NAME = {
    2: u"Sorensen H.263",
    3: u"Screen video",
    4: u"On2 VP6",
}

FRAME_TYPE = {
    1: u"keyframe",
    2: u"inter frame",
    3: u"disposable inter frame",
}

class Header(FieldSet):
    def createFields(self):
        yield String(self, "signature", 3, "FLV format signature", charset="ASCII")
        yield UInt8(self, "version")

        yield NullBits(self, "reserved[]", 5)
        yield Bit(self, "type_flags_audio")
        yield NullBits(self, "reserved[]", 1)
        yield Bit(self, "type_flags_video")

        yield UInt32(self, "data_offset")

def parseAudio(parent, size):
    yield Enum(Bits(parent, "codec", 4, "Audio codec"), AUDIO_CODEC_NAME)
    yield Enum(Bits(parent, "sampling_rate", 2, "Sampling rate"), SAMPLING_RATE_TEXT)
    yield Bit(parent, "is_16bit", "16-bit or 8-bit per sample")
    yield Bit(parent, "is_stereo", "Stereo or mono channel")

    size -= 1
    if 0 < size:
        if parent["codec"].value == AUDIO_CODEC_MP3 :
            yield Frame(parent, "music_data", size=size*8)
        else:
            yield RawBytes(parent, "music_data", size)

def parseVideo(parent, size):
    yield Enum(Bits(parent, "frame_type", 4, "Frame type"), FRAME_TYPE)
    yield Enum(Bits(parent, "codec", 4, "Video codec"), VIDEO_CODEC_NAME)
    if 1 < size:
        yield RawBytes(parent, "data", size-1)

def parseAMF(parent, size):
    while parent.current_size < parent.size:
        yield AMFObject(parent, "entry[]")

class Chunk(FieldSet):
    tag_info = {
         8: ("audio[]", parseAudio, ""),
         9: ("video[]", parseVideo, ""),
        18: ("metadata", parseAMF, ""),
    }

    def __init__(self, *args, **kw):
        FieldSet.__init__(self, *args, **kw)
        self._size = (11 + self["size"].value) * 8
        tag = self["tag"].value
        if tag in self.tag_info:
            self._name, self.parser, self._description = self.tag_info[tag]
        else:
            self.parser = None

    def createFields(self):
        yield UInt8(self, "tag")
        yield UInt24(self, "size", "Content size")
        yield UInt24(self, "timestamp", "Timestamp in millisecond")
        yield NullBytes(self, "reserved", 4)
        size = self["size"].value
        if size:
            if self.parser:
                for field in self.parser(self, size):
                    yield field
            else:
                yield RawBytes(self, "content", size)

    def getSampleRate(self):
        try:
            return SAMPLING_RATE_VALUE[self["sampling_rate"].value]
        except LookupError:
            return None

class FlvFile(Parser):
    PARSER_TAGS = {
        "id": "flv",
        "category": "video",
        "file_ext": ("flv",),
        "mime": (u"video/x-flv",),
        "min_size": 9*4,
        "magic": (
            # Signature, version=1, flags=5 (video+audio), header size=9
            ("FLV\1\x05\0\0\0\x09", 0),
            # Signature, version=1, flags=5 (video), header size=9
            ("FLV\1\x01\0\0\0\x09", 0),
        ),
        "description": u"Macromedia Flash video"
    }
    endian = BIG_ENDIAN

    def validate(self):
        if self.stream.readBytes(0, 3) != "FLV":
            return "Wrong file signature"
        if self["header/data_offset"].value != 9:
            return "Unknown data offset in main header"
        return True

    def createFields(self):
        yield Header(self, "header")
        yield UInt32(self, "prev_size[]", "Size of previous chunk")
        while not self.eof:
            yield Chunk(self, "chunk[]")
            yield UInt32(self, "prev_size[]", "Size of previous chunk")

    def createDescription(self):
        return u"Macromedia Flash video version %s" % self["header/version"].value


########NEW FILE########
__FILENAME__ = fourcc
#
# fourcc are codes to specify the encoding method a audio or video string
# in RIFF file (.avi and .wav).
#
# The following lists come from mmpython project:
#    file: mmpython/video/fourcc.py
#    url:  http://sourceforge.net/projects/mmpython/
#

# List of codecs with no compression (compression rate=1.0)
UNCOMPRESSED_AUDIO = set((1,3,6))

audio_codec_name = {
0x0000: u'Microsoft Unknown Wave Format',
0x0001: u'Microsoft Pulse Code Modulation (PCM)',
0x0002: u'Microsoft ADPCM',
0x0003: u'IEEE Float',
0x0004: u'Compaq Computer VSELP',
0x0005: u'IBM CVSD',
0x0006: u'Microsoft A-Law',
0x0007: u'Microsoft mu-Law',
0x0010: u'OKI ADPCM',
0x0011: u'Intel DVI/IMA ADPCM',
0x0012: u'Videologic MediaSpace ADPCM',
0x0013: u'Sierra Semiconductor ADPCM',
0x0014: u'Antex Electronics G.723 ADPCM',
0x0015: u'DSP Solutions DigiSTD',
0x0016: u'DSP Solutions DigiFIX',
0x0017: u'Dialogic OKI ADPCM',
0x0018: u'MediaVision ADPCM',
0x0019: u'Hewlett-Packard CU',
0x0020: u'Yamaha ADPCM',
0x0021: u'Speech Compression Sonarc',
0x0022: u'DSP Group TrueSpeech',
0x0023: u'Echo Speech EchoSC1',
0x0024: u'Audiofile AF36',
0x0025: u'Audio Processing Technology APTX',
0x0026: u'AudioFile AF10',
0x0027: u'Prosody 1612',
0x0028: u'LRC',
0x0030: u'Dolby AC2',
0x0031: u'Microsoft GSM 6.10',
0x0032: u'MSNAudio',
0x0033: u'Antex Electronics ADPCME',
0x0034: u'Control Resources VQLPC',
0x0035: u'DSP Solutions DigiREAL',
0x0036: u'DSP Solutions DigiADPCM',
0x0037: u'Control Resources CR10',
0x0038: u'Natural MicroSystems VBXADPCM',
0x0039: u'Crystal Semiconductor IMA ADPCM',
0x003A: u'EchoSC3',
0x003B: u'Rockwell ADPCM',
0x003C: u'Rockwell Digit LK',
0x003D: u'Xebec',
0x0040: u'Antex Electronics G.721 ADPCM',
0x0041: u'G.728 CELP',
0x0042: u'MSG723',
0x0050: u'Microsoft MPEG',
0x0052: u'RT24',
0x0053: u'PAC',
0x0055: u'MPEG Layer 3',
0x0059: u'Lucent G.723',
0x0060: u'Cirrus',
0x0061: u'ESPCM',
0x0062: u'Voxware',
0x0063: u'Canopus Atrac',
0x0064: u'G.726 ADPCM',
0x0065: u'G.722 ADPCM',
0x0066: u'DSAT',
0x0067: u'DSAT Display',
0x0069: u'Voxware Byte Aligned',
0x0070: u'Voxware AC8',
0x0071: u'Voxware AC10',
0x0072: u'Voxware AC16',
0x0073: u'Voxware AC20',
0x0074: u'Voxware MetaVoice',
0x0075: u'Voxware MetaSound',
0x0076: u'Voxware RT29HW',
0x0077: u'Voxware VR12',
0x0078: u'Voxware VR18',
0x0079: u'Voxware TQ40',
0x0080: u'Softsound',
0x0081: u'Voxware TQ60',
0x0082: u'MSRT24',
0x0083: u'G.729A',
0x0084: u'MVI MV12',
0x0085: u'DF G.726',
0x0086: u'DF GSM610',
0x0088: u'ISIAudio',
0x0089: u'Onlive',
0x0091: u'SBC24',
0x0092: u'Dolby AC3 SPDIF',
0x0097: u'ZyXEL ADPCM',
0x0098: u'Philips LPCBB',
0x0099: u'Packed',
0x0100: u'Rhetorex ADPCM',
0x0101: u'IBM mu-law',
0x0102: u'IBM A-law',
0x0103: u'IBM AVC Adaptive Differential Pulse Code Modulation (ADPCM)',
0x0111: u'Vivo G.723',
0x0112: u'Vivo Siren',
0x0123: u'Digital G.723',
0x0140: u'Windows Media Video V8',
0x0161: u'Windows Media Audio V7 / V8 / V9',
0x0162: u'Windows Media Audio Professional V9',
0x0163: u'Windows Media Audio Lossless V9',
0x0200: u'Creative Labs ADPCM',
0x0202: u'Creative Labs Fastspeech8',
0x0203: u'Creative Labs Fastspeech10',
0x0220: u'Quarterdeck',
0x0300: u'FM Towns Snd',
0x0300: u'Fujitsu FM Towns Snd',
0x0400: u'BTV Digital',
0x0680: u'VME VMPCM',
0x1000: u'Olivetti GSM',
0x1001: u'Olivetti ADPCM',
0x1002: u'Olivetti CELP',
0x1003: u'Olivetti SBC',
0x1004: u'Olivetti OPR',
0x1100: u'Lernout & Hauspie LH Codec',
0x1400: u'Norris',
0x1401: u'AT&T ISIAudio',
0x1500: u'Soundspace Music Compression',
0x2000: u'AC3',
0x7A21: u'GSM-AMR (CBR, no SID)',
0x7A22: u'GSM-AMR (VBR, including SID)',
0xFFFF: u'Development codec'
}

video_fourcc_name = {
'3IV1': u'3ivx v1',
'3IV2': u'3ivx v2',
'AASC': u'Autodesk Animator',
'ABYR': u'Kensington ?ABYR?',
'AEMI': u'Array VideoONE MPEG1-I Capture',
'AFLC': u'Autodesk Animator FLC',
'AFLI': u'Autodesk Animator FLI',
'AMPG': u'Array VideoONE MPEG',
'ANIM': u'Intel RDX (ANIM)',
'AP41': u'AngelPotion Definitive',
'ASV1': u'Asus Video v1',
'ASV2': u'Asus Video v2',
'ASVX': u'Asus Video 2.0 (audio)',
'AUR2': u'Aura 2 Codec - YUV 4:2:2',
'AURA': u'Aura 1 Codec - YUV 4:1:1',
'BINK': u'RAD Game Tools Bink Video',
'BT20': u'Conexant Prosumer Video',
'BTCV': u'Conexant Composite Video Codec',
'BW10': u'Data Translation Broadway MPEG Capture',
'CC12': u'Intel YUV12',
'CDVC': u'Canopus DV',
'CFCC': u'Digital Processing Systems DPS Perception',
'CGDI': u'Microsoft Office 97 Camcorder Video',
'CHAM': u'Winnov Caviara Champagne',
'CJPG': u'Creative WebCam JPEG',
'CLJR': u'Cirrus Logic YUV 4 pixels',
'CMYK': u'Common Data Format in Printing',
'CPLA': u'Weitek 4:2:0 YUV Planar',
'CRAM': u'Microsoft Video 1 (CRAM)',
'CVID': u'Radius Cinepak',
'CWLT': u'?CWLT?',
'CWLT': u'Microsoft Color WLT DIB',
'CYUV': u'Creative Labs YUV',
'CYUY': u'ATI YUV',
'D261': u'H.261',
'D263': u'H.263',
'DIV3': u'DivX v3 MPEG-4 Low-Motion',
'DIV4': u'DivX v3 MPEG-4 Fast-Motion',
'DIV5': u'?DIV5?',
'DIVX': u'DivX v4',
'divx': u'DivX',
'DMB1': u'Matrox Rainbow Runner hardware MJPEG',
'DMB2': u'Paradigm MJPEG',
'DSVD': u'?DSVD?',
'DUCK': u'Duck True Motion 1.0',
'DVAN': u'?DVAN?',
'DVE2': u'InSoft DVE-2 Videoconferencing',
'dvsd': u'DV',
'DVSD': u'DV',
'DVX1': u'DVX1000SP Video Decoder',
'DVX2': u'DVX2000S Video Decoder',
'DVX3': u'DVX3000S Video Decoder',
'DX50': u'DivX v5',
'DXT1': u'Microsoft DirectX Compressed Texture (DXT1)',
'DXT2': u'Microsoft DirectX Compressed Texture (DXT2)',
'DXT3': u'Microsoft DirectX Compressed Texture (DXT3)',
'DXT4': u'Microsoft DirectX Compressed Texture (DXT4)',
'DXT5': u'Microsoft DirectX Compressed Texture (DXT5)',
'DXTC': u'Microsoft DirectX Compressed Texture (DXTC)',
'EKQ0': u'Elsa ?EKQ0?',
'ELK0': u'Elsa ?ELK0?',
'ESCP': u'Eidos Escape',
'ETV1': u'eTreppid Video ETV1',
'ETV2': u'eTreppid Video ETV2',
'ETVC': u'eTreppid Video ETVC',
'FLJP': u'D-Vision Field Encoded Motion JPEG',
'FRWA': u'SoftLab-Nsk Forward Motion JPEG w/ alpha channel',
'FRWD': u'SoftLab-Nsk Forward Motion JPEG',
'FVF1': u'Iterated Systems Fractal Video Frame',
'GLZW': u'Motion LZW (gabest@freemail.hu)',
'GPEG': u'Motion JPEG (gabest@freemail.hu)',
'GWLT': u'Microsoft Greyscale WLT DIB',
'H260': u'Intel ITU H.260 Videoconferencing',
'H261': u'Intel ITU H.261 Videoconferencing',
'H262': u'Intel ITU H.262 Videoconferencing',
'H263': u'Intel ITU H.263 Videoconferencing',
'H264': u'Intel ITU H.264 Videoconferencing',
'H265': u'Intel ITU H.265 Videoconferencing',
'H266': u'Intel ITU H.266 Videoconferencing',
'H267': u'Intel ITU H.267 Videoconferencing',
'H268': u'Intel ITU H.268 Videoconferencing',
'H269': u'Intel ITU H.269 Videoconferencing',
'HFYU': u'Huffman Lossless Codec',
'HMCR': u'Rendition Motion Compensation Format (HMCR)',
'HMRR': u'Rendition Motion Compensation Format (HMRR)',
'i263': u'Intel ITU H.263 Videoconferencing (i263)',
'I420': u'Intel Indeo 4',
'IAN ': u'Intel RDX',
'ICLB': u'InSoft CellB Videoconferencing',
'IGOR': u'Power DVD',
'IJPG': u'Intergraph JPEG',
'ILVC': u'Intel Layered Video',
'ILVR': u'ITU-T H.263+',
'IPDV': u'I-O Data Device Giga AVI DV Codec',
'IR21': u'Intel Indeo 2.1',
'IRAW': u'Intel YUV Uncompressed',
'IV30': u'Ligos Indeo 3.0',
'IV31': u'Ligos Indeo 3.1',
'IV32': u'Ligos Indeo 3.2',
'IV33': u'Ligos Indeo 3.3',
'IV34': u'Ligos Indeo 3.4',
'IV35': u'Ligos Indeo 3.5',
'IV36': u'Ligos Indeo 3.6',
'IV37': u'Ligos Indeo 3.7',
'IV38': u'Ligos Indeo 3.8',
'IV39': u'Ligos Indeo 3.9',
'IV40': u'Ligos Indeo Interactive 4.0',
'IV41': u'Ligos Indeo Interactive 4.1',
'IV42': u'Ligos Indeo Interactive 4.2',
'IV43': u'Ligos Indeo Interactive 4.3',
'IV44': u'Ligos Indeo Interactive 4.4',
'IV45': u'Ligos Indeo Interactive 4.5',
'IV46': u'Ligos Indeo Interactive 4.6',
'IV47': u'Ligos Indeo Interactive 4.7',
'IV48': u'Ligos Indeo Interactive 4.8',
'IV49': u'Ligos Indeo Interactive 4.9',
'IV50': u'Ligos Indeo Interactive 5.0',
'JBYR': u'Kensington ?JBYR?',
'JPEG': u'Still Image JPEG DIB',
'JPGL': u'Webcam JPEG Light?',
'KMVC': u'Karl Morton\'s Video Codec',
'LEAD': u'LEAD Video Codec',
'Ljpg': u'LEAD MJPEG Codec',
'M261': u'Microsoft H.261',
'M263': u'Microsoft H.263',
'M4S2': u'Microsoft MPEG-4 (M4S2)',
'm4s2': u'Microsoft MPEG-4 (m4s2)',
'MC12': u'ATI Motion Compensation Format (MC12)',
'MCAM': u'ATI Motion Compensation Format (MCAM)',
'MJ2C': u'Morgan Multimedia Motion JPEG2000',
'mJPG': u'IBM Motion JPEG w/ Huffman Tables',
'MJPG': u'Motion JPEG DIB',
'MP42': u'Microsoft MPEG-4 (low-motion)',
'MP43': u'Microsoft MPEG-4 (fast-motion)',
'MP4S': u'Microsoft MPEG-4 (MP4S)',
'mp4s': u'Microsoft MPEG-4 (mp4s)',
'MPEG': u'MPEG 1 Video I-Frame',
'MPG4': u'Microsoft MPEG-4 Video High Speed Compressor',
'MPGI': u'Sigma Designs MPEG',
'MRCA': u'FAST Multimedia Mrcodec',
'MRCA': u'Martin Regen Codec',
'MRLE': u'Microsoft RLE',
'MRLE': u'Run Length Encoding',
'MSVC': u'Microsoft Video 1',
'MTX1': u'Matrox ?MTX1?',
'MTX2': u'Matrox ?MTX2?',
'MTX3': u'Matrox ?MTX3?',
'MTX4': u'Matrox ?MTX4?',
'MTX5': u'Matrox ?MTX5?',
'MTX6': u'Matrox ?MTX6?',
'MTX7': u'Matrox ?MTX7?',
'MTX8': u'Matrox ?MTX8?',
'MTX9': u'Matrox ?MTX9?',
'MV12': u'?MV12?',
'MWV1': u'Aware Motion Wavelets',
'nAVI': u'?nAVI?',
'NTN1': u'Nogatech Video Compression 1',
'NVS0': u'nVidia GeForce Texture (NVS0)',
'NVS1': u'nVidia GeForce Texture (NVS1)',
'NVS2': u'nVidia GeForce Texture (NVS2)',
'NVS3': u'nVidia GeForce Texture (NVS3)',
'NVS4': u'nVidia GeForce Texture (NVS4)',
'NVS5': u'nVidia GeForce Texture (NVS5)',
'NVT0': u'nVidia GeForce Texture (NVT0)',
'NVT1': u'nVidia GeForce Texture (NVT1)',
'NVT2': u'nVidia GeForce Texture (NVT2)',
'NVT3': u'nVidia GeForce Texture (NVT3)',
'NVT4': u'nVidia GeForce Texture (NVT4)',
'NVT5': u'nVidia GeForce Texture (NVT5)',
'PDVC': u'I-O Data Device Digital Video Capture DV codec',
'PGVV': u'Radius Video Vision',
'PHMO': u'IBM Photomotion',
'PIM1': u'Pegasus Imaging ?PIM1?',
'PIM2': u'Pegasus Imaging ?PIM2?',
'PIMJ': u'Pegasus Imaging Lossless JPEG',
'PVEZ': u'Horizons Technology PowerEZ',
'PVMM': u'PacketVideo Corporation MPEG-4',
'PVW2': u'Pegasus Imaging Wavelet Compression',
'QPEG': u'Q-Team QPEG 1.0',
'qpeq': u'Q-Team QPEG 1.1',
'RGBT': u'Computer Concepts 32-bit support',
'RLE ': u'Microsoft Run Length Encoder',
'RLE4': u'Run Length Encoded 4',
'RLE8': u'Run Length Encoded 8',
'RT21': u'Intel Indeo 2.1',
'RT21': u'Intel Real Time Video 2.1',
'rv20': u'RealVideo G2',
'rv30': u'RealVideo 8',
'RVX ': u'Intel RDX (RVX )',
's422': u'Tekram VideoCap C210 YUV 4:2:2',
'SDCC': u'Sun Communication Digital Camera Codec',
'SFMC': u'CrystalNet Surface Fitting Method',
'SMSC': u'Radius SMSC',
'SMSD': u'Radius SMSD',
'smsv': u'WorldConnect Wavelet Video',
'SPIG': u'Radius Spigot',
'SPLC': u'Splash Studios ACM Audio Codec',
'SQZ2': u'Microsoft VXTreme Video Codec V2',
'STVA': u'ST CMOS Imager Data (Bayer)',
'STVB': u'ST CMOS Imager Data (Nudged Bayer)',
'STVC': u'ST CMOS Imager Data (Bunched)',
'STVX': u'ST CMOS Imager Data (Extended CODEC Data Format)',
'STVY': u'ST CMOS Imager Data (Extended CODEC Data Format with Correction Data)',
'SV10': u'Sorenson Video R1',
'SVQ1': u'Sorenson Video',
'SVQ1': u'Sorenson Video R3',
'TLMS': u'TeraLogic Motion Intraframe Codec (TLMS)',
'TLST': u'TeraLogic Motion Intraframe Codec (TLST)',
'TM20': u'Duck TrueMotion 2.0',
'TM2X': u'Duck TrueMotion 2X',
'TMIC': u'TeraLogic Motion Intraframe Codec (TMIC)',
'TMOT': u'Horizons Technology TrueMotion S',
'tmot': u'Horizons TrueMotion Video Compression',
'TR20': u'Duck TrueMotion RealTime 2.0',
'TSCC': u'TechSmith Screen Capture Codec',
'TV10': u'Tecomac Low-Bit Rate Codec',
'TY0N': u'Trident ?TY0N?',
'TY2C': u'Trident ?TY2C?',
'TY2N': u'Trident ?TY2N?',
'UCOD': u'eMajix.com ClearVideo',
'ULTI': u'IBM Ultimotion',
'UYVY': u'UYVY 4:2:2 byte ordering',
'V261': u'Lucent VX2000S',
'V422': u'24 bit YUV 4:2:2 Format',
'V655': u'16 bit YUV 4:2:2 Format',
'VCR1': u'ATI VCR 1.0',
'VCR2': u'ATI VCR 2.0',
'VCR3': u'ATI VCR 3.0',
'VCR4': u'ATI VCR 4.0',
'VCR5': u'ATI VCR 5.0',
'VCR6': u'ATI VCR 6.0',
'VCR7': u'ATI VCR 7.0',
'VCR8': u'ATI VCR 8.0',
'VCR9': u'ATI VCR 9.0',
'VDCT': u'Video Maker Pro DIB',
'VDOM': u'VDOnet VDOWave',
'VDOW': u'VDOnet VDOLive (H.263)',
'VDTZ': u'Darim Vison VideoTizer YUV',
'VGPX': u'VGPixel Codec',
'VIDS': u'Vitec Multimedia YUV 4:2:2 CCIR 601 for V422',
'VIDS': u'YUV 4:2:2 CCIR 601 for V422',
'VIFP': u'?VIFP?',
'VIVO': u'Vivo H.263 v2.00',
'vivo': u'Vivo H.263',
'VIXL': u'Miro Video XL',
'VLV1': u'Videologic VLCAP.DRV',
'VP30': u'On2 VP3.0',
'VP31': u'On2 VP3.1',
'VX1K': u'VX1000S Video Codec',
'VX2K': u'VX2000S Video Codec',
'VXSP': u'VX1000SP Video Codec',
'WBVC': u'Winbond W9960',
'WHAM': u'Microsoft Video 1 (WHAM)',
'WINX': u'Winnov Software Compression',
'WJPG': u'AverMedia Winbond JPEG',
'WMV1': u'Windows Media Video V7',
'WMV2': u'Windows Media Video V8',
'WMV3': u'Windows Media Video V9',
'WNV1': u'Winnov Hardware Compression',
'x263': u'Xirlink H.263',
'XLV0': u'NetXL Video Decoder',
'XMPG': u'Xing MPEG (I-Frame only)',
'XVID': u'XviD MPEG-4',
'XXAN': u'?XXAN?',
'Y211': u'YUV 2:1:1 Packed',
'Y411': u'YUV 4:1:1 Packed',
'Y41B': u'YUV 4:1:1 Planar',
'Y41P': u'PC1 4:1:1',
'Y41T': u'PC1 4:1:1 with transparency',
'Y42B': u'YUV 4:2:2 Planar',
'Y42T': u'PCI 4:2:2 with transparency',
'Y8  ': u'Grayscale video',
'YC12': u'Intel YUV 12 codec',
'YC12': u'Intel YUV12 Codec',
'YUV8': u'Winnov Caviar YUV8',
'YUV9': u'Intel YUV9',
'YUY2': u'Uncompressed YUV 4:2:2',
'YUYV': u'Canopus YUV',
'YV12': u'YVU12 Planar',
'YVU9': u'Intel YVU9 Planar',
'YVYU': u'YVYU 4:2:2 byte ordering',
'ZLIB': u'?ZLIB?',
'ZPEG': u'Metheus Video Zipper'
}


########NEW FILE########
__FILENAME__ = mov
"""
Apple Quicktime Movie (file extension ".mov") parser.

Documents:
- Parsing and Writing QuickTime Files in Java (by Chris Adamson, 02/19/2003)
  http://www.onjava.com/pub/a/onjava/2003/02/19/qt_file_format.html
- QuickTime File Format (official technical reference)
  http://developer.apple.com/documentation/QuickTime/QTFF/qtff.pdf
- Apple QuickTime:
  http://wiki.multimedia.cx/index.php?title=Apple_QuickTime
- File type (ftyp):
  http://www.ftyps.com/

Author: Victor Stinner
Creation: 2 august 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (ParserError, FieldSet, MissingField,
    UInt8, Int16, UInt16, UInt32, TimestampMac32,
    String, PascalString8, CString,
    RawBytes, PaddingBytes)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal

class QTFloat32(FieldSet):
    static_size = 32
    def createFields(self):
        yield Int16(self, "int_part")
        yield UInt16(self, "float_part")
    def createValue(self):
        return self["int_part"].value + float(self["float_part"].value) / 65535
    def createDescription(self):
        return str(self.value)

class AtomList(FieldSet):
    def createFields(self):
        while not self.eof:
            yield Atom(self, "atom[]")

class TrackHeader(FieldSet):
    def createFields(self):
        yield textHandler(UInt8(self, "version"), hexadecimal)

        # TODO: sum of :
        # TrackEnabled = 1;
        # TrackInMovie = 2;
        # TrackInPreview = 4;
        # TrackInPoster = 8
        yield RawBytes(self, "flags", 3)

        yield TimestampMac32(self, "creation_date")
        yield TimestampMac32(self, "lastmod_date")
        yield UInt32(self, "track_id")
        yield PaddingBytes(self, "reserved[]", 8)
        yield UInt32(self, "duration")
        yield PaddingBytes(self, "reserved[]", 8)
        yield Int16(self, "video_layer", "Middle is 0, negative in front")
        yield PaddingBytes(self, "other", 2)
        yield QTFloat32(self, "geom_a", "Width scale")
        yield QTFloat32(self, "geom_b", "Width rotate")
        yield QTFloat32(self, "geom_u", "Width angle")
        yield QTFloat32(self, "geom_c", "Height rotate")
        yield QTFloat32(self, "geom_d", "Height scale")
        yield QTFloat32(self, "geom_v", "Height angle")
        yield QTFloat32(self, "geom_x", "Position X")
        yield QTFloat32(self, "geom_y", "Position Y")
        yield QTFloat32(self, "geom_w", "Divider scale")
        yield QTFloat32(self, "frame_size_width")
        yield QTFloat32(self, "frame_size_height")

class HDLR(FieldSet):
    def createFields(self):
        yield textHandler(UInt8(self, "version"), hexadecimal)
        yield RawBytes(self, "flags", 3)
        yield String(self, "subtype", 8)
        yield String(self, "manufacturer", 4)
        yield UInt32(self, "res_flags")
        yield UInt32(self, "res_flags_mask")
        if self.root.is_mpeg4:
            yield CString(self, "name")
        else:
            yield PascalString8(self, "name")

class MediaHeader(FieldSet):
    def createFields(self):
        yield textHandler(UInt8(self, "version"), hexadecimal)
        yield RawBytes(self, "flags", 3)
        yield TimestampMac32(self, "creation_date")
        yield TimestampMac32(self, "lastmod_date")
        yield UInt32(self, "time_scale")
        yield UInt32(self, "duration")
        yield UInt16(self, "mac_lang")
        yield Int16(self, "quality")

class ELST(FieldSet):
    def createFields(self):
        yield textHandler(UInt8(self, "version"), hexadecimal)
        yield RawBytes(self, "flags", 3)
        yield UInt32(self, "nb_edits")
        yield UInt32(self, "length")
        yield UInt32(self, "start")
        yield QTFloat32(self, "playback_speed")

class Load(FieldSet):
    def createFields(self):
        yield UInt32(self, "start")
        yield UInt32(self, "length")
        yield UInt32(self, "flags") # PreloadAlways = 1 or TrackEnabledPreload = 2
        yield UInt32(self, "hints") # KeepInBuffer = 0x00000004; HighQuality = 0x00000100; SingleFieldVideo = 0x00100000

class MovieHeader(FieldSet):
    def createFields(self):
        yield textHandler(UInt8(self, "version"), hexadecimal)
        yield RawBytes(self, "flags", 3)
        yield TimestampMac32(self, "creation_date")
        yield TimestampMac32(self, "lastmod_date")
        yield UInt32(self, "time_scale")
        yield UInt32(self, "duration")
        yield QTFloat32(self, "play_speed")
        yield UInt16(self, "volume")
        yield PaddingBytes(self, "reserved[]", 10)
        yield QTFloat32(self, "geom_a", "Width scale")
        yield QTFloat32(self, "geom_b", "Width rotate")
        yield QTFloat32(self, "geom_u", "Width angle")
        yield QTFloat32(self, "geom_c", "Height rotate")
        yield QTFloat32(self, "geom_d", "Height scale")
        yield QTFloat32(self, "geom_v", "Height angle")
        yield QTFloat32(self, "geom_x", "Position X")
        yield QTFloat32(self, "geom_y", "Position Y")
        yield QTFloat32(self, "geom_w", "Divider scale")
        yield UInt32(self, "preview_start")
        yield UInt32(self, "preview_length")
        yield UInt32(self, "still_poster")
        yield UInt32(self, "sel_start")
        yield UInt32(self, "sel_length")
        yield UInt32(self, "current_time")
        yield UInt32(self, "next_track")

class FileType(FieldSet):
    def createFields(self):
        yield String(self, "brand", 4, "Major brand")
        yield UInt32(self, "version", "Version")
        while not self.eof:
            yield String(self, "compat_brand[]", 4, "Compatible brand")

class Atom(FieldSet):
    tag_info = {
        # TODO: Use dictionnary of dictionnary, like Matroska parser does
        # "elst" is a child of "edts", but not of "moov" for example
        "moov": (AtomList, "movie", "Movie"),
        "trak": (AtomList, "track", "Track"),
        "mdia": (AtomList, "media", "Media"),
        "edts": (AtomList, "edts", ""),
        "minf": (AtomList, "minf", ""),
        "stbl": (AtomList, "stbl", ""),
        "dinf": (AtomList, "dinf", ""),
        "elst": (ELST, "edts", ""),
        "tkhd": (TrackHeader, "track_hdr", "Track header"),
        "hdlr": (HDLR, "hdlr", ""),
        "mdhd": (MediaHeader, "media_hdr", "Media header"),
        "load": (Load, "load", ""),
        "mvhd": (MovieHeader, "movie_hdr", "Movie header"),
        "ftyp": (FileType, "file_type", "File type"),
    }
    tag_handler = [ item[0] for item in tag_info ]
    tag_desc = [ item[1] for item in tag_info ]

    def createFields(self):
        yield UInt32(self, "size")
        yield String(self, "tag", 4)
        size = self["size"].value
        if size == 1:
            raise ParserError("Extended size is not supported!")
            #yield UInt64(self, "size64")
            size = self["size64"].value
        elif size == 0:
            #size = (self.root.size - self.root.current_size - self.current_size) / 8
            if self._size is None:
                size = (self.parent.size - self.current_size) / 8 - 8
            else:
                size = (self.size - self.current_size) / 8
        else:
            size = size - 8
        if 0 < size:
            tag = self["tag"].value
            if tag in self.tag_info:
                handler, name, desc = self.tag_info[tag]
                yield handler(self, name, desc, size=size*8)
            else:
                yield RawBytes(self, "data", size)

    def createDescription(self):
        return "Atom: %s" % self["tag"].value

class MovFile(Parser):
    PARSER_TAGS = {
        "id": "mov",
        "category": "video",
        "file_ext": ("mov", "qt", "mp4", "m4v", "m4a", "m4p", "m4b"),
        "mime": (u"video/quicktime", u'video/mp4'),
        "min_size": 8*8,
        "magic": (("moov", 4*8),),
        "description": "Apple QuickTime movie"
    }
    BRANDS = {
        # File type brand => MIME type
        'mp41': u'video/mp4',
        'mp42': u'video/mp4',
    }
    endian = BIG_ENDIAN

    def __init__(self, *args, **kw):
        Parser.__init__(self, *args, **kw)
        self.is_mpeg4 = False

    def validate(self):
        # TODO: Write better code, erk!
        size = self.stream.readBits(0, 32, self.endian)
        if size < 8:
            return "Invalid first atom size"
        tag = self.stream.readBytes(4*8, 4)
        return tag in ("ftyp", "moov", "free")

    def createFields(self):
        while not self.eof:
            yield Atom(self, "atom[]")

    def createMimeType(self):
        first = self[0]
        try:
            # Read brands in the file type
            if first['tag'].value != "ftyp":
                return None
            file_type = first["file_type"]
            brand = file_type["brand"].value
            if brand in self.BRANDS:
                return self.BRANDS[brand]
            for field in file_type.array("compat_brand"):
                brand = field.value
                if brand in self.BRANDS:
                    return self.BRANDS[brand]
        except MissingField:
            pass
        return None


########NEW FILE########
__FILENAME__ = mpeg_ts
"""
MPEG-2 Transport Stream parser.

Documentation:
- MPEG-2 Transmission
  http://erg.abdn.ac.uk/research/future-net/digital-video/mpeg2-trans.html

Author: Victor Stinner
Creation date: 13 january 2007
"""

from lib.hachoir_parser import Parser
from lib.hachoir_core.field import (FieldSet, ParserError, MissingField,
    UInt8, Enum, Bit, Bits, RawBytes)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.text_handler import textHandler, hexadecimal

class Packet(FieldSet):
    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        if self["has_error"].value:
            self._size = 204*8
        else:
            self._size = 188*8

    PID = {
        0x0000: "Program Association Table (PAT)",
        0x0001: "Conditional Access Table (CAT)",
        # 0x0002..0x000f: reserved
        # 0x0010..0x1FFE: network PID, program map PID, elementary PID, etc.
        # TODO: Check above values
        #0x0044: "video",
        #0x0045: "audio",
        0x1FFF: "Null packet",
    }

    def createFields(self):
        yield textHandler(UInt8(self, "sync", 8), hexadecimal)
        if self["sync"].value != 0x47:
            raise ParserError("MPEG-2 TS: Invalid synchronization byte")
        yield Bit(self, "has_error")
        yield Bit(self, "payload_unit_start")
        yield Bit(self, "priority")
        yield Enum(textHandler(Bits(self, "pid", 13, "Program identifier"), hexadecimal), self.PID)
        yield Bits(self, "scrambling_control", 2)
        yield Bit(self, "has_adaptation")
        yield Bit(self, "has_payload")
        yield Bits(self, "counter", 4)
        yield RawBytes(self, "payload", 184)
        if self["has_error"].value:
            yield RawBytes(self, "error_correction", 16)

    def createDescription(self):
        text = "Packet: PID %s" % self["pid"].display
        if self["payload_unit_start"].value:
            text += ", start of payload"
        return text

    def isValid(self):
        if not self["has_payload"].value and not self["has_adaptation"].value:
            return u"No payload and no adaptation"
        pid = self["pid"].value
        if (0x0002 <= pid <= 0x000f) or (0x2000 <= pid):
            return u"Invalid program identifier (%s)" % self["pid"].display
        return ""

class MPEG_TS(Parser):
    PARSER_TAGS = {
        "id": "mpeg_ts",
        "category": "video",
        "file_ext": ("ts",),
        "min_size": 188*8,
        "description": u"MPEG-2 Transport Stream"
    }
    endian = BIG_ENDIAN

    def validate(self):
        sync = self.stream.searchBytes("\x47", 0, 204*8)
        if sync is None:
            return "Unable to find synchronization byte"
        for index in xrange(5):
            try:
                packet = self["packet[%u]" % index]
            except (ParserError, MissingField):
                if index and self.eof:
                    return True
                else:
                    return "Unable to get packet #%u" % index
            err = packet.isValid()
            if err:
                return "Packet #%u is invalid: %s" % (index, err)
        return True

    def createFields(self):
        sync = self.stream.searchBytes("\x47", 0, 204*8)
        if sync is None:
            raise ParserError("Unable to find synchronization byte")
        elif sync:
            yield RawBytes(self, "incomplete_packet", sync//8)
        while not self.eof:
            yield Packet(self, "packet[]")


########NEW FILE########
__FILENAME__ = mpeg_video
"""
Moving Picture Experts Group (MPEG) video version 1 and 2 parser.

Information:
- http://www.mpucoder.com/DVD/
- http://dvd.sourceforge.net/dvdinfo/
- http://www.mit.jyu.fi/mweber/leffakone/software/parsempegts/
- http://homepage.mac.com/rnc/EditMpegHeaderIFO.html
- http://standards.iso.org/ittf/PubliclyAvailableStandards/c025029_ISO_IEC_TR_11172-5_1998(E)_Software_Simulation.zip
    This is a sample encoder/decoder implementation for MPEG-1.

Author: Victor Stinner
Creation date: 15 september 2006
"""

from lib.hachoir_parser import Parser
from lib.hachoir_parser.audio.mpeg_audio import MpegAudioFile
from lib.hachoir_core.field import (FieldSet,
    FieldError, ParserError,
    Bit, Bits, Bytes, RawBits, PaddingBits, NullBits,
    UInt8, UInt16,
    RawBytes, PaddingBytes,
    Enum)
from lib.hachoir_core.endian import BIG_ENDIAN
from lib.hachoir_core.stream import StringInputStream
from lib.hachoir_core.text_handler import textHandler, hexadecimal

class FragmentGroup:
    def __init__(self, parser):
        self.items = []
        self.parser = parser
        self.args = {}

    def add(self, item):
        self.items.append(item)

    def createInputStream(self):
        # FIXME: Use lazy stream creation
        data = []
        for item in self.items:
            if 'rawdata' in item:
                data.append( item["rawdata"].value )
        data = "".join(data)

        # FIXME: Use smarter code to send arguments
        tags = {"class": self.parser, "args": self.args}
        tags = tags.iteritems()
        return StringInputStream(data, "<fragment group>", tags=tags)

class CustomFragment(FieldSet):
    def __init__(self, parent, name, size, parser, description=None, group=None):
        FieldSet.__init__(self, parent, name, description, size=size)
        if not group:
            group = FragmentGroup(parser)
        self.group = group
        self.group.add(self)

    def createFields(self):
        yield RawBytes(self, "rawdata", self.size//8)

    def _createInputStream(self, **args):
        return self.group.createInputStream()

class Timestamp(FieldSet):
    static_size = 36

    def createValue(self):
        return (self["c"].value << 30) + (self["b"].value << 15) + self["a"].value

    def createFields(self):
        yield Bits(self, "c", 3)
        yield Bit(self, "sync[]") # =True
        yield Bits(self, "b", 15)
        yield Bit(self, "sync[]") # =True
        yield Bits(self, "a", 15)
        yield Bit(self, "sync[]") # =True

class SCR(FieldSet):
    static_size = 35

    def createFields(self):
        yield Bits(self, "scr_a", 3)
        yield Bit(self, "sync[]") # =True
        yield Bits(self, "scr_b", 15)
        yield Bit(self, "sync[]") # =True
        yield Bits(self, "scr_c", 15)

class PackHeader(FieldSet):
    def createFields(self):
        if self.stream.readBits(self.absolute_address, 2, self.endian) == 1:
            # MPEG version 2
            yield Bits(self, "sync[]", 2)
            yield SCR(self, "scr")
            yield Bit(self, "sync[]")
            yield Bits(self, "scr_ext", 9)
            yield Bit(self, "sync[]")
            yield Bits(self, "mux_rate", 22)
            yield Bits(self, "sync[]", 2)
            yield PaddingBits(self, "reserved", 5, pattern=1)
            yield Bits(self, "stuffing_length", 3)
            count = self["stuffing_length"].value
            if count:
                yield PaddingBytes(self, "stuffing", count, pattern="\xff")
        else:
            # MPEG version 1
            yield Bits(self, "sync[]", 4)
            yield Bits(self, "scr_a", 3)
            yield Bit(self, "sync[]")
            yield Bits(self, "scr_b", 15)
            yield Bit(self, "sync[]")
            yield Bits(self, "scr_c", 15)
            yield Bits(self, "sync[]", 2)
            yield Bits(self, "mux_rate", 22)
            yield Bit(self, "sync[]")

    def validate(self):
        if self["mux_rate"].value == 0:
            return "Invalid mux rate"
        sync0 = self["sync[0]"]
        if (sync0.size == 2 and sync0.value == 1):
            # MPEG2
            pass
            if not self["sync[1]"].value \
            or not self["sync[2]"].value \
            or self["sync[3]"].value != 3:
                return "Invalid synchronisation bits"
        elif (sync0.size == 4 and sync0.value == 2):
            # MPEG1
            if not self["sync[1]"].value \
            or not self["sync[2]"].value \
            or self["sync[3]"].value != 3 \
            or not self["sync[4]"].value:
                return "Invalid synchronisation bits"
        else:
            return "Unknown version"
        return True

class SystemHeader(FieldSet):
    def createFields(self):
        yield Bits(self, "marker[]", 1)
        yield Bits(self, "rate_bound", 22)
        yield Bits(self, "marker[]", 1)
        yield Bits(self, "audio_bound", 6)
        yield Bit(self, "fixed_bitrate")
        yield Bit(self, "csps", description="Constrained system parameter stream")
        yield Bit(self, "audio_lock")
        yield Bit(self, "video_lock")
        yield Bits(self, "marker[]", 1)
        yield Bits(self, "video_bound", 5)
        length = self['../length'].value-5
        if length:
            yield RawBytes(self, "raw[]", length)

class defaultParser(FieldSet):
    def createFields(self):
        yield RawBytes(self, "data", self["../length"].value)

class Padding(FieldSet):
    def createFields(self):
        yield PaddingBytes(self, "data", self["../length"].value)

class VideoExtension2(FieldSet):
    def createFields(self):
        yield Bit(self, "sync[]") # =True
        yield Bits(self, "ext_length", 7)
        yield NullBits(self, "reserved[]", 8)
        size = self["ext_length"].value
        if size:
            yield RawBytes(self, "ext_bytes", size)

class VideoExtension1(FieldSet):
    def createFields(self):
        yield Bit(self, "has_private")
        yield Bit(self, "has_pack_lgth")
        yield Bit(self, "has_pack_seq")
        yield Bit(self, "has_pstd_buffer")
        yield Bits(self, "sync[]", 3) # =7
        yield Bit(self, "has_extension2")

        if self["has_private"].value:
            yield RawBytes(self, "private", 16)

        if self["has_pack_lgth"].value:
            yield UInt8(self, "pack_lgth")

        if self["has_pack_seq"].value:
            yield Bit(self, "sync[]") # =True
            yield Bits(self, "pack_seq_counter", 7)
            yield Bit(self, "sync[]") # =True
            yield Bit(self, "mpeg12_id")
            yield Bits(self, "orig_stuffing_length", 6)

        if self["has_pstd_buffer"].value:
            yield Bits(self, "sync[]", 2) # =1
            yield Enum(Bit(self, "pstd_buffer_scale"),
                {True: "128 bytes", False: "1024 bytes"})
            yield Bits(self, "pstd_size", 13)

class VideoSeqHeader(FieldSet):
    ASPECT=["forbidden", "1.0000 (VGA etc.)", "0.6735",
            "0.7031 (16:9, 625line)", "0.7615", "0.8055",
            "0.8437 (16:9, 525line)", "0.8935",
            "0.9157 (CCIR601, 625line)", "0.9815", "1.0255", "1.0695",
            "1.0950 (CCIR601, 525line)", "1.1575", "1.2015", "reserved"]
    FRAMERATE=["forbidden", "23.976 fps", "24 fps", "25 fps", "29.97 fps",
               "30 fps", "50 fps", "59.94 fps", "60 fps"]
    def createFields(self):
        yield Bits(self, "width", 12)
        yield Bits(self, "height", 12)
        yield Enum(Bits(self, "aspect", 4), self.ASPECT)
        yield Enum(Bits(self, "frame_rate", 4), self.FRAMERATE)
        yield Bits(self, "bit_rate", 18, "Bit rate in units of 50 bytes")
        yield Bits(self, "sync[]", 1) # =1
        yield Bits(self, "vbv_size", 10, "Video buffer verifier size, in units of 16768")
        yield Bit(self, "constrained_params_flag")
        yield Bit(self, "has_intra_quantizer")
        if self["has_intra_quantizer"].value:
            for i in range(64):
                yield Bits(self, "intra_quantizer[]", 8)
        yield Bit(self, "has_non_intra_quantizer")
        if self["has_non_intra_quantizer"].value:
            for i in range(64):
                yield Bits(self, "non_intra_quantizer[]", 8)

class GroupStart(FieldSet):
    def createFields(self):
        yield Bit(self, "drop_frame")
        yield Bits(self, "time_hh", 5)
        yield Bits(self, "time_mm", 6)
        yield PaddingBits(self, "time_pad[]", 1)
        yield Bits(self, "time_ss", 6)
        yield Bits(self, "time_ff", 6)
        yield Bit(self, "closed_group")
        yield Bit(self, "broken_group")
        yield PaddingBits(self, "pad[]", 5)

class PacketElement(FieldSet):
    def createFields(self):
        yield Bits(self, "sync[]", 2) # =2
        if self["sync[0]"].value != 2:
            raise ParserError("Unknown video elementary data")
        yield Bits(self, "is_scrambled", 2)
        yield Bits(self, "priority", 1)
        yield Bit(self, "alignment")
        yield Bit(self, "is_copyrighted")
        yield Bit(self, "is_original")
        yield Bit(self, "has_pts", "Presentation Time Stamp")
        yield Bit(self, "has_dts", "Decode Time Stamp")
        yield Bit(self, "has_escr", "Elementary Stream Clock Reference")
        yield Bit(self, "has_es_rate", "Elementary Stream rate")
        yield Bit(self, "dsm_trick_mode")
        yield Bit(self, "has_copy_info")
        yield Bit(self, "has_prev_crc", "If True, previous PES packet CRC follows")
        yield Bit(self, "has_extension")
        yield UInt8(self, "size")

        # Time stamps
        if self["has_pts"].value:
            yield Bits(self, "sync[]", 4) # =2, or 3 if has_dts=True
            yield Timestamp(self, "pts")
        if self["has_dts"].value:
            if not(self["has_pts"].value):
                raise ParserError("Invalid PTS/DTS values")
            yield Bits(self, "sync[]", 4) # =1
            yield Timestamp(self, "dts")

        if self["has_escr"].value:
            yield Bits(self, "sync[]", 2) # =0
            yield SCR(self, "escr")

        if self["has_es_rate"].value:
            yield Bit(self, "sync[]") # =True
            yield Bits(self, "es_rate", 14) # in units of 50 bytes/second
            yield Bit(self, "sync[]") # =True

        if self["has_copy_info"].value:
            yield Bit(self, "sync[]") # =True
            yield Bits(self, "copy_info", 7)

        if self["has_prev_crc"].value:
            yield textHandler(UInt16(self, "prev_crc"), hexadecimal)

        # --- Extension ---
        if self["has_extension"].value:
            yield VideoExtension1(self, "extension")
            if self["extension/has_extension2"].value:
                yield VideoExtension2(self, "extension2")

class VideoExtension(FieldSet):
    EXT_TYPE = {1:'Sequence',2:'Sequence Display',8:'Picture Coding'}
    def createFields(self):
        yield Enum(Bits(self, "ext_type", 4), self.EXT_TYPE)
        ext_type=self['ext_type'].value
        if ext_type==1:
            # Sequence extension
            yield Bits(self, 'profile_and_level', 8)
            yield Bit(self, 'progressive_sequence')
            yield Bits(self, 'chroma_format', 2)
            yield Bits(self, 'horiz_size_ext', 2)
            yield Bits(self, 'vert_size_ext', 2)
            yield Bits(self, 'bit_rate_ext', 12)
            yield Bits(self, 'pad[]', 1)
            yield Bits(self, 'vbv_buffer_size_ext', 8)
            yield Bit(self, 'low_delay')
            yield Bits(self, 'frame_rate_ext_n', 2)
            yield Bits(self, 'frame_rate_ext_d', 5)
        elif ext_type==2:
            # Sequence Display extension
            yield Bits(self, 'video_format', 3)
            yield Bit(self, 'color_desc_present')
            if self['color_desc_present'].value:
                yield UInt8(self, 'color_primaries')
                yield UInt8(self, 'transfer_characteristics')
                yield UInt8(self, 'matrix_coeffs')
            yield Bits(self, 'display_horiz_size', 14)
            yield Bits(self, 'pad[]', 1)
            yield Bits(self, 'display_vert_size', 14)
            yield NullBits(self, 'pad[]', 3)
        elif ext_type==8:
            yield Bits(self, 'f_code[0][0]', 4, description="forward horizontal")
            yield Bits(self, 'f_code[0][1]', 4, description="forward vertical")
            yield Bits(self, 'f_code[1][0]', 4, description="backward horizontal")
            yield Bits(self, 'f_code[1][1]', 4, description="backward vertical")
            yield Bits(self, 'intra_dc_precision', 2)
            yield Bits(self, 'picture_structure', 2)
            yield Bit(self, 'top_field_first')
            yield Bit(self, 'frame_pred_frame_dct')
            yield Bit(self, 'concealment_motion_vectors')
            yield Bit(self, 'q_scale_type')
            yield Bit(self, 'intra_vlc_format')
            yield Bit(self, 'alternate_scan')
            yield Bit(self, 'repeat_first_field')
            yield Bit(self, 'chroma_420_type')
            yield Bit(self, 'progressive_frame')
            yield Bit(self, 'composite_display')
            if self['composite_display'].value:
                yield Bit(self, 'v_axis')
                yield Bits(self, 'field_sequence', 3)
                yield Bit(self, 'sub_carrier')
                yield Bits(self, 'burst_amplitude', 7)
                yield Bits(self, 'sub_carrier_phase', 8)
                yield NullBits(self, 'pad[]', 2)
            else:
                yield NullBits(self, 'pad[]', 6)
        else:
            yield RawBits(self, "raw[]", 4)

class VideoPicture(FieldSet):
    CODING_TYPE = ["forbidden","intra-coded (I)",
                   "predictive-coded (P)",
                   "bidirectionally-predictive-coded (B)",
                   "dc intra-coded (D)", "reserved",
                   "reserved", "reserved"]
    def createFields(self):
        yield Bits(self, "temporal_ref", 10)
        yield Enum(Bits(self, "coding_type", 3), self.CODING_TYPE)
        yield Bits(self, "vbv_delay", 16)
        if self['coding_type'].value in (2,3):
            # predictive coding
            yield Bit(self, 'full_pel_fwd_vector')
            yield Bits(self, 'forward_f_code', 3)
        if self['coding_type'].value == 3:
            # bidi predictive coding
            yield Bit(self, 'full_pel_back_vector')
            yield Bits(self, 'backward_f_code', 3)
        yield Bits(self, "padding", 8-(self.current_size % 8))

class VideoSlice(FieldSet):
    def createFields(self):
        yield Bits(self, "quantizer_scale", 5)
        start=self.absolute_address+self.current_size+3
        pos=self.stream.searchBytes('\0\0\1',start,start+1024*1024*8) # seek forward by at most 1MB
        if pos is None: pos=self.root.size
        yield RawBits(self, "data", pos-start+3)

class VideoChunk(FieldSet):
    tag_info = {
        0x00: ("pict_start[]",   VideoPicture,  "Picture start"),
        0xB2: ("data_start[]",   None,          "Data start"),
        0xB3: ("seq_hdr[]",      VideoSeqHeader,"Sequence header"),
        0xB4: ("seq_err[]",      None,          "Sequence error"),
        0xB5: ("ext_start[]",    VideoExtension,"Extension start"),
        0xB7: ("seq_end[]",      None,          "Sequence end"),
        0xB8: ("group_start[]",  GroupStart,    "Group start"),
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        tag = self["tag"].value
        if tag in self.tag_info:
            self._name, self.parser, self._description = self.tag_info[tag]
            if not self.parser:
                self.parser = defaultParser
        elif 0x01 <= tag <= 0xaf:
            self._name, self.parser, self._description = ('slice[]', VideoSlice, 'Picture slice')
        else:
            self.parser = defaultParser

    def createFields(self):
        yield Bytes(self, "sync", 3)
        yield textHandler(UInt8(self, "tag"), hexadecimal)
        if self.parser and self['tag'].value != 0xb7:
            yield self.parser(self, "content")

class VideoStream(Parser):
    endian = BIG_ENDIAN
    def createFields(self):
        while self.current_size < self.size:
            pos=self.stream.searchBytes('\0\0\1',self.current_size,self.current_size+1024*1024*8) # seek forward by at most 1MB
            if pos is not None:
                padsize = pos-self.current_size
                if padsize:
                    yield PaddingBytes(self, "pad[]", padsize//8)
            yield VideoChunk(self, "chunk[]")

class Stream(FieldSet):
    def createFields(self):
        padding=0
        position=0
        while True:
            next=ord(self.parent.stream.readBytes(self.absolute_address+self.current_size+position, 1))
            if next == 0xff:
                padding+=1
                position+=8
            elif padding:
                yield PaddingBytes(self, "pad[]", padding)
                padding=None
                position=0
            elif 0x40 <= next <= 0x7f:
                yield Bits(self, "scale_marker", 2) # 1
                yield Bit(self, "scale")
                scale=self['scale'].value
                if scale:
                    scaleval=1024
                else:
                    scaleval=128
                yield textHandler(Bits(self, "size", 13), lambda field:str(field.value*scaleval))
            elif 0x00 <= next <= 0x3f:
                yield Bits(self, "ts_marker", 2) # 0
                yield Bit(self, "has_pts")
                yield Bit(self, "has_dts")
                if self['has_pts'].value:
                    yield Timestamp(self, "pts")
                if self['has_dts'].value:
                    yield PaddingBits(self, "pad[]", 4)
                    yield Timestamp(self, "dts")
                if self.current_size % 8 == 4:
                    yield PaddingBits(self, "pad[]", 4)
                break
            elif 0x80 <= next <= 0xbf:
                # MPEG-2 extension
                yield PacketElement(self, "pkt")
                break
            else:
                # 0xc0 - 0xfe: unknown
                break
        length = self["../length"].value - self.current_size//8
        if length:
            tag=self['../tag'].value
            group=self.root.streamgroups[tag]
            parname=self.parent._name
            if parname.startswith('audio'):
                frag = CustomFragment(self, "data", length*8, MpegAudioFile, group=group)
            elif parname.startswith('video'):
                frag = CustomFragment(self, "data", length*8, VideoStream, group=group)
            else:
                frag = CustomFragment(self, "data", length*8, None, group=group)
            self.root.streamgroups[tag]=frag.group
            yield frag

class Chunk(FieldSet):
    ISO_END_CODE = 0xB9
    tag_info = {
        0xB9: ("end",            None,          "End"),
        0xBA: ("pack_start[]",   PackHeader,    "Pack start"),
        0xBB: ("system_start[]", SystemHeader,  "System start"),
        # streams
        0xBD: ("private[]",      Stream,        "Private elementary"),
        0xBE: ("padding[]",      Stream,        "Padding"),
        # 0xC0 to 0xFE handled specially
        0xFF: ("directory[]",    Stream,        "Program Stream Directory"),
    }

    def __init__(self, *args):
        FieldSet.__init__(self, *args)
        if not hasattr(self.root,'streamgroups'):
            self.root.streamgroups={}
            for tag in range(0xBC, 0x100):
                self.root.streamgroups[tag]=None
        tag = self["tag"].value
        if tag in self.tag_info:
            self._name, self.parser, self._description = self.tag_info[tag]
        elif 0xBC <= tag <= 0xFF:
            if 0xC0 <= tag < 0xE0:
                # audio
                streamid = tag-0xC0
                self._name, self.parser, self._description = ("audio[%i][]"%streamid, Stream, "Audio Stream %i Packet"%streamid)
            elif 0xE0 <= tag < 0xF0:
                # video
                streamid = tag-0xE0
                self._name, self.parser, self._description = ("video[%i][]"%streamid, Stream, "Video Stream %i Packet"%streamid)
            else:
                self._name, self.parser, self._description = ("stream[]", Stream, "Data Stream Packet")
        else:
            self.parser = defaultParser
        
        if not self.parser:
            self.parser = defaultParser
        elif self.parser != PackHeader and "length" in self:
            self._size = (6 + self["length"].value) * 8

    def createFields(self):
        yield Bytes(self, "sync", 3)
        yield textHandler(UInt8(self, "tag"), hexadecimal)
        if self.parser:
            if self.parser != PackHeader:
                yield UInt16(self, "length")
                if not self["length"].value:
                    return
            yield self.parser(self, "content")

    def createDescription(self):
        return "Chunk: tag %s" % self["tag"].display

class MPEGVideoFile(Parser):
    PARSER_TAGS = {
        "id": "mpeg_video",
        "category": "video",
        "file_ext": ("mpeg", "mpg", "mpe", "vob"),
        "mime": (u"video/mpeg", u"video/mp2p"),
        "min_size": 12*8,
#TODO:        "magic": xxx,
        "description": "MPEG video, version 1 or 2"
    }
    endian = BIG_ENDIAN
    version = None

    def createFields(self):
        while self.current_size < self.size:
            pos=self.stream.searchBytes('\0\0\1',self.current_size,self.current_size+1024*1024*8) # seek forward by at most 1MB
            if pos is not None:
                padsize = pos-self.current_size
                if padsize:
                    yield PaddingBytes(self, "pad[]", padsize//8)
            chunk=Chunk(self, "chunk[]")
            try:
                # force chunk to be processed, so that CustomFragments are complete
                chunk['content/data']
            except: pass
            yield chunk

    def validate(self):
        try:
            pack = self[0]
        except FieldError:
            return "Unable to create first chunk"
        if pack.name != "pack_start[0]":
            return "Invalid first chunk"
        if pack["sync"].value != "\0\0\1":
            return "Invalid synchronisation"
        return pack["content"].validate()

    def getVersion(self):
        if not self.version:
            if self["pack_start[0]/content/sync[0]"].size == 2:
                self.version = 2
            else:
                self.version = 1
        return self.version

    def createDescription(self):
        if self.getVersion() == 2:
            return "MPEG-2 video"
        else:
            return "MPEG-1 video"


########NEW FILE########
__FILENAME__ = iri2uri
"""
iri2uri

Converts an IRI to a URI.

"""
__author__ = "Joe Gregorio (joe@bitworking.org)"
__copyright__ = "Copyright 2006, Joe Gregorio"
__contributors__ = []
__version__ = "1.0.0"
__license__ = "MIT"
__history__ = """
"""

import urlparse


# Convert an IRI to a URI following the rules in RFC 3987
# 
# The characters we need to enocde and escape are defined in the spec:
#
# iprivate =  %xE000-F8FF / %xF0000-FFFFD / %x100000-10FFFD
# ucschar = %xA0-D7FF / %xF900-FDCF / %xFDF0-FFEF
#         / %x10000-1FFFD / %x20000-2FFFD / %x30000-3FFFD
#         / %x40000-4FFFD / %x50000-5FFFD / %x60000-6FFFD
#         / %x70000-7FFFD / %x80000-8FFFD / %x90000-9FFFD
#         / %xA0000-AFFFD / %xB0000-BFFFD / %xC0000-CFFFD
#         / %xD0000-DFFFD / %xE1000-EFFFD

escape_range = [
   (0xA0, 0xD7FF ),
   (0xE000, 0xF8FF ),
   (0xF900, 0xFDCF ),
   (0xFDF0, 0xFFEF),
   (0x10000, 0x1FFFD ),
   (0x20000, 0x2FFFD ),
   (0x30000, 0x3FFFD),
   (0x40000, 0x4FFFD ),
   (0x50000, 0x5FFFD ),
   (0x60000, 0x6FFFD),
   (0x70000, 0x7FFFD ),
   (0x80000, 0x8FFFD ),
   (0x90000, 0x9FFFD),
   (0xA0000, 0xAFFFD ),
   (0xB0000, 0xBFFFD ),
   (0xC0000, 0xCFFFD),
   (0xD0000, 0xDFFFD ),
   (0xE1000, 0xEFFFD),
   (0xF0000, 0xFFFFD ),
   (0x100000, 0x10FFFD)
]
 
def encode(c):
    retval = c
    i = ord(c)
    for low, high in escape_range:
        if i < low:
            break
        if i >= low and i <= high:
            retval = "".join(["%%%2X" % ord(o) for o in c.encode('utf-8')])
            break
    return retval


def iri2uri(uri):
    """Convert an IRI to a URI. Note that IRIs must be 
    passed in a unicode strings. That is, do not utf-8 encode
    the IRI before passing it into the function.""" 
    if isinstance(uri ,unicode):
        (scheme, authority, path, query, fragment) = urlparse.urlsplit(uri)
        authority = authority.encode('idna')
        # For each character in 'ucschar' or 'iprivate'
        #  1. encode as utf-8
        #  2. then %-encode each octet of that utf-8 
        uri = urlparse.urlunsplit((scheme, authority, path, query, fragment))
        uri = "".join([encode(c) for c in uri])
    return uri
        
if __name__ == "__main__":
    import unittest

    class Test(unittest.TestCase):

        def test_uris(self):
            """Test that URIs are invariant under the transformation."""
            invariant = [ 
                u"ftp://ftp.is.co.za/rfc/rfc1808.txt",
                u"http://www.ietf.org/rfc/rfc2396.txt",
                u"ldap://[2001:db8::7]/c=GB?objectClass?one",
                u"mailto:John.Doe@example.com",
                u"news:comp.infosystems.www.servers.unix",
                u"tel:+1-816-555-1212",
                u"telnet://192.0.2.16:80/",
                u"urn:oasis:names:specification:docbook:dtd:xml:4.1.2" ]
            for uri in invariant:
                self.assertEqual(uri, iri2uri(uri))
            
        def test_iri(self):
            """ Test that the right type of escaping is done for each part of the URI."""
            self.assertEqual("http://xn--o3h.com/%E2%98%84", iri2uri(u"http://\N{COMET}.com/\N{COMET}"))
            self.assertEqual("http://bitworking.org/?fred=%E2%98%84", iri2uri(u"http://bitworking.org/?fred=\N{COMET}"))
            self.assertEqual("http://bitworking.org/#%E2%98%84", iri2uri(u"http://bitworking.org/#\N{COMET}"))
            self.assertEqual("#%E2%98%84", iri2uri(u"#\N{COMET}"))
            self.assertEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(u"/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}"))
            self.assertEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(iri2uri(u"/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}")))
            self.assertNotEqual("/fred?bar=%E2%98%9A#%E2%98%84", iri2uri(u"/fred?bar=\N{BLACK LEFT POINTING INDEX}#\N{COMET}".encode('utf-8')))

    unittest.main()

    

########NEW FILE########
__FILENAME__ = config
import sys

class LocalClasses(dict):
    def add(self, cls):
        self[cls.__name__] = cls

class Config(object):
    """
    This is pretty much used exclusively for the 'jsonclass' 
    functionality... set use_jsonclass to False to turn it off.
    You can change serialize_method and ignore_attribute, or use
    the local_classes.add(class) to include "local" classes.
    """
    use_jsonclass = True
    # Change to False to keep __jsonclass__ entries raw.
    serialize_method = '_serialize'
    # The serialize_method should be a string that references the
    # method on a custom class object which is responsible for 
    # returning a tuple of the constructor arguments and a dict of
    # attributes.
    ignore_attribute = '_ignore'
    # The ignore attribute should be a string that references the
    # attribute on a custom class object which holds strings and / or
    # references of the attributes the class translator should ignore.
    classes = LocalClasses()
    # The list of classes to use for jsonclass translation.
    version = 2.0
    # Version of the JSON-RPC spec to support
    user_agent = 'jsonrpclib/0.1 (Python %s)' % \
        '.'.join([str(ver) for ver in sys.version_info[0:3]])
    # User agent to use for calls.
    _instance = None
    
    @classmethod
    def instance(cls):
        if not cls._instance:
            cls._instance = cls()
        return cls._instance

########NEW FILE########
__FILENAME__ = history
class History(object):
    """
    This holds all the response and request objects for a
    session. A server using this should call "clear" after
    each request cycle in order to keep it from clogging 
    memory.
    """
    requests = []
    responses = []
    _instance = None
    
    @classmethod
    def instance(cls):
        if not cls._instance:
            cls._instance = cls()
        return cls._instance

    def add_response(self, response_obj):
        self.responses.append(response_obj)
    
    def add_request(self, request_obj):
        self.requests.append(request_obj)

    @property
    def request(self):
        if len(self.requests) == 0:
            return None
        else:
            return self.requests[-1]

    @property
    def response(self):
        if len(self.responses) == 0:
            return None
        else:
            return self.responses[-1]

    def clear(self):
        del self.requests[:]
        del self.responses[:]

########NEW FILE########
__FILENAME__ = jsonclass
import types
import inspect
import re
import traceback

from lib.jsonrpclib import config

iter_types = [
    types.DictType,
    types.ListType,
    types.TupleType
]

string_types = [
    types.StringType,
    types.UnicodeType
]

numeric_types = [
    types.IntType,
    types.LongType,
    types.FloatType
]

value_types = [
    types.BooleanType,
    types.NoneType
]

supported_types = iter_types+string_types+numeric_types+value_types
invalid_module_chars = r'[^a-zA-Z0-9\_\.]'

class TranslationError(Exception):
    pass

def dump(obj, serialize_method=None, ignore_attribute=None, ignore=[]):
    if not serialize_method:
        serialize_method = config.serialize_method
    if not ignore_attribute:
        ignore_attribute = config.ignore_attribute
    obj_type = type(obj)
    # Parse / return default "types"...
    if obj_type in numeric_types+string_types+value_types:
        return obj
    if obj_type in iter_types:
        if obj_type in (types.ListType, types.TupleType):
            new_obj = []
            for item in obj:
                new_obj.append(dump(item, serialize_method,
                                     ignore_attribute, ignore))
            if obj_type is types.TupleType:
                new_obj = tuple(new_obj)
            return new_obj
        # It's a dict...
        else:
            new_obj = {}
            for key, value in obj.iteritems():
                new_obj[key] = dump(value, serialize_method,
                                     ignore_attribute, ignore)
            return new_obj
    # It's not a standard type, so it needs __jsonclass__
    module_name = inspect.getmodule(obj).__name__
    class_name = obj.__class__.__name__
    json_class = class_name
    if module_name not in ['', '__main__']:
        json_class = '%s.%s' % (module_name, json_class)
    return_obj = {"__jsonclass__":[json_class,]}
    # If a serialization method is defined..
    if serialize_method in dir(obj):
        # Params can be a dict (keyword) or list (positional)
        # Attrs MUST be a dict.
        serialize = getattr(obj, serialize_method)
        params, attrs = serialize()
        return_obj['__jsonclass__'].append(params)
        return_obj.update(attrs)
        return return_obj
    # Otherwise, try to figure it out
    # Obviously, we can't assume to know anything about the
    # parameters passed to __init__
    return_obj['__jsonclass__'].append([])
    attrs = {}
    ignore_list = getattr(obj, ignore_attribute, [])+ignore
    for attr_name, attr_value in obj.__dict__.iteritems():
        if type(attr_value) in supported_types and \
                attr_name not in ignore_list and \
                attr_value not in ignore_list:
            attrs[attr_name] = dump(attr_value, serialize_method,
                                     ignore_attribute, ignore)
    return_obj.update(attrs)
    return return_obj

def load(obj):
    if type(obj) in string_types+numeric_types+value_types:
        return obj
    if type(obj) is types.ListType:
        return_list = []
        for entry in obj:
            return_list.append(load(entry))
        return return_list
    # Othewise, it's a dict type
    if '__jsonclass__' not in obj.keys():
        return_dict = {}
        for key, value in obj.iteritems():
            new_value = load(value)
            return_dict[key] = new_value
        return return_dict
    # It's a dict, and it's a __jsonclass__
    orig_module_name = obj['__jsonclass__'][0]
    params = obj['__jsonclass__'][1]
    if orig_module_name == '':
        raise TranslationError('Module name empty.')
    json_module_clean = re.sub(invalid_module_chars, '', orig_module_name)
    if json_module_clean != orig_module_name:
        raise TranslationError('Module name %s has invalid characters.' %
                               orig_module_name)
    json_module_parts = json_module_clean.split('.')
    json_class = None
    if len(json_module_parts) == 1:
        # Local class name -- probably means it won't work
        if json_module_parts[0] not in config.classes.keys():
            raise TranslationError('Unknown class or module %s.' %
                                   json_module_parts[0])
        json_class = config.classes[json_module_parts[0]]
    else:
        json_class_name = json_module_parts.pop()
        json_module_tree = '.'.join(json_module_parts)
        try:
            temp_module = __import__(json_module_tree)
        except ImportError:
            raise TranslationError('Could not import %s from module %s.' %
                                   (json_class_name, json_module_tree))
        json_class = getattr(temp_module, json_class_name)
    # Creating the object...
    new_obj = None
    if type(params) is types.ListType:
        new_obj = json_class(*params)
    elif type(params) is types.DictType:
        new_obj = json_class(**params)
    else:
        raise TranslationError('Constructor args must be a dict or list.')
    for key, value in obj.iteritems():
        if key == '__jsonclass__':
            continue
        setattr(new_obj, key, value)
    return new_obj

########NEW FILE########
__FILENAME__ = jsonrpc
"""
Licensed under the Apache License, Version 2.0 (the "License"); 
you may not use this file except in compliance with the License. 
You may obtain a copy of the License at 

   http://www.apache.org/licenses/LICENSE-2.0 

Unless required by applicable law or agreed to in writing, software 
distributed under the License is distributed on an "AS IS" BASIS, 
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
See the License for the specific language governing permissions and 
limitations under the License. 

============================
JSONRPC Library (jsonrpclib)
============================

This library is a JSON-RPC v.2 (proposed) implementation which
follows the xmlrpclib API for portability between clients. It
uses the same Server / ServerProxy, loads, dumps, etc. syntax,
while providing features not present in XML-RPC like:

* Keyword arguments
* Notifications
* Versioning
* Batches and batch notifications

Eventually, I'll add a SimpleXMLRPCServer compatible library,
and other things to tie the thing off nicely. :)

For a quick-start, just open a console and type the following,
replacing the server address, method, and parameters 
appropriately.
>>> import jsonrpclib
>>> server = jsonrpclib.Server('http://localhost:8181')
>>> server.add(5, 6)
11
>>> server._notify.add(5, 6)
>>> batch = jsonrpclib.MultiCall(server)
>>> batch.add(3, 50)
>>> batch.add(2, 3)
>>> batch._notify.add(3, 5)
>>> batch()
[53, 5]

See http://code.google.com/p/jsonrpclib/ for more info.
"""

import types
import sys
from xmlrpclib import Transport as XMLTransport
from xmlrpclib import SafeTransport as XMLSafeTransport
from xmlrpclib import ServerProxy as XMLServerProxy
from xmlrpclib import _Method as XML_Method
import time
import string
import random

# Library includes
import lib.jsonrpclib
from lib.jsonrpclib import config
from lib.jsonrpclib import history

# JSON library importing
cjson = None
json = None
try:
    import cjson
except ImportError:
    try:
        import json
    except ImportError:
        try:
            import lib.simplejson as json
        except ImportError:
            raise ImportError(
                'You must have the cjson, json, or simplejson ' +
                'module(s) available.'
            )

IDCHARS = string.ascii_lowercase+string.digits

class UnixSocketMissing(Exception):
    """ 
    Just a properly named Exception if Unix Sockets usage is 
    attempted on a platform that doesn't support them (Windows)
    """
    pass

#JSON Abstractions

def jdumps(obj, encoding='utf-8'):
    # Do 'serialize' test at some point for other classes
    global cjson
    if cjson:
        return cjson.encode(obj)
    else:
        return json.dumps(obj, encoding=encoding)

def jloads(json_string):
    global cjson
    if cjson:
        return cjson.decode(json_string)
    else:
        return json.loads(json_string)


# XMLRPClib re-implementations

class ProtocolError(Exception):
    pass

class TransportMixIn(object):
    """ Just extends the XMLRPC transport where necessary. """
    user_agent = config.user_agent
    # for Python 2.7 support
    _connection = None

    def send_content(self, connection, request_body):
        connection.putheader("Content-Type", "application/json-rpc")
        connection.putheader("Content-Length", str(len(request_body)))
        connection.endheaders()
        if request_body:
            connection.send(request_body)

    def getparser(self):
        target = JSONTarget()
        return JSONParser(target), target

class JSONParser(object):
    def __init__(self, target):
        self.target = target

    def feed(self, data):
        self.target.feed(data)

    def close(self):
        pass

class JSONTarget(object):
    def __init__(self):
        self.data = []

    def feed(self, data):
        self.data.append(data)

    def close(self):
        return ''.join(self.data)

class Transport(TransportMixIn, XMLTransport):
    pass

class SafeTransport(TransportMixIn, XMLSafeTransport):
    pass
from httplib import HTTP, HTTPConnection
from socket import socket

USE_UNIX_SOCKETS = False

try: 
    from socket import AF_UNIX, SOCK_STREAM
    USE_UNIX_SOCKETS = True
except ImportError:
    pass
    
if (USE_UNIX_SOCKETS):
    
    class UnixHTTPConnection(HTTPConnection):
        def connect(self):
            self.sock = socket(AF_UNIX, SOCK_STREAM)
            self.sock.connect(self.host)

    class UnixHTTP(HTTP):
        _connection_class = UnixHTTPConnection

    class UnixTransport(TransportMixIn, XMLTransport):
        def make_connection(self, host):
            import httplib
            host, extra_headers, x509 = self.get_host_info(host)
            return UnixHTTP(host)

    
class ServerProxy(XMLServerProxy):
    """
    Unfortunately, much more of this class has to be copied since
    so much of it does the serialization.
    """

    def __init__(self, uri, transport=None, encoding=None, 
                 verbose=0, version=None):
        import urllib
        if not version:
            version = config.version
        self.__version = version
        schema, uri = urllib.splittype(uri)
        if schema not in ('http', 'https', 'unix'):
            raise IOError('Unsupported JSON-RPC protocol.')
        if schema == 'unix':
            if not USE_UNIX_SOCKETS:
                # Don't like the "generic" Exception...
                raise UnixSocketMissing("Unix sockets not available.")
            self.__host = uri
            self.__handler = '/'
        else:
            self.__host, self.__handler = urllib.splithost(uri)
            if not self.__handler:
                # Not sure if this is in the JSON spec?
                #self.__handler = '/'
                self.__handler == '/'
        if transport is None:
            if schema == 'unix':
                transport = UnixTransport()
            elif schema == 'https':
                transport = SafeTransport()
            else:
                transport = Transport()
        self.__transport = transport
        self.__encoding = encoding
        self.__verbose = verbose

    def _request(self, methodname, params, rpcid=None):
        request = dumps(params, methodname, encoding=self.__encoding,
                        rpcid=rpcid, version=self.__version)
        response = self._run_request(request)
        check_for_errors(response)
        return response['result']

    def _request_notify(self, methodname, params, rpcid=None):
        request = dumps(params, methodname, encoding=self.__encoding,
                        rpcid=rpcid, version=self.__version, notify=True)
        response = self._run_request(request, notify=True)
        check_for_errors(response)
        return

    def _run_request(self, request, notify=None):
        history.add_request(request)

        response = self.__transport.request(
            self.__host,
            self.__handler,
            request,
            verbose=self.__verbose
        )
        
        # Here, the XMLRPC library translates a single list
        # response to the single value -- should we do the
        # same, and require a tuple / list to be passed to
        # the response object, or expect the Server to be 
        # outputting the response appropriately?
        
        history.add_response(response)
        if not response:
            return None
        return_obj = loads(response)
        return return_obj

    def __getattr__(self, name):
        # Same as original, just with new _Method reference
        return _Method(self._request, name)

    @property
    def _notify(self):
        # Just like __getattr__, but with notify namespace.
        return _Notify(self._request_notify)


class _Method(XML_Method):
    
    def __call__(self, *args, **kwargs):
        if len(args) > 0 and len(kwargs) > 0:
            raise ProtocolError('Cannot use both positional ' +
                'and keyword arguments (according to JSON-RPC spec.)')
        if len(args) > 0:
            return self.__send(self.__name, args)
        else:
            return self.__send(self.__name, kwargs)

    def __getattr__(self, name):
        self.__name = '%s.%s' % (self.__name, name)
        return self
        # The old method returned a new instance, but this seemed wasteful.
        # The only thing that changes is the name.
        #return _Method(self.__send, "%s.%s" % (self.__name, name))

class _Notify(object):
    def __init__(self, request):
        self._request = request

    def __getattr__(self, name):
        return _Method(self._request, name)
        
# Batch implementation

class MultiCallMethod(object):
    
    def __init__(self, method, notify=False):
        self.method = method
        self.params = []
        self.notify = notify

    def __call__(self, *args, **kwargs):
        if len(kwargs) > 0 and len(args) > 0:
            raise ProtocolError('JSON-RPC does not support both ' +
                                'positional and keyword arguments.')
        if len(kwargs) > 0:
            self.params = kwargs
        else:
            self.params = args

    def request(self, encoding=None, rpcid=None):
        return dumps(self.params, self.method, version=2.0,
                     encoding=encoding, rpcid=rpcid, notify=self.notify)

    def __repr__(self):
        return '%s' % self.request()
        
    def __getattr__(self, method):
        new_method = '%s.%s' % (self.method, method)
        self.method = new_method
        return self

class MultiCallNotify(object):
    
    def __init__(self, multicall):
        self.multicall = multicall

    def __getattr__(self, name):
        new_job = MultiCallMethod(name, notify=True)
        self.multicall._job_list.append(new_job)
        return new_job

class MultiCallIterator(object):
    
    def __init__(self, results):
        self.results = results

    def __iter__(self):
        for i in range(0, len(self.results)):
            yield self[i]
        raise StopIteration

    def __getitem__(self, i):
        item = self.results[i]
        check_for_errors(item)
        return item['result']

    def __len__(self):
        return len(self.results)

class MultiCall(object):
    
    def __init__(self, server):
        self._server = server
        self._job_list = []

    def _request(self):
        if len(self._job_list) < 1:
            # Should we alert? This /is/ pretty obvious.
            return
        request_body = '[ %s ]' % ','.join([job.request() for
                                          job in self._job_list])
        responses = self._server._run_request(request_body)
        del self._job_list[:]
        if not responses:
            responses = []
        return MultiCallIterator(responses)

    @property
    def _notify(self):
        return MultiCallNotify(self)

    def __getattr__(self, name):
        new_job = MultiCallMethod(name)
        self._job_list.append(new_job)
        return new_job

    __call__ = _request

# These lines conform to xmlrpclib's "compatibility" line. 
# Not really sure if we should include these, but oh well.
Server = ServerProxy

class Fault(object):
    # JSON-RPC error class
    def __init__(self, code=-32000, message='Server error', rpcid=None):
        self.faultCode = code
        self.faultString = message
        self.rpcid = rpcid

    def error(self):
        return {'code':self.faultCode, 'message':self.faultString}

    def response(self, rpcid=None, version=None):
        if not version:
            version = config.version
        if rpcid:
            self.rpcid = rpcid
        return dumps(
            self, methodresponse=True, rpcid=self.rpcid, version=version
        )

    def __repr__(self):
        return '<Fault %s: %s>' % (self.faultCode, self.faultString)

def random_id(length=8):
    return_id = ''
    for i in range(length):
        return_id += random.choice(IDCHARS)
    return return_id

class Payload(dict):
    def __init__(self, rpcid=None, version=None):
        if not version:
            version = config.version
        self.id = rpcid
        self.version = float(version)
    
    def request(self, method, params=[]):
        if type(method) not in types.StringTypes:
            raise ValueError('Method name must be a string.')
        if not self.id:
            self.id = random_id()
        request = { 'id':self.id, 'method':method }
        if params:
            request['params'] = params
        if self.version >= 2:
            request['jsonrpc'] = str(self.version)
        return request

    def notify(self, method, params=[]):
        request = self.request(method, params)
        if self.version >= 2:
            del request['id']
        else:
            request['id'] = None
        return request

    def response(self, result=None):
        response = {'result':result, 'id':self.id}
        if self.version >= 2:
            response['jsonrpc'] = str(self.version)
        else:
            response['error'] = None
        return response

    def error(self, code=-32000, message='Server error.'):
        error = self.response()
        if self.version >= 2:
            del error['result']
        else:
            error['result'] = None
        error['error'] = {'code':code, 'message':message}
        return error

def dumps(params=[], methodname=None, methodresponse=None, 
        encoding=None, rpcid=None, version=None, notify=None):
    """
    This differs from the Python implementation in that it implements 
    the rpcid argument since the 2.0 spec requires it for responses.
    """
    if not version:
        version = config.version
    valid_params = (types.TupleType, types.ListType, types.DictType)
    if methodname in types.StringTypes and \
            type(params) not in valid_params and \
            not isinstance(params, Fault):
        """ 
        If a method, and params are not in a listish or a Fault,
        error out.
        """
        raise TypeError('Params must be a dict, list, tuple or Fault ' +
                        'instance.')
    # Begin parsing object
    payload = Payload(rpcid=rpcid, version=version)
    if not encoding:
        encoding = 'utf-8'
    if type(params) is Fault:
        response = payload.error(params.faultCode, params.faultString)
        return jdumps(response, encoding=encoding)
    if type(methodname) not in types.StringTypes and methodresponse != True:
        raise ValueError('Method name must be a string, or methodresponse '+
                         'must be set to True.')
    if config.use_jsonclass == True:
        from lib.jsonrpclib import jsonclass
        params = jsonclass.dump(params)
    if methodresponse is True:
        if rpcid is None:
            raise ValueError('A method response must have an rpcid.')
        response = payload.response(params)
        return jdumps(response, encoding=encoding)
    request = None
    if notify == True:
        request = payload.notify(methodname, params)
    else:
        request = payload.request(methodname, params)
    return jdumps(request, encoding=encoding)

def loads(data):
    """
    This differs from the Python implementation, in that it returns
    the request structure in Dict format instead of the method, params.
    It will return a list in the case of a batch request / response.
    """
    if data == '':
        # notification
        return None
    result = jloads(data)
    # if the above raises an error, the implementing server code 
    # should return something like the following:
    # { 'jsonrpc':'2.0', 'error': fault.error(), id: None }
    if config.use_jsonclass == True:
        from lib.jsonrpclib import jsonclass
        result = jsonclass.load(result)
    return result

def check_for_errors(result):
    if not result:
        # Notification
        return result
    if type(result) is not types.DictType:
        raise TypeError('Response is not a dict.')
    if 'jsonrpc' in result.keys() and float(result['jsonrpc']) > 2.0:
        raise NotImplementedError('JSON-RPC version not yet supported.')
    if 'result' not in result.keys() and 'error' not in result.keys():
        raise ValueError('Response does not have a result or error key.')
    if 'error' in result.keys() and result['error'] != None:
        code = result['error']['code']
        message = result['error']['message']
        raise ProtocolError((code, message))
    return result

def isbatch(result):
    if type(result) not in (types.ListType, types.TupleType):
        return False
    if len(result) < 1:
        return False
    if type(result[0]) is not types.DictType:
        return False
    if 'jsonrpc' not in result[0].keys():
        return False
    try:
        version = float(result[0]['jsonrpc'])
    except ValueError:
        raise ProtocolError('"jsonrpc" key must be a float(able) value.')
    if version < 2:
        return False
    return True

def isnotification(request):
    if 'id' not in request.keys():
        # 2.0 notification
        return True
    if request['id'] == None:
        # 1.0 notification
        return True
    return False

########NEW FILE########
__FILENAME__ = SimpleJSONRPCServer
import lib.jsonrpclib
from lib.jsonrpclib import Fault
from lib.jsonrpclib.jsonrpc import USE_UNIX_SOCKETS
import SimpleXMLRPCServer
import SocketServer
import socket
import logging
import os
import types
import traceback
import sys
try:
    import fcntl
except ImportError:
    # For Windows
    fcntl = None

def get_version(request):
    # must be a dict
    if 'jsonrpc' in request.keys():
        return 2.0
    if 'id' in request.keys():
        return 1.0
    return None
    
def validate_request(request):
    if type(request) is not types.DictType:
        fault = Fault(
            -32600, 'Request must be {}, not %s.' % type(request)
        )
        return fault
    rpcid = request.get('id', None)
    version = get_version(request)
    if not version:
        fault = Fault(-32600, 'Request %s invalid.' % request, rpcid=rpcid)
        return fault        
    request.setdefault('params', [])
    method = request.get('method', None)
    params = request.get('params')
    param_types = (types.ListType, types.DictType, types.TupleType)
    if not method or type(method) not in types.StringTypes or \
        type(params) not in param_types:
        fault = Fault(
            -32600, 'Invalid request parameters or method.', rpcid=rpcid
        )
        return fault
    return True

class SimpleJSONRPCDispatcher(SimpleXMLRPCServer.SimpleXMLRPCDispatcher):

    def __init__(self, encoding=None):
        SimpleXMLRPCServer.SimpleXMLRPCDispatcher.__init__(self,
                                        allow_none=True,
                                        encoding=encoding)

    def _marshaled_dispatch(self, data, dispatch_method = None):
        response = None
        try:
            request = jsonrpclib.loads(data)
        except Exception, e:
            fault = Fault(-32700, 'Request %s invalid. (%s)' % (data, e))
            response = fault.response()
            return response
        if not request:
            fault = Fault(-32600, 'Request invalid -- no request data.')
            return fault.response()
        if type(request) is types.ListType:
            # This SHOULD be a batch, by spec
            responses = []
            for req_entry in request:
                result = validate_request(req_entry)
                if type(result) is Fault:
                    responses.append(result.response())
                    continue
                resp_entry = self._marshaled_single_dispatch(req_entry)
                if resp_entry is not None:
                    responses.append(resp_entry)
            if len(responses) > 0:
                response = '[%s]' % ','.join(responses)
            else:
                response = ''
        else:    
            result = validate_request(request)
            if type(result) is Fault:
                return result.response()
            response = self._marshaled_single_dispatch(request)
        return response

    def _marshaled_single_dispatch(self, request):
        # TODO - Use the multiprocessing and skip the response if
        # it is a notification
        # Put in support for custom dispatcher here
        # (See SimpleXMLRPCServer._marshaled_dispatch)
        method = request.get('method')
        params = request.get('params')
        try:
            response = self._dispatch(method, params)
        except:
            exc_type, exc_value, exc_tb = sys.exc_info()
            fault = Fault(-32603, '%s:%s' % (exc_type, exc_value))
            return fault.response()
        if 'id' not in request.keys() or request['id'] == None:
            # It's a notification
            return None
        try:
            response = jsonrpclib.dumps(response,
                                        methodresponse=True,
                                        rpcid=request['id']
                                        )
            return response
        except:
            exc_type, exc_value, exc_tb = sys.exc_info()
            fault = Fault(-32603, '%s:%s' % (exc_type, exc_value))
            return fault.response()

    def _dispatch(self, method, params):
        func = None
        try:
            func = self.funcs[method]
        except KeyError:
            if self.instance is not None:
                if hasattr(self.instance, '_dispatch'):
                    return self.instance._dispatch(method, params)
                else:
                    try:
                        func = SimpleXMLRPCServer.resolve_dotted_attribute(
                            self.instance,
                            method,
                            True
                            )
                    except AttributeError:
                        pass
        if func is not None:
            try:
                if type(params) is types.ListType:
                    response = func(*params)
                else:
                    response = func(**params)
                return response
            except TypeError:
                return Fault(-32602, 'Invalid parameters.')
            except:
                err_lines = traceback.format_exc().splitlines()
                trace_string = '%s | %s' % (err_lines[-3], err_lines[-1])
                fault = jsonrpclib.Fault(-32603, 'Server error: %s' % 
                                         trace_string)
                return fault
        else:
            return Fault(-32601, 'Method %s not supported.' % method)

class SimpleJSONRPCRequestHandler(
        SimpleXMLRPCServer.SimpleXMLRPCRequestHandler):
    
    def do_POST(self):
        if not self.is_rpc_path_valid():
            self.report_404()
            return
        try:
            max_chunk_size = 10*1024*1024
            size_remaining = int(self.headers["content-length"])
            L = []
            while size_remaining:
                chunk_size = min(size_remaining, max_chunk_size)
                L.append(self.rfile.read(chunk_size))
                size_remaining -= len(L[-1])
            data = ''.join(L)
            response = self.server._marshaled_dispatch(data)
            self.send_response(200)
        except Exception, e:
            self.send_response(500)
            err_lines = traceback.format_exc().splitlines()
            trace_string = '%s | %s' % (err_lines[-3], err_lines[-1])
            fault = jsonrpclib.Fault(-32603, 'Server error: %s' % trace_string)
            response = fault.response()
        if response == None:
            response = ''
        self.send_header("Content-type", "application/json-rpc")
        self.send_header("Content-length", str(len(response)))
        self.end_headers()
        self.wfile.write(response)
        self.wfile.flush()
        self.connection.shutdown(1)

class SimpleJSONRPCServer(SocketServer.TCPServer, SimpleJSONRPCDispatcher):

    allow_reuse_address = True

    def __init__(self, addr, requestHandler=SimpleJSONRPCRequestHandler,
                 logRequests=True, encoding=None, bind_and_activate=True,
                 address_family=socket.AF_INET):
        self.logRequests = logRequests
        SimpleJSONRPCDispatcher.__init__(self, encoding)
        # TCPServer.__init__ has an extra parameter on 2.6+, so
        # check Python version and decide on how to call it
        vi = sys.version_info
        self.address_family = address_family
        if USE_UNIX_SOCKETS and address_family == socket.AF_UNIX:
            # Unix sockets can't be bound if they already exist in the
            # filesystem. The convention of e.g. X11 is to unlink
            # before binding again.
            if os.path.exists(addr): 
                try:
                    os.unlink(addr)
                except OSError:
                    logging.warning("Could not unlink socket %s", addr)
        # if python 2.5 and lower
        if vi[0] < 3 and vi[1] < 6:
            SocketServer.TCPServer.__init__(self, addr, requestHandler)
        else:
            SocketServer.TCPServer.__init__(self, addr, requestHandler,
                bind_and_activate)
        if fcntl is not None and hasattr(fcntl, 'FD_CLOEXEC'):
            flags = fcntl.fcntl(self.fileno(), fcntl.F_GETFD)
            flags |= fcntl.FD_CLOEXEC
            fcntl.fcntl(self.fileno(), fcntl.F_SETFD, flags)

class CGIJSONRPCRequestHandler(SimpleJSONRPCDispatcher):

    def __init__(self, encoding=None):
        SimpleJSONRPCDispatcher.__init__(self, encoding)

    def handle_jsonrpc(self, request_text):
        response = self._marshaled_dispatch(request_text)
        print 'Content-Type: application/json-rpc'
        print 'Content-Length: %d' % len(response)
        print
        sys.stdout.write(response)

    handle_xmlrpc = handle_jsonrpc

########NEW FILE########
__FILENAME__ = MultipartPostHandler
#!/usr/bin/python

####
# 06/2010 Nic Wolfe <nic@wolfeden.ca>
# 02/2006 Will Holcomb <wholcomb@gmail.com>
# 
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
# 
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#

import urllib
import urllib2
import mimetools, mimetypes
import os, sys

# Controls how sequences are uncoded. If true, elements may be given multiple values by
#  assigning a sequence.
doseq = 1

class MultipartPostHandler(urllib2.BaseHandler):
    handler_order = urllib2.HTTPHandler.handler_order - 10 # needs to run first

    def http_request(self, request):
        data = request.get_data()
        if data is not None and type(data) != str:
            v_files = []
            v_vars = []
            try:
                for(key, value) in data.items():
                    if type(value) in (file, list, tuple):
                        v_files.append((key, value))
                    else:
                        v_vars.append((key, value))
            except TypeError:
                systype, value, traceback = sys.exc_info()
                raise TypeError, "not a valid non-string sequence or mapping object", traceback

            if len(v_files) == 0:
                data = urllib.urlencode(v_vars, doseq)
            else:
                boundary, data = MultipartPostHandler.multipart_encode(v_vars, v_files)
                contenttype = 'multipart/form-data; boundary=%s' % boundary
                if(request.has_header('Content-Type')
                   and request.get_header('Content-Type').find('multipart/form-data') != 0):
                    print "Replacing %s with %s" % (request.get_header('content-type'), 'multipart/form-data')
                request.add_unredirected_header('Content-Type', contenttype)

            request.add_data(data)
        return request

    @staticmethod
    def multipart_encode(vars, files, boundary = None, buffer = None):
        if boundary is None:
            boundary = mimetools.choose_boundary()
        if buffer is None:
            buffer = ''
        for(key, value) in vars:
            buffer += '--%s\r\n' % boundary
            buffer += 'Content-Disposition: form-data; name="%s"' % key
            buffer += '\r\n\r\n' + value + '\r\n'
        for(key, fd) in files:
            
            # allow them to pass in a file or a tuple with name & data
            if type(fd) == file:
                name_in = fd.name
                fd.seek(0)
                data_in = fd.read()
            elif type(fd) in (tuple, list):
                name_in, data_in = fd
                
            filename = os.path.basename(name_in)
            contenttype = mimetypes.guess_type(filename)[0] or 'application/octet-stream'
            buffer += '--%s\r\n' % boundary
            buffer += 'Content-Disposition: form-data; name="%s"; filename="%s"\r\n' % (key, filename)
            buffer += 'Content-Type: %s\r\n' % contenttype
            # buffer += 'Content-Length: %s\r\n' % file_size
            buffer += '\r\n' + data_in + '\r\n'
        buffer += '--%s--\r\n\r\n' % boundary
        return boundary, buffer

    https_request = http_request


########NEW FILE########
__FILENAME__ = profilehooks
"""
Profiling hooks

This module contains a couple of decorators (`profile` and `coverage`) that
can be used to wrap functions and/or methods to produce profiles and line
coverage reports.  There's a third convenient decorator (`timecall`) that
measures the duration of function execution without the extra profiling
overhead.

Usage example (Python 2.4 or newer)::

    from profilehooks import profile, coverage

    @profile    # or @coverage
    def fn(n):
        if n < 2: return 1
        else: return n * fn(n-1)

    print fn(42)

Usage example (Python 2.3 or older)::

    from profilehooks import profile, coverage

    def fn(n):
        if n < 2: return 1
        else: return n * fn(n-1)

    # Now wrap that function in a decorator
    fn = profile(fn) # or coverage(fn)

    print fn(42)

Reports for all thusly decorated functions will be printed to sys.stdout
on program termination.  You can alternatively request for immediate
reports for each call by passing immediate=True to the profile decorator.

There's also a @timecall decorator for printing the time to sys.stderr
every time a function is called, when you just want to get a rough measure
instead of a detailed (but costly) profile.

Caveats

  A thread on python-dev convinced me that hotshot produces bogus numbers.
  See http://mail.python.org/pipermail/python-dev/2005-November/058264.html

  I don't know what will happen if a decorated function will try to call
  another decorated function.  All decorators probably need to explicitly
  support nested profiling (currently TraceFuncCoverage is the only one
  that supports this, while HotShotFuncProfile has support for recursive
  functions.)

  Profiling with hotshot creates temporary files (*.prof for profiling,
  *.cprof for coverage) in the current directory.  These files are not
  cleaned up.  Exception: when you specify a filename to the profile
  decorator (to store the pstats.Stats object for later inspection),
  the temporary file will be the filename you specified with '.raw'
  appended at the end.

  Coverage analysis with hotshot seems to miss some executions resulting
  in lower line counts and some lines errorneously marked as never
  executed.  For this reason coverage analysis now uses trace.py which is
  slower, but more accurate.

Copyright (c) 2004--2008 Marius Gedminas <marius@pov.lt>
Copyright (c) 2007 Hanno Schlichting
Copyright (c) 2008 Florian Schulze

Released under the MIT licence since December 2006:

    Permission is hereby granted, free of charge, to any person obtaining a
    copy of this software and associated documentation files (the "Software"),
    to deal in the Software without restriction, including without limitation
    the rights to use, copy, modify, merge, publish, distribute, sublicense,
    and/or sell copies of the Software, and to permit persons to whom the
    Software is furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in
    all copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    DEALINGS IN THE SOFTWARE.

(Previously it was distributed under the GNU General Public Licence.)
"""
# $Id: profilehooks.py 29 2010-08-13 16:29:20Z mg $

__author__ = "Marius Gedminas (marius@gedmin.as)"
__copyright__ = "Copyright 2004-2009 Marius Gedminas"
__license__ = "MIT"
__version__ = "1.4"
__date__ = "2009-03-31"


import atexit
import inspect
import sys
import re

# For profiling
from profile import Profile
import pstats

# For hotshot profiling (inaccurate!)
try:
    import hotshot
    import hotshot.stats
except ImportError:
    hotshot = None

# For trace.py coverage
import trace

# For hotshot coverage (inaccurate!; uses undocumented APIs; might break)
if hotshot is not None:
    import _hotshot
    import hotshot.log

# For cProfile profiling (best)
try:
    import cProfile
except ImportError:
    cProfile = None

# For timecall
import time


# registry of available profilers
AVAILABLE_PROFILERS = {}


def profile(fn=None, skip=0, filename=None, immediate=False, dirs=False,
            sort=None, entries=40,
            profiler=('cProfile', 'profile', 'hotshot')):
    """Mark `fn` for profiling.

    If `skip` is > 0, first `skip` calls to `fn` will not be profiled.

    If `immediate` is False, profiling results will be printed to
    sys.stdout on program termination.  Otherwise results will be printed
    after each call.

    If `dirs` is False only the name of the file will be printed.
    Otherwise the full path is used.

    `sort` can be a list of sort keys (defaulting to ['cumulative',
    'time', 'calls']).  The following ones are recognized::

        'calls'      -- call count
        'cumulative' -- cumulative time
        'file'       -- file name
        'line'       -- line number
        'module'     -- file name
        'name'       -- function name
        'nfl'        -- name/file/line
        'pcalls'     -- call count
        'stdname'    -- standard name
        'time'       -- internal time

    `entries` limits the output to the first N entries.

    `profiler` can be used to select the preferred profiler, or specify a
    sequence of them, in order of preference.  The default is ('cProfile'.
    'profile', 'hotshot').

    If `filename` is specified, the profile stats will be stored in the
    named file.  You can load them pstats.Stats(filename).

    Usage::

        def fn(...):
            ...
        fn = profile(fn, skip=1)

    If you are using Python 2.4, you should be able to use the decorator
    syntax::

        @profile(skip=3)
        def fn(...):
            ...

    or just ::

        @profile
        def fn(...):
            ...

    """
    if fn is None: # @profile() syntax -- we are a decorator maker
        def decorator(fn):
            return profile(fn, skip=skip, filename=filename,
                           immediate=immediate, dirs=dirs,
                           sort=sort, entries=entries,
                           profiler=profiler)
        return decorator
    # @profile syntax -- we are a decorator.
    if isinstance(profiler, str):
        profiler = [profiler]
    for p in profiler:
        if p in AVAILABLE_PROFILERS:
            profiler_class = AVAILABLE_PROFILERS[p]
            break
    else:
        raise ValueError('only these profilers are available: %s'
                             % ', '.join(AVAILABLE_PROFILERS))
    fp = profiler_class(fn, skip=skip, filename=filename,
                        immediate=immediate, dirs=dirs,
                        sort=sort, entries=entries)
    # fp = HotShotFuncProfile(fn, skip=skip, filename=filename, ...)
         # or HotShotFuncProfile
    # We cannot return fp or fp.__call__ directly as that would break method
    # definitions, instead we need to return a plain function.
    def new_fn(*args, **kw):
        return fp(*args, **kw)
    new_fn.__doc__ = fn.__doc__
    new_fn.__name__ = fn.__name__
    new_fn.__dict__ = fn.__dict__
    new_fn.__module__ = fn.__module__
    return new_fn


def coverage(fn):
    """Mark `fn` for line coverage analysis.

    Results will be printed to sys.stdout on program termination.

    Usage::

        def fn(...):
            ...
        fn = coverage(fn)

    If you are using Python 2.4, you should be able to use the decorator
    syntax::

        @coverage
        def fn(...):
            ...

    """
    fp = TraceFuncCoverage(fn) # or HotShotFuncCoverage
    # We cannot return fp or fp.__call__ directly as that would break method
    # definitions, instead we need to return a plain function.
    def new_fn(*args, **kw):
        return fp(*args, **kw)
    new_fn.__doc__ = fn.__doc__
    new_fn.__name__ = fn.__name__
    new_fn.__dict__ = fn.__dict__
    new_fn.__module__ = fn.__module__
    return new_fn


def coverage_with_hotshot(fn):
    """Mark `fn` for line coverage analysis.

    Uses the 'hotshot' module for fast coverage analysis.

    BUG: Produces inaccurate results.

    See the docstring of `coverage` for usage examples.
    """
    fp = HotShotFuncCoverage(fn)
    # We cannot return fp or fp.__call__ directly as that would break method
    # definitions, instead we need to return a plain function.
    def new_fn(*args, **kw):
        return fp(*args, **kw)
    new_fn.__doc__ = fn.__doc__
    new_fn.__name__ = fn.__name__
    new_fn.__dict__ = fn.__dict__
    new_fn.__module__ = fn.__module__
    return new_fn


class FuncProfile(object):
    """Profiler for a function (uses profile)."""

    # This flag is shared between all instances
    in_profiler = False

    Profile = Profile

    def __init__(self, fn, skip=0, filename=None, immediate=False, dirs=False,
                 sort=None, entries=40):
        """Creates a profiler for a function.

        Every profiler has its own log file (the name of which is derived
        from the function name).

        FuncProfile registers an atexit handler that prints profiling
        information to sys.stderr when the program terminates.
        """
        self.fn = fn
        self.skip = skip
        self.filename = filename
        self.immediate = immediate
        self.dirs = dirs
        self.sort = sort or ('cumulative', 'time', 'calls')
        if isinstance(self.sort, str):
            self.sort = (self.sort, )
        self.entries = entries
        self.reset_stats()
        atexit.register(self.atexit)

    def __call__(self, *args, **kw):
        """Profile a singe call to the function."""
        self.ncalls += 1
        if self.skip > 0:
            self.skip -= 1
            self.skipped += 1
            return self.fn(*args, **kw)
        if FuncProfile.in_profiler:
            # handle recursive calls
            return self.fn(*args, **kw)
        # You cannot reuse the same profiler for many calls and accumulate
        # stats that way.  :-/
        profiler = self.Profile()
        try:
            FuncProfile.in_profiler = True
            return profiler.runcall(self.fn, *args, **kw)
        finally:
            FuncProfile.in_profiler = False
            self.stats.add(profiler)
            if self.immediate:
                self.print_stats()
                self.reset_stats()

    def print_stats(self):
        """Print profile information to sys.stdout."""
        funcname = self.fn.__name__
        filename = self.fn.func_code.co_filename
        lineno = self.fn.func_code.co_firstlineno
        print
        print "*** PROFILER RESULTS ***"
        print "%s (%s:%s)" % (funcname, filename, lineno)
        print "function called %d times" % self.ncalls,
        if self.skipped:
            print "(%d calls not profiled)" % self.skipped
        else:
            print
        print
        stats = self.stats
        if self.filename:
            stats.dump_stats(self.filename)
        if not self.dirs:
            stats.strip_dirs()
        stats.sort_stats(*self.sort)
        stats.print_stats(self.entries)

    def reset_stats(self):
        """Reset accumulated profiler statistics."""
        # Note: not using self.Profile, since pstats.Stats() fails then
        self.stats = pstats.Stats(Profile())
        self.ncalls = 0
        self.skipped = 0

    def atexit(self):
        """Stop profiling and print profile information to sys.stdout.

        This function is registered as an atexit hook.
        """
        if not self.immediate:
            self.print_stats()


AVAILABLE_PROFILERS['profile'] = FuncProfile


if cProfile is not None:

    class CProfileFuncProfile(FuncProfile):
        """Profiler for a function (uses cProfile)."""

        Profile = cProfile.Profile

    AVAILABLE_PROFILERS['cProfile'] = CProfileFuncProfile


if hotshot is not None:

    class HotShotFuncProfile(object):
        """Profiler for a function (uses hotshot)."""

        # This flag is shared between all instances
        in_profiler = False

        def __init__(self, fn, skip=0, filename=None):
            """Creates a profiler for a function.

            Every profiler has its own log file (the name of which is derived
            from the function name).

            HotShotFuncProfile registers an atexit handler that prints
            profiling information to sys.stderr when the program terminates.

            The log file is not removed and remains there to clutter the
            current working directory.
            """
            self.fn = fn
            self.filename = filename
            if self.filename:
                self.logfilename = filename + ".raw"
            else:
                self.logfilename = fn.__name__ + ".prof"
            self.profiler = hotshot.Profile(self.logfilename)
            self.ncalls = 0
            self.skip = skip
            self.skipped = 0
            atexit.register(self.atexit)

        def __call__(self, *args, **kw):
            """Profile a singe call to the function."""
            self.ncalls += 1
            if self.skip > 0:
                self.skip -= 1
                self.skipped += 1
                return self.fn(*args, **kw)
            if HotShotFuncProfile.in_profiler:
                # handle recursive calls
                return self.fn(*args, **kw)
            try:
                HotShotFuncProfile.in_profiler = True
                return self.profiler.runcall(self.fn, *args, **kw)
            finally:
                HotShotFuncProfile.in_profiler = False

        def atexit(self):
            """Stop profiling and print profile information to sys.stderr.

            This function is registered as an atexit hook.
            """
            self.profiler.close()
            funcname = self.fn.__name__
            filename = self.fn.func_code.co_filename
            lineno = self.fn.func_code.co_firstlineno
            print
            print "*** PROFILER RESULTS ***"
            print "%s (%s:%s)" % (funcname, filename, lineno)
            print "function called %d times" % self.ncalls,
            if self.skipped:
                print "(%d calls not profiled)" % self.skipped
            else:
                print
            print
            stats = hotshot.stats.load(self.logfilename)
            # hotshot.stats.load takes ages, and the .prof file eats megabytes, but
            # a saved stats object is small and fast
            if self.filename:
                stats.dump_stats(self.filename)
            # it is best to save before strip_dirs
            stats.strip_dirs()
            stats.sort_stats('cumulative', 'time', 'calls')
            stats.print_stats(40)

    AVAILABLE_PROFILERS['hotshot'] = HotShotFuncProfile


    class HotShotFuncCoverage:
        """Coverage analysis for a function (uses _hotshot).

        HotShot coverage is reportedly faster than trace.py, but it appears to
        have problems with exceptions; also line counts in coverage reports
        are generally lower from line counts produced by TraceFuncCoverage.
        Is this my bug, or is it a problem with _hotshot?
        """

        def __init__(self, fn):
            """Creates a profiler for a function.

            Every profiler has its own log file (the name of which is derived
            from the function name).

            HotShotFuncCoverage registers an atexit handler that prints
            profiling information to sys.stderr when the program terminates.

            The log file is not removed and remains there to clutter the
            current working directory.
            """
            self.fn = fn
            self.logfilename = fn.__name__ + ".cprof"
            self.profiler = _hotshot.coverage(self.logfilename)
            self.ncalls = 0
            atexit.register(self.atexit)

        def __call__(self, *args, **kw):
            """Profile a singe call to the function."""
            self.ncalls += 1
            return self.profiler.runcall(self.fn, args, kw)

        def atexit(self):
            """Stop profiling and print profile information to sys.stderr.

            This function is registered as an atexit hook.
            """
            self.profiler.close()
            funcname = self.fn.__name__
            filename = self.fn.func_code.co_filename
            lineno = self.fn.func_code.co_firstlineno
            print
            print "*** COVERAGE RESULTS ***"
            print "%s (%s:%s)" % (funcname, filename, lineno)
            print "function called %d times" % self.ncalls
            print
            fs = FuncSource(self.fn)
            reader = hotshot.log.LogReader(self.logfilename)
            for what, (filename, lineno, funcname), tdelta in reader:
                if filename != fs.filename:
                    continue
                if what == hotshot.log.LINE:
                    fs.mark(lineno)
                if what == hotshot.log.ENTER:
                    # hotshot gives us the line number of the function definition
                    # and never gives us a LINE event for the first statement in
                    # a function, so if we didn't perform this mapping, the first
                    # statement would be marked as never executed
                    if lineno == fs.firstlineno:
                        lineno = fs.firstcodelineno
                    fs.mark(lineno)
            reader.close()
            print fs


class TraceFuncCoverage:
    """Coverage analysis for a function (uses trace module).

    HotShot coverage analysis is reportedly faster, but it appears to have
    problems with exceptions.
    """

    # Shared between all instances so that nested calls work
    tracer = trace.Trace(count=True, trace=False,
                         ignoredirs=[sys.prefix, sys.exec_prefix])

    # This flag is also shared between all instances
    tracing = False

    def __init__(self, fn):
        """Creates a profiler for a function.

        Every profiler has its own log file (the name of which is derived
        from the function name).

        TraceFuncCoverage registers an atexit handler that prints
        profiling information to sys.stderr when the program terminates.

        The log file is not removed and remains there to clutter the
        current working directory.
        """
        self.fn = fn
        self.logfilename = fn.__name__ + ".cprof"
        self.ncalls = 0
        atexit.register(self.atexit)

    def __call__(self, *args, **kw):
        """Profile a singe call to the function."""
        self.ncalls += 1
        if TraceFuncCoverage.tracing:
            return self.fn(*args, **kw)
        try:
            TraceFuncCoverage.tracing = True
            return self.tracer.runfunc(self.fn, *args, **kw)
        finally:
            TraceFuncCoverage.tracing = False

    def atexit(self):
        """Stop profiling and print profile information to sys.stderr.

        This function is registered as an atexit hook.
        """
        funcname = self.fn.__name__
        filename = self.fn.func_code.co_filename
        lineno = self.fn.func_code.co_firstlineno
        print
        print "*** COVERAGE RESULTS ***"
        print "%s (%s:%s)" % (funcname, filename, lineno)
        print "function called %d times" % self.ncalls
        print
        fs = FuncSource(self.fn)
        for (filename, lineno), count in self.tracer.counts.items():
            if filename != fs.filename:
                continue
            fs.mark(lineno, count)
        print fs
        never_executed = fs.count_never_executed()
        if never_executed:
            print "%d lines were not executed." % never_executed


class FuncSource:
    """Source code annotator for a function."""

    blank_rx = re.compile(r"^\s*finally:\s*(#.*)?$")

    def __init__(self, fn):
        self.fn = fn
        self.filename = inspect.getsourcefile(fn)
        self.source, self.firstlineno = inspect.getsourcelines(fn)
        self.sourcelines = {}
        self.firstcodelineno = self.firstlineno
        self.find_source_lines()

    def find_source_lines(self):
        """Mark all executable source lines in fn as executed 0 times."""
        strs = trace.find_strings(self.filename)
        lines = trace.find_lines_from_code(self.fn.func_code, strs)
        self.firstcodelineno = sys.maxint
        for lineno in lines:
            self.firstcodelineno = min(self.firstcodelineno, lineno)
            self.sourcelines.setdefault(lineno, 0)
        if self.firstcodelineno == sys.maxint:
            self.firstcodelineno = self.firstlineno

    def mark(self, lineno, count=1):
        """Mark a given source line as executed count times.

        Multiple calls to mark for the same lineno add up.
        """
        self.sourcelines[lineno] = self.sourcelines.get(lineno, 0) + count

    def count_never_executed(self):
        """Count statements that were never executed."""
        lineno = self.firstlineno
        counter = 0
        for line in self.source:
            if self.sourcelines.get(lineno) == 0:
                if not self.blank_rx.match(line):
                    counter += 1
            lineno += 1
        return counter

    def __str__(self):
        """Return annotated source code for the function."""
        lines = []
        lineno = self.firstlineno
        for line in self.source:
            counter = self.sourcelines.get(lineno)
            if counter is None:
                prefix = ' ' * 7
            elif counter == 0:
                if self.blank_rx.match(line):
                    prefix = ' ' * 7
                else:
                    prefix = '>' * 6 + ' '
            else:
                prefix = '%5d: ' % counter
            lines.append(prefix + line)
            lineno += 1
        return ''.join(lines)


def timecall(fn=None, immediate=True, timer=time.time):
    """Wrap `fn` and print its execution time.

    Example::

        @timecall
        def somefunc(x, y):
            time.sleep(x * y)

        somefunc(2, 3)

    will print the time taken by somefunc on every call.  If you want just
    a summary at program termination, use

        @timecall(immediate=False)

    You can also choose a timing method other than the default ``time.time()``,
    e.g.:

        @timecall(timer=time.clock)

    """
    if fn is None: # @timecall() syntax -- we are a decorator maker
        def decorator(fn):
            return timecall(fn, immediate=immediate, timer=timer)
        return decorator
    # @timecall syntax -- we are a decorator.
    fp = FuncTimer(fn, immediate=immediate, timer=timer)
    # We cannot return fp or fp.__call__ directly as that would break method
    # definitions, instead we need to return a plain function.
    def new_fn(*args, **kw):
        return fp(*args, **kw)
    new_fn.__doc__ = fn.__doc__
    new_fn.__name__ = fn.__name__
    new_fn.__dict__ = fn.__dict__
    new_fn.__module__ = fn.__module__
    return new_fn


class FuncTimer(object):

    def __init__(self, fn, immediate, timer):
        self.fn = fn
        self.ncalls = 0
        self.totaltime = 0
        self.immediate = immediate
        self.timer = timer
        if not immediate:
            atexit.register(self.atexit)

    def __call__(self, *args, **kw):
        """Profile a singe call to the function."""
        fn = self.fn
        timer = self.timer
        self.ncalls += 1
        try:
            start = timer()
            return fn(*args, **kw)
        finally:
            duration = timer() - start
            self.totaltime += duration
            if self.immediate:
                funcname = fn.__name__
                filename = fn.func_code.co_filename
                lineno = fn.func_code.co_firstlineno
                print >> sys.stderr, "\n  %s (%s:%s):\n    %.3f seconds\n" % (
                                        funcname, filename, lineno, duration)
    def atexit(self):
        if not self.ncalls:
            return
        funcname = self.fn.__name__
        filename = self.fn.func_code.co_filename
        lineno = self.fn.func_code.co_firstlineno
        print ("\n  %s (%s:%s):\n"
               "    %d calls, %.3f seconds (%.3f seconds per call)\n" % (
                                funcname, filename, lineno, self.ncalls,
                                self.totaltime, self.totaltime / self.ncalls))


########NEW FILE########
__FILENAME__ = pynma
#!/usr/bin/python

from xml.dom.minidom import parseString
from httplib import HTTPSConnection
from urllib import urlencode

__version__ = "0.1"

API_SERVER = 'nma.usk.bz'
ADD_PATH   = '/publicapi/notify'

USER_AGENT="PyNMA/v%s"%__version__

def uniq_preserve(seq): # Dave Kirby
    # Order preserving
    seen = set()
    return [x for x in seq if x not in seen and not seen.add(x)]

def uniq(seq):
    # Not order preserving
    return {}.fromkeys(seq).keys()

class PyNMA(object):
    """PyNMA(apikey=[], developerkey=None)
        takes 2 optional arguments:
            - (opt) apykey:      might me a string containing 1 key or an array of keys
            - (opt) developerkey: where you can store your developer key
    """

    def __init__(self, apikey=[], developerkey=None):
        self._developerkey = None
        self.developerkey(developerkey)
        if apikey:
            if type(apikey) == str:
                apikey = [apikey]
        self._apikey          = uniq(apikey)

    def addkey(self, key):
        "Add a key (register ?)"
        if type(key) == str:
            if not key in self._apikey:
                self._apikey.append(key)
        elif type(key) == list:
            for k in key:
                if not k in self._apikey:
                    self._apikey.append(k)

    def delkey(self, key):
        "Removes a key (unregister ?)"
        if type(key) == str:
            if key in self._apikey:
                self._apikey.remove(key)
        elif type(key) == list:
            for k in key:
                if key in self._apikey:
                    self._apikey.remove(k)

    def developerkey(self, developerkey):
        "Sets the developer key (and check it has the good length)"
        if type(developerkey) == str and len(developerkey) == 48:
            self._developerkey = developerkey

    def push(self, application="", event="", description="", url="", priority=0, batch_mode=False):
        """Pushes a message on the registered API keys.
            takes 5 arguments:
                - (req) application: application name [256]
                - (req) event:       event name       [1000]
                - (req) description: description      [10000]
                - (opt) url:         url              [512]
                - (opt) priority:    from -2 (lowest) to 2 (highest) (def:0)
                - (opt) batch_mode:  call API 5 by 5 (def:False)

            Warning: using batch_mode will return error only if all API keys are bad
            cf: http://nma.usk.bz/api.php
        """
        datas = {
            'application': application[:256].encode('utf8'),
            'event':       event[:1024].encode('utf8'),
            'description': description[:10000].encode('utf8'),
            'priority':    priority
        }

        if url:
            datas['url'] = url[:512]

        if self._developerkey:
            datas['developerkey'] = self._developerkey

        results = {}

        if not batch_mode:
            for key in self._apikey:
                datas['apikey'] = key
                res = self.callapi('POST', ADD_PATH, datas)
                results[key] = res
        else:
            for i in range(0, len(self._apikey), 5):
                datas['apikey'] = ",".join(self._apikey[i:i+5])
                res = self.callapi('POST', ADD_PATH, datas)
                results[datas['apikey']] = res
        return results
        
    def callapi(self, method, path, args):
        headers = { 'User-Agent': USER_AGENT }
        if method == "POST":
            headers['Content-type'] = "application/x-www-form-urlencoded"
        http_handler = HTTPSConnection(API_SERVER)
        http_handler.request(method, path, urlencode(args), headers)
        resp = http_handler.getresponse()

        try:
            res = self._parse_reponse(resp.read())
        except Exception, e:
            res = {'type':    "pynmaerror",
                   'code':    600,
                   'message': str(e)
                   }
            pass
        
        return res

    def _parse_reponse(self, response):
        root = parseString(response).firstChild
        for elem in root.childNodes:
            if elem.nodeType == elem.TEXT_NODE: continue
            if elem.tagName == 'success':
                res = dict(elem.attributes.items())
                res['message'] = ""
                res['type']    = elem.tagName
                return res
            if elem.tagName == 'error':
                res = dict(elem.attributes.items())
                res['message'] = elem.firstChild.nodeValue
                res['type']    = elem.tagName
                return res
                                        
    

########NEW FILE########
__FILENAME__ = decoder
"""Implementation of JSONDecoder
"""
import re
import sys
import struct

from lib.simplejson.scanner import make_scanner
try:
    from lib.simplejson._speedups import scanstring as c_scanstring
except ImportError:
    c_scanstring = None

__all__ = ['JSONDecoder']

FLAGS = re.VERBOSE | re.MULTILINE | re.DOTALL

def _floatconstants():
    _BYTES = '7FF80000000000007FF0000000000000'.decode('hex')
    if sys.byteorder != 'big':
        _BYTES = _BYTES[:8][::-1] + _BYTES[8:][::-1]
    nan, inf = struct.unpack('dd', _BYTES)
    return nan, inf, -inf

NaN, PosInf, NegInf = _floatconstants()


def linecol(doc, pos):
    lineno = doc.count('\n', 0, pos) + 1
    if lineno == 1:
        colno = pos
    else:
        colno = pos - doc.rindex('\n', 0, pos)
    return lineno, colno


def errmsg(msg, doc, pos, end=None):
    # Note that this function is called from _speedups
    lineno, colno = linecol(doc, pos)
    if end is None:
        #fmt = '{0}: line {1} column {2} (char {3})'
        #return fmt.format(msg, lineno, colno, pos)
        fmt = '%s: line %d column %d (char %d)'
        return fmt % (msg, lineno, colno, pos)
    endlineno, endcolno = linecol(doc, end)
    #fmt = '{0}: line {1} column {2} - line {3} column {4} (char {5} - {6})'
    #return fmt.format(msg, lineno, colno, endlineno, endcolno, pos, end)
    fmt = '%s: line %d column %d - line %d column %d (char %d - %d)'
    return fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)


_CONSTANTS = {
    '-Infinity': NegInf,
    'Infinity': PosInf,
    'NaN': NaN,
}

STRINGCHUNK = re.compile(r'(.*?)(["\\\x00-\x1f])', FLAGS)
BACKSLASH = {
    '"': u'"', '\\': u'\\', '/': u'/',
    'b': u'\b', 'f': u'\f', 'n': u'\n', 'r': u'\r', 't': u'\t',
}

DEFAULT_ENCODING = "utf-8"

def py_scanstring(s, end, encoding=None, strict=True, _b=BACKSLASH, _m=STRINGCHUNK.match):
    """Scan the string s for a JSON string. End is the index of the
    character in s after the quote that started the JSON string.
    Unescapes all valid JSON string escape sequences and raises ValueError
    on attempt to decode an invalid string. If strict is False then literal
    control characters are allowed in the string.
    
    Returns a tuple of the decoded string and the index of the character in s
    after the end quote."""
    if encoding is None:
        encoding = DEFAULT_ENCODING
    chunks = []
    _append = chunks.append
    begin = end - 1
    while 1:
        chunk = _m(s, end)
        if chunk is None:
            raise ValueError(
                errmsg("Unterminated string starting at", s, begin))
        end = chunk.end()
        content, terminator = chunk.groups()
        # Content is contains zero or more unescaped string characters
        if content:
            if not isinstance(content, unicode):
                content = unicode(content, encoding)
            _append(content)
        # Terminator is the end of string, a literal control character,
        # or a backslash denoting that an escape sequence follows
        if terminator == '"':
            break
        elif terminator != '\\':
            if strict:
                msg = "Invalid control character %r at" % (terminator,)
                #msg = "Invalid control character {0!r} at".format(terminator)
                raise ValueError(errmsg(msg, s, end))
            else:
                _append(terminator)
                continue
        try:
            esc = s[end]
        except IndexError:
            raise ValueError(
                errmsg("Unterminated string starting at", s, begin))
        # If not a unicode escape sequence, must be in the lookup table
        if esc != 'u':
            try:
                char = _b[esc]
            except KeyError:
                msg = "Invalid \\escape: " + repr(esc)
                raise ValueError(errmsg(msg, s, end))
            end += 1
        else:
            # Unicode escape sequence
            esc = s[end + 1:end + 5]
            next_end = end + 5
            if len(esc) != 4:
                msg = "Invalid \\uXXXX escape"
                raise ValueError(errmsg(msg, s, end))
            uni = int(esc, 16)
            # Check for surrogate pair on UCS-4 systems
            if 0xd800 <= uni <= 0xdbff and sys.maxunicode > 65535:
                msg = "Invalid \\uXXXX\\uXXXX surrogate pair"
                if not s[end + 5:end + 7] == '\\u':
                    raise ValueError(errmsg(msg, s, end))
                esc2 = s[end + 7:end + 11]
                if len(esc2) != 4:
                    raise ValueError(errmsg(msg, s, end))
                uni2 = int(esc2, 16)
                uni = 0x10000 + (((uni - 0xd800) << 10) | (uni2 - 0xdc00))
                next_end += 6
            char = unichr(uni)
            end = next_end
        # Append the unescaped character
        _append(char)
    return u''.join(chunks), end


# Use speedup if available
scanstring = c_scanstring or py_scanstring

WHITESPACE = re.compile(r'[ \t\n\r]*', FLAGS)
WHITESPACE_STR = ' \t\n\r'

def JSONObject((s, end), encoding, strict, scan_once, object_hook, _w=WHITESPACE.match, _ws=WHITESPACE_STR):
    pairs = {}
    # Use a slice to prevent IndexError from being raised, the following
    # check will raise a more specific ValueError if the string is empty
    nextchar = s[end:end + 1]
    # Normally we expect nextchar == '"'
    if nextchar != '"':
        if nextchar in _ws:
            end = _w(s, end).end()
            nextchar = s[end:end + 1]
        # Trivial empty object
        if nextchar == '}':
            return pairs, end + 1
        elif nextchar != '"':
            raise ValueError(errmsg("Expecting property name", s, end))
    end += 1
    while True:
        key, end = scanstring(s, end, encoding, strict)

        # To skip some function call overhead we optimize the fast paths where
        # the JSON key separator is ": " or just ":".
        if s[end:end + 1] != ':':
            end = _w(s, end).end()
            if s[end:end + 1] != ':':
                raise ValueError(errmsg("Expecting : delimiter", s, end))

        end += 1

        try:
            if s[end] in _ws:
                end += 1
                if s[end] in _ws:
                    end = _w(s, end + 1).end()
        except IndexError:
            pass

        try:
            value, end = scan_once(s, end)
        except StopIteration:
            raise ValueError(errmsg("Expecting object", s, end))
        pairs[key] = value

        try:
            nextchar = s[end]
            if nextchar in _ws:
                end = _w(s, end + 1).end()
                nextchar = s[end]
        except IndexError:
            nextchar = ''
        end += 1

        if nextchar == '}':
            break
        elif nextchar != ',':
            raise ValueError(errmsg("Expecting , delimiter", s, end - 1))

        try:
            nextchar = s[end]
            if nextchar in _ws:
                end += 1
                nextchar = s[end]
                if nextchar in _ws:
                    end = _w(s, end + 1).end()
                    nextchar = s[end]
        except IndexError:
            nextchar = ''

        end += 1
        if nextchar != '"':
            raise ValueError(errmsg("Expecting property name", s, end - 1))

    if object_hook is not None:
        pairs = object_hook(pairs)
    return pairs, end

def JSONArray((s, end), scan_once, _w=WHITESPACE.match, _ws=WHITESPACE_STR):
    values = []
    nextchar = s[end:end + 1]
    if nextchar in _ws:
        end = _w(s, end + 1).end()
        nextchar = s[end:end + 1]
    # Look-ahead for trivial empty array
    if nextchar == ']':
        return values, end + 1
    _append = values.append
    while True:
        try:
            value, end = scan_once(s, end)
        except StopIteration:
            raise ValueError(errmsg("Expecting object", s, end))
        _append(value)
        nextchar = s[end:end + 1]
        if nextchar in _ws:
            end = _w(s, end + 1).end()
            nextchar = s[end:end + 1]
        end += 1
        if nextchar == ']':
            break
        elif nextchar != ',':
            raise ValueError(errmsg("Expecting , delimiter", s, end))

        try:
            if s[end] in _ws:
                end += 1
                if s[end] in _ws:
                    end = _w(s, end + 1).end()
        except IndexError:
            pass

    return values, end

class JSONDecoder(object):
    """Simple JSON <http://json.org> decoder

    Performs the following translations in decoding by default:

    +---------------+-------------------+
    | JSON          | Python            |
    +===============+===================+
    | object        | dict              |
    +---------------+-------------------+
    | array         | list              |
    +---------------+-------------------+
    | string        | unicode           |
    +---------------+-------------------+
    | number (int)  | int, long         |
    +---------------+-------------------+
    | number (real) | float             |
    +---------------+-------------------+
    | true          | True              |
    +---------------+-------------------+
    | false         | False             |
    +---------------+-------------------+
    | null          | None              |
    +---------------+-------------------+

    It also understands ``NaN``, ``Infinity``, and ``-Infinity`` as
    their corresponding ``float`` values, which is outside the JSON spec.

    """

    def __init__(self, encoding=None, object_hook=None, parse_float=None,
            parse_int=None, parse_constant=None, strict=True):
        """``encoding`` determines the encoding used to interpret any ``str``
        objects decoded by this instance (utf-8 by default).  It has no
        effect when decoding ``unicode`` objects.

        Note that currently only encodings that are a superset of ASCII work,
        strings of other encodings should be passed in as ``unicode``.

        ``object_hook``, if specified, will be called with the result
        of every JSON object decoded and its return value will be used in
        place of the given ``dict``.  This can be used to provide custom
        deserializations (e.g. to support JSON-RPC class hinting).

        ``parse_float``, if specified, will be called with the string
        of every JSON float to be decoded. By default this is equivalent to
        float(num_str). This can be used to use another datatype or parser
        for JSON floats (e.g. decimal.Decimal).

        ``parse_int``, if specified, will be called with the string
        of every JSON int to be decoded. By default this is equivalent to
        int(num_str). This can be used to use another datatype or parser
        for JSON integers (e.g. float).

        ``parse_constant``, if specified, will be called with one of the
        following strings: -Infinity, Infinity, NaN.
        This can be used to raise an exception if invalid JSON numbers
        are encountered.

        """
        self.encoding = encoding
        self.object_hook = object_hook
        self.parse_float = parse_float or float
        self.parse_int = parse_int or int
        self.parse_constant = parse_constant or _CONSTANTS.__getitem__
        self.strict = strict
        self.parse_object = JSONObject
        self.parse_array = JSONArray
        self.parse_string = scanstring
        self.scan_once = make_scanner(self)

    def decode(self, s, _w=WHITESPACE.match):
        """Return the Python representation of ``s`` (a ``str`` or ``unicode``
        instance containing a JSON document)

        """
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
        end = _w(s, end).end()
        if end != len(s):
            raise ValueError(errmsg("Extra data", s, end, len(s)))
        return obj

    def raw_decode(self, s, idx=0):
        """Decode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning
        with a JSON document) and return a 2-tuple of the Python
        representation and the index in ``s`` where the document ended.

        This can be used to decode a JSON document from a string that may
        have extraneous data at the end.

        """
        try:
            obj, end = self.scan_once(s, idx)
        except StopIteration:
            raise ValueError("No JSON object could be decoded")
        return obj, end

########NEW FILE########
__FILENAME__ = encoder
"""Implementation of JSONEncoder
"""
import re

try:
    from lib.simplejson._speedups import encode_basestring_ascii as c_encode_basestring_ascii
except ImportError:
    c_encode_basestring_ascii = None
try:
    from lib.simplejson._speedups import make_encoder as c_make_encoder
except ImportError:
    c_make_encoder = None

ESCAPE = re.compile(r'[\x00-\x1f\\"\b\f\n\r\t]')
ESCAPE_ASCII = re.compile(r'([\\"]|[^\ -~])')
HAS_UTF8 = re.compile(r'[\x80-\xff]')
ESCAPE_DCT = {
    '\\': '\\\\',
    '"': '\\"',
    '\b': '\\b',
    '\f': '\\f',
    '\n': '\\n',
    '\r': '\\r',
    '\t': '\\t',
}
for i in range(0x20):
    #ESCAPE_DCT.setdefault(chr(i), '\\u{0:04x}'.format(i))
    ESCAPE_DCT.setdefault(chr(i), '\\u%04x' % (i,))

# Assume this produces an infinity on all machines (probably not guaranteed)
INFINITY = float('1e66666')
FLOAT_REPR = repr

def encode_basestring(s):
    """Return a JSON representation of a Python string

    """
    def replace(match):
        return ESCAPE_DCT[match.group(0)]
    return '"' + ESCAPE.sub(replace, s) + '"'


def py_encode_basestring_ascii(s):
    """Return an ASCII-only JSON representation of a Python string

    """
    if isinstance(s, str) and HAS_UTF8.search(s) is not None:
        s = s.decode('utf-8')
    def replace(match):
        s = match.group(0)
        try:
            return ESCAPE_DCT[s]
        except KeyError:
            n = ord(s)
            if n < 0x10000:
                #return '\\u{0:04x}'.format(n)
                return '\\u%04x' % (n,)
            else:
                # surrogate pair
                n -= 0x10000
                s1 = 0xd800 | ((n >> 10) & 0x3ff)
                s2 = 0xdc00 | (n & 0x3ff)
                #return '\\u{0:04x}\\u{1:04x}'.format(s1, s2)
                return '\\u%04x\\u%04x' % (s1, s2)
    return '"' + str(ESCAPE_ASCII.sub(replace, s)) + '"'


encode_basestring_ascii = c_encode_basestring_ascii or py_encode_basestring_ascii

class JSONEncoder(object):
    """Extensible JSON <http://json.org> encoder for Python data structures.

    Supports the following objects and types by default:

    +-------------------+---------------+
    | Python            | JSON          |
    +===================+===============+
    | dict              | object        |
    +-------------------+---------------+
    | list, tuple       | array         |
    +-------------------+---------------+
    | str, unicode      | string        |
    +-------------------+---------------+
    | int, long, float  | number        |
    +-------------------+---------------+
    | True              | true          |
    +-------------------+---------------+
    | False             | false         |
    +-------------------+---------------+
    | None              | null          |
    +-------------------+---------------+

    To extend this to recognize other objects, subclass and implement a
    ``.default()`` method with another method that returns a serializable
    object for ``o`` if possible, otherwise it should call the superclass
    implementation (to raise ``TypeError``).

    """
    item_separator = ', '
    key_separator = ': '
    def __init__(self, skipkeys=False, ensure_ascii=True,
            check_circular=True, allow_nan=True, sort_keys=False,
            indent=None, separators=None, encoding='utf-8', default=None):
        """Constructor for JSONEncoder, with sensible defaults.

        If skipkeys is false, then it is a TypeError to attempt
        encoding of keys that are not str, int, long, float or None.  If
        skipkeys is True, such items are simply skipped.

        If ensure_ascii is true, the output is guaranteed to be str
        objects with all incoming unicode characters escaped.  If
        ensure_ascii is false, the output will be unicode object.

        If check_circular is true, then lists, dicts, and custom encoded
        objects will be checked for circular references during encoding to
        prevent an infinite recursion (which would cause an OverflowError).
        Otherwise, no such check takes place.

        If allow_nan is true, then NaN, Infinity, and -Infinity will be
        encoded as such.  This behavior is not JSON specification compliant,
        but is consistent with most JavaScript based encoders and decoders.
        Otherwise, it will be a ValueError to encode such floats.

        If sort_keys is true, then the output of dictionaries will be
        sorted by key; this is useful for regression tests to ensure
        that JSON serializations can be compared on a day-to-day basis.

        If indent is a non-negative integer, then JSON array
        elements and object members will be pretty-printed with that
        indent level.  An indent level of 0 will only insert newlines.
        None is the most compact representation.

        If specified, separators should be a (item_separator, key_separator)
        tuple.  The default is (', ', ': ').  To get the most compact JSON
        representation you should specify (',', ':') to eliminate whitespace.

        If specified, default is a function that gets called for objects
        that can't otherwise be serialized.  It should return a JSON encodable
        version of the object or raise a ``TypeError``.

        If encoding is not None, then all input strings will be
        transformed into unicode using that encoding prior to JSON-encoding.
        The default is UTF-8.

        """

        self.skipkeys = skipkeys
        self.ensure_ascii = ensure_ascii
        self.check_circular = check_circular
        self.allow_nan = allow_nan
        self.sort_keys = sort_keys
        self.indent = indent
        if separators is not None:
            self.item_separator, self.key_separator = separators
        if default is not None:
            self.default = default
        self.encoding = encoding

    def default(self, o):
        """Implement this method in a subclass such that it returns
        a serializable object for ``o``, or calls the base implementation
        (to raise a ``TypeError``).

        For example, to support arbitrary iterators, you could
        implement default like this::

            def default(self, o):
                try:
                    iterable = iter(o)
                except TypeError:
                    pass
                else:
                    return list(iterable)
                return JSONEncoder.default(self, o)

        """
        raise TypeError(repr(o) + " is not JSON serializable")

    def encode(self, o):
        """Return a JSON string representation of a Python data structure.

        >>> JSONEncoder().encode({"foo": ["bar", "baz"]})
        '{"foo": ["bar", "baz"]}'

        """
        # This is for extremely simple cases and benchmarks.
        if isinstance(o, basestring):
            if isinstance(o, str):
                _encoding = self.encoding
                if (_encoding is not None
                        and not (_encoding == 'utf-8')):
                    o = o.decode(_encoding)
            if self.ensure_ascii:
                return encode_basestring_ascii(o)
            else:
                return encode_basestring(o)
        # This doesn't pass the iterator directly to ''.join() because the
        # exceptions aren't as detailed.  The list call should be roughly
        # equivalent to the PySequence_Fast that ''.join() would do.
        chunks = self.iterencode(o, _one_shot=True)
        if not isinstance(chunks, (list, tuple)):
            chunks = list(chunks)
        return ''.join(chunks)

    def iterencode(self, o, _one_shot=False):
        """Encode the given object and yield each string
        representation as available.

        For example::

            for chunk in JSONEncoder().iterencode(bigobject):
                mysocket.write(chunk)

        """
        if self.check_circular:
            markers = {}
        else:
            markers = None
        if self.ensure_ascii:
            _encoder = encode_basestring_ascii
        else:
            _encoder = encode_basestring
        if self.encoding != 'utf-8':
            def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):
                if isinstance(o, str):
                    o = o.decode(_encoding)
                return _orig_encoder(o)

        def floatstr(o, allow_nan=self.allow_nan, _repr=FLOAT_REPR, _inf=INFINITY, _neginf=-INFINITY):
            # Check for specials.  Note that this type of test is processor- and/or
            # platform-specific, so do tests which don't depend on the internals.

            if o != o:
                text = 'NaN'
            elif o == _inf:
                text = 'Infinity'
            elif o == _neginf:
                text = '-Infinity'
            else:
                return _repr(o)

            if not allow_nan:
                raise ValueError(
                    "Out of range float values are not JSON compliant: " +
                    repr(o))

            return text


        if _one_shot and c_make_encoder is not None and not self.indent and not self.sort_keys:
            _iterencode = c_make_encoder(
                markers, self.default, _encoder, self.indent,
                self.key_separator, self.item_separator, self.sort_keys,
                self.skipkeys, self.allow_nan)
        else:
            _iterencode = _make_iterencode(
                markers, self.default, _encoder, self.indent, floatstr,
                self.key_separator, self.item_separator, self.sort_keys,
                self.skipkeys, _one_shot)
        return _iterencode(o, 0)

def _make_iterencode(markers, _default, _encoder, _indent, _floatstr, _key_separator, _item_separator, _sort_keys, _skipkeys, _one_shot,
        ## HACK: hand-optimized bytecode; turn globals into locals
        False=False,
        True=True,
        ValueError=ValueError,
        basestring=basestring,
        dict=dict,
        float=float,
        id=id,
        int=int,
        isinstance=isinstance,
        list=list,
        long=long,
        str=str,
        tuple=tuple,
    ):

    def _iterencode_list(lst, _current_indent_level):
        if not lst:
            yield '[]'
            return
        if markers is not None:
            markerid = id(lst)
            if markerid in markers:
                raise ValueError("Circular reference detected")
            markers[markerid] = lst
        buf = '['
        if _indent is not None:
            _current_indent_level += 1
            newline_indent = '\n' + (' ' * (_indent * _current_indent_level))
            separator = _item_separator + newline_indent
            buf += newline_indent
        else:
            newline_indent = None
            separator = _item_separator
        first = True
        for value in lst:
            if first:
                first = False
            else:
                buf = separator
            if isinstance(value, basestring):
                yield buf + _encoder(value)
            elif value is None:
                yield buf + 'null'
            elif value is True:
                yield buf + 'true'
            elif value is False:
                yield buf + 'false'
            elif isinstance(value, (int, long)):
                yield buf + str(value)
            elif isinstance(value, float):
                yield buf + _floatstr(value)
            else:
                yield buf
                if isinstance(value, (list, tuple)):
                    chunks = _iterencode_list(value, _current_indent_level)
                elif isinstance(value, dict):
                    chunks = _iterencode_dict(value, _current_indent_level)
                else:
                    chunks = _iterencode(value, _current_indent_level)
                for chunk in chunks:
                    yield chunk
        if newline_indent is not None:
            _current_indent_level -= 1
            yield '\n' + (' ' * (_indent * _current_indent_level))
        yield ']'
        if markers is not None:
            del markers[markerid]

    def _iterencode_dict(dct, _current_indent_level):
        if not dct:
            yield '{}'
            return
        if markers is not None:
            markerid = id(dct)
            if markerid in markers:
                raise ValueError("Circular reference detected")
            markers[markerid] = dct
        yield '{'
        if _indent is not None:
            _current_indent_level += 1
            newline_indent = '\n' + (' ' * (_indent * _current_indent_level))
            item_separator = _item_separator + newline_indent
            yield newline_indent
        else:
            newline_indent = None
            item_separator = _item_separator
        first = True
        if _sort_keys:
            items = dct.items()
            items.sort(key=lambda kv: kv[0])
        else:
            items = dct.iteritems()
        for key, value in items:
            if isinstance(key, basestring):
                pass
            # JavaScript is weakly typed for these, so it makes sense to
            # also allow them.  Many encoders seem to do something like this.
            elif isinstance(key, float):
                key = _floatstr(key)
            elif key is True:
                key = 'true'
            elif key is False:
                key = 'false'
            elif key is None:
                key = 'null'
            elif isinstance(key, (int, long)):
                key = str(key)
            elif _skipkeys:
                continue
            else:
                raise TypeError("key " + repr(key) + " is not a string")
            if first:
                first = False
            else:
                yield item_separator
            yield _encoder(key)
            yield _key_separator
            if isinstance(value, basestring):
                yield _encoder(value)
            elif value is None:
                yield 'null'
            elif value is True:
                yield 'true'
            elif value is False:
                yield 'false'
            elif isinstance(value, (int, long)):
                yield str(value)
            elif isinstance(value, float):
                yield _floatstr(value)
            else:
                if isinstance(value, (list, tuple)):
                    chunks = _iterencode_list(value, _current_indent_level)
                elif isinstance(value, dict):
                    chunks = _iterencode_dict(value, _current_indent_level)
                else:
                    chunks = _iterencode(value, _current_indent_level)
                for chunk in chunks:
                    yield chunk
        if newline_indent is not None:
            _current_indent_level -= 1
            yield '\n' + (' ' * (_indent * _current_indent_level))
        yield '}'
        if markers is not None:
            del markers[markerid]

    def _iterencode(o, _current_indent_level):
        if isinstance(o, basestring):
            yield _encoder(o)
        elif o is None:
            yield 'null'
        elif o is True:
            yield 'true'
        elif o is False:
            yield 'false'
        elif isinstance(o, (int, long)):
            yield str(o)
        elif isinstance(o, float):
            yield _floatstr(o)
        elif isinstance(o, (list, tuple)):
            for chunk in _iterencode_list(o, _current_indent_level):
                yield chunk
        elif isinstance(o, dict):
            for chunk in _iterencode_dict(o, _current_indent_level):
                yield chunk
        else:
            if markers is not None:
                markerid = id(o)
                if markerid in markers:
                    raise ValueError("Circular reference detected")
                markers[markerid] = o
            o = _default(o)
            for chunk in _iterencode(o, _current_indent_level):
                yield chunk
            if markers is not None:
                del markers[markerid]

    return _iterencode

########NEW FILE########
__FILENAME__ = scanner
"""JSON token scanner
"""
import re
try:
    from lib.simplejson._speedups import make_scanner as c_make_scanner
except ImportError:
    c_make_scanner = None

__all__ = ['make_scanner']

NUMBER_RE = re.compile(
    r'(-?(?:0|[1-9]\d*))(\.\d+)?([eE][-+]?\d+)?',
    (re.VERBOSE | re.MULTILINE | re.DOTALL))

def py_make_scanner(context):
    parse_object = context.parse_object
    parse_array = context.parse_array
    parse_string = context.parse_string
    match_number = NUMBER_RE.match
    encoding = context.encoding
    strict = context.strict
    parse_float = context.parse_float
    parse_int = context.parse_int
    parse_constant = context.parse_constant
    object_hook = context.object_hook

    def _scan_once(string, idx):
        try:
            nextchar = string[idx]
        except IndexError:
            raise StopIteration

        if nextchar == '"':
            return parse_string(string, idx + 1, encoding, strict)
        elif nextchar == '{':
            return parse_object((string, idx + 1), encoding, strict, _scan_once, object_hook)
        elif nextchar == '[':
            return parse_array((string, idx + 1), _scan_once)
        elif nextchar == 'n' and string[idx:idx + 4] == 'null':
            return None, idx + 4
        elif nextchar == 't' and string[idx:idx + 4] == 'true':
            return True, idx + 4
        elif nextchar == 'f' and string[idx:idx + 5] == 'false':
            return False, idx + 5

        m = match_number(string, idx)
        if m is not None:
            integer, frac, exp = m.groups()
            if frac or exp:
                res = parse_float(integer + (frac or '') + (exp or ''))
            else:
                res = parse_int(integer)
            return res, m.end()
        elif nextchar == 'N' and string[idx:idx + 3] == 'NaN':
            return parse_constant('NaN'), idx + 3
        elif nextchar == 'I' and string[idx:idx + 8] == 'Infinity':
            return parse_constant('Infinity'), idx + 8
        elif nextchar == '-' and string[idx:idx + 9] == '-Infinity':
            return parse_constant('-Infinity'), idx + 9
        else:
            raise StopIteration

    return _scan_once

make_scanner = c_make_scanner or py_make_scanner

########NEW FILE########
__FILENAME__ = gprof2dot
#!/usr/bin/env python
#
# Copyright 2008 Jose Fonseca
#
# This program is free software: you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#

"""Generate a dot graph from the output of several profilers."""

__author__ = "Jose Fonseca"

__version__ = "1.0"


import sys
import math
import os.path
import re
import textwrap
import optparse


try:
    # Debugging helper module
    import debug
except ImportError:
    pass


def percentage(p):
    return "%.02f%%" % (p*100.0,)

def add(a, b):
    return a + b

def equal(a, b):
    if a == b:
        return a
    else:
        return None

def fail(a, b):
    assert False


def ratio(numerator, denominator):
    numerator = float(numerator)
    denominator = float(denominator)
    assert 0.0 <= numerator
    assert numerator <= denominator
    try:
        return numerator/denominator
    except ZeroDivisionError:
        # 0/0 is undefined, but 1.0 yields more useful results
        return 1.0


class UndefinedEvent(Exception):
    """Raised when attempting to get an event which is undefined."""
    
    def __init__(self, event):
        Exception.__init__(self)
        self.event = event

    def __str__(self):
        return 'unspecified event %s' % self.event.name


class Event(object):
    """Describe a kind of event, and its basic operations."""

    def __init__(self, name, null, aggregator, formatter = str):
        self.name = name
        self._null = null
        self._aggregator = aggregator
        self._formatter = formatter

    def __eq__(self, other):
        return self is other

    def __hash__(self):
        return id(self)

    def null(self):
        return self._null

    def aggregate(self, val1, val2):
        """Aggregate two event values."""
        assert val1 is not None
        assert val2 is not None
        return self._aggregator(val1, val2)
    
    def format(self, val):
        """Format an event value."""
        assert val is not None
        return self._formatter(val)


MODULE = Event("Module", None, equal)
PROCESS = Event("Process", None, equal)

CALLS = Event("Calls", 0, add)
SAMPLES = Event("Samples", 0, add)

TIME = Event("Time", 0.0, add, lambda x: '(' + str(x) + ')')
TIME_RATIO = Event("Time ratio", 0.0, add, lambda x: '(' + percentage(x) + ')')
TOTAL_TIME = Event("Total time", 0.0, fail)
TOTAL_TIME_RATIO = Event("Total time ratio", 0.0, fail, percentage)

CALL_RATIO = Event("Call ratio", 0.0, add, percentage)

PRUNE_RATIO = Event("Prune ratio", 0.0, add, percentage)


class Object(object):
    """Base class for all objects in profile which can store events."""

    def __init__(self, events=None):
        if events is None:
            self.events = {}
        else:
            self.events = events

    def __hash__(self):
        return id(self)

    def __eq__(self, other):
        return self is other

    def __contains__(self, event):
        return event in self.events
    
    def __getitem__(self, event):
        try:
            return self.events[event]
        except KeyError:
            raise UndefinedEvent(event)
    
    def __setitem__(self, event, value):
        if value is None:
            if event in self.events:
                del self.events[event]
        else:
            self.events[event] = value


class Call(Object):
    """A call between functions.
    
    There should be at most one call object for every pair of functions.
    """

    def __init__(self, callee_id):
        Object.__init__(self)
        self.callee_id = callee_id    


class Function(Object):
    """A function."""

    def __init__(self, id, name):
        Object.__init__(self)
        self.id = id
        self.name = name
        self.calls = {}
        self.cycle = None
    
    def add_call(self, call):
        if call.callee_id in self.calls:
            sys.stderr.write('warning: overwriting call from function %s to %s\n' % (str(self.id), str(call.callee_id)))
        self.calls[call.callee_id] = call

    # TODO: write utility functions

    def __repr__(self):
        return self.name


class Cycle(Object):
    """A cycle made from recursive function calls."""

    def __init__(self):
        Object.__init__(self)
        # XXX: Do cycles need an id?
        self.functions = set()

    def add_function(self, function):
        assert function not in self.functions
        self.functions.add(function)
        # XXX: Aggregate events?
        if function.cycle is not None:
            for other in function.cycle.functions:
                if function not in self.functions:
                    self.add_function(other)
        function.cycle = self


class Profile(Object):
    """The whole profile."""

    def __init__(self):
        Object.__init__(self)
        self.functions = {}
        self.cycles = []

    def add_function(self, function):
        if function.id in self.functions:
            sys.stderr.write('warning: overwriting function %s (id %s)\n' % (function.name, str(function.id)))
        self.functions[function.id] = function

    def add_cycle(self, cycle):
        self.cycles.append(cycle)

    def validate(self):
        """Validate the edges."""

        for function in self.functions.itervalues():
            for callee_id in function.calls.keys():
                assert function.calls[callee_id].callee_id == callee_id
                if callee_id not in self.functions:
                    sys.stderr.write('warning: call to undefined function %s from function %s\n' % (str(callee_id), function.name))
                    del function.calls[callee_id]

    def find_cycles(self):
        """Find cycles using Tarjan's strongly connected components algorithm."""

        # Apply the Tarjan's algorithm successively until all functions are visited
        visited = set()
        for function in self.functions.itervalues():
            if function not in visited:
                self._tarjan(function, 0, [], {}, {}, visited)
        cycles = []
        for function in self.functions.itervalues():
            if function.cycle is not None and function.cycle not in cycles:
                cycles.append(function.cycle)
        self.cycles = cycles
        if 0:
            for cycle in cycles:
                sys.stderr.write("Cycle:\n")
                for member in cycle.functions:
                    sys.stderr.write("\t%s\n" % member.name)
    
    def _tarjan(self, function, order, stack, orders, lowlinks, visited):
        """Tarjan's strongly connected components algorithm.

        See also:
        - http://en.wikipedia.org/wiki/Tarjan's_strongly_connected_components_algorithm
        """

        visited.add(function)
        orders[function] = order
        lowlinks[function] = order
        order += 1
        pos = len(stack)
        stack.append(function)
        for call in function.calls.itervalues():
            callee = self.functions[call.callee_id]
            # TODO: use a set to optimize lookup
            if callee not in orders:
                order = self._tarjan(callee, order, stack, orders, lowlinks, visited)
                lowlinks[function] = min(lowlinks[function], lowlinks[callee])
            elif callee in stack:
                lowlinks[function] = min(lowlinks[function], orders[callee])
        if lowlinks[function] == orders[function]:
            # Strongly connected component found
            members = stack[pos:]
            del stack[pos:]
            if len(members) > 1:
                cycle = Cycle()
                for member in members:
                    cycle.add_function(member)
        return order

    def call_ratios(self, event):
        # Aggregate for incoming calls
        cycle_totals = {}
        for cycle in self.cycles:
            cycle_totals[cycle] = 0.0
        function_totals = {}
        for function in self.functions.itervalues():
            function_totals[function] = 0.0
        for function in self.functions.itervalues():
            for call in function.calls.itervalues():
                if call.callee_id != function.id:
                    callee = self.functions[call.callee_id]
                    function_totals[callee] += call[event]
                    if callee.cycle is not None and callee.cycle is not function.cycle:
                        cycle_totals[callee.cycle] += call[event]

        # Compute the ratios
        for function in self.functions.itervalues():
            for call in function.calls.itervalues():
                assert CALL_RATIO not in call
                if call.callee_id != function.id:
                    callee = self.functions[call.callee_id]
                    if callee.cycle is not None and callee.cycle is not function.cycle:
                        total = cycle_totals[callee.cycle]
                    else:
                        total = function_totals[callee]
                    call[CALL_RATIO] = ratio(call[event], total)

    def integrate(self, outevent, inevent):
        """Propagate function time ratio allong the function calls.

        Must be called after finding the cycles.

        See also:
        - http://citeseer.ist.psu.edu/graham82gprof.html
        """

        # Sanity checking
        assert outevent not in self
        for function in self.functions.itervalues():
            assert outevent not in function
            assert inevent in function
            for call in function.calls.itervalues():
                assert outevent not in call
                if call.callee_id != function.id:
                    assert CALL_RATIO in call

        # Aggregate the input for each cycle 
        for cycle in self.cycles:
            total = inevent.null()
            for function in self.functions.itervalues():
                total = inevent.aggregate(total, function[inevent])
            self[inevent] = total

        # Integrate along the edges
        total = inevent.null()
        for function in self.functions.itervalues():
            total = inevent.aggregate(total, function[inevent])
            self._integrate_function(function, outevent, inevent)
        self[outevent] = total

    def _integrate_function(self, function, outevent, inevent):
        if function.cycle is not None:
            return self._integrate_cycle(function.cycle, outevent, inevent)
        else:
            if outevent not in function:
                total = function[inevent]
                for call in function.calls.itervalues():
                    if call.callee_id != function.id:
                        total += self._integrate_call(call, outevent, inevent)
                function[outevent] = total
            return function[outevent]
    
    def _integrate_call(self, call, outevent, inevent):
        assert outevent not in call
        assert CALL_RATIO in call
        callee = self.functions[call.callee_id]
        subtotal = call[CALL_RATIO]*self._integrate_function(callee, outevent, inevent)
        call[outevent] = subtotal
        return subtotal

    def _integrate_cycle(self, cycle, outevent, inevent):
        if outevent not in cycle:

            total = inevent.null()
            for member in cycle.functions:
                subtotal = member[inevent]
                for call in member.calls.itervalues():
                    callee = self.functions[call.callee_id]
                    if callee.cycle is not cycle:
                        subtotal += self._integrate_call(call, outevent, inevent)
                total += subtotal
            cycle[outevent] = total
            
            callees = {}
            for function in self.functions.itervalues():
                if function.cycle is not cycle:
                    for call in function.calls.itervalues():
                        callee = self.functions[call.callee_id]
                        if callee.cycle is cycle:
                            try:
                                callees[callee] += call[CALL_RATIO]
                            except KeyError:
                                callees[callee] = call[CALL_RATIO]

            for callee, call_ratio in callees.iteritems():
                ranks = {}
                call_ratios = {}
                partials = {}
                self._rank_cycle_function(cycle, callee, 0, ranks)
                self._call_ratios_cycle(cycle, callee, ranks, call_ratios, set())
                partial = self._integrate_cycle_function(cycle, callee, call_ratio, partials, ranks, call_ratios, outevent, inevent)
                assert partial == max(partials.values())
                assert not total or abs(1.0 - partial/(call_ratio*total)) <= 0.001
            
        return cycle[outevent]

    def _rank_cycle_function(self, cycle, function, rank, ranks):
        if function not in ranks or ranks[function] > rank:
            ranks[function] = rank
            for call in function.calls.itervalues():
                if call.callee_id != function.id:
                    callee = self.functions[call.callee_id]
                    if callee.cycle is cycle:
                        self._rank_cycle_function(cycle, callee, rank + 1, ranks)

    def _call_ratios_cycle(self, cycle, function, ranks, call_ratios, visited):
        if function not in visited:
            visited.add(function)
            for call in function.calls.itervalues():
                if call.callee_id != function.id:
                    callee = self.functions[call.callee_id]
                    if callee.cycle is cycle:
                        if ranks[callee] > ranks[function]:
                            call_ratios[callee] = call_ratios.get(callee, 0.0) + call[CALL_RATIO]
                            self._call_ratios_cycle(cycle, callee, ranks, call_ratios, visited)

    def _integrate_cycle_function(self, cycle, function, partial_ratio, partials, ranks, call_ratios, outevent, inevent):
        if function not in partials:
            partial = partial_ratio*function[inevent]
            for call in function.calls.itervalues():
                if call.callee_id != function.id:
                    callee = self.functions[call.callee_id]
                    if callee.cycle is not cycle:
                        assert outevent in call
                        partial += partial_ratio*call[outevent]
                    else:
                        if ranks[callee] > ranks[function]:
                            callee_partial = self._integrate_cycle_function(cycle, callee, partial_ratio, partials, ranks, call_ratios, outevent, inevent)
                            call_ratio = ratio(call[CALL_RATIO], call_ratios[callee])
                            call_partial = call_ratio*callee_partial
                            try:
                                call[outevent] += call_partial
                            except UndefinedEvent:
                                call[outevent] = call_partial
                            partial += call_partial
            partials[function] = partial
            try:
                function[outevent] += partial
            except UndefinedEvent:
                function[outevent] = partial
        return partials[function]

    def aggregate(self, event):
        """Aggregate an event for the whole profile."""

        total = event.null()
        for function in self.functions.itervalues():
            try:
                total = event.aggregate(total, function[event])
            except UndefinedEvent:
                return
        self[event] = total

    def ratio(self, outevent, inevent):
        assert outevent not in self
        assert inevent in self
        for function in self.functions.itervalues():
            assert outevent not in function
            assert inevent in function
            function[outevent] = ratio(function[inevent], self[inevent])
            for call in function.calls.itervalues():
                assert outevent not in call
                if inevent in call:
                    call[outevent] = ratio(call[inevent], self[inevent])
        self[outevent] = 1.0

    def prune(self, node_thres, edge_thres):
        """Prune the profile"""

        # compute the prune ratios
        for function in self.functions.itervalues():
            try:
                function[PRUNE_RATIO] = function[TOTAL_TIME_RATIO]
            except UndefinedEvent:
                pass

            for call in function.calls.itervalues():
                callee = self.functions[call.callee_id]

                if TOTAL_TIME_RATIO in call:
                    # handle exact cases first
                    call[PRUNE_RATIO] = call[TOTAL_TIME_RATIO] 
                else:
                    try:
                        # make a safe estimate
                        call[PRUNE_RATIO] = min(function[TOTAL_TIME_RATIO], callee[TOTAL_TIME_RATIO]) 
                    except UndefinedEvent:
                        pass

        # prune the nodes
        for function_id in self.functions.keys():
            function = self.functions[function_id]
            try:
                if function[PRUNE_RATIO] < node_thres:
                    del self.functions[function_id]
            except UndefinedEvent:
                pass

        # prune the egdes
        for function in self.functions.itervalues():
            for callee_id in function.calls.keys():
                call = function.calls[callee_id]
                try:
                    if callee_id not in self.functions or call[PRUNE_RATIO] < edge_thres:
                        del function.calls[callee_id]
                except UndefinedEvent:
                    pass
    
    def dump(self):
        for function in self.functions.itervalues():
            sys.stderr.write('Function %s:\n' % (function.name,))
            self._dump_events(function.events)
            for call in function.calls.itervalues():
                callee = self.functions[call.callee_id]
                sys.stderr.write('  Call %s:\n' % (callee.name,))
                self._dump_events(call.events)

    def _dump_events(self, events):
        for event, value in events.iteritems():
            sys.stderr.write('    %s: %s\n' % (event.name, event.format(value)))


class Struct:
    """Masquerade a dictionary with a structure-like behavior."""

    def __init__(self, attrs = None):
        if attrs is None:
            attrs = {}
        self.__dict__['_attrs'] = attrs
    
    def __getattr__(self, name):
        try:
            return self._attrs[name]
        except KeyError:
            raise AttributeError(name)

    def __setattr__(self, name, value):
        self._attrs[name] = value

    def __str__(self):
        return str(self._attrs)

    def __repr__(self):
        return repr(self._attrs)
    

class ParseError(Exception):
    """Raised when parsing to signal mismatches."""

    def __init__(self, msg, line):
        self.msg = msg
        # TODO: store more source line information
        self.line = line

    def __str__(self):
        return '%s: %r' % (self.msg, self.line)


class Parser:
    """Parser interface."""

    def __init__(self):
        pass

    def parse(self):
        raise NotImplementedError

    
class LineParser(Parser):
    """Base class for parsers that read line-based formats."""

    def __init__(self, file):
        Parser.__init__(self)
        self._file = file
        self.__line = None
        self.__eof = False

    def readline(self):
        line = self._file.readline()
        if not line:
            self.__line = ''
            self.__eof = True
        self.__line = line.rstrip('\r\n')

    def lookahead(self):
        assert self.__line is not None
        return self.__line

    def consume(self):
        assert self.__line is not None
        line = self.__line
        self.readline()
        return line

    def eof(self):
        assert self.__line is not None
        return self.__eof


class GprofParser(Parser):
    """Parser for GNU gprof output.

    See also:
    - Chapter "Interpreting gprof's Output" from the GNU gprof manual
      http://sourceware.org/binutils/docs-2.18/gprof/Call-Graph.html#Call-Graph
    - File "cg_print.c" from the GNU gprof source code
      http://sourceware.org/cgi-bin/cvsweb.cgi/~checkout~/src/gprof/cg_print.c?rev=1.12&cvsroot=src
    """

    def __init__(self, fp):
        Parser.__init__(self)
        self.fp = fp
        self.functions = {}
        self.cycles = {}

    def readline(self):
        line = self.fp.readline()
        if not line:
            sys.stderr.write('error: unexpected end of file\n')
            sys.exit(1)
        line = line.rstrip('\r\n')
        return line

    _int_re = re.compile(r'^\d+$')
    _float_re = re.compile(r'^\d+\.\d+$')

    def translate(self, mo):
        """Extract a structure from a match object, while translating the types in the process."""
        attrs = {}
        groupdict = mo.groupdict()
        for name, value in groupdict.iteritems():
            if value is None:
                value = None
            elif self._int_re.match(value):
                value = int(value)
            elif self._float_re.match(value):
                value = float(value)
            attrs[name] = (value)
        return Struct(attrs)

    _cg_header_re = re.compile(
        # original gprof header
        r'^\s+called/total\s+parents\s*$|' +
        r'^index\s+%time\s+self\s+descendents\s+called\+self\s+name\s+index\s*$|' +
        r'^\s+called/total\s+children\s*$|' +
        # GNU gprof header
        r'^index\s+%\s+time\s+self\s+children\s+called\s+name\s*$'
    )

    _cg_ignore_re = re.compile(
        # spontaneous
        r'^\s+<spontaneous>\s*$|'
        # internal calls (such as "mcount")
        r'^.*\((\d+)\)$'
    )

    _cg_primary_re = re.compile(
        r'^\[(?P<index>\d+)\]' + 
        r'\s+(?P<percentage_time>\d+\.\d+)' + 
        r'\s+(?P<self>\d+\.\d+)' + 
        r'\s+(?P<descendants>\d+\.\d+)' + 
        r'\s+(?:(?P<called>\d+)(?:\+(?P<called_self>\d+))?)?' + 
        r'\s+(?P<name>\S.*?)' +
        r'(?:\s+<cycle\s(?P<cycle>\d+)>)?' +
        r'\s\[(\d+)\]$'
    )

    _cg_parent_re = re.compile(
        r'^\s+(?P<self>\d+\.\d+)?' + 
        r'\s+(?P<descendants>\d+\.\d+)?' + 
        r'\s+(?P<called>\d+)(?:/(?P<called_total>\d+))?' + 
        r'\s+(?P<name>\S.*?)' +
        r'(?:\s+<cycle\s(?P<cycle>\d+)>)?' +
        r'\s\[(?P<index>\d+)\]$'
    )

    _cg_child_re = _cg_parent_re

    _cg_cycle_header_re = re.compile(
        r'^\[(?P<index>\d+)\]' + 
        r'\s+(?P<percentage_time>\d+\.\d+)' + 
        r'\s+(?P<self>\d+\.\d+)' + 
        r'\s+(?P<descendants>\d+\.\d+)' + 
        r'\s+(?:(?P<called>\d+)(?:\+(?P<called_self>\d+))?)?' + 
        r'\s+<cycle\s(?P<cycle>\d+)\sas\sa\swhole>' +
        r'\s\[(\d+)\]$'
    )

    _cg_cycle_member_re = re.compile(
        r'^\s+(?P<self>\d+\.\d+)?' + 
        r'\s+(?P<descendants>\d+\.\d+)?' + 
        r'\s+(?P<called>\d+)(?:\+(?P<called_self>\d+))?' + 
        r'\s+(?P<name>\S.*?)' +
        r'(?:\s+<cycle\s(?P<cycle>\d+)>)?' +
        r'\s\[(?P<index>\d+)\]$'
    )

    _cg_sep_re = re.compile(r'^--+$')

    def parse_function_entry(self, lines):
        parents = []
        children = []

        while True:
            if not lines:
                sys.stderr.write('warning: unexpected end of entry\n')
            line = lines.pop(0)
            if line.startswith('['):
                break
        
            # read function parent line
            mo = self._cg_parent_re.match(line)
            if not mo:
                if self._cg_ignore_re.match(line):
                    continue
                sys.stderr.write('warning: unrecognized call graph entry: %r\n' % line)
            else:
                parent = self.translate(mo)
                parents.append(parent)

        # read primary line
        mo = self._cg_primary_re.match(line)
        if not mo:
            sys.stderr.write('warning: unrecognized call graph entry: %r\n' % line)
            return
        else:
            function = self.translate(mo)

        while lines:
            line = lines.pop(0)
            
            # read function subroutine line
            mo = self._cg_child_re.match(line)
            if not mo:
                if self._cg_ignore_re.match(line):
                    continue
                sys.stderr.write('warning: unrecognized call graph entry: %r\n' % line)
            else:
                child = self.translate(mo)
                children.append(child)
        
        function.parents = parents
        function.children = children

        self.functions[function.index] = function

    def parse_cycle_entry(self, lines):

        # read cycle header line
        line = lines[0]
        mo = self._cg_cycle_header_re.match(line)
        if not mo:
            sys.stderr.write('warning: unrecognized call graph entry: %r\n' % line)
            return
        cycle = self.translate(mo)

        # read cycle member lines
        cycle.functions = []
        for line in lines[1:]:
            mo = self._cg_cycle_member_re.match(line)
            if not mo:
                sys.stderr.write('warning: unrecognized call graph entry: %r\n' % line)
                continue
            call = self.translate(mo)
            cycle.functions.append(call)
        
        self.cycles[cycle.cycle] = cycle

    def parse_cg_entry(self, lines):
        if lines[0].startswith("["):
            self.parse_cycle_entry(lines)
        else:
            self.parse_function_entry(lines)

    def parse_cg(self):
        """Parse the call graph."""

        # skip call graph header
        while not self._cg_header_re.match(self.readline()):
            pass
        line = self.readline()
        while self._cg_header_re.match(line):
            line = self.readline()

        # process call graph entries
        entry_lines = []
        while line != '\014': # form feed
            if line and not line.isspace():
                if self._cg_sep_re.match(line):
                    self.parse_cg_entry(entry_lines)
                    entry_lines = []
                else:
                    entry_lines.append(line)            
            line = self.readline()
    
    def parse(self):
        self.parse_cg()
        self.fp.close()

        profile = Profile()
        profile[TIME] = 0.0
        
        cycles = {}
        for index in self.cycles.iterkeys():
            cycles[index] = Cycle()

        for entry in self.functions.itervalues():
            # populate the function
            function = Function(entry.index, entry.name)
            function[TIME] = entry.self
            if entry.called is not None:
                function[CALLS] = entry.called
            if entry.called_self is not None:
                call = Call(entry.index)
                call[CALLS] = entry.called_self
                function[CALLS] += entry.called_self
            
            # populate the function calls
            for child in entry.children:
                call = Call(child.index)
                
                assert child.called is not None
                call[CALLS] = child.called

                if child.index not in self.functions:
                    # NOTE: functions that were never called but were discovered by gprof's 
                    # static call graph analysis dont have a call graph entry so we need
                    # to add them here
                    missing = Function(child.index, child.name)
                    function[TIME] = 0.0
                    function[CALLS] = 0
                    profile.add_function(missing)

                function.add_call(call)

            profile.add_function(function)

            if entry.cycle is not None:
                cycles[entry.cycle].add_function(function)

            profile[TIME] = profile[TIME] + function[TIME]

        for cycle in cycles.itervalues():
            profile.add_cycle(cycle)

        # Compute derived events
        profile.validate()
        profile.ratio(TIME_RATIO, TIME)
        profile.call_ratios(CALLS)
        profile.integrate(TOTAL_TIME, TIME)
        profile.ratio(TOTAL_TIME_RATIO, TOTAL_TIME)

        return profile


class OprofileParser(LineParser):
    """Parser for oprofile callgraph output.
    
    See also:
    - http://oprofile.sourceforge.net/doc/opreport.html#opreport-callgraph
    """

    _fields_re = {
        'samples': r'(?P<samples>\d+)',
        '%': r'(?P<percentage>\S+)',
        'linenr info': r'(?P<source>\(no location information\)|\S+:\d+)',
        'image name': r'(?P<image>\S+(?:\s\(tgid:[^)]*\))?)',
        'app name': r'(?P<application>\S+)',
        'symbol name': r'(?P<symbol>\(no symbols\)|.+?)',
    }

    def __init__(self, infile):
        LineParser.__init__(self, infile)
        self.entries = {}
        self.entry_re = None

    def add_entry(self, callers, function, callees):
        try:
            entry = self.entries[function.id]
        except KeyError:
            self.entries[function.id] = (callers, function, callees)
        else:
            callers_total, function_total, callees_total = entry
            self.update_subentries_dict(callers_total, callers)
            function_total.samples += function.samples
            self.update_subentries_dict(callees_total, callees)
    
    def update_subentries_dict(self, totals, partials):
        for partial in partials.itervalues():
            try:
                total = totals[partial.id]
            except KeyError:
                totals[partial.id] = partial
            else:
                total.samples += partial.samples
        
    def parse(self):
        # read lookahead
        self.readline()

        self.parse_header()
        while self.lookahead():
            self.parse_entry()

        profile = Profile()

        reverse_call_samples = {}
        
        # populate the profile
        profile[SAMPLES] = 0
        for _callers, _function, _callees in self.entries.itervalues():
            function = Function(_function.id, _function.name)
            function[SAMPLES] = _function.samples
            profile.add_function(function)
            profile[SAMPLES] += _function.samples

            if _function.application:
                function[PROCESS] = os.path.basename(_function.application)
            if _function.image:
                function[MODULE] = os.path.basename(_function.image)

            total_callee_samples = 0
            for _callee in _callees.itervalues():
                total_callee_samples += _callee.samples

            for _callee in _callees.itervalues():
                if not _callee.self:
                    call = Call(_callee.id)
                    call[SAMPLES] = _callee.samples
                    function.add_call(call)
                
        # compute derived data
        profile.validate()
        profile.find_cycles()
        profile.ratio(TIME_RATIO, SAMPLES)
        profile.call_ratios(SAMPLES)
        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)

        return profile

    def parse_header(self):
        while not self.match_header():
            self.consume()
        line = self.lookahead()
        fields = re.split(r'\s\s+', line)
        entry_re = r'^\s*' + r'\s+'.join([self._fields_re[field] for field in fields]) + r'(?P<self>\s+\[self\])?$'
        self.entry_re = re.compile(entry_re)
        self.skip_separator()

    def parse_entry(self):
        callers = self.parse_subentries()
        if self.match_primary():
            function = self.parse_subentry()
            if function is not None:
                callees = self.parse_subentries()
                self.add_entry(callers, function, callees)
        self.skip_separator()

    def parse_subentries(self):
        subentries = {}
        while self.match_secondary():
            subentry = self.parse_subentry()
            subentries[subentry.id] = subentry
        return subentries

    def parse_subentry(self):
        entry = Struct()
        line = self.consume()
        mo = self.entry_re.match(line)
        if not mo:
            raise ParseError('failed to parse', line)
        fields = mo.groupdict()
        entry.samples = int(fields.get('samples', 0))
        entry.percentage = float(fields.get('percentage', 0.0))
        if 'source' in fields and fields['source'] != '(no location information)':
            source = fields['source']
            filename, lineno = source.split(':')
            entry.filename = filename
            entry.lineno = int(lineno)
        else:
            source = ''
            entry.filename = None
            entry.lineno = None
        entry.image = fields.get('image', '')
        entry.application = fields.get('application', '')
        if 'symbol' in fields and fields['symbol'] != '(no symbols)':
            entry.symbol = fields['symbol']
        else:
            entry.symbol = ''
        if entry.symbol.startswith('"') and entry.symbol.endswith('"'):
            entry.symbol = entry.symbol[1:-1]
        entry.id = ':'.join((entry.application, entry.image, source, entry.symbol))
        entry.self = fields.get('self', None) != None
        if entry.self:
            entry.id += ':self'
        if entry.symbol:
            entry.name = entry.symbol
        else:
            entry.name = entry.image
        return entry

    def skip_separator(self):
        while not self.match_separator():
            self.consume()
        self.consume()

    def match_header(self):
        line = self.lookahead()
        return line.startswith('samples')

    def match_separator(self):
        line = self.lookahead()
        return line == '-'*len(line)

    def match_primary(self):
        line = self.lookahead()
        return not line[:1].isspace()
    
    def match_secondary(self):
        line = self.lookahead()
        return line[:1].isspace()


class SharkParser(LineParser):
    """Parser for MacOSX Shark output.

    Author: tom@dbservice.com
    """

    def __init__(self, infile):
        LineParser.__init__(self, infile)
        self.stack = []
        self.entries = {}

    def add_entry(self, function):
        try:
            entry = self.entries[function.id]
        except KeyError:
            self.entries[function.id] = (function, { })
        else:
            function_total, callees_total = entry
            function_total.samples += function.samples
    
    def add_callee(self, function, callee):
        func, callees = self.entries[function.id]
        try:
            entry = callees[callee.id]
        except KeyError:
            callees[callee.id] = callee
        else:
            entry.samples += callee.samples
        
    def parse(self):
        self.readline()
        self.readline()
        self.readline()
        self.readline()

        match = re.compile(r'(?P<prefix>[|+ ]*)(?P<samples>\d+), (?P<symbol>[^,]+), (?P<image>.*)')

        while self.lookahead():
            line = self.consume()
            mo = match.match(line)
            if not mo:
                raise ParseError('failed to parse', line)

            fields = mo.groupdict()
            prefix = len(fields.get('prefix', 0)) / 2 - 1

            symbol = str(fields.get('symbol', 0))
            image = str(fields.get('image', 0))

            entry = Struct()
            entry.id = ':'.join([symbol, image])
            entry.samples = int(fields.get('samples', 0))

            entry.name = symbol
            entry.image = image

            # adjust the callstack
            if prefix < len(self.stack):
                del self.stack[prefix:]

            if prefix == len(self.stack):
                self.stack.append(entry)

            # if the callstack has had an entry, it's this functions caller
            if prefix > 0:
                self.add_callee(self.stack[prefix - 1], entry)
                
            self.add_entry(entry)
                
        profile = Profile()
        profile[SAMPLES] = 0
        for _function, _callees in self.entries.itervalues():
            function = Function(_function.id, _function.name)
            function[SAMPLES] = _function.samples
            profile.add_function(function)
            profile[SAMPLES] += _function.samples

            if _function.image:
                function[MODULE] = os.path.basename(_function.image)

            for _callee in _callees.itervalues():
                call = Call(_callee.id)
                call[SAMPLES] = _callee.samples
                function.add_call(call)
                
        # compute derived data
        profile.validate()
        profile.find_cycles()
        profile.ratio(TIME_RATIO, SAMPLES)
        profile.call_ratios(SAMPLES)
        profile.integrate(TOTAL_TIME_RATIO, TIME_RATIO)

        return profile


class PstatsParser:
    """Parser python profiling statistics saved with te pstats module."""

    def __init__(self, *filename):
        import pstats
        self.stats = pstats.Stats(*filename)
        self.profile = Profile()
        self.function_ids = {}

    def get_function_name(self, (filename, line, name)):
        module = os.path.splitext(filename)[0]
        module = os.path.basename(module)
        return "%s:%d:%s" % (module, line, name)

    def get_function(self, key):
        try:
            id = self.function_ids[key]
        except KeyError:
            id = len(self.function_ids)
            name = self.get_function_name(key)
            function = Function(id, name)
            self.profile.functions[id] = function
            self.function_ids[key] = id
        else:
            function = self.profile.functions[id]
        return function

    def parse(self):
        self.profile[TIME] = 0.0
        self.profile[TOTAL_TIME] = self.stats.total_tt
        for fn, (cc, nc, tt, ct, callers) in self.stats.stats.iteritems():
            callee = self.get_function(fn)
            callee[CALLS] = nc
            callee[TOTAL_TIME] = ct
            callee[TIME] = tt
            self.profile[TIME] += tt
            self.profile[TOTAL_TIME] = max(self.profile[TOTAL_TIME], ct)
            for fn, value in callers.iteritems():
                caller = self.get_function(fn)
                call = Call(callee.id)
                if isinstance(value, tuple):
                    for i in xrange(0, len(value), 4):
                        nc, cc, tt, ct = value[i:i+4]
                        if CALLS in call:
                            call[CALLS] += cc
                        else:
                            call[CALLS] = cc

                        if TOTAL_TIME in call:
                            call[TOTAL_TIME] += ct
                        else:
                            call[TOTAL_TIME] = ct

                else:
                    call[CALLS] = value
                    call[TOTAL_TIME] = ratio(value, nc)*ct

                caller.add_call(call)
        #self.stats.print_stats()
        #self.stats.print_callees()

        # Compute derived events
        self.profile.validate()
        self.profile.ratio(TIME_RATIO, TIME)
        self.profile.ratio(TOTAL_TIME_RATIO, TOTAL_TIME)

        return self.profile


class Theme:

    def __init__(self, 
            bgcolor = (0.0, 0.0, 1.0),
            mincolor = (0.0, 0.0, 0.0),
            maxcolor = (0.0, 0.0, 1.0),
            fontname = "Arial",
            minfontsize = 10.0,
            maxfontsize = 10.0,
            minpenwidth = 0.5,
            maxpenwidth = 4.0,
            gamma = 2.2):
        self.bgcolor = bgcolor
        self.mincolor = mincolor
        self.maxcolor = maxcolor
        self.fontname = fontname
        self.minfontsize = minfontsize
        self.maxfontsize = maxfontsize
        self.minpenwidth = minpenwidth
        self.maxpenwidth = maxpenwidth
        self.gamma = gamma

    def graph_bgcolor(self):
        return self.hsl_to_rgb(*self.bgcolor)

    def graph_fontname(self):
        return self.fontname

    def graph_fontsize(self):
        return self.minfontsize

    def node_bgcolor(self, weight):
        return self.color(weight)

    def node_fgcolor(self, weight):
        return self.graph_bgcolor()

    def node_fontsize(self, weight):
        return self.fontsize(weight)

    def edge_color(self, weight):
        return self.color(weight)

    def edge_fontsize(self, weight):
        return self.fontsize(weight)

    def edge_penwidth(self, weight):
        return max(weight*self.maxpenwidth, self.minpenwidth)

    def edge_arrowsize(self, weight):
        return 0.5 * math.sqrt(self.edge_penwidth(weight))

    def fontsize(self, weight):
        return max(weight**2 * self.maxfontsize, self.minfontsize)

    def color(self, weight):
        weight = min(max(weight, 0.0), 1.0)
    
        hmin, smin, lmin = self.mincolor
        hmax, smax, lmax = self.maxcolor

        h = hmin + weight*(hmax - hmin)
        s = smin + weight*(smax - smin)
        l = lmin + weight*(lmax - lmin)

        return self.hsl_to_rgb(h, s, l)

    def hsl_to_rgb(self, h, s, l):
        """Convert a color from HSL color-model to RGB.

        See also:
        - http://www.w3.org/TR/css3-color/#hsl-color
        """

        h = h % 1.0
        s = min(max(s, 0.0), 1.0)
        l = min(max(l, 0.0), 1.0)

        if l <= 0.5:
            m2 = l*(s + 1.0)
        else:
            m2 = l + s - l*s
        m1 = l*2.0 - m2
        r = self._hue_to_rgb(m1, m2, h + 1.0/3.0)
        g = self._hue_to_rgb(m1, m2, h)
        b = self._hue_to_rgb(m1, m2, h - 1.0/3.0)

        # Apply gamma correction
        r **= self.gamma
        g **= self.gamma
        b **= self.gamma

        return (r, g, b)

    def _hue_to_rgb(self, m1, m2, h):
        if h < 0.0:
            h += 1.0
        elif h > 1.0:
            h -= 1.0
        if h*6 < 1.0:
            return m1 + (m2 - m1)*h*6.0
        elif h*2 < 1.0:
            return m2
        elif h*3 < 2.0:
            return m1 + (m2 - m1)*(2.0/3.0 - h)*6.0
        else:
            return m1


TEMPERATURE_COLORMAP = Theme(
    mincolor = (2.0/3.0, 0.80, 0.25), # dark blue
    maxcolor = (0.0, 1.0, 0.5), # satured red
    gamma = 1.0
)

PINK_COLORMAP = Theme(
    mincolor = (0.0, 1.0, 0.90), # pink
    maxcolor = (0.0, 1.0, 0.5), # satured red
)

GRAY_COLORMAP = Theme(
    mincolor = (0.0, 0.0, 0.85), # light gray
    maxcolor = (0.0, 0.0, 0.0), # black
)

BW_COLORMAP = Theme(
    minfontsize = 8.0,
    maxfontsize = 24.0,
    mincolor = (0.0, 0.0, 0.0), # black
    maxcolor = (0.0, 0.0, 0.0), # black
    minpenwidth = 0.1,
    maxpenwidth = 8.0,
)


class DotWriter:
    """Writer for the DOT language.

    See also:
    - "The DOT Language" specification
      http://www.graphviz.org/doc/info/lang.html
    """

    def __init__(self, fp):
        self.fp = fp

    def graph(self, profile, theme):
        self.begin_graph()

        fontname = theme.graph_fontname()

        self.attr('graph', fontname=fontname, ranksep=0.25, nodesep=0.125)
        self.attr('node', fontname=fontname, shape="box", style="filled,rounded", fontcolor="white", width=0, height=0)
        self.attr('edge', fontname=fontname)

        for function in profile.functions.itervalues():
            labels = []
            for event in PROCESS, MODULE:
                if event in function.events:
                    label = event.format(function[event])
                    labels.append(label)
            labels.append(function.name)
            for event in TOTAL_TIME_RATIO, TIME_RATIO, CALLS:
                if event in function.events:
                    label = event.format(function[event])
                    labels.append(label)

            try:
                weight = function[PRUNE_RATIO]
            except UndefinedEvent:
                weight = 0.0

            label = '\n'.join(labels)
            self.node(function.id, 
                label = label, 
                color = self.color(theme.node_bgcolor(weight)), 
                fontcolor = self.color(theme.node_fgcolor(weight)), 
                fontsize = "%.2f" % theme.node_fontsize(weight),
            )

            for call in function.calls.itervalues():
                callee = profile.functions[call.callee_id]

                labels = []
                for event in TOTAL_TIME_RATIO, CALLS:
                    if event in call.events:
                        label = event.format(call[event])
                        labels.append(label)

                try:
                    weight = call[PRUNE_RATIO]
                except UndefinedEvent:
                    try:
                        weight = callee[PRUNE_RATIO]
                    except UndefinedEvent:
                        weight = 0.0

                label = '\n'.join(labels)

                self.edge(function.id, call.callee_id, 
                    label = label, 
                    color = self.color(theme.edge_color(weight)), 
                    fontcolor = self.color(theme.edge_color(weight)),
                    fontsize = "%.2f" % theme.edge_fontsize(weight), 
                    penwidth = "%.2f" % theme.edge_penwidth(weight), 
                    labeldistance = "%.2f" % theme.edge_penwidth(weight), 
                    arrowsize = "%.2f" % theme.edge_arrowsize(weight),
                )

        self.end_graph()

    def begin_graph(self):
        self.write('digraph {\n')

    def end_graph(self):
        self.write('}\n')

    def attr(self, what, **attrs):
        self.write("\t")
        self.write(what)
        self.attr_list(attrs)
        self.write(";\n")

    def node(self, node, **attrs):
        self.write("\t")
        self.id(node)
        self.attr_list(attrs)
        self.write(";\n")

    def edge(self, src, dst, **attrs):
        self.write("\t")
        self.id(src)
        self.write(" -> ")
        self.id(dst)
        self.attr_list(attrs)
        self.write(";\n")

    def attr_list(self, attrs):
        if not attrs:
            return
        self.write(' [')
        first = True
        for name, value in attrs.iteritems():
            if first:
                first = False
            else:
                self.write(", ")
            self.id(name)
            self.write('=')
            self.id(value)
        self.write(']')

    def id(self, id):
        if isinstance(id, (int, float)):
            s = str(id)
        elif isinstance(id, str):
            if id.isalnum():
                s = id
            else:
                s = self.escape(id)
        else:
            raise TypeError
        self.write(s)

    def color(self, (r, g, b)):

        def float2int(f):
            if f <= 0.0:
                return 0
            if f >= 1.0:
                return 255
            return int(255.0*f + 0.5)

        return "#" + "".join(["%02x" % float2int(c) for c in (r, g, b)])

    def escape(self, s):
        s = s.encode('utf-8')
        s = s.replace('\\', r'\\')
        s = s.replace('\n', r'\n')
        s = s.replace('\t', r'\t')
        s = s.replace('"', r'\"')
        return '"' + s + '"'

    def write(self, s):
        self.fp.write(s)


class Main:
    """Main program."""

    themes = {
            "color": TEMPERATURE_COLORMAP,
            "pink": PINK_COLORMAP,
            "gray": GRAY_COLORMAP,
            "bw": BW_COLORMAP,
    }

    def main(self):
        """Main program."""

        parser = optparse.OptionParser(
            usage="\n\t%prog [options] [file] ...",
            version="%%prog %s" % __version__)
        parser.add_option(
            '-o', '--output', metavar='FILE',
            type="string", dest="output",
            help="output filename [stdout]")
        parser.add_option(
            '-n', '--node-thres', metavar='PERCENTAGE',
            type="float", dest="node_thres", default=0.5,
            help="eliminate nodes below this threshold [default: %default]")
        parser.add_option(
            '-e', '--edge-thres', metavar='PERCENTAGE',
            type="float", dest="edge_thres", default=0.1,
            help="eliminate edges below this threshold [default: %default]")
        parser.add_option(
            '-f', '--format',
            type="choice", choices=('prof', 'oprofile', 'pstats', 'shark'),
            dest="format", default="prof",
            help="profile format: prof, oprofile, or pstats [default: %default]")
        parser.add_option(
            '-c', '--colormap',
            type="choice", choices=('color', 'pink', 'gray', 'bw'),
            dest="theme", default="color",
            help="color map: color, pink, gray, or bw [default: %default]")
        parser.add_option(
            '-s', '--strip',
            action="store_true",
            dest="strip", default=False,
            help="strip function parameters, template parameters, and const modifiers from demangled C++ function names")
        parser.add_option(
            '-w', '--wrap',
            action="store_true",
            dest="wrap", default=False,
            help="wrap function names")
        (self.options, self.args) = parser.parse_args(sys.argv[1:])

        if len(self.args) > 1 and self.options.format != 'pstats':
            parser.error('incorrect number of arguments')

        try:
            self.theme = self.themes[self.options.theme]
        except KeyError:
            parser.error('invalid colormap \'%s\'' % self.options.theme)

        if self.options.format == 'prof':
            if not self.args:
                fp = sys.stdin
            else:
                fp = open(self.args[0], 'rt')
            parser = GprofParser(fp)
        elif self.options.format == 'oprofile':
            if not self.args:
                fp = sys.stdin
            else:
                fp = open(self.args[0], 'rt')
            parser = OprofileParser(fp)
        elif self.options.format == 'pstats':
            if not self.args:
                parser.error('at least a file must be specified for pstats input')
            parser = PstatsParser(*self.args)
        elif self.options.format == 'shark':
            if not self.args:
                fp = sys.stdin
            else:
                fp = open(self.args[0], 'rt')
            parser = SharkParser(fp)
        else:
            parser.error('invalid format \'%s\'' % self.options.format)

        self.profile = parser.parse()
        
        if self.options.output is None:
            self.output = sys.stdout
        else:
            self.output = open(self.options.output, 'wt')

        self.write_graph()

    _parenthesis_re = re.compile(r'\([^()]*\)')
    _angles_re = re.compile(r'<[^<>]*>')
    _const_re = re.compile(r'\s+const$')

    def strip_function_name(self, name):
        """Remove extraneous information from C++ demangled function names."""

        # Strip function parameters from name by recursively removing paired parenthesis
        while True:
            name, n = self._parenthesis_re.subn('', name)
            if not n:
                break

        # Strip const qualifier
        name = self._const_re.sub('', name)

        # Strip template parameters from name by recursively removing paired angles
        while True:
            name, n = self._angles_re.subn('', name)
            if not n:
                break

        return name

    def wrap_function_name(self, name):
        """Split the function name on multiple lines."""

        if len(name) > 32:
            ratio = 2.0/3.0
            height = max(int(len(name)/(1.0 - ratio) + 0.5), 1)
            width = max(len(name)/height, 32)
            # TODO: break lines in symbols
            name = textwrap.fill(name, width, break_long_words=False)

        # Take away spaces
        name = name.replace(", ", ",")
        name = name.replace("> >", ">>")
        name = name.replace("> >", ">>") # catch consecutive

        return name

    def compress_function_name(self, name):
        """Compress function name according to the user preferences."""

        if self.options.strip:
            name = self.strip_function_name(name)

        if self.options.wrap:
            name = self.wrap_function_name(name)

        # TODO: merge functions with same resulting name

        return name

    def write_graph(self):
        dot = DotWriter(self.output)
        profile = self.profile
        profile.prune(self.options.node_thres/100.0, self.options.edge_thres/100.0)

        for function in profile.functions.itervalues():
            function.name = self.compress_function_name(function.name)

        dot.graph(profile, self.theme)


if __name__ == '__main__':
    Main().main()

########NEW FILE########
__FILENAME__ = runtests
#!/usr/bin/env python
#encoding:utf-8
#author:dbr/Ben
#project:tvdb_api
#repository:http://github.com/dbr/tvdb_api
#license:unlicense (http://unlicense.org/)

import sys
import unittest

import test_tvdb_api

def main():
    suite = unittest.TestSuite([
        unittest.TestLoader().loadTestsFromModule(test_tvdb_api)
    ])
    
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    if result.wasSuccessful():
        return 0
    else:
        return 1

if __name__ == '__main__':
    sys.exit(
        int(main())
    )

########NEW FILE########
__FILENAME__ = test_tvdb_api
#!/usr/bin/env python
#encoding:utf-8
#author:dbr/Ben
#project:tvdb_api
#repository:http://github.com/dbr/tvdb_api
#license:unlicense (http://unlicense.org/)

"""Unittests for tvdb_api
"""

import os
import sys
import datetime
import unittest

# Force parent directory onto path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import tvdb_api
import tvdb_ui
from tvdb_api import (tvdb_shownotfound, tvdb_seasonnotfound,
tvdb_episodenotfound, tvdb_attributenotfound)

class test_tvdb_basic(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t = None
    
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, banners = False)
     
    def test_different_case(self):
        """Checks the auto-correction of show names is working.
        It should correct the weirdly capitalised 'sCruBs' to 'Scrubs'
        """
        self.assertEquals(self.t['scrubs'][1][4]['episodename'], 'My Old Lady')
        self.assertEquals(self.t['sCruBs']['seriesname'], 'Scrubs')

    def test_spaces(self):
        """Checks shownames with spaces
        """
        self.assertEquals(self.t['My Name Is Earl']['seriesname'], 'My Name Is Earl')
        self.assertEquals(self.t['My Name Is Earl'][1][4]['episodename'], 'Faked His Own Death')

    def test_numeric(self):
        """Checks numeric show names
        """
        self.assertEquals(self.t['24'][2][20]['episodename'], 'Day 2: 3:00 A.M.-4:00 A.M.')
        self.assertEquals(self.t['24']['seriesname'], '24')

    def test_show_iter(self):
        """Iterating over a show returns each seasons
        """
        self.assertEquals(
            len(
                [season for season in self.t['Life on Mars']]
            ),
            2
        )
    
    def test_season_iter(self):
        """Iterating over a show returns episodes
        """
        self.assertEquals(
            len(
                [episode for episode in self.t['Life on Mars'][1]]
            ),
            8
        )

    def test_get_episode_overview(self):
        """Checks episode overview is retrieved correctly.
        """
        self.assertEquals(
            self.t['Battlestar Galactica (2003)'][1][6]['overview'].startswith(
                'When a new copy of Doral, a Cylon who had been previously'),
            True
        )

    def test_get_parent(self):
        """Check accessing series from episode instance
        """
        show = self.t['Battlestar Galactica (2003)']
        season = show[1]
        episode = show[1][1]

        self.assertEquals(
            season.show,
            show
        )

        self.assertEquals(
            episode.season,
            season
        )

        self.assertEquals(
            episode.season.show,
            show
        )

    def test_no_season(self):
        show = self.t['Katekyo Hitman Reborn']
        print tvdb_api
        print show[1][1]

class test_tvdb_errors(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t = None
    
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, banners = False)

    def test_seasonnotfound(self):
        """Checks exception is thrown when season doesn't exist.
        """
        self.assertRaises(tvdb_seasonnotfound, lambda:self.t['CNNNN'][10][1])

    def test_shownotfound(self):
        """Checks exception is thrown when episode doesn't exist.
        """
        self.assertRaises(tvdb_shownotfound, lambda:self.t['the fake show thingy'])
    
    def test_episodenotfound(self):
        """Checks exception is raised for non-existent episode
        """
        self.assertRaises(tvdb_episodenotfound, lambda:self.t['Scrubs'][1][30])

    def test_attributenamenotfound(self):
        """Checks exception is thrown for if an attribute isn't found.
        """
        self.assertRaises(tvdb_attributenotfound, lambda:self.t['CNNNN'][1][6]['afakeattributething'])
        self.assertRaises(tvdb_attributenotfound, lambda:self.t['CNNNN']['afakeattributething'])

class test_tvdb_search(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t = None
    
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, banners = False)

    def test_search_len(self):
        """There should be only one result matching
        """
        self.assertEquals(len(self.t['My Name Is Earl'].search('Faked His Own Death')), 1)

    def test_search_checkname(self):
        """Checks you can get the episode name of a search result
        """
        self.assertEquals(self.t['Scrubs'].search('my first')[0]['episodename'], 'My First Day')
        self.assertEquals(self.t['My Name Is Earl'].search('Faked His Own Death')[0]['episodename'], 'Faked His Own Death')
    
    def test_search_multiresults(self):
        """Checks search can return multiple results
        """
        self.assertEquals(len(self.t['Scrubs'].search('my first')) >= 3, True)

    def test_search_no_params_error(self):
        """Checks not supplying search info raises TypeError"""
        self.assertRaises(
            TypeError,
            lambda: self.t['Scrubs'].search()
        )

    def test_search_season(self):
        """Checks the searching of a single season"""
        self.assertEquals(
            len(self.t['Scrubs'][1].search("First")),
            3
        )
    
    def test_search_show(self):
        """Checks the searching of an entire show"""
        self.assertEquals(
            len(self.t['CNNNN'].search('CNNNN', key='episodename')),
            3
        )

    def test_aired_on(self):
        """Tests airedOn show method"""
        sr = self.t['Scrubs'].airedOn(datetime.date(2001, 10, 2))
        self.assertEquals(len(sr), 1)
        self.assertEquals(sr[0]['episodename'], u'My First Day')

class test_tvdb_data(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t = None
    
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, banners = False)

    def test_episode_data(self):
        """Check the firstaired value is retrieved
        """
        self.assertEquals(
            self.t['lost']['firstaired'],
            '2004-09-22'
        )

class test_tvdb_misc(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t = None
    
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, banners = False)

    def test_repr_show(self):
        """Check repr() of Season
        """
        self.assertEquals(
            repr(self.t['CNNNN']),
            "<Show Chaser Non-Stop News Network (CNNNN) (containing 3 seasons)>"
        )
    def test_repr_season(self):
        """Check repr() of Season
        """
        self.assertEquals(
            repr(self.t['CNNNN'][1]),
            "<Season instance (containing 9 episodes)>"
        )
    def test_repr_episode(self):
        """Check repr() of Episode
        """
        self.assertEquals(
            repr(self.t['CNNNN'][1][1]),
            "<Episode 01x01 - Terror Alert>"
        )
    def test_have_all_languages(self):
        """Check valid_languages is up-to-date (compared to languages.xml)
        """
        et = self.t._getetsrc(
            "http://thetvdb.com/api/%s/languages.xml" % (
                self.t.config['apikey']
            )
        )
        languages = [x.find("abbreviation").text for x in et.findall("Language")]
        
        self.assertEquals(
            sorted(languages),
            sorted(self.t.config['valid_languages'])
        )
        
class test_tvdb_languages(unittest.TestCase):
    def test_episode_name_french(self):
        """Check episode data is in French (language="fr")
        """
        t = tvdb_api.Tvdb(cache = True, language = "fr")
        self.assertEquals(
            t['scrubs'][1][1]['episodename'],
            "Mon premier jour"
        )
        self.assertTrue(
            t['scrubs']['overview'].startswith(
                u"J.D. est un jeune m\xe9decin qui d\xe9bute"
            )
        )

    def test_episode_name_spanish(self):
        """Check episode data is in Spanish (language="es")
        """
        t = tvdb_api.Tvdb(cache = True, language = "es")
        self.assertEquals(
            t['scrubs'][1][1]['episodename'],
            "Mi Primer Dia"
        )
        self.assertTrue(
            t['scrubs']['overview'].startswith(
                u'Scrubs es una divertida comedia'
            )
        )

    def test_multilanguage_selection(self):
        """Check selected language is used
        """
        class SelectEnglishUI(tvdb_ui.BaseUI):
            def selectSeries(self, allSeries):
                return [x for x in allSeries if x['language'] == "en"][0]

        class SelectItalianUI(tvdb_ui.BaseUI):
            def selectSeries(self, allSeries):
                return [x for x in allSeries if x['language'] == "it"][0]

        t_en = tvdb_api.Tvdb(
            cache=True,
            custom_ui = SelectEnglishUI,
            language = "en")
        t_it = tvdb_api.Tvdb(
            cache=True,
            custom_ui = SelectItalianUI,
            language = "it")

        self.assertEquals(
            t_en['dexter'][1][2]['episodename'], "Crocodile"
        )
        self.assertEquals(
            t_it['dexter'][1][2]['episodename'], "Lacrime di coccodrillo"
        )


class test_tvdb_unicode(unittest.TestCase):
    def test_search_in_chinese(self):
        """Check searching for show with language=zh returns Chinese seriesname
        """
        t = tvdb_api.Tvdb(cache = True, language = "zh")
        show = t[u'T\xecnh Ng\u01b0\u1eddi Hi\u1ec7n \u0110\u1ea1i']
        self.assertEquals(
            type(show),
            tvdb_api.Show
        )
        
        self.assertEquals(
            show['seriesname'],
            u'T\xecnh Ng\u01b0\u1eddi Hi\u1ec7n \u0110\u1ea1i'
        )

    def test_search_in_all_languages(self):
        """Check search_all_languages returns Chinese show, with language=en
        """
        t = tvdb_api.Tvdb(cache = True, search_all_languages = True, language="en")
        show = t[u'T\xecnh Ng\u01b0\u1eddi Hi\u1ec7n \u0110\u1ea1i']
        self.assertEquals(
            type(show),
            tvdb_api.Show
        )
        
        self.assertEquals(
            show['seriesname'],
            u'Virtues Of Harmony II'
        )

class test_tvdb_banners(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t = None
    
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, banners = True)

    def test_have_banners(self):
        """Check banners at least one banner is found
        """
        self.assertEquals(
            len(self.t['scrubs']['_banners']) > 0,
            True
        )

    def test_banner_url(self):
        """Checks banner URLs start with http://
        """
        for banner_type, banner_data in self.t['scrubs']['_banners'].items():
            for res, res_data in banner_data.items():
                for bid, banner_info in res_data.items():
                    self.assertEquals(
                        banner_info['_bannerpath'].startswith("http://"),
                        True
                    )

    def test_episode_image(self):
        """Checks episode 'filename' image is fully qualified URL
        """
        self.assertEquals(
            self.t['scrubs'][1][1]['filename'].startswith("http://"),
            True
        )
    
    def test_show_artwork(self):
        """Checks various image URLs within season data are fully qualified
        """
        for key in ['banner', 'fanart', 'poster']:
            self.assertEquals(
                self.t['scrubs'][key].startswith("http://"),
                True
            )

class test_tvdb_actors(unittest.TestCase):
    t = None
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, actors = True)

    def test_actors_is_correct_datatype(self):
        """Check show/_actors key exists and is correct type"""
        self.assertTrue(
            isinstance(
                self.t['scrubs']['_actors'],
                tvdb_api.Actors
            )
        )
    
    def test_actors_has_actor(self):
        """Check show has at least one Actor
        """
        self.assertTrue(
            isinstance(
                self.t['scrubs']['_actors'][0],
                tvdb_api.Actor
            )
        )
    
    def test_actor_has_name(self):
        """Check first actor has a name"""
        self.assertEquals(
            self.t['scrubs']['_actors'][0]['name'],
            "Zach Braff"
        )

    def test_actor_image_corrected(self):
        """Check image URL is fully qualified
        """
        for actor in self.t['scrubs']['_actors']:
            if actor['image'] is not None:
                # Actor's image can be None, it displays as the placeholder
                # image on thetvdb.com
                self.assertTrue(
                    actor['image'].startswith("http://")
                )

class test_tvdb_doctest(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t = None
    
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, banners = False)
    
    def test_doctest(self):
        """Check docstring examples works"""
        import doctest
        doctest.testmod(tvdb_api)


class test_tvdb_custom_caching(unittest.TestCase):
    def test_true_false_string(self):
        """Tests setting cache to True/False/string

        Basic tests, only checking for errors
        """

        tvdb_api.Tvdb(cache = True)
        tvdb_api.Tvdb(cache = False)
        tvdb_api.Tvdb(cache = "/tmp")

    def test_invalid_cache_option(self):
        """Tests setting cache to invalid value
        """

        try:
            tvdb_api.Tvdb(cache = 2.3)
        except ValueError:
            pass
        else:
            self.fail("Expected ValueError from setting cache to float")

    def test_custom_urlopener(self):
        class UsedCustomOpener(Exception):
            pass

        import urllib2
        class TestOpener(urllib2.BaseHandler):
            def default_open(self, request):
                print request.get_method()
                raise UsedCustomOpener("Something")

        custom_opener = urllib2.build_opener(TestOpener())
        t = tvdb_api.Tvdb(cache = custom_opener)
        try:
            t['scrubs']
        except UsedCustomOpener:
            pass
        else:
            self.fail("Did not use custom opener")

class test_tvdb_by_id(unittest.TestCase):
    t = None
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, actors = True)

    def test_actors_is_correct_datatype(self):
        """Check show/_actors key exists and is correct type"""
        self.assertEquals(
            self.t[76156]['seriesname'],
            'Scrubs'
            )


class test_tvdb_zip(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t = None

    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, useZip = True)

    def test_get_series_from_zip(self):
        """
        """
        self.assertEquals(self.t['scrubs'][1][4]['episodename'], 'My Old Lady')
        self.assertEquals(self.t['sCruBs']['seriesname'], 'Scrubs')

    def test_spaces_from_zip(self):
        """Checks shownames with spaces
        """
        self.assertEquals(self.t['My Name Is Earl']['seriesname'], 'My Name Is Earl')
        self.assertEquals(self.t['My Name Is Earl'][1][4]['episodename'], 'Faked His Own Death')


class test_tvdb_show_ordering(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t_dvd = None
    t_air = None

    def setUp(self):
        if self.t_dvd is None:
            self.t_dvd = tvdb_api.Tvdb(cache = True, useZip = True, dvdorder=True)

        if self.t_air is None:
            self.t_air = tvdb_api.Tvdb(cache = True, useZip = True)

    def test_ordering(self):
        """Test Tvdb.search method
        """
        self.assertEquals(u'The Train Job', self.t_air['Firefly'][1][1]['episodename'])
        self.assertEquals(u'Serenity', self.t_dvd['Firefly'][1][1]['episodename'])

        self.assertEquals(u'The Cat & the Claw (Part 1)', self.t_air['Batman The Animated Series'][1][1]['episodename'])
        self.assertEquals(u'On Leather Wings', self.t_dvd['Batman The Animated Series'][1][1]['episodename'])

class test_tvdb_show_search(unittest.TestCase):
    # Used to store the cached instance of Tvdb()
    t = None

    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, useZip = True)

    def test_search(self):
        """Test Tvdb.search method
        """
        results = self.t.search("my name is earl")
        all_ids = [x['seriesid'] for x in results]
        self.assertTrue('75397' in all_ids)


class test_tvdb_alt_names(unittest.TestCase):
    t = None
    def setUp(self):
        if self.t is None:
            self.__class__.t = tvdb_api.Tvdb(cache = True, actors = True)

    def test_1(self):
        """Tests basic access of series name alias
        """
        results = self.t.search("Don't Trust the B---- in Apartment 23")
        series = results[0]
        self.assertTrue(
            'Apartment 23' in series['aliasnames']
        )


if __name__ == '__main__':
    runner = unittest.TextTestRunner(verbosity = 2)
    unittest.main(testRunner = runner)

########NEW FILE########
__FILENAME__ = tvdb_api
#!/usr/bin/env python
#encoding:utf-8
#author:dbr/Ben
#project:tvdb_api
#repository:http://github.com/dbr/tvdb_api
#license:unlicense (http://unlicense.org/)

"""Simple-to-use Python interface to The TVDB's API (thetvdb.com)

Example usage:

>>> from tvdb_api import Tvdb
>>> t = Tvdb()
>>> t['Lost'][4][11]['episodename']
u'Cabin Fever'
"""
__author__ = "dbr/Ben"
__version__ = "1.9"

import os
import time
import urllib
import urllib2
import getpass
import StringIO
import tempfile
import warnings
import logging
import datetime
import zipfile

try:
    import xml.etree.cElementTree as ElementTree
except ImportError:
    import xml.etree.ElementTree as ElementTree

try:
    import gzip
except ImportError:
    gzip = None


from tvdb_cache import CacheHandler

from tvdb_ui import BaseUI, ConsoleUI
from tvdb_exceptions import (tvdb_error, tvdb_userabort, tvdb_shownotfound,
    tvdb_seasonnotfound, tvdb_episodenotfound, tvdb_attributenotfound)

lastTimeout = None

def log():
    return logging.getLogger("tvdb_api")


class ShowContainer(dict):
    """Simple dict that holds a series of Show instances
    """

    def __init__(self):
        self._stack = []
        self._lastgc = time.time()

    def __setitem__(self, key, value):
        self._stack.append(key)

        #keep only the 100th latest results
        if time.time() - self._lastgc > 20:
            for o in self._stack[:-100]:
                del self[o]
            self._stack = self._stack[-100:]

            self._lastgc = time.time()

        super(ShowContainer, self).__setitem__(key, value)


class Show(dict):
    """Holds a dict of seasons, and show data.
    """
    def __init__(self):
        dict.__init__(self)
        self.data = {}

    def __repr__(self):
        return "<Show %s (containing %s seasons)>" % (
            self.data.get(u'seriesname', 'instance'),
            len(self)
        )

    def __getitem__(self, key):
        if key in self:
            # Key is an episode, return it
            return dict.__getitem__(self, key)

        if key in self.data:
            # Non-numeric request is for show-data
            return dict.__getitem__(self.data, key)

        # Data wasn't found, raise appropriate error
        if isinstance(key, int) or key.isdigit():
            # Episode number x was not found
            raise tvdb_seasonnotfound("Could not find season %s" % (repr(key)))
        else:
            # If it's not numeric, it must be an attribute name, which
            # doesn't exist, so attribute error.
            raise tvdb_attributenotfound("Cannot find attribute %s" % (repr(key)))

    def airedOn(self, date):
        ret = self.search(str(date), 'firstaired')
        if len(ret) == 0:
            raise tvdb_episodenotfound("Could not find any episodes that aired on %s" % date)
        return ret

    def search(self, term = None, key = None):
        """
        Search all episodes in show. Can search all data, or a specific key (for
        example, episodename)

        Always returns an array (can be empty). First index contains the first
        match, and so on.

        Each array index is an Episode() instance, so doing
        search_results[0]['episodename'] will retrieve the episode name of the
        first match.

        Search terms are converted to lower case (unicode) strings.

        # Examples
        
        These examples assume t is an instance of Tvdb():
        
        >>> t = Tvdb()
        >>>

        To search for all episodes of Scrubs with a bit of data
        containing "my first day":

        >>> t['Scrubs'].search("my first day")
        [<Episode 01x01 - My First Day>]
        >>>

        Search for "My Name Is Earl" episode named "Faked His Own Death":

        >>> t['My Name Is Earl'].search('Faked His Own Death', key = 'episodename')
        [<Episode 01x04 - Faked His Own Death>]
        >>>

        To search Scrubs for all episodes with "mentor" in the episode name:

        >>> t['scrubs'].search('mentor', key = 'episodename')
        [<Episode 01x02 - My Mentor>, <Episode 03x15 - My Tormented Mentor>]
        >>>

        # Using search results

        >>> results = t['Scrubs'].search("my first")
        >>> print results[0]['episodename']
        My First Day
        >>> for x in results: print x['episodename']
        My First Day
        My First Step
        My First Kill
        >>>
        """
        results = []
        for cur_season in self.values():
            searchresult = cur_season.search(term = term, key = key)
            if len(searchresult) != 0:
                results.extend(searchresult)

        return results


class Season(dict):
    def __init__(self, show = None):
        """The show attribute points to the parent show
        """
        self.show = show

    def __repr__(self):
        return "<Season instance (containing %s episodes)>" % (
            len(self.keys())
        )

    def __getitem__(self, episode_number):
        if episode_number not in self:
            raise tvdb_episodenotfound("Could not find episode %s" % (repr(episode_number)))
        else:
            return dict.__getitem__(self, episode_number)

    def search(self, term = None, key = None):
        """Search all episodes in season, returns a list of matching Episode
        instances.

        >>> t = Tvdb()
        >>> t['scrubs'][1].search('first day')
        [<Episode 01x01 - My First Day>]
        >>>

        See Show.search documentation for further information on search
        """
        results = []
        for ep in self.values():
            searchresult = ep.search(term = term, key = key)
            if searchresult is not None:
                results.append(
                    searchresult
                )
        return results


class Episode(dict):
    def __init__(self, season = None):
        """The season attribute points to the parent season
        """
        self.season = season

    def __repr__(self):
        seasno = int(self.get(u'seasonnumber', 0))
        epno = int(self.get(u'episodenumber', 0))
        epname = self.get(u'episodename')
        if epname is not None:
            return "<Episode %02dx%02d - %s>" % (seasno, epno, epname)
        else:
            return "<Episode %02dx%02d>" % (seasno, epno)

    def __getitem__(self, key):
        try:
            return dict.__getitem__(self, key)
        except KeyError:
            raise tvdb_attributenotfound("Cannot find attribute %s" % (repr(key)))

    def search(self, term = None, key = None):
        """Search episode data for term, if it matches, return the Episode (self).
        The key parameter can be used to limit the search to a specific element,
        for example, episodename.
        
        This primarily for use use by Show.search and Season.search. See
        Show.search for further information on search

        Simple example:

        >>> e = Episode()
        >>> e['episodename'] = "An Example"
        >>> e.search("examp")
        <Episode 00x00 - An Example>
        >>>

        Limiting by key:

        >>> e.search("examp", key = "episodename")
        <Episode 00x00 - An Example>
        >>>
        """
        if term == None:
            raise TypeError("must supply string to search for (contents)")

        term = unicode(term).lower()
        for cur_key, cur_value in self.items():
            cur_key, cur_value = unicode(cur_key).lower(), unicode(cur_value).lower()
            if key is not None and cur_key != key:
                # Do not search this key
                continue
            if cur_value.find( unicode(term).lower() ) > -1:
                return self


class Actors(list):
    """Holds all Actor instances for a show
    """
    pass


class Actor(dict):
    """Represents a single actor. Should contain..

    id,
    image,
    name,
    role,
    sortorder
    """
    def __repr__(self):
        return "<Actor \"%s\">" % (self.get("name"))


class Tvdb:
    """Create easy-to-use interface to name of season/episode name
    >>> t = Tvdb()
    >>> t['Scrubs'][1][24]['episodename']
    u'My Last Day'
    """
    def __init__(self,
                interactive = False,
                select_first = False,
                debug = False,
                cache = True,
                banners = False,
                actors = False,
                custom_ui = None,
                language = None,
                search_all_languages = False,
                apikey = None,
                forceConnect=False,
                useZip=False,
                dvdorder=False):

        """interactive (True/False):
            When True, uses built-in console UI is used to select the correct show.
            When False, the first search result is used.

        select_first (True/False):
            Automatically selects the first series search result (rather
            than showing the user a list of more than one series).
            Is overridden by interactive = False, or specifying a custom_ui

        debug (True/False) DEPRECATED:
             Replaced with proper use of logging module. To show debug messages:

                 >>> import logging
                 >>> logging.basicConfig(level = logging.DEBUG)

        cache (True/False/str/unicode/urllib2 opener):
            Retrieved XML are persisted to to disc. If true, stores in
            tvdb_api folder under your systems TEMP_DIR, if set to
            str/unicode instance it will use this as the cache
            location. If False, disables caching.  Can also be passed
            an arbitrary Python object, which is used as a urllib2
            opener, which should be created by urllib2.build_opener

        banners (True/False):
            Retrieves the banners for a show. These are accessed
            via the _banners key of a Show(), for example:

            >>> Tvdb(banners=True)['scrubs']['_banners'].keys()
            ['fanart', 'poster', 'series', 'season']

        actors (True/False):
            Retrieves a list of the actors for a show. These are accessed
            via the _actors key of a Show(), for example:

            >>> t = Tvdb(actors=True)
            >>> t['scrubs']['_actors'][0]['name']
            u'Zach Braff'

        custom_ui (tvdb_ui.BaseUI subclass):
            A callable subclass of tvdb_ui.BaseUI (overrides interactive option)

        language (2 character language abbreviation):
            The language of the returned data. Is also the language search
            uses. Default is "en" (English). For full list, run..

            >>> Tvdb().config['valid_languages'] #doctest: +ELLIPSIS
            ['da', 'fi', 'nl', ...]

        search_all_languages (True/False):
            By default, Tvdb will only search in the language specified using
            the language option. When this is True, it will search for the
            show in and language
        
        apikey (str/unicode):
            Override the default thetvdb.com API key. By default it will use
            tvdb_api's own key (fine for small scripts), but you can use your
            own key if desired - this is recommended if you are embedding
            tvdb_api in a larger application)
            See http://thetvdb.com/?tab=apiregister to get your own key

        forceConnect (bool):
            If true it will always try to connect to theTVDB.com even if we
            recently timed out. By default it will wait one minute before
            trying again, and any requests within that one minute window will
            return an exception immediately.

        useZip (bool):
            Download the zip archive where possibale, instead of the xml.
            This is only used when all episodes are pulled.
            And only the main language xml is used, the actor and banner xml are lost.
        """
        
        global lastTimeout
        
        # if we're given a lastTimeout that is less than 1 min just give up
        if not forceConnect and lastTimeout != None and datetime.datetime.now() - lastTimeout < datetime.timedelta(minutes=1):
            raise tvdb_error("We recently timed out, so giving up early this time")
        
        self.shows = ShowContainer() # Holds all Show classes
        self.corrections = {} # Holds show-name to show_id mapping

        self.config = {}

        if apikey is not None:
            self.config['apikey'] = apikey
        else:
            self.config['apikey'] = "0629B785CE550C8D" # tvdb_api's API key

        self.config['debug_enabled'] = debug # show debugging messages

        self.config['custom_ui'] = custom_ui

        self.config['interactive'] = interactive # prompt for correct series?

        self.config['select_first'] = select_first

        self.config['search_all_languages'] = search_all_languages

        self.config['useZip'] = useZip

        self.config['dvdorder'] = dvdorder

        if cache is True:
            self.config['cache_enabled'] = True
            self.config['cache_location'] = self._getTempDir()
            self.urlopener = urllib2.build_opener(
                CacheHandler(self.config['cache_location'])
            )

        elif cache is False:
            self.config['cache_enabled'] = False
            self.urlopener = urllib2.build_opener() # default opener with no caching

        elif isinstance(cache, basestring):
            self.config['cache_enabled'] = True
            self.config['cache_location'] = cache
            self.urlopener = urllib2.build_opener(
                CacheHandler(self.config['cache_location'])
            )

        elif isinstance(cache, urllib2.OpenerDirector):
            # If passed something from urllib2.build_opener, use that
            log().debug("Using %r as urlopener" % cache)
            self.config['cache_enabled'] = True
            self.urlopener = cache

        else:
            raise ValueError("Invalid value for Cache %r (type was %s)" % (cache, type(cache)))

        self.config['banners_enabled'] = banners
        self.config['actors_enabled'] = actors

        if self.config['debug_enabled']:
            warnings.warn("The debug argument to tvdb_api.__init__ will be removed in the next version. "
            "To enable debug messages, use the following code before importing: "
            "import logging; logging.basicConfig(level=logging.DEBUG)")
            logging.basicConfig(level=logging.DEBUG)


        # List of language from http://thetvdb.com/api/0629B785CE550C8D/languages.xml
        # Hard-coded here as it is realtively static, and saves another HTTP request, as
        # recommended on http://thetvdb.com/wiki/index.php/API:languages.xml
        self.config['valid_languages'] = [
            "da", "fi", "nl", "de", "it", "es", "fr","pl", "hu","el","tr",
            "ru","he","ja","pt","zh","cs","sl", "hr","ko","en","sv","no"
        ]

        # thetvdb.com should be based around numeric language codes,
        # but to link to a series like http://thetvdb.com/?tab=series&id=79349&lid=16
        # requires the language ID, thus this mapping is required (mainly
        # for usage in tvdb_ui - internally tvdb_api will use the language abbreviations)
        self.config['langabbv_to_id'] = {'el': 20, 'en': 7, 'zh': 27,
        'it': 15, 'cs': 28, 'es': 16, 'ru': 22, 'nl': 13, 'pt': 26, 'no': 9,
        'tr': 21, 'pl': 18, 'fr': 17, 'hr': 31, 'de': 14, 'da': 10, 'fi': 11,
        'hu': 19, 'ja': 25, 'he': 24, 'ko': 32, 'sv': 8, 'sl': 30}

        if language is None:
            self.config['language'] = 'en'
        else:
            if language not in self.config['valid_languages']:
                raise ValueError("Invalid language %s, options are: %s" % (
                    language, self.config['valid_languages']
                ))
            else:
                self.config['language'] = language

        # The following url_ configs are based of the
        # http://thetvdb.com/wiki/index.php/Programmers_API
        self.config['base_url'] = "http://thetvdb.com"

        if self.config['search_all_languages']:
            self.config['url_getSeries'] = u"%(base_url)s/api/GetSeries.php?seriesname=%%s&language=all" % self.config
        else:
            self.config['url_getSeries'] = u"%(base_url)s/api/GetSeries.php?seriesname=%%s&language=%(language)s" % self.config

        self.config['url_epInfo'] = u"%(base_url)s/api/%(apikey)s/series/%%s/all/%%s.xml" % self.config
        self.config['url_epInfo_zip'] = u"%(base_url)s/api/%(apikey)s/series/%%s/all/%%s.zip" % self.config

        self.config['url_seriesInfo'] = u"%(base_url)s/api/%(apikey)s/series/%%s/%%s.xml" % self.config
        self.config['url_actorsInfo'] = u"%(base_url)s/api/%(apikey)s/series/%%s/actors.xml" % self.config

        self.config['url_seriesBanner'] = u"%(base_url)s/api/%(apikey)s/series/%%s/banners.xml" % self.config
        self.config['url_artworkPrefix'] = u"%(base_url)s/banners/%%s" % self.config

    def _getTempDir(self):
        """Returns the [system temp dir]/tvdb_api-u501 (or
        tvdb_api-myuser)
        """
        if hasattr(os, 'getuid'):
            uid = "u%d" % (os.getuid())
        else:
            # For Windows
            try:
                uid = getpass.getuser()
            except ImportError:
                return os.path.join(tempfile.gettempdir(), "tvdb_api")

        return os.path.join(tempfile.gettempdir(), "tvdb_api-%s" % (uid))

    def _loadUrl(self, url, recache = False, language=None):
        global lastTimeout
        try:
            log().debug("Retrieving URL %s" % url)
            resp = self.urlopener.open(url)
            if 'x-local-cache' in resp.headers:
                log().debug("URL %s was cached in %s" % (
                    url,
                    resp.headers['x-local-cache'])
                )
                if recache:
                    log().debug("Attempting to recache %s" % url)
                    resp.recache()
        except (IOError, urllib2.URLError), errormsg:
            if not str(errormsg).startswith('HTTP Error'):
                lastTimeout = datetime.datetime.now()
            raise tvdb_error("Could not connect to server: %s" % (errormsg))

        
        # handle gzipped content,
        # http://dbr.lighthouseapp.com/projects/13342/tickets/72-gzipped-data-patch
        if 'gzip' in resp.headers.get("Content-Encoding", ''):
            if gzip:
                stream = StringIO.StringIO(resp.read())
                gz = gzip.GzipFile(fileobj=stream)
                return gz.read()

            raise tvdb_error("Received gzip data from thetvdb.com, but could not correctly handle it")

        if 'application/zip' in resp.headers.get("Content-Type", ''):
            try:
                # TODO: The zip contains actors.xml and banners.xml, which are currently ignored [GH-20]
                log().debug("We recived a zip file unpacking now ...")
                zipdata = StringIO.StringIO()
                zipdata.write(resp.read())
                myzipfile = zipfile.ZipFile(zipdata)
                return myzipfile.read('%s.xml' % language)
            except zipfile.BadZipfile:
                if 'x-local-cache' in resp.headers:
                    resp.delete_cache()
                raise tvdb_error("Bad zip file received from thetvdb.com, could not read it")

        return resp.read()

    def _getetsrc(self, url, language=None):
        """Loads a URL using caching, returns an ElementTree of the source
        """
        src = self._loadUrl(url, language=language)
        try:
            # TVDB doesn't sanitize \r (CR) from user input in some fields,
            # remove it to avoid errors. Change from SickBeard, from will14m
            return ElementTree.fromstring(src.rstrip("\r"))
        except SyntaxError:
            src = self._loadUrl(url, recache=True, language=language)
            try:
                return ElementTree.fromstring(src.rstrip("\r"))
            except SyntaxError, exceptionmsg:
                errormsg = "There was an error with the XML retrieved from thetvdb.com:\n%s" % (
                    exceptionmsg
                )

                if self.config['cache_enabled']:
                    errormsg += "\nFirst try emptying the cache folder at..\n%s" % (
                        self.config['cache_location']
                    )

                errormsg += "\nIf this does not resolve the issue, please try again later. If the error persists, report a bug on"
                errormsg += "\nhttp://dbr.lighthouseapp.com/projects/13342-tvdb_api/overview\n"
                raise tvdb_error(errormsg)

    def _setItem(self, sid, seas, ep, attrib, value):
        """Creates a new episode, creating Show(), Season() and
        Episode()s as required. Called by _getShowData to populate show

        Since the nice-to-use tvdb[1][24]['name] interface
        makes it impossible to do tvdb[1][24]['name] = "name"
        and still be capable of checking if an episode exists
        so we can raise tvdb_shownotfound, we have a slightly
        less pretty method of setting items.. but since the API
        is supposed to be read-only, this is the best way to
        do it!
        The problem is that calling tvdb[1][24]['episodename'] = "name"
        calls __getitem__ on tvdb[1], there is no way to check if
        tvdb.__dict__ should have a key "1" before we auto-create it
        """
        if sid not in self.shows:
            self.shows[sid] = Show()
        if seas not in self.shows[sid]:
            self.shows[sid][seas] = Season(show = self.shows[sid])
        if ep not in self.shows[sid][seas]:
            self.shows[sid][seas][ep] = Episode(season = self.shows[sid][seas])
        self.shows[sid][seas][ep][attrib] = value

    def _setShowData(self, sid, key, value):
        """Sets self.shows[sid] to a new Show instance, or sets the data
        """
        if sid not in self.shows:
            self.shows[sid] = Show()
        self.shows[sid].data[key] = value

    def _cleanData(self, data):
        """Cleans up strings returned by TheTVDB.com

        Issues corrected:
        - Replaces &amp; with &
        - Trailing whitespace
        """
        data = data.replace(u"&amp;", u"&")
        data = data.strip()
        return data

    def search(self, series):
        """This searches TheTVDB.com for the series name
        and returns the result list
        """
        series = urllib.quote(series.encode("utf-8"))
        log().debug("Searching for show %s" % series)
        seriesEt = self._getetsrc(self.config['url_getSeries'] % (series))
        allSeries = []
        for series in seriesEt:
            result = dict((k.tag.lower(), k.text) for k in series.getchildren())
            result['id'] = int(result['id'])
            result['lid'] = self.config['langabbv_to_id'][result['language']]
            if 'aliasnames' in result:
                result['aliasnames'] = result['aliasnames'].split("|")
            log().debug('Found series %(seriesname)s' % result)
            allSeries.append(result)
        
        return allSeries

    def _getSeries(self, series):
        """This searches TheTVDB.com for the series name,
        If a custom_ui UI is configured, it uses this to select the correct
        series. If not, and interactive == True, ConsoleUI is used, if not
        BaseUI is used to select the first result.
        """
        allSeries = self.search(series)

        if len(allSeries) == 0:
            log().debug('Series result returned zero')
            raise tvdb_shownotfound("Show-name search returned zero results (cannot find show on TVDB)")

        if self.config['custom_ui'] is not None:
            log().debug("Using custom UI %s" % (repr(self.config['custom_ui'])))
            ui = self.config['custom_ui'](config = self.config)
        else:
            if not self.config['interactive']:
                log().debug('Auto-selecting first search result using BaseUI')
                ui = BaseUI(config = self.config)
            else:
                log().debug('Interactively selecting show using ConsoleUI')
                ui = ConsoleUI(config = self.config)

        return ui.selectSeries(allSeries)

    def _parseBanners(self, sid):
        """Parses banners XML, from
        http://thetvdb.com/api/[APIKEY]/series/[SERIES ID]/banners.xml

        Banners are retrieved using t['show name]['_banners'], for example:

        >>> t = Tvdb(banners = True)
        >>> t['scrubs']['_banners'].keys()
        ['fanart', 'poster', 'series', 'season']
        >>> t['scrubs']['_banners']['poster']['680x1000']['35308']['_bannerpath']
        u'http://thetvdb.com/banners/posters/76156-2.jpg'
        >>>

        Any key starting with an underscore has been processed (not the raw
        data from the XML)

        This interface will be improved in future versions.
        """
        log().debug('Getting season banners for %s' % (sid))
        bannersEt = self._getetsrc( self.config['url_seriesBanner'] % (sid) )
        banners = {}
        for cur_banner in bannersEt.findall('Banner'):
            bid = cur_banner.find('id').text
            btype = cur_banner.find('BannerType')
            btype2 = cur_banner.find('BannerType2')
            if btype is None or btype2 is None:
                continue
            btype, btype2 = btype.text, btype2.text
            if not btype in banners:
                banners[btype] = {}
            if not btype2 in banners[btype]:
                banners[btype][btype2] = {}
            if not bid in banners[btype][btype2]:
                banners[btype][btype2][bid] = {}

            for cur_element in cur_banner.getchildren():
                tag = cur_element.tag.lower()
                value = cur_element.text
                if tag is None or value is None:
                    continue
                tag, value = tag.lower(), value.lower()
                banners[btype][btype2][bid][tag] = value

            for k, v in banners[btype][btype2][bid].items():
                if k.endswith("path"):
                    new_key = "_%s" % (k)
                    log().debug("Transforming %s to %s" % (k, new_key))
                    new_url = self.config['url_artworkPrefix'] % (v)
                    banners[btype][btype2][bid][new_key] = new_url

        self._setShowData(sid, "_banners", banners)

    def _parseActors(self, sid):
        """Parsers actors XML, from
        http://thetvdb.com/api/[APIKEY]/series/[SERIES ID]/actors.xml

        Actors are retrieved using t['show name]['_actors'], for example:

        >>> t = Tvdb(actors = True)
        >>> actors = t['scrubs']['_actors']
        >>> type(actors)
        <class 'tvdb_api.Actors'>
        >>> type(actors[0])
        <class 'tvdb_api.Actor'>
        >>> actors[0]
        <Actor "Zach Braff">
        >>> sorted(actors[0].keys())
        ['id', 'image', 'name', 'role', 'sortorder']
        >>> actors[0]['name']
        u'Zach Braff'
        >>> actors[0]['image']
        u'http://thetvdb.com/banners/actors/43640.jpg'

        Any key starting with an underscore has been processed (not the raw
        data from the XML)
        """
        log().debug("Getting actors for %s" % (sid))
        actorsEt = self._getetsrc(self.config['url_actorsInfo'] % (sid))

        cur_actors = Actors()
        for curActorItem in actorsEt.findall("Actor"):
            curActor = Actor()
            for curInfo in curActorItem:
                tag = curInfo.tag.lower()
                value = curInfo.text
                if value is not None:
                    if tag == "image":
                        value = self.config['url_artworkPrefix'] % (value)
                    else:
                        value = self._cleanData(value)
                curActor[tag] = value
            cur_actors.append(curActor)
        self._setShowData(sid, '_actors', cur_actors)

    def _getShowData(self, sid, language):
        """Takes a series ID, gets the epInfo URL and parses the TVDB
        XML file into the shows dict in layout:
        shows[series_id][season_number][episode_number]
        """

        if self.config['language'] is None:
            log().debug('Config language is none, using show language')
            if language is None:
                raise tvdb_error("config['language'] was None, this should not happen")
            getShowInLanguage = language
        else:
            log().debug(
                'Configured language %s override show language of %s' % (
                    self.config['language'],
                    language
                )
            )
            getShowInLanguage = self.config['language']

        # Parse show information
        log().debug('Getting all series data for %s' % (sid))
        seriesInfoEt = self._getetsrc(
            self.config['url_seriesInfo'] % (sid, getShowInLanguage)
        )
        for curInfo in seriesInfoEt.findall("Series")[0]:
            tag = curInfo.tag.lower()
            value = curInfo.text

            if value is not None:
                if tag in ['banner', 'fanart', 'poster']:
                    value = self.config['url_artworkPrefix'] % (value)
                else:
                    value = self._cleanData(value)

            self._setShowData(sid, tag, value)

        # Parse banners
        if self.config['banners_enabled']:
            self._parseBanners(sid)

        # Parse actors
        if self.config['actors_enabled']:
            self._parseActors(sid)

        # Parse episode data
        log().debug('Getting all episodes of %s' % (sid))

        if self.config['useZip']:
            url = self.config['url_epInfo_zip'] % (sid, language)
        else:
            url = self.config['url_epInfo'] % (sid, language)

        epsEt = self._getetsrc( url, language=language)

        for cur_ep in epsEt.findall("Episode"):

            if self.config['dvdorder']:
                log().debug('Using DVD ordering.')
                use_dvd = cur_ep.find('DVD_season').text != None and cur_ep.find('DVD_episodenumber').text != None
            else:
                use_dvd = False

            if use_dvd:
                elem_seasnum, elem_epno = cur_ep.find('DVD_season'), cur_ep.find('DVD_episodenumber')
            else:
                elem_seasnum, elem_epno = cur_ep.find('SeasonNumber'), cur_ep.find('EpisodeNumber')

            if elem_seasnum is None or elem_epno is None:
                log().warning("An episode has incomplete season/episode number (season: %r, episode: %r)" % (
                    elem_seasnum, elem_epno))
                log().debug(
                    " ".join(
                        "%r is %r" % (child.tag, child.text) for child in cur_ep.getchildren()))
                # TODO: Should this happen?
                continue # Skip to next episode

            # float() is because https://github.com/dbr/tvnamer/issues/95 - should probably be fixed in TVDB data
            seas_no = int(float(elem_seasnum.text))
            ep_no = int(float(elem_epno.text))

            for cur_item in cur_ep.getchildren():
                tag = cur_item.tag.lower()
                value = cur_item.text
                if value is not None:
                    if tag == 'filename':
                        value = self.config['url_artworkPrefix'] % (value)
                    else:
                        value = self._cleanData(value)
                self._setItem(sid, seas_no, ep_no, tag, value)

    def _nameToSid(self, name):
        """Takes show name, returns the correct series ID (if the show has
        already been grabbed), or grabs all episodes and returns
        the correct SID.
        """
        if name in self.corrections:
            log().debug('Correcting %s to %s' % (name, self.corrections[name]) )
            sid = self.corrections[name]
        else:
            log().debug('Getting show %s' % (name))
            selected_series = self._getSeries( name )
            sname, sid = selected_series['seriesname'], selected_series['id']
            log().debug('Got %(seriesname)s, id %(id)s' % selected_series)

            self.corrections[name] = sid
            self._getShowData(selected_series['id'], selected_series['language'])

        return sid

    def __getitem__(self, key):
        """Handles tvdb_instance['seriesname'] calls.
        The dict index should be the show id
        """
        if isinstance(key, (int, long)):
            # Item is integer, treat as show id
            if key not in self.shows:
                self._getShowData(key, self.config['language'])
            return self.shows[key]
        
        key = key.lower() # make key lower case
        sid = self._nameToSid(key)
        log().debug('Got series id %s' % (sid))
        return self.shows[sid]

    def __repr__(self):
        return str(self.shows)


def main():
    """Simple example of using tvdb_api - it just
    grabs an episode name interactively.
    """
    import logging
    logging.basicConfig(level=logging.DEBUG)

    tvdb_instance = Tvdb(interactive=True, cache=False)
    print tvdb_instance['Lost']['seriesname']
    print tvdb_instance['Lost'][1][4]['episodename']

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = tvdb_cache
#!/usr/bin/env python
#encoding:utf-8
#author:dbr/Ben
#project:tvdb_api
#repository:http://github.com/dbr/tvdb_api
#license:unlicense (http://unlicense.org/)

"""
urllib2 caching handler
Modified from http://code.activestate.com/recipes/491261/
"""
from __future__ import with_statement

__author__ = "dbr/Ben"
__version__ = "1.9"

import os
import time
import errno
import httplib
import urllib2
import StringIO
from hashlib import md5
from threading import RLock

cache_lock = RLock()

def locked_function(origfunc):
    """Decorator to execute function under lock"""
    def wrapped(*args, **kwargs):
        cache_lock.acquire()
        try:
            return origfunc(*args, **kwargs)
        finally:
            cache_lock.release()
    return wrapped

def calculate_cache_path(cache_location, url):
    """Checks if [cache_location]/[hash_of_url].headers and .body exist
    """
    thumb = md5(url).hexdigest()
    header = os.path.join(cache_location, thumb + ".headers")
    body = os.path.join(cache_location, thumb + ".body")
    return header, body

def check_cache_time(path, max_age):
    """Checks if a file has been created/modified in the [last max_age] seconds.
    False means the file is too old (or doesn't exist), True means it is
    up-to-date and valid"""
    if not os.path.isfile(path):
        return False
    cache_modified_time = os.stat(path).st_mtime
    time_now = time.time()
    if cache_modified_time < time_now - max_age:
        # Cache is old
        return False
    else:
        return True

@locked_function
def exists_in_cache(cache_location, url, max_age):
    """Returns if header AND body cache file exist (and are up-to-date)"""
    hpath, bpath = calculate_cache_path(cache_location, url)
    if os.path.exists(hpath) and os.path.exists(bpath):
        return(
            check_cache_time(hpath, max_age)
            and check_cache_time(bpath, max_age)
        )
    else:
        # File does not exist
        return False

@locked_function
def store_in_cache(cache_location, url, response):
    """Tries to store response in cache."""
    hpath, bpath = calculate_cache_path(cache_location, url)
    try:
        outf = open(hpath, "wb")
        headers = str(response.info())
        outf.write(headers)
        outf.close()

        outf = open(bpath, "wb")
        outf.write(response.read())
        outf.close()
    except IOError:
        return True
    else:
        return False
        
@locked_function
def delete_from_cache(cache_location, url):
    """Deletes a response in cache."""
    hpath, bpath = calculate_cache_path(cache_location, url)
    try:
        if os.path.exists(hpath):
            os.remove(hpath)
        if os.path.exists(bpath):
            os.remove(bpath)
    except IOError:
        return True
    else:
        return False

class CacheHandler(urllib2.BaseHandler):
    """Stores responses in a persistant on-disk cache.

    If a subsequent GET request is made for the same URL, the stored
    response is returned, saving time, resources and bandwidth
    """
    @locked_function
    def __init__(self, cache_location, max_age = 21600):
        """The location of the cache directory"""
        self.max_age = max_age
        self.cache_location = cache_location
        if not os.path.exists(self.cache_location):
            try:
                os.mkdir(self.cache_location)
            except OSError, e:
                if e.errno == errno.EEXIST and os.path.isdir(self.cache_location):
                    # File exists, and it's a directory,
                    # another process beat us to creating this dir, that's OK.
                    pass
                else:
                    # Our target dir is already a file, or different error,
                    # relay the error!
                    raise

    def default_open(self, request):
        """Handles GET requests, if the response is cached it returns it
        """
        if request.get_method() is not "GET":
            return None # let the next handler try to handle the request

        if exists_in_cache(
            self.cache_location, request.get_full_url(), self.max_age
        ):
            return CachedResponse(
                self.cache_location,
                request.get_full_url(),
                set_cache_header = True
            )
        else:
            return None

    def http_response(self, request, response):
        """Gets a HTTP response, if it was a GET request and the status code
        starts with 2 (200 OK etc) it caches it and returns a CachedResponse
        """
        if (request.get_method() == "GET"
            and str(response.code).startswith("2")
        ):
            if 'x-local-cache' not in response.info():
                # Response is not cached
                set_cache_header = store_in_cache(
                    self.cache_location,
                    request.get_full_url(),
                    response
                )
            else:
                set_cache_header = True

            return CachedResponse(
                self.cache_location,
                request.get_full_url(),
                set_cache_header = set_cache_header
            )
        else:
            return response

class CachedResponse(StringIO.StringIO):
    """An urllib2.response-like object for cached responses.

    To determine if a response is cached or coming directly from
    the network, check the x-local-cache header rather than the object type.
    """

    @locked_function
    def __init__(self, cache_location, url, set_cache_header=True):
        self.cache_location = cache_location
        hpath, bpath = calculate_cache_path(cache_location, url)

        StringIO.StringIO.__init__(self, file(bpath, "rb").read())

        self.url     = url
        self.code    = 200
        self.msg     = "OK"
        headerbuf = file(hpath, "rb").read()
        if set_cache_header:
            headerbuf += "x-local-cache: %s\r\n" % (bpath)
        self.headers = httplib.HTTPMessage(StringIO.StringIO(headerbuf))

    def info(self):
        """Returns headers
        """
        return self.headers

    def geturl(self):
        """Returns original URL
        """
        return self.url

    @locked_function
    def recache(self):
        new_request = urllib2.urlopen(self.url)
        set_cache_header = store_in_cache(
            self.cache_location,
            new_request.url,
            new_request
        )
        CachedResponse.__init__(self, self.cache_location, self.url, True)

    @locked_function
    def delete_cache(self):
        delete_from_cache(
            self.cache_location,
            self.url
        )
    

if __name__ == "__main__":
    def main():
        """Quick test/example of CacheHandler"""
        opener = urllib2.build_opener(CacheHandler("/tmp/"))
        response = opener.open("http://google.com")
        print response.headers
        print "Response:", response.read()

        response.recache()
        print response.headers
        print "After recache:", response.read()

        # Test usage in threads
        from threading import Thread
        class CacheThreadTest(Thread):
            lastdata = None
            def run(self):
                req = opener.open("http://google.com")
                newdata = req.read()
                if self.lastdata is None:
                    self.lastdata = newdata
                assert self.lastdata == newdata, "Data was not consistent, uhoh"
                req.recache()
        threads = [CacheThreadTest() for x in range(50)]
        print "Starting threads"
        [t.start() for t in threads]
        print "..done"
        print "Joining threads"
        [t.join() for t in threads]
        print "..done"
    main()

########NEW FILE########
__FILENAME__ = tvdb_exceptions
#!/usr/bin/env python
#encoding:utf-8
#author:dbr/Ben
#project:tvdb_api
#repository:http://github.com/dbr/tvdb_api
#license:unlicense (http://unlicense.org/)

"""Custom exceptions used or raised by tvdb_api
"""

__author__ = "dbr/Ben"
__version__ = "1.9"

__all__ = ["tvdb_error", "tvdb_userabort", "tvdb_shownotfound",
"tvdb_seasonnotfound", "tvdb_episodenotfound", "tvdb_attributenotfound"]

class tvdb_exception(Exception):
    """Any exception generated by tvdb_api
    """
    pass

class tvdb_error(tvdb_exception):
    """An error with thetvdb.com (Cannot connect, for example)
    """
    pass

class tvdb_userabort(tvdb_exception):
    """User aborted the interactive selection (via
    the q command, ^c etc)
    """
    pass

class tvdb_shownotfound(tvdb_exception):
    """Show cannot be found on thetvdb.com (non-existant show)
    """
    pass

class tvdb_seasonnotfound(tvdb_exception):
    """Season cannot be found on thetvdb.com
    """
    pass

class tvdb_episodenotfound(tvdb_exception):
    """Episode cannot be found on thetvdb.com
    """
    pass

class tvdb_attributenotfound(tvdb_exception):
    """Raised if an episode does not have the requested
    attribute (such as a episode name)
    """
    pass

########NEW FILE########
__FILENAME__ = tvdb_ui
#!/usr/bin/env python
#encoding:utf-8
#author:dbr/Ben
#project:tvdb_api
#repository:http://github.com/dbr/tvdb_api
#license:unlicense (http://unlicense.org/)

"""Contains included user interfaces for Tvdb show selection.

A UI is a callback. A class, it's __init__ function takes two arguments:

- config, which is the Tvdb config dict, setup in tvdb_api.py
- log, which is Tvdb's logger instance (which uses the logging module). You can
call log.info() log.warning() etc

It must have a method "selectSeries", this is passed a list of dicts, each dict
contains the the keys "name" (human readable show name), and "sid" (the shows
ID as on thetvdb.com). For example:

[{'name': u'Lost', 'sid': u'73739'},
 {'name': u'Lost Universe', 'sid': u'73181'}]

The "selectSeries" method must return the appropriate dict, or it can raise
tvdb_userabort (if the selection is aborted), tvdb_shownotfound (if the show
cannot be found).

A simple example callback, which returns a random series:

>>> import random
>>> from tvdb_ui import BaseUI
>>> class RandomUI(BaseUI):
...    def selectSeries(self, allSeries):
...            import random
...            return random.choice(allSeries)

Then to use it..

>>> from tvdb_api import Tvdb
>>> t = Tvdb(custom_ui = RandomUI)
>>> random_matching_series = t['Lost']
>>> type(random_matching_series)
<class 'tvdb_api.Show'>
"""

__author__ = "dbr/Ben"
__version__ = "1.9"

import logging
import warnings

from tvdb_exceptions import tvdb_userabort

def log():
    return logging.getLogger(__name__)

class BaseUI:
    """Default non-interactive UI, which auto-selects first results
    """
    def __init__(self, config, log = None):
        self.config = config
        if log is not None:
            warnings.warn("the UI's log parameter is deprecated, instead use\n"
                "use import logging; logging.getLogger('ui').info('blah')\n"
                "The self.log attribute will be removed in the next version")
            self.log = logging.getLogger(__name__)

    def selectSeries(self, allSeries):
        return allSeries[0]


class ConsoleUI(BaseUI):
    """Interactively allows the user to select a show from a console based UI
    """

    def _displaySeries(self, allSeries, limit = 6):
        """Helper function, lists series with corresponding ID
        """
        if limit is not None:
            toshow = allSeries[:limit]
        else:
            toshow = allSeries

        print "TVDB Search Results:"
        for i, cshow in enumerate(toshow):
            i_show = i + 1 # Start at more human readable number 1 (not 0)
            log().debug('Showing allSeries[%s], series %s)' % (i_show, allSeries[i]['seriesname']))
            if i == 0:
                extra = " (default)"
            else:
                extra = ""

            print "%s -> %s [%s] # http://thetvdb.com/?tab=series&id=%s&lid=%s%s" % (
                i_show,
                cshow['seriesname'].encode("UTF-8", "ignore"),
                cshow['language'].encode("UTF-8", "ignore"),
                str(cshow['id']),
                cshow['lid'],
                extra
            )

    def selectSeries(self, allSeries):
        self._displaySeries(allSeries)

        if len(allSeries) == 1:
            # Single result, return it!
            print "Automatically selecting only result"
            return allSeries[0]

        if self.config['select_first'] is True:
            print "Automatically returning first search result"
            return allSeries[0]

        while True: # return breaks this loop
            try:
                print "Enter choice (first number, return for default, 'all', ? for help):"
                ans = raw_input()
            except KeyboardInterrupt:
                raise tvdb_userabort("User aborted (^c keyboard interupt)")
            except EOFError:
                raise tvdb_userabort("User aborted (EOF received)")

            log().debug('Got choice of: %s' % (ans))
            try:
                selected_id = int(ans) - 1 # The human entered 1 as first result, not zero
            except ValueError: # Input was not number
                if len(ans.strip()) == 0:
                    # Default option
                    log().debug('Default option, returning first series')
                    return allSeries[0]
                if ans == "q":
                    log().debug('Got quit command (q)')
                    raise tvdb_userabort("User aborted ('q' quit command)")
                elif ans == "?":
                    print "## Help"
                    print "# Enter the number that corresponds to the correct show."
                    print "# a - display all results"
                    print "# all - display all results"
                    print "# ? - this help"
                    print "# q - abort tvnamer"
                    print "# Press return with no input to select first result"
                elif ans.lower() in ["a", "all"]:
                    self._displaySeries(allSeries, limit = None)
                else:
                    log().debug('Unknown keypress %s' % (ans))
            else:
                log().debug('Trying to return ID: %d' % (selected_id))
                try:
                    return allSeries[selected_id]
                except IndexError:
                    log().debug('Invalid show number entered!')
                    print "Invalid number (%s) selected!"
                    self._displaySeries(allSeries)


########NEW FILE########
__FILENAME__ = autoPostProcesser
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import os.path

import sickbeard

from sickbeard import logger
from sickbeard import encodingKludge as ek
from sickbeard import processTV


class PostProcesser():

    def run(self):
        if not sickbeard.PROCESS_AUTOMATICALLY:
            return

        if not ek.ek(os.path.isdir, sickbeard.TV_DOWNLOAD_DIR):
            logger.log(u"Automatic post-processing attempted but dir " + sickbeard.TV_DOWNLOAD_DIR + " doesn't exist", logger.ERROR)
            return

        if not ek.ek(os.path.isabs, sickbeard.TV_DOWNLOAD_DIR):
            logger.log(u"Automatic post-processing attempted but dir " + sickbeard.TV_DOWNLOAD_DIR + " is relative (and probably not what you really want to process)", logger.ERROR)
            return

        processTV.processDir(sickbeard.TV_DOWNLOAD_DIR, method='Automatic')

########NEW FILE########
__FILENAME__ = browser
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import os
import string
import cherrypy

from sickbeard import encodingKludge as ek

# use the built-in if it's available (python 2.6), if not use the included library
try:
    import json
except ImportError:
    from lib import simplejson as json

# this is for the drive letter code, it only works on windows
if os.name == 'nt':
    from ctypes import windll

# adapted from http://stackoverflow.com/questions/827371/is-there-a-way-to-list-all-the-available-drive-letters-in-python/827490
def getWinDrives():
    """ Return list of detected drives """
    assert os.name == 'nt'

    drives = []
    bitmask = windll.kernel32.GetLogicalDrives() #@UndefinedVariable
    for letter in string.uppercase:
        if bitmask & 1:
            drives.append(letter)
        bitmask >>= 1

    return drives


def foldersAtPath(path, includeParent=False):
    """ Returns a list of dictionaries with the folders contained at the given path
        Give the empty string as the path to list the contents of the root path
        under Unix this means "/", on Windows this will be a list of drive letters)
    """

    # walk up the tree until we find a valid path
    while path and not os.path.isdir(path):
        if path == os.path.dirname(path):
            path = ''
            break
        else:
            path = os.path.dirname(path)

    if path == "":
        if os.name == 'nt':
            entries = [{'current_path': 'Root'}]
            for letter in getWinDrives():
                letterPath = letter + ':\\'
                entries.append({'name': letterPath, 'path': letterPath})
            return entries
        else:
            path = '/'

    # fix up the path and find the parent
    path = os.path.abspath(os.path.normpath(path))
    parentPath = os.path.dirname(path)

    # if we're at the root then the next step is the meta-node showing our drive letters
    if path == parentPath and os.name == 'nt':
        parentPath = ""

    fileList = [{ 'name': filename, 'path': ek.ek(os.path.join, path, filename) } for filename in ek.ek(os.listdir, path)]
    fileList = filter(lambda entry: ek.ek(os.path.isdir, entry['path']), fileList)

    # prune out directories to proect the user from doing stupid things (already lower case the dir to reduce calls)
    hideList = ["boot", "bootmgr", "cache", "msocache", "recovery", "$recycle.bin", "recycler", "system volume information", "temporary internet files"] # windows specific
    hideList += [".fseventd", ".spotlight", ".trashes", ".vol", "cachedmessages", "caches", "trash"] # osx specific
    fileList = filter(lambda entry: entry['name'].lower() not in hideList, fileList)

    fileList = sorted(fileList, lambda x, y: cmp(os.path.basename(x['name']).lower(), os.path.basename(y['path']).lower()))

    entries = [{'current_path': path}]
    if includeParent and parentPath != path:
        entries.append({ 'name': "..", 'path': parentPath })
    entries.extend(fileList)

    return entries


class WebFileBrowser:

    @cherrypy.expose
    def index(self, path=''):
        cherrypy.response.headers['Content-Type'] = "application/json"
        return json.dumps(foldersAtPath(path, True))

    @cherrypy.expose
    def complete(self, term):
        cherrypy.response.headers['Content-Type'] = "application/json"
        paths = [entry['path'] for entry in foldersAtPath(os.path.dirname(term)) if 'path' in entry]
        return json.dumps( paths )

########NEW FILE########
__FILENAME__ = classes
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.



import sickbeard

import urllib
import datetime

from common import USER_AGENT, Quality

class SickBeardURLopener(urllib.FancyURLopener):
    version = USER_AGENT

class AuthURLOpener(SickBeardURLopener):
    """
    URLOpener class that supports http auth without needing interactive password entry.
    If the provided username/password don't work it simply fails.
    
    user: username to use for HTTP auth
    pw: password to use for HTTP auth
    """
    def __init__(self, user, pw):
        self.username = user
        self.password = pw

        # remember if we've tried the username/password before
        self.numTries = 0
        
        # call the base class
        urllib.FancyURLopener.__init__(self)

    def prompt_user_passwd(self, host, realm):
        """
        Override this function and instead of prompting just give the
        username/password that were provided when the class was instantiated.
        """

        # if this is the first try then provide a username/password
        if self.numTries == 0:
            self.numTries = 1
            return (self.username, self.password)
        
        # if we've tried before then return blank which cancels the request
        else:
            return ('', '')

    # this is pretty much just a hack for convenience
    def openit(self, url):
        self.numTries = 0
        return SickBeardURLopener.open(self, url)

class SearchResult:
    """
    Represents a search result from an indexer.
    """

    def __init__(self, episodes):
        self.provider = -1

        # URL to the NZB/torrent file
        self.url = ""

        # used by some providers to store extra info associated with the result
        self.extraInfo = []

        # list of TVEpisode objects that this result is associated with
        self.episodes = episodes

        # quality of the release
        self.quality = Quality.UNKNOWN

        # release name
        self.name = ""

    def __str__(self):

        if self.provider == None:
            return "Invalid provider, unable to print self"

        myString = self.provider.name + " @ " + self.url + "\n"
        myString += "Extra Info:\n"
        for extra in self.extraInfo:
            myString += "  " + extra + "\n"
        return myString

    def fileName(self):
        return self.episodes[0].prettyName() + "." + self.resultType

class NZBSearchResult(SearchResult):
    """
    Regular NZB result with an URL to the NZB
    """
    resultType = "nzb"

class NZBDataSearchResult(SearchResult):
    """
    NZB result where the actual NZB XML data is stored in the extraInfo
    """
    resultType = "nzbdata"

class TorrentSearchResult(SearchResult):
    """
    Torrent result with an URL to the torrent
    """
    resultType = "torrent"


class ShowListUI:
    """
    This class is for tvdb-api. Instead of prompting with a UI to pick the
    desired result out of a list of shows it tries to be smart about it
    based on what shows are in SB. 
    """
    def __init__(self, config, log=None):
        self.config = config
        self.log = log

    def selectSeries(self, allSeries):
        idList = [x.tvdbid for x in sickbeard.showList]

        # try to pick a show that's in my show list
        for curShow in allSeries:
            if int(curShow['id']) in idList:
                return curShow

        # if nothing matches then just go with the first match I guess
        return allSeries[0]

class Proper:
    def __init__(self, name, url, date):
        self.name = name
        self.url = url
        self.date = date
        self.provider = None
        self.quality = Quality.UNKNOWN

        self.tvdbid = -1
        self.season = -1
        self.episode = -1

    def __str__(self):
        return str(self.date)+" "+self.name+" "+str(self.season)+"x"+str(self.episode)+" of "+str(self.tvdbid)


class ErrorViewer():
    """
    Keeps a static list of UIErrors to be displayed on the UI and allows
    the list to be cleared.
    """

    errors = []

    def __init__(self):
        ErrorViewer.errors = []

    @staticmethod
    def add(error):
        ErrorViewer.errors.append(error)

    @staticmethod
    def clear():
        ErrorViewer.errors = []


class UIError():
    """
    Represents an error to be displayed in the web UI.
    """
    def __init__(self, message):
        self.message = message
        self.time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

########NEW FILE########
__FILENAME__ = common
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import os.path
import operator
import platform
import re

from sickbeard import version

USER_AGENT = 'Sick Beard/alpha2-' + version.SICKBEARD_VERSION.replace(' ', '-') + ' (' + platform.system() + ' ' + platform.release() + ')'

mediaExtensions = ['avi', 'mkv', 'mpg', 'mpeg', 'wmv',
                   'ogm', 'mp4', 'iso', 'img', 'divx',
                   'm2ts', 'm4v', 'ts', 'flv', 'f4v',
                   'mov', 'rmvb', 'vob', 'dvr-ms', 'wtv',
                   'ogv', '3gp', 'webm']

### Other constants
MULTI_EP_RESULT = -1
SEASON_RESULT = -2

### Notification Types
NOTIFY_SNATCH = 1
NOTIFY_DOWNLOAD = 2

notifyStrings = {}
notifyStrings[NOTIFY_SNATCH] = "Started Download"
notifyStrings[NOTIFY_DOWNLOAD] = "Download Finished"

### Episode statuses
UNKNOWN = -1         # should never happen
UNAIRED = 1          # episodes that haven't aired yet
SNATCHED = 2         # qualified with quality
WANTED = 3           # episodes we don't have but want to get
DOWNLOADED = 4       # qualified with quality
SKIPPED = 5          # episodes we don't want
ARCHIVED = 6         # episodes that you don't have locally (counts toward download completion stats)
IGNORED = 7          # episodes that you don't want included in your download stats
SNATCHED_PROPER = 9  # qualified with quality

NAMING_REPEAT = 1
NAMING_EXTEND = 2
NAMING_DUPLICATE = 4
NAMING_LIMITED_EXTEND = 8
NAMING_SEPARATED_REPEAT = 16
NAMING_LIMITED_EXTEND_E_PREFIXED = 32

multiEpStrings = {}
multiEpStrings[NAMING_REPEAT] = "Repeat"
multiEpStrings[NAMING_SEPARATED_REPEAT] = "Repeat (Separated)"
multiEpStrings[NAMING_DUPLICATE] = "Duplicate"
multiEpStrings[NAMING_EXTEND] = "Extend"
multiEpStrings[NAMING_LIMITED_EXTEND] = "Extend (Limited)"
multiEpStrings[NAMING_LIMITED_EXTEND_E_PREFIXED] = "Extend (Limited, E-prefixed)"


class Quality:
    NONE = 0               # 0
    SDTV = 1               # 1
    SDDVD = 1 << 1         # 2
    HDTV = 1 << 2          # 4
    RAWHDTV = 1 << 3       # 8  -- 720p/1080i mpeg2 (trollhd releases)
    FULLHDTV = 1 << 4      # 16 -- 1080p HDTV (QCF releases)
    HDWEBDL = 1 << 5       # 32
    FULLHDWEBDL = 1 << 6   # 64 -- 1080p web-dl
    HDBLURAY = 1 << 7      # 128
    FULLHDBLURAY = 1 << 8  # 256

    # put these bits at the other end of the spectrum, far enough out that they shouldn't interfere
    UNKNOWN = 1 << 15      # 32768

    qualityStrings = {NONE: "N/A",
                      UNKNOWN: "Unknown",
                      SDTV: "SD TV",
                      SDDVD: "SD DVD",
                      HDTV: "HD TV",
                      RAWHDTV: "RawHD TV",
                      FULLHDTV: "1080p HD TV",
                      HDWEBDL: "720p WEB-DL",
                      FULLHDWEBDL: "1080p WEB-DL",
                      HDBLURAY: "720p BluRay",
                      FULLHDBLURAY: "1080p BluRay"}

    statusPrefixes = {DOWNLOADED: "Downloaded",
                      SNATCHED: "Snatched"}

    @staticmethod
    def _getStatusStrings(status):
        toReturn = {}
        for x in Quality.qualityStrings.keys():
            toReturn[Quality.compositeStatus(status, x)] = Quality.statusPrefixes[status] + " (" + Quality.qualityStrings[x] + ")"
        return toReturn

    @staticmethod
    def combineQualities(anyQualities, bestQualities):
        anyQuality = 0
        bestQuality = 0
        if anyQualities:
            anyQuality = reduce(operator.or_, anyQualities)
        if bestQualities:
            bestQuality = reduce(operator.or_, bestQualities)
        return anyQuality | (bestQuality << 16)

    @staticmethod
    def splitQuality(quality):
        anyQualities = []
        bestQualities = []
        for curQual in Quality.qualityStrings.keys():
            if curQual & quality:
                anyQualities.append(curQual)
            if curQual << 16 & quality:
                bestQualities.append(curQual)

        return (sorted(anyQualities), sorted(bestQualities))

    @staticmethod
    def nameQuality(name):
        name = os.path.basename(name)

        # if we have our exact text then assume we put it there
        for x in sorted(Quality.qualityStrings, reverse=True):
            if x == Quality.UNKNOWN:
                continue

            regex = '\W' + Quality.qualityStrings[x].replace(' ', '\W') + '\W'
            regex_match = re.search(regex, name, re.I)
            if regex_match:
                return x

        checkName = lambda namelist, func: func([re.search(x, name, re.I) for x in namelist])

        if checkName(["(pdtv|hdtv|dsr|tvrip).(xvid|x264)"], all) and not checkName(["(720|1080)[pi]"], all) and not checkName(["hr.ws.pdtv.x264"], any):
            return Quality.SDTV
        elif checkName(["web.dl|webrip", "xvid|x264|h.?264"], all) and not checkName(["(720|1080)[pi]"], all):
            return Quality.SDTV
        elif checkName(["(dvdrip|bdrip)(.ws)?.(xvid|divx|x264)"], any) and not checkName(["(720|1080)[pi]"], all):
            return Quality.SDDVD
        elif checkName(["720p", "hdtv", "x264"], all) or checkName(["hr.ws.pdtv.x264"], any) and not checkName(["(1080)[pi]"], all):
            return Quality.HDTV
        elif checkName(["720p|1080i", "hdtv", "mpeg-?2"], all) or checkName(["1080[pi].hdtv", "h.?264"], all):
            return Quality.RAWHDTV
        elif checkName(["1080p", "hdtv", "x264"], all):
            return Quality.FULLHDTV
        elif checkName(["720p", "web.dl|webrip"], all) or checkName(["720p", "itunes", "h.?264"], all):
            return Quality.HDWEBDL
        elif checkName(["1080p", "web.dl|webrip"], all) or checkName(["1080p", "itunes", "h.?264"], all):
            return Quality.FULLHDWEBDL
        elif checkName(["720p", "bluray|hddvd", "x264"], all):
            return Quality.HDBLURAY
        elif checkName(["1080p", "bluray|hddvd", "x264"], all):
            return Quality.FULLHDBLURAY
        else:
            return Quality.UNKNOWN

    @staticmethod
    def assumeQuality(name):
        if name.lower().endswith((".avi", ".mp4")):
            return Quality.SDTV
        elif name.lower().endswith(".mkv"):
            return Quality.HDTV
        elif name.lower().endswith(".ts"):
            return Quality.RAWHDTV
        else:
            return Quality.UNKNOWN

    @staticmethod
    def compositeStatus(status, quality):
        return status + 100 * quality

    @staticmethod
    def qualityDownloaded(status):
        return (status - DOWNLOADED) / 100

    @staticmethod
    def splitCompositeStatus(status):
        """Returns a tuple containing (status, quality)"""
        if status == UNKNOWN:
            return (UNKNOWN, Quality.UNKNOWN)

        for x in sorted(Quality.qualityStrings.keys(), reverse=True):
            if status > x * 100:
                return (status - x * 100, x)

        return (status, Quality.NONE)

    @staticmethod
    def statusFromName(name, assume=True):
        quality = Quality.nameQuality(name)
        if assume and quality == Quality.UNKNOWN:
            quality = Quality.assumeQuality(name)
        return Quality.compositeStatus(DOWNLOADED, quality)

    DOWNLOADED = None
    SNATCHED = None
    SNATCHED_PROPER = None

Quality.DOWNLOADED = [Quality.compositeStatus(DOWNLOADED, x) for x in Quality.qualityStrings.keys()]
Quality.SNATCHED = [Quality.compositeStatus(SNATCHED, x) for x in Quality.qualityStrings.keys()]
Quality.SNATCHED_PROPER = [Quality.compositeStatus(SNATCHED_PROPER, x) for x in Quality.qualityStrings.keys()]

SD = Quality.combineQualities([Quality.SDTV, Quality.SDDVD], [])
HD = Quality.combineQualities([Quality.HDTV, Quality.FULLHDTV, Quality.HDWEBDL, Quality.FULLHDWEBDL, Quality.HDBLURAY, Quality.FULLHDBLURAY], [])  # HD720p + HD1080p
HD720p = Quality.combineQualities([Quality.HDTV, Quality.HDWEBDL, Quality.HDBLURAY], [])
HD1080p = Quality.combineQualities([Quality.FULLHDTV, Quality.FULLHDWEBDL, Quality.FULLHDBLURAY], [])
ANY = Quality.combineQualities([Quality.SDTV, Quality.SDDVD, Quality.HDTV, Quality.FULLHDTV, Quality.HDWEBDL, Quality.FULLHDWEBDL, Quality.HDBLURAY, Quality.FULLHDBLURAY, Quality.UNKNOWN], [])  # SD + HD

qualityPresets = (SD, HD, HD720p, HD1080p, ANY)
qualityPresetStrings = {SD: "SD",
                        HD: "HD",
                        HD720p: "HD720p",
                        HD1080p: "HD1080p",
                        ANY: "Any"}


class StatusStrings:
    def __init__(self):
        self.statusStrings = {UNKNOWN: "Unknown",
                              UNAIRED: "Unaired",
                              SNATCHED: "Snatched",
                              DOWNLOADED: "Downloaded",
                              SKIPPED: "Skipped",
                              SNATCHED_PROPER: "Snatched (Proper)",
                              WANTED: "Wanted",
                              ARCHIVED: "Archived",
                              IGNORED: "Ignored"}

    def __getitem__(self, name):
        if name in Quality.DOWNLOADED + Quality.SNATCHED + Quality.SNATCHED_PROPER:
            status, quality = Quality.splitCompositeStatus(name)
            if quality == Quality.NONE:
                return self.statusStrings[status]
            else:
                return self.statusStrings[status] + " (" + Quality.qualityStrings[quality] + ")"
        else:
            return self.statusStrings[name]

    def has_key(self, name):
        return name in self.statusStrings or name in Quality.DOWNLOADED or name in Quality.SNATCHED or name in Quality.SNATCHED_PROPER

statusStrings = StatusStrings()


class Overview:
    UNAIRED = UNAIRED  # 1
    QUAL = 2
    WANTED = WANTED  # 3
    GOOD = 4
    SKIPPED = SKIPPED  # 5

    # For both snatched statuses. Note: SNATCHED/QUAL have same value and break dict.
    SNATCHED = SNATCHED_PROPER  # 9

    overviewStrings = {SKIPPED: "skipped",
                       WANTED: "wanted",
                       QUAL: "qual",
                       GOOD: "good",
                       UNAIRED: "unaired",
                       SNATCHED: "snatched"}

# Get our xml namespaces correct for lxml
XML_NSMAP = {'xsi': 'http://www.w3.org/2001/XMLSchema-instance',
             'xsd': 'http://www.w3.org/2001/XMLSchema'}


countryList = {'Australia': 'AU',
               'Canada': 'CA',
               'USA': 'US'
               }

########NEW FILE########
__FILENAME__ = config
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import cherrypy
import os.path
import datetime
import re
import urlparse

from sickbeard import encodingKludge as ek
from sickbeard import helpers
from sickbeard import logger
from sickbeard import naming
from sickbeard import db

import sickbeard

naming_ep_type = ("%(seasonnumber)dx%(episodenumber)02d",
                  "s%(seasonnumber)02de%(episodenumber)02d",
                   "S%(seasonnumber)02dE%(episodenumber)02d",
                   "%(seasonnumber)02dx%(episodenumber)02d")
naming_ep_type_text = ("1x02", "s01e02", "S01E02", "01x02")

naming_multi_ep_type = {0: ["-%(episodenumber)02d"] * len(naming_ep_type),
                        1: [" - " + x for x in naming_ep_type],
                        2: [x + "%(episodenumber)02d" for x in ("x", "e", "E", "x")]}
naming_multi_ep_type_text = ("extend", "duplicate", "repeat")

naming_sep_type = (" - ", " ")
naming_sep_type_text = (" - ", "space")


def change_HTTPS_CERT(https_cert):

    if https_cert == '':
        sickbeard.HTTPS_CERT = ''
        return True

    if os.path.normpath(sickbeard.HTTPS_CERT) != os.path.normpath(https_cert):
        if helpers.makeDir(os.path.dirname(os.path.abspath(https_cert))):
            sickbeard.HTTPS_CERT = os.path.normpath(https_cert)
            logger.log(u"Changed https cert path to " + https_cert)
        else:
            return False

    return True


def change_HTTPS_KEY(https_key):

    if https_key == '':
        sickbeard.HTTPS_KEY = ''
        return True

    if os.path.normpath(sickbeard.HTTPS_KEY) != os.path.normpath(https_key):
        if helpers.makeDir(os.path.dirname(os.path.abspath(https_key))):
            sickbeard.HTTPS_KEY = os.path.normpath(https_key)
            logger.log(u"Changed https key path to " + https_key)
        else:
            return False

    return True


def change_LOG_DIR(log_dir, web_log):

    log_dir_changed = False
    abs_log_dir = os.path.normpath(os.path.join(sickbeard.DATA_DIR, log_dir))
    web_log_value = checkbox_to_value(web_log)

    if os.path.normpath(sickbeard.LOG_DIR) != abs_log_dir:
        if helpers.makeDir(abs_log_dir):
            sickbeard.ACTUAL_LOG_DIR = os.path.normpath(log_dir)
            sickbeard.LOG_DIR = abs_log_dir

            logger.sb_log_instance.initLogging()
            logger.log(u"Initialized new log file in " + sickbeard.LOG_DIR)
            log_dir_changed = True

        else:
            return False

    if sickbeard.WEB_LOG != web_log_value or log_dir_changed == True:

        sickbeard.WEB_LOG = web_log_value

        if sickbeard.WEB_LOG:
            cherry_log = os.path.join(sickbeard.LOG_DIR, "cherrypy.log")
            logger.log(u"Change cherry log file to " + cherry_log)
        else:
            cherry_log = None
            logger.log(u"Disable cherry logging")

        cherrypy.config.update({'log.access_file': cherry_log})

    return True


def change_NZB_DIR(nzb_dir):

    if nzb_dir == '':
        sickbeard.NZB_DIR = ''
        return True

    if os.path.normpath(sickbeard.NZB_DIR) != os.path.normpath(nzb_dir):
        if helpers.makeDir(nzb_dir):
            sickbeard.NZB_DIR = os.path.normpath(nzb_dir)
            logger.log(u"Changed NZB folder to " + nzb_dir)
        else:
            return False

    return True


def change_TORRENT_DIR(torrent_dir):

    if torrent_dir == '':
        sickbeard.TORRENT_DIR = ''
        return True

    if os.path.normpath(sickbeard.TORRENT_DIR) != os.path.normpath(torrent_dir):
        if helpers.makeDir(torrent_dir):
            sickbeard.TORRENT_DIR = os.path.normpath(torrent_dir)
            logger.log(u"Changed torrent folder to " + torrent_dir)
        else:
            return False

    return True


def change_TV_DOWNLOAD_DIR(tv_download_dir):

    if tv_download_dir == '':
        sickbeard.TV_DOWNLOAD_DIR = ''
        return True

    if os.path.normpath(sickbeard.TV_DOWNLOAD_DIR) != os.path.normpath(tv_download_dir):
        if helpers.makeDir(tv_download_dir):
            sickbeard.TV_DOWNLOAD_DIR = os.path.normpath(tv_download_dir)
            logger.log(u"Changed TV download folder to " + tv_download_dir)
        else:
            return False

    return True


def change_SEARCH_FREQUENCY(freq):

    sickbeard.SEARCH_FREQUENCY = to_int(freq, default=sickbeard.DEFAULT_SEARCH_FREQUENCY)

    if sickbeard.SEARCH_FREQUENCY < sickbeard.MIN_SEARCH_FREQUENCY:
        sickbeard.SEARCH_FREQUENCY = sickbeard.MIN_SEARCH_FREQUENCY

    sickbeard.currentSearchScheduler.cycleTime = datetime.timedelta(minutes=sickbeard.SEARCH_FREQUENCY)
    sickbeard.backlogSearchScheduler.cycleTime = datetime.timedelta(minutes=sickbeard.get_backlog_cycle_time())


def change_VERSION_NOTIFY(version_notify):

    oldSetting = sickbeard.VERSION_NOTIFY

    sickbeard.VERSION_NOTIFY = version_notify

    if version_notify == False:
        sickbeard.NEWEST_VERSION_STRING = None

    if oldSetting == False and version_notify == True:
        sickbeard.versionCheckScheduler.action.run()  # @UndefinedVariable


def CheckSection(CFG, sec):
    """ Check if INI section exists, if not create it """
    try:
        CFG[sec]
        return True
    except:
        CFG[sec] = {}
        return False


def checkbox_to_value(option, value_on=1, value_off=0):
    """
    Turns checkbox option 'on' or 'true' to value_on (1)
    any other value returns value_off (0)
    """
    if option == 'on' or option == 'true':
        return value_on

    return value_off


def clean_host(host, default_port=None):
    """
    Returns host or host:port or empty string from a given url or host
    If no port is found and default_port is given use host:default_port
    """

    host = host.strip()

    if host:

        match_host_port = re.search(r'(?:http.*://)?(?P<host>[^:/]+).?(?P<port>[0-9]*).*', host)

        cleaned_host = match_host_port.group('host')
        cleaned_port = match_host_port.group('port')

        if cleaned_host:

            if cleaned_port:
                host = cleaned_host + ':' + cleaned_port

            elif default_port:
                host = cleaned_host + ':' + str(default_port)

            else:
                host = cleaned_host

        else:
            host = ''

    return host


def clean_hosts(hosts, default_port=None):

    cleaned_hosts = []

    for cur_host in [x.strip() for x in hosts.split(",")]:
        if cur_host:
            cleaned_host = clean_host(cur_host, default_port)
            if cleaned_host:
                cleaned_hosts.append(cleaned_host)

    if cleaned_hosts:
        cleaned_hosts = ",".join(cleaned_hosts)

    else:
        cleaned_hosts = ''

    return cleaned_hosts


def clean_url(url):
    """
    Returns an cleaned url starting with a scheme and folder with trailing /
    or an empty string
    """

    if url and url.strip():

        url = url.strip()

        if '://' not in url:
            url = '//' + url

        scheme, netloc, path, query, fragment = urlparse.urlsplit(url, 'http')

        if not path.endswith('/'):
            basename, ext = ek.ek(os.path.splitext, ek.ek(os.path.basename, path))  # @UnusedVariable
            if not ext:
                path = path + '/'

        cleaned_url = urlparse.urlunsplit((scheme, netloc, path, query, fragment))

    else:
        cleaned_url = ''

    return cleaned_url


def to_int(val, default=0):
    """ Return int value of val or default on error """

    try:
        val = int(val)
    except:
        val = default

    return val


################################################################################
# Check_setting_int                                                            #
################################################################################
def minimax(val, default, low, high):
    """ Return value forced within range """

    val = to_int(val, default=default)

    if val < low:
        return low
    if val > high:
        return high

    return val


################################################################################
# Check_setting_int                                                            #
################################################################################
def check_setting_int(config, cfg_name, item_name, def_val):
    try:
        my_val = int(config[cfg_name][item_name])
    except:
        my_val = def_val
        try:
            config[cfg_name][item_name] = my_val
        except:
            config[cfg_name] = {}
            config[cfg_name][item_name] = my_val
    logger.log(item_name + " -> " + str(my_val), logger.DEBUG)
    return my_val


################################################################################
# Check_setting_float                                                          #
################################################################################
def check_setting_float(config, cfg_name, item_name, def_val):
    try:
        my_val = float(config[cfg_name][item_name])
    except:
        my_val = def_val
        try:
            config[cfg_name][item_name] = my_val
        except:
            config[cfg_name] = {}
            config[cfg_name][item_name] = my_val

    logger.log(item_name + " -> " + str(my_val), logger.DEBUG)
    return my_val


################################################################################
# Check_setting_str                                                            #
################################################################################
def check_setting_str(config, cfg_name, item_name, def_val, log=True):
    try:
        my_val = config[cfg_name][item_name]
    except:
        my_val = def_val
        try:
            config[cfg_name][item_name] = my_val
        except:
            config[cfg_name] = {}
            config[cfg_name][item_name] = my_val

    if log:
        logger.log(item_name + " -> " + my_val, logger.DEBUG)
    else:
        logger.log(item_name + " -> ******", logger.DEBUG)
    return my_val


class ConfigMigrator():

    def __init__(self, config_obj):
        """
        Initializes a config migrator that can take the config from the version indicated in the config
        file up to the version required by SB
        """

        self.config_obj = config_obj

        # check the version of the config
        self.config_version = check_setting_int(config_obj, 'General', 'config_version', sickbeard.CONFIG_VERSION)
        self.expected_config_version = sickbeard.CONFIG_VERSION
        self.migration_names = {1: 'Custom naming',
                                2: 'Sync backup number with version number',
                                3: 'Rename omgwtfnzb variables',
                                4: 'Add newznab catIDs',
                                5: 'Metadata update'
                                }

    def migrate_config(self):
        """
        Calls each successive migration until the config is the same version as SB expects
        """

        if self.config_version > self.expected_config_version:
            logger.log_error_and_exit(u"Your config version (" + str(self.config_version) + ") has been incremented past what this version of Sick Beard supports (" + str(self.expected_config_version) + ").\n" + \
                                      "If you have used other forks or a newer version of Sick Beard, your config file may be unusable due to their modifications.")

        sickbeard.CONFIG_VERSION = self.config_version

        while self.config_version < self.expected_config_version:

            next_version = self.config_version + 1

            if next_version in self.migration_names:
                migration_name = ': ' + self.migration_names[next_version]
            else:
                migration_name = ''

            logger.log(u"Backing up config before upgrade")
            if not helpers.backupVersionedFile(sickbeard.CONFIG_FILE, self.config_version):
                logger.log_error_and_exit(u"Config backup failed, abort upgrading config")
            else:
                logger.log(u"Proceeding with upgrade")

            # do the migration, expect a method named _migrate_v<num>
            logger.log(u"Migrating config up to version " + str(next_version) + migration_name)
            getattr(self, '_migrate_v' + str(next_version))()
            self.config_version = next_version

            # save new config after migration
            sickbeard.CONFIG_VERSION = self.config_version
            logger.log(u"Saving config file to disk")
            sickbeard.save_config()

    # Migration v1: Custom naming
    def _migrate_v1(self):
        """
        Reads in the old naming settings from your config and generates a new config template from them.
        """

        sickbeard.NAMING_PATTERN = self._name_to_pattern()
        logger.log("Based on your old settings I'm setting your new naming pattern to: " + sickbeard.NAMING_PATTERN)

        sickbeard.NAMING_CUSTOM_ABD = bool(check_setting_int(self.config_obj, 'General', 'naming_dates', 0))

        if sickbeard.NAMING_CUSTOM_ABD:
            sickbeard.NAMING_ABD_PATTERN = self._name_to_pattern(True)
            logger.log("Adding a custom air-by-date naming pattern to your config: " + sickbeard.NAMING_ABD_PATTERN)
        else:
            sickbeard.NAMING_ABD_PATTERN = naming.name_abd_presets[0]

        sickbeard.NAMING_MULTI_EP = int(check_setting_int(self.config_obj, 'General', 'naming_multi_ep_type', 1))

        # see if any of their shows used season folders
        myDB = db.DBConnection()
        season_folder_shows = myDB.select("SELECT * FROM tv_shows WHERE flatten_folders = 0")

        # if any shows had season folders on then prepend season folder to the pattern
        if season_folder_shows:

            old_season_format = check_setting_str(self.config_obj, 'General', 'season_folders_format', 'Season %02d')

            if old_season_format:
                try:
                    new_season_format = old_season_format % 9
                    new_season_format = new_season_format.replace('09', '%0S')
                    new_season_format = new_season_format.replace('9', '%S')

                    logger.log(u"Changed season folder format from " + old_season_format + " to " + new_season_format + ", prepending it to your naming config")
                    sickbeard.NAMING_PATTERN = new_season_format + os.sep + sickbeard.NAMING_PATTERN

                except (TypeError, ValueError):
                    logger.log(u"Can't change " + old_season_format + " to new season format", logger.ERROR)

        # if no shows had it on then don't flatten any shows and don't put season folders in the config
        else:

            logger.log(u"No shows were using season folders before so I'm disabling flattening on all shows")

            # don't flatten any shows at all
            myDB.action("UPDATE tv_shows SET flatten_folders = 0")

        sickbeard.NAMING_FORCE_FOLDERS = naming.check_force_season_folders()

    def _name_to_pattern(self, abd=False):

        # get the old settings from the file
        use_periods = bool(check_setting_int(self.config_obj, 'General', 'naming_use_periods', 0))
        ep_type = check_setting_int(self.config_obj, 'General', 'naming_ep_type', 0)
        sep_type = check_setting_int(self.config_obj, 'General', 'naming_sep_type', 0)
        use_quality = bool(check_setting_int(self.config_obj, 'General', 'naming_quality', 0))

        use_show_name = bool(check_setting_int(self.config_obj, 'General', 'naming_show_name', 1))
        use_ep_name = bool(check_setting_int(self.config_obj, 'General', 'naming_ep_name', 1))

        # make the presets into templates
        naming_ep_type = ("%Sx%0E",
                          "s%0Se%0E",
                          "S%0SE%0E",
                          "%0Sx%0E")
        naming_sep_type = (" - ", " ")

        # set up our data to use
        if use_periods:
            show_name = '%S.N'
            ep_name = '%E.N'
            ep_quality = '%Q.N'
            abd_string = '%A.D'
        else:
            show_name = '%SN'
            ep_name = '%EN'
            ep_quality = '%QN'
            abd_string = '%A-D'

        if abd:
            ep_string = abd_string
        else:
            ep_string = naming_ep_type[ep_type]

        finalName = ""

        # start with the show name
        if use_show_name:
            finalName += show_name + naming_sep_type[sep_type]

        # add the season/ep stuff
        finalName += ep_string

        # add the episode name
        if use_ep_name:
            finalName += naming_sep_type[sep_type] + ep_name

        # add the quality
        if use_quality:
            finalName += naming_sep_type[sep_type] + ep_quality

        if use_periods:
            finalName = re.sub("\s+", ".", finalName)

        return finalName

    # Migration v2: Dummy migration to sync backup number with config version number
    def _migrate_v2(self):
        return

    # Migration v2: Rename omgwtfnzb variables
    def _migrate_v3(self):
        """
        Reads in the old naming settings from your config and generates a new config template from them.
        """
        # get the old settings from the file and store them in the new variable names
        sickbeard.OMGWTFNZBS_USERNAME = check_setting_str(self.config_obj, 'omgwtfnzbs', 'omgwtfnzbs_uid', '')
        sickbeard.OMGWTFNZBS_APIKEY = check_setting_str(self.config_obj, 'omgwtfnzbs', 'omgwtfnzbs_key', '')

    # Migration v4: Add default newznab catIDs
    def _migrate_v4(self):
        """ Update newznab providers so that the category IDs can be set independently via the config """

        new_newznab_data = []
        old_newznab_data = check_setting_str(self.config_obj, 'Newznab', 'newznab_data', '')

        if old_newznab_data:
            old_newznab_data_list = old_newznab_data.split("!!!")

            for cur_provider_data in old_newznab_data_list:
                try:
                    name, url, key, enabled = cur_provider_data.split("|")
                except ValueError:
                    logger.log(u"Skipping Newznab provider string: '" + cur_provider_data + "', incorrect format", logger.ERROR)
                    continue

                if name == 'Sick Beard Index':
                    key = '0'

                if name == 'NZBs.org':
                    catIDs = '5030,5040,5070,5090'
                else:
                    catIDs = '5030,5040'

                cur_provider_data_list = [name, url, key, catIDs, enabled]
                new_newznab_data.append("|".join(cur_provider_data_list))

            sickbeard.NEWZNAB_DATA = "!!!".join(new_newznab_data)

    # Migration v5: Metadata upgrade
    def _migrate_v5(self):
        """ Updates metadata values to the new format """

        """ Quick overview of what the upgrade does:

        new | old | description (new)
        ----+-----+--------------------
          1 |  1  | show metadata
          2 |  2  | episode metadata
          3 |  4  | show fanart
          4 |  3  | show poster
          5 |  -  | show banner
          6 |  5  | episode thumb
          7 |  6  | season poster
          8 |  -  | season banner
          9 |  -  | season all poster
         10 |  -  | season all banner

        Note that the ini places start at 1 while the list index starts at 0.
        old format: 0|0|0|0|0|0 -- 6 places
        new format: 0|0|0|0|0|0|0|0|0|0 -- 10 places

        Drop the use of use_banner option.
        Migrate the poster override to just using the banner option (applies to xbmc only).
        """

        metadata_xbmc = check_setting_str(self.config_obj, 'General', 'metadata_xbmc', '0|0|0|0|0|0')
        metadata_xbmc_12plus = check_setting_str(self.config_obj, 'General', 'metadata_xbmc_12plus', '0|0|0|0|0|0')
        metadata_mediabrowser = check_setting_str(self.config_obj, 'General', 'metadata_mediabrowser', '0|0|0|0|0|0')
        metadata_ps3 = check_setting_str(self.config_obj, 'General', 'metadata_ps3', '0|0|0|0|0|0')
        metadata_wdtv = check_setting_str(self.config_obj, 'General', 'metadata_wdtv', '0|0|0|0|0|0')
        metadata_tivo = check_setting_str(self.config_obj, 'General', 'metadata_tivo', '0|0|0|0|0|0')
        metadata_mede8er = check_setting_str(self.config_obj, 'General', 'metadata_mede8er', '0|0|0|0|0|0')

        use_banner = bool(check_setting_int(self.config_obj, 'General', 'use_banner', 0))

        def _migrate_metadata(metadata, metadata_name, use_banner):
            cur_metadata = metadata.split('|')
            # if target has the old number of values, do upgrade
            if len(cur_metadata) == 6:
                logger.log(u"Upgrading " + metadata_name + " metadata, old value: " + metadata)
                cur_metadata.insert(4, '0')
                cur_metadata.append('0')
                cur_metadata.append('0')
                cur_metadata.append('0')
                # swap show fanart, show poster
                cur_metadata[3], cur_metadata[2] = cur_metadata[2], cur_metadata[3]
                # if user was using use_banner to override the poster, instead enable the banner option and deactivate poster
                if metadata_name == 'XBMC' and use_banner:
                    cur_metadata[4], cur_metadata[3] = cur_metadata[3], '0'
                # write new format
                metadata = '|'.join(cur_metadata)
                logger.log(u"Upgrading " + metadata_name + " metadata, new value: " + metadata)

            elif len(cur_metadata) == 10:
                metadata = '|'.join(cur_metadata)
                logger.log(u"Keeping " + metadata_name + " metadata, value: " + metadata)

            else:
                logger.log(u"Skipping " + metadata_name + " metadata: '" + metadata + "', incorrect format", logger.ERROR)
                metadata = '0|0|0|0|0|0|0|0|0|0'
                logger.log(u"Setting " + metadata_name + " metadata, new value: " + metadata)

            return metadata

        sickbeard.METADATA_XBMC = _migrate_metadata(metadata_xbmc, 'XBMC', use_banner)
        sickbeard.METADATA_XBMC_12PLUS = _migrate_metadata(metadata_xbmc_12plus, 'XBMC 12+', use_banner)
        sickbeard.METADATA_MEDIABROWSER = _migrate_metadata(metadata_mediabrowser, 'MediaBrowser', use_banner)
        sickbeard.METADATA_PS3 = _migrate_metadata(metadata_ps3, 'PS3', use_banner)
        sickbeard.METADATA_WDTV = _migrate_metadata(metadata_wdtv, 'WDTV', use_banner)
        sickbeard.METADATA_TIVO = _migrate_metadata(metadata_tivo, 'TIVO', use_banner)
        sickbeard.METADATA_MEDE8ER = _migrate_metadata(metadata_mede8er, 'Mede8er', use_banner)

    # Migration v6: Synology notifier update
    def _migrate_v6(self):
        """ Updates Synology notifier to reflect that their now is an update library option instead misusing the enable option """

        # clone use_synoindex to update_library since this now has notification options
        sickbeard.SYNOINDEX_UPDATE_LIBRARY = bool(check_setting_int(self.config_obj, 'Synology', 'use_synoindex', 0))

########NEW FILE########
__FILENAME__ = cache_db
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from sickbeard import db


# Add new migrations at the bottom of the list; subclass the previous migration.
class InitialSchema (db.SchemaUpgrade):
    def test(self):
        return self.hasTable("lastUpdate")

    def execute(self):

        queries = [
            ("CREATE TABLE lastUpdate (provider TEXT, time NUMERIC);",),
            ("CREATE TABLE db_version (db_version INTEGER);",),
            ("INSERT INTO db_version (db_version) VALUES (?)", 1),
        ]
        for query in queries:
            if len(query) == 1:
                self.connection.action(query[0])
            else:
                self.connection.action(query[0], query[1:])


class AddSceneExceptions(InitialSchema):
    def test(self):
        return self.hasTable("scene_exceptions")

    def execute(self):
        self.connection.action("CREATE TABLE scene_exceptions (exception_id INTEGER PRIMARY KEY, tvdb_id INTEGER KEY, show_name TEXT, provider TEXT)")


class AddSceneNameCache(AddSceneExceptions):
    def test(self):
        return self.hasTable("scene_names")

    def execute(self):
        self.connection.action("CREATE TABLE scene_names (tvdb_id INTEGER, name TEXT)")


class AddSceneExceptionsProvider(AddSceneNameCache):
    def test(self):
        return self.hasColumn("scene_exceptions", "provider")

    def execute(self):
        if not self.hasColumn("scene_exceptions", "provider"):
            self.addColumn("scene_exceptions", "provider", data_type='TEXT', default='sb_tvdb_scene_exceptions')

########NEW FILE########
__FILENAME__ = mainDB
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import sickbeard
import os.path
import sys

from sickbeard import db, common, helpers, logger

from sickbeard import encodingKludge as ek
from sickbeard.name_parser.parser import NameParser, InvalidNameException

MIN_DB_VERSION = 9  # oldest db version we support migrating from
MAX_DB_VERSION = 15


class MainSanityCheck(db.DBSanityCheck):
    def check(self):
        self.fix_duplicate_shows()
        self.fix_duplicate_episodes()
        self.fix_orphan_episodes()

    def fix_duplicate_shows(self):
        sqlResults = self.connection.select("SELECT show_id, tvdb_id, COUNT(tvdb_id) as count FROM tv_shows GROUP BY tvdb_id HAVING count > 1")

        for cur_duplicate in sqlResults:

            logger.log(u"Duplicate show detected! tvdb_id: " + str(cur_duplicate["tvdb_id"]) + u" count: " + str(cur_duplicate["count"]), logger.DEBUG)

            cur_dupe_results = self.connection.select("SELECT show_id, tvdb_id FROM tv_shows WHERE tvdb_id = ? LIMIT ?",
                                           [cur_duplicate["tvdb_id"], int(cur_duplicate["count"]) - 1]
                                           )

            for cur_dupe_id in cur_dupe_results:
                logger.log(u"Deleting duplicate show with tvdb_id: " + str(cur_dupe_id["tvdb_id"]) + u" show_id: " + str(cur_dupe_id["show_id"]))
                self.connection.action("DELETE FROM tv_shows WHERE show_id = ?", [cur_dupe_id["show_id"]])

        else:
            logger.log(u"No duplicate show, check passed")

    def fix_duplicate_episodes(self):
        sqlResults = self.connection.select("SELECT showid, season, episode, COUNT(showid) as count FROM tv_episodes GROUP BY showid, season, episode HAVING count > 1")

        for cur_duplicate in sqlResults:

            logger.log(u"Duplicate episode detected! showid: " + str(cur_duplicate["showid"]) + u" season: " + str(cur_duplicate["season"]) + u" episode: " + str(cur_duplicate["episode"]) + u" count: " + str(cur_duplicate["count"]), logger.DEBUG)

            cur_dupe_results = self.connection.select("SELECT episode_id FROM tv_episodes WHERE showid = ? AND season = ? and episode = ? ORDER BY episode_id DESC LIMIT ?",
                                           [cur_duplicate["showid"], cur_duplicate["season"], cur_duplicate["episode"], int(cur_duplicate["count"]) - 1]
                                           )

            for cur_dupe_id in cur_dupe_results:
                logger.log(u"Deleting duplicate episode with episode_id: " + str(cur_dupe_id["episode_id"]))
                self.connection.action("DELETE FROM tv_episodes WHERE episode_id = ?", [cur_dupe_id["episode_id"]])

        else:
            logger.log(u"No duplicate episode, check passed")

    def fix_orphan_episodes(self):
        sqlResults = self.connection.select("SELECT episode_id, showid, tv_shows.tvdb_id FROM tv_episodes LEFT JOIN tv_shows ON tv_episodes.showid=tv_shows.tvdb_id WHERE tv_shows.tvdb_id is NULL")

        for cur_orphan in sqlResults:
            logger.log(u"Orphan episode detected! episode_id: " + str(cur_orphan["episode_id"]) + " showid: " + str(cur_orphan["showid"]), logger.DEBUG)
            logger.log(u"Deleting orphan episode with episode_id: " + str(cur_orphan["episode_id"]))
            self.connection.action("DELETE FROM tv_episodes WHERE episode_id = ?", [cur_orphan["episode_id"]])

        else:
            logger.log(u"No orphan episodes, check passed")


def backupDatabase(version):
    logger.log(u"Backing up database before upgrade")

    if not helpers.backupVersionedFile(db.dbFilename(), version):
        logger.log_error_and_exit(u"Database backup failed, abort upgrading database")
    else:
        logger.log(u"Proceeding with upgrade")

# ======================
# = Main DB Migrations =
# ======================
# Add new migrations at the bottom of the list; subclass the previous migration.


# schema is based off v15 - build 504
class InitialSchema (db.SchemaUpgrade):
    def test(self):
        return self.hasTable("tv_shows") and self.hasTable("db_version") and self.checkDBVersion() >= MIN_DB_VERSION and self.checkDBVersion() <= MAX_DB_VERSION

    def execute(self):
        if not self.hasTable("tv_shows") and not self.hasTable("db_version"):
            queries = [
                "CREATE TABLE db_version (db_version INTEGER);",
                "CREATE TABLE history (action NUMERIC, date NUMERIC, showid NUMERIC, season NUMERIC, episode NUMERIC, quality NUMERIC, resource TEXT, provider TEXT);",
                "CREATE TABLE info (last_backlog NUMERIC, last_tvdb NUMERIC);",
                "CREATE TABLE tv_episodes (episode_id INTEGER PRIMARY KEY, showid NUMERIC, tvdbid NUMERIC, name TEXT, season NUMERIC, episode NUMERIC, description TEXT, airdate NUMERIC, hasnfo NUMERIC, hastbn NUMERIC, status NUMERIC, location TEXT, file_size NUMERIC, release_name TEXT);",
                "CREATE TABLE tv_shows (show_id INTEGER PRIMARY KEY, location TEXT, show_name TEXT, tvdb_id NUMERIC, network TEXT, genre TEXT, runtime NUMERIC, quality NUMERIC, airs TEXT, status TEXT, flatten_folders NUMERIC, paused NUMERIC, startyear NUMERIC, tvr_id NUMERIC, tvr_name TEXT, air_by_date NUMERIC, lang TEXT, last_update_tvdb NUMERIC, rls_require_words TEXT, rls_ignore_words TEXT);",
                "CREATE INDEX idx_tv_episodes_showid_airdate ON tv_episodes (showid,airdate);",
                "CREATE INDEX idx_showid ON tv_episodes (showid);",
                "CREATE UNIQUE INDEX idx_tvdb_id ON tv_shows (tvdb_id);",
                "INSERT INTO db_version (db_version) VALUES (15);"
            ]

            for query in queries:
                self.connection.action(query)

        else:
            cur_db_version = self.checkDBVersion()

            if cur_db_version < MIN_DB_VERSION:
                logger.log_error_and_exit(u"Your database version (" + str(cur_db_version) + ") is too old to migrate from what this version of Sick Beard supports (" + \
                                          str(MIN_DB_VERSION) + ").\n" + \
                                          "Upgrade using a previous version (tag) build 496 to build 501 of Sick Beard first or remove database file to begin fresh."
                                          )

            if cur_db_version > MAX_DB_VERSION:
                logger.log_error_and_exit(u"Your database version (" + str(cur_db_version) + ") has been incremented past what this version of Sick Beard supports (" + \
                                          str(MAX_DB_VERSION) + ").\n" + \
                                          "If you have used other forks of Sick Beard, your database may be unusable due to their modifications."
                                          )


# included in build 496 (2012-06-28)
class AddSizeAndSceneNameFields(InitialSchema):
    def test(self):
        return self.checkDBVersion() >= 10

    def execute(self):
        backupDatabase(10)

        if not self.hasColumn("tv_episodes", "file_size"):
            self.addColumn("tv_episodes", "file_size")

        if not self.hasColumn("tv_episodes", "release_name"):
            self.addColumn("tv_episodes", "release_name", "TEXT", "")

        ep_results = self.connection.select("SELECT episode_id, location, file_size FROM tv_episodes")

        logger.log(u"Adding file size to all episodes in DB, please be patient")
        for cur_ep in ep_results:
            if not cur_ep["location"]:
                continue

            # if there is no size yet then populate it for us
            if (not cur_ep["file_size"] or not int(cur_ep["file_size"])) and ek.ek(os.path.isfile, cur_ep["location"]):
                cur_size = ek.ek(os.path.getsize, cur_ep["location"])
                self.connection.action("UPDATE tv_episodes SET file_size = ? WHERE episode_id = ?", [cur_size, int(cur_ep["episode_id"])])

        # check each snatch to see if we can use it to get a release name from
        history_results = self.connection.select("SELECT * FROM history WHERE provider != -1 ORDER BY date ASC")

        logger.log(u"Adding release name to all episodes still in history")
        for cur_result in history_results:
            # find the associated download, if there isn't one then ignore it
            download_results = self.connection.select("SELECT resource FROM history WHERE provider = -1 AND showid = ? AND season = ? AND episode = ? AND date > ?",
                                                    [cur_result["showid"], cur_result["season"], cur_result["episode"], cur_result["date"]])
            if not download_results:
                logger.log(u"Found a snatch in the history for " + cur_result["resource"] + " but couldn't find the associated download, skipping it", logger.DEBUG)
                continue

            nzb_name = cur_result["resource"]
            file_name = ek.ek(os.path.basename, download_results[0]["resource"])

            # take the extension off the filename, it's not needed
            if '.' in file_name:
                file_name = file_name.rpartition('.')[0]

            # find the associated episode on disk
            ep_results = self.connection.select("SELECT episode_id, status FROM tv_episodes WHERE showid = ? AND season = ? AND episode = ? AND location != ''",
                                                [cur_result["showid"], cur_result["season"], cur_result["episode"]])
            if not ep_results:
                logger.log(u"The episode " + nzb_name + " was found in history but doesn't exist on disk anymore, skipping", logger.DEBUG)
                continue

            # get the status/quality of the existing ep and make sure it's what we expect
            ep_status, ep_quality = common.Quality.splitCompositeStatus(int(ep_results[0]["status"]))
            if ep_status != common.DOWNLOADED:
                continue

            if ep_quality != int(cur_result["quality"]):
                continue

            # make sure this is actually a real release name and not a season pack or something
            for cur_name in (nzb_name, file_name):
                logger.log(u"Checking if " + cur_name + " is actually a good release name", logger.DEBUG)
                try:
                    np = NameParser(False)
                    parse_result = np.parse(cur_name)
                except InvalidNameException:
                    continue

                if parse_result.series_name and parse_result.season_number != None and parse_result.episode_numbers and parse_result.release_group:
                    # if all is well by this point we'll just put the release name into the database
                    self.connection.action("UPDATE tv_episodes SET release_name = ? WHERE episode_id = ?", [cur_name, ep_results[0]["episode_id"]])
                    break

        # check each snatch to see if we can use it to get a release name from
        empty_results = self.connection.select("SELECT episode_id, location FROM tv_episodes WHERE release_name = ''")

        logger.log(u"Adding release name to all episodes with obvious scene filenames")
        for cur_result in empty_results:

            ep_file_name = ek.ek(os.path.basename, cur_result["location"])
            ep_file_name = os.path.splitext(ep_file_name)[0]

            # only want to find real scene names here so anything with a space in it is out
            if ' ' in ep_file_name:
                continue

            try:
                np = NameParser(False)
                parse_result = np.parse(ep_file_name)
            except InvalidNameException:
                continue

            if not parse_result.release_group:
                continue

            logger.log(u"Name " + ep_file_name + " gave release group of " + parse_result.release_group + ", seems valid", logger.DEBUG)
            self.connection.action("UPDATE tv_episodes SET release_name = ? WHERE episode_id = ?", [ep_file_name, cur_result["episode_id"]])

        self.incDBVersion()


# included in build 497 (2012-10-16)
class RenameSeasonFolders(AddSizeAndSceneNameFields):
    def test(self):
        return self.checkDBVersion() >= 11

    def execute(self):
        backupDatabase(11)
        # rename the column
        self.connection.action("ALTER TABLE tv_shows RENAME TO tmp_tv_shows")
        self.connection.action("CREATE TABLE tv_shows (show_id INTEGER PRIMARY KEY, location TEXT, show_name TEXT, tvdb_id NUMERIC, network TEXT, genre TEXT, runtime NUMERIC, quality NUMERIC, airs TEXT, status TEXT, flatten_folders NUMERIC, paused NUMERIC, startyear NUMERIC, tvr_id NUMERIC, tvr_name TEXT, air_by_date NUMERIC, lang TEXT)")
        sql = "INSERT INTO tv_shows(show_id, location, show_name, tvdb_id, network, genre, runtime, quality, airs, status, flatten_folders, paused, startyear, tvr_id, tvr_name, air_by_date, lang) SELECT show_id, location, show_name, tvdb_id, network, genre, runtime, quality, airs, status, seasonfolders, paused, startyear, tvr_id, tvr_name, air_by_date, lang FROM tmp_tv_shows"
        self.connection.action(sql)

        # flip the values to be opposite of what they were before
        self.connection.action("UPDATE tv_shows SET flatten_folders = 2 WHERE flatten_folders = 1")
        self.connection.action("UPDATE tv_shows SET flatten_folders = 1 WHERE flatten_folders = 0")
        self.connection.action("UPDATE tv_shows SET flatten_folders = 0 WHERE flatten_folders = 2")
        self.connection.action("DROP TABLE tmp_tv_shows")

        self.incDBVersion()


# included in build 500 (2013-05-11)
class Add1080pAndRawHDQualities(RenameSeasonFolders):
    """Add support for 1080p related qualities along with RawHD

    Quick overview of what the upgrade needs to do:

           quality   | old  | new
        --------------------------
        hdwebdl      | 1<<3 | 1<<5
        hdbluray     | 1<<4 | 1<<7
        fullhdbluray | 1<<5 | 1<<8
        --------------------------
        rawhdtv      |      | 1<<3
        fullhdtv     |      | 1<<4
        fullhdwebdl  |      | 1<<6
    """

    def test(self):
        return self.checkDBVersion() >= 12

    def _update_status(self, old_status):
        (status, quality) = common.Quality.splitCompositeStatus(old_status)
        return common.Quality.compositeStatus(status, self._update_quality(quality))

    def _update_quality(self, old_quality):
        """Update bitwise flags to reflect new quality values

        Check flag bits (clear old then set their new locations) starting
        with the highest bits so we dont overwrite data we need later on
        """

        result = old_quality
        # move fullhdbluray from 1<<5 to 1<<8 if set
        if(result & (1<<5)):
            result = result & ~(1<<5)
            result = result | (1<<8)
        # move hdbluray from 1<<4 to 1<<7 if set
        if(result & (1<<4)):
            result = result & ~(1<<4)
            result = result | (1<<7)
        # move hdwebdl from 1<<3 to 1<<5 if set
        if(result & (1<<3)):
            result = result & ~(1<<3)
            result = result | (1<<5)

        return result

    def _update_composite_qualities(self, status):
        """Unpack, Update, Return new quality values

        Unpack the composite archive/initial values.
        Update either qualities if needed.
        Then return the new compsite quality value.
        """

        best = (status & (0xffff << 16)) >> 16
        initial = status & (0xffff)

        best = self._update_quality(best)
        initial = self._update_quality(initial)

        result = ((best << 16) | initial)
        return result

    def execute(self):
        backupDatabase(12)

        # update the default quality so we dont grab the wrong qualities after migration -- should have really been a config migration
        sickbeard.QUALITY_DEFAULT = self._update_composite_qualities(sickbeard.QUALITY_DEFAULT)
        sickbeard.save_config()

        # upgrade previous HD to HD720p -- shift previous qualities to new placevalues
        old_hd = common.Quality.combineQualities([common.Quality.HDTV, common.Quality.HDWEBDL >> 2, common.Quality.HDBLURAY >> 3], [])
        new_hd = common.Quality.combineQualities([common.Quality.HDTV, common.Quality.HDWEBDL, common.Quality.HDBLURAY], [])

        # update ANY -- shift existing qualities and add new 1080p qualities, note that rawHD was not added to the ANY template
        old_any = common.Quality.combineQualities([common.Quality.SDTV, common.Quality.SDDVD, common.Quality.HDTV, common.Quality.HDWEBDL >> 2, common.Quality.HDBLURAY >> 3, common.Quality.UNKNOWN], [])
        new_any = common.Quality.combineQualities([common.Quality.SDTV, common.Quality.SDDVD, common.Quality.HDTV, common.Quality.FULLHDTV, common.Quality.HDWEBDL, common.Quality.FULLHDWEBDL, common.Quality.HDBLURAY, common.Quality.FULLHDBLURAY, common.Quality.UNKNOWN], [])

        # update qualities (including templates)
        logger.log(u"[1/4] Updating pre-defined templates and the quality for each show...", logger.MESSAGE)
        ql = []
        shows = self.connection.select("SELECT * FROM tv_shows")
        for cur_show in shows:
            if cur_show["quality"] == old_hd:
                new_quality = new_hd
            elif cur_show["quality"] == old_any:
                new_quality = new_any
            else:
                new_quality = self._update_composite_qualities(cur_show["quality"])
            ql.append(["UPDATE tv_shows SET quality = ? WHERE show_id = ?", [new_quality, cur_show["show_id"]]])
        self.connection.mass_action(ql)

        # update status that are are within the old hdwebdl (1<<3 which is 8) and better -- exclude unknown (1<<15 which is 32768)
        logger.log(u"[2/4] Updating the status for the episodes within each show...", logger.MESSAGE)
        ql = []
        episodes = self.connection.select("SELECT * FROM tv_episodes WHERE status < 3276800 AND status >= 800")
        for cur_episode in episodes:
            ql.append(["UPDATE tv_episodes SET status = ? WHERE episode_id = ?", [self._update_status(cur_episode["status"]), cur_episode["episode_id"]]])
        self.connection.mass_action(ql)

        # make two seperate passes through the history since snatched and downloaded (action & quality) may not always coordinate together

        # update previous history so it shows the correct action
        logger.log(u"[3/4] Updating history to reflect the correct action...", logger.MESSAGE)
        ql = []
        historyAction = self.connection.select("SELECT * FROM history WHERE action < 3276800 AND action >= 800")
        for cur_entry in historyAction:
            ql.append(["UPDATE history SET action = ? WHERE showid = ? AND date = ?", [self._update_status(cur_entry["action"]), cur_entry["showid"], cur_entry["date"]]])
        self.connection.mass_action(ql)

        # update previous history so it shows the correct quality
        logger.log(u"[4/4] Updating history to reflect the correct quality...", logger.MESSAGE)
        ql = []
        historyQuality = self.connection.select("SELECT * FROM history WHERE quality < 32768 AND quality >= 8")
        for cur_entry in historyQuality:
            ql.append(["UPDATE history SET quality = ? WHERE showid = ? AND date = ?", [self._update_quality(cur_entry["quality"]), cur_entry["showid"], cur_entry["date"]]])
        self.connection.mass_action(ql)

        self.incDBVersion()

        # cleanup and reduce db if any previous data was removed
        logger.log(u"Performing a vacuum on the database.", logger.DEBUG)
        self.connection.action("VACUUM")


# included in build 502 (2013-11-24)
class AddShowidTvdbidIndex(Add1080pAndRawHDQualities):
    """ Adding index on tvdb_id (tv_shows) and showid (tv_episodes) to speed up searches/queries """

    def test(self):
        return self.checkDBVersion() >= 13

    def execute(self):
        backupDatabase(13)

        logger.log(u"Check for duplicate shows before adding unique index.")
        MainSanityCheck(self.connection).fix_duplicate_shows()

        logger.log(u"Adding index on tvdb_id (tv_shows) and showid (tv_episodes) to speed up searches/queries.")
        if not self.hasTable("idx_showid"):
            self.connection.action("CREATE INDEX idx_showid ON tv_episodes (showid);")
        if not self.hasTable("idx_tvdb_id"):
            self.connection.action("CREATE UNIQUE INDEX idx_tvdb_id ON tv_shows (tvdb_id);")

        self.incDBVersion()


# included in build 502 (2013-11-24)
class AddLastUpdateTVDB(AddShowidTvdbidIndex):
    """ Adding column last_update_tvdb to tv_shows for controlling nightly updates """

    def test(self):
        return self.checkDBVersion() >= 14

    def execute(self):
        backupDatabase(14)

        logger.log(u"Adding column last_update_tvdb to tvshows")
        if not self.hasColumn("tv_shows", "last_update_tvdb"):
            self.addColumn("tv_shows", "last_update_tvdb", default=1)

        self.incDBVersion()


# included in build 504 (2014-04-14)
class AddRequireAndIgnoreWords(AddLastUpdateTVDB):
    """ Adding column rls_require_words and rls_ignore_words to tv_shows """

    def test(self):
        return self.checkDBVersion() >= 15

    def execute(self):
        backupDatabase(15)

        logger.log(u"Adding column rls_require_words to tvshows")
        if not self.hasColumn("tv_shows", "rls_require_words"):
            self.addColumn("tv_shows", "rls_require_words", "TEXT", "")

        logger.log(u"Adding column rls_ignore_words to tvshows")
        if not self.hasColumn("tv_shows", "rls_ignore_words"):
            self.addColumn("tv_shows", "rls_ignore_words", "TEXT", "")

        self.incDBVersion()

########NEW FILE########
__FILENAME__ = db
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import os.path
import re
import sqlite3
import time
import threading

import sickbeard

from sickbeard import encodingKludge as ek
from sickbeard import logger
from sickbeard.exceptions import ex

db_lock = threading.Lock()


def dbFilename(filename="sickbeard.db", suffix=None):
    """
    @param filename: The sqlite database filename to use. If not specified,
                     will be made to be sickbeard.db
    @param suffix: The suffix to append to the filename. A '.' will be added
                   automatically, i.e. suffix='v0' will make dbfile.db.v0
    @return: the correct location of the database file.
    """
    if suffix:
        filename = "%s.%s" % (filename, suffix)
    return ek.ek(os.path.join, sickbeard.DATA_DIR, filename)


class DBConnection:
    def __init__(self, filename="sickbeard.db", suffix=None, row_type=None):

        self.filename = filename
        self.connection = sqlite3.connect(dbFilename(filename), 20)
        if row_type == "dict":
            self.connection.row_factory = self._dict_factory
        else:
            self.connection.row_factory = sqlite3.Row

    def checkDBVersion(self):
        try:
            result = self.select("SELECT db_version FROM db_version")
        except sqlite3.OperationalError, e:
            if "no such table: db_version" in e.args[0]:
                return 0

        if result:
            return int(result[0]["db_version"])
        else:
            return 0

    def mass_action(self, querylist, logTransaction=False):

        with db_lock:

            if querylist == None:
                return

            sqlResult = []
            attempt = 0

            while attempt < 5:
                try:
                    for qu in querylist:
                        if len(qu) == 1:
                            if logTransaction:
                                logger.log(qu[0], logger.DEBUG)
                            sqlResult.append(self.connection.execute(qu[0]))
                        elif len(qu) > 1:
                            if logTransaction:
                                logger.log(qu[0] + " with args " + str(qu[1]), logger.DEBUG)
                            sqlResult.append(self.connection.execute(qu[0], qu[1]))
                    self.connection.commit()
                    logger.log(u"Transaction with " + str(len(querylist)) + u" query's executed", logger.DEBUG)
                    return sqlResult
                except sqlite3.OperationalError, e:
                    sqlResult = []
                    if self.connection:
                        self.connection.rollback()
                    if "unable to open database file" in e.args[0] or "database is locked" in e.args[0]:
                        logger.log(u"DB error: " + ex(e), logger.WARNING)
                        attempt += 1
                        time.sleep(1)
                    else:
                        logger.log(u"DB error: " + ex(e), logger.ERROR)
                        raise
                except sqlite3.DatabaseError, e:
                    sqlResult = []
                    if self.connection:
                        self.connection.rollback()
                    logger.log(u"Fatal error executing query: " + ex(e), logger.ERROR)
                    raise

            return sqlResult

    def action(self, query, args=None):

        with db_lock:

            if query == None:
                return

            sqlResult = None
            attempt = 0

            while attempt < 5:
                try:
                    if args == None:
                        logger.log(self.filename + ": " + query, logger.DEBUG)
                        sqlResult = self.connection.execute(query)
                    else:
                        logger.log(self.filename + ": " + query + " with args " + str(args), logger.DEBUG)
                        sqlResult = self.connection.execute(query, args)
                    self.connection.commit()
                    # get out of the connection attempt loop since we were successful
                    break
                except sqlite3.OperationalError, e:
                    if "unable to open database file" in e.args[0] or "database is locked" in e.args[0]:
                        logger.log(u"DB error: " + ex(e), logger.WARNING)
                        attempt += 1
                        time.sleep(1)
                    else:
                        logger.log(u"DB error: " + ex(e), logger.ERROR)
                        raise
                except sqlite3.DatabaseError, e:
                    logger.log(u"Fatal error executing query: " + ex(e), logger.ERROR)
                    raise

            return sqlResult

    def select(self, query, args=None):

        sqlResults = self.action(query, args).fetchall()

        if sqlResults == None:
            return []

        return sqlResults

    def upsert(self, tableName, valueDict, keyDict):

        changesBefore = self.connection.total_changes

        genParams = lambda myDict: [x + " = ?" for x in myDict.keys()]

        query = "UPDATE " + tableName + " SET " + ", ".join(genParams(valueDict)) + " WHERE " + " AND ".join(genParams(keyDict))

        self.action(query, valueDict.values() + keyDict.values())

        if self.connection.total_changes == changesBefore:
            query = "INSERT INTO " + tableName + " (" + ", ".join(valueDict.keys() + keyDict.keys()) + ")" + \
                     " VALUES (" + ", ".join(["?"] * len(valueDict.keys() + keyDict.keys())) + ")"
            self.action(query, valueDict.values() + keyDict.values())

    def tableInfo(self, tableName):
        # FIXME ? binding is not supported here, but I cannot find a way to escape a string manually
        cursor = self.connection.execute("PRAGMA table_info(%s)" % tableName)
        columns = {}
        for column in cursor:
            columns[column['name']] = { 'type': column['type'] }
        return columns

    # http://stackoverflow.com/questions/3300464/how-can-i-get-dict-from-sqlite-query
    def _dict_factory(self, cursor, row):
        d = {}
        for idx, col in enumerate(cursor.description):
            d[col[0]] = row[idx]
        return d


def sanityCheckDatabase(connection, sanity_check):
    sanity_check(connection).check()


class DBSanityCheck(object):
    def __init__(self, connection):
        self.connection = connection

    def check(self):
        pass

# ===============
# = Upgrade API =
# ===============


def upgradeDatabase(connection, schema):
    logger.log(u"Checking database structure...", logger.MESSAGE)
    _processUpgrade(connection, schema)


def prettyName(class_name):
    return ' '.join([x.group() for x in re.finditer("([A-Z])([a-z0-9]+)", class_name)])


def _processUpgrade(connection, upgradeClass):
    instance = upgradeClass(connection)
    pretty_class_name = prettyName(upgradeClass.__name__)
    logger.log(u"Checking " + pretty_class_name + " database upgrade", logger.DEBUG)
    if not instance.test():
        logger.log(u"Database upgrade required: " + pretty_class_name, logger.MESSAGE)
        try:
            instance.execute()
        except sqlite3.DatabaseError, e:
            print "Error in " + str(pretty_class_name) + ": " + ex(e)
            raise
        logger.log(pretty_class_name + u" upgrade completed", logger.DEBUG)
    else:
        logger.log(pretty_class_name + u" upgrade not required", logger.DEBUG)

    for upgradeSubClass in upgradeClass.__subclasses__():
        _processUpgrade(connection, upgradeSubClass)


# Base migration class. All future DB changes should be subclassed from this class
class SchemaUpgrade (object):
    def __init__(self, connection):
        self.connection = connection

    def hasTable(self, tableName):
        return len(self.connection.action("SELECT 1 FROM sqlite_master WHERE name = ?;", (tableName, )).fetchall()) > 0

    def hasColumn(self, tableName, column):
        return column in self.connection.tableInfo(tableName)

    def addColumn(self, tableName, column, data_type="NUMERIC", default=0):
        self.connection.action("ALTER TABLE %s ADD %s %s" % (tableName, column, data_type))
        self.connection.action("UPDATE %s SET %s = ?" % (tableName, column), (default,))

    def checkDBVersion(self):
        return self.connection.checkDBVersion()

    def incDBVersion(self):
        new_version = self.checkDBVersion() + 1
        self.connection.action("UPDATE db_version SET db_version = ?", [new_version])
        return new_version

########NEW FILE########
__FILENAME__ = encodingKludge
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import os

from sickbeard import logger
import sickbeard

# This module tries to deal with the apparently random behavior of python when dealing with unicode <-> utf-8
# encodings. It tries to just use unicode, but if that fails then it tries forcing it to utf-8. Any functions
# which return something should always return unicode.

def fixStupidEncodings(x, silent=False):
    if type(x) == str:
        try:
            return x.decode(sickbeard.SYS_ENCODING)
        except UnicodeDecodeError:
            logger.log(u"Unable to decode value: "+repr(x), logger.ERROR)
            return None
    elif type(x) == unicode:
        return x
    else:
        logger.log(u"Unknown value passed in, ignoring it: "+str(type(x))+" ("+repr(x)+":"+repr(type(x))+")", logger.DEBUG if silent else logger.ERROR)
        return None

    return None

def fixListEncodings(x):
    if type(x) != list and type(x) != tuple:
        return x
    else:
        return filter(lambda x: x != None, map(fixStupidEncodings, x))

def callPeopleStupid(x):
    try:
        return x.encode(sickbeard.SYS_ENCODING)
    except UnicodeEncodeError:
        logger.log(u"YOUR COMPUTER SUCKS! Your data is being corrupted by a bad locale/encoding setting. Report this error on the forums or IRC please: "+repr(x)+", "+sickbeard.SYS_ENCODING, logger.ERROR)
        return x.encode(sickbeard.SYS_ENCODING, 'ignore')

def ek(func, *args):
    result = None

    if os.name == 'nt':
        result = func(*args)
    else:
        result = func(*[callPeopleStupid(x) if type(x) in (str, unicode) else x for x in args])

    if type(result) in (list, tuple):
        return fixListEncodings(result)
    elif type(result) == str:
        return fixStupidEncodings(result)
    else:
        return result

########NEW FILE########
__FILENAME__ = exceptions
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from sickbeard.encodingKludge import fixStupidEncodings


def ex(e):
    """
    Returns a unicode string from the exception text if it exists.
    """

    e_message = u""

    if not e or not e.args:
        return e_message

    for arg in e.args:

        if arg is not None:
            if isinstance(arg, (str, unicode)):
                fixed_arg = fixStupidEncodings(arg, True)

            else:
                try:
                    fixed_arg = u"error " + fixStupidEncodings(str(arg), True)

                except:
                    fixed_arg = None

            if fixed_arg:
                if not e_message:
                    e_message = fixed_arg

                else:
                    e_message = e_message + " : " + fixed_arg

    return e_message


class SickBeardException(Exception):
    "Generic SickBeard Exception - should never be thrown, only subclassed"


class ConfigErrorException(SickBeardException):
    "Error in the config file"


class LaterException(SickBeardException):
    "Something bad happened that I'll make a real exception for later"


class NoNFOException(SickBeardException):
    "No NFO was found!"


class NoShowDirException(SickBeardException):
    "Unable to find the show's directory"


class FileNotFoundException(SickBeardException):
    "The specified file doesn't exist"


class MultipleDBEpisodesException(SickBeardException):
    "Found multiple episodes in the DB! Must fix DB first"


class MultipleDBShowsException(SickBeardException):
    "Found multiple shows in the DB! Must fix DB first"


class MultipleShowObjectsException(SickBeardException):
    "Found multiple objects for the same show! Something is very wrong"


class WrongShowException(SickBeardException):
    "The episode doesn't belong to the same show as its parent folder"


class ShowNotFoundException(SickBeardException):
    "The show wasn't found on theTVDB"


class EpisodeNotFoundException(SickBeardException):
    "The episode wasn't found on theTVDB"


class NewzbinAPIThrottled(SickBeardException):
    "Newzbin has throttled us, deal with it"


class TVRageException(SickBeardException):
    "TVRage API did something bad"


class ShowDirNotFoundException(SickBeardException):
    "The show dir doesn't exist"


class AuthException(SickBeardException):
    "Your authentication information is incorrect"


class EpisodeDeletedException(SickBeardException):
    "This episode has been deleted"


class CantRefreshException(SickBeardException):
    "The show can't be refreshed right now"


class CantUpdateException(SickBeardException):
    "The show can't be updated right now"


class PostProcessingFailed(SickBeardException):
    "Post-processing the episode failed"

########NEW FILE########
__FILENAME__ = generic_queue
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import threading

from sickbeard import logger

class QueuePriorities:
    LOW = 10
    NORMAL = 20
    HIGH = 30

class GenericQueue(object):

    def __init__(self):

        self.currentItem = None
        self.queue = []

        self.thread = None

        self.queue_name = "QUEUE"

        self.min_priority = 0
        
        self.currentItem = None

    def pause(self):
        logger.log(u"Pausing queue")
        self.min_priority = 999999999999
    
    def unpause(self):
        logger.log(u"Unpausing queue")
        self.min_priority = 0

    def add_item(self, item):
        item.added = datetime.datetime.now()
        self.queue.append(item)
        
        return item

    def run(self):

        # only start a new task if one isn't already going
        if self.thread == None or self.thread.isAlive() == False:

            # if the thread is dead then the current item should be finished
            if self.currentItem != None:
                self.currentItem.finish()
                self.currentItem = None

            # if there's something in the queue then run it in a thread and take it out of the queue
            if len(self.queue) > 0:

                # sort by priority
                def sorter(x,y):
                    """
                    Sorts by priority descending then time ascending
                    """
                    if x.priority == y.priority:
                        if y.added == x.added:
                            return 0
                        elif y.added < x.added:
                            return 1
                        elif y.added > x.added:
                            return -1
                    else:
                        return y.priority-x.priority

                self.queue.sort(cmp=sorter)
                
                queueItem = self.queue[0]

                if queueItem.priority < self.min_priority:
                    return

                # launch the queue item in a thread
                # TODO: improve thread name
                threadName = self.queue_name + '-' + queueItem.get_thread_name()
                self.thread = threading.Thread(None, queueItem.execute, threadName)
                self.thread.start()

                self.currentItem = queueItem

                # take it out of the queue
                del self.queue[0]

class QueueItem:
    def __init__(self, name, action_id = 0):
        self.name = name

        self.inProgress = False

        self.priority = QueuePriorities.NORMAL

        self.thread_name = None

        self.action_id = action_id
        
        self.added = None

    def get_thread_name(self):
        if self.thread_name:
            return self.thread_name
        else:
            return self.name.replace(" ","-").upper()

    def execute(self):
        """Implementing classes should call this"""

        self.inProgress = True

    def finish(self):
        """Implementing Classes should call this"""

        self.inProgress = False



########NEW FILE########
__FILENAME__ = gh_api
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

try:
    import json
except ImportError:
    from lib import simplejson as json

import helpers


class GitHub(object):
    """
    Simple api wrapper for the Github API v3. Currently only supports the small thing that SB
    needs it for - list of commits.
    """

    def __init__(self, github_repo_user, github_repo, branch='master'):

        self.github_repo_user = github_repo_user
        self.github_repo = github_repo
        self.branch = branch

    def _access_API(self, path, params=None):
        """
        Access the API at the path given and with the optional params given.

        path: A list of the path elements to use (eg. ['repos', 'midgetspy', 'Sick-Beard', 'commits'])
        params: Optional dict of name/value pairs for extra params to send. (eg. {'per_page': 10})

        Returns a deserialized json object of the result. Doesn't do any error checking (hope it works).
        """

        url = 'https://api.github.com/' + '/'.join(path)

        if params and type(params) is dict:
            url += '?' + '&'.join([str(x) + '=' + str(params[x]) for x in params.keys()])

        data = helpers.getURL(url)

        if data:
            json_data = json.loads(data)
            return json_data
        else:
            return []

    def commits(self):
        """
        Uses the API to get a list of the 100 most recent commits from the specified user/repo/branch, starting from HEAD.

        user: The github username of the person whose repo you're querying
        repo: The repo name to query
        branch: Optional, the branch name to show commits from

        Returns a deserialized json object containing the commit info. See http://developer.github.com/v3/repos/commits/
        """
        access_API = self._access_API(['repos', self.github_repo_user, self.github_repo, 'commits'], params={'per_page': 100, 'sha': self.branch})
        return access_API

    def compare(self, base, head, per_page=1):
        """
        Uses the API to get a list of compares between base and head.

        user: The github username of the person whose repo you're querying
        repo: The repo name to query
        base: Start compare from branch
        head: Current commit sha or branch name to compare
        per_page: number of items per page

        Returns a deserialized json object containing the compare info. See http://developer.github.com/v3/repos/commits/
        """
        access_API = self._access_API(['repos', self.github_repo_user, self.github_repo, 'compare', base + '...' + head], params={'per_page': per_page})
        return access_API

########NEW FILE########
__FILENAME__ = helpers
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import gzip
import os
import re
import shutil
import socket
import stat
import StringIO
import time
import traceback
import urllib
import urllib2
import zlib

from httplib import BadStatusLine

try:
    import json
except ImportError:
    from lib import simplejson as json

from xml.dom.minidom import Node

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree

import sickbeard

from sickbeard.exceptions import MultipleShowObjectsException, ex
from sickbeard import logger, classes
from sickbeard.common import USER_AGENT, mediaExtensions, XML_NSMAP
from sickbeard.common import mediaExtensions

from sickbeard import db
from sickbeard import encodingKludge as ek
from sickbeard import notifiers

from lib.tvdb_api import tvdb_api, tvdb_exceptions

urllib._urlopener = classes.SickBeardURLopener()


def indentXML(elem, level=0):
    '''
    Does our pretty printing, makes Matt very happy
    '''
    i = "\n" + level * "  "
    if len(elem):
        if not elem.text or not elem.text.strip():
            elem.text = i + "  "
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
        for elem in elem:
            indentXML(elem, level + 1)
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
    else:
        # Strip out the newlines from text
        if elem.text:
            elem.text = elem.text.replace('\n', ' ')
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i


def remove_extension(name):
    """
    Remove download or media extension from name (if any)
    """

    if name and "." in name:
        base_name, sep, extension = name.rpartition('.')  # @UnusedVariable
        if base_name and extension.lower() in ['nzb', 'torrent'] + mediaExtensions:
            name = base_name

    return name


def remove_non_release_groups(name):
    """
    Remove non release groups from name
    """

    if name and "-" in name:
        name_group = name.rsplit('-', 1)
        if name_group[-1].upper() in ["RP", "NZBGEEK"]:
            name = name_group[0]

    return name


def replaceExtension(filename, newExt):
    '''
    >>> replaceExtension('foo.avi', 'mkv')
    'foo.mkv'
    >>> replaceExtension('.vimrc', 'arglebargle')
    '.vimrc'
    >>> replaceExtension('a.b.c', 'd')
    'a.b.d'
    >>> replaceExtension('', 'a')
    ''
    >>> replaceExtension('foo.bar', '')
    'foo.'
    '''
    sepFile = filename.rpartition(".")
    if sepFile[0] == "":
        return filename
    else:
        return sepFile[0] + "." + newExt


def isMediaFile(filename):
    # ignore samples
    if re.search('(^|[\W_])sample\d*[\W_]', filename.lower()):
        return False

    # ignore MAC OS's retarded "resource fork" files
    if filename.startswith('._'):
        return False

    sepFile = filename.rpartition(".")
    if sepFile[2].lower() in mediaExtensions:
        return True
    else:
        return False


def sanitizeFileName(name):
    '''
    >>> sanitizeFileName('a/b/c')
    'a-b-c'
    >>> sanitizeFileName('abc')
    'abc'
    >>> sanitizeFileName('a"b')
    'ab'
    >>> sanitizeFileName('.a.b..')
    'a.b'
    '''

    # remove bad chars from the filename
    name = re.sub(r'[\\/\*]', '-', name)
    name = re.sub(r'[:"<>|?]', '', name)

    # remove leading/trailing periods and spaces
    name = name.strip(' .')

    return name


def getURL(url, post_data=None, headers=[]):
    """
    Returns a byte-string retrieved from the url provider.
    """

    opener = urllib2.build_opener()
    opener.addheaders = [('User-Agent', USER_AGENT), ('Accept-Encoding', 'gzip,deflate')]
    for cur_header in headers:
        opener.addheaders.append(cur_header)

    try:
        usock = opener.open(url, post_data)
        url = usock.geturl()
        encoding = usock.info().get("Content-Encoding")

        if encoding in ('gzip', 'x-gzip', 'deflate'):
            content = usock.read()
            if encoding == 'deflate':
                data = StringIO.StringIO(zlib.decompress(content))
            else:
                data = gzip.GzipFile('', 'rb', 9, StringIO.StringIO(content))
            result = data.read()

        else:
            result = usock.read()

        usock.close()

    except urllib2.HTTPError, e:
        logger.log(u"HTTP error " + str(e.code) + " while loading URL " + url, logger.WARNING)
        return None

    except urllib2.URLError, e:
        logger.log(u"URL error " + str(e.reason) + " while loading URL " + url, logger.WARNING)
        return None

    except BadStatusLine:
        logger.log(u"BadStatusLine error while loading URL " + url, logger.WARNING)
        return None

    except socket.timeout:
        logger.log(u"Timed out while loading URL " + url, logger.WARNING)
        return None

    except ValueError:
        logger.log(u"Unknown error while loading URL " + url, logger.WARNING)
        return None

    except Exception:
        logger.log(u"Unknown exception while loading URL " + url + ": " + traceback.format_exc(), logger.WARNING)
        return None

    return result


def is_hidden_folder(folder):
    """
    Returns True if folder is hidden.
    On Linux based systems hidden folders start with . (dot)
    folder: Full path of folder to check
    """
    if ek.ek(os.path.isdir, folder):
        if ek.ek(os.path.basename, folder).startswith('.'):
            return True

    return False


def findCertainShow(showList, tvdbid):
    results = filter(lambda x: x.tvdbid == tvdbid, showList)
    if len(results) == 0:
        return None
    elif len(results) > 1:
        raise MultipleShowObjectsException()
    else:
        return results[0]


def findCertainTVRageShow(showList, tvrid):

    if tvrid == 0:
        return None

    results = filter(lambda x: x.tvrid == tvrid, showList)

    if len(results) == 0:
        return None
    elif len(results) > 1:
        raise MultipleShowObjectsException()
    else:
        return results[0]


def makeDir(path):
    if not ek.ek(os.path.isdir, path):
        try:
            ek.ek(os.makedirs, path)
            # do the library update for synoindex
            notifiers.synoindex_notifier.addFolder(path)
        except OSError:
            return False
    return True


def searchDBForShow(regShowName):

    showNames = [re.sub('[. -]', ' ', regShowName)]

    myDB = db.DBConnection()

    yearRegex = "([^()]+?)\s*(\()?(\d{4})(?(2)\))$"

    for showName in showNames:

        sqlResults = myDB.select("SELECT * FROM tv_shows WHERE show_name LIKE ? OR tvr_name LIKE ?", [showName, showName])

        if len(sqlResults) == 1:
            return (int(sqlResults[0]["tvdb_id"]), sqlResults[0]["show_name"])

        else:

            # if we didn't get exactly one result then try again with the year stripped off if possible
            match = re.match(yearRegex, showName)
            if match and match.group(1):
                logger.log(u"Unable to match original name but trying to manually strip and specify show year", logger.DEBUG)
                sqlResults = myDB.select("SELECT * FROM tv_shows WHERE (show_name LIKE ? OR tvr_name LIKE ?) AND startyear = ?", [match.group(1) + '%', match.group(1) + '%', match.group(3)])

            if len(sqlResults) == 0:
                logger.log(u"Unable to match a record in the DB for " + showName, logger.DEBUG)
                continue
            elif len(sqlResults) > 1:
                logger.log(u"Multiple results for " + showName + " in the DB, unable to match show name", logger.DEBUG)
                continue
            else:
                return (int(sqlResults[0]["tvdb_id"]), sqlResults[0]["show_name"])

    return None


def sizeof_fmt(num):
    '''
    >>> sizeof_fmt(2)
    '2.0 bytes'
    >>> sizeof_fmt(1024)
    '1.0 KB'
    >>> sizeof_fmt(2048)
    '2.0 KB'
    >>> sizeof_fmt(2**20)
    '1.0 MB'
    >>> sizeof_fmt(1234567)
    '1.2 MB'
    '''
    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:
        if num < 1024.0:
            return "%3.1f %s" % (num, x)
        num /= 1024.0


def listMediaFiles(path):

    if not path or not ek.ek(os.path.isdir, path):
        return []

    files = []
    for curFile in ek.ek(os.listdir, path):
        fullCurFile = ek.ek(os.path.join, path, curFile)

        # if it's a folder do it recursively
        if ek.ek(os.path.isdir, fullCurFile) and not curFile.startswith('.') and not curFile == 'Extras':
            files += listMediaFiles(fullCurFile)

        elif isMediaFile(curFile):
            files.append(fullCurFile)

    return files


def copyFile(srcFile, destFile):
    ek.ek(shutil.copyfile, srcFile, destFile)
    try:
        ek.ek(shutil.copymode, srcFile, destFile)
    except OSError:
        pass


def moveFile(srcFile, destFile):
    try:
        ek.ek(os.rename, srcFile, destFile)
        fixSetGroupID(destFile)
    except OSError:
        copyFile(srcFile, destFile)
        ek.ek(os.unlink, srcFile)


def make_dirs(path):
    """
    Creates any folders that are missing and assigns them the permissions of their
    parents
    """

    logger.log(u"Checking if the path " + path + " already exists", logger.DEBUG)

    if not ek.ek(os.path.isdir, path):
        # Windows, create all missing folders
        if os.name == 'nt' or os.name == 'ce':
            try:
                logger.log(u"Folder " + path + " didn't exist, creating it", logger.DEBUG)
                ek.ek(os.makedirs, path)
            except (OSError, IOError), e:
                logger.log(u"Failed creating " + path + " : " + ex(e), logger.ERROR)
                return False

        # not Windows, create all missing folders and set permissions
        else:
            sofar = ''
            folder_list = path.split(os.path.sep)

            # look through each subfolder and make sure they all exist
            for cur_folder in folder_list:
                sofar += cur_folder + os.path.sep

                # if it exists then just keep walking down the line
                if ek.ek(os.path.isdir, sofar):
                    continue

                try:
                    logger.log(u"Folder " + sofar + " didn't exist, creating it", logger.DEBUG)
                    ek.ek(os.mkdir, sofar)
                    # use normpath to remove end separator, otherwise checks permissions against itself
                    chmodAsParent(ek.ek(os.path.normpath, sofar))
                    # do the library update for synoindex
                    notifiers.synoindex_notifier.addFolder(sofar)
                except (OSError, IOError), e:
                    logger.log(u"Failed creating " + sofar + " : " + ex(e), logger.ERROR)
                    return False

    return True


def rename_ep_file(cur_path, new_path, old_path_length=0):
    """
    Creates all folders needed to move a file to its new location, renames it, then cleans up any folders
    left that are now empty.

    cur_path: The absolute path to the file you want to move/rename
    new_path: The absolute path to the destination for the file WITHOUT THE EXTENSION
    old_path_length: The length of media file path (old name) WITHOUT THE EXTENSION
    """

    new_dest_dir, new_dest_name = os.path.split(new_path)  # @UnusedVariable
    if old_path_length == 0 or old_path_length > len(cur_path):
        # approach from the right
        cur_file_name, cur_file_ext = os.path.splitext(cur_path)  # @UnusedVariable
    else:
        # approach from the left
        cur_file_ext = cur_path[old_path_length:]

    # put the extension on the incoming file
    new_path += cur_file_ext

    make_dirs(os.path.dirname(new_path))

    # move the file
    try:
        logger.log(u"Renaming file from " + cur_path + " to " + new_path)
        ek.ek(os.rename, cur_path, new_path)
    except (OSError, IOError), e:
        logger.log(u"Failed renaming " + cur_path + " to " + new_path + ": " + ex(e), logger.ERROR)
        return False

    # clean up any old folders that are empty
    delete_empty_folders(ek.ek(os.path.dirname, cur_path))

    return True


def delete_empty_folders(check_empty_dir, keep_dir=None):
    """
    Walks backwards up the path and deletes any empty folders found.

    check_empty_dir: The path to clean (absolute path to a folder)
    keep_dir: Clean until this path is reached
    """

    # treat check_empty_dir as empty when it only contains these items
    ignore_items = []

    logger.log(u"Trying to clean any empty folders under " + check_empty_dir)

    # as long as the folder exists and doesn't contain any files, delete it
    while ek.ek(os.path.isdir, check_empty_dir) and check_empty_dir != keep_dir:

        check_files = ek.ek(os.listdir, check_empty_dir)

        if not check_files or (len(check_files) <= len(ignore_items) and all([check_file in ignore_items for check_file in check_files])):
            # directory is empty or contains only ignore_items
            try:
                logger.log(u"Deleting empty folder: " + check_empty_dir)
                # need shutil.rmtree when ignore_items is really implemented
                ek.ek(os.rmdir, check_empty_dir)
                # do the library update for synoindex
                notifiers.synoindex_notifier.deleteFolder(check_empty_dir)
            except OSError, e:
                logger.log(u"Unable to delete " + check_empty_dir + ": " + repr(e) + " / " + str(e), logger.WARNING)
                break
            check_empty_dir = ek.ek(os.path.dirname, check_empty_dir)
        else:
            break


def chmodAsParent(childPath):
    if os.name == 'nt' or os.name == 'ce':
        return

    parentPath = ek.ek(os.path.dirname, childPath)

    if not parentPath:
        logger.log(u"No parent path provided in " + childPath + ", unable to get permissions from it", logger.DEBUG)
        return

    parentPathStat = ek.ek(os.stat, parentPath)
    parentMode = stat.S_IMODE(parentPathStat[stat.ST_MODE])

    childPathStat = ek.ek(os.stat, childPath)
    childPath_mode = stat.S_IMODE(childPathStat[stat.ST_MODE])

    if ek.ek(os.path.isfile, childPath):
        childMode = fileBitFilter(parentMode)
    else:
        childMode = parentMode

    if childPath_mode == childMode:
        return

    childPath_owner = childPathStat.st_uid
    user_id = os.geteuid()  # @UndefinedVariable - only available on UNIX

    if user_id != 0 and user_id != childPath_owner:
        logger.log(u"Not running as root or owner of " + childPath + ", not trying to set permissions", logger.DEBUG)
        return

    try:
        ek.ek(os.chmod, childPath, childMode)
        logger.log(u"Setting permissions for %s to %o as parent directory has %o" % (childPath, childMode, parentMode), logger.DEBUG)
    except OSError:
        logger.log(u"Failed to set permission for %s to %o" % (childPath, childMode), logger.ERROR)


def fileBitFilter(mode):
    for bit in [stat.S_IXUSR, stat.S_IXGRP, stat.S_IXOTH, stat.S_ISUID, stat.S_ISGID]:
        if mode & bit:
            mode -= bit

    return mode


def fixSetGroupID(childPath):
    if os.name == 'nt' or os.name == 'ce':
        return

    parentPath = ek.ek(os.path.dirname, childPath)
    parentStat = os.stat(parentPath)
    parentMode = stat.S_IMODE(parentStat[stat.ST_MODE])

    if parentMode & stat.S_ISGID:
        parentGID = parentStat[stat.ST_GID]
        childStat = ek.ek(os.stat, childPath)
        childGID = childStat[stat.ST_GID]

        if childGID == parentGID:
            return

        childPath_owner = childStat.st_uid
        user_id = os.geteuid()  # @UndefinedVariable - only available on UNIX

        if user_id != 0 and user_id != childPath_owner:
            logger.log(u"Not running as root or owner of " + childPath + ", not trying to set the set-group-ID", logger.DEBUG)
            return

        try:
            ek.ek(os.chown, childPath, -1, parentGID)  # @UndefinedVariable - only available on UNIX
            logger.log(u"Respecting the set-group-ID bit on the parent directory for %s" % (childPath), logger.DEBUG)
        except OSError:
            logger.log(u"Failed to respect the set-group-ID bit on the parent directory for %s (setting group ID %i)" % (childPath, parentGID), logger.ERROR)


def real_path(path):
    """
    Returns: the canonicalized absolute pathname. The resulting path will have no symbolic link, '/./' or '/../' components.
    """
    return ek.ek(os.path.normpath, ek.ek(os.path.normcase, ek.ek(os.path.realpath, path)))


def sanitizeSceneName(name, ezrss=False):
    """
    Takes a show name and returns the "scenified" version of it.

    ezrss: If true the scenified version will follow EZRSS's cracksmoker rules as best as possible

    Returns: A string containing the scene version of the show name given.
    """

    if not ezrss:
        bad_chars = u",:()'!?\u2019"
    # ezrss leaves : and ! in their show names as far as I can tell
    else:
        bad_chars = u",()'?\u2019"

    # strip out any bad chars
    for x in bad_chars:
        name = name.replace(x, "")

    # tidy up stuff that doesn't belong in scene names
    name = name.replace("- ", ".").replace(" ", ".").replace("&", "and").replace('/', '.')
    name = re.sub("\.\.*", ".", name)

    if name.endswith('.'):
        name = name[:-1]

    return name


def create_https_certificates(ssl_cert, ssl_key):
    """
    Create self-signed HTTPS certificares and store in paths 'ssl_cert' and 'ssl_key'
    """
    try:
        from OpenSSL import crypto  # @UnresolvedImport
        from lib.certgen import createKeyPair, createCertRequest, createCertificate, TYPE_RSA, serial  # @UnresolvedImport
    except:
        logger.log(u"pyopenssl module missing, please install for https access", logger.WARNING)
        return False

    # Create the CA Certificate
    cakey = createKeyPair(TYPE_RSA, 1024)
    careq = createCertRequest(cakey, CN='Certificate Authority')
    cacert = createCertificate(careq, (careq, cakey), serial, (0, 60 * 60 * 24 * 365 * 10))  # ten years

    cname = 'SickBeard'
    pkey = createKeyPair(TYPE_RSA, 1024)
    req = createCertRequest(pkey, CN=cname)
    cert = createCertificate(req, (cacert, cakey), serial, (0, 60 * 60 * 24 * 365 * 10))  # ten years

    # Save the key and certificate to disk
    try:
        open(ssl_key, 'w').write(crypto.dump_privatekey(crypto.FILETYPE_PEM, pkey))
        open(ssl_cert, 'w').write(crypto.dump_certificate(crypto.FILETYPE_PEM, cert))
    except:
        logger.log(u"Error creating SSL key and certificate", logger.ERROR)
        return False

    return True

if __name__ == '__main__':
    import doctest
    doctest.testmod()


def parse_json(data):
    """
    Parse json data into a python object

    data: data string containing json

    Returns: parsed data as json or None
    """

    try:
        parsedJSON = json.loads(data)
    except ValueError, e:
        logger.log(u"Error trying to decode json data. Error: " + ex(e), logger.DEBUG)
        return None

    return parsedJSON


def parse_xml(data, del_xmlns=False):
    """
    Parse data into an xml elementtree.ElementTree

    data: data string containing xml
    del_xmlns: if True, removes xmlns namesspace from data before parsing

    Returns: parsed data as elementtree or None
    """

    if del_xmlns:
        data = re.sub(' xmlns="[^"]+"', '', data)

    try:
        parsedXML = etree.fromstring(data)
    except Exception, e:
        logger.log(u"Error trying to parse xml data. Error: " + ex(e), logger.DEBUG)
        parsedXML = None

    return parsedXML


def get_xml_text(element, mini_dom=False):
    """
    Get all text inside a xml element

    element: A xml element either created with elementtree.ElementTree or xml.dom.minidom
    mini_dom: Default False use elementtree, True use minidom

    Returns: text
    """

    text = ""

    if mini_dom:
        node = element
        for child in node.childNodes:
            if child.nodeType in (Node.CDATA_SECTION_NODE, Node.TEXT_NODE):
                text += child.data
    else:
        if element is not None:
            for child in [element] + element.findall('.//*'):
                if child.text:
                    text += child.text

    return text.strip()


def backupVersionedFile(old_file, version):

    numTries = 0

    new_file = old_file + '.' + 'v' + str(version)

    while not ek.ek(os.path.isfile, new_file):
        if not ek.ek(os.path.isfile, old_file):
            logger.log(u"Not creating backup, " + old_file + " doesn't exist", logger.DEBUG)
            break

        try:
            logger.log(u"Trying to back up " + old_file + " to " + new_file, logger.DEBUG)
            shutil.copy(old_file, new_file)
            logger.log(u"Backup done", logger.DEBUG)
            break
        except Exception, e:
            logger.log(u"Error while trying to back up " + old_file + " to " + new_file + " : " + ex(e), logger.WARNING)
            numTries += 1
            time.sleep(1)
            logger.log(u"Trying again.", logger.DEBUG)

        if numTries >= 10:
            logger.log(u"Unable to back up " + old_file + " to " + new_file + " please do it manually.", logger.ERROR)
            return False

    return True

########NEW FILE########
__FILENAME__ = history
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import db
import datetime

from sickbeard.common import SNATCHED, Quality

dateFormat = "%Y%m%d%H%M%S"

def _logHistoryItem(action, showid, season, episode, quality, resource, provider):

    logDate = datetime.datetime.today().strftime(dateFormat)

    myDB = db.DBConnection()
    myDB.action("INSERT INTO history (action, date, showid, season, episode, quality, resource, provider) VALUES (?,?,?,?,?,?,?,?)",
                [action, logDate, showid, season, episode, quality, resource, provider])


def logSnatch(searchResult):

    for curEpObj in searchResult.episodes:

        showid = int(curEpObj.show.tvdbid)
        season = int(curEpObj.season)
        episode = int(curEpObj.episode)
        quality = searchResult.quality

        providerClass = searchResult.provider
        if providerClass != None:
            provider = providerClass.name
        else:
            provider = "unknown"

        action = Quality.compositeStatus(SNATCHED, searchResult.quality)

        resource = searchResult.name

        _logHistoryItem(action, showid, season, episode, quality, resource, provider)

def logDownload(episode, filename, new_ep_quality, release_group=None):

    showid = int(episode.show.tvdbid)
    season = int(episode.season)
    epNum = int(episode.episode)

    quality = new_ep_quality
    
    # store the release group as the provider if possible
    if release_group:
        provider = release_group
    else:
        provider = -1

    action = episode.status

    _logHistoryItem(action, showid, season, epNum, quality, filename, provider)


########NEW FILE########
__FILENAME__ = image_cache
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import os.path

import sickbeard

from sickbeard import helpers, logger, exceptions
from sickbeard import encodingKludge as ek

from sickbeard.metadata.generic import GenericMetadata

from lib.hachoir_parser import createParser
from lib.hachoir_metadata import extractMetadata

class ImageCache:
    
    def __init__(self):
        pass
    
    def _cache_dir(self):
        """
        Builds up the full path to the image cache directory
        """
        return ek.ek(os.path.abspath, ek.ek(os.path.join, sickbeard.CACHE_DIR, 'images'))

    def poster_path(self, tvdb_id):
        """
        Builds up the path to a poster cache for a given tvdb id

        returns: a full path to the cached poster file for the given tvdb id 
        
        tvdb_id: ID of the show to use in the file name
        """
        poster_file_name = str(tvdb_id) + '.poster.jpg'
        return ek.ek(os.path.join, self._cache_dir(), poster_file_name)
    
    def banner_path(self, tvdb_id):
        """
        Builds up the path to a banner cache for a given tvdb id

        returns: a full path to the cached banner file for the given tvdb id 
        
        tvdb_id: ID of the show to use in the file name
        """
        banner_file_name = str(tvdb_id) + '.banner.jpg'
        return ek.ek(os.path.join, self._cache_dir(), banner_file_name)

    def has_poster(self, tvdb_id):
        """
        Returns true if a cached poster exists for the given tvdb id
        """
        poster_path = self.poster_path(tvdb_id)
        logger.log(u"Checking if file "+str(poster_path)+" exists", logger.DEBUG)
        return ek.ek(os.path.isfile, poster_path)

    def has_banner(self, tvdb_id):
        """
        Returns true if a cached banner exists for the given tvdb id
        """
        banner_path = self.banner_path(tvdb_id)
        logger.log(u"Checking if file "+str(banner_path)+" exists", logger.DEBUG)
        return ek.ek(os.path.isfile, banner_path)

    BANNER = 1
    POSTER = 2
    
    def which_type(self, path):
        """
        Analyzes the image provided and attempts to determine whether it is a poster or banner.
        
        returns: BANNER, POSTER if it concluded one or the other, or None if the image was neither (or didn't exist)
        
        path: full path to the image
        """

        if not ek.ek(os.path.isfile, path):
            logger.log(u"Couldn't check the type of "+str(path)+" cause it doesn't exist", logger.WARNING)
            return None

        # use hachoir to parse the image for us
        img_parser = createParser(path)
        img_metadata = extractMetadata(img_parser)

        if not img_metadata:
            logger.log(u"Unable to get metadata from "+str(path)+", not using your existing image", logger.DEBUG)
            return None
        
        img_ratio = float(img_metadata.get('width'))/float(img_metadata.get('height'))

        img_parser.stream._input.close()

        # most posters are around 0.68 width/height ratio (eg. 680/1000)
        if 0.55 < img_ratio < 0.8:
            return self.POSTER
        
        # most banners are around 5.4 width/height ratio (eg. 758/140)
        elif 5 < img_ratio < 6:
            return self.BANNER
        else:
            logger.log(u"Image has size ratio of "+str(img_ratio)+", unknown type", logger.WARNING)
            return None
    
    def _cache_image_from_file(self, image_path, img_type, tvdb_id):
        """
        Takes the image provided and copies it to the cache folder
        
        returns: bool representing success
        
        image_path: path to the image we're caching
        img_type: BANNER or POSTER
        tvdb_id: id of the show this image belongs to
        """

        # generate the path based on the type & tvdb_id
        if img_type == self.POSTER:
            dest_path = self.poster_path(tvdb_id)
        elif img_type == self.BANNER:
            dest_path = self.banner_path(tvdb_id)
        else:
            logger.log(u"Invalid cache image type: "+str(img_type), logger.ERROR)
            return False

        # make sure the cache folder exists before we try copying to it
        if not ek.ek(os.path.isdir, self._cache_dir()):
            logger.log(u"Image cache dir didn't exist, creating it at "+str(self._cache_dir()))
            ek.ek(os.makedirs, self._cache_dir())

        logger.log(u"Copying from "+image_path+" to "+dest_path)
        helpers.copyFile(image_path, dest_path)
        
        return True

    def _cache_image_from_tvdb(self, show_obj, img_type):
        """
        Retrieves an image of the type specified from TVDB and saves it to the cache folder
        
        returns: bool representing success
        
        show_obj: TVShow object that we want to cache an image for
        img_type: BANNER or POSTER
        """

        # generate the path based on the type & tvdb_id
        if img_type == self.POSTER:
            img_type_name = 'poster'
            dest_path = self.poster_path(show_obj.tvdbid)
        elif img_type == self.BANNER:
            img_type_name = 'banner'
            dest_path = self.banner_path(show_obj.tvdbid)
        else:
            logger.log(u"Invalid cache image type: "+str(img_type), logger.ERROR)
            return False

        # retrieve the image from TVDB using the generic metadata class
        #TODO: refactor
        metadata_generator = GenericMetadata()
        img_data = metadata_generator._retrieve_show_image(img_type_name, show_obj)
        result = metadata_generator._write_image(img_data, dest_path)

        return result
    
    def fill_cache(self, show_obj):
        """
        Caches all images for the given show. Copies them from the show dir if possible, or
        downloads them from TVDB if they aren't in the show dir.
        
        show_obj: TVShow object to cache images for
        """

        logger.log(u"Checking if we need any cache images for show "+str(show_obj.tvdbid), logger.DEBUG)

        # check if the images are already cached or not
        need_images = {self.POSTER: not self.has_poster(show_obj.tvdbid),
                       self.BANNER: not self.has_banner(show_obj.tvdbid),
                       }
        
        if not need_images[self.POSTER] and not need_images[self.BANNER]:
            logger.log(u"No new cache images needed, not retrieving new ones")
            return
        
        # check the show dir for images and use them
        try:
            for cur_provider in sickbeard.metadata_provider_dict.values():
                logger.log(u"Checking if we can use the show image from the "+cur_provider.name+" metadata", logger.DEBUG)
                if ek.ek(os.path.isfile, cur_provider.get_poster_path(show_obj)):
                    cur_file_name = os.path.abspath(cur_provider.get_poster_path(show_obj))
                    cur_file_type = self.which_type(cur_file_name)
                    
                    if cur_file_type == None:
                        logger.log(u"Unable to retrieve image type, not using the image from "+str(cur_file_name), logger.WARNING)
                        continue

                    logger.log(u"Checking if image "+cur_file_name+" (type "+str(cur_file_type)+" needs metadata: "+str(need_images[cur_file_type]), logger.DEBUG)
                    
                    if cur_file_type in need_images and need_images[cur_file_type]:
                        logger.log(u"Found an image in the show dir that doesn't exist in the cache, caching it: "+cur_file_name+", type "+str(cur_file_type), logger.DEBUG)
                        self._cache_image_from_file(cur_file_name, cur_file_type, show_obj.tvdbid)
                        need_images[cur_file_type] = False
        except exceptions.ShowDirNotFoundException:
            logger.log(u"Unable to search for images in show dir because it doesn't exist", logger.WARNING)
                    
        # download from TVDB for missing ones
        for cur_image_type in [self.POSTER, self.BANNER]:
            logger.log(u"Seeing if we still need an image of type "+str(cur_image_type)+": "+str(need_images[cur_image_type]), logger.DEBUG)
            if cur_image_type in need_images and need_images[cur_image_type]:
                self._cache_image_from_tvdb(show_obj, cur_image_type)
        

        logger.log(u"Done cache check")

########NEW FILE########
__FILENAME__ = logger
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import os
import sys
import threading

import logging

import sickbeard

from sickbeard import classes


# number of log files to keep
NUM_LOGS = 3

# log size in bytes
LOG_SIZE = 10000000  # 10 megs

ERROR = logging.ERROR
WARNING = logging.WARNING
MESSAGE = logging.INFO
DEBUG = logging.DEBUG

reverseNames = {u'ERROR': ERROR,
                u'WARNING': WARNING,
                u'INFO': MESSAGE,
                u'DEBUG': DEBUG}


class SBRotatingLogHandler(object):

    def __init__(self, log_file, num_files, num_bytes):
        self.num_files = num_files
        self.num_bytes = num_bytes

        self.log_file = log_file
        self.log_file_path = log_file
        self.cur_handler = None

        self.writes_since_check = 0

        self.log_lock = threading.Lock()
        self.console_logging = False

    def close_log(self, handler=None):
        if not handler:
            handler = self.cur_handler

        if handler:
            sb_logger = logging.getLogger('sickbeard')
            sb_logger.removeHandler(handler)
            handler.flush()
            handler.close()

    def initLogging(self, consoleLogging=False):

        if consoleLogging:
            self.console_logging = consoleLogging

        old_handler = None

        # get old handler in case we want to close it
        if self.cur_handler:
            old_handler = self.cur_handler
        else:
            # only start consoleLogging on first initialize
            if self.console_logging:
                # define a Handler which writes INFO messages or higher to the sys.stderr
                console = logging.StreamHandler()

                console.setLevel(logging.INFO)

                # set a format which is simpler for console use
                console.setFormatter(logging.Formatter('%(asctime)s %(levelname)s::%(message)s', '%H:%M:%S'))

                # add the handler to the root logger
                logging.getLogger('sickbeard').addHandler(console)

        self.log_file_path = os.path.join(sickbeard.LOG_DIR, self.log_file)
        self.cur_handler = self._config_handler()
        logging.getLogger('sickbeard').addHandler(self.cur_handler)
        logging.getLogger('sickbeard').setLevel(logging.DEBUG)

        # already logging in new log folder, close the old handler
        if old_handler:
            self.close_log(old_handler)

    def _config_handler(self):
        """
        Configure a file handler to log at file_name and return it.
        """
        file_handler = logging.FileHandler(self.log_file_path, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)-8s %(message)s', '%Y-%m-%d %H:%M:%S'))
        return file_handler

    def _log_file_name(self, i):
        """
        Returns a numbered log file name depending on i. If i==0 it just uses logName, if not it appends
        it to the extension (blah.log.3 for i == 3)

        i: Log number to ues
        """
        return self.log_file_path + ('.' + str(i) if i else '')

    def _num_logs(self):
        """
        Scans the log folder and figures out how many log files there are already on disk

        Returns: The number of the last used file (eg. mylog.log.3 would return 3). If there are no logs it returns -1
        """
        cur_log = 0
        while os.path.isfile(self._log_file_name(cur_log)):
            cur_log += 1
        return cur_log - 1

    def _rotate_logs(self):

        sb_logger = logging.getLogger('sickbeard')

        # delete the old handler
        if self.cur_handler:
            self.close_log()

        # rename or delete all the old log files
        for i in range(self._num_logs(), -1, -1):
            cur_file_name = self._log_file_name(i)
            try:
                if i >= NUM_LOGS:
                    os.remove(cur_file_name)
                else:
                    os.rename(cur_file_name, self._log_file_name(i + 1))
            except OSError:
                pass

        # the new log handler will always be on the un-numbered .log file
        new_file_handler = self._config_handler()

        self.cur_handler = new_file_handler

        sb_logger.addHandler(new_file_handler)

    def log(self, toLog, logLevel=MESSAGE):

        with self.log_lock:

            # check the size and see if we need to rotate
            if self.writes_since_check >= 10:
                if os.path.isfile(self.log_file_path) and os.path.getsize(self.log_file_path) >= LOG_SIZE:
                    self._rotate_logs()
                self.writes_since_check = 0
            else:
                self.writes_since_check += 1

            meThread = threading.currentThread().getName()
            message = meThread + u" :: " + toLog

            out_line = message

            sb_logger = logging.getLogger('sickbeard')

            try:
                if logLevel == DEBUG:
                    sb_logger.debug(out_line)
                elif logLevel == MESSAGE:
                    sb_logger.info(out_line)
                elif logLevel == WARNING:
                    sb_logger.warning(out_line)
                elif logLevel == ERROR:
                    sb_logger.error(out_line)

                    # add errors to the UI logger
                    classes.ErrorViewer.add(classes.UIError(message))
                else:
                    sb_logger.log(logLevel, out_line)
            except ValueError:
                pass

    def log_error_and_exit(self, error_msg):
        log(error_msg, ERROR)

        if not self.console_logging:
            sys.exit(error_msg.encode(sickbeard.SYS_ENCODING, 'xmlcharrefreplace'))
        else:
            sys.exit(1)


sb_log_instance = SBRotatingLogHandler('sickbeard.log', NUM_LOGS, LOG_SIZE)


def close():
    sb_log_instance.close_log()


def log_error_and_exit(error_msg):
    sb_log_instance.log_error_and_exit(error_msg)


def log(toLog, logLevel=MESSAGE):
    sb_log_instance.log(toLog, logLevel)

########NEW FILE########
__FILENAME__ = generic
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import os.path

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree

import re

import sickbeard

from sickbeard import exceptions, helpers
from sickbeard.metadata import helpers as metadata_helpers
from sickbeard import logger
from sickbeard import encodingKludge as ek
from sickbeard.exceptions import ex

from lib.tvdb_api import tvdb_api, tvdb_exceptions


class GenericMetadata():
    """
    Base class for all metadata providers. Default behavior is meant to mostly
    follow XBMC 12+ metadata standards. Has support for:
    - show metadata file
    - episode metadata file
    - episode thumbnail
    - show fanart
    - show poster
    - show banner
    - season thumbnails (poster)
    - season thumbnails (banner)
    - season all poster
    - season all banner
    """

    def __init__(self,
                 show_metadata=False,
                 episode_metadata=False,
                 fanart=False,
                 poster=False,
                 banner=False,
                 episode_thumbnails=False,
                 season_posters=False,
                 season_banners=False,
                 season_all_poster=False,
                 season_all_banner=False):

        self.name = "Generic"

        self._ep_nfo_extension = "nfo"
        self._show_metadata_filename = "tvshow.nfo"

        self.fanart_name = "fanart.jpg"
        self.poster_name = "poster.jpg"
        self.banner_name = "banner.jpg"

        self.season_all_poster_name = "season-all-poster.jpg"
        self.season_all_banner_name = "season-all-banner.jpg"

        self.show_metadata = show_metadata
        self.episode_metadata = episode_metadata
        self.fanart = fanart
        self.poster = poster
        self.banner = banner
        self.episode_thumbnails = episode_thumbnails
        self.season_posters = season_posters
        self.season_banners = season_banners
        self.season_all_poster = season_all_poster
        self.season_all_banner = season_all_banner

    def get_config(self):
        config_list = [self.show_metadata, self.episode_metadata, self.fanart, self.poster, self.banner, self.episode_thumbnails, self.season_posters, self.season_banners, self.season_all_poster, self.season_all_banner]
        return '|'.join([str(int(x)) for x in config_list])

    def get_id(self):
        return GenericMetadata.makeID(self.name)

    @staticmethod
    def makeID(name):
        name_id = re.sub("[+]", "plus", name)
        name_id = re.sub("[^\w\d_]", "_", name_id).lower()
        return name_id

    def set_config(self, string):
        config_list = [bool(int(x)) for x in string.split('|')]
        self.show_metadata = config_list[0]
        self.episode_metadata = config_list[1]
        self.fanart = config_list[2]
        self.poster = config_list[3]
        self.banner = config_list[4]
        self.episode_thumbnails = config_list[5]
        self.season_posters = config_list[6]
        self.season_banners = config_list[7]
        self.season_all_poster = config_list[8]
        self.season_all_banner = config_list[9]

    def _has_show_metadata(self, show_obj):
        result = ek.ek(os.path.isfile, self.get_show_file_path(show_obj))
        logger.log(u"Checking if " + self.get_show_file_path(show_obj) + " exists: " + str(result), logger.DEBUG)
        return result

    def _has_episode_metadata(self, ep_obj):
        result = ek.ek(os.path.isfile, self.get_episode_file_path(ep_obj))
        logger.log(u"Checking if " + self.get_episode_file_path(ep_obj) + " exists: " + str(result), logger.DEBUG)
        return result

    def _has_fanart(self, show_obj):
        result = ek.ek(os.path.isfile, self.get_fanart_path(show_obj))
        logger.log(u"Checking if " + self.get_fanart_path(show_obj) + " exists: " + str(result), logger.DEBUG)
        return result

    def _has_poster(self, show_obj):
        result = ek.ek(os.path.isfile, self.get_poster_path(show_obj))
        logger.log(u"Checking if " + self.get_poster_path(show_obj) + " exists: " + str(result), logger.DEBUG)
        return result

    def _has_banner(self, show_obj):
        result = ek.ek(os.path.isfile, self.get_banner_path(show_obj))
        logger.log(u"Checking if " + self.get_banner_path(show_obj) + " exists: " + str(result), logger.DEBUG)
        return result

    def _has_episode_thumb(self, ep_obj):
        location = self.get_episode_thumb_path(ep_obj)
        result = location != None and ek.ek(os.path.isfile, location)
        if location:
            logger.log(u"Checking if " + location + " exists: " + str(result), logger.DEBUG)
        return result

    def _has_season_poster(self, show_obj, season):
        location = self.get_season_poster_path(show_obj, season)
        result = location != None and ek.ek(os.path.isfile, location)
        if location:
            logger.log(u"Checking if " + location + " exists: " + str(result), logger.DEBUG)
        return result

    def _has_season_banner(self, show_obj, season):
        location = self.get_season_banner_path(show_obj, season)
        result = location != None and ek.ek(os.path.isfile, location)
        if location:
            logger.log(u"Checking if " + location + " exists: " + str(result), logger.DEBUG)
        return result

    def _has_season_all_poster(self, show_obj):
        result = ek.ek(os.path.isfile, self.get_season_all_poster_path(show_obj))
        logger.log(u"Checking if " + self.get_season_all_poster_path(show_obj) + " exists: " + str(result), logger.DEBUG)
        return result

    def _has_season_all_banner(self, show_obj):
        result = ek.ek(os.path.isfile, self.get_season_all_banner_path(show_obj))
        logger.log(u"Checking if " + self.get_season_all_banner_path(show_obj) + " exists: " + str(result), logger.DEBUG)
        return result

    def get_show_file_path(self, show_obj):
        return ek.ek(os.path.join, show_obj.location, self._show_metadata_filename)

    def get_episode_file_path(self, ep_obj):
        return helpers.replaceExtension(ep_obj.location, self._ep_nfo_extension)

    def get_fanart_path(self, show_obj):
        return ek.ek(os.path.join, show_obj.location, self.fanart_name)

    def get_poster_path(self, show_obj):
        return ek.ek(os.path.join, show_obj.location, self.poster_name)

    def get_banner_path(self, show_obj):
        return ek.ek(os.path.join, show_obj.location, self.banner_name)

    def get_episode_thumb_path(self, ep_obj):
        """
        Returns the path where the episode thumbnail should be stored.
        ep_obj: a TVEpisode instance for which to create the thumbnail
        """
        if ek.ek(os.path.isfile, ep_obj.location):

            tbn_filename = ep_obj.location.rpartition(".")

            if tbn_filename[0] == "":
                tbn_filename = ep_obj.location + "-thumb.jpg"
            else:
                tbn_filename = tbn_filename[0] + "-thumb.jpg"
        else:
            return None

        return tbn_filename

    def get_season_poster_path(self, show_obj, season):
        """
        Returns the full path to the file for a given season poster.

        show_obj: a TVShow instance for which to generate the path
        season: a season number to be used for the path. Note that season 0
                means specials.
        """

        # Our specials thumbnail is, well, special
        if season == 0:
            season_poster_filename = 'season-specials'
        else:
            season_poster_filename = 'season' + str(season).zfill(2)

        return ek.ek(os.path.join, show_obj.location, season_poster_filename + '-poster.jpg')

    def get_season_banner_path(self, show_obj, season):
        """
        Returns the full path to the file for a given season banner.

        show_obj: a TVShow instance for which to generate the path
        season: a season number to be used for the path. Note that season 0
                means specials.
        """

        # Our specials thumbnail is, well, special
        if season == 0:
            season_banner_filename = 'season-specials'
        else:
            season_banner_filename = 'season' + str(season).zfill(2)

        return ek.ek(os.path.join, show_obj.location, season_banner_filename + '-banner.jpg')

    def get_season_all_poster_path(self, show_obj):
        return ek.ek(os.path.join, show_obj.location, self.season_all_poster_name)

    def get_season_all_banner_path(self, show_obj):
        return ek.ek(os.path.join, show_obj.location, self.season_all_banner_name)

    def _show_data(self, show_obj):
        """
        This should be overridden by the implementing class. It should
        provide the content of the show metadata file.
        """
        return None

    def _ep_data(self, ep_obj):
        """
        This should be overridden by the implementing class. It should
        provide the content of the episode metadata file.
        """
        return None

    def create_show_metadata(self, show_obj):
        if self.show_metadata and show_obj and not self._has_show_metadata(show_obj):
            logger.log(u"Metadata provider " + self.name + " creating show metadata for " + show_obj.name, logger.DEBUG)
            return self.write_show_file(show_obj)
        return False

    def create_episode_metadata(self, ep_obj):
        if self.episode_metadata and ep_obj and not self._has_episode_metadata(ep_obj):
            logger.log(u"Metadata provider " + self.name + " creating episode metadata for " + ep_obj.prettyName(), logger.DEBUG)
            return self.write_ep_file(ep_obj)
        return False

    def create_fanart(self, show_obj):
        if self.fanart and show_obj and not self._has_fanart(show_obj):
            logger.log(u"Metadata provider " + self.name + " creating fanart for " + show_obj.name, logger.DEBUG)
            return self.save_fanart(show_obj)
        return False

    def create_poster(self, show_obj):
        if self.poster and show_obj and not self._has_poster(show_obj):
            logger.log(u"Metadata provider " + self.name + " creating poster for " + show_obj.name, logger.DEBUG)
            return self.save_poster(show_obj)
        return False

    def create_banner(self, show_obj):
        if self.banner and show_obj and not self._has_banner(show_obj):
            logger.log(u"Metadata provider " + self.name + " creating banner for " + show_obj.name, logger.DEBUG)
            return self.save_banner(show_obj)
        return False

    def create_episode_thumb(self, ep_obj):
        if self.episode_thumbnails and ep_obj and not self._has_episode_thumb(ep_obj):
            logger.log(u"Metadata provider " + self.name + " creating episode thumbnail for " + ep_obj.prettyName(), logger.DEBUG)
            return self.save_thumbnail(ep_obj)
        return False

    def create_season_posters(self, show_obj):
        if self.season_posters and show_obj:
            result = []
            for season, episodes in show_obj.episodes.iteritems():  # @UnusedVariable
                if not self._has_season_poster(show_obj, season):
                    logger.log(u"Metadata provider " + self.name + " creating season posters for " + show_obj.name, logger.DEBUG)
                    result = result + [self.save_season_posters(show_obj, season)]
            return all(result)
        return False

    def create_season_banners(self, show_obj):
        if self.season_banners and show_obj:
            result = []
            for season, episodes in show_obj.episodes.iteritems():  # @UnusedVariable
                if not self._has_season_banner(show_obj, season):
                    logger.log(u"Metadata provider " + self.name + " creating season banners for " + show_obj.name, logger.DEBUG)
                    result = result + [self.save_season_banners(show_obj, season)]
            return all(result)
        return False

    def create_season_all_poster(self, show_obj):
        if self.season_all_poster and show_obj and not self._has_season_all_poster(show_obj):
            logger.log(u"Metadata provider " + self.name + " creating season all poster for " + show_obj.name, logger.DEBUG)
            return self.save_season_all_poster(show_obj)
        return False

    def create_season_all_banner(self, show_obj):
        if self.season_all_banner and show_obj and not self._has_season_all_banner(show_obj):
            logger.log(u"Metadata provider " + self.name + " creating season all banner for " + show_obj.name, logger.DEBUG)
            return self.save_season_all_banner(show_obj)
        return False

    def _get_episode_thumb_url(self, ep_obj):
        """
        Returns the URL to use for downloading an episode's thumbnail. Uses
        theTVDB.com data.

        ep_obj: a TVEpisode object for which to grab the thumb URL
        """
        all_eps = [ep_obj] + ep_obj.relatedEps

        tvdb_lang = ep_obj.show.lang

        # get a TVDB object
        try:
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if tvdb_lang and not tvdb_lang == 'en':
                ltvdb_api_parms['language'] = tvdb_lang

            t = tvdb_api.Tvdb(actors=True, **ltvdb_api_parms)
            tvdb_show_obj = t[ep_obj.show.tvdbid]
        except tvdb_exceptions.tvdb_shownotfound, e:
            raise exceptions.ShowNotFoundException(e.message)
        except tvdb_exceptions.tvdb_error, e:
            logger.log(u"Unable to connect to TVDB while creating meta files - skipping - " + ex(e), logger.ERROR)
            return None

        # try all included episodes in case some have thumbs and others don't
        for cur_ep in all_eps:
            try:
                myEp = tvdb_show_obj[cur_ep.season][cur_ep.episode]
            except (tvdb_exceptions.tvdb_episodenotfound, tvdb_exceptions.tvdb_seasonnotfound):
                logger.log(u"Unable to find episode " + str(cur_ep.season) + "x" + str(cur_ep.episode) + " on tvdb... has it been removed? Should I delete from db?")
                continue

            thumb_url = myEp["filename"]

            if thumb_url:
                return thumb_url

        return None

    def write_show_file(self, show_obj):
        """
        Generates and writes show_obj's metadata under the given path to the
        filename given by get_show_file_path()

        show_obj: TVShow object for which to create the metadata

        path: An absolute or relative path where we should put the file. Note that
                the file name will be the default show_file_name.

        Note that this method expects that _show_data will return an ElementTree
        object. If your _show_data returns data in another format you'll need to
        override this method.
        """

        data = self._show_data(show_obj)

        if not data:
            return False

        nfo_file_path = self.get_show_file_path(show_obj)
        nfo_file_dir = ek.ek(os.path.dirname, nfo_file_path)

        try:
            if not ek.ek(os.path.isdir, nfo_file_dir):
                logger.log(u"Metadata dir didn't exist, creating it at " + nfo_file_dir, logger.DEBUG)
                ek.ek(os.makedirs, nfo_file_dir)
                helpers.chmodAsParent(nfo_file_dir)

            logger.log(u"Writing show nfo file to " + nfo_file_path, logger.DEBUG)

            nfo_file = ek.ek(open, nfo_file_path, 'w')

            data.write(nfo_file, encoding="utf-8")
            nfo_file.close()
            helpers.chmodAsParent(nfo_file_path)
        except IOError, e:
            logger.log(u"Unable to write file to " + nfo_file_path + " - are you sure the folder is writable? " + ex(e), logger.ERROR)
            return False

        return True

    def write_ep_file(self, ep_obj):
        """
        Generates and writes ep_obj's metadata under the given path with the
        given filename root. Uses the episode's name with the extension in
        _ep_nfo_extension.

        ep_obj: TVEpisode object for which to create the metadata

        file_name_path: The file name to use for this metadata. Note that the extension
                will be automatically added based on _ep_nfo_extension. This should
                include an absolute path.

        Note that this method expects that _ep_data will return an ElementTree
        object. If your _ep_data returns data in another format you'll need to
        override this method.
        """

        data = self._ep_data(ep_obj)

        if not data:
            return False

        nfo_file_path = self.get_episode_file_path(ep_obj)
        nfo_file_dir = ek.ek(os.path.dirname, nfo_file_path)

        try:
            if not ek.ek(os.path.isdir, nfo_file_dir):
                logger.log(u"Metadata dir didn't exist, creating it at " + nfo_file_dir, logger.DEBUG)
                ek.ek(os.makedirs, nfo_file_dir)
                helpers.chmodAsParent(nfo_file_dir)

            logger.log(u"Writing episode nfo file to " + nfo_file_path, logger.DEBUG)

            nfo_file = ek.ek(open, nfo_file_path, 'w')

            data.write(nfo_file, encoding="utf-8")
            nfo_file.close()
            helpers.chmodAsParent(nfo_file_path)
        except IOError, e:
            logger.log(u"Unable to write file to " + nfo_file_path + " - are you sure the folder is writable? " + ex(e), logger.ERROR)
            return False

        return True

    def save_thumbnail(self, ep_obj):
        """
        Retrieves a thumbnail and saves it to the correct spot. This method should not need to
        be overridden by implementing classes, changing get_episode_thumb_path and
        _get_episode_thumb_url should suffice.

        ep_obj: a TVEpisode object for which to generate a thumbnail
        """

        file_path = self.get_episode_thumb_path(ep_obj)

        if not file_path:
            logger.log(u"Unable to find a file path to use for this thumbnail, not generating it", logger.DEBUG)
            return False

        thumb_url = self._get_episode_thumb_url(ep_obj)

        # if we can't find one then give up
        if not thumb_url:
            logger.log(u"No thumb is available for this episode, not creating a thumb", logger.DEBUG)
            return False

        thumb_data = metadata_helpers.getShowImage(thumb_url)

        result = self._write_image(thumb_data, file_path)

        if not result:
            return False

        for cur_ep in [ep_obj] + ep_obj.relatedEps:
            cur_ep.hastbn = True

        return True

    def save_fanart(self, show_obj, which=None):
        """
        Downloads a fanart image and saves it to the filename specified by fanart_name
        inside the show's root folder.

        show_obj: a TVShow object for which to download fanart
        """

        # use the default fanart name
        fanart_path = self.get_fanart_path(show_obj)

        fanart_data = self._retrieve_show_image('fanart', show_obj, which)

        if not fanart_data:
            logger.log(u"No fanart image was retrieved, unable to write fanart", logger.DEBUG)
            return False

        return self._write_image(fanart_data, fanart_path)

    def save_poster(self, show_obj, which=None):
        """
        Downloads a poster image and saves it to the filename specified by poster_name
        inside the show's root folder.

        show_obj: a TVShow object for which to download a poster
        """

        # use the default poster name
        poster_path = self.get_poster_path(show_obj)

        poster_data = self._retrieve_show_image('poster', show_obj, which)

        if not poster_data:
            logger.log(u"No show poster image was retrieved, unable to write poster", logger.DEBUG)
            return False

        return self._write_image(poster_data, poster_path)

    def save_banner(self, show_obj, which=None):
        """
        Downloads a banner image and saves it to the filename specified by banner_name
        inside the show's root folder.

        show_obj: a TVShow object for which to download a banner
        """

        # use the default banner name
        banner_path = self.get_banner_path(show_obj)

        banner_data = self._retrieve_show_image('banner', show_obj, which)

        if not banner_data:
            logger.log(u"No show banner image was retrieved, unable to write banner", logger.DEBUG)
            return False

        return self._write_image(banner_data, banner_path)

    def save_season_posters(self, show_obj, season):
        """
        Saves all season posters to disk for the given show.

        show_obj: a TVShow object for which to save the season thumbs

        Cycles through all seasons and saves the season posters if possible. This
        method should not need to be overridden by implementing classes, changing
        _season_posters_dict and get_season_poster_path should be good enough.
        """

        season_dict = self._season_posters_dict(show_obj, season)
        result = []

        # Returns a nested dictionary of season art with the season
        # number as primary key. It's really overkill but gives the option
        # to present to user via ui to pick down the road.
        for cur_season in season_dict:

            cur_season_art = season_dict[cur_season]

            if len(cur_season_art) == 0:
                continue

            # Just grab whatever's there for now
            art_id, season_url = cur_season_art.popitem()  # @UnusedVariable

            season_poster_file_path = self.get_season_poster_path(show_obj, cur_season)

            if not season_poster_file_path:
                logger.log(u"Path for season " + str(cur_season) + " came back blank, skipping this season", logger.DEBUG)
                continue

            seasonData = metadata_helpers.getShowImage(season_url)

            if not seasonData:
                logger.log(u"No season poster data available, skipping this season", logger.DEBUG)
                continue

            result = result + [self._write_image(seasonData, season_poster_file_path)]
        if result:
            return all(result)
        else:
            return False

        return True

    def save_season_banners(self, show_obj, season):
        """
        Saves all season banners to disk for the given show.

        show_obj: a TVShow object for which to save the season thumbs

        Cycles through all seasons and saves the season banners if possible. This
        method should not need to be overridden by implementing classes, changing
        _season_banners_dict and get_season_banner_path should be good enough.
        """

        season_dict = self._season_banners_dict(show_obj, season)
        result = []

        # Returns a nested dictionary of season art with the season
        # number as primary key. It's really overkill but gives the option
        # to present to user via ui to pick down the road.
        for cur_season in season_dict:

            cur_season_art = season_dict[cur_season]

            if len(cur_season_art) == 0:
                continue

            # Just grab whatever's there for now
            art_id, season_url = cur_season_art.popitem()  # @UnusedVariable

            season_banner_file_path = self.get_season_banner_path(show_obj, cur_season)

            if not season_banner_file_path:
                logger.log(u"Path for season " + str(cur_season) + " came back blank, skipping this season", logger.DEBUG)
                continue

            seasonData = metadata_helpers.getShowImage(season_url)

            if not seasonData:
                logger.log(u"No season banner data available, skipping this season", logger.DEBUG)
                continue

            result = result + [self._write_image(seasonData, season_banner_file_path)]
        if result:
            return all(result)
        else:
            return False

        return True

    def save_season_all_poster(self, show_obj, which=None):
        # use the default season all poster name
        poster_path = self.get_season_all_poster_path(show_obj)

        poster_data = self._retrieve_show_image('poster', show_obj, which)

        if not poster_data:
            logger.log(u"No show poster image was retrieved, unable to write season all poster", logger.DEBUG)
            return False

        return self._write_image(poster_data, poster_path)

    def save_season_all_banner(self, show_obj, which=None):
        # use the default season all banner name
        banner_path = self.get_season_all_banner_path(show_obj)

        banner_data = self._retrieve_show_image('banner', show_obj, which)

        if not banner_data:
            logger.log(u"No show banner image was retrieved, unable to write season all banner", logger.DEBUG)
            return False

        return self._write_image(banner_data, banner_path)

    def _write_image(self, image_data, image_path):
        """
        Saves the data in image_data to the location image_path. Returns True/False
        to represent success or failure.

        image_data: binary image data to write to file
        image_path: file location to save the image to
        """

        # don't bother overwriting it
        if ek.ek(os.path.isfile, image_path):
            logger.log(u"Image already exists, not downloading", logger.DEBUG)
            return False

        if not image_data:
            logger.log(u"Unable to retrieve image, skipping", logger.WARNING)
            return False

        image_dir = ek.ek(os.path.dirname, image_path)

        try:
            if not ek.ek(os.path.isdir, image_dir):
                logger.log(u"Metadata dir didn't exist, creating it at " + image_dir, logger.DEBUG)
                ek.ek(os.makedirs, image_dir)
                helpers.chmodAsParent(image_dir)

            outFile = ek.ek(open, image_path, 'wb')
            outFile.write(image_data)
            outFile.close()
            helpers.chmodAsParent(image_path)
        except IOError, e:
            logger.log(u"Unable to write image to " + image_path + " - are you sure the show folder is writable? " + ex(e), logger.ERROR)
            return False

        return True

    def _retrieve_show_image(self, image_type, show_obj, which=None):
        """
        Gets an image URL from theTVDB.com, downloads it and returns the data.

        image_type: type of image to retrieve (currently supported: fanart, poster, banner)
        show_obj: a TVShow object to use when searching for the image
        which: optional, a specific numbered poster to look for

        Returns: the binary image data if available, or else None
        """

        tvdb_lang = show_obj.lang

        try:
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if tvdb_lang and not tvdb_lang == 'en':
                ltvdb_api_parms['language'] = tvdb_lang

            t = tvdb_api.Tvdb(banners=True, **ltvdb_api_parms)
            tvdb_show_obj = t[show_obj.tvdbid]
        except (tvdb_exceptions.tvdb_error, IOError), e:
            logger.log(u"Unable to look up show on TVDB, not downloading images: " + ex(e), logger.ERROR)
            return None

        if image_type not in ('fanart', 'poster', 'banner'):
            logger.log(u"Invalid image type " + str(image_type) + ", couldn't find it in the TVDB object", logger.ERROR)
            return None

        image_url = tvdb_show_obj[image_type]

        image_data = metadata_helpers.getShowImage(image_url, which)

        return image_data

    def _season_posters_dict(self, show_obj, season):
        """
        Should return a dict like:

        result = {<season number>:
                    {1: '<url 1>', 2: <url 2>, ...},}
        """

        # This holds our resulting dictionary of season art
        result = {}

        tvdb_lang = show_obj.lang

        try:
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if tvdb_lang and not tvdb_lang == 'en':
                ltvdb_api_parms['language'] = tvdb_lang

            t = tvdb_api.Tvdb(banners=True, **ltvdb_api_parms)
            tvdb_show_obj = t[show_obj.tvdbid]
        except (tvdb_exceptions.tvdb_error, IOError), e:
            logger.log(u"Unable to look up show on TVDB, not downloading images: " + ex(e), logger.ERROR)
            return result

        # if we have no season banners then just finish
        if 'season' not in tvdb_show_obj['_banners'] or 'season' not in tvdb_show_obj['_banners']['season']:
            return result

        # Give us just the normal poster-style season graphics
        seasonsArtObj = tvdb_show_obj['_banners']['season']['season']

        # Returns a nested dictionary of season art with the season
        # number as primary key. It's really overkill but gives the option
        # to present to user via ui to pick down the road.

        result[season] = {}

        # find the correct season in the tvdb object and just copy the dict into our result dict
        for seasonArtID in seasonsArtObj.keys():
            if int(seasonsArtObj[seasonArtID]['season']) == season and seasonsArtObj[seasonArtID]['language'] == 'en':
                result[season][seasonArtID] = seasonsArtObj[seasonArtID]['_bannerpath']

        return result

    def _season_banners_dict(self, show_obj, season):
        """
        Should return a dict like:

        result = {<season number>:
                    {1: '<url 1>', 2: <url 2>, ...},}
        """

        # This holds our resulting dictionary of season art
        result = {}

        tvdb_lang = show_obj.lang

        try:
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if tvdb_lang and not tvdb_lang == 'en':
                ltvdb_api_parms['language'] = tvdb_lang

            t = tvdb_api.Tvdb(banners=True, **ltvdb_api_parms)
            tvdb_show_obj = t[show_obj.tvdbid]
        except (tvdb_exceptions.tvdb_error, IOError), e:
            logger.log(u"Unable to look up show on TVDB, not downloading images: " + ex(e), logger.ERROR)
            return result

        # if we have no season banners then just finish
        if 'season' not in tvdb_show_obj['_banners'] or 'seasonwide' not in tvdb_show_obj['_banners']['season']:
            return result

        # Give us just the normal season graphics
        seasonsArtObj = tvdb_show_obj['_banners']['season']['seasonwide']

        # Returns a nested dictionary of season art with the season
        # number as primary key. It's really overkill but gives the option
        # to present to user via ui to pick down the road.

        result[season] = {}

        # find the correct season in the tvdb object and just copy the dict into our result dict
        for seasonArtID in seasonsArtObj.keys():
            if int(seasonsArtObj[seasonArtID]['season']) == season and seasonsArtObj[seasonArtID]['language'] == 'en':
                result[season][seasonArtID] = seasonsArtObj[seasonArtID]['_bannerpath']

        return result

    def retrieveShowMetadata(self, folder):
        """
        Used only when mass adding Existing Shows, using previously generated Show metadata to reduce the need to query TVDB.
        """

        empty_return = (None, None)

        metadata_path = ek.ek(os.path.join, folder, self._show_metadata_filename)

        if not ek.ek(os.path.isdir, folder) or not ek.ek(os.path.isfile, metadata_path):
            logger.log(u"Can't load the metadata file from " + repr(metadata_path) + ", it doesn't exist", logger.DEBUG)
            return empty_return

        logger.log(u"Loading show info from metadata file in " + folder, logger.DEBUG)

        try:
            with ek.ek(open, metadata_path, 'r') as xmlFileObj:
                showXML = etree.ElementTree(file=xmlFileObj)

            if showXML.findtext('title') == None or (showXML.findtext('tvdbid') == None and showXML.findtext('id') == None):
                logger.log(u"Invalid info in tvshow.nfo (missing name or id):" \
                    + str(showXML.findtext('title')) + " " \
                    + str(showXML.findtext('tvdbid')) + " " \
                    + str(showXML.findtext('id')))
                return empty_return

            name = showXML.findtext('title')
            if showXML.findtext('tvdbid') != None:
                tvdb_id = int(showXML.findtext('tvdbid'))
            elif showXML.findtext('id'):
                tvdb_id = int(showXML.findtext('id'))
            else:
                logger.log(u"Empty <id> or <tvdbid> field in NFO, unable to find an ID", logger.WARNING)
                return empty_return

            if not tvdb_id:
                logger.log(u"Invalid tvdb id (" + str(tvdb_id) + "), not using metadata file", logger.WARNING)
                return empty_return

        except Exception, e:
            logger.log(u"There was an error parsing your existing metadata file: '" + metadata_path + "' error: " + ex(e), logger.WARNING)
            return empty_return

        return (tvdb_id, name)

########NEW FILE########
__FILENAME__ = helpers
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from sickbeard import helpers
from sickbeard import logger


def getShowImage(url, imgNum=None):

    image_data = None  # @UnusedVariable

    if url == None:
        return None

    # if they provided a fanart number try to use it instead
    if imgNum != None:
        tempURL = url.split('-')[0] + "-" + str(imgNum) + ".jpg"
    else:
        tempURL = url

    logger.log(u"Fetching image from " + tempURL, logger.DEBUG)

    image_data = helpers.getURL(tempURL)

    if image_data is None:
        logger.log(u"There was an error trying to retrieve the image, aborting", logger.ERROR)
        return None

    return image_data

########NEW FILE########
__FILENAME__ = mede8er
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime

import sickbeard

import mediabrowser

from sickbeard import logger, exceptions, helpers
from lib.tvdb_api import tvdb_api, tvdb_exceptions
from sickbeard.exceptions import ex

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree


class Mede8erMetadata(mediabrowser.MediaBrowserMetadata):
    """
    Metadata generation class for Mede8er based on the MediaBrowser.

    The following file structure is used:

    show_root/series.xml                    (show metadata)
    show_root/folder.jpg                    (poster)
    show_root/fanart.jpg                    (fanart)
    show_root/Season ##/folder.jpg          (season thumb)
    show_root/Season ##/filename.ext        (*)
    show_root/Season ##/filename.xml        (episode metadata)
    show_root/Season ##/filename.jpg        (episode thumb)
    """

    def __init__(self,
                 show_metadata=False,
                 episode_metadata=False,
                 fanart=False,
                 poster=False,
                 banner=False,
                 episode_thumbnails=False,
                 season_posters=False,
                 season_banners=False,
                 season_all_poster=False,
                 season_all_banner=False):

        mediabrowser.MediaBrowserMetadata.__init__(self,
                                         show_metadata,
                                         episode_metadata,
                                         fanart,
                                         poster,
                                         banner,
                                         episode_thumbnails,
                                         season_posters,
                                         season_banners,
                                         season_all_poster,
                                         season_all_banner)

        self.name = "Mede8er"

        self.fanart_name = "fanart.jpg"

        # web-ui metadata template
        # self.eg_show_metadata = "series.xml"
        self.eg_episode_metadata = "Season##\\<i>filename</i>.xml"
        self.eg_fanart = "fanart.jpg"
        # self.eg_poster = "folder.jpg"
        # self.eg_banner = "banner.jpg"
        self.eg_episode_thumbnails = "Season##\\<i>filename</i>.jpg"
        # self.eg_season_posters = "Season##\\folder.jpg"
        # self.eg_season_banners = "Season##\\banner.jpg"
        # self.eg_season_all_poster = "<i>not supported</i>"
        # self.eg_season_all_banner = "<i>not supported</i>"

    def get_episode_file_path(self, ep_obj):
        return helpers.replaceExtension(ep_obj.location, self._ep_nfo_extension)

    def get_episode_thumb_path(self, ep_obj):
        return helpers.replaceExtension(ep_obj.location, 'jpg')

    def _show_data(self, show_obj):
        """
        Creates an elementTree XML structure for a MediaBrowser-style series.xml
        returns the resulting data object.

        show_obj: a TVShow instance to create the NFO for
        """

        tvdb_lang = show_obj.lang
        # There's gotta be a better way of doing this but we don't wanna
        # change the language value elsewhere
        ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

        if tvdb_lang and not tvdb_lang == 'en':
            ltvdb_api_parms['language'] = tvdb_lang

        t = tvdb_api.Tvdb(actors=True, **ltvdb_api_parms)

        rootNode = etree.Element("details")
        tv_node = etree.SubElement(rootNode, "movie")
        tv_node.attrib["isExtra"] = "false"
        tv_node.attrib["isSet"] = "false"
        tv_node.attrib["isTV"] = "true"

        try:
            myShow = t[int(show_obj.tvdbid)]
        except tvdb_exceptions.tvdb_shownotfound:
            logger.log(u"Unable to find show with id " + str(show_obj.tvdbid) + " on tvdb, skipping it", logger.ERROR)
            raise

        except tvdb_exceptions.tvdb_error:
            logger.log(u"TVDB is down, can't use its data to make the NFO", logger.ERROR)
            raise

        # check for title and id
        try:
            if myShow['seriesname'] == None or myShow['seriesname'] == "" or myShow['id'] == None or myShow['id'] == "":
                logger.log(u"Incomplete info for show with id " + str(show_obj.tvdbid) + " on tvdb, skipping it", logger.ERROR)
                return False
        except tvdb_exceptions.tvdb_attributenotfound:
            logger.log(u"Incomplete info for show with id " + str(show_obj.tvdbid) + " on tvdb, skipping it", logger.ERROR)
            return False

        SeriesName = etree.SubElement(tv_node, "title")
        if myShow['seriesname'] != None:
            SeriesName.text = myShow['seriesname']
        else:
            SeriesName.text = ""

        Genres = etree.SubElement(tv_node, "genres")
        if myShow["genre"] != None:
            for genre in myShow['genre'].split('|'):
                if genre and genre.strip():
                    cur_genre = etree.SubElement(Genres, "Genre")
                    cur_genre.text = genre.strip()

        FirstAired = etree.SubElement(tv_node, "premiered")
        if myShow['firstaired'] != None:
            FirstAired.text = myShow['firstaired']

        year = etree.SubElement(tv_node, "year")
        if myShow["firstaired"] != None:
            try:
                year_text = str(datetime.datetime.strptime(myShow["firstaired"], '%Y-%m-%d').year)
                if year_text:
                    year.text = year_text
            except:
                pass

        if myShow['rating'] != None:
            try:
                rating = int((float(myShow['rating']) * 10))
            except ValueError:
                rating = 0
            Rating = etree.SubElement(tv_node, "rating")
            rating_text = str(rating)
            if rating_text != None:
                Rating.text = rating_text

        Status = etree.SubElement(tv_node, "status")
        if myShow['status'] != None:
            Status.text = myShow['status']

        mpaa = etree.SubElement(tv_node, "mpaa")
        if myShow["contentrating"] != None:
            mpaa.text = myShow["contentrating"]

        IMDB_ID = etree.SubElement(tv_node, "id")
        if myShow['imdb_id'] != None:
            IMDB_ID.attrib["moviedb"] = "imdb"
            IMDB_ID.text = myShow['imdb_id']

        tvdbid = etree.SubElement(tv_node, "tvdbid")
        if myShow['id'] != None:
            tvdbid.text = myShow['id']

        Runtime = etree.SubElement(tv_node, "runtime")
        if myShow['runtime'] != None:
            Runtime.text = myShow['runtime']

        cast = etree.SubElement(tv_node, "cast")

        if myShow["_actors"] != None:
            for actor in myShow['_actors']:
                cur_actor_name_text = actor['name']

                if cur_actor_name_text != None and cur_actor_name_text.strip():
                    cur_actor = etree.SubElement(cast, "actor")
                    cur_actor.text = cur_actor_name_text.strip()

        helpers.indentXML(rootNode)

        data = etree.ElementTree(rootNode)

        return data

    def _ep_data(self, ep_obj):
        """
        Creates an elementTree XML structure for a MediaBrowser style episode.xml
        and returns the resulting data object.

        show_obj: a TVShow instance to create the NFO for
        """

        eps_to_write = [ep_obj] + ep_obj.relatedEps

        tvdb_lang = ep_obj.show.lang

        try:
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if tvdb_lang and not tvdb_lang == 'en':
                ltvdb_api_parms['language'] = tvdb_lang

            t = tvdb_api.Tvdb(actors=True, **ltvdb_api_parms)
            myShow = t[ep_obj.show.tvdbid]
        except tvdb_exceptions.tvdb_shownotfound, e:
            raise exceptions.ShowNotFoundException(e.message)
        except tvdb_exceptions.tvdb_error, e:
            logger.log(u"Unable to connect to TVDB while creating meta files - skipping - " + ex(e), logger.ERROR)
            return False

        rootNode = etree.Element("details")
        movie = etree.SubElement(rootNode, "movie")

        movie.attrib["isExtra"] = "false"
        movie.attrib["isSet"] = "false"
        movie.attrib["isTV"] = "true"

        # write an MediaBrowser XML containing info for all matching episodes
        for curEpToWrite in eps_to_write:

            try:
                myEp = myShow[curEpToWrite.season][curEpToWrite.episode]
            except (tvdb_exceptions.tvdb_episodenotfound, tvdb_exceptions.tvdb_seasonnotfound):
                logger.log(u"Unable to find episode " + str(curEpToWrite.season) + "x" + str(curEpToWrite.episode) + " on tvdb... has it been removed? Should I delete from db?")
                return None

            if curEpToWrite == ep_obj:
                # root (or single) episode

                # default to today's date for specials if firstaired is not set
                if myEp['firstaired'] == None and ep_obj.season == 0:
                    myEp['firstaired'] = str(datetime.date.fromordinal(1))

                if myEp['episodename'] == None or myEp['firstaired'] == None:
                    return None

                episode = movie

                EpisodeName = etree.SubElement(episode, "title")
                if curEpToWrite.name != None:
                    EpisodeName.text = curEpToWrite.name
                else:
                    EpisodeName.text = ""

                SeasonNumber = etree.SubElement(episode, "season")
                SeasonNumber.text = str(curEpToWrite.season)

                EpisodeNumber = etree.SubElement(episode, "episode")
                EpisodeNumber.text = str(ep_obj.episode)

                year = etree.SubElement(episode, "year")
                if myShow["firstaired"] != None:
                    try:
                        year_text = str(datetime.datetime.strptime(myShow["firstaired"], '%Y-%m-%d').year)
                        if year_text:
                            year.text = year_text
                    except:
                        pass

                plot = etree.SubElement(episode, "plot")
                if myShow["overview"] != None:
                    plot.text = myShow["overview"]

                Overview = etree.SubElement(episode, "episodeplot")
                if curEpToWrite.description != None:
                    Overview.text = curEpToWrite.description
                else:
                    Overview.text = ""

                mpaa = etree.SubElement(episode, "mpaa")
                if myShow["contentrating"] != None:
                    mpaa.text = myShow["contentrating"]

                if not ep_obj.relatedEps:
                    if myEp["rating"] != None:
                        try:
                            rating = int((float(myEp['rating']) * 10))
                        except ValueError:
                            rating = 0
                        Rating = etree.SubElement(episode, "rating")
                        rating_text = str(rating)
                        if rating_text != None:
                            Rating.text = rating_text

                director = etree.SubElement(episode, "director")
                director_text = myEp['director']
                if director_text != None:
                    director.text = director_text

                credits = etree.SubElement(episode, "credits")
                credits_text = myEp['writer']
                if credits_text != None:
                    credits.text = credits_text

                cast = etree.SubElement(episode, "cast")

                if myShow["_actors"] != None:
                    for actor in myShow['_actors']:
                        cur_actor_name_text = actor['name']

                        if cur_actor_name_text != None and cur_actor_name_text.strip():
                            cur_actor = etree.SubElement(cast, "actor")
                            cur_actor.text = cur_actor_name_text.strip()

            else:
                # append data from (if any) related episodes

                if curEpToWrite.name:
                    if not EpisodeName.text:
                        EpisodeName.text = curEpToWrite.name
                    else:
                        EpisodeName.text = EpisodeName.text + ", " + curEpToWrite.name

                if curEpToWrite.description:
                    if not Overview.text:
                        Overview.text = curEpToWrite.description
                    else:
                        Overview.text = Overview.text + "\r" + curEpToWrite.description

        helpers.indentXML(rootNode)
        data = etree.ElementTree(rootNode)

        return data


# present a standard "interface" from the module
metadata_class = Mede8erMetadata

########NEW FILE########
__FILENAME__ = mediabrowser
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import os
import re

import sickbeard

import generic

from sickbeard import logger, exceptions, helpers
from sickbeard import encodingKludge as ek
from lib.tvdb_api import tvdb_api, tvdb_exceptions
from sickbeard.exceptions import ex

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree


class MediaBrowserMetadata(generic.GenericMetadata):
    """
    Metadata generation class for Media Browser 2.x/3.x - Standard Mode.

    The following file structure is used:

    show_root/series.xml                       (show metadata)
    show_root/folder.jpg                       (poster)
    show_root/backdrop.jpg                     (fanart)
    show_root/Season ##/folder.jpg             (season thumb)
    show_root/Season ##/filename.ext           (*)
    show_root/Season ##/metadata/filename.xml  (episode metadata)
    show_root/Season ##/metadata/filename.jpg  (episode thumb)
    """

    def __init__(self,
                 show_metadata=False,
                 episode_metadata=False,
                 fanart=False,
                 poster=False,
                 banner=False,
                 episode_thumbnails=False,
                 season_posters=False,
                 season_banners=False,
                 season_all_poster=False,
                 season_all_banner=False):

        generic.GenericMetadata.__init__(self,
                                         show_metadata,
                                         episode_metadata,
                                         fanart,
                                         poster,
                                         banner,
                                         episode_thumbnails,
                                         season_posters,
                                         season_banners,
                                         season_all_poster,
                                         season_all_banner)

        self.name = "MediaBrowser"

        self._ep_nfo_extension = "xml"
        self._show_metadata_filename = "series.xml"

        self.fanart_name = "backdrop.jpg"
        self.poster_name = "folder.jpg"

        # web-ui metadata template
        self.eg_show_metadata = "series.xml"
        self.eg_episode_metadata = "Season##\\metadata\\<i>filename</i>.xml"
        self.eg_fanart = "backdrop.jpg"
        self.eg_poster = "folder.jpg"
        self.eg_banner = "banner.jpg"
        self.eg_episode_thumbnails = "Season##\\metadata\\<i>filename</i>.jpg"
        self.eg_season_posters = "Season##\\folder.jpg"
        self.eg_season_banners = "Season##\\banner.jpg"
        self.eg_season_all_poster = "<i>not supported</i>"
        self.eg_season_all_banner = "<i>not supported</i>"

    # Override with empty methods for unsupported features
    def retrieveShowMetadata(self, folder):
        # while show metadata is generated, it is not supported for our lookup
        return (None, None)

    def create_season_all_poster(self, show_obj):
        pass

    def create_season_all_banner(self, show_obj):
        pass

    def get_episode_file_path(self, ep_obj):
        """
        Returns a full show dir/metadata/episode.xml path for MediaBrowser
        episode metadata files

        ep_obj: a TVEpisode object to get the path for
        """

        if ek.ek(os.path.isfile, ep_obj.location):
            xml_file_name = helpers.replaceExtension(ek.ek(os.path.basename, ep_obj.location), self._ep_nfo_extension)
            metadata_dir_name = ek.ek(os.path.join, ek.ek(os.path.dirname, ep_obj.location), 'metadata')
            xml_file_path = ek.ek(os.path.join, metadata_dir_name, xml_file_name)
        else:
            logger.log(u"Episode location doesn't exist: " + str(ep_obj.location), logger.DEBUG)
            return ''

        return xml_file_path

    def get_episode_thumb_path(self, ep_obj):
        """
        Returns a full show dir/metadata/episode.jpg path for MediaBrowser
        episode thumbs.

        ep_obj: a TVEpisode object to get the path from
        """

        if ek.ek(os.path.isfile, ep_obj.location):
            tbn_file_name = helpers.replaceExtension(ek.ek(os.path.basename, ep_obj.location), 'jpg')
            metadata_dir_name = ek.ek(os.path.join, ek.ek(os.path.dirname, ep_obj.location), 'metadata')
            tbn_file_path = ek.ek(os.path.join, metadata_dir_name, tbn_file_name)
        else:
            return None

        return tbn_file_path

    def get_season_poster_path(self, show_obj, season):
        """
        Season thumbs for MediaBrowser go in Show Dir/Season X/folder.jpg

        If no season folder exists, None is returned
        """

        dir_list = [x for x in ek.ek(os.listdir, show_obj.location) if ek.ek(os.path.isdir, ek.ek(os.path.join, show_obj.location, x))]

        season_dir_regex = '^Season\s+(\d+)$'

        season_dir = None

        for cur_dir in dir_list:
            # MediaBrowser 1.x only supports 'Specials'
            # MediaBrowser 2.x looks to only support 'Season 0'
            # MediaBrowser 3.x looks to mimic XBMC/Plex support
            if season == 0 and cur_dir == "Specials":
                season_dir = cur_dir
                break

            match = re.match(season_dir_regex, cur_dir, re.I)
            if not match:
                continue

            cur_season = int(match.group(1))

            if cur_season == season:
                season_dir = cur_dir
                break

        if not season_dir:
            logger.log(u"Unable to find a season dir for season " + str(season), logger.DEBUG)
            return None

        logger.log(u"Using " + str(season_dir) + "/folder.jpg as season dir for season " + str(season), logger.DEBUG)

        return ek.ek(os.path.join, show_obj.location, season_dir, 'folder.jpg')

    def get_season_banner_path(self, show_obj, season):
        """
        Season thumbs for MediaBrowser go in Show Dir/Season X/banner.jpg

        If no season folder exists, None is returned
        """

        dir_list = [x for x in ek.ek(os.listdir, show_obj.location) if ek.ek(os.path.isdir, ek.ek(os.path.join, show_obj.location, x))]

        season_dir_regex = '^Season\s+(\d+)$'

        season_dir = None

        for cur_dir in dir_list:
            # MediaBrowser 1.x only supports 'Specials'
            # MediaBrowser 2.x looks to only support 'Season 0'
            # MediaBrowser 3.x looks to mimic XBMC/Plex support
            if season == 0 and cur_dir == "Specials":
                season_dir = cur_dir
                break

            match = re.match(season_dir_regex, cur_dir, re.I)
            if not match:
                continue

            cur_season = int(match.group(1))

            if cur_season == season:
                season_dir = cur_dir
                break

        if not season_dir:
            logger.log(u"Unable to find a season dir for season " + str(season), logger.DEBUG)
            return None

        logger.log(u"Using " + str(season_dir) + "/banner.jpg as season dir for season " + str(season), logger.DEBUG)

        return ek.ek(os.path.join, show_obj.location, season_dir, 'banner.jpg')

    def _show_data(self, show_obj):
        """
        Creates an elementTree XML structure for a MediaBrowser-style series.xml
        returns the resulting data object.

        show_obj: a TVShow instance to create the NFO for
        """

        tvdb_lang = show_obj.lang
        # There's gotta be a better way of doing this but we don't wanna
        # change the language value elsewhere
        ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

        if tvdb_lang and not tvdb_lang == 'en':
            ltvdb_api_parms['language'] = tvdb_lang

        t = tvdb_api.Tvdb(actors=True, **ltvdb_api_parms)

        tv_node = etree.Element("Series")

        try:
            myShow = t[int(show_obj.tvdbid)]
        except tvdb_exceptions.tvdb_shownotfound:
            logger.log(u"Unable to find show with id " + str(show_obj.tvdbid) + " on tvdb, skipping it", logger.ERROR)
            raise

        except tvdb_exceptions.tvdb_error:
            logger.log(u"TVDB is down, can't use its data to make the NFO", logger.ERROR)
            raise

        # check for title and id
        try:
            if myShow['seriesname'] == None or myShow['seriesname'] == "" or myShow['id'] == None or myShow['id'] == "":
                logger.log(u"Incomplete info for show with id " + str(show_obj.tvdbid) + " on tvdb, skipping it", logger.ERROR)
                return False
        except tvdb_exceptions.tvdb_attributenotfound:
            logger.log(u"Incomplete info for show with id " + str(show_obj.tvdbid) + " on tvdb, skipping it", logger.ERROR)
            return False

        tvdbid = etree.SubElement(tv_node, "id")
        if myShow['id'] != None:
            tvdbid.text = myShow['id']

        SeriesName = etree.SubElement(tv_node, "SeriesName")
        if myShow['seriesname'] != None:
            SeriesName.text = myShow['seriesname']

        Status = etree.SubElement(tv_node, "Status")
        if myShow['status'] != None:
            Status.text = myShow['status']

        Network = etree.SubElement(tv_node, "Network")
        if myShow['network'] != None:
            Network.text = myShow['network']

        Airs_Time = etree.SubElement(tv_node, "Airs_Time")
        if myShow['airs_time'] != None:
            Airs_Time.text = myShow['airs_time']

        Airs_DayOfWeek = etree.SubElement(tv_node, "Airs_DayOfWeek")
        if myShow['airs_dayofweek'] != None:
            Airs_DayOfWeek.text = myShow['airs_dayofweek']

        FirstAired = etree.SubElement(tv_node, "FirstAired")
        if myShow['firstaired'] != None:
            FirstAired.text = myShow['firstaired']

        ContentRating = etree.SubElement(tv_node, "ContentRating")
        MPAARating = etree.SubElement(tv_node, "MPAARating")
        certification = etree.SubElement(tv_node, "certification")
        if myShow['contentrating'] != None:
            ContentRating.text = myShow['contentrating']
            MPAARating.text = myShow['contentrating']
            certification.text = myShow['contentrating']

        MetadataType = etree.SubElement(tv_node, "Type")
        MetadataType.text = "Series"

        Overview = etree.SubElement(tv_node, "Overview")
        if myShow['overview'] != None:
            Overview.text = myShow['overview']

        PremiereDate = etree.SubElement(tv_node, "PremiereDate")
        if myShow['firstaired'] != None:
            PremiereDate.text = myShow['firstaired']

        Rating = etree.SubElement(tv_node, "Rating")
        if myShow['rating'] != None:
            Rating.text = myShow['rating']

        ProductionYear = etree.SubElement(tv_node, "ProductionYear")
        if myShow['firstaired'] != None:
            try:
                year_text = str(datetime.datetime.strptime(myShow['firstaired'], '%Y-%m-%d').year)
                if year_text:
                    ProductionYear.text = year_text
            except:
                pass

        RunningTime = etree.SubElement(tv_node, "RunningTime")
        Runtime = etree.SubElement(tv_node, "Runtime")
        if myShow['runtime'] != None:
            RunningTime.text = myShow['runtime']
            Runtime.text = myShow['runtime']

        IMDB_ID = etree.SubElement(tv_node, "IMDB_ID")
        IMDB = etree.SubElement(tv_node, "IMDB")
        IMDbId = etree.SubElement(tv_node, "IMDbId")
        if myShow['imdb_id'] != None:
            IMDB_ID.text = myShow['imdb_id']
            IMDB.text = myShow['imdb_id']
            IMDbId.text = myShow['imdb_id']

        Zap2ItId = etree.SubElement(tv_node, "Zap2ItId")
        if myShow['zap2it_id'] != None:
            Zap2ItId.text = myShow['zap2it_id']

        Genres = etree.SubElement(tv_node, "Genres")
        if myShow["genre"] != None:
            for genre in myShow['genre'].split('|'):
                if genre and genre.strip():
                    cur_genre = etree.SubElement(Genres, "Genre")
                    cur_genre.text = genre.strip()

        Genre = etree.SubElement(tv_node, "Genre")
        if myShow["genre"] != None:
            Genre.text = "|".join([x.strip() for x in myShow["genre"].split('|') if x and x.strip()])

        Studios = etree.SubElement(tv_node, "Studios")
        Studio = etree.SubElement(Studios, "Studio")
        if myShow["network"] != None:
            Studio.text = myShow['network']

        Persons = etree.SubElement(tv_node, "Persons")

        if myShow["_actors"] != None:
            for actor in myShow["_actors"]:
                cur_actor_name_text = actor['name']

                if cur_actor_name_text != None and cur_actor_name_text.strip():
                    cur_actor = etree.SubElement(Persons, "Person")
                    cur_actor_name = etree.SubElement(cur_actor, "Name")
                    cur_actor_name.text = cur_actor_name_text.strip()

                    cur_actor_type = etree.SubElement(cur_actor, "Type")
                    cur_actor_type.text = "Actor"

                    cur_actor_role = etree.SubElement(cur_actor, "Role")
                    cur_actor_role_text = actor['role']
                    if cur_actor_role_text != None:
                        cur_actor_role.text = cur_actor_role_text

        helpers.indentXML(tv_node)

        data = etree.ElementTree(tv_node)

        return data

    def _ep_data(self, ep_obj):
        """
        Creates an elementTree XML structure for a MediaBrowser style episode.xml
        and returns the resulting data object.

        show_obj: a TVShow instance to create the NFO for
        """

        eps_to_write = [ep_obj] + ep_obj.relatedEps

        persons_dict = {}
        persons_dict['Director'] = []
        persons_dict['GuestStar'] = []
        persons_dict['Writer'] = []

        tvdb_lang = ep_obj.show.lang

        try:
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if tvdb_lang and not tvdb_lang == 'en':
                ltvdb_api_parms['language'] = tvdb_lang

            t = tvdb_api.Tvdb(actors=True, **ltvdb_api_parms)
            myShow = t[ep_obj.show.tvdbid]
        except tvdb_exceptions.tvdb_shownotfound, e:
            raise exceptions.ShowNotFoundException(e.message)
        except tvdb_exceptions.tvdb_error, e:
            logger.log(u"Unable to connect to TVDB while creating meta files - skipping - " + ex(e), logger.ERROR)
            return False

        rootNode = etree.Element("Item")

        # write an MediaBrowser XML containing info for all matching episodes
        for curEpToWrite in eps_to_write:

            try:
                myEp = myShow[curEpToWrite.season][curEpToWrite.episode]
            except (tvdb_exceptions.tvdb_episodenotfound, tvdb_exceptions.tvdb_seasonnotfound):
                logger.log(u"Unable to find episode " + str(curEpToWrite.season) + "x" + str(curEpToWrite.episode) + " on tvdb... has it been removed? Should I delete from db?")
                return None

            if curEpToWrite == ep_obj:
                # root (or single) episode

                # default to today's date for specials if firstaired is not set
                if myEp['firstaired'] == None and ep_obj.season == 0:
                    myEp['firstaired'] = str(datetime.date.fromordinal(1))

                if myEp['episodename'] == None or myEp['firstaired'] == None:
                    return None

                episode = rootNode

                EpisodeName = etree.SubElement(episode, "EpisodeName")
                if curEpToWrite.name != None:
                    EpisodeName.text = curEpToWrite.name
                else:
                    EpisodeName.text = ""

                EpisodeNumber = etree.SubElement(episode, "EpisodeNumber")
                EpisodeNumber.text = str(ep_obj.episode)

                if ep_obj.relatedEps:
                    EpisodeNumberEnd = etree.SubElement(episode, "EpisodeNumberEnd")
                    EpisodeNumberEnd.text = str(curEpToWrite.episode)

                SeasonNumber = etree.SubElement(episode, "SeasonNumber")
                SeasonNumber.text = str(curEpToWrite.season)

                if not ep_obj.relatedEps:
                    absolute_number = etree.SubElement(episode, "absolute_number")
                    absolute_number.text = myEp['absolute_number']

                FirstAired = etree.SubElement(episode, "FirstAired")
                if curEpToWrite.airdate != datetime.date.fromordinal(1):
                    FirstAired.text = str(curEpToWrite.airdate)
                else:
                    FirstAired.text = ""

                MetadataType = etree.SubElement(episode, "Type")
                MetadataType.text = "Episode"

                Overview = etree.SubElement(episode, "Overview")
                if curEpToWrite.description != None:
                    Overview.text = curEpToWrite.description
                else:
                    Overview.text = ""

                if not ep_obj.relatedEps:
                    Rating = etree.SubElement(episode, "Rating")
                    rating_text = myEp['rating']
                    if rating_text != None:
                        Rating.text = rating_text

                    IMDB_ID = etree.SubElement(episode, "IMDB_ID")
                    IMDB = etree.SubElement(episode, "IMDB")
                    IMDbId = etree.SubElement(episode, "IMDbId")
                    if myShow['imdb_id'] != None:
                        IMDB_ID.text = myShow['imdb_id']
                        IMDB.text = myShow['imdb_id']
                        IMDbId.text = myShow['imdb_id']

                TvDbId = etree.SubElement(episode, "TvDbId")
                TvDbId.text = str(curEpToWrite.tvdbid)

                Persons = etree.SubElement(episode, "Persons")

                Language = etree.SubElement(episode, "Language")
                Language.text = myEp['language']

                thumb = etree.SubElement(episode, "filename")
                # TODO: See what this is needed for.. if its still needed
                # just write this to the NFO regardless of whether it actually exists or not
                # note: renaming files after nfo generation will break this, tough luck
                thumb_text = self.get_episode_thumb_path(ep_obj)
                if thumb_text:
                    thumb.text = thumb_text

            else:
                # append data from (if any) related episodes
                EpisodeNumberEnd.text = str(curEpToWrite.episode)

                if curEpToWrite.name:
                    if not EpisodeName.text:
                        EpisodeName.text = curEpToWrite.name
                    else:
                        EpisodeName.text = EpisodeName.text + ", " + curEpToWrite.name

                if curEpToWrite.description:
                    if not Overview.text:
                        Overview.text = curEpToWrite.description
                    else:
                        Overview.text = Overview.text + "\r" + curEpToWrite.description

            # collect all directors, guest stars and writers
            if myEp['director']:
                persons_dict['Director'] += [x.strip() for x in myEp['director'].split('|') if x and x.strip()]
            if myEp['gueststars']:
                persons_dict['GuestStar'] += [x.strip() for x in myEp['gueststars'].split('|') if x and x.strip()]
            if myEp['writer']:
                persons_dict['Writer'] += [x.strip() for x in myEp['writer'].split('|') if x and x.strip()]

        # fill in Persons section with collected directors, guest starts and writers
        for person_type, names in persons_dict.iteritems():
            # remove doubles
            names = list(set(names))
            for cur_name in names:
                Person = etree.SubElement(Persons, "Person")
                cur_person_name = etree.SubElement(Person, "Name")
                cur_person_name.text = cur_name
                cur_person_type = etree.SubElement(Person, "Type")
                cur_person_type.text = person_type

        helpers.indentXML(rootNode)
        data = etree.ElementTree(rootNode)

        return data


# present a standard "interface" from the module
metadata_class = MediaBrowserMetadata

########NEW FILE########
__FILENAME__ = ps3
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import os

import generic

from sickbeard import encodingKludge as ek


class PS3Metadata(generic.GenericMetadata):
    """
    Metadata generation class for Sony PS3.

    The following file structure is used:

    show_root/cover.jpg                         (poster)
    show_root/Season ##/filename.ext            (*)
    show_root/Season ##/filename.ext.cover.jpg  (episode thumb)
    """

    def __init__(self,
                 show_metadata=False,
                 episode_metadata=False,
                 fanart=False,
                 poster=False,
                 banner=False,
                 episode_thumbnails=False,
                 season_posters=False,
                 season_banners=False,
                 season_all_poster=False,
                 season_all_banner=False):

        generic.GenericMetadata.__init__(self,
                                         show_metadata,
                                         episode_metadata,
                                         fanart,
                                         poster,
                                         banner,
                                         episode_thumbnails,
                                         season_posters,
                                         season_banners,
                                         season_all_poster,
                                         season_all_banner)

        self.name = "Sony PS3"

        self.poster_name = "cover.jpg"

        # web-ui metadata template
        self.eg_show_metadata = "<i>not supported</i>"
        self.eg_episode_metadata = "<i>not supported</i>"
        self.eg_fanart = "<i>not supported</i>"
        self.eg_poster = "cover.jpg"
        self.eg_banner = "<i>not supported</i>"
        self.eg_episode_thumbnails = "Season##\\<i>filename</i>.ext.cover.jpg"
        self.eg_season_posters = "<i>not supported</i>"
        self.eg_season_banners = "<i>not supported</i>"
        self.eg_season_all_poster = "<i>not supported</i>"
        self.eg_season_all_banner = "<i>not supported</i>"

    # Override with empty methods for unsupported features
    def retrieveShowMetadata(self, folder):
        # no show metadata generated, we abort this lookup function
        return (None, None)

    def create_show_metadata(self, show_obj):
        pass

    def get_show_file_path(self, show_obj):
        pass

    def create_episode_metadata(self, ep_obj):
        pass

    def create_fanart(self, show_obj):
        pass

    def create_banner(self, show_obj):
        pass

    def create_season_posters(self, show_obj):
        pass

    def create_season_banners(self, ep_obj):
        pass

    def create_season_all_poster(self, show_obj):
        pass

    def create_season_all_banner(self, show_obj):
        pass

    def get_episode_thumb_path(self, ep_obj):
        """
        Returns the path where the episode thumbnail should be stored. Defaults to
        the same path as the episode file but with a .cover.jpg extension.

        ep_obj: a TVEpisode instance for which to create the thumbnail
        """
        if ek.ek(os.path.isfile, ep_obj.location):
            tbn_filename = ep_obj.location + ".cover.jpg"
        else:
            return None

        return tbn_filename


# present a standard "interface" from the module
metadata_class = PS3Metadata

########NEW FILE########
__FILENAME__ = tivo
# Author: Nic Wolfe <nic@wolfeden.ca>
# Author: Gordon Turner <gordonturner@gordonturner.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import datetime
import os

import sickbeard

from sickbeard import logger, exceptions, helpers
from sickbeard.metadata import generic
from sickbeard import encodingKludge as ek
from sickbeard.exceptions import ex

from lib.tvdb_api import tvdb_api, tvdb_exceptions


class TIVOMetadata(generic.GenericMetadata):
    """
    Metadata generation class for TIVO

    The following file structure is used:

    show_root/Season ##/filename.ext            (*)
    show_root/Season ##/.meta/filename.ext.txt  (episode metadata)

    This class only generates episode specific metadata files, it does NOT generate a default.txt file.
    """

    def __init__(self,
                 show_metadata=False,
                 episode_metadata=False,
                 fanart=False,
                 poster=False,
                 banner=False,
                 episode_thumbnails=False,
                 season_posters=False,
                 season_banners=False,
                 season_all_poster=False,
                 season_all_banner=False):

        generic.GenericMetadata.__init__(self,
                                         show_metadata,
                                         episode_metadata,
                                         fanart,
                                         poster,
                                         banner,
                                         episode_thumbnails,
                                         season_posters,
                                         season_banners,
                                         season_all_poster,
                                         season_all_banner)

        self.name = 'TIVO'

        self._ep_nfo_extension = "txt"

        # web-ui metadata template
        self.eg_show_metadata = "<i>not supported</i>"
        self.eg_episode_metadata = "Season##\\.meta\\<i>filename</i>.ext.txt"
        self.eg_fanart = "<i>not supported</i>"
        self.eg_poster = "<i>not supported</i>"
        self.eg_banner = "<i>not supported</i>"
        self.eg_episode_thumbnails = "<i>not supported</i>"
        self.eg_season_posters = "<i>not supported</i>"
        self.eg_season_banners = "<i>not supported</i>"
        self.eg_season_all_poster = "<i>not supported</i>"
        self.eg_season_all_banner = "<i>not supported</i>"

    # Override with empty methods for unsupported features
    def retrieveShowMetadata(self, folder):
        # no show metadata generated, we abort this lookup function
        return (None, None)

    def create_show_metadata(self, show_obj):
        pass

    def get_show_file_path(self, show_obj):
        pass

    def create_fanart(self, show_obj):
        pass

    def create_poster(self, show_obj):
        pass

    def create_banner(self, show_obj):
        pass

    def create_episode_thumb(self, ep_obj):
        pass

    def get_episode_thumb_path(self, ep_obj):
        pass

    def create_season_posters(self, ep_obj):
        pass

    def create_season_banners(self, ep_obj):
        pass

    def create_season_all_poster(self, show_obj):
        pass

    def create_season_all_banner(self, show_obj):
        pass

    # Override generic class
    def get_episode_file_path(self, ep_obj):
        """
        Returns a full show dir/.meta/episode.txt path for Tivo
        episode metadata files.

        Note, that pyTivo requires the metadata filename to include the original extention.

        ie If the episode name is foo.avi, the metadata name is foo.avi.txt

        ep_obj: a TVEpisode object to get the path for
        """
        if ek.ek(os.path.isfile, ep_obj.location):
            metadata_file_name = ek.ek(os.path.basename, ep_obj.location) + "." + self._ep_nfo_extension
            metadata_dir_name = ek.ek(os.path.join, ek.ek(os.path.dirname, ep_obj.location), '.meta')
            metadata_file_path = ek.ek(os.path.join, metadata_dir_name, metadata_file_name)
        else:
            logger.log(u"Episode location doesn't exist: " + str(ep_obj.location), logger.DEBUG)
            return ''
        return metadata_file_path

    def _ep_data(self, ep_obj):
        """
        Creates a key value structure for a Tivo episode metadata file and
        returns the resulting data object.

        ep_obj: a TVEpisode instance to create the metadata file for.

        Lookup the show in http://thetvdb.com/ using the python library:

        https://github.com/dbr/tvdb_api/

        The results are saved in the object myShow.

        The key values for the tivo metadata file are from:

        http://pytivo.sourceforge.net/wiki/index.php/Metadata
        """

        data = ""

        eps_to_write = [ep_obj] + ep_obj.relatedEps

        tvdb_lang = ep_obj.show.lang

        try:
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if tvdb_lang and not tvdb_lang == 'en':
                ltvdb_api_parms['language'] = tvdb_lang

            t = tvdb_api.Tvdb(actors=True, **ltvdb_api_parms)
            myShow = t[ep_obj.show.tvdbid]
        except tvdb_exceptions.tvdb_shownotfound, e:
            raise exceptions.ShowNotFoundException(str(e))
        except tvdb_exceptions.tvdb_error, e:
            logger.log(u"Unable to connect to TVDB while creating meta files - skipping - " + str(e), logger.ERROR)
            return False

        for curEpToWrite in eps_to_write:

            try:
                myEp = myShow[curEpToWrite.season][curEpToWrite.episode]
            except (tvdb_exceptions.tvdb_episodenotfound, tvdb_exceptions.tvdb_seasonnotfound):
                logger.log(u"Unable to find episode " + str(curEpToWrite.season) + "x" + str(curEpToWrite.episode) + " on tvdb... has it been removed? Should I delete from db?")
                return None

            if myEp["firstaired"] == None and ep_obj.season == 0:
                myEp["firstaired"] = str(datetime.date.fromordinal(1))

            if myEp["episodename"] == None or myEp["firstaired"] == None:
                return None

            if myShow["seriesname"] != None:
                data += ("title : " + myShow["seriesname"] + "\n")
                data += ("seriesTitle : " + myShow["seriesname"] + "\n")

            data += ("episodeTitle : " + curEpToWrite._format_pattern('%Sx%0E %EN') + "\n")

            # This should be entered for episodic shows and omitted for movies. The standard tivo format is to enter
            # the season number followed by the episode number for that season. For example, enter 201 for season 2
            # episode 01.

            # This only shows up if you go into the Details from the Program screen.

            # This seems to disappear once the video is transferred to TiVo.

            # NOTE: May not be correct format, missing season, but based on description from wiki leaving as is.
            data += ("episodeNumber : " + str(curEpToWrite.episode) + "\n")

            # Must be entered as true or false. If true, the year from originalAirDate will be shown in parentheses
            # after the episode's title and before the description on the Program screen.

            # FIXME: Hardcode isEpisode to true for now, not sure how to handle movies
            data += ("isEpisode : true\n")

            # Write the synopsis of the video here
            # Micrsoft Word's smartquotes can die in a fire.
            sanitizedDescription = curEpToWrite.description
            # Replace double curly quotes
            sanitizedDescription = sanitizedDescription.replace(u"\u201c", "\"").replace(u"\u201d", "\"")
            # Replace single curly quotes
            sanitizedDescription = sanitizedDescription.replace(u"\u2018", "'").replace(u"\u2019", "'").replace(u"\u02BC", "'")

            data += ("description : " + sanitizedDescription + "\n")

            # Usually starts with "SH" and followed by 6-8 digits.
            # Tivo uses zap2it for thier data, so the series id is the zap2it_id.
            if myShow["zap2it_id"] != None:
                data += ("seriesId : " + myShow["zap2it_id"] + "\n")

            # This is the call sign of the channel the episode was recorded from.
            if myShow["network"] != None:
                data += ("callsign : " + myShow["network"] + "\n")

            # This must be entered as yyyy-mm-ddThh:mm:ssZ (the t is capitalized and never changes, the Z is also
            # capitalized and never changes). This is the original air date of the episode.
            # NOTE: Hard coded the time to T00:00:00Z as we really don't know when during the day the first run happened.
            if curEpToWrite.airdate != datetime.date.fromordinal(1):
                data += ("originalAirDate : " + str(curEpToWrite.airdate) + "T00:00:00Z\n")

            # This shows up at the beginning of the description on the Program screen and on the Details screen.
            if myShow["actors"]:
                for actor in myShow["actors"].split('|'):
                    if actor != None and actor.strip():
                        data += ("vActor : " + actor.strip() + "\n")

            # This is shown on both the Program screen and the Details screen.
            if myEp["rating"] != None:
                try:
                    rating = float(myEp['rating'])
                except ValueError:
                    rating = 0.0
                # convert 10 to 4 star rating. 4 * rating / 10
                # only whole numbers or half numbers work. multiply by 2, round, divide by 2.0
                rating = round(8 * rating / 10) / 2.0
                data += ("starRating : " + str(rating) + "\n")

            # This is shown on both the Program screen and the Details screen.
            # It uses the standard TV rating system of: TV-Y7, TV-Y, TV-G, TV-PG, TV-14, TV-MA and TV-NR.
            if myShow["contentrating"]:
                data += ("tvRating : " + str(myShow["contentrating"]) + "\n")

            # This field can be repeated as many times as necessary or omitted completely.
            if ep_obj.show.genre:
                for genre in ep_obj.show.genre.split('|'):
                    if genre and genre.strip():
                        data += ("vProgramGenre : " + str(genre.strip()) + "\n")

            # NOTE: The following are metadata keywords are not used
            # displayMajorNumber
            # showingBits
            # displayMinorNumber
            # colorCode
            # vSeriesGenre
            # vGuestStar, vDirector, vExecProducer, vProducer, vWriter, vHost, vChoreographer
            # partCount
            # partIndex

        return data

    def write_ep_file(self, ep_obj):
        """
        Generates and writes ep_obj's metadata under the given path with the
        given filename root. Uses the episode's name with the extension in
        _ep_nfo_extension.

        ep_obj: TVEpisode object for which to create the metadata

        file_name_path: The file name to use for this metadata. Note that the extension
                will be automatically added based on _ep_nfo_extension. This should
                include an absolute path.
        """
        data = self._ep_data(ep_obj)

        if not data:
            return False

        nfo_file_path = self.get_episode_file_path(ep_obj)
        nfo_file_dir = ek.ek(os.path.dirname, nfo_file_path)

        try:
            if not ek.ek(os.path.isdir, nfo_file_dir):
                logger.log(u"Metadata dir didn't exist, creating it at " + nfo_file_dir, logger.DEBUG)
                ek.ek(os.makedirs, nfo_file_dir)
                helpers.chmodAsParent(nfo_file_dir)

            logger.log(u"Writing episode nfo file to " + nfo_file_path, logger.DEBUG)

            with ek.ek(open, nfo_file_path, 'w') as nfo_file:
                # Calling encode directly, b/c often descriptions have wonky characters.
                nfo_file.write(data.encode("utf-8"))

            helpers.chmodAsParent(nfo_file_path)

        except EnvironmentError, e:
            logger.log(u"Unable to write file to " + nfo_file_path + " - are you sure the folder is writable? " + ex(e), logger.ERROR)
            return False

        return True


# present a standard "interface" from the module
metadata_class = TIVOMetadata

########NEW FILE########
__FILENAME__ = wdtv
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import os
import re

import sickbeard

import generic

from sickbeard import logger, exceptions, helpers
from sickbeard import encodingKludge as ek
from lib.tvdb_api import tvdb_api, tvdb_exceptions
from sickbeard.exceptions import ex

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree


class WDTVMetadata(generic.GenericMetadata):
    """
    Metadata generation class for WDTV

    The following file structure is used:

    show_root/folder.jpg                    (poster)
    show_root/Season ##/folder.jpg          (season thumb)
    show_root/Season ##/filename.ext        (*)
    show_root/Season ##/filename.metathumb  (episode thumb)
    show_root/Season ##/filename.xml        (episode metadata)
    """

    def __init__(self,
                 show_metadata=False,
                 episode_metadata=False,
                 fanart=False,
                 poster=False,
                 banner=False,
                 episode_thumbnails=False,
                 season_posters=False,
                 season_banners=False,
                 season_all_poster=False,
                 season_all_banner=False):

        generic.GenericMetadata.__init__(self,
                                         show_metadata,
                                         episode_metadata,
                                         fanart,
                                         poster,
                                         banner,
                                         episode_thumbnails,
                                         season_posters,
                                         season_banners,
                                         season_all_poster,
                                         season_all_banner)

        self.name = 'WDTV'

        self._ep_nfo_extension = 'xml'

        self.poster_name = "folder.jpg"

        # web-ui metadata template
        self.eg_show_metadata = "<i>not supported</i>"
        self.eg_episode_metadata = "Season##\\<i>filename</i>.xml"
        self.eg_fanart = "<i>not supported</i>"
        self.eg_poster = "folder.jpg"
        self.eg_banner = "<i>not supported</i>"
        self.eg_episode_thumbnails = "Season##\\<i>filename</i>.metathumb"
        self.eg_season_posters = "Season##\\folder.jpg"
        self.eg_season_banners = "<i>not supported</i>"
        self.eg_season_all_poster = "<i>not supported</i>"
        self.eg_season_all_banner = "<i>not supported</i>"

    # Override with empty methods for unsupported features
    def retrieveShowMetadata(self, folder):
        # no show metadata generated, we abort this lookup function
        return (None, None)

    def create_show_metadata(self, show_obj):
        pass

    def get_show_file_path(self, show_obj):
        pass

    def create_fanart(self, show_obj):
        pass

    def create_banner(self, show_obj):
        pass

    def create_season_banners(self, show_obj):
        pass

    def create_season_all_poster(self, show_obj):
        pass

    def create_season_all_banner(self, show_obj):
        pass

    def get_episode_thumb_path(self, ep_obj):
        """
        Returns the path where the episode thumbnail should be stored. Defaults to
        the same path as the episode file but with a .metathumb extension.

        ep_obj: a TVEpisode instance for which to create the thumbnail
        """
        if ek.ek(os.path.isfile, ep_obj.location):
            tbn_filename = helpers.replaceExtension(ep_obj.location, 'metathumb')
        else:
            return None

        return tbn_filename

    def get_season_poster_path(self, show_obj, season):
        """
        Season thumbs for WDTV go in Show Dir/Season X/folder.jpg

        If no season folder exists, None is returned
        """

        dir_list = [x for x in ek.ek(os.listdir, show_obj.location) if ek.ek(os.path.isdir, ek.ek(os.path.join, show_obj.location, x))]

        season_dir_regex = '^Season\s+(\d+)$'

        season_dir = None

        for cur_dir in dir_list:
            if season == 0 and cur_dir == "Specials":
                season_dir = cur_dir
                break

            match = re.match(season_dir_regex, cur_dir, re.I)
            if not match:
                continue

            cur_season = int(match.group(1))

            if cur_season == season:
                season_dir = cur_dir
                break

        if not season_dir:
            logger.log(u"Unable to find a season dir for season " + str(season), logger.DEBUG)
            return None

        logger.log(u"Using " + str(season_dir) + "/folder.jpg as season dir for season " + str(season), logger.DEBUG)

        return ek.ek(os.path.join, show_obj.location, season_dir, 'folder.jpg')

    def _ep_data(self, ep_obj):
        """
        Creates an elementTree XML structure for a WDTV style episode.xml
        and returns the resulting data object.

        ep_obj: a TVShow instance to create the NFO for
        """

        eps_to_write = [ep_obj] + ep_obj.relatedEps

        tvdb_lang = ep_obj.show.lang

        try:
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if tvdb_lang and not tvdb_lang == 'en':
                ltvdb_api_parms['language'] = tvdb_lang

            t = tvdb_api.Tvdb(actors=True, **ltvdb_api_parms)
            myShow = t[ep_obj.show.tvdbid]
        except tvdb_exceptions.tvdb_shownotfound, e:
            raise exceptions.ShowNotFoundException(e.message)
        except tvdb_exceptions.tvdb_error, e:
            logger.log(u"Unable to connect to TVDB while creating meta files - skipping - " + ex(e), logger.ERROR)
            return False

        rootNode = etree.Element("details")

        # write an WDTV XML containing info for all matching episodes
        for curEpToWrite in eps_to_write:

            try:
                myEp = myShow[curEpToWrite.season][curEpToWrite.episode]
            except (tvdb_exceptions.tvdb_episodenotfound, tvdb_exceptions.tvdb_seasonnotfound):
                logger.log(u"Unable to find episode " + str(curEpToWrite.season) + "x" + str(curEpToWrite.episode) + " on tvdb... has it been removed? Should I delete from db?")
                return None

            if myEp["firstaired"] == None and ep_obj.season == 0:
                myEp["firstaired"] = str(datetime.date.fromordinal(1))

            if myEp["episodename"] == None or myEp["firstaired"] == None:
                return None

            if len(eps_to_write) > 1:
                episode = etree.SubElement(rootNode, "details")
            else:
                episode = rootNode

            # TODO: get right EpisodeID
            episodeID = etree.SubElement(episode, "id")
            episodeID.text = str(curEpToWrite.tvdbid)

            title = etree.SubElement(episode, "title")
            title.text = ep_obj.prettyName()

            seriesName = etree.SubElement(episode, "series_name")
            if myShow["seriesname"] != None:
                seriesName.text = myShow["seriesname"]

            episodeName = etree.SubElement(episode, "episode_name")
            if curEpToWrite.name != None:
                episodeName.text = curEpToWrite.name

            seasonNumber = etree.SubElement(episode, "season_number")
            seasonNumber.text = str(curEpToWrite.season)

            episodeNum = etree.SubElement(episode, "episode_number")
            episodeNum.text = str(curEpToWrite.episode)

            firstAired = etree.SubElement(episode, "firstaired")

            if curEpToWrite.airdate != datetime.date.fromordinal(1):
                firstAired.text = str(curEpToWrite.airdate)

            year = etree.SubElement(episode, "year")
            if myShow["firstaired"] != None:
                try:
                    year_text = str(datetime.datetime.strptime(myShow["firstaired"], '%Y-%m-%d').year)
                    if year_text:
                        year.text = year_text
                except:
                    pass

            runtime = etree.SubElement(episode, "runtime")
            if curEpToWrite.season != 0:
                if myShow["runtime"] != None:
                    runtime.text = myShow["runtime"]

            genre = etree.SubElement(episode, "genre")
            if myShow["genre"] != None:
                genre.text = " / ".join([x.strip() for x in myShow["genre"].split('|') if x and x.strip()])

            director = etree.SubElement(episode, "director")
            director_text = myEp['director']
            if director_text != None:
                director.text = director_text

            if myShow["_actors"] != None:
                for actor in myShow["_actors"]:
                    cur_actor_name_text = actor['name']

                    if cur_actor_name_text != None and cur_actor_name_text.strip():
                        cur_actor = etree.SubElement(episode, "actor")
                        cur_actor_name = etree.SubElement(cur_actor, "name")
                        cur_actor_name.text = cur_actor_name_text.strip()

                        cur_actor_role = etree.SubElement(cur_actor, "role")
                        cur_actor_role_text = actor['role']
                        if cur_actor_role_text != None:
                            cur_actor_role.text = cur_actor_role_text

            overview = etree.SubElement(episode, "overview")
            if curEpToWrite.description != None:
                overview.text = curEpToWrite.description

            # Make it purdy
            helpers.indentXML(rootNode)
            data = etree.ElementTree(rootNode)

        return data


# present a standard "interface" from the module
metadata_class = WDTVMetadata

########NEW FILE########
__FILENAME__ = xbmc
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import xbmc_12plus

import os

from sickbeard import helpers
from sickbeard import encodingKludge as ek


class XBMCMetadata(xbmc_12plus.XBMC_12PlusMetadata):
    """
    Metadata generation class for XBMC (legacy).

    The following file structure is used:

    show_root/tvshow.nfo              (show metadata)
    show_root/fanart.jpg              (fanart)
    show_root/folder.jpg              (poster)
    show_root/folder.jpg              (banner)
    show_root/Season ##/filename.ext  (*)
    show_root/Season ##/filename.nfo  (episode metadata)
    show_root/Season ##/filename.tbn  (episode thumb)
    show_root/season##.tbn            (season posters)
    show_root/season-all.tbn          (season all poster)
    """

    def __init__(self,
                 show_metadata=False,
                 episode_metadata=False,
                 fanart=False,
                 poster=False,
                 banner=False,
                 episode_thumbnails=False,
                 season_posters=False,
                 season_banners=False,
                 season_all_poster=False,
                 season_all_banner=False):

        xbmc_12plus.XBMC_12PlusMetadata.__init__(self,
                                         show_metadata,
                                         episode_metadata,
                                         fanart,
                                         poster,
                                         banner,
                                         episode_thumbnails,
                                         season_posters,
                                         season_banners,
                                         season_all_poster,
                                         season_all_banner)

        self.name = 'XBMC'

        self.poster_name = self.banner_name = "folder.jpg"
        self.season_all_poster_name = "season-all.tbn"

        # web-ui metadata template
        # self.eg_show_metadata = "tvshow.nfo"
        # self.eg_episode_metadata = "Season##\\<i>filename</i>.nfo"
        # self.eg_fanart = "fanart.jpg"
        self.eg_poster = "folder.jpg"
        self.eg_banner = "folder.jpg"
        self.eg_episode_thumbnails = "Season##\\<i>filename</i>.tbn"
        self.eg_season_posters = "season##.tbn"
        self.eg_season_banners = "<i>not supported</i>"
        self.eg_season_all_poster = "season-all.tbn"
        self.eg_season_all_banner = "<i>not supported</i>"

    # Override with empty methods for unsupported features
    def create_season_banners(self, ep_obj):
        pass

    def create_season_all_banner(self, show_obj):
        pass

    def get_episode_thumb_path(self, ep_obj):
        """
        Returns the path where the episode thumbnail should be stored. Defaults to
        the same path as the episode file but with a .tbn extension.

        ep_obj: a TVEpisode instance for which to create the thumbnail
        """
        if ek.ek(os.path.isfile, ep_obj.location):
            tbn_filename = helpers.replaceExtension(ep_obj.location, 'tbn')
        else:
            return None

        return tbn_filename

    def get_season_poster_path(self, show_obj, season):
        """
        Returns the full path to the file for a given season poster.

        show_obj: a TVShow instance for which to generate the path
        season: a season number to be used for the path. Note that season 0
                means specials.
        """

        # Our specials thumbnail is, well, special
        if season == 0:
            season_poster_filename = 'season-specials'
        else:
            season_poster_filename = 'season' + str(season).zfill(2)

        return ek.ek(os.path.join, show_obj.location, season_poster_filename + '.tbn')


# present a standard "interface" from the module
metadata_class = XBMCMetadata

########NEW FILE########
__FILENAME__ = xbmc_12plus
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import generic

import datetime

from lib.tvdb_api import tvdb_api, tvdb_exceptions

import sickbeard
from sickbeard import logger, exceptions, helpers
from sickbeard.exceptions import ex

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree


class XBMC_12PlusMetadata(generic.GenericMetadata):
    """
    Metadata generation class for XBMC 12+.

    The following file structure is used:

    show_root/tvshow.nfo                    (show metadata)
    show_root/fanart.jpg                    (fanart)
    show_root/poster.jpg                    (poster)
    show_root/banner.jpg                    (banner)
    show_root/Season ##/filename.ext        (*)
    show_root/Season ##/filename.nfo        (episode metadata)
    show_root/Season ##/filename-thumb.jpg  (episode thumb)
    show_root/season##-poster.jpg           (season posters)
    show_root/season##-banner.jpg           (season banners)
    show_root/season-all-poster.jpg         (season all poster)
    show_root/season-all-banner.jpg         (season all banner)
    """

    def __init__(self,
                 show_metadata=False,
                 episode_metadata=False,
                 fanart=False,
                 poster=False,
                 banner=False,
                 episode_thumbnails=False,
                 season_posters=False,
                 season_banners=False,
                 season_all_poster=False,
                 season_all_banner=False):

        generic.GenericMetadata.__init__(self,
                                         show_metadata,
                                         episode_metadata,
                                         fanart,
                                         poster,
                                         banner,
                                         episode_thumbnails,
                                         season_posters,
                                         season_banners,
                                         season_all_poster,
                                         season_all_banner)

        self.name = 'XBMC 12+'

        self.poster_name = "poster.jpg"
        self.season_all_poster_name = "season-all-poster.jpg"

        # web-ui metadata template
        self.eg_show_metadata = "tvshow.nfo"
        self.eg_episode_metadata = "Season##\\<i>filename</i>.nfo"
        self.eg_fanart = "fanart.jpg"
        self.eg_poster = "poster.jpg"
        self.eg_banner = "banner.jpg"
        self.eg_episode_thumbnails = "Season##\\<i>filename</i>-thumb.jpg"
        self.eg_season_posters = "season##-poster.jpg"
        self.eg_season_banners = "season##-banner.jpg"
        self.eg_season_all_poster = "season-all-poster.jpg"
        self.eg_season_all_banner = "season-all-banner.jpg"

    def _show_data(self, show_obj):
        """
        Creates an elementTree XML structure for an XBMC-style tvshow.nfo and
        returns the resulting data object.

        show_obj: a TVShow instance to create the NFO for
        """

        show_ID = show_obj.tvdbid

        tvdb_lang = show_obj.lang
        # There's gotta be a better way of doing this but we don't wanna
        # change the language value elsewhere
        ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

        if tvdb_lang and not tvdb_lang == 'en':
            ltvdb_api_parms['language'] = tvdb_lang

        t = tvdb_api.Tvdb(actors=True, **ltvdb_api_parms)

        tv_node = etree.Element("tvshow")

        try:
            myShow = t[int(show_ID)]
        except tvdb_exceptions.tvdb_shownotfound:
            logger.log(u"Unable to find show with id " + str(show_ID) + " on tvdb, skipping it", logger.ERROR)
            raise

        except tvdb_exceptions.tvdb_error:
            logger.log(u"TVDB is down, can't use its data to add this show", logger.ERROR)
            raise

        # check for title and id
        try:
            if myShow["seriesname"] == None or myShow["seriesname"] == "" or myShow["id"] == None or myShow["id"] == "":
                logger.log(u"Incomplete info for show with id " + str(show_ID) + " on tvdb, skipping it", logger.ERROR)

                return False
        except tvdb_exceptions.tvdb_attributenotfound:
            logger.log(u"Incomplete info for show with id " + str(show_ID) + " on tvdb, skipping it", logger.ERROR)

            return False

        title = etree.SubElement(tv_node, "title")
        if myShow["seriesname"] != None:
            title.text = myShow["seriesname"]

        rating = etree.SubElement(tv_node, "rating")
        if myShow["rating"] != None:
            rating.text = myShow["rating"]

        year = etree.SubElement(tv_node, "year")
        if myShow["firstaired"] != None:
            try:
                year_text = str(datetime.datetime.strptime(myShow["firstaired"], '%Y-%m-%d').year)
                if year_text:
                    year.text = year_text
            except:
                pass

        plot = etree.SubElement(tv_node, "plot")
        if myShow["overview"] != None:
            plot.text = myShow["overview"]

        episodeguide = etree.SubElement(tv_node, "episodeguide")
        episodeguideurl = etree.SubElement(episodeguide, "url")
        episodeguideurl2 = etree.SubElement(tv_node, "episodeguideurl")
        if myShow["id"] != None:
            showurl = sickbeard.TVDB_BASE_URL + '/series/' + myShow["id"] + '/all/en.zip'
            episodeguideurl.text = showurl
            episodeguideurl2.text = showurl

        mpaa = etree.SubElement(tv_node, "mpaa")
        if myShow["contentrating"] != None:
            mpaa.text = myShow["contentrating"]

        tvdbid = etree.SubElement(tv_node, "id")
        if myShow["id"] != None:
            tvdbid.text = myShow["id"]

        genre = etree.SubElement(tv_node, "genre")
        if myShow["genre"] != None:
            genre.text = " / ".join([x.strip() for x in myShow["genre"].split('|') if x and x.strip()])

        premiered = etree.SubElement(tv_node, "premiered")
        if myShow["firstaired"] != None:
            premiered.text = myShow["firstaired"]

        studio = etree.SubElement(tv_node, "studio")
        if myShow["network"] != None:
            studio.text = myShow["network"]

        if myShow["_actors"] != None:
            for actor in myShow["_actors"]:
                cur_actor_name_text = actor['name']

                if cur_actor_name_text != None and cur_actor_name_text.strip():
                    cur_actor = etree.SubElement(tv_node, "actor")
                    cur_actor_name = etree.SubElement(cur_actor, "name")
                    cur_actor_name.text = cur_actor_name_text.strip()

                    cur_actor_role = etree.SubElement(cur_actor, "role")
                    cur_actor_role_text = actor['role']
                    if cur_actor_role_text != None:
                        cur_actor_role.text = cur_actor_role_text

                    cur_actor_thumb = etree.SubElement(cur_actor, "thumb")
                    cur_actor_thumb_text = actor['image']
                    if cur_actor_thumb_text != None:
                        cur_actor_thumb.text = cur_actor_thumb_text

        # Make it purdy
        helpers.indentXML(tv_node)

        data = etree.ElementTree(tv_node)

        return data

    def _ep_data(self, ep_obj):
        """
        Creates an elementTree XML structure for an XBMC-style episode.nfo and
        returns the resulting data object.
            show_obj: a TVEpisode instance to create the NFO for
        """

        eps_to_write = [ep_obj] + ep_obj.relatedEps

        tvdb_lang = ep_obj.show.lang
        # There's gotta be a better way of doing this but we don't wanna
        # change the language value elsewhere
        ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

        if tvdb_lang and not tvdb_lang == 'en':
            ltvdb_api_parms['language'] = tvdb_lang

        try:
            t = tvdb_api.Tvdb(actors=True, **ltvdb_api_parms)
            myShow = t[ep_obj.show.tvdbid]
        except tvdb_exceptions.tvdb_shownotfound, e:
            raise exceptions.ShowNotFoundException(e.message)
        except tvdb_exceptions.tvdb_error, e:
            logger.log(u"Unable to connect to TVDB while creating meta files - skipping - " + ex(e), logger.ERROR)
            return

        if len(eps_to_write) > 1:
            rootNode = etree.Element("xbmcmultiepisode")
        else:
            rootNode = etree.Element("episodedetails")

        # write an NFO containing info for all matching episodes
        for curEpToWrite in eps_to_write:

            try:
                myEp = myShow[curEpToWrite.season][curEpToWrite.episode]
            except (tvdb_exceptions.tvdb_episodenotfound, tvdb_exceptions.tvdb_seasonnotfound):
                logger.log(u"Unable to find episode " + str(curEpToWrite.season) + "x" + str(curEpToWrite.episode) + " on tvdb... has it been removed? Should I delete from db?")
                return None

            if not myEp["firstaired"]:
                myEp["firstaired"] = str(datetime.date.fromordinal(1))

            if not myEp["episodename"]:
                logger.log(u"Not generating nfo because the ep has no title", logger.DEBUG)
                return None

            logger.log(u"Creating metadata for episode " + str(ep_obj.season) + "x" + str(ep_obj.episode), logger.DEBUG)

            if len(eps_to_write) > 1:
                episode = etree.SubElement(rootNode, "episodedetails")
            else:
                episode = rootNode

            title = etree.SubElement(episode, "title")
            if curEpToWrite.name != None:
                title.text = curEpToWrite.name

            showtitle = etree.SubElement(episode, "showtitle")
            if curEpToWrite.show.name != None:
                showtitle.text = curEpToWrite.show.name

            season = etree.SubElement(episode, "season")
            season.text = str(curEpToWrite.season)

            episodenum = etree.SubElement(episode, "episode")
            episodenum.text = str(curEpToWrite.episode)

            uniqueid = etree.SubElement(episode, "uniqueid")
            uniqueid.text = str(curEpToWrite.tvdbid)

            aired = etree.SubElement(episode, "aired")
            if curEpToWrite.airdate != datetime.date.fromordinal(1):
                aired.text = str(curEpToWrite.airdate)
            else:
                aired.text = ''

            plot = etree.SubElement(episode, "plot")
            if curEpToWrite.description != None:
                plot.text = curEpToWrite.description

            runtime = etree.SubElement(episode, "runtime")
            if curEpToWrite.season != 0:
                if myShow["runtime"] != None:
                    runtime.text = myShow["runtime"]

            displayseason = etree.SubElement(episode, "displayseason")
            if 'airsbefore_season' in myEp:
                displayseason_text = myEp['airsbefore_season']
                if displayseason_text != None:
                    displayseason.text = displayseason_text

            displayepisode = etree.SubElement(episode, "displayepisode")
            if 'airsbefore_episode' in myEp:
                displayepisode_text = myEp['airsbefore_episode']
                if displayepisode_text != None:
                    displayepisode.text = displayepisode_text

            thumb = etree.SubElement(episode, "thumb")
            thumb_text = myEp['filename']
            if thumb_text != None:
                thumb.text = thumb_text

            watched = etree.SubElement(episode, "watched")
            watched.text = 'false'

            credits = etree.SubElement(episode, "credits")
            credits_text = myEp['writer']
            if credits_text != None:
                credits.text = credits_text

            director = etree.SubElement(episode, "director")
            director_text = myEp['director']
            if director_text != None:
                director.text = director_text

            rating = etree.SubElement(episode, "rating")
            rating_text = myEp['rating']
            if rating_text != None:
                rating.text = rating_text

            gueststar_text = myEp['gueststars']
            if gueststar_text != None:
                for actor in (x.strip() for x in gueststar_text.split('|') if x and x.strip()):
                    cur_actor = etree.SubElement(episode, "actor")
                    cur_actor_name = etree.SubElement(cur_actor, "name")
                    cur_actor_name.text = actor

            if myShow['_actors'] != None:
                for actor in myShow['_actors']:
                    cur_actor_name_text = actor['name']

                    if cur_actor_name_text != None and cur_actor_name_text.strip():
                        cur_actor = etree.SubElement(episode, "actor")
                        cur_actor_name = etree.SubElement(cur_actor, "name")
                        cur_actor_name.text = cur_actor_name_text.strip()

                        cur_actor_role = etree.SubElement(cur_actor, "role")
                        cur_actor_role_text = actor['role']
                        if cur_actor_role_text != None:
                            cur_actor_role.text = cur_actor_role_text

                        cur_actor_thumb = etree.SubElement(cur_actor, "thumb")
                        cur_actor_thumb_text = actor['image']
                        if cur_actor_thumb_text != None:
                            cur_actor_thumb.text = cur_actor_thumb_text

        # Make it purdy
        helpers.indentXML(rootNode)

        data = etree.ElementTree(rootNode)

        return data


# present a standard "interface" from the module
metadata_class = XBMC_12PlusMetadata

########NEW FILE########
__FILENAME__ = name_cache
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from sickbeard import db
from sickbeard.helpers import sanitizeSceneName


def addNameToCache(name, tvdb_id):
    """
    Adds the show & tvdb id to the scene_names table in cache.db.

    name: The show name to cache
    tvdb_id: The tvdb id that this show should be cached with (can be None/0 for unknown)
    """

    # standardize the name we're using to account for small differences in providers
    name = sanitizeSceneName(name)

    if not tvdb_id:
        tvdb_id = 0

    cacheDB = db.DBConnection('cache.db')
    cacheDB.action("INSERT INTO scene_names (tvdb_id, name) VALUES (?, ?)", [tvdb_id, name])


def retrieveNameFromCache(name):
    """
    Looks up the given name in the scene_names table in cache.db.

    name: The show name to look up.

    Returns: the tvdb id that resulted from the cache lookup or None if the show wasn't found in the cache
    """

    # standardize the name we're using to account for small differences in providers
    name = sanitizeSceneName(name)

    cacheDB = db.DBConnection('cache.db')
    cache_results = cacheDB.select("SELECT * FROM scene_names WHERE name = ?", [name])

    if not cache_results:
        return None

    return int(cache_results[0]["tvdb_id"])


def clearCache():
    """
    Deletes all "unknown" entries from the cache (names with tvdb_id of 0).
    """
    cacheDB = db.DBConnection('cache.db')
    cacheDB.action("DELETE FROM scene_names WHERE tvdb_id = ?", [0])

########NEW FILE########
__FILENAME__ = parser
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import os.path
import re

import regexes

import sickbeard

from sickbeard import logger
from sickbeard import encodingKludge as ek
from sickbeard import helpers



class NameParser(object):
    def __init__(self, is_file_name=True):

        self.is_file_name = is_file_name
        self.compiled_regexes = []
        self._compile_regexes()

    def clean_series_name(self, series_name):
        """Cleans up series name by removing any . and _
        characters, along with any trailing hyphens.

        Is basically equivalent to replacing all _ and . with a
        space, but handles decimal numbers in string, for example:

        >>> cleanRegexedSeriesName("an.example.1.0.test")
        'an example 1.0 test'
        >>> cleanRegexedSeriesName("an_example_1.0_test")
        'an example 1.0 test'

        Stolen from dbr's tvnamer
        """

        series_name = re.sub("(\D)\.(?!\s)(\D)", "\\1 \\2", series_name)
        series_name = re.sub("(\d)\.(\d{4})", "\\1 \\2", series_name)  # if it ends in a year then don't keep the dot
        series_name = re.sub("(\D)\.(?!\s)", "\\1 ", series_name)
        series_name = re.sub("\.(?!\s)(\D)", " \\1", series_name)
        series_name = series_name.replace("_", " ")
        series_name = re.sub("-$", "", series_name)
        return series_name.strip()

    def _compile_regexes(self):
        for (cur_pattern_name, cur_pattern) in regexes.ep_regexes:
            try:
                cur_regex = re.compile(cur_pattern, re.VERBOSE | re.IGNORECASE)
            except re.error, errormsg:
                logger.log(u"WARNING: Invalid episode_pattern, %s. %s" % (errormsg, cur_pattern))
            else:
                self.compiled_regexes.append((cur_pattern_name, cur_regex))

    def _parse_string(self, name):

        if not name:
            return None

        for (cur_regex_name, cur_regex) in self.compiled_regexes:
            match = cur_regex.match(name)

            if not match:
                continue

            result = ParseResult(name)
            result.which_regex = [cur_regex_name]

            named_groups = match.groupdict().keys()

            if 'series_name' in named_groups:
                result.series_name = match.group('series_name')
                if result.series_name:
                    result.series_name = self.clean_series_name(result.series_name)

            if 'season_num' in named_groups:
                tmp_season = int(match.group('season_num'))
                if cur_regex_name == 'bare' and tmp_season in (19, 20):
                    continue
                result.season_number = tmp_season

            if 'ep_num' in named_groups:
                ep_num = self._convert_number(match.group('ep_num'))
                if 'extra_ep_num' in named_groups and match.group('extra_ep_num'):
                    result.episode_numbers = range(ep_num, self._convert_number(match.group('extra_ep_num')) + 1)
                else:
                    result.episode_numbers = [ep_num]

            if 'air_year' in named_groups and 'air_month' in named_groups and 'air_day' in named_groups:
                year = int(match.group('air_year'))
                month = int(match.group('air_month'))
                day = int(match.group('air_day'))

                # make an attempt to detect YYYY-DD-MM formats
                if month > 12:
                    tmp_month = month
                    month = day
                    day = tmp_month

                try:
                    result.air_date = datetime.date(year, month, day)
                except ValueError, e:
                    raise InvalidNameException(e.message)

            result.is_proper = False

            if 'extra_info' in named_groups:

                tmp_extra_info = match.group('extra_info')

                # Check if it's a proper
                if tmp_extra_info:
                    result.is_proper = re.search('(^|[\. _-])(proper|repack)([\. _-]|$)', tmp_extra_info, re.I) is not None

                # Show.S04.Special or Show.S05.Part.2.Extras is almost certainly not every episode in the season
                if tmp_extra_info and cur_regex_name == 'season_only' and re.search(r'([. _-]|^)(special|extra)s?\w*([. _-]|$)', tmp_extra_info, re.I):
                    continue
                result.extra_info = tmp_extra_info

            if 'release_group' in named_groups:
                result.release_group = match.group('release_group')

            return result

        return None

    def _combine_results(self, first, second, attr):
        # if the first doesn't exist then return the second or nothing
        if not first:
            if not second:
                return None
            else:
                return getattr(second, attr)

        # if the second doesn't exist then return the first
        if not second:
            return getattr(first, attr)

        a = getattr(first, attr)
        b = getattr(second, attr)

        # if a is good use it
        if a != None or (type(a) == list and len(a)):
            return a
        # if not use b (if b isn't set it'll just be default)
        else:
            return b

    def _unicodify(self, obj, encoding="utf-8"):
        if isinstance(obj, basestring):
            if not isinstance(obj, unicode):
                obj = unicode(obj, encoding)
        return obj

    def _convert_number(self, org_number):
        """
        Convert org_number into an integer
        org_number: integer or representation of a number: string or unicode
        Try force converting to int first, on error try converting from Roman numerals
        returns integer or 0
        """

        try:
            # try forcing to int
            if org_number:
                number = int(org_number)
            else:
                number = 0

        except:
            # on error try converting from Roman numerals
            roman_to_int_map = (('M', 1000), ('CM', 900), ('D', 500), ('CD', 400), ('C', 100),
                                ('XC', 90), ('L', 50), ('XL', 40), ('X', 10),
                                ('IX', 9), ('V', 5), ('IV', 4), ('I', 1)
                               )

            roman_numeral = str(org_number).upper()
            number = 0
            index = 0

            for numeral, integer in roman_to_int_map:
                while roman_numeral[index:index + len(numeral)] == numeral:
                    number += integer
                    index += len(numeral)

        return number

    def parse(self, name):

        name = self._unicodify(name)

        cached = name_parser_cache.get(name)
        if cached:
            return cached

        # break it into parts if there are any (dirname, file name, extension)
        dir_name, file_name = ek.ek(os.path.split, name)

        if self.is_file_name:
            base_file_name = helpers.remove_extension(file_name)
        else:
            base_file_name = file_name

        # use only the direct parent dir
        dir_name = ek.ek(os.path.basename, dir_name)

        # set up a result to use
        final_result = ParseResult(name)

        # try parsing the file name
        file_name_result = self._parse_string(base_file_name)

        # parse the dirname for extra info if needed
        dir_name_result = self._parse_string(dir_name)

        # build the ParseResult object
        final_result.air_date = self._combine_results(file_name_result, dir_name_result, 'air_date')

        if not final_result.air_date:
            final_result.season_number = self._combine_results(file_name_result, dir_name_result, 'season_number')
            final_result.episode_numbers = self._combine_results(file_name_result, dir_name_result, 'episode_numbers')

        final_result.is_proper = self._combine_results(file_name_result, dir_name_result, 'is_proper')

        # if the dirname has a release group/show name I believe it over the filename
        final_result.series_name = self._combine_results(dir_name_result, file_name_result, 'series_name')
        final_result.extra_info = self._combine_results(dir_name_result, file_name_result, 'extra_info')
        final_result.release_group = self._combine_results(dir_name_result, file_name_result, 'release_group')

        final_result.which_regex = []
        if final_result == file_name_result:
            final_result.which_regex = file_name_result.which_regex
        elif final_result == dir_name_result:
            final_result.which_regex = dir_name_result.which_regex
        else:
            if file_name_result:
                final_result.which_regex += file_name_result.which_regex
            if dir_name_result:
                final_result.which_regex += dir_name_result.which_regex

        # if there's no useful info in it then raise an exception
        if final_result.season_number == None and not final_result.episode_numbers and final_result.air_date == None and not final_result.series_name:
            raise InvalidNameException("Unable to parse " + name.encode(sickbeard.SYS_ENCODING, 'xmlcharrefreplace'))

        name_parser_cache.add(name, final_result)
        # return it
        return final_result


class ParseResult(object):
    def __init__(self,
                 original_name,
                 series_name=None,
                 season_number=None,
                 episode_numbers=None,
                 extra_info=None,
                 release_group=None,
                 air_date=None
                 ):

        self.original_name = original_name

        self.series_name = series_name
        self.season_number = season_number
        if not episode_numbers:
            self.episode_numbers = []
        else:
            self.episode_numbers = episode_numbers

        self.extra_info = extra_info
        self.release_group = release_group

        self.air_date = air_date

        self.which_regex = None

    def __eq__(self, other):
        if not other:
            return False

        if self.series_name != other.series_name:
            return False
        if self.season_number != other.season_number:
            return False
        if self.episode_numbers != other.episode_numbers:
            return False
        if self.extra_info != other.extra_info:
            return False
        if self.release_group != other.release_group:
            return False
        if self.air_date != other.air_date:
            return False

        return True

    def __str__(self):
        if self.series_name != None:
            to_return = self.series_name + u' - '
        else:
            to_return = u''
        if self.season_number != None:
            to_return += 'S' + str(self.season_number)
        if self.episode_numbers and len(self.episode_numbers):
            for e in self.episode_numbers:
                to_return += 'E' + str(e)

        if self.air_by_date:
            to_return += str(self.air_date)

        if self.extra_info:
            to_return += ' - ' + self.extra_info
        if self.release_group:
            to_return += ' (' + self.release_group + ')'

        to_return += ' [ABD: ' + str(self.air_by_date) + ']'

        return to_return.encode('utf-8')

    def _is_air_by_date(self):
        if self.season_number == None and len(self.episode_numbers) == 0 and self.air_date:
            return True
        return False
    air_by_date = property(_is_air_by_date)


class NameParserCache(object):
    #TODO: check if the fifo list can beskiped and only use one dict
    _previous_parsed_list = []  # keep a fifo list of the cached items
    _previous_parsed = {}
    _cache_size = 100

    def add(self, name, parse_result):
        self._previous_parsed[name] = parse_result
        self._previous_parsed_list.append(name)
        while len(self._previous_parsed_list) > self._cache_size:
            del_me = self._previous_parsed_list.pop(0)
            self._previous_parsed.pop(del_me)

    def get(self, name):
        if name in self._previous_parsed:
            logger.log("Using cached parse result for: " + name, logger.DEBUG)
            return self._previous_parsed[name]
        else:
            return None

name_parser_cache = NameParserCache()


class InvalidNameException(Exception):
    "The given name is not valid"

########NEW FILE########
__FILENAME__ = regexes
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

# all regexes are case insensitive

ep_regexes = [
              ('standard_repeat',
               # Show.Name.S01E02.S01E03.Source.Quality.Etc-Group
               # Show Name - S01E02 - S01E03 - S01E04 - Ep Name
               '''
               ^(?P<series_name>.+?)[. _-]+                # Show_Name and separator
               s(?P<season_num>\d+)[. _-]*                 # S01 and optional separator
               e(?P<ep_num>\d+)                            # E02 and separator
               ([. _-]+s(?P=season_num)[. _-]*             # S01 and optional separator
               e(?P<extra_ep_num>\d+))+                    # E03/etc and separator
               [. _-]*((?P<extra_info>.+?)                 # Source_Quality_Etc-
               ((?<![. _-])(?<!WEB)                        # Make sure this is really the release group
               -(?P<release_group>[^- ]+))?)?$              # Group
               '''),
              
              ('fov_repeat',
               # Show.Name.1x02.1x03.Source.Quality.Etc-Group
               # Show Name - 1x02 - 1x03 - 1x04 - Ep Name
               '''
               ^(?P<series_name>.+?)[. _-]+                # Show_Name and separator
               (?P<season_num>\d+)x                        # 1x
               (?P<ep_num>\d+)                             # 02 and separator
               ([. _-]+(?P=season_num)x                    # 1x
               (?P<extra_ep_num>\d+))+                     # 03/etc and separator
               [. _-]*((?P<extra_info>.+?)                 # Source_Quality_Etc-
               ((?<![. _-])(?<!WEB)                        # Make sure this is really the release group
               -(?P<release_group>[^- ]+))?)?$              # Group
               '''),
              
              ('standard',
               # Show.Name.S01E02.Source.Quality.Etc-Group
               # Show Name - S01E02 - My Ep Name
               # Show.Name.S01.E03.My.Ep.Name
               # Show.Name.S01E02E03.Source.Quality.Etc-Group
               # Show Name - S01E02-03 - My Ep Name
               # Show.Name.S01.E02.E03
               '''
               ^((?P<series_name>.+?)[. _-]+)?             # Show_Name and separator
               s(?P<season_num>\d+)[. _-]*                 # S01 and optional separator
               e(?P<ep_num>\d+)                            # E02 and separator
               (([. _-]*e|-)                               # linking e/- char
               (?P<extra_ep_num>(?!(1080|720)[pi])\d+))*   # additional E03/etc
               [. _-]*((?P<extra_info>.+?)                 # Source_Quality_Etc-
               ((?<![. _-])(?<!WEB)                        # Make sure this is really the release group
               -(?P<release_group>[^- ]+))?)?$              # Group
               '''),

              ('fov',
               # Show_Name.1x02.Source_Quality_Etc-Group
               # Show Name - 1x02 - My Ep Name
               # Show_Name.1x02x03x04.Source_Quality_Etc-Group
               # Show Name - 1x02-03-04 - My Ep Name
               '''
               ^((?P<series_name>.+?)[\[. _-]+)?           # Show_Name and separator
               (?P<season_num>\d+)x                        # 1x
               (?P<ep_num>\d+)                             # 02 and separator
               (([. _-]*x|-)                               # linking x/- char
               (?P<extra_ep_num>
               (?!(1080|720)[pi])(?!(?<=x)264)             # ignore obviously wrong multi-eps
               \d+))*                                      # additional x03/etc
               [\]. _-]*((?P<extra_info>.+?)               # Source_Quality_Etc-
               ((?<![. _-])(?<!WEB)                        # Make sure this is really the release group
               -(?P<release_group>[^- ]+))?)?$              # Group
               '''),
        
              ('scene_date_format',
               # Show.Name.2010.11.23.Source.Quality.Etc-Group
               # Show Name - 2010-11-23 - Ep Name
               '''
               ^((?P<series_name>.+?)[. _-]+)?             # Show_Name and separator
               (?P<air_year>\d{4})[. _-]+                  # 2010 and separator
               (?P<air_month>\d{2})[. _-]+                 # 11 and separator
               (?P<air_day>\d{2})                          # 23 and separator
               [. _-]*((?P<extra_info>.+?)                 # Source_Quality_Etc-
               ((?<![. _-])(?<!WEB)                        # Make sure this is really the release group
               -(?P<release_group>[^- ]+))?)?$              # Group
               '''),
              
              ('stupid',
               # tpz-abc102
               '''
               (?P<release_group>.+?)-\w+?[\. ]?           # tpz-abc
               (?!264)                                     # don't count x264
               (?P<season_num>\d{1,2})                     # 1
               (?P<ep_num>\d{2})$                          # 02
               '''),
              
              ('verbose',
               # Show Name Season 1 Episode 2 Ep Name
               '''
               ^(?P<series_name>.+?)[. _-]+                # Show Name and separator
               season[. _-]+                               # season and separator
               (?P<season_num>\d+)[. _-]+                  # 1
               episode[. _-]+                              # episode and separator
               (?P<ep_num>\d+)[. _-]+                      # 02 and separator
               (?P<extra_info>.+)$                         # Source_Quality_Etc-
               '''),
              
              ('season_only',
               # Show.Name.S01.Source.Quality.Etc-Group
               '''
               ^((?P<series_name>.+?)[. _-]+)?             # Show_Name and separator
               s(eason[. _-])?                             # S01/Season 01
               (?P<season_num>\d+)[. _-]*                  # S01 and optional separator
               [. _-]*((?P<extra_info>.+?)                 # Source_Quality_Etc-
               ((?<![. _-])(?<!WEB)                        # Make sure this is really the release group
               -(?P<release_group>[^- ]+))?)?$              # Group
               '''
               ),

              ('no_season_multi_ep',
               # Show.Name.E02-03
               # Show.Name.E02.2010
               '''
               ^((?P<series_name>.+?)[. _-]+)?             # Show_Name and separator
               (e(p(isode)?)?|part|pt)[. _-]?              # e, ep, episode, or part
               (?P<ep_num>(\d+|[ivx]+))                    # first ep num
               ((([. _-]+(and|&|to)[. _-]+)|-)             # and/&/to joiner
               (?P<extra_ep_num>(?!(1080|720)[pi])(\d+|[ivx]+))[. _-])            # second ep num
               ([. _-]*(?P<extra_info>.+?)                 # Source_Quality_Etc-
               ((?<![. _-])(?<!WEB)                        # Make sure this is really the release group
               -(?P<release_group>[^- ]+))?)?$              # Group
               '''
               ),

              ('no_season_general',
               # Show.Name.E23.Test
               # Show.Name.Part.3.Source.Quality.Etc-Group
               # Show.Name.Part.1.and.Part.2.Blah-Group
               '''
               ^((?P<series_name>.+?)[. _-]+)?             # Show_Name and separator
               (e(p(isode)?)?|part|pt)[. _-]?              # e, ep, episode, or part
               (?P<ep_num>(\d+|([ivx]+(?=[. _-]))))                    # first ep num
               ([. _-]+((and|&|to)[. _-]+)?                # and/&/to joiner
               ((e(p(isode)?)?|part|pt)[. _-]?)           # e, ep, episode, or part
               (?P<extra_ep_num>(?!(1080|720)[pi])
               (\d+|([ivx]+(?=[. _-]))))[. _-])*            # second ep num
               ([. _-]*(?P<extra_info>.+?)                 # Source_Quality_Etc-
               ((?<![. _-])(?<!WEB)                        # Make sure this is really the release group
               -(?P<release_group>[^- ]+))?)?$              # Group
               '''
               ),

              ('bare',
               # Show.Name.102.Source.Quality.Etc-Group
               '''
               ^(?P<series_name>.+?)[. _-]+                # Show_Name and separator
               (?P<season_num>\d{1,2})                     # 1
               (?P<ep_num>\d{2})                           # 02 and separator
               ([. _-]+(?P<extra_info>(?!\d{3}[. _-]+)[^-]+) # Source_Quality_Etc-
               (-(?P<release_group>.+))?)?$                # Group
               '''),
              
              ('no_season',
               # Show Name - 01 - Ep Name
               # 01 - Ep Name
               # 01 - Ep Name
               '''
               ^((?P<series_name>.+?)(?:[. _-]{2,}|[. _]))?             # Show_Name and separator
               (?P<ep_num>\d{1,2})                           # 02
               (?:-(?P<extra_ep_num>\d{1,2}))*               # 02
               [. _-]+((?P<extra_info>.+?)                 # Source_Quality_Etc-
               ((?<![. _-])(?<!WEB)                        # Make sure this is really the release group
               -(?P<release_group>[^- ]+))?)?$              # Group
               '''
               ),
              ]


########NEW FILE########
__FILENAME__ = naming
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import os

import sickbeard
from sickbeard import encodingKludge as ek
from sickbeard import tv
from sickbeard import common
from sickbeard import logger
from sickbeard.name_parser.parser import NameParser, InvalidNameException

from common import Quality, DOWNLOADED

name_presets = ('%SN - %Sx%0E - %EN',
                '%S.N.S%0SE%0E.%E.N',
                '%Sx%0E - %EN',
                'S%0SE%0E - %EN',
                'Season %0S/%S.N.S%0SE%0E.%Q.N-%RG'
                )

name_abd_presets = ('%SN - %A-D - %EN',
                    '%S.N.%A.D.%E.N.%Q.N',
                    '%Y/%0M/%S.N.%A.D.%E.N-%RG'
                    )

class TVShow():
    def __init__(self):
        self.name = "Show Name"
        self.genre = "Comedy"
        self.air_by_date = 0

class TVEpisode(tv.TVEpisode):
    def __init__(self, season, episode, name):
        self.relatedEps = []
        self._name = name
        self._season = season
        self._episode = episode
        self._airdate = datetime.date(2010, 3, 9)
        self.show = TVShow()
        self._status = Quality.compositeStatus(common.DOWNLOADED, common.Quality.SDTV)
        self._release_name = 'Show.Name.S02E03.HDTV.XviD-RLSGROUP'

def check_force_season_folders(pattern=None, multi=None):
    """
    Checks if the name can still be parsed if you strip off the folders to determine if we need to force season folders
    to be enabled or not.
    
    Returns true if season folders need to be forced on or false otherwise.
    """
    if pattern == None:
        pattern = sickbeard.NAMING_PATTERN
    
    valid = not validate_name(pattern, None, file_only=True) 
    
    if multi != None:
        valid = valid or not validate_name(pattern, multi, file_only=True)

    return valid

def check_valid_naming(pattern=None, multi=None):
    """
    Checks if the name is can be parsed back to its original form for both single and multi episodes.
    
    Returns true if the naming is valid, false if not.
    """
    if pattern == None:
        pattern = sickbeard.NAMING_PATTERN
        
    logger.log(u"Checking whether the pattern "+pattern+" is valid for a single episode", logger.DEBUG)
    valid = validate_name(pattern, None)

    if multi != None:
        logger.log(u"Checking whether the pattern "+pattern+" is valid for a multi episode", logger.DEBUG)
        valid = valid and validate_name(pattern, multi)

    return valid

def check_valid_abd_naming(pattern=None):
    """
    Checks if the name is can be parsed back to its original form for an air-by-date format.
    
    Returns true if the naming is valid, false if not.
    """
    if pattern == None:
        pattern = sickbeard.NAMING_PATTERN
        
    logger.log(u"Checking whether the pattern "+pattern+" is valid for an air-by-date episode", logger.DEBUG)
    valid = validate_name(pattern, abd=True)

    return valid


def validate_name(pattern, multi=None, file_only=False, abd=False):
    ep = _generate_sample_ep(multi, abd)

    parser = NameParser(True)

    new_name = ep.formatted_filename(pattern, multi) + '.ext'
    new_path = ep.formatted_dir(pattern, multi)
    if not file_only:
        new_name = ek.ek(os.path.join, new_path, new_name)

    if not new_name:
        logger.log(u"Unable to create a name out of "+pattern, logger.DEBUG)
        return False

    logger.log(u"Trying to parse "+new_name, logger.DEBUG)

    try:
        result = parser.parse(new_name)
    except InvalidNameException:
        logger.log(u"Unable to parse "+new_name+", not valid", logger.DEBUG)
        return False
    
    logger.log("The name "+new_name + " parsed into " + str(result), logger.DEBUG)

    if abd:
        if result.air_date != ep.airdate:
            logger.log(u"Air date incorrect in parsed episode, pattern isn't valid", logger.DEBUG)
            return False
    else:
        if result.season_number != ep.season:
            logger.log(u"Season incorrect in parsed episode, pattern isn't valid", logger.DEBUG)
            return False
        if result.episode_numbers != [x.episode for x in [ep] + ep.relatedEps]:
            logger.log(u"Episode incorrect in parsed episode, pattern isn't valid", logger.DEBUG)
            return False

    return True

def _generate_sample_ep(multi=None, abd=False):
    # make a fake episode object
    ep = TVEpisode(2,3,"Ep Name")
    ep._status = Quality.compositeStatus(DOWNLOADED, Quality.HDTV)
    ep._airdate = datetime.date(2011, 3, 9)
    if abd:
        ep._release_name = 'Show.Name.2011.03.09.HDTV.XviD-RLSGROUP'
    else:
        ep._release_name = 'Show.Name.S02E03.HDTV.XviD-RLSGROUP'

    if multi != None:
        ep._name = "Ep Name (1)"
        ep._release_name = 'Show.Name.S02E03E04E05.HDTV.XviD-RLSGROUP'

        secondEp = TVEpisode(2,4,"Ep Name (2)")
        secondEp._status = Quality.compositeStatus(DOWNLOADED, Quality.HDTV)
        secondEp._release_name = ep._release_name

        thirdEp = TVEpisode(2,5,"Ep Name (3)")
        thirdEp._status = Quality.compositeStatus(DOWNLOADED, Quality.HDTV)
        thirdEp._release_name = ep._release_name

        ep.relatedEps.append(secondEp)
        ep.relatedEps.append(thirdEp)

    return ep

def test_name(pattern, multi=None, abd=False):

    ep = _generate_sample_ep(multi, abd)

    return {'name': ep.formatted_filename(pattern, multi), 'dir': ep.formatted_dir(pattern, multi)}
########NEW FILE########
__FILENAME__ = boxcar
# Author: Marvin Pinto <me@marvinp.ca>
# Author: Dennis Lutter <lad1337@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib
import urllib2
import time

import sickbeard

from sickbeard import logger
from sickbeard.common import notifyStrings, NOTIFY_SNATCH, NOTIFY_DOWNLOAD
from sickbeard.exceptions import ex

API_URL = "https://boxcar.io/devices/providers/fWc4sgSmpcN6JujtBmR6/notifications"


class BoxcarNotifier:

    def _sendBoxcar(self, msg, title, email, subscribe=False):
        """
        Sends a boxcar notification to the address provided

        msg: The message to send (unicode)
        title: The title of the message
        email: The email address to send the message to (or to subscribe with)
        subscribe: If true then instead of sending a message this function will send a subscription notification (optional, default is False)

        returns: True if the message succeeded, False otherwise
        """

        # build up the URL and parameters
        msg = msg.strip()
        curUrl = API_URL

        # if this is a subscription notification then act accordingly
        if subscribe:
            data = urllib.urlencode({'email': email})
            curUrl = curUrl + "/subscribe"

        # for normal requests we need all these parameters
        else:
            data = urllib.urlencode({
                'email': email,
                'notification[from_screen_name]': title,
                'notification[message]': msg.encode('utf-8'),
                'notification[from_remote_service_id]': int(time.time())
                })

        # send the request to boxcar
        try:
            req = urllib2.Request(curUrl)
            handle = urllib2.urlopen(req, data)
            handle.close()

        except urllib2.URLError, e:
            # if we get an error back that doesn't have an error code then who knows what's really happening
            if not hasattr(e, 'code'):
                logger.log(u"BOXCAR: Notification failed." + ex(e), logger.ERROR)
                return False
            else:
                logger.log(u"BOXCAR: Notification failed. Error code: " + str(e.code), logger.ERROR)

            # HTTP status 404 if the provided email address isn't a Boxcar user.
            if e.code == 404:
                logger.log(u"BOXCAR: Username is wrong/not a boxcar email. Boxcar will send an email to it", logger.WARNING)
                return False

            # For HTTP status code 401's, it is because you are passing in either an invalid token, or the user has not added your service.
            elif e.code == 401:

                # If the user has already added your service, we'll return an HTTP status code of 401.
                if subscribe:
                    logger.log(u"BOXCAR: Already subscribed to service", logger.ERROR)
                    # i dont know if this is true or false ... its neither but i also dont know how we got here in the first place
                    return False

                # HTTP status 401 if the user doesn't have the service added
                else:
                    subscribeNote = self._sendBoxcar(msg, title, email, True)
                    if subscribeNote:
                        logger.log(u"BOXCAR: Subscription sent.", logger.DEBUG)
                        return True
                    else:
                        logger.log(u"BOXCAR: Subscription could not be sent.", logger.ERROR)
                        return False

            # If you receive an HTTP status code of 400, it is because you failed to send the proper parameters
            elif e.code == 400:
                logger.log(u"BOXCAR: Wrong data sent to boxcar.", logger.ERROR)
                return False

        logger.log(u"BOXCAR: Notification successful.", logger.MESSAGE)
        return True

    def _notify(self, title, message, username=None, force=False):
        """
        Sends a boxcar notification based on the provided info or SB config

        title: The title of the notification to send
        message: The message string to send
        username: The username to send the notification to (optional, defaults to the username in the config)
        force: If True then the notification will be sent even if Boxcar is disabled in the config
        """

        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_BOXCAR and not force:
            return False

        # if no username was given then use the one from the config
        if not username:
            username = sickbeard.BOXCAR_USERNAME

        logger.log(u"BOXCAR: Sending notification for " + message, logger.DEBUG)

        return self._sendBoxcar(message, title, username)

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.BOXCAR_NOTIFY_ONSNATCH:
            self._notify(notifyStrings[NOTIFY_SNATCH], ep_name)

    def notify_download(self, ep_name):
        if sickbeard.BOXCAR_NOTIFY_ONDOWNLOAD:
            self._notify(notifyStrings[NOTIFY_DOWNLOAD], ep_name)

    def test_notify(self, boxcar_username):
        return self._notify("This is a test notification from Sick Beard", "Test", boxcar_username, force=True)

    def update_library(self, ep_obj=None):
        pass

notifier = BoxcarNotifier

########NEW FILE########
__FILENAME__ = growl
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import socket
import sickbeard

from sickbeard import logger, common
from sickbeard.exceptions import ex

from lib.growl import gntp


class GrowlNotifier:

    def _send_growl(self, options, message=None):
        # send notification
        notice = gntp.GNTPNotice()

        #Required
        notice.add_header('Application-Name', options['app'])
        notice.add_header('Notification-Name', options['name'])
        notice.add_header('Notification-Title', options['title'])

        if options['password']:
            notice.set_password(options['password'])

        # optional
        if options['sticky']:
            notice.add_header('Notification-Sticky', options['sticky'])
        if options['priority']:
            notice.add_header('Notification-Priority', options['priority'])
        if options['icon']:
            notice.add_header('Notification-Icon', 'http://www.sickbeard.com/notify.png')

        if message:
            notice.add_header('Notification-Text', message)

        response = self._send(options['host'], options['port'], notice.encode(), options['debug'])
        if isinstance(response, gntp.GNTPOK):
            return True

        return False

    def _send(self, host, port, data, debug=False):
        if debug:
            print '<Sending>\n', data, '\n</Sending>'

        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((host, port))
        s.send(data)
        response = gntp.parse_gntp(s.recv(1024))
        s.close()

        if debug:
            print '<Recieved>\n', response, '\n</Recieved>'

        return response

    def _notify(self, title="Sick Beard Notification", message=None, name=None, host=None, password=None, force=False):
        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_GROWL and not force:
            return False

        # fill in omitted parameters
        if not name:
            name = title

        if not host:
            hostParts = sickbeard.GROWL_HOST.split(':')
        else:
            hostParts = host.split(':')

        if len(hostParts) != 2 or hostParts[1] == '':
            port = 23053
        else:
            port = int(hostParts[1])

        growlHosts = [(hostParts[0], port)]

        opts = {}

        opts['name'] = name

        opts['title'] = title
        opts['app'] = 'SickBeard'

        opts['sticky'] = None
        opts['priority'] = None
        opts['debug'] = False

        if not password:
            opts['password'] = sickbeard.GROWL_PASSWORD
        else:
            opts['password'] = password

        opts['icon'] = True

        # TODO: Multi hosts does not seem to work... registration only happens for the first
        for pc in growlHosts:
            print pc
            opts['host'] = pc[0]
            opts['port'] = pc[1]
            logger.log(u"GROWL: Sending message '" + message + "' to " + opts['host'] + ":" + str(opts['port']), logger.DEBUG)
            try:
                return self._send_growl(opts, message)
            except Exception, e:
                logger.log(u"GROWL: Unable to send growl to " + opts['host'] + ":" + str(opts['port']) + " - " + ex(e), logger.WARNING)
                return False

    def _sendRegistration(self, host=None, password=None, name="Sick Beard Notification"):
        opts = {}

        if not host:
            hostParts = sickbeard.GROWL_HOST.split(':')
        else:
            hostParts = host.split(':')

        if len(hostParts) != 2 or hostParts[1] == '':
            port = 23053
        else:
            port = int(hostParts[1])

        opts['host'] = hostParts[0]
        opts['port'] = port

        if not password:
            opts['password'] = sickbeard.GROWL_PASSWORD
        else:
            opts['password'] = password

        opts['app'] = 'SickBeard'
        opts['debug'] = False

        # send registration
        register = gntp.GNTPRegister()
        register.add_header('Application-Name', opts['app'])
        register.add_header('Application-Icon', 'http://www.sickbeard.com/notify.png')

        register.add_notification('Test', True)
        register.add_notification(common.notifyStrings[common.NOTIFY_SNATCH], True)
        register.add_notification(common.notifyStrings[common.NOTIFY_DOWNLOAD], True)

        if opts['password']:
            register.set_password(opts['password'])

        try:
            return self._send(opts['host'], opts['port'], register.encode(), opts['debug'])
        except Exception, e:
            logger.log(u"GROWL: Unable to send growl to " + opts['host'] + ":" + str(opts['port']) + " - " + ex(e), logger.WARNING)
            return False

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.GROWL_NOTIFY_ONSNATCH:
            self._notify(common.notifyStrings[common.NOTIFY_SNATCH], ep_name)

    def notify_download(self, ep_name):
        if sickbeard.GROWL_NOTIFY_ONDOWNLOAD:
            self._notify(common.notifyStrings[common.NOTIFY_DOWNLOAD], ep_name)

    def test_notify(self, host, password):
        result = self._sendRegistration(host, password, "Test")
        if result:
            return self._notify("Test Growl", "Testing Growl settings from Sick Beard", "Test", host, password, force=True)
        else:
            return result

    def update_library(self, ep_obj=None):
        pass

notifier = GrowlNotifier

########NEW FILE########
__FILENAME__ = libnotify
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import os
import cgi
import sickbeard

from sickbeard import logger, common


def diagnose():
    '''
    Check the environment for reasons libnotify isn't working.  Return a
    user-readable message indicating possible issues.
    '''
    try:
        import pynotify  # @UnusedImport
    except ImportError:
        return (u"<p>Error: pynotify isn't installed.  On Ubuntu/Debian, install the "
                u"<a href=\"apt:python-notify\">python-notify</a> package.")
    if 'DISPLAY' not in os.environ and 'DBUS_SESSION_BUS_ADDRESS' not in os.environ:
        return (u"<p>Error: Environment variables DISPLAY and DBUS_SESSION_BUS_ADDRESS "
                u"aren't set.  libnotify will only work when you run Sick Beard "
                u"from a desktop login.")
    try:
        import dbus
    except ImportError:
        pass
    else:
        try:
            bus = dbus.SessionBus()
        except dbus.DBusException, e:
            return (u"<p>Error: unable to connect to D-Bus session bus: <code>%s</code>."
                    u"<p>Are you running Sick Beard in a desktop session?") % (cgi.escape(e),)
        try:
            bus.get_object('org.freedesktop.Notifications',
                           '/org/freedesktop/Notifications')
        except dbus.DBusException, e:
            return (u"<p>Error: there doesn't seem to be a notification daemon available: <code>%s</code> "
                    u"<p>Try installing notification-daemon or notify-osd.") % (cgi.escape(e),)
    return u"<p>Error: Unable to send notification."


class LibnotifyNotifier:
    def __init__(self):
        self.pynotify = None
        self.gobject = None

    def init_pynotify(self):
        if self.pynotify is not None:
            return True
        try:
            import pynotify
        except ImportError:
            logger.log(u"LIBNOTIFY: Unable to import pynotify. libnotify notifications won't work.", logger.ERROR)
            return False
        try:
            import gobject
        except ImportError:
            logger.log(u"LIBNOTIFY: Unable to import gobject. We can't catch a GError in display.", logger.ERROR)
            return False
        if not pynotify.init('Sick Beard'):
            logger.log(u"LIBNOTIFY: Initialization of pynotify failed. libnotify notifications won't work.", logger.ERROR)
            return False
        self.pynotify = pynotify
        self.gobject = gobject
        return True

    def _notify(self, title, message, force=False):
        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_LIBNOTIFY and not force:
            return False

        # detect if we can use pynotify
        if not self.init_pynotify():
            return False

        # Can't make this a global constant because PROG_DIR isn't available
        # when the module is imported.
        icon_path = os.path.join(sickbeard.PROG_DIR, "data/images/sickbeard_touch_icon.png")
        icon_uri = "file://" + os.path.abspath(icon_path)

        # If the session bus can't be acquired here a bunch of warning messages
        # will be printed but the call to show() will still return True.
        # pynotify doesn't seem too keen on error handling.
        n = self.pynotify.Notification(title, message, icon_uri)
        try:
            return n.show()
        except self.gobject.GError:
            return False

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.LIBNOTIFY_NOTIFY_ONSNATCH:
            self._notify(common.notifyStrings[common.NOTIFY_SNATCH], ep_name)

    def notify_download(self, ep_name):
        if sickbeard.LIBNOTIFY_NOTIFY_ONDOWNLOAD:
            self._notify(common.notifyStrings[common.NOTIFY_DOWNLOAD], ep_name)

    def test_notify(self):
        return self._notify("Test notification", "This is a test notification from Sick Beard", force=True)

    def update_library(self, ep_obj=None):
        pass

notifier = LibnotifyNotifier

########NEW FILE########
__FILENAME__ = nma
# Author: Adam Landry
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import sickbeard

from sickbeard import logger, common
from lib.pynma import pynma


class NMA_Notifier:

    def _sendNMA(self, nma_api=None, nma_priority=None, event=None, message=None, force=False):

        title = "Sick Beard"

        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_NMA and not force:
            return False

        if nma_api == None:
            nma_api = sickbeard.NMA_API

        if nma_priority == None:
            nma_priority = sickbeard.NMA_PRIORITY

        batch = False

        p = pynma.PyNMA()
        keys = nma_api.split(',')
        p.addkey(keys)

        if len(keys) > 1:
            batch = True

        logger.log("NMA: Sending notice with details: event=\"%s\", message=\"%s\", priority=%s, batch=%s" % (event, message, nma_priority, batch), logger.DEBUG)
        response = p.push(title, event, message, priority=nma_priority, batch_mode=batch)

        if not response[nma_api][u'code'] == u'200':
            logger.log(u"NMA: Could not send notification to NotifyMyAndroid", logger.ERROR)
            return False
        else:
            logger.log(u"NMA: Notification sent to NotifyMyAndroid", logger.MESSAGE)
            return True

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.NMA_NOTIFY_ONSNATCH:
            self._sendNMA(nma_api=None, nma_priority=None, event=common.notifyStrings[common.NOTIFY_SNATCH], message=ep_name)

    def notify_download(self, ep_name):
        if sickbeard.NMA_NOTIFY_ONDOWNLOAD:
            self._sendNMA(nma_api=None, nma_priority=None, event=common.notifyStrings[common.NOTIFY_DOWNLOAD], message=ep_name)

    def test_notify(self, nma_api, nma_priority):
        return self._sendNMA(nma_api, nma_priority, event="Test", message="Testing NMA settings from Sick Beard", force=True)

    def update_library(self, ep_obj=None):
        pass

notifier = NMA_Notifier

########NEW FILE########
__FILENAME__ = nmj
# Author: Nico Berlee http://nico.berlee.nl/
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib
import urllib2
import sickbeard
import telnetlib
import re

from sickbeard import logger
from sickbeard.exceptions import ex

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import xml.etree.ElementTree as etree


class NMJNotifier:
    def notify_settings(self, host):
        """
        Retrieves the settings from a NMJ/Popcorn Hour

        host: The hostname/IP of the Popcorn Hour server

        Returns: True if the settings were retrieved successfully, False otherwise
        """

        # establish a terminal session to the PC
        terminal = False
        try:
            terminal = telnetlib.Telnet(host)
        except Exception:
            logger.log(u"NMJ: Unable to get a telnet session to %s" % (host), logger.WARNING)
            return False

        # tell the terminal to output the necessary info to the screen so we can search it later
        logger.log(u"NMJ: Connected to %s via telnet" % (host), logger.DEBUG)
        terminal.read_until("sh-3.00# ")
        terminal.write("cat /tmp/source\n")
        terminal.write("cat /tmp/netshare\n")
        terminal.write("exit\n")
        tnoutput = terminal.read_all()

        database = ""
        device = ""
        match = re.search(r"(.+\.db)\r\n?(.+)(?=sh-3.00# cat /tmp/netshare)", tnoutput)

        # if we found the database in the terminal output then save that database to the config
        if match:
            database = match.group(1)
            device = match.group(2)
            logger.log(u"NMJ: Found NMJ database %s on device %s" % (database, device), logger.DEBUG)
            sickbeard.NMJ_DATABASE = database
        else:
            logger.log(u"NMJ: Could not get current NMJ database on %s, NMJ is probably not running!" % (host), logger.WARNING)
            return False

        # if the device is a remote host then try to parse the mounting URL and save it to the config
        if device.startswith("NETWORK_SHARE/"):
            match = re.search(".*(?=\r\n?%s)" % (re.escape(device[14:])), tnoutput)

            if match:
                mount = match.group().replace("127.0.0.1", host)
                logger.log(u"NMJ: Found mounting url on the Popcorn Hour in configuration: %s" % (mount), logger.DEBUG)
                sickbeard.NMJ_MOUNT = mount
            else:
                logger.log(u"NMJ: Detected a network share on the Popcorn Hour, but could not get the mounting url", logger.WARNING)
                return False

        return True

    def _sendNMJ(self, host, database, mount=None):
        """
        Sends a NMJ update command to the specified machine

        host: The hostname/IP to send the request to (no port)
        database: The database to send the request to
        mount: The mount URL to use (optional)

        Returns: True if the request succeeded, False otherwise
        """

        # if a mount URL is provided then attempt to open a handle to that URL
        if mount:
            try:
                req = urllib2.Request(mount)
                logger.log(u"NMJ: Try to mount network drive via url: %s" % (mount), logger.DEBUG)
                handle = urllib2.urlopen(req)
            except IOError, e:
                if hasattr(e, 'reason'):
                    logger.log(u"NMJ: Could not contact Popcorn Hour on host %s: %s" % (host, e.reason), logger.WARNING)
                elif hasattr(e, 'code'):
                    logger.log(u"NMJ: Problem with Popcorn Hour on host %s: %s" % (host, e.code), logger.WARNING)
                return False
            except Exception, e:
                logger.log(u"NMJ: Unknown exception: " + ex(e), logger.ERROR)
                return False

        # build up the request URL and parameters
        UPDATE_URL = "http://%(host)s:8008/metadata_database?%(params)s"
        params = {
            "arg0": "scanner_start",
            "arg1": database,
            "arg2": "background",
            "arg3": ""
        }
        params = urllib.urlencode(params)
        updateUrl = UPDATE_URL % {"host": host, "params": params}

        # send the request to the server
        try:
            req = urllib2.Request(updateUrl)
            logger.log(u"NMJ: Sending NMJ scan update command via url: %s" % (updateUrl), logger.DEBUG)
            handle = urllib2.urlopen(req)
            response = handle.read()
        except IOError, e:
            if hasattr(e, 'reason'):
                logger.log(u"NMJ: Could not contact Popcorn Hour on host %s: %s" % (host, e.reason), logger.WARNING)
            elif hasattr(e, 'code'):
                logger.log(u"NMJ: Problem with Popcorn Hour on host %s: %s" % (host, e.code), logger.WARNING)
            return False
        except Exception, e:
            logger.log(u"NMJ: Unknown exception: " + ex(e), logger.ERROR)
            return False

        # try to parse the resulting XML
        try:
            et = etree.fromstring(response)
            result = et.findtext("returnValue")
        except SyntaxError, e:
            logger.log(u"NMJ: Unable to parse XML returned from the Popcorn Hour: %s" % (e), logger.ERROR)
            return False

        # if the result was a number then consider that an error
        if int(result) > 0:
            logger.log(u"NMJ: Popcorn Hour returned an errorcode: %s" % (result), logger.ERROR)
            return False
        else:
            logger.log(u"NMJ: Started background scan.", logger.MESSAGE)
            return True

    def _notifyNMJ(self, host=None, database=None, mount=None, force=False):
        """
        Sends a NMJ update command based on the SB config settings

        host: The host to send the command to (optional, defaults to the host in the config)
        database: The database to use (optional, defaults to the database in the config)
        mount: The mount URL (optional, defaults to the mount URL in the config)
        force: If True then the notification will be sent even if NMJ is disabled in the config
        """
        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_NMJ and not force:
            return False

        # fill in omitted parameters
        if not host:
            host = sickbeard.NMJ_HOST
        if not database:
            database = sickbeard.NMJ_DATABASE
        if not mount:
            mount = sickbeard.NMJ_MOUNT

        logger.log(u"NMJ: Sending scan command.", logger.DEBUG)

        return self._sendNMJ(host, database, mount)

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        pass

    def notify_download(self, ep_name):
        pass

    def test_notify(self, host, database, mount):
        return self._notifyNMJ(host, database, mount, force=True)

    def update_library(self, ep_obj=None):
        if sickbeard.USE_NMJ:
            self._notifyNMJ()

notifier = NMJNotifier

########NEW FILE########
__FILENAME__ = nmjv2
# Author: Jasper Lanting
# Based on nmj.py by Nico Berlee: http://nico.berlee.nl/
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib2
from xml.dom.minidom import parseString
import sickbeard
import time

from sickbeard import logger

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import xml.etree.ElementTree as etree


class NMJv2Notifier:

    def notify_settings(self, host, dbloc, instance):
        """
        Retrieves the NMJv2 database location from Popcorn Hour

        host: The hostname/IP of the Popcorn Hour server
        dbloc: 'local' for PCH internal harddrive. 'network' for PCH network shares
        instance: Allows for selection of different DB in case of multiple databases

        Returns: True if the settings were retrieved successfully, False otherwise
        """
        try:
            url_loc = "http://" + host + ":8008/file_operation?arg0=list_user_storage_file&arg1=&arg2=" + instance + "&arg3=20&arg4=true&arg5=true&arg6=true&arg7=all&arg8=name_asc&arg9=false&arg10=false"
            req = urllib2.Request(url_loc)
            handle1 = urllib2.urlopen(req)
            response1 = handle1.read()
            # TODO: convert to etree?
            xml = parseString(response1)
            time.sleep(0.5)
            for node in xml.getElementsByTagName('path'):
                xmlTag = node.toxml()
                xmlData = xmlTag.replace('<path>', '').replace('</path>', '').replace('[=]', '')
                url_db = "http://" + host + ":8008/metadata_database?arg0=check_database&arg1=" + xmlData
                reqdb = urllib2.Request(url_db)
                handledb = urllib2.urlopen(reqdb)
                responsedb = handledb.read()
                xmldb = parseString(responsedb)
                returnvalue = xmldb.getElementsByTagName('returnValue')[0].toxml().replace('<returnValue>', '').replace('</returnValue>', '')
                if returnvalue == "0":
                    DB_path = xmldb.getElementsByTagName('database_path')[0].toxml().replace('<database_path>', '').replace('</database_path>', '').replace('[=]', '')
                    if dbloc == "local" and DB_path.find("localhost") > -1:
                        sickbeard.NMJv2_HOST = host
                        sickbeard.NMJv2_DATABASE = DB_path
                        return True
                    if dbloc == "network" and DB_path.find("://") > -1:
                        sickbeard.NMJv2_HOST = host
                        sickbeard.NMJv2_DATABASE = DB_path
                        return True
        except IOError, e:
            logger.log(u"NMJv2: Could not contact Popcorn Hour on host %s: %s" % (host, e), logger.WARNING)
            return False

        return False

    def _sendNMJ(self, host):
        """
        Sends a NMJ update command to the specified machine

        host: The hostname/IP to send the request to (no port)
        database: The database to send the request to
        mount: The mount URL to use (optional)

        Returns: True if the request succeeded, False otherwise
        """

        #if a host is provided then attempt to open a handle to that URL
        try:
            url_scandir = "http://" + host + ":8008/metadata_database?arg0=update_scandir&arg1=" + sickbeard.NMJv2_DATABASE + "&arg2=&arg3=update_all"
            logger.log(u"NMJv2: Scan update command send to host: %s" % (host), logger.DEBUG)
            url_updatedb = "http://" + host + ":8008/metadata_database?arg0=scanner_start&arg1=" + sickbeard.NMJv2_DATABASE + "&arg2=background&arg3="
            logger.log(u"NMJv2: Try to mount network drive via url: %s" % (host), logger.DEBUG)
            prereq = urllib2.Request(url_scandir)
            req = urllib2.Request(url_updatedb)
            handle1 = urllib2.urlopen(prereq)
            response1 = handle1.read()
            time.sleep(0.5)
            handle2 = urllib2.urlopen(req)
            response2 = handle2.read()
        except IOError, e:
            logger.log(u"NMJv2: Could not contact Popcorn Hour on host %s: %s" % (host, e), logger.WARNING)
            return False

        try:
            et = etree.fromstring(response1)
            result1 = et.findtext("returnValue")
        except SyntaxError, e:
            logger.log(u"NMJv2: Unable to parse XML returned from the Popcorn Hour: update_scandir, %s" % (e), logger.ERROR)
            return False

        try:
            et = etree.fromstring(response2)
            result2 = et.findtext("returnValue")
        except SyntaxError, e:
            logger.log(u"NMJv2: Unable to parse XML returned from the Popcorn Hour: scanner_start, %s" % (e), logger.ERROR)
            return False

        # if the result was a number then consider that an error
        error_codes = ["8", "11", "22", "49", "50", "51", "60"]
        error_messages = ["Invalid parameter(s)/argument(s)",
                        "Invalid database path",
                        "Insufficient size",
                        "Database write error",
                        "Database read error",
                        "Open fifo pipe failed",
                        "Read only file system"]

        if int(result1) > 0:
            index = error_codes.index(result1)
            logger.log(u"NMJv2: Popcorn Hour returned an error: %s" % (error_messages[index]), logger.ERROR)
            return False
        else:
            if int(result2) > 0:
                index = error_codes.index(result2)
                logger.log(u"NMJv2: Popcorn Hour returned an error: %s" % (error_messages[index]), logger.ERROR)
                return False
            else:
                logger.log(u"NMJv2: Started background scan.", logger.MESSAGE)
                return True

    def _notifyNMJ(self, host=None, force=False):
        """
        Sends a NMJ update command based on the SB config settings

        host: The host to send the command to (optional, defaults to the host in the config)
        database: The database to use (optional, defaults to the database in the config)
        mount: The mount URL (optional, defaults to the mount URL in the config)
        force: If True then the notification will be sent even if NMJ is disabled in the config
        """
        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_NMJv2 and not force:
            return False

        # fill in omitted parameters
        if not host:
            host = sickbeard.NMJv2_HOST

        logger.log(u"NMJv2: Sending scan command.", logger.DEBUG)

        return self._sendNMJ(host)

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        pass

    def notify_download(self, ep_name):
        pass

    def test_notify(self, host):
        return self._notifyNMJ(host, force=True)

    def update_library(self, ep_obj=None):
        if sickbeard.USE_NMJv2:
            self._notifyNMJ()

notifier = NMJv2Notifier

########NEW FILE########
__FILENAME__ = plex
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib
import urllib2
import base64

import sickbeard

from sickbeard import logger
from sickbeard import common
from sickbeard.exceptions import ex
from sickbeard.encodingKludge import fixStupidEncodings

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree


class PLEXNotifier:

    def _send_to_plex(self, command, host, username=None, password=None):
        """Handles communication to Plex hosts via HTTP API

        Args:
            command: Dictionary of field/data pairs, encoded via urllib and passed to the legacy xbmcCmds HTTP API
            host: Plex host:port
            username: Plex API username
            password: Plex API password

        Returns:
            Returns 'OK' for successful commands or False if there was an error

        """

        # fill in omitted parameters
        if not username:
            username = sickbeard.PLEX_USERNAME
        if not password:
            password = sickbeard.PLEX_PASSWORD

        if not host:
            logger.log(u"PLEX: No host specified, check your settings", logger.ERROR)
            return False

        for key in command:
            if type(command[key]) == unicode:
                command[key] = command[key].encode('utf-8')

        enc_command = urllib.urlencode(command)
        logger.log(u"PLEX: Encoded API command: " + enc_command, logger.DEBUG)

        url = 'http://%s/xbmcCmds/xbmcHttp/?%s' % (host, enc_command)
        try:
            req = urllib2.Request(url)
            # if we have a password, use authentication
            if password:
                base64string = base64.encodestring('%s:%s' % (username, password))[:-1]
                authheader = "Basic %s" % base64string
                req.add_header("Authorization", authheader)
                logger.log(u"PLEX: Contacting (with auth header) via url: " + url, logger.DEBUG)
            else:
                logger.log(u"PLEX: Contacting via url: " + url, logger.DEBUG)

            response = urllib2.urlopen(req)

            result = response.read().decode(sickbeard.SYS_ENCODING)
            response.close()

            logger.log(u"PLEX: HTTP response: " + result.replace('\n', ''), logger.DEBUG)
            # could return result response = re.compile('<html><li>(.+\w)</html>').findall(result)
            return 'OK'

        except (urllib2.URLError, IOError), e:
            logger.log(u"PLEX: Warning: Couldn't contact Plex at " + fixStupidEncodings(url) + " " + ex(e), logger.WARNING)
            return False

    def _notify(self, message, title="Sick Beard", host=None, username=None, password=None, force=False):
        """Internal wrapper for the notify_snatch and notify_download functions

        Args:
            message: Message body of the notice to send
            title: Title of the notice to send
            host: Plex Media Client(s) host:port
            username: Plex username
            password: Plex password
            force: Used for the Test method to override config safety checks

        Returns:
            Returns a list results in the format of host:ip:result
            The result will either be 'OK' or False, this is used to be parsed by the calling function.

        """

        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_PLEX and not force:
            return False

        # fill in omitted parameters
        if not host:
            host = sickbeard.PLEX_HOST
        if not username:
            username = sickbeard.PLEX_USERNAME
        if not password:
            password = sickbeard.PLEX_PASSWORD

        result = ''
        for curHost in [x.strip() for x in host.split(",")]:
            logger.log(u"PLEX: Sending notification to '" + curHost + "' - " + message, logger.MESSAGE)

            command = {'command': 'ExecBuiltIn', 'parameter': 'Notification(' + title.encode("utf-8") + ',' + message.encode("utf-8") + ')'}
            notifyResult = self._send_to_plex(command, curHost, username, password)
            if notifyResult:
                result += curHost + ':' + str(notifyResult)

        return result

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.PLEX_NOTIFY_ONSNATCH:
            self._notify(ep_name, common.notifyStrings[common.NOTIFY_SNATCH])

    def notify_download(self, ep_name):
        if sickbeard.PLEX_NOTIFY_ONDOWNLOAD:
            self._notify(ep_name, common.notifyStrings[common.NOTIFY_DOWNLOAD])

    def test_notify(self, host, username, password):
        return self._notify("Testing Plex notifications from Sick Beard", "Test Notification", host, username, password, force=True)

    def update_library(self, ep_obj=None):
        """Handles updating the Plex Media Server host via HTTP API

        Plex Media Server currently only supports updating the whole video library and not a specific path.

        Returns:
            Returns True or False

        """

        if sickbeard.USE_PLEX and sickbeard.PLEX_UPDATE_LIBRARY:
            if not sickbeard.PLEX_SERVER_HOST:
                logger.log(u"PLEX: No Plex Media Server host specified, check your settings", logger.DEBUG)
                return False

            logger.log(u"PLEX: Updating library for the Plex Media Server host: " + sickbeard.PLEX_SERVER_HOST, logger.MESSAGE)

            url = "http://%s/library/sections" % sickbeard.PLEX_SERVER_HOST
            try:
                xml_tree = etree.parse(urllib.urlopen(url))
                media_container = xml_tree.getroot()
            except IOError, e:
                logger.log(u"PLEX: Error while trying to contact Plex Media Server: " + ex(e), logger.ERROR)
                return False

            sections = media_container.findall('.//Directory')
            if not sections:
                logger.log(u"PLEX: Plex Media Server not running on: " + sickbeard.PLEX_SERVER_HOST, logger.MESSAGE)
                return False

            for section in sections:
                if section.attrib['type'] == "show":
                    url = "http://%s/library/sections/%s/refresh" % (sickbeard.PLEX_SERVER_HOST, section.attrib['key'])
                    try:
                        urllib.urlopen(url)
                    except Exception, e:
                        logger.log(u"PLEX: Error updating library section for Plex Media Server: " + ex(e), logger.ERROR)
                        return False

            return True

notifier = PLEXNotifier

########NEW FILE########
__FILENAME__ = prowl
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from httplib import HTTPSConnection
from urllib import urlencode

import sickbeard

from sickbeard.exceptions import ex
from sickbeard import common
from sickbeard import logger


class ProwlNotifier:

    def _notify(self, prowl_api=None, prowl_priority=None, event=None, message=None, force=False):

        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_PROWL and not force:
            return False

        # fill in omitted parameters
        if not prowl_api:
            prowl_api = sickbeard.PROWL_API
        if not prowl_priority:
            prowl_priority = sickbeard.PROWL_PRIORITY

        title = "Sick Beard"

        logger.log("PROWL: Sending notice with details: event=\"%s\", message=\"%s\", priority=%s, api=%s" % (event, message, prowl_priority, prowl_api), logger.DEBUG)

        try:

            http_handler = HTTPSConnection("api.prowlapp.com")

            data = {'apikey': prowl_api,
                    'application': title,
                    'event': event,
                    'description': message.encode('utf-8'),
                    'priority': prowl_priority
                    }

            http_handler.request("POST",
                                 "/publicapi/add",
                                 headers={'Content-type': "application/x-www-form-urlencoded"},
                                 body=urlencode(data)
                                 )

            response = http_handler.getresponse()
            request_status = response.status

        except Exception, e:
            logger.log(u"PROWL: Notification failed: " + ex(e), logger.ERROR)
            return False

        if request_status == 200:
            logger.log(u"PROWL: Notifications sent.", logger.MESSAGE)
            return True
        elif request_status == 401:
            logger.log(u"PROWL: Auth failed: %s" % response.reason, logger.ERROR)
            return False
        else:
            logger.log(u"PROWL: Notification failed.", logger.ERROR)
            return False

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.PROWL_NOTIFY_ONSNATCH:
            self._notify(prowl_api=None, prowl_priority=None, event=common.notifyStrings[common.NOTIFY_SNATCH], message=ep_name)

    def notify_download(self, ep_name):
        if sickbeard.PROWL_NOTIFY_ONDOWNLOAD:
            self._notify(prowl_api=None, prowl_priority=None, event=common.notifyStrings[common.NOTIFY_DOWNLOAD], message=ep_name)

    def test_notify(self, prowl_api, prowl_priority):
        return self._notify(prowl_api, prowl_priority, event="Test", message="Testing Prowl settings from Sick Beard", force=True)

    def update_library(self, ep_obj=None):
        pass

notifier = ProwlNotifier

########NEW FILE########
__FILENAME__ = pushover
# Author: Marvin Pinto <me@marvinp.ca>
# Author: Dennis Lutter <lad1337@gmail.com>
# Author: Aaron Bieber <deftly@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib
import urllib2
import time

import sickbeard

from sickbeard import logger
from sickbeard.common import notifyStrings, NOTIFY_SNATCH, NOTIFY_DOWNLOAD
from sickbeard.exceptions import ex

API_URL = "https://api.pushover.net/1/messages.json"
API_KEY = "OKCXmkvHN1syU2e8xvpefTnyvVWGv5"


class PushoverNotifier:

    def _sendPushover(self, msg, title, userKey=None):
        """
        Sends a pushover notification to the address provided

        msg: The message to send (unicode)
        title: The title of the message
        userKey: The pushover user id to send the message to (or to subscribe with)

        returns: True if the message succeeded, False otherwise
        """

        if not userKey:
            userKey = sickbeard.PUSHOVER_USERKEY

        # build up the URL and parameters
        msg = msg.strip()
        curUrl = API_URL

        data = urllib.urlencode({
            'token': API_KEY,
            'title': title,
            'user': userKey,
            'message': msg.encode('utf-8'),
            'timestamp': int(time.time())
            })

        # send the request to pushover
        try:
            req = urllib2.Request(curUrl)
            handle = urllib2.urlopen(req, data)
            handle.close()

        except urllib2.URLError, e:
            # if we get an error back that doesn't have an error code then who knows what's really happening
            if not hasattr(e, 'code'):
                logger.log(u"PUSHOVER: Notification failed." + ex(e), logger.ERROR)
                return False
            else:
                logger.log(u"PUSHOVER: Notification failed. Error code: " + str(e.code), logger.ERROR)

            # HTTP status 404 if the provided email address isn't a Pushover user.
            if e.code == 404:
                logger.log(u"PUSHOVER: Username is wrong/not a Pushover email. Pushover will send an email to it", logger.WARNING)
                return False

            # For HTTP status code 401's, it is because you are passing in either an invalid token, or the user has not added your service.
            elif e.code == 401:

                #HTTP status 401 if the user doesn't have the service added
                subscribeNote = self._sendPushover(msg, title, userKey )
                if subscribeNote:
                    logger.log(u"PUSHOVER: Subscription sent", logger.DEBUG)
                    return True
                else:
                    logger.log(u"PUSHOVER: Subscription could not be sent", logger.ERROR)
                    return False

            # If you receive an HTTP status code of 400, it is because you failed to send the proper parameters
            elif e.code == 400:
                logger.log(u"PUSHOVER: Wrong data sent to Pushover", logger.ERROR)
                return False

        logger.log(u"PUSHOVER: Notification successful.", logger.MESSAGE)
        return True

    def _notify(self, title, message, userKey=None, force=False):
        """
        Sends a pushover notification based on the provided info or SB config

        title: The title of the notification to send
        message: The message string to send
        userKey: The userKey to send the notification to
        """

        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_PUSHOVER and not force:
            return False

        # fill in omitted parameters
        if not userKey:
            userKey = sickbeard.PUSHOVER_USERKEY

        logger.log(u"PUSHOVER: Sending notification for " + message, logger.DEBUG)

        return self._sendPushover(message, title)

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.PUSHOVER_NOTIFY_ONSNATCH:
            self._notify(notifyStrings[NOTIFY_SNATCH], ep_name)

    def notify_download(self, ep_name):
        if sickbeard.PUSHOVER_NOTIFY_ONDOWNLOAD:
            self._notify(notifyStrings[NOTIFY_DOWNLOAD], ep_name)

    def test_notify(self, userKey=None):
        return self._notify("This is a test notification from Sick Beard", "Test", userKey, force=True)

    def update_library(self, ep_obj=None):
        pass

notifier = PushoverNotifier

########NEW FILE########
__FILENAME__ = pytivo
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import os
import sickbeard

from urllib import urlencode
from urllib2 import Request, urlopen

from sickbeard import logger
from sickbeard.exceptions import ex
from sickbeard import encodingKludge as ek


class pyTivoNotifier:

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        pass

    def notify_download(self, ep_name):
        pass

    def update_library(self, ep_obj=None):

        if not sickbeard.USE_PYTIVO:
            return False

        host = sickbeard.PYTIVO_HOST
        shareName = sickbeard.PYTIVO_SHARE_NAME
        tsn = sickbeard.PYTIVO_TIVO_NAME

        # There are two more values required, the container and file.
        #
        # container: The share name, show name and season
        #
        # file: The file name
        #
        # Some slicing and dicing of variables is required to get at these values.

        # Calculated values
        showPath = ep_obj.show.location
        showName = ep_obj.show.name
        rootShowAndSeason = ek.ek(os.path.dirname, ep_obj.location)
        absPath = ep_obj.location

        # Some show names have colons in them which are illegal in a path location, so strip them out.
        # (Are there other characters?)
        showName = showName.replace(":", "")

        root = showPath.replace(showName, "")
        showAndSeason = rootShowAndSeason.replace(root, "")

        container = shareName + "/" + showAndSeason
        mediaFile = "/" + absPath.replace(root, "")

        # Finally create the url and make request
        requestUrl = "http://" + host + "/TiVoConnect?" + urlencode( {'Command': 'Push', 'Container': container, 'File': mediaFile, 'tsn': tsn})

        logger.log(u"PYTIVO: Requesting " + requestUrl, logger.DEBUG)

        request = Request(requestUrl)

        try:
            response = urlopen(request)  # @UnusedVariable
        except IOError, e:
            if hasattr(e, 'reason'):
                logger.log(u"PYTIVO: Failed to reach server '%s' - %s" % (host, e.reason), logger.WARNING)
            elif hasattr(e, 'code'):
                logger.log(u"PYTIVO: The server could not fulfill the request '%s' - %s" % (host, e.code), logger.WARNING)
            return False
        except Exception, e:
            logger.log(u"PYTIVO: Unknown exception: " + ex(e), logger.ERROR)
            return False
        else:
            logger.log(u"PYTIVO: Successfully requested transfer of file", logger.MESSAGE)
            return True

notifier = pyTivoNotifier

########NEW FILE########
__FILENAME__ = synoindex
# Author: Sebastien Erard <sebastien_erard@hotmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import os
import subprocess

import sickbeard

from sickbeard import logger
from sickbeard.common import notifyStrings, NOTIFY_SNATCH, NOTIFY_DOWNLOAD
from sickbeard import encodingKludge as ek
from sickbeard.exceptions import ex


class synoIndexNotifier:

    def moveFolder(self, old_path, new_path):
        self.moveObject(old_path, new_path)

    def moveFile(self, old_file, new_file):
        self.moveObject(old_file, new_file)

    def moveObject(self, old_path, new_path):
        if sickbeard.USE_SYNOINDEX:
            synoindex_cmd = ['/usr/syno/bin/synoindex', '-N', ek.ek(os.path.abspath, new_path), ek.ek(os.path.abspath, old_path)]
            logger.log(u"SYNOINDEX: Executing command " + str(synoindex_cmd), logger.DEBUG)
            logger.log(u"SYNOINDEX: Absolute path to command: " + ek.ek(os.path.abspath, synoindex_cmd[0]), logger.DEBUG)
            try:
                p = subprocess.Popen(synoindex_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=sickbeard.PROG_DIR)
                out, err = p.communicate()  # @UnusedVariable
                logger.log(u"SYNOINDEX: Script result: " + str(out), logger.DEBUG)
            except OSError, e:
                logger.log(u"SYNOINDEX: Unable to run synoindex: " + ex(e), logger.WARNING)

    def deleteFolder(self, cur_path):
        self.makeObject('-D', cur_path)

    def addFolder(self, cur_path):
        self.makeObject('-A', cur_path)

    def deleteFile(self, cur_file):
        self.makeObject('-d', cur_file)

    def addFile(self, cur_file):
        self.makeObject('-a', cur_file)

    def makeObject(self, cmd_arg, cur_path):
        if sickbeard.USE_SYNOINDEX:
            synoindex_cmd = ['/usr/syno/bin/synoindex', cmd_arg, ek.ek(os.path.abspath, cur_path)]
            logger.log(u"SYNOINDEX: Executing command " + str(synoindex_cmd), logger.DEBUG)
            logger.log(u"SYNOINDEX: Absolute path to command: " + ek.ek(os.path.abspath, synoindex_cmd[0]), logger.DEBUG)
            try:
                p = subprocess.Popen(synoindex_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=sickbeard.PROG_DIR)
                out, err = p.communicate()  # @UnusedVariable
                logger.log(u"SYNOINDEX: Script result: " + str(out), logger.DEBUG)
            except OSError, e:
                logger.log(u"SYNOINDEX: Unable to run synoindex: " + ex(e), logger.WARNING)

    def _notify(self, message, title, force=False):
        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_SYNOINDEX and not force:
            return False

        synodsmnotify_cmd = ['/usr/syno/bin/synodsmnotify', '@administrators', title, message]
        logger.log(u"SYNOINDEX: Executing command " + str(synodsmnotify_cmd), logger.DEBUG)

        try:
            p = subprocess.Popen(synodsmnotify_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                                 cwd=sickbeard.PROG_DIR)

            output, err = p.communicate()  # @UnusedVariable
            exit_status = p.returncode

            logger.log(u"SYNOINDEX: Script result: " + str(output), logger.DEBUG)

            if exit_status == 0:
                return True
            else:
                return False

        except OSError, e:
            logger.log(u"SYNOINDEX: Unable to run synodsmnotify: " + ex(e), logger.WARNING)
            return False

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.SYNOINDEX_NOTIFY_ONSNATCH:
            self._notify(notifyStrings[NOTIFY_SNATCH], ep_name)

    def notify_download(self, ep_name):
        if sickbeard.SYNOINDEX_NOTIFY_ONDOWNLOAD:
            self._notify(notifyStrings[NOTIFY_DOWNLOAD], ep_name)

    def test_notify(self):
        return self._notify("This is a test notification from Sick Beard", "Test", force=True)

    def update_library(self, ep_obj=None):
        if sickbeard.USE_SYNOINDEX:
            self.addFile(ep_obj.location)

notifier = synoIndexNotifier

########NEW FILE########
__FILENAME__ = trakt
# Author: Dieter Blomme <dieterblomme@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib2

from hashlib import sha1

try:
    import json
except ImportError:
    from lib import simplejson as json

import sickbeard

from sickbeard import logger


class TraktNotifier:

    def _notifyTrakt(self, method, api, username, password, data={}, force=False):
        """
        A generic method for communicating with trakt. Uses the method and data provided along
        with the auth info to send the command.

        method: The URL to use at trakt, relative, no leading slash.
        api: The API string to provide to trakt
        username: The username to use when logging in
        password: The unencrypted password to use when logging in

        Returns: A boolean representing success
        """
        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_TRAKT and not force:
            return False

        logger.log(u"TRAKT: Calling method " + method, logger.DEBUG)

        # if the API isn't given then use the config API
        if not api:
            api = sickbeard.TRAKT_API

        # if the username isn't given then use the config username
        if not username:
            username = sickbeard.TRAKT_USERNAME

        # if the password isn't given then use the config password
        if not password:
            password = sickbeard.TRAKT_PASSWORD
        password = sha1(password).hexdigest()

        # append apikey to method
        method += api

        data["username"] = username
        data["password"] = password

        # take the URL params and make a json object out of them
        encoded_data = json.dumps(data)

        # request the URL from trakt and parse the result as json
        try:
            logger.log(u"TRAKT: Calling method http://api.trakt.tv/" + method + ", with data" + encoded_data, logger.DEBUG)
            stream = urllib2.urlopen("http://api.trakt.tv/" + method, encoded_data)
            resp = stream.read()

            resp = json.loads(resp)

            if ("error" in resp):
                raise Exception(resp["error"])

        except (IOError):
            logger.log(u"TRAKT: Failed calling method", logger.ERROR)
            return False

        if (resp["status"] == "success"):
            logger.log(u"TRAKT: Succeeded calling method. Result: " + resp["message"], logger.MESSAGE)
            return True

        logger.log(u"TRAKT: Failed calling method", logger.ERROR)
        return False

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        pass

    def notify_download(self, ep_name):
        pass

    def test_notify(self, api, username, password):
        """
        Sends a test notification to trakt with the given authentication info and returns a boolean
        representing success.

        api: The api string to use
        username: The username to use
        password: The password to use

        Returns: True if the request succeeded, False otherwise
        """

        method = "account/test/"
        return self._notifyTrakt(method, api, username, password, {}, force=True)

    def update_library(self, ep_obj=None):
        """
        Sends a request to trakt indicating that the given episode is part of our library.

        ep_obj: The TVEpisode object to add to trakt
        """

        if sickbeard.USE_TRAKT:
            method = "show/episode/library/"

            # URL parameters
            data = {
                    'tvdb_id': ep_obj.show.tvdbid,
                    'title': ep_obj.show.name,
                    'year': ep_obj.show.startyear,
                    'episodes': [ {
                                   'season': ep_obj.season,
                                   'episode': ep_obj.episode
                                   } ]
                    }

            if data is not None:
                self._notifyTrakt(method, None, None, None, data)

notifier = TraktNotifier

########NEW FILE########
__FILENAME__ = tweet
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import sickbeard

from sickbeard import logger, common
from sickbeard.exceptions import ex

# parse_qsl moved to urlparse module in v2.6
try:
    from urlparse import parse_qsl  # @UnusedImport
except:
    from cgi import parse_qsl  # @Reimport

import lib.oauth2 as oauth
import lib.pythontwitter as twitter


class TwitterNotifier:

    consumer_key = "vHHtcB6WzpWDG6KYlBMr8g"
    consumer_secret = "zMqq5CB3f8cWKiRO2KzWPTlBanYmV0VYxSXZ0Pxds0E"

    REQUEST_TOKEN_URL = "https://api.twitter.com/oauth/request_token"
    ACCESS_TOKEN_URL = "https://api.twitter.com/oauth/access_token"
    AUTHORIZATION_URL = "https://api.twitter.com/oauth/authorize"
    SIGNIN_URL = "https://api.twitter.com/oauth/authenticate"

    def _get_authorization(self):

        signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()  # @UnusedVariable
        oauth_consumer = oauth.Consumer(key=self.consumer_key, secret=self.consumer_secret)
        oauth_client = oauth.Client(oauth_consumer)

        logger.log(u'TWITTER: Requesting temp token from Twitter', logger.DEBUG)

        resp, content = oauth_client.request(self.REQUEST_TOKEN_URL, 'GET')

        if resp['status'] != '200':
            logger.log(u"TWITTER: Invalid respond from Twitter requesting temp token: %s" % resp['status'], logger.ERROR)
        else:
            request_token = dict(parse_qsl(content))

            sickbeard.TWITTER_USERNAME = request_token['oauth_token']
            sickbeard.TWITTER_PASSWORD = request_token['oauth_token_secret']

            return self.AUTHORIZATION_URL + "?oauth_token=" + request_token['oauth_token']

    def _get_credentials(self, key):
        request_token = {}

        request_token['oauth_token'] = sickbeard.TWITTER_USERNAME
        request_token['oauth_token_secret'] = sickbeard.TWITTER_PASSWORD
        request_token['oauth_callback_confirmed'] = 'true'

        token = oauth.Token(request_token['oauth_token'], request_token['oauth_token_secret'])
        token.set_verifier(key)

        logger.log(u"TWITTER: Generating and signing request for an access token using key " + key, logger.DEBUG)

        signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()  # @UnusedVariable
        oauth_consumer = oauth.Consumer(key=self.consumer_key, secret=self.consumer_secret)
        logger.log(u"TWITTER: oauth_consumer: " + str(oauth_consumer), logger.DEBUG)
        oauth_client = oauth.Client(oauth_consumer, token)
        logger.log(u"TWITTER: oauth_client: " + str(oauth_client), logger.DEBUG)
        resp, content = oauth_client.request(self.ACCESS_TOKEN_URL, method='POST', body='oauth_verifier=%s' % key)
        logger.log(u"TWITTER: resp, content: " + str(resp) + "," + str(content), logger.DEBUG)

        access_token = dict(parse_qsl(content))
        logger.log(u"TWITTER: access_token: " + str(access_token), logger.DEBUG)

        logger.log(u"TWITTER: resp[status] = " + str(resp['status']), logger.DEBUG)
        if resp['status'] != '200':
            logger.log(u"TWITTER: The request for a token with did not succeed: " + str(resp['status']), logger.ERROR)
            return False
        else:
            logger.log(u"TWITTER: Your Twitter Access Token key: %s" % access_token['oauth_token'], logger.DEBUG)
            logger.log(u"TWITTER: Access Token secret: %s" % access_token['oauth_token_secret'], logger.DEBUG)
            sickbeard.TWITTER_USERNAME = access_token['oauth_token']
            sickbeard.TWITTER_PASSWORD = access_token['oauth_token_secret']
            return True

    def _send_tweet(self, message=None):

        username = self.consumer_key
        password = self.consumer_secret
        access_token_key = sickbeard.TWITTER_USERNAME
        access_token_secret = sickbeard.TWITTER_PASSWORD

        logger.log(u"TWITTER: Sending tweet: " + message, logger.DEBUG)

        api = twitter.Api(username, password, access_token_key, access_token_secret)

        try:
            api.PostUpdate(message)
        except Exception, e:
            logger.log(u"TWITTER: Error Sending Tweet: " + ex(e), logger.ERROR)
            return False

        return True

    def _notify(self, message='', force=False):
        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_TWITTER and not force:
            return False

        return self._send_tweet(sickbeard.TWITTER_PREFIX + ": " + message)

##############################################################################
# Public functions
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.TWITTER_NOTIFY_ONSNATCH:
            self._notify(common.notifyStrings[common.NOTIFY_SNATCH] + ': ' + ep_name)

    def notify_download(self, ep_name):
        if sickbeard.TWITTER_NOTIFY_ONDOWNLOAD:
            self._notify(common.notifyStrings[common.NOTIFY_DOWNLOAD] + ': ' + ep_name)

    def test_notify(self):
        return self._notify("This is a test notification from Sick Beard", force=True)

    def update_library(self, ep_obj):
        pass

notifier = TwitterNotifier

########NEW FILE########
__FILENAME__ = xbmc
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib
import urllib2
import socket
import base64
import time

import sickbeard

from sickbeard import logger
from sickbeard import common
from sickbeard.exceptions import ex
from sickbeard.encodingKludge import fixStupidEncodings

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import xml.etree.ElementTree as etree

try:
    import json
except ImportError:
    from lib import simplejson as json


class XBMCNotifier:

    sb_logo_url = "http://www.sickbeard.com/notify.png"

    def _get_xbmc_version(self, host, username, password):
        """Returns XBMC JSON-RPC API version (odd # = dev, even # = stable)

        Sends a request to the XBMC host using the JSON-RPC to determine if
        the legacy API or if the JSON-RPC API functions should be used.

        Fallback to testing legacy HTTPAPI before assuming it is just a badly configured host.

        Args:
            host: XBMC webserver host:port
            username: XBMC webserver username
            password: XBMC webserver password

        Returns:
            Returns API number or False

            List of possible known values:
                API | XBMC Version
               -----+---------------
                 2  | v10 (Dharma)
                 3  | (pre Eden)
                 4  | v11 (Eden)
                 5  | (pre Frodo)
                 6  | v12 (Frodo) / v13 (Gotham)

        """

        # since we need to maintain python 2.5 compatibility we can not pass a timeout delay to urllib2 directly (python 2.6+)
        # override socket timeout to reduce delay for this call alone
        socket.setdefaulttimeout(10)

        checkCommand = '{"jsonrpc":"2.0","method":"JSONRPC.Version","id":1}'
        result = self._send_to_xbmc_json(checkCommand, host, username, password)

        # revert back to default socket timeout
        socket.setdefaulttimeout(sickbeard.SOCKET_TIMEOUT)

        if result:
            return result["result"]["version"]
        else:
            # fallback to legacy HTTPAPI method
            testCommand = {'command': 'Help'}
            request = self._send_to_xbmc(testCommand, host, username, password)
            if request:
                # return a fake version number, so it uses the legacy method
                return 1
            else:
                return False

    def _notify(self, message, title="Sick Beard", host=None, username=None, password=None, force=False):
        """Internal wrapper for the notify_snatch and notify_download functions

        Detects JSON-RPC version then branches the logic for either the JSON-RPC or legacy HTTP API methods.

        Args:
            message: Message body of the notice to send
            title: Title of the notice to send
            host: XBMC webserver host:port
            username: XBMC webserver username
            password: XBMC webserver password
            force: Used for the Test method to override config safety checks

        Returns:
            Returns a list results in the format of host:ip:result
            The result will either be 'OK' or False, this is used to be parsed by the calling function.

        """

        # suppress notifications if the notifier is disabled but the notify options are checked
        if not sickbeard.USE_XBMC and not force:
            return False

        # fill in omitted parameters
        if not host:
            host = sickbeard.XBMC_HOST
        if not username:
            username = sickbeard.XBMC_USERNAME
        if not password:
            password = sickbeard.XBMC_PASSWORD

        result = ''
        for curHost in [x.strip() for x in host.split(",")]:
            logger.log(u"XBMC: Sending XBMC notification to '" + curHost + "' - " + message, logger.MESSAGE)

            xbmcapi = self._get_xbmc_version(curHost, username, password)
            if xbmcapi:
                if (xbmcapi <= 4):
                    logger.log(u"XBMC: Detected XBMC version <= 11, using XBMC HTTP API", logger.DEBUG)
                    command = {'command': 'ExecBuiltIn', 'parameter': 'Notification(' + title.encode("utf-8") + ',' + message.encode("utf-8") + ')'}
                    notifyResult = self._send_to_xbmc(command, curHost, username, password)
                    if notifyResult:
                        result += curHost + ':' + str(notifyResult)
                else:
                    logger.log(u"XBMC: Detected XBMC version >= 12, using XBMC JSON API", logger.DEBUG)
                    command = '{"jsonrpc":"2.0","method":"GUI.ShowNotification","params":{"title":"%s","message":"%s", "image": "%s"},"id":1}' % (title.encode("utf-8"), message.encode("utf-8"), self.sb_logo_url)
                    notifyResult = self._send_to_xbmc_json(command, curHost, username, password)
                    if notifyResult:
                        result += curHost + ':' + notifyResult["result"].decode(sickbeard.SYS_ENCODING)
            else:
                if sickbeard.XBMC_ALWAYS_ON or force:
                    logger.log(u"XBMC: Failed to detect XBMC version for '" + curHost + "', check configuration and try again.", logger.ERROR)
                result += curHost + ':False'

        return result

##############################################################################
# Legacy HTTP API (pre XBMC 12) methods
##############################################################################

    def _send_to_xbmc(self, command, host=None, username=None, password=None):
        """Handles communication to XBMC servers via HTTP API

        Args:
            command: Dictionary of field/data pairs, encoded via urllib and passed to the XBMC API via HTTP
            host: XBMC webserver host:port
            username: XBMC webserver username
            password: XBMC webserver password

        Returns:
            Returns response.result for successful commands or False if there was an error

        """

        # fill in omitted parameters
        if not username:
            username = sickbeard.XBMC_USERNAME
        if not password:
            password = sickbeard.XBMC_PASSWORD

        if not host:
            logger.log(u"XBMC: No host specified, check your settings", logger.DEBUG)
            return False

        for key in command:
            if type(command[key]) == unicode:
                command[key] = command[key].encode('utf-8')

        enc_command = urllib.urlencode(command)
        logger.log(u"XBMC: Encoded API command: " + enc_command, logger.DEBUG)

        url = 'http://%s/xbmcCmds/xbmcHttp/?%s' % (host, enc_command)
        try:
            req = urllib2.Request(url)
            # if we have a password, use authentication
            if password:
                base64string = base64.encodestring('%s:%s' % (username, password))[:-1]
                authheader = "Basic %s" % base64string
                req.add_header("Authorization", authheader)

            response = urllib2.urlopen(req)
            result = response.read().decode(sickbeard.SYS_ENCODING)
            response.close()

            logger.log(u"XBMC: HTTP response: " + result.replace('\n', ''), logger.DEBUG)
            return result

        except (urllib2.URLError, IOError), e:
            logger.log(u"XBMC: Could not contact XBMC HTTP at " + fixStupidEncodings(url) + " " + ex(e), logger.WARNING)
            return False

    def _update_library(self, host=None, showName=None):
        """Handles updating XBMC host via HTTP API

        Attempts to update the XBMC video library for a specific tv show if passed,
        otherwise update the whole library if enabled.

        Args:
            host: XBMC webserver host:port
            showName: Name of a TV show to specifically target the library update for

        Returns:
            Returns True or False

        """

        if not host:
            logger.log(u"XBMC: No host specified, check your settings", logger.DEBUG)
            return False

        # if we're doing per-show
        if showName:
            logger.log(u"XBMC: Updating library via HTTP method for show " + showName, logger.MESSAGE)

            pathSql = 'select path.strPath from path, tvshow, tvshowlinkpath where ' \
                'tvshow.c00 = "%s" and tvshowlinkpath.idShow = tvshow.idShow ' \
                'and tvshowlinkpath.idPath = path.idPath' % (showName)

            # use this to get xml back for the path lookups
            xmlCommand = {'command': 'SetResponseFormat(webheader;false;webfooter;false;header;<xml>;footer;</xml>;opentag;<tag>;closetag;</tag>;closefinaltag;false)'}
            # sql used to grab path(s)
            sqlCommand = {'command': 'QueryVideoDatabase(%s)' % (pathSql)}
            # set output back to default
            resetCommand = {'command': 'SetResponseFormat()'}

            # set xml response format, if this fails then don't bother with the rest
            request = self._send_to_xbmc(xmlCommand, host)
            if not request:
                return False

            sqlXML = self._send_to_xbmc(sqlCommand, host)
            request = self._send_to_xbmc(resetCommand, host)

            if not sqlXML:
                logger.log(u"XBMC: Invalid response for " + showName + " on " + host, logger.DEBUG)
                return False

            encSqlXML = urllib.quote(sqlXML, ':\\/<>')
            try:
                et = etree.fromstring(encSqlXML)
            except SyntaxError, e:
                logger.log(u"XBMC: Unable to parse XML returned from XBMC: " + ex(e), logger.ERROR)
                return False

            paths = et.findall('.//field')

            if not paths:
                logger.log(u"XBMC: No valid paths found for " + showName + " on " + host, logger.DEBUG)
                return False

            for path in paths:
                # we do not need it double-encoded, gawd this is dumb
                unEncPath = urllib.unquote(path.text).decode(sickbeard.SYS_ENCODING)
                logger.log(u"XBMC: Updating " + showName + " on " + host + " at " + unEncPath, logger.MESSAGE)
                updateCommand = {'command': 'ExecBuiltIn', 'parameter': 'XBMC.updatelibrary(video, %s)' % (unEncPath)}
                request = self._send_to_xbmc(updateCommand, host)
                if not request:
                    logger.log(u"XBMC: Update of show directory failed on " + showName + " on " + host + " at " + unEncPath, logger.WARNING)
                    return False
                # sleep for a few seconds just to be sure xbmc has a chance to finish each directory
                if len(paths) > 1:
                    time.sleep(5)
        # do a full update if requested
        else:
            logger.log(u"XBMC: Doing Full Library update via HTTP method for host: " + host, logger.MESSAGE)
            updateCommand = {'command': 'ExecBuiltIn', 'parameter': 'XBMC.updatelibrary(video)'}
            request = self._send_to_xbmc(updateCommand, host)

            if not request:
                logger.log(u"XBMC: Full Library update failed on: " + host, logger.ERROR)
                return False

        return True

##############################################################################
# JSON-RPC API (XBMC 12+) methods
##############################################################################

    def _send_to_xbmc_json(self, command, host=None, username=None, password=None):
        """Handles communication to XBMC servers via JSONRPC

        Args:
            command: Dictionary of field/data pairs, encoded via urllib and passed to the XBMC JSON-RPC via HTTP
            host: XBMC webserver host:port
            username: XBMC webserver username
            password: XBMC webserver password

        Returns:
            Returns response.result for successful commands or False if there was an error

        """

        # fill in omitted parameters
        if not username:
            username = sickbeard.XBMC_USERNAME
        if not password:
            password = sickbeard.XBMC_PASSWORD

        if not host:
            logger.log(u"XBMC: No host specified, check your settings", logger.DEBUG)
            return False

        command = command.encode('utf-8')
        logger.log(u"XBMC: JSON command: " + command, logger.DEBUG)

        url = 'http://%s/jsonrpc' % (host)
        try:
            req = urllib2.Request(url, command)
            req.add_header("Content-type", "application/json")
            # if we have a password, use authentication
            if password:
                base64string = base64.encodestring('%s:%s' % (username, password))[:-1]
                authheader = "Basic %s" % base64string
                req.add_header("Authorization", authheader)

            try:
                response = urllib2.urlopen(req)
            except urllib2.URLError, e:
                logger.log(u"XBMC: Error while trying to retrieve XBMC API version for " + host + ": " + ex(e), logger.WARNING)
                return False

            # parse the json result
            try:
                result = json.load(response)
                response.close()
                logger.log(u"XBMC: JSON response: " + str(result), logger.DEBUG)
                return result  # need to return response for parsing
            except ValueError, e:
                logger.log(u"XBMC: Unable to decode JSON: " + response, logger.WARNING)
                return False

        except IOError, e:
            logger.log(u"XBMC: Could not contact XBMC JSON API at " + fixStupidEncodings(url) + " " + ex(e), logger.WARNING)
            return False

    def _update_library_json(self, host=None, showName=None):
        """Handles updating XBMC host via HTTP JSON-RPC

        Attempts to update the XBMC video library for a specific tv show if passed,
        otherwise update the whole library if enabled.

        Args:
            host: XBMC webserver host:port
            showName: Name of a TV show to specifically target the library update for

        Returns:
            Returns True or False

        """

        if not host:
            logger.log(u"XBMC: No host specified, check your settings", logger.DEBUG)
            return False

        # if we're doing per-show
        if showName:
            tvshowid = -1
            logger.log(u"XBMC: Updating library via JSON method for show " + showName, logger.MESSAGE)

            # get tvshowid by showName
            showsCommand = '{"jsonrpc":"2.0","method":"VideoLibrary.GetTVShows","id":1}'
            showsResponse = self._send_to_xbmc_json(showsCommand, host)

            if showsResponse and "result" in showsResponse and "tvshows" in showsResponse["result"]:
                shows = showsResponse["result"]["tvshows"]
            else:
                logger.log(u"XBMC: No tvshows in XBMC TV show list", logger.DEBUG)
                return False

            for show in shows:
                if (show["label"] == showName):
                    tvshowid = show["tvshowid"]
                    break  # exit out of loop otherwise the label and showname will not match up

            # this can be big, so free some memory
            del shows

            # we didn't find the show (exact match), thus revert to just doing a full update if enabled
            if (tvshowid == -1):
                logger.log(u"XBMC: Exact show name not matched in XBMC TV show list", logger.DEBUG)
                return False

            # lookup tv-show path
            pathCommand = '{"jsonrpc":"2.0","method":"VideoLibrary.GetTVShowDetails","params":{"tvshowid":%d, "properties": ["file"]},"id":1}' % (tvshowid)
            pathResponse = self._send_to_xbmc_json(pathCommand, host)

            path = pathResponse["result"]["tvshowdetails"]["file"]
            logger.log(u"XBMC: Received Show: " + show["label"] + " with ID: " + str(tvshowid) + " Path: " + path, logger.DEBUG)

            if (len(path) < 1):
                logger.log(u"XBMC: No valid path found for " + showName + " with ID: " + str(tvshowid) + " on " + host, logger.WARNING)
                return False

            logger.log(u"XBMC: Updating " + showName + " on " + host + " at " + path, logger.MESSAGE)
            updateCommand = '{"jsonrpc":"2.0","method":"VideoLibrary.Scan","params":{"directory":%s},"id":1}' % (json.dumps(path))
            request = self._send_to_xbmc_json(updateCommand, host)
            if not request:
                logger.log(u"XBMC: Update of show directory failed on " + showName + " on " + host + " at " + path, logger.WARNING)
                return False

            # catch if there was an error in the returned request
            for r in request:
                if 'error' in r:
                    logger.log(u"XBMC: Error while attempting to update show directory for " + showName + " on " + host + " at " + path, logger.ERROR)
                    return False

        # do a full update if requested
        else:
            logger.log(u"XBMC: Doing Full Library update via JSON method for host: " + host, logger.MESSAGE)
            updateCommand = '{"jsonrpc":"2.0","method":"VideoLibrary.Scan","id":1}'
            request = self._send_to_xbmc_json(updateCommand, host, sickbeard.XBMC_USERNAME, sickbeard.XBMC_PASSWORD)

            if not request:
                logger.log(u"XBMC: Full Library update failed on: " + host, logger.ERROR)
                return False

        return True

##############################################################################
# Public functions which will call the JSON or Legacy HTTP API methods
##############################################################################

    def notify_snatch(self, ep_name):
        if sickbeard.XBMC_NOTIFY_ONSNATCH:
            self._notify(ep_name, common.notifyStrings[common.NOTIFY_SNATCH])

    def notify_download(self, ep_name):
        if sickbeard.XBMC_NOTIFY_ONDOWNLOAD:
            self._notify(ep_name, common.notifyStrings[common.NOTIFY_DOWNLOAD])

    def test_notify(self, host, username, password):
        return self._notify("Testing XBMC notifications from Sick Beard", "Test Notification", host, username, password, force=True)

    def update_library(self, ep_obj=None, show_obj=None):
        """Public wrapper for the update library functions to branch the logic for JSON-RPC or legacy HTTP API

        Checks the XBMC API version to branch the logic to call either the legacy HTTP API or the newer JSON-RPC over HTTP methods.
        Do the ability of accepting a list of hosts delimited by comma, we split off the first host to send the update to.
        This is a workaround for SQL backend users as updating multiple clients causes duplicate entries.
        Future plan is to revisit how we store the host/ip/username/pw/options so that it may be more flexible.

        Args:
            showName: Name of a TV show to specifically target the library update for

        Returns:
            Returns True or False

        """

        if ep_obj:
            showName = ep_obj.show.name
        elif show_obj:
            showName = show_obj.name
        else:
            showName = None

        if sickbeard.USE_XBMC and sickbeard.XBMC_UPDATE_LIBRARY:
            if not sickbeard.XBMC_HOST:
                logger.log(u"XBMC: No host specified, check your settings", logger.DEBUG)
                return False

            if sickbeard.XBMC_UPDATE_ONLYFIRST:
                # only send update to first host in the list if requested -- workaround for xbmc sql backend users
                host = sickbeard.XBMC_HOST.split(",")[0].strip()
            else:
                host = sickbeard.XBMC_HOST

            result = 0
            for curHost in [x.strip() for x in host.split(",")]:
                logger.log(u"XBMC: Sending request to update library for host: '" + curHost + "'", logger.MESSAGE)

                xbmcapi = self._get_xbmc_version(curHost, sickbeard.XBMC_USERNAME, sickbeard.XBMC_PASSWORD)
                if xbmcapi:
                    if (xbmcapi <= 4):
                        # try to update for just the show, if it fails, do full update if enabled
                        if not self._update_library(curHost, showName):
                            if showName and sickbeard.XBMC_UPDATE_FULL:
                                self._update_library(curHost)
                    else:
                        # try to update for just the show, if it fails, do full update if enabled
                        if not self._update_library_json(curHost, showName):
                            if showName and sickbeard.XBMC_UPDATE_FULL:
                                self._update_library_json(curHost)
                else:
                    if sickbeard.XBMC_ALWAYS_ON:
                        logger.log(u"XBMC: Failed to detect XBMC version for '" + curHost + "', check configuration and try again.", logger.ERROR)
                    result = result + 1

            # needed for the 'update xbmc' submenu command
            # as it only cares of the final result vs the individual ones
            if result == 0:
                return True
            else:
                return False

notifier = XBMCNotifier

########NEW FILE########
__FILENAME__ = nzbget
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.


import httplib
import datetime
import urllib
import urlparse

import sickbeard

from base64 import standard_b64encode
import xmlrpclib

from sickbeard.exceptions import ex
from sickbeard.providers.generic import GenericProvider
from sickbeard import config
from sickbeard import logger
from common import Quality


def sendNZB(nzb):

    if not sickbeard.NZBGET_HOST:
        logger.log(u"No NZBGet host found in configuration. Please configure it.", logger.ERROR)
        return False

    nzb_filename = nzb.name + ".nzb"

    try:
        url = config.clean_url(sickbeard.NZBGET_HOST)

        scheme, netloc, path, query, fragment = urlparse.urlsplit(url)  # @UnusedVariable

        if sickbeard.NZBGET_USERNAME or sickbeard.NZBGET_PASSWORD:
            netloc = urllib.quote_plus(sickbeard.NZBGET_USERNAME.encode("utf-8", 'ignore')) + u":" + urllib.quote_plus(sickbeard.NZBGET_PASSWORD.encode("utf-8", 'ignore')) + u"@" + netloc

        url = urlparse.urlunsplit((scheme, netloc, u"/xmlrpc", "", ""))

        logger.log(u"Sending NZB to NZBGet")
        logger.log(u"NZBGet URL: " + url, logger.DEBUG)

        nzbGetRPC = xmlrpclib.ServerProxy(url.encode("utf-8", 'ignore'))

        if nzbGetRPC.writelog("INFO", "SickBeard connected to drop off " + nzb_filename + " any moment now."):
            logger.log(u"Successful connected to NZBGet", logger.DEBUG)
        else:
            logger.log(u"Successful connected to NZBGet, but unable to send a message", logger.ERROR)

    except httplib.socket.error:
        logger.log(u"Please check if NZBGet is running. NZBGet is not responding.", logger.ERROR)
        return False

    except xmlrpclib.ProtocolError, e:
        if (e.errmsg == "Unauthorized"):
            logger.log(u"NZBGet username or password is incorrect.", logger.ERROR)
        else:
            logger.log(u"NZBGet protocol error: " + e.errmsg, logger.ERROR)
        return False

    except Exception, e:
        logger.log(u"NZBGet sendNZB failed. URL: " + url + " Error: " + ex(e), logger.ERROR)
        return False

    # if it aired recently make it high priority and generate dupekey/dupescore
    add_to_top = False
    nzbgetprio = dupescore = 0
    dupekey = ""

    for curEp in nzb.episodes:
        if dupekey == "":
            dupekey = "SickBeard-" + str(curEp.show.tvdbid)

        dupekey += "-" + str(curEp.season) + "." + str(curEp.episode)

        if datetime.date.today() - curEp.airdate <= datetime.timedelta(days=7):
            add_to_top = True
            nzbgetprio = 100

    # tweak dupescore based off quality, higher score wins
    if nzb.quality != Quality.UNKNOWN:
        dupescore = nzb.quality * 100

    if nzb.quality == Quality.SNATCHED_PROPER:
        dupescore += 10

    nzbget_result = None
    nzbcontent64 = None

    # if we get a raw data result we encode contents and pass that
    if nzb.resultType == "nzbdata":
        data = nzb.extraInfo[0]
        nzbcontent64 = standard_b64encode(data)

    logger.log(u"Attempting to send NZB to NZBGet (" + sickbeard.NZBGET_CATEGORY + ")", logger.DEBUG)

    try:
        # find out nzbget version to branch logic, 0.8.x and older will return 0
        nzbget_version_str = nzbGetRPC.version()
        nzbget_version = config.to_int(nzbget_version_str[:nzbget_version_str.find(".")])

        # v8 and older, no priority or dupe info
        if nzbget_version == 0:
            if nzbcontent64:
                nzbget_result = nzbGetRPC.append(nzb_filename, sickbeard.NZBGET_CATEGORY, add_to_top, nzbcontent64)

            else:
                # appendurl not supported on older versions, so d/l nzb data from url ourselves
                if nzb.resultType == "nzb":
                    genProvider = GenericProvider("")
                    data = genProvider.getURL(nzb.url)

                    if data:
                        nzbcontent64 = standard_b64encode(data)
                        nzbget_result = nzbGetRPC.append(nzb_filename, sickbeard.NZBGET_CATEGORY, add_to_top, nzbcontent64)

        # v12+ pass dupekey + dupescore
        elif nzbget_version >= 12:
            if nzbcontent64:
                nzbget_result = nzbGetRPC.append(nzb_filename, sickbeard.NZBGET_CATEGORY, nzbgetprio, False, nzbcontent64, False, dupekey, dupescore, "score")
            else:
                nzbget_result = nzbGetRPC.appendurl(nzb_filename, sickbeard.NZBGET_CATEGORY, nzbgetprio, False, nzb.url, False, dupekey, dupescore, "score")

        # v9+ pass priority, no dupe info
        else:
            if nzbcontent64:
                nzbget_result = nzbGetRPC.append(nzb_filename, sickbeard.NZBGET_CATEGORY, nzbgetprio, False, nzbcontent64)
            else:
                nzbget_result = nzbGetRPC.appendurl(nzb_filename, sickbeard.NZBGET_CATEGORY, nzbgetprio, False, nzb.url)

        if nzbget_result:
            logger.log(u"NZB sent to NZBGet successfully", logger.DEBUG)
            return True
        else:
            logger.log(u"NZBGet could not add " + nzb_filename + " to the queue", logger.ERROR)
            return False

    except:
        logger.log(u"Connect Error to NZBGet: could not add " + nzb_filename + " to the queue", logger.ERROR)
        return False

    return False

########NEW FILE########
__FILENAME__ = nzbSplitter
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import urllib2
import xml.etree.cElementTree as etree
import xml.etree
import re

from name_parser.parser import NameParser, InvalidNameException

from sickbeard import logger, classes, helpers
from sickbeard.common import Quality
from sickbeard import encodingKludge as ek
from sickbeard.exceptions import ex


def getSeasonNZBs(name, urlData, season):

    try:
        showXML = etree.ElementTree(etree.XML(urlData))
    except SyntaxError:
        logger.log(u"Unable to parse the XML of " + name + ", not splitting it", logger.ERROR)
        return ({}, '')

    filename = name.replace(".nzb", "")

    nzbElement = showXML.getroot()

    regex = '([\w\._\ ]+)[\. ]S%02d[\. ]([\w\._\-\ ]+)[\- ]([\w_\-\ ]+?)' % season

    sceneNameMatch = re.search(regex, filename, re.I)
    if sceneNameMatch:
        showName, qualitySection, groupName = sceneNameMatch.groups()  # @UnusedVariable
    else:
        logger.log(u"Unable to parse " + name + " into a scene name. If it's a valid one log a bug.", logger.ERROR)
        return ({}, '')

    regex = '(' + re.escape(showName) + '\.S%02d(?:[E0-9]+)\.[\w\._]+\-\w+' % season + ')'
    regex = regex.replace(' ', '.')

    epFiles = {}
    xmlns = None

    for curFile in nzbElement.getchildren():
        xmlnsMatch = re.match("\{(http:\/\/[A-Za-z0-9_\.\/]+\/nzb)\}file", curFile.tag)
        if not xmlnsMatch:
            continue
        else:
            xmlns = xmlnsMatch.group(1)
        match = re.search(regex, curFile.get("subject"), re.I)
        if not match:
            #print curFile.get("subject"), "doesn't match", regex
            continue
        curEp = match.group(1)
        if curEp not in epFiles:
            epFiles[curEp] = [curFile]
        else:
            epFiles[curEp].append(curFile)

    return (epFiles, xmlns)


def createNZBString(fileElements, xmlns):

    rootElement = etree.Element("nzb")
    if xmlns:
        rootElement.set("xmlns", xmlns)

    for curFile in fileElements:
        rootElement.append(stripNS(curFile, xmlns))

    return xml.etree.ElementTree.tostring(rootElement, 'utf-8')


def saveNZB(nzbName, nzbString):

    try:
        with ek.ek(open, nzbName + ".nzb", 'w') as nzb_fh:
            nzb_fh.write(nzbString)

    except EnvironmentError, e:
        logger.log(u"Unable to save NZB: " + ex(e), logger.ERROR)


def stripNS(element, ns):
    element.tag = element.tag.replace("{" + ns + "}", "")
    for curChild in element.getchildren():
        stripNS(curChild, ns)

    return element


def splitResult(result):

    urlData = helpers.getURL(result.url)

    if urlData is None:
        logger.log(u"Unable to load url " + result.url + ", can't download season NZB", logger.ERROR)
        return False

    # parse the season ep name
    try:
        np = NameParser(False)
        parse_result = np.parse(result.name)
    except InvalidNameException:
        logger.log(u"Unable to parse the filename " + result.name + " into a valid episode", logger.WARNING)
        return False

    # bust it up
    season = parse_result.season_number if parse_result.season_number != None else 1

    separateNZBs, xmlns = getSeasonNZBs(result.name, urlData, season)

    resultList = []

    for newNZB in separateNZBs:

        logger.log(u"Split out " + newNZB + " from " + result.name, logger.DEBUG)

        # parse the name
        try:
            np = NameParser(False)
            parse_result = np.parse(newNZB)
        except InvalidNameException:
            logger.log(u"Unable to parse the filename " + newNZB + " into a valid episode", logger.WARNING)
            return False

        # make sure the result is sane
        if (parse_result.season_number != None and parse_result.season_number != season) or (parse_result.season_number == None and season != 1):
            logger.log(u"Found " + newNZB + " inside " + result.name + " but it doesn't seem to belong to the same season, ignoring it", logger.WARNING)
            continue
        elif len(parse_result.episode_numbers) == 0:
            logger.log(u"Found " + newNZB + " inside " + result.name + " but it doesn't seem to be a valid episode NZB, ignoring it", logger.WARNING)
            continue

        wantEp = True
        for epNo in parse_result.episode_numbers:
            if not result.extraInfo[0].wantEpisode(season, epNo, result.quality):
                logger.log(u"Ignoring result " + newNZB + " because we don't want an episode that is " + Quality.qualityStrings[result.quality], logger.DEBUG)
                wantEp = False
                break
        if not wantEp:
            continue

        # get all the associated episode objects
        epObjList = []
        for curEp in parse_result.episode_numbers:
            epObjList.append(result.extraInfo[0].getEpisode(season, curEp))

        # make a result
        curResult = classes.NZBDataSearchResult(epObjList)
        curResult.name = newNZB
        curResult.provider = result.provider
        curResult.quality = result.quality
        curResult.extraInfo = [createNZBString(separateNZBs[newNZB], xmlns)]

        resultList.append(curResult)

    return resultList

########NEW FILE########
__FILENAME__ = postProcessor
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import glob
import os
import re
import subprocess

import sickbeard

from sickbeard import db
from sickbeard import classes
from sickbeard import common
from sickbeard import exceptions
from sickbeard import helpers
from sickbeard import history
from sickbeard import logger
from sickbeard import notifiers
from sickbeard import show_name_helpers
from sickbeard import scene_exceptions

from sickbeard import encodingKludge as ek
from sickbeard.exceptions import ex

from sickbeard.name_parser.parser import NameParser, InvalidNameException

from lib.tvdb_api import tvdb_api, tvdb_exceptions


class PostProcessor(object):
    """
    A class which will process a media file according to the post processing settings in the config.
    """

    EXISTS_LARGER = 1
    EXISTS_SAME = 2
    EXISTS_SMALLER = 3
    DOESNT_EXIST = 4

    NZB_NAME = 1
    FOLDER_NAME = 2
    FILE_NAME = 3

    def __init__(self, file_path, nzb_name=None, pp_options={}):
        """
        Creates a new post processor with the given file path and optionally an NZB name.

        file_path: The path to the file to be processed
        nzb_name: The name of the NZB which resulted in this file being downloaded (optional)
        """
        # absolute path to the folder that is being processed
        self.folder_path = ek.ek(os.path.dirname, ek.ek(os.path.abspath, file_path))

        # full path to file
        self.file_path = file_path

        # file name only
        self.file_name = ek.ek(os.path.basename, file_path)

        # the name of the folder only
        self.folder_name = ek.ek(os.path.basename, self.folder_path)

        # name of the NZB that resulted in this folder
        self.nzb_name = nzb_name

        self.force_replace = pp_options.get('force_replace', False)

        self.in_history = False

        self.release_group = None

        self.release_name = None

        self.is_proper = False

        self.log = ''

    def _log(self, message, level=logger.MESSAGE):
        """
        A wrapper for the internal logger which also keeps track of messages and saves them to a string for later.

        message: The string to log (unicode)
        level: The log level to use (optional)
        """
        logger.log(message, level)
        self.log += message + '\n'

    def _checkForExistingFile(self, existing_file):
        """
        Checks if a file exists already and if it does whether it's bigger or smaller than
        the file we are post processing

        existing_file: The file to compare to

        Returns:
            DOESNT_EXIST if the file doesn't exist
            EXISTS_LARGER if the file exists and is larger than the file we are post processing
            EXISTS_SMALLER if the file exists and is smaller than the file we are post processing
            EXISTS_SAME if the file exists and is the same size as the file we are post processing
        """

        if not existing_file:
            self._log(u"There is no existing file", logger.DEBUG)
            return PostProcessor.DOESNT_EXIST

        # if the new file exists, return the appropriate code depending on the size
        if ek.ek(os.path.isfile, existing_file):

            # see if it's bigger than our old file
            if ek.ek(os.path.getsize, existing_file) > ek.ek(os.path.getsize, self.file_path):
                self._log(u"File " + existing_file + " is larger than " + self.file_path, logger.DEBUG)
                return PostProcessor.EXISTS_LARGER

            elif ek.ek(os.path.getsize, existing_file) == ek.ek(os.path.getsize, self.file_path):
                self._log(u"File " + existing_file + " is the same size as " + self.file_path, logger.DEBUG)
                return PostProcessor.EXISTS_SAME

            else:
                self._log(u"File " + existing_file + " is smaller than " + self.file_path, logger.DEBUG)
                return PostProcessor.EXISTS_SMALLER

        else:
            self._log(u"File " + existing_file + " doesn't exist", logger.DEBUG)
            return PostProcessor.DOESNT_EXIST

    def list_associated_files(self, file_path, base_name_only=False):
        """
        For a given file path searches for files with the same name but different extension and returns their absolute paths

        file_path: The file to check for associated files
        base_name_only: False add extra '.' (conservative search) to file_path minus extension
        Returns: A list containing all files which are associated to the given file
        """

        if not file_path:
            return []

        file_path_list = []
        base_name = file_path.rpartition('.')[0]

        if not base_name_only:
            base_name = base_name + '.'

        # don't strip it all and use cwd by accident
        if not base_name:
            return []

        # don't confuse glob with chars we didn't mean to use
        base_name = re.sub(r'[\[\]\*\?]', r'[\g<0>]', base_name)

        for associated_file_path in ek.ek(glob.glob, base_name + '*'):
            # only add associated to list
            if associated_file_path == file_path:
                continue

            if ek.ek(os.path.isfile, associated_file_path):
                file_path_list.append(associated_file_path)

        return file_path_list

    def _delete(self, file_path, associated_files=False):
        """
        Deletes the file and optionally all associated files.

        file_path: The file to delete
        associated_files: True to delete all files which differ only by extension, False to leave them
        """

        if not file_path:
            return

        # figure out which files we want to delete
        file_list = [file_path]
        if associated_files:
            file_list = file_list + self.list_associated_files(file_path, base_name_only=True)

        if not file_list:
            self._log(u"There were no files associated with " + file_path + ", not deleting anything", logger.DEBUG)
            return

        # delete the file and any other files which we want to delete
        for cur_file in file_list:
            if ek.ek(os.path.isfile, cur_file):
                self._log(u"Deleting file " + cur_file, logger.DEBUG)
                ek.ek(os.remove, cur_file)
                # do the library update for synoindex
                notifiers.synoindex_notifier.deleteFile(cur_file)

    def _combined_file_operation(self, file_path, new_path, new_base_name, associated_files=False, action=None):
        """
        Performs a generic operation (move or copy) on a file. Can rename the file as well as change its location,
        and optionally move associated files too.

        file_path: The full path of the media file to act on
        new_path: Destination path where we want to move/copy the file to
        new_base_name: The base filename (no extension) to use during the copy. Use None to keep the same name.
        associated_files: Boolean, whether we should copy similarly-named files too
        action: function that takes an old path and new path and does an operation with them (move/copy)
        """

        if not action:
            self._log(u"Must provide an action for the combined file operation", logger.ERROR)
            return

        file_list = [file_path]
        if associated_files:
            file_list = file_list + self.list_associated_files(file_path)

        if not file_list:
            self._log(u"There were no files associated with " + file_path + ", not moving anything", logger.DEBUG)
            return

        # create base name with file_path (media_file without .extension)
        old_base_name = file_path.rpartition('.')[0]
        old_base_name_length = len(old_base_name)

        # deal with all files
        for cur_file_path in file_list:
            cur_file_name = ek.ek(os.path.basename, cur_file_path)
            # get the extension without .
            cur_extension = cur_file_path[old_base_name_length + 1:]

            # replace .nfo with .nfo-orig to avoid conflicts
            if cur_extension == 'nfo':
                cur_extension = 'nfo-orig'

            # If new base name then convert name
            if new_base_name:
                new_file_name = new_base_name + '.' + cur_extension
            # if we're not renaming we still want to change extensions sometimes
            else:
                new_file_name = helpers.replaceExtension(cur_file_name, cur_extension)

            new_file_path = ek.ek(os.path.join, new_path, new_file_name)

            action(cur_file_path, new_file_path)

    def _move(self, file_path, new_path, new_base_name, associated_files=False):
        """
        file_path: The full path of the media file to move
        new_path: Destination path where we want to move the file to
        new_base_name: The base filename (no extension) to use during the move. Use None to keep the same name.
        associated_files: Boolean, whether we should move similarly-named files too
        """

        def _int_move(cur_file_path, new_file_path):

            self._log(u"Moving file from " + cur_file_path + " to " + new_file_path, logger.DEBUG)
            try:
                helpers.moveFile(cur_file_path, new_file_path)
                helpers.chmodAsParent(new_file_path)
            except (IOError, OSError), e:
                self._log("Unable to move file " + cur_file_path + " to " + new_file_path + ": " + ex(e), logger.ERROR)
                raise e

        self._combined_file_operation(file_path, new_path, new_base_name, associated_files, action=_int_move)

    def _copy(self, file_path, new_path, new_base_name, associated_files=False):
        """
        file_path: The full path of the media file to copy
        new_path: Destination path where we want to copy the file to
        new_base_name: The base filename (no extension) to use during the copy. Use None to keep the same name.
        associated_files: Boolean, whether we should copy similarly-named files too
        """

        def _int_copy(cur_file_path, new_file_path):

            self._log(u"Copying file from " + cur_file_path + " to " + new_file_path, logger.DEBUG)
            try:
                helpers.copyFile(cur_file_path, new_file_path)
                helpers.chmodAsParent(new_file_path)
            except (IOError, OSError), e:
                logger.log(u"Unable to copy file " + cur_file_path + " to " + new_file_path + ": " + ex(e), logger.ERROR)
                raise e

        self._combined_file_operation(file_path, new_path, new_base_name, associated_files, action=_int_copy)

    def _history_lookup(self):
        """
        Look up the NZB name in the history and see if it contains a record for self.nzb_name

        Returns a (tvdb_id, season, [], quality) tuple. tvdb_id, season, quality may be None and episodes may be [].
        """

        to_return = (None, None, [], None)

        # if we don't have either of these then there's nothing to use to search the history for anyway
        if not self.nzb_name and not self.folder_name:
            self.in_history = False
            return to_return

        # make a list of possible names to use in the search
        names = []
        if self.nzb_name:
            names.append(self.nzb_name)
            if '.' in self.nzb_name:
                names.append(self.nzb_name.rpartition(".")[0])
        if self.folder_name:
            names.append(self.folder_name)

        myDB = db.DBConnection()

        # search the database for a possible match and return immediately if we find one
        for curName in names:
            # The underscore character ( _ ) represents a single character to match a pattern from a word or string
            search_name = re.sub("[\.\-\ ]", "_", curName)
            sql_results = myDB.select("SELECT * FROM history WHERE resource LIKE ?", [search_name])

            if len(sql_results) == 0:
                continue

            tvdb_id = int(sql_results[0]["showid"])
            season = int(sql_results[0]["season"])
            quality = int(sql_results[0]["quality"])

            if quality == common.Quality.UNKNOWN:
                quality = None

            self.in_history = True
            to_return = (tvdb_id, season, [], quality)
            self._log("Found result in history: " + str(to_return), logger.DEBUG)

            return to_return

        self.in_history = False
        return to_return

    def _analyze_name(self, name, file_name=True):
        """
        Takes a name and tries to figure out a show, season, and episode from it.

        name: A string which we want to analyze to determine show info from (unicode)

        Returns a (tvdb_id, season, [episodes], quality) tuple. tvdb_id, season, quality may be None and episodes may be [].
        if none were found.
        """

        logger.log(u"Analyzing name " + repr(name))

        to_return = (None, None, [], None)

        if not name:
            return to_return

        name = helpers.remove_non_release_groups(helpers.remove_extension(name))

        # parse the name to break it into show name, season, and episode
        np = NameParser(False)
        parse_result = np.parse(name)
        self._log(u"Parsed " + name + " into " + str(parse_result).decode('utf-8', 'xmlcharrefreplace'), logger.DEBUG)

        if parse_result.air_by_date:
            season = -1
            episodes = [parse_result.air_date]
        else:
            season = parse_result.season_number
            episodes = parse_result.episode_numbers

        to_return = (None, season, episodes, None)

        # do a scene reverse-lookup to get a list of all possible names
        name_list = show_name_helpers.sceneToNormalShowNames(parse_result.series_name)

        if not name_list:
            return (None, season, episodes, None)

        # try finding name in DB
        for cur_name in name_list:
            self._log(u"Looking up " + cur_name + u" in the DB", logger.DEBUG)
            db_result = helpers.searchDBForShow(cur_name)
            if db_result:
                self._log(u"Lookup successful, using tvdb id " + str(db_result[0]), logger.DEBUG)
                self._finalize(parse_result)
                return (int(db_result[0]), season, episodes, None)

        # try finding name in scene exceptions
        for cur_name in name_list:
            self._log(u"Checking scene exceptions for a match on " + cur_name, logger.DEBUG)
            scene_id = scene_exceptions.get_scene_exception_by_name(cur_name)
            if scene_id:
                self._log(u"Scene exception lookup got tvdb id " + str(scene_id) + u", using that", logger.DEBUG)
                self._finalize(parse_result)
                return (scene_id, season, episodes, None)

        # try finding name on TVDB
        for cur_name in name_list:
            try:
                t = tvdb_api.Tvdb(custom_ui=classes.ShowListUI, **sickbeard.TVDB_API_PARMS)

                self._log(u"Looking up name " + cur_name + u" on TVDB", logger.DEBUG)
                showObj = t[cur_name]
            except (tvdb_exceptions.tvdb_exception):
                # if none found, search on all languages
                try:
                    # There's gotta be a better way of doing this but we don't wanna
                    # change the language value elsewhere
                    ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

                    ltvdb_api_parms['search_all_languages'] = True
                    t = tvdb_api.Tvdb(custom_ui=classes.ShowListUI, **ltvdb_api_parms)

                    self._log(u"Looking up name " + cur_name + u" in all languages on TVDB", logger.DEBUG)
                    showObj = t[cur_name]
                except (tvdb_exceptions.tvdb_exception, IOError):
                    pass

                continue
            except (IOError):
                continue

            self._log(u"Lookup successful, using tvdb id " + str(showObj["id"]), logger.DEBUG)
            self._finalize(parse_result)
            return (int(showObj["id"]), season, episodes, None)

        self._finalize(parse_result)
        return to_return

    def _finalize(self, parse_result):
        self.release_group = parse_result.release_group

        # remember whether it's a proper
        self.is_proper = parse_result.is_proper

        # if the result is complete then remember that for later
        if parse_result.series_name and parse_result.season_number is not None and parse_result.episode_numbers and parse_result.release_group:
            if not self.release_name:
                self.release_name = helpers.remove_extension(ek.ek(os.path.basename, parse_result.original_name))

        else:
            logger.log(u"Parse result not sufficient (all following have to be set). will not save release name", logger.DEBUG)
            logger.log(u"Parse result(series_name): " + str(parse_result.series_name), logger.DEBUG)
            logger.log(u"Parse result(season_number): " + str(parse_result.season_number), logger.DEBUG)
            logger.log(u"Parse result(episode_numbers): " + str(parse_result.episode_numbers), logger.DEBUG)
            logger.log(u"Parse result(release_group): " + str(parse_result.release_group), logger.DEBUG)

    def _find_info(self):
        """
        For a given file try to find the showid, season, and episode.
        """

        tvdb_id = season = quality = None
        episodes = []

                        # try to look up the nzb in history
        attempt_list = [self._history_lookup,

                        # try to analyze the nzb name
                        lambda: self._analyze_name(self.nzb_name),

                        # try to analyze the file name
                        lambda: self._analyze_name(self.file_name),

                        # try to analyze the dir name
                        lambda: self._analyze_name(self.folder_name),

                        # try to analyze the file + dir names together
                        lambda: self._analyze_name(self.file_path),

                        # try to analyze the dir + file name together as one name
                        lambda: self._analyze_name(self.folder_name + u' ' + self.file_name)

                        ]

        # attempt every possible method to get our info
        for cur_attempt in attempt_list:

            try:
                (cur_tvdb_id, cur_season, cur_episodes, cur_quality) = cur_attempt()
            except InvalidNameException, e:
                logger.log(u"Unable to parse, skipping: " + ex(e), logger.DEBUG)
                continue

            # if we already did a successful history lookup then keep that tvdb_id value
            if cur_tvdb_id and not (self.in_history and tvdb_id):
                tvdb_id = cur_tvdb_id

            if cur_quality and not (self.in_history and quality):
                quality = cur_quality

            if cur_season is not None:
                season = cur_season

            if cur_episodes:
                episodes = cur_episodes

            # for air-by-date shows we need to look up the season/episode from database
            if season == -1 and tvdb_id and episodes:
                self._log(u"Looks like this is an air-by-date show, attempting to convert the date to season/episode", logger.DEBUG)
                airdate = episodes[0].toordinal()
                myDB = db.DBConnection()
                sql_result = myDB.select("SELECT season, episode FROM tv_episodes WHERE showid = ? and airdate = ?", [tvdb_id, airdate])

                if sql_result:
                    season = int(sql_result[0][0])
                    episodes = [int(sql_result[0][1])]
                else:
                    self._log(u"Unable to find episode with date " + str(episodes[0]) + u" for show " + str(tvdb_id) + u", skipping", logger.DEBUG)
                    # we don't want to leave dates in the episode list if we couldn't convert them to real episode numbers
                    episodes = []
                    continue

            # if there's no season then we can hopefully just use 1 automatically
            elif season == None and tvdb_id:
                myDB = db.DBConnection()
                numseasonsSQlResult = myDB.select("SELECT COUNT(DISTINCT season) as numseasons FROM tv_episodes WHERE showid = ? and season != 0", [tvdb_id])
                if int(numseasonsSQlResult[0][0]) == 1 and season == None:
                    self._log(u"Don't have a season number, but this show appears to only have 1 season, setting seasonnumber to 1...", logger.DEBUG)
                    season = 1

            if tvdb_id and season and episodes:
                return (tvdb_id, season, episodes, quality)

        return (tvdb_id, season, episodes, quality)

    def _get_ep_obj(self, tvdb_id, season, episodes):
        """
        Retrieve the TVEpisode object requested.

        tvdb_id: The TVDBID of the show (int)
        season: The season of the episode (int)
        episodes: A list of episodes to find (list of ints)

        If the episode(s) can be found then a TVEpisode object with the correct related eps will
        be instantiated and returned. If the episode can't be found then None will be returned.
        """

        show_obj = None

        self._log(u"Loading show object for tvdb_id " + str(tvdb_id), logger.DEBUG)
        # find the show in the showlist
        try:
            show_obj = helpers.findCertainShow(sickbeard.showList, tvdb_id)
        except exceptions.MultipleShowObjectsException:
            raise  # TODO: later I'll just log this, for now I want to know about it ASAP

        # if we can't find the show then there's nothing we can really do
        if not show_obj:
            error_msg = u"This show isn't in your list, you need to add it to SB before post-processing an episode"
            self._log(error_msg, logger.ERROR)
            raise exceptions.PostProcessingFailed(error_msg)

        root_ep = None
        for cur_episode in episodes:
            episode = int(cur_episode)

            self._log(u"Retrieving episode object for " + str(season) + "x" + str(episode), logger.DEBUG)

            # now that we've figured out which episode this file is just load it manually
            try:
                curEp = show_obj.getEpisode(season, episode)
            except exceptions.EpisodeNotFoundException, e:
                error_msg = u"Unable to create episode: " + ex(e)
                self._log(error_msg, logger.DEBUG)
                raise exceptions.PostProcessingFailed(error_msg)

            # associate all the episodes together under a single root episode
            if root_ep == None:
                root_ep = curEp
                root_ep.relatedEps = []
            elif curEp not in root_ep.relatedEps:
                root_ep.relatedEps.append(curEp)

        return root_ep

    def _get_quality(self, ep_obj):
        """
        Determines the quality of the file that is being post processed by parsing through the data available.

        ep_obj: The TVEpisode object related to the file we are post processing

        Returns: A quality value found in common.Quality
        """

        ep_quality = common.Quality.UNKNOWN

        # nzb name is the most reliable if it exists, followed by folder name and lastly file name
        name_list = [self.nzb_name, self.folder_name, self.file_name]

        # search all possible names for our new quality, in case the file or dir doesn't have it
        for cur_name in name_list:

            # some stuff might be None at this point still
            if not cur_name:
                continue

            ep_quality = common.Quality.nameQuality(cur_name)
            self._log(u"Looking up quality for name " + cur_name + u", got " + common.Quality.qualityStrings[ep_quality], logger.DEBUG)

            # if we find a good one then use it
            if ep_quality != common.Quality.UNKNOWN:
                logger.log(cur_name + u" looks like it has quality " + common.Quality.qualityStrings[ep_quality] + ", using that", logger.DEBUG)
                return ep_quality

        # Try getting quality from the episode (snatched) status
        if ep_obj.status in common.Quality.SNATCHED + common.Quality.SNATCHED_PROPER:
            oldStatus, ep_quality = common.Quality.splitCompositeStatus(ep_obj.status)  # @UnusedVariable
            if ep_quality != common.Quality.UNKNOWN:
                self._log(u"The old status had a quality in it, using that: " + common.Quality.qualityStrings[ep_quality], logger.DEBUG)
                return ep_quality

        # Try guessing quality from the file name
        ep_quality = common.Quality.assumeQuality(self.file_name)
        self._log(u"Guessing quality for name " + self.file_name + u", got " + common.Quality.qualityStrings[ep_quality], logger.DEBUG)
        if ep_quality != common.Quality.UNKNOWN:
            logger.log(self.file_name + u" looks like it has quality " + common.Quality.qualityStrings[ep_quality] + ", using that", logger.DEBUG)
            return ep_quality

        return ep_quality

    def _run_extra_scripts(self, ep_obj):
        """
        Executes any extra scripts defined in the config.

        ep_obj: The object to use when calling the extra script
        """
        for curScriptName in sickbeard.EXTRA_SCRIPTS:

            # generate a safe command line string to execute the script and provide all the parameters
            try:
                script_cmd = [piece for piece in re.split("( |\\\".*?\\\"|'.*?')", curScriptName) if piece.strip()]

                script_cmd = script_cmd + [ep_obj.location.encode(sickbeard.SYS_ENCODING),
                                           self.file_path.encode(sickbeard.SYS_ENCODING),
                                           str(ep_obj.show.tvdbid),
                                           str(ep_obj.season),
                                           str(ep_obj.episode),
                                           str(ep_obj.airdate)
                                           ]

                # use subprocess to run the command and capture output
                self._log(u"Executing command " + str(script_cmd))

                p = subprocess.Popen(script_cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=sickbeard.PROG_DIR)
                out, err = p.communicate()  # @UnusedVariable
                self._log(u"Script result: " + str(out), logger.DEBUG)

            except Exception, e:
                self._log(u"Unable to run extra_script: " + ex(e))

    def _safe_replace(self, ep_obj, new_ep_quality):
        """
        Determines if the new episode can safely replace old episode.
        Episodes which are expected (snatched) or larger than the existing episode are priority, others are not.

        ep_obj: The TVEpisode object in question
        new_ep_quality: The quality of the episode that is being processed

        Returns: True if the episode can safely replace old episode, False otherwise.
        """

        # if SB snatched this then assume it's safe
        if ep_obj.status in common.Quality.SNATCHED + common.Quality.SNATCHED_PROPER:
            self._log(u"Sick Beard snatched this episode, marking it safe to replace", logger.DEBUG)
            return True

        old_ep_status, old_ep_quality = common.Quality.splitCompositeStatus(ep_obj.status)

        # if old episode is not downloaded/archived then it's safe
        if old_ep_status != common.DOWNLOADED and old_ep_status != common.ARCHIVED:
            self._log(u"Existing episode status is not downloaded/archived, marking it safe to replace", logger.DEBUG)
            return True

        if old_ep_status == common.ARCHIVED:
            self._log(u"Existing episode status is archived, marking it unsafe to replace", logger.DEBUG)
            return False

        # Status downloaded. Quality/ size checks

        # if manual post process option is set to force_replace then it's safe
        if self.force_replace:
            self._log(u"Processed episode is set to force replace existing episode, marking it safe to replace", logger.DEBUG)
            return True

        # if the file processed is higher quality than the existing episode then it's safe
        if new_ep_quality > old_ep_quality:
            if new_ep_quality != common.Quality.UNKNOWN:
                self._log(u"Existing episode status is not snatched but processed episode appears to be better quality than existing episode, marking it safe to replace", logger.DEBUG)
                return True

            else:
                self._log(u"Episode already exists in database and processed episode has unknown quality, marking it unsafe to replace", logger.DEBUG)
                return False

        # if there's an existing downloaded file with same quality, check filesize to decide
        if new_ep_quality == old_ep_quality:
            self._log(u"Episode already exists in database and has same quality as processed episode", logger.DEBUG)

            # check for an existing file
            self._log(u"Checking size of existing file: " + ep_obj.location, logger.DEBUG)
            existing_file_status = self._checkForExistingFile(ep_obj.location)

            if existing_file_status in (PostProcessor.EXISTS_LARGER, PostProcessor.EXISTS_SAME):
                self._log(u"File exists and new file is same/smaller, marking it unsafe to replace", logger.DEBUG)
                return False

            elif existing_file_status == PostProcessor.EXISTS_SMALLER:
                self._log(u"File exists and new file is larger, marking it safe to replace", logger.DEBUG)
                return True

            elif existing_file_status == PostProcessor.DOESNT_EXIST:
                if not ek.ek(os.path.isdir, ep_obj.show._location) and not sickbeard.CREATE_MISSING_SHOW_DIRS:
                    self._log(u"File and Show location doesn't exist, marking it unsafe to replace", logger.DEBUG)
                    return False

                else:
                    self._log(u"File doesn't exist, marking it safe to replace", logger.DEBUG)
                    return True

            else:
                self._log(u"Unknown file status for: " + ep_obj.location + "This should never happen, please log this as a bug.", logger.ERROR)
                return False

        # if there's an existing file with better quality
        if new_ep_quality < old_ep_quality and old_ep_quality != common.Quality.UNKNOWN:
            self._log(u"Episode already exists in database and processed episode has lower quality, marking it unsafe to replace", logger.DEBUG)
            return False

        self._log(u"None of the conditions were met, marking it unsafe to replace", logger.DEBUG)

        return False

    def process(self):
        """
        Post-process a given file
        """

        self._log(u"Processing " + self.file_path + " (" + str(self.nzb_name) + ")")

        if ek.ek(os.path.isdir, self.file_path):
            self._log(u"File " + self.file_path + " seems to be a directory")
            return False

        # reset per-file stuff
        self.in_history = False

        # try to find the file info
        (tvdb_id, season, episodes, quality) = self._find_info()

        # if we don't have it then give up
        if not tvdb_id or season == None or not episodes:
            self._log(u"Not enough information to determine what episode this is", logger.DEBUG)
            self._log(u"Quitting post-processing", logger.DEBUG)
            return False

        # retrieve/create the corresponding TVEpisode objects
        ep_obj = self._get_ep_obj(tvdb_id, season, episodes)

        # get the quality of the episode we're processing
        if quality:
            self._log(u"Snatch history had a quality in it, using that: " + common.Quality.qualityStrings[quality], logger.DEBUG)
            new_ep_quality = quality
        else:
            new_ep_quality = self._get_quality(ep_obj)

        logger.log(u"Quality of the processing episode: " + str(new_ep_quality), logger.DEBUG)

        # see if it's safe to replace existing episode (is download snatched, PROPER, better quality)
        safe_replace = self._safe_replace(ep_obj, new_ep_quality)

        # if it's not safe to replace, stop here
        if not safe_replace:
            self._log(u"Quitting post-processing", logger.DEBUG)
            return False

        # if the file is safe to replace then we're going to replace it even if it exists
        else:
            self._log(u"This download is marked as safe to replace existing file", logger.DEBUG)

        # delete the existing file (and company)
        for cur_ep in [ep_obj] + ep_obj.relatedEps:
            try:
                self._delete(cur_ep.location, associated_files=True)
                # clean up any left over folders
                if cur_ep.location:
                    helpers.delete_empty_folders(ek.ek(os.path.dirname, cur_ep.location), keep_dir=ep_obj.show._location)
            except (OSError, IOError):
                raise exceptions.PostProcessingFailed(u"Unable to delete the existing files")

        # if the show directory doesn't exist then make it if allowed
        if not ek.ek(os.path.isdir, ep_obj.show._location) and sickbeard.CREATE_MISSING_SHOW_DIRS:
            self._log(u"Show directory doesn't exist, creating it", logger.DEBUG)
            try:
                ek.ek(os.mkdir, ep_obj.show._location)
                # do the library update for synoindex
                notifiers.synoindex_notifier.addFolder(ep_obj.show._location)

            except (OSError, IOError):
                raise exceptions.PostProcessingFailed(u"Unable to create the show directory: " + ep_obj.show._location)

            # get metadata for the show (but not episode because it hasn't been fully processed)
            ep_obj.show.writeMetadata(True)

        # update the ep info before we rename so the quality & release name go into the name properly
        for cur_ep in [ep_obj] + ep_obj.relatedEps:

            if self.release_name:
                self._log("Found release name " + self.release_name, logger.DEBUG)
                cur_ep.release_name = self.release_name
            else:
                cur_ep.release_name = ""

            cur_ep.status = common.Quality.compositeStatus(common.DOWNLOADED, new_ep_quality)

        # find the destination folder
        try:
            proper_path = ep_obj.proper_path()
            proper_absolute_path = ek.ek(os.path.join, ep_obj.show.location, proper_path)
            dest_path = ek.ek(os.path.dirname, proper_absolute_path)

        except exceptions.ShowDirNotFoundException:
            raise exceptions.PostProcessingFailed(u"Unable to post-process an episode if the show dir doesn't exist, quitting")

        self._log(u"Destination folder for this episode: " + dest_path, logger.DEBUG)

        # create any folders we need
        if not helpers.make_dirs(dest_path):
            raise exceptions.PostProcessingFailed(u"Unable to create destination folder: " + dest_path)

        # figure out the base name of the resulting episode file
        if sickbeard.RENAME_EPISODES:
            orig_extension = self.file_name.rpartition('.')[-1]
            new_base_name = ek.ek(os.path.basename, proper_path)
            new_file_name = new_base_name + '.' + orig_extension

        else:
            # if we're not renaming then there's no new base name, we'll just use the existing name
            new_base_name = None
            new_file_name = self.file_name

        try:
            # move the episode and associated files to the show dir
            if sickbeard.KEEP_PROCESSED_DIR:
                self._copy(self.file_path, dest_path, new_base_name, sickbeard.MOVE_ASSOCIATED_FILES)
            else:
                self._move(self.file_path, dest_path, new_base_name, sickbeard.MOVE_ASSOCIATED_FILES)
        except (OSError, IOError):
            raise exceptions.PostProcessingFailed(u"Unable to move the files to destination folder: " + dest_path)

        # put the new location in the database
        for cur_ep in [ep_obj] + ep_obj.relatedEps:
            with cur_ep.lock:
                cur_ep.location = ek.ek(os.path.join, dest_path, new_file_name)
                cur_ep.saveToDB()

        # log it to history
        history.logDownload(ep_obj, self.file_path, new_ep_quality, self.release_group)

        # send notifiers download notification
        notifiers.notify_download(ep_obj.prettyName())

        # generate nfo/tbn
        ep_obj.createMetaFiles()
        ep_obj.saveToDB()

        # send notifiers library update
        notifiers.update_library(ep_obj)

        self._run_extra_scripts(ep_obj)

        return True

########NEW FILE########
__FILENAME__ = processTV
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import os
import shutil
import time

import sickbeard
from sickbeard import common
from sickbeard import postProcessor

from sickbeard import db, helpers, exceptions

from sickbeard import encodingKludge as ek
from sickbeard.exceptions import ex

from sickbeard import logger


def delete_folder(folder, check_empty=True):

    # check if it's a folder
    if not ek.ek(os.path.isdir, folder):
        return False

    # check if it isn't TV_DOWNLOAD_DIR
    if sickbeard.TV_DOWNLOAD_DIR:
        if helpers.real_path(folder) == helpers.real_path(sickbeard.TV_DOWNLOAD_DIR):
            return False

    # check if it's empty folder when wanted checked
    if check_empty:
        check_files = ek.ek(os.listdir, folder)
        if check_files:
            return False

    # try deleting folder
    try:
        logger.log(u"Deleting folder: " + folder)
        shutil.rmtree(folder)
    except (OSError, IOError), e:
        logger.log(u"Warning: unable to delete folder: " + folder + ": " + ex(e), logger.WARNING)
        return False

    return True


def logHelper(logMessage, logLevel=logger.MESSAGE):
    logger.log(logMessage, logLevel)
    return logMessage + u"\n"


def processDir(dirName, nzbName=None, method=None, recurse=False, pp_options={}):
    """
    Scans through the files in dirName and processes whatever media files it finds

    dirName: The folder name to look in
    nzbName: The NZB name which resulted in this folder being downloaded
    method:  The method of postprocessing: Automatic, Script, Manual
    recurse: Boolean for whether we should descend into subfolders or not
    """

    returnStr = u""

    returnStr += logHelper(u"Processing folder: " + dirName, logger.DEBUG)

    # if they passed us a real dir then assume it's the one we want
    if ek.ek(os.path.isdir, dirName):
        dirName = ek.ek(os.path.realpath, dirName)

    # if they've got a download dir configured then use it
    elif sickbeard.TV_DOWNLOAD_DIR and ek.ek(os.path.isdir, sickbeard.TV_DOWNLOAD_DIR) \
            and ek.ek(os.path.normpath, dirName) != ek.ek(os.path.normpath, sickbeard.TV_DOWNLOAD_DIR):
        dirName = ek.ek(os.path.join, sickbeard.TV_DOWNLOAD_DIR, ek.ek(os.path.abspath, dirName).split(os.path.sep)[-1])
        returnStr += logHelper(u"Trying to use folder: " + dirName, logger.DEBUG)

    # if we didn't find a real dir then quit
    if not ek.ek(os.path.isdir, dirName):
        returnStr += logHelper(u"Unable to figure out what folder to process. If your downloader and Sick Beard aren't on the same PC make sure you fill out your TV download dir in the config.", logger.DEBUG)
        return returnStr

    # TODO: check if it's failed and deal with it if it is
    if ek.ek(os.path.basename, dirName).startswith('_FAILED_'):
        returnStr += logHelper(u"The directory name indicates it failed to extract, cancelling", logger.DEBUG)
        return returnStr
    elif ek.ek(os.path.basename, dirName).startswith('_UNDERSIZED_'):
        returnStr += logHelper(u"The directory name indicates that it was previously rejected for being undersized, cancelling", logger.DEBUG)
        return returnStr
    elif ek.ek(os.path.basename, dirName).upper().startswith('_UNPACK'):
        returnStr += logHelper(u"The directory name indicates that this release is in the process of being unpacked, skipping", logger.DEBUG)
        return returnStr

    # make sure the dir isn't inside a show dir
    myDB = db.DBConnection()
    sqlResults = myDB.select("SELECT * FROM tv_shows")
    for sqlShow in sqlResults:
        if dirName.lower().startswith(ek.ek(os.path.realpath, sqlShow["location"]).lower() + os.sep) or dirName.lower() == ek.ek(os.path.realpath, sqlShow["location"]).lower():
            returnStr += logHelper(u"You're trying to post process an existing show directory: " + dirName, logger.ERROR)
            returnStr += u"\n"
            return returnStr

    fileList = ek.ek(os.listdir, dirName)

    # split the list into video files and folders
    folders = filter(lambda x: ek.ek(os.path.isdir, ek.ek(os.path.join, dirName, x)), fileList)

    # videoFiles, sorted by size, process biggest file first. Leaves smaller same named file behind
    videoFiles = sorted(filter(helpers.isMediaFile, fileList), key=lambda x: ek.ek(os.path.getsize, ek.ek(os.path.join, dirName, x)), reverse=True)
    remaining_video_files = list(videoFiles)

    num_videoFiles = len(videoFiles)

    # if there are no videofiles in parent and only one subfolder, pass the nzbName to child
    if num_videoFiles == 0 and len(folders) == 1:
        parent_nzbName = nzbName
    else:
        parent_nzbName = None

    # recursively process all the folders
    for cur_folder in folders:

        returnStr += u"\n"
        # use full path
        cur_folder = ek.ek(os.path.join, dirName, cur_folder)

        if helpers.is_hidden_folder(cur_folder):
            returnStr += logHelper(u"Ignoring hidden folder: " + cur_folder, logger.DEBUG)
        else:
            returnStr += logHelper(u"Recursively processing a folder: " + cur_folder, logger.DEBUG)
            returnStr += processDir(cur_folder, nzbName=parent_nzbName, recurse=True, method=method, pp_options=pp_options)

    remainingFolders = filter(lambda x: ek.ek(os.path.isdir, ek.ek(os.path.join, dirName, x)), fileList)

    if num_videoFiles == 0:
        returnStr += u"\n"
        returnStr += logHelper(u"There are no videofiles in folder: " + dirName, logger.DEBUG)

        # if there a no videofiles, try deleting empty folder
        if method != 'Manual':
            if delete_folder(dirName, check_empty=True):
                returnStr += logHelper(u"Deleted empty folder: " + dirName, logger.DEBUG)

    # if there's more than one videofile in the folder, files can be lost (overwritten) when nzbName contains only one episode.
    if num_videoFiles >= 2:
        nzbName = None

    # process any files in the dir
    for cur_video_file in videoFiles:

        cur_video_file_path = ek.ek(os.path.join, dirName, cur_video_file)

        if method == 'Automatic':
            # check if we processed this video file before
            cur_video_file_path_size = ek.ek(os.path.getsize, cur_video_file_path)

            myDB = db.DBConnection()
            search_sql = "SELECT tv_episodes.tvdbid, history.resource FROM tv_episodes INNER JOIN history ON history.showid=tv_episodes.showid"
            search_sql += " WHERE history.season=tv_episodes.season and history.episode=tv_episodes.episode"
            search_sql += " and tv_episodes.status IN (" + ",".join([str(x) for x in common.Quality.DOWNLOADED]) + ")"
            search_sql += " and history.resource LIKE ? and tv_episodes.file_size = ?"
            sql_results = myDB.select(search_sql, [cur_video_file_path, cur_video_file_path_size])

            if len(sql_results):
                returnStr += logHelper(u"Ignoring file: " + cur_video_file_path + " looks like it's been processed already", logger.DEBUG)
                continue

        try:
            returnStr += u"\n"
            processor = postProcessor.PostProcessor(cur_video_file_path, nzb_name=nzbName, pp_options=pp_options)
            process_result = processor.process()
            process_fail_message = ""

        except exceptions.PostProcessingFailed, e:
            process_result = False
            process_fail_message = ex(e)

        except Exception, e:
            process_result = False
            process_fail_message = "Post Processor returned unhandled exception: " + ex(e)

        returnStr += processor.log

        # as long as the postprocessing was successful delete the old folder unless the config wants us not to
        if process_result:

            remaining_video_files.remove(cur_video_file)

            if not sickbeard.KEEP_PROCESSED_DIR and len(remaining_video_files) == 0 and len(remainingFolders) == 0:
                if delete_folder(dirName, check_empty=False):
                    returnStr += logHelper(u"Deleted folder: " + dirName, logger.DEBUG)

            returnStr += logHelper(u"Processing succeeded for " + cur_video_file_path)

        else:
            returnStr += logHelper(u"Processing failed for " + cur_video_file_path + ": " + process_fail_message, logger.WARNING)

    return returnStr

########NEW FILE########
__FILENAME__ = properFinder
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import operator

import sickbeard

from sickbeard import db
from sickbeard import exceptions
from sickbeard.exceptions import ex
from sickbeard import helpers, logger, show_name_helpers
from sickbeard import providers
from sickbeard import search
from sickbeard import history

from sickbeard.common import DOWNLOADED, SNATCHED, SNATCHED_PROPER, Quality

from lib.tvdb_api import tvdb_api, tvdb_exceptions

from name_parser.parser import NameParser, InvalidNameException


class ProperFinder():

    def __init__(self):
        self.updateInterval = datetime.timedelta(hours=1)

    def run(self):

        if not sickbeard.DOWNLOAD_PROPERS:
            return

        # look for propers every night at 1 AM
        updateTime = datetime.time(hour=1)
        hourDiff = datetime.datetime.today().time().hour - updateTime.hour

        # if it's less than an interval after the update time then do an update
        if hourDiff >= 0 and hourDiff < self.updateInterval.seconds / 3600:
            logger.log(u"Beginning the search for new propers")
        else:
            return

        propers = self._getProperList()

        self._downloadPropers(propers)

    def _getProperList(self):

        propers = {}

        # for each provider get a list of the propers
        for curProvider in providers.sortedProviderList():

            if not curProvider.isActive():
                continue

            search_date = datetime.datetime.today() - datetime.timedelta(days=2)

            logger.log(u"Searching for any new PROPER releases from " + curProvider.name)
            try:
                curPropers = curProvider.findPropers(search_date)
            except exceptions.AuthException, e:
                logger.log(u"Authentication error: " + ex(e), logger.ERROR)
                continue

            # if they haven't been added by a different provider than add the proper to the list
            for x in curPropers:
                name = self._genericName(x.name)

                if not name in propers:
                    logger.log(u"Found new proper: " + x.name, logger.DEBUG)
                    x.provider = curProvider
                    propers[name] = x

        # take the list of unique propers and get it sorted by
        sortedPropers = sorted(propers.values(), key=operator.attrgetter('date'), reverse=True)
        finalPropers = []

        for curProper in sortedPropers:

            # parse the file name
            try:
                myParser = NameParser(False)
                parse_result = myParser.parse(curProper.name)
            except InvalidNameException:
                logger.log(u"Unable to parse the filename " + curProper.name + " into a valid episode", logger.DEBUG)
                continue

            if not parse_result.episode_numbers:
                logger.log(u"Ignoring " + curProper.name + " because it's for a full season rather than specific episode", logger.DEBUG)
                continue

            # populate our Proper instance
            if parse_result.air_by_date:
                curProper.season = -1
                curProper.episode = parse_result.air_date
            else:
                curProper.season = parse_result.season_number if parse_result.season_number != None else 1
                curProper.episode = parse_result.episode_numbers[0]
            curProper.quality = Quality.nameQuality(curProper.name)

            # for each show in our list
            for curShow in sickbeard.showList:

                if not parse_result.series_name:
                    continue

                genericName = self._genericName(parse_result.series_name)

                # get the scene name masks
                sceneNames = set(show_name_helpers.makeSceneShowSearchStrings(curShow))

                # for each scene name mask
                for curSceneName in sceneNames:

                    # if it matches
                    if genericName == self._genericName(curSceneName):
                        logger.log(u"Successful match! Result " + parse_result.series_name + " matched to show " + curShow.name, logger.DEBUG)

                        # set the tvdbid in the db to the show's tvdbid
                        curProper.tvdbid = curShow.tvdbid

                        # since we found it, break out
                        break

                # if we found something in the inner for loop break out of this one
                if curProper.tvdbid != -1:
                    break

            if curProper.tvdbid == -1:
                continue

            if not show_name_helpers.filterBadReleases(curProper.name):
                logger.log(u"Proper " + curProper.name + " isn't a valid scene release that we want, ignoring it", logger.DEBUG)
                continue

            show = helpers.findCertainShow(sickbeard.showList, curProper.tvdbid)
            if not show:
                logger.log(u"Unable to find the show with tvdbid " + str(curProper.tvdbid), logger.ERROR)
                continue

            if show.rls_ignore_words and search.filter_release_name(curProper.name, show.rls_ignore_words):
                logger.log(u"Ignoring " + curProper.name + " based on ignored words filter: " + show.rls_ignore_words, logger.MESSAGE)
                continue

            if show.rls_require_words and not search.filter_release_name(curProper.name, show.rls_require_words):
                logger.log(u"Ignoring " + curProper.name + " based on required words filter: " + show.rls_require_words, logger.MESSAGE)
                continue

            # if we have an air-by-date show then get the real season/episode numbers
            if curProper.season == -1 and curProper.tvdbid:

                tvdb_lang = show.lang
                # There's gotta be a better way of doing this but we don't wanna
                # change the language value elsewhere
                ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

                if tvdb_lang and not tvdb_lang == 'en':
                    ltvdb_api_parms['language'] = tvdb_lang

                try:
                    t = tvdb_api.Tvdb(**ltvdb_api_parms)
                    epObj = t[curProper.tvdbid].airedOn(curProper.episode)[0]
                    curProper.season = int(epObj["seasonnumber"])
                    curProper.episodes = [int(epObj["episodenumber"])]
                except tvdb_exceptions.tvdb_episodenotfound:
                    logger.log(u"Unable to find episode with date " + str(curProper.episode) + " for show " + parse_result.series_name + ", skipping", logger.WARNING)
                    continue

            # check if we actually want this proper (if it's the right quality)
            sqlResults = db.DBConnection().select("SELECT status FROM tv_episodes WHERE showid = ? AND season = ? AND episode = ?", [curProper.tvdbid, curProper.season, curProper.episode])
            if not sqlResults:
                continue
            oldStatus, oldQuality = Quality.splitCompositeStatus(int(sqlResults[0]["status"]))

            # only keep the proper if we have already retrieved the same quality ep (don't get better/worse ones)
            if oldStatus not in (DOWNLOADED, SNATCHED) or oldQuality != curProper.quality:
                continue

            # if the show is in our list and there hasn't been a proper already added for that particular episode then add it to our list of propers
            if curProper.tvdbid != -1 and (curProper.tvdbid, curProper.season, curProper.episode) not in map(operator.attrgetter('tvdbid', 'season', 'episode'), finalPropers):
                logger.log(u"Found a proper that we need: " + str(curProper.name))
                finalPropers.append(curProper)

        return finalPropers

    def _downloadPropers(self, properList):

        for curProper in properList:

            historyLimit = datetime.datetime.today() - datetime.timedelta(days=30)

            # make sure the episode has been downloaded before
            myDB = db.DBConnection()
            historyResults = myDB.select(
                "SELECT resource FROM history "
                "WHERE showid = ? AND season = ? AND episode = ? AND quality = ? AND date >= ? "
                "AND action IN (" + ",".join([str(x) for x in Quality.SNATCHED]) + ")",
                        [curProper.tvdbid, curProper.season, curProper.episode, curProper.quality, historyLimit.strftime(history.dateFormat)])

            # if we didn't download this episode in the first place we don't know what quality to use for the proper so we can't do it
            if len(historyResults) == 0:
                logger.log(u"Unable to find an original history entry for proper " + curProper.name + " so I'm not downloading it.")
                continue

            else:

                # make sure that none of the existing history downloads are the same proper we're trying to download
                isSame = False
                for curResult in historyResults:
                    # if the result exists in history already we need to skip it
                    if self._genericName(curResult["resource"]) == self._genericName(curProper.name):
                        isSame = True
                        break
                if isSame:
                    logger.log(u"This proper is already in history, skipping it", logger.DEBUG)
                    continue

                # get the episode object
                showObj = helpers.findCertainShow(sickbeard.showList, curProper.tvdbid)
                if showObj == None:
                    logger.log(u"Unable to find the show with tvdbid " + str(curProper.tvdbid) + " so unable to download the proper", logger.ERROR)
                    continue
                epObj = showObj.getEpisode(curProper.season, curProper.episode)

                # make the result object
                result = curProper.provider.getResult([epObj])
                result.url = curProper.url
                result.name = curProper.name
                result.quality = curProper.quality

                # snatch it
                search.snatchEpisode(result, SNATCHED_PROPER)

    def _genericName(self, name):
        return name.replace(".", " ").replace("-", " ").replace("_", " ").lower()

########NEW FILE########
__FILENAME__ = btn
# coding=utf-8
# Author: Danil Heimans
# URL: http://code.google.com/p/sickbeard
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import sickbeard
import generic

from sickbeard import classes
from sickbeard import scene_exceptions
from sickbeard import logger
from sickbeard import tvcache
from sickbeard.helpers import sanitizeSceneName
from sickbeard.common import Quality
from sickbeard.exceptions import ex, AuthException

from lib import jsonrpclib
from datetime import datetime
import time
import socket
import math


class BTNProvider(generic.TorrentProvider):

    def __init__(self):

        generic.TorrentProvider.__init__(self, "BTN")

        self.supportsBacklog = True
        self.cache = BTNCache(self)

        self.url = "http://broadcasthe.net"

    def isEnabled(self):
        return sickbeard.BTN

    def imageName(self):
        return 'btn.png'

    def _checkAuth(self):
        if not sickbeard.BTN_API_KEY:
            raise AuthException("Your authentication credentials for " + self.name + " are missing, check your config.")

        return True

    def _checkAuthFromData(self, parsedJSON):

        if parsedJSON is None:
            return self._checkAuth()

        if 'api-error' in parsedJSON:
                    logger.log(u"Incorrect authentication credentials for " + self.name + " : " + parsedJSON['api-error'], logger.DEBUG)
                    raise AuthException("Your authentication credentials for " + self.name + " are incorrect, check your config.")

        return True

    def _doSearch(self, search_params, show=None, age=0):

        self._checkAuth()

        params = {}
        apikey = sickbeard.BTN_API_KEY

        # age in seconds
        if age:
            params['age'] = "<=" + str(int(age))

        if search_params:
            params.update(search_params)

        parsedJSON = self._api_call(apikey, params)

        if not parsedJSON:
            logger.log(u"No data returned from " + self.name, logger.ERROR)
            return []

        if self._checkAuthFromData(parsedJSON):

            if 'torrents' in parsedJSON:
                found_torrents = parsedJSON['torrents']
            else:
                found_torrents = {}

            # We got something, we know the API sends max 1000 results at a time.
            # See if there are more than 1000 results for our query, if not we
            # keep requesting until we've got everything.
            # max 150 requests per hour so limit at that. Scan every 15 minutes. 60 / 15 = 4.
            max_pages = 35
            results_per_page = 1000

            if 'results' in parsedJSON and int(parsedJSON['results']) >= results_per_page:
                pages_needed = int(math.ceil(int(parsedJSON['results']) / results_per_page))
                if pages_needed > max_pages:
                    pages_needed = max_pages

                # +1 because range(1,4) = 1, 2, 3
                for page in range(1, pages_needed + 1):
                    parsedJSON = self._api_call(apikey, params, results_per_page, page * results_per_page)
                    # Note that this these are individual requests and might time out individually. This would result in 'gaps'
                    # in the results. There is no way to fix this though.
                    if 'torrents' in parsedJSON:
                        found_torrents.update(parsedJSON['torrents'])

            results = []

            for torrentid, torrent_info in found_torrents.iteritems():
                (title, url) = self._get_title_and_url(torrent_info)

                if title and url:
                    results.append(torrent_info)

            return results

        return []

    def _api_call(self, apikey, params={}, results_per_page=1000, offset=0):

        server = jsonrpclib.Server('http://api.btnapps.net')
        parsedJSON = {}

        try:
            parsedJSON = server.getTorrents(apikey, params, int(results_per_page), int(offset))

        except jsonrpclib.jsonrpc.ProtocolError, error:
            logger.log(u"JSON-RPC protocol error while accessing " + self.name + ": " + ex(error), logger.ERROR)
            parsedJSON = {'api-error': ex(error)}
            return parsedJSON

        except socket.timeout:
            logger.log(u"Timeout while accessing " + self.name, logger.WARNING)

        except socket.error, error:
            # Note that sometimes timeouts are thrown as socket errors
            logger.log(u"Socket error while accessing " + self.name + ": " + error[1], logger.ERROR)

        except Exception, error:
            errorstring = str(error)
            if(errorstring.startswith('<') and errorstring.endswith('>')):
                errorstring = errorstring[1:-1]
            logger.log(u"Unknown error while accessing " + self.name + ": " + errorstring, logger.ERROR)

        return parsedJSON

    def _get_title_and_url(self, parsedJSON):

        # The BTN API gives a lot of information in response,
        # however Sick Beard is built mostly around Scene or
        # release names, which is why we are using them here.

        if 'ReleaseName' in parsedJSON and parsedJSON['ReleaseName']:
            title = parsedJSON['ReleaseName']

        else:
            # If we don't have a release name we need to get creative
            title = u''
            if 'Series' in parsedJSON:
                title += parsedJSON['Series']
            if 'GroupName' in parsedJSON:
                title += '.' + parsedJSON['GroupName'] if title else parsedJSON['GroupName']
            if 'Resolution' in parsedJSON:
                title += '.' + parsedJSON['Resolution'] if title else parsedJSON['Resolution']
            if 'Source' in parsedJSON:
                title += '.' + parsedJSON['Source'] if title else parsedJSON['Source']
            if 'Codec' in parsedJSON:
                title += '.' + parsedJSON['Codec'] if title else parsedJSON['Codec']
            if title:
                title = title.replace(' ', '.')

        url = None
        if 'DownloadURL' in parsedJSON:
            url = parsedJSON['DownloadURL']
            if url:
                # unescaped / is valid in JSON, but it can be escaped
                url = url.replace("\\/", "/")

        return (title, url)

    def _get_season_search_strings(self, show, season=None):
        if not show:
            return [{}]

        search_params = []

        name_exceptions = scene_exceptions.get_scene_exceptions(show.tvdbid) + [show.name]
        for name in name_exceptions:

            current_params = {}

            if show.tvdbid:
                current_params['tvdb'] = show.tvdbid

            elif show.tvrid:
                current_params['tvrage'] = show.tvrid

            else:
                # Search by name if we don't have tvdb or tvrage id
                current_params['series'] = sanitizeSceneName(name)

            if season != None:
                whole_season_params = current_params.copy()
                partial_season_params = current_params.copy()
                # Search for entire seasons: no need to do special things for air by date shows
                whole_season_params['category'] = 'Season'
                whole_season_params['name'] = 'Season ' + str(season)

                search_params.append(whole_season_params)

                # Search for episodes in the season
                partial_season_params['category'] = 'Episode'

                if show.air_by_date:
                    # Search for the year of the air by date show
                    partial_season_params['name'] = str(season.split('-')[0])
                else:
                    # Search for any result which has Sxx in the name
                    partial_season_params['name'] = 'S%02d' % int(season)

                search_params.append(partial_season_params)

            else:
                search_params.append(current_params)

        return search_params

    def _get_episode_search_strings(self, ep_obj):

        if not ep_obj:
            return [{}]

        search_params = {'category': 'Episode'}

        if ep_obj.show.tvdbid:
            search_params['tvdb'] = ep_obj.show.tvdbid
        elif ep_obj.show.tvrid:
            search_params['tvrage'] = ep_obj.show.rid
        else:
            search_params['series'] = sanitizeSceneName(ep_obj.show_name)

        if ep_obj.show.air_by_date:
            date_str = str(ep_obj.airdate)

            # BTN uses dots in dates, we just search for the date since that
            # combined with the series identifier should result in just one episode
            search_params['name'] = date_str.replace('-', '.')

        else:
            # Do a general name search for the episode, formatted like SXXEYY
            search_params['name'] = "S%02dE%02d" % (ep_obj.season, ep_obj.episode)

        to_return = [search_params]

        # only do scene exceptions if we are searching by name
        if 'series' in search_params:

            # add new query string for every exception
            name_exceptions = scene_exceptions.get_scene_exceptions(ep_obj.show.tvdbid)
            for cur_exception in name_exceptions:

                # don't add duplicates
                if cur_exception == ep_obj.show.name:
                    continue

                # copy all other parameters before setting the show name for this exception
                cur_return = search_params.copy()
                cur_return['series'] = sanitizeSceneName(cur_exception)
                to_return.append(cur_return)

        return to_return

    def _doGeneralSearch(self, search_string):
        # 'search' looks as broad is it can find. Can contain episode overview and title for example,
        # use with caution!
        return self._doSearch({'search': search_string})

    def findPropers(self, search_date=None):
        results = []

        search_terms = ['%.proper.%', '%.repack.%']

        for term in search_terms:
            for item in self._doSearch({'release': term}, age=4 * 24 * 60 * 60):
                if item['Time']:
                    try:
                        result_date = datetime.fromtimestamp(float(item['Time']))
                    except TypeError:
                        result_date = None

                    if result_date:
                        if not search_date or result_date > search_date:
                            title, url = self._get_title_and_url(item)
                            results.append(classes.Proper(title, url, result_date))

        return results


class BTNCache(tvcache.TVCache):

    def __init__(self, provider):
        tvcache.TVCache.__init__(self, provider)

        # At least 15 minutes between queries
        self.minTime = 15

    def updateCache(self):

        if not self.shouldUpdate():
            return

        if self._checkAuth(None):

            data = self._getRSSData()

            # As long as we got something from the provider we count it as an update
            if data:
                self.setLastUpdate()
            else:
                return []

            logger.log(u"Clearing " + self.provider.name + " cache and updating with new information")
            self._clearCache()

            if self._checkAuth(data):
                # By now we know we've got data and no auth errors, all we need to do is put it in the database
                for item in data:
                    self._parseItem(item)

            else:
                raise AuthException("Your authentication info for " + self.provider.name + " is incorrect, check your config")

        else:
            return []

    def _getRSSData(self):
        # Get the torrents uploaded since last check.
        seconds_since_last_update = math.ceil(time.time() - time.mktime(self._getLastUpdate().timetuple()))

        # default to 15 minutes
        seconds_minTime = self.minTime * 60
        if seconds_since_last_update < seconds_minTime:
            seconds_since_last_update = seconds_minTime

        # Set maximum to 24 hours (24 * 60 * 60 = 86400 seconds) of "RSS" data search, older things will need to be done through backlog
        if seconds_since_last_update > 86400:
            logger.log(u"The last known successful update on " + self.provider.name + " was more than 24 hours ago, only trying to fetch the last 24 hours!", logger.WARNING)
            seconds_since_last_update = 86400

        data = self.provider._doSearch(search_params=None, age=seconds_since_last_update)

        return data

    def _parseItem(self, item):
        (title, url) = self.provider._get_title_and_url(item)

        if title and url:
            logger.log(u"Adding item to results: " + title, logger.DEBUG)
            self._addCacheEntry(title, url)
        else:
            logger.log(u"The data returned from the " + self.provider.name + " is incomplete, this result is unusable", logger.ERROR)
            return

    def _checkAuth(self, data):
        return self.provider._checkAuthFromData(data)

provider = BTNProvider()

########NEW FILE########
__FILENAME__ = ezrss
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib
import re
try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree

import sickbeard
import generic

from sickbeard.common import Quality
from sickbeard import logger
from sickbeard import tvcache
from sickbeard import helpers


class EZRSSProvider(generic.TorrentProvider):

    def __init__(self):

        generic.TorrentProvider.__init__(self, "EZRSS")

        self.supportsBacklog = True

        self.cache = EZRSSCache(self)

        self.url = 'https://www.ezrss.it/'

    def isEnabled(self):
        return sickbeard.EZRSS

    def imageName(self):
        return 'ezrss.png'

    def getQuality(self, item):

        filename = helpers.get_xml_text(item.find('{http://xmlns.ezrss.it/0.1/}torrent/{http://xmlns.ezrss.it/0.1/}fileName'))
        quality = Quality.nameQuality(filename)

        return quality

    def findSeasonResults(self, show, season):

        results = {}

        if show.air_by_date:
            logger.log(self.name + u" doesn't support air-by-date backlog because of limitations on their RSS search.", logger.WARNING)
            return results

        results = generic.TorrentProvider.findSeasonResults(self, show, season)

        return results

    def _get_season_search_strings(self, show, season=None):

        params = {}

        if not show:
            return params

        params['show_name'] = helpers.sanitizeSceneName(show.name, ezrss=True).replace('.', ' ').encode('utf-8')

        if season != None:
            params['season'] = season

        return [params]

    def _get_episode_search_strings(self, ep_obj):

        params = {}

        if not ep_obj:
            return params

        params['show_name'] = helpers.sanitizeSceneName(ep_obj.show.name, ezrss=True).replace('.', ' ').encode('utf-8')

        if ep_obj.show.air_by_date:
            params['date'] = str(ep_obj.airdate)
        else:
            params['season'] = ep_obj.season
            params['episode'] = ep_obj.episode

        return [params]

    def _doSearch(self, search_params, show=None):

        params = {"mode": "rss"}

        if search_params:
            params.update(search_params)

        search_url = self.url + 'search/index.php?' + urllib.urlencode(params)

        logger.log(u"Search string: " + search_url, logger.DEBUG)

        data = self.getURL(search_url)

        if not data:
            logger.log(u"No data returned from " + search_url, logger.ERROR)
            return []

        parsedXML = helpers.parse_xml(data)

        if parsedXML is None:
            logger.log(u"Error trying to load " + self.name + " RSS feed", logger.ERROR)
            return []

        items = parsedXML.findall('.//item')

        results = []

        for curItem in items:

            (title, url) = self._get_title_and_url(curItem)

            if title and url:
                logger.log(u"Adding item from RSS to results: " + title, logger.DEBUG)
                results.append(curItem)
            else:
                logger.log(u"The XML returned from the " + self.name + " RSS feed is incomplete, this result is unusable", logger.ERROR)

        return results

    def _get_title_and_url(self, item):
        (title, url) = generic.TorrentProvider._get_title_and_url(self, item)

        filename = helpers.get_xml_text(item.find('{http://xmlns.ezrss.it/0.1/}torrent/{http://xmlns.ezrss.it/0.1/}fileName'))

        if filename:
            new_title = self._extract_name_from_filename(filename)
            if new_title:
                title = new_title
                logger.log(u"Extracted the name " + title + " from the torrent link", logger.DEBUG)

        return (title, url)

    def _extract_name_from_filename(self, filename):
        name_regex = '(.*?)\.?(\[.*]|\d+\.TPB)\.torrent$'
        logger.log(u"Comparing " + name_regex + " against " + filename, logger.DEBUG)
        match = re.match(name_regex, filename, re.I)
        if match:
            return match.group(1)
        return None


class EZRSSCache(tvcache.TVCache):

    def __init__(self, provider):

        tvcache.TVCache.__init__(self, provider)

        # only poll EZRSS every 15 minutes max
        self.minTime = 15

    def _getRSSData(self):

        rss_url = self.provider.url + 'feed/'
        logger.log(self.provider.name + " cache update URL: " + rss_url, logger.DEBUG)

        data = self.provider.getURL(rss_url)

        if not data:
            logger.log(u"No data returned from " + rss_url, logger.ERROR)
            return None

        return data

    def _parseItem(self, item):

        (title, url) = self.provider._get_title_and_url(item)

        if title and url:
            logger.log(u"Adding item from RSS to cache: " + title, logger.DEBUG)
            url = self._translateLinkURL(url)
            self._addCacheEntry(title, url)

        else:
            logger.log(u"The XML returned from the " + self.provider.name + " feed is incomplete, this result is unusable", logger.ERROR)
            return

provider = EZRSSProvider()

########NEW FILE########
__FILENAME__ = generic
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import datetime
import os
import re

import sickbeard

from sickbeard import helpers, classes, logger, db

from sickbeard.common import Quality, MULTI_EP_RESULT, SEASON_RESULT
from sickbeard import tvcache
from sickbeard import encodingKludge as ek
from sickbeard.exceptions import ex

from lib.hachoir_parser import createParser

from sickbeard.name_parser.parser import NameParser, InvalidNameException


class GenericProvider:

    NZB = "nzb"
    TORRENT = "torrent"

    def __init__(self, name):

        # these need to be set in the subclass
        self.providerType = None
        self.name = name
        self.url = ''

        self.supportsBacklog = False

        self.cache = tvcache.TVCache(self)

    def getID(self):
        return GenericProvider.makeID(self.name)

    @staticmethod
    def makeID(name):
        return re.sub("[^\w\d_]", "_", name.strip().lower())

    def imageName(self):
        return self.getID() + '.png'

    def _checkAuth(self):
        return

    def isActive(self):
        if self.providerType == GenericProvider.NZB and sickbeard.USE_NZBS:
            return self.isEnabled()
        elif self.providerType == GenericProvider.TORRENT and sickbeard.USE_TORRENTS:
            return self.isEnabled()
        else:
            return False

    def isEnabled(self):
        """
        This should be overridden and should return the config setting eg. sickbeard.MYPROVIDER
        """
        return False

    def getResult(self, episodes):
        """
        Returns a result of the correct type for this provider
        """

        if self.providerType == GenericProvider.NZB:
            result = classes.NZBSearchResult(episodes)
        elif self.providerType == GenericProvider.TORRENT:
            result = classes.TorrentSearchResult(episodes)
        else:
            result = classes.SearchResult(episodes)

        result.provider = self

        return result

    def getURL(self, url, post_data=None, headers=None):
        """
        By default this is just a simple urlopen call but this method should be overridden
        for providers with special URL requirements (like cookies)
        """

        if not headers:
            headers = []

        data = helpers.getURL(url, post_data, headers)

        if not data:
            logger.log(u"Error loading " + self.name + " URL: " + url, logger.ERROR)
            return None

        return data

    def downloadResult(self, result):
        """
        Save the result to disk.
        """

        logger.log(u"Downloading a result from " + self.name + " at " + result.url)

        data = self.getURL(result.url)

        if not data:
            return False

        # use the appropriate watch folder
        if self.providerType == GenericProvider.NZB:
            saveDir = sickbeard.NZB_DIR
            writeMode = 'w'
        elif self.providerType == GenericProvider.TORRENT:
            saveDir = sickbeard.TORRENT_DIR
            writeMode = 'wb'
        else:
            return False

        # use the result name as the filename
        file_name = ek.ek(os.path.join, saveDir, helpers.sanitizeFileName(result.name) + '.' + self.providerType)

        logger.log(u"Saving to " + file_name, logger.DEBUG)

        try:
            with open(file_name, writeMode) as fileOut:
                fileOut.write(data)
            helpers.chmodAsParent(file_name)
        except EnvironmentError, e:
            logger.log(u"Unable to save the file: " + ex(e), logger.ERROR)
            return False

        # as long as it's a valid download then consider it a successful snatch
        return self._verify_download(file_name)

    def _verify_download(self, file_name=None):
        """
        Checks the saved file to see if it was actually valid, if not then consider the download a failure.
        """

        # primitive verification of torrents, just make sure we didn't get a text file or something
        if self.providerType == GenericProvider.TORRENT:
            parser = createParser(file_name)
            if parser:
                mime_type = parser._getMimeType()
                try:
                    parser.stream._input.close()
                except:
                    pass
                if mime_type != 'application/x-bittorrent':
                    logger.log(u"Result is not a valid torrent file", logger.WARNING)
                    return False

        return True

    def searchRSS(self):

        self._checkAuth()
        self.cache.updateCache()

        return self.cache.findNeededEpisodes()

    def getQuality(self, item):
        """
        Figures out the quality of the given RSS item node

        item: An elementtree.ElementTree element representing the <item> tag of the RSS feed

        Returns a Quality value obtained from the node's data

        """
        (title, url) = self._get_title_and_url(item)  # @UnusedVariable
        quality = Quality.nameQuality(title)
        return quality

    def _doSearch(self):
        return []

    def _get_season_search_strings(self, show, season, episode=None):
        return []

    def _get_episode_search_strings(self, ep_obj):
        return []

    def _get_title_and_url(self, item):
        """
        Retrieves the title and URL data from the item XML node

        item: An elementtree.ElementTree element representing the <item> tag of the RSS feed

        Returns: A tuple containing two strings representing title and URL respectively
        """
        title = helpers.get_xml_text(item.find('title'))
        if title:
            title = title.replace(' ', '.')

        url = helpers.get_xml_text(item.find('link'))
        if url:
            url = url.replace('&amp;', '&')

        return (title, url)

    def findEpisode(self, episode, manualSearch=False):

        logger.log(u"Searching " + self.name + " for " + episode.prettyName())

        self.cache.updateCache()
        results = self.cache.searchCache(episode, manualSearch)
        logger.log(u"Cache results: " + str(results), logger.DEBUG)

        # if we got some results then use them no matter what.
        # OR
        # return anyway unless we're doing a manual search
        if results or not manualSearch:
            return results

        itemList = []

        for cur_search_string in self._get_episode_search_strings(episode):
            itemList += self._doSearch(cur_search_string, show=episode.show)

        for item in itemList:

            (title, url) = self._get_title_and_url(item)

            # parse the file name
            try:
                myParser = NameParser()
                parse_result = myParser.parse(title)
            except InvalidNameException:
                logger.log(u"Unable to parse the filename " + title + " into a valid episode", logger.WARNING)
                continue

            if episode.show.air_by_date:
                if parse_result.air_date != episode.airdate:

                    logger.log(u"Episode " + title + " didn't air on " + str(episode.airdate) + ", skipping it", logger.DEBUG)
                    continue

            elif parse_result.season_number != episode.season or episode.episode not in parse_result.episode_numbers:
                logger.log(u"Episode " + title + " isn't " + str(episode.season) + "x" + str(episode.episode) + ", skipping it", logger.DEBUG)
                continue

            quality = self.getQuality(item)

            if not episode.show.wantEpisode(episode.season, episode.episode, quality, manualSearch):
                logger.log(u"Ignoring result " + title + " because we don't want an episode that is " + Quality.qualityStrings[quality], logger.DEBUG)
                continue

            logger.log(u"Found result " + title + " at " + url, logger.DEBUG)

            result = self.getResult([episode])
            result.url = url
            result.name = title
            result.quality = quality

            results.append(result)

        return results

    def findSeasonResults(self, show, season):

        itemList = []
        results = {}

        for cur_string in self._get_season_search_strings(show, season):
            itemList += self._doSearch(cur_string)

        for item in itemList:

            (title, url) = self._get_title_and_url(item)

            quality = self.getQuality(item)

            # parse the file name
            try:
                myParser = NameParser(False)
                parse_result = myParser.parse(title)
            except InvalidNameException:
                logger.log(u"Unable to parse the filename " + title + " into a valid episode", logger.WARNING)
                continue

            if not show.air_by_date:
                # this check is meaningless for non-season searches
                if (parse_result.season_number != None and parse_result.season_number != season) or (parse_result.season_number == None and season != 1):
                    logger.log(u"The result " + title + " doesn't seem to be a valid episode for season " + str(season) + ", ignoring")
                    continue

                # we just use the existing info for normal searches
                actual_season = season
                actual_episodes = parse_result.episode_numbers

            else:
                if not parse_result.air_by_date:
                    logger.log(u"This is supposed to be an air-by-date search but the result " + title + " didn't parse as one, skipping it", logger.DEBUG)
                    continue

                myDB = db.DBConnection()
                sql_results = myDB.select("SELECT season, episode FROM tv_episodes WHERE showid = ? AND airdate = ?", [show.tvdbid, parse_result.air_date.toordinal()])

                if len(sql_results) != 1:
                    logger.log(u"Tried to look up the date for the episode " + title + " but the database didn't give proper results, skipping it", logger.WARNING)
                    continue

                actual_season = int(sql_results[0]["season"])
                actual_episodes = [int(sql_results[0]["episode"])]

            # make sure we want the episode
            wantEp = True
            for epNo in actual_episodes:
                if not show.wantEpisode(actual_season, epNo, quality):
                    wantEp = False
                    break

            if not wantEp:
                logger.log(u"Ignoring result " + title + " because we don't want an episode that is " + Quality.qualityStrings[quality], logger.DEBUG)
                continue

            logger.log(u"Found result " + title + " at " + url, logger.DEBUG)

            # make a result object
            epObj = []
            for curEp in actual_episodes:
                epObj.append(show.getEpisode(actual_season, curEp))

            result = self.getResult(epObj)
            result.url = url
            result.name = title
            result.quality = quality

            if len(epObj) == 1:
                epNum = epObj[0].episode
            elif len(epObj) > 1:
                epNum = MULTI_EP_RESULT
                logger.log(u"Separating multi-episode result to check for later - result contains episodes: " + str(parse_result.episode_numbers), logger.DEBUG)
            elif len(epObj) == 0:
                epNum = SEASON_RESULT
                result.extraInfo = [show]
                logger.log(u"Separating full season result to check for later", logger.DEBUG)

            if epNum in results:
                results[epNum].append(result)
            else:
                results[epNum] = [result]

        return results

    def findPropers(self, search_date=None):

        results = self.cache.listPropers(search_date)

        return [classes.Proper(x['name'], x['url'], datetime.datetime.fromtimestamp(x['time'])) for x in results]


class NZBProvider(GenericProvider):

    def __init__(self, name):

        GenericProvider.__init__(self, name)

        self.providerType = GenericProvider.NZB


class TorrentProvider(GenericProvider):

    def __init__(self, name):

        GenericProvider.__init__(self, name)

        self.providerType = GenericProvider.TORRENT

########NEW FILE########
__FILENAME__ = hdbits
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import urllib
import generic
import sickbeard

from sickbeard import classes
from sickbeard import logger, tvcache, exceptions
from sickbeard import helpers
from sickbeard.common import Quality
from sickbeard.exceptions import ex, AuthException
from sickbeard.name_parser.parser import NameParser, InvalidNameException

try:
    import json
except ImportError:
    from lib import simplejson as json


class HDBitsProvider(generic.TorrentProvider):

    def __init__(self):

        generic.TorrentProvider.__init__(self, "HDBits")

        self.supportsBacklog = True

        self.cache = HDBitsCache(self)

        self.url = 'https://hdbits.org'
        self.search_url = 'https://hdbits.org/api/torrents'
        self.rss_url = 'https://hdbits.org/api/torrents'
        self.download_url = 'http://hdbits.org/download.php?'

    def isEnabled(self):
        return sickbeard.HDBITS

    def _checkAuth(self):

        if not sickbeard.HDBITS_USERNAME  or not sickbeard.HDBITS_PASSKEY:
            raise AuthException("Your authentication credentials for " + self.name + " are missing, check your config.")

        return True

    def _checkAuthFromData(self, parsedJSON):

        if parsedJSON is None:
            return self._checkAuth()

        if 'status' in parsedJSON and 'message' in parsedJSON:
            if parsedJSON.get('status') == 5:
                logger.log(u"Incorrect authentication credentials for " + self.name + " : " + parsedJSON['message'], logger.DEBUG)
                raise AuthException("Your authentication credentials for " + self.name + " are incorrect, check your config.")

        return True

    def _get_season_search_strings(self, show, season):
        season_search_string = [self._make_post_data_JSON(show=show, season=season)]
        return season_search_string

    def _get_episode_search_strings(self, episode):
        episode_search_string = [self._make_post_data_JSON(show=episode.show, episode=episode)]
        return episode_search_string

    def _get_title_and_url(self, item):

        title = item['name']
        if title:
            title = title.replace(' ', '.')

        url = self.download_url + urllib.urlencode({'id': item['id'], 'passkey': sickbeard.HDBITS_PASSKEY})

        return (title, url)

    def _doSearch(self, search_params, show=None):

        self._checkAuth()

        logger.log(u"Search url: " + self.search_url + " search_params: " + search_params, logger.DEBUG)

        data = self.getURL(self.search_url, post_data=search_params)

        if not data:
            logger.log(u"No data returned from " + self.search_url, logger.ERROR)
            return []

        parsedJSON = helpers.parse_json(data)

        if parsedJSON is None:
            logger.log(u"Error trying to load " + self.name + " JSON data", logger.ERROR)
            return []

        if self._checkAuthFromData(parsedJSON):
            results = []

            if parsedJSON and 'data' in parsedJSON:
                items = parsedJSON['data']
            else:
                logger.log(u"Resulting JSON from " + self.name + " isn't correct, not parsing it", logger.ERROR)
                items = []

            for item in items:
                results.append(item)

        return results

    def findPropers(self, search_date=None):
        results = []

        search_terms = [' proper ', ' repack ']

        for term in search_terms:
            for item in self._doSearch(self._make_post_data_JSON(search_term=term)):
                if item['utadded']:
                    try:
                        result_date = datetime.datetime.fromtimestamp(int(item['utadded']))
                    except:
                        result_date = None

                    if result_date:
                        if not search_date or result_date > search_date:
                            title, url = self._get_title_and_url(item)
                            results.append(classes.Proper(title, url, result_date))

        return results

    def _make_post_data_JSON(self, show=None, episode=None, season=None, search_term=None):

        post_data = {
            'username': sickbeard.HDBITS_USERNAME,
            'passkey': sickbeard.HDBITS_PASSKEY,
            'category': [2],  # TV Category
        }

        if episode:
            post_data['tvdb'] = {
                'id': show.tvdbid,
                'season': episode.season,
                'episode': episode.episode
            }

        if season:
            post_data['tvdb'] = {
                'id': show.tvdbid,
                'season': season,
            }

        if search_term:
            post_data['search'] = search_term

        return json.dumps(post_data)


class HDBitsCache(tvcache.TVCache):

    def __init__(self, provider):

        tvcache.TVCache.__init__(self, provider)

        # only poll HDBits every 15 minutes max
        self.minTime = 15

    def updateCache(self):

        if not self.shouldUpdate():
            return

        if self._checkAuth(None):

            data = self._getRSSData()

            # As long as we got something from the provider we count it as an update
            if data:
                self.setLastUpdate()
            else:
                return []

            logger.log(u"Clearing " + self.provider.name + " cache and updating with new information")
            self._clearCache()

            parsedJSON = helpers.parse_json(data)

            if parsedJSON is None:
                logger.log(u"Error trying to load " + self.provider.name + " JSON feed", logger.ERROR)
                return []

            if self._checkAuth(parsedJSON):
                if parsedJSON and 'data' in parsedJSON:
                    items = parsedJSON['data']
                else:
                    logger.log(u"Resulting JSON from " + self.provider.name + " isn't correct, not parsing it", logger.ERROR)
                    return []

                for item in items:
                    self._parseItem(item)

            else:
                raise exceptions.AuthException("Your authentication info for " + self.provider.name + " is incorrect, check your config")

        else:
            return []

    def _getRSSData(self):
        return self.provider.getURL(self.provider.rss_url, post_data=self.provider._make_post_data_JSON())

    def _parseItem(self, item):

        (title, url) = self.provider._get_title_and_url(item)

        if title and url:
            logger.log(u"Adding item to results: " + title, logger.DEBUG)
            self._addCacheEntry(title, url)
        else:
            logger.log(u"The data returned from the " + self.provider.name + " is incomplete, this result is unusable", logger.ERROR)
            return

    def _checkAuth(self, data):
        return self.provider._checkAuthFromData(data)

provider = HDBitsProvider()

########NEW FILE########
__FILENAME__ = newznab
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib
import email.utils
import datetime
import re
import os

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree

import sickbeard
import generic

from sickbeard import classes
from sickbeard import helpers
from sickbeard import scene_exceptions
from sickbeard import encodingKludge as ek

from sickbeard import logger
from sickbeard import tvcache
from sickbeard.exceptions import ex, AuthException


class NewznabProvider(generic.NZBProvider):

    def __init__(self, name, url, key='', catIDs='5030,5040'):

        generic.NZBProvider.__init__(self, name)

        self.cache = NewznabCache(self)

        self.url = url

        self.key = key

        # a 0 in the key spot indicates that no key is needed
        if self.key == '0':
            self.needs_auth = False
        else:
            self.needs_auth = True

        if catIDs:
            self.catIDs = catIDs
        else:
            self.catIDs = '5030,5040'

        self.enabled = True
        self.supportsBacklog = True

        self.default = False

    def configStr(self):
        return self.name + '|' + self.url + '|' + self.key + '|' + self.catIDs + '|' + str(int(self.enabled))

    def imageName(self):
        if ek.ek(os.path.isfile, ek.ek(os.path.join, sickbeard.PROG_DIR, 'data', 'images', 'providers', self.getID() + '.png')):
            return self.getID() + '.png'
        return 'newznab.png'

    def isEnabled(self):
        return self.enabled

    def _get_season_search_strings(self, show, season=None):

        if not show:
            return [{}]

        to_return = []

        # add new query strings for exceptions
        name_exceptions = scene_exceptions.get_scene_exceptions(show.tvdbid) + [show.name]
        for cur_exception in name_exceptions:

            cur_params = {}

            # search directly by tvrage id
            if show.tvrid:
                cur_params['rid'] = show.tvrid
            # if we can't then fall back on a very basic name search
            else:
                cur_params['q'] = helpers.sanitizeSceneName(cur_exception)

            if season != None:
                # air-by-date means &season=2010&q=2010.03, no other way to do it atm
                if show.air_by_date:
                    cur_params['season'] = season.split('-')[0]
                    if 'q' in cur_params:
                        cur_params['q'] += '.' + season.replace('-', '.')
                    else:
                        cur_params['q'] = season.replace('-', '.')
                else:
                    cur_params['season'] = season

            # hack to only add a single result if it's a rageid search
            if not ('rid' in cur_params and to_return):
                to_return.append(cur_params)

        return to_return

    def _get_episode_search_strings(self, ep_obj):

        params = {}

        if not ep_obj:
            return [params]

        # search directly by tvrage id
        if ep_obj.show.tvrid:
            params['rid'] = ep_obj.show.tvrid
        # if we can't then fall back on a very basic name search
        else:
            params['q'] = helpers.sanitizeSceneName(ep_obj.show.name)

        if ep_obj.show.air_by_date:
            date_str = str(ep_obj.airdate)

            params['season'] = date_str.partition('-')[0]
            params['ep'] = date_str.partition('-')[2].replace('-', '/')
        else:
            params['season'] = ep_obj.season
            params['ep'] = ep_obj.episode

        to_return = [params]

        # only do exceptions if we are searching by name
        if 'q' in params:

            # add new query strings for exceptions
            name_exceptions = scene_exceptions.get_scene_exceptions(ep_obj.show.tvdbid)
            for cur_exception in name_exceptions:

                # don't add duplicates
                if cur_exception == ep_obj.show.name:
                    continue

                cur_return = params.copy()
                cur_return['q'] = helpers.sanitizeSceneName(cur_exception)
                to_return.append(cur_return)

        return to_return

    def _doGeneralSearch(self, search_string):
        return self._doSearch({'q': search_string})

    def _checkAuth(self):

        if self.needs_auth and not self.key:
            logger.log(u"Incorrect authentication credentials for " + self.name + " : " + "API key is missing", logger.DEBUG)
            raise AuthException("Your authentication credentials for " + self.name + " are missing, check your config.")

        return True

    def _checkAuthFromData(self, parsedXML):

        if parsedXML is None:
            return self._checkAuth()

        if parsedXML.tag == 'error':
            code = parsedXML.attrib['code']

            if code == '100':
                raise AuthException("Your API key for " + self.name + " is incorrect, check your config.")
            elif code == '101':
                raise AuthException("Your account on " + self.name + " has been suspended, contact the administrator.")
            elif code == '102':
                raise AuthException("Your account isn't allowed to use the API on " + self.name + ", contact the administrator")
            else:
                logger.log(u"Unknown error given from " + self.name + ": " + parsedXML.attrib['description'], logger.ERROR)
                return False

        return True

    def _doSearch(self, search_params, show=None, max_age=0):

        self._checkAuth()

        params = {"t": "tvsearch",
                  "maxage": sickbeard.USENET_RETENTION,
                  "limit": 100,
                  "cat": self.catIDs}

        # if max_age is set, use it, don't allow it to be missing
        if max_age or not params['maxage']:
            params['maxage'] = max_age

        if search_params:
            params.update(search_params)

        if self.needs_auth and self.key:
            params['apikey'] = self.key

        search_url = self.url + 'api?' + urllib.urlencode(params)

        logger.log(u"Search url: " + search_url, logger.DEBUG)

        data = self.getURL(search_url)

        if not data:
            logger.log(u"No data returned from " + search_url, logger.ERROR)
            return []

        # hack this in until it's fixed server side
        if not data.startswith('<?xml'):
            data = '<?xml version="1.0" encoding="ISO-8859-1" ?>' + data

        parsedXML = helpers.parse_xml(data)

        if parsedXML is None:
            logger.log(u"Error trying to load " + self.name + " XML data", logger.ERROR)
            return []

        if self._checkAuthFromData(parsedXML):

            if parsedXML.tag == 'rss':
                items = parsedXML.findall('.//item')

            else:
                logger.log(u"Resulting XML from " + self.name + " isn't RSS, not parsing it", logger.ERROR)
                return []

            results = []

            for curItem in items:
                (title, url) = self._get_title_and_url(curItem)

                if title and url:
                    logger.log(u"Adding item from RSS to results: " + title, logger.DEBUG)
                    results.append(curItem)
                else:
                    logger.log(u"The XML returned from the " + self.name + " RSS feed is incomplete, this result is unusable", logger.DEBUG)

            return results

        return []

    def findPropers(self, search_date=None):

        search_terms = ['.proper.', '.repack.']

        cache_results = self.cache.listPropers(search_date)
        results = [classes.Proper(x['name'], x['url'], datetime.datetime.fromtimestamp(x['time'])) for x in cache_results]

        for term in search_terms:
            for item in self._doSearch({'q': term}, max_age=4):

                (title, url) = self._get_title_and_url(item)

                description_node = item.find('pubDate')
                description_text = helpers.get_xml_text(description_node)

                try:
                    # we could probably do dateStr = descriptionStr but we want date in this format
                    date_text = re.search('(\w{3}, \d{1,2} \w{3} \d{4} \d\d:\d\d:\d\d) [\+\-]\d{4}', description_text).group(1)
                except:
                    date_text = None

                if not date_text:
                    logger.log(u"Unable to figure out the date for entry " + title + ", skipping it")
                    continue
                else:

                    result_date = email.utils.parsedate(date_text)
                    if result_date:
                        result_date = datetime.datetime(*result_date[0:6])

                if not search_date or result_date > search_date:
                    search_result = classes.Proper(title, url, result_date)
                    results.append(search_result)

        return results


class NewznabCache(tvcache.TVCache):

    def __init__(self, provider):

        tvcache.TVCache.__init__(self, provider)

        # only poll newznab providers every 15 minutes max
        self.minTime = 15

    def _getRSSData(self):

        params = {"t": "tvsearch",
                  "cat": self.provider.catIDs}

        if self.provider.needs_auth and self.provider.key:
            params['apikey'] = self.provider.key

        rss_url = self.provider.url + 'api?' + urllib.urlencode(params)

        logger.log(self.provider.name + " cache update URL: " + rss_url, logger.DEBUG)

        data = self.provider.getURL(rss_url)

        if not data:
            logger.log(u"No data returned from " + rss_url, logger.ERROR)
            return None

        # hack this in until it's fixed server side
        if data and not data.startswith('<?xml'):
            data = '<?xml version="1.0" encoding="ISO-8859-1" ?>' + data

        return data

    def _checkAuth(self, parsedXML):
            return self.provider._checkAuthFromData(parsedXML)

########NEW FILE########
__FILENAME__ = omgwtfnzbs
# Author: Jordon Smith <smith@jordon.me.uk>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard. If not, see <http://www.gnu.org/licenses/>.

import urllib
import generic
import sickbeard

from sickbeard import tvcache
from sickbeard import helpers
from sickbeard import classes
from sickbeard import logger
from sickbeard.exceptions import ex, AuthException
from sickbeard import show_name_helpers
from datetime import datetime

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree

try:
    import json
except ImportError:
    from lib import simplejson as json


class OmgwtfnzbsProvider(generic.NZBProvider):

    def __init__(self):
        generic.NZBProvider.__init__(self, "omgwtfnzbs")
        self.cache = OmgwtfnzbsCache(self)
        self.url = 'https://omgwtfnzbs.org/'
        self.supportsBacklog = True

    def isEnabled(self):
        return sickbeard.OMGWTFNZBS

    def _checkAuth(self):

        if not sickbeard.OMGWTFNZBS_USERNAME  or not sickbeard.OMGWTFNZBS_APIKEY:
            raise AuthException("Your authentication credentials for " + self.name + " are missing, check your config.")

        return True

    def _checkAuthFromData(self, parsed_data, is_XML=True):

        if parsed_data is None:
            return self._checkAuth()

        if is_XML:
            # provider doesn't return xml on error
            return True

        else:
            parsedJSON = parsed_data

            if 'notice' in parsedJSON:
                description_text = parsedJSON.get('notice')

                if 'information is incorrect' in parsedJSON.get('notice'):
                    logger.log(u"Incorrect authentication credentials for " + self.name + " : " + str(description_text), logger.DEBUG)
                    raise AuthException("Your authentication credentials for " + self.name + " are incorrect, check your config.")

                elif '0 results matched your terms' in parsedJSON.get('notice'):
                    return True

                else:
                    logger.log(u"Unknown error given from " + self.name + " : " + str(description_text), logger.DEBUG)
                    return False

            return True

    def _get_season_search_strings(self, show, season):
        return [x for x in show_name_helpers.makeSceneSeasonSearchString(show, season)]

    def _get_episode_search_strings(self, ep_obj):
        return [x for x in show_name_helpers.makeSceneSearchString(ep_obj)]

    def _get_title_and_url(self, item):
        return (item['release'], item['getnzb'])

    def _doSearch(self, search, show=None, retention=0):

        self._checkAuth()

        params = {'user': sickbeard.OMGWTFNZBS_USERNAME,
                  'api': sickbeard.OMGWTFNZBS_APIKEY,
                  'eng': 1,
                  'catid': '19,20',  # SD,HD
                  'retention': sickbeard.USENET_RETENTION,
                  'search': search}

        if retention or not params['retention']:
            params['retention'] = retention

        search_url = 'https://api.omgwtfnzbs.org/json/?' + urllib.urlencode(params)
        logger.log(u"Search url: " + search_url, logger.DEBUG)

        data = self.getURL(search_url)

        if not data:
            logger.log(u"No data returned from " + search_url, logger.ERROR)
            return []

        parsedJSON = helpers.parse_json(data)

        if parsedJSON is None:
            logger.log(u"Error trying to load " + self.name + " JSON data", logger.ERROR)
            return []

        if self._checkAuthFromData(parsedJSON, is_XML=False):

            results = []

            for item in parsedJSON:
                if 'release' in item and 'getnzb' in item:
                    results.append(item)

            return results

        return []

    def findPropers(self, search_date=None):
        search_terms = ['.PROPER.', '.REPACK.']
        results = []

        for term in search_terms:
            for item in self._doSearch(term, retention=4):
                if 'usenetage' in item:

                    title, url = self._get_title_and_url(item)
                    try:
                        result_date = datetime.fromtimestamp(item['usenetage'])
                    except TypeError:
                        result_date = None

                    if result_date:
                        results.append(classes.Proper(title, url, result_date))

        return results


class OmgwtfnzbsCache(tvcache.TVCache):

    def __init__(self, provider):
        tvcache.TVCache.__init__(self, provider)
        self.minTime = 20

    def _getRSSData(self):
        params = {'user': sickbeard.OMGWTFNZBS_USERNAME,
                  'api': sickbeard.OMGWTFNZBS_APIKEY,
                  'eng': 1,
                  'catid': '19,20'}  # SD,HD

        rss_url = 'https://rss.omgwtfnzbs.org/rss-download.php?' + urllib.urlencode(params)

        logger.log(self.provider.name + u" cache update URL: " + rss_url, logger.DEBUG)

        data = self.provider.getURL(rss_url)

        if not data:
            logger.log(u"No data returned from " + rss_url, logger.ERROR)
            return None

        return data

    def _checkAuth(self, parsedXML):
            return self.provider._checkAuthFromData(parsedXML)

provider = OmgwtfnzbsProvider()

########NEW FILE########
__FILENAME__ = torrentleech
# Author: Robert Massa <robertmassa@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is based upon tvtorrents.py.
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import sickbeard
import generic

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree

from sickbeard import helpers, logger, tvcache
from sickbeard.exceptions import ex, AuthException


class TorrentLeechProvider(generic.TorrentProvider):

    def __init__(self):
        generic.TorrentProvider.__init__(self, "TorrentLeech")

        self.supportsBacklog = False
        self.cache = TorrentLeechCache(self)
        self.url = 'http://www.torrentleech.org/'

    def isEnabled(self):
        return sickbeard.TORRENTLEECH

    def imageName(self):
        return 'torrentleech.png'

    def _checkAuth(self):

        if not sickbeard.TORRENTLEECH_KEY:
            raise AuthException("Your authentication credentials for " + self.name + " are missing, check your config.")
        return True

    def _checkAuthFromData(self, parsedXML):

        if parsedXML is None:
            return self._checkAuth()

        description_text = helpers.get_xml_text(parsedXML.find('.//channel/item/description'))

        if "Your RSS key is invalid" in description_text:
            logger.log(u"Incorrect authentication credentials for " + self.name + " : " + str(description_text), logger.DEBUG)
            raise AuthException(u"Your authentication credentials for " + self.name + " are incorrect, check your config")

        return True


class TorrentLeechCache(tvcache.TVCache):

    def __init__(self, provider):
        tvcache.TVCache.__init__(self, provider)

        # only poll every 15 minutes
        self.minTime = 15

    def _getRSSData(self):

        rss_url = 'http://rss.torrentleech.org/' + sickbeard.TORRENTLEECH_KEY
        logger.log(self.provider.name + u" cache update URL: " + rss_url, logger.DEBUG)

        data = self.provider.getURL(rss_url)

        if not data:
            logger.log(u"No data returned from " + rss_url, logger.ERROR)
            return None

        return data

    def _checkAuth(self, parsedXML):
            return self.provider._checkAuthFromData(parsedXML)

provider = TorrentLeechProvider()

########NEW FILE########
__FILENAME__ = tvtorrents
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree

import sickbeard
import generic

from sickbeard.exceptions import ex, AuthException
from sickbeard import helpers
from sickbeard import logger
from sickbeard import tvcache


class TvTorrentsProvider(generic.TorrentProvider):

    def __init__(self):

        generic.TorrentProvider.__init__(self, "TvTorrents")

        self.supportsBacklog = False

        self.cache = TvTorrentsCache(self)

        self.url = 'http://www.tvtorrents.com/'

    def isEnabled(self):
        return sickbeard.TVTORRENTS

    def imageName(self):
        return 'tvtorrents.png'

    def _checkAuth(self):

        if not sickbeard.TVTORRENTS_DIGEST or not sickbeard.TVTORRENTS_HASH:
            raise AuthException("Your authentication credentials for " + self.name + " are missing, check your config.")

        return True

    def _checkAuthFromData(self, parsedXML):

        if parsedXML is None:
            return self._checkAuth()

        description_text = helpers.get_xml_text(parsedXML.find('.//channel/description'))

        if "User can't be found" in description_text or "Invalid Hash" in description_text:
            logger.log(u"Incorrect authentication credentials for " + self.name + " : " + str(description_text), logger.DEBUG)
            raise AuthException(u"Your authentication credentials for " + self.name + " are incorrect, check your config")

        return True


class TvTorrentsCache(tvcache.TVCache):

    def __init__(self, provider):

        tvcache.TVCache.__init__(self, provider)

        # only poll TvTorrents every 15 minutes max
        self.minTime = 15

    def _getRSSData(self):

        # These will be ignored on the serverside.
        ignore_regex = "all.month|month.of|season[\s\d]*complete"

        rss_url = self.provider.url + 'RssServlet?digest=' + sickbeard.TVTORRENTS_DIGEST + '&hash=' + sickbeard.TVTORRENTS_HASH + '&fname=true&exclude=(' + ignore_regex + ')'
        logger.log(self.provider.name + u" cache update URL: " + rss_url, logger.DEBUG)

        data = self.provider.getURL(rss_url)

        if not data:
            logger.log(u"No data returned from " + rss_url, logger.ERROR)
            return None

        return data

    def _checkAuth(self, parsedXML):
            return self.provider._checkAuthFromData(parsedXML)

provider = TvTorrentsProvider()

########NEW FILE########
__FILENAME__ = womble
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import sickbeard
import generic

from sickbeard import logger
from sickbeard import tvcache


class WombleProvider(generic.NZBProvider):

    def __init__(self):
        generic.NZBProvider.__init__(self, "Womble's Index")
        self.cache = WombleCache(self)
        self.url = 'http://newshost.co.za/'

    def isEnabled(self):
        return sickbeard.WOMBLE


class WombleCache(tvcache.TVCache):

    def __init__(self, provider):
        tvcache.TVCache.__init__(self, provider)
        # only poll Womble's Index every 15 minutes max
        self.minTime = 15

    def _getRSSData(self):
        url = self.provider.url + 'rss/?sec=TV-x264&fr=false'
        logger.log(u"Womble's Index cache update URL: " + url, logger.DEBUG)
        data = self.provider.getURL(url)
        return data

    def _checkAuth(self, data):
        return data != 'Invalid Link'

provider = WombleProvider()

########NEW FILE########
__FILENAME__ = sab
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.



import urllib, httplib
import datetime

import sickbeard

from lib import MultipartPostHandler
import urllib2, cookielib
try:
    import json
except ImportError:
    from lib import simplejson as json

from sickbeard.common import USER_AGENT
from sickbeard import logger
from sickbeard.exceptions import ex

def sendNZB(nzb):
    """
    Sends an NZB to SABnzbd via the API.
    
    nzb: The NZBSearchResult object to send to SAB
    """

    # set up a dict with the URL params in it
    params = {}
    if sickbeard.SAB_USERNAME != None:
        params['ma_username'] = sickbeard.SAB_USERNAME
    if sickbeard.SAB_PASSWORD != None:
        params['ma_password'] = sickbeard.SAB_PASSWORD
    if sickbeard.SAB_APIKEY != None:
        params['apikey'] = sickbeard.SAB_APIKEY
    if sickbeard.SAB_CATEGORY != None:
        params['cat'] = sickbeard.SAB_CATEGORY

    # if it aired recently make it high priority
    for curEp in nzb.episodes:
        if datetime.date.today() - curEp.airdate <= datetime.timedelta(days=7):
            params['priority'] = 1

    # if it's a normal result we just pass SAB the URL
    if nzb.resultType == "nzb":
        params['mode'] = 'addurl'
        params['name'] = nzb.url

    # if we get a raw data result we want to upload it to SAB
    elif nzb.resultType == "nzbdata":
        params['mode'] = 'addfile'
        multiPartParams = {"nzbfile": (nzb.name + ".nzb", nzb.extraInfo[0])}

    url = sickbeard.SAB_HOST + "api?" + urllib.urlencode(params)

    logger.log(u"Sending NZB to SABnzbd")
    logger.log(u"URL: " + url, logger.DEBUG)

    try:
        # if we have the URL to an NZB then we've built up the SAB API URL already so just call it 
        if nzb.resultType == "nzb":
            f = urllib.urlopen(url)
        
        # if we are uploading the NZB data to SAB then we need to build a little POST form and send it
        elif nzb.resultType == "nzbdata":
            cookies = cookielib.CookieJar()
            opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookies),
                                          MultipartPostHandler.MultipartPostHandler)
            req = urllib2.Request(url,
                                  multiPartParams,
                                  headers={'User-Agent': USER_AGENT})

            f = opener.open(req)

    except (EOFError, IOError), e:
        logger.log(u"Unable to connect to SAB: " + ex(e), logger.ERROR)
        return False

    except httplib.InvalidURL, e:
        logger.log(u"Invalid SAB host, check your config: " + ex(e), logger.ERROR)
        return False

    # this means we couldn't open the connection or something just as bad
    if f == None:
        logger.log(u"No data returned from SABnzbd, NZB not sent", logger.ERROR)
        return False

    # if we opened the URL connection then read the result from SAB
    try:
        result = f.readlines()
    except Exception, e:
        logger.log(u"Error trying to get result from SAB, NZB not sent: " + ex(e), logger.ERROR)
        return False

    # SAB shouldn't return a blank result, this most likely (but not always) means that it timed out and didn't recieve the NZB
    if len(result) == 0:
        logger.log(u"No data returned from SABnzbd, NZB not sent", logger.ERROR)
        return False

    # massage the result a little bit
    sabText = result[0].strip()

    logger.log(u"Result text from SAB: " + sabText, logger.DEBUG)

    # do some crude parsing of the result text to determine what SAB said
    if sabText == "ok":
        logger.log(u"NZB sent to SAB successfully", logger.DEBUG)
        return True
    elif sabText == "Missing authentication":
        logger.log(u"Incorrect username/password sent to SAB, NZB not sent", logger.ERROR)
        return False
    else:
        logger.log(u"Unknown failure sending NZB to sab. Return text is: " + sabText, logger.ERROR)
        return False

def _checkSabResponse(f):
    try:
        result = f.readlines()
    except Exception, e:
        logger.log(u"Error trying to get result from SAB" + ex(e), logger.ERROR)
        return False, "Error from SAB"

    if len(result) == 0:
        logger.log(u"No data returned from SABnzbd, NZB not sent", logger.ERROR)
        return False, "No data from SAB"

    sabText = result[0].strip()
    sabJson = {}
    try:
        sabJson = json.loads(sabText)
    except ValueError, e:
        pass

    if sabText == "Missing authentication":
        logger.log(u"Incorrect username/password sent to SAB", logger.ERROR)
        return False, "Incorrect username/password sent to SAB"
    elif 'error' in sabJson:
        logger.log(sabJson['error'], logger.ERROR)
        return False, sabJson['error']
    else:
        return True, sabText

def _sabURLOpenSimple(url):
    try:
        f = urllib.urlopen(url)
    except (EOFError, IOError), e:
        logger.log(u"Unable to connect to SAB: " + ex(e), logger.ERROR)
        return False, "Unable to connect"
    except httplib.InvalidURL, e:
        logger.log(u"Invalid SAB host, check your config: " + ex(e), logger.ERROR)
        return False, "Invalid SAB host"
    if f == None:
        logger.log(u"No data returned from SABnzbd", logger.ERROR)
        return False, "No data returned from SABnzbd"
    else:
        return True, f

def getSabAccesMethod(host=None, username=None, password=None, apikey=None):
    url = host + "api?mode=auth"
    
    result, f = _sabURLOpenSimple(url)
    if not result:
        return False, f

    result, sabText = _checkSabResponse(f)
    if not result:
        return False, sabText

    return True, sabText

def testAuthentication(host=None, username=None, password=None, apikey=None):
    """
    Sends a simple API request to SAB to determine if the given connection information is connect
    
    host: The host where SAB is running (incl port)
    username: The username to use for the HTTP request
    password: The password to use for the HTTP request
    apikey: The API key to provide to SAB
    
    Returns: A tuple containing the success boolean and a message
    """
    
    # build up the URL parameters
    params = {}
    params['mode'] = 'queue'
    params['output'] = 'json'
    params['ma_username'] = username
    params['ma_password'] = password
    params['apikey'] = apikey
    url = host + "api?" + urllib.urlencode(params)
    
    # send the test request
    logger.log(u"SABnzbd test URL: " + url, logger.DEBUG)
    result, f = _sabURLOpenSimple(url)
    if not result:
        return False, f

    # check the result and determine if it's good or not
    result, sabText = _checkSabResponse(f)
    if not result:
        return False, sabText
    
    return True, "Success"
    

########NEW FILE########
__FILENAME__ = scene_exceptions
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import re

from sickbeard import helpers
from sickbeard import name_cache
from sickbeard import logger
from sickbeard import db


def get_scene_exceptions(tvdb_id):
    """
    Given a tvdb_id, return a list of all the scene exceptions.
    """

    myDB = db.DBConnection("cache.db")
    exceptions = myDB.select("SELECT DISTINCT show_name FROM scene_exceptions WHERE tvdb_id = ?", [tvdb_id])
    return [cur_exception["show_name"] for cur_exception in exceptions]


def get_scene_exception_by_name(show_name):
    """
    Given a show name, return the tvdbid of the exception, None if no exception
    is present.
    """

    myDB = db.DBConnection("cache.db")

    # try the obvious case first
    exception_result = myDB.select("SELECT tvdb_id FROM scene_exceptions WHERE LOWER(show_name) = ?", [show_name.lower()])
    if exception_result:
        return int(exception_result[0]["tvdb_id"])

    all_exception_results = myDB.select("SELECT DISTINCT show_name, tvdb_id FROM scene_exceptions")
    for cur_exception in all_exception_results:

        cur_exception_name = cur_exception["show_name"]
        cur_tvdb_id = int(cur_exception["tvdb_id"])

        if show_name.lower() in (cur_exception_name.lower(), helpers.sanitizeSceneName(cur_exception_name).lower().replace('.', ' ')):
            logger.log(u"Scene exception lookup got tvdb id " + str(cur_tvdb_id) + u", using that", logger.DEBUG)
            return cur_tvdb_id

    return None


def retrieve_exceptions():
    """
    Looks up the exceptions on github, parses them into a dict, and inserts them into the
    scene_exceptions table in cache.db. Also clears the scene name cache.
    """

    provider = 'sb_tvdb_scene_exceptions'
    remote_exception_dict = {}
    local_exception_dict = {}
    query_list = []

    # remote exceptions are stored on github pages
    url = 'http://midgetspy.github.io/sb_tvdb_scene_exceptions/exceptions.txt'

    logger.log(u"Check scene exceptions update")

    # get remote exceptions
    url_data = helpers.getURL(url)

    if not url_data:
        # when url_data is None, trouble connecting to github
        logger.log(u"Check scene exceptions update failed. Unable to get URL: " + url, logger.ERROR)
        return False

    else:
        # each exception is on one line with the format tvdb_id: 'show name 1', 'show name 2', etc
        for cur_line in url_data.splitlines():
            cur_line = cur_line.decode('utf-8')
            tvdb_id, sep, aliases = cur_line.partition(':')  # @UnusedVariable

            if not aliases:
                continue

            cur_tvdb_id = int(tvdb_id)

            # regex out the list of shows, taking \' into account
            alias_list = [re.sub(r'\\(.)', r'\1', x) for x in re.findall(r"'(.*?)(?<!\\)',?", aliases)]

            remote_exception_dict[cur_tvdb_id] = alias_list

        # get local exceptions
        myDB = db.DBConnection("cache.db", row_type="dict")
        sql_result = myDB.select("SELECT tvdb_id, show_name FROM scene_exceptions WHERE provider=?;", [provider])

        for cur_result in sql_result:
            cur_tvdb_id = cur_result["tvdb_id"]
            if cur_tvdb_id not in local_exception_dict:
                local_exception_dict[cur_tvdb_id] = []
            local_exception_dict[cur_tvdb_id].append(cur_result["show_name"])

        # check remote against local for added exceptions
        for cur_tvdb_id in remote_exception_dict:
            if cur_tvdb_id not in local_exception_dict:
                local_exception_dict[cur_tvdb_id] = []

            for cur_exception_name in remote_exception_dict[cur_tvdb_id]:
                if cur_exception_name not in local_exception_dict[cur_tvdb_id]:
                    query_list.append(["INSERT INTO scene_exceptions (tvdb_id,show_name,provider) VALUES (?,?,?);", [cur_tvdb_id, cur_exception_name, provider]])

        # check local against remote for removed exceptions
        for cur_tvdb_id in local_exception_dict:
            if cur_tvdb_id not in remote_exception_dict:
                query_list.append(["DELETE FROM scene_exceptions WHERE tvdb_id=? AND provider=?;", [cur_tvdb_id, provider]])

            else:
                for cur_exception_name in local_exception_dict[cur_tvdb_id]:
                    if cur_exception_name not in remote_exception_dict[cur_tvdb_id]:
                        query_list.append(["DELETE FROM scene_exceptions WHERE tvdb_id= ? AND show_name=? AND provider=?;", [cur_tvdb_id, cur_exception_name, provider]])

        if query_list:
            logger.log(u"Updating scene exceptions")
            myDB.mass_action(query_list, logTransaction=True)

            logger.log(u"Clear name cache")
            name_cache.clearCache()

            logger.log(u"Performing a vacuum on database: " + myDB.filename)
            myDB.action("VACUUM")

        else:
            logger.log(u"No scene exceptions update needed")

    return True

########NEW FILE########
__FILENAME__ = scheduler
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import time
import threading
import traceback

from sickbeard import logger
from sickbeard.exceptions import ex


class Scheduler:

    def __init__(self, action, cycleTime=datetime.timedelta(minutes=10), run_delay=datetime.timedelta(minutes=0), threadName="ScheduledThread", silent=False):

        self.lastRun = datetime.datetime.now() + run_delay - cycleTime

        self.action = action
        self.cycleTime = cycleTime

        self.thread = None
        self.threadName = threadName
        self.silent = silent

        self.initThread()

        self.abort = False

    def initThread(self):
        if self.thread == None or not self.thread.isAlive():
            self.thread = threading.Thread(None, self.runAction, self.threadName)

    def timeLeft(self):
        return self.cycleTime - (datetime.datetime.now() - self.lastRun)

    def forceRun(self):
        if not self.action.amActive:
            self.lastRun = datetime.datetime.fromordinal(1)
            return True
        return False

    def runAction(self):

        while True:

            currentTime = datetime.datetime.now()

            if currentTime - self.lastRun >= self.cycleTime:
                self.lastRun = currentTime
                try:
                    if not self.silent:
                        logger.log(u"Starting new thread: " + self.threadName, logger.DEBUG)
                    self.action.run()
                except Exception, e:
                    logger.log(u"Exception generated in thread " + self.threadName + ": " + ex(e), logger.ERROR)
                    logger.log(repr(traceback.format_exc()), logger.DEBUG)

            if self.abort:
                self.abort = False
                self.thread = None
                return

            time.sleep(1)

########NEW FILE########
__FILENAME__ = search
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import os
import traceback
import re

import sickbeard

from common import SNATCHED, Quality, SEASON_RESULT, MULTI_EP_RESULT

from sickbeard import logger, db, show_name_helpers, exceptions, helpers
from sickbeard import sab
from sickbeard import nzbget
from sickbeard import history
from sickbeard import notifiers
from sickbeard import nzbSplitter
from sickbeard import ui
from sickbeard import encodingKludge as ek
from sickbeard import providers

from sickbeard.exceptions import ex
from sickbeard.providers.generic import GenericProvider


def _downloadResult(result):
    """
    Downloads a result to the appropriate black hole folder.

    Returns a bool representing success.

    result: SearchResult instance to download.
    """

    resProvider = result.provider

    newResult = False

    if resProvider == None:
        logger.log(u"Invalid provider name - this is a coding error, report it please", logger.ERROR)
        return False

    # nzbs with an URL can just be downloaded from the provider
    if result.resultType == "nzb":
        newResult = resProvider.downloadResult(result)

    # if it's an nzb data result
    elif result.resultType == "nzbdata":

        # get the final file path to the nzb
        fileName = ek.ek(os.path.join, sickbeard.NZB_DIR, result.name + ".nzb")

        logger.log(u"Saving NZB to " + fileName)

        newResult = True

        # save the data to disk
        try:
            with ek.ek(open, fileName, 'w') as fileOut:
                fileOut.write(result.extraInfo[0])

            helpers.chmodAsParent(fileName)

        except EnvironmentError, e:
            logger.log(u"Error trying to save NZB to black hole: " + ex(e), logger.ERROR)
            newResult = False

    elif resProvider.providerType == "torrent":
        newResult = resProvider.downloadResult(result)

    else:
        logger.log(u"Invalid provider type - this is a coding error, report it please", logger.ERROR)
        return False

    return newResult


def snatchEpisode(result, endStatus=SNATCHED):
    """
    Contains the internal logic necessary to actually "snatch" a result that
    has been found.

    Returns a bool representing success.

    result: SearchResult instance to be snatched.
    endStatus: the episode status that should be used for the episode object once it's snatched.
    """

    # NZBs can be sent straight to SAB or saved to disk
    if result.resultType in ("nzb", "nzbdata"):
        if sickbeard.NZB_METHOD == "blackhole":
            dlResult = _downloadResult(result)
        elif sickbeard.NZB_METHOD == "sabnzbd":
            dlResult = sab.sendNZB(result)
        elif sickbeard.NZB_METHOD == "nzbget":
            dlResult = nzbget.sendNZB(result)
        else:
            logger.log(u"Unknown NZB action specified in config: " + sickbeard.NZB_METHOD, logger.ERROR)
            dlResult = False

    # torrents are always saved to disk
    elif result.resultType == "torrent":
        dlResult = _downloadResult(result)
    else:
        logger.log(u"Unknown result type, unable to download it", logger.ERROR)
        dlResult = False

    if dlResult == False:
        return False

    ui.notifications.message('Episode snatched', result.name)

    history.logSnatch(result)

    # don't notify when we re-download an episode
    for curEpObj in result.episodes:
        with curEpObj.lock:
            curEpObj.status = Quality.compositeStatus(endStatus, result.quality)
            curEpObj.saveToDB()

        if curEpObj.status not in Quality.DOWNLOADED:
            notifiers.notify_snatch(curEpObj.prettyName())

    return True


def searchForNeededEpisodes():

    logger.log(u"Searching all providers for any needed episodes")

    foundResults = {}

    didSearch = False

    # ask all providers for any episodes it finds
    for curProvider in providers.sortedProviderList():

        if not curProvider.isActive():
            continue

        curFoundResults = {}

        try:
            curFoundResults = curProvider.searchRSS()
        except exceptions.AuthException, e:
            logger.log(u"Authentication error: " + ex(e), logger.ERROR)
            continue
        except Exception, e:
            logger.log(u"Error while searching " + curProvider.name + ", skipping: " + ex(e), logger.ERROR)
            logger.log(traceback.format_exc(), logger.DEBUG)
            continue

        didSearch = True

        # pick a single result for each episode, respecting existing results
        for curEp in curFoundResults:

            if curEp.show.paused:
                logger.log(u"Show " + curEp.show.name + " is paused, ignoring all RSS items for " + curEp.prettyName(), logger.DEBUG)
                continue

            # find the best result for the current episode
            bestResult = None
            for curResult in curFoundResults[curEp]:
                if not bestResult or bestResult.quality < curResult.quality:
                    bestResult = curResult

            bestResult = pickBestResult(curFoundResults[curEp], curEp.show)

            # if all results were rejected move on to the next episode
            if not bestResult:
                logger.log(u"All found results for " + curEp.prettyName() + " were rejected.", logger.DEBUG)
                continue

            # if it's already in the list (from another provider) and the newly found quality is no better then skip it
            if curEp in foundResults and bestResult.quality <= foundResults[curEp].quality:
                continue

            foundResults[curEp] = bestResult

    if not didSearch:
        logger.log(u"No NZB/Torrent providers found or enabled in the sickbeard config. Please check your settings.", logger.ERROR)

    return foundResults.values()


def filter_release_name(name, filter_words):
    """
    Filters out results based on filter_words

    name: name to check
    filter_words : Words to filter on, separated by comma

    Returns: False if the release name is OK, True if it contains one of the filter_words
    """
    if filter_words:
        for test_word in filter_words.split(','):
            test_word = test_word.strip()

            if test_word:
                if re.search('(^|[\W_])' + test_word + '($|[\W_])', name, re.I):
                    logger.log(u"" + name + " contains word: " + test_word, logger.DEBUG)
                    return True

    return False


def pickBestResult(results, show, quality_list=None):

    logger.log(u"Picking the best result out of " + str([x.name for x in results]), logger.DEBUG)

    # find the best result for the current episode
    bestResult = None
    for cur_result in results:
        logger.log(u"Quality of " + cur_result.name + " is " + Quality.qualityStrings[cur_result.quality])

        if quality_list and cur_result.quality not in quality_list:
            logger.log(cur_result.name + " is a quality we know we don't want, rejecting it", logger.DEBUG)
            continue

        if show.rls_ignore_words and filter_release_name(cur_result.name, show.rls_ignore_words):
            logger.log(u"Ignoring " + cur_result.name + " based on ignored words filter: " + show.rls_ignore_words, logger.MESSAGE)
            continue

        if show.rls_require_words and not filter_release_name(cur_result.name, show.rls_require_words):
            logger.log(u"Ignoring " + cur_result.name + " based on required words filter: " + show.rls_require_words, logger.MESSAGE)
            continue

        if not bestResult or bestResult.quality < cur_result.quality and cur_result.quality != Quality.UNKNOWN:
            bestResult = cur_result

        elif bestResult.quality == cur_result.quality:
            if "proper" in cur_result.name.lower() or "repack" in cur_result.name.lower():
                bestResult = cur_result
            elif "internal" in bestResult.name.lower() and "internal" not in cur_result.name.lower():
                bestResult = cur_result

    if bestResult:
        logger.log(u"Picked " + bestResult.name + " as the best", logger.MESSAGE)
    else:
        logger.log(u"No result picked.", logger.DEBUG)

    return bestResult


def isFinalResult(result):
    """
    Checks if the given result is good enough quality that we can stop searching for other ones.

    If the result is the highest quality in both the any/best quality lists then this function
    returns True, if not then it's False

    """

    logger.log(u"Checking if we should keep searching after we've found " + result.name, logger.DEBUG)

    show_obj = result.episodes[0].show

    any_qualities, best_qualities = Quality.splitQuality(show_obj.quality)

    # if there is a redownload that's higher than this then we definitely need to keep looking
    if best_qualities and result.quality < max(best_qualities):
        return False

    # if there's no redownload that's higher (above) and this is the highest initial download then we're good
    elif any_qualities and result.quality == max(any_qualities):
        return True

    elif best_qualities and result.quality == max(best_qualities):

        # if this is the best redownload but we have a higher initial download then keep looking
        if any_qualities and result.quality < max(any_qualities):
            return False

        # if this is the best redownload and we don't have a higher initial download then we're done
        else:
            return True

    # if we got here than it's either not on the lists, they're empty, or it's lower than the highest required
    else:
        return False


def findEpisode(episode, manualSearch=False):

    logger.log(u"Searching for " + episode.prettyName())

    foundResults = []

    didSearch = False

    for curProvider in providers.sortedProviderList():

        if not curProvider.isActive():
            continue

        try:
            curFoundResults = curProvider.findEpisode(episode, manualSearch=manualSearch)
        except exceptions.AuthException, e:
            logger.log(u"Authentication error: " + ex(e), logger.ERROR)
            continue
        except Exception, e:
            logger.log(u"Error while searching " + curProvider.name + ", skipping: " + ex(e), logger.ERROR)
            logger.log(traceback.format_exc(), logger.DEBUG)
            continue

        didSearch = True

        # skip non-tv crap
        curFoundResults = filter(lambda x: show_name_helpers.filterBadReleases(x.name) and show_name_helpers.isGoodResult(x.name, episode.show), curFoundResults)

        # loop all results and see if any of them are good enough that we can stop searching
        done_searching = False
        for cur_result in curFoundResults:
            done_searching = isFinalResult(cur_result)
            logger.log(u"Should we stop searching after finding " + cur_result.name + ": " + str(done_searching), logger.DEBUG)
            if done_searching:
                break

        foundResults += curFoundResults

        # if we did find a result that's good enough to stop then don't continue
        if done_searching:
            break

    if not didSearch:
        logger.log(u"No NZB/Torrent providers found or enabled in the sickbeard config. Please check your settings.", logger.ERROR)

    bestResult = pickBestResult(foundResults, episode.show)

    return bestResult


def findSeason(show, season):

    logger.log(u"Searching for stuff we need from " + show.name + " season " + str(season))

    foundResults = {}

    didSearch = False

    for curProvider in providers.sortedProviderList():

        if not curProvider.isActive():
            continue

        try:
            curResults = curProvider.findSeasonResults(show, season)

            # make a list of all the results for this provider
            for curEp in curResults:

                # skip non-tv crap
                curResults[curEp] = filter(lambda x: show_name_helpers.filterBadReleases(x.name) and show_name_helpers.isGoodResult(x.name, show), curResults[curEp])

                if curEp in foundResults:
                    foundResults[curEp] += curResults[curEp]
                else:
                    foundResults[curEp] = curResults[curEp]

        except exceptions.AuthException, e:
            logger.log(u"Authentication error: " + ex(e), logger.ERROR)
            continue
        except Exception, e:
            logger.log(u"Error while searching " + curProvider.name + ", skipping: " + ex(e), logger.ERROR)
            logger.log(traceback.format_exc(), logger.DEBUG)
            continue

        didSearch = True

    if not didSearch:
        logger.log(u"No NZB/Torrent providers found or enabled in the sickbeard config. Please check your settings.", logger.ERROR)

    finalResults = []

    anyQualities, bestQualities = Quality.splitQuality(show.quality)

    # pick the best season NZB
    bestSeasonNZB = None
    if SEASON_RESULT in foundResults:
        bestSeasonNZB = pickBestResult(foundResults[SEASON_RESULT], show, anyQualities + bestQualities)

    highest_quality_overall = 0
    for cur_season in foundResults:
        for cur_result in foundResults[cur_season]:
            if cur_result.quality != Quality.UNKNOWN and cur_result.quality > highest_quality_overall:
                highest_quality_overall = cur_result.quality
    logger.log(u"The highest quality of any match is " + Quality.qualityStrings[highest_quality_overall], logger.DEBUG)

    # see if every episode is wanted
    if bestSeasonNZB:

        # get the quality of the season nzb
        seasonQual = Quality.nameQuality(bestSeasonNZB.name)
        seasonQual = bestSeasonNZB.quality
        logger.log(u"The quality of the season NZB is " + Quality.qualityStrings[seasonQual], logger.DEBUG)

        myDB = db.DBConnection()
        allEps = [int(x["episode"]) for x in myDB.select("SELECT episode FROM tv_episodes WHERE showid = ? AND season = ?", [show.tvdbid, season])]
        logger.log(u"Episode list: " + str(allEps), logger.DEBUG)

        allWanted = True
        anyWanted = False
        for curEpNum in allEps:
            if not show.wantEpisode(season, curEpNum, seasonQual):
                allWanted = False
            else:
                anyWanted = True

        # if we need every ep in the season and there's nothing better then just download this and be done with it
        if allWanted and bestSeasonNZB.quality == highest_quality_overall:
            logger.log(u"Every ep in this season is needed, downloading the whole NZB " + bestSeasonNZB.name)
            epObjs = []
            for curEpNum in allEps:
                epObjs.append(show.getEpisode(season, curEpNum))
            bestSeasonNZB.episodes = epObjs
            return [bestSeasonNZB]

        elif not anyWanted:
            logger.log(u"No eps from this season are wanted at this quality, ignoring the result of " + bestSeasonNZB.name, logger.DEBUG)

        else:

            if bestSeasonNZB.provider.providerType == GenericProvider.NZB:
                logger.log(u"Breaking apart the NZB and adding the individual ones to our results", logger.DEBUG)

                # if not, break it apart and add them as the lowest priority results
                individualResults = nzbSplitter.splitResult(bestSeasonNZB)

                individualResults = filter(lambda x: show_name_helpers.filterBadReleases(x.name) and show_name_helpers.isGoodResult(x.name, show), individualResults)

                for curResult in individualResults:
                    if len(curResult.episodes) == 1:
                        epNum = curResult.episodes[0].episode
                    elif len(curResult.episodes) > 1:
                        epNum = MULTI_EP_RESULT

                    if epNum in foundResults:
                        foundResults[epNum].append(curResult)
                    else:
                        foundResults[epNum] = [curResult]

            # If this is a torrent all we can do is leech the entire torrent, user will have to select which eps not do download in his torrent client
            else:

                # Season result from BTN must be a full-season torrent, creating multi-ep result for it.
                logger.log(u"Adding multi-ep result for full-season torrent. Set the episodes you don't want to 'don't download' in your torrent client if desired!")
                epObjs = []
                for curEpNum in allEps:
                    epObjs.append(show.getEpisode(season, curEpNum))
                bestSeasonNZB.episodes = epObjs

                epNum = MULTI_EP_RESULT
                if epNum in foundResults:
                    foundResults[epNum].append(bestSeasonNZB)
                else:
                    foundResults[epNum] = [bestSeasonNZB]

    # go through multi-ep results and see if we really want them or not, get rid of the rest
    multiResults = {}
    if MULTI_EP_RESULT in foundResults:
        for multiResult in foundResults[MULTI_EP_RESULT]:

            logger.log(u"Seeing if we want to bother with multi-episode result " + multiResult.name, logger.DEBUG)

            # see how many of the eps that this result covers aren't covered by single results
            neededEps = []
            notNeededEps = []
            for epObj in multiResult.episodes:
                epNum = epObj.episode
                # if we have results for the episode
                if epNum in foundResults and len(foundResults[epNum]) > 0:
                    neededEps.append(epNum)
                else:
                    neededEps.append(epNum)

            logger.log(u"Single-ep check result is neededEps: " + str(neededEps) + ", notNeededEps: " + str(notNeededEps), logger.DEBUG)

            if not neededEps:
                logger.log(u"All of these episodes were covered by single nzbs, ignoring this multi-ep result", logger.DEBUG)
                continue

            # check if these eps are already covered by another multi-result
            multiNeededEps = []
            multiNotNeededEps = []
            for epObj in multiResult.episodes:
                epNum = epObj.episode
                if epNum in multiResults:
                    multiNotNeededEps.append(epNum)
                else:
                    multiNeededEps.append(epNum)

            logger.log(u"Multi-ep check result is multiNeededEps: " + str(multiNeededEps) + ", multiNotNeededEps: " + str(multiNotNeededEps), logger.DEBUG)

            if not multiNeededEps:
                logger.log(u"All of these episodes were covered by another multi-episode nzbs, ignoring this multi-ep result", logger.DEBUG)
                continue

            # if we're keeping this multi-result then remember it
            for epObj in multiResult.episodes:
                multiResults[epObj.episode] = multiResult

            # don't bother with the single result if we're going to get it with a multi result
            for epObj in multiResult.episodes:
                epNum = epObj.episode
                if epNum in foundResults:
                    logger.log(u"A needed multi-episode result overlaps with a single-episode result for ep #" + str(epNum) + ", removing the single-episode results from the list", logger.DEBUG)
                    del foundResults[epNum]

    finalResults += set(multiResults.values())

    # of all the single ep results narrow it down to the best one for each episode
    for curEp in foundResults:
        if curEp in (MULTI_EP_RESULT, SEASON_RESULT):
            continue

        if len(foundResults[curEp]) == 0:
            continue

        finalResults.append(pickBestResult(foundResults[curEp], show))

    return finalResults

########NEW FILE########
__FILENAME__ = searchBacklog
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import datetime
import threading

import sickbeard

from sickbeard import db, scheduler
from sickbeard import search_queue
from sickbeard import logger
from sickbeard import ui
#from sickbeard.common import *

class BacklogSearchScheduler(scheduler.Scheduler):

    def forceSearch(self):
        self.action._set_lastBacklog(1)
        self.lastRun = datetime.datetime.fromordinal(1)

    def nextRun(self):
        if self.action._lastBacklog <= 1:
            return datetime.date.today()
        else:
            return datetime.date.fromordinal(self.action._lastBacklog + self.action.cycleTime)

class BacklogSearcher:

    def __init__(self):

        self._lastBacklog = self._get_lastBacklog()
        self.cycleTime = 7
        self.lock = threading.Lock()
        self.amActive = False
        self.amPaused = False
        self.amWaiting = False

        self._resetPI()

    def _resetPI(self):
        self.percentDone = 0
        self.currentSearchInfo = {'title': 'Initializing'}

    def getProgressIndicator(self):
        if self.amActive:
            return ui.ProgressIndicator(self.percentDone, self.currentSearchInfo)
        else:
            return None

    def am_running(self):
        logger.log(u"amWaiting: "+str(self.amWaiting)+", amActive: "+str(self.amActive), logger.DEBUG)
        return (not self.amWaiting) and self.amActive

    def searchBacklog(self, which_shows=None):

        if which_shows:
            show_list = which_shows
        else:
            show_list = sickbeard.showList

        if self.amActive == True:
            logger.log(u"Backlog is still running, not starting it again", logger.DEBUG)
            return

        self._get_lastBacklog()

        curDate = datetime.date.today().toordinal()
        fromDate = datetime.date.fromordinal(1)

        if not which_shows and not curDate - self._lastBacklog >= self.cycleTime:
            logger.log(u"Running limited backlog on recently missed episodes only")
            fromDate = datetime.date.today() - datetime.timedelta(days=7)

        self.amActive = True
        self.amPaused = False

        #myDB = db.DBConnection()
        #numSeasonResults = myDB.select("SELECT DISTINCT(season), showid FROM tv_episodes ep, tv_shows show WHERE season != 0 AND ep.showid = show.tvdb_id AND show.paused = 0 AND ep.airdate > ?", [fromDate.toordinal()])

        # get separate lists of the season/date shows
        #season_shows = [x for x in show_list if not x.air_by_date]
        air_by_date_shows = [x for x in show_list if x.air_by_date]

        # figure out how many segments of air by date shows we're going to do
        air_by_date_segments = []
        for cur_id in [x.tvdbid for x in air_by_date_shows]:
            air_by_date_segments += self._get_air_by_date_segments(cur_id, fromDate) 

        logger.log(u"Air-by-date segments: "+str(air_by_date_segments), logger.DEBUG)

        #totalSeasons = float(len(numSeasonResults) + len(air_by_date_segments))
        #numSeasonsDone = 0.0

        # go through non air-by-date shows and see if they need any episodes
        for curShow in show_list:

            if curShow.paused:
                continue

            if curShow.air_by_date:
                segments = [x[1] for x in self._get_air_by_date_segments(curShow.tvdbid, fromDate)]
            else:
                segments = self._get_season_segments(curShow.tvdbid, fromDate)

            for cur_segment in segments:

                self.currentSearchInfo = {'title': curShow.name + " Season "+str(cur_segment)}

                backlog_queue_item = search_queue.BacklogQueueItem(curShow, cur_segment)

                if not backlog_queue_item.wantSeason:
                    logger.log(u"Nothing in season "+str(cur_segment)+" needs to be downloaded, skipping this season", logger.DEBUG)
                else:
                    sickbeard.searchQueueScheduler.action.add_item(backlog_queue_item)  #@UndefinedVariable

        # don't consider this an actual backlog search if we only did recent eps
        # or if we only did certain shows
        if fromDate == datetime.date.fromordinal(1) and not which_shows:
            self._set_lastBacklog(curDate)

        self.amActive = False
        self._resetPI()

    def _get_lastBacklog(self):

        logger.log(u"Retrieving the last check time from the DB", logger.DEBUG)

        myDB = db.DBConnection()
        sqlResults = myDB.select("SELECT * FROM info")

        if len(sqlResults) == 0:
            lastBacklog = 1
        elif sqlResults[0]["last_backlog"] == None or sqlResults[0]["last_backlog"] == "":
            lastBacklog = 1
        else:
            lastBacklog = int(sqlResults[0]["last_backlog"])
            if lastBacklog > datetime.date.today().toordinal():
                lastBacklog = 1

        self._lastBacklog = lastBacklog
        return self._lastBacklog

    def _get_season_segments(self, tvdb_id, fromDate):
        myDB = db.DBConnection()
        sqlResults = myDB.select("SELECT DISTINCT(season) as season FROM tv_episodes WHERE showid = ? AND season > 0 and airdate > ?", [tvdb_id, fromDate.toordinal()])
        return [int(x["season"]) for x in sqlResults]

    def _get_air_by_date_segments(self, tvdb_id, fromDate):
        # query the DB for all dates for this show
        myDB = db.DBConnection()
        num_air_by_date_results = myDB.select("SELECT airdate, showid FROM tv_episodes ep, tv_shows show WHERE season != 0 AND ep.showid = show.tvdb_id AND show.paused = 0 ANd ep.airdate > ? AND ep.showid = ?",
                                 [fromDate.toordinal(), tvdb_id])

        # break them apart into month/year strings
        air_by_date_segments = []
        for cur_result in num_air_by_date_results:
            cur_date = datetime.date.fromordinal(int(cur_result["airdate"]))
            cur_date_str = str(cur_date)[:7]
            cur_tvdb_id = int(cur_result["showid"])
            
            cur_result_tuple = (cur_tvdb_id, cur_date_str)
            if cur_result_tuple not in air_by_date_segments:
                air_by_date_segments.append(cur_result_tuple)
        
        return air_by_date_segments

    def _set_lastBacklog(self, when):

        logger.log(u"Setting the last backlog in the DB to " + str(when), logger.DEBUG)

        myDB = db.DBConnection()
        sqlResults = myDB.select("SELECT * FROM info")

        if len(sqlResults) == 0:
            myDB.action("INSERT INTO info (last_backlog, last_TVDB) VALUES (?,?)", [str(when), 0])
        else:
            myDB.action("UPDATE info SET last_backlog=" + str(when))


    def run(self):
        try:
            self.searchBacklog()
        except:
            self.amActive = False
            raise

########NEW FILE########
__FILENAME__ = searchCurrent
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import sickbeard

from sickbeard import search_queue

import threading

class CurrentSearcher():

    def __init__(self):
        self.lock = threading.Lock()

        self.amActive = False

    def run(self):
        search_queue_item = search_queue.RSSSearchQueueItem()
        sickbeard.searchQueueScheduler.action.add_item(search_queue_item) #@UndefinedVariable

########NEW FILE########
__FILENAME__ = search_queue
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import datetime
import time

import sickbeard
from sickbeard import db, logger, common, exceptions, helpers
from sickbeard import generic_queue
from sickbeard import search
from sickbeard import ui

BACKLOG_SEARCH = 10
RSS_SEARCH = 20
MANUAL_SEARCH = 30


class SearchQueue(generic_queue.GenericQueue):

    def __init__(self):
        generic_queue.GenericQueue.__init__(self)
        self.queue_name = "SEARCHQUEUE"

    def is_in_queue(self, show, segment):
        for cur_item in self.queue:
            if isinstance(cur_item, BacklogQueueItem) and cur_item.show == show and cur_item.segment == segment:
                return True
        return False

    def is_ep_in_queue(self, ep_obj):
        for cur_item in self.queue:
            if isinstance(cur_item, ManualSearchQueueItem) and cur_item.ep_obj == ep_obj:
                return True
        return False

    def pause_backlog(self):
        self.min_priority = generic_queue.QueuePriorities.HIGH

    def unpause_backlog(self):
        self.min_priority = 0

    def is_backlog_paused(self):
        # backlog priorities are NORMAL, this should be done properly somewhere
        return self.min_priority >= generic_queue.QueuePriorities.NORMAL

    def is_backlog_in_progress(self):
        for cur_item in self.queue + [self.currentItem]:
            if isinstance(cur_item, BacklogQueueItem):
                return True
        return False

    def add_item(self, item):
        if isinstance(item, RSSSearchQueueItem):
            generic_queue.GenericQueue.add_item(self, item)
        # don't do duplicates
        elif isinstance(item, BacklogQueueItem) and not self.is_in_queue(item.show, item.segment):
            generic_queue.GenericQueue.add_item(self, item)
        elif isinstance(item, ManualSearchQueueItem) and not self.is_ep_in_queue(item.ep_obj):
            generic_queue.GenericQueue.add_item(self, item)
        else:
            logger.log(u"Not adding item, it's already in the queue", logger.DEBUG)


class ManualSearchQueueItem(generic_queue.QueueItem):
    def __init__(self, ep_obj):
        generic_queue.QueueItem.__init__(self, 'Manual Search', MANUAL_SEARCH)
        self.priority = generic_queue.QueuePriorities.HIGH

        self.ep_obj = ep_obj

        self.success = None

    def execute(self):
        generic_queue.QueueItem.execute(self)

        logger.log(u"Beginning manual search for " + self.ep_obj.prettyName())

        foundEpisode = search.findEpisode(self.ep_obj, manualSearch=True)
        result = False

        if not foundEpisode:
            ui.notifications.message('No downloads were found', "Couldn't find a download for <i>%s</i>" % self.ep_obj.prettyName())
            logger.log(u"Unable to find a download for " + self.ep_obj.prettyName())

        else:

            # just use the first result for now
            logger.log(u"Downloading episode from " + foundEpisode.url)
            result = search.snatchEpisode(foundEpisode)
            providerModule = foundEpisode.provider
            if not result:
                ui.notifications.error('Error while attempting to snatch ' + foundEpisode.name + ', check your logs')
            elif providerModule == None:
                ui.notifications.error('Provider is configured incorrectly, unable to download')

        self.success = result

    def finish(self):
        # don't let this linger if something goes wrong
        if self.success == None:
            self.success = False
        generic_queue.QueueItem.finish(self)


class RSSSearchQueueItem(generic_queue.QueueItem):
    def __init__(self):
        generic_queue.QueueItem.__init__(self, 'RSS Search', RSS_SEARCH)

    def execute(self):
        generic_queue.QueueItem.execute(self)

        self._changeMissingEpisodes()

        logger.log(u"Beginning search for new episodes on RSS")

        foundResults = search.searchForNeededEpisodes()

        if not len(foundResults):
            logger.log(u"No needed episodes found on the RSS feeds")
        else:
            for curResult in foundResults:
                search.snatchEpisode(curResult)
                time.sleep(2)

        generic_queue.QueueItem.finish(self)

    def _changeMissingEpisodes(self):

        logger.log(u"Changing all old missing episodes to status WANTED")

        curDate = datetime.date.today().toordinal()

        myDB = db.DBConnection()
        sqlResults = myDB.select("SELECT * FROM tv_episodes WHERE status = ? AND airdate < ?", [common.UNAIRED, curDate])

        for sqlEp in sqlResults:

            try:
                show = helpers.findCertainShow(sickbeard.showList, int(sqlEp["showid"]))
            except exceptions.MultipleShowObjectsException:
                logger.log(u"ERROR: expected to find a single show matching " + sqlEp["showid"])
                return None

            if show == None:
                logger.log(u"Unable to find the show with ID " + str(sqlEp["showid"]) + " in your show list! DB value was " + str(sqlEp), logger.ERROR)
                return None

            ep = show.getEpisode(sqlEp["season"], sqlEp["episode"])
            with ep.lock:
                if ep.show.paused:
                    ep.status = common.SKIPPED
                else:
                    ep.status = common.WANTED
                ep.saveToDB()


class BacklogQueueItem(generic_queue.QueueItem):
    def __init__(self, show, segment):
        generic_queue.QueueItem.__init__(self, 'Backlog', BACKLOG_SEARCH)
        self.priority = generic_queue.QueuePriorities.LOW
        self.thread_name = 'BACKLOG-' + str(show.tvdbid)

        self.show = show
        self.segment = segment

        logger.log(u"Seeing if we need any episodes from " + self.show.name + " season " + str(self.segment))

        myDB = db.DBConnection()

        # see if there is anything in this season worth searching for
        if not self.show.air_by_date:
            statusResults = myDB.select("SELECT status FROM tv_episodes WHERE showid = ? AND season = ?", [self.show.tvdbid, self.segment])
        else:
            segment_year, segment_month = map(int, self.segment.split('-'))
            min_date = datetime.date(segment_year, segment_month, 1)

            # it's easier to just hard code this than to worry about rolling the year over or making a month length map
            if segment_month == 12:
                max_date = datetime.date(segment_year, 12, 31)
            else:
                max_date = datetime.date(segment_year, segment_month + 1, 1) - datetime.timedelta(days=1)

            statusResults = myDB.select("SELECT status FROM tv_episodes WHERE showid = ? AND airdate >= ? AND airdate <= ?",
                                        [self.show.tvdbid, min_date.toordinal(), max_date.toordinal()])

        anyQualities, bestQualities = common.Quality.splitQuality(self.show.quality)  # @UnusedVariable
        self.wantSeason = self._need_any_episodes(statusResults, bestQualities)

    def execute(self):

        generic_queue.QueueItem.execute(self)

        results = search.findSeason(self.show, self.segment)

        # download whatever we find
        for curResult in results:
            if curResult:
                search.snatchEpisode(curResult)
                time.sleep(5)

        self.finish()

    def _need_any_episodes(self, statusResults, bestQualities):

        wantSeason = False

        # check through the list of statuses to see if we want any
        for curStatusResult in statusResults:
            curCompositeStatus = int(curStatusResult["status"])
            curStatus, curQuality = common.Quality.splitCompositeStatus(curCompositeStatus)

            if bestQualities:
                highestBestQuality = max(bestQualities)
            else:
                highestBestQuality = 0

            # if we need a better one then say yes
            if (curStatus in (common.DOWNLOADED, common.SNATCHED, common.SNATCHED_PROPER) and curQuality < highestBestQuality) or curStatus == common.WANTED:
                wantSeason = True
                break

        return wantSeason

########NEW FILE########
__FILENAME__ = showUpdater
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import os

import sickbeard

from sickbeard import logger
from sickbeard import exceptions
from sickbeard import ui
from sickbeard.exceptions import ex
from sickbeard import encodingKludge as ek
from sickbeard import db


class ShowUpdater():

    def __init__(self):
        self.updateInterval = datetime.timedelta(hours=1)

    def run(self, force=False):

        # update at 3 AM
        run_updater_time = datetime.time(hour=3)

        update_datetime = datetime.datetime.today()
        update_date = update_datetime.date()
        hour_diff = update_datetime.time().hour - run_updater_time.hour

        # if it's less than an interval after the update time then do an update (or if we're forcing it)
        if hour_diff >= 0 and hour_diff < self.updateInterval.seconds / 3600 or force:
            logger.log(u"Doing full update on all shows")
        else:
            return

        # clean out cache directory, remove everything > 12 hours old
        if sickbeard.CACHE_DIR:
            cache_dir = sickbeard.TVDB_API_PARMS['cache']
            logger.log(u"Trying to clean cache folder " + cache_dir)

            # Does our cache_dir exists
            if not ek.ek(os.path.isdir, cache_dir):
                logger.log(u"Can't clean " + cache_dir + " if it doesn't exist", logger.WARNING)
            else:
                max_age = datetime.timedelta(hours=12)
                # Get all our cache files
                cache_files = ek.ek(os.listdir, cache_dir)

                for cache_file in cache_files:
                    cache_file_path = ek.ek(os.path.join, cache_dir, cache_file)

                    if ek.ek(os.path.isfile, cache_file_path):
                        cache_file_modified = datetime.datetime.fromtimestamp(ek.ek(os.path.getmtime, cache_file_path))

                        if update_datetime - cache_file_modified > max_age:
                            try:
                                ek.ek(os.remove, cache_file_path)
                            except OSError, e:
                                logger.log(u"Unable to clean " + cache_dir + ": " + repr(e) + " / " + str(e), logger.WARNING)
                                break

        # select 10 'Ended' tv_shows updated more than 90 days ago to include in this update
        stale_should_update = []
        stale_update_date = (update_date - datetime.timedelta(days=90)).toordinal()

        myDB = db.DBConnection()
        # last_update_date <= 90 days, sorted ASC because dates are ordinal
        sql_result = myDB.select("SELECT tvdb_id FROM tv_shows WHERE status = 'Ended' AND last_update_tvdb <= ? ORDER BY last_update_tvdb ASC LIMIT 10;", [stale_update_date])

        for cur_result in sql_result:
            stale_should_update.append(cur_result['tvdb_id'])

        # start update process
        piList = []
        for curShow in sickbeard.showList:

            try:
                # if should_update returns True (not 'Ended') or show is selected stale 'Ended' then update, otherwise just refresh
                if curShow.should_update(update_date=update_date) or curShow.tvdbid in stale_should_update:
                    curQueueItem = sickbeard.showQueueScheduler.action.updateShow(curShow, True)  # @UndefinedVariable
                else:
                    logger.log(u"Not updating episodes for show " + curShow.name + " because it's marked as ended and last/next episode is not within the grace period.", logger.DEBUG)
                    curQueueItem = sickbeard.showQueueScheduler.action.refreshShow(curShow, True)  # @UndefinedVariable

                piList.append(curQueueItem)

            except (exceptions.CantUpdateException, exceptions.CantRefreshException), e:
                logger.log(u"Automatic update failed: " + ex(e), logger.ERROR)

        ui.ProgressIndicators.setIndicator('dailyUpdate', ui.QueueProgressIndicator("Daily Update", piList))

########NEW FILE########
__FILENAME__ = show_name_helpers
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import sickbeard

from sickbeard.common import countryList
from sickbeard.helpers import sanitizeSceneName
from sickbeard.scene_exceptions import get_scene_exceptions
from sickbeard import logger
from sickbeard import db

import re
import datetime

from name_parser.parser import NameParser, InvalidNameException

resultFilters = ["sub(bed|ed|pack|s)", "(dk|fin|heb|kor|nl|nor|nordic|pl|swe)sub(bed|ed|s)?",
                 "(dir|sample|sub|nfo)fix", "sample", "(dvd)?extras",
                 "dub(bed)?"]


def filterBadReleases(name):
    """
    Filters out non-english and just all-around stupid releases by comparing them
    to the resultFilters contents.

    name: the release name to check

    Returns: True if the release name is OK, False if it's bad.
    """

    try:
        fp = NameParser()
        parse_result = fp.parse(name)
    except InvalidNameException:
        logger.log(u"Unable to parse the filename " + name + " into a valid episode", logger.WARNING)
        return False

    # use the extra info and the scene group to filter against
    check_string = ''
    if parse_result.extra_info:
        check_string = parse_result.extra_info
    if parse_result.release_group:
        if check_string:
            check_string = check_string + '-' + parse_result.release_group
        else:
            check_string = parse_result.release_group

    # if there's no info after the season info then assume it's fine
    if not check_string:
        return True

    # if any of the bad strings are in the name then say no
    for ignore_word in resultFilters + sickbeard.IGNORE_WORDS.split(','):
        ignore_word = ignore_word.strip()
        if ignore_word:
            if re.search('(^|[\W_])' + ignore_word + '($|[\W_])', check_string, re.I):
                logger.log(u"Invalid scene release: " + name + " contains " + ignore_word + ", ignoring it", logger.DEBUG)
                return False

    return True


def sceneToNormalShowNames(name):
    """
    Takes a show name from a scene dirname and converts it to a more "human-readable" format.

    name: The show name to convert

    Returns: a list of all the possible "normal" names
    """

    if not name:
        return []

    name_list = [name]

    # use both and and &
    new_name = re.sub('(?i)([\. ])and([\. ])', '\\1&\\2', name, re.I)
    if new_name not in name_list:
        name_list.append(new_name)

    results = []

    for cur_name in name_list:
        # add brackets around the year
        results.append(re.sub('(\D)(\d{4})$', '\\1(\\2)', cur_name))

        # add brackets around the country
        country_match_str = '|'.join(countryList.values())
        results.append(re.sub('(?i)([. _-])(' + country_match_str + ')$', '\\1(\\2)', cur_name))

    results += name_list

    return list(set(results))


def makeSceneShowSearchStrings(show):

    showNames = allPossibleShowNames(show)

    # scenify the names
    return map(sanitizeSceneName, showNames)


def makeSceneSeasonSearchString(show, segment, extraSearchType=None):

    myDB = db.DBConnection()

    if show.air_by_date:
        numseasons = 0

        # the search string for air by date shows is just
        seasonStrings = [segment]

    else:
        numseasonsSQlResult = myDB.select("SELECT COUNT(DISTINCT season) as numseasons FROM tv_episodes WHERE showid = ? and season != 0", [show.tvdbid])
        numseasons = int(numseasonsSQlResult[0][0])

        seasonStrings = ["S%02d" % segment]

    showNames = set(makeSceneShowSearchStrings(show))

    toReturn = []

    # search each show name
    for curShow in showNames:
        # most providers all work the same way
        if not extraSearchType:
            # if there's only one season then we can just use the show name straight up
            if numseasons == 1:
                toReturn.append(curShow)
            # for providers that don't allow multiple searches in one request we only search for Sxx style stuff
            else:
                for cur_season in seasonStrings:
                    toReturn.append(curShow + "." + cur_season)

    return toReturn


def makeSceneSearchString(episode):

    myDB = db.DBConnection()
    numseasonsSQlResult = myDB.select("SELECT COUNT(DISTINCT season) as numseasons FROM tv_episodes WHERE showid = ? and season != 0", [episode.show.tvdbid])
    numseasons = int(numseasonsSQlResult[0][0])
    numepisodesSQlResult = myDB.select("SELECT COUNT(episode) as numepisodes FROM tv_episodes WHERE showid = ? and season != 0", [episode.show.tvdbid])
    numepisodes = int(numepisodesSQlResult[0][0])

    # see if we should use dates instead of episodes
    if episode.show.air_by_date and episode.airdate != datetime.date.fromordinal(1):
        epStrings = [str(episode.airdate)]
    else:
        epStrings = ["S%02iE%02i" % (int(episode.season), int(episode.episode)),
                    "%ix%02i" % (int(episode.season), int(episode.episode))]

    # for single-season shows just search for the show name -- if total ep count (exclude s0) is less than 11
    # due to the amount of qualities and releases, it is easy to go over the 50 result limit on rss feeds otherwise
    if numseasons == 1 and numepisodes < 11:
        epStrings = ['']

    showNames = set(makeSceneShowSearchStrings(episode.show))

    toReturn = []

    for curShow in showNames:
        for curEpString in epStrings:
            toReturn.append(curShow + '.' + curEpString)

    return toReturn


def isGoodResult(name, show, log=True):
    """
    Use an automatically-created regex to make sure the result actually is the show it claims to be
    """

    all_show_names = allPossibleShowNames(show)
    showNames = map(sanitizeSceneName, all_show_names) + all_show_names

    for curName in set(showNames):
        escaped_name = re.sub('\\\\[\\s.-]', '\W+', re.escape(curName))
        if show.startyear:
            escaped_name += "(?:\W+" + str(show.startyear) + ")?"
        curRegex = '^' + escaped_name + '\W+(?:(?:S\d[\dE._ -])|(?:\d\d?x)|(?:\d{4}\W\d\d\W\d\d)|(?:(?:part|pt)[\._ -]?(\d|[ivx]))|Season\W+\d+\W+|E\d+\W+)'
        if log:
            logger.log(u"Checking if show " + name + " matches " + curRegex, logger.DEBUG)

        match = re.search(curRegex, name, re.I)

        if match:
            logger.log(u"Matched " + curRegex + " to " + name, logger.DEBUG)
            return True

    if log:
        logger.log(u"Provider gave result " + name + " but that doesn't seem like a valid result for " + show.name + " so I'm ignoring it")
    return False


def allPossibleShowNames(show):
    """
    Figures out every possible variation of the name for a particular show. Includes TVDB name, TVRage name,
    country codes on the end, eg. "Show Name (AU)", and any scene exception names.

    show: a TVShow object that we should get the names of

    Returns: a list of all the possible show names
    """

    showNames = [show.name]
    showNames += [name for name in get_scene_exceptions(show.tvdbid)]

    # if we have a tvrage name then use it
    if show.tvrname != "" and show.tvrname != None:
        showNames.append(show.tvrname)

    newShowNames = []

    country_list = countryList
    country_list.update(dict(zip(countryList.values(), countryList.keys())))

    # if we have "Show Name Australia" or "Show Name (Australia)" this will add "Show Name (AU)" for
    # any countries defined in common.countryList
    # (and vice versa)
    for curName in set(showNames):
        if not curName:
            continue
        for curCountry in country_list:
            if curName.endswith(' ' + curCountry):
                newShowNames.append(curName.replace(' ' + curCountry, ' (' + country_list[curCountry] + ')'))
            elif curName.endswith(' (' + curCountry + ')'):
                newShowNames.append(curName.replace(' (' + curCountry + ')', ' (' + country_list[curCountry] + ')'))

    showNames += newShowNames

    return showNames

########NEW FILE########
__FILENAME__ = show_queue
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import traceback

import sickbeard

from lib.tvdb_api import tvdb_exceptions, tvdb_api

from sickbeard.common import SKIPPED, WANTED

from sickbeard.tv import TVShow
from sickbeard import exceptions, logger, ui, db
from sickbeard import generic_queue
from sickbeard import name_cache
from sickbeard.exceptions import ex


class ShowQueue(generic_queue.GenericQueue):

    def __init__(self):
        generic_queue.GenericQueue.__init__(self)
        self.queue_name = "SHOWQUEUE"

    def _isInQueue(self, show, actions):
        return show in [x.show for x in self.queue if x.action_id in actions]

    def _isBeingSomethinged(self, show, actions):
        return self.currentItem != None and show == self.currentItem.show and \
                self.currentItem.action_id in actions

    def isInUpdateQueue(self, show):
        return self._isInQueue(show, (ShowQueueActions.UPDATE, ShowQueueActions.FORCEUPDATE))

    def isInRefreshQueue(self, show):
        return self._isInQueue(show, (ShowQueueActions.REFRESH,))

    def isInRenameQueue(self, show):
        return self._isInQueue(show, (ShowQueueActions.RENAME,))

    def isBeingAdded(self, show):
        return self._isBeingSomethinged(show, (ShowQueueActions.ADD,))

    def isBeingUpdated(self, show):
        return self._isBeingSomethinged(show, (ShowQueueActions.UPDATE, ShowQueueActions.FORCEUPDATE))

    def isBeingRefreshed(self, show):
        return self._isBeingSomethinged(show, (ShowQueueActions.REFRESH,))

    def isBeingRenamed(self, show):
        return self._isBeingSomethinged(show, (ShowQueueActions.RENAME,))

    def _getLoadingShowList(self):
        return [x for x in self.queue + [self.currentItem] if x != None and x.isLoading]

    loadingShowList = property(_getLoadingShowList)

    def updateShow(self, show, force=False):

        if self.isBeingAdded(show):
            raise exceptions.CantUpdateException("Show is still being added, wait until it is finished before you update.")

        if self.isBeingUpdated(show):
            raise exceptions.CantUpdateException("This show is already being updated, can't update again until it's done.")

        if self.isInUpdateQueue(show):
            raise exceptions.CantUpdateException("This show is already being updated, can't update again until it's done.")

        if not force:
            queueItemObj = QueueItemUpdate(show)
        else:
            queueItemObj = QueueItemForceUpdate(show)

        self.add_item(queueItemObj)

        return queueItemObj

    def refreshShow(self, show, force=False):

        if self.isBeingRefreshed(show) and not force:
            raise exceptions.CantRefreshException("This show is already being refreshed, not refreshing again.")

        if (self.isBeingUpdated(show) or self.isInUpdateQueue(show)) and not force:
            logger.log(u"A refresh was attempted but there is already an update queued or in progress. Since updates do a refres at the end anyway I'm skipping this request.", logger.DEBUG)
            return

        queueItemObj = QueueItemRefresh(show)

        self.add_item(queueItemObj)

        return queueItemObj

    def renameShowEpisodes(self, show, force=False):

        queueItemObj = QueueItemRename(show)

        self.add_item(queueItemObj)

        return queueItemObj

    def addShow(self, tvdb_id, showDir, default_status=None, quality=None, flatten_folders=None, lang="en"):
        queueItemObj = QueueItemAdd(tvdb_id, showDir, default_status, quality, flatten_folders, lang)

        self.add_item(queueItemObj)

        return queueItemObj


class ShowQueueActions:
    REFRESH = 1
    ADD = 2
    UPDATE = 3
    FORCEUPDATE = 4
    RENAME = 5

    names = {REFRESH: 'Refresh',
                    ADD: 'Add',
                    UPDATE: 'Update',
                    FORCEUPDATE: 'Force Update',
                    RENAME: 'Rename',
                    }


class ShowQueueItem(generic_queue.QueueItem):
    """
    Represents an item in the queue waiting to be executed

    Can be either:
    - show being added (may or may not be associated with a show object)
    - show being refreshed
    - show being updated
    - show being force updated
    """
    def __init__(self, action_id, show):
        generic_queue.QueueItem.__init__(self, ShowQueueActions.names[action_id], action_id)
        self.show = show

    def isInQueue(self):
        return self in sickbeard.showQueueScheduler.action.queue + [sickbeard.showQueueScheduler.action.currentItem] #@UndefinedVariable

    def _getName(self):
        return str(self.show.tvdbid)

    def _isLoading(self):
        return False

    show_name = property(_getName)

    isLoading = property(_isLoading)


class QueueItemAdd(ShowQueueItem):
    def __init__(self, tvdb_id, showDir, default_status, quality, flatten_folders, lang):

        self.tvdb_id = tvdb_id
        self.showDir = showDir
        self.default_status = default_status
        self.quality = quality
        self.flatten_folders = flatten_folders
        self.lang = lang

        self.show = None

        # this will initialize self.show to None
        ShowQueueItem.__init__(self, ShowQueueActions.ADD, self.show)

    def _getName(self):
        """
        Returns the show name if there is a show object created, if not returns
        the dir that the show is being added to.
        """
        if self.show == None:
            return self.showDir
        return self.show.name

    show_name = property(_getName)

    def _isLoading(self):
        """
        Returns True if we've gotten far enough to have a show object, or False
        if we still only know the folder name.
        """
        if self.show == None:
            return True
        return False

    isLoading = property(_isLoading)

    def execute(self):

        ShowQueueItem.execute(self)

        logger.log(u"Starting to add show " + self.showDir)

        try:
            # make sure the tvdb ids are valid
            try:
                ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()
                if self.lang:
                    ltvdb_api_parms['language'] = self.lang

                logger.log(u"TVDB: " + repr(ltvdb_api_parms))

                t = tvdb_api.Tvdb(**ltvdb_api_parms)
                s = t[self.tvdb_id]

                # this usually only happens if they have an NFO in their show dir which gave us a TVDB ID that has no proper english version of the show
                if not s['seriesname']:
                    logger.log(u"Show in " + self.showDir + " has no name on TVDB, probably the wrong language used to search with.", logger.ERROR)
                    ui.notifications.error("Unable to add show", "Show in " + self.showDir + " has no name on TVDB, probably the wrong language. Delete .nfo and add manually in the correct language.")
                    self._finishEarly()
                    return
                # if the show has no episodes/seasons
                if not s:
                    logger.log(u"Show " + str(s['seriesname']) + " is on TVDB but contains no season/episode data.", logger.ERROR)
                    ui.notifications.error("Unable to add show", "Show " + str(s['seriesname']) + " is on TVDB but contains no season/episode data.")
                    self._finishEarly()
                    return
            except tvdb_exceptions.tvdb_exception, e:
                logger.log(u"Error contacting TVDB: " + ex(e), logger.ERROR)
                ui.notifications.error("Unable to add show", "Unable to look up the show in " + self.showDir + " on TVDB, not using the NFO. Delete .nfo and add manually in the correct language.")
                self._finishEarly()
                return

            # clear the name cache
            name_cache.clearCache()

            newShow = TVShow(self.tvdb_id, self.lang)
            newShow.loadFromTVDB()

            self.show = newShow

            # set up initial values
            self.show.location = self.showDir
            self.show.quality = self.quality if self.quality else sickbeard.QUALITY_DEFAULT
            self.show.flatten_folders = self.flatten_folders if self.flatten_folders != None else sickbeard.FLATTEN_FOLDERS_DEFAULT
            self.show.paused = 0

            # be smartish about this
            if self.show.genre and "talk show" in self.show.genre.lower():
                self.show.air_by_date = 1
            if self.show.genre and "documentary" in self.show.genre.lower():
                self.show.air_by_date = 0

        except tvdb_exceptions.tvdb_exception, e:
            logger.log(u"Unable to add show due to an error with TVDB: " + ex(e), logger.ERROR)
            if self.show:
                ui.notifications.error("Unable to add " + str(self.show.name) + " due to an error with TVDB")
            else:
                ui.notifications.error("Unable to add show due to an error with TVDB")
            self._finishEarly()
            return

        except exceptions.MultipleShowObjectsException:
            logger.log(u"The show in " + self.showDir + " is already in your show list, skipping", logger.ERROR)
            ui.notifications.error('Show skipped', "The show in " + self.showDir + " is already in your show list")
            self._finishEarly()
            return

        except Exception, e:
            logger.log(u"Error trying to add show: " + ex(e), logger.ERROR)
            logger.log(traceback.format_exc(), logger.DEBUG)
            self._finishEarly()
            raise

        try:
            self.show.saveToDB()

        except Exception, e:
            logger.log(u"Error saving the show to the database: " + ex(e), logger.ERROR)
            logger.log(traceback.format_exc(), logger.DEBUG)
            self._finishEarly()
            raise

        # add it to the show list
        sickbeard.showList.append(self.show)

        try:
            self.show.loadEpisodesFromTVDB()

        except Exception, e:
            logger.log(u"Error with TVDB, not creating episode list: " + ex(e), logger.ERROR)
            logger.log(traceback.format_exc(), logger.DEBUG)

        try:
            self.show.setTVRID()

        except Exception, e:
            logger.log(u"Error with TVRage, not setting tvrid" + ex(e), logger.ERROR)

        try:
            self.show.loadEpisodesFromDir()

        except Exception, e:
            logger.log(u"Error searching dir for episodes: " + ex(e), logger.ERROR)
            logger.log(traceback.format_exc(), logger.DEBUG)

        # if they gave a custom status then change all the eps to it
        if self.default_status != SKIPPED:
            logger.log(u"Setting all episodes to the specified default status: " + str(self.default_status))
            myDB = db.DBConnection()
            myDB.action("UPDATE tv_episodes SET status = ? WHERE status = ? AND showid = ? AND season != 0", [self.default_status, SKIPPED, self.show.tvdbid])

        # if they started with WANTED eps then run the backlog
        if self.default_status == WANTED:
            logger.log(u"Launching backlog for this show since its episodes are WANTED")
            sickbeard.backlogSearchScheduler.action.searchBacklog([self.show]) #@UndefinedVariable

        self.show.writeMetadata()
        self.show.populateCache()

        self.show.flushEpisodes()

        self.finish()

    def _finishEarly(self):
        if self.show != None:
            self.show.deleteShow()

        self.finish()


class QueueItemRefresh(ShowQueueItem):
    def __init__(self, show=None):
        ShowQueueItem.__init__(self, ShowQueueActions.REFRESH, show)

        # do refreshes first because they're quick
        self.priority = generic_queue.QueuePriorities.HIGH

    def execute(self):

        ShowQueueItem.execute(self)

        logger.log(u"Performing refresh on " + self.show.name)

        self.show.refreshDir()
        self.show.writeMetadata()
        self.show.populateCache()

        self.inProgress = False


class QueueItemRename(ShowQueueItem):
    def __init__(self, show=None):
        ShowQueueItem.__init__(self, ShowQueueActions.RENAME, show)

    def execute(self):

        ShowQueueItem.execute(self)

        logger.log(u"Performing rename on " + self.show.name)

        try:
            show_loc = self.show.location
        except exceptions.ShowDirNotFoundException:
            logger.log(u"Can't perform rename on " + self.show.name + " when the show dir is missing.", logger.WARNING)
            return

        ep_obj_rename_list = []

        ep_obj_list = self.show.getAllEpisodes(has_location=True)
        for cur_ep_obj in ep_obj_list:
            # Only want to rename if we have a location
            if cur_ep_obj.location:
                if cur_ep_obj.relatedEps:
                    # do we have one of multi-episodes in the rename list already
                    have_already = False
                    for cur_related_ep in cur_ep_obj.relatedEps + [cur_ep_obj]:
                        if cur_related_ep in ep_obj_rename_list:
                            have_already = True
                            break
                    if not have_already:
                        ep_obj_rename_list.append(cur_ep_obj)

                else:
                    ep_obj_rename_list.append(cur_ep_obj)

        for cur_ep_obj in ep_obj_rename_list:
            cur_ep_obj.rename()

        self.inProgress = False


class QueueItemUpdate(ShowQueueItem):
    def __init__(self, show=None):
        ShowQueueItem.__init__(self, ShowQueueActions.UPDATE, show)
        self.force = False

    def execute(self):

        ShowQueueItem.execute(self)

        logger.log(u"Beginning update of " + self.show.name)

        logger.log(u"Retrieving show info from TVDB", logger.DEBUG)
        try:
            self.show.loadFromTVDB(cache=not self.force)

        except tvdb_exceptions.tvdb_error, e:
            logger.log(u"Unable to contact TVDB, aborting: " + ex(e), logger.WARNING)
            return

        except tvdb_exceptions.tvdb_attributenotfound, e:
            logger.log(u"Data retrieved from TVDB was incomplete, aborting: " + ex(e), logger.ERROR)
            return

        # get episode list from DB
        logger.log(u"Loading all episodes from the database", logger.DEBUG)
        DBEpList = self.show.loadEpisodesFromDB()

        # get episode list from TVDB
        logger.log(u"Loading all episodes from theTVDB", logger.DEBUG)
        try:
            TVDBEpList = self.show.loadEpisodesFromTVDB(cache=not self.force)

        except tvdb_exceptions.tvdb_exception, e:
            logger.log(u"Unable to get info from TVDB, the show info will not be refreshed: " + ex(e), logger.ERROR)
            TVDBEpList = None

        if TVDBEpList == None:
            logger.log(u"No data returned from TVDB, unable to update this show", logger.ERROR)

        else:

            # for each ep we found on TVDB delete it from the DB list
            for curSeason in TVDBEpList:
                for curEpisode in TVDBEpList[curSeason]:
                    logger.log(u"Removing " + str(curSeason) + "x" + str(curEpisode) + " from the DB list", logger.DEBUG)
                    if curSeason in DBEpList and curEpisode in DBEpList[curSeason]:
                        del DBEpList[curSeason][curEpisode]

            # for the remaining episodes in the DB list just delete them from the DB
            for curSeason in DBEpList:
                for curEpisode in DBEpList[curSeason]:
                    logger.log(u"Permanently deleting episode " + str(curSeason) + "x" + str(curEpisode) + " from the database", logger.MESSAGE)
                    curEp = self.show.getEpisode(curSeason, curEpisode)
                    try:
                        curEp.deleteEpisode()
                    except exceptions.EpisodeDeletedException:
                        pass

        # now that we've updated the DB from TVDB see if there's anything we can add from TVRage
        with self.show.lock:
            logger.log(u"Attempting to supplement show info with info from TVRage", logger.DEBUG)
            self.show.loadLatestFromTVRage()
            if self.show.tvrid == 0:
                self.show.setTVRID()

        sickbeard.showQueueScheduler.action.refreshShow(self.show, True) #@UndefinedVariable


class QueueItemForceUpdate(QueueItemUpdate):
    def __init__(self, show=None):
        ShowQueueItem.__init__(self, ShowQueueActions.FORCEUPDATE, show)
        self.force = True

########NEW FILE########
__FILENAME__ = tv
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import os.path
import datetime
import threading
import re
import glob

import sickbeard

import xml.etree.cElementTree as etree

from name_parser.parser import NameParser, InvalidNameException

from lib.tvdb_api import tvdb_api, tvdb_exceptions

from sickbeard import db
from sickbeard import helpers, exceptions, logger
from sickbeard.exceptions import ex
from sickbeard import tvrage
from sickbeard import image_cache
from sickbeard import postProcessor

from sickbeard import encodingKludge as ek

from common import Quality, Overview, statusStrings
from common import DOWNLOADED, SNATCHED, SNATCHED_PROPER, ARCHIVED, IGNORED, UNAIRED, WANTED, SKIPPED, UNKNOWN
from common import NAMING_DUPLICATE, NAMING_EXTEND, NAMING_LIMITED_EXTEND, NAMING_SEPARATED_REPEAT, NAMING_LIMITED_EXTEND_E_PREFIXED


class TVShow(object):

    def __init__(self, tvdbid, lang=""):

        self.tvdbid = tvdbid

        self._location = ""
        self.name = ""
        self.tvrid = 0
        self.tvrname = ""
        self.network = ""
        self.genre = ""
        self.runtime = 0
        self.quality = int(sickbeard.QUALITY_DEFAULT)
        self.flatten_folders = int(sickbeard.FLATTEN_FOLDERS_DEFAULT)

        self.status = ""
        self.airs = ""
        self.startyear = 0
        self.paused = 0
        self.air_by_date = 0
        self.lang = lang
        self.last_update_tvdb = 1

        self.rls_ignore_words = ""
        self.rls_require_words = ""

        self.lock = threading.Lock()
        self._isDirGood = False

        self.episodes = {}

        otherShow = helpers.findCertainShow(sickbeard.showList, self.tvdbid)
        if otherShow != None:
            raise exceptions.MultipleShowObjectsException("Can't create a show if it already exists")

        self.loadFromDB()

    def _getLocation(self):
        # no dir check needed if missing show dirs are created during post-processing
        if sickbeard.CREATE_MISSING_SHOW_DIRS:
            return self._location

        if ek.ek(os.path.isdir, self._location):
            return self._location
        else:
            raise exceptions.ShowDirNotFoundException("Show folder doesn't exist, you shouldn't be using it")

        if self._isDirGood:
            return self._location
        else:
            raise exceptions.NoNFOException("Show folder doesn't exist, you shouldn't be using it")

    def _setLocation(self, newLocation):
        logger.log(u"Setter sets location to " + newLocation, logger.DEBUG)
        # Don't validate dir if user wants to add shows without creating a dir
        if sickbeard.ADD_SHOWS_WO_DIR or ek.ek(os.path.isdir, newLocation):
            self._location = newLocation
            self._isDirGood = True
        else:
            raise exceptions.NoNFOException("Invalid folder for the show!")

    location = property(_getLocation, _setLocation)

    # delete references to anything that's not in the internal lists
    def flushEpisodes(self):

        for curSeason in self.episodes:
            for curEp in self.episodes[curSeason]:
                myEp = self.episodes[curSeason][curEp]
                self.episodes[curSeason][curEp] = None
                del myEp

    def getAllEpisodes(self, season=None, has_location=False):

        myDB = db.DBConnection()

        sql_selection = "SELECT season, episode, "

        # subselection to detect multi-episodes early, share_location > 0
        sql_selection = sql_selection + " (SELECT COUNT (*) FROM tv_episodes WHERE showid = tve.showid AND season = tve.season AND location != '' AND location = tve.location AND episode != tve.episode) AS share_location "

        sql_selection = sql_selection + " FROM tv_episodes tve WHERE showid = " + str(self.tvdbid)

        if season is not None:
            sql_selection = sql_selection + " AND season = " + str(season)
        if has_location:
            sql_selection = sql_selection + " AND location != '' "

        # need ORDER episode ASC to rename multi-episodes in order S01E01-02
        sql_selection = sql_selection + " ORDER BY season ASC, episode ASC"

        results = myDB.select(sql_selection)

        ep_list = []
        for cur_result in results:
            cur_ep = self.getEpisode(int(cur_result["season"]), int(cur_result["episode"]))
            if cur_ep:
                cur_ep.relatedEps = []
                if cur_ep.location:
                    # if there is a location, check if it's a multi-episode (share_location > 0) and put them in relatedEps
                    if cur_result["share_location"] > 0:
                        related_eps_result = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND season = ? AND location = ? AND episode != ? ORDER BY episode ASC", [self.tvdbid, cur_ep.season, cur_ep.location, cur_ep.episode])
                        for cur_related_ep in related_eps_result:
                            related_ep = self.getEpisode(int(cur_related_ep["season"]), int(cur_related_ep["episode"]))
                            if related_ep not in cur_ep.relatedEps:
                                cur_ep.relatedEps.append(related_ep)
                ep_list.append(cur_ep)

        return ep_list

    def getEpisode(self, season, episode, file=None, noCreate=False):

        #return TVEpisode(self, season, episode)

        if not season in self.episodes:
            self.episodes[season] = {}

        ep = None

        if not episode in self.episodes[season] or self.episodes[season][episode] == None:
            if noCreate:
                return None

            logger.log(str(self.tvdbid) + u": An object for episode " + str(season) + "x" + str(episode) + " didn't exist in the cache, trying to create it", logger.DEBUG)

            if file != None:
                ep = TVEpisode(self, season, episode, file)
            else:
                ep = TVEpisode(self, season, episode)

            if ep != None:
                self.episodes[season][episode] = ep

        return self.episodes[season][episode]

    def should_update(self, update_date=datetime.date.today()):

        # if show is not 'Ended' always update (status 'Continuing' or '')
        if self.status != 'Ended':
            return True

        # run logic against the current show latest aired and next unaired data to see if we should bypass 'Ended' status
        cur_tvdbid = self.tvdbid

        graceperiod = datetime.timedelta(days=30)

        myDB = db.DBConnection()
        last_airdate = datetime.date.fromordinal(1)

        # get latest aired episode to compare against today - graceperiod and today + graceperiod
        sql_result = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND season > '0' AND airdate > '1' AND status > '1' ORDER BY airdate DESC LIMIT 1", [cur_tvdbid])

        if sql_result:
            last_airdate = datetime.date.fromordinal(sql_result[0]['airdate'])
            if last_airdate >= (update_date - graceperiod) and last_airdate <= (update_date + graceperiod):
                return True

        # get next upcoming UNAIRED episode to compare against today + graceperiod
        sql_result = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND season > '0' AND airdate > '1' AND status = '1' ORDER BY airdate ASC LIMIT 1", [cur_tvdbid])

        if sql_result:
            next_airdate = datetime.date.fromordinal(sql_result[0]['airdate'])
            if next_airdate <= (update_date + graceperiod):
                return True

        last_update_tvdb = datetime.date.fromordinal(self.last_update_tvdb)

        # in the first year after ended (last airdate), update every 30 days
        if (update_date - last_airdate) < datetime.timedelta(days=450) and (update_date - last_update_tvdb) > datetime.timedelta(days=30):
            return True

        return False

    def writeShowNFO(self):

        result = False

        if not ek.ek(os.path.isdir, self._location):
            logger.log(str(self.tvdbid) + u": Show dir doesn't exist, skipping NFO generation")
            return False

        logger.log(str(self.tvdbid) + u": Writing NFOs for show")
        for cur_provider in sickbeard.metadata_provider_dict.values():
            result = cur_provider.create_show_metadata(self) or result

        return result

    def writeMetadata(self, show_only=False):

        if not ek.ek(os.path.isdir, self._location):
            logger.log(str(self.tvdbid) + u": Show dir doesn't exist, skipping NFO generation")
            return

        self.getImages()

        self.writeShowNFO()

        if not show_only:
            self.writeEpisodeNFOs()

    def writeEpisodeNFOs(self):

        if not ek.ek(os.path.isdir, self._location):
            logger.log(str(self.tvdbid) + u": Show dir doesn't exist, skipping NFO generation")
            return

        logger.log(str(self.tvdbid) + u": Writing NFOs for all episodes")

        myDB = db.DBConnection()
        sqlResults = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND location != ''", [self.tvdbid])

        for epResult in sqlResults:
            logger.log(str(self.tvdbid) + u": Retrieving/creating episode " + str(epResult["season"]) + u"x" + str(epResult["episode"]), logger.DEBUG)
            curEp = self.getEpisode(epResult["season"], epResult["episode"])
            curEp.createMetaFiles()

    # find all media files in the show folder and create episodes for as many as possible
    def loadEpisodesFromDir(self):

        if not ek.ek(os.path.isdir, self._location):
            logger.log(str(self.tvdbid) + u": Show dir doesn't exist, not loading episodes from disk")
            return

        logger.log(str(self.tvdbid) + u": Loading all episodes from the show directory " + self._location)

        # get file list
        mediaFiles = helpers.listMediaFiles(self._location)

        # create TVEpisodes from each media file (if possible)
        for mediaFile in mediaFiles:

            curEpisode = None

            logger.log(str(self.tvdbid) + u": Creating episode from " + mediaFile, logger.DEBUG)
            try:
                curEpisode = self.makeEpFromFile(ek.ek(os.path.join, self._location, mediaFile))
            except (exceptions.ShowNotFoundException, exceptions.EpisodeNotFoundException), e:
                logger.log(u"Episode " + mediaFile + " returned an exception: " + ex(e), logger.ERROR)
                continue
            except exceptions.EpisodeDeletedException:
                logger.log(u"The episode deleted itself when I tried making an object for it", logger.DEBUG)

            if curEpisode is None:
                continue

            if not curEpisode.release_name:
                ep_file_name = ek.ek(os.path.basename, curEpisode.location)
                ep_base_name = helpers.remove_non_release_groups(helpers.remove_extension(ep_file_name))

                parse_result = None
                try:
                    np = NameParser(False)
                    parse_result = np.parse(ep_base_name)
                except InvalidNameException:
                    pass

                if not ' ' in ep_base_name and parse_result and parse_result.release_group:
                    logger.log(u"Name " + ep_base_name + u" gave release group of " + parse_result.release_group + ", seems valid", logger.DEBUG)
                    curEpisode.release_name = ep_base_name

            # store the reference in the show
            if curEpisode != None:
                curEpisode.saveToDB()

    def loadEpisodesFromDB(self):

        logger.log(u"Loading all episodes from the DB")

        myDB = db.DBConnection()
        sql = "SELECT * FROM tv_episodes WHERE showid = ?"
        sqlResults = myDB.select(sql, [self.tvdbid])

        scannedEps = {}

        ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

        if self.lang:
            ltvdb_api_parms['language'] = self.lang

        t = tvdb_api.Tvdb(**ltvdb_api_parms)

        cachedShow = t[self.tvdbid]
        cachedSeasons = {}

        for curResult in sqlResults:

            deleteEp = False

            curSeason = int(curResult["season"])
            curEpisode = int(curResult["episode"])
            if curSeason not in cachedSeasons:
                try:
                    cachedSeasons[curSeason] = cachedShow[curSeason]
                except tvdb_exceptions.tvdb_seasonnotfound, e:
                    logger.log(u"Error when trying to load the episode from TVDB: " + e.message, logger.WARNING)
                    deleteEp = True

            if not curSeason in scannedEps:
                scannedEps[curSeason] = {}

            logger.log(u"Loading episode " + str(curSeason) + "x" + str(curEpisode) + " from the DB", logger.DEBUG)

            try:
                curEp = self.getEpisode(curSeason, curEpisode)

                # if we found out that the ep is no longer on TVDB then delete it from our database too
                if deleteEp:
                    curEp.deleteEpisode()

                curEp.loadFromDB(curSeason, curEpisode)
                curEp.loadFromTVDB(tvapi=t, cachedSeason=cachedSeasons[curSeason])
                scannedEps[curSeason][curEpisode] = True
            except exceptions.EpisodeDeletedException:
                logger.log(u"Tried loading an episode from the DB that should have been deleted, skipping it", logger.DEBUG)
                continue

        return scannedEps

    def loadEpisodesFromTVDB(self, cache=True):

        # There's gotta be a better way of doing this but we don't wanna
        # change the cache value elsewhere
        ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

        if not cache:
            ltvdb_api_parms['cache'] = False

        if self.lang:
            ltvdb_api_parms['language'] = self.lang

        try:
            t = tvdb_api.Tvdb(**ltvdb_api_parms)
            showObj = t[self.tvdbid]
        except tvdb_exceptions.tvdb_error:
            logger.log(u"TVDB timed out, unable to update episodes from TVDB", logger.ERROR)
            return None

        logger.log(str(self.tvdbid) + u": Loading all episodes from theTVDB...")

        scannedEps = {}

        for season in showObj:
            scannedEps[season] = {}
            for episode in showObj[season]:
                # need some examples of wtf episode 0 means to decide if we want it or not
                if episode == 0:
                    continue
                try:
                    #ep = TVEpisode(self, season, episode)
                    ep = self.getEpisode(season, episode)
                except exceptions.EpisodeNotFoundException:
                    logger.log(str(self.tvdbid) + u": TVDB object for " + str(season) + "x" + str(episode) + " is incomplete, skipping this episode")
                    continue
                else:
                    try:
                        ep.loadFromTVDB(tvapi=t)
                    except exceptions.EpisodeDeletedException:
                        logger.log(u"The episode was deleted, skipping the rest of the load")
                        continue

                with ep.lock:
                    logger.log(str(self.tvdbid) + u": Loading info from theTVDB for episode " + str(season) + "x" + str(episode), logger.DEBUG)
                    ep.loadFromTVDB(season, episode, tvapi=t)
                    if ep.dirty:
                        ep.saveToDB()

                scannedEps[season][episode] = True

        # Done updating save last update date
        self.last_update_tvdb = datetime.date.today().toordinal()
        self.saveToDB()

        return scannedEps

    def setTVRID(self, force=False):

        if self.tvrid != 0 and not force:
            logger.log(u"No need to get the TVRage ID, it's already populated", logger.DEBUG)
            return

        logger.log(u"Attempting to retrieve the TVRage ID", logger.DEBUG)

        try:
            # load the tvrage object, it will set the ID in its constructor if possible
            tvrage.TVRage(self)
            self.saveToDB()
        except exceptions.TVRageException, e:
            logger.log(u"Couldn't get TVRage ID because we're unable to sync TVDB and TVRage: " + ex(e), logger.DEBUG)
            return

    def getImages(self, fanart=None, poster=None):
        fanart_result = poster_result = banner_result = False
        season_posters_result = season_banners_result = season_all_poster_result = season_all_banner_result = False

        for cur_provider in sickbeard.metadata_provider_dict.values():
            # FIXME: Needs to not show this message if the option is not enabled?
            logger.log(u"Running metadata routines for " + cur_provider.name, logger.DEBUG)

            fanart_result = cur_provider.create_fanart(self) or fanart_result
            poster_result = cur_provider.create_poster(self) or poster_result
            banner_result = cur_provider.create_banner(self) or banner_result

            season_posters_result = cur_provider.create_season_posters(self) or season_posters_result
            season_banners_result = cur_provider.create_season_banners(self) or season_banners_result
            season_all_poster_result = cur_provider.create_season_all_poster(self) or season_all_poster_result
            season_all_banner_result = cur_provider.create_season_all_banner(self) or season_all_banner_result

        return fanart_result or poster_result or banner_result or season_posters_result or season_banners_result or season_all_poster_result or season_all_banner_result

    def loadLatestFromTVRage(self):

        try:
            # load the tvrage object
            tvr = tvrage.TVRage(self)

            newEp = tvr.findLatestEp()

            if newEp != None:
                logger.log(u"TVRage gave us an episode object - saving it for now", logger.DEBUG)
                newEp.saveToDB()

            # make an episode out of it
        except exceptions.TVRageException, e:
            logger.log(u"Unable to add TVRage info: " + ex(e), logger.WARNING)

    # make a TVEpisode object from a media file
    def makeEpFromFile(self, file):

        if not ek.ek(os.path.isfile, file):
            logger.log(str(self.tvdbid) + u": That isn't even a real file dude... " + file)
            return None

        logger.log(str(self.tvdbid) + u": Creating episode object from " + file, logger.DEBUG)

        try:
            myParser = NameParser()
            parse_result = myParser.parse(file)
        except InvalidNameException:
            logger.log(u"Unable to parse the filename " + file + " into a valid episode", logger.ERROR)
            return None

        if len(parse_result.episode_numbers) == 0 and not parse_result.air_by_date:
            logger.log(u"parse_result: " + str(parse_result))
            logger.log(u"No episode number found in " + file + ", ignoring it", logger.ERROR)
            return None

        # for now lets assume that any episode in the show dir belongs to that show
        season = parse_result.season_number if parse_result.season_number != None else 1
        episodes = parse_result.episode_numbers
        rootEp = None

        # if we have an air-by-date show then get the real season/episode numbers
        if parse_result.air_by_date:
            try:
                # There's gotta be a better way of doing this but we don't wanna
                # change the cache value elsewhere
                ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

                if self.lang:
                    ltvdb_api_parms['language'] = self.lang

                t = tvdb_api.Tvdb(**ltvdb_api_parms)

                epObj = t[self.tvdbid].airedOn(parse_result.air_date)[0]
                season = int(epObj["seasonnumber"])
                episodes = [int(epObj["episodenumber"])]
            except tvdb_exceptions.tvdb_episodenotfound:
                logger.log(u"Unable to find episode with date " + str(parse_result.air_date) + " for show " + self.name + ", skipping", logger.WARNING)
                return None
            except tvdb_exceptions.tvdb_error, e:
                logger.log(u"Unable to contact TVDB: " + ex(e), logger.WARNING)
                return None

        for curEpNum in episodes:

            episode = int(curEpNum)

            logger.log(str(self.tvdbid) + u": " + file + " parsed to " + self.name + " " + str(season) + "x" + str(episode), logger.DEBUG)

            checkQualityAgain = False
            same_file = False
            curEp = self.getEpisode(season, episode)

            if curEp == None:
                try:
                    curEp = self.getEpisode(season, episode, file)
                except exceptions.EpisodeNotFoundException:
                    logger.log(str(self.tvdbid) + u": Unable to figure out what this file is, skipping", logger.ERROR)
                    continue

            else:
                # if there is a new file associated with this ep then re-check the quality
                if curEp.location and ek.ek(os.path.normpath, curEp.location) != ek.ek(os.path.normpath, file):
                    logger.log(u"The old episode had a different file associated with it, I will re-check the quality based on the new filename " + file, logger.DEBUG)
                    checkQualityAgain = True

                with curEp.lock:
                    old_size = curEp.file_size
                    curEp.location = file
                    # if the sizes are the same then it's probably the same file
                    if old_size and curEp.file_size == old_size:
                        same_file = True
                    else:
                        same_file = False

                    curEp.checkForMetaFiles()

            if rootEp == None:
                rootEp = curEp
            else:
                if curEp not in rootEp.relatedEps:
                    rootEp.relatedEps.append(curEp)

            # if it's a new file then
            if not same_file:
                curEp.release_name = ''

            # if they replace a file on me I'll make some attempt at re-checking the quality unless I know it's the same file
            if checkQualityAgain and not same_file:
                newQuality = Quality.nameQuality(file)
                logger.log(u"Since this file has been renamed, I checked " + file + " and found quality " + Quality.qualityStrings[newQuality], logger.DEBUG)
                if newQuality != Quality.UNKNOWN:
                    curEp.status = Quality.compositeStatus(DOWNLOADED, newQuality)

            # check for status/quality changes as long as it's a new file
            elif not same_file and sickbeard.helpers.isMediaFile(file) and curEp.status not in Quality.DOWNLOADED + [ARCHIVED, IGNORED]:

                oldStatus, oldQuality = Quality.splitCompositeStatus(curEp.status)
                newQuality = Quality.nameQuality(file)
                if newQuality == Quality.UNKNOWN:
                    newQuality = Quality.assumeQuality(file)

                newStatus = None

                # if it was snatched and now exists then set the status correctly
                if oldStatus == SNATCHED and oldQuality <= newQuality:
                    logger.log(u"STATUS: this ep used to be snatched with quality " + Quality.qualityStrings[oldQuality] + " but a file exists with quality " + Quality.qualityStrings[newQuality] + " so I'm setting the status to DOWNLOADED", logger.DEBUG)
                    newStatus = DOWNLOADED

                # if it was snatched proper and we found a higher quality one then allow the status change
                elif oldStatus == SNATCHED_PROPER and oldQuality < newQuality:
                    logger.log(u"STATUS: this ep used to be snatched proper with quality " + Quality.qualityStrings[oldQuality] + " but a file exists with quality " + Quality.qualityStrings[newQuality] + " so I'm setting the status to DOWNLOADED", logger.DEBUG)
                    newStatus = DOWNLOADED

                elif oldStatus not in (SNATCHED, SNATCHED_PROPER):
                    newStatus = DOWNLOADED

                if newStatus != None:
                    with curEp.lock:
                        logger.log(u"STATUS: we have an associated file, so setting the status from " + str(curEp.status) + " to DOWNLOADED/" + str(Quality.statusFromName(file)), logger.DEBUG)
                        curEp.status = Quality.compositeStatus(newStatus, newQuality)

            with curEp.lock:
                curEp.saveToDB()

        return rootEp

    def loadFromDB(self, skipNFO=False):

        logger.log(str(self.tvdbid) + u": Loading show info from database")

        myDB = db.DBConnection()

        sqlResults = myDB.select("SELECT * FROM tv_shows WHERE tvdb_id = ?", [self.tvdbid])

        if len(sqlResults) > 1:
            raise exceptions.MultipleDBShowsException()
        elif len(sqlResults) == 0:
            logger.log(str(self.tvdbid) + u": Unable to find the show in the database")
            return
        else:
            if self.name == "":
                self.name = sqlResults[0]["show_name"]
            self.tvrname = sqlResults[0]["tvr_name"]
            if self.network == "":
                self.network = sqlResults[0]["network"]
            if self.genre == "":
                self.genre = sqlResults[0]["genre"]

            self.runtime = sqlResults[0]["runtime"]

            self.status = sqlResults[0]["status"]
            if self.status == None:
                self.status = ""
            self.airs = sqlResults[0]["airs"]
            if self.airs == None:
                self.airs = ""
            self.startyear = sqlResults[0]["startyear"]
            if self.startyear == None:
                self.startyear = 0

            self.air_by_date = sqlResults[0]["air_by_date"]
            if self.air_by_date == None:
                self.air_by_date = 0

            self.quality = int(sqlResults[0]["quality"])
            self.flatten_folders = int(sqlResults[0]["flatten_folders"])
            self.paused = int(sqlResults[0]["paused"])

            self._location = sqlResults[0]["location"]

            if self.tvrid == 0:
                self.tvrid = int(sqlResults[0]["tvr_id"])

            if self.lang == "":
                self.lang = sqlResults[0]["lang"]

            self.last_update_tvdb = sqlResults[0]["last_update_tvdb"]

            self.rls_ignore_words = sqlResults[0]["rls_ignore_words"]
            self.rls_require_words = sqlResults[0]["rls_require_words"]

    def loadFromTVDB(self, cache=True, tvapi=None, cachedSeason=None):

        logger.log(str(self.tvdbid) + u": Loading show info from theTVDB")

        # There's gotta be a better way of doing this but we don't wanna
        # change the cache value elsewhere
        if tvapi is None:
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if not cache:
                ltvdb_api_parms['cache'] = False

            if self.lang:
                ltvdb_api_parms['language'] = self.lang

            t = tvdb_api.Tvdb(**ltvdb_api_parms)

        else:
            t = tvapi

        myEp = t[self.tvdbid]

        try:
            self.name = myEp["seriesname"].strip()
        except AttributeError:
            raise tvdb_exceptions.tvdb_attributenotfound("Found %s, but attribute 'seriesname' was empty." % (self.tvdbid))

        self.genre = myEp['genre']
        self.network = myEp['network']

        if myEp["airs_dayofweek"] != None and myEp["airs_time"] != None:
            self.airs = myEp["airs_dayofweek"] + " " + myEp["airs_time"]

        if myEp["firstaired"] != None and myEp["firstaired"]:
            self.startyear = int(myEp["firstaired"].split('-')[0])

        if self.airs == None:
            self.airs = ""

        if myEp["status"] != None:
            self.status = myEp["status"]

        if self.status == None:
            self.status = ""

        self.saveToDB()

    def nextEpisode(self):

        logger.log(str(self.tvdbid) + u": Finding the episode which airs next", logger.DEBUG)

        myDB = db.DBConnection()
        innerQuery = "SELECT airdate FROM tv_episodes WHERE showid = ? AND airdate >= ? AND status = ? ORDER BY airdate ASC LIMIT 1"
        innerParams = [self.tvdbid, datetime.date.today().toordinal(), UNAIRED]
        query = "SELECT * FROM tv_episodes WHERE showid = ? AND airdate >= ? AND airdate <= (" + innerQuery + ") and status = ?"
        params = [self.tvdbid, datetime.date.today().toordinal()] + innerParams + [UNAIRED]
        sqlResults = myDB.select(query, params)

        if sqlResults == None or len(sqlResults) == 0:
            logger.log(str(self.tvdbid) + u": No episode found... need to implement tvrage and also show status", logger.DEBUG)
            return []
        else:
            logger.log(str(self.tvdbid) + u": Found episode " + str(sqlResults[0]["season"]) + "x" + str(sqlResults[0]["episode"]), logger.DEBUG)
            foundEps = []
            for sqlEp in sqlResults:
                curEp = self.getEpisode(int(sqlEp["season"]), int(sqlEp["episode"]))
                foundEps.append(curEp)
            return foundEps

        # if we didn't get an episode then try getting one from tvrage

        # load tvrage info

        # extract NextEpisode info

        # verify that we don't have it in the DB somehow (ep mismatch)

    def deleteShow(self):

        myDB = db.DBConnection()
        myDB.action("DELETE FROM tv_episodes WHERE showid = ?", [self.tvdbid])
        myDB.action("DELETE FROM tv_shows WHERE tvdb_id = ?", [self.tvdbid])

        # remove self from show list
        sickbeard.showList = [x for x in sickbeard.showList if x.tvdbid != self.tvdbid]

        # clear the cache
        image_cache_dir = ek.ek(os.path.join, sickbeard.CACHE_DIR, 'images')
        for cache_file in ek.ek(glob.glob, ek.ek(os.path.join, image_cache_dir, str(self.tvdbid) + '.*')):
            logger.log(u"Deleting cache file " + cache_file)
            os.remove(cache_file)

    def populateCache(self):
        cache_inst = image_cache.ImageCache()

        logger.log(u"Checking & filling cache for show " + self.name)
        cache_inst.fill_cache(self)

    def refreshDir(self):

        # make sure the show dir is where we think it is unless dirs are created on the fly
        if not ek.ek(os.path.isdir, self._location) and not sickbeard.CREATE_MISSING_SHOW_DIRS:
            return False

        # load from dir
        self.loadEpisodesFromDir()

        # run through all locations from DB, check that they exist
        logger.log(str(self.tvdbid) + u": Loading all episodes with a location from the database")

        myDB = db.DBConnection()
        sqlResults = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND location != ''", [self.tvdbid])

        for ep in sqlResults:
            curLoc = os.path.normpath(ep["location"])
            season = int(ep["season"])
            episode = int(ep["episode"])

            try:
                curEp = self.getEpisode(season, episode)
            except exceptions.EpisodeDeletedException:
                logger.log(u"The episode was deleted while we were refreshing it, moving on to the next one", logger.DEBUG)
                continue

            # if the path doesn't exist or if it's not in our show dir
            if not ek.ek(os.path.isfile, curLoc) or not os.path.normpath(curLoc).startswith(os.path.normpath(self.location)):

                with curEp.lock:
                    # if it used to have a file associated with it and it doesn't anymore then set it to IGNORED
                    if curEp.location and curEp.status in Quality.DOWNLOADED:
                        logger.log(str(self.tvdbid) + u": Location for " + str(season) + "x" + str(episode) + " doesn't exist, removing it and changing our status to IGNORED", logger.DEBUG)
                        curEp.status = IGNORED
                    curEp.location = ''
                    curEp.hasnfo = False
                    curEp.hastbn = False
                    curEp.release_name = ''
                    curEp.saveToDB()

    def saveToDB(self):

        logger.log(str(self.tvdbid) + u": Saving show info to database", logger.DEBUG)

        myDB = db.DBConnection()

        controlValueDict = {"tvdb_id": self.tvdbid}
        newValueDict = {"show_name": self.name,
                        "tvr_id": self.tvrid,
                        "location": self._location,
                        "network": self.network,
                        "genre": self.genre,
                        "runtime": self.runtime,
                        "quality": self.quality,
                        "airs": self.airs,
                        "status": self.status,
                        "flatten_folders": self.flatten_folders,
                        "paused": self.paused,
                        "air_by_date": self.air_by_date,
                        "startyear": self.startyear,
                        "tvr_name": self.tvrname,
                        "lang": self.lang,
                        "last_update_tvdb": self.last_update_tvdb,
                        "rls_ignore_words": self.rls_ignore_words,
                        "rls_require_words": self.rls_require_words
                        }

        myDB.upsert("tv_shows", newValueDict, controlValueDict)

    def __str__(self):
        toReturn = ""
        toReturn += "name: " + self.name + "\n"
        toReturn += "location: " + self._location + "\n"
        toReturn += "tvdbid: " + str(self.tvdbid) + "\n"
        if self.network != None:
            toReturn += "network: " + self.network + "\n"
        if self.airs != None:
            toReturn += "airs: " + self.airs + "\n"
        if self.status != None:
            toReturn += "status: " + self.status + "\n"
        toReturn += "startyear: " + str(self.startyear) + "\n"
        toReturn += "genre: " + self.genre + "\n"
        toReturn += "runtime: " + str(self.runtime) + "\n"
        toReturn += "quality: " + str(self.quality) + "\n"
        return toReturn

    def wantEpisode(self, season, episode, quality, manualSearch=False):

        logger.log(u"Checking if found episode " + str(season) + "x" + str(episode) + " is wanted at quality " + Quality.qualityStrings[quality], logger.DEBUG)

        # if the quality isn't one we want under any circumstances then just say no
        anyQualities, bestQualities = Quality.splitQuality(self.quality)
        logger.log(u"any,best = " + str(anyQualities) + " " + str(bestQualities) + " and found " + str(quality), logger.DEBUG)

        if quality not in anyQualities + bestQualities:
            logger.log(u"Don't want this quality, ignoring found episode", logger.DEBUG)
            return False

        myDB = db.DBConnection()
        sqlResults = myDB.select("SELECT status FROM tv_episodes WHERE showid = ? AND season = ? AND episode = ?", [self.tvdbid, season, episode])

        if not sqlResults or not len(sqlResults):
            logger.log(u"Unable to find a matching episode in database, ignoring found episode", logger.DEBUG)
            return False

        epStatus = int(sqlResults[0]["status"])
        epStatus_text = statusStrings[epStatus]

        logger.log(u"Existing episode status: " + str(epStatus) + " (" + epStatus_text + ")", logger.DEBUG)

        # if we know we don't want it then just say no
        if epStatus in (SKIPPED, IGNORED, ARCHIVED) and not manualSearch:
            logger.log(u"Existing episode status is skipped/ignored/archived, ignoring found episode", logger.DEBUG)
            return False

        # if it's one of these then we want it as long as it's in our allowed initial qualities
        if quality in anyQualities + bestQualities:
            if epStatus in (WANTED, UNAIRED, SKIPPED):
                logger.log(u"Existing episode status is wanted/unaired/skipped, getting found episode", logger.DEBUG)
                return True
            elif manualSearch:
                logger.log(u"Usually ignoring found episode, but forced search allows the quality, getting found episode", logger.DEBUG)
                return True
            else:
                logger.log(u"Quality is on wanted list, need to check if it's better than existing quality", logger.DEBUG)

        curStatus, curQuality = Quality.splitCompositeStatus(epStatus)

        # if we are re-downloading then we only want it if it's in our bestQualities list and better than what we have
        if curStatus in Quality.DOWNLOADED + Quality.SNATCHED + Quality.SNATCHED_PROPER and quality in bestQualities and quality > curQuality:
            logger.log(u"Episode already exists but the found episode has better quality, getting found episode", logger.DEBUG)
            return True
        else:
            logger.log(u"Episode already exists and the found episode has same/lower quality, ignoring found episode", logger.DEBUG)

        logger.log(u"None of the conditions were met, ignoring found episode", logger.DEBUG)
        return False

    def getOverview(self, epStatus):

        if epStatus == WANTED:
            return Overview.WANTED
        elif epStatus in (UNAIRED, UNKNOWN):
            return Overview.UNAIRED
        elif epStatus in (SKIPPED, IGNORED):
            return Overview.SKIPPED
        elif epStatus == ARCHIVED:
            return Overview.GOOD
        elif epStatus in Quality.DOWNLOADED + Quality.SNATCHED + Quality.SNATCHED_PROPER:

            anyQualities, bestQualities = Quality.splitQuality(self.quality)  # @UnusedVariable
            if bestQualities:
                maxBestQuality = max(bestQualities)
            else:
                maxBestQuality = None

            epStatus, curQuality = Quality.splitCompositeStatus(epStatus)

            if epStatus in (SNATCHED, SNATCHED_PROPER):
                return Overview.SNATCHED
            # if they don't want re-downloads then we call it good if they have anything
            elif maxBestQuality == None:
                return Overview.GOOD
            # if they have one but it's not the best they want then mark it as qual
            elif curQuality < maxBestQuality:
                return Overview.QUAL
            # if it's >= maxBestQuality then it's good
            else:
                return Overview.GOOD


def dirty_setter(attr_name):
    def wrapper(self, val):
        if getattr(self, attr_name) != val:
            setattr(self, attr_name, val)
            self.dirty = True
    return wrapper


class TVEpisode(object):

    def __init__(self, show, season, episode, file=""):

        self._name = ""
        self._season = season
        self._episode = episode
        self._description = ""
        self._airdate = datetime.date.fromordinal(1)
        self._hasnfo = False
        self._hastbn = False
        self._status = UNKNOWN
        self._tvdbid = 0
        self._file_size = 0
        self._release_name = ''

        # setting any of the above sets the dirty flag
        self.dirty = True

        self.show = show
        self._location = file

        self.lock = threading.Lock()

        self.specifyEpisode(self.season, self.episode)

        self.relatedEps = []

        self.checkForMetaFiles()

    name = property(lambda self: self._name, dirty_setter("_name"))
    season = property(lambda self: self._season, dirty_setter("_season"))
    episode = property(lambda self: self._episode, dirty_setter("_episode"))
    description = property(lambda self: self._description, dirty_setter("_description"))
    airdate = property(lambda self: self._airdate, dirty_setter("_airdate"))
    hasnfo = property(lambda self: self._hasnfo, dirty_setter("_hasnfo"))
    hastbn = property(lambda self: self._hastbn, dirty_setter("_hastbn"))
    status = property(lambda self: self._status, dirty_setter("_status"))
    tvdbid = property(lambda self: self._tvdbid, dirty_setter("_tvdbid"))
    #location = property(lambda self: self._location, dirty_setter("_location"))
    file_size = property(lambda self: self._file_size, dirty_setter("_file_size"))
    release_name = property(lambda self: self._release_name, dirty_setter("_release_name"))

    def _set_location(self, new_location):
        logger.log(u"Setter sets location to " + new_location, logger.DEBUG)

        #self._location = newLocation
        dirty_setter("_location")(self, new_location)

        if new_location and ek.ek(os.path.isfile, new_location):
            self.file_size = ek.ek(os.path.getsize, new_location)
        else:
            self.file_size = 0

    location = property(lambda self: self._location, _set_location)

    def checkForMetaFiles(self):

        oldhasnfo = self.hasnfo
        oldhastbn = self.hastbn

        cur_nfo = False
        cur_tbn = False

        # check for nfo and tbn
        if ek.ek(os.path.isfile, self.location):
            for cur_provider in sickbeard.metadata_provider_dict.values():
                if cur_provider.episode_metadata:
                    new_result = cur_provider._has_episode_metadata(self)
                else:
                    new_result = False
                cur_nfo = new_result or cur_nfo

                if cur_provider.episode_thumbnails:
                    new_result = cur_provider._has_episode_thumb(self)
                else:
                    new_result = False
                cur_tbn = new_result or cur_tbn

        self.hasnfo = cur_nfo
        self.hastbn = cur_tbn

        # if either setting has changed return true, if not return false
        return oldhasnfo != self.hasnfo or oldhastbn != self.hastbn

    def specifyEpisode(self, season, episode):

        sqlResult = self.loadFromDB(season, episode)

        if not sqlResult:
            # only load from NFO if we didn't load from DB
            if ek.ek(os.path.isfile, self.location):
                try:
                    self.loadFromNFO(self.location)
                except exceptions.NoNFOException:
                    logger.log(str(self.show.tvdbid) + u": There was an error loading the NFO for episode " + str(season) + "x" + str(episode), logger.ERROR)
                    pass

                # if we tried loading it from NFO and didn't find the NFO, use TVDB
                if self.hasnfo == False:
                    try:
                        result = self.loadFromTVDB(season, episode)
                    except exceptions.EpisodeDeletedException:
                        result = False

                    # if we failed SQL *and* NFO, TVDB then fail
                    if result == False:
                        raise exceptions.EpisodeNotFoundException("Couldn't find episode " + str(season) + "x" + str(episode))

        # don't update if not needed
        if self.dirty:
            self.saveToDB()

    def loadFromDB(self, season, episode):

        logger.log(str(self.show.tvdbid) + u": Loading episode details from DB for episode " + str(season) + "x" + str(episode), logger.DEBUG)

        myDB = db.DBConnection()
        sqlResults = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND season = ? AND episode = ?", [self.show.tvdbid, season, episode])

        if len(sqlResults) > 1:
            raise exceptions.MultipleDBEpisodesException("Your DB has two records for the same show somehow.")
        elif len(sqlResults) == 0:
            logger.log(str(self.show.tvdbid) + u": Episode " + str(self.season) + "x" + str(self.episode) + " not found in the database", logger.DEBUG)
            return False
        else:
            #NAMEIT logger.log(u"AAAAA from" + str(self.season)+"x"+str(self.episode) + " -" + self.name + " to " + str(sqlResults[0]["name"]))
            if sqlResults[0]["name"] != None:
                self.name = sqlResults[0]["name"]
            self.season = season
            self.episode = episode
            self.description = sqlResults[0]["description"]
            if self.description == None:
                self.description = ""
            self.airdate = datetime.date.fromordinal(int(sqlResults[0]["airdate"]))
            #logger.log(u"1 Status changes from " + str(self.status) + " to " + str(sqlResults[0]["status"]), logger.DEBUG)
            self.status = int(sqlResults[0]["status"])

            # don't overwrite my location
            if sqlResults[0]["location"] != "" and sqlResults[0]["location"] != None:
                self.location = os.path.normpath(sqlResults[0]["location"])
            if sqlResults[0]["file_size"]:
                self.file_size = int(sqlResults[0]["file_size"])
            else:
                self.file_size = 0

            self.tvdbid = int(sqlResults[0]["tvdbid"])

            if sqlResults[0]["release_name"] != None:
                self.release_name = sqlResults[0]["release_name"]

            self.dirty = False
            return True

    def loadFromTVDB(self, season=None, episode=None, cache=True, tvapi=None, cachedSeason=None):

        if season == None:
            season = self.season
        if episode == None:
            episode = self.episode

        logger.log(str(self.show.tvdbid) + u": Loading episode details from theTVDB for episode " + str(season) + "x" + str(episode), logger.DEBUG)

        tvdb_lang = self.show.lang

        try:
            if cachedSeason is None:
                if tvapi is None:
                    # There's gotta be a better way of doing this but we don't wanna
                    # change the cache value elsewhere
                    ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

                    if not cache:
                        ltvdb_api_parms['cache'] = False

                    if tvdb_lang:
                            ltvdb_api_parms['language'] = tvdb_lang

                    t = tvdb_api.Tvdb(**ltvdb_api_parms)
                else:
                    t = tvapi
                myEp = t[self.show.tvdbid][season][episode]
            else:
                myEp = cachedSeason[episode]

        except (tvdb_exceptions.tvdb_error, IOError), e:
            logger.log(u"TVDB threw up an error: " + ex(e), logger.DEBUG)
            # if the episode is already valid just log it, if not throw it up
            if self.name:
                logger.log(u"TVDB timed out but we have enough info from other sources, allowing the error", logger.DEBUG)
                return
            else:
                logger.log(u"TVDB timed out, unable to create the episode", logger.ERROR)
                return False
        except (tvdb_exceptions.tvdb_episodenotfound, tvdb_exceptions.tvdb_seasonnotfound):
            logger.log(u"Unable to find the episode on tvdb... has it been removed? Should I delete from db?", logger.DEBUG)
            # if I'm no longer on TVDB but I once was then delete myself from the DB
            if self.tvdbid != -1:
                self.deleteEpisode()
            return

        if not myEp["firstaired"] or myEp["firstaired"] == "0000-00-00":
            myEp["firstaired"] = str(datetime.date.fromordinal(1))

        if myEp["episodename"] == None or myEp["episodename"] == "":
            logger.log(u"This episode (" + self.show.name + " - " + str(season) + "x" + str(episode) + ") has no name on TVDB")
            # if I'm incomplete on TVDB but I once was complete then just delete myself from the DB for now
            if self.tvdbid != -1:
                self.deleteEpisode()
            return False

        #NAMEIT logger.log(u"BBBBBBBB from " + str(self.season)+"x"+str(self.episode) + " -" +self.name+" to "+myEp["episodename"])
        self.name = myEp["episodename"]
        self.season = season
        self.episode = episode
        tmp_description = myEp["overview"]
        if tmp_description == None:
            self.description = ""
        else:
            self.description = tmp_description
        rawAirdate = [int(x) for x in myEp["firstaired"].split("-")]
        try:
            self.airdate = datetime.date(rawAirdate[0], rawAirdate[1], rawAirdate[2])
        except ValueError:
            logger.log(u"Malformed air date retrieved from TVDB (" + self.show.name + " - " + str(season) + "x" + str(episode) + ")", logger.ERROR)
            # if I'm incomplete on TVDB but I once was complete then just delete myself from the DB for now
            if self.tvdbid != -1:
                self.deleteEpisode()
            return False

        #early conversion to int so that episode doesn't get marked dirty
        self.tvdbid = int(myEp["id"])

        #don't update show status if show dir is missing, unless missing show dirs are created during post-processing
        if not ek.ek(os.path.isdir, self.show._location) and not sickbeard.CREATE_MISSING_SHOW_DIRS:
            logger.log(u"The show dir is missing, not bothering to change the episode statuses since it'd probably be invalid")
            return

        logger.log(str(self.show.tvdbid) + u": Setting status for " + str(season) + "x" + str(episode) + " based on status " + str(self.status) + " and existence of " + self.location, logger.DEBUG)

        if not ek.ek(os.path.isfile, self.location):

            # if we don't have the file
            if self.airdate >= datetime.date.today() and self.status not in Quality.SNATCHED + Quality.SNATCHED_PROPER:
                # and it hasn't aired yet set the status to UNAIRED
                logger.log(u"Episode airs in the future, changing status from " + str(self.status) + " to " + str(UNAIRED), logger.DEBUG)
                self.status = UNAIRED
            # if there's no airdate then set it to skipped (and respect ignored)
            elif self.airdate == datetime.date.fromordinal(1):
                if self.status == IGNORED:
                    logger.log(u"Episode has no air date, but it's already marked as ignored", logger.DEBUG)
                else:
                    logger.log(u"Episode has no air date, automatically marking it skipped", logger.DEBUG)
                    self.status = SKIPPED
            # if we don't have the file and the airdate is in the past
            else:
                if self.status == UNAIRED:
                    self.status = WANTED

                # if we somehow are still UNKNOWN then just skip it
                elif self.status == UNKNOWN:
                    self.status = SKIPPED

                else:
                    logger.log(u"Not touching status because we have no ep file, the airdate is in the past, and the status is " + str(self.status), logger.DEBUG)

        # if we have a media file then it's downloaded
        elif sickbeard.helpers.isMediaFile(self.location):
            # leave propers alone, you have to either post-process them or manually change them back
            if self.status not in Quality.SNATCHED_PROPER + Quality.DOWNLOADED + Quality.SNATCHED + [ARCHIVED]:
                logger.log(u"5 Status changes from " + str(self.status) + " to " + str(Quality.statusFromName(self.location)), logger.DEBUG)
                self.status = Quality.statusFromName(self.location)

        # shouldn't get here probably
        else:
            logger.log(u"6 Status changes from " + str(self.status) + " to " + str(UNKNOWN), logger.DEBUG)
            self.status = UNKNOWN

        # hasnfo, hastbn, status?

    def loadFromNFO(self, location):

        if not ek.ek(os.path.isdir, self.show._location):
            logger.log(str(self.show.tvdbid) + u": The show dir is missing, not bothering to try loading the episode NFO")
            return

        logger.log(str(self.show.tvdbid) + u": Loading episode details from the NFO file associated with " + location, logger.DEBUG)

        self.location = location

        if self.location != "":

            if self.status == UNKNOWN:
                if sickbeard.helpers.isMediaFile(self.location):
                    logger.log(u"7 Status changes from " + str(self.status) + " to " + str(Quality.statusFromName(self.location)), logger.DEBUG)
                    self.status = Quality.statusFromName(self.location)

            nfoFile = sickbeard.helpers.replaceExtension(self.location, "nfo")
            logger.log(str(self.show.tvdbid) + u": Using NFO name " + nfoFile, logger.DEBUG)

            if ek.ek(os.path.isfile, nfoFile):
                try:
                    showXML = etree.ElementTree(file=nfoFile)
                except (SyntaxError, ValueError), e:
                    logger.log(u"Error loading the NFO, backing up the NFO and skipping for now: " + ex(e), logger.ERROR)  # TODO: figure out what's wrong and fix it
                    try:
                        ek.ek(os.rename, nfoFile, nfoFile + ".old")
                    except Exception, e:
                        logger.log(u"Failed to rename your episode's NFO file - you need to delete it or fix it: " + ex(e), logger.ERROR)
                    raise exceptions.NoNFOException("Error in NFO format")

                for epDetails in showXML.getiterator('episodedetails'):
                    if epDetails.findtext('season') == None or int(epDetails.findtext('season')) != self.season or \
                       epDetails.findtext('episode') == None or int(epDetails.findtext('episode')) != self.episode:
                        logger.log(str(self.show.tvdbid) + u": NFO has an <episodedetails> block for a different episode - wanted " + str(self.season) + "x" + str(self.episode) + " but got " + str(epDetails.findtext('season')) + "x" + str(epDetails.findtext('episode')), logger.DEBUG)
                        continue

                    if epDetails.findtext('title') == None or epDetails.findtext('aired') == None:
                        raise exceptions.NoNFOException("Error in NFO format (missing episode title or airdate)")

                    self.name = epDetails.findtext('title')
                    self.episode = int(epDetails.findtext('episode'))
                    self.season = int(epDetails.findtext('season'))

                    self.description = epDetails.findtext('plot')
                    if self.description == None:
                        self.description = ""

                    if epDetails.findtext('aired'):
                        rawAirdate = [int(x) for x in epDetails.findtext('aired').split("-")]
                        self.airdate = datetime.date(rawAirdate[0], rawAirdate[1], rawAirdate[2])
                    else:
                        self.airdate = datetime.date.fromordinal(1)

                    self.hasnfo = True
            else:
                self.hasnfo = False

            if ek.ek(os.path.isfile, sickbeard.helpers.replaceExtension(nfoFile, "tbn")):
                self.hastbn = True
            else:
                self.hastbn = False

    def __str__(self):

        toReturn = ""
        toReturn += str(self.show.name) + " - " + str(self.season) + "x" + str(self.episode) + " - " + str(self.name) + "\n"
        toReturn += "location: " + str(self.location) + "\n"
        toReturn += "description: " + str(self.description) + "\n"
        toReturn += "airdate: " + str(self.airdate.toordinal()) + " (" + str(self.airdate) + ")\n"
        toReturn += "hasnfo: " + str(self.hasnfo) + "\n"
        toReturn += "hastbn: " + str(self.hastbn) + "\n"
        toReturn += "status: " + str(self.status) + "\n"
        return toReturn

    def createMetaFiles(self, force=False):

        if not ek.ek(os.path.isdir, self.show._location):
            logger.log(str(self.show.tvdbid) + u": The show dir is missing, not bothering to try to create metadata")
            return

        self.createNFO(force)
        self.createThumbnail(force)

        if self.checkForMetaFiles():
            self.saveToDB()

    def createNFO(self, force=False):

        result = False

        for cur_provider in sickbeard.metadata_provider_dict.values():
            result = cur_provider.create_episode_metadata(self) or result

        return result

    def createThumbnail(self, force=False):

        result = False

        for cur_provider in sickbeard.metadata_provider_dict.values():
            result = cur_provider.create_episode_thumb(self) or result

        return result

    def deleteEpisode(self):

        logger.log(u"Deleting " + self.show.name + " " + str(self.season) + "x" + str(self.episode) + " from the DB", logger.DEBUG)

        # remove myself from the show dictionary
        if self.show.getEpisode(self.season, self.episode, noCreate=True) == self:
            logger.log(u"Removing myself from my show's list", logger.DEBUG)
            del self.show.episodes[self.season][self.episode]

        # delete myself from the DB
        logger.log(u"Deleting myself from the database", logger.DEBUG)
        myDB = db.DBConnection()
        sql = "DELETE FROM tv_episodes WHERE showid=" + str(self.show.tvdbid) + " AND season=" + str(self.season) + " AND episode=" + str(self.episode)
        myDB.action(sql)

        raise exceptions.EpisodeDeletedException()

    def saveToDB(self, forceSave=False):
        """
        Saves this episode to the database if any of its data has been changed since the last save.

        forceSave: If True it will save to the database even if no data has been changed since the
                    last save (aka if the record is not dirty).
        """

        if not self.dirty and not forceSave:
            logger.log(str(self.show.tvdbid) + u": Not saving episode to db - record is not dirty", logger.DEBUG)
            return

        logger.log(str(self.show.tvdbid) + u": Saving episode details to database", logger.DEBUG)

        logger.log(u"STATUS IS " + str(self.status), logger.DEBUG)

        myDB = db.DBConnection()
        newValueDict = {"tvdbid": self.tvdbid,
                        "name": self.name,
                        "description": self.description,
                        "airdate": self.airdate.toordinal(),
                        "hasnfo": self.hasnfo,
                        "hastbn": self.hastbn,
                        "status": self.status,
                        "location": self.location,
                        "file_size": self.file_size,
                        "release_name": self.release_name}
        controlValueDict = {"showid": self.show.tvdbid,
                            "season": self.season,
                            "episode": self.episode}

        # use a custom update/insert method to get the data into the DB
        myDB.upsert("tv_episodes", newValueDict, controlValueDict)

    def fullPath(self):
        if self.location == None or self.location == "":
            return None
        else:
            return ek.ek(os.path.join, self.show.location, self.location)

    def prettyName(self):
        """
        Returns the name of this episode in a "pretty" human-readable format. Used for logging
        and notifications and such.

        Returns: A string representing the episode's name and season/ep numbers
        """

        return self._format_pattern('%SN - %Sx%0E - %EN')

    def _ep_name(self):
        """
        Returns the name of the episode to use during renaming. Combines the names of related episodes.
        Eg. "Ep Name (1)" and "Ep Name (2)" becomes "Ep Name"
            "Ep Name" and "Other Ep Name" becomes "Ep Name & Other Ep Name"
        """

        multiNameRegex = "(.*) \(\d{1,2}\)"

        self.relatedEps = sorted(self.relatedEps, key=lambda x: x.episode)

        if len(self.relatedEps) == 0:
            goodName = self.name

        else:
            goodName = ''

            singleName = True
            curGoodName = None

            for curName in [self.name] + [x.name for x in self.relatedEps]:
                match = re.match(multiNameRegex, curName)
                if not match:
                    singleName = False
                    break

                if curGoodName == None:
                    curGoodName = match.group(1)
                elif curGoodName != match.group(1):
                    singleName = False
                    break

            if singleName:
                goodName = curGoodName
            else:
                goodName = self.name
                for relEp in self.relatedEps:
                    goodName += " & " + relEp.name

        return goodName

    def _replace_map(self):
        """
        Generates a replacement map for this episode which maps all possible custom naming patterns to the correct
        value for this episode.

        Returns: A dict with patterns as the keys and their replacement values as the values.
        """

        ep_name = self._ep_name()

        def dot(name):
            return helpers.sanitizeSceneName(name)

        def us(name):
            return re.sub('[ -]', '_', name)

        def release_name(name):
            if name:
                name = helpers.remove_non_release_groups(helpers.remove_extension(name))
            return name

        def release_group(name):

            if name:
                name = helpers.remove_non_release_groups(helpers.remove_extension(name))
            else:
                return ""

            np = NameParser(False)

            try:
                parse_result = np.parse(name)
            except InvalidNameException, e:
                logger.log(u"Unable to get parse release_group: " + ex(e), logger.DEBUG)
                return ""

            if not parse_result.release_group:
                return ""

            return parse_result.release_group

        epStatus, epQual = Quality.splitCompositeStatus(self.status)  # @UnusedVariable

        return {
                   '%SN': self.show.name,
                   '%S.N': dot(self.show.name),
                   '%S_N': us(self.show.name),
                   '%EN': ep_name,
                   '%E.N': dot(ep_name),
                   '%E_N': us(ep_name),
                   '%QN': Quality.qualityStrings[epQual],
                   '%Q.N': dot(Quality.qualityStrings[epQual]),
                   '%Q_N': us(Quality.qualityStrings[epQual]),
                   '%S': str(self.season),
                   '%0S': '%02d' % self.season,
                   '%E': str(self.episode),
                   '%0E': '%02d' % self.episode,
                   '%RN': release_name(self.release_name),
                   '%RG': release_group(self.release_name),
                   '%AD': str(self.airdate).replace('-', ' '),
                   '%A.D': str(self.airdate).replace('-', '.'),
                   '%A_D': us(str(self.airdate)),
                   '%A-D': str(self.airdate),
                   '%Y': str(self.airdate.year),
                   '%M': str(self.airdate.month),
                   '%D': str(self.airdate.day),
                   '%0M': '%02d' % self.airdate.month,
                   '%0D': '%02d' % self.airdate.day,
                   }

    def _format_string(self, pattern, replace_map):
        """
        Replaces all template strings with the correct value
        """

        result_name = pattern

        # do the replacements
        for cur_replacement in sorted(replace_map.keys(), reverse=True):
            result_name = result_name.replace(cur_replacement, helpers.sanitizeFileName(replace_map[cur_replacement]))
            result_name = result_name.replace(cur_replacement.lower(), helpers.sanitizeFileName(replace_map[cur_replacement].lower()))

        return result_name

    def _format_pattern(self, pattern=None, multi=None):
        """
        Manipulates an episode naming pattern and then fills the template in
        """

        if pattern == None:
            pattern = sickbeard.NAMING_PATTERN

        if multi == None:
            multi = sickbeard.NAMING_MULTI_EP

        replace_map = self._replace_map()

        result_name = pattern

        # if there's no release group then replace it with a reasonable facsimile
        if not replace_map['%RN']:
            if self.show.air_by_date:
                result_name = result_name.replace('%RN', '%S.N.%A.D.%E.N-SiCKBEARD')
                result_name = result_name.replace('%rn', '%s.n.%A.D.%e.n-sickbeard')

            else:
                result_name = result_name.replace('%RN', '%S.N.S%0SE%0E.%E.N-SiCKBEARD')
                result_name = result_name.replace('%rn', '%s.n.s%0se%0e.%e.n-sickbeard')

            logger.log(u"Episode has no release name, replacing it with a generic one: " + result_name, logger.DEBUG)

        if not replace_map['%RG']:
            result_name = result_name.replace('%RG', 'SiCKBEARD')
            result_name = result_name.replace('%rg', 'sickbeard')
            logger.log(u"Episode has no release group, replacing it with a generic one: " + result_name, logger.DEBUG)

        # split off ep name part only
        name_groups = re.split(r'[\\/]', result_name)

        # figure out the double-ep numbering style for each group, if applicable
        for cur_name_group in name_groups:

            season_format = sep = ep_sep = ep_format = None

            season_ep_regex = '''
                                (?P<pre_sep>[ _.-]*)
                                ((?:s(?:eason|eries)?\s*)?%0?S(?![._]?N))
                                (.*?)
                                (%0?E(?![._]?N))
                                (?P<post_sep>[ _.-]*)
                              '''
            ep_only_regex = '(E?%0?E(?![._]?N))'

            # try the normal way
            season_ep_match = re.search(season_ep_regex, cur_name_group, re.I | re.X)
            ep_only_match = re.search(ep_only_regex, cur_name_group, re.I | re.X)

            # if we have a season and episode then collect the necessary data
            if season_ep_match:
                season_format = season_ep_match.group(2)
                ep_sep = season_ep_match.group(3)
                ep_format = season_ep_match.group(4)
                sep = season_ep_match.group('pre_sep')
                if not sep:
                    sep = season_ep_match.group('post_sep')
                if not sep:
                    sep = ' '

                # force 2-3-4 format if they chose to extend
                if multi in (NAMING_EXTEND, NAMING_LIMITED_EXTEND, NAMING_LIMITED_EXTEND_E_PREFIXED):
                    ep_sep = '-'

                regex_used = season_ep_regex

            # if there's no season then there's not much choice so we'll just force them to use 03-04-05 style
            elif ep_only_match:
                season_format = ''
                ep_sep = '-'
                ep_format = ep_only_match.group(1)
                sep = ''
                regex_used = ep_only_regex

            else:
                continue

            # we need at least this much info to continue
            if not ep_sep or not ep_format:
                continue

            # start with the ep string, eg. E03
            ep_string = self._format_string(ep_format.upper(), replace_map)
            for other_ep in self.relatedEps:

                # for limited extend we only append the last ep
                if multi in (NAMING_LIMITED_EXTEND, NAMING_LIMITED_EXTEND_E_PREFIXED) and other_ep != self.relatedEps[-1]:
                    continue

                elif multi == NAMING_DUPLICATE:
                    # add " - S01"
                    ep_string += sep + season_format

                elif multi == NAMING_SEPARATED_REPEAT:
                    ep_string += sep

                # add "E04"
                ep_string += ep_sep

                if multi == NAMING_LIMITED_EXTEND_E_PREFIXED:
                    ep_string += 'E'

                ep_string += other_ep._format_string(ep_format.upper(), other_ep._replace_map())

            if season_ep_match:
                regex_replacement = r'\g<pre_sep>\g<2>\g<3>' + ep_string + r'\g<post_sep>'
            elif ep_only_match:
                regex_replacement = ep_string

            # fill out the template for this piece and then insert this piece into the actual pattern
            cur_name_group_result = re.sub('(?i)(?x)' + regex_used, regex_replacement, cur_name_group)
            #cur_name_group_result = cur_name_group.replace(ep_format, ep_string)
            #logger.log(u"found "+ep_format+" as the ep pattern using "+regex_used+" and replaced it with "+regex_replacement+" to result in "+cur_name_group_result+" from "+cur_name_group, logger.DEBUG)
            result_name = result_name.replace(cur_name_group, cur_name_group_result)

        result_name = self._format_string(result_name, replace_map)

        logger.log(u"formatting pattern: " + pattern + " -> " + result_name, logger.DEBUG)

        return result_name

    def proper_path(self):
        """
        Figures out the path where this episode SHOULD live according to the renaming rules, relative from the show dir
        """

        result = self.formatted_filename()

        # if they want us to flatten it and we're allowed to flatten it then we will
        if self.show.flatten_folders and not sickbeard.NAMING_FORCE_FOLDERS:
            return result

        # if not we append the folder on and use that
        else:
            result = ek.ek(os.path.join, self.formatted_dir(), result)

        return result

    def formatted_dir(self, pattern=None, multi=None):
        """
        Just the folder name of the episode
        """

        if pattern == None:
            # we only use ABD if it's enabled, this is an ABD show, AND this is not a multi-ep
            if self.show.air_by_date and sickbeard.NAMING_CUSTOM_ABD and not self.relatedEps:
                pattern = sickbeard.NAMING_ABD_PATTERN
            else:
                pattern = sickbeard.NAMING_PATTERN

        # split off the dirs only, if they exist
        name_groups = re.split(r'[\\/]', pattern)

        if len(name_groups) == 1:
            return ''
        else:
            return self._format_pattern(os.sep.join(name_groups[:-1]), multi)

    def formatted_filename(self, pattern=None, multi=None):
        """
        Just the filename of the episode, formatted based on the naming settings
        """

        if pattern == None:
            # we only use ABD if it's enabled, this is an ABD show, AND this is not a multi-ep
            if self.show.air_by_date and sickbeard.NAMING_CUSTOM_ABD and not self.relatedEps:
                pattern = sickbeard.NAMING_ABD_PATTERN
            else:
                pattern = sickbeard.NAMING_PATTERN

        # split off the filename only, if they exist
        name_groups = re.split(r'[\\/]', pattern)

        return self._format_pattern(name_groups[-1], multi)

    def rename(self):
        """
        Renames an episode file and all related files to the location and filename as specified
        in the naming settings.
        """

        if not ek.ek(os.path.isfile, self.location):
            logger.log(u"Can't perform rename on " + self.location + " when it doesn't exist, skipping", logger.WARNING)
            return

        proper_path = self.proper_path()
        absolute_proper_path = ek.ek(os.path.join, self.show.location, proper_path)
        absolute_current_path_no_ext, file_ext = ek.ek(os.path.splitext, self.location)
        absolute_current_path_no_ext_length = len(absolute_current_path_no_ext)

        current_path = absolute_current_path_no_ext

        if absolute_current_path_no_ext.startswith(self.show.location):
            current_path = absolute_current_path_no_ext[len(self.show.location):]

        logger.log(u"Renaming/moving episode from the base path " + self.location + " to " + absolute_proper_path, logger.DEBUG)

        # if it's already named correctly then don't do anything
        if proper_path == current_path:
            logger.log(str(self.tvdbid) + u": File " + self.location + " is already named correctly, skipping", logger.DEBUG)
            return

        related_files = postProcessor.PostProcessor(self.location).list_associated_files(self.location, base_name_only=True)
        logger.log(u"Files associated to " + self.location + ": " + str(related_files), logger.DEBUG)

        # move the ep file
        result = helpers.rename_ep_file(self.location, absolute_proper_path, absolute_current_path_no_ext_length)

        # move related files
        for cur_related_file in related_files:
            cur_result = helpers.rename_ep_file(cur_related_file, absolute_proper_path, absolute_current_path_no_ext_length)
            if cur_result == False:
                logger.log(str(self.tvdbid) + u": Unable to rename file " + cur_related_file, logger.ERROR)

        # save the ep
        with self.lock:
            if result != False:
                self.location = absolute_proper_path + file_ext
                for relEp in self.relatedEps:
                    relEp.location = absolute_proper_path + file_ext

        # in case something changed with the metadata just do a quick check
        for curEp in [self] + self.relatedEps:
            curEp.checkForMetaFiles()

        # save any changes to the database
        with self.lock:
            self.saveToDB()
            for relEp in self.relatedEps:
                relEp.saveToDB()

########NEW FILE########
__FILENAME__ = tvcache
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import time
import datetime
import sqlite3

import sickbeard

from sickbeard import db
from sickbeard import logger
from sickbeard.common import Quality

from sickbeard import helpers, show_name_helpers
from sickbeard import name_cache
from sickbeard.exceptions import ex, AuthException

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import elementtree.ElementTree as etree

from lib.tvdb_api import tvdb_api, tvdb_exceptions

from name_parser.parser import NameParser, InvalidNameException


class CacheDBConnection(db.DBConnection):

    def __init__(self, providerName):
        db.DBConnection.__init__(self, "cache.db")

        # Create the table if it's not already there
        try:
            sql = "CREATE TABLE [" + providerName + "] (name TEXT, season NUMERIC, episodes TEXT, tvrid NUMERIC, tvdbid NUMERIC, url TEXT, time NUMERIC, quality TEXT);"
            self.connection.execute(sql)
            self.connection.commit()
        except sqlite3.OperationalError, e:
            if str(e) != "table [" + providerName + "] already exists":
                raise

        # Create the table if it's not already there
        try:
            sql = "CREATE TABLE lastUpdate (provider TEXT, time NUMERIC);"
            self.connection.execute(sql)
            self.connection.commit()
        except sqlite3.OperationalError, e:
            if str(e) != "table lastUpdate already exists":
                raise


class TVCache():

    def __init__(self, provider):

        self.provider = provider
        self.providerID = self.provider.getID()
        self.minTime = 10

    def _getDB(self):

        return CacheDBConnection(self.providerID)

    def _clearCache(self):

        myDB = self._getDB()

        myDB.action("DELETE FROM [" + self.providerID + "] WHERE 1")

    def _getRSSData(self):

        data = None

        return data

    def _checkAuth(self, parsedXML):
        return True

    def _checkItemAuth(self, title, url):
        return True

    def updateCache(self):

        if not self.shouldUpdate():
            return

        if self._checkAuth(None):

            data = self._getRSSData()

            # as long as the http request worked we count this as an update
            if data:
                self.setLastUpdate()
            else:
                return []

            # now that we've loaded the current RSS feed lets delete the old cache
            logger.log(u"Clearing " + self.provider.name + " cache and updating with new information")
            self._clearCache()

            parsedXML = helpers.parse_xml(data)

            if parsedXML is None:
                logger.log(u"Error trying to load " + self.provider.name + " RSS feed", logger.ERROR)
                return []

            if self._checkAuth(parsedXML):

                if parsedXML.tag == 'rss':
                    items = parsedXML.findall('.//item')

                else:
                    logger.log(u"Resulting XML from " + self.provider.name + " isn't RSS, not parsing it", logger.ERROR)
                    return []

                for item in items:
                    self._parseItem(item)

            else:
                raise AuthException(u"Your authentication credentials for " + self.provider.name + " are incorrect, check your config")

        return []

    def _translateTitle(self, title):
        return title.replace(' ', '.')

    def _translateLinkURL(self, url):
        return url.replace('&amp;', '&')

    def _parseItem(self, item):

        title = helpers.get_xml_text(item.find('title'))
        url = helpers.get_xml_text(item.find('link'))

        self._checkItemAuth(title, url)

        if title and url:
            title = self._translateTitle(title)
            url = self._translateLinkURL(url)

            logger.log(u"Adding item from RSS to cache: " + title, logger.DEBUG)
            self._addCacheEntry(title, url)

        else:
            logger.log(u"The XML returned from the " + self.provider.name + " feed is incomplete, this result is unusable", logger.DEBUG)
            return

    def _getLastUpdate(self):
        myDB = self._getDB()
        sqlResults = myDB.select("SELECT time FROM lastUpdate WHERE provider = ?", [self.providerID])

        if sqlResults:
            lastTime = int(sqlResults[0]["time"])
            if lastTime > int(time.mktime(datetime.datetime.today().timetuple())):
                lastTime = 0

        else:
            lastTime = 0

        return datetime.datetime.fromtimestamp(lastTime)

    def setLastUpdate(self, toDate=None):

        if not toDate:
            toDate = datetime.datetime.today()

        myDB = self._getDB()
        myDB.upsert("lastUpdate",
                    {'time': int(time.mktime(toDate.timetuple()))},
                    {'provider': self.providerID})

    lastUpdate = property(_getLastUpdate)

    def shouldUpdate(self):
        # if we've updated recently then skip the update
        if datetime.datetime.today() - self.lastUpdate < datetime.timedelta(minutes=self.minTime):
            logger.log(u"Last update was too soon, using old cache: today()-" + str(self.lastUpdate) + "<" + str(datetime.timedelta(minutes=self.minTime)), logger.DEBUG)
            return False

        return True

    def _addCacheEntry(self, name, url, season=None, episodes=None, tvdb_id=0, tvrage_id=0, quality=None, extraNames=[]):

        myDB = self._getDB()

        parse_result = None

        # if we don't have complete info then parse the filename to get it
        for curName in [name] + extraNames:
            try:
                myParser = NameParser()
                parse_result = myParser.parse(curName)
            except InvalidNameException:
                logger.log(u"Unable to parse the filename " + curName + " into a valid episode", logger.DEBUG)
                continue

        if not parse_result:
            logger.log(u"Giving up because I'm unable to parse this name: " + name, logger.DEBUG)
            return False

        if not parse_result.series_name:
            logger.log(u"No series name retrieved from " + name + ", unable to cache it", logger.DEBUG)
            return False

        tvdb_lang = None

        # if we need tvdb_id or tvrage_id then search the DB for them
        if not tvdb_id or not tvrage_id:

            # if we have only the tvdb_id, use the database
            if tvdb_id:
                showObj = helpers.findCertainShow(sickbeard.showList, tvdb_id)
                if showObj:
                    tvrage_id = showObj.tvrid
                    tvdb_lang = showObj.lang
                else:
                    logger.log(u"We were given a TVDB id " + str(tvdb_id) + " but it doesn't match a show we have in our list, so leaving tvrage_id empty", logger.DEBUG)
                    tvrage_id = 0

            # if we have only a tvrage_id then use the database
            elif tvrage_id:
                showObj = helpers.findCertainTVRageShow(sickbeard.showList, tvrage_id)
                if showObj:
                    tvdb_id = showObj.tvdbid
                    tvdb_lang = showObj.lang
                else:
                    logger.log(u"We were given a TVRage id " + str(tvrage_id) + " but it doesn't match a show we have in our list, so leaving tvdb_id empty", logger.DEBUG)
                    tvdb_id = 0

            # if they're both empty then fill out as much info as possible by searching the show name
            else:

                # check the name cache and see if we already know what show this is
                logger.log(u"Checking the cache to see if we already know the tvdb id of " + parse_result.series_name, logger.DEBUG)
                tvdb_id = name_cache.retrieveNameFromCache(parse_result.series_name)

                # remember if the cache lookup worked or not so we know whether we should bother updating it later
                if tvdb_id == None:
                    logger.log(u"No cache results returned, continuing on with the search", logger.DEBUG)
                    from_cache = False
                else:
                    logger.log(u"Cache lookup found " + repr(tvdb_id) + ", using that", logger.DEBUG)
                    from_cache = True

                # if the cache failed, try looking up the show name in the database
                if tvdb_id == None:
                    logger.log(u"Trying to look the show up in the show database", logger.DEBUG)
                    showResult = helpers.searchDBForShow(parse_result.series_name)
                    if showResult:
                        logger.log(u"" + parse_result.series_name + " was found to be show " + showResult[1] + " (" + str(showResult[0]) + ") in our DB.", logger.DEBUG)
                        tvdb_id = showResult[0]

                # if the DB lookup fails then do a comprehensive regex search
                if tvdb_id == None:
                    logger.log(u"Couldn't figure out a show name straight from the DB, trying a regex search instead", logger.DEBUG)
                    for curShow in sickbeard.showList:
                        if show_name_helpers.isGoodResult(name, curShow, False):
                            logger.log(u"Successfully matched " + name + " to " + curShow.name + " with regex", logger.DEBUG)
                            tvdb_id = curShow.tvdbid
                            tvdb_lang = curShow.lang
                            break

                # if tvdb_id was anything but None (0 or a number) then
                if not from_cache:
                    name_cache.addNameToCache(parse_result.series_name, tvdb_id)

                # if we came out with tvdb_id = None it means we couldn't figure it out at all, just use 0 for that
                if tvdb_id == None:
                    tvdb_id = 0

                # if we found the show then retrieve the show object
                if tvdb_id:
                    showObj = helpers.findCertainShow(sickbeard.showList, tvdb_id)
                    if showObj:
                        tvrage_id = showObj.tvrid
                        tvdb_lang = showObj.lang

        # if we weren't provided with season/episode information then get it from the name that we parsed
        if not season:
            season = parse_result.season_number if parse_result.season_number != None else 1
        if not episodes:
            episodes = parse_result.episode_numbers

        # if we have an air-by-date show then get the real season/episode numbers
        if parse_result.air_by_date and tvdb_id:
            try:
                # There's gotta be a better way of doing this but we don't wanna
                # change the language value elsewhere
                ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

                if not (tvdb_lang == "" or tvdb_lang == "en" or tvdb_lang == None):
                    ltvdb_api_parms['language'] = tvdb_lang

                t = tvdb_api.Tvdb(**ltvdb_api_parms)
                epObj = t[tvdb_id].airedOn(parse_result.air_date)[0]
                season = int(epObj["seasonnumber"])
                episodes = [int(epObj["episodenumber"])]
            except tvdb_exceptions.tvdb_episodenotfound:
                logger.log(u"Unable to find episode with date " + str(parse_result.air_date) + " for show " + parse_result.series_name + ", skipping", logger.WARNING)
                return False
            except tvdb_exceptions.tvdb_error, e:
                logger.log(u"Unable to contact TVDB: " + ex(e), logger.WARNING)
                return False

        episodeText = "|" + "|".join(map(str, episodes)) + "|"

        # get the current timestamp
        curTimestamp = int(time.mktime(datetime.datetime.today().timetuple()))

        if not quality:
            quality = Quality.nameQuality(name)

        myDB.action("INSERT INTO [" + self.providerID + "] (name, season, episodes, tvrid, tvdbid, url, time, quality) VALUES (?,?,?,?,?,?,?,?)",
                    [name, season, episodeText, tvrage_id, tvdb_id, url, curTimestamp, quality])

    def searchCache(self, episode, manualSearch=False):
        neededEps = self.findNeededEpisodes(episode, manualSearch)
        return neededEps[episode]

    def listPropers(self, date=None, delimiter="."):

        myDB = self._getDB()

        sql = "SELECT * FROM [" + self.providerID + "] WHERE name LIKE '%.PROPER.%' OR name LIKE '%.REPACK.%'"

        if date != None:
            sql += " AND time >= " + str(int(time.mktime(date.timetuple())))

        #return filter(lambda x: x['tvdbid'] != 0, myDB.select(sql))
        return myDB.select(sql)

    def findNeededEpisodes(self, episode=None, manualSearch=False):
        neededEps = {}

        if episode:
            neededEps[episode] = []

        myDB = self._getDB()

        if not episode:
            sqlResults = myDB.select("SELECT * FROM [" + self.providerID + "]")
        else:
            sqlResults = myDB.select("SELECT * FROM [" + self.providerID + "] WHERE tvdbid = ? AND season = ? AND episodes LIKE ?", [episode.show.tvdbid, episode.season, "%|" + str(episode.episode) + "|%"])

        # for each cache entry
        for curResult in sqlResults:

            # skip non-tv crap
            if not show_name_helpers.filterBadReleases(curResult["name"]):
                continue

            # get the show object, or if it's not one of our shows then ignore it
            showObj = helpers.findCertainShow(sickbeard.showList, int(curResult["tvdbid"]))
            if not showObj:
                continue

            # get season and ep data (ignoring multi-eps for now)
            curSeason = int(curResult["season"])
            if curSeason == -1:
                continue
            curEp = curResult["episodes"].split("|")[1]
            if not curEp:
                continue
            curEp = int(curEp)
            curQuality = int(curResult["quality"])

            # if the show says we want that episode then add it to the list
            if not showObj.wantEpisode(curSeason, curEp, curQuality, manualSearch):
                logger.log(u"Skipping " + curResult["name"] + " because we don't want an episode that's " + Quality.qualityStrings[curQuality], logger.DEBUG)

            else:

                if episode:
                    epObj = episode
                else:
                    epObj = showObj.getEpisode(curSeason, curEp)

                # build a result object
                title = curResult["name"]
                url = curResult["url"]

                logger.log(u"Found result " + title + " at " + url)

                result = self.provider.getResult([epObj])
                result.url = url
                result.name = title
                result.quality = curQuality

                # add it to the list
                if epObj not in neededEps:
                    neededEps[epObj] = [result]
                else:
                    neededEps[epObj].append(result)

        return neededEps

########NEW FILE########
__FILENAME__ = tvrage
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import urllib
import urllib2
import datetime
import traceback

import sickbeard

from sickbeard import logger
from sickbeard.common import UNAIRED

from sickbeard import db
from sickbeard import exceptions, helpers
from sickbeard.exceptions import ex

from lib.tvdb_api import tvdb_api, tvdb_exceptions


class TVRage:

    def __init__(self, show):

        self.show = show

        self.lastEpInfo = None
        self.nextEpInfo = None

        self._tvrid = 0
        self._tvrname = None

        if self.show.tvrid == 0:

            # if it's the right show then use the tvrage ID that the last lookup found (cached in self._trvid)
            show_is_right = self.confirmShow() or self.checkSync()

            if not show_is_right:
                raise exceptions.TVRageException("Shows aren't the same, aborting")

            if self._tvrid == 0 or self._tvrname == None:
                raise exceptions.TVRageException("We confirmed sync but got invalid data (no ID/name)")

            if show_is_right:

                logger.log(u"Setting TVRage ID for " + show.name + " to " + str(self._tvrid))
                self.show.tvrid = self._tvrid
                self.show.saveToDB()

        if not self.show.tvrname:

            if self._tvrname == None:
                self._getTVRageInfo()

            logger.log(u"Setting TVRage Show Name for " + show.name + " to " + self._tvrname)
            self.show.tvrname = self._tvrname
            self.show.saveToDB()

    def confirmShow(self, force=False):

        if self.show.tvrid != 0 and not force:
            logger.log(u"We already have a TVRage ID, skipping confirmation", logger.DEBUG)
            return True

        logger.log(u"Checking the first episode of each season to see if the air dates match between TVDB and TVRage")

        tvdb_lang = self.show.lang

        try:

            try:
                # There's gotta be a better way of doing this but we don't wanna
                # change the language value elsewhere
                ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

                if tvdb_lang and not tvdb_lang == 'en':
                    ltvdb_api_parms['language'] = tvdb_lang

                t = tvdb_api.Tvdb(**ltvdb_api_parms)
            except tvdb_exceptions.tvdb_exception, e:
                logger.log(u"Currently this doesn't work with TVDB down but with some DB magic it can be added", logger.DEBUG)
                return None

            # check the first episode of every season
            for curSeason in t[self.show.tvdbid]:

                logger.log(u"Checking TVDB and TVRage sync for season " + str(curSeason), logger.DEBUG)

                airdate = None

                try:

                    # don't do specials and don't do seasons with no episode 1
                    if curSeason == 0 or 1 not in t[self.show.tvdbid]:
                        continue

                    # get the episode info from TVDB
                    ep = t[self.show.tvdbid][curSeason][1]

                    # make sure we have a date to compare with
                    if ep["firstaired"] == "" or ep["firstaired"] == None or ep["firstaired"] == "0000-00-00":
                        continue

                    # get a datetime object
                    rawAirdate = [int(x) for x in ep["firstaired"].split("-")]
                    airdate = datetime.date(rawAirdate[0], rawAirdate[1], rawAirdate[2])

                    # get the episode info from TVRage
                    info = self._getTVRageInfo(curSeason, 1)

                    # make sure we have enough info
                    if info == None or not info.has_key('Episode Info'):
                        logger.log(u"TVRage doesn't have the episode info, skipping it", logger.DEBUG)
                        continue

                    # parse the episode info
                    curEpInfo = self._getEpInfo(info['Episode Info'])

                    # make sure we got some info back
                    if curEpInfo == None:
                        continue

                # if we couldn't compare with TVDB try comparing it with the local database
                except tvdb_exceptions.tvdb_exception, e:
                    logger.log(u"Unable to check TVRage info against TVDB: " + ex(e))

                    logger.log(u"Trying against DB instead", logger.DEBUG)

                    myDB = db.DBConnection()
                    sqlResults = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND season = ? and episode = ?", [self.show.tvdbid, self.lastEpInfo['season'], self.lastEpInfo['episode']])

                    if len(sqlResults) == 0:
                        raise exceptions.EpisodeNotFoundException("Unable to find episode in DB")
                    else:
                        airdate = datetime.date.fromordinal(int(sqlResults[0]["airdate"]))

                # check if TVRage and TVDB have the same airdate for this episode
                if curEpInfo['airdate'] == airdate:
                    logger.log(u"Successful match for TVRage and TVDB data for episode " + str(curSeason) + "x1)", logger.DEBUG)
                    return True

                logger.log(u"Date from TVDB for episode " + str(curSeason) + "x1: " + str(airdate), logger.DEBUG)
                logger.log(u"Date from TVRage for episode " + str(curSeason) + "x1: " + str(curEpInfo['airdate']), logger.DEBUG)

        except Exception, e:
            logger.log(u"Error encountered while checking TVRage<->TVDB sync: " + ex(e), logger.WARNING)
            logger.log(traceback.format_exc(), logger.DEBUG)

        return False

    def checkSync(self, info=None):

        logger.log(u"Checking the last aired episode to see if the dates match between TVDB and TVRage")

        if self.lastEpInfo == None or self.nextEpInfo == None:
            self._saveLatestInfo(info)

        if self.nextEpInfo['season'] == 0 or self.nextEpInfo['episode'] == 0:
            return None

        try:

            airdate = None

            tvdb_lang = self.show.lang
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            if tvdb_lang and not tvdb_lang == 'en':
                ltvdb_api_parms['language'] = tvdb_lang

            # make sure the last TVDB episode matches our last episode
            try:
                t = tvdb_api.Tvdb(**ltvdb_api_parms)
                ep = t[self.show.tvdbid][self.lastEpInfo['season']][self.lastEpInfo['episode']]

                if ep["firstaired"] == "" or ep["firstaired"] == None:
                    return None

                rawAirdate = [int(x) for x in ep["firstaired"].split("-")]
                airdate = datetime.date(rawAirdate[0], rawAirdate[1], rawAirdate[2])

            except tvdb_exceptions.tvdb_exception, e:
                logger.log(u"Unable to check TVRage info against TVDB: " + ex(e))

                logger.log(u"Trying against DB instead", logger.DEBUG)

                myDB = db.DBConnection()
                sqlResults = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND season = ? and episode = ?", [self.show.tvdbid, self.lastEpInfo['season'], self.lastEpInfo['episode']])

                if len(sqlResults) == 0:
                    raise exceptions.EpisodeNotFoundException("Unable to find episode in DB")
                else:
                    airdate = datetime.date.fromordinal(int(sqlResults[0]["airdate"]))

            logger.log(u"Date from TVDB for episode " + str(self.lastEpInfo['season']) + "x" + str(self.lastEpInfo['episode']) + ": " + str(airdate), logger.DEBUG)
            logger.log(u"Date from TVRage for episode " + str(self.lastEpInfo['season']) + "x" + str(self.lastEpInfo['episode']) + ": " + str(self.lastEpInfo['airdate']), logger.DEBUG)

            if self.lastEpInfo['airdate'] == airdate:
                return True

        except Exception, e:
            logger.log(u"Error encountered while checking TVRage<->TVDB sync: " + ex(e), logger.WARNING)
            logger.log(traceback.format_exc(), logger.DEBUG)

        return False

    def _getTVRageInfo(self, season=None, episode=None, full=False):

        url = "http://services.tvrage.com/tools/quickinfo.php?"

        # if we need full info OR if we don't have a tvrage id, use show name
        if full == True or self.show.tvrid == 0:
            if self.show.tvrname != "" and self.show.tvrname != None:
                showName = self.show.tvrname
            else:
                showName = self.show.name

            urlData = {'show': showName.encode('utf-8')}

        # if we don't need full info and we have a tvrage id, use it
        else:
            urlData = {'sid': self.show.tvrid}

        if season != None and episode != None:
            urlData['ep'] = str(season) + 'x' + str(episode)

        # build the URL
        url += urllib.urlencode(urlData)

        logger.log(u"Loading TVRage info from URL: " + url, logger.DEBUG)
        result = helpers.getURL(url)

        if result is None:
            raise exceptions.TVRageException("urlopen call to " + url + " failed")
        else:
            result = result.decode('utf-8')

        urlData = result.splitlines()
        info = {}

        for x in urlData:
            if x.startswith("No Show Results Were Found"):
                logger.log(u"TVRage returned: " + x.encode('utf-8'), logger.WARNING)
                return info

            if "@" in x:
                key, value = x.split("@")
                if key:
                    key = key.replace('<pre>', '')
                    info[key] = value.strip()
            else:
                logger.log(u"TVRage returned: " + x.encode('utf-8'), logger.WARNING)
                return info

        # save it for later in case somebody is curious
        if 'Show ID' in info:
            self._tvrid = info['Show ID']

        if 'Show Name' in info:
            self._tvrname = info['Show Name']

        return info

    def _saveLatestInfo(self, info=None):

        if info == None:
            info = self._getTVRageInfo()

        if 'Next Episode' not in info or 'Latest Episode' not in info:
            raise exceptions.TVRageException("TVRage doesn't have all the required info for this show")

        self.lastEpInfo = self._getEpInfo(info['Latest Episode'])
        self.nextEpInfo = self._getEpInfo(info['Next Episode'])

        if self.lastEpInfo == None or self.nextEpInfo == None:
            raise exceptions.TVRageException("TVRage has malformed data, unable to update the show")

    def _getEpInfo(self, epString):

        month_dict = {"Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4, "May": 5, "Jun": 6, "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12}

        logger.log(u"Parsing info from TVRage: " + epString, logger.DEBUG)

        ep_info = epString.split('^')

        num_info = [int(x) for x in ep_info[0].split('x')]

        date_info = ep_info[2]

        try:
            air_date = year = month = day = None

            date_info_list = date_info.split("/")
            year = date_info_list[2]

            if date_info_list[0] in month_dict:
                month = month_dict[date_info_list[0]]
                day = date_info_list[1]

            else:
                day = date_info_list[0]
                month = month_dict[date_info_list[1]]

            air_date = datetime.date(int(year), int(month), int(day))

        except:
            air_date = None

        if not air_date:
            logger.log(u"Unable to figure out the time from the TVRage data " + ep_info[2])
            return None

        toReturn = {'season': int(num_info[0]), 'episode': num_info[1], 'name': ep_info[1], 'airdate': air_date}

        logger.log(u"Result of parse: " + str(toReturn), logger.DEBUG)

        return toReturn

    def findLatestEp(self):

        # will use tvrage name if it got set in the constructor, or tvdb name if not
        info = self._getTVRageInfo(full=True)

        if not self.checkSync(info):
            raise exceptions.TVRageException("TVRage info isn't in sync with TVDB, not using data")

        myDB = db.DBConnection()

        # double check that it's not already in there
        sqlResults = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND season = ? AND episode = ?", [self.show.tvdbid, self.nextEpInfo['season'], self.nextEpInfo['episode']])

        if len(sqlResults) > 0:
            raise exceptions.TVRageException("Show is already in database, not adding the TVRage info")

        # insert it
        myDB.action("INSERT INTO tv_episodes (showid, tvdbid, name, season, episode, description, airdate, hasnfo, hastbn, status, location) VALUES (?,?,?,?,?,?,?,?,?,?,?)", \
                    [self.show.tvdbid, -1, self.nextEpInfo['name'], self.nextEpInfo['season'], self.nextEpInfo['episode'], '', self.nextEpInfo['airdate'].toordinal(), 0, 0, UNAIRED, ''])

        # once it's in the DB make an object and return it
        ep = None

        try:
            ep = self.show.getEpisode(self.nextEpInfo['season'], self.nextEpInfo['episode'])
        except exceptions.SickBeardException, e:
            logger.log(u"Unable to create episode from tvrage (could be for a variety of reasons): " + ex(e))

        return ep

########NEW FILE########
__FILENAME__ = ui
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import datetime
import cherrypy
import sickbeard

MESSAGE = 'notice'
ERROR = 'error'

class Notifications(object):
    """
    A queue of Notification objects.
    """
    def __init__(self):
        self._messages = []
        self._errors = []
        
    def message(self, title, message=''):
        """
        Add a regular notification to the queue
        
        title: The title of the notification
        message: The message portion of the notification  
        """
        self._messages.append(Notification(title, message, MESSAGE))

    def error(self, title, message=''):
        """
        Add an error notification to the queue

        title: The title of the notification
        message: The message portion of the notification  
        """
        self._errors.append(Notification(title, message, ERROR))

    def get_notifications(self):
        """
        Return all the available notifications in a list. Marks them all as seen
        as it returns them. Also removes timed out Notifications from the queue.
        
        Returns: A list of Notification objects
        """
        
        # filter out expired notifications 
        self._errors = [x for x in self._errors if not x.is_expired()]
        self._messages = [x for x in self._messages if not x.is_expired()]
        
        # return any notifications that haven't been shown to the client already
        return [x.see() for x in self._errors + self._messages if x.is_new()]

# static notification queue object
notifications = Notifications()

    
class Notification(object):
    """
    Represents a single notification. Tracks its own timeout and a list of which clients have
    seen it before.
    """
    def __init__(self, title, message='', type=None, timeout=None):
        self.title = title
        self.message = message
        
        self._when = datetime.datetime.now()
        self._seen = []

        if type:
            self.type = type
        else:
            self.type = MESSAGE
        
        if timeout:
            self._timeout = timeout
        else:
            self._timeout = datetime.timedelta(minutes=1)

    def is_new(self):
        """
        Returns True if the notification hasn't been displayed to the current client (aka IP address).
        """
        return cherrypy.request.remote.ip not in self._seen
    
    def is_expired(self):
        """
        Returns True if the notification is older than the specified timeout value.
        """
        return datetime.datetime.now() - self._when > self._timeout

    
    def see(self):
        """
        Returns this notification object and marks it as seen by the client ip
        """
        self._seen.append(cherrypy.request.remote.ip)
        return self

class ProgressIndicator():

    def __init__(self, percentComplete=0, currentStatus={'title': ''}):
        self.percentComplete = percentComplete
        self.currentStatus = currentStatus

class ProgressIndicators():
    _pi = {'massUpdate': [],
           'massAdd': [],
           'dailyUpdate': []
           }

    @staticmethod
    def getIndicator(name):
        if name not in ProgressIndicators._pi:
            return []

        # if any of the progress indicators are done take them off the list
        for curPI in ProgressIndicators._pi[name]:
            if curPI != None and curPI.percentComplete() == 100:
                ProgressIndicators._pi[name].remove(curPI)

        # return the list of progress indicators associated with this name
        return ProgressIndicators._pi[name]

    @staticmethod
    def setIndicator(name, indicator):
        ProgressIndicators._pi[name].append(indicator)

class QueueProgressIndicator():
    """
    A class used by the UI to show the progress of the queue or a part of it.
    """
    def __init__(self, name, queueItemList):
        self.queueItemList = queueItemList
        self.name = name

    def numTotal(self):
        return len(self.queueItemList)

    def numFinished(self):
        return len([x for x in self.queueItemList if not x.isInQueue()])

    def numRemaining(self):
        return len([x for x in self.queueItemList if x.isInQueue()])

    def nextName(self):
        for curItem in [sickbeard.showQueueScheduler.action.currentItem]+sickbeard.showQueueScheduler.action.queue: #@UndefinedVariable
            if curItem in self.queueItemList:
                return curItem.name

        return "Unknown"

    def percentComplete(self):
        numFinished = self.numFinished()
        numTotal = self.numTotal()

        if numTotal == 0:
            return 0
        else:
            return int(float(numFinished)/float(numTotal)*100)

class LoadingTVShow():
    def __init__(self, dir):
        self.dir = dir
        self.show = None



########NEW FILE########
__FILENAME__ = version
SICKBEARD_VERSION = "master"
########NEW FILE########
__FILENAME__ = versionChecker
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import sickbeard
from sickbeard import helpers
from sickbeard import version, ui
from sickbeard import logger
from sickbeard import scene_exceptions
from sickbeard.exceptions import ex
from sickbeard import encodingKludge as ek

import os
import platform
import shutil
import subprocess
import re

import urllib

import zipfile
import tarfile

import gh_api as github


class CheckVersion():
    """
    Version check class meant to run as a thread object with the SB scheduler.
    """

    def __init__(self):
        self.install_type = self.find_install_type()

        if self.install_type == 'win':
            self.updater = WindowsUpdateManager()
        elif self.install_type == 'git':
            self.updater = GitUpdateManager()
        elif self.install_type == 'source':
            self.updater = SourceUpdateManager()
        else:
            self.updater = None

    def run(self):
        self.check_for_new_version()

        # refresh scene exceptions too
        scene_exceptions.retrieve_exceptions()

    def find_install_type(self):
        """
        Determines how this copy of SB was installed.

        returns: type of installation. Possible values are:
            'win': any compiled windows build
            'git': running from source using git
            'source': running from source without git
        """

        # check if we're a windows build
        if sickbeard.version.SICKBEARD_VERSION.startswith('build '):
            install_type = 'win'
        elif os.path.isdir(ek.ek(os.path.join, sickbeard.PROG_DIR, u'.git')):
            install_type = 'git'
        else:
            install_type = 'source'

        return install_type

    def check_for_new_version(self, force=False):
        """
        Checks the internet for a newer version.

        returns: bool, True for new version or False for no new version.

        force: if true the VERSION_NOTIFY setting will be ignored and a check will be forced
        """

        if not sickbeard.VERSION_NOTIFY and not force:
            logger.log(u"Version checking is disabled, not checking for the newest version")
            return False

        logger.log(u"Checking if " + self.install_type + " needs an update")
        if not self.updater.need_update():
            sickbeard.NEWEST_VERSION_STRING = None
            logger.log(u"No update needed")

            if force:
                ui.notifications.message('No update needed')
            return False

        self.updater.set_newest_text()
        return True

    def update(self):
        if self.updater.need_update():
            return self.updater.update()


class UpdateManager():

    def get_github_repo_user(self):
        return 'midgetspy'

    def get_github_repo(self):
        return 'Sick-Beard'

    def get_update_url(self):
        return sickbeard.WEB_ROOT + "/home/update/?pid=" + str(sickbeard.PID)


class WindowsUpdateManager(UpdateManager):

    def __init__(self):
        self.github_repo_user = self.get_github_repo_user()
        self.github_repo = self.get_github_repo()
        self.branch = 'windows_binaries'

        self._cur_version = None
        self._cur_commit_hash = None
        self._newest_version = None

        self.releases_url = "https://github.com/" + self.github_repo_user + "/" + self.github_repo + "/" + "releases" + "/"
        self.version_url = "https://raw.github.com/" + self.github_repo_user + "/" + self.github_repo + "/" + self.branch + "/updates.txt"

    def _find_installed_version(self):
        try:
            version = sickbeard.version.SICKBEARD_VERSION
            return int(version[6:])
        except ValueError:
            logger.log(u"Unknown SickBeard Windows binary release: " + version, logger.ERROR)
            return None

    def _find_newest_version(self, whole_link=False):
        """
        Checks git for the newest Windows binary build. Returns either the
        build number or the entire build URL depending on whole_link's value.

        whole_link: If True, returns the entire URL to the release. If False, it returns
                    only the build number. default: False
        """

        regex = ".*SickBeard\-win32\-alpha\-build(\d+)(?:\.\d+)?\.zip"

        version_url_data = helpers.getURL(self.version_url)

        if version_url_data is None:
            return None
        else:
            for curLine in version_url_data.splitlines():
                logger.log(u"checking line " + curLine, logger.DEBUG)
                match = re.match(regex, curLine)
                if match:
                    logger.log(u"found a match", logger.DEBUG)
                    if whole_link:
                        return curLine.strip()
                    else:
                        return int(match.group(1))

        return None

    def need_update(self):
        self._cur_version = self._find_installed_version()
        self._newest_version = self._find_newest_version()

        logger.log(u"newest version: " + repr(self._newest_version), logger.DEBUG)

        if self._newest_version and self._newest_version > self._cur_version:
            return True

        return False

    def set_newest_text(self):

        sickbeard.NEWEST_VERSION_STRING = None

        if not self._cur_version:
            newest_text = "Unknown SickBeard Windows binary version. Not updating with original version."
        else:
            newest_text = 'There is a <a href="' + self.releases_url + '" onclick="window.open(this.href); return false;">newer version available</a> (build ' + str(self._newest_version) + ')'
            newest_text += "&mdash; <a href=\"" + self.get_update_url() + "\">Update Now</a>"

        sickbeard.NEWEST_VERSION_STRING = newest_text

    def update(self):

        zip_download_url = self._find_newest_version(True)
        logger.log(u"new_link: " + repr(zip_download_url), logger.DEBUG)

        if not zip_download_url:
            logger.log(u"Unable to find a new version link, not updating")
            return False

        try:
            # prepare the update dir
            sb_update_dir = ek.ek(os.path.join, sickbeard.PROG_DIR, u'sb-update')

            if os.path.isdir(sb_update_dir):
                logger.log(u"Clearing out update folder " + sb_update_dir + " before extracting")
                shutil.rmtree(sb_update_dir)

            logger.log(u"Creating update folder " + sb_update_dir + " before extracting")
            os.makedirs(sb_update_dir)

            # retrieve file
            logger.log(u"Downloading update from " + zip_download_url)
            zip_download_path = os.path.join(sb_update_dir, u'sb-update.zip')
            urllib.urlretrieve(zip_download_url, zip_download_path)

            if not ek.ek(os.path.isfile, zip_download_path):
                logger.log(u"Unable to retrieve new version from " + zip_download_url + ", can't update", logger.ERROR)
                return False

            if not ek.ek(zipfile.is_zipfile, zip_download_path):
                logger.log(u"Retrieved version from " + zip_download_url + " is corrupt, can't update", logger.ERROR)
                return False

            # extract to sb-update dir
            logger.log(u"Unzipping from " + str(zip_download_path) + " to " + sb_update_dir)
            update_zip = zipfile.ZipFile(zip_download_path, 'r')
            update_zip.extractall(sb_update_dir)
            update_zip.close()

            # delete the zip
            logger.log(u"Deleting zip file from " + str(zip_download_path))
            os.remove(zip_download_path)

            # find update dir name
            update_dir_contents = [x for x in os.listdir(sb_update_dir) if os.path.isdir(os.path.join(sb_update_dir, x))]

            if len(update_dir_contents) != 1:
                logger.log(u"Invalid update data, update failed. Maybe try deleting your sb-update folder?", logger.ERROR)
                return False

            content_dir = os.path.join(sb_update_dir, update_dir_contents[0])
            old_update_path = os.path.join(content_dir, u'updater.exe')
            new_update_path = os.path.join(sickbeard.PROG_DIR, u'updater.exe')
            logger.log(u"Copying new update.exe file from " + old_update_path + " to " + new_update_path)
            shutil.move(old_update_path, new_update_path)

        except Exception, e:
            logger.log(u"Error while trying to update: " + ex(e), logger.ERROR)
            return False

        return True


class GitUpdateManager(UpdateManager):

    def __init__(self):
        self._git_path = self._find_working_git()
        self.github_repo_user = self.get_github_repo_user()
        self.github_repo = self.get_github_repo()
        self.branch = self._find_git_branch()

        self._cur_commit_hash = None
        self._newest_commit_hash = None
        self._num_commits_behind = 0
        self._num_commits_ahead = 0

    def _git_error(self):
        error_message = 'Unable to find your git executable - Shutdown SickBeard and EITHER <a href="http://code.google.com/p/sickbeard/wiki/AdvancedSettings" onclick="window.open(this.href); return false;">set git_path in your config.ini</a> OR delete your .git folder and run from source to enable updates.'
        sickbeard.NEWEST_VERSION_STRING = error_message

    def _find_working_git(self):
        test_cmd = 'version'

        if sickbeard.GIT_PATH:
            main_git = '"' + sickbeard.GIT_PATH + '"'
        else:
            main_git = 'git'

        logger.log(u"Checking if we can use git commands: " + main_git + ' ' + test_cmd, logger.DEBUG)
        output, err, exit_status = self._run_git(main_git, test_cmd)  # @UnusedVariable

        if exit_status == 0:
            logger.log(u"Using: " + main_git, logger.DEBUG)
            return main_git
        else:
            logger.log(u"Not using: " + main_git, logger.DEBUG)

        # trying alternatives

        alternative_git = []

        # osx people who start SB from launchd have a broken path, so try a hail-mary attempt for them
        if platform.system().lower() == 'darwin':
            alternative_git.append('/usr/local/git/bin/git')

        if platform.system().lower() == 'windows':
            if main_git != main_git.lower():
                alternative_git.append(main_git.lower())

        if alternative_git:
            logger.log(u"Trying known alternative git locations", logger.DEBUG)

            for cur_git in alternative_git:
                logger.log(u"Checking if we can use git commands: " + cur_git + ' ' + test_cmd, logger.DEBUG)
                output, err, exit_status = self._run_git(cur_git, test_cmd)  # @UnusedVariable

                if exit_status == 0:
                    logger.log(u"Using: " + cur_git, logger.DEBUG)
                    return cur_git
                else:
                    logger.log(u"Not using: " + cur_git, logger.DEBUG)

        # Still haven't found a working git
        error_message = 'Unable to find your git executable - Shutdown SickBeard and EITHER <a href="http://code.google.com/p/sickbeard/wiki/AdvancedSettings" onclick="window.open(this.href); return false;">set git_path in your config.ini</a> OR delete your .git folder and run from source to enable updates.'
        sickbeard.NEWEST_VERSION_STRING = error_message

        return None

    def _run_git(self, git_path, args):

        output = err = exit_status = None

        if not git_path:
            logger.log(u"No git specified, can't use git commands", logger.ERROR)
            exit_status = 1
            return (output, err, exit_status)

        cmd = git_path + ' ' + args

        try:
            logger.log(u"Executing " + cmd + " with your shell in " + sickbeard.PROG_DIR, logger.DEBUG)
            p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, cwd=sickbeard.PROG_DIR)
            output, err = p.communicate()
            exit_status = p.returncode

            if output:
                output = output.strip()
            logger.log(u"git output: " + output, logger.DEBUG)

        except OSError:
            logger.log(u"Command " + cmd + " didn't work")
            exit_status = 1

        if exit_status == 0:
            logger.log(cmd + u" : returned successful", logger.DEBUG)
            exit_status = 0

        elif exit_status == 1:
            logger.log(cmd + u" returned : " + output, logger.ERROR)
            exit_status = 1

        elif exit_status == 128 or 'fatal:' in output or err:
            logger.log(cmd + u" returned : " + output, logger.ERROR)
            exit_status = 128

        else:
            logger.log(cmd + u" returned : " + output + u", treat as error for now", logger.ERROR)
            exit_status = 1

        return (output, err, exit_status)

    def _find_installed_version(self):
        """
        Attempts to find the currently installed version of Sick Beard.

        Uses git show to get commit version.

        Returns: True for success or False for failure
        """

        output, err, exit_status = self._run_git(self._git_path, 'rev-parse HEAD')  # @UnusedVariable

        if exit_status == 0 and output:
            cur_commit_hash = output.strip()
            if not re.match('^[a-z0-9]+$', cur_commit_hash):
                logger.log(u"Output doesn't look like a hash, not using it", logger.ERROR)
                return False
            self._cur_commit_hash = cur_commit_hash
            return True
        else:
            return False

    def _find_git_branch(self):
        branch_info, err, exit_status = self._run_git(self._git_path, 'symbolic-ref -q HEAD')  # @UnusedVariable
        if exit_status == 0 and branch_info:
            branch = branch_info.strip().replace('refs/heads/', '', 1)
            if branch:
                sickbeard.version.SICKBEARD_VERSION = branch
        return sickbeard.version.SICKBEARD_VERSION

    def _check_github_for_update(self):
        """
        Uses git commands to check if there is a newer version that the provided
        commit hash. If there is a newer version it sets _num_commits_behind.
        """

        self._newest_commit_hash = None
        self._num_commits_behind = 0
        self._num_commits_ahead = 0

        # get all new info from github
        output, err, exit_status = self._run_git(self._git_path, 'fetch origin')  # @UnusedVariable

        if not exit_status == 0:
            logger.log(u"Unable to contact github, can't check for update", logger.ERROR)
            return

        # get latest commit_hash from remote
        output, err, exit_status = self._run_git(self._git_path, 'rev-parse --verify --quiet "@{upstream}"')  # @UnusedVariable

        if exit_status == 0 and output:
            cur_commit_hash = output.strip()

            if not re.match('^[a-z0-9]+$', cur_commit_hash):
                logger.log(u"Output doesn't look like a hash, not using it", logger.DEBUG)
                return

            else:
                self._newest_commit_hash = cur_commit_hash
        else:
            logger.log(u"git didn't return newest commit hash", logger.DEBUG)
            return

        # get number of commits behind and ahead (option --count not supported git < 1.7.2)
        output, err, exit_status = self._run_git(self._git_path, 'rev-list --left-right "@{upstream}"...HEAD')  # @UnusedVariable

        if exit_status == 0 and output:

            try:
                self._num_commits_behind = int(output.count("<"))
                self._num_commits_ahead = int(output.count(">"))

            except:
                logger.log(u"git didn't return numbers for behind and ahead, not using it", logger.DEBUG)
                return

        logger.log(u"cur_commit = " + str(self._cur_commit_hash) + u", newest_commit = " + str(self._newest_commit_hash)
                   + u", num_commits_behind = " + str(self._num_commits_behind) + u", num_commits_ahead = " + str(self._num_commits_ahead), logger.DEBUG)

    def set_newest_text(self):

        # if we're up to date then don't set this
        sickbeard.NEWEST_VERSION_STRING = None

        if self._num_commits_ahead:
            logger.log(u"Local branch is ahead of " + self.branch + ". Automatic update not possible.", logger.ERROR)
            newest_text = "Local branch is ahead of " + self.branch + ". Automatic update not possible."

        elif self._num_commits_behind > 0:

            base_url = 'http://github.com/' + self.github_repo_user + '/' + self.github_repo
            if self._newest_commit_hash:
                url = base_url + '/compare/' + self._cur_commit_hash + '...' + self._newest_commit_hash
            else:
                url = base_url + '/commits/'

            newest_text = 'There is a <a href="' + url + '" onclick="window.open(this.href); return false;">newer version available</a> '
            newest_text += " (you're " + str(self._num_commits_behind) + " commit"
            if self._num_commits_behind > 1:
                newest_text += 's'
            newest_text += ' behind)' + "&mdash; <a href=\"" + self.get_update_url() + "\">Update Now</a>"

        else:
            return

        sickbeard.NEWEST_VERSION_STRING = newest_text

    def need_update(self):
        self._find_installed_version()

        if not self._cur_commit_hash:
            return True
        else:
            try:
                self._check_github_for_update()
            except Exception, e:
                logger.log(u"Unable to contact github, can't check for update: " + repr(e), logger.ERROR)
                return False

            if self._num_commits_behind > 0:
                return True

        return False

    def update(self):
        """
        Calls git pull origin <branch> in order to update Sick Beard. Returns a bool depending
        on the call's success.
        """

        output, err, exit_status = self._run_git(self._git_path, 'pull origin ' + self.branch)  # @UnusedVariable

        if exit_status == 0:
            return True
        else:
            return False

        return False


class SourceUpdateManager(UpdateManager):

    def __init__(self):
        self.github_repo_user = self.get_github_repo_user()
        self.github_repo = self.get_github_repo()
        self.branch = sickbeard.version.SICKBEARD_VERSION

        self._cur_commit_hash = None
        self._newest_commit_hash = None
        self._num_commits_behind = 0

    def _find_installed_version(self):

        version_file = ek.ek(os.path.join, sickbeard.PROG_DIR, u'version.txt')

        if not os.path.isfile(version_file):
            self._cur_commit_hash = None
            return

        try:
            with open(version_file, 'r') as fp:
                self._cur_commit_hash = fp.read().strip(' \n\r')
        except EnvironmentError, e:
            logger.log(u"Unable to open 'version.txt': " + ex(e), logger.DEBUG)

        if not self._cur_commit_hash:
            self._cur_commit_hash = None

    def need_update(self):

        self._find_installed_version()

        try:
            self._check_github_for_update()
        except Exception, e:
            logger.log(u"Unable to contact github, can't check for update: " + repr(e), logger.ERROR)
            return False

        if not self._cur_commit_hash or self._num_commits_behind > 0:
            return True

        return False

    def _check_github_for_update(self):
        """
        Uses pygithub to ask github if there is a newer version that the provided
        commit hash. If there is a newer version it sets Sick Beard's version text.

        commit_hash: hash that we're checking against
        """

        self._num_commits_behind = 0
        self._newest_commit_hash = None

        gh = github.GitHub(self.github_repo_user, self.github_repo, self.branch)

        # try to get newest commit hash and commits behind directly by comparing branch and current commit
        if self._cur_commit_hash:
            branch_compared = gh.compare(base=self.branch, head=self._cur_commit_hash)

            if 'base_commit' in branch_compared:
                self._newest_commit_hash = branch_compared['base_commit']['sha']

            if 'behind_by' in branch_compared:
                self._num_commits_behind = int(branch_compared['behind_by'])

        # fall back and iterate over last 100 (items per page in gh_api) commits
        if not self._newest_commit_hash:

            for curCommit in gh.commits():
                if not self._newest_commit_hash:
                    self._newest_commit_hash = curCommit['sha']
                    if not self._cur_commit_hash:
                        break

                if curCommit['sha'] == self._cur_commit_hash:
                    break

                # when _cur_commit_hash doesn't match anything _num_commits_behind == 100
                self._num_commits_behind += 1

        logger.log(u"cur_commit = " + str(self._cur_commit_hash) + u", newest_commit = " + str(self._newest_commit_hash)
                   + u", num_commits_behind = " + str(self._num_commits_behind), logger.DEBUG)

    def set_newest_text(self):

        # if we're up to date then don't set this
        sickbeard.NEWEST_VERSION_STRING = None

        if not self._cur_commit_hash:
            logger.log(u"Unknown current version number, don't know if we should update or not", logger.DEBUG)

            newest_text = "Unknown current version number: If you've never used the Sick Beard upgrade system before then current version is not set."
            newest_text += "&mdash; <a href=\"" + self.get_update_url() + "\">Update Now</a>"

        elif self._num_commits_behind > 0:
                base_url = 'http://github.com/' + self.github_repo_user + '/' + self.github_repo
                if self._newest_commit_hash:
                    url = base_url + '/compare/' + self._cur_commit_hash + '...' + self._newest_commit_hash
                else:
                    url = base_url + '/commits/'

                newest_text = 'There is a <a href="' + url + '" onclick="window.open(this.href); return false;">newer version available</a>'
                newest_text += " (you're " + str(self._num_commits_behind) + " commit"
                if self._num_commits_behind > 1:
                    newest_text += "s"
                newest_text += " behind)" + "&mdash; <a href=\"" + self.get_update_url() + "\">Update Now</a>"
        else:
            return

        sickbeard.NEWEST_VERSION_STRING = newest_text

    def update(self):
        """
        Downloads the latest source tarball from github and installs it over the existing version.
        """
        base_url = 'https://github.com/' + self.github_repo_user + '/' + self.github_repo
        tar_download_url = base_url + '/tarball/' + self.branch
        version_path = ek.ek(os.path.join, sickbeard.PROG_DIR, u'version.txt')

        try:
            # prepare the update dir
            sb_update_dir = ek.ek(os.path.join, sickbeard.PROG_DIR, u'sb-update')

            if os.path.isdir(sb_update_dir):
                logger.log(u"Clearing out update folder " + sb_update_dir + " before extracting")
                shutil.rmtree(sb_update_dir)

            logger.log(u"Creating update folder " + sb_update_dir + " before extracting")
            os.makedirs(sb_update_dir)

            # retrieve file
            logger.log(u"Downloading update from " + repr(tar_download_url))
            tar_download_path = os.path.join(sb_update_dir, u'sb-update.tar')
            urllib.urlretrieve(tar_download_url, tar_download_path)

            if not ek.ek(os.path.isfile, tar_download_path):
                logger.log(u"Unable to retrieve new version from " + tar_download_url + ", can't update", logger.ERROR)
                return False

            if not ek.ek(tarfile.is_tarfile, tar_download_path):
                logger.log(u"Retrieved version from " + tar_download_url + " is corrupt, can't update", logger.ERROR)
                return False

            # extract to sb-update dir
            logger.log(u"Extracting file " + tar_download_path)
            tar = tarfile.open(tar_download_path)
            tar.extractall(sb_update_dir)
            tar.close()

            # delete .tar.gz
            logger.log(u"Deleting file " + tar_download_path)
            os.remove(tar_download_path)

            # find update dir name
            update_dir_contents = [x for x in os.listdir(sb_update_dir) if os.path.isdir(os.path.join(sb_update_dir, x))]
            if len(update_dir_contents) != 1:
                logger.log(u"Invalid update data, update failed: " + str(update_dir_contents), logger.ERROR)
                return False
            content_dir = os.path.join(sb_update_dir, update_dir_contents[0])

            # walk temp folder and move files to main folder
            logger.log(u"Moving files from " + content_dir + " to " + sickbeard.PROG_DIR)
            for dirname, dirnames, filenames in os.walk(content_dir):  # @UnusedVariable
                dirname = dirname[len(content_dir) + 1:]
                for curfile in filenames:
                    old_path = os.path.join(content_dir, dirname, curfile)
                    new_path = os.path.join(sickbeard.PROG_DIR, dirname, curfile)

                    if os.path.isfile(new_path):
                        os.remove(new_path)
                    os.renames(old_path, new_path)

            # update version.txt with commit hash
            try:
                with open(version_path, 'w') as ver_file:
                    ver_file.write(self._newest_commit_hash)
            except EnvironmentError, e:
                logger.log(u"Unable to write version file, update not complete: " + ex(e), logger.ERROR)
                return False

        except Exception, e:
            logger.log(u"Error while trying to update: " + ex(e), logger.ERROR)
            return False

        return True

########NEW FILE########
__FILENAME__ = webapi
# Author: Dennis Lutter <lad1337@gmail.com>
# Author: Jonathon Saine <thezoggy@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import os
import time
import urllib
import datetime
import threading
import re
import traceback

import cherrypy
import sickbeard
import webserve
from sickbeard import db, logger, exceptions, history, ui, helpers
from sickbeard.exceptions import ex
from sickbeard import encodingKludge as ek
from sickbeard import search_queue
from sickbeard.common import SNATCHED, SNATCHED_PROPER, DOWNLOADED, SKIPPED, UNAIRED, IGNORED, ARCHIVED, WANTED, UNKNOWN
from common import Quality, qualityPresetStrings, statusStrings
from sickbeard import image_cache
from lib.tvdb_api import tvdb_api, tvdb_exceptions

try:
    import json
except ImportError:
    from lib import simplejson as json

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import xml.etree.ElementTree as etree

dateFormat = "%Y-%m-%d"
dateTimeFormat = "%Y-%m-%d %H:%M"


RESULT_SUCCESS = 10  # only use inside the run methods
RESULT_FAILURE = 20  # only use inside the run methods
RESULT_TIMEOUT = 30  # not used yet :(
RESULT_ERROR = 40  # only use outside of the run methods !
RESULT_FATAL = 50  # only use in Api.default() ! this is the "we encountered an internal error" error
RESULT_DENIED = 60  # only use in Api.default() ! this is the acces denied error
result_type_map = {RESULT_SUCCESS: "success",
                  RESULT_FAILURE: "failure",
                  RESULT_TIMEOUT: "timeout",
                  RESULT_ERROR: "error",
                  RESULT_FATAL: "fatal",
                  RESULT_DENIED: "denied",
                  }
# basically everything except RESULT_SUCCESS / success is bad


class Api:
    """ api class that returns json results """
    version = 4  # use an int since float-point is unpredictable
    intent = 4

    @cherrypy.expose
    def default(self, *args, **kwargs):

        self.apiKey = sickbeard.API_KEY
        access, accessMsg, args, kwargs = self._grand_access(self.apiKey, args, kwargs)

        # set the output callback
        # default json
        outputCallbackDict = {'default': self._out_as_json,
                              'image': lambda x: x['image'],
                              }

        # do we have acces ?
        if access:
            logger.log(accessMsg, logger.DEBUG)
        else:
            logger.log(accessMsg, logger.WARNING)
            return outputCallbackDict['default'](_responds(RESULT_DENIED, msg=accessMsg))

        # set the original call_dispatcher as the local _call_dispatcher
        _call_dispatcher = call_dispatcher
        # if profile was set wrap "_call_dispatcher" in the profile function
        if 'profile' in kwargs:
            from lib.profilehooks import profile
            _call_dispatcher = profile(_call_dispatcher, immediate=True)
            del kwargs["profile"]

        # if debug was set call the "_call_dispatcher"
        if 'debug' in kwargs:
            # this way we can debug the cherry.py traceback in the browser
            outDict = _call_dispatcher(args, kwargs)
            del kwargs["debug"]
        # if debug was not set we wrap the "call_dispatcher" in a try block to assure a json output
        else:
            try:
                outDict = _call_dispatcher(args, kwargs)
            # seems like cherrypy uses exceptions for redirecting apparently this can happen when requesting images but it is ok so lets re raise it
            except cherrypy.HTTPRedirect:
                raise
            # real internal error oohhh nooo :(
            except Exception, e:
                logger.log(u"API :: " + ex(e), logger.ERROR)
                errorData = {"error_msg": ex(e),
                             "args": args,
                             "kwargs": kwargs}
                outDict = _responds(RESULT_FATAL, errorData, "SickBeard encountered an internal error! Please report to the Devs")

        if 'outputType' in outDict:
            outputCallback = outputCallbackDict[outDict['outputType']]
        else:
            outputCallback = outputCallbackDict['default']

        return outputCallback(outDict)

    @cherrypy.expose
    def builder(self):
        """ expose the api-builder template """
        t = webserve.PageTemplate(file="apiBuilder.tmpl")

        def titler(x):
            if not x:
                return x
            if x.lower().startswith('a '):
                x = x[2:]
            elif x.lower().startswith('the '):
                x = x[4:]
            return x

        # enforce a 100 show limit to ensure performance
        t.sortedShowList = sorted(sickbeard.showList, lambda x, y: cmp(titler(x.name), titler(y.name)))[0:100]

        myDB = db.DBConnection(row_type="dict")
        seasonSQLResults = {}
        episodeSQLResults = {}

        for curShow in t.sortedShowList:
            seasonSQLResults[curShow.tvdbid] = myDB.select("SELECT DISTINCT season FROM tv_episodes WHERE showid = ? ORDER BY season DESC", [curShow.tvdbid])

        for curShow in t.sortedShowList:
            episodeSQLResults[curShow.tvdbid] = myDB.select("SELECT DISTINCT season,episode FROM tv_episodes WHERE showid = ? ORDER BY season DESC, episode DESC", [curShow.tvdbid])

        t.seasonSQLResults = seasonSQLResults
        t.episodeSQLResults = episodeSQLResults

        myDB.connection.close()
        if len(sickbeard.API_KEY) == 32:
            t.apikey = sickbeard.API_KEY
        else:
            t.apikey = "api key not generated"

        return webserve._munge(t)

    def _out_as_json(self, dict):
        """ set cherrypy response to json """
        response = cherrypy.response
        request = cherrypy.request
        response.headers['Content-Type'] = 'application/json;charset=UTF-8'
        try:
            out = json.dumps(dict, indent=self.intent, sort_keys=True)
            callback = request.params.get('callback') or request.params.get('jsonp')
            if callback != None:
                # wrap with JSONP call if requested
                out = callback + '(' + out + ');'
        # if we fail to generate the output fake an error
        except Exception, e:
            logger.log(u"API :: " + traceback.format_exc(), logger.DEBUG)
            out = '{"result":"' + result_type_map[RESULT_ERROR] + '", "message": "error while composing output: "' + ex(e) + '"}'
        return out

    def _grand_access(self, realKey, args, kwargs):
        """ validate api key and log result """
        remoteIp = cherrypy.request.remote.ip
        apiKey = kwargs.get("apikey", None)
        if not apiKey:
            # if we have keyless vars we assume first one is the api key, always !
            if args:
                apiKey = args[0]
                # remove the apikey from the args tuple
                args = args[1:]
        else:
            del kwargs["apikey"]

        if sickbeard.USE_API != True:
            msg = u"API :: " + remoteIp + " - SB API Disabled. ACCESS DENIED"
            return False, msg, args, kwargs
        elif apiKey == realKey:
            msg = u"API :: " + remoteIp + " - gave correct API KEY. ACCESS GRANTED"
            return True, msg, args, kwargs
        elif not apiKey:
            msg = u"API :: " + remoteIp + " - gave NO API KEY. ACCESS DENIED"
            return False, msg, args, kwargs
        else:
            msg = u"API :: " + remoteIp + " - gave WRONG API KEY " + apiKey + ". ACCESS DENIED"
            return False, msg, args, kwargs


def call_dispatcher(args, kwargs):
    """ calls the appropriate CMD class
        looks for a cmd in args and kwargs
        or calls the TVDBShorthandWrapper when the first args element is a number
        or returns an error that there is no such cmd
    """
    logger.log(u"API :: all args: '" + str(args) + "'", logger.DEBUG)
    logger.log(u"API :: all kwargs: '" + str(kwargs) + "'", logger.DEBUG)

    cmds = None
    if args:
        cmds = args[0]
        args = args[1:]

    if "cmd" in kwargs:
        cmds = kwargs["cmd"]
        del kwargs["cmd"]

    outDict = {}
    if cmds != None:
        cmds = cmds.split("|")
        multiCmds = bool(len(cmds) > 1)
        for cmd in cmds:
            curArgs, curKwargs = filter_params(cmd, args, kwargs)
            cmdIndex = None
            # was a index used for this cmd ?
            if len(cmd.split("_")) > 1:
                # this gives us the clear cmd and the index
                cmd, cmdIndex = cmd.split("_")

            logger.log(u"API :: " + cmd + ": curKwargs " + str(curKwargs), logger.DEBUG)
            # skip these cmd while chaining
            if not (multiCmds and cmd in ('show.getposter', 'show.getbanner')):
                try:
                    if cmd in _functionMaper:
                        # get the cmd class, init it and run()
                        curOutDict = _functionMaper.get(cmd)(curArgs, curKwargs).run()
                    elif _is_int(cmd):
                        curOutDict = TVDBShorthandWrapper(curArgs, curKwargs, cmd).run()
                    else:
                        curOutDict = _responds(RESULT_ERROR, "No such cmd: '" + cmd + "'")
                # Api errors that we raised, they are harmless
                except ApiError, e:
                    curOutDict = _responds(RESULT_ERROR, msg=ex(e))
            # if someone chained one of the forbidden cmds they will get an error for this one cmd
            else:
                curOutDict = _responds(RESULT_ERROR, msg="The cmd '" + cmd + "' is not supported while chaining")

            if multiCmds:
                # note: if multiple same cmds are issued but one has not an index defined it will override all others
                # or the other way around, this depends on the order of the cmds this is not a bug

                # do we need a index dict for this cmd ?
                if cmdIndex is None:
                    outDict[cmd] = curOutDict
                else:
                    if not cmd in outDict:
                        outDict[cmd] = {}
                    outDict[cmd][cmdIndex] = curOutDict
            else:
                outDict = curOutDict

        # if we had multiple cmds we have to wrap it in a response dict
        if multiCmds:
            outDict = _responds(RESULT_SUCCESS, outDict)
    # index / no cmd given
    else:
        outDict = CMD_SickBeard(args, kwargs).run()

    return outDict


def filter_params(cmd, args, kwargs):
    """ return only params kwargs that are for cmd
        and rename them to a clean version (remove "<cmd>_")
        args are shared across all cmds

        all args and kwarks are lowerd

        cmd are separated by "|" e.g. &cmd=shows|future
        kwargs are namespaced with "." e.g. show.tvdbid=101501
        if a karg has no namespace asing it anyways (global)

        full e.g.
        /api?apikey=1234&cmd=show.seasonlist_asd|show.seasonlist_2&show.seasonlist_asd.tvdbid=101501&show.seasonlist_2.tvdbid=79488&sort=asc

        two calls of show.seasonlist
        one has the index "asd" the other one "2"
        the "tvdbid" kwargs / params have the indexed cmd as a namspace
        and the kwarg / param "sort" is a used as a global
    """
    curArgs = []
    for arg in args:
        curArgs.append(arg.lower())
    curArgs = tuple(curArgs)

    curKwargs = {}
    for kwarg in kwargs:
        if kwarg.find(cmd + ".") == 0:
            cleanKey = kwarg.rpartition(".")[2]
            curKwargs[cleanKey] = kwargs[kwarg].lower()
        # the kwarg was not namespaced therefore a "global"
        elif not "." in kwarg:
            curKwargs[kwarg] = kwargs[kwarg]
    return curArgs, curKwargs


class ApiCall(object):
    _help = {"desc": "No help message available. Please tell the devs that a help msg is missing for this cmd"}

    def __init__(self, args, kwargs):
        # missing
        try:
            if self._missing:
                self.run = self.return_missing
        except AttributeError:
            pass
        # help
        if 'help' in kwargs:
            self.run = self.return_help

    def run(self):
        # override with real output function in subclass
        return {}

    def return_help(self):
        try:
            if self._requiredParams:
                pass
        except AttributeError:
            self._requiredParams = []
        try:
            if self._optionalParams:
                pass
        except AttributeError:
            self._optionalParams = []

        for paramDict, type in [(self._requiredParams, "requiredParameters"),
                          (self._optionalParams, "optionalParameters")]:

            if type in self._help:
                for paramName in paramDict:
                    if not paramName in self._help[type]:
                        self._help[type][paramName] = {}
                    if paramDict[paramName]["allowedValues"]:
                        self._help[type][paramName]["allowedValues"] = paramDict[paramName]["allowedValues"]
                    else:
                        self._help[type][paramName]["allowedValues"] = "see desc"
                    self._help[type][paramName]["defaultValue"] = paramDict[paramName]["defaultValue"]

            elif paramDict:
                for paramName in paramDict:
                    self._help[type] = {}
                    self._help[type][paramName] = paramDict[paramName]
            else:
                self._help[type] = {}
        msg = "No description available"
        if "desc" in self._help:
            msg = self._help["desc"]
            del self._help["desc"]
        return _responds(RESULT_SUCCESS, self._help, msg)

    def return_missing(self):
        if len(self._missing) == 1:
            msg = "The required parameter: '" + self._missing[0] + "' was not set"
        else:
            msg = "The required parameters: '" + "','".join(self._missing) + "' where not set"
        return _responds(RESULT_ERROR, msg=msg)

    def check_params(self, args, kwargs, key, default, required, type, allowedValues):
        """ function to check passed params for the shorthand wrapper
            and to detect missing/required param
        """
        missing = True
        orgDefault = default

        if type == "bool":
            allowedValues = [0, 1]

        if args:
            default = args[0]
            missing = False
            args = args[1:]
        if kwargs.get(key):
            default = kwargs.get(key)
            missing = False
        if required:
            try:
                self._missing
                self._requiredParams.append(key)
            except AttributeError:
                self._missing = []
                self._requiredParams = {}
                self._requiredParams[key] = {"allowedValues": allowedValues,
                                             "defaultValue": orgDefault}
            if missing and key not in self._missing:
                self._missing.append(key)
        else:
            try:
                self._optionalParams[key] = {"allowedValues": allowedValues,
                                             "defaultValue": orgDefault}
            except AttributeError:
                self._optionalParams = {}
                self._optionalParams[key] = {"allowedValues": allowedValues,
                                             "defaultValue": orgDefault}

        if default:
            default = self._check_param_type(default, key, type)
            if type == "bool":
                type = []
            self._check_param_value(default, key, allowedValues)

        return default, args

    def _check_param_type(self, value, name, type):
        """ checks if value can be converted / parsed to type
            will raise an error on failure
            or will convert it to type and return new converted value
            can check for:
            - int: will be converted into int
            - bool: will be converted to False / True
            - list: will always return a list
            - string: will do nothing for now
            - ignore: will ignore it, just like "string"
        """
        error = False
        if type == "int":
            if _is_int(value):
                value = int(value)
            else:
                error = True
        elif type == "bool":
            if value in ("0", "1"):
                value = bool(int(value))
            elif value in ("true", "True", "TRUE"):
                value = True
            elif value in ("false", "False", "FALSE"):
                value = False
            else:
                error = True
        elif type == "list":
            value = value.split("|")
        elif type == "string":
            pass
        elif type == "ignore":
            pass
        else:
            logger.log(u"API :: Invalid param type set " + str(type) + " can not check or convert ignoring it", logger.ERROR)

        if error:
            # this is a real ApiError !!
            raise ApiError(u"param: '" + str(name) + "' with given value: '" + str(value) + "' could not be parsed into '" + str(type) + "'")

        return value

    def _check_param_value(self, value, name, allowedValues):
        """ will check if value (or all values in it ) are in allowed values
            will raise an exception if value is "out of range"
            if bool(allowedValue) == False a check is not performed and all values are excepted
        """
        if allowedValues:
            error = False
            if isinstance(value, list):
                for item in value:
                    if not item in allowedValues:
                        error = True
            else:
                if not value in allowedValues:
                    error = True

            if error:
                # this is kinda a ApiError but raising an error is the only way of quitting here
                raise ApiError(u"param: '" + str(name) + "' with given value: '" + str(value) + "' is out of allowed range '" + str(allowedValues) + "'")


class TVDBShorthandWrapper(ApiCall):
    _help = {"desc": "this is an internal function wrapper. call the help command directly for more information"}

    def __init__(self, args, kwargs, sid):
        self.origArgs = args
        self.kwargs = kwargs
        self.sid = sid

        self.s, args = self.check_params(args, kwargs, "s", None, False, "ignore", [])
        self.e, args = self.check_params(args, kwargs, "e", None, False, "ignore", [])
        self.args = args

        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ internal function wrapper """
        args = (self.sid,) + self.origArgs
        if self.e:
            return CMD_Episode(args, self.kwargs).run()
        elif self.s:
            return CMD_ShowSeasons(args, self.kwargs).run()
        else:
            return CMD_Show(args, self.kwargs).run()


################################
#     helper functions         #
################################

def _sizeof_fmt(num):
    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:
        if num < 1024.00:
            return "%3.2f %s" % (num, x)
        num /= 1024.00


def _is_int(data):
    try:
        int(data)
    except (TypeError, ValueError, OverflowError):
        return False
    else:
        return True


def _rename_element(dict, oldKey, newKey):
    try:
        dict[newKey] = dict[oldKey]
        del dict[oldKey]
    except (ValueError, TypeError, NameError):
        pass
    return dict


def _responds(result_type, data=None, msg=""):
    """
    result is a string of given "type" (success/failure/timeout/error)
    message is a human readable string, can be empty
    data is either a dict or a array, can be a empty dict or empty array
    """
    if data is None:
        data = {}
    return {"result": result_type_map[result_type],
            "message": msg,
            "data": data}


def _get_quality_string(q):
    qualityString = "Custom"
    if q in qualityPresetStrings:
        qualityString = qualityPresetStrings[q]
    elif q in Quality.qualityStrings:
        qualityString = Quality.qualityStrings[q]
    return qualityString


def _get_status_Strings(s):
    return statusStrings[s]


def _ordinal_to_dateTimeForm(ordinal):
    # workaround for episodes with no airdate
    if int(ordinal) != 1:
        date = datetime.date.fromordinal(ordinal)
    else:
        return ""
    return date.strftime(dateTimeFormat)


def _ordinal_to_dateForm(ordinal):
    if int(ordinal) != 1:
        date = datetime.date.fromordinal(ordinal)
    else:
        return ""
    return date.strftime(dateFormat)


def _historyDate_to_dateTimeForm(timeString):
    date = datetime.datetime.strptime(timeString, history.dateFormat)
    return date.strftime(dateTimeFormat)


def _replace_statusStrings_with_statusCodes(statusStrings):
    statusCodes = []
    if "snatched" in statusStrings:
        statusCodes += Quality.SNATCHED
    if "downloaded" in statusStrings:
        statusCodes += Quality.DOWNLOADED
    if "skipped" in statusStrings:
        statusCodes.append(SKIPPED)
    if "wanted" in statusStrings:
        statusCodes.append(WANTED)
    if "archived" in statusStrings:
        statusCodes.append(ARCHIVED)
    if "ignored" in statusStrings:
        statusCodes.append(IGNORED)
    if "unaired" in statusStrings:
        statusCodes.append(UNAIRED)
    return statusCodes


def _mapQuality(showObj):
    quality_map = _getQualityMap()

    anyQualities = []
    bestQualities = []

    iqualityID, aqualityID = Quality.splitQuality(int(showObj))
    if iqualityID:
        for quality in iqualityID:
            anyQualities.append(quality_map[quality])
    if aqualityID:
        for quality in aqualityID:
            bestQualities.append(quality_map[quality])
    return anyQualities, bestQualities


def _getQualityMap():
    return {Quality.SDTV: 'sdtv',
            Quality.SDDVD: 'sddvd',
            Quality.HDTV: 'hdtv',
            Quality.RAWHDTV: 'rawhdtv',
            Quality.FULLHDTV: 'fullhdtv',
            Quality.HDWEBDL: 'hdwebdl',
            Quality.FULLHDWEBDL: 'fullhdwebdl',
            Quality.HDBLURAY: 'hdbluray',
            Quality.FULLHDBLURAY: 'fullhdbluray',
            Quality.UNKNOWN: 'unknown'}


def _getRootDirs():
    if sickbeard.ROOT_DIRS == "":
        return {}

    rootDir = {}
    root_dirs = sickbeard.ROOT_DIRS.split('|')
    default_index = int(sickbeard.ROOT_DIRS.split('|')[0])

    rootDir["default_index"] = int(sickbeard.ROOT_DIRS.split('|')[0])
    # remove default_index value from list (this fixes the offset)
    root_dirs.pop(0)

    if len(root_dirs) < default_index:
        return {}

    # clean up the list - replace %xx escapes by their single-character equivalent
    root_dirs = [urllib.unquote_plus(x) for x in root_dirs]

    default_dir = root_dirs[default_index]

    dir_list = []
    for root_dir in root_dirs:
        valid = 1
        try:
            ek.ek(os.listdir, root_dir)
        except:
            valid = 0
        default = 0
        if root_dir is default_dir:
            default = 1

        curDir = {}
        curDir['valid'] = valid
        curDir['location'] = root_dir
        curDir['default'] = default
        dir_list.append(curDir)

    return dir_list


class ApiError(Exception):
    "Generic API error"


class IntParseError(Exception):
    "A value could not be parsed into a int. But should be parsable to a int "

#-------------------------------------------------------------------------------------#


class CMD_Help(ApiCall):
    _help = {"desc": "display help information for a given subject/command",
             "optionalParameters": {"subject": {"desc": "command - the top level command"},
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        self.subject, args = self.check_params(args, kwargs, "subject", "help", False, "string", _functionMaper.keys())
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display help information for a given subject/command """
        if self.subject in _functionMaper:
            out = _responds(RESULT_SUCCESS, _functionMaper.get(self.subject)((), {"help": 1}).run())
        else:
            out = _responds(RESULT_FAILURE, msg="No such cmd")
        return out


class CMD_ComingEpisodes(ApiCall):
    _help = {"desc": "display the coming episodes",
             "optionalParameters": {"sort": {"desc": "change the sort order"},
                                   "type": {"desc": "one or more of allowedValues separated by |"},
                                   "paused": {"desc": "0 to exclude paused shows, 1 to include them, or omitted to use the SB default"},
                                   }
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        self.sort, args = self.check_params(args, kwargs, "sort", "date", False, "string", ["date", "show", "network"])
        self.type, args = self.check_params(args, kwargs, "type", "today|missed|soon|later", False, "list", ["missed", "later", "today", "soon"])
        self.paused, args = self.check_params(args, kwargs, "paused", sickbeard.COMING_EPS_DISPLAY_PAUSED, False, "int", [0, 1])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display the coming episodes """
        today = datetime.date.today().toordinal()
        next_week = (datetime.date.today() + datetime.timedelta(days=7)).toordinal()
        recently = (datetime.date.today() - datetime.timedelta(days=3)).toordinal()

        done_show_list = []
        qualList = Quality.DOWNLOADED + Quality.SNATCHED + [ARCHIVED, IGNORED]

        myDB = db.DBConnection(row_type="dict")
        sql_results = myDB.select("SELECT airdate, airs, episode, name AS 'ep_name', description AS 'ep_plot', network, season, showid AS 'tvdbid', show_name, tv_shows.quality AS quality, tv_shows.status AS 'show_status', tv_shows.paused AS 'paused' FROM tv_episodes, tv_shows WHERE season != 0 AND airdate >= ? AND airdate < ? AND tv_shows.tvdb_id = tv_episodes.showid AND tv_episodes.status NOT IN (" + ','.join(['?'] * len(qualList)) + ")", [today, next_week] + qualList)
        for cur_result in sql_results:
            done_show_list.append(int(cur_result["tvdbid"]))

        more_sql_results = myDB.select("SELECT airdate, airs, episode, name AS 'ep_name', description AS 'ep_plot', network, season, showid AS 'tvdbid', show_name, tv_shows.quality AS quality, tv_shows.status AS 'show_status', tv_shows.paused AS 'paused' FROM tv_episodes outer_eps, tv_shows WHERE season != 0 AND showid NOT IN (" + ','.join(['?'] * len(done_show_list)) + ") AND tv_shows.tvdb_id = outer_eps.showid AND airdate = (SELECT airdate FROM tv_episodes inner_eps WHERE inner_eps.season != 0 AND inner_eps.showid = outer_eps.showid AND inner_eps.airdate >= ? ORDER BY inner_eps.airdate ASC LIMIT 1) AND outer_eps.status NOT IN (" + ','.join(['?'] * len(Quality.DOWNLOADED + Quality.SNATCHED)) + ")", done_show_list + [next_week] + Quality.DOWNLOADED + Quality.SNATCHED)
        sql_results += more_sql_results

        more_sql_results = myDB.select("SELECT airdate, airs, episode, name AS 'ep_name', description AS 'ep_plot', network, season, showid AS 'tvdbid', show_name, tv_shows.quality AS quality, tv_shows.status AS 'show_status', tv_shows.paused AS 'paused' FROM tv_episodes, tv_shows WHERE season != 0 AND tv_shows.tvdb_id = tv_episodes.showid AND airdate < ? AND airdate >= ? AND tv_episodes.status = ? AND tv_episodes.status NOT IN (" + ','.join(['?'] * len(qualList)) + ")", [today, recently, WANTED] + qualList)
        sql_results += more_sql_results

        # sort by air date
        sorts = {
            'date': (lambda x, y: cmp(int(x["airdate"]), int(y["airdate"]))),
            'show': (lambda a, b: cmp(a["show_name"], b["show_name"])),
            'network': (lambda a, b: cmp(a["network"], b["network"])),
        }

        sql_results.sort(sorts[self.sort])
        finalEpResults = {}

        # add all requested types or all
        for curType in self.type:
            finalEpResults[curType] = []

        for ep in sql_results:
            """
                Missed:   yesterday... (less than 1week)
                Today:    today
                Soon:     tomorrow till next week
                Later:    later than next week
            """

            if ep["paused"] and not self.paused:
                continue

            status = "soon"
            if ep["airdate"] < today:
                status = "missed"
            elif ep["airdate"] >= next_week:
                status = "later"
            elif ep["airdate"] >= today and ep["airdate"] < next_week:
                if ep["airdate"] == today:
                    status = "today"
                else:
                    status = "soon"

            # skip unwanted
            if self.type != None and not status in self.type:
                continue

            ordinalAirdate = int(ep["airdate"])
            if not ep["network"]:
                ep["network"] = ""
            ep["airdate"] = _ordinal_to_dateForm(ordinalAirdate)
            ep["quality"] = _get_quality_string(ep["quality"])
            # clean up tvdb horrible airs field
            ep["airs"] = str(ep["airs"]).replace('am', ' AM').replace('pm', ' PM').replace('  ', ' ')
            # start day of the week on 1 (monday)
            ep["weekday"] = 1 + datetime.date.fromordinal(ordinalAirdate).weekday()

            # TODO: check if this obsolete
            if not status in finalEpResults:
                finalEpResults[status] = []

            finalEpResults[status].append(ep)
        myDB.connection.close()
        return _responds(RESULT_SUCCESS, finalEpResults)


class CMD_Episode(ApiCall):
    _help = {"desc": "display detailed info about an episode",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                   "season": {"desc": "the season number"},
                                   "episode": {"desc": "the episode number"}
                                  },
             "optionalParameters": {"full_path": {"desc": "show the full absolute path (if valid) instead of a relative path for the episode location"}
                                     }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        self.s, args = self.check_params(args, kwargs, "season", None, True, "int", [])
        self.e, args = self.check_params(args, kwargs, "episode", None, True, "int", [])
        # optional
        self.fullPath, args = self.check_params(args, kwargs, "full_path", 0, False, "bool", [])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display detailed info about an episode """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        myDB = db.DBConnection(row_type="dict")
        sqlResults = myDB.select("SELECT name, description, airdate, status, location, file_size, release_name FROM tv_episodes WHERE showid = ? AND episode = ? AND season = ?", [self.tvdbid, self.e, self.s])
        if not len(sqlResults) == 1:
            raise ApiError("Episode not found")
        episode = sqlResults[0]
        # handle path options
        # absolute vs relative vs broken
        showPath = None
        try:
            showPath = showObj.location
        except sickbeard.exceptions.ShowDirNotFoundException:
            pass

        if bool(self.fullPath) == True and showPath:
            pass
        elif bool(self.fullPath) == False and showPath:
            # using the length because lstrip removes to much
            showPathLength = len(showPath) + 1  # the / or \ yeah not that nice i know
            episode["location"] = episode["location"][showPathLength:]
        elif not showPath:  # show dir is broken ... episode path will be empty
            episode["location"] = ""
        # convert stuff to human form
        episode["airdate"] = _ordinal_to_dateForm(episode["airdate"])
        status, quality = Quality.splitCompositeStatus(int(episode["status"]))
        episode["status"] = _get_status_Strings(status)
        episode["quality"] = _get_quality_string(quality)
        episode["file_size_human"] = _sizeof_fmt(episode["file_size"])

        myDB.connection.close()
        return _responds(RESULT_SUCCESS, episode)


class CMD_EpisodeSearch(ApiCall):
    _help = {"desc": "search for an episode. the response might take some time",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                   "season": {"desc": "the season number"},
                                   "episode": {"desc": "the episode number"}
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        self.s, args = self.check_params(args, kwargs, "season", None, True, "int", [])
        self.e, args = self.check_params(args, kwargs, "episode", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ search for an episode """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        # retrieve the episode object and fail if we can't get one
        epObj = showObj.getEpisode(int(self.s), int(self.e))
        if isinstance(epObj, str):
            return _responds(RESULT_FAILURE, msg="Episode not found")

        # make a queue item for it and put it on the queue
        ep_queue_item = search_queue.ManualSearchQueueItem(epObj)
        sickbeard.searchQueueScheduler.action.add_item(ep_queue_item)  # @UndefinedVariable

        # wait until the queue item tells us whether it worked or not
        while ep_queue_item.success == None:  # @UndefinedVariable
            time.sleep(1)

        # return the correct json value
        if ep_queue_item.success:
            status, quality = Quality.splitCompositeStatus(epObj.status)  # @UnusedVariable
            # TODO: split quality and status?
            return _responds(RESULT_SUCCESS, {"quality": _get_quality_string(quality)}, "Snatched (" + _get_quality_string(quality) + ")")

        return _responds(RESULT_FAILURE, msg='Unable to find episode')


class CMD_EpisodeSetStatus(ApiCall):
    _help = {"desc": "set status of an episode or season (when no ep is provided)",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                   "season": {"desc": "the season number"},
                                   "status": {"desc": "the status values: wanted, skipped, archived, ignored"}
                                  },
             "optionalParameters": {"episode": {"desc": "the episode number"},
                                    "force": {"desc": "should we replace existing (downloaded) episodes or not"}
                                     }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        self.s, args = self.check_params(args, kwargs, "season", None, True, "int", [])
        self.status, args = self.check_params(args, kwargs, "status", None, True, "string", ["wanted", "skipped", "archived", "ignored"])
        # optional
        self.e, args = self.check_params(args, kwargs, "episode", None, False, "int", [])
        self.force, args = self.check_params(args, kwargs, "force", 0, False, "bool", [])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ set status of an episode or a season (when no ep is provided) """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        # convert the string status to a int
        for status in statusStrings.statusStrings:
            if str(statusStrings[status]).lower() == str(self.status).lower():
                self.status = status
                break
        # if we dont break out of the for loop we got here.
        else:
            # the allowed values has at least one item that could not be matched against the internal status strings
            raise ApiError("The status string could not be matched to a status. Report to Devs!")

        ep_list = []
        if self.e:
            epObj = showObj.getEpisode(self.s, self.e)
            if epObj == None:
                return _responds(RESULT_FAILURE, msg="Episode not found")
            ep_list = [epObj]
        else:
            # get all episode numbers frome self,season
            ep_list = showObj.getAllEpisodes(season=self.s)

        def _epResult(result_code, ep, msg=""):
            return {'season': ep.season, 'episode': ep.episode, 'status': _get_status_Strings(ep.status), 'result': result_type_map[result_code], 'message': msg}

        ep_results = []
        failure = False
        start_backlog = False
        ep_segment = None
        for epObj in ep_list:
            if ep_segment == None and self.status == WANTED:
                # figure out what segment the episode is in and remember it so we can backlog it
                if showObj.air_by_date:
                    ep_segment = str(epObj.airdate)[:7]
                else:
                    ep_segment = epObj.season

            with epObj.lock:
                # don't let them mess up UNAIRED episodes
                if epObj.status == UNAIRED:
                    # setting the status of a unaired is only considert a failure if we directly wanted this episode, but is ignored on a season request
                    if self.e != None:
                        ep_results.append(_epResult(RESULT_FAILURE, epObj, "Refusing to change status because it is UNAIRED"))
                        failure = True
                    continue

                # allow the user to force setting the status for an already downloaded episode
                if epObj.status in Quality.DOWNLOADED and not self.force:
                    ep_results.append(_epResult(RESULT_FAILURE, epObj, "Refusing to change status because it is already marked as DOWNLOADED"))
                    failure = True
                    continue

                epObj.status = self.status
                epObj.saveToDB()

                if self.status == WANTED:
                    start_backlog = True
                ep_results.append(_epResult(RESULT_SUCCESS, epObj))

        extra_msg = ""
        if start_backlog:
            cur_backlog_queue_item = search_queue.BacklogQueueItem(showObj, ep_segment)
            sickbeard.searchQueueScheduler.action.add_item(cur_backlog_queue_item)  # @UndefinedVariable
            logger.log(u"API :: Starting backlog for " + showObj.name + " season " + str(ep_segment) + " because some episodes were set to WANTED")
            extra_msg = " Backlog started"

        if failure:
            return _responds(RESULT_FAILURE, ep_results, 'Failed to set all or some status. Check data.' + extra_msg)
        else:
            return _responds(RESULT_SUCCESS, msg='All status set successfully.' + extra_msg)


class CMD_Exceptions(ApiCall):
    _help = {"desc": "display scene exceptions for all or a given show",
             "optionalParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, False, "int", [])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display scene exceptions for all or a given show """
        myDB = db.DBConnection("cache.db", row_type="dict")

        if self.tvdbid == None:
            sqlResults = myDB.select("SELECT show_name, tvdb_id AS 'tvdbid' FROM scene_exceptions")
            scene_exceptions = {}
            for row in sqlResults:
                tvdbid = row["tvdbid"]
                if not tvdbid in scene_exceptions:
                    scene_exceptions[tvdbid] = []
                scene_exceptions[tvdbid].append(row["show_name"])

        else:
            showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
            if not showObj:
                return _responds(RESULT_FAILURE, msg="Show not found")

            sqlResults = myDB.select("SELECT show_name, tvdb_id AS 'tvdbid' FROM scene_exceptions WHERE tvdb_id = ?", [self.tvdbid])
            scene_exceptions = []
            for row in sqlResults:
                scene_exceptions.append(row["show_name"])

        myDB.connection.close()
        return _responds(RESULT_SUCCESS, scene_exceptions)


class CMD_History(ApiCall):
    _help = {"desc": "display sickbeard downloaded/snatched history",
             "optionalParameters": {"limit": {"desc": "limit returned results"},
                                    "type": {"desc": "only show a specific type of results"},
                                   }
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        self.limit, args = self.check_params(args, kwargs, "limit", 100, False, "int", [])
        self.type, args = self.check_params(args, kwargs, "type", None, False, "string", ["downloaded", "snatched"])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display sickbeard downloaded/snatched history """

        typeCodes = []
        if self.type == "downloaded":
            self.type = "Downloaded"
            typeCodes = Quality.DOWNLOADED
        elif self.type == "snatched":
            self.type = "Snatched"
            typeCodes = Quality.SNATCHED
        else:
            typeCodes = Quality.SNATCHED + Quality.DOWNLOADED

        myDB = db.DBConnection(row_type="dict")

        ulimit = min(int(self.limit), 100)
        if ulimit == 0:
            sqlResults = myDB.select("SELECT h.*, show_name FROM history h, tv_shows s WHERE h.showid=s.tvdb_id AND action in (" + ','.join(['?'] * len(typeCodes)) + ") ORDER BY date DESC", typeCodes)
        else:
            sqlResults = myDB.select("SELECT h.*, show_name FROM history h, tv_shows s WHERE h.showid=s.tvdb_id AND action in (" + ','.join(['?'] * len(typeCodes)) + ") ORDER BY date DESC LIMIT ?", typeCodes + [ulimit])

        results = []
        for row in sqlResults:
            status, quality = Quality.splitCompositeStatus(int(row["action"]))
            status = _get_status_Strings(status)
            if self.type and not status == self.type:
                continue
            row["status"] = status
            row["quality"] = _get_quality_string(quality)
            row["date"] = _historyDate_to_dateTimeForm(str(row["date"]))
            del row["action"]
            _rename_element(row, "showid", "tvdbid")
            row["resource_path"] = os.path.dirname(row["resource"])
            row["resource"] = os.path.basename(row["resource"])
            results.append(row)

        myDB.connection.close()
        return _responds(RESULT_SUCCESS, results)


class CMD_HistoryClear(ApiCall):
    _help = {"desc": "clear sickbeard's history",
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ clear sickbeard's history """
        myDB = db.DBConnection()
        myDB.action("DELETE FROM history WHERE 1=1")

        myDB.connection.close()
        return _responds(RESULT_SUCCESS, msg="History cleared")


class CMD_HistoryTrim(ApiCall):
    _help = {"desc": "trim sickbeard's history by removing entries greater than 30 days old"
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ trim sickbeard's history """
        myDB = db.DBConnection()
        myDB.action("DELETE FROM history WHERE date < " + str((datetime.datetime.today() - datetime.timedelta(days=30)).strftime(history.dateFormat)))

        myDB.connection.close()
        return _responds(RESULT_SUCCESS, msg="Removed history entries greater than 30 days old")


class CMD_Logs(ApiCall):
    _help = {"desc": "view sickbeard's log",
             "optionalParameters": {"min_level ": {"desc": "the minimum level classification of log entries to show, with each level inherting its above level"} }
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        self.min_level, args = self.check_params(args, kwargs, "min_level", "error", False, "string", ["error", "warning", "info", "debug"])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ view sickbeard's log """
        # 10 = Debug / 20 = Info / 30 = Warning / 40 = Error
        minLevel = logger.reverseNames[str(self.min_level).upper()]

        data = []
        if os.path.isfile(logger.sb_log_instance.log_file_path):
            with ek.ek(open, logger.sb_log_instance.log_file_path) as f:
                data = f.readlines()

        regex = "^(\d\d\d\d)\-(\d\d)\-(\d\d)\s*(\d\d)\:(\d\d):(\d\d)\s*([A-Z]+)\s*(.+?)\s*\:\:\s*(.*)$"

        finalData = []

        numLines = 0
        lastLine = False
        numToShow = min(50, len(data))

        for x in reversed(data):

            x = x.decode('utf-8')
            match = re.match(regex, x)

            if match:
                level = match.group(7)
                if level not in logger.reverseNames:
                    lastLine = False
                    continue

                if logger.reverseNames[level] >= minLevel:
                    lastLine = True
                    finalData.append(x.rstrip("\n"))
                else:
                    lastLine = False
                    continue

            elif lastLine:
                finalData.append("AA" + x)

            numLines += 1

            if numLines >= numToShow:
                break

        return _responds(RESULT_SUCCESS, finalData)


class CMD_SickBeard(ApiCall):
    _help = {"desc": "display misc sickbeard related information"}

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display misc sickbeard related information """
        data = {"sb_version": sickbeard.version.SICKBEARD_VERSION, "api_version": Api.version, "api_commands": sorted(_functionMaper.keys())}
        return _responds(RESULT_SUCCESS, data)


class CMD_SickBeardAddRootDir(ApiCall):
    _help = {"desc": "add a sickbeard user's parent directory",
             "requiredParameters": {"location": {"desc": "the full path to root (parent) directory"}
                                    },
             "optionalParameters": {"default": {"desc": "make the location passed the default root (parent) directory"}
                                    }
             }

    def __init__(self, args, kwargs):
        # required
        self.location, args = self.check_params(args, kwargs, "location", None, True, "string", [])
        # optional
        self.default, args = self.check_params(args, kwargs, "default", 0, False, "bool", [])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ add a parent directory to sickbeard's config """

        self.location = urllib.unquote_plus(self.location)
        location_matched = 0

        # dissallow adding/setting an invalid dir
        if not ek.ek(os.path.isdir, self.location):
            return _responds(RESULT_FAILURE, msg="Location is invalid")

        root_dirs = []

        if sickbeard.ROOT_DIRS == "":
            self.default = 1
        else:
            root_dirs = sickbeard.ROOT_DIRS.split('|')
            index = int(sickbeard.ROOT_DIRS.split('|')[0])
            root_dirs.pop(0)
            # clean up the list - replace %xx escapes by their single-character equivalent
            root_dirs = [urllib.unquote_plus(x) for x in root_dirs]
            for x in root_dirs:
                if(x == self.location):
                    location_matched = 1
                    if (self.default == 1):
                        index = root_dirs.index(self.location)
                    break

        if(location_matched == 0):
            if (self.default == 1):
                index = 0
                root_dirs.insert(0, self.location)
            else:
                root_dirs.append(self.location)

        root_dirs_new = [urllib.unquote_plus(x) for x in root_dirs]
        root_dirs_new.insert(0, index)
        root_dirs_new = '|'.join(unicode(x) for x in root_dirs_new)

        sickbeard.ROOT_DIRS = root_dirs_new
        return _responds(RESULT_SUCCESS, _getRootDirs(), msg="Root directories updated")


class CMD_SickBeardCheckScheduler(ApiCall):
    _help = {"desc": "query the scheduler"}

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ query the scheduler """
        myDB = db.DBConnection()
        sqlResults = myDB.select("SELECT last_backlog FROM info")

        backlogPaused = sickbeard.searchQueueScheduler.action.is_backlog_paused()  # @UndefinedVariable
        backlogRunning = sickbeard.searchQueueScheduler.action.is_backlog_in_progress()  # @UndefinedVariable
        searchStatus = sickbeard.currentSearchScheduler.action.amActive  # @UndefinedVariable
        nextSearch = str(sickbeard.currentSearchScheduler.timeLeft()).split('.')[0]
        nextBacklog = sickbeard.backlogSearchScheduler.nextRun().strftime(dateFormat).decode(sickbeard.SYS_ENCODING)

        myDB.connection.close()
        data = {"backlog_is_paused": int(backlogPaused), "backlog_is_running": int(backlogRunning), "last_backlog": _ordinal_to_dateForm(sqlResults[0]["last_backlog"]), "search_is_running": int(searchStatus), "next_search": nextSearch, "next_backlog": nextBacklog}
        return _responds(RESULT_SUCCESS, data)


class CMD_SickBeardDeleteRootDir(ApiCall):
    _help = {"desc": "delete a sickbeard user's parent directory",
             "requiredParameters": {"location": {"desc": "the full path to root (parent) directory"} }
             }

    def __init__(self, args, kwargs):
        # required
        self.location, args = self.check_params(args, kwargs, "location", None, True, "string", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ delete a parent directory from sickbeard's config """
        if sickbeard.ROOT_DIRS == "":
            return _responds(RESULT_FAILURE, _getRootDirs(), msg="No root directories detected")

        root_dirs_new = []
        root_dirs = sickbeard.ROOT_DIRS.split('|')
        index = int(root_dirs[0])
        root_dirs.pop(0)
        # clean up the list - replace %xx escapes by their single-character equivalent
        root_dirs = [urllib.unquote_plus(x) for x in root_dirs]
        old_root_dir = root_dirs[index]
        for curRootDir in root_dirs:
            if not curRootDir == self.location:
                root_dirs_new.append(curRootDir)
            else:
                newIndex = 0

        for curIndex, curNewRootDir in enumerate(root_dirs_new):
            if curNewRootDir is old_root_dir:
                newIndex = curIndex
                break

        root_dirs_new = [urllib.unquote_plus(x) for x in root_dirs_new]
        if len(root_dirs_new) > 0:
            root_dirs_new.insert(0, newIndex)
        root_dirs_new = "|".join(unicode(x) for x in root_dirs_new)

        sickbeard.ROOT_DIRS = root_dirs_new
        # what if the root dir was not found?
        return _responds(RESULT_SUCCESS, _getRootDirs(), msg="Root directory deleted")


class CMD_SickBeardForceSearch(ApiCall):
    _help = {"desc": "force the episode search early"
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ force the episode search early """
        # Changing all old missing episodes to status WANTED
        # Beginning search for new episodes on RSS
        # Searching all providers for any needed episodes
        result = sickbeard.currentSearchScheduler.forceRun()
        if result:
            return _responds(RESULT_SUCCESS, msg="Episode search forced")
        return _responds(RESULT_FAILURE, msg="Can not search for episode")


class CMD_SickBeardGetDefaults(ApiCall):
    _help = {"desc": "get sickbeard user defaults"}

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ get sickbeard user defaults """

        anyQualities, bestQualities = _mapQuality(sickbeard.QUALITY_DEFAULT)

        data = {"status": statusStrings[sickbeard.STATUS_DEFAULT].lower(), "flatten_folders": int(sickbeard.FLATTEN_FOLDERS_DEFAULT), "initial": anyQualities, "archive": bestQualities, "future_show_paused": int(sickbeard.COMING_EPS_DISPLAY_PAUSED) }
        return _responds(RESULT_SUCCESS, data)


class CMD_SickBeardGetMessages(ApiCall):
    _help = {"desc": "get all messages"}

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        messages = []
        for cur_notification in ui.notifications.get_notifications():
            messages.append({"title": cur_notification.title,
                           "message": cur_notification.message,
                           "type": cur_notification.type})
        return _responds(RESULT_SUCCESS, messages)


class CMD_SickBeardGetRootDirs(ApiCall):
    _help = {"desc": "get sickbeard user parent directories"}

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ get the parent directories defined in sickbeard's config """

        return _responds(RESULT_SUCCESS, _getRootDirs())


class CMD_SickBeardPauseBacklog(ApiCall):
    _help = {"desc": "pause the backlog search",
             "optionalParameters": {"pause ": {"desc": "pause or unpause the global backlog"} }
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        self.pause, args = self.check_params(args, kwargs, "pause", 0, False, "bool", [])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ pause the backlog search """
        if self.pause == True:
            sickbeard.searchQueueScheduler.action.pause_backlog()  # @UndefinedVariable
            return _responds(RESULT_SUCCESS, msg="Backlog paused")
        else:
            sickbeard.searchQueueScheduler.action.unpause_backlog()  # @UndefinedVariable
            return _responds(RESULT_SUCCESS, msg="Backlog unpaused")


class CMD_SickBeardPing(ApiCall):
    _help = {"desc": "check to see if sickbeard is running"}

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ check to see if sickbeard is running """
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"
        if sickbeard.started:
            return _responds(RESULT_SUCCESS, {"pid": sickbeard.PID}, "Pong")
        else:
            return _responds(RESULT_SUCCESS, msg="Pong")


class CMD_SickBeardRestart(ApiCall):
    _help = {"desc": "restart sickbeard"}

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ restart sickbeard """
        threading.Timer(2, sickbeard.invoke_restart, [False]).start()
        return _responds(RESULT_SUCCESS, msg="SickBeard is restarting...")


class CMD_SickBeardSearchTVDB(ApiCall):
    _help = {"desc": "search for show at tvdb with a given string and language",
             "optionalParameters": {"name": {"desc": "name of the show you want to search for"},
                                   "tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                     "lang": {"desc": "the 2 letter abbreviation lang id"}
                                     }
             }

    valid_languages = {
            'el': 20, 'en': 7, 'zh': 27, 'it': 15, 'cs': 28, 'es': 16, 'ru': 22,
            'nl': 13, 'pt': 26, 'no': 9, 'tr': 21, 'pl': 18, 'fr': 17, 'hr': 31,
            'de': 14, 'da': 10, 'fi': 11, 'hu': 19, 'ja': 25, 'he': 24, 'ko': 32,
            'sv': 8, 'sl': 30}

    def __init__(self, args, kwargs):
        # required
        # optional
        self.name, args = self.check_params(args, kwargs, "name", None, False, "string", [])
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, False, "int", [])
        self.lang, args = self.check_params(args, kwargs, "lang", "en", False, "string", self.valid_languages.keys())
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ search for show at tvdb with a given string and language """
        # only name was given
        if self.name and not self.tvdbid:
            baseURL = "http://thetvdb.com/api/GetSeries.php?"
            params = {"seriesname": str(self.name).encode('utf-8'), 'language': self.lang}
            finalURL = baseURL + urllib.urlencode(params)
            urlData = sickbeard.helpers.getURL(finalURL)

            if urlData is None:
                return _responds(RESULT_FAILURE, msg="Did not get result from tvdb")
            else:
                try:
                    seriesXML = etree.ElementTree(etree.XML(urlData))
                except Exception, e:
                    logger.log(u"API :: Unable to parse XML for some reason: " + ex(e) + " from XML: " + urlData, logger.ERROR)
                    return _responds(RESULT_FAILURE, msg="Unable to read result from tvdb")

                series = seriesXML.getiterator('Series')
                results = []
                for curSeries in series:
                    results.append({"tvdbid": int(curSeries.findtext('seriesid')),
                                    "name": curSeries.findtext('SeriesName'),
                                    "first_aired": curSeries.findtext('FirstAired')})

                lang_id = self.valid_languages[self.lang]
                return _responds(RESULT_SUCCESS, {"results": results, "langid": lang_id})

        elif self.tvdbid:
            # There's gotta be a better way of doing this but we don't wanna
            # change the language value elsewhere
            ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()

            lang_id = self.valid_languages[self.lang]
            if self.lang and not self.lang == 'en':
                ltvdb_api_parms['language'] = self.lang

            t = tvdb_api.Tvdb(actors=False, **ltvdb_api_parms)

            try:
                myShow = t[int(self.tvdbid)]
            except (tvdb_exceptions.tvdb_shownotfound, tvdb_exceptions.tvdb_error):
                logger.log(u"API :: Unable to find show with id " + str(self.tvdbid), logger.WARNING)
                return _responds(RESULT_SUCCESS, {"results": [], "langid": lang_id})

            if not myShow.data['seriesname']:
                logger.log(u"API :: Found show with tvdbid " + str(self.tvdbid) + ", however it contained no show name", logger.DEBUG)
                return _responds(RESULT_FAILURE, msg="Show contains no name, invalid result")

            showOut = [{"tvdbid": self.tvdbid,
                       "name": unicode(myShow.data['seriesname']),
                       "first_aired": myShow.data['firstaired']}]

            return _responds(RESULT_SUCCESS, {"results": showOut, "langid": lang_id})
        else:
            return _responds(RESULT_FAILURE, msg="Either tvdbid or name is required")


class CMD_SickBeardSetDefaults(ApiCall):
    _help = {"desc": "set sickbeard user defaults",
             "optionalParameters": {"initial": {"desc": "initial quality for the show"},
                                    "archive": {"desc": "archive quality for the show"},
                                    "flatten_folders": {"desc": "flatten subfolders within the show directory"},
                                    "status": {"desc": "status of missing episodes"}
                                    }
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        self.initial, args = self.check_params(args, kwargs, "initial", None, False, "list", ["sdtv", "sddvd", "hdtv", "rawhdtv", "fullhdtv", "hdwebdl", "fullhdwebdl", "hdbluray", "fullhdbluray", "unknown"])
        self.archive, args = self.check_params(args, kwargs, "archive", None, False, "list", ["sddvd", "hdtv", "rawhdtv", "fullhdtv", "hdwebdl", "fullhdwebdl", "hdbluray", "fullhdbluray"])
        self.future_show_paused, args = self.check_params(args, kwargs, "future_show_paused", None, False, "bool", [])
        self.flatten_folders, args = self.check_params(args, kwargs, "flatten_folders", None, False, "bool", [])
        self.status, args = self.check_params(args, kwargs, "status", None, False, "string", ["wanted", "skipped", "archived", "ignored"])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ set sickbeard user defaults """

        quality_map = {'sdtv': Quality.SDTV,
                       'sddvd': Quality.SDDVD,
                       'hdtv': Quality.HDTV,
                       'rawhdtv': Quality.RAWHDTV,
                       'fullhdtv': Quality.FULLHDTV,
                       'hdwebdl': Quality.HDWEBDL,
                       'fullhdwebdl': Quality.FULLHDWEBDL,
                       'hdbluray': Quality.HDBLURAY,
                       'fullhdbluray': Quality.FULLHDBLURAY,
                       'unknown': Quality.UNKNOWN}

        iqualityID = []
        aqualityID = []

        if self.initial:
            for quality in self.initial:
                iqualityID.append(quality_map[quality])
        if self.archive:
            for quality in self.archive:
                aqualityID.append(quality_map[quality])

        if iqualityID or aqualityID:
            sickbeard.QUALITY_DEFAULT = Quality.combineQualities(iqualityID, aqualityID)

        if self.status:
            # convert the string status to a int
            for status in statusStrings.statusStrings:
                if statusStrings[status].lower() == str(self.status).lower():
                    self.status = status
                    break
            # this should be obsolete bcause of the above
            if not self.status in statusStrings.statusStrings:
                raise ApiError("Invalid Status")
            #only allow the status options we want
            if int(self.status) not in (3, 5, 6, 7):
                raise ApiError("Status Prohibited")
            sickbeard.STATUS_DEFAULT = self.status

        if self.flatten_folders != None:
            sickbeard.FLATTEN_FOLDERS_DEFAULT = int(self.flatten_folders)

        if self.future_show_paused != None:
            sickbeard.COMING_EPS_DISPLAY_PAUSED = int(self.future_show_paused)

        return _responds(RESULT_SUCCESS, msg="Saved defaults")


class CMD_SickBeardShutdown(ApiCall):
    _help = {"desc": "shutdown sickbeard"}

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ shutdown sickbeard """
        threading.Timer(2, sickbeard.invoke_shutdown).start()
        return _responds(RESULT_SUCCESS, msg="SickBeard is shutting down...")


class CMD_Show(ApiCall):
    _help = {"desc": "display information for a given show",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display information for a given show """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        showDict = {}
        showDict["season_list"] = CMD_ShowSeasonList((), {"tvdbid": self.tvdbid}).run()["data"]
        showDict["cache"] = CMD_ShowCache((), {"tvdbid": self.tvdbid}).run()["data"]

        genreList = []
        if showObj.genre:
            genreListTmp = showObj.genre.split("|")
            for genre in genreListTmp:
                if genre:
                    genreList.append(genre)
        showDict["genre"] = genreList
        showDict["quality"] = _get_quality_string(showObj.quality)

        anyQualities, bestQualities = _mapQuality(showObj.quality)
        showDict["quality_details"] = {"initial": anyQualities, "archive": bestQualities}

        try:
            showDict["location"] = showObj.location
        except sickbeard.exceptions.ShowDirNotFoundException:
            showDict["location"] = ""

        showDict["language"] = showObj.lang
        showDict["show_name"] = showObj.name
        showDict["paused"] = showObj.paused
        showDict["air_by_date"] = showObj.air_by_date
        showDict["flatten_folders"] = showObj.flatten_folders
        #clean up tvdb horrible airs field
        showDict["airs"] = str(showObj.airs).replace('am', ' AM').replace('pm', ' PM').replace('  ', ' ')
        showDict["tvrage_id"] = showObj.tvrid
        showDict["tvrage_name"] = showObj.tvrname
        showDict["network"] = showObj.network
        if not showDict["network"]:
            showDict["network"] = ""
        showDict["status"] = showObj.status

        nextAirdate = ''
        nextEps = showObj.nextEpisode()
        if (len(nextEps) != 0):
            nextAirdate = _ordinal_to_dateForm(nextEps[0].airdate.toordinal())
        showDict["next_ep_airdate"] = nextAirdate

        return _responds(RESULT_SUCCESS, showDict)


class CMD_ShowAddExisting(ApiCall):
    _help = {"desc": "add a show in sickbeard with an existing folder",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                    "location": {"desc": "full path to the existing folder for the show"}
                                },
             "optionalParameters": {"initial": {"desc": "initial quality for the show"},
                                    "archive": {"desc": "archive quality for the show"},
                                    "flatten_folders": {"desc": "flatten subfolders for the show"}
                                    }
             }

    def __init__(self, args, kwargs):
        # required
        self.location, args = self.check_params(args, kwargs, "location", None, True, "string", [])
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        self.initial, args = self.check_params(args, kwargs, "initial", None, False, "list", ["sdtv", "sddvd", "hdtv", "rawhdtv", "fullhdtv", "hdwebdl", "fullhdwebdl", "hdbluray", "fullhdbluray", "unknown"])
        self.archive, args = self.check_params(args, kwargs, "archive", None, False, "list", ["sddvd", "hdtv", "rawhdtv", "fullhdtv", "hdwebdl", "fullhdwebdl", "hdbluray", "fullhdbluray"])
        self.flatten_folders, args = self.check_params(args, kwargs, "flatten_folders", str(sickbeard.FLATTEN_FOLDERS_DEFAULT), False, "bool", [])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ add a show in sickbeard with an existing folder """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if showObj:
            return _responds(RESULT_FAILURE, msg="An existing tvdbid already exists in the database")

        if not ek.ek(os.path.isdir, self.location):
            return _responds(RESULT_FAILURE, msg='Not a valid location')

        tvdbName = None
        tvdbResult = CMD_SickBeardSearchTVDB([], {"tvdbid": self.tvdbid}).run()

        if tvdbResult['result'] == result_type_map[RESULT_SUCCESS]:
            if not tvdbResult['data']['results']:
                return _responds(RESULT_FAILURE, msg="Empty results returned, check tvdbid and try again")
            if len(tvdbResult['data']['results']) == 1 and 'name' in tvdbResult['data']['results'][0]:
                tvdbName = tvdbResult['data']['results'][0]['name']

        if not tvdbName:
            return _responds(RESULT_FAILURE, msg="Unable to retrieve information from tvdb")

        quality_map = {'sdtv': Quality.SDTV,
                       'sddvd': Quality.SDDVD,
                       'hdtv': Quality.HDTV,
                       'rawhdtv': Quality.RAWHDTV,
                       'fullhdtv': Quality.FULLHDTV,
                       'hdwebdl': Quality.HDWEBDL,
                       'fullhdwebdl': Quality.FULLHDWEBDL,
                       'hdbluray': Quality.HDBLURAY,
                       'fullhdbluray': Quality.FULLHDBLURAY,
                       'unknown': Quality.UNKNOWN}

        #use default quality as a failsafe
        newQuality = int(sickbeard.QUALITY_DEFAULT)
        iqualityID = []
        aqualityID = []

        if self.initial:
            for quality in self.initial:
                iqualityID.append(quality_map[quality])
        if self.archive:
            for quality in self.archive:
                aqualityID.append(quality_map[quality])

        if iqualityID or aqualityID:
            newQuality = Quality.combineQualities(iqualityID, aqualityID)

        sickbeard.showQueueScheduler.action.addShow(int(self.tvdbid), self.location, SKIPPED, newQuality, int(self.flatten_folders))  # @UndefinedVariable
        return _responds(RESULT_SUCCESS, {"name": tvdbName}, tvdbName + " has been queued to be added")


class CMD_ShowAddNew(ApiCall):
    _help = {"desc": "add a new show to sickbeard",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"}
                                },
             "optionalParameters": {"initial": {"desc": "initial quality for the show"},
                                    "location": {"desc": "base path for where the show folder is to be created"},
                                    "archive": {"desc": "archive quality for the show"},
                                    "flatten_folders": {"desc": "flatten subfolders for the show"},
                                    "status": {"desc": "status of missing episodes"},
                                    "lang": {"desc": "the 2 letter lang abbreviation id"}
                                    }
             }

    valid_languages = {
            'el': 20, 'en': 7, 'zh': 27, 'it': 15, 'cs': 28, 'es': 16, 'ru': 22,
            'nl': 13, 'pt': 26, 'no': 9, 'tr': 21, 'pl': 18, 'fr': 17, 'hr': 31,
            'de': 14, 'da': 10, 'fi': 11, 'hu': 19, 'ja': 25, 'he': 24, 'ko': 32,
            'sv': 8, 'sl': 30}

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        self.location, args = self.check_params(args, kwargs, "location", None, False, "string", [])
        self.initial, args = self.check_params(args, kwargs, "initial", None, False, "list", ["sdtv", "sddvd", "hdtv", "rawhdtv", "fullhdtv", "hdwebdl", "fullhdwebdl", "hdbluray", "fullhdbluray", "unknown"])
        self.archive, args = self.check_params(args, kwargs, "archive", None, False, "list", ["sddvd", "hdtv", "rawhdtv", "fullhdtv", "hdwebdl", "fullhdwebdl", "hdbluray", "fullhdbluray"])
        self.flatten_folders, args = self.check_params(args, kwargs, "flatten_folders", str(sickbeard.FLATTEN_FOLDERS_DEFAULT), False, "bool", [])
        self.status, args = self.check_params(args, kwargs, "status", None, False, "string", ["wanted", "skipped", "archived", "ignored"])
        self.lang, args = self.check_params(args, kwargs, "lang", "en", False, "string", self.valid_languages.keys())
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ add a show in sickbeard with an existing folder """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if showObj:
            return _responds(RESULT_FAILURE, msg="An existing tvdbid already exists in database")

        if not self.location:
            if sickbeard.ROOT_DIRS != "":
                root_dirs = sickbeard.ROOT_DIRS.split('|')
                root_dirs.pop(0)
                default_index = int(sickbeard.ROOT_DIRS.split('|')[0])
                self.location = root_dirs[default_index]
            else:
                return _responds(RESULT_FAILURE, msg="Root directory is not set, please provide a location")

        if not ek.ek(os.path.isdir, self.location):
            return _responds(RESULT_FAILURE, msg="'" + self.location + "' is not a valid location")

        quality_map = {'sdtv': Quality.SDTV,
                       'sddvd': Quality.SDDVD,
                       'hdtv': Quality.HDTV,
                       'rawhdtv': Quality.RAWHDTV,
                       'fullhdtv': Quality.FULLHDTV,
                       'hdwebdl': Quality.HDWEBDL,
                       'fullhdwebdl': Quality.FULLHDWEBDL,
                       'hdbluray': Quality.HDBLURAY,
                       'fullhdbluray': Quality.FULLHDBLURAY,
                       'unknown': Quality.UNKNOWN}

        # use default quality as a failsafe
        newQuality = int(sickbeard.QUALITY_DEFAULT)
        iqualityID = []
        aqualityID = []

        if self.initial:
            for quality in self.initial:
                iqualityID.append(quality_map[quality])
        if self.archive:
            for quality in self.archive:
                aqualityID.append(quality_map[quality])

        if iqualityID or aqualityID:
            newQuality = Quality.combineQualities(iqualityID, aqualityID)

        # use default status as a failsafe
        newStatus = sickbeard.STATUS_DEFAULT
        if self.status:
            # convert the string status to a int
            for status in statusStrings.statusStrings:
                if statusStrings[status].lower() == str(self.status).lower():
                    self.status = status
                    break
            #TODO: check if obsolete
            if not self.status in statusStrings.statusStrings:
                raise ApiError("Invalid Status")
            # only allow the status options we want
            if int(self.status) not in (3, 5, 6, 7):
                return _responds(RESULT_FAILURE, msg="Status prohibited")
            newStatus = self.status

        tvdbName = None
        tvdbResult = CMD_SickBeardSearchTVDB([], {"tvdbid": self.tvdbid}).run()

        if tvdbResult['result'] == result_type_map[RESULT_SUCCESS]:
            if not tvdbResult['data']['results']:
                return _responds(RESULT_FAILURE, msg="Empty results returned, check tvdbid and try again")
            if len(tvdbResult['data']['results']) == 1 and 'name' in tvdbResult['data']['results'][0]:
                tvdbName = tvdbResult['data']['results'][0]['name']

        if not tvdbName:
            return _responds(RESULT_FAILURE, msg="Unable to retrieve information from tvdb")

        # moved the logic check to the end in an attempt to eliminate empty directory being created from previous errors
        showPath = ek.ek(os.path.join, self.location, helpers.sanitizeFileName(tvdbName))

        # don't create show dir if config says not to
        if sickbeard.ADD_SHOWS_WO_DIR:
            logger.log(u"Skipping initial creation of " + showPath + " due to config.ini setting")
        else:
            dir_exists = helpers.makeDir(showPath)
            if not dir_exists:
                logger.log(u"API :: Unable to create the folder " + showPath + ", can't add the show", logger.ERROR)
                return _responds(RESULT_FAILURE, {"path": showPath}, "Unable to create the folder " + showPath + ", can't add the show")
            else:
                helpers.chmodAsParent(showPath)

        sickbeard.showQueueScheduler.action.addShow(int(self.tvdbid), showPath, newStatus, newQuality, int(self.flatten_folders), self.lang)  # @UndefinedVariable
        return _responds(RESULT_SUCCESS, {"name": tvdbName}, tvdbName + " has been queued to be added")


class CMD_ShowCache(ApiCall):
    _help = {"desc": "check sickbeard's cache to see if the banner or poster image for a show is valid",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"}
                                    }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ check sickbeard's cache to see if the banner or poster image for a show is valid """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        #TODO: catch if cache dir is missing/invalid.. so it doesn't break show/show.cache
        #return {"poster": 0, "banner": 0}

        cache_obj = image_cache.ImageCache()

        has_poster = 0
        has_banner = 0

        if ek.ek(os.path.isfile, cache_obj.poster_path(showObj.tvdbid)):
            has_poster = 1
        if ek.ek(os.path.isfile, cache_obj.banner_path(showObj.tvdbid)):
            has_banner = 1

        return _responds(RESULT_SUCCESS, {"poster": has_poster, "banner": has_banner})


class CMD_ShowDelete(ApiCall):
    _help = {"desc": "delete a show in sickbeard",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ delete a show in sickbeard """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        if sickbeard.showQueueScheduler.action.isBeingAdded(showObj) or sickbeard.showQueueScheduler.action.isBeingUpdated(showObj):  # @UndefinedVariable
            return _responds(RESULT_FAILURE, msg="Show can not be deleted while being added or updated")

        showObj.deleteShow()
        return _responds(RESULT_SUCCESS, msg=u"" + showObj.name + " has been deleted")


class CMD_ShowGetQuality(ApiCall):
    _help = {"desc": "get quality setting for a show in sickbeard",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"}
                                }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ get quality setting for a show in sickbeard """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        anyQualities, bestQualities = _mapQuality(showObj.quality)

        return _responds(RESULT_SUCCESS, {"initial": anyQualities, "archive": bestQualities})


class CMD_ShowGetPoster(ApiCall):
    _help = {"desc": "get the poster stored for a show in sickbeard",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"}
                                }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ get the poster for a show in sickbeard """
        return {'outputType': 'image', 'image': webserve.WebInterface().showPoster(self.tvdbid, 'poster')}


class CMD_ShowGetBanner(ApiCall):
    _help = {"desc": "get the banner stored for a show in sickbeard",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"}
                                }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ get the banner for a show in sickbeard """
        return {'outputType': 'image', 'image': webserve.WebInterface().showPoster(self.tvdbid, 'banner')}


class CMD_ShowPause(ApiCall):
    _help = {"desc": "set a show's paused state in sickbeard",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                  },
             "optionalParameters": {"pause": {"desc": "set the pause state of the show"}
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        self.pause, args = self.check_params(args, kwargs, "pause", 0, False, "bool", [])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ set a show's paused state in sickbeard """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        if self.pause == True:
            showObj.paused = 1
            return _responds(RESULT_SUCCESS, msg=u"" + showObj.name + " has been paused")
        else:
            showObj.paused = 0
            return _responds(RESULT_SUCCESS, msg=u"" + showObj.name + " has been unpaused")

        return _responds(RESULT_FAILURE, msg=u"" + showObj.name + " was unable to be paused")


class CMD_ShowRefresh(ApiCall):
    _help = {"desc": "refresh a show in sickbeard",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ refresh a show in sickbeard """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        try:
            sickbeard.showQueueScheduler.action.refreshShow(showObj)  # @UndefinedVariable
            return _responds(RESULT_SUCCESS, msg=u"" + showObj.name + " has queued to be refreshed")
        except exceptions.CantRefreshException, e:
            logger.log(u"API:: Unable to refresh " + showObj.name + ". " + str(ex(e)), logger.ERROR)
            return _responds(RESULT_FAILURE, msg=u"Unable to refresh " + showObj.name)


class CMD_ShowSeasonList(ApiCall):
    _help = {"desc": "display the season list for a given show",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                    },
             "optionalParameters": {"sort": {"desc": "change the sort order from descending to ascending"}
                                     }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        self.sort, args = self.check_params(args, kwargs, "sort", "desc", False, "string", ["asc", "desc"])  # "asc" and "desc" default and fallback is "desc"
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display the season list for a given show """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        myDB = db.DBConnection(row_type="dict")
        if self.sort == "asc":
            sqlResults = myDB.select("SELECT DISTINCT season FROM tv_episodes WHERE showid = ? ORDER BY season ASC", [self.tvdbid])
        else:
            sqlResults = myDB.select("SELECT DISTINCT season FROM tv_episodes WHERE showid = ? ORDER BY season DESC", [self.tvdbid])
        # a list with all season numbers
        seasonList = []
        for row in sqlResults:
            seasonList.append(int(row["season"]))

        myDB.connection.close()
        return _responds(RESULT_SUCCESS, seasonList)


class CMD_ShowSeasons(ApiCall):
    _help = {"desc": "display a listing of episodes for all or a given season",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                  },
             "optionalParameters": {"season": {"desc": "the season number"},
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        self.season, args = self.check_params(args, kwargs, "season", None, False, "int", [])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display a listing of episodes for all or a given show """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        myDB = db.DBConnection(row_type="dict")

        if self.season == None:
            sqlResults = myDB.select("SELECT name, episode, airdate, status, season FROM tv_episodes WHERE showid = ?", [self.tvdbid])
            seasons = {}
            for row in sqlResults:
                status, quality = Quality.splitCompositeStatus(int(row["status"]))
                row["status"] = _get_status_Strings(status)
                row["quality"] = _get_quality_string(quality)
                row["airdate"] = _ordinal_to_dateForm(row["airdate"])
                curSeason = int(row["season"])
                curEpisode = int(row["episode"])
                del row["season"]
                del row["episode"]
                if not curSeason in seasons:
                    seasons[curSeason] = {}
                seasons[curSeason][curEpisode] = row

        else:
            sqlResults = myDB.select("SELECT name, episode, airdate, status FROM tv_episodes WHERE showid = ? AND season = ?", [self.tvdbid, self.season])
            if len(sqlResults) is 0:
                return _responds(RESULT_FAILURE, msg="Season not found")
            seasons = {}
            for row in sqlResults:
                curEpisode = int(row["episode"])
                del row["episode"]
                status, quality = Quality.splitCompositeStatus(int(row["status"]))
                row["status"] = _get_status_Strings(status)
                row["quality"] = _get_quality_string(quality)
                row["airdate"] = _ordinal_to_dateForm(row["airdate"])
                if not curEpisode in seasons:
                    seasons[curEpisode] = {}
                seasons[curEpisode] = row

        myDB.connection.close()
        return _responds(RESULT_SUCCESS, seasons)


class CMD_ShowSetQuality(ApiCall):
    _help = {"desc": "set desired quality of a show in sickbeard. if neither initial or archive are provided then the config default quality will be used",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"}
                                },
             "optionalParameters": {"initial": {"desc": "initial quality for the show"},
                                    "archive": {"desc": "archive quality for the show"}
                                    }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # this for whatever reason removes hdbluray not sdtv... which is just wrong. reverting to previous code.. plus we didnt use the new code everywhere.
        # self.archive, args = self.check_params(args, kwargs, "archive", None, False, "list", _getQualityMap().values()[1:])
        self.initial, args = self.check_params(args, kwargs, "initial", None, False, "list", ["sdtv", "sddvd", "hdtv", "rawhdtv", "fullhdtv", "hdwebdl", "fullhdwebdl", "hdbluray", "fullhdbluray", "unknown"])
        self.archive, args = self.check_params(args, kwargs, "archive", None, False, "list", ["sddvd", "hdtv", "rawhdtv", "fullhdtv", "hdwebdl", "fullhdwebdl", "hdbluray", "fullhdbluray"])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ set the quality for a show in sickbeard by taking in a deliminated
            string of qualities, map to their value and combine for new values
        """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        quality_map = {'sdtv': Quality.SDTV,
                       'sddvd': Quality.SDDVD,
                       'hdtv': Quality.HDTV,
                       'rawhdtv': Quality.RAWHDTV,
                       'fullhdtv': Quality.FULLHDTV,
                       'hdwebdl': Quality.HDWEBDL,
                       'fullhdwebdl': Quality.FULLHDWEBDL,
                       'hdbluray': Quality.HDBLURAY,
                       'fullhdbluray': Quality.FULLHDBLURAY,
                       'unknown': Quality.UNKNOWN}

        #use default quality as a failsafe
        newQuality = int(sickbeard.QUALITY_DEFAULT)
        iqualityID = []
        aqualityID = []

        if self.initial:
            for quality in self.initial:
                iqualityID.append(quality_map[quality])
        if self.archive:
            for quality in self.archive:
                aqualityID.append(quality_map[quality])

        if iqualityID or aqualityID:
            newQuality = Quality.combineQualities(iqualityID, aqualityID)
        showObj.quality = newQuality

        return _responds(RESULT_SUCCESS, msg=showObj.name + " quality has been changed to " + _get_quality_string(showObj.quality))


class CMD_ShowStats(ApiCall):
    _help = {"desc": "display episode statistics for a given show",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display episode statistics for a given show """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        # show stats
        episode_status_counts_total = {}
        episode_status_counts_total["total"] = 0
        for status in statusStrings.statusStrings.keys():
            if status in [UNKNOWN, DOWNLOADED, SNATCHED, SNATCHED_PROPER]:
                continue
            episode_status_counts_total[status] = 0

        # add all the downloaded qualities
        episode_qualities_counts_download = {}
        episode_qualities_counts_download["total"] = 0
        for statusCode in Quality.DOWNLOADED:
            status, quality = Quality.splitCompositeStatus(statusCode)
            if quality in [Quality.NONE]:
                continue
            episode_qualities_counts_download[statusCode] = 0

        # add all snatched qualities
        episode_qualities_counts_snatch = {}
        episode_qualities_counts_snatch["total"] = 0
        for statusCode in Quality.SNATCHED + Quality.SNATCHED_PROPER:
            status, quality = Quality.splitCompositeStatus(statusCode)
            if quality in [Quality.NONE]:
                continue
            episode_qualities_counts_snatch[statusCode] = 0

        myDB = db.DBConnection(row_type="dict")
        sqlResults = myDB.select("SELECT status, season FROM tv_episodes WHERE season != 0 AND showid = ?", [self.tvdbid])
        # the main loop that goes through all episodes
        for row in sqlResults:
            status, quality = Quality.splitCompositeStatus(int(row["status"]))

            episode_status_counts_total["total"] += 1

            if status in Quality.DOWNLOADED:
                episode_qualities_counts_download["total"] += 1
                episode_qualities_counts_download[int(row["status"])] += 1
            elif status in Quality.SNATCHED + Quality.SNATCHED_PROPER:
                episode_qualities_counts_snatch["total"] += 1
                episode_qualities_counts_snatch[int(row["status"])] += 1
            # we dont count NONE = 0 = N/A
            elif status == 0:
                pass
            else:
                episode_status_counts_total[status] += 1

        # the outgoing container
        episodes_stats = {}
        episodes_stats["downloaded"] = {}
        # turning codes into strings
        for statusCode in episode_qualities_counts_download:
            if statusCode is "total":
                episodes_stats["downloaded"]["total"] = episode_qualities_counts_download[statusCode]
                continue
            status, quality = Quality.splitCompositeStatus(int(statusCode))
            statusString = Quality.qualityStrings[quality].lower().replace(" ", "_").replace("(", "").replace(")", "")
            episodes_stats["downloaded"][statusString] = episode_qualities_counts_download[statusCode]

        episodes_stats["snatched"] = {}
        # truning codes into strings
        # and combining proper and normal
        for statusCode in episode_qualities_counts_snatch:
            if statusCode is "total":
                episodes_stats["snatched"]["total"] = episode_qualities_counts_snatch[statusCode]
                continue
            status, quality = Quality.splitCompositeStatus(int(statusCode))
            statusString = Quality.qualityStrings[quality].lower().replace(" ", "_").replace("(", "").replace(")", "")
            if Quality.qualityStrings[quality] in episodes_stats["snatched"]:
                episodes_stats["snatched"][statusString] += episode_qualities_counts_snatch[statusCode]
            else:
                episodes_stats["snatched"][statusString] = episode_qualities_counts_snatch[statusCode]

        #episodes_stats["total"] = {}
        for statusCode in episode_status_counts_total:
            if statusCode is "total":
                episodes_stats["total"] = episode_status_counts_total[statusCode]
                continue
            status, quality = Quality.splitCompositeStatus(int(statusCode))
            statusString = statusStrings.statusStrings[statusCode].lower().replace(" ", "_").replace("(", "").replace(")", "")
            episodes_stats[statusString] = episode_status_counts_total[statusCode]

        myDB.connection.close()
        return _responds(RESULT_SUCCESS, episodes_stats)


class CMD_ShowUpdate(ApiCall):
    _help = {"desc": "update a show in sickbeard",
             "requiredParameters": {"tvdbid": {"desc": "thetvdb.com unique id of a show"},
                                  }
             }

    def __init__(self, args, kwargs):
        # required
        self.tvdbid, args = self.check_params(args, kwargs, "tvdbid", None, True, "int", [])
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ update a show in sickbeard """
        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(self.tvdbid))
        if not showObj:
            return _responds(RESULT_FAILURE, msg="Show not found")

        try:
            sickbeard.showQueueScheduler.action.updateShow(showObj, True)  # @UndefinedVariable
            return _responds(RESULT_SUCCESS, msg=u"" + showObj.name + " has queued to be updated")
        except exceptions.CantUpdateException, e:
            logger.log(u"API:: Unable to update " + showObj.name + ". " + str(ex(e)), logger.ERROR)
            return _responds(RESULT_FAILURE, msg=u"Unable to update " + showObj.name)


class CMD_Shows(ApiCall):
    _help = {"desc": "display all shows in sickbeard",
             "optionalParameters": {"sort": {"desc": "sort the list of shows by show name instead of tvdbid"},
                                    "paused": {"desc": "only show the shows that are set to paused"},
                                  },
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        self.sort, args = self.check_params(args, kwargs, "sort", "id", False, "string", ["id", "name"])
        self.paused, args = self.check_params(args, kwargs, "paused", None, False, "bool", [])
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display_is_int_multi( self.tvdbid )shows in sickbeard """
        shows = {}
        for curShow in sickbeard.showList:
            nextAirdate = ''
            nextEps = curShow.nextEpisode()
            if (len(nextEps) != 0):
                nextAirdate = _ordinal_to_dateForm(nextEps[0].airdate.toordinal())

            if self.paused != None and bool(self.paused) != bool(curShow.paused):
                continue

            showDict = {"paused": curShow.paused,
                        "quality": _get_quality_string(curShow.quality),
                        "language": curShow.lang,
                        "air_by_date": curShow.air_by_date,
                        "tvdbid": curShow.tvdbid,
                        "tvrage_id": curShow.tvrid,
                        "tvrage_name": curShow.tvrname,
                        "network": curShow.network,
                        "show_name": curShow.name,
                        "status": curShow.status,
                        "next_ep_airdate": nextAirdate}
            showDict["cache"] = CMD_ShowCache((), {"tvdbid": curShow.tvdbid}).run()["data"]
            if not showDict["network"]:
                showDict["network"] = ""
            if self.sort == "name":
                shows[curShow.name] = showDict
            else:
                shows[curShow.tvdbid] = showDict
        return _responds(RESULT_SUCCESS, shows)


class CMD_ShowsStats(ApiCall):
    _help = {"desc": "display the global shows and episode stats"
             }

    def __init__(self, args, kwargs):
        # required
        # optional
        # super, missing, help
        ApiCall.__init__(self, args, kwargs)

    def run(self):
        """ display the global shows and episode stats """
        stats = {}

        myDB = db.DBConnection()
        today = str(datetime.date.today().toordinal())
        stats["shows_total"] = len(sickbeard.showList)
        stats["shows_active"] = len([show for show in sickbeard.showList if show.paused == 0 and show.status != "Ended"])
        stats["ep_downloaded"] = myDB.select("SELECT COUNT(*) FROM tv_episodes WHERE status IN (" + ",".join([str(show) for show in Quality.DOWNLOADED + [ARCHIVED]]) + ") AND season != 0 and episode != 0 AND airdate <= " + today + "")[0][0]
        stats["ep_total"] = myDB.select("SELECT COUNT(*) FROM tv_episodes WHERE season != 0 AND episode != 0 AND (airdate != 1 OR status IN (" + ",".join([str(show) for show in (Quality.DOWNLOADED + Quality.SNATCHED + Quality.SNATCHED_PROPER) + [ARCHIVED]]) + ")) AND airdate <= " + today + " AND status != " + str(IGNORED) + "")[0][0]

        myDB.connection.close()
        return _responds(RESULT_SUCCESS, stats)

# WARNING: never define a cmd call string that contains a "_" (underscore)
# this is reserved for cmd indexes used while cmd chaining

# WARNING: never define a param name that contains a "." (dot)
# this is reserved for cmd namspaces used while cmd chaining
_functionMaper = {"help": CMD_Help,
                  "future": CMD_ComingEpisodes,
                  "episode": CMD_Episode,
                  "episode.search": CMD_EpisodeSearch,
                  "episode.setstatus": CMD_EpisodeSetStatus,
                  "exceptions": CMD_Exceptions,
                  "history": CMD_History,
                  "history.clear": CMD_HistoryClear,
                  "history.trim": CMD_HistoryTrim,
                  "logs": CMD_Logs,
                  "sb": CMD_SickBeard,
                  "sb.addrootdir": CMD_SickBeardAddRootDir,
                  "sb.checkscheduler": CMD_SickBeardCheckScheduler,
                  "sb.deleterootdir": CMD_SickBeardDeleteRootDir,
                  "sb.forcesearch": CMD_SickBeardForceSearch,
                  "sb.getdefaults": CMD_SickBeardGetDefaults,
                  "sb.getmessages": CMD_SickBeardGetMessages,
                  "sb.getrootdirs": CMD_SickBeardGetRootDirs,
                  "sb.pausebacklog": CMD_SickBeardPauseBacklog,
                  "sb.ping": CMD_SickBeardPing,
                  "sb.restart": CMD_SickBeardRestart,
                  "sb.searchtvdb": CMD_SickBeardSearchTVDB,
                  "sb.setdefaults": CMD_SickBeardSetDefaults,
                  "sb.shutdown": CMD_SickBeardShutdown,
                  "show": CMD_Show,
                  "show.addexisting": CMD_ShowAddExisting,
                  "show.addnew": CMD_ShowAddNew,
                  "show.cache": CMD_ShowCache,
                  "show.delete": CMD_ShowDelete,
                  "show.getquality": CMD_ShowGetQuality,
                  "show.getposter": CMD_ShowGetPoster,
                  "show.getbanner": CMD_ShowGetBanner,
                  "show.pause": CMD_ShowPause,
                  "show.refresh": CMD_ShowRefresh,
                  "show.seasonlist": CMD_ShowSeasonList,
                  "show.seasons": CMD_ShowSeasons,
                  "show.setquality": CMD_ShowSetQuality,
                  "show.stats": CMD_ShowStats,
                  "show.update": CMD_ShowUpdate,
                  "shows": CMD_Shows,
                  "shows.stats": CMD_ShowsStats
                  }

########NEW FILE########
__FILENAME__ = webserve
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import os.path

import time
import urllib
import re
import threading
import datetime
import random

from Cheetah.Template import Template
import cherrypy.lib

import sickbeard

from sickbeard import config, sab
from sickbeard import history, notifiers, processTV
from sickbeard import ui
from sickbeard import logger, helpers, exceptions, classes, db
from sickbeard import encodingKludge as ek
from sickbeard import search_queue
from sickbeard import image_cache
from sickbeard import naming

from sickbeard.providers import newznab
from sickbeard.common import Quality, Overview, statusStrings
from sickbeard.common import SNATCHED, SKIPPED, UNAIRED, IGNORED, ARCHIVED, WANTED
from sickbeard.exceptions import ex
from sickbeard.webapi import Api

from lib.tvdb_api import tvdb_api, tvdb_exceptions

try:
    import json
except ImportError:
    from lib import simplejson as json

try:
    import xml.etree.cElementTree as etree
except ImportError:
    import xml.etree.ElementTree as etree

from sickbeard import browser


class PageTemplate (Template):
    def __init__(self, *args, **KWs):
        KWs['file'] = os.path.join(sickbeard.PROG_DIR, "data/interfaces/default/", KWs['file'])
        super(PageTemplate, self).__init__(*args, **KWs)
        self.sbRoot = sickbeard.WEB_ROOT
        self.sbHttpPort = sickbeard.WEB_PORT
        self.sbHttpsPort = sickbeard.WEB_PORT
        self.sbHttpsEnabled = sickbeard.ENABLE_HTTPS
        if cherrypy.request.headers['Host'][0] == '[':
            self.sbHost = re.match("^\[.*\]", cherrypy.request.headers['Host'], re.X|re.M|re.S).group(0)
        else:
            self.sbHost = re.match("^[^:]+", cherrypy.request.headers['Host'], re.X|re.M|re.S).group(0)
        self.projectHomePage = "http://code.google.com/p/sickbeard/"

        if "X-Forwarded-Host" in cherrypy.request.headers:
            self.sbHost = cherrypy.request.headers['X-Forwarded-Host']
        if "X-Forwarded-Port" in cherrypy.request.headers:
            self.sbHttpPort = cherrypy.request.headers['X-Forwarded-Port']
            self.sbHttpsPort = self.sbHttpPort
        if "X-Forwarded-Proto" in cherrypy.request.headers:
            self.sbHttpsEnabled = True if cherrypy.request.headers['X-Forwarded-Proto'] == 'https' else False

        logPageTitle = 'Logs &amp; Errors'
        if len(classes.ErrorViewer.errors):
            logPageTitle += ' (' + str(len(classes.ErrorViewer.errors)) + ')'
        self.logPageTitle = logPageTitle
        self.sbPID = str(sickbeard.PID)
        self.menu = [
            { 'title': 'Home',            'key': 'home'           },
            { 'title': 'Coming Episodes', 'key': 'comingEpisodes' },
            { 'title': 'History',         'key': 'history'        },
            { 'title': 'Manage',          'key': 'manage'         },
            { 'title': 'Config',          'key': 'config'         },
            { 'title': logPageTitle,      'key': 'errorlogs'      },
        ]


def redirect(abspath, *args, **KWs):
    assert abspath[0] == '/'
    raise cherrypy.HTTPRedirect(sickbeard.WEB_ROOT + abspath, *args, **KWs)


class TVDBWebUI:
    def __init__(self, config, log=None):
        self.config = config
        self.log = log

    def selectSeries(self, allSeries):

        searchList = ",".join([x['id'] for x in allSeries])
        showDirList = ""
        for curShowDir in self.config['_showDir']:
            showDirList += "showDir=" + curShowDir + "&"
        redirect("/home/addShows/addShow?" + showDirList + "seriesList=" + searchList)


def _munge(string):
    return unicode(string).encode('utf-8', 'xmlcharrefreplace')


def _genericMessage(subject, message):
    t = PageTemplate(file="genericMessage.tmpl")
    t.submenu = HomeMenu()
    t.subject = subject
    t.message = message
    return _munge(t)


def _getEpisode(show, season, episode):

    if show == None or season == None or episode == None:
        return "Invalid parameters"

    showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

    if showObj == None:
        return "Show not in show list"

    epObj = showObj.getEpisode(int(season), int(episode))

    if epObj == None:
        return "Episode couldn't be retrieved"

    return epObj

ManageMenu = [
    { 'title': 'Backlog Overview',          'path': 'manage/backlogOverview/' },
    { 'title': 'Manage Searches',           'path': 'manage/manageSearches/'  },
    { 'title': 'Episode Status Management', 'path': 'manage/episodeStatuses/' },
]


class ManageSearches:

    @cherrypy.expose
    def index(self):
        t = PageTemplate(file="manage_manageSearches.tmpl")
        #t.backlogPI = sickbeard.backlogSearchScheduler.action.getProgressIndicator()
        t.backlogPaused = sickbeard.searchQueueScheduler.action.is_backlog_paused()  # @UndefinedVariable
        t.backlogRunning = sickbeard.searchQueueScheduler.action.is_backlog_in_progress()  # @UndefinedVariable
        t.searchStatus = sickbeard.currentSearchScheduler.action.amActive  # @UndefinedVariable
        t.submenu = ManageMenu

        return _munge(t)

    @cherrypy.expose
    def forceSearch(self):

        # force it to run the next time it looks
        result = sickbeard.currentSearchScheduler.forceRun()
        if result:
            logger.log(u"Search forced")
            ui.notifications.message('Episode search started',
                          'Note: RSS feeds may not be updated if retrieved recently')

        redirect("/manage/manageSearches/")

    @cherrypy.expose
    def pauseBacklog(self, paused=None):
        if paused == "1":
            sickbeard.searchQueueScheduler.action.pause_backlog()  # @UndefinedVariable
        else:
            sickbeard.searchQueueScheduler.action.unpause_backlog()  # @UndefinedVariable

        redirect("/manage/manageSearches/")

    @cherrypy.expose
    def forceVersionCheck(self):

        # force a check to see if there is a new version
        result = sickbeard.versionCheckScheduler.action.check_for_new_version(force=True)  # @UndefinedVariable
        if result:
            logger.log(u"Forcing version check")

        redirect("/manage/manageSearches/")


class Manage:

    manageSearches = ManageSearches()

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="manage.tmpl")
        t.submenu = ManageMenu
        return _munge(t)

    @cherrypy.expose
    def showEpisodeStatuses(self, tvdb_id, whichStatus):
        myDB = db.DBConnection()

        status_list = [int(whichStatus)]
        if status_list[0] == SNATCHED:
            status_list = Quality.SNATCHED + Quality.SNATCHED_PROPER

        cur_show_results = myDB.select("SELECT season, episode, name FROM tv_episodes WHERE showid = ? AND season != 0 AND status IN (" + ','.join(['?'] * len(status_list)) + ")", [int(tvdb_id)] + status_list)

        result = {}
        for cur_result in cur_show_results:
            cur_season = int(cur_result["season"])
            cur_episode = int(cur_result["episode"])

            if cur_season not in result:
                result[cur_season] = {}

            result[cur_season][cur_episode] = cur_result["name"]

        return json.dumps(result)

    @cherrypy.expose
    def episodeStatuses(self, whichStatus=None):

        if whichStatus:
            whichStatus = int(whichStatus)
            status_list = [whichStatus]
            if status_list[0] == SNATCHED:
                status_list = Quality.SNATCHED + Quality.SNATCHED_PROPER
        else:
            status_list = []

        t = PageTemplate(file="manage_episodeStatuses.tmpl")
        t.submenu = ManageMenu
        t.whichStatus = whichStatus

        # if we have no status then this is as far as we need to go
        if not status_list:
            return _munge(t)

        myDB = db.DBConnection()
        status_results = myDB.select("SELECT show_name, tv_shows.tvdb_id as tvdb_id FROM tv_episodes, tv_shows WHERE tv_episodes.status IN (" + ','.join(['?'] * len(status_list)) + ") AND season != 0 AND tv_episodes.showid = tv_shows.tvdb_id ORDER BY show_name", status_list)

        ep_counts = {}
        show_names = {}
        sorted_show_ids = []
        for cur_status_result in status_results:
            cur_tvdb_id = int(cur_status_result["tvdb_id"])
            if cur_tvdb_id not in ep_counts:
                ep_counts[cur_tvdb_id] = 1
            else:
                ep_counts[cur_tvdb_id] += 1

            show_names[cur_tvdb_id] = cur_status_result["show_name"]
            if cur_tvdb_id not in sorted_show_ids:
                sorted_show_ids.append(cur_tvdb_id)

        t.show_names = show_names
        t.ep_counts = ep_counts
        t.sorted_show_ids = sorted_show_ids
        return _munge(t)

    @cherrypy.expose
    def changeEpisodeStatuses(self, oldStatus, newStatus, *args, **kwargs):

        status_list = [int(oldStatus)]
        if status_list[0] == SNATCHED:
            status_list = Quality.SNATCHED + Quality.SNATCHED_PROPER

        to_change = {}

        # make a list of all shows and their associated args
        for arg in kwargs:
            tvdb_id, what = arg.split('-')

            # we don't care about unchecked checkboxes
            if kwargs[arg] != 'on':
                continue

            if tvdb_id not in to_change:
                to_change[tvdb_id] = []

            to_change[tvdb_id].append(what)

        myDB = db.DBConnection()

        for cur_tvdb_id in to_change:

            # get a list of all the eps we want to change if they just said "all"
            if 'all' in to_change[cur_tvdb_id]:
                all_eps_results = myDB.select("SELECT season, episode FROM tv_episodes WHERE status IN (" + ','.join(['?'] * len(status_list)) + ") AND season != 0 AND showid = ?", status_list + [cur_tvdb_id])
                all_eps = [str(x["season"]) + 'x' + str(x["episode"]) for x in all_eps_results]
                to_change[cur_tvdb_id] = all_eps

            Home().setStatus(cur_tvdb_id, '|'.join(to_change[cur_tvdb_id]), newStatus, direct=True)

        redirect("/manage/episodeStatuses/")

    @cherrypy.expose
    def backlogShow(self, tvdb_id):

        show_obj = helpers.findCertainShow(sickbeard.showList, int(tvdb_id))

        if show_obj:
            sickbeard.backlogSearchScheduler.action.searchBacklog([show_obj])  # @UndefinedVariable

        redirect("/manage/backlogOverview/")

    @cherrypy.expose
    def backlogOverview(self):

        t = PageTemplate(file="manage_backlogOverview.tmpl")
        t.submenu = ManageMenu

        myDB = db.DBConnection()

        showCounts = {}
        showCats = {}
        showSQLResults = {}

        for curShow in sickbeard.showList:

            epCounts = {}
            epCats = {}
            epCounts[Overview.SKIPPED] = 0
            epCounts[Overview.WANTED] = 0
            epCounts[Overview.QUAL] = 0
            epCounts[Overview.GOOD] = 0
            epCounts[Overview.UNAIRED] = 0
            epCounts[Overview.SNATCHED] = 0

            sqlResults = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? ORDER BY season DESC, episode DESC", [curShow.tvdbid])

            for curResult in sqlResults:

                curEpCat = curShow.getOverview(int(curResult["status"]))
                epCats[str(curResult["season"]) + "x" + str(curResult["episode"])] = curEpCat
                epCounts[curEpCat] += 1

            showCounts[curShow.tvdbid] = epCounts
            showCats[curShow.tvdbid] = epCats
            showSQLResults[curShow.tvdbid] = sqlResults

        t.showCounts = showCounts
        t.showCats = showCats
        t.showSQLResults = showSQLResults

        return _munge(t)

    @cherrypy.expose
    def massEdit(self, toEdit=None):

        t = PageTemplate(file="manage_massEdit.tmpl")
        t.submenu = ManageMenu

        if not toEdit:
            redirect("/manage/")

        showIDs = toEdit.split("|")
        showList = []
        for curID in showIDs:
            curID = int(curID)
            showObj = helpers.findCertainShow(sickbeard.showList, curID)
            if showObj:
                showList.append(showObj)

        flatten_folders_all_same = True
        last_flatten_folders = None

        paused_all_same = True
        last_paused = None

        quality_all_same = True
        last_quality = None

        root_dir_list = []

        for curShow in showList:

            cur_root_dir = ek.ek(os.path.dirname, curShow._location)
            if cur_root_dir not in root_dir_list:
                root_dir_list.append(cur_root_dir)

            # if we know they're not all the same then no point even bothering
            if paused_all_same:
                # if we had a value already and this value is different then they're not all the same
                if last_paused not in (curShow.paused, None):
                    paused_all_same = False
                else:
                    last_paused = curShow.paused

            if flatten_folders_all_same:
                if last_flatten_folders not in (None, curShow.flatten_folders):
                    flatten_folders_all_same = False
                else:
                    last_flatten_folders = curShow.flatten_folders

            if quality_all_same:
                if last_quality not in (None, curShow.quality):
                    quality_all_same = False
                else:
                    last_quality = curShow.quality

        t.showList = toEdit
        t.paused_value = last_paused if paused_all_same else None
        t.flatten_folders_value = last_flatten_folders if flatten_folders_all_same else None
        t.quality_value = last_quality if quality_all_same else None
        t.root_dir_list = root_dir_list

        return _munge(t)

    @cherrypy.expose
    def massEditSubmit(self, paused=None, flatten_folders=None, quality_preset=False,
                       anyQualities=[], bestQualities=[], toEdit=None, *args, **kwargs):

        dir_map = {}
        for cur_arg in kwargs:
            if not cur_arg.startswith('orig_root_dir_'):
                continue
            which_index = cur_arg.replace('orig_root_dir_', '')
            end_dir = kwargs['new_root_dir_' + which_index]
            dir_map[kwargs[cur_arg]] = end_dir

        showIDs = toEdit.split("|")
        errors = []
        for curShow in showIDs:
            curErrors = []
            showObj = helpers.findCertainShow(sickbeard.showList, int(curShow))
            if not showObj:
                continue

            cur_root_dir = ek.ek(os.path.dirname, showObj._location)
            cur_show_dir = ek.ek(os.path.basename, showObj._location)
            if cur_root_dir in dir_map and cur_root_dir != dir_map[cur_root_dir]:
                new_show_dir = ek.ek(os.path.join, dir_map[cur_root_dir], cur_show_dir)
                logger.log(u"For show " + showObj.name + " changing dir from " + showObj._location + " to " + new_show_dir)
            else:
                new_show_dir = showObj._location

            if paused == 'keep':
                new_paused = showObj.paused
            else:
                new_paused = True if paused == 'enable' else False
            new_paused = 'on' if new_paused else 'off'

            if flatten_folders == 'keep':
                new_flatten_folders = showObj.flatten_folders
            else:
                new_flatten_folders = True if flatten_folders == 'enable' else False
            new_flatten_folders = 'on' if new_flatten_folders else 'off'

            if quality_preset == 'keep':
                anyQualities, bestQualities = Quality.splitQuality(showObj.quality)

            curErrors += Home().editShow(curShow, new_show_dir, anyQualities, bestQualities, new_flatten_folders, new_paused, directCall=True)

            if curErrors:
                logger.log(u"Errors: " + str(curErrors), logger.ERROR)
                errors.append('<b>%s:</b>\n<ul>' % showObj.name + ' '.join(['<li>%s</li>' % error for error in curErrors]) + "</ul>")

        if len(errors) > 0:
            ui.notifications.error('%d error%s while saving changes:' % (len(errors), "" if len(errors) == 1 else "s"),
                        " ".join(errors))

        redirect("/manage/")

    @cherrypy.expose
    def massUpdate(self, toUpdate=None, toRefresh=None, toRename=None, toDelete=None, toMetadata=None):

        if toUpdate != None:
            toUpdate = toUpdate.split('|')
        else:
            toUpdate = []

        if toRefresh != None:
            toRefresh = toRefresh.split('|')
        else:
            toRefresh = []

        if toRename != None:
            toRename = toRename.split('|')
        else:
            toRename = []

        if toDelete != None:
            toDelete = toDelete.split('|')
        else:
            toDelete = []

        if toMetadata != None:
            toMetadata = toMetadata.split('|')
        else:
            toMetadata = []

        errors = []
        refreshes = []
        updates = []
        renames = []

        for curShowID in set(toUpdate + toRefresh + toRename + toDelete + toMetadata):

            if curShowID == '':
                continue

            showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(curShowID))

            if showObj == None:
                continue

            if curShowID in toDelete:
                showObj.deleteShow()
                # don't do anything else if it's being deleted
                continue

            if curShowID in toUpdate:
                try:
                    sickbeard.showQueueScheduler.action.updateShow(showObj, True)  # @UndefinedVariable
                    updates.append(showObj.name)
                except exceptions.CantUpdateException, e:
                    errors.append("Unable to update show " + showObj.name + ": " + ex(e))

            # don't bother refreshing shows that were updated anyway
            if curShowID in toRefresh and curShowID not in toUpdate:
                try:
                    sickbeard.showQueueScheduler.action.refreshShow(showObj)  # @UndefinedVariable
                    refreshes.append(showObj.name)
                except exceptions.CantRefreshException, e:
                    errors.append("Unable to refresh show " + showObj.name + ": " + ex(e))

            if curShowID in toRename:
                sickbeard.showQueueScheduler.action.renameShowEpisodes(showObj)  # @UndefinedVariable
                renames.append(showObj.name)

        if len(errors) > 0:
            ui.notifications.error("Errors encountered",
                        '<br >\n'.join(errors))

        messageDetail = ""

        if len(updates) > 0:
            messageDetail += "<br /><b>Updates</b><br /><ul><li>"
            messageDetail += "</li><li>".join(updates)
            messageDetail += "</li></ul>"

        if len(refreshes) > 0:
            messageDetail += "<br /><b>Refreshes</b><br /><ul><li>"
            messageDetail += "</li><li>".join(refreshes)
            messageDetail += "</li></ul>"

        if len(renames) > 0:
            messageDetail += "<br /><b>Renames</b><br /><ul><li>"
            messageDetail += "</li><li>".join(renames)
            messageDetail += "</li></ul>"

        if len(updates + refreshes + renames) > 0:
            ui.notifications.message("The following actions were queued:",
                          messageDetail)

        redirect("/manage/")


class History:

    @cherrypy.expose
    def index(self, limit=100):

        myDB = db.DBConnection()

        if limit == "0":
            sqlResults = myDB.select("SELECT h.*, show_name FROM history h, tv_shows s WHERE h.showid=s.tvdb_id ORDER BY date DESC")
        else:
            sqlResults = myDB.select("SELECT h.*, show_name FROM history h, tv_shows s WHERE h.showid=s.tvdb_id ORDER BY date DESC LIMIT ?", [limit])

        t = PageTemplate(file="history.tmpl")
        t.historyResults = sqlResults
        t.limit = limit
        t.submenu = [
            { 'title': 'Clear History', 'path': 'history/clearHistory/' },
            { 'title': 'Trim History',  'path': 'history/trimHistory/'  },
        ]

        return _munge(t)

    @cherrypy.expose
    def clearHistory(self):

        myDB = db.DBConnection()
        myDB.action("DELETE FROM history WHERE 1=1")
        ui.notifications.message('History cleared')
        redirect("/history/")

    @cherrypy.expose
    def trimHistory(self):

        myDB = db.DBConnection()
        myDB.action("DELETE FROM history WHERE date < " + str((datetime.datetime.today() - datetime.timedelta(days=30)).strftime(history.dateFormat)))
        ui.notifications.message('Removed history entries greater than 30 days old')
        redirect("/history/")


ConfigMenu = [
    { 'title': 'General',           'path': 'config/general/'          },
    { 'title': 'Search Settings',   'path': 'config/search/'           },
    { 'title': 'Search Providers',  'path': 'config/providers/'        },
    { 'title': 'Post Processing',   'path': 'config/postProcessing/'   },
    { 'title': 'Notifications',     'path': 'config/notifications/'    },
]


class ConfigGeneral:

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="config_general.tmpl")
        t.submenu = ConfigMenu
        return _munge(t)

    @cherrypy.expose
    def saveRootDirs(self, rootDirString=None):
        sickbeard.ROOT_DIRS = rootDirString

    @cherrypy.expose
    def saveAddShowDefaults(self, defaultFlattenFolders, defaultStatus, anyQualities, bestQualities):

        if anyQualities:
            anyQualities = anyQualities.split(',')
        else:
            anyQualities = []

        if bestQualities:
            bestQualities = bestQualities.split(',')
        else:
            bestQualities = []

        newQuality = Quality.combineQualities(map(int, anyQualities), map(int, bestQualities))

        sickbeard.STATUS_DEFAULT = int(defaultStatus)
        sickbeard.QUALITY_DEFAULT = int(newQuality)

        sickbeard.FLATTEN_FOLDERS_DEFAULT = config.checkbox_to_value(defaultFlattenFolders)

    @cherrypy.expose
    def generateKey(self):
        """ Return a new randomized API_KEY """

        try:
            from hashlib import md5
        except ImportError:
            from md5 import md5

        # Create some values to seed md5
        t = str(time.time())
        r = str(random.random())

        # Create the md5 instance and give it the current time
        m = md5(t)

        # Update the md5 instance with the random variable
        m.update(r)

        # Return a hex digest of the md5, eg 49f68a5c8493ec2c0bf489821c21fc3b
        logger.log(u"New SB API key generated")
        return m.hexdigest()

    @cherrypy.expose
    def saveGeneral(self, log_dir=None, web_port=None, web_log=None, web_ipv6=None,
                    launch_browser=None, web_username=None, use_api=None, api_key=None,
                    web_password=None, version_notify=None, enable_https=None, https_cert=None, https_key=None):

        results = []

        # Misc
        sickbeard.LAUNCH_BROWSER = config.checkbox_to_value(launch_browser)
        config.change_VERSION_NOTIFY(config.checkbox_to_value(version_notify))
        # sickbeard.LOG_DIR is set in config.change_LOG_DIR()

        # Web Interface
        sickbeard.WEB_IPV6 = config.checkbox_to_value(web_ipv6)
        # sickbeard.WEB_LOG is set in config.change_LOG_DIR()

        if not config.change_LOG_DIR(log_dir, web_log):
            results += ["Unable to create directory " + os.path.normpath(log_dir) + ", log directory not changed."]

        sickbeard.WEB_PORT = config.to_int(web_port, default=8081)

        sickbeard.WEB_USERNAME = web_username
        sickbeard.WEB_PASSWORD = web_password

        sickbeard.ENABLE_HTTPS = config.checkbox_to_value(enable_https)

        if not config.change_HTTPS_CERT(https_cert):
            results += ["Unable to create directory " + os.path.normpath(https_cert) + ", https cert directory not changed."]

        if not config.change_HTTPS_KEY(https_key):
            results += ["Unable to create directory " + os.path.normpath(https_key) + ", https key directory not changed."]

        # API
        sickbeard.USE_API = config.checkbox_to_value(use_api)
        sickbeard.API_KEY = api_key

        sickbeard.save_config()

        if len(results) > 0:
            for x in results:
                logger.log(x, logger.ERROR)
            ui.notifications.error('Error(s) Saving Configuration',
                        '<br />\n'.join(results))
        else:
            ui.notifications.message('Configuration Saved', ek.ek(os.path.join, sickbeard.CONFIG_FILE))

        redirect("/config/general/")


class ConfigSearch:

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="config_search.tmpl")
        t.submenu = ConfigMenu
        return _munge(t)

    @cherrypy.expose
    def saveSearch(self, use_nzbs=None, use_torrents=None, nzb_dir=None, sab_username=None, sab_password=None,
                       sab_apikey=None, sab_category=None, sab_host=None, nzbget_username=None, nzbget_password=None, nzbget_category=None, nzbget_host=None,
                       torrent_dir=None, nzb_method=None, usenet_retention=None, search_frequency=None, download_propers=None, ignore_words=None):

        results = []

        # Episode Search
        sickbeard.DOWNLOAD_PROPERS = config.checkbox_to_value(download_propers)

        config.change_SEARCH_FREQUENCY(search_frequency)
        sickbeard.USENET_RETENTION = config.to_int(usenet_retention, default=500)

        sickbeard.IGNORE_WORDS = ignore_words

        # NZB Search
        sickbeard.USE_NZBS = config.checkbox_to_value(use_nzbs)
        sickbeard.NZB_METHOD = nzb_method

        sickbeard.SAB_HOST = config.clean_url(sab_host)
        sickbeard.SAB_USERNAME = sab_username
        sickbeard.SAB_PASSWORD = sab_password
        sickbeard.SAB_APIKEY = sab_apikey.strip()
        sickbeard.SAB_CATEGORY = sab_category

        if not config.change_NZB_DIR(nzb_dir):
            results += ["Unable to create directory " + os.path.normpath(nzb_dir) + ", directory not changed."]

        sickbeard.NZBGET_HOST = config.clean_url(nzbget_host)
        sickbeard.NZBGET_USERNAME = nzbget_username
        sickbeard.NZBGET_PASSWORD = nzbget_password
        sickbeard.NZBGET_CATEGORY = nzbget_category

        # Torrent Search
        sickbeard.USE_TORRENTS = config.checkbox_to_value(use_torrents)

        if not config.change_TORRENT_DIR(torrent_dir):
            results += ["Unable to create directory " + os.path.normpath(torrent_dir) + ", directory not changed."]

        sickbeard.save_config()

        if len(results) > 0:
            for x in results:
                logger.log(x, logger.ERROR)
            ui.notifications.error('Error(s) Saving Configuration',
                        '<br />\n'.join(results))
        else:
            ui.notifications.message('Configuration Saved', ek.ek(os.path.join, sickbeard.CONFIG_FILE))

        redirect("/config/search/")


class ConfigPostProcessing:

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="config_postProcessing.tmpl")
        t.submenu = ConfigMenu
        return _munge(t)

    @cherrypy.expose
    def savePostProcessing(self, naming_pattern=None, naming_multi_ep=None,
                    xbmc_data=None, xbmc_12plus_data=None, mediabrowser_data=None, sony_ps3_data=None, wdtv_data=None, tivo_data=None, mede8er_data=None,
                    keep_processed_dir=None, process_automatically=None, rename_episodes=None,
                    move_associated_files=None, tv_download_dir=None, naming_custom_abd=None, naming_abd_pattern=None):

        results = []

        # Post-Processing
        if not config.change_TV_DOWNLOAD_DIR(tv_download_dir):
            results += ["Unable to create directory " + os.path.normpath(tv_download_dir) + ", dir not changed."]

        sickbeard.KEEP_PROCESSED_DIR = config.checkbox_to_value(keep_processed_dir)
        sickbeard.MOVE_ASSOCIATED_FILES = config.checkbox_to_value(move_associated_files)
        sickbeard.RENAME_EPISODES = config.checkbox_to_value(rename_episodes)

        sickbeard.PROCESS_AUTOMATICALLY = config.checkbox_to_value(process_automatically)

        if sickbeard.PROCESS_AUTOMATICALLY:
            sickbeard.autoPostProcesserScheduler.silent = False
        else:
            sickbeard.autoPostProcesserScheduler.silent = True

        # Naming
        sickbeard.NAMING_CUSTOM_ABD = config.checkbox_to_value(naming_custom_abd)

        if self.isNamingValid(naming_pattern, naming_multi_ep) != "invalid":
            sickbeard.NAMING_PATTERN = naming_pattern
            sickbeard.NAMING_MULTI_EP = int(naming_multi_ep)
            sickbeard.NAMING_FORCE_FOLDERS = naming.check_force_season_folders()
        else:
            results.append("You tried saving an invalid naming config, not saving your naming settings")

        if self.isNamingValid(naming_abd_pattern, None, True) != "invalid":
            sickbeard.NAMING_ABD_PATTERN = naming_abd_pattern
        elif naming_custom_abd:
            results.append("You tried saving an invalid air-by-date naming config, not saving your air-by-date settings")

        # Metadata
        sickbeard.METADATA_XBMC = xbmc_data
        sickbeard.METADATA_XBMC_12PLUS = xbmc_12plus_data
        sickbeard.METADATA_MEDIABROWSER = mediabrowser_data
        sickbeard.METADATA_PS3 = sony_ps3_data
        sickbeard.METADATA_WDTV = wdtv_data
        sickbeard.METADATA_TIVO = tivo_data
        sickbeard.METADATA_MEDE8ER = mede8er_data

        sickbeard.metadata_provider_dict['XBMC'].set_config(sickbeard.METADATA_XBMC)
        sickbeard.metadata_provider_dict['XBMC 12+'].set_config(sickbeard.METADATA_XBMC_12PLUS)
        sickbeard.metadata_provider_dict['MediaBrowser'].set_config(sickbeard.METADATA_MEDIABROWSER)
        sickbeard.metadata_provider_dict['Sony PS3'].set_config(sickbeard.METADATA_PS3)
        sickbeard.metadata_provider_dict['WDTV'].set_config(sickbeard.METADATA_WDTV)
        sickbeard.metadata_provider_dict['TIVO'].set_config(sickbeard.METADATA_TIVO)
        sickbeard.metadata_provider_dict['Mede8er'].set_config(sickbeard.METADATA_MEDE8ER)

        # Save changes
        sickbeard.save_config()

        if len(results) > 0:
            for x in results:
                logger.log(x, logger.ERROR)
            ui.notifications.error('Error(s) Saving Configuration',
                        '<br />\n'.join(results))
        else:
            ui.notifications.message('Configuration Saved', ek.ek(os.path.join, sickbeard.CONFIG_FILE))

        redirect("/config/postProcessing/")

    @cherrypy.expose
    def testNaming(self, pattern=None, multi=None, abd=False):

        if multi != None:
            multi = int(multi)

        result = naming.test_name(pattern, multi, abd)

        result = ek.ek(os.path.join, result['dir'], result['name'])

        return result

    @cherrypy.expose
    def isNamingValid(self, pattern=None, multi=None, abd=False):
        if pattern == None:
            return "invalid"

        # air by date shows just need one check, we don't need to worry about season folders
        if abd:
            is_valid = naming.check_valid_abd_naming(pattern)
            require_season_folders = False

        else:
            # check validity of single and multi ep cases for the whole path
            is_valid = naming.check_valid_naming(pattern, multi)

            # check validity of single and multi ep cases for only the file name
            require_season_folders = naming.check_force_season_folders(pattern, multi)

        if is_valid and not require_season_folders:
            return "valid"
        elif is_valid and require_season_folders:
            return "seasonfolders"
        else:
            return "invalid"


class ConfigProviders:

    @cherrypy.expose
    def index(self):
        t = PageTemplate(file="config_providers.tmpl")
        t.submenu = ConfigMenu
        return _munge(t)

    @cherrypy.expose
    def canAddNewznabProvider(self, name):

        if not name:
            return json.dumps({'error': 'No Provider Name specified'})

        providerDict = dict(zip([x.getID() for x in sickbeard.newznabProviderList], sickbeard.newznabProviderList))

        tempProvider = newznab.NewznabProvider(name, '')

        if tempProvider.getID() in providerDict:
            return json.dumps({'error': 'Provider Name already exists as ' + providerDict[tempProvider.getID()].name})
        else:
            return json.dumps({'success': tempProvider.getID()})

    @cherrypy.expose
    def saveNewznabProvider(self, name, url, key=''):

        if not name or not url:
            return '0'

        providerDict = dict(zip([x.name for x in sickbeard.newznabProviderList], sickbeard.newznabProviderList))

        if name in providerDict:
            if not providerDict[name].default:
                providerDict[name].name = name
                providerDict[name].url = config.clean_url(url)

            providerDict[name].key = key
            # a 0 in the key spot indicates that no key is needed
            if key == '0':
                providerDict[name].needs_auth = False
            else:
                providerDict[name].needs_auth = True

            return providerDict[name].getID() + '|' + providerDict[name].configStr()

        else:

            newProvider = newznab.NewznabProvider(name, url, key=key)
            sickbeard.newznabProviderList.append(newProvider)
            return newProvider.getID() + '|' + newProvider.configStr()

    @cherrypy.expose
    def deleteNewznabProvider(self, nnid):

        providerDict = dict(zip([x.getID() for x in sickbeard.newznabProviderList], sickbeard.newznabProviderList))

        if nnid not in providerDict or providerDict[nnid].default:
            return '0'

        # delete it from the list
        sickbeard.newznabProviderList.remove(providerDict[nnid])

        if nnid in sickbeard.PROVIDER_ORDER:
            sickbeard.PROVIDER_ORDER.remove(nnid)

        return '1'

    @cherrypy.expose
    def saveProviders(self,
                      newznab_string='',
                      omgwtfnzbs_username=None, omgwtfnzbs_apikey=None,
                      tvtorrents_digest=None, tvtorrents_hash=None,
                      torrentleech_key=None,
                      btn_api_key=None, hdbits_username=None, hdbits_passkey=None,
                      provider_order=None):

        results = []

        provider_str_list = provider_order.split()
        provider_list = []

        newznabProviderDict = dict(zip([x.getID() for x in sickbeard.newznabProviderList], sickbeard.newznabProviderList))

        finishedNames = []

        # add all the newznab info we got into our list
        if newznab_string:
            for curNewznabProviderStr in newznab_string.split('!!!'):

                if not curNewznabProviderStr:
                    continue

                cur_name, cur_url, cur_key = curNewznabProviderStr.split('|')
                cur_url = config.clean_url(cur_url)

                newProvider = newznab.NewznabProvider(cur_name, cur_url, key=cur_key)

                cur_id = newProvider.getID()

                # if it already exists then update it
                if cur_id in newznabProviderDict:
                    newznabProviderDict[cur_id].name = cur_name
                    newznabProviderDict[cur_id].url = cur_url
                    newznabProviderDict[cur_id].key = cur_key
                    # a 0 in the key spot indicates that no key is needed
                    if cur_key == '0':
                        newznabProviderDict[cur_id].needs_auth = False
                    else:
                        newznabProviderDict[cur_id].needs_auth = True

                else:
                    sickbeard.newznabProviderList.append(newProvider)

                finishedNames.append(cur_id)

            # delete anything that is missing
            for curProvider in sickbeard.newznabProviderList:
                if curProvider.getID() not in finishedNames:
                    sickbeard.newznabProviderList.remove(curProvider)

        # do the enable/disable
        for curProviderStr in provider_str_list:
            curProvider, curEnabled = curProviderStr.split(':')
            curEnabled = config.to_int(curEnabled)

            provider_list.append(curProvider)

            if curProvider == 'womble_s_index':
                sickbeard.WOMBLE = curEnabled
            elif curProvider == 'omgwtfnzbs':
                sickbeard.OMGWTFNZBS = curEnabled
            elif curProvider == 'ezrss':
                sickbeard.EZRSS = curEnabled
            elif curProvider == 'hdbits':
                sickbeard.HDBITS = curEnabled
            elif curProvider == 'tvtorrents':
                sickbeard.TVTORRENTS = curEnabled
            elif curProvider == 'torrentleech':
                sickbeard.TORRENTLEECH = curEnabled
            elif curProvider == 'btn':
                sickbeard.BTN = curEnabled
            elif curProvider in newznabProviderDict:
                newznabProviderDict[curProvider].enabled = bool(curEnabled)
            else:
                logger.log(u"don't know what " + curProvider + " is, skipping")

        sickbeard.HDBITS_USERNAME = hdbits_username.strip()
        sickbeard.HDBITS_PASSKEY = hdbits_passkey.strip()

        sickbeard.TVTORRENTS_DIGEST = tvtorrents_digest.strip()
        sickbeard.TVTORRENTS_HASH = tvtorrents_hash.strip()

        sickbeard.TORRENTLEECH_KEY = torrentleech_key.strip()

        sickbeard.BTN_API_KEY = btn_api_key.strip()

        sickbeard.OMGWTFNZBS_USERNAME = omgwtfnzbs_username.strip()
        sickbeard.OMGWTFNZBS_APIKEY = omgwtfnzbs_apikey.strip()

        sickbeard.NEWZNAB_DATA = '!!!'.join([x.configStr() for x in sickbeard.newznabProviderList])
        sickbeard.PROVIDER_ORDER = provider_list

        sickbeard.save_config()

        if len(results) > 0:
            for x in results:
                logger.log(x, logger.ERROR)
            ui.notifications.error('Error(s) Saving Configuration',
                        '<br />\n'.join(results))
        else:
            ui.notifications.message('Configuration Saved', ek.ek(os.path.join, sickbeard.CONFIG_FILE))

        redirect("/config/providers/")


class ConfigNotifications:

    @cherrypy.expose
    def index(self):
        t = PageTemplate(file="config_notifications.tmpl")
        t.submenu = ConfigMenu
        return _munge(t)

    @cherrypy.expose
    def saveNotifications(self,
                          use_xbmc=None, xbmc_always_on=None, xbmc_notify_onsnatch=None, xbmc_notify_ondownload=None, xbmc_update_onlyfirst=None,
                              xbmc_update_library=None, xbmc_update_full=None, xbmc_host=None, xbmc_username=None, xbmc_password=None,
                          use_plex=None, plex_notify_onsnatch=None, plex_notify_ondownload=None, plex_update_library=None,
                              plex_server_host=None, plex_host=None, plex_username=None, plex_password=None,
                          use_growl=None, growl_notify_onsnatch=None, growl_notify_ondownload=None, growl_host=None, growl_password=None,
                          use_prowl=None, prowl_notify_onsnatch=None, prowl_notify_ondownload=None, prowl_api=None, prowl_priority=0,
                          use_twitter=None, twitter_notify_onsnatch=None, twitter_notify_ondownload=None,
                          use_boxcar=None, boxcar_notify_onsnatch=None, boxcar_notify_ondownload=None, boxcar_username=None,
                          use_pushover=None, pushover_notify_onsnatch=None, pushover_notify_ondownload=None, pushover_userkey=None,
                          use_libnotify=None, libnotify_notify_onsnatch=None, libnotify_notify_ondownload=None,
                          use_nmj=None, nmj_host=None, nmj_database=None, nmj_mount=None,
                          use_synoindex=None, synoindex_notify_onsnatch=None, synoindex_notify_ondownload=None, synoindex_update_library=None,
                          use_nmjv2=None, nmjv2_host=None, nmjv2_dbloc=None, nmjv2_database=None,
                          use_trakt=None, trakt_username=None, trakt_password=None, trakt_api=None,
                          use_pytivo=None, pytivo_notify_onsnatch=None, pytivo_notify_ondownload=None, pytivo_update_library=None,
                              pytivo_host=None, pytivo_share_name=None, pytivo_tivo_name=None,
                          use_nma=None, nma_notify_onsnatch=None, nma_notify_ondownload=None, nma_api=None, nma_priority=0):

        results = []

        # Home Theater / NAS
        sickbeard.USE_XBMC = config.checkbox_to_value(use_xbmc)
        sickbeard.XBMC_ALWAYS_ON = config.checkbox_to_value(xbmc_always_on)
        sickbeard.XBMC_NOTIFY_ONSNATCH = config.checkbox_to_value(xbmc_notify_onsnatch)
        sickbeard.XBMC_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(xbmc_notify_ondownload)
        sickbeard.XBMC_UPDATE_LIBRARY = config.checkbox_to_value(xbmc_update_library)
        sickbeard.XBMC_UPDATE_FULL = config.checkbox_to_value(xbmc_update_full)
        sickbeard.XBMC_UPDATE_ONLYFIRST = config.checkbox_to_value(xbmc_update_onlyfirst)
        sickbeard.XBMC_HOST = config.clean_hosts(xbmc_host)
        sickbeard.XBMC_USERNAME = xbmc_username
        sickbeard.XBMC_PASSWORD = xbmc_password

        sickbeard.USE_PLEX = config.checkbox_to_value(use_plex)
        sickbeard.PLEX_NOTIFY_ONSNATCH = config.checkbox_to_value(plex_notify_onsnatch)
        sickbeard.PLEX_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(plex_notify_ondownload)
        sickbeard.PLEX_UPDATE_LIBRARY = config.checkbox_to_value(plex_update_library)
        sickbeard.PLEX_SERVER_HOST = config.clean_host(plex_server_host)
        sickbeard.PLEX_HOST = config.clean_hosts(plex_host)
        sickbeard.PLEX_USERNAME = plex_username
        sickbeard.PLEX_PASSWORD = plex_password

        sickbeard.USE_NMJ = config.checkbox_to_value(use_nmj)
        sickbeard.NMJ_HOST = config.clean_host(nmj_host)
        sickbeard.NMJ_DATABASE = nmj_database
        sickbeard.NMJ_MOUNT = nmj_mount

        sickbeard.USE_NMJv2 = config.checkbox_to_value(use_nmjv2)
        sickbeard.NMJv2_HOST = config.clean_host(nmjv2_host)
        sickbeard.NMJv2_DATABASE = nmjv2_database
        sickbeard.NMJv2_DBLOC = nmjv2_dbloc

        sickbeard.USE_SYNOINDEX = config.checkbox_to_value(use_synoindex)
        sickbeard.SYNOINDEX_NOTIFY_ONSNATCH = config.checkbox_to_value(synoindex_notify_onsnatch)
        sickbeard.SYNOINDEX_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(synoindex_notify_ondownload)
        sickbeard.SYNOINDEX_UPDATE_LIBRARY = config.checkbox_to_value(synoindex_update_library)

        sickbeard.USE_PYTIVO = config.checkbox_to_value(use_pytivo)
        # sickbeard.PYTIVO_NOTIFY_ONSNATCH = config.checkbox_to_value(pytivo_notify_onsnatch)
        # sickbeard.PYTIVO_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(pytivo_notify_ondownload)
        # sickbeard.PYTIVO_UPDATE_LIBRARY = config.checkbox_to_value(pytivo_update_library)
        sickbeard.PYTIVO_HOST = config.clean_host(pytivo_host)
        sickbeard.PYTIVO_SHARE_NAME = pytivo_share_name
        sickbeard.PYTIVO_TIVO_NAME = pytivo_tivo_name

        # Devices
        sickbeard.USE_GROWL = config.checkbox_to_value(use_growl)
        sickbeard.GROWL_NOTIFY_ONSNATCH = config.checkbox_to_value(growl_notify_onsnatch)
        sickbeard.GROWL_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(growl_notify_ondownload)
        sickbeard.GROWL_HOST = config.clean_host(growl_host, default_port=23053)
        sickbeard.GROWL_PASSWORD = growl_password

        sickbeard.USE_PROWL = config.checkbox_to_value(use_prowl)
        sickbeard.PROWL_NOTIFY_ONSNATCH = config.checkbox_to_value(prowl_notify_onsnatch)
        sickbeard.PROWL_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(prowl_notify_ondownload)
        sickbeard.PROWL_API = prowl_api
        sickbeard.PROWL_PRIORITY = prowl_priority

        sickbeard.USE_LIBNOTIFY = config.checkbox_to_value(use_libnotify)
        sickbeard.LIBNOTIFY_NOTIFY_ONSNATCH = config.checkbox_to_value(libnotify_notify_onsnatch)
        sickbeard.LIBNOTIFY_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(libnotify_notify_ondownload)

        sickbeard.USE_PUSHOVER = config.checkbox_to_value(use_pushover)
        sickbeard.PUSHOVER_NOTIFY_ONSNATCH = config.checkbox_to_value(pushover_notify_onsnatch)
        sickbeard.PUSHOVER_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(pushover_notify_ondownload)
        sickbeard.PUSHOVER_USERKEY = pushover_userkey

        sickbeard.USE_BOXCAR = config.checkbox_to_value(use_boxcar)
        sickbeard.BOXCAR_NOTIFY_ONSNATCH = config.checkbox_to_value(boxcar_notify_onsnatch)
        sickbeard.BOXCAR_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(boxcar_notify_ondownload)
        sickbeard.BOXCAR_USERNAME = boxcar_username

        sickbeard.USE_NMA = config.checkbox_to_value(use_nma)
        sickbeard.NMA_NOTIFY_ONSNATCH = config.checkbox_to_value(nma_notify_onsnatch)
        sickbeard.NMA_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(nma_notify_ondownload)
        sickbeard.NMA_API = nma_api
        sickbeard.NMA_PRIORITY = nma_priority

        # Online
        sickbeard.USE_TWITTER = config.checkbox_to_value(use_twitter)
        sickbeard.TWITTER_NOTIFY_ONSNATCH = config.checkbox_to_value(twitter_notify_onsnatch)
        sickbeard.TWITTER_NOTIFY_ONDOWNLOAD = config.checkbox_to_value(twitter_notify_ondownload)

        sickbeard.USE_TRAKT = config.checkbox_to_value(use_trakt)
        sickbeard.TRAKT_USERNAME = trakt_username
        sickbeard.TRAKT_PASSWORD = trakt_password
        sickbeard.TRAKT_API = trakt_api

        sickbeard.save_config()

        if len(results) > 0:
            for x in results:
                logger.log(x, logger.ERROR)
            ui.notifications.error('Error(s) Saving Configuration',
                        '<br />\n'.join(results))
        else:
            ui.notifications.message('Configuration Saved', ek.ek(os.path.join, sickbeard.CONFIG_FILE))

        redirect("/config/notifications/")


class ConfigHidden:

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="config_hidden.tmpl")
        t.submenu = ConfigMenu
        return _munge(t)

    @cherrypy.expose
    def saveHidden(self, anon_redirect=None, git_path=None, extra_scripts=None, create_missing_show_dirs=None, add_shows_wo_dir=None):

        results = []

        sickbeard.ANON_REDIRECT = anon_redirect
        sickbeard.GIT_PATH = git_path
        sickbeard.EXTRA_SCRIPTS = [x.strip() for x in extra_scripts.split('|') if x.strip()]
        sickbeard.CREATE_MISSING_SHOW_DIRS = config.checkbox_to_value(create_missing_show_dirs)
        sickbeard.ADD_SHOWS_WO_DIR = config.checkbox_to_value(add_shows_wo_dir)

        sickbeard.save_config()

        if len(results) > 0:
            for x in results:
                logger.log(x, logger.ERROR)
            ui.notifications.error('Error(s) Saving Configuration',
                        '<br />\n'.join(results))
        else:
            ui.notifications.message('Configuration Saved', ek.ek(os.path.join, sickbeard.CONFIG_FILE))

        redirect("/config/hidden/")

    @cherrypy.expose
    def sbEnded(self, username=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        ltvdb_api_parms = sickbeard.TVDB_API_PARMS.copy()
        t = tvdb_api.Tvdb(**ltvdb_api_parms)

        results = []
        errMatch = []
        changeState = []

        myDB = db.DBConnection()
        sql_result = myDB.select("SELECT tvdb_id,show_name,status FROM tv_shows WHERE status != 'Continuing' ORDER BY show_id DESC LIMIT 400")
        myDB.connection.close()

        if (len(sql_result)) > 1:
            logger.log(u"There were " + str(len(sql_result)) + " shows in your database that need checking (limited to 400).", logger.MESSAGE)
            results.append("There were <b>" + str(len(sql_result)) + "</b> shows in your database that need checking (limited to 400).<br>")
        else:
            logger.log(u"There were no shows that needed to be checked at this time.", logger.MESSAGE)
            results.append("There were no shows that needed to be checked at this time.<br>")

        for ended_show in sql_result:

            tvdb_id = ended_show['tvdb_id']
            show_name = ended_show['show_name']
            status = ended_show['status']

            try:
                show = t[show_name]
            except:
                logger.log(u"Issue found when looking up \"%s\"" % (show_name), logger.ERROR)
                continue

            logger.log(u"Checking \"%s\" with local status \"%s\" against thetvdb" % (show_name, status), logger.MESSAGE)

            show_id = show['id']
            if int(tvdb_id) != int(show_id):
                logger.log("Warning: Issue matching \"%s\" on tvdb. Got \"%s\" and \"%s\"" % (show_name, tvdb_id, show_id), logger.ERROR)
                errMatch.append("<tr><td class='tvShow'><a target='_blank' href='%s/home/displayShow?show=%s'>%s</a></td><td>%s</td><td>%s</td>" % (sickbeard.WEB_ROOT, tvdb_id, show_name, tvdb_id, show_id))
            else:
                show_status = show['status']

                if not show_status:
                    show_status = ""

                if show_status != status:
                    changeState.append("<tr><td class='tvShow'><a target='_blank' href='%s/home/displayShow?show=%s'>%s</a></td><td>%s</td><td>%s</td>" % (sickbeard.WEB_ROOT, tvdb_id, show_name, status, show_status))

            show.clear()  # needed to free up memory since python's garbage collection would keep this around

        if len(errMatch):
            errMatch.insert(0, "<br>These shows need to be removed then added back to Sick Beard to correct their TVDBID.<br><table class='tablesorter'><thead><tr><th>show name</th><th>local tvdbid</th><th>remote tvdbid</th></tr></thead>")
            errMatch.append("</table>")
            results += errMatch

        if len(changeState):
            changeState.insert(0, "<br>These shows need to have 'force full update' ran on them to correct their status.<br><table class='tablesorter'><thead><tr><th>show name</th><th>local status</th><th>remote status</th></tr></thead>")
            changeState.append("</table>")
            results += changeState

        return results


class Config:

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="config.tmpl")
        t.submenu = ConfigMenu
        return _munge(t)

    general = ConfigGeneral()

    search = ConfigSearch()

    postProcessing = ConfigPostProcessing()

    providers = ConfigProviders()

    notifications = ConfigNotifications()

    hidden = ConfigHidden()


def haveXBMC():
    return sickbeard.USE_XBMC and sickbeard.XBMC_UPDATE_LIBRARY


def havePLEX():
    return sickbeard.USE_PLEX and sickbeard.PLEX_UPDATE_LIBRARY


def HomeMenu():
    return [
        { 'title': 'Add Shows',              'path': 'home/addShows/',                                          },
        { 'title': 'Manual Post-Processing', 'path': 'home/postprocess/'                                        },
        { 'title': 'Update XBMC',            'path': 'home/updateXBMC/', 'requires': haveXBMC                   },
        { 'title': 'Update Plex',            'path': 'home/updatePLEX/', 'requires': havePLEX                   },
        { 'title': 'Restart',                'path': 'home/restart/?pid=' + str(sickbeard.PID), 'confirm': True   },
        { 'title': 'Shutdown',               'path': 'home/shutdown/?pid=' + str(sickbeard.PID), 'confirm': True  },
    ]


class HomePostProcess:

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="home_postprocess.tmpl")
        t.submenu = HomeMenu()
        return _munge(t)

    @cherrypy.expose
    def processEpisode(self, dir=None, nzbName=None, method=None, jobName=None, quiet=None, *args, **kwargs):

        if not dir:
            redirect("/home/postprocess/")
        else:
            pp_options = {}
            for key, value in kwargs.iteritems():
                if value == 'on':
                    value = True
                pp_options[key] = value

            result = processTV.processDir(dir, nzbName, method=method, pp_options=pp_options)
            if quiet != None and int(quiet) == 1:
                return result

            result = result.replace("\n", "<br />\n")
            return _genericMessage("Postprocessing results", result)


class NewHomeAddShows:

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="home_addShows.tmpl")
        t.submenu = HomeMenu()
        return _munge(t)

    @cherrypy.expose
    def getTVDBLanguages(self):
        result = tvdb_api.Tvdb().config['valid_languages']

        # Make sure list is sorted alphabetically but 'en' is in front
        if 'en' in result:
            del result[result.index('en')]
        result.sort()
        result.insert(0, 'en')

        return json.dumps({'results': result})

    @cherrypy.expose
    def sanitizeFileName(self, name):
        return helpers.sanitizeFileName(name)

    @cherrypy.expose
    def searchTVDBForShowName(self, name, lang="en"):
        if not lang or lang == 'null':
                lang = "en"

        baseURL = "http://thetvdb.com/api/GetSeries.php?"
        nameUTF8 = name.encode('utf-8')

        logger.log(u"Trying to find Show on thetvdb.com with: " + nameUTF8.decode('utf-8'), logger.DEBUG)

        # Use each word in the show's name as a possible search term
        keywords = nameUTF8.split(' ')

        # Insert the whole show's name as the first search term so best results are first
        # ex: keywords = ['Some Show Name', 'Some', 'Show', 'Name']
        if len(keywords) > 1:
            keywords.insert(0, nameUTF8)

        # Query the TVDB for each search term and build the list of results
        results = []

        for searchTerm in keywords:
            params = {'seriesname': searchTerm,
                  'language': lang}

            finalURL = baseURL + urllib.urlencode(params)

            logger.log(u"Searching for Show with searchterm: \'" + searchTerm.decode('utf-8') + u"\' on URL " + finalURL, logger.DEBUG)
            urlData = helpers.getURL(finalURL)

            if urlData is None:
                # When urlData is None, trouble connecting to TVDB, don't try the rest of the keywords
                logger.log(u"Unable to get URL: " + finalURL, logger.ERROR)
                break
            else:
                try:
                    seriesXML = etree.ElementTree(etree.XML(urlData))
                    series = seriesXML.getiterator('Series')

                except Exception, e:
                    # use finalURL in log, because urlData can be too much information
                    logger.log(u"Unable to parse XML for some reason: " + ex(e) + " from XML: " + finalURL, logger.ERROR)
                    series = ''

                # add each result to our list
                for curSeries in series:
                    tvdb_id = int(curSeries.findtext('seriesid'))

                    # don't add duplicates
                    if tvdb_id in [x[0] for x in results]:
                        continue

                    results.append((tvdb_id, curSeries.findtext('SeriesName'), curSeries.findtext('FirstAired')))

        lang_id = tvdb_api.Tvdb().config['langabbv_to_id'][lang]

        return json.dumps({'results': results, 'langid': lang_id})

    @cherrypy.expose
    def massAddTable(self, rootDir=None):
        t = PageTemplate(file="home_massAddTable.tmpl")
        t.submenu = HomeMenu()

        myDB = db.DBConnection()

        if not rootDir:
            return "No folders selected."
        elif type(rootDir) != list:
            root_dirs = [rootDir]
        else:
            root_dirs = rootDir

        root_dirs = [urllib.unquote_plus(x) for x in root_dirs]

        if sickbeard.ROOT_DIRS:
            default_index = int(sickbeard.ROOT_DIRS.split('|')[0])
        else:
            default_index = 0

        if len(root_dirs) > default_index:
            tmp = root_dirs[default_index]
            if tmp in root_dirs:
                root_dirs.remove(tmp)
                root_dirs = [tmp] + root_dirs

        dir_list = []

        for root_dir in root_dirs:
            try:
                file_list = ek.ek(os.listdir, root_dir)
            except:
                continue

            for cur_file in file_list:

                cur_path = ek.ek(os.path.normpath, ek.ek(os.path.join, root_dir, cur_file))
                if not ek.ek(os.path.isdir, cur_path):
                    continue

                cur_dir = {
                           'dir': cur_path,
                           'display_dir': '<b>' + ek.ek(os.path.dirname, cur_path) + os.sep + '</b>' + ek.ek(os.path.basename, cur_path),
                           }

                # see if the folder is in XBMC already
                dirResults = myDB.select("SELECT * FROM tv_shows WHERE location = ?", [cur_path])

                if dirResults:
                    cur_dir['added_already'] = True
                else:
                    cur_dir['added_already'] = False

                dir_list.append(cur_dir)

                tvdb_id = ''
                show_name = ''
                for cur_provider in sickbeard.metadata_provider_dict.values():
                    (tvdb_id, show_name) = cur_provider.retrieveShowMetadata(cur_path)
                    if tvdb_id and show_name:
                        break

                cur_dir['existing_info'] = (tvdb_id, show_name)

                if tvdb_id and helpers.findCertainShow(sickbeard.showList, tvdb_id):
                    cur_dir['added_already'] = True

        t.dirList = dir_list

        return _munge(t)

    @cherrypy.expose
    def newShow(self, show_to_add=None, other_shows=None):
        """
        Display the new show page which collects a tvdb id, folder, and extra options and
        posts them to addNewShow
        """
        t = PageTemplate(file="home_newShow.tmpl")
        t.submenu = HomeMenu()

        show_dir, tvdb_id, show_name = self.split_extra_show(show_to_add)

        if tvdb_id and show_name:
            use_provided_info = True
        else:
            use_provided_info = False

        # tell the template whether we're giving it show name & TVDB ID
        t.use_provided_info = use_provided_info

        # use the given show_dir for the tvdb search if available
        if not show_dir:
            t.default_show_name = ''
        elif not show_name:
            t.default_show_name = ek.ek(os.path.basename, ek.ek(os.path.normpath, show_dir)).replace('.', ' ')
        else:
            t.default_show_name = show_name

        # carry a list of other dirs if given
        if not other_shows:
            other_shows = []
        elif type(other_shows) != list:
            other_shows = [other_shows]

        if use_provided_info:
            t.provided_tvdb_id = tvdb_id
            t.provided_tvdb_name = show_name

        t.provided_show_dir = show_dir
        t.other_shows = other_shows

        return _munge(t)

    @cherrypy.expose
    def addNewShow(self, whichSeries=None, tvdbLang="en", rootDir=None, defaultStatus=None,
                   anyQualities=None, bestQualities=None, flatten_folders=None, fullShowPath=None,
                   other_shows=None, skipShow=None):
        """
        Receive tvdb id, dir, and other options and create a show from them. If extra show dirs are
        provided then it forwards back to newShow, if not it goes to /home.
        """

        # grab our list of other dirs if given
        if not other_shows:
            other_shows = []
        elif type(other_shows) != list:
            other_shows = [other_shows]

        def finishAddShow():
            # if there are no extra shows then go home
            if not other_shows:
                redirect("/home/")

            # peel off the next one
            next_show_dir = other_shows[0]
            rest_of_show_dirs = other_shows[1:]

            # go to add the next show
            return self.newShow(next_show_dir, rest_of_show_dirs)

        # if we're skipping then behave accordingly
        if skipShow:
            return finishAddShow()

        # sanity check on our inputs
        if (not rootDir and not fullShowPath) or not whichSeries:
            return "Missing params, no tvdb id or folder:" + repr(whichSeries) + " and " + repr(rootDir) + "/" + repr(fullShowPath)

        # figure out what show we're adding and where
        series_pieces = whichSeries.partition('|')
        if len(series_pieces) < 3:
            return "Error with show selection."

        tvdb_id = int(series_pieces[0])
        show_name = series_pieces[2]

        # use the whole path if it's given, or else append the show name to the root dir to get the full show path
        if fullShowPath:
            show_dir = ek.ek(os.path.normpath, fullShowPath)
        else:
            show_dir = ek.ek(os.path.join, rootDir, helpers.sanitizeFileName(show_name))

        # blanket policy - if the dir exists you should have used "add existing show" numbnuts
        if ek.ek(os.path.isdir, show_dir) and not fullShowPath:
            ui.notifications.error("Unable to add show", "Folder " + show_dir + " exists already")
            redirect("/home/addShows/existingShows/")

        # don't create show dir if config says not to
        if sickbeard.ADD_SHOWS_WO_DIR:
            logger.log(u"Skipping initial creation of " + show_dir + " due to config.ini setting")
        else:
            dir_exists = helpers.makeDir(show_dir)
            if not dir_exists:
                logger.log(u"Unable to create the folder " + show_dir + ", can't add the show", logger.ERROR)
                ui.notifications.error("Unable to add show", "Unable to create the folder " + show_dir + ", can't add the show")
                redirect("/home/")
            else:
                helpers.chmodAsParent(show_dir)

        # prepare the inputs for passing along
        flatten_folders = config.checkbox_to_value(flatten_folders)

        if not anyQualities:
            anyQualities = []
        if not bestQualities:
            bestQualities = []
        if type(anyQualities) != list:
            anyQualities = [anyQualities]
        if type(bestQualities) != list:
            bestQualities = [bestQualities]
        newQuality = Quality.combineQualities(map(int, anyQualities), map(int, bestQualities))

        # add the show
        sickbeard.showQueueScheduler.action.addShow(tvdb_id, show_dir, int(defaultStatus), newQuality, flatten_folders, tvdbLang)  # @UndefinedVariable
        ui.notifications.message('Show added', 'Adding the specified show into ' + show_dir)

        return finishAddShow()

    @cherrypy.expose
    def existingShows(self):
        """
        Prints out the page to add existing shows from a root dir
        """
        t = PageTemplate(file="home_addExistingShow.tmpl")
        t.submenu = HomeMenu()

        return _munge(t)

    def split_extra_show(self, extra_show):
        if not extra_show:
            return (None, None, None)
        split_vals = extra_show.split('|')
        if len(split_vals) < 3:
            return (extra_show, None, None)
        show_dir = split_vals[0]
        tvdb_id = split_vals[1]
        show_name = '|'.join(split_vals[2:])

        return (show_dir, tvdb_id, show_name)

    @cherrypy.expose
    def addExistingShows(self, shows_to_add=None, promptForSettings=None):
        """
        Receives a dir list and add them. Adds the ones with given TVDB IDs first, then forwards
        along to the newShow page.
        """

        # grab a list of other shows to add, if provided
        if not shows_to_add:
            shows_to_add = []
        elif type(shows_to_add) != list:
            shows_to_add = [shows_to_add]

        shows_to_add = [urllib.unquote_plus(x) for x in shows_to_add]

        promptForSettings = config.checkbox_to_value(promptForSettings)

        tvdb_id_given = []
        dirs_only = []
        # separate all the ones with TVDB IDs
        for cur_dir in shows_to_add:
            if not '|' in cur_dir:
                dirs_only.append(cur_dir)
            else:
                show_dir, tvdb_id, show_name = self.split_extra_show(cur_dir)
                if not show_dir or not tvdb_id or not show_name:
                    continue
                tvdb_id_given.append((show_dir, int(tvdb_id), show_name))

        # if they want me to prompt for settings then I will just carry on to the newShow page
        if promptForSettings and shows_to_add:
            return self.newShow(shows_to_add[0], shows_to_add[1:])

        # if they don't want me to prompt for settings then I can just add all the nfo shows now
        num_added = 0
        for cur_show in tvdb_id_given:
            show_dir, tvdb_id, show_name = cur_show

            # add the show
            sickbeard.showQueueScheduler.action.addShow(tvdb_id, show_dir, SKIPPED, sickbeard.QUALITY_DEFAULT, sickbeard.FLATTEN_FOLDERS_DEFAULT)  # @UndefinedVariable
            num_added += 1

        if num_added:
            ui.notifications.message("Shows Added", "Automatically added " + str(num_added) + " from their existing metadata files")

        # if we're done then go home
        if not dirs_only:
            redirect("/home/")

        # for the remaining shows we need to prompt for each one, so forward this on to the newShow page
        return self.newShow(dirs_only[0], dirs_only[1:])


ErrorLogsMenu = [
    { 'title': 'Clear Errors', 'path': 'errorlogs/clearerrors/' },
    #{ 'title': 'View Log',  'path': 'errorlogs/viewlog'  },
]


class ErrorLogs:

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="errorlogs.tmpl")
        t.submenu = ErrorLogsMenu

        return _munge(t)

    @cherrypy.expose
    def clearerrors(self):
        classes.ErrorViewer.clear()
        redirect("/errorlogs/")

    @cherrypy.expose
    def viewlog(self, minLevel=logger.MESSAGE, maxLines=500):

        t = PageTemplate(file="viewlogs.tmpl")
        t.submenu = ErrorLogsMenu

        minLevel = int(minLevel)

        data = []
        if os.path.isfile(logger.sb_log_instance.log_file_path):
            with ek.ek(open, logger.sb_log_instance.log_file_path) as f:
                data = f.readlines()

        regex = "^(\d\d\d\d)\-(\d\d)\-(\d\d)\s*(\d\d)\:(\d\d):(\d\d)\s*([A-Z]+)\s*(.+?)\s*\:\:\s*(.*)$"

        finalData = []

        numLines = 0
        lastLine = False
        numToShow = min(maxLines, len(data))

        for x in reversed(data):

            x = x.decode('utf-8')
            match = re.match(regex, x)

            if match:
                level = match.group(7)
                if level not in logger.reverseNames:
                    lastLine = False
                    continue

                if logger.reverseNames[level] >= minLevel:
                    lastLine = True
                    finalData.append(x)
                else:
                    lastLine = False
                    continue

            elif lastLine:
                finalData.append("AA" + x)

            numLines += 1

            if numLines >= numToShow:
                break

        result = "".join(finalData)

        t.logLines = result
        t.minLevel = minLevel

        return _munge(t)


class Home:

    @cherrypy.expose
    def is_alive(self, *args, **kwargs):
        if 'callback' in kwargs and '_' in kwargs:
            callback, _ = kwargs['callback'], kwargs['_']
        else:
            return "Error: Unsupported Request. Send jsonp request with 'callback' variable in the query stiring."
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"
        cherrypy.response.headers['Content-Type'] = 'text/javascript'
        cherrypy.response.headers['Access-Control-Allow-Origin'] = '*'
        cherrypy.response.headers['Access-Control-Allow-Headers'] = 'x-requested-with'

        if sickbeard.started:
            return callback + '(' + json.dumps({"msg": str(sickbeard.PID)}) + ');'
        else:
            return callback + '(' + json.dumps({"msg": "nope"}) + ');'

    @cherrypy.expose
    def index(self):

        t = PageTemplate(file="home.tmpl")
        t.submenu = HomeMenu()
        return _munge(t)

    addShows = NewHomeAddShows()

    postprocess = HomePostProcess()

    @cherrypy.expose
    def testSABnzbd(self, host=None, username=None, password=None, apikey=None):

        host = config.clean_url(host)

        connection, accesMsg = sab.getSabAccesMethod(host, username, password, apikey)
        if connection:
            authed, authMsg = sab.testAuthentication(host, username, password, apikey)  # @UnusedVariable
            if authed:
                return "Success. Connected and authenticated"
            else:
                return "Authentication failed. SABnzbd expects '" + accesMsg + "' as authentication method"
        else:
            return "Unable to connect to host"

    @cherrypy.expose
    def testGrowl(self, host=None, password=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        host = config.clean_host(host, default_port=23053)

        result = notifiers.growl_notifier.test_notify(host, password)
        if password == None or password == '':
            pw_append = ''
        else:
            pw_append = " with password: " + password

        if result:
            return "Registered and Tested growl successfully " + urllib.unquote_plus(host) + pw_append
        else:
            return "Registration and Testing of growl failed " + urllib.unquote_plus(host) + pw_append

    @cherrypy.expose
    def testProwl(self, prowl_api=None, prowl_priority=0):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        result = notifiers.prowl_notifier.test_notify(prowl_api, prowl_priority)
        if result:
            return "Test prowl notice sent successfully"
        else:
            return "Test prowl notice failed"

    @cherrypy.expose
    def testBoxcar(self, username=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        result = notifiers.boxcar_notifier.test_notify(username)
        if result:
            return "Boxcar notification succeeded. Check your Boxcar clients to make sure it worked"
        else:
            return "Error sending Boxcar notification"

    @cherrypy.expose
    def testPushover(self, userKey=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        result = notifiers.pushover_notifier.test_notify(userKey)
        if result:
            return "Pushover notification succeeded. Check your Pushover clients to make sure it worked"
        else:
            return "Error sending Pushover notification"

    @cherrypy.expose
    def twitterStep1(self):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        return notifiers.twitter_notifier._get_authorization()

    @cherrypy.expose
    def twitterStep2(self, key):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        result = notifiers.twitter_notifier._get_credentials(key)
        logger.log(u"result: " + str(result))
        if result:
            return "Key verification successful"
        else:
            return "Unable to verify key"

    @cherrypy.expose
    def testTwitter(self):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        result = notifiers.twitter_notifier.test_notify()
        if result:
            return "Tweet successful, check your twitter to make sure it worked"
        else:
            return "Error sending Tweet"

    @cherrypy.expose
    def testXBMC(self, host=None, username=None, password=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        host = config.clean_hosts(host)
        finalResult = ''

        for curHost in [x.strip() for x in host.split(",")]:
            curResult = notifiers.xbmc_notifier.test_notify(urllib.unquote_plus(curHost), username, password)
            if len(curResult.split(":")) > 2 and 'OK' in curResult.split(":")[2]:
                finalResult += "Test XBMC notice sent successfully to " + urllib.unquote_plus(curHost)
            else:
                finalResult += "Test XBMC notice failed to " + urllib.unquote_plus(curHost)
            finalResult += "<br />\n"

        return finalResult

    @cherrypy.expose
    def testPLEX(self, host=None, username=None, password=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        host = config.clean_hosts(host)
        finalResult = ''

        for curHost in [x.strip() for x in host.split(",")]:
            curResult = notifiers.plex_notifier.test_notify(urllib.unquote_plus(curHost), username, password)
            if len(curResult.split(":")) > 2 and 'OK' in curResult.split(":")[2]:
                finalResult += "Test Plex notice sent successfully to " + urllib.unquote_plus(curHost)
            else:
                finalResult += "Test Plex notice failed to " + urllib.unquote_plus(curHost)
            finalResult += "<br />\n"

        return finalResult

    @cherrypy.expose
    def testLibnotify(self):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        if notifiers.libnotify_notifier.test_notify():
            return "Tried sending desktop notification via libnotify"
        else:
            return notifiers.libnotify.diagnose()

    @cherrypy.expose
    def testNMJ(self, host=None, database=None, mount=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        host = config.clean_host(host)
        result = notifiers.nmj_notifier.test_notify(urllib.unquote_plus(host), database, mount)
        if result:
            return "Successfully started the scan update for NMJ"
        else:
            return "Failed to start the scan update for NMJ"

    @cherrypy.expose
    def settingsNMJ(self, host=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        host = config.clean_host(host)
        result = notifiers.nmj_notifier.notify_settings(urllib.unquote_plus(host))
        if result:
            return '{"message": "Got settings from %(host)s", "database": "%(database)s", "mount": "%(mount)s"}' % {"host": host, "database": sickbeard.NMJ_DATABASE, "mount": sickbeard.NMJ_MOUNT}
        else:
            return '{"message": "Failed! Make sure your Popcorn is on and NMJ is running. (see Log & Errors -> Debug for detailed info)", "database": "", "mount": ""}'

    @cherrypy.expose
    def testNMJv2(self, host=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        host = config.clean_host(host)
        result = notifiers.nmjv2_notifier.test_notify(urllib.unquote_plus(host))
        if result:
            return "Successfully started the scan update for NMJv2"
        else:
            return "Failed to start the scan update for NMJv2"

    @cherrypy.expose
    def settingsNMJv2(self, host=None, dbloc=None, instance=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        host = config.clean_host(host)
        result = notifiers.nmjv2_notifier.notify_settings(urllib.unquote_plus(host), dbloc, instance)
        if result:
            return '{"message": "NMJv2 Database found at: %(host)s", "database": "%(database)s"}' % {"host": host, "database": sickbeard.NMJv2_DATABASE}
        else:
            return '{"message": "Unable to find NMJv2 Database at location: %(dbloc)s. Is the right location selected and PCH running?", "database": ""}' % {"dbloc": dbloc}

    @cherrypy.expose
    def testTrakt(self, api=None, username=None, password=None):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        result = notifiers.trakt_notifier.test_notify(api, username, password)
        if result:
            return "Test notice sent successfully to Trakt"
        else:
            return "Test notice failed to Trakt"

    @cherrypy.expose
    def testNMA(self, nma_api=None, nma_priority=0):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        result = notifiers.nma_notifier.test_notify(nma_api, nma_priority)
        if result:
            return "Test NMA notice sent successfully"
        else:
            return "Test NMA notice failed"

    @cherrypy.expose
    def testSynoNotify(self):
        cherrypy.response.headers['Cache-Control'] = "max-age=0,no-cache,no-store"

        result = notifiers.synoindex_notifier.test_notify()
        if result:
            return "Test Synology notice sent successfully"
        else:
            return "Test Synology notice failed"

    @cherrypy.expose
    def shutdown(self, pid=None):

        if str(pid) != str(sickbeard.PID):
            redirect("/home/")

        threading.Timer(2, sickbeard.invoke_shutdown).start()

        title = "Shutting down"
        message = "Sick Beard is shutting down..."

        return _genericMessage(title, message)

    @cherrypy.expose
    def restart(self, pid=None):

        if str(pid) != str(sickbeard.PID):
            redirect("/home/")

        t = PageTemplate(file="restart.tmpl")
        t.submenu = HomeMenu()

        # do a soft restart
        threading.Timer(2, sickbeard.invoke_restart, [False]).start()

        return _munge(t)

    @cherrypy.expose
    def update(self, pid=None):

        if str(pid) != str(sickbeard.PID):
            redirect("/home/")

        updated = sickbeard.versionCheckScheduler.action.update()  # @UndefinedVariable

        if updated:
            # do a hard restart
            threading.Timer(2, sickbeard.invoke_restart, [False]).start()
            t = PageTemplate(file="restart_bare.tmpl")
            return _munge(t)
        else:
            return _genericMessage("Update Failed", "Update wasn't successful, not restarting. Check your log for more information.")

    @cherrypy.expose
    def displayShow(self, show=None):

        if show == None:
            return _genericMessage("Error", "Invalid show ID")
        else:
            showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

            if showObj == None:
                return _genericMessage("Error", "Show not in show list")

        myDB = db.DBConnection()

        seasonResults = myDB.select(
            "SELECT DISTINCT season FROM tv_episodes WHERE showid = ? ORDER BY season desc",
            [showObj.tvdbid]
        )

        sqlResults = myDB.select(
            "SELECT * FROM tv_episodes WHERE showid = ? ORDER BY season DESC, episode DESC",
            [showObj.tvdbid]
        )

        t = PageTemplate(file="displayShow.tmpl")
        t.submenu = [ { 'title': 'Edit', 'path': 'home/editShow?show=%d' % showObj.tvdbid } ]

        try:
            t.showLoc = (showObj.location, True)
        except sickbeard.exceptions.ShowDirNotFoundException:
            t.showLoc = (showObj._location, False)

        show_message = ''

        if sickbeard.showQueueScheduler.action.isBeingAdded(showObj):  # @UndefinedVariable
            show_message = 'This show is in the process of being downloaded from theTVDB.com - the info below is incomplete.'

        elif sickbeard.showQueueScheduler.action.isBeingUpdated(showObj):  # @UndefinedVariable
            show_message = 'The information below is in the process of being updated.'

        elif sickbeard.showQueueScheduler.action.isBeingRefreshed(showObj):  # @UndefinedVariable
            show_message = 'The episodes below are currently being refreshed from disk'

        elif sickbeard.showQueueScheduler.action.isInRefreshQueue(showObj):  # @UndefinedVariable
            show_message = 'This show is queued to be refreshed.'

        elif sickbeard.showQueueScheduler.action.isInUpdateQueue(showObj):  # @UndefinedVariable
            show_message = 'This show is queued and awaiting an update.'

        if not sickbeard.showQueueScheduler.action.isBeingAdded(showObj):  # @UndefinedVariable
            if not sickbeard.showQueueScheduler.action.isBeingUpdated(showObj):  # @UndefinedVariable
                t.submenu.append({ 'title': 'Delete',               'path': 'home/deleteShow?show=%d' % showObj.tvdbid, 'confirm': True })
                t.submenu.append({ 'title': 'Re-scan files',        'path': 'home/refreshShow?show=%d' % showObj.tvdbid })
                t.submenu.append({ 'title': 'Force Full Update',    'path': 'home/updateShow?show=%d&amp;force=1' % showObj.tvdbid })
                t.submenu.append({ 'title': 'Update show in XBMC',  'path': 'home/updateXBMC?show=%d' % showObj.tvdbid, 'requires': haveXBMC })
                t.submenu.append({ 'title': 'Preview Rename',       'path': 'home/testRename?show=%d' % showObj.tvdbid })

        t.show = showObj
        t.sqlResults = sqlResults
        t.seasonResults = seasonResults
        t.show_message = show_message

        epCounts = {}
        epCats = {}
        epCounts[Overview.SKIPPED] = 0
        epCounts[Overview.WANTED] = 0
        epCounts[Overview.QUAL] = 0
        epCounts[Overview.GOOD] = 0
        epCounts[Overview.UNAIRED] = 0
        epCounts[Overview.SNATCHED] = 0

        for curResult in sqlResults:

            curEpCat = showObj.getOverview(int(curResult["status"]))
            epCats[str(curResult["season"]) + "x" + str(curResult["episode"])] = curEpCat
            epCounts[curEpCat] += 1

        def titler(x):
            if not x:
                return x
            if x.lower().startswith('a '):
                    x = x[2:]
            elif x.lower().startswith('the '):
                    x = x[4:]
            return x
        t.sortedShowList = sorted(sickbeard.showList, lambda x, y: cmp(titler(x.name), titler(y.name)))

        t.epCounts = epCounts
        t.epCats = epCats

        return _munge(t)

    @cherrypy.expose
    def plotDetails(self, show, season, episode):
        result = db.DBConnection().action("SELECT description FROM tv_episodes WHERE showid = ? AND season = ? AND episode = ?", (show, season, episode)).fetchone()
        return result['description'] if result else 'Episode not found.'

    @cherrypy.expose
    def editShow(self, show=None, location=None, anyQualities=[], bestQualities=[], flatten_folders=None, paused=None, directCall=False, air_by_date=None, tvdbLang=None, rls_ignore_words=None, rls_require_words=None):

        if show == None:
            errString = "Invalid show ID: " + str(show)
            if directCall:
                return [errString]
            else:
                return _genericMessage("Error", errString)

        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

        if showObj == None:
            errString = "Unable to find the specified show: " + str(show)
            if directCall:
                return [errString]
            else:
                return _genericMessage("Error", errString)

        if not location and not anyQualities and not bestQualities and not flatten_folders:

            t = PageTemplate(file="editShow.tmpl")
            t.submenu = HomeMenu()
            with showObj.lock:
                t.show = showObj

            return _munge(t)

        flatten_folders = config.checkbox_to_value(flatten_folders)
        paused = config.checkbox_to_value(paused)
        air_by_date = config.checkbox_to_value(air_by_date)

        if tvdbLang and tvdbLang in tvdb_api.Tvdb().config['valid_languages']:
            tvdb_lang = tvdbLang
        else:
            tvdb_lang = showObj.lang

        # if we changed the language then kick off an update
        if tvdb_lang == showObj.lang:
            do_update = False
        else:
            do_update = True

        if type(anyQualities) != list:
            anyQualities = [anyQualities]

        if type(bestQualities) != list:
            bestQualities = [bestQualities]

        errors = []
        with showObj.lock:
            newQuality = Quality.combineQualities(map(int, anyQualities), map(int, bestQualities))
            showObj.quality = newQuality

            # reversed for now
            if bool(showObj.flatten_folders) != bool(flatten_folders):
                showObj.flatten_folders = flatten_folders
                try:
                    sickbeard.showQueueScheduler.action.refreshShow(showObj)  # @UndefinedVariable
                except exceptions.CantRefreshException, e:
                    errors.append("Unable to refresh this show: " + ex(e))

            showObj.paused = paused

            # if this routine was called via the mass edit, do not change the options that are not passed
            if not directCall:
                showObj.air_by_date = air_by_date
                showObj.lang = tvdb_lang
                showObj.rls_ignore_words = rls_ignore_words.strip()
                showObj.rls_require_words = rls_require_words.strip()

            # if we change location clear the db of episodes, change it, write to db, and rescan
            if os.path.normpath(showObj._location) != os.path.normpath(location):
                logger.log(os.path.normpath(showObj._location) + " != " + os.path.normpath(location), logger.DEBUG)
                if not ek.ek(os.path.isdir, location):
                    errors.append("New location <tt>%s</tt> does not exist" % location)

                # don't bother if we're going to update anyway
                elif not do_update:
                    # change it
                    try:
                        showObj.location = location
                        try:
                            sickbeard.showQueueScheduler.action.refreshShow(showObj)  # @UndefinedVariable
                        except exceptions.CantRefreshException, e:
                            errors.append("Unable to refresh this show:" + ex(e))
                        # grab updated info from TVDB
                        #showObj.loadEpisodesFromTVDB()
                        # rescan the episodes in the new folder
                    except exceptions.NoNFOException:
                        errors.append("The folder at <tt>%s</tt> doesn't contain a tvshow.nfo - copy your files to that folder before you change the directory in Sick Beard." % location)

            # save it to the DB
            showObj.saveToDB()

        # force the update
        if do_update:
            try:
                sickbeard.showQueueScheduler.action.updateShow(showObj, True)  # @UndefinedVariable
                time.sleep(1)
            except exceptions.CantUpdateException, e:
                errors.append("Unable to force an update on the show.")

        if directCall:
            return errors

        if len(errors) > 0:
            ui.notifications.error('%d error%s while saving changes:' % (len(errors), "" if len(errors) == 1 else "s"),
                        '<ul>' + '\n'.join(['<li>%s</li>' % error for error in errors]) + "</ul>")

        redirect("/home/displayShow?show=" + show)

    @cherrypy.expose
    def deleteShow(self, show=None):

        if show == None:
            return _genericMessage("Error", "Invalid show ID")

        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

        if showObj == None:
            return _genericMessage("Error", "Unable to find the specified show")

        if sickbeard.showQueueScheduler.action.isBeingAdded(showObj) or sickbeard.showQueueScheduler.action.isBeingUpdated(showObj):  # @UndefinedVariable
            return _genericMessage("Error", "Shows can't be deleted while they're being added or updated.")

        showObj.deleteShow()

        ui.notifications.message('<b>%s</b> has been deleted' % showObj.name)
        redirect("/home/")

    @cherrypy.expose
    def refreshShow(self, show=None):

        if show == None:
            return _genericMessage("Error", "Invalid show ID")

        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

        if showObj == None:
            return _genericMessage("Error", "Unable to find the specified show")

        # force the update from the DB
        try:
            sickbeard.showQueueScheduler.action.refreshShow(showObj)  # @UndefinedVariable
        except exceptions.CantRefreshException, e:
            ui.notifications.error("Unable to refresh this show.", ex(e))

        time.sleep(3)

        redirect("/home/displayShow?show=" + str(showObj.tvdbid))

    @cherrypy.expose
    def updateShow(self, show=None, force=0):

        if show == None:
            return _genericMessage("Error", "Invalid show ID")

        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

        if showObj == None:
            return _genericMessage("Error", "Unable to find the specified show")

        # force the update
        try:
            sickbeard.showQueueScheduler.action.updateShow(showObj, bool(force))  # @UndefinedVariable
        except exceptions.CantUpdateException, e:
            ui.notifications.error("Unable to update this show.", ex(e))

        # just give it some time
        time.sleep(3)

        redirect("/home/displayShow?show=" + str(showObj.tvdbid))

    @cherrypy.expose
    def updateXBMC(self, show=None):
        if sickbeard.XBMC_UPDATE_ONLYFIRST:
            # only send update to first host in the list -- workaround for xbmc sql backend users
            host = sickbeard.XBMC_HOST.split(",")[0].strip()
        else:
            host = sickbeard.XBMC_HOST

        if show:
            show_obj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))
        else:
            show_obj = None

        if notifiers.xbmc_notifier.update_library(show_obj=show_obj):
            ui.notifications.message("Library update command sent to XBMC host(s): " + host)
        else:
            ui.notifications.error("Unable to contact one or more XBMC host(s): " + host)
        redirect("/home/")

    @cherrypy.expose
    def updatePLEX(self):
        if notifiers.plex_notifier.update_library():
            ui.notifications.message("Library update command sent to Plex Media Server host: " + sickbeard.PLEX_SERVER_HOST)
        else:
            ui.notifications.error("Unable to contact Plex Media Server host: " + sickbeard.PLEX_SERVER_HOST)
        redirect("/home/")

    @cherrypy.expose
    def setStatus(self, show=None, eps=None, status=None, direct=False):

        if show == None or eps == None or status == None:
            errMsg = "You must specify a show and at least one episode"
            if direct:
                ui.notifications.error('Error', errMsg)
                return json.dumps({'result': 'error'})
            else:
                return _genericMessage("Error", errMsg)

        if not statusStrings.has_key(int(status)):
            errMsg = "Invalid status"
            if direct:
                ui.notifications.error('Error', errMsg)
                return json.dumps({'result': 'error'})
            else:
                return _genericMessage("Error", errMsg)

        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

        if showObj == None:
            errMsg = "Error", "Show not in show list"
            if direct:
                ui.notifications.error('Error', errMsg)
                return json.dumps({'result': 'error'})
            else:
                return _genericMessage("Error", errMsg)

        segment_list = []

        if eps != None:

            for curEp in eps.split('|'):

                logger.log(u"Attempting to set status on episode " + curEp + " to " + status, logger.DEBUG)

                epInfo = curEp.split('x')

                epObj = showObj.getEpisode(int(epInfo[0]), int(epInfo[1]))

                if int(status) == WANTED:
                    # figure out what segment the episode is in and remember it so we can backlog it
                    if epObj.show.air_by_date:
                        ep_segment = str(epObj.airdate)[:7]
                    else:
                        ep_segment = epObj.season

                    if ep_segment not in segment_list:
                        segment_list.append(ep_segment)

                if epObj == None:
                    return _genericMessage("Error", "Episode couldn't be retrieved")

                with epObj.lock:
                    # don't let them mess up UNAIRED episodes
                    if epObj.status == UNAIRED:
                        logger.log(u"Refusing to change status of " + curEp + " because it is UNAIRED", logger.ERROR)
                        continue

                    if int(status) in Quality.DOWNLOADED and epObj.status not in Quality.SNATCHED + Quality.SNATCHED_PROPER + Quality.DOWNLOADED + [IGNORED] and not ek.ek(os.path.isfile, epObj.location):
                        logger.log(u"Refusing to change status of " + curEp + " to DOWNLOADED because it's not SNATCHED/DOWNLOADED", logger.ERROR)
                        continue

                    epObj.status = int(status)
                    epObj.saveToDB()

        msg = "Backlog was automatically started for the following seasons of <b>" + showObj.name + "</b>:<br /><ul>"
        for cur_segment in segment_list:
            msg += "<li>Season " + str(cur_segment) + "</li>"
            logger.log(u"Sending backlog for " + showObj.name + " season " + str(cur_segment) + " because some eps were set to wanted")
            cur_backlog_queue_item = search_queue.BacklogQueueItem(showObj, cur_segment)
            sickbeard.searchQueueScheduler.action.add_item(cur_backlog_queue_item)  # @UndefinedVariable
        msg += "</ul>"

        if segment_list:
            ui.notifications.message("Backlog started", msg)

        if direct:
            return json.dumps({'result': 'success'})
        else:
            redirect("/home/displayShow?show=" + show)

    @cherrypy.expose
    def testRename(self, show=None):

        if show == None:
            return _genericMessage("Error", "You must specify a show")

        showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

        if showObj == None:
            return _genericMessage("Error", "Show not in show list")

        try:
            show_loc = showObj.location  # @UnusedVariable
        except exceptions.ShowDirNotFoundException:
            return _genericMessage("Error", "Can't rename episodes when the show dir is missing.")

        ep_obj_rename_list = []

        ep_obj_list = showObj.getAllEpisodes(has_location=True)

        for cur_ep_obj in ep_obj_list:
            # Only want to rename if we have a location
            if cur_ep_obj.location:
                if cur_ep_obj.relatedEps:
                    # do we have one of multi-episodes in the rename list already
                    have_already = False
                    for cur_related_ep in cur_ep_obj.relatedEps + [cur_ep_obj]:
                        if cur_related_ep in ep_obj_rename_list:
                            have_already = True
                            break
                    if not have_already:
                        ep_obj_rename_list.append(cur_ep_obj)

                else:
                    ep_obj_rename_list.append(cur_ep_obj)

        if ep_obj_rename_list:
            # present season DESC episode DESC on screen
            ep_obj_rename_list.reverse()

        t = PageTemplate(file="testRename.tmpl")
        t.submenu = [{'title': 'Edit', 'path': 'home/editShow?show=%d' % showObj.tvdbid}]
        t.ep_obj_list = ep_obj_rename_list
        t.show = showObj

        return _munge(t)

    @cherrypy.expose
    def doRename(self, show=None, eps=None):

        if show == None or eps == None:
            errMsg = "You must specify a show and at least one episode"
            return _genericMessage("Error", errMsg)

        show_obj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

        if show_obj == None:
            errMsg = "Error", "Show not in show list"
            return _genericMessage("Error", errMsg)

        try:
            show_loc = show_obj.location  # @UnusedVariable
        except exceptions.ShowDirNotFoundException:
            return _genericMessage("Error", "Can't rename episodes when the show dir is missing.")

        myDB = db.DBConnection()

        if eps == None:
            redirect("/home/displayShow?show=" + show)

        for curEp in eps.split('|'):

            epInfo = curEp.split('x')

            # this is probably the worst possible way to deal with double eps but I've kinda painted myself into a corner here with this stupid database
            ep_result = myDB.select("SELECT * FROM tv_episodes WHERE showid = ? AND season = ? AND episode = ? AND 5=5", [show, epInfo[0], epInfo[1]])
            if not ep_result:
                logger.log(u"Unable to find an episode for " + curEp + ", skipping", logger.WARNING)
                continue

            related_eps_result = myDB.select("SELECT * FROM tv_episodes WHERE location = ? AND episode != ?", [ep_result[0]["location"], epInfo[1]])

            root_ep_obj = show_obj.getEpisode(int(epInfo[0]), int(epInfo[1]))
            root_ep_obj.relatedEps = []

            for cur_related_ep in related_eps_result:
                related_ep_obj = show_obj.getEpisode(int(cur_related_ep["season"]), int(cur_related_ep["episode"]))
                if related_ep_obj not in root_ep_obj.relatedEps:
                    root_ep_obj.relatedEps.append(related_ep_obj)

            root_ep_obj.rename()

        redirect("/home/displayShow?show=" + show)

    @cherrypy.expose
    def searchEpisode(self, show=None, season=None, episode=None):

        # retrieve the episode object and fail if we can't get one
        ep_obj = _getEpisode(show, season, episode)
        if isinstance(ep_obj, str):
            return json.dumps({'result': 'failure'})

        # make a queue item for it and put it on the queue
        ep_queue_item = search_queue.ManualSearchQueueItem(ep_obj)
        sickbeard.searchQueueScheduler.action.add_item(ep_queue_item)  # @UndefinedVariable

        # wait until the queue item tells us whether it worked or not
        while ep_queue_item.success == None:  # @UndefinedVariable
            time.sleep(1)

        # return the correct json value
        if ep_queue_item.success:
            return json.dumps({'result': statusStrings[ep_obj.status]})

        return json.dumps({'result': 'failure'})


class UI:

    @cherrypy.expose
    def add_message(self):

        ui.notifications.message('Test 1', 'This is test number 1')
        ui.notifications.error('Test 2', 'This is test number 2')

        return "ok"

    @cherrypy.expose
    def get_messages(self):
        messages = {}
        cur_notification_num = 1
        for cur_notification in ui.notifications.get_notifications():
            messages['notification-' + str(cur_notification_num)] = {'title': cur_notification.title,
                                                                   'message': cur_notification.message,
                                                                   'type': cur_notification.type}
            cur_notification_num += 1

        return json.dumps(messages)


class WebInterface:

    @cherrypy.expose
    def robots_txt(self):
        """ Keep web crawlers out """
        cherrypy.response.headers['Content-Type'] = 'text/plain'
        return 'User-agent: *\nDisallow: /\n'

    @cherrypy.expose
    def index(self):

        redirect("/home/")

    @cherrypy.expose
    def showPoster(self, show=None, which=None):

        if which == 'poster':
            default_image_name = 'poster.png'
        else:
            default_image_name = 'banner.png'

        default_image_path = ek.ek(os.path.join, sickbeard.PROG_DIR, 'data', 'images', default_image_name)
        if show is None:
            return cherrypy.lib.static.serve_file(default_image_path, content_type="image/png")
        else:
            showObj = sickbeard.helpers.findCertainShow(sickbeard.showList, int(show))

        if showObj is None:
            return cherrypy.lib.static.serve_file(default_image_path, content_type="image/png")

        cache_obj = image_cache.ImageCache()

        if which == 'poster':
            image_file_name = cache_obj.poster_path(showObj.tvdbid)
        # this is for 'banner' but also the default case
        else:
            image_file_name = cache_obj.banner_path(showObj.tvdbid)

        if ek.ek(os.path.isfile, image_file_name):
            # use startup argument to prevent using PIL even if installed
            if sickbeard.NO_RESIZE:
                return cherrypy.lib.static.serve_file(image_file_name, content_type="image/jpeg")
            try:
                from PIL import Image
                from cStringIO import StringIO
            except ImportError:  # PIL isn't installed
                return cherrypy.lib.static.serve_file(image_file_name, content_type="image/jpeg")
            else:
                im = Image.open(image_file_name)
                if im.mode == 'P':  # Convert GIFs to RGB
                    im = im.convert('RGB')
                if which == 'banner':
                    size = 606, 112
                elif which == 'poster':
                    size = 136, 200
                else:
                    return cherrypy.lib.static.serve_file(image_file_name, content_type="image/jpeg")
                im = im.resize(size, Image.ANTIALIAS)
                imgbuffer = StringIO()
                im.save(imgbuffer, 'JPEG', quality=85)
                cherrypy.response.headers['Content-Type'] = 'image/jpeg'
                return imgbuffer.getvalue()
        else:
            return cherrypy.lib.static.serve_file(default_image_path, content_type="image/png")

    @cherrypy.expose
    def setComingEpsLayout(self, layout):
        if layout not in ('poster', 'banner', 'list'):
            layout = 'banner'

        sickbeard.COMING_EPS_LAYOUT = layout

        redirect("/comingEpisodes/")

    @cherrypy.expose
    def toggleComingEpsDisplayPaused(self):

        sickbeard.COMING_EPS_DISPLAY_PAUSED = not sickbeard.COMING_EPS_DISPLAY_PAUSED

        redirect("/comingEpisodes/")

    @cherrypy.expose
    def setComingEpsSort(self, sort):
        if sort not in ('date', 'network', 'show'):
            sort = 'date'

        sickbeard.COMING_EPS_SORT = sort

        redirect("/comingEpisodes/")

    @cherrypy.expose
    def comingEpisodes(self, layout="None"):

        myDB = db.DBConnection()

        today = datetime.date.today().toordinal()
        next_week = (datetime.date.today() + datetime.timedelta(days=7)).toordinal()
        recently = (datetime.date.today() - datetime.timedelta(days=3)).toordinal()

        done_show_list = []
        qualList = Quality.DOWNLOADED + Quality.SNATCHED + [ARCHIVED, IGNORED]
        sql_results = myDB.select("SELECT *, tv_shows.status as show_status FROM tv_episodes, tv_shows WHERE season != 0 AND airdate >= ? AND airdate < ? AND tv_shows.tvdb_id = tv_episodes.showid AND tv_episodes.status NOT IN (" + ','.join(['?'] * len(qualList)) + ")", [today, next_week] + qualList)
        for cur_result in sql_results:
            done_show_list.append(int(cur_result["showid"]))

        more_sql_results = myDB.select("SELECT *, tv_shows.status as show_status FROM tv_episodes outer_eps, tv_shows WHERE season != 0 AND showid NOT IN (" + ','.join(['?'] * len(done_show_list)) + ") AND tv_shows.tvdb_id = outer_eps.showid AND airdate = (SELECT airdate FROM tv_episodes inner_eps WHERE inner_eps.season != 0 AND inner_eps.showid = outer_eps.showid AND inner_eps.airdate >= ? ORDER BY inner_eps.airdate ASC LIMIT 1) AND outer_eps.status NOT IN (" + ','.join(['?'] * len(Quality.DOWNLOADED + Quality.SNATCHED)) + ")", done_show_list + [next_week] + Quality.DOWNLOADED + Quality.SNATCHED)
        sql_results += more_sql_results

        more_sql_results = myDB.select("SELECT *, tv_shows.status as show_status FROM tv_episodes, tv_shows WHERE season != 0 AND tv_shows.tvdb_id = tv_episodes.showid AND airdate < ? AND airdate >= ? AND tv_episodes.status = ? AND tv_episodes.status NOT IN (" + ','.join(['?'] * len(qualList)) + ")", [today, recently, WANTED] + qualList)
        sql_results += more_sql_results

        # sort by air date
        sorts = {
            'date': (lambda x, y: cmp(int(x["airdate"]), int(y["airdate"]))),
            'show': (lambda a, b: cmp(a["show_name"], b["show_name"])),
            'network': (lambda a, b: cmp(a["network"], b["network"])),
        }

        sql_results.sort(sorts[sickbeard.COMING_EPS_SORT])

        t = PageTemplate(file="comingEpisodes.tmpl")
        paused_item = { 'title': '', 'path': 'toggleComingEpsDisplayPaused' }
        paused_item['title'] = 'Hide Paused' if sickbeard.COMING_EPS_DISPLAY_PAUSED else 'Show Paused'
        t.submenu = [
            { 'title': 'Sort by:', 'path': {'Date': 'setComingEpsSort/?sort=date',
                                            'Show': 'setComingEpsSort/?sort=show',
                                            'Network': 'setComingEpsSort/?sort=network',
                                           }},

            { 'title': 'Layout:', 'path': {'Banner': 'setComingEpsLayout/?layout=banner',
                                           'Poster': 'setComingEpsLayout/?layout=poster',
                                           'List': 'setComingEpsLayout/?layout=list',
                                           }},
            paused_item,
        ]

        t.next_week = next_week
        t.today = today
        t.sql_results = sql_results

        # allow local overriding of layout parameter
        if layout and layout in ('poster', 'banner', 'list'):
            t.layout = layout
        else:
            t.layout = sickbeard.COMING_EPS_LAYOUT

        return _munge(t)

    manage = Manage()

    history = History()

    config = Config()

    home = Home()

    api = Api()

    browser = browser.WebFileBrowser()

    errorlogs = ErrorLogs()

    ui = UI()

########NEW FILE########
__FILENAME__ = webserveInit
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import cherrypy.lib.auth_basic
import os.path

import sickbeard

from sickbeard import logger
from sickbeard.webserve import WebInterface

from sickbeard.helpers import create_https_certificates


def initWebServer(options={}):
    options.setdefault('port', 8081)
    options.setdefault('host', '0.0.0.0')
    options.setdefault('log_dir', None)
    options.setdefault('username', '')
    options.setdefault('password', '')
    options.setdefault('web_root', '/')
    assert isinstance(options['port'], int)
    assert 'data_root' in options

    def http_error_401_hander(status, message, traceback, version):
        """ Custom handler for 401 error """
        if status != "401 Unauthorized":
            logger.log(u"CherryPy caught an error: %s %s" % (status, message), logger.ERROR)
            logger.log(traceback, logger.DEBUG)
        return r'''<!DOCTYPE html>
<html>
    <head>
        <title>%s</title>
    </head>
    <body>
        <br/>
        <font color="#0000FF">Error %s: You need to provide a valid username and password.</font>
    </body>
</html>
''' % ('Access denied', status)

    def http_error_404_hander(status, message, traceback, version):
        """ Custom handler for 404 error, redirect back to main page """
        return r'''<!DOCTYPE html>
<html>
    <head>
        <title>404</title>
        <script type="text/javascript" charset="utf-8">
          <!--
          location.href = "%s/home/"
          //-->
        </script>
    </head>
    <body>
        <br/>
    </body>
</html>
''' % options['web_root']

    # cherrypy setup
    enable_https = options['enable_https']
    https_cert = options['https_cert']
    https_key = options['https_key']

    if enable_https:
        # If either the HTTPS certificate or key do not exist, make some self-signed ones.
        if not (https_cert and os.path.exists(https_cert)) or not (https_key and os.path.exists(https_key)):
            if not create_https_certificates(https_cert, https_key):
                logger.log(u"Unable to create CERT/KEY files, disabling HTTPS")
                sickbeard.ENABLE_HTTPS = False
                enable_https = False

        if not (os.path.exists(https_cert) and os.path.exists(https_key)):
            logger.log(u"Disabled HTTPS because of missing CERT and KEY files", logger.WARNING)
            sickbeard.ENABLE_HTTPS = False
            enable_https = False

    mime_gzip = ('text/html',
                 'text/plain',
                 'text/css',
                 'text/javascript',
                 'application/javascript',
                 'text/x-javascript',
                 'application/x-javascript',
                 'text/x-json',
                 'application/json'
                 )

    options_dict = {
        'server.socket_port': options['port'],
        'server.socket_host': options['host'],
        'log.screen': False,
        'engine.autoreload.on': False,
        'engine.autoreload.frequency': 100,
        'engine.reexec_retry': 100,
        'tools.gzip.on': True,
        'tools.gzip.mime_types': mime_gzip,
        'error_page.401': http_error_401_hander,
        'error_page.404': http_error_404_hander,
    }

    if enable_https:
        options_dict['server.ssl_certificate'] = https_cert
        options_dict['server.ssl_private_key'] = https_key
        protocol = "https"
    else:
        protocol = "http"

    logger.log(u"Starting Sick Beard on " + protocol + "://" + str(options['host']) + ":" + str(options['port']) + "/")
    cherrypy.config.update(options_dict)

    # setup cherrypy logging
    if options['log_dir'] and os.path.isdir(options['log_dir']):
        cherrypy.config.update({ 'log.access_file': os.path.join(options['log_dir'], "cherrypy.log") })
        logger.log(u'Using %s for cherrypy log' % cherrypy.config['log.access_file'])

    conf = {
        '/': {
            'tools.staticdir.root': options['data_root'],
            'tools.encode.on': True,
            'tools.encode.encoding': 'utf-8',
        },
        '/images': {
            'tools.staticdir.on': True,
            'tools.staticdir.dir': 'images'
        },
        '/js': {
            'tools.staticdir.on': True,
            'tools.staticdir.dir': 'js'
        },
        '/css': {
            'tools.staticdir.on': True,
            'tools.staticdir.dir': 'css'
        },
    }
    app = cherrypy.tree.mount(WebInterface(), options['web_root'], conf)

    # auth
    if options['username'] != "" and options['password'] != "":
        checkpassword = cherrypy.lib.auth_basic.checkpassword_dict({options['username']: options['password']})
        app.merge({
            '/': {
                'tools.auth_basic.on': True,
                'tools.auth_basic.realm': 'SickBeard',
                'tools.auth_basic.checkpassword': checkpassword
            },
            '/api': {
                'tools.auth_basic.on': False
            },
            '/api/builder': {
                'tools.auth_basic.on': True,
                'tools.auth_basic.realm': 'SickBeard',
                'tools.auth_basic.checkpassword': checkpassword
            }
        })

    cherrypy.server.start()
    cherrypy.server.wait()

########NEW FILE########
__FILENAME__ = SickBeard
#!/usr/bin/env python
# Author: Nic Wolfe <nic@wolfeden.ca>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

# Check needed software dependencies to nudge users to fix their setup
import sys
if sys.version_info < (2, 5):
    sys.exit("Sorry, requires Python 2.5, 2.6 or 2.7.")

try:
    import Cheetah
    if Cheetah.Version[0] != '2':
        raise ValueError
except ValueError:
    sys.exit("Sorry, requires Python module Cheetah 2.1.0 or newer.")
except:
    sys.exit("The Python module Cheetah is required")

# We only need this for compiling an EXE and I will just always do that on 2.6+
if sys.hexversion >= 0x020600F0:
    from multiprocessing import freeze_support  # @UnresolvedImport

import locale
import os
import threading
import time
import signal
import traceback
import getopt

import sickbeard

from sickbeard import db
from sickbeard.tv import TVShow
from sickbeard import logger
from sickbeard.version import SICKBEARD_VERSION

from sickbeard.webserveInit import initWebServer

from lib.configobj import ConfigObj

signal.signal(signal.SIGINT, sickbeard.sig_handler)
signal.signal(signal.SIGTERM, sickbeard.sig_handler)


def loadShowsFromDB():
    """
    Populates the showList with shows from the database
    """

    myDB = db.DBConnection()
    sqlResults = myDB.select("SELECT * FROM tv_shows")

    for sqlShow in sqlResults:
        try:
            curShow = TVShow(int(sqlShow["tvdb_id"]))
            sickbeard.showList.append(curShow)
        except Exception, e:
            logger.log(u"There was an error creating the show in " + sqlShow["location"] + ": " + str(e).decode('utf-8'), logger.ERROR)
            logger.log(traceback.format_exc(), logger.DEBUG)

        # TODO: update the existing shows if the showlist has something in it


def daemonize():
    """
    Fork off as a daemon
    """

    # pylint: disable=E1101
    # Make a non-session-leader child process
    try:
        pid = os.fork()  # @UndefinedVariable - only available in UNIX
        if pid != 0:
            os._exit(0)
    except OSError, e:
        sys.stderr.write("fork #1 failed: %d (%s)\n" % (e.errno, e.strerror))
        sys.exit(1)

    os.setsid()  # @UndefinedVariable - only available in UNIX

    # Make sure I can read my own files and shut out others
    prev = os.umask(0)
    os.umask(prev and int('077', 8))

    # Make the child a session-leader by detaching from the terminal
    try:
        pid = os.fork()  # @UndefinedVariable - only available in UNIX
        if pid != 0:
            os._exit(0)
    except OSError, e:
        sys.stderr.write("fork #2 failed: %d (%s)\n" % (e.errno, e.strerror))
        sys.exit(1)

    # Write pid
    if sickbeard.CREATEPID:
        pid = str(os.getpid())
        logger.log(u"Writing PID: " + pid + " to " + str(sickbeard.PIDFILE))
        try:
            file(sickbeard.PIDFILE, 'w').write("%s\n" % pid)
        except IOError, e:
            logger.log_error_and_exit(u"Unable to write PID file: " + sickbeard.PIDFILE + " Error: " + str(e.strerror) + " [" + str(e.errno) + "]")

    # Redirect all output
    sys.stdout.flush()
    sys.stderr.flush()

    devnull = getattr(os, 'devnull', '/dev/null')
    stdin = file(devnull, 'r')
    stdout = file(devnull, 'a+')
    stderr = file(devnull, 'a+')
    os.dup2(stdin.fileno(), sys.stdin.fileno())
    os.dup2(stdout.fileno(), sys.stdout.fileno())
    os.dup2(stderr.fileno(), sys.stderr.fileno())


def help_message():
    """
    print help message for commandline options
    """
    help_msg = "\n"
    help_msg += "Usage: " + sickbeard.MY_FULLNAME + " <option> <another option>\n"
    help_msg += "\n"
    help_msg += "Options:\n"
    help_msg += "\n"
    help_msg += "    -h          --help              Prints this message\n"
    help_msg += "    -f          --forceupdate       Force update all shows in the DB (from tvdb) on startup\n"
    help_msg += "    -q          --quiet             Disables logging to console\n"
    help_msg += "                --nolaunch          Suppress launching web browser on startup\n"

    if sys.platform == 'win32':
        help_msg += "    -d          --daemon            Running as real daemon is not supported on Windows\n"
        help_msg += "                                    On Windows, --daemon is substituted with: --quiet --nolaunch\n"
    else:
        help_msg += "    -d          --daemon            Run as double forked daemon (includes options --quiet --nolaunch)\n"
        help_msg += "                --pidfile=<path>    Combined with --daemon creates a pidfile (full path including filename)\n"

    help_msg += "    -p <port>   --port=<port>       Override default/configured port to listen on\n"
    help_msg += "                --datadir=<path>    Override folder (full path) as location for\n"
    help_msg += "                                    storing database, configfile, cache, logfiles \n"
    help_msg += "                                    Default: " + sickbeard.PROG_DIR + "\n"
    help_msg += "                --config=<path>     Override config filename (full path including filename)\n"
    help_msg += "                                    to load configuration from \n"
    help_msg += "                                    Default: config.ini in " + sickbeard.PROG_DIR + " or --datadir location\n"
    help_msg += "                --noresize          Prevent resizing of the banner/posters even if PIL is installed\n"

    return help_msg


def main():
    """
    TV for me
    """

    # do some preliminary stuff
    sickbeard.MY_FULLNAME = os.path.normpath(os.path.abspath(__file__))
    sickbeard.MY_NAME = os.path.basename(sickbeard.MY_FULLNAME)
    sickbeard.PROG_DIR = os.path.dirname(sickbeard.MY_FULLNAME)
    sickbeard.DATA_DIR = sickbeard.PROG_DIR
    sickbeard.MY_ARGS = sys.argv[1:]
    sickbeard.DAEMON = False
    sickbeard.CREATEPID = False

    sickbeard.SYS_ENCODING = None

    try:
        locale.setlocale(locale.LC_ALL, "")
        sickbeard.SYS_ENCODING = locale.getpreferredencoding()
    except (locale.Error, IOError):
        pass

    # For OSes that are poorly configured I'll just randomly force UTF-8
    if not sickbeard.SYS_ENCODING or sickbeard.SYS_ENCODING in ('ANSI_X3.4-1968', 'US-ASCII', 'ASCII'):
        sickbeard.SYS_ENCODING = 'UTF-8'

    if not hasattr(sys, "setdefaultencoding"):
        reload(sys)

    try:
        # pylint: disable=E1101
        # On non-unicode builds this will raise an AttributeError, if encoding type is not valid it throws a LookupError
        sys.setdefaultencoding(sickbeard.SYS_ENCODING)
    except:
        sys.exit("Sorry, you MUST add the Sick Beard folder to the PYTHONPATH environment variable\n" +
            "or find another way to force Python to use " + sickbeard.SYS_ENCODING + " for string encoding.")

    # Need console logging for SickBeard.py and SickBeard-console.exe
    consoleLogging = (not hasattr(sys, "frozen")) or (sickbeard.MY_NAME.lower().find('-console') > 0)

    # Rename the main thread
    threading.currentThread().name = "MAIN"

    try:
        opts, args = getopt.getopt(sys.argv[1:], "hfqdp::", ['help', 'forceupdate', 'quiet', 'nolaunch', 'daemon', 'pidfile=', 'port=', 'datadir=', 'config=', 'noresize'])  # @UnusedVariable
    except getopt.GetoptError:
        sys.exit(help_message())

    forceUpdate = False
    forcedPort = None
    noLaunch = False

    for o, a in opts:

        # Prints help message
        if o in ('-h', '--help'):
            sys.exit(help_message())

        # Should we update (from tvdb) all shows in the DB right away?
        if o in ('-f', '--forceupdate'):
            forceUpdate = True

        # Disables logging to console
        if o in ('-q', '--quiet'):
            consoleLogging = False

        # Suppress launching web browser
        # Needed for OSes without default browser assigned
        # Prevent duplicate browser window when restarting in the app
        if o in ('--nolaunch',):
            noLaunch = True

        # Run as a double forked daemon
        if o in ('-d', '--daemon'):
            sickbeard.DAEMON = True
            # When running as daemon disable consoleLogging and don't start browser
            consoleLogging = False
            noLaunch = True

            if sys.platform == 'win32':
                sickbeard.DAEMON = False

        # Write a pidfile if requested
        if o in ('--pidfile',):
            sickbeard.CREATEPID = True
            sickbeard.PIDFILE = str(a)

            # If the pidfile already exists, sickbeard may still be running, so exit
            if os.path.exists(sickbeard.PIDFILE):
                sys.exit("PID file: " + sickbeard.PIDFILE + " already exists. Exiting.")

        # Override default/configured port
        if o in ('-p', '--port'):
            try:
                forcedPort = int(a)
            except ValueError:
                sys.exit("Port: " + str(a) + " is not a number. Exiting.")

        # Specify folder to use as data dir (storing database, configfile, cache, logfiles)
        if o in ('--datadir',):
            sickbeard.DATA_DIR = os.path.abspath(a)

        # Specify filename to load the config information from
        if o in ('--config',):
            sickbeard.CONFIG_FILE = os.path.abspath(a)

        # Prevent resizing of the banner/posters even if PIL is installed
        if o in ('--noresize',):
            sickbeard.NO_RESIZE = True

    # The pidfile is only useful in daemon mode, make sure we can write the file properly
    if sickbeard.CREATEPID:
        if sickbeard.DAEMON:
            pid_dir = os.path.dirname(sickbeard.PIDFILE)
            if not os.access(pid_dir, os.F_OK):
                sys.exit("PID dir: " + pid_dir + " doesn't exist. Exiting.")
            if not os.access(pid_dir, os.W_OK):
                sys.exit("PID dir: " + pid_dir + " must be writable (write permissions). Exiting.")

        else:
            if consoleLogging:
                sys.stdout.write("Not running in daemon mode. PID file creation disabled.\n")

            sickbeard.CREATEPID = False

    # If they don't specify a config file then put it in the data dir
    if not sickbeard.CONFIG_FILE:
        sickbeard.CONFIG_FILE = os.path.join(sickbeard.DATA_DIR, "config.ini")

    # Make sure that we can create the data dir
    if not os.access(sickbeard.DATA_DIR, os.F_OK):
        try:
            os.makedirs(sickbeard.DATA_DIR, 0744)
        except os.error:
            sys.exit("Unable to create data directory: " + sickbeard.DATA_DIR + " Exiting.")

    # Make sure we can write to the data dir
    if not os.access(sickbeard.DATA_DIR, os.W_OK):
        sys.exit("Data directory: " + sickbeard.DATA_DIR + " must be writable (write permissions). Exiting.")

    # Make sure we can write to the config file
    if not os.access(sickbeard.CONFIG_FILE, os.W_OK):
        if os.path.isfile(sickbeard.CONFIG_FILE):
            sys.exit("Config file: " + sickbeard.CONFIG_FILE + " must be writeable (write permissions). Exiting.")
        elif not os.access(os.path.dirname(sickbeard.CONFIG_FILE), os.W_OK):
            sys.exit("Config file directory: " + os.path.dirname(sickbeard.CONFIG_FILE) + " must be writeable (write permissions). Exiting")

    os.chdir(sickbeard.DATA_DIR)

    if consoleLogging:
        sys.stdout.write("Starting up Sick Beard " + SICKBEARD_VERSION + "\n")
        if not os.path.isfile(sickbeard.CONFIG_FILE):
            sys.stdout.write("Unable to find '" + sickbeard.CONFIG_FILE + "' , all settings will be default!" + "\n")

    # Load the config and publish it to the sickbeard package
    sickbeard.CFG = ConfigObj(sickbeard.CONFIG_FILE)

    # Initialize the config and our threads
    sickbeard.initialize(consoleLogging=consoleLogging)

    sickbeard.showList = []

    if sickbeard.DAEMON:
        daemonize()

    # Use this PID for everything
    sickbeard.PID = os.getpid()

    if forcedPort:
        logger.log(u"Forcing web server to port " + str(forcedPort))
        startPort = forcedPort
    else:
        startPort = sickbeard.WEB_PORT

    if sickbeard.WEB_LOG:
        log_dir = sickbeard.LOG_DIR
    else:
        log_dir = None

    # sickbeard.WEB_HOST is available as a configuration value in various
    # places but is not configurable. It is supported here for historic reasons.
    if sickbeard.WEB_HOST and sickbeard.WEB_HOST != '0.0.0.0':
        webhost = sickbeard.WEB_HOST
    else:
        if sickbeard.WEB_IPV6:
            webhost = '::'
        else:
            webhost = '0.0.0.0'

    try:
        initWebServer({
                      'port': startPort,
                      'host': webhost,
                      'data_root': os.path.join(sickbeard.PROG_DIR, 'data'),
                      'web_root': sickbeard.WEB_ROOT,
                      'log_dir': log_dir,
                      'username': sickbeard.WEB_USERNAME,
                      'password': sickbeard.WEB_PASSWORD,
                      'enable_https': sickbeard.ENABLE_HTTPS,
                      'https_cert': sickbeard.HTTPS_CERT,
                      'https_key': sickbeard.HTTPS_KEY,
                      })
    except IOError:
        logger.log(u"Unable to start web server, is something else running on port: " + str(startPort), logger.ERROR)
        if sickbeard.LAUNCH_BROWSER and not sickbeard.DAEMON:
            logger.log(u"Launching browser and exiting", logger.ERROR)
            sickbeard.launchBrowser(startPort)
        sys.exit("Unable to start web server, is something else running on port: " + str(startPort))

    # Build from the DB to start with
    logger.log(u"Loading initial show list")
    loadShowsFromDB()

    # Fire up all our threads
    sickbeard.start()

    # Launch browser if we're supposed to
    if sickbeard.LAUNCH_BROWSER and not noLaunch and not sickbeard.DAEMON:
        sickbeard.launchBrowser(startPort)

    # Start an update if we're supposed to
    if forceUpdate:
        sickbeard.showUpdateScheduler.action.run(force=True)  # @UndefinedVariable

    # Stay alive while my threads do the work
    while (True):

        if sickbeard.invoked_command:
            sickbeard.invoked_command()
            sickbeard.invoked_command = None

        time.sleep(1)

    return

if __name__ == "__main__":
    if sys.hexversion >= 0x020600F0:
        freeze_support()
    main()

########NEW FILE########
__FILENAME__ = all_tests
#!/usr/bin/env python
# coding=UTF-8
# Author: Dennis Lutter <lad1337@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

if __name__ == "__main__":
    import glob
    import unittest

    test_file_strings = [ x for x in glob.glob('*_tests.py') if not x in __file__]
    module_strings = [file_string[0:len(file_string) - 3] for file_string in test_file_strings]
    suites = [unittest.defaultTestLoader.loadTestsFromName(file_string) for file_string in module_strings]
    testSuite = unittest.TestSuite(suites)


    print "=================="
    print "STARTING - ALL TESTS"
    print "=================="
    print "this will include"
    for includedfiles in test_file_strings:
        print "- " + includedfiles


    text_runner = unittest.TextTestRunner().run(testSuite)

########NEW FILE########
__FILENAME__ = common_tests
import unittest

import sys
import os.path
sys.path.append(os.path.abspath('..'))

from sickbeard import common


class QualityTests(unittest.TestCase):

    # TODO: repack / proper ? air-by-date ? season rip? multi-ep?

    def test_SDTV(self):
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.PDTV.XViD-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.PDTV.x264-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.HDTV.XViD-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.HDTV.x264-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.DSR.XViD-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.DSR.x264-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.TVRip.XViD-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.TVRip.x264-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.WEBRip.XViD-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.WEBRip.x264-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.WEB-DL.x264-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.WEB-DL.AAC2.0.H.264-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02 WEB-DL H 264-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02_WEB-DL_H_264-GROUP"))
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test.Show.S01E02.WEB-DL.AAC2.0.H264-GROUP"))

    def test_SDDVD(self):
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.DVDRiP.XViD-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.DVDRiP.DiVX-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.DVDRiP.x264-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.DVDRip.WS.XViD-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.DVDRip.WS.DiVX-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.DVDRip.WS.x264-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.BDRIP.XViD-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.BDRIP.DiVX-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.BDRIP.x264-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.BDRIP.WS.XViD-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.BDRIP.WS.DiVX-GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test.Show.S01E02.BDRIP.WS.x264-GROUP"))

    def test_HDTV(self):
        self.assertEqual(common.Quality.HDTV, common.Quality.nameQuality("Test.Show.S01E02.720p.HDTV.x264-GROUP"))
        self.assertEqual(common.Quality.HDTV, common.Quality.nameQuality("Test.Show.S01E02.HR.WS.PDTV.x264-GROUP"))

    def test_RAWHDTV(self):
        self.assertEqual(common.Quality.RAWHDTV, common.Quality.nameQuality("Test.Show.S01E02.720p.HDTV.DD5.1.MPEG2-GROUP"))
        self.assertEqual(common.Quality.RAWHDTV, common.Quality.nameQuality("Test.Show.S01E02.1080i.HDTV.DD2.0.MPEG2-GROUP"))
        self.assertEqual(common.Quality.RAWHDTV, common.Quality.nameQuality("Test.Show.S01E02.1080i.HDTV.H.264.DD2.0-GROUP"))
        self.assertEqual(common.Quality.RAWHDTV, common.Quality.nameQuality("Test Show - S01E02 - 1080i HDTV MPA1.0 H.264 - GROUP"))
        self.assertEqual(common.Quality.RAWHDTV, common.Quality.nameQuality("Test.Show.S01E02.1080i.HDTV.DD.5.1.h264-GROUP"))
        self.assertEqual(common.Quality.RAWHDTV, common.Quality.nameQuality("Test.Show.S01E02.1080i.HDTV.MPEG2.DD5.1-GROUP"))
        self.assertEqual(common.Quality.RAWHDTV, common.Quality.nameQuality("Test Show S01E02 1080p HDTV DD5.1 H.264-GROUP"))

    def test_FULLHDTV(self):
        self.assertEqual(common.Quality.FULLHDTV, common.Quality.nameQuality("Test.Show.S01E02.1080p.HDTV.x264-GROUP"))
        self.assertEqual(common.Quality.FULLHDTV, common.Quality.nameQuality("Test.Show.S01E02.1080p.HDTV.DD5.1.x264-GROUP"))

    def test_HDWEBDL(self):
        self.assertEqual(common.Quality.HDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.720p.WEB-DL-GROUP"))
        self.assertEqual(common.Quality.HDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.720p.WEBRip-GROUP"))
        self.assertEqual(common.Quality.HDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.WEBRip.720p.H.264.AAC.2.0-GROUP"))
        self.assertEqual(common.Quality.HDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.720p.WEB-DL.AAC2.0.H.264-GROUP"))
        self.assertEqual(common.Quality.HDWEBDL, common.Quality.nameQuality("Test Show S01E02 720p WEB-DL AAC2 0 H 264-GROUP"))
        self.assertEqual(common.Quality.HDWEBDL, common.Quality.nameQuality("Test_Show.S01E02_720p_WEB-DL_AAC2.0_H264-GROUP"))
        self.assertEqual(common.Quality.HDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.720p.WEB-DL.AAC2.0.H264-GROUP"))
        self.assertEqual(common.Quality.HDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.720p.iTunes.Rip.H264.AAC-GROUP"))

    def test_FULLHDWEBDL(self):
        self.assertEqual(common.Quality.FULLHDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.1080p.WEB-DL-GROUP"))
        self.assertEqual(common.Quality.FULLHDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.1080p.WEBRip-GROUP"))
        self.assertEqual(common.Quality.FULLHDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.WEBRip.1080p.H.264.AAC.2.0-GROUP"))
        self.assertEqual(common.Quality.FULLHDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.WEBRip.1080p.H264.AAC.2.0-GROUP"))
        self.assertEqual(common.Quality.FULLHDWEBDL, common.Quality.nameQuality("Test.Show.S01E02.1080p.iTunes.H.264.AAC-GROUP"))
        self.assertEqual(common.Quality.FULLHDWEBDL, common.Quality.nameQuality("Test Show S01E02 1080p iTunes H 264 AAC-GROUP"))
        self.assertEqual(common.Quality.FULLHDWEBDL, common.Quality.nameQuality("Test_Show_S01E02_1080p_iTunes_H_264_AAC-GROUP"))

    def test_HDBLURAY(self):
        self.assertEqual(common.Quality.HDBLURAY, common.Quality.nameQuality("Test.Show.S01E02.720p.BluRay.x264-GROUP"))
        self.assertEqual(common.Quality.HDBLURAY, common.Quality.nameQuality("Test.Show.S01E02.720p.HDDVD.x264-GROUP"))

    def test_FULLHDBLURAY(self):
        self.assertEqual(common.Quality.FULLHDBLURAY, common.Quality.nameQuality("Test.Show.S01E02.1080p.BluRay.x264-GROUP"))
        self.assertEqual(common.Quality.FULLHDBLURAY, common.Quality.nameQuality("Test.Show.S01E02.1080p.HDDVD.x264-GROUP"))

    def test_UNKNOWN(self):
        self.assertEqual(common.Quality.UNKNOWN, common.Quality.nameQuality("Test.Show.S01E02-SiCKBEARD"))

    def test_reverse_parsing(self):
        self.assertEqual(common.Quality.SDTV, common.Quality.nameQuality("Test Show - S01E02 - SD TV - GROUP"))
        self.assertEqual(common.Quality.SDDVD, common.Quality.nameQuality("Test Show - S01E02 - SD DVD - GROUP"))
        self.assertEqual(common.Quality.HDTV, common.Quality.nameQuality("Test Show - S01E02 - HD TV - GROUP"))
        self.assertEqual(common.Quality.RAWHDTV, common.Quality.nameQuality("Test Show - S01E02 - RawHD TV - GROUP"))
        self.assertEqual(common.Quality.FULLHDTV, common.Quality.nameQuality("Test Show - S01E02 - 1080p HD TV - GROUP"))
        self.assertEqual(common.Quality.HDWEBDL, common.Quality.nameQuality("Test Show - S01E02 - 720p WEB-DL - GROUP"))
        self.assertEqual(common.Quality.FULLHDWEBDL, common.Quality.nameQuality("Test Show - S01E02 - 1080p WEB-DL - GROUP"))
        self.assertEqual(common.Quality.HDBLURAY, common.Quality.nameQuality("Test Show - S01E02 - 720p BluRay - GROUP"))
        self.assertEqual(common.Quality.FULLHDBLURAY, common.Quality.nameQuality("Test Show - S01E02 - 1080p BluRay - GROUP"))
        self.assertEqual(common.Quality.UNKNOWN, common.Quality.nameQuality("Test Show - S01E02 - Unknown - SiCKBEARD"))

if __name__ == '__main__':
    suite = unittest.TestLoader().loadTestsFromTestCase(QualityTests)
    unittest.TextTestRunner(verbosity=2).run(suite)

########NEW FILE########
__FILENAME__ = db_tests
# coding=UTF-8
# Author: Dennis Lutter <lad1337@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import unittest
import test_lib as test


class DBBasicTests(test.SickbeardTestDBCase):

    def setUp(self):
        super(DBBasicTests, self).setUp()
        self.db = test.db.DBConnection()

    def test_select(self):
        self.db.select("SELECT * FROM tv_episodes WHERE showid = ? AND location != ''", [0000])


if __name__ == '__main__':
    print "=================="
    print "STARTING - DB TESTS"
    print "=================="
    print "######################################################################"
    suite = unittest.TestLoader().loadTestsFromTestCase(DBBasicTests)
    unittest.TextTestRunner(verbosity=2).run(suite)

########NEW FILE########
__FILENAME__ = name_parser_tests
import datetime
import unittest

import sys, os.path
sys.path.append(os.path.abspath('..'))
sys.path.append(os.path.abspath('../lib'))

from sickbeard.name_parser import parser

import sickbeard
sickbeard.SYS_ENCODING = 'UTF-8'

DEBUG = VERBOSE = False

simple_test_cases = {
              'standard': {
              'Mr.Show.Name.S01E02.Source.Quality.Etc-Group': parser.ParseResult(None, 'Mr Show Name', 1, [2], 'Source.Quality.Etc', 'Group'),
              'Show.Name.S01E02': parser.ParseResult(None, 'Show Name', 1, [2]),
              'Show Name - S01E02 - My Ep Name': parser.ParseResult(None, 'Show Name', 1, [2], 'My Ep Name'),
              'Show.1.0.Name.S01.E03.My.Ep.Name-Group': parser.ParseResult(None, 'Show 1.0 Name', 1, [3], 'My.Ep.Name', 'Group'),
              'Show.Name.S01E02E03.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', 1, [2,3], 'Source.Quality.Etc', 'Group'),
              'Mr. Show Name - S01E02-03 - My Ep Name': parser.ParseResult(None, 'Mr. Show Name', 1, [2,3], 'My Ep Name'),
              'Show.Name.S01.E02.E03': parser.ParseResult(None, 'Show Name', 1, [2,3]),
              'Show.Name-0.2010.S01E02.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name-0 2010', 1, [2], 'Source.Quality.Etc', 'Group'),
              'S01E02 Ep Name': parser.ParseResult(None, None, 1, [2], 'Ep Name'),
              'Show Name - S06E01 - 2009-12-20 - Ep Name': parser.ParseResult(None, 'Show Name', 6, [1], '2009-12-20 - Ep Name'),
              'Show Name - S06E01 - -30-': parser.ParseResult(None, 'Show Name', 6, [1], '30-' ),
              'Show-Name-S06E01-720p': parser.ParseResult(None, 'Show-Name', 6, [1], '720p' ),
              'Show-Name-S06E01-1080i': parser.ParseResult(None, 'Show-Name', 6, [1], '1080i' ),
              'Show.Name.S06E01.Other.WEB-DL': parser.ParseResult(None, 'Show Name', 6, [1], 'Other.WEB-DL' ),
              'Show.Name.S06E01 Some-Stuff Here': parser.ParseResult(None, 'Show Name', 6, [1], 'Some-Stuff Here' ),
              },
              
              'fov': {
              'Show_Name.1x02.Source_Quality_Etc-Group': parser.ParseResult(None, 'Show Name', 1, [2], 'Source_Quality_Etc', 'Group'),
              'Show Name 1x02': parser.ParseResult(None, 'Show Name', 1, [2]),
              'Show Name 1x02 x264 Test': parser.ParseResult(None, 'Show Name', 1, [2], 'x264 Test'),
              'Show Name - 1x02 - My Ep Name': parser.ParseResult(None, 'Show Name', 1, [2], 'My Ep Name'),
              'Show_Name.1x02x03x04.Source_Quality_Etc-Group': parser.ParseResult(None, 'Show Name', 1, [2,3,4], 'Source_Quality_Etc', 'Group'),
              'Show Name - 1x02-03-04 - My Ep Name': parser.ParseResult(None, 'Show Name', 1, [2,3,4], 'My Ep Name'),
              '1x02 Ep Name': parser.ParseResult(None, None, 1, [2], 'Ep Name'),
              'Show-Name-1x02-720p': parser.ParseResult(None, 'Show-Name', 1, [2], '720p'),
              'Show-Name-1x02-1080i': parser.ParseResult(None, 'Show-Name', 1, [2], '1080i'),
              'Show Name [05x12] Ep Name': parser.ParseResult(None, 'Show Name', 5, [12], 'Ep Name'),
              'Show.Name.1x02.WEB-DL': parser.ParseResult(None, 'Show Name', 1, [2], 'WEB-DL'),
              },

              'standard_repeat': {
              'Show.Name.S01E02.S01E03.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', 1, [2,3], 'Source.Quality.Etc', 'Group'),
              'Show.Name.S01E02.S01E03': parser.ParseResult(None, 'Show Name', 1, [2,3]),
              'Show Name - S01E02 - S01E03 - S01E04 - Ep Name': parser.ParseResult(None, 'Show Name', 1, [2,3,4], 'Ep Name'),
              'Show.Name.S01E02.S01E03.WEB-DL': parser.ParseResult(None, 'Show Name', 1, [2,3], 'WEB-DL'),
              },
              
              'fov_repeat': {
              'Show.Name.1x02.1x03.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', 1, [2,3], 'Source.Quality.Etc', 'Group'),
              'Show.Name.1x02.1x03': parser.ParseResult(None, 'Show Name', 1, [2,3]),
              'Show Name - 1x02 - 1x03 - 1x04 - Ep Name': parser.ParseResult(None, 'Show Name', 1, [2,3,4], 'Ep Name'),
              'Show.Name.1x02.1x03.WEB-DL': parser.ParseResult(None, 'Show Name', 1, [2,3], 'WEB-DL'),
              },

              'bare': {
              'Show.Name.102.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', 1, [2], 'Source.Quality.Etc', 'Group'),
              'show.name.2010.123.source.quality.etc-group': parser.ParseResult(None, 'show name 2010', 1, [23], 'source.quality.etc', 'group'),
              'show.name.2010.222.123.source.quality.etc-group': parser.ParseResult(None, 'show name 2010.222', 1, [23], 'source.quality.etc', 'group'),
              'Show.Name.102': parser.ParseResult(None, 'Show Name', 1, [2]),
              'the.event.401.hdtv-lol': parser.ParseResult(None, 'the event', 4, [1], 'hdtv', 'lol'),
              'show.name.2010.special.hdtv-blah': None,
              },
              
              'stupid': {
              'tpz-abc102': parser.ParseResult(None, None, 1, [2], None, 'tpz'),
              'tpz-abc.102': parser.ParseResult(None, None, 1, [2], None, 'tpz'),
              },
              
              'no_season': {
              'Show Name - 01 - Ep Name': parser.ParseResult(None, 'Show Name', None, [1], 'Ep Name'),
              '01 - Ep Name': parser.ParseResult(None, None, None, [1], 'Ep Name'),
              'Show Name - 01 - Ep Name - WEB-DL': parser.ParseResult(None, 'Show Name', None, [1], 'Ep Name - WEB-DL'),
              },

              'no_season_general': {
              'Show.Name.E23.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', None, [23], 'Source.Quality.Etc', 'Group'),
              'Show Name - Episode 01 - Ep Name': parser.ParseResult(None, 'Show Name', None, [1], 'Ep Name'),
              'Show.Name.Part.3.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', None, [3], 'Source.Quality.Etc', 'Group'),
              'Show.Name.Part.1.and.Part.2.Blah-Group': parser.ParseResult(None, 'Show Name', None, [1,2], 'Blah', 'Group'),
              'Show.Name.Part.IV.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', None, [4], 'Source.Quality.Etc', 'Group'),
              'Deconstructed.E07.1080i.HDTV.DD5.1.MPEG2-TrollHD': parser.ParseResult(None, 'Deconstructed', None, [7], '1080i.HDTV.DD5.1.MPEG2', 'TrollHD'),
              'Show.Name.E23.WEB-DL': parser.ParseResult(None, 'Show Name', None, [23], 'WEB-DL'),
              },

              'no_season_multi_ep': {
              'Show.Name.E23-24.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', None, [23,24], 'Source.Quality.Etc', 'Group'),
              'Show Name - Episode 01-02 - Ep Name': parser.ParseResult(None, 'Show Name', None, [1,2], 'Ep Name'),
              'Show.Name.E23-24.WEB-DL': parser.ParseResult(None, 'Show Name', None, [23,24], 'WEB-DL'),
              },

              'season_only': {
              'Show.Name.S02.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', 2, [], 'Source.Quality.Etc', 'Group'),
              'Show Name Season 2': parser.ParseResult(None, 'Show Name', 2),
              'Season 02': parser.ParseResult(None, None, 2),
              },
              
              'scene_date_format': {
              'Show.Name.2010.11.23.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', None, [], 'Source.Quality.Etc', 'Group', datetime.date(2010,11,23)),
              'Show Name - 2010.11.23': parser.ParseResult(None, 'Show Name', air_date = datetime.date(2010,11,23)),
              'Show.Name.2010.23.11.Source.Quality.Etc-Group': parser.ParseResult(None, 'Show Name', None, [], 'Source.Quality.Etc', 'Group', datetime.date(2010,11,23)),
              'Show Name - 2010-11-23 - Ep Name': parser.ParseResult(None, 'Show Name', extra_info = 'Ep Name', air_date = datetime.date(2010,11,23)),
              '2010-11-23 - Ep Name': parser.ParseResult(None, extra_info = 'Ep Name', air_date = datetime.date(2010,11,23)),
              'Show.Name.2010.11.23.WEB-DL': parser.ParseResult(None, 'Show Name', None, [], 'WEB-DL', None, datetime.date(2010,11,23)),
               }
              }

combination_test_cases = [
                          ('/test/path/to/Season 02/03 - Ep Name.avi',
                           parser.ParseResult(None, None, 2, [3], 'Ep Name'),
                           ['no_season', 'season_only']),
                          
                          ('Show.Name.S02.Source.Quality.Etc-Group/tpz-sn203.avi',
                           parser.ParseResult(None, 'Show Name', 2, [3], 'Source.Quality.Etc', 'Group'),
                           ['stupid', 'season_only']),

                          ('MythBusters.S08E16.720p.HDTV.x264-aAF/aaf-mb.s08e16.720p.mkv',
                           parser.ParseResult(None, 'MythBusters', 8, [16], '720p.HDTV.x264', 'aAF'),
                           ['standard']),
                           
                          ('/home/drop/storage/TV/Terminator The Sarah Connor Chronicles/Season 2/S02E06 The Tower is Tall, But the Fall is Short.mkv',
                           parser.ParseResult(None, None, 2, [6], 'The Tower is Tall, But the Fall is Short'),
                           ['standard']),
                           
                          (r'/Test/TV/Jimmy Fallon/Season 2/Jimmy Fallon - 2010-12-15 - blah.avi',
                           parser.ParseResult(None, 'Jimmy Fallon', extra_info = 'blah', air_date = datetime.date(2010,12,15)),
                           ['scene_date_format']),

                          (r'/X/30 Rock/Season 4/30 Rock - 4x22 -.avi',
                           parser.ParseResult(None, '30 Rock', 4, [22]),
                           ['fov']),
                           
                          ('Season 2\\Show Name - 03-04 - Ep Name.ext',
                           parser.ParseResult(None, 'Show Name', 2, [3,4], extra_info = 'Ep Name'),
                           ['no_season', 'season_only']),
                           
                          ('Season 02\\03-04-05 - Ep Name.ext',
                           parser.ParseResult(None, None, 2, [3,4,5], extra_info = 'Ep Name'),
                           ['no_season', 'season_only']),
                          ]

unicode_test_cases = [
                      (u'The.Big.Bang.Theory.2x07.The.Panty.Pi\xf1ata.Polarization.720p.HDTV.x264.AC3-SHELDON.mkv',
                       parser.ParseResult(None, 'The.Big.Bang.Theory', 2, [7], '720p.HDTV.x264.AC3', 'SHELDON')
                       ),
                      ('The.Big.Bang.Theory.2x07.The.Panty.Pi\xc3\xb1ata.Polarization.720p.HDTV.x264.AC3-SHELDON.mkv',
                       parser.ParseResult(None, 'The.Big.Bang.Theory', 2, [7], '720p.HDTV.x264.AC3', 'SHELDON')
                       ),
                      ]

failure_cases = ['7sins-jfcs01e09-720p-bluray-x264']


class UnicodeTests(unittest.TestCase):
    
    def _test_unicode(self, name, result):
        np = parser.NameParser(True)
        parse_result = np.parse(name)
        # this shouldn't raise an exception
        a = repr(str(parse_result))
    
    def test_unicode(self):
        for (name, result) in unicode_test_cases:
            self._test_unicode(name, result)

class FailureCaseTests(unittest.TestCase):
    
    def _test_name(self, name):
        np = parser.NameParser(True)
        try:
            parse_result = np.parse(name)
        except parser.InvalidNameException:
            return True
        
        if VERBOSE:
            print 'Actual: ', parse_result.which_regex, parse_result
        return False
    
    def test_failures(self):
        for name in failure_cases:
            self.assertTrue(self._test_name(name))

class ComboTests(unittest.TestCase):
    
    def _test_combo(self, name, result, which_regexes):
        
        if VERBOSE:
            print
            print 'Testing', name 
        
        np = parser.NameParser(True)
        test_result = np.parse(name)
        
        if DEBUG:
            print test_result, test_result.which_regex
            print result, which_regexes
            

        self.assertEqual(test_result, result)
        for cur_regex in which_regexes:
            self.assertTrue(cur_regex in test_result.which_regex)
        self.assertEqual(len(which_regexes), len(test_result.which_regex))

    def test_combos(self):
        
        for (name, result, which_regexes) in combination_test_cases:
            # Normalise the paths. Converts UNIX-style paths into Windows-style
            # paths when test is run on Windows.
            self._test_combo(os.path.normpath(name), result, which_regexes)

class BasicTests(unittest.TestCase):

    def _test_names(self, np, section, transform=None, verbose=False):

        if VERBOSE or verbose:
            print
            print 'Running', section, 'tests'
        for cur_test_base in simple_test_cases[section]:
            if transform:
                cur_test = transform(cur_test_base)
            else:
                cur_test = cur_test_base
            if VERBOSE or verbose:
                print 'Testing', cur_test

            result = simple_test_cases[section][cur_test_base]
            if not result:
                self.assertRaises(parser.InvalidNameException, np.parse, cur_test)
                return
            else:
                test_result = np.parse(cur_test)
            
            if DEBUG or verbose:
                print 'air_by_date:', test_result.air_by_date, 'air_date:', test_result.air_date
                print test_result
                print result
            self.assertEqual(test_result.which_regex, [section])
            self.assertEqual(test_result, result)

    def test_standard_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'standard')

    def test_standard_repeat_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'standard_repeat')

    def test_fov_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'fov')

    def test_fov_repeat_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'fov_repeat')

    def test_bare_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'bare')

    def test_stupid_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'stupid')

    def test_no_season_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'no_season')

    def test_no_season_general_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'no_season_general')

    def test_no_season_multi_ep_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'no_season_multi_ep')

    def test_season_only_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'season_only')

    def test_scene_date_format_names(self):
        np = parser.NameParser(False)
        self._test_names(np, 'scene_date_format')

    def test_standard_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'standard', lambda x: x + '.avi')

    def test_standard_repeat_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'standard_repeat', lambda x: x + '.avi')

    def test_fov_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'fov', lambda x: x + '.avi')

    def test_fov_repeat_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'fov_repeat', lambda x: x + '.avi')

    def test_bare_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'bare', lambda x: x + '.avi')

    def test_stupid_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'stupid', lambda x: x + '.avi')

    def test_no_season_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'no_season', lambda x: x + '.avi')

    def test_no_season_general_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'no_season_general', lambda x: x + '.avi')

    def test_no_season_multi_ep_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'no_season_multi_ep', lambda x: x + '.avi')

    def test_season_only_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'season_only', lambda x: x + '.avi')

    def test_scene_date_format_file_names(self):
        np = parser.NameParser()
        self._test_names(np, 'scene_date_format', lambda x: x + '.avi')

    def test_combination_names(self):
        pass

if __name__ == '__main__':
    if len(sys.argv) > 1:
        suite = unittest.TestLoader().loadTestsFromName('name_parser_tests.BasicTests.test_'+sys.argv[1])
    else:
        suite = unittest.TestLoader().loadTestsFromTestCase(BasicTests)
    unittest.TextTestRunner(verbosity=2).run(suite)

    suite = unittest.TestLoader().loadTestsFromTestCase(ComboTests)
    unittest.TextTestRunner(verbosity=2).run(suite)

    suite = unittest.TestLoader().loadTestsFromTestCase(UnicodeTests)
    unittest.TextTestRunner(verbosity=2).run(suite)

    suite = unittest.TestLoader().loadTestsFromTestCase(FailureCaseTests)
    unittest.TextTestRunner(verbosity=2).run(suite)

########NEW FILE########
__FILENAME__ = pp_tests
# coding=UTF-8
# Author: Dennis Lutter <lad1337@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import random
import unittest

import test_lib as test

import sys, os.path

from sickbeard.postProcessor import PostProcessor
import sickbeard
from sickbeard.tv import TVEpisode, TVShow


class PPInitTests(unittest.TestCase):

    def setUp(self):
        self.pp = PostProcessor(test.FILEPATH)

    def test_init_file_name(self):
        self.assertEqual(self.pp.file_name, test.FILENAME)

    def test_init_folder_name(self):
        self.assertEqual(self.pp.folder_name, test.SHOWNAME)


class PPPrivateTests(test.SickbeardTestDBCase):


    def setUp(self):
        super(PPPrivateTests, self).setUp()

        sickbeard.showList = [TVShow(0000), TVShow(0001)]

        self.pp = PostProcessor(test.FILEPATH)
        self.show_obj = TVShow(0002)

        self.db = test.db.DBConnection()
        newValueDict = {"tvdbid": 1002,
                        "name": test.SHOWNAME,
                        "description": "description",
                        "airdate": 1234,
                        "hasnfo": 1,
                        "hastbn": 1,
                        "status": 404,
                        "location": test.FILEPATH}
        controlValueDict = {"showid": 0002,
                            "season": test.SEASON,
                            "episode": test.EPISODE}

        # use a custom update/insert method to get the data into the DB
        self.db.upsert("tv_episodes", newValueDict, controlValueDict)

        self.ep_obj = TVEpisode(self.show_obj, test.SEASON, test.EPISODE, test.FILEPATH)

    def test__find_ep_destination_folder(self):
        self.show_obj.location = test.FILEDIR
        self.ep_obj.show.seasonfolders = 1
        sickbeard.SEASON_FOLDERS_FORMAT = 'Season %02d'
        calculatedPath = self.pp._find_ep_destination_folder(self.ep_obj)
        ecpectedPath = os.path.join(test.FILEDIR, "Season 0" + str(test.SEASON))
        self.assertEqual(calculatedPath, ecpectedPath)


class PPBasicTests(test.SickbeardTestDBCase):

    def test_process(self):
        show = TVShow(3)
        show.name = test.SHOWNAME
        show.location = test.SHOWDIR
        show.saveToDB()

        sickbeard.showList = [show]
        ep = TVEpisode(show, test.SEASON, test.EPISODE)
        ep.name = "some ep name"
        ep.saveToDB()

        pp = PostProcessor(test.FILEPATH)
        self.assertTrue(pp.process())


if __name__ == '__main__':
    print "=================="
    print "STARTING - PostProcessor TESTS"
    print "=================="
    print "######################################################################"
    suite = unittest.TestLoader().loadTestsFromTestCase(PPInitTests)
    unittest.TextTestRunner(verbosity=2).run(suite)
    print "######################################################################"
    suite = unittest.TestLoader().loadTestsFromTestCase(PPPrivateTests)
    unittest.TextTestRunner(verbosity=2).run(suite)
    print "######################################################################"
    suite = unittest.TestLoader().loadTestsFromTestCase(PPBasicTests)
    unittest.TextTestRunner(verbosity=2).run(suite)

########NEW FILE########
__FILENAME__ = scene_helpers_tests
import unittest
import test_lib as test

import sys, os.path
sys.path.append(os.path.abspath('..'))

from sickbeard import show_name_helpers, scene_exceptions, common, name_cache

import sickbeard
from sickbeard import db
from sickbeard.databases import cache_db
from sickbeard.tv import TVShow as Show


class SceneTests(test.SickbeardTestDBCase):

    def _test_sceneToNormalShowNames(self, name, expected):
        result = show_name_helpers.sceneToNormalShowNames(name)
        self.assertTrue(len(set(expected).intersection(set(result))) == len(expected))

        dot_result = show_name_helpers.sceneToNormalShowNames(name.replace(' ', '.'))
        dot_expected = [x.replace(' ', '.') for x in expected]
        self.assertTrue(len(set(dot_expected).intersection(set(dot_result))) == len(dot_expected))

    def _test_allPossibleShowNames(self, name, tvdbid=0, tvrname=None, expected=[]):
        s = Show(tvdbid)
        s.name = name
        s.tvrname = tvrname

        result = show_name_helpers.allPossibleShowNames(s)
        self.assertTrue(len(set(expected).intersection(set(result))) == len(expected))

    def _test_filterBadReleases(self, name, expected):
        result = show_name_helpers.filterBadReleases(name)
        self.assertEqual(result, expected)

    def _test_isGoodName(self, name, show):
        self.assertTrue(show_name_helpers.isGoodResult(name, show))

    def test_isGoodName(self):
        listOfcases = [('Show.Name.S01E02.Test-Test', 'Show/Name'),
                        ('Show.Name.S01E02.Test-Test', 'Show. Name'),
                        ('Show.Name.S01E02.Test-Test', 'Show- Name'),
                        ('Show.Name.Part.IV.Test-Test', 'Show Name'),
                        ('Show.Name.S01.Test-Test', 'Show Name'),
                        ('Show.Name.E02.Test-Test', 'Show: Name'),
                        ('Show Name Season 2 Test', 'Show: Name'),
                        ]

        for testCase in listOfcases:
            scene_name, show_name = testCase
            s = Show(0)
            s.name = show_name
            self._test_isGoodName(scene_name, s)

    def test_sceneToNormalShowNames(self):
        self._test_sceneToNormalShowNames('Show Name 2010', ['Show Name 2010', 'Show Name (2010)'])
        self._test_sceneToNormalShowNames('Show Name US', ['Show Name US', 'Show Name (US)'])
        self._test_sceneToNormalShowNames('Show Name AU', ['Show Name AU', 'Show Name (AU)'])
        self._test_sceneToNormalShowNames('Show Name CA', ['Show Name CA', 'Show Name (CA)'])
        self._test_sceneToNormalShowNames('Show and Name', ['Show and Name', 'Show & Name'])
        self._test_sceneToNormalShowNames('Show and Name 2010', ['Show and Name 2010', 'Show & Name 2010', 'Show and Name (2010)', 'Show & Name (2010)'])
        self._test_sceneToNormalShowNames('show name us', ['show name us', 'show name (us)'])
        self._test_sceneToNormalShowNames('Show And Name', ['Show And Name', 'Show & Name'])

        # failure cases
        self._test_sceneToNormalShowNames('Show Name 90210', ['Show Name 90210'])
        self._test_sceneToNormalShowNames('Show Name YA', ['Show Name YA'])

    def test_allPossibleShowNames(self):
        #common.sceneExceptions[-1] = ['Exception Test']
        myDB = db.DBConnection("cache.db")
        myDB.action("INSERT INTO scene_exceptions (tvdb_id, show_name) VALUES (?,?)", [-1, 'Exception Test'])
        common.countryList['Full Country Name'] = 'FCN'

        self._test_allPossibleShowNames('Show Name', expected=['Show Name'])
        self._test_allPossibleShowNames('Show Name', -1, expected=['Show Name', 'Exception Test'])
        self._test_allPossibleShowNames('Show Name', tvrname='TVRage Name', expected=['Show Name', 'TVRage Name'])
        self._test_allPossibleShowNames('Show Name FCN', expected=['Show Name FCN', 'Show Name (Full Country Name)'])
        self._test_allPossibleShowNames('Show Name (FCN)', expected=['Show Name (FCN)', 'Show Name (Full Country Name)'])
        self._test_allPossibleShowNames('Show Name Full Country Name', expected=['Show Name Full Country Name', 'Show Name (FCN)'])
        self._test_allPossibleShowNames('Show Name (Full Country Name)', expected=['Show Name (Full Country Name)', 'Show Name (FCN)'])
        self._test_allPossibleShowNames('Show Name (FCN)', -1, 'TVRage Name', expected=['Show Name (FCN)', 'Show Name (Full Country Name)', 'Exception Test', 'TVRage Name'])

    def test_filterBadReleases(self):
        self._test_filterBadReleases('Show.S02.German.Stuff-Grp', False)
        self._test_filterBadReleases('Show.S02.Some.Stuff-Core2HD', False)
        self._test_filterBadReleases('Show.S02.Some.German.Stuff-Grp', False)
        self._test_filterBadReleases('German.Show.S02.Some.Stuff-Grp', True)
        self._test_filterBadReleases('Show.S02.This.Is.German', False)


class SceneExceptionTestCase(test.SickbeardTestDBCase):

    def setUp(self):
        super(SceneExceptionTestCase, self).setUp()
        scene_exceptions.retrieve_exceptions()

    def test_sceneExceptionsEmpty(self):
        self.assertEqual(scene_exceptions.get_scene_exceptions(0), [])

    def test_sceneExceptionsBabylon5(self):
        self.assertEqual(sorted(scene_exceptions.get_scene_exceptions(70726)), ['Babylon 5', 'Babylon5'])

    def test_sceneExceptionByName(self):
        self.assertEqual(scene_exceptions.get_scene_exception_by_name('Babylon5'), 70726)
        self.assertEqual(scene_exceptions.get_scene_exception_by_name('babylon 5'), 70726)
        self.assertEqual(scene_exceptions.get_scene_exception_by_name('Carlos 2010'), 164451)

    def test_sceneExceptionByNameEmpty(self):
        self.assertEqual(scene_exceptions.get_scene_exception_by_name('nothing useful'), None)

    def test_sceneExceptionsResetNameCache(self):
        # clear the exceptions
        myDB = db.DBConnection("cache.db")
        myDB.action("DELETE FROM scene_exceptions")

        # put something in the cache
        name_cache.addNameToCache('Cached Name', 0)

        # updating should clear the cache so our previously "Cached Name" won't be in there
        scene_exceptions.retrieve_exceptions()
        self.assertEqual(name_cache.retrieveNameFromCache('Cached Name'), None)

        # put something in the cache
        name_cache.addNameToCache('Cached Name', 0)

        # updating should not clear the cache this time since our exceptions didn't change
        scene_exceptions.retrieve_exceptions()
        self.assertEqual(name_cache.retrieveNameFromCache('Cached Name'), 0)


if __name__ == '__main__':
    if len(sys.argv) > 1:
        suite = unittest.TestLoader().loadTestsFromName('scene_helpers_tests.SceneExceptionTestCase.test_' + sys.argv[1])
        unittest.TextTestRunner(verbosity=2).run(suite)
    else:
        suite = unittest.TestLoader().loadTestsFromTestCase(SceneTests)
        unittest.TextTestRunner(verbosity=2).run(suite)
        suite = unittest.TestLoader().loadTestsFromTestCase(SceneExceptionTestCase)
        unittest.TextTestRunner(verbosity=2).run(suite)

########NEW FILE########
__FILENAME__ = snatch_tests
# coding=UTF-8
# Author: Dennis Lutter <lad1337@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import random
import unittest

import test_lib as test

import sys, os.path

import sickbeard.search as search
import sickbeard
from sickbeard.tv import TVEpisode, TVShow
import sickbeard.common as c

tests = {"Dexter": {"a": 1, "q": c.HD, "s": 5, "e": [7], "b": 'Dexter.S05E07.720p.BluRay.X264-REWARD', "i": ['Dexter.S05E07.720p.BluRay.X264-REWARD', 'Dexter.S05E07.720p.X264-REWARD']},
         "House": {"a": 1, "q": c.HD, "s": 4, "e": [5], "b": 'House.4x5.720p.BluRay.X264-REWARD', "i": ['Dexter.S05E04.720p.X264-REWARD', 'House.4x5.720p.BluRay.X264-REWARD']},
         "Hells Kitchen": {"a": 1, "q": c.SD, "s": 6, "e": [14, 15], "b": 'Hells.Kitchen.s6e14e15.HDTV.XviD-ASAP', "i": ['Hells.Kitchen.S06E14.HDTV.XviD-ASAP', 'Hells.Kitchen.6x14.HDTV.XviD-ASAP', 'Hells.Kitchen.s6e14e15.HDTV.XviD-ASAP']}
       }


def _create_fake_xml(items):
    xml = '<?xml version="1.0" encoding="UTF-8" ?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:newznab="http://www.newznab.com/DTD/2010/feeds/attributes/" encoding="utf-8"><channel>'
    for item in items:
        xml += '<item><title>' + item + '</title>\n'
        xml += '<link>http://fantasy.com/' + item + '</link></item>'
    xml += '</channel></rss>'
    return xml


searchItems = []


class SearchTest(test.SickbeardTestDBCase):

    def _fake_getURL(self, url, headers=None):
        global searchItems
        return _create_fake_xml(searchItems)

    def _fake_isActive(self):
        return True

    def __init__(self, something):
        for provider in sickbeard.providers.sortedProviderList():
            provider.getURL = self._fake_getURL
            #provider.isActive = self._fake_isActive

        super(SearchTest, self).__init__(something)


def test_generator(tvdbdid, show_name, curData, forceSearch):

    def test(self):
        global searchItems
        searchItems = curData["i"]
        show = TVShow(tvdbdid)
        show.name = show_name
        show.quality = curData["q"]
        show.saveToDB()
        sickbeard.showList.append(show)

        for epNumber in curData["e"]:
            episode = TVEpisode(show, curData["s"], epNumber)
            episode.status = c.WANTED
            episode.saveToDB()

        bestResult = search.findEpisode(episode, forceSearch)
        if not bestResult:
            self.assertEqual(curData["b"], bestResult)
        self.assertEqual(curData["b"], bestResult.name) #first is expected, second is choosen one
    return test

if __name__ == '__main__':
    print "=================="
    print "STARTING - Snatch TESTS"
    print "=================="
    print "######################################################################"
    # create the test methods
    tvdbdid = 1
    for forceSearch in (True, False):
        for name, curData in tests.items():
            if not curData["a"]:
                continue
            fname = name.replace(' ', '_')
            if forceSearch:
                test_name = 'test_manual_%s_%s' % (fname, tvdbdid)
            else:
                test_name = 'test_%s_%s' % (fname, tvdbdid)

            test = test_generator(tvdbdid, name, curData, forceSearch)
            setattr(SearchTest, test_name, test)
            tvdbdid += 1

    suite = unittest.TestLoader().loadTestsFromTestCase(SearchTest)
    unittest.TextTestRunner(verbosity=2).run(suite)

########NEW FILE########
__FILENAME__ = test_lib
# coding=UTF-8
# Author: Dennis Lutter <lad1337@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import with_statement

import unittest

import sqlite3

import sys
import os.path
sys.path.append(os.path.abspath('..'))
sys.path.append(os.path.abspath('../lib'))

import sickbeard
import shutil

from sickbeard import encodingKludge as ek, providers, tvcache
from sickbeard import db
from sickbeard.databases import mainDB
from sickbeard.databases import cache_db

#=================
# test globals
#=================
TESTDIR = os.path.abspath('.')
TESTDBNAME = "sickbeard.db"
TESTCACHEDBNAME = "cache.db"


SHOWNAME = u"show name"
SEASON = 4
EPISODE = 2
FILENAME = u"show name - s0" + str(SEASON) + "e0" + str(EPISODE) + ".mkv"
FILEDIR = os.path.join(TESTDIR, SHOWNAME)
FILEPATH = os.path.join(FILEDIR, FILENAME)

SHOWDIR = os.path.join(TESTDIR, SHOWNAME + " final")

#sickbeard.logger.sb_log_instance = sickbeard.logger.SBRotatingLogHandler(os.path.join(TESTDIR, 'sickbeard.log'), sickbeard.logger.NUM_LOGS, sickbeard.logger.LOG_SIZE)
sickbeard.logger.SBRotatingLogHandler.log_file = os.path.join(os.path.join(TESTDIR, 'Logs'), 'test_sickbeard.log')


#=================
# prepare env functions
#=================
def createTestLogFolder():
    if not os.path.isdir(sickbeard.LOG_DIR):
        os.mkdir(sickbeard.LOG_DIR)

# call env functions at appropriate time during sickbeard var setup

#=================
# sickbeard globals
#=================
sickbeard.SYS_ENCODING = 'UTF-8'
sickbeard.showList = []
sickbeard.QUALITY_DEFAULT = 4  # hdtv
sickbeard.FLATTEN_FOLDERS_DEFAULT = 0

sickbeard.NAMING_PATTERN = ''
sickbeard.NAMING_ABD_PATTERN = ''
sickbeard.NAMING_MULTI_EP = 1


sickbeard.PROVIDER_ORDER = ["sick_beard_index"]
sickbeard.newznabProviderList = providers.getNewznabProviderList("Sick Beard Index|http://lolo.sickbeard.com/|0|5030,5040|0!!!NZBs.org|http://nzbs.org/||5030,5040,5070,5090|0!!!Usenet-Crawler|http://www.usenet-crawler.com/||5030,5040|0")
sickbeard.providerList = providers.makeProviderList()

sickbeard.PROG_DIR = os.path.abspath('..')
sickbeard.DATA_DIR = sickbeard.PROG_DIR
sickbeard.LOG_DIR = os.path.join(TESTDIR, 'Logs')
createTestLogFolder()
sickbeard.logger.sb_log_instance.initLogging(False)


#=================
# dummy functions
#=================
def _dummy_saveConfig():
    return True
# this overrides the sickbeard save_config which gets called during a db upgrade
# this might be considered a hack
mainDB.sickbeard.save_config = _dummy_saveConfig


# the real one tries to contact tvdb just stop it from getting more info on the ep
def _fake_specifyEP(self, season, episode):
    pass

sickbeard.tv.TVEpisode.specifyEpisode = _fake_specifyEP


#=================
# test classes
#=================
class SickbeardTestDBCase(unittest.TestCase):
    def setUp(self):
        sickbeard.showList = []
        setUp_test_db()
        setUp_test_episode_file()
        setUp_test_show_dir()

    def tearDown(self):
        sickbeard.showList = []
        tearDown_test_db()
        tearDown_test_episode_file()
        tearDown_test_show_dir()


class TestDBConnection(db.DBConnection, object):

    def __init__(self, dbFileName=TESTDBNAME):
        dbFileName = os.path.join(TESTDIR, dbFileName)
        super(TestDBConnection, self).__init__(dbFileName)


class TestCacheDBConnection(TestDBConnection, object):

    def __init__(self, providerName):
        db.DBConnection.__init__(self, os.path.join(TESTDIR, TESTCACHEDBNAME))

        # Create the table if it's not already there
        try:
            sql = "CREATE TABLE " + providerName + " (name TEXT, season NUMERIC, episodes TEXT, tvrid NUMERIC, tvdbid NUMERIC, url TEXT, time NUMERIC, quality TEXT);"
            self.connection.execute(sql)
            self.connection.commit()
        except sqlite3.OperationalError, e:
            if str(e) != "table " + providerName + " already exists":
                raise

        # Create the table if it's not already there
        try:
            sql = "CREATE TABLE lastUpdate (provider TEXT, time NUMERIC);"
            self.connection.execute(sql)
            self.connection.commit()
        except sqlite3.OperationalError, e:
            if str(e) != "table lastUpdate already exists":
                raise

# this will override the normal db connection
sickbeard.db.DBConnection = TestDBConnection
sickbeard.tvcache.CacheDBConnection = TestCacheDBConnection


#=================
# test functions
#=================
def setUp_test_db():
    """upgrades the db to the latest version
    """
    # upgrading the db
    db.upgradeDatabase(db.DBConnection(), mainDB.InitialSchema)
    # fix up any db problems
    db.sanityCheckDatabase(db.DBConnection(), mainDB.MainSanityCheck)

    #and for cache.b too
    db.upgradeDatabase(db.DBConnection("cache.db"), cache_db.InitialSchema)


def tearDown_test_db():
    """Deletes the test db
        although this seams not to work on my system it leaves me with an zero kb file
    """
    # uncomment next line so leave the db intact between test and at the end
    #return False
    if os.path.exists(os.path.join(TESTDIR, TESTDBNAME)):
        os.remove(os.path.join(TESTDIR, TESTDBNAME))
    if os.path.exists(os.path.join(TESTDIR, TESTCACHEDBNAME)):
        os.remove(os.path.join(TESTDIR, TESTCACHEDBNAME))


def setUp_test_episode_file():
    if not os.path.exists(FILEDIR):
        os.makedirs(FILEDIR)

    try:
        with open(FILEPATH, 'w') as f:
            f.write("foo bar")
    except EnvironmentError:
        print "Unable to set up test episode"
        raise


def tearDown_test_episode_file():
    shutil.rmtree(FILEDIR)


def setUp_test_show_dir():
    if not os.path.exists(SHOWDIR):
        os.makedirs(SHOWDIR)


def tearDown_test_show_dir():
    shutil.rmtree(SHOWDIR)

tearDown_test_db()

if __name__ == '__main__':
    print "=================="
    print "Dont call this directly"
    print "=================="
    print "you might want to call"

    dirList = os.listdir(TESTDIR)
    for fname in dirList:
        if (fname.find("_test") > 0) and (fname.find("pyc") < 0):
            print "- " + fname

    print "=================="
    print "or just call all_tests.py"

########NEW FILE########
__FILENAME__ = tv_tests
# coding=UTF-8
# Author: Dennis Lutter <lad1337@gmail.com>
# URL: http://code.google.com/p/sickbeard/
#
# This file is part of Sick Beard.
#
# Sick Beard is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Sick Beard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.

import unittest
import test_lib as test

import sickbeard
from sickbeard.tv import TVEpisode, TVShow


class TVShowTests(test.SickbeardTestDBCase):

    def setUp(self):
        super(TVShowTests, self).setUp()
        sickbeard.showList = []

    def test_init_tvdbid(self):
        show = TVShow(0001, "en")
        self.assertEqual(show.tvdbid, 0001)

    def test_change_tvdbid(self):
        show = TVShow(0001, "en")
        show.name = "show name"
        show.tvrname = "show name"
        show.network = "cbs"
        show.genre = "crime"
        show.runtime = 40
        show.status = "5"
        show.airs = "monday"
        show.startyear = 1987

        show.saveToDB()
        show.loadFromDB(skipNFO=True)

        show.tvdbid = 0002
        show.saveToDB()
        show.loadFromDB(skipNFO=True)

        self.assertEqual(show.tvdbid, 0002)

    def test_set_name(self):
        show = TVShow(0001, "en")
        show.name = "newName"
        show.saveToDB()
        show.loadFromDB(skipNFO=True)
        self.assertEqual(show.name, "newName")


class TVEpisodeTests(test.SickbeardTestDBCase):

    def setUp(self):
        super(TVEpisodeTests, self).setUp()
        sickbeard.showList = []

    def test_init_empty_db(self):
        show = TVShow(0001, "en")
        ep = TVEpisode(show, 1, 1)
        ep.name = "asdasdasdajkaj"
        ep.saveToDB()
        ep.loadFromDB(1, 1)
        self.assertEqual(ep.name, "asdasdasdajkaj")


class TVTests(test.SickbeardTestDBCase):

    def setUp(self):
        super(TVTests, self).setUp()
        sickbeard.showList = []

    def test_getEpisode(self):
        show = TVShow(0001, "en")
        show.name = "show name"
        show.tvrname = "show name"
        show.network = "cbs"
        show.genre = "crime"
        show.runtime = 40
        show.status = "5"
        show.airs = "monday"
        show.startyear = 1987
        show.saveToDB()
        sickbeard.showList = [show]
        #TODO: implement


if __name__ == '__main__':
    print "=================="
    print "STARTING - TV TESTS"
    print "=================="
    print "######################################################################"
    suite = unittest.TestLoader().loadTestsFromTestCase(TVShowTests)
    unittest.TextTestRunner(verbosity=2).run(suite)
    print "######################################################################"
    suite = unittest.TestLoader().loadTestsFromTestCase(TVEpisodeTests)
    unittest.TextTestRunner(verbosity=2).run(suite)
    print "######################################################################"
    suite = unittest.TestLoader().loadTestsFromTestCase(TVTests)
    unittest.TextTestRunner(verbosity=2).run(suite)

########NEW FILE########

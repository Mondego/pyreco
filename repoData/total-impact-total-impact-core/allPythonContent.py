__FILENAME__ = config
import os
import re
import sys
import argparse

app_name = {
    'production': "total-impact-core",
    'staging': "total-impact-core-staging"
}



##################################################################
#
# helper functions
#
##################################################################

def get_config_str_rules(source):
    config_rules = {
        "heroku": {
            "comment": "=",
            "sep": ":"
        },
        ".env": {
            "comment": "#",
            "sep": "="
        }
    }
    return config_rules[source]


def read_config_str(str, source):
    rules = get_config_str_rules(source)
    for line in str.split("\n"):

        m = re.search(rules["comment"], line)
        if m is not None and m.start() == 0:
            continue

        try:
            k, v = line.split(rules["sep"], 1)  # first occurance only
            yield k.strip(), v.strip()
        except ValueError:
            continue  # line wasn't a value assignment, move on

def get_dot_env_path(environment):
    vars_filename = ".env-" + environment
    path = os.path.abspath(os.path.join(os.path.dirname(__file__), vars_filename))
    return path


def get_dot_env_str_from_file(environment):
    try:
        path = get_dot_env_path(environment)
        with open(path, "r") as f:
            str = f.read()
            return str
    except IOError:
        return None





##################################################################
#
# main functions
#
##################################################################


def set_env_vars_from_dot_env():
    str = get_dot_env_str_from_file("local")
    if str is None:  # there was no config file for that environment
        return False

    for k, v in read_config_str(str, ".env"):
        print "setting {k} to {v}".format(k=k, v=v)
        os.environ[k] = v


def push_dot_env_to_heroku(environment):
    str = get_dot_env_str_from_file(environment)
    lines = []
    for kv_tuple in read_config_str(str, ".env"):
        if  kv_tuple[0].find("HEROKU_POSTGRESQL") == 0:
            continue
        else:
            lines.append(kv_tuple[0] + '=' + kv_tuple[1])

    space_delimited_lines = " ".join(lines)

    command_str = "heroku config:add {lines} --app {app}".format(
        lines=space_delimited_lines,
        app=app_name[environment]
    )
    print "running this command: " + command_str
    os.system(command_str)


def pull_dot_env_from_heroku(environment):
    name = app_name[environment]
    str = os.popen('heroku config --app ' + name).read()

    lines = []
    for key_value_pair in read_config_str(str, "heroku"):
        lines.append('='.join(key_value_pair))
    config_str = "\n".join(lines)


    path = get_dot_env_path(environment)
    print "writing heroku {env} configs to {path}: {str}".format(
        env=environment,
        path=path,
        str=config_str)

    with open(path, "w") as f:
        f.write(config_str)




##################################################################
#
# called from command line
#
##################################################################



if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='pull heroku env vars to local .env files')

    parser.add_argument('--production', help="pull from the production app instead of staging", action="store_true")
    args = parser.parse_args()
    if args.production:
        pull_dot_env_from_heroku("production")
    else:
        pull_dot_env_from_heroku("staging")









########NEW FILE########
__FILENAME__ = load_batch_pmc
import couchdb
import argparse
import requests
import logging
import os
import sys
import json
import re
import datetime
import calendar
from totalimpact import provider_batch_data

logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format='[%(process)d] %(levelname)8s %(threadName)30s %(name)s - %(message)s'
)
logger = logging.getLogger("load_batch_pmc")

def last_minute_of_a_month(year, month):
    first_day = datetime.datetime(year, month, 1)
    number_of_days = calendar.monthrange(year, month)[1]
    first_day_next_month = first_day + datetime.timedelta(days=number_of_days)
    last_day_last_minute = first_day_next_month - datetime.timedelta(microseconds=1)
    return last_day_last_minute

def build_batch_dict(page, options_dict):
    pmids = re.findall('pubmed-id="(\d{2,20})"', page)

    year = int(options_dict["year"])
    month = int(options_dict["month"])
    min_date = datetime.datetime(year, month, 1)
    max_date = last_minute_of_a_month(year, month)
    batch_dict = {
       "_id": "pmc{year}{month}".format(**options_dict),
       "type": "provider_data_dump",
       "provider": "pmc",
       "provider_raw_version": 1,
       "created": datetime.datetime.now().isoformat(),
       "min_event_date": min_date.isoformat(),
       "max_event_date": max_date.isoformat(),
       "aliases": {
           "pmid": pmids
       },
       "raw": page
    }
    return batch_dict

def get_pmc_stats_page(options_dict):
    #add leading zero if not given
    if len(str(options_dict["month"]))==1:
        options_dict["month"] = "0" + str(options_dict["month"])

    pmc_download_template = "http://www.pubmedcentral.nih.gov/utils/publisher/pmcstat/pmcstat.cgi?year={year}&month={month}&jrid={journal}&form=xml&user={user}&password={password}"
    url = pmc_download_template.format(**options_dict)
    print url
    resp = requests.get(url)
    page = resp.text
    return page

def write_batch_dict(batch_dict):
    logger.info("connected to postgres at " + os.getenv("POSTGRESQL_URL"))
    new_object = provider_batch_data.create_objects_from_doc(batch_dict)
    print "added to db if it wasn't already there"

    print "current batch data:"
    matches = provider_batch_data.ProviderBatchData.query.filter_by(provider="pmc").order_by("min_event_date").all()
    for match in matches:
        print match        




if __name__ == '__main__':
    # get args from the command line:
    parser = argparse.ArgumentParser(description="Import PMC monthly stats")
    parser.add_argument("-y", "--year", default="2013")
    parser.add_argument("-m", "--month", required=True)
    parser.add_argument("-j", "--journal", default="elife")
    parser.add_argument("-u", "--user", default="elife_pmc")
    parser.add_argument("-p", "--password", required=True)

    options_dict = vars(parser.parse_args())
    print options_dict

    page = get_pmc_stats_page(options_dict)
    batch_dict = build_batch_dict(page, options_dict)
    print(json.dumps(batch_dict, indent=4))
    if batch_dict["aliases"]["pmid"]:
        write_batch_dict(batch_dict)
    else:
        print "no data for this month, not saving anything"





########NEW FILE########
__FILENAME__ = get_refsets
import random_pmids
import requests
import os
import json
import re
import random

def get_nth_figshare_id(url_template, year, n):
    (page_minus_one, offset) = divmod(n, 10)  #figshare api always has 10 things on a page
    url = url_template.format(page=page_minus_one+1, year=year)
    print url
    r = requests.get(url, timeout=10)    
    response = json.loads(r.text)
    try:
        items = response["items"]
        item = items[offset]
        doi = item["DOI"]
        year_of_item = item["published_date"][-4:]
    except (AttributeError):
        print "no doi found ", item
        doi = None
    except KeyError:
        print response
        # try the next page, as a workaround
        return get_nth_figshare_id(url_template, year, n+10)

    doi = doi.replace("http://dx.doi.org/", "")
    return({"doi":doi, "year":year_of_item})

def get_random_figshare_dois(year, sample_size, email, query, seed=None):
    if year < 2011:
        return []

    url_template = "http://api.figshare.com/v1/articles/search?search_for=*&from_date={year}-01-01&to_date={year}-12-31&page={page}&has_publisher_id=0"

    # Do an initial query to get the total number of hits
    url = url_template.format(page=1, year=year)
    print url
    r = requests.get(url)
    initial_response = json.loads(r.text)
    print initial_response
    population_size = initial_response["items_found"]
    print "Number of figshare items returned by this query: %i" %population_size
    print "Off to randomly sample %i of them!" %sample_size

    if seed:
        random.seed(seed)
        print "Seed has been set before sampling."

    if sample_size > population_size:
        print "sample size is bigger than population size, so using population size"
        sample_size = population_size
    random_indexes = random.sample(range(1, population_size), sample_size)

    figshare_ids = []
    for random_index in random_indexes:
        figshare_dict = get_nth_figshare_id(url_template, year, random_index)
        doi = figshare_dict["doi"]
        print "figshare_id:" + doi
        figshare_ids.append(doi)
    return figshare_ids


def get_random_dryad_dois(year, sample_size, email, query, seed=None):
    if year < 2009:
        return []

    url = "http://datadryad.org/solr/search/select/?q=location:l2+dc.date.available_dt:[{year}-01-01T00:00:00Z%20TO%20{year}-12-31T23:59:59Z]&fl=dc.identifier&rows=1000000".format(year=year)
    print url
    try:
        r = requests.get(url, timeout=10)
    except requests.Timeout:
        print "dryad isn't working right now (timed out); sending back an empty list."
        return []
    # sick of parsing xml to get simple things.  I'm using regex this time, darn it.
    all_dois = re.findall("<str>doi:(10.5061/dryad.[a-z0-9]+)</str>", r.text)
    print all_dois[1]
    print len(all_dois)

    if seed:
        random.seed(seed)
    random_dois = random.sample(all_dois, sample_size)
    return random_dois

def get_random_crossref_dois(year, sample_size):
    url = "http://random.labs.crossref.org/dois?from=2000&count=n" + str(sample_size)
    try:
        r = requests.get(url, timeout=10)
    except requests.Timeout:
        print "the random doi service isn't working right now (timed out); sending back an empty list."
        return []
    dois = json.loads(r.text)
    return dois

def get_random_github_dict():
    from totalimpact.providers import github
    # downloaded from http://archive.org/details/archiveteam-github-repository-index-201212
    filename = "/Users/hpiwowar/Documents/Projects/tiv2/total-impact-core/extras/build_refsets/github-repositories.txt"

    import collections
    import subprocess
    command = "wc " + filename
    p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
    out, error = p.communicate()
    num_repos = int(out.strip().split(" ")[0])
    print num_repos
    seed = 42
    random.seed(seed)

    provider = github.Github()

    random_repos = collections.defaultdict(list)

    i = 0
    while (len(random_repos["2009"]) < 100):
        i += 1
        line_number = random.randint(0, num_repos)
        command = "tail -n +{line_number} {filename} | head -n 1".format(
            line_number=line_number, filename=filename)
        p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
        repo_id, error = p.communicate()
        print repo_id
        repo_url = "https://github.com/" + repo_id.strip()
        biblio = provider.biblio([("url", repo_url)])
        try:
            year = biblio["create_date"][0:4]
        except KeyError:
            continue
        random_repos[year] += [repo_url]
        print "\nn={number_sampled}".format(number_sampled=i)
        for year in sorted(random_repos.keys()):
            print "{year}: {num}".format(year=year, num=len(random_repos[year]))
        if i%100 == 0:
            print random_repos
    print random_repos
    return random_repos

def get_random_github_ids(year, sample_size, email, query, seed):
    if year < 2009:
        return []

    filename = "/Users/hpiwowar/Documents/Projects/tiv2/total-impact-core/extras/build_refsets/random_github_repos.py"
    from random_github_repos import random_github_repos

    if seed:
        random.seed(seed)
    random_ids = random.sample(random_github_repos[str(year)], sample_size)
    return random_ids


def make_collection(namespace, nids, title, refset_metadata):
    aliases = [(namespace, nid) for nid in nids]
    url = api_url + "/v1/collection?key=" + os.getenv("API_KEY")
    resp = requests.post(
        url,
        data=json.dumps({
            "aliases": aliases,
            "title": title,
            "refset_metadata": refset_metadata
        }),
        headers={'Content-type': 'application/json'}
    )
    collection_id = json.loads(resp.text)["collection"]["_id"]
    print collection_id
    return collection_id

def collection_metadata(genre, refset_name, year, seed, sample_size):
    refset_metadata = {
        "genre": genre, 
        "version": 0.1,
        "name": refset_name, 
        "year": year, 
        "seed": seed,
        "sample_size": sample_size
    }  
    return refset_metadata


def build_collections(refset_name, get_random_ids_func, genre, namespace, query_template, title_template, year, sample_size, seed):
    collection_ids = []
    print refset_name
    if query_template:
        query = query_template[refset_name].format(year=year)
    else:
        query = None
    ids = get_random_ids_func(year, sample_size, email, query, seed)
    if ids:
        print ids
        refset_metadata = collection_metadata(genre, refset_name, year, seed, sample_size)
        title = title_template.format(**refset_metadata)
        collection_id = make_collection(namespace, ids, title, refset_metadata)
        print collection_id
        collection_ids.append(collection_id)
    return collection_ids



def build_reference_sets(refset_name, query_templates, title_template, years, sample_size, seed):
    collection_ids = []
    for year in years:
        print year
        if refset_name == "eutils":
            collection_ids += build_collections(refset_name, random_pmids.get_random_pmids, "article", "pmid", query_template, title_template, year, sample_size, seed)
        elif refset_name == "crossref":
            collection_ids += build_collections(refset_name, get_random_crossref_dois, "article", "doi", None, title_template, year, sample_size, "")
        elif refset_name == "dryad":
            collection_ids += build_collections(refset_name, get_random_dryad_dois, "dataset", "doi", None, title_template, year, sample_size, seed)
        elif refset_name == "figshare":
            collection_ids += build_collections(refset_name, get_random_figshare_dois, "dataset", "doi", None, title_template, year, sample_size, seed)
        elif refset_name == "github":
            collection_ids += build_collections(refset_name, get_random_github_ids, "software", "url", None, title_template, year, sample_size, seed)
    return(collection_ids)

#Useful references for queries:
#http://www.nlm.nih.gov/bsd/funding_support.html
#http://www.nlm.nih.gov/bsd/grant_acronym.html#pub_health
query_templates = {
    #"eutils": {
        #'pubmed':   'Journal Article[pt] AND {year}[dp]',
        #'nih':          "(Research Support, U.S. Gov't, P.H.S. [pt] OR nih [gr]) AND Journal Article[pt] AND {year}[dp]",
        #'plosone':      '"plos one"[journal] AND Journal Article[pt] AND {year}[dp]',
        #'nature':       '"nature"[journal] AND Journal Article[pt] AND {year}[dp]',
        #'science':      '"science"[journal] AND Journal Article[pt] AND {year}[dp]',
    #    },
    }


title_template = "REFSET {name}, {year} ({genre}) n={sample_size}"
email = "team@impactstory.org"
# export API_ROOT=total-impact-core.herokuapp.com
# export API_ROOT=total-impact-core-staging.herokuapp.com
# api_url = "http://" + os.getenv("API_ROOT")
api_url = "http://total-impact-core-staging.herokuapp.com"

sample_size = 100
seed = 42
years = range(2013, 2014)
refset_name = "dryad"

collection_ids = build_reference_sets(refset_name, query_templates, title_template, years, sample_size, seed)
for collection_id in collection_ids:
    print collection_id


########NEW FILE########
__FILENAME__ = random_pmids
import os
import re
import argparse
import requests
import time
import BeautifulSoup
import random

""" Returns random PubMed IDs that match a given PubMed query. """

# Please read NCBI eUtils Terms of Use: http://www.ncbi.nlm.nih.gov/books/NBK25497/#chapter2.Usage_Guidelines_and_Requiremen
# At the time of this writing, it includes:
""" NCBI recommends that users post no more than three URL requests per second 
and limit large jobs to either weekends or between 9:00 PM and 5:00 AM Eastern time 
during weekdays. Failure to comply with this policy may result in an IP address being 
blocked from accessing NCBI. If NCBI blocks an IP address, service will not be restored 
unless the developers of the software accessing the E-utilities register values of 
the tool and email parameters with NCBI"""

def parse_random_doi_args():
	# get args from the command line:
	parser = argparse.ArgumentParser(description="""Returns random PubMed IDs that match a given PubMed query.  
		May contain duplicates.
		Returns three PMIDs per second""")
	parser.add_argument("sample_size", type=int, help="number of PubMed IDs to return")
	parser.add_argument("email", type=str, help="email address to send in api request to NCBI")
	parser.add_argument("query", type=str, help="query to filter PubMed IDs (surround the query with 'single quotes')")
	parser.add_argument("--seed", type=str, help='seed for random numbers, for reproducability.  (default is no set seed.  Example seed: 42)')
	args = parser.parse_args()
	return args

url_template = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&email={email}&term={query}&retmax=1&retstart={random_index}"

def get_nth_pmid(index, query, email):
	url = url_template.format(random_index=index, query=query, email=email)
	r = requests.get(url)
	return(r)

def get_random_pmids(sample_size, email, query, seed=None):
	# Do an initial query to get the total number of hits
	url = url_template.format(random_index=1, query=query, email=email)
	r = requests.get(url)
	initial_response = r.text
	soup = BeautifulSoup.BeautifulStoneSoup(initial_response)
	translated_query = soup.querytranslation.string
	population_size = int(soup.esearchresult.count.string)

	print "Double-check PubMed's translation of your query: %s" %translated_query
	print "Number of PMIDs returned by this query: %i" %population_size
	print "Off to randomly sample %i of them!" %sample_size

	if seed:
		random.seed(seed)
		print "Seed has been set before sampling."

	pmid_pattern = re.compile("<Id>(?P<pmid>\d+)</Id>")  # do this as an re because it is simple and fast

	if sample_size > population_size:
		print "sample size is bigger than population size, so using population size"
		sample_size = population_size
	random_indexes = random.sample(range(1, population_size), sample_size)

	pmids = []
	for random_index in random_indexes:
		r = get_nth_pmid(random_index, query, email)
		try:
			pmid = pmid_pattern.search(r.text).group("pmid")

		#hope this is transient, try the random number + 1
		except AttributeError:
			print "got an error extracting pmid, trying again with subsequent index"
			r = get_nth_pmid(random_index+1, query, email)
			pmid = pmid_pattern.search(r.text).group("pmid")

		print "pmid:" + pmid
		pmids.append(pmid)
		time.sleep(1/3)  #NCBI requests no more than three requests per second at http://www.ncbi.nlm.nih.gov/books/NBK25497/#chapter2.Usage_Guidelines_and_Requiremen
	return pmids

if __name__ == '__main__':
	args = parse_random_doi_args()
	pmids = get_random_pmids(args.sample_size, args.email, args.query, args.seed)



########NEW FILE########
__FILENAME__ = wos-refsets
import re
import random
import json
import requests

# from get_refsets.py
def make_collection(api_url, namespace, nids, title, refset_metadata):
    aliases = [(namespace, nid) for nid in nids]
    url = api_url + "/collection"
    resp = requests.post(
        url,
        data=json.dumps({
            "aliases": aliases,
            "title": title,
            "refset_metadata": refset_metadata
        }),
        headers={'Content-type': 'application/json'}
    )
    collection_id = json.loads(resp.text)["collection"]["_id"]
    return collection_id

# cat */*.bib > all.bib
#filename = "/Users/hpiwowar/Dropbox/ti/wos-refset/all.bib"
filename = "/Users/hpiwowar/Dropbox/ti/wos-refset/2012/all2012.bib"
contents = open(filename, "r").read()
doi_pattern = re.compile("DOI = {{(.+)}},")
all_dois = doi_pattern.findall(contents)

year_pattern = re.compile("Year = {{(.+)}}")
all_years = year_pattern.findall(contents)

doi_years = zip(all_dois, all_years)

title_template = "REFSET {name}, {year} ({genre}) n={sample_size}"
#api_url = "http://total-impact-core-staging.herokuapp.com"
api_url = "http://total-impact-core.herokuapp.com"

for refset_year in range(2012, 2013):
    print refset_year
    dois = [doi for (doi, year) in doi_years if year==unicode(refset_year)]
    print len(dois)
    seed = 42
    random.seed(seed)
    sample_size = 100
    selected = random.sample(dois, min(sample_size, len(dois)))
    refset_metadata = {
        "genre": "article", 
        "version": 0.1,
        "name": "WoS",
        "year": refset_year, 
        "seed": seed,
        "sample_size": sample_size
    }
    title = title_template.format(**refset_metadata)
    collection_id = make_collection(api_url, "doi", selected, title, refset_metadata)
    print collection_id


########NEW FILE########
__FILENAME__ = build_collections_from_issn
import requests, os, json, time
from totalimpact.updater import active_publishers

def make_collection(namespace, nids, title, api_url):
    aliases = [(namespace, nid) for nid in nids]
    url = api_url + "/v1/collection?key=Heather"
    data_payload = json.dumps({
            "aliases": aliases,
            "title": title
        })
    resp = requests.post(url, data=data_payload, headers={'Content-type': 'application/json'})
    print resp.status_code
    collection_response = resp.json
    collection_id = collection_response["collection"]["_id"]
    print collection_id
    return collection_id

def get_all_crossref_dois_for_an_issn(issn):
    crossref_api_page_number = 1
    crossref_api_page_size = 100
    dois = []
    while crossref_api_page_number:
        print("page {crossref_api_page_number}".format(
            issn=issn, crossref_api_page_number=crossref_api_page_number))
        url = "http://search.labs.crossref.org/dois?q={issn}&rows={page_size}&page={page}&header=true".format(
            issn=issn, page=crossref_api_page_number, page_size=crossref_api_page_size)
        #print url
        try:
            r = requests.get(url, timeout=10)
        except requests.Timeout:
            print "timeout"
            return []
        dois_on_page = [item["doi"] for item in r.json["items"]]
        #print dois_on_page
        dois += dois_on_page
        if crossref_api_page_number*crossref_api_page_size > r.json["totalResults"]:
            crossref_api_page_number = None
        else:
            crossref_api_page_number += 1
    return dois

def create_issn_collections(publishers_dict, title_template, items_per_collection, api_url):
    collection_ids = []
    for publisher in publishers_dict:
        print "\n***", publisher
        for journal in publishers_dict[publisher]["journals"]:
            print "\n", journal
            title = title_template.format(
                issn=journal["issn"], title=journal["title"], publisher=publisher)
            issn_dois = get_all_crossref_dois_for_an_issn(journal["issn"])
            print("got {num} dois".format(
                num = len(issn_dois)))
            if issn_dois:
                dois_in_pages = [issn_dois[i:i+items_per_collection] for i in xrange(0, len(issn_dois), items_per_collection)]
                for dois_for_collection in dois_in_pages:
                    #print dois
                    if True:
                        collection_id = make_collection("doi", dois_for_collection, title, api_url)
                        print title        
                        collection_ids.append(collection_id)
                        print "sleeping to let collection get made slowly\n"
                        time.sleep(30)  #seconds
    return(collection_ids)


# export API_ROOT=total-impact-core.herokuapp.com
# export API_ROOT=total-impact-core-staging.herokuapp.com


#api_url = "http://" + os.getenv("API_ROOT")
api_url = "http://total-impact-core.herokuapp.com"
title_template = "ISSN {issn}: {title}, published by {publisher}"
items_per_collection = 20

collection_ids = create_issn_collections(active_publishers, title_template, items_per_collection, api_url)
for collection_id in collection_ids:
    print collection_id




########NEW FILE########
__FILENAME__ = couch_maint
import couchdb, os, logging, sys, collections
from pprint import pprint
import time
import requests

# run in heroku by a) commiting, b) pushing to heroku, and c) running
# heroku run python extras/db_housekeeping/couch_maint.py

logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format='[%(process)d] %(levelname)8s %(threadName)30s %(name)s - %(message)s'
)

logger = logging.getLogger("couch_maint")
 
cloudant_db = os.getenv("CLOUDANT_DB")
cloudant_url = os.getenv("CLOUDANT_URL")

couch = couchdb.Server(url=cloudant_url)
db = couch[cloudant_db]
logger.info(u"connected to couch at " + cloudant_url + " / " + cloudant_db)

"""
admin/items_with_pmids

function(doc) {
  if (doc.type == "item") {
    if (typeof doc.aliases.pmid != "undefined") {
     emit(doc._id, 1);
    }
  }
}"""
def del_pmids():
    view_name = "admin/items_with_pmids"
    tiids = []
    for row in db.view(view_name, include_docs=True):
        tiid = row.id
        tiids += [tiid]
        del(row.doc["aliases"]["pmid"])
        logger.info(u"saving doc '{id}'".format(id=row.id))
        db.save(row.doc)

    logger.info(u"finished looking, found {num_tiids} tiids with pmids".format(
        num_tiids=len(tiids)))



"""
admin/multiple_dois

function(doc) {
    // lists tiids by individual alias namespaces and ids
    if (doc.type == "item") {
        // expecting every alias object has a tiid
        tiid = doc["_id"];
    if (typeof doc.aliases.doi != "undefined") {
       if (doc.aliases.doi.length > 1) {
           emit(doc._id, 1)
       }
    }
    }
}

admin/collections_by_tiid
function(doc) {
    // lists tiids by individual alias namespaces and ids
    if (doc.type == "collection") {
    doc.item_tiids.forEach(function(tiid) {
           emit(tiid, doc._id)
        })
   }
}

"""
def bad_doi_cleanup():
    view_name = "admin/multiple_dois"

    changed_rows = 0
    for row in db.view(view_name, include_docs=True):

        logger.info(u"got doc '{id}' back from {view_name}".format(
            id=row.id,
            view_name=view_name
        ))

        doc = row.doc
        tiid = row.id

        logger.info(u"\n\nPROCESSING ITEM {id}".format(id=tiid))            
        pprint(doc["aliases"])

        # delete tiid from collections
        collections_with_tiid = db.view("admin/collections_by_tiid", include_docs=True, key=tiid)
        for collection_row in collections_with_tiid:
            collection = collection_row.doc
            logger.info(u"deleting from collection {id}".format(id=collection_row.id))
            #pprint(collection)            
            good_tiids = [x for x in collection["item_tiids"] if tiid not in x]
            collection["item_tiids"] = good_tiids
            db.save(collection)

        # delete snaps
        snaps_with_tiid = db.view("admin/snaps_by_tiid", include_docs=True, key=tiid)
        for snap_row in snaps_with_tiid:
            snap = snap_row.doc
            pprint(snap)            
            logger.info(u"deleting SNAP {id}".format(id=snap_row.id))            
            #db.delete(snap)

        # delete the item
        logger.info(u"deleting item {id}".format(id=tiid))            
        #db.delete(doc)

        changed_rows += 1

    logger.info(u"finished the update.")
    logger.info(u"changed %i rows" %changed_rows)


"""
function(doc) {
    emit([doc.type, doc._id], doc);
}
"""
def build_alias_items_in_collections():
    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, 
            include_docs=True, 
            start_key=["collection", "0000000000"], 
            endkey=["collection", "zzzzzzzzzz"])
    total_rows = len(view_rows)
    logger.info(u"total rows = %i" % total_rows)
    row_count = 0

    for row in view_rows:
        row_count += 1
        logger.info(u"now on rows = %i of %i, id %s" % (row_count, total_rows, row.id))
        collection = row.doc
        #pprint(collection)
        if collection.has_key("alias_tiids"):
            logger.info(u"skipping")
        else:
            item_tiids = collection["item_tiids"]
            alias_tiids = dict(zip(["unknown-"+tiid for tiid in item_tiids], item_tiids))
            collection["alias_tiids"] = alias_tiids
            db.save(collection)
            logger.info(u"saving")



"""
function(doc) {
    emit([doc.type, doc._id], doc);
}
"""
def delete_item_tiids_from_collection():
    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, 
            include_docs=True, 
            start_key=["collection", "0000000000"], 
            endkey=["collection", "zzzzzzzzzz"])
    total_rows = len(view_rows)
    logger.info(u"total rows = %i" % total_rows)
    row_count = 0

    for row in view_rows:
        row_count += 1
        logger.info(u"now on rows = %i of %i, id %s" % (row_count, total_rows, row.id))
        collection = row.doc
        #pprint(collection)
        if collection.has_key("item_tiids"):
            del collection["item_tiids"]
            db.save(collection)
            logger.info(u"saving")



"""
function(doc) {
    emit([doc.type, doc._id], doc);
}
"""
def put_snaps_in_items():
    logger.debug(u"running put_snaps_in_items() now.")
    starttime = time.time()
    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500

    start_key = ["metric_snap", "000000000"]
    end_key = ["metric_snap", "zzzzzzzzzzzzzzzzzzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)

    #for row in view_rows[startkey:endkey]:
    while page:
        for row in page:
            if not "metric_snap" in row.key[0]:
                #print "not a metric_snap so skipping", row.key
                continue
            #print row.key
            row_count += 1
            snap = row.doc
            item = db.get(snap["tiid"])

            if item:
                saving = True
                while saving:
                    try:
                        from totalimpact import item
                        updated_item = item.add_snap_data(item, snap)

                        # to decide the proper last modified date
                        snap_last_modified = snap["created"]
                        item_last_modified = item["last_modified"]
                        updated_item["last_modified"] = max(snap_last_modified, item_last_modified)
                        
                        logger.info(u"now on snap row %i, saving item %s back to db, deleting snap %s" % 
                            (row_count, updated_item["_id"], snap["_id"]))

                        db.save(updated_item)
                        #db.delete(snap)
                        saving = False
                    except couchdb.http.ResourceConflict:
                        logger.warning(u"couch conflict.  trying again")
                        pass
            else:
                logger.warning(u"now on snap row %i, couldn't get item %s for snap %s" % 
                    (row_count, snap["tiid"], snap["_id"]))

        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None

    logger.info(u"updated {rows} rows in {elapsed} seconds".format(
        rows=row_count, elapsed=round(time.time() - starttime)
    ))


"""
function(doc) {
    emit([doc.type, doc._id], doc);
}
"""
def ensure_all_metric_values_are_ints():
    #except F1000 Yes's
    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500

    start_key = ["item", "000000000"]
    end_key = ["item", "zzzzzzzzzzzzzzzzzzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)

    while page:
        for row in page:
            row_count += 1
            item = row.doc
            save_me = False
            try:
                metric_names = item["metrics"].keys()
                for metric_name in metric_names:
                    raw = item["metrics"][metric_name]["values"]["raw"]
                    if "scopus:citations" in metric_name:
                        logger.info(item["metrics"][metric_name])
                    if isinstance(raw, basestring):
                        if raw != "Yes":
                            logger.info(u"casting to int")
                            #logger.info(item)
                            item["metrics"][metric_name]["values"]["raw"] = int(raw)
                            raw = int(raw)
                            save_me=True
                    if not raw:
                        logger.info(u"removing a zero")
                        #logger.info(item)
                        del item["metrics"][metric_name]
                        save_me=True
                if save_me:
                    logger.info(u"saving")
                    db.save(item)
                else:
                    #logger.info(u"%i." %row_count)
                    pass
            except KeyError:
                pass
        logger.info(u"%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None

def delete_test_collections():
    view_name = "queues/latest-collections"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500

    start_key = [1, "000000000"]
    end_key = [1, "9999999"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)
    number_deleted = 0
    number_items = 0

    try:
        while page:
            for row in page:
                row_count += 1
                collection = row.doc
                logger.info(u"deleting test collection {cid}:{title}".format(
                    cid=collection["_id"], title=collection["title"]))
                number_deleted += 1
                number_items += len(collection["alias_tiids"])
                #db.delete(collection)
            logger.info(u"%i. getting new page, last id was %s" %(row_count, row.id))
            if page.has_next:
                page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
            else:
                page = None

    except TypeError:
        pass

    print "number deleted = ", number_deleted
    print "number items = ", number_items

"""
function(doc) {
  if (doc.type=="collection") {
    if (doc.alias_tiids) {
      for (var alias in doc.alias_tiids) {
    var item_id = doc.alias_tiids[alias]
        emit(item_id, 1);
      }
    }
  }
}
"""
def delete_orphan_items():
    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500

    start_key = ["item", "000000000"]
    end_key = ["item", "zzzzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)
    number_deleted = 0
    date_deleted = collections.defaultdict(int)

    while page:
        for row in page:
            row_count += 1
            tiid = row.key[1]
            item = row.doc

            tiid_in_collection_response = db.view("tiids_in_collections/tiids_in_collections", include_docs=False, key=tiid)
            tiid_in_collection = tiid_in_collection_response.rows
            print tiid_in_collection
            if len(tiid_in_collection) > 0:
                logger.info(u"\nitem {tiid} is in a collection, not deleting".format(tiid=tiid))
            else:
                logger.info(u"\nitem {tiid} is not in a collection, deleting.".format(tiid=tiid))
                try:
                    #db.delete(item)
                    number_deleted += 1
                    date_deleted[item["created"][0:10]] += 1
                except (TypeError, couchdb.http.ResourceNotFound):  #happens sometimes if already deleted
                    pass

        logger.info(u"%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None
        print "number of items deleted", number_deleted
        print date_deleted


def remove_unused_item_doc_keys():
    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500
    start_key = ["item", "000000000"]
    end_key = ["item", "zzzzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)
    number_edited = 0

    while page:
        for row in page:
            item = row.doc
            edited = False
            row_count += 1
            try:
                if "providers_run" in item:
                    del item["providers_run"]
                    edited = True
                if "providersRunCounter" in item:
                    del item["providersRunCounter"]
                    edited = True
                if "providersWithMetricsCount" in item:
                    del item["providersWithMetricsCount"]
                    edited = True
                if "created" in item["aliases"]:
                    del item["aliases"]["created"]
                    edited = True
                if "last_modified" in item["aliases"]:
                    del item["aliases"]["last_modified"]
                    edited = True
                if "h1" in item["biblio"]:
                    h1_orig = item["biblio"]["h1"]
                    h1_updated = item["biblio"]["h1"].strip()
                    if h1_updated:
                        if h1_updated != h1_orig:
                            item["biblio"]["h1"] = h1_updated    
                            edited = True
                    else:                        
                        del item["biblio"]["h1"]
                        edited = True
            except TypeError:  #item sometimes NoneType
                pass

            if edited:
                print row.id
                print row.doc.keys(), row.doc["aliases"].keys(), row.doc["biblio"].keys()
                print item.keys(), item["aliases"].keys(), item["biblio"].keys()
                logger.info(u"saving modified item {tiid}\n".format(
                    tiid=item["_id"]))
                number_edited += 1
                db.save(item)
            else:
                logger.info(u".")

        print "number edited = ", number_edited
        print "number items = ", row_count
        logger.info(u"%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None

    print "number edited = ", number_edited
    print "number items = ", row_count


def lowercase_aliases():
    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500
    start_key = ["item", "000000000"]
    end_key = ["item", "zzzzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)
    number_edited = 0

    while page:
        for row in page:
            item = row.doc
            edited = False
            row_count += 1
            if not item:
                continue
            if "aliases" in item:
                orig_aliases_dict = item["aliases"]

                lowercase_aliases_dict = {}
                for orig_namespace in orig_aliases_dict:
                    lowercase_namespace = orig_namespace.lower()
                    if lowercase_namespace == "doi":
                        lowercase_aliases_dict[lowercase_namespace] = [doi.lower() for doi in orig_aliases_dict[orig_namespace]]
                    else:
                        lowercase_aliases_dict[lowercase_namespace] = orig_aliases_dict[orig_namespace]

                if orig_aliases_dict != lowercase_aliases_dict:
                    print "\ndifference detected \n{orig}\n{lower}\n".format(
                        orig=orig_aliases_dict, lower=lowercase_aliases_dict)
                    item["aliases"] = lowercase_aliases_dict
                    number_edited += 1
                    db.save(item)
                else:
                    logger.info(u".")
                        
        print "number edited = ", number_edited
        print "number items = ", row_count
        logger.info(u"%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None

    print "number edited = ", number_edited
    print "number items = ", row_count


def update_github():
    from totalimpact import item, tiredis
    myredis = tiredis.from_url(os.getenv("REDISTOGO_URL"), db=0)

    view_name = "queues/by_alias"
    view_rows = db.view(view_name, include_docs=False)
    row_count = 0
    page_size = 500
    start_key = ["url", "https://github.0000000"]
    end_key = ["url", "https://github.zzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=False, start_key=start_key, end_key=end_key)

    while page:
        for row in page:
            tiid = row.id
            item.start_item_update([tiid], myredis, db, sleep_in_seconds=0.05)                        
            row_count += 1
            print "."
        logger.info(u"%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None

    print "number items = ", row_count


def fix_github_year():
    from totalimpact import item, tiredis
    myredis = tiredis.from_url(os.getenv("REDISTOGO_URL"), db=0)

    view_name = "queues/by_alias"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500
    start_key = ["url", "https://github.0000000"]
    end_key = ["url", "https://github.zzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)

    while page:
        for row in page:
            doc = row.doc
            print row.id
            try:
                doc["biblio"]["year"] = doc["biblio"]["create_date"][0:4]
                db.save(doc)
            except KeyError:
                pass
            row_count += 1
            print "."
        logger.info(u"%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None

    print "number items = ", row_count



"""
function(doc) {
  if (doc.type == "item") {
    if (typeof doc.biblio.h1 != "undefined") {
      if (doc.biblio.h1.indexOf("\n") > -1) {
      emit([doc.last_modified, doc._id, doc.biblio.h1], doc.aliases);
      }
      if (doc.biblio.title.indexOf("\n") > -1) {
      emit([doc.last_modified, doc._id, doc.biblio.title], doc.aliases);
      }
    }
  }
}
"""
def clean_up_bad_h1():
    pass



"""
function(doc) {
    emit([doc.type, doc._id], doc);
}
"""
def delete_all_delicious_and_facebook():
    #except F1000 Yes's
    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500

    start_key = ["item", "000000000"]
    end_key = ["item", "zzzzzzzzzzzzzzzzzzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)

    number_saved = 0
    while page:
        for row in page:
            row_count += 1
            item = row.doc
            try:
                if item["metrics"]:
                    metric_names = item["metrics"].keys()
                    save_me = False
                    for metric_name in metric_names:
                        if metric_name.startswith("facebook") or metric_name.startswith("delicious"):
                            logger.info(u"{tiid} deleting {metric_name}".format(
                                metric_name=metric_name,
                                tiid=row.id))
                            del item["metrics"][metric_name]
                            save_me=True
                    if save_me:                    
                        db.save(item)
                        number_saved += 1
            except (KeyError, TypeError):
                pass
        logger.info(u"number saved {num}".format(
            num=number_saved))
        logger.info(u"%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None


if (cloudant_db == "ti"):
    print "\n\nTHIS MAY BE THE PRODUCTION DATABASE!!!"
else:
    print "\n\nThis doesn't appear to be the production database"
confirm = None
confirm = raw_input("\nType YES if you are sure you want to run this test:")
if confirm=="YES":
    ### call the function here
    delete_all_delicious_and_facebook()
else:
    print "nevermind, then."


########NEW FILE########
__FILENAME__ = couch_paginator
# -*- coding: utf-8 -*-
# vim:tabstop=4:expandtab:sw=4:softtabstop=4

# originally from https://github.com/cpinto/python-couchdb-paginator/blob/master/paginator.py
# credit to Celso Pinto, cpinto on github
# subsequently modified by total-impact

MAX_ITEMS_IN_LIST = 10

class CouchPaginator(object):
    has_next = False
    next = None

    has_previous = False
    previous = None

    page_size = 0
    start_key = None

    def __init__(self,database,view_name,page_size=None,start_key=None,end_key=None,forward=True,include_docs=False):
        """ Paginate through a set of a CouchDB's view results.

            Usage example:

            > dbconn = ... # get couch database instance
            > paginator = CouchPaginator(dbconn,"queues/by_type_and_id",10)
            > for record in paginator:
            >   #do something with record
            >   pass
            > next_page = CouchPaginator(dbconn,view,10,paginator.next)
            > first_page = CouchPaginator(dbconn,view,10,next_page.previous)

            NOTE:   The paginator only works for views that are naturaly sorted ascendingly by their key.
                    _It only supports this kind of views_.

            @param database A connection to the database in which to execute the view
            @param view_name The name of a couchdb view
            @param page_size How many records you need
            @param start_key Start navigation from this key
            @param forward Set to False if you want the previous page
        """
        if page_size is not None:
            self.page_size = page_size
        else:
            self.page_size = MAX_ITEMS_IN_LIST
        self.start_key = start_key
        self.end_key = end_key
        self.include_docs = include_docs
        self._execute_view(database,view_name,forward,include_docs)

    def _execute_view(self,database,view_name,forward,include_docs):
        # IMPORTANT: offset explanation
        #
        # if moving forward then I'll peek into an extra record to see if there is
        # one more page
        #
        # if moving backward, I need to get two extra records, one represents the document
        # with the base key, which must be ignored, another one is to check if there is
        # a previous page
        #
        offset = 1 if forward else 2
        default_descending = False
        sort_descending = not (default_descending ^ forward)

        if self.start_key is not None:
            if self.end_key is not None:
                results = database.view(view_name,startkey=self.start_key,endkey=self.end_key,limit=self.page_size+offset,descending=sort_descending,include_docs=include_docs)
            else:
                results = database.view(view_name,startkey=self.start_key,limit=self.page_size+offset,descending=sort_descending,include_docs=include_docs)
        else:
            results = database.view(view_name,limit=self.page_size+offset,descending=sort_descending,include_docs=include_docs)

        page_end = results.offset + self.page_size

        if not forward and len(results) < self.page_size:
            # this is _expensive_ and _very specific_ to handivi.com
            # basically what this does is fetch the entire first page of
            # results if we're moving back and if that page isn't complete
            # then retrieve the first page as if moving the cursor forward
            # Note to self: hope you understand this when you read it one
            # year from now
            results = database.view(view_name,limit=self.page_size+1,include_docs=include_docs)
            forward = True

        #force it to be a list so that we can navigate it like a list, including slicing

        if forward:
            self.results = list(results)
            #right, moving forward, the easy case

            if len(self.results) == self.page_size+1:
                #there is a next page so lets trim out the last key and use
                #it as the base index for that next page
                self.next = results.rows[-1].key
                self.results = self.results[:-1]
                self.has_next = True

            if results.offset > 0:
                self.has_previous = True
                #in case there is a previous page, use this current
                #page index to use as base index for the next one
                try:
                    self.previous = results.rows[0].key
                except IndexError:
                    self.next = None
                    self.results = None
                    self.has_next = False

        else:
            results.rows.reverse()
            self.results = list(results)
            #self.results.reverse()

            #I'm moving the cursor back...
            self.has_next = results.rows[-1].key == self.start_key
            if self.has_next:
                #can use the input key as base for next page
                self.next = self.start_key
                self.results = self.results[:-1]

            self.has_previous = len(self.results) == self.page_size+1
            if self.has_previous:
                #there is a previous page!
                self.previous = results.rows[1].key
                self.results = self.results[1:]

    def __str__(self):
        return str(self.results)
    
    def __len__(self):
        return len(self.results)
         
    def __iter__(self):
        return iter(self.results)

    def __getitem__(self,index):
        return self.results[index]

########NEW FILE########
__FILENAME__ = create_sqlalchemy_tables
# creates all sqlalchemy tables
# will create tables that don't exist yet and leave untouched those already there
# heroku run python extras/db_housekeeping/create_sqlalchemy_tables.py --app total-impact-core-staging

# need to start with the import because that also imports the sqlalchemy class definitions

import argparse
from totalimpact import db

def main(drop = False):
	if drop:
		print "dropping all sqlalchemy tables"
		db.drop_all()
	print "creating all sqlalchemy tables"
	db.create_all()

if __name__ == "__main__":
    # get args from the command line:
    parser = argparse.ArgumentParser(description="create all sql tables")
    parser.add_argument('--drop', 
    	default=False,
    	action='store_true', 
    	help="drop tables before creating them")
    args = vars(parser.parse_args())
    print args
    print "create_sqlalchemy_tables.py starting."
    main(args["drop"])



########NEW FILE########
__FILENAME__ = dedup_collection_dois
import couchdb, os, logging, sys, collections
from pprint import pprint
import time
import requests
import copy

# run in heroku by a) commiting, b) pushing to heroku, and c) running
# heroku run python extras/db_housekeeping/dedup_collection_dois.py

logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format='[%(process)d] %(levelname)8s %(threadName)30s %(name)s - %(message)s'
)

logger = logging.getLogger("dedup_collection_dois")
 
cloudant_db = os.getenv("CLOUDANT_DB")
cloudant_url = os.getenv("CLOUDANT_URL")

couch = couchdb.Server(url=cloudant_url)
db = couch[cloudant_db]
logger.info("connected to couch at " + cloudant_url + " / " + cloudant_db)

def dedup_collections(collection_id):
    url = "http://{api_root}/v1/collection/{collection_id}?key=samplekey".format(
        api_root=os.getenv("API_ROOT"), collection_id=collection_id)
    print url
    try:
        r = requests.get(url, timeout=120)
    except requests.Timeout:
        print "timeout"
        return []
    try:
        collection = r.json
    except TypeError:
        print "TypeError" #unicode() argument 2 must be string, not None
        return

    if not collection:
        return
    items = collection["items"]

    unique_tiids = set()
    unique_dois = set()
    tiids_to_delete = []

    for item in items:
        try:
            dois = item["aliases"]["doi"]
            for doi in dois:
                if doi not in unique_dois:
                    unique_dois.add(doi)
                    unique_tiids.add(item["_id"])
        except KeyError:
            pass #no dois

    for item in items:
        if "doi" in item["aliases"]:
            if item["_id"] not in unique_tiids:
                tiids_to_delete += [item["_id"]]

    if tiids_to_delete:
        delete_tiids_from_collection(collection_id, tiids_to_delete)

def delete_tiids_from_collection(collection_id, tiids_to_delete):
    doc = db.get(collection_id)
    print "tiids_to_delete", tiids_to_delete
    for (alias, tiid) in doc["alias_tiids"].items():
        if tiid in tiids_to_delete:
            del doc["alias_tiids"][alias]
    db.save(doc)
    print "saved deduped collection", collection_id

def dedup_merged_collections():
    from totalimpact import item, tiredis

    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500
    start_key = ["collection", "000"]
    end_key = ["collection", "00zz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)

    number_of_collections = {}
    size_of_collections = {}
    size_of_profile_collections = {}

    while page:
        for row in page:
            dedup_collections(row.id)
            #doc = row.doc

            row_count += 1
        logger.info("%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None

if (cloudant_db == "ti"):
    print "\n\nTHIS MAY BE THE PRODUCTION DATABASE!!!"
else:
    print "\n\nThis doesn't appear to be the production database"
confirm = None
confirm = raw_input("\nType YES if you are sure you want to run this test:")
if confirm=="YES":
    ### call the function here
    dedup_merged_collections()
else:
    print "nevermind, then."


########NEW FILE########
__FILENAME__ = merge_collections
import couchdb, os, logging, sys, collections
from pprint import pprint
import time
import requests
import copy

# run in heroku by a) commiting, b) pushing to heroku, and c) running
# heroku run python extras/db_housekeeping/merge_collections.py

logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format='[%(process)d] %(levelname)8s %(threadName)30s %(name)s - %(message)s'
)

logger = logging.getLogger("merge_collections")
 
cloudant_db = os.getenv("CLOUDANT_DB")
cloudant_url = os.getenv("CLOUDANT_URL")

couch = couchdb.Server(url=cloudant_url)
db = couch[cloudant_db]
logger.info("connected to couch at " + cloudant_url + " / " + cloudant_db)

def make_similar_collection(base_doc, alias_tiid_tuples):
    collection_doc = {}
    for key in ["type", "owner", "last_modified", "ip_address", "key_hash", "created"]:
        try:
            collection_doc[key] = base_doc[key]
        except KeyError:
            pass  #some collections don't have key_hash it looks like

    collection_doc["_id"] = "00" + base_doc["_id"][2:]
    collection_doc["title"] = "all my products"
    collection_doc["alias_tiids"] = {} 
    for (alias, tiid) in alias_tiid_tuples:
        collection_doc["alias_tiids"][alias] = tiid
    return collection_doc

def merge_collections_for_profile():
    from totalimpact import item, tiredis

    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    page_size = 500
    start_key = ["user", "00000000000"]
    end_key = ["user", "zzzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)

    while page:
        for row in page:
            row_count += 1
            user_doc = row.doc

            if "profile_collection" in user_doc:
                #already updated
                if not user_doc["colls"]:
                    user_doc["profile_collection"] = None
                    print "updating profile_collection with None because no collections", row.id
                    db.save(user_doc)
                continue 

            alias_tiid_tuples = []

            print "\nstill needs a profile_collection:", row.id, 
            print user_doc

            try:
                my_collections = user_doc["colls"]
                for coll in my_collections:
                    collection_doc = db.get(coll)
                    alias_tiids = collection_doc["alias_tiids"]
                    alias_tiid_tuples += alias_tiids.items()

                profile_collection = None    
                if (len(my_collections) == 1):
                    profile_collection = collection_doc["_id"]
                    print "only one collection so merged collection not needed"
                elif (len(my_collections) > 1):
                    merged_collection = make_similar_collection(collection_doc, alias_tiid_tuples)
                    
                    #save new collection
                    del collection_doc["_rev"]
                    try:
                        db.save(merged_collection)
                        print "saved merged collection", merged_collection["_id"]
                    except couchdb.http.ResourceConflict:
                        print "didn't save new merged collection because of document conflict... maybe already saved"

                    profile_collection = merged_collection["_id"]

                print profile_collection
                user_doc["profile_collection"] = profile_collection
                db.save(user_doc)
                print "saved user_doc with updated profile collection"
            except KeyError:
                raise
        logger.info("%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None


if (cloudant_db == "ti"):
    print "\n\nTHIS MAY BE THE PRODUCTION DATABASE!!!"
else:
    print "\n\nThis doesn't appear to be the production database"
confirm = None
confirm = raw_input("\nType YES if you are sure you want to run this test:")
if confirm=="YES":
    ### call the function here
    merge_collections_for_profile()
else:
    print "nevermind, then."


########NEW FILE########
__FILENAME__ = migrate_user_docs
import couchdb, os, logging, sys, collections
from pprint import pprint
import time
import requests
import copy
import random
import datetime
from werkzeug.security import generate_password_hash

from totalimpact import dao
import psycopg2

# run in heroku by a) commiting, b) pushing to heroku, and c) running
# heroku run python extras/db_housekeeping/migrate_user_docs.py

logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format='[%(process)d] %(levelname)8s %(threadName)30s %(name)s - %(message)s'
)

logger = logging.getLogger("merge_collections")
 
# set up couchdb
cloudant_db = os.getenv("CLOUDANT_DB")
cloudant_url = os.getenv("CLOUDANT_URL")
couch = couchdb.Server(url=cloudant_url)
db = couch[cloudant_db]
logger.info("connected to couch at " + cloudant_url + " / " + cloudant_db)

# set up postgres
mypostgresdao = dao.PostgresDao(os.environ["POSTGRESQL_URL"])
cur = mypostgresdao.get_cursor()
logger.info("connected to postgres")


default_password = "welcome"
default_password_hash = generate_password_hash(default_password)

def insert_unless_error(select_statement, save_list):
    #print "try to insert"
    #cur.executemany(select_statement, save_list)
    for l in save_list:
        print cur.mogrify(select_statement, l)
        try:
            #cur.execute(select_statement, l)
            pass
        except psycopg2.IntegrityError:
            print "insert already exists"

def insert_string(tablename, colnames):
    colnames_string = ", ".join(colnames)
    percent_colnames_string = ", ".join(["%("+col+")s" for col in colnames])
    insert = "INSERT INTO {tablename} ({colnames_string}) VALUES ({percent_colnames_string});\t".format(
        tablename=tablename, 
        colnames_string=colnames_string, 
        percent_colnames_string=percent_colnames_string)
    return insert


def merge_collections_for_profile():
    from totalimpact import item, tiredis

    view_name = "queues/by_type_and_id"
    view_rows = db.view(view_name, include_docs=True)
    row_count = 0
    sql_statement_count = 0
    page_size = 500
    start_key = ["user", "00000000000"]
    end_key = ["user", "zzzzzzzzz"]

    from couch_paginator import CouchPaginator
    page = CouchPaginator(db, view_name, page_size, include_docs=True, start_key=start_key, end_key=end_key)

    email_data_strings = []

    while page:
        for row in page:

            row_count += 1
            user_doc = row.doc

            rowdata = {}
            rowdata["email"] = user_doc["_id"]
            if not user_doc["profile_collection"]:
                #print "not migrating this doc because it has no collections"
                continue
            rowdata["collection_id"] = user_doc["profile_collection"]
            try:
                rowdata["created"] = user_doc["created"]
            except KeyError:
                rowdata["created"] = datetime.datetime(2013, 1, 1).isoformat()                
            rowdata["password_hash"] = default_password_hash
            rowdata["url_slug"] = "user" + str(50000 + row_count)
            rowdata["given_name"] = "Firstname"
            rowdata["surname"] = "Lastname"

            insert_unless_error(insert_string('"user"', rowdata.keys()), [rowdata])
            sql_statement_count += 1

            # pull information together to send out surveymonkey email
            profile_id = user_doc["profile_collection"]
            email = user_doc["_id"]
            profile_doc = db.get(profile_id)
            my_collections = user_doc["colls"]

            title = profile_doc["title"]
            if (len(my_collections) > 1):
                title = ""
                for cid in my_collections:
                    coll_doc = db.get(cid)
                    collection_title = coll_doc["title"]
                    if collection_title != "My Collection":
                        title += "*" + collection_title

            try:
                collections_string = str(";".join(my_collections.keys()))
            except UnicodeEncodeError:
                print "UnicodeEncodeError on ", email, "so setting collections to blank"
                collections_string = ""

            email_data_strings += [u"{url_slug}|{profile_id}|{len_profile}|{email}|{created}|{title}|{collections_string}".format(
                url_slug=rowdata["url_slug"],
                profile_id=profile_id,
                email=email,
                len_profile=len(profile_doc["alias_tiids"]),
                created=rowdata["created"],
                title=title,
                collections_string=collections_string)]

        logger.info("%i. getting new page, last id was %s" %(row_count, row.id))
        if page.has_next:
            page = CouchPaginator(db, view_name, page_size, start_key=page.next, end_key=end_key, include_docs=True)
        else:
            page = None

    print "Number of rows: ", row_count
    print "Number of sql statements: ", sql_statement_count

    print "\n\n\n"
    for line in email_data_strings:
        print line

if (cloudant_db == "ti"):
    print "\n\nTHIS MAY BE THE PRODUCTION DATABASE!!!"
else:
    print "\n\nThis doesn't appear to be the production database"

# remove check so can pipe output to a file: this script does not change the db so it is safe!
# confirm = None
# confirm = raw_input("\nType YES if you are sure you want to run this test:")
##if confirm=="YES":
    ### call the function here
merge_collections_for_profile()
#else:
#    print "nevermind, then."


########NEW FILE########
__FILENAME__ = postgres_mirror
import couchdb, os, logging, sys, collections
from pprint import pprint
import time, datetime, json
import requests

from couch_paginator import CouchPaginator
from totalimpact import dao
import psycopg2

# run in heroku by a) commiting, b) pushing to heroku, and c) running
# heroku run python extras/db_housekeeping/postgres_mirror.py

logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format='[%(process)d] %(levelname)8s %(threadName)30s %(name)s - %(message)s'
)

logger = logging.getLogger("postgres_mirror")
 


def action_on_a_page_single_doc(page):
    docs = [row.doc for row in page]
    for doc in docs:
        doc["tiid"] = doc["_id"]
        try:
            doc["last_update_run"]
        except KeyError:
            doc["last_update_run"] = None        

        print "try"
        try:
            print doc["tiid"]
            cur.execute("""INSERT INTO items(tiid, created, last_modified, last_update_run) 
                            VALUES (%(tiid)s, %(created)s, %(last_modified)s, %(last_update_run)s)""", doc)
            #conn.commit()
        except psycopg2.IntegrityError:
            print "row already exists"
            mypostgresdao.conn.rollback()
        except:
            mypostgresdao.conn.rollback()
        finally:
            pass

def build_items_save_list(items):
    items_save_list = []
    for item in items:
        item["tiid"] = item["_id"]
        try:
            item["last_update_run"]
        except KeyError:
            item["last_update_run"] = None 
        items_save_list += [item]
    return items_save_list  

def build_metrics_save_list(items):
    metrics_save_list = []
    for item in items:
        if "metrics" in item:
            for full_metric_name in item["metrics"]:
                for timestamp in item["metrics"][full_metric_name]["values"]["raw_history"]:
                    (provider, bare_metric_name) = full_metric_name.split(":")
                    metrics_save_list += [{"tiid":item["_id"],
                                "provider":provider,
                                "metric_name":bare_metric_name,
                                "collected_date":timestamp,
                                "drilldown_url":item["metrics"][full_metric_name]["provenance_url"],
                                "raw_value":item["metrics"][full_metric_name]["values"]["raw_history"][timestamp]
                                }]
    return metrics_save_list

def build_aliases_save_list(items):
    aliases_save_list = []
    for item in items:
        if "aliases" in item:
            for namespace in item["aliases"]:
                for nid in item["aliases"][namespace]:
                    aliases_save_list += [{"tiid":item["_id"],
                                "provider":"unknown",
                                "namespace":namespace,
                                "nid":nid,
                                "collected_date":now
                                }]
    return aliases_save_list

class NoneDict(dict):
    # returns None if key not defined instead of throwing KeyError
    def __getitem__(self, key):
        return dict.get(self, key)

def build_biblio_save_list(items):
    biblio_save_list = []
    for item in items:
        if "biblio" in item:
            biblio_save = NoneDict()
            biblio_save.update(item["biblio"])

            biblio_save["tiid"] = item["_id"]
            biblio_save["collected_date"] = now

            biblio_save["authors_lnames"] = None

            if "owner" in biblio_save:
                biblio_save["provider"] = "github"
                biblio_save["host"] = "github"
            else:
                biblio_save["provider"] = "unknown"

            if "year" in biblio_save:
                biblio_save["year_published"] = int(biblio_save["year"])
            if "owner" in biblio_save:
                biblio_save["authors_raw"] = biblio_save["owner"]
            if "create_date" in biblio_save:
                biblio_save["date_published"] = biblio_save["create_date"]
            if "journal" in biblio_save:
                biblio_save["host"] = biblio_save["journal"]

            biblio_save_list += [biblio_save]

    return biblio_save_list    

"""
CREATE TABLE items (
    tiid text NOT NULL,
    created timestamptz,
    last_modified timestamptz,
    last_update_run timestamptz,
    PRIMARY KEY (tiid)
);

CREATE TABLE metrics (
    tiid text NOT NULL,
    provider text NOT NULL,
    metric_name text NOT NULL,
    collected_date timestamptz NOT NULL,
    drilldown_url text,
    raw_value text,
    PRIMARY KEY (tiid,provider,metric_name,collected_date)
);

CREATE TABLE aliases (
    tiid text NOT NULL,
    "namespace" text NOT NULL,
    nid text NOT NULL,
    last_modified timestamptz,
    PRIMARY KEY (tiid, "namespace", nid))

CREATE TABLE biblio (
    tiid text NOT NULL,
    provider text NOT NULL,
    last_modified timestamptz,
    title text,
    year_published numeric(25),
    date_published timestamptz,
    authors_lnames text,
    authors_raw text,
    "host" text,
    url text,
    description text,
    PRIMARY KEY (tiid, provider))

CREATE TABLE email (
    id text NOT NULL,
    created timestamptz,
    payload text NOT NULL,
    PRIMARY KEY (id))

"""

def insert_unless_error(select_statement, save_list):
    print "try to insert"
    #cur.executemany(select_statement, save_list)
    for l in save_list:
        print cur.mogrify(select_statement, l)
        try:
            #cur.execute(select_statement, l)
            pass
        except psycopg2.IntegrityError:
            print "insert already exists"

def item_action_on_a_page(page):
    items = [row.doc for row in page]

    print "ITEMS"
    print datetime.datetime.now().isoformat()
    items_save_list = build_items_save_list(items)
    print datetime.datetime.now().isoformat()
    insert_unless_error("""INSERT INTO items(tiid, created, last_modified, last_update_run) 
                        VALUES (%(tiid)s, %(created)s, %(last_modified)s, %(last_update_run)s);""", 
                        items_save_list)

    print "BIBLIO"
    print datetime.datetime.now().isoformat()
    biblio_save_list = build_biblio_save_list(items)
    print datetime.datetime.now().isoformat()
    insert_unless_error("""INSERT INTO biblio(tiid, provider, collected_date, title, year_published, date_published, authors_lnames, authors_raw, host, url, description) 
                        VALUES (%(tiid)s, %(provider)s, %(collected_date)s, %(title)s, %(year_published)s, %(date_published)s, %(authors_lnames)s, %(authors_raw)s, %(host)s, %(url)s, %(description)s)""", 
                        biblio_save_list)

    print "ALIASES"
    print datetime.datetime.now().isoformat()
    aliases_save_list = build_aliases_save_list(items)
    print datetime.datetime.now().isoformat()
    insert_unless_error("""INSERT INTO aliases(tiid, namespace, nid, collected_date) 
                        VALUES (%(tiid)s, %(namespace)s, %(nid)s, %(collected_date)s)""", 
                        aliases_save_list)

    print "METRICS"
    print datetime.datetime.now().isoformat()
    metrics_save_list = build_metrics_save_list(items)
    print datetime.datetime.now().isoformat()
    insert_unless_error("""INSERT INTO metrics(tiid, provider, metric_name, collected_date, drilldown_url, raw_value) 
                        VALUES (%(tiid)s, %(provider)s, %(metric_name)s, %(collected_date)s, %(drilldown_url)s, %(raw_value)s)""", 
                        metrics_save_list)


    print "done"

def build_email_save_list(emails):
    email_save_list = []
    for email in emails:
        print email["_id"]
        email_save = {}
        email_save["id"] = email["_id"]
        email_save["created"] = email["created"]
        email_save["payload"] = json.dumps(email["payload"])
        email_save_list += [email_save]
    return email_save_list  

def email_action_on_a_page(page):
    emails = [row.doc for row in page]
    print "EMAILS"
    print datetime.datetime.now().isoformat()
    emails_save_list = build_email_save_list(emails)
    print datetime.datetime.now().isoformat()
    insert_unless_error("""INSERT INTO email(id, created, payload) 
                    VALUES (%(id)s, %(created)s, %(payload)s);""", 
                    emails_save_list)
    print datetime.datetime.now().isoformat()
    print "done"


def build_api_users_save_list(docs):
    api_users_save_list = []
    registered_items_save_list = []
    for doc in docs:
        print doc["_id"]
        api_users_save = {}
        api_users_save["api_key"] = doc["current_key"]
        api_users_save["max_registered_items"] = doc["max_registered_items"]
        api_users_save["created"] = doc["created"]
        for key in doc["meta"]:
            api_users_save[key] = doc["meta"][key]
        api_users_save_list += [api_users_save]
        for alias in doc["registered_items"]:
            registered_items_save_list += [{
                "api_key":api_users_save["api_key"], 
                "registered_date":doc["registered_items"][alias]["registered_date"],
                "alias":alias}]    
    return (api_users_save_list, registered_items_save_list)  

def insert_string(tablename, colnames):
    colnames_string = ", ".join(colnames)
    percent_colnames_string = ", ".join(["%("+col+")s" for col in colnames])
    insert = "INSERT INTO {tablename} ({colnames_string}) VALUES ({percent_colnames_string});\t".format(
        tablename=tablename, 
        colnames_string=colnames_string, 
        percent_colnames_string=percent_colnames_string)
    return insert

def api_users_action_on_a_page(page):
    docs = [row.doc for row in page]
    print "API USERS"
    print datetime.datetime.now().isoformat()
    (api_users_save_list, registerted_items_save_list) = build_api_users_save_list(docs)
    print datetime.datetime.now().isoformat()

    colnames = "api_key, max_registered_items, created, planned_use, example_url, api_key_owner, notes, email, organization".split(", ")
    insert_unless_error(insert_string("api_users", colnames), api_users_save_list)

    colnames = "api_key, alias, registered_date".split(", ")
    insert_unless_error(insert_string("registered_items", colnames), registerted_items_save_list)

    print datetime.datetime.now().isoformat()
    print "done"


def run_on_documents(func_page, view_name, start_key, end_key, row_count=0, page_size=500):
    couch_page = CouchPaginator(db, view_name, page_size, start_key=start_key, end_key=end_key, include_docs=True)

    while couch_page:
        func_page(couch_page)
        row_count += page_size

        logger.info("%i. getting new page" %(row_count))
        if couch_page.has_next:
            couch_page = CouchPaginator(db, view_name, page_size, start_key=couch_page.next, end_key=end_key, include_docs=True)
        else:
            couch_page = None

    print "number items = ", row_count


#run


# set up postgres
mypostgresdao = dao.PostgresDao(os.environ["POSTGRESQL_URL"])
cur = mypostgresdao.get_cursor()
logger.info("connected to postgres")

# set up couchdb
cloudant_db = os.getenv("CLOUDANT_DB")
cloudant_url = os.getenv("CLOUDANT_URL")
couch = couchdb.Server(url=cloudant_url)
db = couch[cloudant_db]
logger.info("connected to couch at " + cloudant_url + " / " + cloudant_db)

# do a few preventative checks
if (cloudant_db == "ti"):
    print "\n\nTHIS MAY BE THE PRODUCTION DATABASE!!!"
else:
    print "\n\nThis doesn't appear to be the production database\n\n"
confirm = None
#confirm = raw_input("\nType YES if you are sure you want to run this test:")
confirm = "YES"
if not confirm=="YES":
    print "nevermind, then."
    exit()

# set up the action code
#myview_name = "queues/by_alias"
#mystart_key = ["url", "https://github.0000000"]
#myend_key = ["url", "https://github.zzzzzzzz"]

myview_name = "by_type/by_type"
mystart_key = ["api_user"]
myend_key = ["api_user"]

now = datetime.datetime.now().isoformat()

run_on_documents(api_users_action_on_a_page, 
    view_name=myview_name, 
    start_key=mystart_key, 
    end_key=myend_key, 
    page_size=500)

cur.close()
mypostgresdao.close()

#    try:
#        cur.execute("CREATE TABLE phonebook(phone VARCHAR(32), firstname VARCHAR(32), lastname VARCHAR(32), address VARCHAR(64));")
#    except psycopg2.ProgrammingError:
#        print "table already exists"

#    cur.execute("SELECT * FROM phonebook ORDER BY lastname;")
#    print cur.fetchone()    


########NEW FILE########
__FILENAME__ = postgres_sqlalchemy_move
import couchdb, os, logging, sys, collections
from pprint import pprint
import time, datetime, json
import requests
import argparse
import string
import threading

from couch_paginator import CouchPaginator
from totalimpact import dao
import psycopg2
from sqlalchemy.exc import OperationalError

from totalimpact import db, app
from totalimpact import collection
from totalimpact.collection import Collection, CollectionTiid, AddedItem
from totalimpact import item as item_module
from totalimpact.item import Item, Alias, Metric, Biblio, create_biblio_objects

# run in heroku by a) commiting, b) pushing to heroku, and c) running
# heroku run python extras/db_housekeeping/postgres_sqlalchemy_move.py --collections

logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format='[%(process)d] %(levelname)8s %(threadName)30s %(name)s - %(message)s'
)

logger = logging.getLogger("postgres_sqlalchemy_move")
 
# print out extra debugging
#logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)

logging.getLogger('ti.collection').setLevel(logging.INFO)
logging.getLogger('ti.item').setLevel(logging.INFO)

def item_missing_biblios_on_a_page(page, skip_till_key="0000"):
    items = [row.doc for row in page]

    for item_doc in items:
        tiid = item_doc["_id"]
        if tiid > skip_till_key:
            if not "biblio" in item_doc["aliases"]:
                print tiid, "doesn't have biblio in aliases"
                if "title" in item_doc["biblio"]:
                    print "... but does have a title in biblio.  Mirroring."
                    item_obj = Item.query.filter_by(tiid=tiid).first()
                    db.session.merge(item_obj)
                    item_obj.biblios = create_biblio_objects([item_doc["biblio"]])
    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in add_biblio_to_item_object for {tiid}, rolling back.  Message: {message}".format(
            tiid=tiid, 
            message=e.message)) 

    print "just finished", tiid
    return


def item_action_on_a_page(page, skip_till_key="0000"):
    items = [row.doc for row in page]

    for item_doc in items:
        if item_doc["_id"] > skip_till_key:
            new_item_object = item_module.create_objects_from_item_doc(item_doc, skip_if_exists=True)
    print "just finished", item_doc["_id"]
    return


def collection_action_on_a_page(page, skip_till_key="0000"):
    collections = [row.doc for row in page]

    for coll_doc in collections:
        if coll_doc["_id"] > skip_till_key:        
         new_coll_object = collection.create_objects_from_collection_doc(coll_doc)
    print "just finished", coll_doc["_id"]
    return


def run_on_documents(func_page, view_name, start_key, end_key, skip_till_key, row_count=0, page_size=500):
    couch_page = CouchPaginator(couch_db, view_name, page_size, start_key=start_key, end_key=end_key, include_docs=True)
    start_time = datetime.datetime.now()

    print "starting to loop through first {page_size} from {start_key} to {end_key}".format(
        page_size=page_size, 
        start_key=start_key, 
        end_key=end_key)

    while couch_page:
        func_page(couch_page, skip_till_key)
        row_count += page_size

        logger.info("%i. getting new page" %(row_count))
        elapsed_time = datetime.datetime.now() - start_time
        number_db_threads = max(1, threading.active_count() - 1)
        print "\n****** {timestamp} {start_key} took {elapsed_seconds} seconds to do {row_count} docs, so {minutes_per_10k} minutes per 10k docs per thread, {total}mins total *****".format(
            timestamp=datetime.datetime.now().isoformat(),
            start_key=start_key,
            row_count=row_count, 
            elapsed_seconds=elapsed_time.seconds, 
            minutes_per_10k=(elapsed_time.seconds)*10000/(row_count*60),
            total=((elapsed_time.seconds)*10000/(row_count*60))/number_db_threads
            )

        if couch_page.has_next:
            couch_page = CouchPaginator(couch_db, view_name, page_size, start_key=couch_page.next, end_key=end_key, include_docs=True)
        else:
            couch_page = None

    print "number items = ", row_count
    elapsed_time = datetime.datetime.now() - start_time

    print "took {elapsed_time} to do {row_count}".format(
        row_count=row_count, 
        elapsed_time=elapsed_time)

    db.session.remove()


def run_through_pages(doc_type, doc_func, skip_till_key="00000", page_size=100, number_of_threads=1):
    if number_of_threads == 1:
        run_on_documents(doc_func, 
            "temp/by_type_and_id", 
            [doc_type, skip_till_key], 
            [doc_type, "zzzzz"], 
            skip_till_key,
            0,
            page_size)
    else:    
        starts = string.digits + string.ascii_lowercase
        starts = starts[int(skip_till_key):]
        step_size = int(len(starts) / number_of_threads)
        boundaries = starts[0::step_size] + 'z'
        key_pages = zip(boundaries[:-1], boundaries[1:])
        for (startkey, endkey) in key_pages:
            mystart_key = [doc_type, startkey*5] # repeats it 5 times
            myend_key = [doc_type, endkey*5]

            print "launching loop through first {page_size} from {start_key} to {end_key}".format(
                page_size=page_size, 
                start_key=mystart_key, 
                end_key=myend_key)

            t = threading.Thread(target=run_on_documents, 
                args=(doc_func, 
                    "temp/by_type_and_id", 
                    mystart_key, 
                    myend_key, 
                    skip_till_key,
                    0,
                    page_size),
                name="{mystart_key} to {myend_key}".format(
                    mystart_key=mystart_key, 
                    myend_key=myend_key)
                )
            t.start()


def setup_postgres(drop_all=False):

    # set up postgres
    logger.info("connecting to postgres")

    #export POSTGRESQL_URL=postgres://localhost/core_migration
    #export POSTGRESQL_URL=postgres://localhost/corecopy

    if "def95794hs4ou7" in app.config["SQLALCHEMY_DATABASE_URI"]:
        print "\n\nTHIS MAY BE THE PRODUCTION DATABASE!!!"
        confirm = raw_input("\nType YES if you are sure you want to run this test:")
        if not confirm=="YES":
            print "nevermind, then."
            exit()

    if drop_all:
        print "the postgres database is ", app.config["SQLALCHEMY_DATABASE_URI"]
        confirm = raw_input("\nType YES if you are sure you want to drop tables:")
        if not confirm=="YES":
            print "nevermind, then."
            exit()
        try:
            db.drop_all()
        except OperationalError, e:  #database "database" does not exist
            print e
    db.create_all()
    logger.info("connected to postgres {uri}".format(
        uri=app.config["SQLALCHEMY_DATABASE_URI"]))


def setup_couch():
    # set up couchdb
    cloudant_db = os.getenv("CLOUDANT_DB")
    cloudant_url = os.getenv("CLOUDANT_URL")

    # do a few preventative checks
    if (cloudant_db == "ti"):
        print "\n\nTHIS MAY BE THE PRODUCTION DATABASE!!!"
        confirm = raw_input("\nType YES if you are sure you want to run this test:")
        if not confirm=="YES":
            print "nevermind, then."
            exit()
    else:
        print "\n\nThis doesn't appear to be the production database\n\n"

    couch = couchdb.Server(url=cloudant_url)
    couch_db = couch[cloudant_db]
    print couch_db
    logger.info("connected to couch at " + cloudant_url + " / " + cloudant_db)

    return couch_db



if __name__ == "__main__":
    # get args from the command line:
    parser = argparse.ArgumentParser(description="copy data from couch into postgres")
    parser.add_argument('--drop', 
        default=False,
        action='store_true', 
        help="drop tables before creating them")
    parser.add_argument('--collections', 
        default=False,
        action='store_true', 
        help="iterate over collections, copying collections and their related info (collection tiids, added_items)")
    parser.add_argument('--items', 
        default=False,
        action='store_true', 
        help="iterate over items, copying items and their related info (biblio, metrics, aliases)")
    parser.add_argument('--pagesize', 
        default=100,
        type=int,
        help="number of documents to get from couch in each batch")
    parser.add_argument('--threads', 
        default=1,
        type=int,
        help="number of db threads")
    parser.add_argument('--itemsskiptillkey', 
        default="000000",
        type=str,
        help="id to start the item view")
    parser.add_argument('--collectionsskiptillkey', 
        default="000000",
        type=str,
        help="id to start the collection view")
    args = vars(parser.parse_args())
    print args
    print "postgres_sqlalchemy_move.py starting."

    setup_postgres(drop_all=args["drop"])
    couch_db = setup_couch()
    if args["collections"]:
        run_through_pages("collection", collection_action_on_a_page, args["collectionsskiptillkey"], args["pagesize"], args["threads"])
    if args["items"]:
        #run_through_pages("item", item_action_on_a_page, args["itemsskiptillkey"], args["pagesize"], args["threads"])
        run_through_pages("item", item_missing_biblios_on_a_page, args["itemsskiptillkey"], args["pagesize"], args["threads"])

    db.session.close_all()

  


########NEW FILE########
__FILENAME__ = register_posthoc
import re, os
from collections import defaultdict

from totalimpact import item
from totalimpact import dao
from totalimpact import api_user

run_post_file = False

if run_post_file:
    # zgrep "key=" 2012-*.gz | grep method=POST | grep -v "key=VANWIJIKc233acaa" | grep -v "key=EXAMPLE" | grep -v "key=Heather" | grep -v embed | grep -v GREEK_YOGURT | grep -v "key=KEY" | grep -v YOURKEY > registers_by_post.txt
    #2012-11-20.gz:207432773781471232   2012-11-20T06:30:49Z    2012-11-20T06:30:49Z    1075101 ti-core 67.202.23.119   User    Notice  heroku/router   at=info method=POST path=/v1/doi/10.1039/b704980c?key=87e02f1090c3438db9ac5b48d3dc3c1c host=api.impactstory.org fwd= dyno=web.1 queue=0 wait=0ms connect=2ms service=5ms status=404 bytes=238
    filename = "/Users/hpiwowar/Dropbox/ti/papertrail archives/registers_by_post.txt"
    contents = open(filename, "r").read()
    register_pattern = re.compile("2012.*\t(?P<timestamp>2012-.*)Z\t.*/v1/item/(?P<namespace>[a-z]*?)/(?P<id>.*?)\?key=(?P<apikey>.*?) host")
    all_registrations = register_pattern.findall(contents)
else:
    #zgrep "key=" 2012-*.gz | grep method=GET | grep register | grep -v "key=VANWIJIKc233acaa" | grep -v "key=EXAMPLE" | grep -v "key=Heather" | grep -v embed | grep -v GREEK_YOGURT | grep -v "key=KEY" | grep -v YOURKEY > registers_by_get.txt
    # 2012-12-16.gz:216792413250023424  2012-12-16T02:22:41Z    2012-12-16T02:22:41Z    1075101 ti-core 107.21.167.86   User    Notice  heroku/router   at=info method=GET path=/v1/item/doi/10.3897/zookeys.57.477?key=pensoft-127b7fd8&register=true&_=1355624578069 host=total-impact-core.herokuapp.com fwd=201.141.18.134 dyno=web.1 queue=0 wait=0ms connect=2ms service=170ms status=200 bytes=5953
    filename = "/Users/hpiwowar/Dropbox/ti/papertrail archives/registers_by_get.txt"
    contents = open(filename, "r").read()
    register_pattern = re.compile("2012.*\t(?P<timestamp>2012-.*)Z\t.*/v1/item/(?P<namespace>[a-z]*?)/(?P<id>.*?)\?key=(?P<apikey>.*?)&")
    all_registrations = register_pattern.findall(contents)


mydao = dao.Dao(os.environ["CLOUDANT_URL"], os.environ["CLOUDANT_DB"])

registration_dict = defaultdict(dict)
for registration in all_registrations:
    (timestamp, namespace, nid, api_key) = registration

    if api_key in ["test", "api-docs", "ekjpt55agtzy10441yv7nh302"]:
        continue
    if len(api_key) > 40:
        continue

    alias = (namespace, nid)
    registration_dict[api_key][alias] = {"registered":timestamp}

for api_key in registration_dict.keys():
    for alias in registration_dict[api_key].keys():
        (namespace, nid) = alias
        tiid = item.get_tiid_by_alias(namespace, nid, None, mydao)
        if not tiid:
            print "****************** no tiid, skipping*****************"
            raw_input("hit enter to continue")
            continue

        api_user.register_item(alias, tiid, api_key, mydao)


for reg in registration_dict.keys():
    print reg
    print len(registration_dict[reg])

print "\n\n"

for reg in registration_dict.keys():
    print reg
    for key in registration_dict[reg].keys():
        print key

########NEW FILE########
__FILENAME__ = twitter_auth
from birdy.twitter import AppClient
from requests.auth import HTTPBasicAuth
from requests_oauthlib import OAuth1Session, OAuth2Session
from oauthlib.oauth2 import BackendApplicationClient
import requests
import json

import os

TWITTER_API_VERSION = '1.1'
TWITTER_BASE_API_URL = 'https://api.twitter.com'

api_version=TWITTER_API_VERSION
base_api_url=TWITTER_BASE_API_URL

consumer_key=os.getenv("TWITTER_CONSUMER_KEY")
print consumer_key
consumer_secret = os.getenv("TWITTER_CONSUMER_SECRET")
print consumer_secret
access_token = os.getenv("TWITTER_ACCESS_TOKEN")
print access_token

request_token_url = '%s/oauth2/token' % base_api_url

auth = HTTPBasicAuth(consumer_key, consumer_secret)

client = BackendApplicationClient(consumer_key)

token = {'access_token': access_token, 'token_type': "bearer" }

session = OAuth2Session(client=client, token=token)

print session.verify
print session.token

a = session.get("https://api.twitter.com/1.1/users/show.json?screen_name=researchremix")

print a
print a.text


# data = {'grant_type': 'client_credentials'}

# response = session.post(request_token_url, auth=auth, data=data)
# print response

# data = json.loads(response.content.decode('utf-8'))
# print data
# access_token = data['access_token']


# temp_client = AppClient(os.getenv("TWITTER_CONSUMER_KEY"), 
#                     os.getenv("TWITTER_CONSUMER_SECRET"))

# access_token = temp_client.get_access_token()

# print "generated access token", access_token, os.getenv("TWITTER_ACCESS_TOKEN")

# client = AppClient(os.getenv("TWITTER_CONSUMER_KEY"), 
#                     os.getenv("TWITTER_CONSUMER_SECRET"),
#                     os.getenv("TWITTER_ACCESS_TOKEN"))

########NEW FILE########
__FILENAME__ = alt_functional_test
#!/usr/bin/env python
#
# A basic functional test of the total impact API
#

import mechanize
import urllib2
import urllib
import json
import time
import sys
import pickle
import urlparse
from pprint import pprint
from optparse import OptionParser


REQUEST_IDS = [
                ("crossref", ('doi', '10.1371/journal.pcbi.1000361')), 
                ("delicious", ('url', 'http://total-impact.org/')),
                ("dryad", ('doi','10.5061/dryad.18')), 
                ("github", ('github', 'egonw,cdk')),
                ("mendeley", ('doi', '10.1371/journal.pcbi.1000361')), 
                ("topsy", ('url', 'http://total-impact.org')),
                ("webpage", ('url', 'http://nescent.org/')),
                ("wikipedia", ('doi', '10.1371/journal.pcbi.1000361')) 
]

GOLD_RESPONSES = {
    'crossref' : { 
        'aliases': ['doi', "title", "url"],
        'biblio': [u'authors', u'journal', u'year', u'title'],
        'metrics' : {}
    },
    'delicious' : { 
        'aliases': ["url"],
        'biblio': [],
        'metrics' : {
            'delicious:bookmarks' : 65
        }
    },
    'dryad' : { 
        'aliases': ['doi', 'url', 'title'],
        'biblio': [u'authors', u'year', u'repository', u'title'],
        'metrics' : {
            'dryad:most_downloaded_file' : 63,
            'dryad:package_views' : 149,
            'dryad:total_downloads' : 169
        }
    },    
    'github' : { 
        'aliases': ['github', 'url', 'title'],
        'biblio': [u'last_push_date', u'create_date', u'description', u'title', u'url', u'owner'],
        'metrics' : {
            'github:forks' : 27,
            'github:watchers' : 31
        }
    },
    'mendeley' : { 
        'aliases': [u'url', u'doi', u'title'],
        'biblio': [u'authors', u'journal', u'year', u'title'],
        'metrics' : {
            'mendeley:readers' : 50,
            'mendeley:groups' : 4
        }
    },
    'topsy' : { 
        'aliases': ["url"],
        'biblio': [],
        'metrics' : {
            'topsy:tweets' : 282,
            'topsy:influential_tweets' : 26
        }
    },
    'webpage' : { 
        'aliases': ['url'],
        'biblio': [u'title', "h1"],
        'metrics' : {}
    },
    'wikipedia' : { 
        'aliases': ['doi'],
        'biblio': [],
        'metrics' : {
            'wikipedia:mentions' : 1
        }
    }
}


def request_provider_item(provider, nid, section):
    base_url = 'http://localhost:5001/'
    nid = urlparse.unquote(nid)
    url = base_url + urllib.quote('provider/%s/%s/%s' % (provider, section, nid))
    if options.debug:
        print "\n", url
    req = urllib2.Request(url)
    try:
        response = urllib2.urlopen(req)
        result = json.loads(response.read())
    except urllib2.HTTPError:
        if options.debug:
            print("HTTPError on %s %s %s, perhaps not implemented" % (provider, section, nid))
        result = {}
    if options.debug:
        print result

    return result


def checkItem(provider, id, section, api_response, options):
    if options.debug:
        print "Checking %s result (%s)..." % (provider, id)
    
    # Check aliases are correct
    if section=="aliases":
        aliases = GOLD_RESPONSES[provider]['aliases']
        alias_result = set([namespace for (namespace, nid) in api_response])
        expected_result = set(aliases)
        if (alias_result == expected_result):
            if options.debug:
                print "ALIASES CORRECT! %s" %(alias_result)
        else:
            if options.debug:
                print "ALIASES **NOT** CORRECT, have %s, want %s" %(alias_result, expected_result)
            return False

    # Check biblio are correct
    elif section=="biblio":
        biblio = GOLD_RESPONSES[provider]['biblio']    
        if api_response:
            biblio_result = set(api_response.keys())
        else:
            biblio_result = set([])
        expected_result = set(biblio)

        if (biblio_result == expected_result):
            if options.debug:
                print "BIBLIO CORRECT! %s" %(biblio_result)
        else:
            if options.debug:
                print "BIBLIO **NOT** CORRECT, have %s, want %s" %(biblio_result, expected_result)
            return False

    # Check we've got some metric values
    elif section=="metrics":
        metrics = GOLD_RESPONSES[provider]['metrics']
        for metric in metrics.keys():
            try:
                metric_data = api_response[metric]
            except KeyError:
                # didn't return anything.  problem!
                print "METRICS **NOT** CORRECT for %s: metric missing" % (metric)
                pprint(api_response)
                return False

            # expect the returned value to be equal or larger than reference
            if metric_data >= metrics[metric]:
                if options.debug:
                    print "METRICS CORRECT! %s" %(metric_data)
            else:
                if options.debug:
                    print "METRICS **NOT** CORRECT for %s - %s, expected at least %s" % (metric, metric_data, metrics[metric])
                pprint(api_response)
                return False


    return True


def make_call(nid, provider, options):
    all_successful = True
    for section in ["biblio", "aliases", "metrics"]:
        api_response = request_provider_item(provider, nid, section)
        is_response_correct = checkItem(provider,
            nid, 
            section,
            api_response,
            options
        )
        if is_response_correct:
            if not options.quiet:            
                print "happy %s" % section
        else:
            if not options.quiet:            
                print "INCORRECT %s" % section
            all_successful = False
        if options.printdata:
            pprint(api_response)
            print("\n")  
    return(all_successful)


if __name__ == '__main__':
    parser = OptionParser()
    parser.add_option("-s", "--simultaneous", dest="simultaneous", default=1,
                      help="Number of simultaneous requests to make")
    parser.add_option("-q", "--quiet", dest="quiet", default=False, action="store_true", help="Print less output")
    parser.add_option("-v", "--debug", dest="debug", default=False, action="store_true", help="Print debug output")
    parser.add_option("-p", "--printdata", dest="printdata", default=False, 
        action="store_true", help="Display item data")
    (options, args) = parser.parse_args()

    all_successful = True
    for (provider, alias) in REQUEST_IDS:
        print "\n**** %s *****" %(provider.upper())

        (namespace, nid) = alias
        print "\nCANNED DATA"
        canned_success = make_call("example", provider, options)

        print "\nLIVE DATA with item (%s, %s)" %(namespace, nid)
        live_success = make_call(nid, provider, options)
 
        all_successful = all_successful and canned_success and live_success

        time.sleep(0.5)
    if all_successful:
        print "\nAll provider responses were HAPPY."
    else:
        print "\nSome provider responses had errors"


########NEW FILE########
__FILENAME__ = alt_providers_test_proxy
#!/usr/bin/env python
#
# Providers Test Proxy
#
# This is a very basic webserver which can be used to simluate commuicating
# providers. It performs basic response replay for known data items. Response
# data is stored in test/data/<provider>
#

from SimpleHTTPServer import SimpleHTTPRequestHandler
from BaseHTTPServer import BaseHTTPRequestHandler
import SocketServer
from optparse import OptionParser
import logging
import os


class ProvidersTestProxy(BaseHTTPRequestHandler):

    def do_GET(self):
        datadir = os.path.join(os.path.split(__file__)[0], "sample_provider_pages")

        # remove the first character because includes /
        submitted_url = self.path[1:len(self.path)+1]
        # separate everything after & because is id
        try:
            (url_part, arg_part) = submitted_url.split("?")
        except ValueError:
            url_part = submitted_url

        # don't try to serve up the favicon, just exit
        if url_part == "favicon.ico":
            return

        sample_provider_page_path = os.path.join(datadir, url_part)
        print sample_provider_page_path
        try:
            text = open(sample_provider_page_path).read()
            print text
            print "Found:", submitted_url
            self.send_response(200)
            self.end_headers()
            self.wfile.write(text)
        except IOError:
            print "Not Found:", submitted_url
            self.send_response(500, "Test Proxy: Unknown URL")
        print "done with do_GET"



if __name__ == '__main__':

    parser = OptionParser()
    parser.add_option("-p", "--port",
                      action="store", dest="port", default=8080,
                      help="Port to run the server on (default 8080)")
    parser.add_option("-v", "--verbose",
                      action="store_true", dest="verbose", default=False,
                      help="print debugging output")

    (options, args) = parser.parse_args()

    if not options.verbose:
        logger = logging.getLogger('ti.providers')
        logger.setLevel(logging.WARNING)

    class ReuseServer(SocketServer.TCPServer):
        allow_reuse_address = True

    handler = ProvidersTestProxy
    httpd = ReuseServer(("", int(options.port)), handler)
    print "listening on port", options.port
    httpd.serve_forever()



########NEW FILE########
__FILENAME__ = functional_test
#!/usr/bin/env python
#
# A basic functional test of the total impact API
#

import urllib2
import urllib
import json
import time
import sys
import pickle
from pprint import pprint
from optparse import OptionParser

TEST_ITEMS = {
    ('doi', '10.1371/journal.pcbi.1000361') :
        { 
            'aliases': ['doi', "title", "url"],
            'biblio': [u'authors', u'journal', u'year', u'title'],
            'metrics' : {
                'wikipedia:mentions' : 1,
                u'plosalm:crossref': 133,
                 'plosalm:html_views': 17455,
                 'plosalm:pdf_views': 2106,
                 u'plosalm:pmc_abstract': 19,
                 u'plosalm:pmc_figure': 71,
                 u'plosalm:pmc_full-text': 1092,
                 u'plosalm:pmc_pdf': 419,
                 u'plosalm:pmc_supp-data': 157,
                 u'plosalm:pmc_unique-ip': 963,
                 u'plosalm:pubmed_central': 102,
                 u'plosalm:scopus': 218
            }
        },
    ('url', 'http://total-impact.org/') : #note trailing slash
        { 
            'aliases': ["url"],
            'biblio': ['title'],
            'metrics' : {
                'delicious:bookmarks' : 65
            }
        },
    ('url', 'http://total-impact.org'): #no trailing slash
        { 
            'aliases': ["url"],
            'biblio': ['title'],
            'metrics' : {
                'topsy:tweets' : 282,
                'topsy:influential_tweets' : 26
            }
        },                    
    ('doi', '10.5061/dryad.18') : 
        { 
            'aliases': ['doi', 'url', 'title'],
            'biblio': [u'authors', u'year', u'repository', u'title'],
            'metrics' : {
                'dryad:most_downloaded_file' : 63,
                'dryad:package_views' : 149,
                'dryad:total_downloads' : 169
            }
        },
    ('github', 'egonw,cdk') :
        { 
            'aliases': ['github', 'url', 'title'],
            'biblio': [u'last_push_date', u'create_date', u'description', u'title', u'url', u'owner', 'h1'],
            'metrics' : {
                'github:forks' : 27,
                'github:watchers' : 31
            }
        },                    
    ('url', 'http://nescent.org/'):
        { 
            'aliases': ['url'],
            'biblio': [u'title', "h1"],
            'metrics' : {}
        },
    ('url', 'http://www.slideshare.net/cavlec/manufacturing-serendipity-12176916') :
        {
            'aliases' : ['url', 'title'],
            'biblio': [u'username', u'repository', u'created', u'h1', u'genre', u'title'],
            'metrics' : {
                'slideshare:downloads' : 4,
                'slideshare:views' : 337,
                'slideshare:favorites' : 2
            }
        }
}

class TotalImpactAPI:
    base_url = 'http://localhost:5001/'

    def request_item(self, alias):
        """ Attempt to obtain an item from the server using the given
            namespace and namespace id. For example, 
              namespace = 'pubmed', nid = '234234232'
            Will request the item related to pubmed item 234234232
        """

        (namespace, nid) = alias
        url = self.base_url + urllib.quote('item/%s/%s' % (namespace, nid))
        req = urllib2.Request(url)
        data = {} # fake a POST
        response = urllib2.urlopen(req, data)
        tiid = json.loads(urllib.unquote(response.read()))
        print "tiid %s for %s" %(tiid, alias)
        return tiid

    def request_item_result(self, item_id):
        url = self.base_url + urllib.quote('item/%s' % (item_id))
        req = urllib2.Request(url)
        response = urllib2.urlopen(req)
        return json.loads(response.read())
        

def checkItem(item, data, alias, items_for_use, options):
    if options.debug: 
        print "Checking %s result (%s)..." % (alias, item)
    
    success = True
    for section in ["biblio", "aliases", "metrics"]:
        result = checkItemSection(alias, 
                item, 
                section, 
                data[section], 
                items_for_use[alias], 
                options)
        if not result:
            success = False
    return success

def checkItemSection(alias, id, section, api_response, gold_item, options):
    success = True
    if options.debug:
        print "Checking %s result (%s)..." % (alias, id)

    # Check aliases are correct
    if section=="aliases":
        gold_aliases = gold_item['aliases']
        alias_result = set(api_response.keys())
        expected_result = set(gold_aliases + [u'last_modified', u'created'])
        if (alias_result == expected_result):
            if options.debug:
                print "ALIASES CORRECT! %s" %(alias_result)
        else:
            if options.debug:
                print "ALIASES **NOT** CORRECT, for %s, %s, have %s, want %s" %(alias, id, alias_result, expected_result)
            success = False

    # Check biblio are correct
    elif section=="biblio":
        gold_biblio = gold_item['biblio']    
        if api_response:
            biblio_result = set(api_response.keys())
        else:
            biblio_result = set([])
        expected_result = set(gold_biblio + ['genre'])

        if (biblio_result == expected_result):
            if options.debug:
                print "BIBLIO CORRECT! %s" %(biblio_result)
        else:
            if options.debug:
                print "BIBLIO **NOT** CORRECT, have %s, want %s" %(biblio_result, expected_result)
            success = False

    # Check we've got some metric values
    elif section=="metrics":
        gold_metrics = gold_item['metrics']
        for metric in gold_metrics.keys():
            try:
                metric_data = api_response[metric].values()[0]
            except KeyError:
                # didn't return anything.  problem!
                if options.debug:
                    print "METRICS **NOT** CORRECT for %s: metric missing" % (metric)
                success = False

            # expect the returned value to be equal or larger than reference
            if success:
                if metric_data >= gold_metrics:
                    if options.debug:
                        print "METRICS CORRECT! %s" %(metric_data)
                else:
                    if options.debug:
                        print "METRICS **NOT** CORRECT for %s - %s, expected at least %s" % (metric, metric_data, gold_metrics)
                    return False

    if options.debug:
        print #blank line

    return success




if __name__ == '__main__':


    parser = OptionParser()
    parser.add_option("-n", "--numrepeats", dest="numrepeats", 
            default=1, help="Number of repeated requests to make")
    parser.add_option("-i", "--items", dest="numdiverseitems", 
            default=999, 
            help="Number of diverse items to use (up to max defined)")
    parser.add_option("-m", "--missing", dest="missing", 
            default=False, action="store_true", 
            help="Display any outstanding items")
    parser.add_option("-p", "--printdata", dest="printdata", 
            default=False, action="store_true", help="Display item data")
    parser.add_option("-v", "--verbose", dest="debug", 
            default=False, action="store_true", help="Display verbose debug data")
    (options, args) = parser.parse_args()


    item_count = int(options.numrepeats)
    num_diverse_items = min(len(TEST_ITEMS), int(options.numdiverseitems))
    aliases = TEST_ITEMS.keys()[0:num_diverse_items]
    items_for_use = dict((alias, TEST_ITEMS[alias]) for alias in aliases)
    
    ti = TotalImpactAPI()

    complete = {}
    itemid = {}



    for alias in aliases:
        complete[alias] = {}
        itemid[alias] = {}
        for idx in range(item_count):
            # Request the items to be generated
            itemid[alias][idx] = ti.request_item(alias)
            complete[alias][idx] = False

    while True:
        for idx in range(item_count):
            for alias in aliases:
                if not complete[alias][idx]:
                    if options.missing:
                        print alias, idx, itemid[alias][idx]
                    
                    itemdata = ti.request_item_result(itemid[alias][idx])

                    complete[alias][idx] = checkItem(
                        itemid[alias][idx], 
                        itemdata,
                        alias, 
                        items_for_use,
                        options
                    )
                    if complete[alias][idx] and options.printdata:
                        pprint(itemdata)

        total = sum([sum(complete[alias].values()) for alias in aliases])
        print "%i of %i responses are complete" %(total, item_count * len(aliases))
        if total == item_count * len(aliases):
            sys.exit(0)    

        time.sleep(0.5)


########NEW FILE########
__FILENAME__ = providers_check
#!/usr/bin/env python
#
# Providers Check
#
# This is currently a very basic check for providers
#

from totalimpact.dao import Dao
from totalimpact.models import Item, ItemFactory, Aliases

from totalimpact.providers.github import Github
from totalimpact.providers.wikipedia import Wikipedia
from totalimpact.providers.dryad import Dryad

import sys
import test
import time

from pprint import pprint

from totalimpact.providers.github import Github
from totalimpact.providers.wikipedia import Wikipedia
from totalimpact.api import app

import logging
import traceback

from optparse import OptionParser





################################################################################
#
#
#
# TODO this is TOTALLY BROKEN
# and will be until it's refactored to use the new, refactored items and alias
# dicts (no longer objects).
#
#
################################################################################







class ProvidersCheck:

    def __init__(self):
        self.mydao = Dao(app.config["DB_NAME"], app.config["DB_URL"], app.config["DB_USERNAME"], app.config["DB_PASSWORD"])

    # Aux methods which record failures in appropriate member variables 
    # so they can be reported and acted upon

    def check_aliases(self, name, result, expected):
        if expected not in result:
            self.errors['aliases'].append("Aliases error for %s - Result '%s' does not contain expected value '%s'" % (
                name, result, expected ))

    def check_metric(self, name, result, expected):
        if result != expected:
            self.errors['metrics'].append("Metric error for %s - Result '%s' does not match expected value '%s'" % (
                name, result, expected ))

    def check_members(self, name, result, expected):
        if result != expected:
            self.errors['members'].append("Members error for %s - Result '%s' does not match expected value '%s'" % (
                name, result, expected ))

    def checkDryad(self):
        # Test reading data from Dryad
        item = ItemFactory.make_simple(self.mydao)
        item.aliases.add_alias('doi', '10.5061/dryad.7898')
        item_aliases_list = item.aliases.get_aliases_list()

        dryad = Dryad()
        new_aliases = dryad.aliases(item_aliases_list)
        new_metrics = dryad.metrics(item_aliases_list)

        self.check_aliases('dryad.url', new_aliases, ("url", 'http://hdl.handle.net/10255/dryad.7898'))
        self.check_aliases('dryad.title', new_aliases, ("title", 'data from: can clone size serve as a proxy for clone age? an exploration using microsatellite divergence in populus tremuloides'))
    
    def checkWikipedia(self):
        # Test reading data from Wikipedia
        item = ItemFactory.make_simple(self.mydao)
        item.aliases.add_alias("doi", "10.1371/journal.pcbi.1000361")
        #item.aliases.add_alias("url", "http://cottagelabs.com")

        item_aliases_list = item.aliases.get_aliases_list()

        wikipedia = Wikipedia()
        # No aliases for wikipedia
        #new_aliases = wikipedia.aliases(item_aliases_list)
        new_metrics = wikipedia.metrics(item_aliases_list)

        self.check_metric('wikipedia:mentions', new_metrics['wikipedia:mentions'], 1)

    def checkGithub(self):
        item = ItemFactory.make_simple(self.mydao)

        github = Github()
        members = github.member_items("egonw")
        self.check_members('github.github_user', members, 
            [('github', ('egonw', 'blueobelisk.debian')),
             ('github', ('egonw', 'ron')),
             ('github', ('egonw', 'pubchem-cdk')),
             ('github', ('egonw', 'org.openscience.cdk')),
             ('github', ('egonw', 'java-rdfa')),
             ('github', ('egonw', 'cdk')),
             ('github', ('egonw', 'RobotDF')),
             ('github', ('egonw', 'egonw.github.com')),
             ('github', ('egonw', 'knime-chemspider')),
             ('github', ('egonw', 'gtd')),
             ('github', ('egonw', 'cheminfbenchmark')),
             ('github', ('egonw', 'cdk-taverna')),
             ('github', ('egonw', 'groovy-jcp')),
             ('github', ('egonw', 'jnchem')),
             ('github', ('egonw', 'acsrdf2010')),
             ('github', ('egonw', 'Science-3.0')),
             ('github', ('egonw', 'SNORQL')),
             ('github', ('egonw', 'ctr-cdk-groovy')),
             ('github', ('egonw', 'CDKitty')),
             ('github', ('egonw', 'rednael')),
             ('github', ('egonw', 'de.ipbhalle.msbi')),
             ('github', ('egonw', 'collaborative.cheminformatics')),
             ('github', ('egonw', 'xws-taverna')),
             ('github', ('egonw', 'cheminformatics.classics')),
             ('github', ('egonw', 'chembl.rdf')),
             ('github', ('egonw', 'blueobelisk.userscript')),
             ('github', ('egonw', 'ojdcheck')),
             ('github', ('egonw', 'nmrshiftdb-rdf')),
             ('github', ('egonw', 'bioclipse.ons')),
             ('github', ('egonw', 'medea_bmc_article'))])

        item.aliases.add_alias("github", "egonw,gtd")
        item_aliases_list = item.aliases.get_aliases_list()

        new_metrics = github.metrics(item_aliases_list)

        self.check_metric('github:forks', new_metrics['github:forks'], 0)
        self.check_metric('github:watchers', new_metrics['github:watchers'], 7)

    def checkAll(self):
        # This will get appended to by each check if it finds any data mismatches
        self.errors = {'aliases':[], 'metrics':[], 'members':[]}
        exceptions = 0

        if not self.quiet: print "Checking Dryad provider"
        try:
            self.checkDryad()
        except Exception, e:
            print "Error running providers check for Dryad:", e
            exc_type, exc_value, exc_traceback = sys.exc_info()
            traceback.print_tb(exc_traceback)
            exceptions += 1

        if not self.quiet: print "Checking Wikipedia provider"
        try:
            self.checkWikipedia()
        except Exception, e:
            print "Error running providers check for Wikipedia:", e
            exc_type, exc_value, exc_traceback = sys.exc_info()
            traceback.print_tb(exc_traceback)
            exceptions += 1


        if not self.quiet: print "Checking Github provider"
        try:
            self.checkGithub()
        except Exception, e:
            print "Error running providers check for Github:", e
            exc_type, exc_value, exc_traceback = sys.exc_info()
            traceback.print_tb(exc_traceback)
            exceptions += 1

        if exceptions:
            print "Checks complete, exceptions raised"
            return False
    
        if sum([len(self.errors[key]) for key in ['aliases','metrics','members']]) > 0:
            print "Checks complete, the following data inconsistencies were found"
            for key in self.errors.keys():
                print "\n== %s ===============================" % key
                for error in self.errors[key]:
                    print error
            return False
        else:
            if not self.quiet: print "Checks complete, no data inconsistencies were found"
            return True
       


if __name__ == '__main__':

    parser = OptionParser()
    parser.add_option("-v", "--verbose",
                      action="store_true", dest="verbose", default=False,
                      help="Print detailed debugging outputs")
    parser.add_option("-q", "--quiet",
                      action="store_true", dest="quiet", default=False,
                      help="Only print errors on failures")

    (options, args) = parser.parse_args()
    
    if options.verbose:
        ch = logging.StreamHandler()
        ch.setLevel(logging.DEBUG)
        # Nicer formatting to show different providers
        formatter = logging.Formatter('  %(name)s - %(message)s')
        ch.setFormatter(formatter)
        logger = logging.getLogger('ti.providers_check')
        logger.addHandler(ch)

    check = ProvidersCheck()    
    check.quiet = options.quiet
    if not check.checkAll():
        sys.exit(1)



########NEW FILE########
__FILENAME__ = providers_test_proxy
#!/usr/bin/env python
#
# Providers Test Proxy
#
# This is a very basic webserver which can be used to simluate commuicating
# providers. It performs basic response replay for known data items. Response
# data is stored in test/data/<provider>
#

from SimpleHTTPServer import SimpleHTTPRequestHandler
from BaseHTTPServer import BaseHTTPRequestHandler
import SocketServer
from optparse import OptionParser
import logging
import os

import re

responses = {'dryad':{},'wikipedia':{},'github':{},'mendeley':{},'crossref':{}}

def load_test_data(provider, filename):
    datadir = os.path.join(os.path.split(__file__)[0], "../test/data/", provider)
    return open(os.path.join(datadir, filename)).read()

responses['dryad']['aliases'] = (200, load_test_data('dryad', 'sample_extract_aliases_page.xml'))
responses['dryad']['metrics'] = (200, load_test_data('dryad', 'sample_extract_metrics_page.html'))
responses['dryad']['10.5061'] = (200, load_test_data('dryad', 'dryad_info_10.5061.xml'))
responses['wikipedia']['metrics'] = (200, load_test_data('wikipedia', 'wikipedia_response.xml'))
responses['wikipedia']['10.1186'] = (200, load_test_data('wikipedia', 'wikipedia_10.1186_response.xml'))
responses['wikipedia']['10.5061'] = (200, load_test_data('wikipedia', 'wikipedia_10.5061_response.xml'))
responses['wikipedia']['cottagelabs'] = (200, load_test_data('wikipedia', 'wikipedia_cottagelabs.xml'))
responses['github']['members'] = (200, load_test_data('github', 'egonw_gtd_member_response.json'))
responses['github']['metrics'] = (200, load_test_data('github', 'egonw_gtd_metric_response.json'))
responses['mendeley']['aliases-10.5061'] = (404, load_test_data('mendeley', 'mendeley-aliases-10.5061'))
responses['crossref']['aliases-10.5061'] = (200, load_test_data('crossref', 'crossref-aliases-10.5061'))

urlmap = {

    ###################################################################################
    ##
    ## Dryad Provider
    ##  

    "http://datadryad.org/solr/search/select/?q=dc.identifier:10.5061/dryad.7898&fl=dc.identifier.uri,dc.title": responses['dryad']['aliases'],
    "http://datadryad.org/solr/search/select/?q=dc.identifier:10.5061/dryad.7898&fl=dc.date.accessioned.year,dc.identifier.uri,dc.title_ac,dc.contributor.author_ac" : responses['dryad']['10.5061'],
    "http://dx.doi.org/10.5061/dryad.7898": responses['dryad']['metrics'],

    ###################################################################################
    ##
    ## Wikipedia Provider
    ##

    # Metrics information for various test items
    "http://en.wikipedia.org/w/api.php?action=query&list=search&srprop=timestamp&format=xml&srsearch='10.1371/journal.pcbi.1000361'": responses['wikipedia']['metrics'],
    "http://en.wikipedia.org/w/api.php?action=query&list=search&srprop=timestamp&format=xml&srsearch='10.1186/1745-6215-11-32'": responses['wikipedia']['10.1186'],
    "http://en.wikipedia.org/w/api.php?action=query&list=search&srprop=timestamp&format=xml&srsearch='10.5061/dryad.7898'": responses['wikipedia']['10.5061'],
    "http://en.wikipedia.org/w/api.php?action=query&list=search&srprop=timestamp&format=xml&srsearch='http://cottagelabs.com'": responses['wikipedia']['cottagelabs'],

    ###################################################################################
    ##
    ## Github Provider
    ##

    # member_items results for egonw
    "https://api.github.com/users/egonw/repos": responses['github']['members'],
    # metrics results for ('github', 'egonw,gtd')
    "https://github.com/api/v2/json/repos/show/egonw/gtd": responses['github']['metrics'],

    ###################################################################################
    ##
    ## Mendeley Provider
    ##

    re.compile(r"http://api.mendeley.com/oapi/documents/details/10.5061%252Fdryad.7898\?type=doi&consumer_key=.*"): responses['mendeley']['aliases-10.5061'],

    ###################################################################################
    ##
    ## Crossref Provider
    ##

    re.compile(r"http://doi.crossref.org/servlet/query\?pid=(.*)&qdata=10.5061/dryad.7898&format=unixref"): responses["crossref"]['aliases-10.5061'],

    ###################################################################################
    ##
    ## Test Item
    ##
    ## This is just so you can check http://proxy:port/test to see if this is running ok
    ##

    "/test": responses['github']['members'],
}

class ProvidersTestProxy(BaseHTTPRequestHandler):

    def do_GET(self):
        # Find match, including regex
        match = None
        for key in urlmap.keys():
            if isinstance(key, str):
                if self.path == key:
                    match = key
            else:
                if key.match(self.path):
                    match = key
        if match:
            print "Found:", self.path
            (code, response) = urlmap[match]
            self.send_response(code)
            self.end_headers()
            self.wfile.write(response)
        else: 
            print "Not Found:", self.path
            self.send_response(500, "Test Proxy: Unknown URL")

if __name__ == '__main__':

    parser = OptionParser()
    parser.add_option("-p", "--port",
                      action="store", dest="port", default=8081,
                      help="Port to run the server on (default 8081)")
    parser.add_option("-v", "--verbose",
                      action="store_true", dest="verbose", default=False,
                      help="print debugging output")
    parser.add_option("-l", "--log",
                      action="store", dest="log", default=None,
                      help="runtime log")
    parser.add_option("-q", "--quiet",
                      action="store_true", dest="quiet", default=False,
                      help="Only print errors on failures")

    (options, args) = parser.parse_args()

    if options.verbose:
        ch = logging.StreamHandler()
        ch.setLevel(logging.DEBUG)
        # Nicer formatting to show different providers
        formatter = logging.Formatter('  %(name)s - %(message)s')
        ch.setFormatter(formatter)
        logger = logging.getLogger('')
        logger.addHandler(ch)
    else:
        logger = logging.getLogger('ti.providers')
        logger.setLevel(logging.WARNING)

    class ReuseServer(SocketServer.TCPServer):
        allow_reuse_address = True

    handler = ProvidersTestProxy
    httpd = ReuseServer(("", int(options.port)), handler)
    print "listening on port", options.port
    httpd.serve_forever()
    


########NEW FILE########
__FILENAME__ = run_collection_test
import argparse, redis, os
from totalimpact import testers

""" Runs interaction tests; takes type of interaction from the argument it's called with."""

# get args from the command line:
parser = argparse.ArgumentParser(description="Run interaction tests from the command line")
parser.add_argument("action_type", type=str, help="The action to test; available actions listed in fakes.py")
args = vars(parser.parse_args())
print args
print "run_collection_test.py starting."

# this assumes you're testing collections; must be adapted when we add provider tests...
collection_tester = testers.CollectionTester()
report = collection_tester.test(args["action_type"])

# storing them in redis so we can look at them in the /test/collection/<create|read> report
redis = redis.from_url(os.getenv("REDISTOGO_URL"))
report_key = "test.collection." + args["action_type"]
print "saving report in redis with key " + report_key
redis.hmset(report_key, report)






########NEW FILE########
__FILENAME__ = load_test
#!/usr/bin/env python
#
# A basic functional test of the total impact API
#

import mechanize
import urllib2
import urllib
import json
import time
import sys
import pickle
from pprint import pprint

class TotalImpactAPI:
    base_url = 'http://localhost:5001/'

    def request_item(self, namespace, nid):
        """ Attempt to obtain an item from the server using the given
            namespace and namespace id. For example, 
              namespace = 'pubmed', nid = '234234232'
            Will request the item related to pubmed item 234234232
        """

        url = self.base_url + urllib.quote('item/%s/%s' % (namespace, nid))
        print url
        req = urllib2.Request(url)
        data = {} # fake a POST
        response = urllib2.urlopen(req, data)
        item_id = json.loads(urllib.unquote(response.read()))
        return item_id

    def request_item_result(self, item_id):
        url = self.base_url + urllib.quote('item/%s' % (item_id))
        req = urllib2.Request(url)
        response = urllib2.urlopen(req)
        return json.loads(response.read())
        

from optparse import OptionParser


def checkItem(item, data, item_type, debug=False):
    checks = {
        'wikipedia' : { 
            'aliases': ['doi'],
            'metrics' : {
                'wikipedia:mentions' : 1
            }
        },
        'github' : { 
            'aliases': ['github', 'url', 'title'],
            'metrics' : {
                'github:forks' : 0,
                'github:watchers' : 7
            }
        },
        'dryad' : { 
            'aliases': ['doi','url','title'],
            'metrics' : {
                'dryad:most_downloaded_file' : 63,
                'dryad:package_views' : 149,
                'dryad:total_downloads' : 169
            }
        }
    }

    aliases = checks[item_type]['aliases']
    metrics = checks[item_type]['metrics']

    if debug: print "Checking %s result (%s)..." % (item_type, item)
    
    # Check aliases are correct
    alias_result = set(data['aliases'].keys())
    if alias_result != set(['created','last_modified','last_completed'] + aliases):
        if debug: print "Aliases is not correct, have %s" % alias_result
        return False

    # Check we've got some metric values
    for metric in metrics.keys():
        metric_data = data['metrics'][metric]['values']
        if len(metric_data) != 1:
            if debug: print "Incorrect number of metric results for %s - %i" % (metric, len(metric_data))
            return False
        else:
            if metric_data.values()[0] < metrics[metric]:
                if debug: print "Incorrect metric result for %s - %s" % (metric, metric_data.values()[0])
                return False

    return True





from twisted.internet import reactor
from twisted.web.client import getPage 

total_responses = 0

REQUEST_DETAILS_RETRY_DELAY=5

class TotalImpactAPIAsync:
    base_url = 'http://localhost:5001/'
    debug = False
    
    def __init__(self):
        self.item_ids = {}
        self.item_details = {}
        self.total_responses = 0
        self.details_responses = 0
        self.item_idx = 0
    
    def handleRequestItemResponse(self, response, idx):
        if self.debug: print 'Response received for %i' % idx
        self.item_ids[idx] = json.loads(urllib.unquote(response))
        self.total_responses += 1

    def handleRequestItemFailure(self, failure, idx):
        print 'Failure received for %i' % idx, failure
        self.total_responses += 1
        reactor.stop()

    def handleItemDetailsResponse(self, response, idx, item_type, count):
        if self.debug: print 'Details received for %i' % idx
        self.item_details[idx] = json.loads(urllib.unquote(response))
        if not checkItem(self.item_ids[idx], self.item_details[idx], item_type):
            # This item isn't read yet, lets keep re-requesting
            if self.debug: print "not ready yet, calling later"
            reactor.callLater(REQUEST_DETAILS_RETRY_DELAY, self._request_details, idx, item_type, count+1)
        else:
            self.details_responses += 1

    def handleItemDetailsFailure(self, failure, idx):
        print 'Item details failed for %i' % idx, failure
        self.details_responses += 1
        reactor.stop()

    def request_item(self, namespace, nid):
        """ Attempt to obtain an item from the server using the given
            namespace and namespace id. For example, 
              namespace = 'pubmed', nid = '234234232'
            Will request the item related to pubmed item 234234232
        """
        url = self.base_url + urllib.quote('item/%s/%s' % (namespace, nid))
        d = getPage(url, method='POST')
        idx = self.item_idx
        self.item_idx += 1
        d.addCallback(self.handleRequestItemResponse, idx)
        d.addErrback(self.handleRequestItemFailure, idx)
        return idx

    def request_details(self, idx, item_type):
        """ Get an item's details. Keep retrying until results 
            are obtained and valid """
        return self._request_details(idx, item_type, count=1)
   
    def _request_details(self, idx, item_type, count=1):
        item_id = self.item_ids[idx]
        url = self.base_url + urllib.quote('item/%s' % (item_id))
        if self.debug: print "Getting %i / %s" % (idx, url)
        d = getPage(url, method='GET')
        d.addCallback(self.handleItemDetailsResponse, idx, item_type, count)
        d.addErrback(self.handleItemDetailsFailure, idx)


class TotalImpactTest:
    """ Controller for handling the sequence of operations
        asynchronously.
    """

    def __init__(self, item_count):
        self.ti = TotalImpactAPIAsync()
        self.item_count = item_count
        self.itemid = {}
        for item_type in ['dryad','wikipedia','github']:
            self.itemid[item_type] = {}

    # Flow Methods ##############################################

    def runTests(self):
        self.requestItems()
        reactor.callLater(1, self.waitForRequestsToComplete)

    def waitForRequestsToComplete(self):
        if self.ti.total_responses != self.item_count * 3:
            print "Requesting items (%i/%i)" % (self.ti.total_responses, self.item_count * 3)
            reactor.callLater(1, self.waitForRequestsToComplete)
        else:
            print "All items requested, now waiting for results"
            self.getItemResults()
            reactor.callLater(1, self.waitForDetailsToComplete)
    
    def waitForDetailsToComplete(self):
        if self.ti.details_responses != self.item_count * 3:
            print "Results received (%i/%i)" % (self.ti.details_responses, self.item_count * 3)
            reactor.callLater(1, self.waitForDetailsToComplete)
        else:
            print "All details complete"
            reactor.stop()

    #############################################################

    def getItemResults(self):
        for idx in range(self.item_count):
            for item_type in ['dryad','wikipedia','github']:
                self.ti.request_details(self.itemid[item_type][idx], item_type)

    def requestItems(self):
        for idx in range(self.item_count):
            # Request the items to be generated
            self.itemid['dryad'][idx] = self.ti.request_item('doi','10.5061/dryad.7898')
            self.itemid['wikipedia'][idx] = self.ti.request_item('doi', '10.1371/journal.pcbi.1000361')
            self.itemid['github'][idx] = self.ti.request_item('github', 'egonw,cdk')



if __name__ == '__main__':

    parser = OptionParser()
    parser.add_option("-s", "--simultaneous", dest="simultaneous", default=1,
                      help="Number of simultaneous requests to make")
    parser.add_option("-m", "--missing", dest="missing", default=False, action="store_true",
                      help="Display any outstanding items")
    parser.add_option("-p", "--printdata", dest="printdata", default=False, action="store_true",
                      help="Display item data")
    (options, args) = parser.parse_args()


    item_count = int(options.simultaneous)
    
    tester = TotalImpactTest(item_count)
    tester.runTests()
    reactor.run()

    print "The following are not yet complete..."

    # Print anything outstanding
    for item_type in ['dryad','wikipedia','github']:
        for idx in tester.itemid[item_type].values():
            if not checkItem(tester.ti.item_ids[idx], tester.ti.item_details[idx], item_type):
                print item_type, tester.ti.item_ids[idx]


########NEW FILE########
__FILENAME__ = run_profiling
# in ipython or similar

import yappi
import os
from totalimpact import backend

rootdir = "."
logfile = '/tmp/total-impact.log'


yappi.clear_stats()
yappi.start()
backend.main(logfile)

### Now, in another window run
# ./services/api start
# ./services/proxy start
# ./extras/functional_test.py -i 6 -n 6
# then when it is done, in python do a Cntl C to stop the backend and return to python prompt

yappi.stop()

yappi.print_stats(sort_type=yappi.SORTTYPE_TTOT, limit=30, thread_stats_on=False)

########NEW FILE########
__FILENAME__ = run
import config
config.set_env_vars_from_dot_env()

from totalimpact import app
app.run(port=5001, debug=True)

########NEW FILE########
__FILENAME__ = run_backend
import config
config.set_env_vars_from_dot_env()

from totalimpact import backend
backend.main()
########NEW FILE########
__FILENAME__ = scratchpad
import threading, Queue, time, random

class Tools():

    def __init__(self, foo):
        self.foo = foo

    def add_stuff(self, val_to_add, q):
        print "starting add_stuff"
        while True:
            item = q.get()
            print "got item: ", str(item)
            ts = str(time.time()).split(".")[1]

            time.sleep(random.random())
            item[ts] = self.fooify(val_to_add)
            print "i added ", val_to_add, "to item."
            q.task_done()

    def fooify(self, input):
        return self.foo + str(input)




my_queue = Queue.Queue()
item = {}
tools = Tools("myfoo")

worker = threading.Thread(target=tools.add_stuff, args=(1, my_queue))
worker2= threading.Thread(target=tools.add_stuff, args=(2, my_queue))
worker.start()
worker2.start()
for i in range(1,10):
    my_queue.put(item)

time.sleep(1)
print "done sleeping"
my_queue.join()
print "donezo. here's item: "
print item


########NEW FILE########
__FILENAME__ = mocks
import time, logging

from totalimpact.providers.provider import Provider
from totalimpact.providers.provider import ProviderClientError, ProviderServerError
from totalimpact import item



def dao_init_mock(self, config):
    pass

class MockDao(object):

    responses = []
    index = 0

    def get(self, id):
        ret = self.responses[self.index]
        self.index = self.index + 1
        return ret

    def __getitem__(self, item):
        return get(item)
    
    def setResponses(self, responses):
        self.responses = responses
        self.index = 0

    def save(self, doc):
        self.responses.append(doc)

    def view(self, viewname, **kwargs):
        return None






################################################################################
# TODO this class is TOTALLY BROKEN
# and will be until it's refactored to use the new, refactored items and alias
# dicts (no longer objects).
################################################################################
class QueueMock(object):

    def __init__(self, max_items = None):
        self.none_count = 0
        self.current_item = 0
        self.max_items = max_items
        self.items = {}
        self.thread_id = 'Queue Mock'
        self.queue_name = "Queue Mock"

    def first(self):
        if self.none_count >= 3:
            if self.max_items:
                if self.current_item > self.max_items:
                    return None
            # Generate a mock item with initial alias ('mock', id)
            item = item.make()
            item.id = self.current_item
            item.aliases['mock'] = str(item.id)
            self.items[self.current_item] = item
            return item
        else:
            self.none_count += 1
            return None

    def dequeue(self):
        item = self.first()
        if item:
            self.current_item += 1
        return item

    def save(self, item):
        logger.debug(u"Saving item %s" % item.id)

    def add_to_metrics_queues(self, item):
        pass



class ProviderMock(Provider):
    """ Mock object to simulate a provider for testing 

    """
    provides_members = True
    provides_aliases = True
    provides_metrics = True
    provides_biblio = True

    metrics_returns = {
        "mock:pdf": (1, "http://drilldownurl.org"),
        "mock:html": (2, "http://drilldownurl.org")
    }
    aliases_returns = [('doi','10.1')]
    biblio_returns = {"title": "fake item"}

    exception_to_raise = None
    url = "http://fakeproviderurl.com"

    def __init__(self, provider_name=None):
        Provider.__init__(self, None)
        if provider_name:
            self.provider_name = provider_name
        else:
            self.provider_name = 'mock_provider'

    def metric_names(self):
        return(["wikipedia:mentions"])

    def aliases(self, aliases, url=None, cache_enabled=True):
        self._raise_preset_exception()
        return self.aliases_returns

    def metrics(self, aliases, url=None, cache_enabled=True):
        self._raise_preset_exception()
        return self.metrics_returns

    def biblio(self, aliases, url=None, cache_enabled=True):
        self._raise_preset_exception()
        return self.biblio_returns

    def _raise_preset_exception(self):
        if self.exception_to_raise:
            raise self.exception_to_raise
        

########NEW FILE########
__FILENAME__ = common
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderFactory
from totalimpact.providers.provider import ProviderError, ProviderTimeout, ProviderServerError, ProviderClientError
from totalimpact.providers.provider import ProviderHttpError, ProviderContentMalformedError, ProviderValidationFailedError

import os, unittest
import simplejson
from nose.tools import nottest, raises, assert_equals
from nose.plugins.skip import SkipTest

# prepare a monkey patch to override the http_get method of the Provider
class DummyResponse(object):
    def __init__(self, status, content):
        self.status_code = status
        self.text = content  
        self.headers = {"header1":"header_value_1"}  
        self.url = "http://example.com"

def get_member_items_html_success(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    f = open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE, "r")
    return DummyResponse(200, f.read())

def get_member_items_html_zero_items(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    f = open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE_ZERO_ITEMS, "r")
    return DummyResponse(200, f.read())

def get_aliases_html_success(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    f = open(SAMPLE_EXTRACT_ALIASES_PAGE, "r")
    return DummyResponse(200, f.read())

def get_metrics_html_success(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
    return DummyResponse(200, f.read())

def get_biblio_html_success(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
    return DummyResponse(200, f.read())

def get_nonsense_xml(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    return DummyResponse(200, '<?xml version="1.0" encoding="UTF-8"?><nothingtoseehere>nonsense</nothingtoseehere>')

def get_nonsense_txt(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    return DummyResponse(200, "nonsense")

def get_empty(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    return DummyResponse(200, "")

def get_400(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    return DummyResponse(400, "")

def get_500(self, url, headers=None, timeout=None, error_conf=None, cache_enabled=True, allow_redirects=False):
    return DummyResponse(500, "")


class ProviderTestCase:
    """ Base class to help in writing tests of providers

        To this class effectively, use it as the base class of your
        provider test and set self.provider to point to an instance
        of the provider you want to test.

        You also want to set the following so there is a suitable
        aliase which will be passed to aliases and metrics for testing.

          self.testitem_aliases = ('ns','val')
          self.testitem_metrics = ('ns','val')

        This test will ensure the following properties are covered:

          * Signature is fully defined
          * aliases method handles http errors correctly
          * metrics method handles http errors correctly
          * member_items method handles http errors correctly
          * biblio method handles http errors correctly
    
        The setup and tear down methods will do some temporary saves
        for monkey-patching, which allows you to replace 
        provider.http_get safely in your tests, if required.
        """

    # overwrite in providers for supported methods
    testitem_aliases = ()
    testitem_metrics = ()
    testitem_biblio = ()

    def setUp(self):
        self.provider = ProviderFactory.get_provider(self.provider_name)
        self.old_http_get = Provider.http_get

    def tearDown(self):
        Provider.http_get = self.old_http_get

    def test_0003_provider_interface(self):
        """ test_provider_interface

            Ensure that the implementation has all the relevant provider 
            definition fields defined.
        """
        # Function methods (may return NotImplementedError)
        assert hasattr(self.provider, "member_items")
        assert hasattr(self.provider, "aliases")
        assert hasattr(self.provider, "metrics")
        assert hasattr(self.provider, "biblio")

        # Class members for provider definition
        assert hasattr(self.provider, "provides_members")
        assert hasattr(self.provider, "provides_aliases")
        assert hasattr(self.provider, "provides_metrics")
        assert hasattr(self.provider, "provides_biblio")
        assert hasattr(self.provider, "provides_static_meta")

        assert hasattr(self.provider, "provider_name")

    ###################################################################
    ##
    ## Check member_items method 
    ##

    @raises(ProviderClientError, ProviderServerError)
    def test_provider_member_items_400(self):
        if not self.provider.provides_members:
            raise SkipTest
        Provider.http_get = get_400
        members = self.provider.member_items(self.testitem_members)

    @raises(ProviderServerError)
    def test_provider_member_items_500(self):
        if not self.provider.provides_members:
            raise SkipTest
        Provider.http_get = get_500
        members = self.provider.member_items(self.testitem_members)

    @raises(ProviderContentMalformedError)
    def test_provider_member_items_empty(self):
        if not self.provider.provides_members:
            raise SkipTest
        Provider.http_get = get_empty
        members = self.provider.member_items(self.testitem_members)

    @raises(ProviderContentMalformedError)
    def test_provider_member_items_nonsense_txt(self):
        if not self.provider.provides_members:
            raise SkipTest
        Provider.http_get = get_nonsense_txt
        members = self.provider.member_items(self.testitem_members)

    @raises(ProviderContentMalformedError)
    def test_provider_member_items_nonsense_xml(self):
        if not self.provider.provides_members:
            raise SkipTest
        Provider.http_get = get_nonsense_xml
        members = self.provider.member_items(self.testitem_members)

    ###################################################################
    ##
    ## Check aliases method 
    ##

    @raises(ProviderClientError, ProviderServerError)
    def test_provider_aliases_400(self):
        if not self.provider.provides_aliases:
            raise SkipTest
        Provider.http_get = get_400
        new_aliases = self.provider.aliases([self.testitem_aliases])

    @raises(ProviderServerError)
    def test_provider_aliases_500(self):
        if not self.provider.provides_aliases:
            raise SkipTest
        Provider.http_get = get_500
        new_aliases = self.provider.aliases([self.testitem_aliases])

    @nottest
    def test_provider_aliases_empty(self):
        if not self.provider.provides_aliases:
            raise SkipTest
        Provider.http_get = get_empty
        aliases = self.provider.aliases([self.testitem_aliases])
        assert_equals(aliases, [])

    @nottest
    @raises(ProviderContentMalformedError)
    def test_provider_aliases_nonsense_txt(self):
        if not self.provider.provides_aliases:
            raise SkipTest
        Provider.http_get = get_nonsense_txt
        new_aliases = self.provider.aliases([self.testitem_aliases])

    @nottest
    @raises(ProviderContentMalformedError)
    def test_provider_aliases_nonsense_xml(self):
        if not self.provider.provides_aliases:
            raise SkipTest
        Provider.http_get = get_nonsense_xml
        new_aliases = self.provider.aliases([self.testitem_aliases])

    ###################################################################
    ##
    ## Check metrics method
    ##

    @raises(ProviderClientError, ProviderServerError)
    def test_provider_metrics_400(self):
        if not self.provider.provides_metrics:
            raise SkipTest
        Provider.http_get = get_400
        metrics = self.provider.metrics([self.testitem_metrics])

    @raises(ProviderServerError)
    def test_provider_metrics_500(self):
        if not self.provider.provides_metrics:
            raise SkipTest
        Provider.http_get = get_500
        metrics = self.provider.metrics([self.testitem_metrics])

    @raises(ProviderContentMalformedError)
    def test_provider_metrics_empty(self):
        if not self.provider.provides_metrics:
            raise SkipTest
        Provider.http_get = get_empty
        metrics = self.provider.metrics([self.testitem_metrics])

    @raises(ProviderContentMalformedError)
    def test_provider_metrics_nonsense_txt(self):
        if not self.provider.provides_metrics:
            raise SkipTest
        Provider.http_get = get_nonsense_txt
        metrics = self.provider.metrics([self.testitem_metrics])

    @raises(ProviderContentMalformedError)
    def test_provider_metrics_nonsense_xml(self):
        if not self.provider.provides_metrics:
            raise SkipTest
        Provider.http_get = get_nonsense_xml
        metrics = self.provider.metrics([self.testitem_metrics])

    ###################################################################
    ##
    ## Check biblio method
    ##

    @raises(ProviderClientError, ProviderServerError)
    def test_provider_biblio_400(self):
        if not self.provider.provides_biblio:
            raise SkipTest
        Provider.http_get = get_400
        biblio = self.provider.biblio([self.testitem_biblio])

    @raises(ProviderServerError)
    def test_provider_biblio_500(self):
        if not self.provider.provides_biblio:
            raise SkipTest
        Provider.http_get = get_500
        biblio = self.provider.biblio([self.testitem_biblio])

    @nottest
    @raises(ProviderContentMalformedError)
    def test_provider_biblio_empty(self):
        if not self.provider.provides_biblio:
            raise SkipTest
        Provider.http_get = get_empty
        biblio = self.provider.biblio([self.testitem_biblio])

    @nottest
    @raises(ProviderContentMalformedError)
    def test_provider_biblio_nonsense_txt(self):
        if not self.provider.provides_biblio:
            raise SkipTest
        Provider.http_get = get_nonsense_txt
        biblio = self.provider.biblio([self.testitem_biblio])

    @nottest
    @raises(ProviderContentMalformedError)
    def test_provider_biblio_nonsense_xml(self):
        if not self.provider.provides_biblio:
            raise SkipTest
        Provider.http_get = get_nonsense_xml
        biblio = self.provider.biblio([self.testitem_biblio])
        


########NEW FILE########
__FILENAME__ = test_altmetric_com
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, assert_items_equal, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/altmetric_com")
SAMPLE_EXTRACT_ALIASES_PAGE = os.path.join(datadir, "aliases")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")

TEST_ID = "10.1101/gr.161315.113"

class TestAltmetric_Com(ProviderTestCase):

    provider_name = "altmetric_com"

    testitem_aliases = ("doi", TEST_ID)
    testitem_metrics = ("doi", TEST_ID)

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

    def test_extract_aliases_success(self):
        f = open(SAMPLE_EXTRACT_ALIASES_PAGE, "r")
        good_page = f.read()
        aliases_list = self.provider._extract_aliases(good_page)
        expected = [('altmetric_com', '1870595')]
        assert_equals(aliases_list, expected)

    def test_extract_metrics_success_twitter(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics_twitter(good_page)
        expected = {'altmetric_com:tweets': 55}
        assert_equals(metrics_dict, expected)

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("tweets", 
            [self.testitem_aliases])
        expected = ""
        assert_equals(provenance_url, expected)

        provenance_url = self.provider.provenance_url("tweets", 
            [self.testitem_aliases, ("altmetric_com", "1870595")])
        expected = 'http://www.altmetric.com/details.php?citation_id=1870595&src=impactstory.org'
        assert_equals(provenance_url, expected)

    @http
    def test_aliases(self):
        aliases = self.provider.aliases([self.testitem_aliases])
        print aliases
        expected = [('altmetric_com', '1870595')]
        assert_equals(aliases, expected)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([("altmetric_com", "1870595")])
        expected = {'altmetric_com:gplus_posts': (1, 'http://www.altmetric.com/details.php?citation_id=1870595&src=impactstory.org'), 'altmetric_com:facebook_posts': (1, 'http://www.altmetric.com/details.php?citation_id=1870595&src=impactstory.org'), 'altmetric_com:tweets': (55, 'http://www.altmetric.com/details.php?citation_id=1870595&src=impactstory.org'), 'altmetric_com:blog_posts': (2, 'http://www.altmetric.com/details.php?citation_id=1870595&src=impactstory.org')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        pass



########NEW FILE########
__FILENAME__ = test_arxiv
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact.providers import provider
from test.utils import http

import os
import collections
from nose.tools import assert_equals, assert_items_equal, raises

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/arxiv")
SAMPLE_EXTRACT_ALIASES_PAGE = os.path.join(datadir, "aliases")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE = os.path.join(datadir, "members")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

class TestArxiv(ProviderTestCase):

    provider_name = "arxiv"

    testitem_members = "arxiv:1305.3328"
    testitem_aliases = ("arxiv", "1305.3328")
    testitem_biblio = ("arxiv", "1305.3328")

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("doi", "NOT A GITHUB ID")), False)
  
    def test_members(self):
        members = self.provider.member_items({"arxiv_id_input": self.testitem_members})
        print members
        expected = [('arxiv', '1305.3328')]
        assert_equals(members, expected)

    def test_aliases(self):
        aliases = self.provider.aliases([self.testitem_aliases])
        print aliases
        expected = [('url', 'http://arxiv.org/abs/1305.3328')]
        assert_equals(aliases, expected)

    def test_extract_biblio(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        biblio = self.provider._extract_biblio(f.read(), "1305.3328")
        expected = {'repository': 'arXiv', 'title': u'Altmetrics in the wild: Using social media to explore scholarly impact', 'year': u'2012', 'free_fulltext_url': 'http://arxiv.org/abs/1305.3328', 'authors': u'Priem, Piwowar, Hemminger', 'date': u'2012-03-20T19:46:25Z'}
        print biblio
        assert_items_equal(biblio, expected)

    @http
    def test_biblio(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio])
        print biblio_dict
        expected = {'repository': 'arXiv', 'title': u'Riding the crest of the altmetrics wave: How librarians can help prepare\n  faculty for the next generation of research impact metrics', 'year': u'2013', 'free_fulltext_url': 'http://arxiv.org/abs/1305.3328', 'authors': u'Lapinski, Piwowar, Priem', 'date': u'2013-05-15T00:46:53Z'}
        assert_items_equal(biblio_dict.keys(), expected.keys())


    def test_provider_member_items_400(self):
        pass
    def test_provider_member_items_500(self):
        pass
    def test_provider_member_items_empty(self):
        pass
    def test_provider_member_items_nonsense_txt(self):
        pass
    def test_provider_member_items_nonsense_xml(self):
        pass


    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass




########NEW FILE########
__FILENAME__ = test_bibjson
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderServerError


import os, json
from nose.tools import assert_equals, raises, nottest


SAMPLE_EXTRACT_MEMBER_ITEMS_SHORT =  [
  {
           "authors": [
               {
                   "name": "H A Piwowar"
               }, 
               {
                   "name": "W W Chapman"
               }
           ], 
           "booktitle": "AMIA Annual Symposium proceedings / AMIA Symposium. AMIA Symposium,", 
           "marker": "Piwowar, Chapman, 2008", 
           "pages": "596--600", 
           "rawString": "Piwowar, H.A. and Chapman, W.W., (2008). Identifying data sharing in biomedical literature., AMIA Annual Symposium proceedings / AMIA Symposium. AMIA Symposium, pp. 596-600", 
           "title": "Identifying data sharing in biomedical literature.,", 
           "year": "2008"
       }, 
       {
           "authors": [
               {
                   "name": "H A Piwowar"
               }, 
               {
                   "name": "W W Chapman"
               }
           ], 
           "journal": "Journal of Informetrics,", 
           "marker": "Piwowar, Chapman, 2010", 
           "pages": "148--156", 
           "rawString": "Piwowar, H.A. and Chapman, W.W., (2010). Public sharing of research datasets: A pilot study of associations, Journal of Informetrics, vol. 4, no. 2, pp. 148-156", 
           "title": "Public sharing of research datasets: A pilot study of associations,", 
           "volume": "4", 
           "year": "2010"
       }
   ]



class TestBibjson(ProviderTestCase):

    provider_name = "bibjson"

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_member_items(self):
        file_contents = SAMPLE_EXTRACT_MEMBER_ITEMS_SHORT
        response = self.provider.member_items(file_contents)

        print json.dumps(response, indent=4)

        expected = [
              (
                  "biblio", 
                  {
                      "rawString": "Piwowar, H.A. and Chapman, W.W., (2008). Identifying data sharing in biomedical literature., AMIA Annual Symposium proceedings / AMIA Symposium. AMIA Symposium, pp. 596-600", 
                      "title": "AMIA Annual Symposium proceedings / AMIA Symposium. AMIA Symposium,", 
                      "first_author": "Piwowar", 
                      "booktitle": "AMIA Annual Symposium proceedings / AMIA Symposium. AMIA Symposium,", 
                      "year": "2008", 
                      "first_page": "596", 
                      "authors": "Piwowar, Chapman", 
                      "marker": "Piwowar, Chapman, 2008", 
                      "pages": "596--600"
                  }
              ), 
              (
                  "biblio", 
                  {
                      "volume": "4", 
                      "first_author": "Piwowar", 
                      "first_page": "148", 
                      "rawString": "Piwowar, H.A. and Chapman, W.W., (2010). Public sharing of research datasets: A pilot study of associations, Journal of Informetrics, vol. 4, no. 2, pp. 148-156", 
                      "authors": "Piwowar, Chapman", 
                      "marker": "Piwowar, Chapman, 2010", 
                      "journal": "Journal of Informetrics,", 
                      "title": "Public sharing of research datasets: A pilot study of associations,", 
                      "year": "2010", 
                      "pages": "148--156"
                  }
              )
          ]
        assert_equals(response, expected)


########NEW FILE########
__FILENAME__ = test_bibtex
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderServerError


import os, json
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/bibtex")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE = os.path.join(datadir, "Priem.bib")
with open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE, "r") as f:
  SAMPLE_EXTRACT_MEMBER_ITEMS_CONTENTS = f.read()

SAMPLE_PARSED_MEMBER_ITEMS_CONTENTS = [{'title': 'Scientometrics 2.0: New metrics of scholarly impact on the social Web', 'first_author': 'Priem', 'journal': 'First Monday', 'number': '7', 'volume': '15', 'first_page': '', 'authors': 'Priem, Hemminger', 'year': '2010'}, {'title': 'Data for free: Using LMS activity logs to measure community in online courses', 'first_author': 'Black', 'journal': 'The Internet and Higher Education', 'number': '2', 'volume': '11', 'first_page': '65', 'authors': 'Black, Dawson, Priem', 'year': '2008'}, {'title': 'How and why scholars cite on Twitter', 'first_author': 'Priem', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': 'Priem, Costello', 'year': '2010'}, {'title': u'Archiving scholars\u2019 tweets', 'first_author': 'COSTELLO', 'journal': '', 'number': '', 'volume': '', 'first_page': '', 'authors': 'COSTELLO, PRIEM', 'year': ''}, {'title': 'Altmetrics in the wild: An exploratory study of impact metrics based on social media', 'first_author': 'Priem', 'journal': '', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Priem, Piwowar, Hemminger', 'year': ''}, {'title': 'Frontiers: Decoupling the scholarly journal', 'first_author': 'Priem', 'journal': 'Frontiers in Computational Neuroscience', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Priem, Hemminger', 'year': ''}, {'title': 'FAIL BETTER: TOWARD A TAXONOMY OF E-LEARNING ERROR', 'first_author': 'PRIEM', 'journal': '', 'number': '', 'volume': '', 'first_page': '', 'authors': 'PRIEM', 'year': ''}, {'title': 'Shaking it up: embracing new methods for publishing, finding, discussing, and measuring our research output', 'first_author': 'Garnett', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'number': '1', 'volume': '48', 'first_page': '1', 'authors': 'Garnett, Holmberg, Pikas, Piwowar, Priem, Weber', 'year': '2011'}, {'title': 'Shaken and stirred: ASIST 2011 attendee reactions to Shaking it up: embracing new methods for publishing, finding, discussing, and measuring our research output', 'first_author': 'Garnett', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'number': '1', 'volume': '48', 'first_page': '1', 'authors': 'Garnett, Piwowar, Holmberg, Priem, Pikas, Weber', 'year': '2011'}, {'title': 'Altmetrics: a manifesto', 'first_author': 'Priem', 'journal': '', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Priem, Taraborelli, Groth, Neylon', 'year': '2010'}, {'title': 'Altmetrics in the wild: Using social media to explore scholarly impact', 'first_author': 'Priem', 'journal': 'arXiv preprint arXiv:1203.4745', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Priem, Piwowar, Hemminger', 'year': '2012'}, {'title': 'Uncovering impacts: a case study in using altmetrics tools', 'first_author': 'Priem', 'journal': 'Workshop on the Semantic Publishing (SePublica 2012) 9 th Extended Semantic Web Conference Hersonissos, Crete, Greece, May 28, 2012', 'number': '', 'volume': '', 'first_page': '40', 'authors': 'Priem, Parra, Piwowar, Groth, Waagmeester', 'year': '2012'}, {'title': "Beyond citations: Scholars' visibility on the social Web", 'first_author': 'Bar-Ilan', 'journal': 'arXiv preprint arXiv:1205.5611', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Bar-Ilan, Haustein, Peters, Priem, Shema, Terliesner', 'year': '2012'}, {'title': 'The Altmetrics Collection', 'first_author': 'Priem', 'journal': 'PloS one', 'number': '11', 'volume': '7', 'first_page': 'e48753', 'authors': 'Priem, Groth, Taraborelli', 'year': '2012'}, {'title': 'Information visualization state of the art and future directions', 'first_author': u'Milojevi\u0107', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'number': '1', 'volume': '49', 'first_page': '1', 'authors': u'Milojevi\u0107, Hemminger, Priem, Chen, Leydesdorff, Weingart', 'year': '2012'}]

SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE2 = os.path.join(datadir, "Piwowar.bib")
with open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE2, "r") as f:
  SAMPLE_EXTRACT_MEMBER_ITEMS_CONTENTS2 = f.read()

SAMPLE_EXTRACT_MEMBER_ITEMS_UNICODE = r"""
@article{oberg2003luftutslapp,
  title={Luftutsl{\"a}pp av organiska milj{\"o}gifter fr{\aa}n ljusb{\aa}gsugnar: F{\"o}rekomst och m{\"o}jliga {\aa}tg{\"a}rder f{\"o}r att minska milj{\"o}p{\aa}verkan},
  author={{\"O}berg, T.},
  year={2003},
  publisher={Jernkontoret,}
}
"""

SAMPLE_EXTRACT_MEMBER_ITEMS_BROKEN = """
@article{test1,
  year={2009},
},
@article{test2,
  year={2009},
  test={no closing
}
"""

SAMPLE_EXTRACT_MEMBER_ITEMS_ARXIV = """
@article{priem2012altmetrics,
  title={Altmetrics in the wild: Using social media to explore scholarly impact},
  author={Priem, J. and Piwowar, H.A. and Hemminger, B.M.},
  journal={arXiv preprint arXiv:1203.4745},
  year={2012}
}
"""

SAMPLE_EXTRACT_MEMBER_ITEMS_SHORT = """
@phdthesis{rogers2006identity,
  title={Identity crisis| Modernity and fragmentation},
  author={Rogers, K.L.},
  year={2006},
  school={UNIVERSITY OF COLORADO AT BOULDER}
}

@article{rogers2008affirming,
  title={Affirming Complexity:" White Teeth" and Cosmopolitanism},
  author={Rogers, K.},
  journal={Interdisciplinary Literary Studies},
  volume={9},
  number={2},
  pages={45--61},
  year={2008},
  publisher={Penn State Altoona}
}

@phdthesis{rogers2010trauma,
  title={Trauma and the representation of the unsayable in late twentieth-century fiction},
  author={Rogers, K.L.},
  year={2010},
  school={UNIVERSITY OF COLORADO AT BOULDER}
}
"""



class TestBibtex(ProviderTestCase):

    provider_name = "bibtex"
    testitem_members = "egonw"

    def setUp(self):
        ProviderTestCase.setUp(self)
        
    def test_parse_none(self):
        file_contents = ""
        response = self.provider.parse(file_contents)
        assert_equals(len(response), 0)

    def test_parse_short(self):
        file_contents = SAMPLE_EXTRACT_MEMBER_ITEMS_SHORT
        response = self.provider.parse(file_contents)
        print response
        assert_equals(len(response), 3)

    def test_paginate_unicode(self):
        file_contents = SAMPLE_EXTRACT_MEMBER_ITEMS_UNICODE
        response = self.provider.parse(file_contents)
        print response
        expected = [{'title': u'Luftutsl\xe4pp av organiska milj\xf6gifter fr\xe5n ljusb\xe5gsugnar: F\xf6rekomst och m\xf6jliga \xe5tg\xe4rder f\xf6r att minska milj\xf6p\xe5verkan', 'first_author': u'\xd6berg', 'journal': '', 'year': '2003', 'number': '', 'volume': '', 'first_page': '', 'authors': u'\xd6berg'}]
        assert_equals(response, expected)

    def test_parse_long(self):
        file_contents = SAMPLE_EXTRACT_MEMBER_ITEMS_CONTENTS
        response = self.provider.parse(file_contents)

        # make sure it's json-serializable
        json_str = json.dumps(response)
        assert_equals(json_str[0:3], '[{"')

        assert_equals(len(response), 15) # 15 total articles
        print response
        assert_equals(response, SAMPLE_PARSED_MEMBER_ITEMS_CONTENTS)


    def test_parse_long2(self):
        file_contents = SAMPLE_EXTRACT_MEMBER_ITEMS_CONTENTS2
        response = self.provider.parse(file_contents)

        assert_equals(len(response), 40) # 40 total articles
        print response
        expected = [{'title': u'Sharing detailed research data is associated with increased citation rate', 'first_author': u'Piwowar', 'journal': u'PLoS One', 'year': '2007', 'number': '3', 'volume': '2', 'first_page': 'e308', 'authors': u'Piwowar, Day, Fridsma'}, {'title': u'Towards a data sharing culture: recommendations for leadership from academic health centers', 'first_author': u'Piwowar', 'journal': u'PLoS medicine', 'year': '2008', 'number': '9', 'volume': '5', 'first_page': 'e183', 'authors': u'Piwowar, Becich, Bilofsky, Crowley'}, {'title': u'A review of journal policies for sharing research data', 'first_author': u'Piwowar', 'journal': '', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Chapman'}, {'title': u'Public sharing of research datasets: A pilot study of associations', 'first_author': u'Piwowar', 'journal': u'Journal of informetrics', 'year': '2010', 'number': '2', 'volume': '4', 'first_page': '148', 'authors': u'Piwowar, Chapman'}, {'title': u'Identifying data sharing in biomedical literature', 'first_author': u'Piwowar', 'journal': '', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Chapman'}, {'title': u'Recall and bias of retrieving gene expression microarray datasets through PubMed identifiers', 'first_author': u'Piwowar', 'journal': u'Journal of Biomedical Discovery and Collaboration', 'year': '2010', 'number': '', 'volume': '5', 'first_page': '7', 'authors': u'Piwowar, Chapman'}, {'title': u'Foundational studies for measuring the impact, prevalence, and patterns of publicly sharing biomedical research data', 'first_author': u'Piwowar', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar'}, {'title': u'Envisioning a biomedical data reuse registry.', 'first_author': u'Piwowar', 'journal': u'AMIA... Annual Symposium proceedings/AMIA Symposium. AMIA Symposium', 'year': '2008', 'number': '', 'volume': '', 'first_page': '1097', 'authors': u'Piwowar, Chapman'}, {'title': u'Using open access literature to guide full-text query formulation', 'first_author': u'Piwowar', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Chapman'}, {'title': u'Linking database submissions to primary citations with PubMed Central', 'first_author': u'Piwowar', 'journal': u'BioLINK Workshop at ISMB', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Chapman'}, {'title': u'Data archiving is a good investment', 'first_author': u'Piwowar', 'journal': u'Nature', 'year': '2011', 'number': '7347', 'volume': '473', 'first_page': '285', 'authors': u'Piwowar, Vision, Whitlock'}, {'title': u'Prevalence and patterns of microarray data sharing', 'first_author': u'Piwowar', 'journal': '', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Chapman'}, {'title': u'Formulating MEDLINE queries for article retrieval based on PubMed exemplars', 'first_author': u'Garnett', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Garnett, Piwowar, Rasmussen, Illes'}, {'title': u"Who Shares? Who Doesn't? Factors Associated with Openly Archiving Raw Research Data", 'first_author': u'Piwowar', 'journal': u'PloS one', 'year': '2011', 'number': '7', 'volume': '6', 'first_page': 'e18657', 'authors': u'Piwowar'}, {'title': u'Examining the uses of shared data', 'first_author': u'Piwowar', 'journal': '', 'year': '2007', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Fridsma'}, {'title': u'Data from: Sharing detailed research data is associated with increased citation rate', 'first_author': u'Piwowar', 'journal': '', 'year': '2007', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Day, Fridsma'}, {'title': u'FOUNDATIONAL STUDIES FOR MEASURINGTHE IMPACT, PREVALENCE, AND PATTERNSOF PUBLICLY SHARING BIOMEDICAL RESEARCH DATA', 'first_author': u'Piwowar', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar'}, {'title': u'Biology needs a modern assessment system for professional productivity', 'first_author': u'McDade', 'journal': u'BioScience', 'year': '2011', 'number': '8', 'volume': '61', 'first_page': '619', 'authors': u'McDade, Maddison, Guralnick, Piwowar, Jameson, Helgen, Herendeen, Hill, Vis'}, {'title': u'Altmetrics in the wild: An exploratory study of impact metrics based on social media', 'first_author': u'Priem', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Priem, Piwowar, Hemminger'}, {'title': u'A method to track dataset reuse in biomedicine: filtered GEO accession numbers in PubMed Central', 'first_author': u'Piwowar', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': u'Piwowar'}, {'title': u'Proposed Foundations for Evaluating Data Sharing and Reuse in the Biomedical Literature', 'first_author': u'Piwowar', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar'}, {'title': u'Data citation in the wild', 'first_author': u'Enriquez', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Enriquez, Judson, Walker, Allard, Cook, Piwowar, Sandusky, Vision, Wilson'}, {'title': u'Envisioning a data reuse registry', 'first_author': u'Piwowar', 'journal': '', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Chapman'}, {'title': u'Data from: Public sharing of research datasets: a pilot study of associations', 'first_author': u'Piwowar', 'journal': '', 'year': '2009', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Chapman'}, {'title': u'Uncovering impacts: CitedIn and total-impact, two new tools for gathering altmetrics.', 'first_author': u'Priem', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Priem, Parra, Waagmeester, Piwowar'}, {'title': u"Who shares? Who doesn't? Bibliometric factors associated with open archiving of biomedical datasets", 'first_author': u'Piwowar', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': u'Piwowar'}, {'title': u'PhD Thesis: Foundational studies for measuring the impact, prevalence, and patterns of publicly sharing biomedical research data', 'first_author': u'Piwowar', 'journal': u'Database', 'year': '2010', 'number': '3', 'volume': '25', 'first_page': '27', 'authors': u'Piwowar'}, {'title': u'Data from: Data archiving is a good investment', 'first_author': u'Piwowar', 'journal': '', 'year': '2011', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Vision, Whitlock'}, {'title': u'Expediting medical literature coding with query-building', 'first_author': u'Garnett', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': u'Garnett, Piwowar, Rasmussen, Illes'}, {'title': u'Neuroethics and fMRI: Mapping a Fledgling Relationship', 'first_author': u'Garnett', 'journal': u'PloS one', 'year': '2011', 'number': '4', 'volume': '6', 'first_page': 'e18537', 'authors': u'Garnett, Whiteley, Piwowar, Rasmussen, Illes'}, {'title': u'Beginning to track 1000 datasets from public repositories into the published literature', 'first_author': u'Piwowar', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Carlson, Vision'}, {'title': u'Evaluating data citation and sharing policies in the environmental sciences', 'first_author': u'Weber', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': u'Weber, Piwowar, Vision'}, {'title': u'Data from: Who shares? Who doesn\u2019t? Factors associated with openly archiving raw research data', 'first_author': u'Piwowar', 'journal': '', 'year': '2011', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar'}, {'title': u'Shaking it up: embracing new methods for publishing, finding, discussing, and measuring our research output', 'first_author': u'Garnett', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2011', 'number': '1', 'volume': '48', 'first_page': '1', 'authors': u'Garnett, Holmberg, Pikas, Piwowar, Priem, Weber'}, {'title': u'Shaken and stirred: ASIST 2011 attendee reactions to Shaking it up: embracing new methods for publishing, finding, discussing, and measuring our research output', 'first_author': u'Garnett', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2011', 'number': '1', 'volume': '48', 'first_page': '1', 'authors': u'Garnett, Piwowar, Holmberg, Priem, Pikas, Weber'}, {'title': u'Altmetrics in the wild: Using social media to explore scholarly impact', 'first_author': u'Priem', 'journal': u'arXiv preprint arXiv:1203.4745', 'year': '2012', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Priem, Piwowar, Hemminger'}, {'title': u'Why Are There So Few Efforts to Text Mine the Open Access Subset of PubMed Central?', 'first_author': u'Piwowar', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar'}, {'title': u'Uncovering impacts: a case study in using altmetrics tools', 'first_author': u'Priem', 'journal': u'Workshop on the Semantic Publishing (SePublica 2012) 9 th Extended Semantic Web Conference Hersonissos, Crete, Greece, May 28, 2012', 'year': '2012', 'number': '', 'volume': '', 'first_page': '40', 'authors': u'Priem, Parra, Piwowar, Groth, Waagmeester'}, {'title': u'Uncovering the impact story of open research', 'first_author': u'Piwowar', 'journal': '', 'year': '2012', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar'}, {'title': u'Altmetrics: Value all research products', 'first_author': u'Piwowar', 'journal': u'Nature', 'year': '2013', 'number': '7431', 'volume': '493', 'first_page': '159', 'authors': u'Piwowar'}]
        assert_equals(response, expected)

    # check it doesn't throw an error
    def test_paginate_broken(self):
        file_contents = SAMPLE_EXTRACT_MEMBER_ITEMS_BROKEN
        response = self.provider.parse(file_contents)
        print response
        expected = [{'title': '', 'first_author': '', 'journal': '', 'year': '2009', 'number': '', 'volume': '', 'first_page': '', 'authors': ''}]
        assert_equals(response, expected)

    def test_member_items_arxiv(self):
        file_contents = SAMPLE_EXTRACT_MEMBER_ITEMS_ARXIV
        response = self.provider.member_items(file_contents)
        print response
        expected = [('arxiv', u'1203.4745')]
        assert_equals(response, expected)

    def test_member_items(self):
        file_contents = SAMPLE_EXTRACT_MEMBER_ITEMS_CONTENTS
        response = self.provider.member_items(file_contents)
        print response

        expected = [('biblio', {'title': u'Scientometrics 2.0: New metrics of scholarly impact on the social Web', 'first_author': u'Priem', 'journal': u'First Monday', 'year': '2010', 'number': '7', 'volume': '15', 'first_page': '', 'authors': u'Priem, Hemminger'}), ('biblio', {'title': u'Data for free: Using LMS activity logs to measure community in online courses', 'first_author': u'Black', 'journal': u'The Internet and Higher Education', 'year': '2008', 'number': '2', 'volume': '11', 'first_page': '65', 'authors': u'Black, Dawson, Priem'}), ('biblio', {'title': u'How and why scholars cite on Twitter', 'first_author': u'Priem', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': u'Priem, Costello'}), ('biblio', {'title': u'Archiving scholars\u2019 tweets', 'first_author': u'COSTELLO', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': u'COSTELLO, PRIEM'}), ('biblio', {'title': u'Altmetrics in the wild: An exploratory study of impact metrics based on social media', 'first_author': u'Priem', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Priem, Piwowar, Hemminger'}), ('biblio', {'title': u'Frontiers: Decoupling the scholarly journal', 'first_author': u'Priem', 'journal': u'Frontiers in Computational Neuroscience', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Priem, Hemminger'}), ('biblio', {'title': u'FAIL BETTER: TOWARD A TAXONOMY OF E-LEARNING ERROR', 'first_author': u'PRIEM', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': u'PRIEM'}), ('biblio', {'title': u'Shaking it up: embracing new methods for publishing, finding, discussing, and measuring our research output', 'first_author': u'Garnett', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2011', 'number': '1', 'volume': '48', 'first_page': '1', 'authors': u'Garnett, Holmberg, Pikas, Piwowar, Priem, Weber'}), ('biblio', {'title': u'Shaken and stirred: ASIST 2011 attendee reactions to Shaking it up: embracing new methods for publishing, finding, discussing, and measuring our research output', 'first_author': u'Garnett', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2011', 'number': '1', 'volume': '48', 'first_page': '1', 'authors': u'Garnett, Piwowar, Holmberg, Priem, Pikas, Weber'}), ('biblio', {'title': u'Altmetrics: a manifesto', 'first_author': u'Priem', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Priem, Taraborelli, Groth, Neylon'}), ('arxiv', u'1203.4745'), ('biblio', {'title': u'Uncovering impacts: a case study in using altmetrics tools', 'first_author': u'Priem', 'journal': u'Workshop on the Semantic Publishing (SePublica 2012) 9 th Extended Semantic Web Conference Hersonissos, Crete, Greece, May 28, 2012', 'year': '2012', 'number': '', 'volume': '', 'first_page': '40', 'authors': u'Priem, Parra, Piwowar, Groth, Waagmeester'}), ('arxiv', u'1205.5611'), ('biblio', {'title': u'The Altmetrics Collection', 'first_author': u'Priem', 'journal': u'PloS one', 'year': '2012', 'number': '11', 'volume': '7', 'first_page': 'e48753', 'authors': u'Priem, Groth, Taraborelli'}), ('biblio', {'title': u'Information visualization state of the art and future directions', 'first_author': u'Milojevi\u0107', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2012', 'number': '1', 'volume': '49', 'first_page': '1', 'authors': u'Milojevi\u0107, Hemminger, Priem, Chen, Leydesdorff, Weingart'})]
        assert_equals(response, expected)


########NEW FILE########
__FILENAME__ = test_blog_post
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
import json
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestBlog_post(ProviderTestCase):

    provider_name = "blog_post"

    testitem_aliases = ('blog_post', json.dumps({"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com"}))
    testitem_aliases_not_wordpress_com = ('blog_post', json.dumps({"post_url": "http://jasonpriem.org/2012/05/toward-a-second-revolution-altmetrics-total-impact-and-the-decoupled-journal-video/", "blog_url": "http://jasonpriem.org/blog"}))
    api_key = os.environ["WORDPRESS_OUR_BLOG_API_KEY"]

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix/status/400821465828061184")), False)


    @http
    def test_aliases_wordpress_com(self):
        response = self.provider.aliases([self.testitem_aliases])
        print response
        expected = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        assert_equals(response, expected)

    @http
    def test_aliases_not_wordpress(self):
        response = self.provider.aliases([('blog_post', json.dumps({"post_url": "http://jasonpriem.com/cv", "blog_url": "jasonpriem.com"}))])
        print response
        expected = [('url', u'http://jasonpriem.com/cv')]
        assert_equals(response, expected)


    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_not_wordpress2(self):
        test_alias = ('blog_post', "{\"post_url\": \"http://researchremix.wordpress.com/2011/08/10/personal\", \"blog_url\": \"http://researchremix.wordpress.com\"}")
        response = self.provider.biblio([test_alias])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2011/08/10/personal', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Cancer data: it just got personal'}
        assert_equals(response, expected)

    @http
    @nottest
    def test_metrics(self):
        wordpress_aliases = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        response = self.provider.metrics(wordpress_aliases)
        print response
        expected = {}
        assert_equals(response, expected)


    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        pass
########NEW FILE########
__FILENAME__ = test_citeulike
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact.views import app

import os
import collections
from nose.tools import assert_equals, raises, nottest
from test.utils import http

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/citeulike")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")

TEST_DOI = "10.1371/journal.pcbi.1000361"

class TestCiteulike(ProviderTestCase):

    provider_name = "citeulike"

    testitem_aliases = ("doi", TEST_DOI)
    testitem_metrics = ("doi", TEST_DOI)

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics(good_page)
        expected = {'citeulike:bookmarks': 5}
        assert_equals(metrics_dict, expected)

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("bookmarks", 
            [self.testitem_aliases])
        expected = 'http://www.citeulike.org/doi/10.1371/journal.pcbi.1000361'
        assert_equals(provenance_url, expected)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'citeulike:bookmarks': (59, 'http://www.citeulike.org/doi/10.1371/journal.pcbi.1000361')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]


########NEW FILE########
__FILENAME__ = test_crossref
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact.providers import provider
from totalimpact import app, db
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests

from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/crossref")
SAMPLE_EXTRACT_ALIASES_PAGE = os.path.join(datadir, "aliases")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

TEST_DOI = "10.1371/journal.pcbi.1000361"

class TestCrossRef(ProviderTestCase):

    provider_name = "crossref"

    testitem_aliases = ("doi", TEST_DOI)
    testitem_biblio = ("doi", TEST_DOI)
    testitem_members = "10.123/example\ndoi:10.456/example2\nhttp://doi.org/10.2342/example3"

    def setUp(self):
        ProviderTestCase.setUp(self)
        self.db = setup_postgres_for_unittests(db, app)
        
    def tearDown(self):
        teardown_postgres_for_unittests(self.db)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("github", "NOT A CROSSREF ID")), False)
  
    def test_extract_biblio(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        biblio = self.provider._extract_biblio(f.read())
        expected = {'title': 'Adventures in Semantic Publishing: Exemplar Semantic Enhancements of a Research Article', 'journal': 'PLoS Computational Biology', 'year': '2009', 'repository': 'Public Library of Science', 'authors': 'Shotton, Portwin, Klyne, Miles'}
        print biblio
        assert_equals(biblio, expected)

    def test_member_items(self):
        ret = self.provider.member_items(self.testitem_members)
        expected = [('doi', '10.123/example'), ('doi', '10.456/example2'), ('doi', '10.2342/example3')]
        assert_equals(ret, expected)        

    def test_provider_member_items_400(self):
        pass
    def test_provider_member_items_500(self):
        pass
    def test_provider_member_items_empty(self):
        pass
    def test_provider_member_items_nonsense_txt(self):
        pass
    def test_provider_member_items_nonsense_xml(self):
        pass

    @http
    def test_get_aliases_for_id(self):
        self.db = setup_postgres_for_unittests(db, app)
        new_aliases = self.provider.aliases([self.testitem_aliases])
        print new_aliases
        expected = [('biblio', {'issn': u'15537358', 'repository': u'Public Library of Science (PLoS)', 'title': u'Adventures in Semantic Publishing: Exemplar Semantic Enhancements of a Research Article', 'journal': u'PLoS Computational Biology', 'year': '2009', 'free_fulltext_url': 'http://dx.doi.org/10.1371/journal.pcbi.1000361', 'authors': u'Shotton, Portwin, Klyne, Miles'}), ('url', 'http://dx.doi.org/10.1371/journal.pcbi.1000361'), ('url', u'http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1000361')]
        assert_equals(sorted(new_aliases), sorted(expected))

    @http
    def test_biblio_elife(self):
        biblio = self.provider.biblio([("doi", "10.7554/eLife.00048")])
        expected = {'issn': u'2050084X', 'repository': u'eLife Sciences Publications, Ltd.', 'title': u'The unfolded protein response in fission yeast modulates stability of select mRNAs to maintain protein homeostasis', 'journal': u'eLife', 'year': '2012', 'authors': u'Kimmig, Diaz, Zheng, Williams, Lang, Aragon, Li, Walter'}

        print biblio
        assert_equals(biblio, expected)        

    @http
    def test_biblio_dryad(self):
        biblio = self.provider.biblio([("doi", "10.5061/dryad.3td2f")])
        expected = {'title': u'Data from: Public sharing of research datasets: a pilot study of associations', 'year': u'2011', 'repository': u'Dryad Digital Repository', 'authors_literal': u'Piwowar, Heather A.; Chapman, Wendy W.'}
        print biblio
        assert_equals(biblio, expected) 

    @http
    def test_biblio_pangaea(self):
        biblio = self.provider.biblio([("doi", "10.1594/PANGAEA.339110")])
        expected = {'title': u"Audio record of a 'singing iceberg' from the Weddell Sea, Antarctica, supplement to: M\xfcller, Christian; Schlindwein, Vera; Eckstaller, Alfons; Miller, Heinz (2005): Singing Icebergs. Science, 310(5752), 1299", 'authors_literal': u'M\xfcller, Christian; Schlindwein, Vera; Eckstaller, Alfons; Miller, Heinz', 'repository': u'PANGAEA - Data Publisher for Earth & Environmental Science', 'year': '2005'}
        print biblio
        assert_equals(biblio, expected) 

    @http
    def test_biblio_science(self):
        biblio = self.provider.biblio([("doi", "10.1126/science.169.3946.635")])
        expected = {'issn': u'00368075', 'repository': u'American Association for the Advancement of Science (AAAS)', 'title': u'The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance', 'journal': u'Science', 'year': '1970', 'authors': u'Frank'}
        print biblio
        assert_equals(biblio, expected) 

    @http
    def test_biblio(self):
        #setup_postgres_for_unittests(self.db, app)
        biblio = self.provider.biblio([self.testitem_biblio])
        expected = {'issn': u'15537358', 'repository': u'Public Library of Science (PLoS)', 'title': u'Adventures in Semantic Publishing: Exemplar Semantic Enhancements of a Research Article', 'journal': u'PLoS Computational Biology', 'year': '2009', 'free_fulltext_url': 'http://dx.doi.org/10.1371/journal.pcbi.1000361', 'authors': u'Shotton, Portwin, Klyne, Miles'}
        print biblio
        assert_equals(biblio, expected)        

    @http
    def test_aliases_elife(self):
        aliases = self.provider.aliases([("doi", "10.7554/eLife.00048")])
        expected = [('biblio', {'issn': u'2050084X', 'repository': u'eLife Sciences Publications, Ltd.', 'title': u'The unfolded protein response in fission yeast modulates stability of select mRNAs to maintain protein homeostasis', 'journal': u'eLife', 'year': '2012', 'authors': u'Kimmig, Diaz, Zheng, Williams, Lang, Aragon, Li, Walter'}), ('url', 'http://dx.doi.org/10.7554/eLife.00048'), ('url', u'http://elife.elifesciences.org/content/1/e00048')]
        print aliases
        assert_equals(sorted(aliases), sorted(expected))

    @http
    def test_aliases_elife_table(self):
        aliases = self.provider.aliases([("doi", "10.7554/eLife.00048.020")])
        expected = [('biblio', {'repository': u'eLife Sciences Publications, Ltd.'}), ('url', 'http://dx.doi.org/10.7554/eLife.00048.020'), ('url', u'http://elife.elifesciences.org/content/1/e00048/T1')]
        print aliases
        assert_equals(sorted(aliases), sorted(expected))

    @http
    def test_aliases_from_url(self):
        aliases = self.provider.aliases([("url", "http://dx.doi.org/10.7554/eLife.00048.020")])
        expected = [('biblio', {'repository': u'eLife Sciences Publications, Ltd.'}), ('doi', '10.7554/eLife.00048.020'), ('url', 'http://dx.doi.org/10.7554/eLife.00048.020'), ('url', u'http://elife.elifesciences.org/content/1/e00048/T1')]
        print aliases
        assert_equals(sorted(aliases), sorted(expected))

    @http
    def test_aliases_bad_title(self):
        aliases = self.provider.aliases([("doi", "10.1021/np070361t")])
        expected = [('biblio', {'issn': u'01633864', 'repository': u'American Chemical Society (ACS)', 'title': u' 13 C\u2212 15 N Correlation via Unsymmetrical Indirect Covariance NMR: Application to Vinblastine ', 'journal': u'J. Nat. Prod.', 'year': '2007', 'authors': u'Martin, Hilton, Blinov, Williams'}), ('url', 'http://dx.doi.org/10.1021/np070361t'), ('url', u'http://pubs.acs.org/doi/abs/10.1021/np070361t')]
        print aliases
        assert_equals(sorted(aliases), sorted(expected))  

    @http
    def test_lookup_dois_from_biblio(self):
        biblio = {"first_author": "Piwowar", "journal": "PLoS medicine", "number": "9", "volume": "5", "first_page": "e183", "key": "piwowar2008towards", "year": "2008"}
        doi = self.provider._lookup_doi_from_biblio(biblio, True)
        assert_equals(doi, u'10.1371/journal.pmed.0050183')

    @http
    def test_aliases_from_biblio(self):
        biblio = {"first_author": "Piwowar", "journal": "PLoS medicine", "number": "9", "volume": "5", "first_page": "e183", "key": "piwowar2008towards", "year": "2008"}
        response = self.provider.aliases([("biblio", biblio)])
        print response
        expected = [('biblio', {'issn': u'15491277', 'repository': u'Public Library of Science (PLoS)', 'title': u'Towards a Data Sharing Culture: Recommendations for Leadership from Academic Health Centers', 'journal': u'Plos Med', 'year': '2008', 'authors': u'Piwowar, Becich, Bilofsky, Crowley'}), ('doi', u'10.1371/journal.pmed.0050183'), ('url', u'http://dx.doi.org/10.1371/journal.pmed.0050183'), ('url', u'http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0050183')]
        assert_equals(response, expected)

    @http
    def test_aliases_from_biblio2(self):
        biblio = {'title': 'Evaluating data citation and sharing policies in the environmental sciences', 'first_author': 'Weber', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': 'Weber, Piwowar, Vision'}
        response = self.provider.aliases([("biblio", biblio)])
        print response
        expected = [('biblio', {'issn': u'00447870', 'repository': u'Wiley-Blackwell', 'title': u'Evaluating data citation and sharing policies in the environmental sciences', 'journal': u'Proc. Am. Soc. Info. Sci. Tech.', 'year': '2010', 'authors': u'Weber, Piwowar, Vision'}), ('doi', u'10.1002/meet.14504701445'), ('url', u'http://dx.doi.org/10.1002/meet.14504701445'), ('url', u'http://onlinelibrary.wiley.com/doi/10.1002/meet.14504701445/abstract')]
        assert_equals(response, expected)

    @http
    @raises(provider.ProviderServerError)
    def test_aliases_from_biblio3(self):
        biblio = {u'month': u'Dec', u'title': u'[Maternity in adolescence: obstetrical analysis and review of the influence of cultural, socioeconomic and psychological factors in a retrospective study of 62 cases].', u'year': 2002, u'authors': u'Faucher, Dappe, Madelenat', u'journal': u'Gyn\xe9cologie, obst\xe9trique & fertilit\xe9'}
        response = self.provider.aliases([("biblio", biblio)])
        print response
        expected = []
        assert_equals(response, expected)

    @http
    @nottest #so slow
    def test_aliases_from_biblio_multi(self):
        biblios = [{'title': 'Sharing detailed research data is associated with increased citation rate', 'first_author': 'Piwowar', 'journal': 'PLoS One', 'year': '2007', 'number': '3', 'volume': '2', 'first_page': 'e308', 'authors': 'Piwowar, Day, Fridsma'}, {'title': 'Towards a data sharing culture: recommendations for leadership from academic health centers', 'first_author': 'Piwowar', 'journal': 'PLoS medicine', 'year': '2008', 'number': '9', 'volume': '5', 'first_page': 'e183', 'authors': 'Piwowar, Becich, Bilofsky, Crowley'}, {'title': 'A review of journal policies for sharing research data', 'first_author': 'Piwowar', 'journal': '', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Chapman'}, {'title': 'Public sharing of research datasets: A pilot study of associations', 'first_author': 'Piwowar', 'journal': 'Journal of informetrics', 'year': '2010', 'number': '2', 'volume': '4', 'first_page': '148', 'authors': 'Piwowar, Chapman'}, {'title': 'Identifying data sharing in biomedical literature', 'first_author': 'Piwowar', 'journal': '', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Chapman'}, {'title': 'Recall and bias of retrieving gene expression microarray datasets through PubMed identifiers', 'first_author': 'Piwowar', 'journal': 'Journal of Biomedical Discovery and Collaboration', 'year': '2010', 'number': '', 'volume': '5', 'first_page': '7', 'authors': 'Piwowar, Chapman'}, {'title': 'Foundational studies for measuring the impact, prevalence, and patterns of publicly sharing biomedical research data', 'first_author': 'Piwowar', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar'}, {'title': 'Envisioning a biomedical data reuse registry.', 'first_author': 'Piwowar', 'journal': 'AMIA... Annual Symposium proceedings/AMIA Symposium. AMIA Symposium', 'year': '2008', 'number': '', 'volume': '', 'first_page': '1097', 'authors': 'Piwowar, Chapman'}, {'title': 'Using open access literature to guide full-text query formulation', 'first_author': 'Piwowar', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Chapman'}, {'title': 'Linking database submissions to primary citations with PubMed Central', 'first_author': 'Piwowar', 'journal': 'BioLINK Workshop at ISMB', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Chapman'}, {'title': 'Data archiving is a good investment', 'first_author': 'Piwowar', 'journal': 'Nature', 'year': '2011', 'number': '7347', 'volume': '473', 'first_page': '285', 'authors': 'Piwowar, Vision, Whitlock'}, {'title': 'Prevalence and patterns of microarray data sharing', 'first_author': 'Piwowar', 'journal': '', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Chapman'}, {'title': 'Formulating MEDLINE queries for article retrieval based on PubMed exemplars', 'first_author': 'Garnett', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Garnett, Piwowar, Rasmussen, Illes'}, {'title': "Who Shares? Who Doesn't? Factors Associated with Openly Archiving Raw Research Data", 'first_author': 'Piwowar', 'journal': 'PloS one', 'year': '2011', 'number': '7', 'volume': '6', 'first_page': 'e18657', 'authors': 'Piwowar'}, {'title': 'Examining the uses of shared data', 'first_author': 'Piwowar', 'journal': '', 'year': '2007', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Fridsma'}, {'title': 'Data from: Sharing detailed research data is associated with increased citation rate', 'first_author': 'Piwowar', 'journal': '', 'year': '2007', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Day, Fridsma'}, {'title': 'FOUNDATIONAL STUDIES FOR MEASURINGTHE IMPACT, PREVALENCE, AND PATTERNSOF PUBLICLY SHARING BIOMEDICAL RESEARCH DATA', 'first_author': 'Piwowar', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar'}, {'title': 'Biology needs a modern assessment system for professional productivity', 'first_author': 'McDade', 'journal': 'BioScience', 'year': '2011', 'number': '8', 'volume': '61', 'first_page': '619', 'authors': 'McDade, Maddison, Guralnick, Piwowar, Jameson, Helgen, Herendeen, Hill, Vis'}, {'title': 'Altmetrics in the wild: An exploratory study of impact metrics based on social media', 'first_author': 'Priem', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Priem, Piwowar, Hemminger'}, {'title': 'A method to track dataset reuse in biomedicine: filtered GEO accession numbers in PubMed Central', 'first_author': 'Piwowar', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': 'Piwowar'}, {'title': 'Proposed Foundations for Evaluating Data Sharing and Reuse in the Biomedical Literature', 'first_author': 'Piwowar', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar'}, {'title': 'Data citation in the wild', 'first_author': 'Enriquez', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Enriquez, Judson, Walker, Allard, Cook, Piwowar, Sandusky, Vision, Wilson'}, {'title': 'Envisioning a data reuse registry', 'first_author': 'Piwowar', 'journal': '', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Chapman'}, {'title': 'Data from: Public sharing of research datasets: a pilot study of associations', 'first_author': 'Piwowar', 'journal': '', 'year': '2009', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Chapman'}, {'title': 'Uncovering impacts: CitedIn and total-impact, two new tools for gathering altmetrics.', 'first_author': 'Priem', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Priem, Parra, Waagmeester, Piwowar'}, {'title': "Who shares? Who doesn't? Bibliometric factors associated with open archiving of biomedical datasets", 'first_author': 'Piwowar', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': 'Piwowar'}, {'title': 'PhD Thesis: Foundational studies for measuring the impact, prevalence, and patterns of publicly sharing biomedical research data', 'first_author': 'Piwowar', 'journal': 'Database', 'year': '2010', 'number': '3', 'volume': '25', 'first_page': '27', 'authors': 'Piwowar'}, {'title': 'Data from: Data archiving is a good investment', 'first_author': 'Piwowar', 'journal': '', 'year': '2011', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Vision, Whitlock'}, {'title': 'Expediting medical literature coding with query-building', 'first_author': 'Garnett', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': 'Garnett, Piwowar, Rasmussen, Illes'}, {'title': 'Neuroethics and fMRI: Mapping a Fledgling Relationship', 'first_author': 'Garnett', 'journal': 'PloS one', 'year': '2011', 'number': '4', 'volume': '6', 'first_page': 'e18537', 'authors': 'Garnett, Whiteley, Piwowar, Rasmussen, Illes'}, {'title': 'Beginning to track 1000 datasets from public repositories into the published literature', 'first_author': 'Piwowar', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar, Carlson, Vision'}, {'title': 'Evaluating data citation and sharing policies in the environmental sciences', 'first_author': 'Weber', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'number': '1', 'volume': '47', 'first_page': '1', 'authors': 'Weber, Piwowar, Vision'}, {'title': 'Data from: Who shares? Who doesn\xe2\x80\x99t? Factors associated with openly archiving raw research data', 'first_author': 'Piwowar', 'journal': '', 'year': '2011', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar'}, {'title': 'Shaking it up: embracing new methods for publishing, finding, discussing, and measuring our research output', 'first_author': 'Garnett', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'year': '2011', 'number': '1', 'volume': '48', 'first_page': '1', 'authors': 'Garnett, Holmberg, Pikas, Piwowar, Priem, Weber'}, {'title': 'Shaken and stirred: ASIST 2011 attendee reactions to Shaking it up: embracing new methods for publishing, finding, discussing, and measuring our research output', 'first_author': 'Garnett', 'journal': 'Proceedings of the American Society for Information Science and Technology', 'year': '2011', 'number': '1', 'volume': '48', 'first_page': '1', 'authors': 'Garnett, Piwowar, Holmberg, Priem, Pikas, Weber'}, {'title': 'Altmetrics in the wild: Using social media to explore scholarly impact', 'first_author': 'Priem', 'journal': 'arXiv preprint arXiv:1203.4745', 'year': '2012', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Priem, Piwowar, Hemminger'}, {'title': 'Why Are There So Few Efforts to Text Mine the Open Access Subset of PubMed Central?', 'first_author': 'Piwowar', 'journal': '', 'year': '', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar'}, {'title': 'Uncovering impacts: a case study in using altmetrics tools', 'first_author': 'Priem', 'journal': 'Workshop on the Semantic Publishing (SePublica 2012) 9 th Extended Semantic Web Conference Hersonissos, Crete, Greece, May 28, 2012', 'year': '2012', 'number': '', 'volume': '', 'first_page': '40', 'authors': 'Priem, Parra, Piwowar, Groth, Waagmeester'}, {'title': 'Uncovering the impact story of open research', 'first_author': 'Piwowar', 'journal': '', 'year': '2012', 'number': '', 'volume': '', 'first_page': '', 'authors': 'Piwowar'}, {'title': 'Altmetrics: Value all research products', 'first_author': 'Piwowar', 'journal': 'Nature', 'year': '2013', 'number': '7431', 'volume': '493', 'first_page': '159', 'authors': 'Piwowar'}]
        responses = []
        for biblio in biblios:
            response = self.provider.aliases([("biblio", biblio)])
            print response
            responses += response
        print responses
        expected = [('biblio', {'repository': u'Public Library of Science', 'title': u'Sharing Detailed Research Data Is Associated with Increased Citation Rate', 'journal': u'PLoS ONE', 'year': '2007', 'authors': u'Piwowar, Day, Fridsma'}), ('doi', u'10.1371/journal.pone.0000308'), ('url', u'http://dx.doi.org/10.1371/journal.pone.0000308'), ('url', u'http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000308'), ('biblio', {'repository': u'Public Library of Science', 'title': u'Towards a Data Sharing Culture: Recommendations for Leadership from Academic Health Centers', 'journal': u'PLoS Medicine', 'year': '2008', 'authors': u'Piwowar, Becich, Bilofsky, Crowley'}), ('doi', u'10.1371/journal.pmed.0050183'), ('url', u'http://dx.doi.org/10.1371/journal.pmed.0050183'), ('url', u'http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0050183'), ('biblio', {'repository': u'Elsevier', 'title': u'Public sharing of research datasets: A pilot study of associations', 'journal': u'Journal of Informetrics', 'year': '2010', 'authors': u'Piwowar, Chapman'}), ('doi', u'10.1016/j.joi.2009.11.010'), ('url', u'http://dx.doi.org/10.1016/j.joi.2009.11.010'), ('url', u'http://www.sciencedirect.com/science/article/pii/S1751157709000881'), ('biblio', {'repository': u'Nature Publishing Group', 'title': u'Data archiving is a good investment', 'journal': u'Nature', 'year': '2011', 'authors': u'Piwowar, Vision, Whitlock'}), ('doi', u'10.1038/473285a'), ('url', u'http://dx.doi.org/10.1038/473285a'), ('biblio', {'repository': u'Public Library of Science', 'title': u"Who Shares? Who Doesn't? Factors Associated with Openly Archiving Raw Research Data", 'journal': u'PLoS ONE', 'year': '2011', 'authors': u'Piwowar'}), ('doi', u'10.1371/journal.pone.0018657'), ('url', u'http://dx.doi.org/10.1371/journal.pone.0018657'), ('url', u'http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0018657'), ('biblio', {'repository': u'University of California Press', 'title': u'Biology Needs a Modern Assessment System for Professional Productivity', 'journal': u'BioScience', 'year': '2011', 'authors': u'McDade, Maddison, Guralnick, Piwowar, Jameson, Helgen, Herendeen, Hill, Vis'}), ('doi', u'10.1525/bio.2011.61.8.8'), ('url', u'http://dx.doi.org/10.1525/bio.2011.61.8.8'), ('url', u'http://www.jstor.org/discover/10.1525/bio.2011.61.8.8?uid=3739400&uid=2&uid=3737720&uid=4&sid=21101701097323'), ('biblio', {'repository': u'Wiley Blackwell (John Wiley & Sons)', 'title': u'Expediting medical literature coding with query-building', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'authors': u'Garnett, Piwowar, Rasmussen, Illes'}), ('doi', u'10.1002/meet.14504701421'), ('url', u'http://dx.doi.org/10.1002/meet.14504701421'), ('url', u'http://onlinelibrary.wiley.com/doi/10.1002/meet.14504701421/abstract'), ('biblio', {'repository': u'Public Library of Science', 'title': u'Neuroethics and fMRI: Mapping a Fledgling Relationship', 'journal': u'PLoS ONE', 'year': '2011', 'authors': u'Garnett, Whiteley, Piwowar, Rasmussen, Illes'}), ('doi', u'10.1371/journal.pone.0018537'), ('url', u'http://dx.doi.org/10.1371/journal.pone.0018537'), ('url', u'http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0018537'), ('biblio', {'repository': u'Wiley Blackwell (John Wiley & Sons)', 'title': u'Evaluating data citation and sharing policies in the environmental sciences', 'journal': u'Proceedings of the American Society for Information Science and Technology', 'year': '2010', 'authors': u'Weber, Piwowar, Vision'}), ('doi', u'10.1002/meet.14504701445'), ('url', u'http://dx.doi.org/10.1002/meet.14504701445'), ('url', u'http://onlinelibrary.wiley.com/doi/10.1002/meet.14504701445/abstract')]
        assert_equals(responses, expected)


########NEW FILE########
__FILENAME__ = test_dataone
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/dataone")
SAMPLE_EXTRACT_ALIASES_PAGE = os.path.join(datadir, "aliases")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

class TestDataone(ProviderTestCase):

    provider_name = "dataone"

    testitem_aliases = ("dataone", "esa.44.1")
    testitem_biblio = ("dataone", "esa.44.1")

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("github", "NOT A DATAONE ID")), False)
  
    @nottest
    # Not sure how to test this well with canned data because it calls
    ## a redirected url
    def test_extract_biblio(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        ret = self.provider._extract_biblio(f.read())
        assert_equals(ret, {'title': 'Heron Island coral transition rates', 
                            'published_date' : '2007-08-27'})

    def test_extract_aliases(self):
        # ensure that the dryad reader can interpret an xml doc appropriately
        f = open(SAMPLE_EXTRACT_ALIASES_PAGE, "r")
        aliases = self.provider._extract_aliases(f.read())
        assert_equals(aliases, [
                ('url', u'https://knb.ecoinformatics.org/knb/d1/mn/v1/object/doi:10.5063%2FAA%2Fnrs.373.1'), 
                ('doi', u'10.5063/AA/nrs.373.1')
                ])


########NEW FILE########
__FILENAME__ = test_delicious
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/delicious")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")

TEST_ID = "http://total-impact.org/"
TEST_ID2 = "http://cas-csid.cas.unt.edu/?p=3575"

class TestDelicious(ProviderTestCase):

    provider_name = "delicious"

    testitem_aliases = ("url", TEST_ID)
    testitem_metrics = ("url", TEST_ID)

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics(good_page)
        expected = {'delicious:bookmarks': 65}
        assert_equals(metrics_dict, expected)

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("bookmarks", 
            [self.testitem_aliases])
        expected = "http://delicious.com/link/2d6bf502d610eaa99db37fada1957a95"
        assert_equals(provenance_url, expected)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'delicious:bookmarks': (75, 'http://delicious.com/link/2d6bf502d610eaa99db37fada1957a95')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics2(self):
        metrics_dict = self.provider.metrics([("url", TEST_ID2)])
        expected = {'delicious:bookmarks': (3, 'http://delicious.com/link/41ce360e970852cc77349af2615c9d02')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

########NEW FILE########
__FILENAME__ = test_dryad
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from test.utils import http
from totalimpact.providers.provider import Provider, ProviderItemNotFoundError

import os
from nose.tools import assert_equals, raises
import collections

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/dryad")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_ALIASES_PAGE = os.path.join(datadir, "aliases")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE = os.path.join(datadir, "members")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

TEST_DRYAD_DOI = "10.5061/dryad.7898"
TEST_DRYAD_AUTHOR = "Piwowar, Heather A."
TEST_ALIASES_SEED = {"doi" : [TEST_DRYAD_DOI], "url" : ["http://datadryad.org/resource/doi:10.5061/dryad.7898"]}

class TestDryad(ProviderTestCase):

    provider_name = "dryad"

    testitem_members = TEST_DRYAD_AUTHOR
    testitem_aliases = ("doi", TEST_DRYAD_DOI)
    testitem_metrics = ("doi", TEST_DRYAD_DOI)
    testitem_biblio = ("doi", TEST_DRYAD_DOI)
    testitem_provenance_url = "http://dx.doi.org/10.5061/dryad.7898"


    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate TEST_DRYAD_DOI
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)
        # ensure that it doesn't match an inappropriate TEST_DRYAD_DOI
        assert_equals(self.provider.is_relevant_alias(("doi", "11.12354/NOTDRYADDOI")), False)
    
    def test_extract_stats(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        metrics_dict = self.provider._extract_metrics(f.read())
        print metrics_dict
        assert_equals(len(metrics_dict), 2)
        assert_equals(metrics_dict['dryad:package_views'], 889)
        assert_equals(metrics_dict['dryad:total_downloads'], 237)

    def test_extract_stats_invalid_id(self):
        # If the item has a DOI alias but it's not recognised by dryad, 
        #    then won't find any metrics. Should get a None returned.
        metrics = self.provider.metrics([("doi", "10.9999/NOTADRYADDOI")])
        assert_equals(metrics, {})

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'dryad:package_views': (889, 'http://dx.doi.org/10.5061/dryad.7898'), 
            'dryad:total_downloads': (237, 'http://dx.doi.org/10.5061/dryad.7898')}
        print metrics_dict            
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_biblio(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio])
        print biblio_dict
        expected = {'title': u'Data from: Can clone size serve as a proxy for clone age? An exploration using microsatellite divergence in Populus tremuloides', 'authors': u'Ally, Ritland, Otto', 'repository': u'Dryad Digital Repository', 'year': '2010'}
        assert_equals(biblio_dict, expected)

    @http
    def test_biblio_unicode(self):
        biblio_dict = self.provider.biblio([("doi", "10.5061/dryad.6qh25")])
        print biblio_dict
        expected = {'title': u'Data from: Electrophoretic mobility confirms reassortment bias among geographic isolates of segmented RNA phages', 'authors': u'D\xedaz-Mu\xf1oz, Tenaillon, Goldhill, Brao, Turner, Chao', 'repository': u'Dryad Digital Repository', 'year': '2013'}
        assert_equals(biblio_dict, expected)

    @http
    def test_alias(self):
        aliases = self.provider.aliases([self.testitem_aliases])
        print aliases
        expected = [('biblio', {'title': u'Data from: Can clone size serve as a proxy for clone age? An exploration using microsatellite divergence in Populus tremuloides', 'authors_literal': u'Ally, Dilara; Ritland, Kermit; Otto, Sarah P.', 'repository': u'Dryad Digital Repository', 'year': u'2010'}), ('url', u'http://datadryad.org/resource/doi:10.5061/dryad.7898'), ('url', 'http://dx.doi.org/10.5061/dryad.7898')]
        assert_equals(sorted(aliases), sorted(expected))


########NEW FILE########
__FILENAME__ = test_figshare
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact.providers import provider
from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/figshare")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_ALIASES_PAGE = os.path.join(datadir, "aliases")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE = os.path.join(datadir, "members")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

class TestFigshare(ProviderTestCase):

    provider_name = "figshare"

    testitem_members = "http://figshare.com/authors/schamberlain/96554"
    testitem_aliases = ("doi", "10.6084/m9.figshare.92393")
    testitem_metrics = ("doi", "10.6084/m9.figshare.92393")
    testitem_biblio = ("doi", "10.6084/m9.figshare.865731")

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("doi", "NOT A FIGSHARE ID")), False)
  
    def test_extract_biblio_success(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        biblio_dict = self.provider._extract_biblio(f.read(), id="10.6084/m9.figshare.92393")
        print biblio_dict
        expected = {'genre': 'dataset', 'title': 'Gaussian Job Archive for B2(2-)', 'year': 2012, 'repository': 'figshare', 'published_date': '14:13, Jun 15, 2012'}
        assert_equals(biblio_dict, expected)


    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        metrics_dict = self.provider._extract_metrics(f.read(), id="10.6084/m9.figshare.92393")
        print metrics_dict
        assert_equals(metrics_dict["figshare:downloads"], 19)

    def test_extract_members_success(self):
        f = open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE, "r")
        response = self.provider._extract_members(f.read())
        print response
        expected = [('doi', '10.6084/m9.figshare.806563'), ('doi', '10.6084/m9.figshare.806423'), ('doi', '10.6084/m9.figshare.803123'), ('doi', '10.6084/m9.figshare.791569'), ('doi', '10.6084/m9.figshare.758498'), ('doi', '10.6084/m9.figshare.757866'), ('doi', '10.6084/m9.figshare.739343'), ('doi', '10.6084/m9.figshare.729248'), ('doi', '10.6084/m9.figshare.719786'), ('doi', '10.6084/m9.figshare.669696')]
        assert_equals(response, expected)

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("figshare:downloads", [self.testitem_aliases])
        expected = 'http://dx.doi.org/10.6084/m9.figshare.92393'
        assert_equals(provenance_url, expected)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        print metrics_dict
        expected = {'figshare:downloads': (19, 'http://dx.doi.org/10.6084/m9.figshare.92393')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_biblio(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio])
        print biblio_dict
        expected = {'genre': 'slides', 'title': u'Open Science', 'year': 2013, 'repository': 'figshare', 'published_date': u'03:41, Dec 03, 2013'}
        assert_equals(biblio_dict, expected)

    @http
    def test_alias(self):
        aliases = self.provider.aliases([self.testitem_aliases])
        print aliases
        expected = [('url', u'http://figshare.com/articles/Gaussian_Job_Archive_for_B2(2-)/92393')]
        assert_equals(sorted(aliases), sorted(expected))

    @http
    def test_members(self):
        members = self.provider.member_items(self.testitem_members)
        print members
        expected = [('doi', u'10.6084/m9.figshare.806563'), ('doi', u'10.6084/m9.figshare.806423'), ('doi', u'10.6084/m9.figshare.803123'), ('doi', u'10.6084/m9.figshare.791569'), ('doi', u'10.6084/m9.figshare.758498'), ('doi', u'10.6084/m9.figshare.757866'), ('doi', u'10.6084/m9.figshare.739343'), ('doi', u'10.6084/m9.figshare.729248'), ('doi', u'10.6084/m9.figshare.719786'), ('doi', u'10.6084/m9.figshare.669696'), ('doi', u'10.6084/m9.figshare.106915'), ('doi', u'10.6084/m9.figshare.97222'), ('doi', u'10.6084/m9.figshare.97221'), ('doi', u'10.6084/m9.figshare.97215'), ('doi', u'10.6084/m9.figshare.94296'), ('doi', u'10.6084/m9.figshare.94295'), ('doi', u'10.6084/m9.figshare.94219'), ('doi', u'10.6084/m9.figshare.94218'), ('doi', u'10.6084/m9.figshare.94217'), ('doi', u'10.6084/m9.figshare.94216'), ('doi', u'10.6084/m9.figshare.94090'), ('doi', u'10.6084/m9.figshare.94089'), ('doi', u'10.6084/m9.figshare.94030'), ('doi', u'10.6084/m9.figshare.91145'), ('doi', u'10.6084/m9.figshare.90832')]
        for expected_item in expected:
            assert expected_item in members

    @http
    def test_provider_import(self):
        tabs_input = {"account_name": "http://figshare.com/authors/schamberlain/96554", "standard_dois_input": "10.6084/m9.figshare.92393\nhttps://doi.org/10.6084/m9.figshare.865731"}
        members = provider.import_products("figshare", tabs_input)
        print members
        expected = [('doi', '10.6084/m9.figshare.92393'), ('doi', '10.6084/m9.figshare.865731'), ('doi', u'10.6084/m9.figshare.806563'), ('doi', u'10.6084/m9.figshare.806423'), ('doi', u'10.6084/m9.figshare.803123'), ('doi', u'10.6084/m9.figshare.791569'), ('doi', u'10.6084/m9.figshare.758498'), ('doi', u'10.6084/m9.figshare.757866'), ('doi', u'10.6084/m9.figshare.739343'), ('doi', u'10.6084/m9.figshare.729248'), ('doi', u'10.6084/m9.figshare.719786'), ('doi', u'10.6084/m9.figshare.669696'), ('doi', u'10.6084/m9.figshare.106915'), ('doi', u'10.6084/m9.figshare.97222'), ('doi', u'10.6084/m9.figshare.97221'), ('doi', u'10.6084/m9.figshare.97215'), ('doi', u'10.6084/m9.figshare.94296'), ('doi', u'10.6084/m9.figshare.94295'), ('doi', u'10.6084/m9.figshare.94219'), ('doi', u'10.6084/m9.figshare.94218'), ('doi', u'10.6084/m9.figshare.94217'), ('doi', u'10.6084/m9.figshare.94216'), ('doi', u'10.6084/m9.figshare.94090'), ('doi', u'10.6084/m9.figshare.94089'), ('doi', u'10.6084/m9.figshare.94030'), ('doi', u'10.6084/m9.figshare.91145'), ('doi', u'10.6084/m9.figshare.90832')]
        for member in expected:
            assert(member in members)


########NEW FILE########
__FILENAME__ = test_github
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact.providers import provider
from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/github")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_ALIASES_PAGE = os.path.join(datadir, "aliases")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE = os.path.join(datadir, "members")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

class TestGithub(ProviderTestCase):

    provider_name = "github"

    testitem_members = "egonw"
    testitem_aliases = ("url", "https://github.com/egonw/cdk")
    testitem_metrics = ("url", "https://github.com/egonw/cdk")
    testitem_biblio = ("url", "https://github.com/egonw/cdk")
    testitem_biblio_org = ("url", "https://github.com/openphacts/BridgeDb")

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("doi", "NOT A GITHUB ID")), False)
  
    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        metrics_dict = self.provider._extract_metrics(f.read())
        print metrics_dict
        assert_equals(metrics_dict["github:stars"], 33)

    def test_extract_members_success(self):        
        f = open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE, "r")
        members = self.provider._extract_members(f.read(), self.testitem_members)
        assert len(members) >= 30, (len(members), members)

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("github:forks", [self.testitem_aliases])
        assert_equals(provenance_url, "https://github.com/egonw/cdk/network/members")

        # Not the same as above
        provenance_url = self.provider.provenance_url("github:stars", [self.testitem_aliases])
        assert_equals(provenance_url, "https://github.com/egonw/cdk/stargazers")

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        print metrics_dict
        expected = {'github:forks': (20, 'https://github.com/egonw/cdk/network/members'), 'github:stars': (25, 'https://github.com/egonw/cdk/stargazers')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_github_namespace_deprecated_after_v1(self):
        metrics_dict = self.provider.metrics([("github", "egonw,cdk")])
        print metrics_dict
        expected = {'github:forks': (20, 'https://github.com/egonw/cdk/network/members'), 'github:stars': (25, 'https://github.com/egonw/cdk/stargazers')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_biblio(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio_org])
        print biblio_dict
        expected = {'last_push_date': u'2013-01-08T06:09:29Z', 'create_date': u'2012-01-18T10:49:04Z', 'title': u'BridgeDb', 'url': u'https://github.com/openphacts/BridgeDb', 'year': u'2012', 'owner': u'openphacts'}
        assert_equals(biblio_dict.keys(), expected.keys())
        for key in ["url", "owner", "title", "create_date", "year"]:
            assert_equals(biblio_dict[key], expected[key])

    @http
    def test_members(self):
        members = self.provider.member_items(self.testitem_members)
        print members
        expected = [('url', u'https://github.com/egonw/blueobelisk.debian'), ('url', u'https://github.com/egonw/ACS-Twitter-Visualization'), ('url', u'https://github.com/egonw/bioclipse.cir'), ('url', u'https://github.com/egonw/bioclipse.social'), ('url', u'https://github.com/egonw/bioclipse.sb'), ('url', u'https://github.com/egonw/acsrdf2010'), ('url', u'https://github.com/egonw/bioclipse.brunn'), ('url', u'https://github.com/egonw/5stardata.info'), ('url', u'https://github.com/egonw/biostar-central'), ('url', u'https://github.com/egonw/bioclipse.core'), ('url', u'https://github.com/egonw/bioclipse.statistics'), ('url', u'https://github.com/egonw/bioclipse.rdf'), ('url', u'https://github.com/egonw/bioclipse.wikipathways'), ('url', u'https://github.com/egonw/bioclipse.ds'), ('url', u'https://github.com/egonw/cb'), ('url', u'https://github.com/egonw/bioclipse.bridgedb'), ('url', u'https://github.com/egonw/bioclipse.opentox'), ('url', u'https://github.com/egonw/bopaper'), ('url', u'https://github.com/egonw/bioclipse.openphacts'), ('url', u'https://github.com/egonw/bioclipse.ambit'), ('url', u'https://github.com/egonw/bioclipse.cheminformatics'), ('url', u'https://github.com/egonw/AZOrange'), ('url', u'https://github.com/egonw/blueobelisk.userscript'), ('url', u'https://github.com/egonw/BridgeDb'), ('url', u'https://github.com/egonw/bioclipse.bigcatedu'), ('url', u'https://github.com/egonw/bioclipse.oscar'), ('url', u'https://github.com/egonw/bioclipse.ons'), ('url', u'https://github.com/egonw/bioclipse.experimental'), ('url', u'https://github.com/egonw/biblatex-biomedcentral'), ('url', u'https://github.com/egonw/cacm-article')]
        for member in expected:
            assert(member in members)

    @http
    def test_provider_import(self):
        test_tabs = {"account_name": "egonw", "standard_urls_input": "https://github.com/openphacts/BridgeDb\nhttps://github.com/total-impact/totalimpactcore"}
        members = provider.import_products("github", test_tabs)
        print members
        expected = [('url', u'https://github.com/openphacts/BridgeDb'), ('url', u'https://github.com/total-impact/totalimpactcore'), ('url', u'https://github.com/egonw/blueobelisk.debian'), ('url', u'https://github.com/egonw/ACS-Twitter-Visualization'), ('url', u'https://github.com/egonw/bioclipse.cir'), ('url', u'https://github.com/egonw/bioclipse.social'), ('url', u'https://github.com/egonw/bioclipse.sb'), ('url', u'https://github.com/egonw/acsrdf2010'), ('url', u'https://github.com/egonw/bioclipse.brunn'), ('url', u'https://github.com/egonw/5stardata.info'), ('url', u'https://github.com/egonw/biostar-central'), ('url', u'https://github.com/egonw/bioclipse.core'), ('url', u'https://github.com/egonw/bioclipse.statistics'), ('url', u'https://github.com/egonw/bioclipse.rdf'), ('url', u'https://github.com/egonw/bioclipse.wikipathways'), ('url', u'https://github.com/egonw/bioclipse.ds'), ('url', u'https://github.com/egonw/cb'), ('url', u'https://github.com/egonw/bioclipse.bridgedb'), ('url', u'https://github.com/egonw/bioclipse.opentox'), ('url', u'https://github.com/egonw/bopaper'), ('url', u'https://github.com/egonw/bioclipse.openphacts'), ('url', u'https://github.com/egonw/bioclipse.ambit'), ('url', u'https://github.com/egonw/bioclipse.cheminformatics'), ('url', u'https://github.com/egonw/AZOrange'), ('url', u'https://github.com/egonw/blueobelisk.userscript'), ('url', u'https://github.com/egonw/BridgeDb'), ('url', u'https://github.com/egonw/bioclipse.bigcatedu'), ('url', u'https://github.com/egonw/bioclipse.oscar'), ('url', u'https://github.com/egonw/bioclipse.ons'), ('url', u'https://github.com/egonw/bioclipse.experimental'), ('url', u'https://github.com/egonw/biblatex-biomedcentral'), ('url', u'https://github.com/egonw/cacm-article')]
        for member in expected:
            assert(member in members)


########NEW FILE########
__FILENAME__ = test_mendeley
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderClientError, ProviderServerError
from totalimpact.providers import provider

from test.utils import http
from totalimpact import app, db
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/mendeley")
SAMPLE_EXTRACT_UUID_PAGE = os.path.join(datadir, "uuidlookup")
SAMPLE_EXTRACT_UUID_PAGE_NO_DOI = os.path.join(datadir, "uuidlookup_no_doi")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_PROVENANCE_URL_PAGE = SAMPLE_EXTRACT_METRICS_PAGE
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")
SAMPLE_EXTRACT_BIBLIO_PAGE_OAI = os.path.join(datadir, "biblio_oai")

TEST_DOI = "10.1038/nature10658"  # matches UUID sample page

class TestMendeley(ProviderTestCase):

    provider_name = "mendeley"

    testitem_aliases = ("doi", TEST_DOI)
    testitem_aliases_biblio_no_doi =  ("biblio", {'title': 'Scientometrics 2.0: Toward new metrics of scholarly impact on the social Web', 'first_author': 'Priem', 'journal': 'First Monday', 'number': '7', 'volume': '15', 'first_page': '', 'authors': 'Priem, Hemminger', 'year': '2010'})
    testitem_metrics_dict = {"biblio":[{"year":2011, "authors":"sdf", "title": "Mutations causing syndromic autism define an axis of synaptic pathophysiology"}],"doi":[TEST_DOI]}
    testitem_metrics = [(k, v[0]) for (k, v) in testitem_metrics_dict.items()]
    testitem_metrics_dict_wrong_year = {"biblio":[{"year":9999, "authors":"sdf", "title": "Mutations causing syndromic autism define an axis of synaptic pathophysiology"}],"doi":[TEST_DOI]}
    testitem_metrics_wrong_year = [(k, v[0]) for (k, v) in testitem_metrics_dict_wrong_year.items()]

    def setUp(self):
        self.db = setup_postgres_for_unittests(db, app)
        ProviderTestCase.setUp(self)
        
    def tearDown(self):
        teardown_postgres_for_unittests(self.db)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("github", "egonw,cdk")), False)
  
    def test_extract_biblio_success(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        metrics_dict = self.provider._extract_biblio(f.read())
        expected = {'is_oa_journal': 'False', 'issn': '00280836'}
        assert_equals(metrics_dict, expected)

    def test_extract_biblio_oai_success(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE_OAI, "r")
        biblio_dict = self.provider._extract_biblio(f.read())
        print biblio_dict
        expected = {'oai_id': 'oai:arXiv.org:1012.4872', 'issn': '15322882', 'is_oa_journal': 'None'}
        assert_equals(biblio_dict, expected)

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        metrics_dict = self.provider._extract_metrics(f.read())
        assert_equals(metrics_dict["mendeley:readers"], 102)

    def test_extract_provenance_url(self):
        f = open(SAMPLE_EXTRACT_PROVENANCE_URL_PAGE, "r")
        provenance_url = self.provider._extract_provenance_url(f.read())
        assert_equals(provenance_url, "http://api.mendeley.com/catalog/mutations-causing-syndromic-autism-define-axis-synaptic-pathophysiology/")

    def test_get_ids_from_title(self):
        f = open(SAMPLE_EXTRACT_UUID_PAGE, "r")
        response = self.provider._get_uuid_from_title(self.testitem_metrics_dict, f.read())
        expected = {'uuid': '1f471f70-1e4f-11e1-b17d-0024e8453de6'}
        assert_equals(response, expected)

    def test_get_ids_from_title_no_doi(self):
        f = open(SAMPLE_EXTRACT_UUID_PAGE_NO_DOI, "r")
        response = self.provider._get_uuid_from_title(self.testitem_metrics_dict, f.read())
        print response
        expected = {'url': 'http://www.mendeley.com/catalog/mutations-causing-syndromic-autism-define-axis-synaptic-pathophysiology/', 'uuid': '1f471f70-1e4f-11e1-b17d-0024e8453de6'}
        assert_equals(response, expected)

    def test_get_ids_from_title_no_doi_wrong_year(self):
        f = open(SAMPLE_EXTRACT_UUID_PAGE_NO_DOI, "r")
        response = self.provider._get_uuid_from_title(self.testitem_metrics_dict_wrong_year, f.read())
        expected = None
        assert_equals(response, expected)

    def test_get_metrics_and_drilldown_from_metrics_page(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        response = self.provider._get_metrics_and_drilldown_from_metrics_page(f.read())
        expected = {'mendeley:discipline': ([{'id': 3, 'value': 80, 'name': 'Biological Sciences'}, {'id': 19, 'value': 14, 'name': 'Medicine'}, {'id': 22, 'value': 2, 'name': 'Psychology'}], 'http://api.mendeley.com/catalog/mutations-causing-syndromic-autism-define-axis-synaptic-pathophysiology/'), 'mendeley:country': ([{'name': 'United States', 'value': 42}, {'name': 'Japan', 'value': 12}, {'name': 'United Kingdom', 'value': 9}], 'http://api.mendeley.com/catalog/mutations-causing-syndromic-autism-define-axis-synaptic-pathophysiology/'), 'mendeley:career_stage': ([{'name': 'Ph.D. Student', 'value': 31}, {'name': 'Post Doc', 'value': 21}, {'name': 'Professor', 'value': 7}], 'http://api.mendeley.com/catalog/mutations-causing-syndromic-autism-define-axis-synaptic-pathophysiology/'), 'mendeley:readers': (102, 'http://api.mendeley.com/catalog/mutations-causing-syndromic-autism-define-axis-synaptic-pathophysiology/')}
        assert_equals(response, expected)

    def test_remove_punctuation(self):
        response = self.provider.remove_punctuation(u"sdflkdsjf4r42432098@#$#@$sdlkfj..sdfsdf")
        assert_equals(response, u'sdflkdsjf4r42432098sdlkfjsdfsdf')

    @http
    def test_metrics_pmid(self):
        # at the moment this item 
        metrics_dict = self.provider.metrics([("pmid", "12578738")])
        expected = {'mendeley:discipline': ([{u'id': 6, u'value': 33, u'name': u'Computer and Information Science'}, {u'id': 3, u'value': 33, u'name': u'Biological Sciences'}, {u'id': 19, u'value': 12, u'name': u'Medicine'}], u'http://api.mendeley.com/catalog/value-data/'), 
                'mendeley:country': ([{u'name': u'United States', u'value': 22}, {u'name': u'United Kingdom', u'value': 16}, {u'name': u'Netherlands', u'value': 12}], u'http://api.mendeley.com/catalog/value-data/'), 
                'mendeley:career_stage': ([{u'name': u'Ph.D. Student', u'value': 19}, {u'name': u'Other Professional', u'value': 15}, {u'name': u'Researcher (at an Academic Institution)', u'value': 14}], u'http://api.mendeley.com/catalog/value-data/'), 
                'mendeley:readers': (129, u'http://api.mendeley.com/catalog/value-data/')}
        print metrics_dict
        assert_equals(set(metrics_dict.keys()), set(expected.keys())) 
        # can't tell more about dicsciplines etc because they are percentages and may go up or down

    @http
    def test_metrics_arxiv(self):
        # at the moment this item 
        metrics = self.provider.metrics([("arxiv", "1203.4745")])
        print metrics
        expected = {'mendeley:discipline': ([{u'id': 6, u'value': 34, u'name': u'Computer and Information Science'}, {u'id': 23, u'value': 23, u'name': u'Social Sciences'}, {u'id': 19, u'value': 10, u'name': u'Medicine'}], u'http://www.mendeley.com/catalog/altmetrics-wild-using-social-media-explore-scholarly-impact/'), 'mendeley:country': ([{u'name': u'United States', u'value': 16}, {u'name': u'United Kingdom', u'value': 9}, {u'name': u'Germany', u'value': 5}], u'http://www.mendeley.com/catalog/altmetrics-wild-using-social-media-explore-scholarly-impact/'), 'mendeley:career_stage': ([{u'name': u'Librarian', u'value': 26}, {u'name': u'Ph.D. Student', u'value': 14}, {u'name': u'Other Professional', u'value': 14}], u'http://www.mendeley.com/catalog/altmetrics-wild-using-social-media-explore-scholarly-impact/'), 'mendeley:readers': (153, u'http://www.mendeley.com/catalog/altmetrics-wild-using-social-media-explore-scholarly-impact/')}
        assert_equals(metrics, expected)

    @http
    def test_aliases_no_doi(self):
        # at the moment this item 
        new_aliases = self.provider.aliases([self.testitem_aliases_biblio_no_doi])
        print new_aliases
        expected = [('doi', u'10.5210/fm.v15i7.2874'), ('url', u'http://www.mendeley.com/catalog/scientometrics-20-toward-new-metrics-scholarly-impact-social-web/'), ('uuid', u'f3018369-0eb4-3dbe-a3cc-9ee0bbc4e59e')]
        assert_equals(new_aliases, expected)

    @http
    def test_aliases_biblio(self):
        # at the moment this item 
        alias = (u'biblio', {u'title': u'Altmetrics in the wild: Using social media to explore scholarly impact', u'first_author': u'Priem', u'journal': u'arXiv preprint arXiv:1203.4745', u'authors': u'Priem, Piwowar, Hemminger', u'number': u'', u'volume': u'', u'first_page': u'', u'year': u'2012'})
        new_aliases = self.provider.aliases([alias])
        print new_aliases
        expected = [('doi', u'http://arxiv.org/abs/1203.4745v1'), ('url', u'http://www.mendeley.com/catalog/altmetrics-wild-using-social-media-explore-scholarly-impact/'), ('uuid', u'920cf7e1-02c1-3c40-bd52-18552089248e')]
        assert_equals(new_aliases, expected)

    @http
    def test_biblio_oai_id(self):
        # at the moment this item 
        alias = ("doi", "10.1086/508600")
        new_biblio = self.provider.biblio([alias])
        print new_biblio
        expected = {'oai_id': u'oai:arXiv.org:astro-ph/0603060', 'issn': u'0004637X', 'is_oa_journal': 'None'}
        assert_equals(new_biblio, expected)        

    @http
    def test_biblio_issn(self):
        # at the moment this item 
        alias = ("doi", "10.1371/journal.pcbi.1000361")
        new_biblio = self.provider.biblio([alias])
        print new_biblio
        expected = {'oai_id': u'oai:pubmedcentral.nih.gov:2663789', 'issn': u'15537358', 'free_fulltext_url': 'http://dx.doi.org/10.1371/journal.pcbi.1000361', 'is_oa_journal': 'True'}
        assert_equals(new_biblio, expected) 

    # override common tests
    @raises(ProviderClientError, ProviderServerError)
    def test_provider_metrics_400(self):
        Provider.http_get = common.get_400
        metrics = self.provider.metrics(self.testitem_metrics)

    @raises(ProviderServerError)
    def test_provider_metrics_500(self):
        Provider.http_get = common.get_500
        metrics = self.provider.metrics(self.testitem_metrics)

    @raises(ProviderClientError, ProviderServerError)
    def test_provider_aliases_400(self):
        Provider.http_get = common.get_400
        metrics = self.provider.aliases([self.testitem_aliases_biblio_no_doi])

    @raises(ProviderServerError)
    def test_provider_aliases_500(self):
        Provider.http_get = common.get_500
        metrics = self.provider.aliases([self.testitem_aliases_biblio_no_doi])

    @raises(ProviderContentMalformedError)
    def test_provider_metrics_empty(self):
        Provider.http_get = common.get_empty
        metrics = self.provider.metrics(self.testitem_metrics)

    @raises(ProviderContentMalformedError)
    def test_provider_metrics_nonsense_txt(self):
        Provider.http_get = common.get_nonsense_txt
        metrics = self.provider.metrics(self.testitem_metrics)

    @raises(ProviderContentMalformedError)
    def test_provider_metrics_nonsense_xml(self):
        Provider.http_get = common.get_nonsense_xml
        metrics = self.provider.metrics(self.testitem_metrics)

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass


########NEW FILE########
__FILENAME__ = test_orcid
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from test.utils import http
from totalimpact.providers.provider import Provider, ProviderItemNotFoundError

import os
from nose.tools import assert_equals, assert_items_equal, raises, nottest
import collections

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/orcid")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE = os.path.join(datadir, "members")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE2 = os.path.join(datadir, "members2")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE3 = os.path.join(datadir, "members3")

TEST_ORCID_ID = "0000-0003-1613-5981"
TEST_ORCID_ID2 = "0000-0001-9107-0714"
TEST_ORCID_ID3 = "0000-0002-3127-3891"
# test curl -H "Accept: application/orcid+json" htid.org/0000-0001-9107-0714/orcid-works

SAMPLE_EXTRACT_MEMBER_ITEMS_SHORT = u"""
{"message-version":"1.0.23","orcid-profile":{"orcid":{"value":"0000-0003-2447-0448"},"orcid-id":"http://orcid.org/0000-0003-2447-0448","orcid-preferences":{"locale":"EN"},"orcid-history":{"creation-method":"WEBSITE","completion-date":{"value":1357696486670},"submission-date":{"value":1357696486670},"last-modified-date":{"value":1393873446463},"claimed":{"value":true},"source":null,"visibility":null},"orcid-activities":{"affiliations":null,"orcid-works":{"orcid-work":[
{"put-code":"11614995","work-title":{"title":{"value":"Review of Karl Clausberg, Zwischen den Sternen: Lichtbildarchive: Was Einstein und Uexkll, Benjamin und das Kino der Astronomie des 19. Jahrhunderts verdanken. Berlin: Akademie Verlag, 2006"},"subtitle":null},"journal-title":{"value":"Isis"},"short-description":"Book review","work-citation":{"work-citation-type":"FORMATTED_CHICAGO","citation":"Review of Karl Clausberg, Zwischen den Sternen: Lichtbildarchive: Was Einstein und Uexkll, Benjamin und das Kino der Astronomie des 19. Jahrhunderts verdanken. Berlin: Akademie Verlag, 2006. Isis 100, no. 1 (March 2009): 171."},"work-type":"JOURNAL_ARTICLE","publication-date":{"year":{"value":"2009"},"month":{"value":"03"},"day":null,"media-type":null},
    "work-external-identifiers":{"work-external-identifier":[{"work-external-identifier-type":"DOI","work-external-identifier-id":{"value":"DOI:10.1086/599671"}}],"scope":null},
        "url":null,"work-contributors":{"contributor":[{"contributor-attributes":{"contributor-role":"AUTHOR"}}]},"work-source":{"value":"0000-0003-2447-0448"},"language-code":"en","country":{"value":"US","visibility":null},"visibility":null}],
"scope":null}},"type":"USER","group-type":null,"client-type":null}}
"""

class TestOrcid(ProviderTestCase):

    provider_name = "orcid"

    testitem_members = TEST_ORCID_ID

    def setUp(self):
        ProviderTestCase.setUp(self)
    
    def test_extract_members(self):
        f = open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE, "r")
        members = self.provider._extract_members(f.read(), TEST_ORCID_ID)
        print members
        expected = [('doi', '10.1038/493159a'), ('doi', '10.1038/493159a'), ('doi', '10.6084/m9.figshare.92959'), ('doi', '10.1038/473285a'), ('doi', '10.1002/meet.14504701421'), ('doi', '10.1002/meet.14504701413'), ('doi', '10.1002/meet.2011.14504801337'), ('doi', '10.1002/meet.2011.14504801337'), ('doi', '10.1525/bio.2011.61.8.8'), ('doi', '10.1525/bio.2011.61.8.8'), ('doi', '10.1038/473285a'), ('doi', '10.1038/473285a'), ('doi', '10.5061/dryad.j1fd7'), ('doi', '10.1371/journal.pone.0018537'), ('doi', '10.1371/journal.pone.0018537'), ('doi', '10.1002/meet.2011.14504801327'), ('doi', '10.1002/meet.2011.14504801327'), ('doi', '10.1002/meet.2011.14504801205'), ('doi', '10.1002/meet.2011.14504801205'), ('doi', '10.1371/journal.pone.0018657'), ('doi', '10.1371/journal.pone.0018657'), ('doi', '10.1038/npre.2010.5452.1'), ('doi', '10.1016/j.joi.2009.11.010'), ('doi', '10.1038/npre.2010.4267.1'), ('doi', '10.1002/meet.14504701450'), ('doi', '10.1002/meet.14504701450'), ('doi', '10.1002/meet.14504701445'), ('doi', '10.1002/meet.14504701445'), ('doi', '10.1002/meet.14504701421'), ('doi', '10.1002/meet.14504701421'), ('doi', '10.1016/j.joi.2009.11.010'), ('doi', '10.1016/j.joi.2009.11.010'), ('doi', '10.1002/meet.14504701413'), ('doi', '10.1002/meet.14504701413'), ('doi', '10.1038/npre.2008.2152.1'), ('biblio', {'full_citation': '@article { piwowar2008,\n\ttitle = {A review of journal policies for sharing research data},\n\tjournal = {Open Scholarship: Authority, Community, and Sustainability in the Age of Web 2.0 - Proceedings of the 12th International Conference on Electronic Publishing, ELPUB 2008},\n\tyear = {2008},\n\tpages = {1-14},\n\tauthor = {Piwowar, H.A. and Chapman, W.W.}\n}\n\n', 'title': 'A review of journal policies for sharing research data', 'first_author': u'Piwowar', 'journal': 'Open Scholarship: Authority, Community, and Sustainability in the Age of Web 2.0 - Proceedings of the 12th International Conference on Electronic Publishing, ELPUB 2008', 'year': '2008', 'number': '', 'volume': '', 'first_page': '1-14', 'authors': u'Piwowar, Chapman', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article { piwowar2008,\n\ttitle = {A review of journal policies for sharing research data},\n\tjournal = {Open Scholarship: Authority, Community, and Sustainability in the Age of Web 2.0 - Proceedings of the 12th International Conference on Electronic Publishing, ELPUB 2008},\n\tyear = {2008},\n\tpages = {1-14},\n\tauthor = {Piwowar, H.A. and Chapman, W.W.}\n}\n\n', 'title': 'A review of journal policies for sharing research data', 'first_author': u'Piwowar', 'journal': 'Open Scholarship: Authority, Community, and Sustainability in the Age of Web 2.0 - Proceedings of the 12th International Conference on Electronic Publishing, ELPUB 2008', 'year': '2008', 'number': '', 'volume': '', 'first_page': '1-14', 'authors': u'Piwowar, Chapman', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article { piwowar2008,\n\ttitle = {Envisioning a biomedical data reuse registry.},\n\tjournal = {AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium},\n\tyear = {2008},\n\tauthor = {Piwowar, H.A. and Chapman, W.W.}\n}\n\n', 'title': 'Envisioning a biomedical data reuse registry.', 'first_author': u'Piwowar', 'journal': 'AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Chapman', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article { piwowar2008,\n\ttitle = {Envisioning a biomedical data reuse registry.},\n\tjournal = {AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium},\n\tyear = {2008},\n\tauthor = {Piwowar, H.A. and Chapman, W.W.}\n}\n\n', 'title': 'Envisioning a biomedical data reuse registry.', 'first_author': u'Piwowar', 'journal': 'AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Piwowar, Chapman', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article { piwowar2008,\n\ttitle = {Identifying data sharing in biomedical literature.},\n\tjournal = {AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium},\n\tyear = {2008},\n\tpages = {596-600},\n\tauthor = {Piwowar, H.A. and Chapman, W.W.}\n}\n\n', 'title': 'Identifying data sharing in biomedical literature.', 'first_author': u'Piwowar', 'journal': 'AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium', 'year': '2008', 'number': '', 'volume': '', 'first_page': '596-600', 'authors': u'Piwowar, Chapman', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article { piwowar2008,\n\ttitle = {Identifying data sharing in biomedical literature.},\n\tjournal = {AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium},\n\tyear = {2008},\n\tpages = {596-600},\n\tauthor = {Piwowar, H.A. and Chapman, W.W.}\n}\n\n', 'title': 'Identifying data sharing in biomedical literature.', 'first_author': u'Piwowar', 'journal': 'AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium', 'year': '2008', 'number': '', 'volume': '', 'first_page': '596-600', 'authors': u'Piwowar, Chapman', 'full_citation_type': 'bibtex'}), ('doi', '10.1371/journal.pmed.0050183'), ('doi', '10.1371/journal.pmed.0050183'), ('biblio', {'full_citation': '@article { piwowar2008,\n\ttitle = {Towards a data sharing culture: recommendations for leadership from academic health centers.},\n\tjournal = {PLoS medicine},\n\tyear = {2008},\n\tvolume = {5},\n\tnumber = {9},\n\tauthor = {Piwowar, H.A. and Becich, M.J. and Bilofsky, H. and Crowley, R.S.}\n}\n\n', 'title': 'Towards a data sharing culture: recommendations for leadership from academic health centers.', 'first_author': u'Piwowar', 'journal': 'PLoS medicine', 'year': '2008', 'number': '9', 'volume': '5', 'first_page': '', 'authors': u'Piwowar, Becich, Bilofsky, Crowley', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article { piwowar2008,\n\ttitle = {Towards a data sharing culture: recommendations for leadership from academic health centers.},\n\tjournal = {PLoS medicine},\n\tyear = {2008},\n\tvolume = {5},\n\tnumber = {9},\n\tauthor = {Piwowar, H.A. and Becich, M.J. and Bilofsky, H. and Crowley, R.S.}\n}\n\n', 'title': 'Towards a data sharing culture: recommendations for leadership from academic health centers.', 'first_author': u'Piwowar', 'journal': 'PLoS medicine', 'year': '2008', 'number': '9', 'volume': '5', 'first_page': '', 'authors': u'Piwowar, Becich, Bilofsky, Crowley', 'full_citation_type': 'bibtex'}), ('doi', '10.1038/npre.2007.425.2'), ('doi', '10.1038/npre.2007.361'), ('doi', '10.1371/journal.pone.0000308'), ('doi', '10.1371/journal.pone.0000308')]
        assert_items_equal(members, expected)

    def test_extract_members2(self):
        f = open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE2, "r")
        members = self.provider._extract_members(f.read(), TEST_ORCID_ID2)
        print members
        expected = [('pmid', '24127438'), ('doi', '10.6084/m9.figshare.790739'), ('doi', '10.6084/m9.figshare.791560'), ('doi', '10.1186/1471-2105-14-158'), ('doi', '10.6084/M9.FIGSHARE.799766'), ('doi', '10.1371/journal.pbio.1001468'), ('doi', '10.7287/PEERJ.PREPRINTS.83'), ('doi', '10.1186/2041-1480-4-34'), ('biblio', {'full_citation': '@article{lapp2012,\n\tvolume  = {28},\n\tnumber  = {3},\n\tpages   = {300-305},\n}\n', 'title': '500,000 fish phenotypes: The new informatics landscape for evolutionary and developmental biology of the vertebrate skeleton', 'first_author': '', 'journal': 'Journal of Applied Ichthyology', 'year': '2012', 'number': '3', 'volume': '28', 'first_page': '300-305', 'authors': '', 'full_citation_type': 'bibtex'}), ('doi', '10.1371/journal.pone.0051070'), ('biblio', {'full_citation': '@article{lapp2012,\n\tvolume  = {61},\n\tnumber  = {4},\n\tpages   = {675-689},\n}\n', 'title': 'NeXML: Rich, extensible, and verifiable representation of comparative data and metadata', 'first_author': '', 'journal': 'Systematic Biology', 'year': '2012', 'number': '4', 'volume': '61', 'first_page': '675-689', 'authors': '', 'full_citation_type': 'bibtex'}), ('doi', '10.4056/sigs.3156511'), ('doi', '10.1371/journal.pcbi.1002799'), ('doi', '10.1002/bult.2011.1720370411'), ('biblio', {'full_citation': '@article{lapp2011,\n\tvolume  = {51},\n\tnumber  = {2},\n\tpages   = {215-223},\n}\n', 'title': 'Overview of FEED, the feeding experiments end-user database', 'first_author': '', 'journal': 'Integrative and Comparative Biology', 'year': '2011', 'number': '2', 'volume': '51', 'first_page': '215-223', 'authors': '', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{lapp2011,\n\tvolume  = {2011},\n}\n', 'title': 'The Chado Natural Diversity module: A new generic database schema for large-scale phenotyping and genotyping data', 'first_author': '', 'journal': 'Database', 'year': '2011', 'number': '', 'volume': '2011', 'first_page': '', 'authors': '', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': "Midford, P, Mabee, P, Vision, T, Westerfield, M, Midford, P, Balhoff, J, Dahdul, W, Kothari, C, Lapp, H & Lundberg, J, 2010, 'The Teleost Taxonomy Ontology', <i>Nature Precedings</i>.", 'title': 'The Teleost Taxonomy Ontology', 'journal': '', 'year': '2010', 'authors': '', 'full_citation_type': 'formatted_unspecified'}), ('biblio', {'full_citation': "Vos, R, Vos, R, Lapp, H, Piel, W & Tannen, V, 2010, 'TreeBASE2: Rise of the Machines', <i>Nature Precedings</i>.", 'title': 'TreeBASE2: Rise of the Machines', 'journal': '', 'year': '2010', 'authors': '', 'full_citation_type': 'formatted_unspecified'}), ('doi', '10.1371/journal.pone.0010708'), ('doi', '10.4056/sigs/1403501'), ('doi', '10.1371/journal.pone.0010500'), ('doi', '10.1186/2041-1480-1-8'), ('doi', '10.1111/j.2041-210x.2010.00023.x'), ('biblio', {'full_citation': '@article{lapp2010,\n\tvolume  = {59},\n\tnumber  = {4},\n\tpages   = {369-383},\n}\n', 'title': 'The teleost anatomy ontology: Anatomical representation for the genomics age', 'first_author': '', 'journal': 'Systematic Biology', 'year': '2010', 'number': '4', 'volume': '59', 'first_page': '369-383', 'authors': '', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': "Midford, P, Midford, P, Mabee, P, Vision, T, Lapp, H, Balhoff, J, Dahdul, W, Kothari, C, Lundberg, J & Westerfield, M, 2009, 'Phenoscape: Ontologies for Large Multi-species Phenotype Datasets', <i>Nature Precedings</i>.", 'title': 'Phenoscape: Ontologies for Large Multi-species Phenotype Datasets', 'journal': '', 'year': '2009', 'authors': '', 'full_citation_type': 'formatted_unspecified'}), ('doi', '10.1111/j.1558-5646.2009.00892.x'), ('biblio', {'full_citation': "O'Meara, B, Rabosky, D, Hipp, A, de Vienne, D, Sidlauskas, B, Hunt, G, Desper, R, Smith, S, Jombart, T, Felsenstein, J, Swofford, D, Kembel, S, Harmon, L, Vision, T, Lapp, H, Waddell, P, Loarie, S, Zanne, A, Maddison, W, Alfaro, M, Zwickl, D, Midford, P, Bell, C, Paradis, E, Orme, D, Bolker, B, Price, S, Heibl, C, Butler, M & Cowan, P, 2008, 'Comparative methods in R hackathon', <i>Nature Precedings</i>.", 'title': 'Comparative methods in R hackathon', 'journal': '', 'year': '2008', 'authors': '', 'full_citation_type': 'formatted_unspecified'}), ('biblio', {'full_citation': u'Lapp, Hilmar, Sendu Bala, James P. Balhoff, Amy Bouck, Naohisa Goto, Mark Holder, Richard Holland, et al. 2007. The 2006 NESCent Phyloinformatics Hackathon: A Field Report. Evolutionary Bioinformatics Online 3: 287\u2013296.', 'title': 'The 2006 NESCent Phyloinformatics Hackathon: A Field Report', 'url': 'http://www.la-press.com/the-2006-nescent-phyloinformatics-hackathon-a-field-report-article-a480', 'journal': '', 'year': '2007', 'authors': '', 'full_citation_type': 'formatted_unspecified'}), ('biblio', {'full_citation': "Lapp, H, 2007, 'Persistent BioPerl', <i>Nature Precedings</i>.", 'title': 'Persistent BioPerl', 'journal': '', 'year': '2007', 'authors': '', 'full_citation_type': 'formatted_unspecified'}), ('doi', '10.1007/s00335-005-0145-5'), ('biblio', {'full_citation': '@article{lapp2006,\n\tvolume  = {7},\n\tnumber  = {3},\n\tpages   = {287-296},\n}\n', 'title': 'Open source tools and toolkits for bioinformatics: Significance, and where are we?', 'first_author': '', 'journal': 'Briefings in Bioinformatics', 'year': '2006', 'number': '3', 'volume': '7', 'first_page': '287-296', 'authors': '', 'full_citation_type': 'bibtex'}), ('doi', '10.1016/s0076-6879(05)03001-6'), ('biblio', {'full_citation': '@article{lapp2005,\n\tvolume  = {16},\n\tnumber  = {8},\n\tpages   = {3847-3864},\n}\n', 'title': 'Large-scale profiling of Rab GTPase trafficking networks: The membrome', 'first_author': '', 'journal': 'Molecular Biology of the Cell', 'year': '2005', 'number': '8', 'volume': '16', 'first_page': '3847-3864', 'authors': '', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{lapp2004,\n\tvolume  = {101},\n\tnumber  = {16},\n\tpages   = {6062-6067},\n}\n', 'title': 'A gene atlas of the mouse and human protein-encoding transcriptomes', 'first_author': '', 'journal': 'Proceedings of the National Academy of Sciences of the United States of America', 'year': '2004', 'number': '16', 'volume': '101', 'first_page': '6062-6067', 'authors': '', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{lapp2004,\n\tvolume  = {14},\n\tnumber  = {4},\n\tpages   = {742-749},\n}\n', 'title': 'Applications of a rat multiple tissue gene expression data set', 'first_author': '', 'journal': 'Genome Research', 'year': '2004', 'number': '4', 'volume': '14', 'first_page': '742-749', 'authors': '', 'full_citation_type': 'bibtex'}), ('doi', '10.1007/s00138-002-0114-x'), ('biblio', {'full_citation': '@article{lapp2002,\n\tvolume  = {12},\n\tnumber  = {10},\n\tpages   = {1611-1618},\n}\n', 'title': 'The Bioperl toolkit: Perl modules for the life sciences', 'first_author': '', 'journal': 'Genome Research', 'year': '2002', 'number': '10', 'volume': '12', 'first_page': '1611-1618', 'authors': '', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{lapp2001,\n\tvolume  = {4266},\n\tpages   = {1-12},\n}\n', 'title': 'A generic and robust approach for the analysis of spot array images', 'first_author': '', 'journal': 'Proceedings of SPIE - The International Society for Optical Engineering', 'year': '2001', 'number': '', 'volume': '4266', 'first_page': '1-12', 'authors': '', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{lapp2001,\n\tvolume  = {61},\n\tnumber  = {20},\n\tpages   = {7388-7393},\n}\n', 'title': 'Molecular classification of human carcinomas by use of gene expression signatures', 'first_author': '', 'journal': 'Cancer Research', 'year': '2001', 'number': '20', 'volume': '61', 'first_page': '7388-7393', 'authors': '', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{lapp2000,\n\tvolume  = {8},\n\tpages   = {46-56},\n}\n', 'title': 'Robust parametric and semi-parametric spot fitting for spot array images.', 'first_author': '', 'journal': 'Proceedings / . International Conference on Intelligent Systems for Molecular Biology ; ISMB. International Conference on Intelligent Systems for Molecular Biology', 'year': '2000', 'number': '', 'volume': '8', 'first_page': '46-56', 'authors': '', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{lapp2000,\n\tvolume  = {3},\n}\n', 'title': 'Robust spot fitting for genetic spot array images', 'first_author': '', 'journal': 'IEEE International Conference on Image Processing', 'year': '2000', 'number': '', 'volume': '3', 'first_page': '', 'authors': '', 'full_citation_type': 'bibtex'}), ('doi', '10.1007/3-540-48375-6_43')]
        assert_items_equal(members, expected)

    def test_extract_members3(self):
        f = open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE3, "r")
        members = self.provider._extract_members(f.read(), TEST_ORCID_ID3)
        print members
        expected = [('biblio', {'isbn': '0702251879', 'title': 'Moving Among Strangers: Randolph Stow and My Family', 'first_author': u'Carey', 'journal': '', 'year': '2013', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Carey', 'genre': 'book', 'full_citation': '@book{RID:1202132010973-11,\ntitle = {Moving Among Strangers: Randolph Stow and My Family},\npublisher = {},\nyear = {2013},\nauthor = {Carey, Gabrielle},\neditor = {}\n}', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{RID:1202132010973-13,\ntitle = {Randolph Stow: An Ambivalent Australian},\njournal = {Kill Your Darlings},\nyear = {2013},\nauthor = {Carey, Gabrielle},\nnumber = {12},\npages = {27}\n}', 'title': 'Randolph Stow: An Ambivalent Australian', 'first_author': u'Carey', 'journal': 'Kill Your Darlings', 'year': '2013', 'number': '12', 'volume': '', 'first_page': '27', 'authors': u'Carey', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('biblio', {'isbn': '1742759297', 'title': 'Puberty blues', 'first_author': u'Lette', 'journal': '', 'year': '2012', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Lette, Carey', 'genre': 'book', 'full_citation': '@book{RID:1202132010974-1,\ntitle = {Puberty blues},\npublisher = {},\nyear = {2012},\nauthor = {Lette, Kathy and  Carey, Gabrielle},\neditor = {}\n}', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{RID:1202132010974-15,\ntitle = {Moving Among Strangers, Darkly},\njournal = {},\nyear = {2010},\nauthor = {Gabrielle, Carey}\n}', 'title': 'Moving Among Strangers, Darkly', 'first_author': u'Gabrielle', 'journal': '', 'year': '2010', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Gabrielle', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{RID:1202132010974-9,\ntitle = {High-value niche production: what Australian wineries might learn from a Bordeaux first growth},\njournal = {International journal of technology, policy and management},\nyear = {2009},\nauthor = {Aylward, David and  Carey, Gabrielle},\nvolume = {9},\nnumber = {4},\npages = {342-357}\n}', 'title': 'High-value niche production: what Australian wineries might learn from a Bordeaux first growth', 'first_author': u'Aylward', 'journal': 'International journal of technology, policy and management', 'year': '2009', 'number': '4', 'volume': '9', 'first_page': '342-357', 'authors': u'Aylward, Carey', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('biblio', {'isbn': '1921372621', 'title': 'Waiting Room: A Memoir', 'first_author': u'Carey', 'journal': '', 'year': '2009', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Carey', 'genre': 'book', 'full_citation': '@book{RID:1202132010974-12,\ntitle = {Waiting Room: A Memoir},\npublisher = {},\nyear = {2009},\nauthor = {Carey, Gabrielle},\neditor = {}\n}', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{RID:1202132010974-10,\ntitle = {Literature and Religion as Rivals},\njournal = {Sydney Studies in Religion},\nyear = {2008},\nauthor = {Carey, Gabrielle}\n}', 'title': 'Literature and Religion as Rivals', 'first_author': u'Carey', 'journal': 'Sydney Studies in Religion', 'year': '2008', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Carey', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('doi', '10.1162/daed.2006.135.4.60'), ('biblio', {'isbn': '0733305741', 'title': 'Australian Story: Australian Lives', 'first_author': u'Carey', 'journal': '', 'year': '1997', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Carey', 'genre': 'book', 'full_citation': '@book{RID:1202132010974-7,\ntitle = {Australian Story: Australian Lives},\npublisher = {},\nyear = {1997},\nauthor = {Carey, Gabrielle},\neditor = {}\n}', 'full_citation_type': 'bibtex'}), ('biblio', {'isbn': '0140259384', 'title': 'The Penguin Book of Death', 'first_author': u'Carey', 'journal': '', 'year': '1997', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Carey, Sorensen', 'genre': 'book', 'full_citation': '@book{RID:1202132010975-8,\ntitle = {The Penguin Book of Death},\npublisher = {},\nyear = {1997},\nauthor = {Carey, Gabrielle and  Sorensen, Rosemary Lee},\neditor = {}\n}', 'full_citation_type': 'bibtex'}), ('biblio', {'full_citation': '@article{RID:1202132010975-6,\ntitle = {Prenatal Depression, Postmodern World},\njournal = {Mother love: stories about births, babies & beyond},\nyear = {1996},\nauthor = {Carey, Gabrielle},\npages = {179}\n}', 'title': 'Prenatal Depression, Postmodern World', 'first_author': u'Carey', 'journal': 'Mother love: stories about births, babies & beyond', 'year': '1996', 'number': '', 'volume': '', 'first_page': '179', 'authors': u'Carey', 'genre': 'journal article', 'full_citation_type': 'bibtex'}), ('biblio', {'isbn': '0330355988', 'title': 'The Borrowed Girl', 'first_author': u'Carey', 'journal': '', 'year': '1994', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Carey', 'genre': 'book', 'full_citation': '@book{RID:1202132010975-5,\ntitle = {The Borrowed Girl},\npublisher = {},\nyear = {1994},\nauthor = {Carey, Gabrielle},\neditor = {}\n}', 'full_citation_type': 'bibtex'}), ('biblio', {'isbn': '0330272942', 'title': "In My Father's House", 'first_author': u'Carey', 'journal': '', 'year': '1992', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Carey, Hudson', 'genre': 'book', 'full_citation': "@book{RID:1202132010975-4,\ntitle = {In My Father's House},\npublisher = {},\nyear = {1992},\nauthor = {Carey, Gabrielle and  Hudson, Elaine},\neditor = {}\n}", 'full_citation_type': 'bibtex'}), ('biblio', {'isbn': '0140074252', 'title': 'Just Us', 'first_author': u'Carey', 'journal': '', 'year': '1984', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Carey', 'genre': 'book', 'full_citation': '@book{RID:1202132010975-3,\ntitle = {Just Us},\npublisher = {},\nyear = {1984},\nauthor = {Carey, Gabrielle},\neditor = {}\n}', 'full_citation_type': 'bibtex'}), ('biblio', {'isbn': '0872237680', 'title': 'Puberty blues', 'first_author': u'Carey', 'journal': '', 'year': '1982', 'number': '', 'volume': '', 'first_page': '', 'authors': u'Carey, Lette', 'genre': 'book', 'full_citation': '@book{RID:1202132010975-2,\ntitle = {Puberty blues},\npublisher = {},\nyear = {1982},\nauthor = {Carey, Gabrielle and  Lette, Kathy},\neditor = {}\n}', 'full_citation_type': 'bibtex'})]
        print expected
        assert_items_equal(members, expected)

    def test_extract_members4(self):
        members = self.provider._extract_members(SAMPLE_EXTRACT_MEMBER_ITEMS_SHORT, TEST_ORCID_ID)
        print members
        expected = [('doi', u'10.1086/599671')]
        assert_items_equal(members, expected)

    def test_extract_members_zero_items(self):
        page = """{"message-version":"1.0.6","orcid-profile":{"orcid":{"value":"0000-0003-1613-5981"}}}"""
        members = self.provider._extract_members(page, TEST_ORCID_ID)
        assert_equals(members, [])

    @http
    def test_member_items(self):
        members = self.provider.member_items(TEST_ORCID_ID)
        print members
        expected = [('doi', '10.1002/meet.14504701413'), ('doi', '10.1038/npre.2007.425.2'), ('doi', '10.1002/meet.14504701421'), ('doi', '10.1038/npre.2008.2152.1'), ('doi', '10.1038/npre.2007.361'), ('doi', '10.1038/473285a'), ('doi', '10.1038/npre.2010.4267.1'), ('doi', '10.1016/j.joi.2009.11.010'), ('doi', '10.1038/npre.2010.5452.1')]
        assert len(members) >= len(expected), str(members)

    @http
    def test_member_items_url_format(self):
        members = self.provider.member_items("http://orcid.org/" + TEST_ORCID_ID)
        print members
        expected = [('doi', '10.1002/meet.14504701413'), ('doi', '10.1038/npre.2007.425.2'), ('doi', '10.1002/meet.14504701421'), ('doi', '10.1038/npre.2008.2152.1'), ('doi', '10.1038/npre.2007.361'), ('doi', '10.1038/473285a'), ('doi', '10.1038/npre.2010.4267.1'), ('doi', '10.1016/j.joi.2009.11.010'), ('doi', '10.1038/npre.2010.5452.1')]
        assert len(members) >= len(expected), str(members)

    @http
    def test_member_items_some_missing_dois(self):
        members = self.provider.member_items("0000-0001-5109-3700")  #another.  some don't have dois
        print members
        expected = [('doi', u'10.1087/20120404'), ('doi', u'10.1093/scipol/scs030'), ('doi', u'10.1126/science.caredit.a1200080'), ('doi', u'10.1016/S0896-6273(02)01067-X'), ('doi', u'10.1111/j.1469-7793.2000.t01-2-00019.xm'), ('doi', u'10.1046/j.0022-3042.2001.00727.x'), ('doi', u'10.1097/ACM.0b013e31826d726b'), ('doi', u'10.1126/science.1221840'), ('doi', u'10.1016/j.brainresbull.2006.08.006'), ('doi', u'10.1016/0006-8993(91)91536-A'), ('doi', u'10.1076/jhin.11.1.70.9111'), ('doi', u'10.2139/ssrn.1677993')]
        assert len(members) >= len(expected), str(members)


    @http
    def test_member_items_mla_format(self):
        members = self.provider.member_items("http://orcid.org/" + "0000-0002-3878-917X")
        print members
        expected = "hi"
        assert len(members) >= len(expected), str(members)



########NEW FILE########
__FILENAME__ = test_plosalm
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/plosalm")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")

TEST_DOI = "10.1371/journal.pcbi.1000361"

class TestPlosalm(ProviderTestCase):

    provider_name = "plosalm"

    testitem_aliases = ("doi", TEST_DOI)
    testitem_metrics = ("doi", TEST_DOI)

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

    def test_provenance_url(self):
        # ensure that it matches an appropriate ids
        response = self.provider.provenance_url("plosalm:html_views", [self.testitem_aliases])
        print response
        assert_equals(response, "http://dx.doi.org/10.1371/journal.pcbi.1000361")

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics(good_page)
        print metrics_dict
        expected = {'plosalm:pdf_views': 952, 'plosalm:html_views': 13642}
        assert_equals(metrics_dict, expected)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'plosalm:pdf_views': (1428, 'http://dx.doi.org/10.1371/journal.pcbi.1000361'), 'plosalm:html_views': (20720, 'http://dx.doi.org/10.1371/journal.pcbi.1000361')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]




########NEW FILE########
__FILENAME__ = test_plossearch
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/plossearch")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")

class TestPlossearch(ProviderTestCase):

    provider_name = "plossearch"

    testitem_aliases = ("url", "http://hdl.handle.net/10255/dryad.235")
    testitem_metrics = ("url", "http://hdl.handle.net/10255/dryad.235")

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics(good_page)
        expected = {'plossearch:mentions': 1}
        assert_equals(metrics_dict, expected)

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("tweets", 
            [self.testitem_aliases])
        expected = 'http://www.plosone.org/search/advanced?queryTerm=&unformattedQuery=everything:"hdl.handle.net%2F10255%2Fdryad.235"'
        assert_equals(provenance_url, expected)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'plossearch:mentions': (2, 'http://www.plosone.org/search/advanced?queryTerm=&unformattedQuery=everything:"hdl.handle.net%2F10255%2Fdryad.235"')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_multiple_urls(self):
        metrics_dict = self.provider.metrics([("url","http://dx.doi.org/10.5061/dryad.234"), 
                                                ("doi", "10.5061/dryad.234"), 
                                                ("url","http://hdl.handle.net/10255/dryad.235")])
        expected = {'plossearch:mentions': (2, 'http://www.plosone.org/search/advanced?queryTerm=&unformattedQuery=everything:"hdl.handle.net%2F10255%2Fdryad.235"')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]



########NEW FILE########
__FILENAME__ = test_pmc
import os, collections, simplejson

from totalimpact import db, app
from totalimpact.providers import pmc
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderFactory
from totalimpact import provider_batch_data
from test.utils import http
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests

from nose.tools import assert_equals, raises, nottest, assert_items_equal

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/pmc")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "monthly_download")
SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH = os.path.join(datadir, "monthly_download_different_month")

TEST_PMID = "23066504"

class TestPmc(ProviderTestCase):

    provider_name = "pmc"

    testitem_aliases = ("pmid", TEST_PMID)
    testitem_metrics = ("pmid", TEST_PMID)

    def setUp(self):
        ProviderTestCase.setUp(self)

        self.db = setup_postgres_for_unittests(db, app)

        sample_data_dump = open(SAMPLE_EXTRACT_METRICS_PAGE, "r").read()
        sample_data_dump_different_month = open(SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH, "r").read()

        test_monthly_data = [
            {"_id": "abc", 
                "type": "provider_data_dump", 
                "provider": "pmc", 
                "raw": sample_data_dump,
                "provider_raw_version": 1.0,
                "created": "2012-11-29T07:34:01.126892",
                "aliases": {"pmid":["111", "222"]},
                "min_event_date": "2012-10-01T07:34:01.126892",
                "max_event_date": "2012-10-31T07:34:01.126892"
            },
            {"_id": "def", 
                "type": "provider_data_dump", 
                "provider": "pmc", 
                "raw": sample_data_dump_different_month,
                "provider_raw_version": 1.0,
                "created": "2012-11-29T08:34:01.126892",
                "aliases": {"pmid":["111"]},
                "min_event_date": "2012-01-01T07:34:01.126892",
                "max_event_date": "2012-01-31T07:34:01.126892"
            },
            {
               "_id": "abc123",
               "raw": "<pmc-web-stat><request year=\"2012\" month=\"10\" jrid=\"elife\" eissn=\"2050-084X\"></request><response status=\"0\" collection=\"eLife\"></response><articles><article id=\"PMC3463246\"><meta-data doi=\"10.7554/eLife.00013\" pmcid=\"PMC3463246\" pubmed-id=\"23066504\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00013\"/><usage unique-ip=\"1368\" full-text=\"1464\" pdf=\"722\" abstract=\"119\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"144\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3463247\"><meta-data doi=\"10.7554/eLife.00240\" pmcid=\"PMC3463247\" pubmed-id=\"23066507\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00240\"/><usage unique-ip=\"514\" full-text=\"606\" pdf=\"230\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"9\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3465569\"><meta-data doi=\"10.7554/eLife.00242\" pmcid=\"PMC3465569\" pubmed-id=\"23066508\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00242\"/><usage unique-ip=\"473\" full-text=\"503\" pdf=\"181\" abstract=\"2\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"13\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3465570\"><meta-data doi=\"10.7554/eLife.00243\" pmcid=\"PMC3465570\" pubmed-id=\"23066509\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00243\"/><usage unique-ip=\"547\" full-text=\"636\" pdf=\"227\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"56\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3466591\"><meta-data doi=\"10.7554/eLife.00065\" pmcid=\"PMC3466591\" pubmed-id=\"23066506\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00065\"/><usage unique-ip=\"2516\" full-text=\"2804\" pdf=\"1583\" abstract=\"195\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"405\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3466783\"><meta-data doi=\"10.7554/eLife.00007\" pmcid=\"PMC3466783\" pubmed-id=\"23066503\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00007\"/><usage unique-ip=\"1331\" full-text=\"1412\" pdf=\"898\" abstract=\"224\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"109\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3467772\"><meta-data doi=\"10.7554/eLife.00270\" pmcid=\"PMC3467772\" pubmed-id=\"23066510\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00270\"/><usage unique-ip=\"1396\" full-text=\"1776\" pdf=\"625\" abstract=\"4\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3470722\"><meta-data doi=\"10.7554/eLife.00286\" pmcid=\"PMC3470722\" pubmed-id=\"23071903\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00286\"/><usage unique-ip=\"909\" full-text=\"1030\" pdf=\"376\" abstract=\"6\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3479833\"><meta-data doi=\"10.7554/eLife.00031\" pmcid=\"PMC3479833\" pubmed-id=\"23110253\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00031\"/><usage unique-ip=\"154\" full-text=\"126\" pdf=\"87\" abstract=\"26\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"13\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3470409\"><meta-data doi=\"10.7554/eLife.00048\" pmcid=\"PMC3470409\" pubmed-id=\"23066505\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00048\"/><usage unique-ip=\"1250\" full-text=\"1361\" pdf=\"911\" abstract=\"237\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"317\" supp-data=\"4\" cited-by=\"0\"/></article><article id=\"PMC3482692\"><meta-data doi=\"10.7554/eLife.00102\" pmcid=\"PMC3482692\" pubmed-id=\"23110254\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00102\"/><usage unique-ip=\"259\" full-text=\"232\" pdf=\"133\" abstract=\"36\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"8\" supp-data=\"3\" cited-by=\"0\"/></article><article id=\"PMC3482687\"><meta-data doi=\"10.7554/eLife.00281\" pmcid=\"PMC3482687\" pubmed-id=\"23110255\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00281\"/><usage unique-ip=\"75\" full-text=\"53\" pdf=\"47\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"1\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3482686\"><meta-data doi=\"10.7554/eLife.00005\" pmcid=\"PMC3482686\" pubmed-id=\"23110252\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00005\"/><usage unique-ip=\"324\" full-text=\"249\" pdf=\"263\" abstract=\"71\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"93\" supp-data=\"17\" cited-by=\"0\"/></article></articles></pmc-web-stat>",
               "max_event_date": "2012-10-31T07:34:01.126892",
               "provider": "pmc",
               "aliases": {
                   "pmid": [
                       "23066504",
                       "23066507",
                       "23066508",
                       "23066509",
                       "23066506",
                       "23066503",
                       "23066510",
                       "23071903",
                       "23110253",
                       "23066505",
                       "23110254",
                       "23110255",
                       "23110252"
                   ]
               },
               "provider_raw_version": 1,
               "type": "provider_data_dump",
               "min_event_date": "2012-10-02T07:34:01.126892",
               "created": "2012-11-29T09:34:01.126892"
            }
        ]
        #print test_monthly_data
        for doc in test_monthly_data:
            new_object = provider_batch_data.create_objects_from_doc(doc)
            print new_object

        self.provider = pmc.Pmc()
        print "after pmc"

    def tearDown(self):
        teardown_postgres_for_unittests(self.db)


    def test_has_applicable_batch_data_true(self):
        # ensure that it matches an appropriate ids
        response = self.provider.has_applicable_batch_data("pmid", "111")
        assert_equals(response, True)

    def test_has_applicable_batch_data_false(self):
        # ensure that it matches an appropriate ids
        response = self.provider.has_applicable_batch_data("pmid", "notapmidintheview")
        assert_equals(response, False)

    def test_build_batch_data_dict(self):
        # ensure that it matches an appropriate ids
        response = self.provider.build_batch_data_dict()
        #print response
        print response.keys()
        expected = [('pmid', '23071903'), ('pmid', '23066503'), ('pmid', '111'), ('pmid', '23110254'), ('pmid', '23110252'), ('pmid', '23066505'), ('pmid', '23066504'), ('pmid', '23110255'), ('pmid', '23066507'), ('pmid', '23066506'), ('pmid', '23066510'), ('pmid', '23066509'), ('pmid', '23066508'), ('pmid', '222'), ('pmid', '23110253')]
        assert_items_equal(response.keys(), expected)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics(good_page, id="222")
        print metrics_dict
        expected = {'pmc:unique_ip_views': 514, 'pmc:pdf_downloads': 230, 'pmc:fulltext_views': 606, 'pmc:figure_views': 9}
        assert_equals(metrics_dict, expected)

    def test_provider_metrics_500(self):
        pass  # Not applicable

    def test_provider_metrics_400(self):
        pass  # Not applicable

    def test_provider_metrics_nonsense_xml(self):
        pass  # Not applicable

    def test_provider_metrics_nonsense_txt(self):
        pass  # Not applicable

    def test_provider_metrics_empty(self):
        pass  # Not applicable

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([("pmid", "222")])
        expected = {'pmc:unique_ip_views': (514, ''), 'pmc:pdf_downloads': (230, ''), 'pmc:fulltext_views': (606, ''), 'pmc:figure_views': (9, '')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_multiple_months(self):
        metrics_dict = self.provider.metrics([("pmid", "111")])
        expected = {'pmc:abstract_views': (218, ''), 'pmc:pdf_downloads': (810, ''), 'pmc:fulltext_views': (1530, ''), 'pmc:figure_views': (177, ''), 'pmc:unique_ip_views': (1923, '')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_real(self):
        metrics_dict = self.provider.metrics([("pmid", "23066504")])
        expected = {'pmc:abstract_views': (119, ''), 'pmc:pdf_downloads': (722, ''), 'pmc:fulltext_views': (1464, ''), 'pmc:figure_views': (144, ''), 'pmc:unique_ip_views': (1368, '')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]


########NEW FILE########
__FILENAME__ = test_provider
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderFactory
from totalimpact import app, db
from nose.tools import assert_equals, nottest
from xml.dom import minidom 
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests

import simplejson, BeautifulSoup
import os
from sqlalchemy.sql import text    

sampledir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/")

class Test_Provider():

    TEST_PROVIDER_CONFIG = [
        ("pubmed", { "workers":1 }),
        ("wikipedia", {"workers": 3}),
        ("mendeley", {"workers": 3}),
    ]
    
    TEST_JSON = """{"repository":{"homepage":"","watchers":7,"has_downloads":true,"fork":false,"language":"Java","has_issues":true,"has_wiki":true,"forks":0,"size":4480,"private":false,"created_at":"2008/09/29 04:26:42 -0700","name":"gtd","owner":"egonw","description":"Git-based ToDo tool.","open_issues":2,"url":"https://github.com/egonw/gtd","pushed_at":"2012/02/28 10:21:26 -0800"}}"""

    TEST_XML = open(os.path.join(sampledir, "facebook", "metrics")).read()

    def setUp(self):
        self.db = setup_postgres_for_unittests(db, app)
        
    def tearDown(self):
        teardown_postgres_for_unittests(self.db)

    def test_get_provider(self):
        provider = ProviderFactory.get_provider("wikipedia")
        assert_equals(provider.__class__.__name__, "Wikipedia")
        
    def test_get_providers(self):
        providers = ProviderFactory.get_providers(self.TEST_PROVIDER_CONFIG)
        provider_names = [provider.__class__.__name__ for provider in providers]
        assert_equals(set(provider_names), set(['Mendeley', 'Wikipedia', "Pubmed"]))

    def test_get_providers_filters_by_metrics(self):
        # since all the providers do metrics, "metrics" arg changes nought.
        providers = ProviderFactory.get_providers(self.TEST_PROVIDER_CONFIG, "metrics")
        provider_names = [provider.__class__.__name__ for provider in providers]
        assert_equals(set(provider_names), set(['Mendeley', 'Wikipedia', "Pubmed"]))

    def test_get_providers_filters_by_biblio(self):
        providers = ProviderFactory.get_providers(self.TEST_PROVIDER_CONFIG, "biblio")
        provider_names = [provider.__class__.__name__ for provider in providers]
        assert_equals(set(provider_names), set(['Pubmed', 'Mendeley']))

    def test_get_providers_filters_by_aliases(self):
        providers = ProviderFactory.get_providers(self.TEST_PROVIDER_CONFIG, "aliases")
        provider_names = [provider.__class__.__name__ for provider in providers]
        assert_equals(set(provider_names), set(['Pubmed', 'Mendeley']))

    def test_lookup_json(self):
        page = self.TEST_JSON
        data = simplejson.loads(page)
        response = provider._lookup_json(data, ['repository', 'name'])
        assert_equals(response, u'gtd')

    def test_extract_json(self):
        page = self.TEST_JSON
        dict_of_keylists = {
            'title' : ['repository', 'name'],
            'description' : ['repository', 'description']}

        response = provider._extract_from_json(page, dict_of_keylists)
        assert_equals(response, {'description': u'Git-based ToDo tool.', 'title': u'gtd'})
    
    def test_lookup_xml_from_dom(self):
        page = self.TEST_XML
        doc = minidom.parseString(page.strip())
        response = provider._lookup_xml_from_dom(doc, ['total_count'])
        assert_equals(response, 17)

    def test_lookup_xml_from_soup(self):
        page = self.TEST_XML
        doc = BeautifulSoup.BeautifulStoneSoup(page) 
        response = provider._lookup_xml_from_soup(doc, ['total_count'])
        assert_equals(response, 17)

    def test_extract_xml(self):
        page = self.TEST_XML
        dict_of_keylists = {
            'count' : ['total_count']}

        response = provider._extract_from_xml(page, dict_of_keylists)
        assert_equals(response, {'count': 17})

    def test_doi_from_url_string(self):
        test_url = "https://knb.ecoinformatics.org/knb/d1/mn/v1/object/doi:10.5063%2FAA%2Fnrs.373.1"
        expected = "10.5063/AA/nrs.373.1"
        response = provider.doi_from_url_string(test_url)
        assert_equals(response, expected)

    def test_is_issn_in_doaj_false(self):
        response = provider.is_issn_in_doaj("invalidissn")
        assert_equals(response, False)

    def test_is_issn_in_doaj_true(self):
        zookeys_issn = "13132989"  #this one is in test setup
        response = provider.is_issn_in_doaj(zookeys_issn)
        assert_equals(response, True)

    def test_import_products(self):
        response = provider.import_products("product_id_strings", 
                {"product_id_strings": ["123456", "HTTPS://starbucks.com", "arXiv:1305.3328", "http://doi.org/10.123/ABC"]})
        expected = [('pmid', '123456'), ('url', 'HTTPS://starbucks.com'), ('arxiv', '1305.3328'), ('doi', '10.123/abc')]
        assert_equals(response, expected)

    def test_import_products_bad_providername(self):
        response = provider.import_products("nonexistant", {})
        expected = []
        assert_equals(response, expected)



class TestProviderFactory():

    TEST_PROVIDER_CONFIG = [
        ("pubmed", { "workers":1 }),
        ("wikipedia", {"workers": 3}),
        ("mendeley", {"workers": 3}),
    ]

    def test_get_all_static_meta(self):
        sm = ProviderFactory.get_all_static_meta(self.TEST_PROVIDER_CONFIG)
        expected = 'The number of citations by papers in PubMed Central'
        assert_equals(sm["pubmed:pmc_citations"]["description"], expected)

    def test_get_all_metric_names(self):
        response = ProviderFactory.get_all_metric_names(self.TEST_PROVIDER_CONFIG)
        expected = ['wikipedia:mentions', 'mendeley:country', 'pubmed:pmc_citations_reviews', 'mendeley:discipline', 'pubmed:f1000', 'mendeley:career_stage', 'pubmed:pmc_citations_editorials', 'mendeley:readers', 'pubmed:pmc_citations', 'mendeley:groups']
        assert_equals(response, expected)

    def test_get_all_metadata(self):
        md = ProviderFactory.get_all_metadata(self.TEST_PROVIDER_CONFIG)
        print md["pubmed"]
        assert_equals(md["pubmed"]['url'], 'http://pubmed.gov')


########NEW FILE########
__FILENAME__ = test_pubmed
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from test.utils import http
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact import app, db
from totalimpact.providers import provider
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/pubmed")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_ALIASES_FROM_DOI_PAGE = os.path.join(datadir, "aliases_from_doi")
SAMPLE_EXTRACT_ALIASES_FROM_PMID_PAGE = os.path.join(datadir, "aliases_from_pmid")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")
SAMPLE_EXTRACT_BIBLIO_ELINK_PAGE = os.path.join(datadir, "biblio_elink")
SAMPLE_EXTRACT_PROVENANCE_URL_PAGE = SAMPLE_EXTRACT_METRICS_PAGE

TEST_DOI = "10.1371/journal.pcbi.1000361"
TEST_PMID = "16060722"
TEST_DOI_HAS_NO_PMID = "10.1016/j.artint.2005.10.007"

class TestPubmed(ProviderTestCase):

    provider_name = "pubmed"

    testitem_aliases = ("pmid", TEST_PMID)
    testitem_biblio = ("pmid", TEST_PMID)
    testitem_metrics = ("pmid", TEST_PMID)
    testitem_members = "123\n456\n789"

    def setUp(self):
        self.db = setup_postgres_for_unittests(db, app)
        ProviderTestCase.setUp(self)
        
    def tearDown(self):
        teardown_postgres_for_unittests(self.db)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("github", "egonw,cdk")), False)
  
    def test_extract_aliases_from_doi(self):
        # ensure that the dryad reader can interpret an xml doc appropriately
        f = open(SAMPLE_EXTRACT_ALIASES_FROM_DOI_PAGE, "r")
        aliases = self.provider._extract_aliases_from_doi(f.read(), "10.1371/journal.pcbi.1000361")
        print aliases
        assert_equals(aliases, [('pmid', '19381256')])

    def test_extract_aliases_from_pmid(self):
        # ensure that the dryad reader can interpret an xml doc appropriately
        f = open(SAMPLE_EXTRACT_ALIASES_FROM_PMID_PAGE, "r")
        aliases = self.provider._extract_aliases_from_pmid(f.read(), "17593900")
        print aliases
        expected = [('doi', u'10.1371/journal.pmed.0040215'), ('url', u'http://dx.doi.org/10.1371/journal.pmed.0040215'), ('pmc', u'PMC1896210'), ('url', u'http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1896210')]
        assert_equals(aliases, expected)

    # override default because returns url even if pmid api page is empty
    def test_provider_aliases_empty(self):
        Provider.http_get = common.get_empty
        aliases = self.provider.aliases([self.testitem_aliases])
        print aliases
        assert_equals(aliases, [("url", 'http://www.ncbi.nlm.nih.gov/pubmed/16060722')])

    def test_extract_citing_pmcids(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        pmcids = self.provider._extract_citing_pmcids(f.read())
        assert_equals(len(pmcids), 149)

    def test_extract_biblio_efetch(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        ret = self.provider._extract_biblio_efetch(f.read())
        print ret
        expected = {'title': u'Why most published research findings are false.', 'journal': u'PLoS medicine', 'issn': u'15491676', 'authors': u'Ioannidis', 'year': '2005', 'date': '2005-08-30T00:00:00'}
        assert_equals(ret, expected)

    def test_extract_biblio_elink(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_ELINK_PAGE, "r")
        ret = self.provider._extract_biblio_elink(f.read())
        print ret
        expected = {'free_fulltext_url': 'http://www.nejm.org/doi/abs/10.1056/NEJMp1314561?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%3dwww.ncbi.nlm.nih.gov'}
        assert_equals(ret, expected)

    def test_member_items(self):
        ret = self.provider.member_items(self.testitem_members)
        print ret
        expected = [('pmid', '123'), ('pmid', '456'), ('pmid', '789')]
        assert_equals(ret, expected)        

    def test_provider_member_items_400(self):
        pass
    def test_provider_member_items_500(self):
        pass
    def test_provider_member_items_empty(self):
        pass
    def test_provider_member_items_nonsense_txt(self):
        pass
    def test_provider_member_items_nonsense_xml(self):
        pass

    @http
    def test_biblio(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio])
        print biblio_dict
        expected = {'authors': u'Ioannidis', 'title': u'Why most published research findings are false.', 'date': '2005-08-30T00:00:00', 'free_fulltext_url': 'http://dx.plos.org/10.1371/journal.pmed.0020124', 'journal': u'PLoS medicine', 'issn': u'15491676', 'year': '2005'}
        assert_equals(biblio_dict, expected)

    @http
    def test_biblio_free_full_text(self):
        biblio_dict = self.provider.biblio([("pmid", "24251383")])
        print biblio_dict
        expected = {'authors': u'Collins, Hamburg', 'title': u'First FDA authorization for next-generation sequencer.', 'date': '2013-11-19T00:00:00', 'free_fulltext_url': 'http://www.nejm.org/doi/abs/10.1056/NEJMp1314561?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%3dwww.ncbi.nlm.nih.gov', 'journal': u'The New England journal of medicine', 'issn': u'15334406', 'year': '2013'}
        assert_equals(biblio_dict, expected)        

    @http
    def test_biblio_no_free_full_text(self):
        biblio_dict = self.provider.biblio([("pmid", "20150669")])
        print biblio_dict
        expected =  {'title': u'Integrating data clustering and visualization for the analysis of 3D gene expression data.', 'journal': u'IEEE/ACM transactions on computational biology and bioinformatics / IEEE, ACM', 'issn': u'15579964', 'authors': u'R\xfcbel, Weber, Huang, Bethel, Biggin, Fowlkes, Luengo Hendriks, Ker\xe4nen, Eisen, Knowles, Malik, Hagen, Hamann'}
        assert_equals(biblio_dict, expected)        

    @http
    def test_aliases_from_pmid(self):
        aliases = self.provider.aliases([self.testitem_aliases])
        print aliases
        expected = [('biblio', {'title': u'Why most published research findings are false.', 'journal': u'PLoS medicine', 'issn': u'15491676', 'authors': u'Ioannidis', 'year': '2005', 'date': '2005-08-30T00:00:00'}), ('doi', u'10.1371/journal.pmed.0020124'), ('pmc', u'PMC1182327'), ('url', u'http://dx.doi.org/10.1371/journal.pmed.0020124'), ('url', u'http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182327'), ('url', 'http://www.ncbi.nlm.nih.gov/pubmed/16060722')]
        assert_equals(aliases, expected)

    @http
    def test_aliases_from_pmid_different_date_format(self):
        aliases = self.provider.aliases([("pmid", "11457506")])
        print aliases
        expected = [('biblio', {'title': u'Radiation hybrid mapping of 11 alpha and beta nicotinic acetylcholine receptor genes in Rattus norvegicus.', 'journal': u'Brain research. Molecular brain research', 'issn': u'0169328X', 'month': u'Jul', 'authors': u'Tseng, Kwitek-Black, Erbe, Popper, Jacob, Wackym', 'year': '2001', 'day': 13}), ('url', 'http://www.ncbi.nlm.nih.gov/pubmed/11457506')]
        assert_equals(aliases, expected)


    @http
    def test_aliases_from_pmid_when_doi_fragment(self):
        #this pmid has a partial doi in its doi field.  Make sure we don't include it in our doi field.
        aliases = self.provider.aliases([("pmid", "11244366")])
        print aliases
        expected = [('biblio', {'authors': u'Oshima, Ikeda, Furukawa, Suzuki, Takasaka', 'journal': u'ORL; journal for oto-rhino-laryngology and its related specialties', 'issn': u'03011569', 'title': u'Expression of the voltage-dependent chloride channel ClC-3 in human nasal tissue.'}), ('url', 'http://www.ncbi.nlm.nih.gov/pubmed/11244366')]
        assert_equals(aliases, expected)

    @http
    def test_aliases_from_pmid_when_doi_in_different_part_of_xml(self):
        aliases = self.provider.aliases([("pmid", "23682040")])
        print aliases
        expected = [('biblio', {'title': u'Influenza: marketing vaccine by marketing disease.', 'journal': u'BMJ (Clinical research ed.)', 'issn': u'17561833', 'authors': u'Doshi', 'year': '2013', 'date': '2013-05-16T00:00:00'}), ('doi', u'10.1136/bmj.f3037'), ('url', u'http://dx.doi.org/10.1136/bmj.f3037'), ('url', 'http://www.ncbi.nlm.nih.gov/pubmed/23682040')]
        assert_equals(aliases, expected)

    @http
    def test_aliases_from_doi(self):
        aliases_dict = self.provider.aliases([("doi", TEST_DOI)])
        assert_equals(set(aliases_dict), set([('pmid', '19381256')]))

        aliases_dict = self.provider.aliases([("doi", "TEST_DOI_HAS_NO_PMID")])
        assert_equals(aliases_dict, [])

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        print metrics_dict
        expected = {'pubmed:pmc_citations': (149, 'http://www.ncbi.nlm.nih.gov/pubmed?linkname=pubmed_pubmed_citedin&from_uid=16060722'), 'pubmed:f1000': (True, 'http://f1000.com/pubmed/16060722'), 'pubmed:pmc_citations_reviews': (20, 'http://www.ncbi.nlm.nih.gov/pubmed?term=22182676%20OR%2022065657%20OR%2021998558%20OR%2021890791%20OR%2021788505%20OR%2021407270%20OR%2020967426%20OR%2020637084%20OR%2020571517%20OR%2020420659%20OR%2020382258%20OR%2020307281%20OR%2019956635%20OR%2019860651%20OR%2019207020%20OR%2018834308%20OR%2018612135%20OR%2018603647%20OR%2017705840%20OR%2017587446&cmd=DetailsSearch'), 'pubmed:pmc_citations_editorials': (11, 'http://www.ncbi.nlm.nih.gov/pubmed?term=22515987%20OR%2022285994%20OR%2021693091%20OR%2021153562%20OR%2020876290%20OR%2020596038%20OR%2020420659%20OR%2020040241%20OR%2019967369%20OR%2019949717%20OR%2017880356&cmd=DetailsSearch')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]

            # the drilldown url changes with the metrics for some pubmed metrics, so don't check those
            if (key != "pubmed:pmc_citations_editorials") and (key != "pubmed:pmc_citations_reviews"):
                assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_with_dup_pmids(self):
        metrics_dict = self.provider.metrics([("pmid", "22198174")])
        print metrics_dict
        expected = {'pubmed:pmc_citations': (6, 'http://www.ncbi.nlm.nih.gov/pubmed?linkname=pubmed_pubmed_citedin&from_uid=22198174'), 'pubmed:pmc_citations_reviews': (2, 'http://www.ncbi.nlm.nih.gov/pubmed?term=22886409%2520OR%252022732550&cmd=DetailsSearch')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]

            # the drilldown url changes with the metrics for some pubmed metrics, so don't check those
            if (key != "pubmed:pmc_citations_editorials") and (key != "pubmed:pmc_citations_reviews"):
                assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]



########NEW FILE########
__FILENAME__ = test_scienceseeker
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/scienceseeker")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")

TEST_DOI = "10.1016/j.cbpa.2010.06.169"

class TestScienceseeker(ProviderTestCase):

    provider_name = "scienceseeker"

    testitem_aliases = ("doi", TEST_DOI)
    testitem_metrics = ("doi", TEST_DOI)

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics(good_page)
        print metrics_dict
        expected = {'scienceseeker:blog_posts': 1}
        assert_equals(metrics_dict, expected)

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("blog_posts", 
            [self.testitem_aliases])
        expected = 'http://scienceseeker.org/posts/?type=post&filter0=citation&modifier0=id-all&value0=10.1016/j.cbpa.2010.06.169'
        assert_equals(provenance_url, expected)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'scienceseeker:blog_posts': (1, 'http://scienceseeker.org/posts/?type=post&filter0=citation&modifier0=id-all&value0=10.1016/j.cbpa.2010.06.169')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_another(self):
        metrics_dict = self.provider.metrics([("doi", "10.1371/journal.pone.0035769")])
        expected = {'scienceseeker:blog_posts': (1, 'http://scienceseeker.org/posts/?type=post&filter0=citation&modifier0=id-all&value0=10.1371/journal.pone.0035769')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

########NEW FILE########
__FILENAME__ = test_scopus
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/scopus")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")

TEST_ID = "10.1371/journal.pone.0000308"
TEST_BIBLIO = {"title":"Scientometrics 2.0: Toward new metrics of scholarly impact on the social Web", 
                "journal":"First Monday", 
                "first_author":"Priem"}
TEST_BIBLIO2 = {
                    "authors": "Quezada IM, Gianoli E", 
                    "genre": "article", 
                    "journal": "Polish Journal of Ecology", 
                    "title": "Simulated herbivory limits phenotypic responses to drought in Convolvulus demissus choisy (Convolvulaceae)", 
                    "year": "2006"
                }

class TestScopus(ProviderTestCase):

    provider_name = "scopus"

    testitem_aliases = ("doi", TEST_ID)
    testitem_metrics = ("doi", TEST_ID)

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        relevant_record = self.provider._extract_relevant_record(good_page, id=TEST_ID)
        metrics_dict = self.provider._extract_metrics(relevant_record, id=TEST_ID)
        expected = {'scopus:citations': 97}
        assert_equals(metrics_dict, expected)

    def test_extract_relevant_record_with_doi(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        relevant_record = self.provider._extract_relevant_record(good_page, id=TEST_ID)
        print relevant_record
        expected = {'prism:url': 'http://api.elsevier.com/content/abstract/scopus_id:36248970413', '@_fa': 'true', 'citedby-count': '97'}
        assert_equals(relevant_record, expected)

    def test_extract_relevant_record_with_biblio(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        relevant_record = self.provider._extract_relevant_record(good_page, id=TEST_BIBLIO)
        print relevant_record
        expected = {'prism:url': 'http://api.elsevier.com/content/abstract/scopus_id:36248970413', '@_fa': 'true', 'citedby-count': '97'}
        assert_equals(relevant_record, expected)        

    def test_provenance_url(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        relevant_record = self.provider._extract_relevant_record(good_page, id=TEST_ID)
        provenance_url = self.provider._extract_provenance_url(relevant_record, id=TEST_ID)
        expected = "http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b&scp=36248970413"
        assert_equals(provenance_url, expected)

    @http
    def test_metrics_with_bad_doi(self):
        metrics_dict = self.provider.metrics([("doi", "NOTAVALIDDOI")])
        expected = {}
        print metrics_dict
        assert_equals(metrics_dict, expected)

    @http
    def test_metrics_with_doi(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'scopus:citations': (65, u'http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b&scp=36248970413')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_with_biblio(self):
        metrics_dict = self.provider.metrics([("biblio", TEST_BIBLIO)])
        expected = {'scopus:citations': (20, u'http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b&scp=77956197364')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_with_biblio2(self):
        metrics_dict = self.provider.metrics([("biblio", TEST_BIBLIO2)])
        expected = {'scopus:citations': (7, 'http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b&scp=33750324740')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics2(self):
        metrics_dict = self.provider.metrics([("doi", "10.1371/journal.pbio.0040286")])
        expected = {'scopus:citations': (113, u'http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b&scp=33748598232')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]



    @http
    def test_metrics_case_insensitivity(self):
        metrics_dict = self.provider.metrics([("doi", "10.1017/s0022112005007494")])
        expected = {'scopus:citations': (179, u'http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b&scp=32044436746')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_from_strange_doi(self):
        metrics_dict = self.provider.metrics([("doi", "10.1175/1520-0450(1994)033<0140:astmfm>2.0.co;2")])
        expected = {}
        print metrics_dict
        assert_equals(metrics_dict, expected)

    @http
    def test_metrics_from_strange_doi2(self):
        metrics_dict = self.provider.metrics([
            ("doi", "10.1670/0022-1511(2007)41[483:ADOECD]2.0.CO;2"), 
            ("biblio", {
                    "authors": "McCallum", 
                    "first_author": "McCallum", 
                    "first_page": "483", 
                    "genre": "article", 
                    "journal": "Journal of Herpetology", 
                    "number": "3", 
                    "title": "Amphibian decline or extinction? Current declines dwarf background extinction rate", 
                    "volume": "41", 
                    "year": "2007"
                })])
        expected = {'scopus:citations': (62, 'http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b&scp=35148813206')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]            

    @http
    def test_metrics_many_citations_from_biblio(self):
        biblio = {
                            "authors": "Daly, Neilson, Phillips", 
                            "journal": "Journal of Applied Meteorology", 
                            "title": "A Statistical-Topographic Model for Mapping Climatological Precipitation over Mountainous Terrain", 
                            "year": "1994"
                            }

        metrics_dict = self.provider.metrics([("biblio", biblio)])
        expected = {'scopus:citations': (1091, 'http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b&scp=0028552275')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    def test_provider_metrics_500(self):
        pass



########NEW FILE########
__FILENAME__ = test_slideshare
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact.providers import provider
from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises, nottest, assert_true

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/slideshare")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE = os.path.join(datadir, "members")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_ALIASES_PAGE = os.path.join(datadir, "aliases")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

TEST_URL = "http://www.slideshare.net/cavlec/manufacturing-serendipity-12176916"
TEST_URL2 = "www.slideshare.net/hpiwowar/right-time-right-place-to-change-the-world"
TEST_SLIDESHARE_USER = "cavlec"

class TestSlideshare(ProviderTestCase):

    provider_name = "slideshare"

    testitem_members = "cavlec"
    testitem_aliases = ("url", TEST_URL)
    testitem_metrics = ("url", TEST_URL)
    testitem_biblio = ("url", TEST_URL)

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("github", "egonw,cdk")), False)
  
    def test_extract_members(self):
        f = open(SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE, "r")
        members = self.provider._extract_members(f.read(), TEST_SLIDESHARE_USER)
        assert_equals(len(members), 36)
        assert_true('url', u'http://www.slideshare.net/cavlec/avoiding-heronway' in members)

    def test_extract_biblio(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        ret = self.provider._extract_biblio(f.read())
        assert_equals(ret, {'username': u'cavlec', 'title': u'Manufacturing Serendipity', 'repository': 'Slideshare', 'created': u'Tue Mar 27 10:10:11 -0500 2012'})

    def test_extract_aliases(self):
        # ensure that the dryad reader can interpret an xml doc appropriately
        f = open(SAMPLE_EXTRACT_ALIASES_PAGE, "r")
        aliases = self.provider._extract_aliases(f.read())
        assert_equals(aliases, [('title', u'Manufacturing Serendipity')])

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        metrics_dict = self.provider._extract_metrics(f.read())
        assert_equals(metrics_dict["slideshare:views"], 337)
        assert_equals(metrics_dict["slideshare:downloads"], 4)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'slideshare:downloads': (4, 'http://www.slideshare.net/cavlec/manufacturing-serendipity-12176916'), 'slideshare:views': (543, 'http://www.slideshare.net/cavlec/manufacturing-serendipity-12176916'), 'slideshare:favorites': (2, 'http://www.slideshare.net/cavlec/manufacturing-serendipity-12176916')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_provider_import(self):
        test_tabs = {"account_name": "cavlec", "standard_urls_input": TEST_URL2}
        members = provider.import_products("slideshare", test_tabs)
        print members
        expected = [('url', u'https://www.slideshare.net/hpiwowar/right-time-right-place-to-change-the-world'), ('url', u'http://www.slideshare.net/cavlec/week8-5557551'), ('url', u'http://www.slideshare.net/cavlec/canoe-the-open-content-rapids'), ('url', u'http://www.slideshare.net/cavlec/so-you-think-you-know-libraries'), ('url', u'http://www.slideshare.net/cavlec/what-we-organize'), ('url', u'http://www.slideshare.net/cavlec/escapar-la-carrera-de-la-reina'), ('url', u'http://www.slideshare.net/cavlec/librarians-love-data'), ('url', u'http://www.slideshare.net/cavlec/even-the-loons-are-licensed'), ('url', u'http://www.slideshare.net/cavlec/institutional-repositories-rebirth-of-the-phoenix'), ('url', u'http://www.slideshare.net/cavlec/manufacturing-serendipity-12176916'), ('url', u'http://www.slideshare.net/cavlec/canoe-the-open-content-rapids-2862487'), ('url', u'http://www.slideshare.net/cavlec/encryption-27779361'), ('url', u'http://www.slideshare.net/cavlec/who-owns-our-work-notes'), ('url', u'http://www.slideshare.net/cavlec/rdf-rda-and-other-tlas'), ('url', u'http://www.slideshare.net/cavlec/whats-driving-open-access'), ('url', u'http://www.slideshare.net/cavlec/manufacturing-serendipity'), ('url', u'http://www.slideshare.net/cavlec/i-own-copyright-so-i-pwn-you'), ('url', u'http://www.slideshare.net/cavlec/paying-forit'), ('url', u'http://www.slideshare.net/cavlec/grab-a-bucket-its-raining-data'), ('url', u'http://www.slideshare.net/cavlec/soylent-semantic-web-is-people-with-notes'), ('url', u'http://www.slideshare.net/cavlec/who-owns-our-work'), ('url', u'http://www.slideshare.net/cavlec/soylent-semanticweb-is-people'), ('url', u'http://www.slideshare.net/cavlec/open-sesame-and-other-open-movements'), ('url', u'http://www.slideshare.net/cavlec/digital-preservation-and-institutional-repositories'), ('url', u'http://www.slideshare.net/cavlec/a-successful-failure-community-requirements-gathering-for-dspace'), ('url', u'http://www.slideshare.net/cavlec/project-management-16606291'), ('url', u'http://www.slideshare.net/cavlec/databases-markup-and-regular-expressions'), ('url', u'http://www.slideshare.net/cavlec/solving-problems-with-web-20'), ('url', u'http://www.slideshare.net/cavlec/educators-together'), ('url', u'http://www.slideshare.net/cavlec/le-ir-cest-mort-vive-le-ir'), ('url', u'http://www.slideshare.net/cavlec/open-content'), ('url', u'http://www.slideshare.net/cavlec/so-are-we-winning-yet'), ('url', u'http://www.slideshare.net/cavlec/save-the-cows-data-curation-for-the-rest-of-us-1533252'), ('url', u'http://www.slideshare.net/cavlec/grab-a-bucket-its-raining-data-2134106'), ('url', u'http://www.slideshare.net/cavlec/nsa-27779364'), ('url', u'http://www.slideshare.net/cavlec/is-this-big-data-which-i-see-before-me'), ('url', u'http://www.slideshare.net/cavlec/escaping-the-red-queens-race-with-open-access'), ('url', u'http://www.slideshare.net/cavlec/research-data-and-scholarly-communication'), ('url', u'http://www.slideshare.net/cavlec/so-arewewinningyet-notes'), ('url', u'http://www.slideshare.net/cavlec/week13-5972690'), ('url', u'http://www.slideshare.net/cavlec/privacy-inlibs'), ('url', u'http://www.slideshare.net/cavlec/marc-and-bibframe-linking-libraries-and-archives'), ('url', u'http://www.slideshare.net/cavlec/frbr-and-rda'), ('url', u'http://www.slideshare.net/cavlec/the-social-journal'), ('url', u'http://www.slideshare.net/cavlec/occupy-copyright'), ('url', u'http://www.slideshare.net/cavlec/research-data-and-scholarly-communication-16366049'), ('url', u'http://www.slideshare.net/cavlec/what-youre-up-against'), ('url', u'http://www.slideshare.net/cavlec/escaping-datageddon'), ('url', u'http://www.slideshare.net/cavlec/risk-management-and-auditing'), ('url', u'http://www.slideshare.net/cavlec/the-canonically-bad-digital-humanities-proposal'), ('url', u'http://www.slideshare.net/cavlec/data-and-the-law'), ('url', u'http://www.slideshare.net/cavlec/ejournals-and-open-access'), ('url', u'http://www.slideshare.net/cavlec/preservation-and-institutional-repositories-for-the-digital-arts-and-humanities'), ('url', u'http://www.slideshare.net/cavlec/avoiding-heronway'), ('url', u'http://www.slideshare.net/cavlec/taming-the-monster-digital-preservation-planning-and-implementation-tools'), ('url', u'http://www.slideshare.net/cavlec/library-linked-data')]
        for member in expected:
            assert(member in members)


########NEW FILE########
__FILENAME__ = test_topsy
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, assert_items_equal, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/topsy")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_METRICS_SITE_PAGE = os.path.join(datadir, "metrics_site")

TEST_ID = "http://total-impact.org"

class TestTopsy(ProviderTestCase):

    provider_name = "topsy"

    testitem_aliases = ("url", TEST_ID)
    testitem_metrics = ("url", TEST_ID)

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics(good_page)
        expected = {'topsy:influential_tweets': 26, 'topsy:tweets': 282}
        assert_equals(metrics_dict, expected)

    def test_extract_metrics_site_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_SITE_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics(good_page)
        expected = {'topsy:tweets': 221}
        assert_equals(metrics_dict, expected)


    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("tweets", 
            [self.testitem_aliases])
        expected = 'http://topsy.com/trackback?url=http%3A//total-impact.org&window=a'
        assert_equals(provenance_url, expected)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'topsy:influential_tweets': (7, 'http://topsy.com/trackback?url=http%3A//total-impact.org&window=a'), 'topsy:tweets': (46, 'http://topsy.com/trackback?url=http%3A//total-impact.org&window=a')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics2(self):
        metrics_dict = self.provider.metrics([("url", "http://researchremix.wordpress.com/2011/08/10/personal")])
        expected = {'topsy:influential_tweets': (1, 'http://topsy.com/trackback?url=http%3A//researchremix.wordpress.com/2011/08/10/personal/&window=a'), 'topsy:tweets': (18, 'http://topsy.com/trackback?url=http%3A//researchremix.wordpress.com/2011/08/10/personal/&window=a')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

        #now with trailing slawh
        metrics_dict2 = self.provider.metrics([("url", "http://researchremix.wordpress.com/2011/08/10/personal/")])
        assert_items_equal(metrics_dict, metrics_dict2)


    @http
    def test_metrics_blog(self):
        metrics_dict = self.provider.metrics([("blog", "http://retractionwatch.wordpress.com")])
        expected = {'topsy:tweets': (8639, 'http://topsy.com/s?q=site%3Aretractionwatch.wordpress.com&window=a')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_different_urls(self):
        metrics_dict = self.provider.metrics([("url","http://datadryad.org/handle/10255/dryad.234"), 
                                                ("url", "http://dx.doi.org/10.5061/dryad.234")])
        expected = {'topsy:tweets': (5, 'http://topsy.com/trackback?url=http%3A//dx.doi.org/10.5061/dryad.234&window=a')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_top_tweeted_urls_site(self):
        response = self.provider.top_tweeted_urls("http://blog.impactstory.org", number_to_return=5)
        print response
        expected = [u'http://blog.impactstory.org/2013/09/27/impactstory-awarded-300k-nsf-grant/',
                 u'http://blog.impactstory.org/2013/01/18/github/',
                 u'http://blog.impactstory.org/2013/06/17/sloan/',
                 u'http://blog.impactstory.org/2013/07/04/impactstory-sloan-grant-proposal-details/',
                 u'http://blog.impactstory.org/2013/06/17/impact-profiles/']
        #assert_equals(response, expected)

    @http
    def test_top_tweeted_urls_tweets(self):
        response = self.provider.top_tweeted_urls("impactstory", "twitter_account", number_to_return=10)
        print response
        expected = [u'http://blog.impactstory.org/2013/09/27/impactstory-awarded-300k-nsf-grant/',
                 u'http://blog.impactstory.org/2013/01/18/github/',
                 u'http://blog.impactstory.org/2013/06/17/sloan/',
                 u'http://blog.impactstory.org/2013/07/04/impactstory-sloan-grant-proposal-details/',
                 u'http://blog.impactstory.org/2013/06/17/impact-profiles/']
        #assert_equals(response, expected)        

    @http
    def test_top_tweeted_urls_tweets(self):
        response = self.provider.top_tweeted_urls("jasonpriem", "tweets_about", number_to_return=10)
        print response
        expected = [u'http://blog.impactstory.org/2013/09/27/impactstory-awarded-300k-nsf-grant/',
                 u'http://blog.impactstory.org/2013/01/18/github/',
                 u'http://blog.impactstory.org/2013/06/17/sloan/',
                 u'http://blog.impactstory.org/2013/07/04/impactstory-sloan-grant-proposal-details/',
                 u'http://blog.impactstory.org/2013/06/17/impact-profiles/']
        #assert_equals(response, expected) 

########NEW FILE########
__FILENAME__ = test_twitter_account
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestTwitter_account(ProviderTestCase):

    provider_name = "twitter_account"

    testitem_members = "jasonpriem"
    testitem_aliases = ("url", "http://twitter.com/jasonpriem")
    testitem_metrics = ("url", "http://twitter.com/jasonpriem")
    testitem_biblio = ("url", "http://twitter.com/jasonpriem")

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix/status/400821465828061184")), False)


    def test_screen_name(self):
        # ensure that it matches an appropriate ids
        response = self.provider.screen_name(self.testitem_aliases[1])
        assert_equals(response, "jasonpriem")

  
    def test_extract_members_success(self):        
        members = self.provider.member_items(self.testitem_members)
        print members
        expected = [('url', 'http://twitter.com/jasonpriem')]
        assert_equals(members, expected)


    def test_extract_members_success_with_at(self):        
        members = self.provider.member_items("@" + self.testitem_members)
        print members
        expected = [('url', 'http://twitter.com/jasonpriem')]
        assert_equals(members, expected)


    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("twitter_account:lists", [self.testitem_aliases])
        assert_equals(provenance_url, 'https://twitter.com/jasonpriem/memberships')

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        print metrics_dict
        expected = {'twitter_account:lists': (215, 'https://twitter.com/jasonpriem/memberships'), 'twitter_account:followers': (3069, 'https://twitter.com/jasonpriem/followers')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_bad_twitttername(self):
        metrics_dict = self.provider.metrics([("url", "http://twitter.com/researchremix22")])
        print metrics_dict
        expected = {}
        assert_equals(metrics_dict, expected)


    @http
    def test_biblio(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio])
        print biblio_dict
        expected = {'account': u'@jasonpriem', 'description': u'Info Science PhD student, ImpactStory co-founder. I care hard about #OA, #altmetrics, and bringing scholarly communication into the age of the web.', 'repository': 'Twitter', 'title': u'@jasonpriem', 'created_at': u'Mon Jun 16 19:19:43 +0000 2008', 'is_account': True, 'authors': u'Jason Priem'}
        assert_items_equal(biblio_dict.keys(), expected.keys())
        for key in expected.keys():
            assert_equals(biblio_dict[key], expected[key])

    # not relevant given library approach

    def test_provider_member_items_400(self):
        pass
    def test_provider_member_items_500(self):
        pass
    def test_provider_member_items_empty(self):
        pass
    def test_provider_member_items_nonsense_txt(self):
        pass
    def test_provider_member_items_nonsense_xml(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

########NEW FILE########
__FILENAME__ = test_twitter_tweet
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestTwitter_Tweet(ProviderTestCase):

    provider_name = "twitter_tweet"

    testitem_biblio = ("url", "https://twitter.com/researchremix/status/400821465828061184")

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_biblio), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix")), False)


    def test_screen_name(self):
        # ensure that it matches an appropriate ids
        response = self.provider.screen_name(self.testitem_biblio[1])
        assert_equals(response, "researchremix")

  
    @http
    def test_biblio(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio])
        print biblio_dict
        expected = {'account': u'@researchremix', 'repository': 'Twitter', 'title': u'@researchremix', 'url': 'https://api.twitter.com/1/statuses/oembed.json?id=400821465828061184&hide_media=1&hide_thread=1&maxwidth=650', 'year': '2013', 'tweet_text': u'The FIRST Act Is the Last Open Access Reform We&#39;d Ever Want <a href="https://t.co/CzALjCncyJ">https://t.co/CzALjCncyJ</a> <a href="https://twitter.com/search?q=%23openaccess&amp;src=hash">#openaccess</a>', 'authors': u'Heather Piwowar', 'date': '2013-11-14T00:00:00', 'embed': u'<blockquote class="twitter-tweet" data-cards="hidden" width="550"><p>The FIRST Act Is the Last Open Access Reform We&#39;d Ever Want <a href="https://t.co/CzALjCncyJ">https://t.co/CzALjCncyJ</a> <a href="https://twitter.com/search?q=%23openaccess&amp;src=hash">#openaccess</a></p>&mdash; Heather Piwowar (@researchremix) <a href="https://twitter.com/researchremix/statuses/400821465828061184">November 14, 2013</a></blockquote>\n<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>'}
        assert_items_equal(biblio_dict.keys(), expected.keys())
        for key in expected.keys():
            assert_equals(biblio_dict[key], expected[key])

    # not relevant given library approach
    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

########NEW FILE########
__FILENAME__ = test_vimeo
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, assert_items_equal, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/vimeo")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

class TestVimeo(ProviderTestCase):

    provider_name = "vimeo"

    testitem_aliases = ("url", "http://vimeo.com/48605764")
    testitem_metrics = ("url", "http://vimeo.com/48605764")
    testitem_biblio = ("url", "http://vimeo.com/48605764")

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)
        assert_equals(self.provider.is_relevant_alias(("url", "NOT A VIMEO ID")), False)
  
    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        metrics_dict = self.provider._extract_metrics(f.read(), id=self.testitem_metrics[1])
        print metrics_dict
        assert_equals(metrics_dict["vimeo:plays"], 83)

    def test_extract_biblio_success(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        biblio_dict = self.provider._extract_biblio(f.read(), self.testitem_biblio[1])
        print biblio_dict
        expected = {'repository': 'Vimeo', 'title': 'Wheat Rust Inoculation Protocol Video', 'url': 'http://vimeo.com/48605764', 'year': '2012', 'authors': 'Huang Lab', 'published_date': '2012-08-31 12:20:16'}
        assert_equals(biblio_dict, expected)

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("github:forks", [self.testitem_aliases])
        assert_equals(provenance_url, 'http://vimeo.com/48605764')

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        print metrics_dict
        expected = {'vimeo:plays': (83, 'http://vimeo.com/48605764')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_biblio(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio])
        print biblio_dict
        expected = {'repository': 'Vimeo', 'title': u'Wheat Rust Inoculation Protocol Video', 'url': u'http://vimeo.com/48605764', 'year': u'2012', 'authors': u'Huang Lab', 'published_date': u'2012-08-31 12:20:16'}
        assert_items_equal(biblio_dict.keys(), expected.keys())
        for key in ['year', 'published_date', 'title', 'url']:
            assert_equals(biblio_dict[key], expected[key])


########NEW FILE########
__FILENAME__ = test_webpage
 # -*- coding: utf-8 -*-  # need this line because test utf-8 strings later

import os
import collections

from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import slow, http

from nose.tools import assert_equals, raises

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/webpage")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

class TestWebpage(ProviderTestCase):

    provider_name = "webpage"

    testitem_aliases = ("url", "http://nescent.org")
    testitem_biblio = ("url", "http://nescent.org")
    testitem_members = "http://nescent.org\nhttp://blenz.ca\nhttps://heroku.com"

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("doi", "NOT A GITHUB ID")), False)
  
    def test_extract_biblio(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        ret = self.provider._extract_biblio(f.read())
        expected = {'h1': u'WELCOME', 'title': u'NESCent: The National Evolutionary Synthesis Center'}
        assert_equals(ret, expected)

    def test_extract_biblio_russian(self):
        #from http://www.youtube.com/watch?v=9xBmU0TPZC4
        page = """<html><head><title> .  2010 - YouTube</title></head>
                <body>
                <h1 id="watch-headline-title">
                 .  2010
                </h1>
              </body></html>"""
        ret = self.provider._extract_biblio(page)
        expected = {'h1': u' .  2010', 'title': u" .  2010 - YouTube"} 
        assert_equals(ret, expected)

    # override common because does not raise errors, unlike most other providers
    def test_provider_biblio_400(self):
        Provider.http_get = common.get_400
        biblio = self.provider.biblio([self.testitem_biblio])
        assert_equals(biblio, {})

    # override comon because does not raise errors, unlike most other providers
    def test_provider_biblio_500(self):
        Provider.http_get = common.get_500
        biblio = self.provider.biblio([self.testitem_biblio])
        assert_equals(biblio, {})

    def test_member_items(self):
        ret = self.provider.member_items(self.testitem_members)
        expected = [('url', 'http://nescent.org'), ('url', 'http://blenz.ca'), ('url', 'https://heroku.com')]
        assert_equals(ret, expected)        

    def test_provider_member_items_400(self):
        pass
    def test_provider_member_items_500(self):
        pass
    def test_provider_member_items_empty(self):
        pass
    def test_provider_member_items_nonsense_txt(self):
        pass
    def test_provider_member_items_nonsense_xml(self):
        pass
        
    @http
    def test_biblio(self):
        ret = self.provider.biblio([("url", "http://www.digitalhumanities.org/dhq/vol/2/1/000019/000019.html")])
        expected = {'title': u'DHQ: Digital Humanities Quarterly: As You Can See: Applying Visual Collaborative Filtering to Works of Art'}
        assert_equals(ret, expected)


########NEW FILE########
__FILENAME__ = test_wikipedia
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, raises

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/wikipedia")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_ALIASES_PAGE = os.path.join(datadir, "aliases")
SAMPLE_EXTRACT_MEMBER_ITEMS_PAGE = os.path.join(datadir, "members")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

TEST_DOI = "10.1371/journal.pcbi.1000361"

class TestWikipedia(ProviderTestCase):

    provider_name = "wikipedia"

    testitem_metrics = ("doi", TEST_DOI)
    testitem_aliases = ("doi", TEST_DOI)
    testitem_biblio = None
    testitem_members = None

    def setUp(self):
        ProviderTestCase.setUp(self)

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        ### Is there anything that wikipedia shouldn't match? 
  
    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        good_page = f.read()
        metrics_dict = self.provider._extract_metrics(good_page)
        assert_equals(metrics_dict["wikipedia:mentions"], 1)

    def test_extract_metrics_empty(self):
        # now give it something with no results
        empty_page = """<?xml version="1.0"?>
                <api>
                    <query>
                        <searchinfo totalhits="0" />
                        <search></search>
                    </query>
                </api>
                """
        res = self.provider._extract_metrics(empty_page)
        assert_equals(res, {})
        
    @raises(ProviderContentMalformedError)    
    def test_extract_metrics_invalid(self):
        incorrect_doc = """this isn't the wikipedia search result page you are looking for"""
        self.provider._extract_metrics(incorrect_doc)

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        expected = {'wikipedia:mentions': (1, 'http://en.wikipedia.org/wiki/Special:Search?search="10.1371/journal.pcbi.1000361"&go=Go')}
        print metrics_dict
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]



########NEW FILE########
__FILENAME__ = test_wordpresscom
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, assert_items_equal, raises

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/wordpresscom")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

class TestWordpresscom(ProviderTestCase):

    provider_name = "wordpresscom"

    api_key = os.environ["WORDPRESS_OUR_BLOG_API_KEY"]
    testitem_members = {"blogUrl": "http://researchremix.wordpress.com"}
    testitem_aliases = ("blog", "http://researchremix.wordpress.com")
    testitem_metrics = ("blog", "http://researchremix.wordpress.com")
    testitem_biblio = ("blog", "http://researchremix.wordpress.com")

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("doi", "NOT A WORDPRESS ID")), False)
  

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("wordpresscom:subscribers", [self.testitem_aliases])
        assert_equals(provenance_url, u'http://researchremix.wordpress.com')


    @http
    def test_members(self):
        response = self.provider.member_items(self.testitem_members)
        print response
        expected = [('blog', 'http://researchremix.wordpress.com'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com"}'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2013/05/11/society-oa-options/", "blog_url": "http://researchremix.wordpress.com"}'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/05/29/non-american-please-sign/", "blog_url": "http://researchremix.wordpress.com"}'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2013/03/05/why-google-isnt-good-enough-for-academic-search/", "blog_url": "http://researchremix.wordpress.com"}'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/03/05/talking-text-mining-with-elsevier/", "blog_url": "http://researchremix.wordpress.com"}'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2013/03/13/why-google/", "blog_url": "http://researchremix.wordpress.com"}'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/01/31/31-flavours/", "blog_url": "http://researchremix.wordpress.com"}'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2011/02/18/early_results/", "blog_url": "http://researchremix.wordpress.com"}'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/05/29/dear-research-data-advocate-please-sign-the-petition-oamonday/", "blog_url": "http://researchremix.wordpress.com"}'), ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/01/07/rwa-job-losses/", "blog_url": "http://researchremix.wordpress.com"}')]
        assert_equals(response, expected)

    @http
    def test_aliases(self):
        response = self.provider.aliases([self.testitem_aliases])
        print response
        expected = [('url', 'http://researchremix.wordpress.com'), ('wordpress_blog_id', "1015265")] 
        assert_equals(response, expected)

    @http
    def test_metrics_wordpress_com_api_key_whole_blog(self):
        analytics_credentials = {"wordpress_api_key": self.api_key}
        metrics_dict = self.provider.metrics([self.testitem_metrics], analytics_credentials=analytics_credentials)
        print metrics_dict
        expected = {'wordpresscom:views': (75558, 'http://researchremix.wordpress.com'), 'wordpresscom:subscribers': (66, 'http://researchremix.wordpress.com')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_wordpress_com_wordpress_id_whole_blog(self):
        test_aliases = [('blog', 'http://researchremix.wordpress.com'), ('wordpress_blog_id', "1015265")] 
        metrics_dict = self.provider.metrics(test_aliases)
        print metrics_dict
        expected = {'wordpresscom:comments': (638, 'http://researchremix.wordpress.com'), 'wordpresscom:subscribers': (66, 'http://researchremix.wordpress.com')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]


    @http
    def test_metrics_wordpress_com_api_key_one_post(self):
        analytics_credentials = {"wordpress_api_key": self.api_key}
        wordpress_post_alias = ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com", "wordpress_post_id": 1119}')
        metrics_dict = self.provider.metrics([wordpress_post_alias], analytics_credentials=analytics_credentials)
        print metrics_dict
        expected = {'wordpresscom:comments': (13, None), 'wordpresscom:views': (1863, None)}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]


    @http
    def test_metrics_wordpress_com_without_api_key(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        print metrics_dict
        expected = {'wordpresscom:subscribers': (66, u'http://researchremix.wordpress.com')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_metrics_not_wordpress_com(self):
        metrics_dict = self.provider.metrics([("blog", "http://jasonpriem.com")])
        print metrics_dict
        expected = {}
        assert_equals(metrics_dict, expected)

    @http
    def test_biblio_wordpress_com(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio])
        print biblio_dict
        expected = {'account': 'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'description': u'Blogging about the science, engineering, and human factors of biomedical research data reuse', 'title': u'Research Remix', 'url': 'http://researchremix.wordpress.com', 'is_account': True}
        assert_items_equal(biblio_dict.keys(), expected.keys())

    @http
    def test_biblio_not_wordpress_com(self):
        biblio_dict = self.provider.biblio([("blog", "http://jasonpriem.com")])
        print biblio_dict
        expected = {'url': 'http://jasonpriem.com', 'account': 'jasonpriem.com', 'is_account': True}
        assert_items_equal(biblio_dict.keys(), expected.keys())


    # not relevant given library approach

    def test_provider_member_items_400(self):
        pass
    def test_provider_member_items_500(self):
        pass
    def test_provider_member_items_empty(self):
        pass
    def test_provider_member_items_nonsense_txt(self):
        pass
    def test_provider_member_items_nonsense_xml(self):
        pass

    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

########NEW FILE########
__FILENAME__ = test_youtube
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
from nose.tools import assert_equals, assert_items_equal, raises, nottest

datadir = os.path.join(os.path.split(__file__)[0], "../../../extras/sample_provider_pages/youtube")
SAMPLE_EXTRACT_METRICS_PAGE = os.path.join(datadir, "metrics")
SAMPLE_EXTRACT_BIBLIO_PAGE = os.path.join(datadir, "biblio")

class TestYoutube(ProviderTestCase):

    provider_name = "youtube"

    testitem_aliases = ("url", "http://www.youtube.com/watch?v=d39DL4ed754")
    testitem_metrics = ("url", "http://www.youtube.com/watch?v=d39DL4ed754")
    testitem_biblio = ("url", "http://www.youtube.com/watch?v=d39DL4ed754")

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        # ensure that it matches an appropriate ids
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)
        assert_equals(self.provider.is_relevant_alias(("url", "NOT A YOUTUBE ID")), False)
  
    def test_extract_metrics_success(self):
        f = open(SAMPLE_EXTRACT_METRICS_PAGE, "r")
        metrics_dict = self.provider._extract_metrics(f.read())
        print metrics_dict
        assert_equals(metrics_dict["youtube:views"], 113)

    def test_extract_biblio_success(self):
        f = open(SAMPLE_EXTRACT_BIBLIO_PAGE, "r")
        biblio_dict = self.provider._extract_biblio(f.read(), self.testitem_biblio[1])
        print biblio_dict
        expected = {'channel_title': 'ImpactStory', 'repository': 'YouTube', 'title': 'Y Combinator video outtakes', 'url': 'http://www.youtube.com/watch?v=d39DL4ed754', 'published_date': '2013-10-15T21:48:48.000Z', 'year': '2013'}
        assert_equals(biblio_dict, expected)

    def test_provenance_url(self):
        provenance_url = self.provider.provenance_url("github:forks", [self.testitem_aliases])
        assert_equals(provenance_url, 'http://www.youtube.com/watch?v=d39DL4ed754')

    @http
    def test_metrics(self):
        metrics_dict = self.provider.metrics([self.testitem_metrics])
        print metrics_dict
        expected = {'youtube:views': (123, 'http://www.youtube.com/watch?v=d39DL4ed754'), 'youtube:likes': (3, 'http://www.youtube.com/watch?v=d39DL4ed754')}
        for key in expected:
            assert metrics_dict[key][0] >= expected[key][0], [key, metrics_dict[key], expected[key]]
            assert metrics_dict[key][1] == expected[key][1], [key, metrics_dict[key], expected[key]]

    @http
    def test_biblio(self):
        biblio_dict = self.provider.biblio([self.testitem_biblio])
        print biblio_dict
        expected = {'channel_title': 'ImpactStory', 'repository': 'YouTube', 'title': 'Y Combinator video outtakes', 'url': 'http://www.youtube.com/watch?v=d39DL4ed754', 'published_date': '2013-10-15T21:48:48.000Z', 'year': '2013'}
        assert_items_equal(biblio_dict.keys(), expected.keys())
        for key in ['year', 'published_date', 'title', 'url']:
            assert_equals(biblio_dict[key], expected[key])


########NEW FILE########
__FILENAME__ = test_backend
import json, os, Queue, datetime

from totalimpact import tiredis, backend, default_settings
from totalimpact import db, app
from totalimpact import item as item_module
from totalimpact.providers.provider import Provider, ProviderTimeout, ProviderFactory
from nose.tools import raises, assert_equals, nottest
from test.utils import slow
from test import mocks

from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests


class TestBackend():
    
    def setUp(self):
        self.config = None #placeholder
        self.TEST_PROVIDER_CONFIG = [
            ("wikipedia", {})
        ]
        self.d = None

        # do the same thing for the redis db, set up the test redis database.  We're using DB Number 8
        self.r = tiredis.from_url("redis://localhost:6379", db=8)
        self.r.flushdb()

        provider_queues = {}
        providers = ProviderFactory.get_providers(self.TEST_PROVIDER_CONFIG)
        for provider in providers:
            provider_queues[provider.provider_name] = backend.PythonQueue(provider.provider_name+"_queue")

        self.b = backend.Backend(
            backend.RedisQueue("alias-unittest", self.r), 
            provider_queues, 
            [backend.PythonQueue("couch_queue")], 
            self.r)

        self.fake_item = {
            "_id": "1",
            "type": "item",
            "num_providers_still_updating":1,
            "aliases":{"pmid":["111"]},
            "biblio": {},
            "metrics": {},
            "last_modified": datetime.datetime(2013, 1, 1)
        }
        self.fake_aliases_dict = {"pmid":["222"]}
        self.tiid = "abcd"

        self.db = setup_postgres_for_unittests(db, app)


    def teardown(self):
        self.r.flushdb()

        teardown_postgres_for_unittests(self.db)



class TestProviderWorker(TestBackend):
    # warning: calls live provider right now
    def test_add_to_couch_queue_if_nonzero(self):    
        test_couch_queue = backend.PythonQueue("test_couch_queue")
        provider_worker = backend.ProviderWorker(mocks.ProviderMock("myfakeprovider"), 
                                        None, None, None, {"a": test_couch_queue}, None, self.r)  
        response = provider_worker.add_to_couch_queue_if_nonzero("aaatiid", #start fake tiid with "a" so in first couch queue
                {"doi":["10.5061/dryad.3td2f"]}, 
                "aliases", 
                "dummy")

        # test that it put it on the queue
        in_queue = test_couch_queue.pop()
        expected = {'method_name': 'aliases', 'tiid': 'aaatiid', 'provider_name': 'myfakeprovider', 'analytics_credentials': 'dummy', 'new_content': {'doi': ['10.5061/dryad.3td2f']}}
        assert_equals(in_queue, expected)

    def test_add_to_couch_queue_if_nonzero_given_metrics(self):    
        test_couch_queue = backend.PythonQueue("test_couch_queue")
        provider_worker = backend.ProviderWorker(mocks.ProviderMock("myfakeprovider"), 
                                        None, None, None, {"a": test_couch_queue}, None, self.r)  
        metrics_method_response = {'dryad:package_views': (361, 'http://dx.doi.org/10.5061/dryad.7898'), 
                    'dryad:total_downloads': (176, 'http://dx.doi.org/10.5061/dryad.7898'), 
                    'dryad:most_downloaded_file': (65, 'http://dx.doi.org/10.5061/dryad.7898')}        
        response = provider_worker.add_to_couch_queue_if_nonzero("aaatiid", #start fake tiid with "a" so in first couch queue
                metrics_method_response,
                "metrics", 
                "dummy")

        # test that it put it on the queue
        in_queue = test_couch_queue.pop()
        expected = {'method_name': 'metrics', 'tiid': 'aaatiid', 'provider_name': 'myfakeprovider', 'analytics_credentials': 'dummy', 'new_content': metrics_method_response}
        print in_queue
        assert_equals(in_queue, expected)        

        # check nothing in redis since it had a value
        response = self.r.get_num_providers_currently_updating("aaatiid")
        assert_equals(response, 0)

    def test_add_to_couch_queue_if_nonzero_given_empty_metrics_response(self):    
        test_couch_queue = backend.PythonQueue("test_couch_queue")
        provider_worker = backend.ProviderWorker(mocks.ProviderMock("myfakeprovider"), 
                                        None, None, None, {"a": test_couch_queue}, None, self.r)  
        metrics_method_response = {}
        response = provider_worker.add_to_couch_queue_if_nonzero("aaatiid", #start fake tiid with "a" so in first couch queue
                metrics_method_response,
                "metrics", 
                "dummy")

        # test that it did not put it on the queue
        in_queue = test_couch_queue.pop()
        expected = None
        assert_equals(in_queue, expected)        

        # check decremented in redis since the payload was null
        response = num_left = self.r.get_num_providers_currently_updating("aaatiid")
        assert_equals(response, 0)

    def test_wrapper(self):     
        def fake_callback(tiid, new_content, method_name, analytics_credentials, aliases_providers_run):
            pass

        response = backend.ProviderWorker.wrapper("123", 
                {'url': ['http://somewhere'], 'doi': ['10.123']}, 
                mocks.ProviderMock("myfakeprovider"), 
                "aliases", 
                {}, # credentials
                [], # aliases previously run
                fake_callback)
        print response
        expected = {'url': ['http://somewhere'], 'doi': ['10.1', '10.123']}
        assert_equals(response, expected)

class TestCouchWorker(TestBackend):
    def test_update_item_with_new_aliases(self):
        response = backend.CouchWorker.update_item_with_new_aliases(self.fake_aliases_dict, self.fake_item)
        expected = {'metrics': {}, 'num_providers_still_updating': 1, 'biblio': {}, '_id': '1', 'type': 'item', 
            'aliases': {'pmid': ['222', '111']}, 'last_modified': datetime.datetime(2013, 1, 1, 0, 0)}
        assert_equals(response, expected)

    def test_update_item_with_new_aliases_using_dup_alias(self):
        dup_alias_dict = self.fake_item["aliases"]
        response = backend.CouchWorker.update_item_with_new_aliases(dup_alias_dict, self.fake_item)
        expected = None # don't return the item if it already has all the aliases in it
        assert_equals(response, expected)

    def test_update_item_with_new_biblio(self):
        new_biblio_dict = {"title":"A very good paper", "authors":"Smith, Lee, Khun"}
        response = backend.CouchWorker.update_item_with_new_biblio(new_biblio_dict, self.fake_item)
        expected = new_biblio_dict
        assert_equals(response["biblio"], expected)

    def test_update_item_with_new_biblio_existing_biblio(self):
        item_with_some_biblio = self.fake_item
        item_with_some_biblio["biblio"] = {"title":"Different title"}
        new_biblio_dict = {"title":"A very good paper", "authors":"Smith, Lee, Khun"}
        response = backend.CouchWorker.update_item_with_new_biblio(new_biblio_dict, item_with_some_biblio)
        expected = {"authors": new_biblio_dict["authors"]}
        assert_equals(response["biblio"], expected)

    def test_update_item_with_new_metrics(self):
        response = backend.CouchWorker.update_item_with_new_metrics("mendeley:groups", (3, "http://provenance"), self.fake_item)
        expected = {'mendeley:groups': {'provenance_url': 'http://provenance', 'values': {'raw': 3, 'raw_history': {'2012-09-15T21:39:39.563710': 3}}}}
        print response["metrics"]        
        assert_equals(response["metrics"]['mendeley:groups']["provenance_url"], 'http://provenance')
        assert_equals(response["metrics"]['mendeley:groups']["values"]["raw"], 3)
        assert_equals(response["metrics"]['mendeley:groups']["values"]["raw_history"].values(), [3])
        # check year starts with 20
        assert_equals(response["metrics"]['mendeley:groups']["values"]["raw_history"].keys()[0][0:2], "20")

    def test_run_nothing_in_queue(self):
        test_couch_queue = backend.PythonQueue("test_couch_queue")
        couch_worker = backend.CouchWorker(test_couch_queue, self.r, self.d)
        response = couch_worker.run()
        expected = None
        assert_equals(response, expected)

    def test_run_aliases_in_queue(self):
        test_couch_queue = backend.PythonQueue("test_couch_queue")
        test_couch_queue_dict = {self.fake_item["_id"][0]:test_couch_queue}
        provider_worker = backend.ProviderWorker(mocks.ProviderMock("myfakeprovider"), 
                                        None, None, None, test_couch_queue_dict, None, self.r)  
        response = provider_worker.add_to_couch_queue_if_nonzero(self.fake_item["_id"], 
                {"doi":["10.5061/dryad.3td2f"]}, 
                "aliases", 
                "dummy")

        # save basic item beforehand
        item_obj = item_module.create_objects_from_item_doc(self.fake_item)
        self.db.session.add(item_obj)
        self.db.session.commit()

        # run
        couch_worker = backend.CouchWorker(test_couch_queue, self.r, self.d)
        response = couch_worker.run()
        expected = None
        assert_equals(response, expected)

        # check couch_queue has value after
        response = item_module.get_item(self.fake_item["_id"], {}, self.r)
        print response
        expected = {'pmid': ['111'], 'doi': ['10.5061/dryad.3td2f']}
        assert_equals(response["aliases"], expected)

        # check has updated last_modified time
        now = datetime.datetime.utcnow().isoformat()
        assert_equals(response["last_modified"][0:10], now[0:10])

    def test_run_metrics_in_queue(self):
        test_couch_queue = backend.PythonQueue("test_couch_queue")
        test_couch_queue_dict = {self.fake_item["_id"][0]:test_couch_queue}
        provider_worker = backend.ProviderWorker(mocks.ProviderMock("myfakeprovider"), 
                                        None, None, None, test_couch_queue_dict, None, self.r) 
        metrics_method_response = {'dryad:package_views': (361, 'http://dx.doi.org/10.5061/dryad.7898'), 
                            'dryad:total_downloads': (176, 'http://dx.doi.org/10.5061/dryad.7898'), 
                            'dryad:most_downloaded_file': (65, 'http://dx.doi.org/10.5061/dryad.7898')}                                         
        response = provider_worker.add_to_couch_queue_if_nonzero(self.fake_item["_id"], 
                metrics_method_response,
                "metrics", 
                "dummy")

        # save basic item beforehand
        item_obj = item_module.create_objects_from_item_doc(self.fake_item)
        self.db.session.add(item_obj)
        self.db.session.commit()

        # run
        couch_worker = backend.CouchWorker(test_couch_queue, self.r, self.d)    
        couch_worker.run()
            
        # check couch_queue has value after
        response = item_module.get_item(self.fake_item["_id"], {}, self.r)
        print response
        expected = 361
        assert_equals(response["metrics"]['dryad:package_views']['values']["raw"], expected)


class TestBackendClass(TestBackend):

    def test_decide_who_to_call_next_unknown(self):
        aliases_dict = {"unknownnamespace":["111"]}
        prev_aliases = []
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect blanks
        expected = {'metrics': [], 'biblio': [], 'aliases': ['webpage']}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_unknown_after_webpage(self):
        aliases_dict = {"unknownnamespace":["111"]}
        prev_aliases = ["webpage"]
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect blanks
        expected = {'metrics': ["wikipedia"], 'biblio': ["webpage"], 'aliases': []}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_webpage_no_title(self):
        aliases_dict = {"url":["http://a"]}
        prev_aliases = []
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect all metrics and lookup the biblio
        expected = {'metrics': ['wikipedia'], 'biblio': ['webpage'], 'aliases': []}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_webpage_with_title(self):
        aliases_dict = {"url":["http://a"], "title":["A Great Paper"]}
        prev_aliases = []
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect all metrics, no need to look up biblio
        expected = {'metrics': ['wikipedia'], 'biblio': ['webpage'], 'aliases': []}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_slideshare_no_title(self):
        aliases_dict = {"url":["http://abc.slideshare.net/def"]}
        prev_aliases = []
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect all metrics and look up the biblio
        expected = {'metrics': ['wikipedia'], 'biblio': ['slideshare'], 'aliases': []}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_dryad_no_url(self):
        aliases_dict = {"doi":["10.5061/dryad.3td2f"]}
        prev_aliases = ["altmetric_com"]
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect need to resolve the dryad doi before can go get metrics
        expected = {'metrics': [], 'biblio': [], 'aliases': ['dryad']}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_dryad_with_url(self):
        aliases_dict = {   "doi":["10.5061/dryad.3td2f"],
                                    "url":["http://dryadsomewhere"]}
        prev_aliases = ["altmetric_com"]
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # still need the dx.doi.org url
        expected = {'metrics': [], 'biblio': [], 'aliases': ['dryad']}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_dryad_with_doi_url(self):
        aliases_dict = {   "doi":["10.5061/dryad.3td2f"],
                                    "url":["http://dx.doi.org/10.dryadsomewhere"]}
        prev_aliases = ["altmetric_com", "dryad"]
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # have url so now can go get all the metrics
        expected = {'metrics': ['wikipedia'], 'biblio': ['dryad', 'mendeley'], 'aliases': []}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_crossref_not_run(self):
        aliases_dict = {"pmid":["111"]}
        prev_aliases = ["mendeley"]
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect need to get more aliases
        expected = {'metrics': [], 'biblio': [], 'aliases': ['crossref']}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_pmid_mendeley_not_run(self):
        aliases_dict = {"pmid":["111"]}
        prev_aliases = [""]
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect need to get more aliases
        expected = {'metrics': [], 'biblio': [], 'aliases': ['mendeley']}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_pmid_prev_run(self):
        aliases_dict = {  "pmid":["1111"],
                         "url":["http://pubmedsomewhere"]}
        prev_aliases = ["pubmed", "mendeley"]
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect need to get metrics and biblio
        expected = {'metrics': [], 'biblio': [], 'aliases': ['crossref']}
        assert_equals(response, expected)

    def test_decide_who_to_call_next_doi_with_urls(self):
        aliases_dict = {  "doi":["10.234/345345"],
                                "url":["http://journalsomewhere"]}
        prev_aliases = ["pubmed", "mendeley"]
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect need to get metrics, biblio from crossref
        expected = {'metrics': [], 'biblio': [], 'aliases': ['crossref']}
        assert_equals(response, expected)     

    def test_decide_who_to_call_next_doi_crossref_prev_called(self):
        aliases_dict = { "doi":["10.234/345345"],
                        "url":["http://journalsomewhere"]}
        prev_aliases = ["crossref"]                        
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect need to get metrics, no biblio
        expected = {'metrics': [], 'biblio': [], 'aliases': ['mendeley']}
        assert_equals(response, expected)   

    def test_decide_who_to_call_next_doi_crossref_pubmed_mendeley_prev_called(self):
        aliases_dict = { "doi":["10.234/345345"],
                        "url":["http://journalsomewhere"]}
        prev_aliases = ["crossref", "pubmed", "mendeley", "altmetric_com"]                        
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect need to get metrics, no biblio
        expected = {'metrics': ['wikipedia'], 'biblio': ['crossref', 'pubmed', 'mendeley', 'webpage'], 'aliases': []}
        assert_equals(response, expected)   

    def test_decide_who_to_call_next_pmid_crossref_pubmed_prev_called(self):
        aliases_dict = { "pmid":["1111"],
                        "url":["http://journalsomewhere"]}
        prev_aliases = ["crossref", "pubmed", "mendeley", "altmetric_com"]                        
        response = backend.Backend.sniffer(aliases_dict, prev_aliases, self.TEST_PROVIDER_CONFIG)
        print response
        # expect need to get metrics, no biblio
        expected = {'metrics': ['wikipedia'], 'biblio': ['crossref', 'pubmed', 'mendeley', 'webpage'], 'aliases': []}
        assert_equals(response, expected)   



########NEW FILE########
__FILENAME__ = test_collection
from totalimpact import collection, tiredis
from totalimpact import item as item_module
from totalimpact import db, app
from collections import OrderedDict
import os, json

from nose.tools import raises, assert_equals, assert_true, nottest
import unittest
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests


api_items_loc = os.path.join(
    os.path.split(__file__)[0],
    '../data/items.json')
API_ITEMS_JSON = json.loads(open(api_items_loc, "r").read())


class TestCollection():

    def setUp(self):
        self.d = None

        # do the same thing for the redis db, set up the test redis database.  We're using DB Number 8
        self.r = tiredis.from_url("redis://localhost:6379", db=8)
        self.r.flushdb()

        self.db = setup_postgres_for_unittests(db, app)

        self.aliases = [
            ["doi", "10.123"],
            ["doi", "10.124"],
            ["doi", "10.125"]
        ]

    def tearDown(self):
        teardown_postgres_for_unittests(self.db)


    def test_init_collection(self):
        #make sure nothing there beforehand
        response = collection.Collection.query.filter_by(cid="socrates").first()
        assert_equals(response, None)

        new_collection = collection.Collection("socrates")
        new_cid = new_collection.cid        
        print new_collection

        # still not there
        response = collection.Collection.query.filter_by(cid="socrates").first()
        assert_equals(response, None)

        self.db.session.add(new_collection)
        self.db.session.commit()

        # and now poof there it is
        response = collection.Collection.query.filter_by(cid="socrates").first()
        assert_equals(response.cid, "socrates")


    def test_collection_with_tiids(self):
        test_tiid = "123213213"
        new_collection_tiid = collection.CollectionTiid(tiid=test_tiid)

        new_collection = collection.Collection("socrates")
        new_cid = new_collection.cid        
        print new_collection

        new_collection.tiid_links = [new_collection_tiid]
        print new_collection.tiids
        assert_equals(new_collection.tiid_links, [new_collection_tiid])

        self.db.session.add(new_collection_tiid)
        self.db.session.add(new_collection)
        self.db.session.commit()

        # and now poof there it is
        response = collection.Collection.query.filter_by(cid="socrates").first()
        assert_equals(response.cid, "socrates")
        assert_equals(response.tiid_links, [new_collection_tiid])
        assert_equals(new_collection_tiid.collection.query.all(), [new_collection])


    def test_create_new_collection(self):
        (coll_doc, collection_object) = collection.create_new_collection(
            cid=None,
            title="mah collection",
            aliases=self.aliases,
            ip_address=None,
            refset_metadata=None,
            myredis=self.r, 
            mydao=self.d)

        assert_equals(coll_doc["title"], "mah collection")
        expected = ['doi:10.124', 'doi:10.125', 'doi:10.123']
        assert_equals(coll_doc["alias_tiids"].keys(), expected)

        assert_equals(collection_object.title, "mah collection")
        expected = [(u'doi', u'10.124'), (u'doi', u'10.125'), (u'doi', u'10.123')]
        assert_equals([added_item.alias_tuple for added_item in collection_object.added_items], expected)
        assert_equals(len(collection_object.tiids), 3)
        found_collection_object = collection.Collection.query.filter_by(cid=collection_object.cid).first()
        assert_equals(found_collection_object.tiids, collection_object.tiids)


    def test_create_new_collection_from_tiids(self):
        tiids = ['234', '345']
        (coll_doc, collection_object) = collection.create_new_collection_from_tiids(
            cid=None,
            title="mah collection",
            tiids=tiids,
            ip_address=None,
            refset_metadata=None)

        assert_equals(coll_doc["title"], "mah collection")
        assert_equals(coll_doc["alias_tiids"].keys(), tiids)

        assert_equals(collection_object.title, "mah collection")
        assert_equals(collection_object.added_items, [])
        assert_equals(collection_object.tiids, tiids)
        found_collection_object = collection.Collection.query.filter_by(cid=collection_object.cid).first()
        assert_equals(found_collection_object.tiids, collection_object.tiids)


    def test_make_creates_identifier(self):
        coll = collection.Collection()
        assert_equals(len(coll.cid), 6)

        coll = collection.Collection(cid="socrates")
        assert_equals(coll.cid, "socrates")


    def test_get_titles(self):
        colls = [
            {"collection_id": "1", "title": "title 1"},
            {"collection_id": "2", "title": "title 2"},
            {"collection_id": "3", "title": "title 3"}
        ]

        for collection_params in colls:
            new_collection = collection.Collection(**collection_params)
            self.db.session.add(new_collection)

        self.db.session.commit()
        titlesDict = collection.get_titles(["1", "2", "3"])
        assert_equals(titlesDict["1"], "title 1")
        assert_equals(titlesDict["3"], "title 3")


    def test_get_metric_value_lists(self):
        response = collection.get_metric_value_lists(API_ITEMS_JSON)
        print response
        assert_equals(response['plosalm:pmc_abstract'], [70, 37, 29, 0])

    def create_test_collection(self):
        test_collection = {"_id": "testcollectionid", 
                            "title": "mycollection", 
                            "type":"collection", 
                            "created":  "2012-08-23T14:40:16.888800", 
                            "last_modified":  "2012-08-23T14:40:16.888800", 
                            "alias_tiids": {
                                       "pmid:16023720": "iaw9rzldigp4xc7p20bycnkg",
                                       "pmid:16413797": "itsq6fgx8ogi9ixysbipmtxx"}}
        test_object = collection.create_objects_from_collection_doc(test_collection) 
        db.session.add(test_object) 

        biblio1 = {
               "journal": "The Astrophysical Journal",
               "authors": "Kwok, Purton, Fitzgerald",
               "year": "1978",
               "title": "On the origin of planetary nebulae"
           }
        biblio2 = {
               "journal": "The Astrophysical Journal 2",
               "authors": "Kwok, Purton, Fitzgerald",
               "year": "1900",
               "title": "On the origin of planetary nebulae The Sequel"
           }
        metrics1 = {
           "mendeley:readers": {
               "provenance_url": "http://www.mendeley.com/research/origin-planetary-nebulae/",
               "values": {
                   "raw_history": {
                       "2013-06-22T20:41:03.178277": 4,
                       "2013-01-15T22:21:55.253826": 3,
                       "2013-07-24T17:59:26.817504": 4,
                       "2013-07-24T18:04:41.035841": 9,
                   },
                   "raw": 9
               }
           },
           "mendeley:discipline": {
               "provenance_url": "http://www.mendeley.com/research/origin-planetary-nebulae/",
               "values": {
                   "raw_history": {
                       "2013-06-22T23:03:15.852461": [
                           {
                               "name": "Astronomy / Astrophysics / Space Science",
                               "value": 100,
                               "id": 2
                           }
                       ]
                    }
                }
            }
        }
        metrics2 = {
           "topsy:tweets": {
               "provenance_url": "http://topsydrilldown",
               "values": {
                   "raw_history": {
                       "2013-11-22T20:41:03.178277": 22
                   },
                   "raw": 22
                }
            }
        }

        test_item_docs = [
            {"_id": "iaw9rzldigp4xc7p20bycnkg", "type":"item", "created": "2012-08-23T14:40:16.888800", "last_modified": "2012-08-23T14:40:16.888800", "biblio":biblio1, "metrics":metrics1, "aliases":{"pmid": ["16023720"]}},
            {"_id": "itsq6fgx8ogi9ixysbipmtxx", "type":"item", "created": "2012-08-23T14:40:16.888800", "last_modified": "2012-08-23T14:40:16.888800", "biblio":biblio2, "metrics":metrics2, "aliases":{"pmid": ["16413797"]}}
        ]

        for item_doc in test_item_docs:
            test_object = item_module.create_objects_from_item_doc(item_doc) 
            db.session.add(test_object) 

        db.session.commit() 


    def test_get_collection_with_items_for_client_no_history(self):
        self.create_test_collection()

        (returned_doc, still_updating) = collection.get_collection_with_items_for_client("testcollectionid", None, self.r, self.d)
        assert_equals(still_updating, False)
        print returned_doc
        print json.dumps(returned_doc, sort_keys=True, indent=4)
        expected =  {'created': '2012-08-23T14:40:16.888800', 'items': [{'created': '2012-08-23T14:40:16.888800', 'currently_updating': False, 'metrics': {u'mendeley:discipline': {'provenance_url': u'http://www.mendeley.com/research/origin-planetary-nebulae/', 'values': {'raw': u'[{"name": "Astronomy / Astrophysics / Space Science", "value": 100, "id": 2}]'}, 'static_meta': {'provider_url': 'http://www.mendeley.com/', 'icon': 'http://www.mendeley.com/favicon.ico', 'display_name': 'discipline, top 3 percentages', 'description': 'Percent of readers by discipline, for top three disciplines (csv, api only)', 'provider': 'Mendeley'}}, u'mendeley:readers': {'provenance_url': u'http://www.mendeley.com/research/origin-planetary-nebulae/', 'values': {'raw': u'9'}, 'static_meta': {'provider_url': 'http://www.mendeley.com/', 'icon': 'http://www.mendeley.com/favicon.ico', 'display_name': 'readers', 'description': 'The number of readers who have added the article to their libraries', 'provider': 'Mendeley'}}}, 'last_modified': '2012-08-23T14:40:16.888800', 'biblio': {'genre': 'article'}, '_id': u'iaw9rzldigp4xc7p20bycnkg', 'type': 'item', 'aliases': {u'pmid': [u'16023720']}}, {'created': '2012-08-23T14:40:16.888800', 'currently_updating': False, 'metrics': {u'topsy:tweets': {'provenance_url': u'http://topsydrilldown', 'values': {'raw': u'22'}, 'static_meta': {'provider_url': 'http://www.topsy.com/', 'icon': 'http://twitter.com/phoenix/favicon.ico', 'display_name': 'tweets', 'description': 'Number of times the item has been tweeted', 'provider': 'Topsy'}}}, 'last_modified': '2012-08-23T14:40:16.888800', 'biblio': {'genre': 'article'}, '_id': u'itsq6fgx8ogi9ixysbipmtxx', 'type': 'item', 'aliases': {u'pmid': [u'16413797']}}], 'title': u'mycollection', 'alias_tiids': {u'iaw9rzldigp4xc7p20bycnkg': u'iaw9rzldigp4xc7p20bycnkg', u'itsq6fgx8ogi9ixysbipmtxx': u'itsq6fgx8ogi9ixysbipmtxx'}, 'last_modified': '2012-08-23T14:40:16.888800', '_id': u'testcollectionid', 'type': 'collection'}
        print "**"
        print json.dumps(expected, sort_keys=True, indent=4)

        assert_equals(returned_doc, expected)


    def test_get_collection_with_items_for_client_include_history(self):
        self.create_test_collection()

        (returned_doc, still_updating) = collection.get_collection_with_items_for_client("testcollectionid", None, self.r, self.d, include_history=True)
        assert_equals(still_updating, False)
        print returned_doc
        print json.dumps(returned_doc, sort_keys=True, indent=4)
        expected = {'created': '2012-08-23T14:40:16.888800', 'items': [{'created': '2012-08-23T14:40:16.888800', 'currently_updating': False, 'metrics': {u'mendeley:discipline': {'provenance_url': u'http://www.mendeley.com/research/origin-planetary-nebulae/', 'values': {'raw': u'[{"name": "Astronomy / Astrophysics / Space Science", "value": 100, "id": 2}]', 'raw_history': {'2013-06-22T23:03:15.852461': u'[{"name": "Astronomy / Astrophysics / Space Science", "value": 100, "id": 2}]'}}, 'static_meta': {'provider_url': 'http://www.mendeley.com/', 'icon': 'http://www.mendeley.com/favicon.ico', 'display_name': 'discipline, top 3 percentages', 'description': 'Percent of readers by discipline, for top three disciplines (csv, api only)', 'provider': 'Mendeley'}}, u'mendeley:readers': {'provenance_url': u'http://www.mendeley.com/research/origin-planetary-nebulae/', 'values': {'raw': u'9', 'raw_history': {'2013-07-24T18:04:41.035841': u'9'}}, 'static_meta': {'provider_url': 'http://www.mendeley.com/', 'icon': 'http://www.mendeley.com/favicon.ico', 'display_name': 'readers', 'description': 'The number of readers who have added the article to their libraries', 'provider': 'Mendeley'}}}, 'last_modified': '2012-08-23T14:40:16.888800', 'biblio': {'genre': 'article'}, '_id': u'iaw9rzldigp4xc7p20bycnkg', 'type': 'item', 'aliases': {u'pmid': [u'16023720']}}, {'created': '2012-08-23T14:40:16.888800', 'currently_updating': False, 'metrics': {u'topsy:tweets': {'provenance_url': u'http://topsydrilldown', 'values': {'raw': u'22', 'raw_history': {'2013-11-22T20:41:03.178277': u'22'}}, 'static_meta': {'provider_url': 'http://www.topsy.com/', 'icon': 'http://twitter.com/phoenix/favicon.ico', 'display_name': 'tweets', 'description': 'Number of times the item has been tweeted', 'provider': 'Topsy'}}}, 'last_modified': '2012-08-23T14:40:16.888800', 'biblio': {'genre': 'article'}, '_id': u'itsq6fgx8ogi9ixysbipmtxx', 'type': 'item', 'aliases': {u'pmid': [u'16413797']}}], 'title': u'mycollection', 'alias_tiids': {u'iaw9rzldigp4xc7p20bycnkg': u'iaw9rzldigp4xc7p20bycnkg', u'itsq6fgx8ogi9ixysbipmtxx': u'itsq6fgx8ogi9ixysbipmtxx'}, 'last_modified': '2012-08-23T14:40:16.888800', '_id': u'testcollectionid', 'type': 'collection'}
        #print json.dumps(expected, sort_keys=True, indent=4)

        assert_equals(returned_doc, expected)


    def test_add_items_to_collection(self):
        self.create_test_collection()
        cid = "testcollectionid"

        # try a doi
        assert_equals(len(collection.Collection.query.get(cid).tiids), 2)
        collection_object = collection.add_items_to_collection(
            cid=cid, 
            aliases=[("doi", "10.1371/journal.pone.0004803")], 
            myredis=self.r)
        assert_equals(len(collection_object.tiids), 3)
        assert_equals(len(collection.Collection.query.get(cid).tiids), 3)

        test_biblio = {
                "authors": "Milojevi\u0107, Hemminger, Priem, Chen, Leydesdorff, Weingart",
                "first_author": "Milojevi\u0107",
                "first_page": "1",
                "genre": "unknown",
                "journal": "Proceedings of the American Society for Information Science and Technology",
                "number": "1",
                "title": "Information visualization state of the art and future directions",
                "volume": "49",
                "year": "2012"
            }

        # try a biblio
        collection_object = collection.add_items_to_collection(
            cid=cid, 
            aliases=[("biblio", test_biblio)],
            myredis=self.r)
        assert_equals(len(collection_object.tiids), 4)
        assert_equals(len(collection.Collection.query.get(cid).tiids), 4)

        # try the biblio again... now it adds a new one
        collection_object = collection.add_items_to_collection(
            cid=cid, 
            aliases=[("biblio", test_biblio)],
            myredis=self.r)

        assert_equals(len(collection_object.tiids), 5)
        assert_equals(len(collection.Collection.query.get(cid).tiids), 5)



    def test_make_csv_rows(self):
        csv = collection.make_csv_rows(API_ITEMS_JSON)
        expected = (OrderedDict([('tiid', u'f2dc3f36b1da11e19199c8bcc8937e3f'), ('title', 'Design Principles for Riboswitch Function'), ('doi', '10.1371/journal.pcbi.1000363'), (u'dryad:most_downloaded_file', ''), (u'dryad:package_views', ''), (u'dryad:total_downloads', ''), (u'mendeley:groups', 4), (u'mendeley:readers', 57), (u'plosalm:crossref', 16), (u'plosalm:html_views', 3361), (u'plosalm:pdf_views', 1112), (u'plosalm:pmc_abstract', 37), (u'plosalm:pmc_figure', 54), (u'plosalm:pmc_full-text', 434), (u'plosalm:pmc_pdf', 285), (u'plosalm:pmc_supp-data', 41), (u'plosalm:pmc_unique-ip', 495), (u'plosalm:pubmed_central', 9), (u'plosalm:scopus', 19), (u'wikipedia:mentions', '')]), [OrderedDict([('tiid', u'f2b45fcab1da11e19199c8bcc8937e3f'), ('title', 'Tumor-Immune Interaction, Surgical Treatment, and Cancer Recurrence in a Mathematical Model of Melanoma'), ('doi', '10.1371/journal.pcbi.1000362'), (u'dryad:most_downloaded_file', ''), (u'dryad:package_views', ''), (u'dryad:total_downloads', ''), (u'mendeley:groups', 1), (u'mendeley:readers', 13), (u'plosalm:crossref', 7), (u'plosalm:html_views', 2075), (u'plosalm:pdf_views', 484), (u'plosalm:pmc_abstract', 29), (u'plosalm:pmc_figure', 13), (u'plosalm:pmc_full-text', 232), (u'plosalm:pmc_pdf', 113), (u'plosalm:pmc_supp-data', 0), (u'plosalm:pmc_unique-ip', 251), (u'plosalm:pubmed_central', 2), (u'plosalm:scopus', 11), (u'wikipedia:mentions', '')]), OrderedDict([('tiid', u'c1eba010b1da11e19199c8bcc8937e3f'), ('title', 'Data from: Comparison of quantitative and molecular genetic variation of native vs. invasive populations of purple loosestrife (Lythrum salicaria L., Lythraceae)'), ('doi', '10.5061/dryad.1295'), (u'dryad:most_downloaded_file', 70), (u'dryad:package_views', 537), (u'dryad:total_downloads', 114), (u'mendeley:groups', ''), (u'mendeley:readers', ''), (u'plosalm:crossref', ''), (u'plosalm:html_views', ''), (u'plosalm:pdf_views', ''), (u'plosalm:pmc_abstract', ''), (u'plosalm:pmc_figure', ''), (u'plosalm:pmc_full-text', ''), (u'plosalm:pmc_pdf', ''), (u'plosalm:pmc_supp-data', ''), (u'plosalm:pmc_unique-ip', ''), (u'plosalm:pubmed_central', ''), (u'plosalm:scopus', ''), (u'wikipedia:mentions', '')]), OrderedDict([('tiid', u'c202754cb1da11e19199c8bcc8937e3f'), ('title', 'Adventures in Semantic Publishing: Exemplar Semantic Enhancements of a Research Article'), ('doi', '10.1371/journal.pcbi.1000361'), (u'dryad:most_downloaded_file', ''), (u'dryad:package_views', ''), (u'dryad:total_downloads', ''), (u'mendeley:groups', 4), (u'mendeley:readers', 52), (u'plosalm:crossref', 13), (u'plosalm:html_views', 11521), (u'plosalm:pdf_views', 1097), (u'plosalm:pmc_abstract', 70), (u'plosalm:pmc_figure', 39), (u'plosalm:pmc_full-text', 624), (u'plosalm:pmc_pdf', 149), (u'plosalm:pmc_supp-data', 6), (u'plosalm:pmc_unique-ip', 580), (u'plosalm:pubmed_central', 12), (u'plosalm:scopus', 19), (u'wikipedia:mentions', 1)]), OrderedDict([('tiid', u'f2dc3f36b1da11e19199c8bcc8937e3f'), ('title', 'Design Principles for Riboswitch Function'), ('doi', '10.1371/journal.pcbi.1000363'), (u'dryad:most_downloaded_file', ''), (u'dryad:package_views', ''), (u'dryad:total_downloads', ''), (u'mendeley:groups', 4), (u'mendeley:readers', 57), (u'plosalm:crossref', 16), (u'plosalm:html_views', 3361), (u'plosalm:pdf_views', 1112), (u'plosalm:pmc_abstract', 37), (u'plosalm:pmc_figure', 54), (u'plosalm:pmc_full-text', 434), (u'plosalm:pmc_pdf', 285), (u'plosalm:pmc_supp-data', 41), (u'plosalm:pmc_unique-ip', 495), (u'plosalm:pubmed_central', 9), (u'plosalm:scopus', 19), (u'wikipedia:mentions', '')])])
        assert_equals(csv, expected)

    def test_make_csv_stream(self):
        csv = collection.make_csv_stream(API_ITEMS_JSON)
        expected = 'tiid,title,doi,dryad:most_downloaded_file,dryad:package_views,dryad:total_downloads,mendeley:groups,mendeley:readers,plosalm:crossref,plosalm:html_views,plosalm:pdf_views,plosalm:pmc_abstract,plosalm:pmc_figure,plosalm:pmc_full-text,plosalm:pmc_pdf,plosalm:pmc_supp-data,plosalm:pmc_unique-ip,plosalm:pubmed_central,plosalm:scopus,wikipedia:mentions\r\nf2b45fcab1da11e19199c8bcc8937e3f,"Tumor-Immune Interaction, Surgical Treatment, and Cancer Recurrence in a Mathematical Model of Melanoma",10.1371/journal.pcbi.1000362,,,,1,13,7,2075,484,29,13,232,113,0,251,2,11,\r\nc1eba010b1da11e19199c8bcc8937e3f,"Data from: Comparison of quantitative and molecular genetic variation of native vs. invasive populations of purple loosestrife (Lythrum salicaria L., Lythraceae)",10.5061/dryad.1295,70,537,114,,,,,,,,,,,,,,\r\nc202754cb1da11e19199c8bcc8937e3f,Adventures in Semantic Publishing: Exemplar Semantic Enhancements of a Research Article,10.1371/journal.pcbi.1000361,,,,4,52,13,11521,1097,70,39,624,149,6,580,12,19,1\r\nf2dc3f36b1da11e19199c8bcc8937e3f,Design Principles for Riboswitch Function,10.1371/journal.pcbi.1000363,,,,4,57,16,3361,1112,37,54,434,285,41,495,9,19,\r\n'
        assert_equals(csv, expected)

    def test_get_metric_values_of_reference_sets(self):
        response = collection.get_metric_values_of_reference_sets(API_ITEMS_JSON)
        print response
        assert_equals(response['mendeley:readers'], [57, 52, 13, 0])

    def test_get_normalization_confidence_interval_ranges(self):
        input = {"facebook:shares": [1, 0, 0, 0],
            "mendeley:readers": [10, 9, 8, 7],
            'mendeley:groups': [0, 0, 0, 0]
            }
        table = [(0, 30), (10, 60), (40, 80), (50, 90), (60, 97)]
        response = collection.get_normalization_confidence_interval_ranges(input, table)
        print response
        expected = {'facebook:shares': {0: {'CI95_lower': 0,
                                           'CI95_upper': 80,
                                           'estimate_lower': 0,
                                           'estimate_upper': 50},
                                          1: {'CI95_lower': 50,
                                           'CI95_upper': 90,
                                           'estimate_lower': 75,
                                           'estimate_upper': 75},
                                          2: {'CI95_lower': 61,
                                           'CI95_upper': 100,
                                           'estimate_lower': 100,
                                           'estimate_upper': 100}},
                      'mendeley:groups': {0: {'CI95_lower': 0,
                                           'CI95_upper': 90,
                                           'estimate_lower': 0,
                                           'estimate_upper': 75},
                                          1: {'CI95_lower': 61,
                                           'CI95_upper': 100,
                                           'estimate_lower': 100,
                                           'estimate_upper': 100}},
                      'mendeley:readers': {7: {'CI95_lower': 0,
                                           'CI95_upper': 30,
                                           'estimate_lower': 0,
                                           'estimate_upper': 0},
                                          8: {'CI95_lower': 10,
                                           'CI95_upper': 60,
                                           'estimate_lower': 25,
                                           'estimate_upper': 25},
                                          9: {'CI95_lower': 40,
                                           'CI95_upper': 80,
                                           'estimate_lower': 50,
                                           'estimate_upper': 50},
                                          10: {'CI95_lower': 50,
                                           'CI95_upper': 90,
                                           'estimate_lower': 75,
                                           'estimate_upper': 75},
                                          11: {'CI95_lower': 61,
                                           'CI95_upper': 100,
                                           'estimate_lower': 100,
                                           'estimate_upper': 100}}}

        assert_equals(response, expected)

    def test_calc_table_internals(self):
        # from http://www.milefoot.com/math/stat/ci-medians.htm
        response = collection.calc_confidence_interval_table(9, 0.80, [50])
        assert_equals(response["range_sum"][50], 0.8203125)
        assert_equals(response["limits"][50], (3,7))

        # from https://onlinecourses.science.psu.edu/stat414/book/export/html/231
        response = collection.calc_confidence_interval_table(9, 0.90, [50])
        assert_equals(response["range_sum"][50], 0.9609375)
        assert_equals(response["limits"][50], (2,8))

        # from https://onlinecourses.science.psu.edu/stat414/book/export/html/231
        response = collection.calc_confidence_interval_table(14, 0.90, [50])
        assert_equals(response["range_sum"][50], 0.942626953125)
        assert_equals(response["limits"][50], (4,11))

    def test_calc_table_extremes(self):
        response = collection.calc_confidence_interval_table(9, 0.95, [90])
        assert_equals(response["range_sum"][90], 0.9916689060000001)
        assert_equals(response["limits"][90], (6,10))

        response = collection.calc_confidence_interval_table(9, 0.95, [10])
        assert_equals(response["range_sum"][10], 0.9916689060000002)
        assert_equals(response["limits"][10], (0,4))


    def test_calc_table(self):
        response = collection.calc_confidence_interval_table(9, 0.95, [i*10 for i in range(10)])
        print response["lookup_table"]
        expected = [(10, 30), (10, 40), (10, 60), (20, 60), (30, 70), (40, 80), (50, 90), (60, 90), (70, 90)]
        assert_equals(response["lookup_table"], expected)

        response = collection.calc_confidence_interval_table(50, 0.95, range(100))
        print response["lookup_table"]
        expected = [(1, 9), (1, 13), (2, 15), (3, 17), (5, 21), (6, 23), (7, 25), (8, 27), (10, 29), (12, 33), (13, 35), (14, 37), (16, 39), (18, 41), (20, 43), (21, 45), (22, 47), (24, 49), (26, 50), (28, 52), (30, 54), (32, 56), (33, 58), (34, 60), (36, 62), (38, 64), (40, 66), (42, 67), (44, 68), (46, 70), (48, 72), (50, 74), (51, 76), (53, 78), (55, 79), (57, 80), (59, 82), (61, 84), (63, 86), (65, 87), (67, 88), (71, 90), (73, 92), (75, 93), (77, 94), (79, 95), (83, 97), (85, 98), (87, 99), (91, 99)]
        assert_equals(response["lookup_table"], expected)

########NEW FILE########
__FILENAME__ = test_fakes
import unittest, re
from test.utils import slow, http
from nose.tools import assert_equals, raises
from totalimpact.fakes import IdSampler


class TestDoiSampler(unittest.TestCase):
    
    doi_regex = "10\.\d+" # just tests the front part, won't match whole string 

    @slow
    def test_get_doi(self):
        sampler = IdSampler()
        dois_list = sampler.get_dois()
        try:
            assert re.match(self.doi_regex, dois_list[0]), dois_list
        except IndexError:
            print "random doi is down"

    @slow
    def test_get_multiple_dois(self):
        sampler = IdSampler()
        dois_list = sampler.get_dois(10)
        try:
            dois_list[0]  # test to see if random doi service down first
            assert_equals(len(dois_list), 10)
            assert re.match(self.doi_regex, dois_list[7]), dois_list
        except IndexError:
            print "random doi is down"
        
        
class TestGitHubUserNameSampler(unittest.TestCase):

    @slow
    def test_get_github_username(self):
        sampler = IdSampler()
        username = sampler.get_github_username()
        assert isinstance(username, basestring)
        assert len(username) > 0, username

########NEW FILE########
__FILENAME__ = test_incoming_email
from totalimpact import db, app
from totalimpact import incoming_email
from totalimpact.incoming_email import IncomingEmail

from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy.exc import OperationalError

import os, json, copy

from nose.tools import raises, assert_equals, nottest
import unittest
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests


class TestIncomingEmail():

    def setUp(self):
        self.db = setup_postgres_for_unittests(db, app)

        # example from http://docs.cloudmailin.com/http_post_formats/json/        
        self.example_payload = {
               "headers": {
                   "To": "7be5eb5001593217143f@cloudmailin.net",
                   "Mime-Version": "1.0",
                   "X-Received": "by 10.58.45.134 with SMTP id n6mr13476387vem.35.1361476813304; Thu, 21 Feb 2013 12:00:13 -0800 (PST)",
                   "Received": "by mail-vc0-f202.google.com with SMTP id m8so955261vcd.3 for <7be5eb5001593217143f@cloudmailin.net>; Thu, 21 Feb 2013 12:00:13 -0800",
                   "From": "Google Scholar Alerts <scholaralerts-noreply@google.com>",
                   "DKIM-Signature": "v=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=20120113; h=mime-version:x-received:message-id:date:subject:from:to :content-type; bh=74dhtWOnoX2dYtmZibjD2+Tp65AZ7UnVwRTR7Qwho/o=; b=Fabq5urMfTyUX0s3XgFhVx1pyZ+tW/n38Sm/3T5EXTWeG2k7C6mxbrv1DdmpNpl/a8 Sr70eG6St7oytXii5tg9TrwrlwhftpFZKkJQS8GMWswiEaBkOfnNkoRrN174jRYfBUuZ oKWJr49dxw9hV3uKYoSis0zL6R8P+7GXt1rtqblBELrfIJ3pKC7d7WS65i6hdM2kA+sY va9geqt1fFFN7098U7WELlM2JoXhS4fbIQTev/Z6cF89Sfs4888GXb7PIq0d1kfd6t7c kXK8bV6TkqSP4AxDm646Cv1TR9cfo6+9yCrkK8oW6ihAMzM0Lwobq22NLrRY2QK8494s WAuA==",
                   "Date": "Thu, 21 Feb 2013 20:00:13 +0000",
                   "Message-ID": "<089e0115f968d3b38604d6418577@google.com>",
                   "Content-Type": "text/plain; charset=ISO-8859-1; delsp=yes; format=flowed",
                   "Subject": "Confirm your Google Scholar Alert"
               },
               "reply_plain": None,
               "attachments": [
               ],
               "plain": "Google received a request to start sending Scholar Alerts to  \n7be5eb5001593217143f@cloudmailin.net for the query:\nNew articles in Jonathan A. Eisen's profile\n\nClick to confirm this request:\nhttp://scholar.google.ca/scholar_alerts?update_op=confirm_alert&hl=en&alert_id=IMEzMffmofYJ&email_for_op=7be5eb5001593217143f%40cloudmailin.net\n\nClick to cancel this request:\nhttp://scholar.google.ca/scholar_alerts?view_op=cancel_alert_options&hl=en&alert_id=IMEzMffmofYJ&email_for_op=7be5eb5001593217143f%40cloudmailin.net\n\nThanks,\nThe Google Scholar Team",
               "envelope": {
                   "to": "7be5eb5001593217143f@cloudmailin.net",
                   "helo_domain": "mail-vc0-f202.google.com",
                   "from": "3zXwmURUKAO4iSXebQhQbUhji-dehUfboWeeWbU.Sec@scholar-alerts.bounces.google.com",
                   "remote_ip": "209.85.220.202",
                   "spf": {
                       "domain": "scholar-alerts.bounces.google.com",
                       "result": "neutral"
                   }
               },
               "html": None
            }

    def tearDown(self):
        teardown_postgres_for_unittests(self.db)



    def test_init_incoming_mail(self):
        all_email = IncomingEmail.query.all()
        assert_equals(all_email, [])

        self.existing_email = IncomingEmail(self.example_payload)

        self.db.session.add(self.existing_email)
        self.db.session.commit()

        all_email = IncomingEmail.query.all()
        assert_equals(len(all_email), 1)
        assert_equals(json.loads(all_email[0].payload), self.example_payload)

    def test_save_email(self):
        all_email = IncomingEmail.query.all()
        assert_equals(all_email, [])

        #does the commits etc
        self.existing_email = incoming_email.save_incoming_email(self.example_payload)

        all_email = IncomingEmail.query.all()
        assert_equals(len(all_email), 1)
        assert_equals(json.loads(all_email[0].payload), self.example_payload)


    def test_log_if_google_scholar_notification_confirmation(self):
        self.confirmation_email = IncomingEmail(self.example_payload)
        response = self.confirmation_email.log_if_google_scholar_notification_confirmation()
        print response
        expected = ('Jonathan A. Eisen', 'http://scholar.google.ca/scholar_alerts?update_op=confirm_alert&hl=en&alert_id=IMEzMffmofYJ&email_for_op=7be5eb5001593217143f%40cloudmailin.net')
        assert_equals(response, expected)

        self.example_payload["plain"] = "this is not the email you are looking for"
        different_email = IncomingEmail(self.example_payload)
        response = different_email.log_if_google_scholar_notification_confirmation()
        expected = (None, None)
        assert_equals(response, expected)


    def test_log_if_google_scholar_new_articles(self):
        self.example_payload["headers"]["Subject"] = "Scholar Alert - John P. A. Ioannidis - new articles"
        self.new_articles_email = IncomingEmail(self.example_payload)
        response = self.new_articles_email.log_if_google_scholar_new_articles()
        expected = 'John P. A. Ioannidis'
        assert_equals(response, expected)

        self.example_payload["headers"]["Subject"] = "this is not the email you are looking for"
        different_email = IncomingEmail(self.example_payload)
        response = different_email.log_if_google_scholar_new_articles()
        expected = None
        assert_equals(response, expected)





########NEW FILE########
__FILENAME__ = test_item
from nose.tools import raises, assert_equals, assert_true, assert_greater, assert_items_equal, nottest
import os, unittest, hashlib, json, pprint, datetime
from time import sleep
from werkzeug.security import generate_password_hash
from totalimpact import models, tiredis
from totalimpact import db, app
from totalimpact import item as item_module
from totalimpact.item import Item, Metric, Biblio, Alias
from totalimpact.providers import bibtex, github
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests


class TestItem():

    def setUp(self):
        self.BIBLIO_DATA = {
            "title": "An extension of de Finetti's theorem",
            "journal": "Advances in Applied Probability",
            "author": [
                "Pitman, J"
            ],
            "authors": "Pitman",
            "collection": "pitnoid",
            "volume": "10",
            "id": "p78",
            "year": "1978",
            "pages": "268 to 270"
        }

        self.ALIAS_DATA = {
            "title":["Why Most Published Research Findings Are False"],
            "url":["http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124"],
            "doi": ["10.1371/journal.pmed.0020124"],
            "biblio": [self.BIBLIO_DATA]
        }


        self.KEY1 = '2012-08-23T14:40:16.888888'
        self.KEY2 = '2012-08-23T14:40:16.999999'
        self.VAL1 = 1
        self.VAL2 = 2

        METRICS_DATA = {
            "provenance_url": "http://api.mendeley.com/research/public-chemical-compound-databases/",
            "values":{
                "raw": self.VAL2,
                "raw_history": {
                    self.KEY1: self.VAL1,
                    self.KEY2: self.VAL2
                }
            }
        }

        METRICS_DATA2 = {
            "provenance_url": "http://api.mendeley.com/research/public-chemical-compound-databases/",
            "values":{
                "raw": self.VAL2,
                "raw_history": {
                    self.KEY1: self.VAL1,
                    self.KEY2: self.VAL2
                }
            }
        }

        METRICS_DATA3 = {
            "provenance_url": "http://api.mendeley.com/research/public-chemical-compound-databases/",
            "values":{
                "raw": self.VAL2,
                "raw_history": {
                    self.KEY1: self.VAL1,
                    self.KEY2: self.VAL2
                }
            }
        }


        self.ITEM_DATA = {
            "_id": "test",
            "created": '2012-08-23T14:40:16.399932',
            "last_modified": '2012-08-23T14:40:16.399932',
            "last_update_run": '2012-08-23T14:40:16.399932',
            "aliases": self.ALIAS_DATA,
            "metrics": {
                "wikipedia:mentions": METRICS_DATA,
                "altmetric_com:tweets": METRICS_DATA2
            },
            "biblio": self.BIBLIO_DATA,
            "type": "item"
        }

        self.TEST_PROVIDER_CONFIG = [
            ("wikipedia", {})
        ]

        self.d = None
        
        self.myrefsets = {"nih": {"2011": {
                        "facebook:comments": {0: [1, 99], 1: [91, 99]}, "mendeley:groups": {0: [1, 99], 3: [91, 99]}
                    }}}

        # setup a clean new redis test database.  We're putting unittest redis at DB Number 8.
        self.r = tiredis.from_url("redis://localhost:6379", db=8)
        self.r.flushdb()

        self.db = setup_postgres_for_unittests(db, app)
        


    def tearDown(self):
        teardown_postgres_for_unittests(self.db)


    def save_test_item(self):
        self.TEST_OBJECT = item_module.create_objects_from_item_doc(self.ITEM_DATA)        
        self.db.session.add(self.TEST_OBJECT)
        self.db.session.commit()


    def test_init_item_and_add_aliases(self):
        item_object = Item()
        print item_object

        self.db.session.add(item_object)
        self.db.session.commit()

        # we have an item but it has no aliases
        found_item = Item.query.first()
        assert_true(len(found_item.tiid) > 10)
        assert_equals(found_item.aliases, [])

        test_alias = ("doi", "10.123/abc")
        (test_namespace, test_nid) = test_alias

        #make sure nothing there beforehand
        response = Alias.filter_by_alias(test_alias).first()
        assert_equals(response, None)

        new_alias = Alias(alias_tuple=test_alias)
        print new_alias
        self.db.session.add(item_object)
        item_object.aliases = [new_alias]

        # still not there
        response = Alias.filter_by_alias(test_alias).first()
        assert_equals(response, None)

        self.db.session.commit()

        # and now poof there it is
        response = Alias.query.all()
        assert_equals(response[0].alias_tuple, test_alias)

        response = Alias.query.filter_by(nid=test_alias[1]).first()
        assert_equals(response.nid, test_alias[1])

        response = Alias.filter_by_alias(test_alias).first()
        assert_equals(response.alias_tuple, test_alias)

        response = item_module.get_tiid_by_alias(test_namespace, test_nid)
        assert_equals(response, found_item.tiid)


    def test_add_biblio(self):
        new_item = Item()
        tiid = new_item.tiid
        print new_item

        #add biblio
        self.db.session.add(new_item)
        new_biblio_objects = item_module.create_biblio_objects([self.BIBLIO_DATA]) 
        new_item.biblios = new_biblio_objects
        self.db.session.commit()

        # now poof there is biblio
        found_item = Item.from_tiid(tiid)
        expected = [u'10', u'Pitman', u"An extension of de Finetti's theorem", u'Advances in Applied Probability', [u"Pitman, J"], u'1978', u'p78', u'pitnoid', u'268 to 270']
        assert_equals([bib.biblio_value for bib in found_item.biblios], expected)
        
        assert_equals(Biblio.as_dict_by_tiid(tiid), self.BIBLIO_DATA)


    def test_get_tiid_by_biblio(self):

        new_item = Item()
        self.db.session.add(new_item)
        new_item.biblios = item_module.create_biblio_objects([self.BIBLIO_DATA]) 
        self.db.session.commit()

        found_tiid = item_module.get_tiid_by_biblio(self.BIBLIO_DATA)
        assert_equals(found_tiid, new_item.tiid)


    def test_add_metrics(self):
        test_metrics = {
            "altmetric_com:tweets": {
                "provenance_url": "http://topsy.com/trackback?url=http%3A//elife.elifesciences.org/content/2/e00646",
                "values": {
                    "raw_history": {
                        "2013-03-29T17:57:41.455719": 1,
                        "2013-04-11T12:57:37.260362": 2,
                        "2013-04-19T07:27:23.117982": 3
                        },
                    "raw": 3
                    }
                } 
            }
        new_item = Item()
        tiid = new_item.tiid
        print new_item

        #add metrics
        metric_objects = item_module.create_metric_objects(test_metrics)
        new_item.metrics = metric_objects
        self.db.session.add(new_item)
        self.db.session.commit()

        # now poof there is metrics
        found_item = Item.from_tiid(tiid)
        expected = "hi"
        assert_equals(len(found_item.metrics), 3)
        assert_equals(found_item.metrics[0].tiid, tiid)
        assert_equals(found_item.metrics[0].provider, "altmetric_com")
        assert_equals(found_item.metrics[0].raw_value, 1)
        
        test_metrics2 =  {
            "mendeley:country": {
               "values": {
                   "raw_history": {
                       "2013-08-26T09:25:32.750867": [
                           {
                               "value": 38,
                               "name": "United States"
                           },
                           {
                               "value": 23,
                               "name": "Germany"
                           },
                           {
                               "value": 15,
                               "name": "Brazil"
                           }
                       ],
                       "2013-08-27T14:57:29.173887": [
                           {
                               "value": 38,
                               "name": "United States"
                           },
                           {
                               "value": 25,
                               "name": "Germany"
                           },
                           {
                               "value": 13,
                               "name": "Brazil"
                           }
                       ]
                   },
                   "raw": [
                       {
                           "value": 38,
                           "name": "United States"
                       },
                       {
                           "value": 25,
                           "name": "Germany"
                       },
                       {
                           "value": 13,
                           "name": "Brazil"
                       }
                   ]
               },
               "provenance_url": "http://www.mendeley.com/research/phylogeny-informative-measuring-power-comparative-methods-2/"
           }
        }

        metric_objects = item_module.create_metric_objects(test_metrics2)
        new_item.metrics += metric_objects
        self.db.session.add(new_item)
        self.db.session.commit()

        # now poof there is metrics
        found_item = Item.from_tiid(tiid)
        expected = "hi"
        assert_equals(len(found_item.metrics), 5)
        assert_equals(found_item.metrics[4].tiid, tiid)
        assert_equals(found_item.metrics[4].provider, "mendeley")
        expected = [{u'name': u'United States', u'value': 38}, {u'name': u'Germany', u'value': 25}, {u'name': u'Brazil', u'value': 13}]
        assert_equals(found_item.metrics[4].raw_value, expected)



    def test_make_new(self):
        '''create an item from scratch.'''
        item = item_module.make()
        assert_equals(len(item["_id"]), 24)
        assert_equals(item["aliases"], {})

    def test_adds_genre(self):
        self.TEST_OBJECT = item_module.create_objects_from_item_doc(self.ITEM_DATA)        
        self.db.session.add(self.TEST_OBJECT)
        self.db.session.commit()

        item = item_module.get_item("test", self.myrefsets, self.r)
        assert_equals(item["biblio"]['genre'], "article")

    def test_get_metric_names(self):
        response = item_module.get_metric_names(self.TEST_PROVIDER_CONFIG)
        assert_equals(response, ['wikipedia:mentions'])


    def test_decide_genre_article_doi(self):
        aliases = {"doi":["10:123", "10:456"]}
        (genre, host) = item_module.decide_genre(aliases)
        assert_equals(genre, "article")

    def test_decide_genre_article_pmid(self):
        aliases = {"pmid":["12345678"]}
        (genre, host) = item_module.decide_genre(aliases)
        assert_equals(genre, "article")

    def test_decide_genre_slides(self):
        aliases = {"url":["http://www.slideshare.net/jason/my-slides"]}
        (genre, host) = item_module.decide_genre(aliases)
        assert_equals(genre, "slides")

    def test_decide_genre_software(self):
        aliases = {"url":["http://www.github.com/jasonpriem/my-sofware"]}
        (genre, host) = item_module.decide_genre(aliases)
        assert_equals(genre, "software")

    def test_decide_genre_dataset_dryad(self):
        aliases = {"doi":["10.5061/dryad.18"]}
        (genre, host) = item_module.decide_genre(aliases)
        assert_equals(genre, "dataset")

    def test_decide_genre_dataset_figshare(self):
        aliases = {"doi":["10.6084/m9.figshare.92393"]}
        (genre, host) = item_module.decide_genre(aliases)
        assert_equals(genre, "dataset")

    def test_decide_genre_webpage(self):
        aliases = {"url":["http://www.google.com"]}
        (genre, host) = item_module.decide_genre(aliases)
        assert_equals(genre, "webpage")

    def test_decide_genre_unknown(self):
        aliases = {"unknown_namespace":["myname"]}
        (genre, host) = item_module.decide_genre(aliases)
        assert_equals(genre, "unknown")

    def test_get_biblio_to_update(self):
        first_biblio = {
           "journal": "Space",
           "title": "A real title"
        }        
        other_biblio = {
           "journal": "Earth",
           "title": "A different title"
        }        
        aop_biblio = {
           "journal": "Nature",
           "title": "AOP"
        }
        response = item_module.get_biblio_to_update({}, first_biblio)
        assert_equals(response, first_biblio)

        response = item_module.get_biblio_to_update(first_biblio, other_biblio)
        assert_equals(response, {})

        response = item_module.get_biblio_to_update(aop_biblio, other_biblio)
        print response
        assert_equals(response, {'title': 'A different title'})



    def test_merge_alias_dicts(self):
        aliases1 = {"ns1":["idA", "idB", "id1"]}
        aliases2 = {"ns1":["idA", "id3", "id4"], "ns2":["id1", "id2"]}
        response = item_module.merge_alias_dicts(aliases1, aliases2)
        print response
        expected = {'ns1': ['idA', 'idB', 'id1', 'id3', 'id4'], 'ns2': ['id1', 'id2']}
        assert_equals(response, expected)

    def test_alias_tuples_from_dict(self):
        aliases = {"unknown_namespace":["myname"]}
        alias_tuples = item_module.alias_tuples_from_dict(aliases)
        assert_equals(alias_tuples, [('unknown_namespace', 'myname')])

    def test_alias_dict_from_tuples(self):
        aliases = [('unknown_namespace', 'myname')]
        alias_dict = item_module.alias_dict_from_tuples(aliases)
        assert_equals(alias_dict, {'unknown_namespace': ['myname']})

    def test_as_old_doc(self):
        test_object = item_module.create_objects_from_item_doc(self.ITEM_DATA)        
        new_doc = test_object.as_old_doc()
        print json.dumps(new_doc, sort_keys=True, indent=4)
        print json.dumps(self.ITEM_DATA, sort_keys=True, indent=4)
        assert_equals(new_doc, self.ITEM_DATA)

    def test_build_item_for_client(self):
        item = {'created': '2012-08-23T14:40:16.399932', '_rev': '6-3e0ede6e797af40860e9dadfb39056ce', 'last_modified': '2012-08-23T14:40:16.399932', 'biblio': {'title': 'Perceptual training strongly improves visual motion perception in schizophrenia', 'journal': 'Brain and Cognition', 'year': 2011, 'authors': u'Norton, McBain, \xd6ng\xfcr, Chen'}, '_id': '4mlln04q1rxy6l9oeb3t7ftv', 'type': 'item', 'aliases': {'url': ['http://linkinghub.elsevier.com/retrieve/pii/S0278262611001308', 'http://www.ncbi.nlm.nih.gov/pubmed/21872380'], 'pmid': ['21872380'], 'doi': ['10.1016/j.bandc.2011.08.003'], 'title': ['Perceptual training strongly improves visual motion perception in schizophrenia']}}
        response = item_module.build_item_for_client(item, self.myrefsets, self.r)
        assert_equals(set(response.keys()), set(['currently_updating', 'created', '_rev', 'metrics', 'last_modified', 'biblio', '_id', 'type', 'aliases']))

    def test_build_item_for_client_excludes_history_by_default(self):
        response = item_module.build_item_for_client(self.ITEM_DATA, self.myrefsets, self.r)
        assert_equals(response["metrics"]["wikipedia:mentions"]["values"].keys(), ["raw"])
        assert_equals(response["metrics"]["altmetric_com:tweets"]["values"].keys(), ["raw"])


    def test_add_metrics_data(self):
        item = {'created': '2012-08-23T14:40:16.399932', '_rev': '6-3e0ede6e797af40860e9dadfb39056ce', 'last_modified': '2012-08-23T14:40:16.399932', 'biblio': {'title': 'Perceptual training strongly improves visual motion perception in schizophrenia', 'journal': 'Brain and Cognition', 'year': 2011, 'authors': u'Norton, McBain, \xd6ng\xfcr, Chen'}, '_id': '4mlln04q1rxy6l9oeb3t7ftv', 'type': 'item', 'aliases': {'url': ['http://linkinghub.elsevier.com/retrieve/pii/S0278262611001308', 'http://www.ncbi.nlm.nih.gov/pubmed/21872380'], 'pmid': ['21872380'], 'doi': ['10.1016/j.bandc.2011.08.003'], 'title': ['Perceptual training strongly improves visual motion perception in schizophrenia']}}
        metrics_method_response = (2, 'http://api.mendeley.com/research/perceptual-training-strongly-improves-visual-motion-perception-schizophrenia/')
        response = item_module.add_metrics_data("mendeley:readers", metrics_method_response, item)
        print json.dumps(response, sort_keys=True, indent=4)
        assert_equals(response["metrics"]["mendeley:readers"]["values"]["raw"], 2)
        assert_equals(response["metrics"]["mendeley:readers"]["values"]["raw_history"].values(), [2])
        assert_equals(response["metrics"]["mendeley:readers"]["provenance_url"], 'http://api.mendeley.com/research/perceptual-training-strongly-improves-visual-motion-perception-schizophrenia/')

    def test_clean_for_export_no_key(self):
        self.save_test_item()

        item = item_module.get_item("test", self.myrefsets, self.r)
        item["metrics"]["scopus:citations"] = {"values":{"raw": 22}}
        item["metrics"]["citeulike:bookmarks"] = {"values":{"raw": 33}}
        response = item_module.clean_for_export(item)
        print response["metrics"].keys()
        expected = ['altmetric_com:tweets', 'wikipedia:mentions']
        assert_items_equal(response["metrics"].keys(), expected)

    def test_clean_for_export_given_correct_secret_key(self):
        self.save_test_item()

        item = item_module.get_item("test", self.myrefsets, self.r)
        item["metrics"]["scopus:citations"] = {"values":{"raw": 22}}
        item["metrics"]["citeulike:bookmarks"] = {"values":{"raw": 33}}
        response = item_module.clean_for_export(item, "SECRET", "SECRET")
        print response["metrics"].keys()
        expected = ['altmetric_com:tweets', 'wikipedia:mentions', 'scopus:citations', 'citeulike:bookmarks']
        assert_equals(sorted(response["metrics"].keys()), sorted(expected))

    def test_clean_for_export_given_wrong_secret_key(self):
        self.save_test_item()

        item = item_module.get_item("test", self.myrefsets, self.r)
        item["metrics"]["scopus:citations"] = {"values":{"raw": 22}}
        item["metrics"]["citeulike:bookmarks"] = {"values":{"raw": 33}}
        response = item_module.clean_for_export(item, "WRONG", "SECRET")
        print response["metrics"].keys()
        expected = ['altmetric_com:tweets', 'wikipedia:mentions']
        assert_items_equal(response["metrics"].keys(), expected)

    def test_get_tiids_from_aliases(self):

        self.save_test_item()

        aliases = [ ("doi", "10.1371/journal.pmed.0020124"), 
                    ("doi", "not_a_doi_in_our_db"), 
                    ("url", self.ALIAS_DATA["url"][0])
                    ]
        response = item_module.get_tiids_from_aliases(aliases)
        print response
        expected = {('doi', 'not_a_doi_in_our_db'): None, ('url', 'http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124'): u'test', ('doi', '10.1371/journal.pmed.0020124'): u'test'}
        assert_equals(response, expected)


    def test_create_tiids_from_aliases(self):

        aliases = [('url', 'http://starbucks.com'), ('url', 'http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124')]

        response = item_module.create_tiids_from_aliases(aliases, {}, self.r)
        print response
        assert_equals(len(response.keys()), 2)
        expected = [('url', 'http://starbucks.com'), ('url', 'http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124')]
        assert_items_equal(response.values(), expected)


    def test_create_tiids_from_aliases_biblio(self):

        aliases = [('biblio', self.BIBLIO_DATA)]

        response = item_module.create_tiids_from_aliases(aliases, {}, self.r)
        print response
        assert_equals(len(response.keys()), 1)
        expected = [("biblio", self.BIBLIO_DATA)]
        assert_items_equal(response.values(), expected)


    def test_get_items_from_tiids(self):
        aliases = [('url', 'http://starbucks.com'), ('url', 'http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124')]
        response = item_module.create_tiids_from_aliases(aliases, {}, self.r)
        tiids = response.keys()
        items = item_module.get_items_from_tiids(tiids)
        print [item.alias_tuples for item in items]
        expected = [[(u'url', u'http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124')], [(u'url', u'http://starbucks.com')]]
        assert_items_equal([item.alias_tuples for item in items], expected)


    def test_duplicates_list(self):
        item_docs = [
            {"_id": "a1", "last_modified": "now",
                "aliases": {"doi":["doi.org/aaa"], "url":["111", "def.com"]}}
            ,{"_id": "b2", "last_modified": "now",
                "aliases": {"doi":["doi.org/222"]}}
            ,{"_id": "c2", "last_modified": "now",
                "aliases": {"doi":["doi.org/222"]}}
            ,{"_id": "d2", "last_modified": "now",
                "aliases": {"doi":["doi.org/222"], "url":["foo"]}}
            ,{"_id": "e1",  "last_modified": "now",
                "aliases": {"url":["111"]}}
            ,{"_id": "f3",  "last_modified": "now",
                "aliases": {"doi":["333"], "url":["333"]}}
            ,{"_id": "g4",  "last_modified": "now",
                "aliases": {
                    "biblio": [{"title": "my paper", "authors": "smith"}]
                }}
            ,{"_id": "h4",  "last_modified": "now",
                "aliases": {
                    "biblio": [{"title": "My paper", "authors": "Smith"}]
                }}
            ]

        item_objs = [item_module.create_objects_from_item_doc(item_doc) for item_doc in item_docs]
        item_objs[-1].biblios[0].provider = "user_provided"
        tiids = [item.tiid for item in item_objs]

        response = item_module.build_duplicates_list(tiids)
        print response
        expected = [[{'tiid': u'a1', 'has_user_provided_biblio': False}, {'tiid': u'e1', 'has_user_provided_biblio': False}], [{'tiid': u'b2', 'has_user_provided_biblio': False}, {'tiid': u'c2', 'has_user_provided_biblio': False}, {'tiid': u'd2', 'has_user_provided_biblio': False}], [{'tiid': u'f3', 'has_user_provided_biblio': False}], [{'tiid': u'g4', 'has_user_provided_biblio': False}, {'tiid': u'h4', 'has_user_provided_biblio': True}]]
        assert_equals(response, expected)
       



########NEW FILE########
__FILENAME__ = test_models
from nose.tools import raises, assert_equals, nottest
import os, unittest, hashlib, json, pprint, datetime
from time import sleep
from werkzeug.security import generate_password_hash
from totalimpact import models, tiredis
from totalimpact.providers import bibtex, github


class TestMemberItems():

    def setUp(self):
        # setup a clean new redis database at our unittest redis DB location: Number 8
        self.r = tiredis.from_url("redis://localhost:6379", db=8)
        self.r.flushdb()

        bibtex.Bibtex.paginate = lambda self, x: {"pages": [1,2,3,4], "number_entries":10}
        bibtex.Bibtex.member_items = lambda self, x: ("doi", str(x))
        self.memberitems_resp = [
            ["doi", "1"],
            ["doi", "2"],
            ["doi", "3"],
            ["doi", "4"],
        ]

        self.mi = models.MemberItems(bibtex.Bibtex(), self.r)

    def test_init(self):
        assert_equals(self.mi.__class__.__name__, "MemberItems")
        assert_equals(self.mi.provider.__class__.__name__, "Bibtex")

    def test_start_update(self):
        ret = self.mi.start_update("1234")
        input_hash = hashlib.md5("1234").hexdigest()
        assert_equals(input_hash, ret)

        sleep(.1) # give the thread a chance to finish.
        status = self.r.get_memberitems_status(input_hash)

        assert_equals(status["memberitems"], self.memberitems_resp )
        assert_equals(status["complete"], 4 )

    def test_get_sync(self):
        github.Github.member_items = lambda self, x: \
                [("github", name) for name in ["project1", "project2", "project3"]]
        synch_mi = models.MemberItems(github.Github(), self.r)

        # we haven't put q in redis with MemberItems.start_update(q),
        # so this should update while we wait.
        ret = synch_mi.get_sync("jasonpriem")
        assert_equals(ret["pages"], 1)
        assert_equals(ret["complete"], 1)
        assert_equals(ret["memberitems"],
            [
                ("github", "project1"),
                ("github", "project2"),
                ("github", "project3")
            ]
        )


    def test_get_async(self):
        ret = self.mi.start_update("1234")
        sleep(.1)
        res = self.mi.get_async(ret)
        print res
        assert_equals(res["complete"], 4)
        assert_equals(res["memberitems"], self.memberitems_resp)




########NEW FILE########
__FILENAME__ = test_provider_batch_data
from totalimpact import db, app
from totalimpact import provider_batch_data
from totalimpact.provider_batch_data import ProviderBatchData

from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy.exc import OperationalError

import os, json, copy

from nose.tools import raises, assert_equals, nottest
import unittest
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests


class TestProviderBatchData():

    def setUp(self):
        self.db = setup_postgres_for_unittests(db, app)

        self.test_data = {
               "raw": "<pmc-web-stat><request year=\"2012\" month=\"10\" jrid=\"elife\" eissn=\"2050-084X\"></request><response status=\"0\" collection=\"eLife\"></response><articles><article id=\"PMC3463246\"><meta-data doi=\"10.7554/eLife.00013\" pmcid=\"PMC3463246\" pubmed-id=\"23066504\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00013\"/><usage unique-ip=\"1368\" full-text=\"1464\" pdf=\"722\" abstract=\"119\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"144\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3463247\"><meta-data doi=\"10.7554/eLife.00240\" pmcid=\"PMC3463247\" pubmed-id=\"23066507\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00240\"/><usage unique-ip=\"514\" full-text=\"606\" pdf=\"230\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"9\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3465569\"><meta-data doi=\"10.7554/eLife.00242\" pmcid=\"PMC3465569\" pubmed-id=\"23066508\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00242\"/><usage unique-ip=\"473\" full-text=\"503\" pdf=\"181\" abstract=\"2\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"13\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3465570\"><meta-data doi=\"10.7554/eLife.00243\" pmcid=\"PMC3465570\" pubmed-id=\"23066509\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00243\"/><usage unique-ip=\"547\" full-text=\"636\" pdf=\"227\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"56\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3466591\"><meta-data doi=\"10.7554/eLife.00065\" pmcid=\"PMC3466591\" pubmed-id=\"23066506\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00065\"/><usage unique-ip=\"2516\" full-text=\"2804\" pdf=\"1583\" abstract=\"195\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"405\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3466783\"><meta-data doi=\"10.7554/eLife.00007\" pmcid=\"PMC3466783\" pubmed-id=\"23066503\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00007\"/><usage unique-ip=\"1331\" full-text=\"1412\" pdf=\"898\" abstract=\"224\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"109\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3467772\"><meta-data doi=\"10.7554/eLife.00270\" pmcid=\"PMC3467772\" pubmed-id=\"23066510\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00270\"/><usage unique-ip=\"1396\" full-text=\"1776\" pdf=\"625\" abstract=\"4\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3470722\"><meta-data doi=\"10.7554/eLife.00286\" pmcid=\"PMC3470722\" pubmed-id=\"23071903\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00286\"/><usage unique-ip=\"909\" full-text=\"1030\" pdf=\"376\" abstract=\"6\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3479833\"><meta-data doi=\"10.7554/eLife.00031\" pmcid=\"PMC3479833\" pubmed-id=\"23110253\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00031\"/><usage unique-ip=\"154\" full-text=\"126\" pdf=\"87\" abstract=\"26\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"13\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3470409\"><meta-data doi=\"10.7554/eLife.00048\" pmcid=\"PMC3470409\" pubmed-id=\"23066505\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00048\"/><usage unique-ip=\"1250\" full-text=\"1361\" pdf=\"911\" abstract=\"237\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"317\" supp-data=\"4\" cited-by=\"0\"/></article><article id=\"PMC3482692\"><meta-data doi=\"10.7554/eLife.00102\" pmcid=\"PMC3482692\" pubmed-id=\"23110254\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00102\"/><usage unique-ip=\"259\" full-text=\"232\" pdf=\"133\" abstract=\"36\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"8\" supp-data=\"3\" cited-by=\"0\"/></article><article id=\"PMC3482687\"><meta-data doi=\"10.7554/eLife.00281\" pmcid=\"PMC3482687\" pubmed-id=\"23110255\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00281\"/><usage unique-ip=\"75\" full-text=\"53\" pdf=\"47\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"1\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3482686\"><meta-data doi=\"10.7554/eLife.00005\" pmcid=\"PMC3482686\" pubmed-id=\"23110252\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00005\"/><usage unique-ip=\"324\" full-text=\"249\" pdf=\"263\" abstract=\"71\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"93\" supp-data=\"17\" cited-by=\"0\"/></article></articles></pmc-web-stat>",
               "max_event_date": "2012-10-31T07:34:01.126892",
               "provider": "pmc",
               "aliases": {
                   "pmid": [
                       "23066504",
                       "23066507",
                       "23066508",
                       "23066509",
                       "23066506",
                       "23066503",
                       "23066510",
                       "23071903",
                       "23110253",
                       "23066505",
                       "23110254",
                       "23110255",
                       "23110252"
                   ]
               },
               "provider_raw_version": 1,
               "min_event_date": "2012-10-01T07:34:01.126892",
               "created": "2012-11-29T07:34:01.126892"
            }    

        self.old_doc = {
                   "_id": "pmc201304",
                   "_rev": "1-c9a62778077941736932cfb3510ee382",
                   "raw": "<pmc-web-stat><request year=\"2013\" month=\"04\" jrid=\"elife\" eissn=\"2050-084X\"></request><response status=\"0\" collection=\"eLife\"></response><articles><article id=\"PMC3463246\"><meta-data doi=\"10.7554/eLife.00013\" pmcid=\"PMC3463246\" pubmed-id=\"23066504\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00013\"/><usage unique-ip=\"112\" full-text=\"152\" pdf=\"22\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"31\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3463247\"><meta-data doi=\"10.7554/eLife.00240\" pmcid=\"PMC3463247\" pubmed-id=\"23066507\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00240\"/><usage unique-ip=\"10\" full-text=\"41\" pdf=\"1\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3465569\"><meta-data doi=\"10.7554/eLife.00242\" pmcid=\"PMC3465569\" pubmed-id=\"23066508\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00242\"/><usage unique-ip=\"27\" full-text=\"26\" pdf=\"3\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3465570\"><meta-data doi=\"10.7554/eLife.00243\" pmcid=\"PMC3465570\" pubmed-id=\"23066509\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00243\"/><usage unique-ip=\"41\" full-text=\"50\" pdf=\"9\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"8\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3466591\"><meta-data doi=\"10.7554/eLife.00065\" pmcid=\"PMC3466591\" pubmed-id=\"23066506\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00065\"/><usage unique-ip=\"252\" full-text=\"2522\" pdf=\"41\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"72\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3466783\"><meta-data doi=\"10.7554/eLife.00007\" pmcid=\"PMC3466783\" pubmed-id=\"23066503\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00007\"/><usage unique-ip=\"47\" full-text=\"62\" pdf=\"5\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"10\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3467772\"><meta-data doi=\"10.7554/eLife.00270\" pmcid=\"PMC3467772\" pubmed-id=\"23066510\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00270\"/><usage unique-ip=\"28\" full-text=\"32\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3470409\"><meta-data doi=\"10.7554/eLife.00048\" pmcid=\"PMC3470409\" pubmed-id=\"23066505\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00048\"/><usage unique-ip=\"72\" full-text=\"68\" pdf=\"20\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"26\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3470722\"><meta-data doi=\"10.7554/eLife.00286\" pmcid=\"PMC3470722\" pubmed-id=\"23071903\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00286\"/><usage unique-ip=\"37\" full-text=\"40\" pdf=\"7\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3479833\"><meta-data doi=\"10.7554/eLife.00031\" pmcid=\"PMC3479833\" pubmed-id=\"23110253\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00031\"/><usage unique-ip=\"31\" full-text=\"41\" pdf=\"3\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"7\" supp-data=\"0\" cited-by=\"1\"/></article><article id=\"PMC3482686\"><meta-data doi=\"10.7554/eLife.00005\" pmcid=\"PMC3482686\" pubmed-id=\"23110252\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00005\"/><usage unique-ip=\"189\" full-text=\"238\" pdf=\"33\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"110\" supp-data=\"2\" cited-by=\"0\"/></article><article id=\"PMC3482687\"><meta-data doi=\"10.7554/eLife.00281\" pmcid=\"PMC3482687\" pubmed-id=\"23110255\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00281\"/><usage unique-ip=\"17\" full-text=\"20\" pdf=\"0\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3482692\"><meta-data doi=\"10.7554/eLife.00102\" pmcid=\"PMC3482692\" pubmed-id=\"23110254\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00102\"/><usage unique-ip=\"50\" full-text=\"55\" pdf=\"11\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"7\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3485613\"><meta-data doi=\"10.7554/eLife.00301\" pmcid=\"PMC3485613\" pubmed-id=\"23150799\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00301\"/><usage unique-ip=\"41\" full-text=\"43\" pdf=\"7\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"7\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3485615\"><meta-data doi=\"10.7554/eLife.00049\" pmcid=\"PMC3485615\" pubmed-id=\"23150796\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00049\"/><usage unique-ip=\"390\" full-text=\"558\" pdf=\"108\" abstract=\"11\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"132\" supp-data=\"0\" cited-by=\"4\"/></article><article id=\"PMC3490148\"><meta-data doi=\"10.7554/eLife.00302\" pmcid=\"PMC3490148\" pubmed-id=\"23150800\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00302\"/><usage unique-ip=\"47\" full-text=\"54\" pdf=\"4\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"8\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3490149\"><meta-data doi=\"10.7554/eLife.00068\" pmcid=\"PMC3490149\" pubmed-id=\"23150797\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00068\"/><usage unique-ip=\"42\" full-text=\"53\" pdf=\"12\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"9\" supp-data=\"1\" cited-by=\"0\"/></article><article id=\"PMC3491588\"><meta-data doi=\"10.7554/eLife.00003\" pmcid=\"PMC3491588\" pubmed-id=\"23150794\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00003\"/><usage unique-ip=\"62\" full-text=\"58\" pdf=\"20\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"13\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3492862\"><meta-data doi=\"10.7554/eLife.00011\" pmcid=\"PMC3492862\" pubmed-id=\"23150795\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00011\"/><usage unique-ip=\"68\" full-text=\"70\" pdf=\"9\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"18\" supp-data=\"6\" cited-by=\"0\"/></article><article id=\"PMC3494066\"><meta-data doi=\"10.7554/eLife.00173\" pmcid=\"PMC3494066\" pubmed-id=\"23150798\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00173\"/><usage unique-ip=\"39\" full-text=\"47\" pdf=\"3\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"12\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510452\"><meta-data doi=\"10.7554/eLife.00051\" pmcid=\"PMC3510452\" pubmed-id=\"23240081\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00051\"/><usage unique-ip=\"16\" full-text=\"16\" pdf=\"5\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510453\"><meta-data doi=\"10.7554/eLife.00078\" pmcid=\"PMC3510453\" pubmed-id=\"23240084\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00078\"/><usage unique-ip=\"33\" full-text=\"33\" pdf=\"8\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"1\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510454\"><meta-data doi=\"10.7554/eLife.00171\" pmcid=\"PMC3510454\" pubmed-id=\"23240086\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00171\"/><usage unique-ip=\"28\" full-text=\"34\" pdf=\"4\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"14\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510455\"><meta-data doi=\"10.7554/eLife.00067\" pmcid=\"PMC3510455\" pubmed-id=\"23240082\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00067\"/><usage unique-ip=\"31\" full-text=\"34\" pdf=\"7\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"10\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510456\"><meta-data doi=\"10.7554/eLife.00070\" pmcid=\"PMC3510456\" pubmed-id=\"23240083\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00070\"/><usage unique-ip=\"37\" full-text=\"34\" pdf=\"4\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"20\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510471\"><meta-data doi=\"10.7554/eLife.00365\" pmcid=\"PMC3510471\" pubmed-id=\"23240092\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00365\"/><usage unique-ip=\"13\" full-text=\"14\" pdf=\"1\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510472\"><meta-data doi=\"10.7554/eLife.00340\" pmcid=\"PMC3510472\" pubmed-id=\"23240089\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00340\"/><usage unique-ip=\"8\" full-text=\"10\" pdf=\"0\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510473\"><meta-data doi=\"10.7554/eLife.00351\" pmcid=\"PMC3510473\" pubmed-id=\"0\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00351\"/><usage unique-ip=\"8\" full-text=\"9\" pdf=\"1\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510474\"><meta-data doi=\"10.7554/eLife.00184\" pmcid=\"PMC3510474\" pubmed-id=\"23240087\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00184\"/><usage unique-ip=\"38\" full-text=\"41\" pdf=\"8\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"4\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510475\"><meta-data doi=\"10.7554/eLife.00347\" pmcid=\"PMC3510475\" pubmed-id=\"23240090\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00347\"/><usage unique-ip=\"17\" full-text=\"18\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"2\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510476\"><meta-data doi=\"10.7554/eLife.00353\" pmcid=\"PMC3510476\" pubmed-id=\"23240091\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00353\"/><usage unique-ip=\"22\" full-text=\"36\" pdf=\"4\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"4\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3510477\"><meta-data doi=\"10.7554/eLife.00326\" pmcid=\"PMC3510477\" pubmed-id=\"23240088\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00326\"/><usage unique-ip=\"14\" full-text=\"10\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"6\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3514886\"><meta-data doi=\"10.7554/eLife.00109\" pmcid=\"PMC3514886\" pubmed-id=\"23240085\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00109\"/><usage unique-ip=\"34\" full-text=\"31\" pdf=\"11\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"15\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3524649\"><meta-data doi=\"10.7554/eLife.00090\" pmcid=\"PMC3524649\" pubmed-id=\"23256041\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00090\"/><usage unique-ip=\"34\" full-text=\"37\" pdf=\"6\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"9\" supp-data=\"0\" cited-by=\"1\"/></article><article id=\"PMC3524793\"><meta-data doi=\"10.7554/eLife.00352\" pmcid=\"PMC3524793\" pubmed-id=\"23256044\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00352\"/><usage unique-ip=\"15\" full-text=\"18\" pdf=\"0\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"2\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3524794\"><meta-data doi=\"10.7554/eLife.00093\" pmcid=\"PMC3524794\" pubmed-id=\"23251784\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00093\"/><usage unique-ip=\"19\" full-text=\"26\" pdf=\"1\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"1\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3524795\"><meta-data doi=\"10.7554/eLife.00311\" pmcid=\"PMC3524795\" pubmed-id=\"23251785\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00311\"/><usage unique-ip=\"33\" full-text=\"38\" pdf=\"11\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"9\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3524796\"><meta-data doi=\"10.7554/eLife.00181\" pmcid=\"PMC3524796\" pubmed-id=\"23256042\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00181\"/><usage unique-ip=\"25\" full-text=\"24\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"12\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3524800\"><meta-data doi=\"10.7554/eLife.00386\" pmcid=\"PMC3524800\" pubmed-id=\"23256046\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00386\"/><usage unique-ip=\"15\" full-text=\"17\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"4\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3524801\"><meta-data doi=\"10.7554/eLife.00047\" pmcid=\"PMC3524801\" pubmed-id=\"23251783\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00047\"/><usage unique-ip=\"47\" full-text=\"51\" pdf=\"16\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"18\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3524826\"><meta-data doi=\"10.7554/eLife.00387\" pmcid=\"PMC3524826\" pubmed-id=\"23256047\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00387\"/><usage unique-ip=\"52\" full-text=\"60\" pdf=\"6\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"12\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3524827\"><meta-data doi=\"10.7554/eLife.00385\" pmcid=\"PMC3524827\" pubmed-id=\"23256045\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00385\"/><usage unique-ip=\"13\" full-text=\"16\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"2\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3524939\"><meta-data doi=\"10.7554/eLife.00205\" pmcid=\"PMC3524939\" pubmed-id=\"23256043\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00205\"/><usage unique-ip=\"63\" full-text=\"75\" pdf=\"19\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"38\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3525924\"><meta-data doi=\"10.7554/eLife.00117\" pmcid=\"PMC3525924\" pubmed-id=\"23275833\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00117\"/><usage unique-ip=\"44\" full-text=\"46\" pdf=\"5\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"22\" supp-data=\"1\" cited-by=\"0\"/></article><article id=\"PMC3533262\"><meta-data doi=\"10.7554/eLife.00475\" pmcid=\"PMC3533262\" pubmed-id=\"23326638\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00475\"/><usage unique-ip=\"17\" full-text=\"19\" pdf=\"1\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3534202\"><meta-data doi=\"10.7554/eLife.00248\" pmcid=\"PMC3534202\" pubmed-id=\"23330067\" pub-year=\"2012\" volume=\"1\" issue=\"\" first-page=\"e00248\"/><usage unique-ip=\"34\" full-text=\"40\" pdf=\"9\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"12\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3539330\"><meta-data doi=\"10.7554/eLife.00450\" pmcid=\"PMC3539330\" pubmed-id=\"23326643\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00450\"/><usage unique-ip=\"19\" full-text=\"20\" pdf=\"1\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"1\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3539331\"><meta-data doi=\"10.7554/eLife.00452\" pmcid=\"PMC3539331\" pubmed-id=\"0\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00452\"/><usage unique-ip=\"11\" full-text=\"14\" pdf=\"0\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"1\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3539332\"><meta-data doi=\"10.7554/eLife.00160\" pmcid=\"PMC3539332\" pubmed-id=\"23326640\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00160\"/><usage unique-ip=\"45\" full-text=\"46\" pdf=\"11\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"17\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3539393\"><meta-data doi=\"10.7554/eLife.00170\" pmcid=\"PMC3539393\" pubmed-id=\"23326641\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00170\"/><usage unique-ip=\"67\" full-text=\"80\" pdf=\"12\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"36\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3545443\"><meta-data doi=\"10.7554/eLife.00231\" pmcid=\"PMC3545443\" pubmed-id=\"23326642\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00231\"/><usage unique-ip=\"43\" full-text=\"50\" pdf=\"8\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"22\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3545444\"><meta-data doi=\"10.7554/eLife.00116\" pmcid=\"PMC3545444\" pubmed-id=\"23326639\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00116\"/><usage unique-ip=\"68\" full-text=\"85\" pdf=\"14\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"16\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3552422\"><meta-data doi=\"10.7554/eLife.00012\" pmcid=\"PMC3552422\" pubmed-id=\"23359858\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00012\"/><usage unique-ip=\"48\" full-text=\"44\" pdf=\"14\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"8\" supp-data=\"0\" cited-by=\"1\"/></article><article id=\"PMC3552423\"><meta-data doi=\"10.7554/eLife.00308\" pmcid=\"PMC3552423\" pubmed-id=\"23358411\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00308\"/><usage unique-ip=\"71\" full-text=\"95\" pdf=\"17\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"23\" supp-data=\"3\" cited-by=\"0\"/></article><article id=\"PMC3552424\"><meta-data doi=\"10.7554/eLife.00178\" pmcid=\"PMC3552424\" pubmed-id=\"23359859\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00178\"/><usage unique-ip=\"64\" full-text=\"80\" pdf=\"16\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"51\" supp-data=\"11\" cited-by=\"1\"/></article><article id=\"PMC3552425\"><meta-data doi=\"10.7554/eLife.00476\" pmcid=\"PMC3552425\" pubmed-id=\"23358458\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00476\"/><usage unique-ip=\"24\" full-text=\"25\" pdf=\"1\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"2\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3552426\"><meta-data doi=\"10.7554/eLife.00477\" pmcid=\"PMC3552426\" pubmed-id=\"23359861\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00477\"/><usage unique-ip=\"22\" full-text=\"26\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"8\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3552427\"><meta-data doi=\"10.7554/eLife.00491\" pmcid=\"PMC3552427\" pubmed-id=\"23359862\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00491\"/><usage unique-ip=\"28\" full-text=\"32\" pdf=\"5\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"1\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3552618\"><meta-data doi=\"10.7554/eLife.00183\" pmcid=\"PMC3552618\" pubmed-id=\"23358702\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00183\"/><usage unique-ip=\"34\" full-text=\"39\" pdf=\"4\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"4\" supp-data=\"73\" cited-by=\"0\"/></article><article id=\"PMC3552619\"><meta-data doi=\"10.7554/eLife.00230\" pmcid=\"PMC3552619\" pubmed-id=\"23359860\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00230\"/><usage unique-ip=\"34\" full-text=\"36\" pdf=\"4\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"2\" supp-data=\"14\" cited-by=\"0\"/></article><article id=\"PMC3557904\"><meta-data doi=\"10.7554/eLife.00563\" pmcid=\"PMC3557904\" pubmed-id=\"23386979\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00563\"/><usage unique-ip=\"51\" full-text=\"60\" pdf=\"19\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"10\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3557905\"><meta-data doi=\"10.7554/eLife.00471\" pmcid=\"PMC3557905\" pubmed-id=\"23386978\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00471\"/><usage unique-ip=\"267\" full-text=\"265\" pdf=\"107\" abstract=\"3\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"111\" supp-data=\"13\" cited-by=\"2\"/></article><article id=\"PMC3564445\"><meta-data doi=\"10.7554/eLife.00515\" pmcid=\"PMC3564445\" pubmed-id=\"23390590\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00515\"/><usage unique-ip=\"22\" full-text=\"25\" pdf=\"0\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"4\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3564446\"><meta-data doi=\"10.7554/eLife.00306\" pmcid=\"PMC3564446\" pubmed-id=\"23390587\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00306\"/><usage unique-ip=\"41\" full-text=\"52\" pdf=\"13\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"11\" supp-data=\"1\" cited-by=\"0\"/></article><article id=\"PMC3564447\"><meta-data doi=\"10.7554/eLife.00329\" pmcid=\"PMC3564447\" pubmed-id=\"23390589\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00329\"/><usage unique-ip=\"57\" full-text=\"83\" pdf=\"13\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"24\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3564448\"><meta-data doi=\"10.7554/eLife.00321\" pmcid=\"PMC3564448\" pubmed-id=\"23390588\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00321\"/><usage unique-ip=\"16\" full-text=\"19\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"1\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3564474\"><meta-data doi=\"10.7554/eLife.00105\" pmcid=\"PMC3564474\" pubmed-id=\"23390586\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00105\"/><usage unique-ip=\"31\" full-text=\"34\" pdf=\"4\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"8\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3569938\"><meta-data doi=\"10.7554/eLife.00565\" pmcid=\"PMC3569938\" pubmed-id=\"23408481\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00565\"/><usage unique-ip=\"12\" full-text=\"14\" pdf=\"0\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3576708\"><meta-data doi=\"10.7554/eLife.00573\" pmcid=\"PMC3576708\" pubmed-id=\"23426864\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00573\"/><usage unique-ip=\"31\" full-text=\"33\" pdf=\"8\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"6\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3576709\"><meta-data doi=\"10.7554/eLife.00571\" pmcid=\"PMC3576709\" pubmed-id=\"23426887\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00571\"/><usage unique-ip=\"21\" full-text=\"23\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"2\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3576710\"><meta-data doi=\"10.7554/eLife.00572\" pmcid=\"PMC3576710\" pubmed-id=\"23426937\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00572\"/><usage unique-ip=\"39\" full-text=\"35\" pdf=\"11\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"12\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3576711\"><meta-data doi=\"10.7554/eLife.00291\" pmcid=\"PMC3576711\" pubmed-id=\"23426999\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00291\"/><usage unique-ip=\"112\" full-text=\"133\" pdf=\"50\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"60\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3576727\"><meta-data doi=\"10.7554/eLife.00461\" pmcid=\"PMC3576727\" pubmed-id=\"23427024\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00461\"/><usage unique-ip=\"76\" full-text=\"79\" pdf=\"19\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"43\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3576809\"><meta-data doi=\"10.7554/eLife.00290\" pmcid=\"PMC3576809\" pubmed-id=\"23425906\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00290\"/><usage unique-ip=\"46\" full-text=\"58\" pdf=\"4\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"27\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3578201\"><meta-data doi=\"10.7554/eLife.00333\" pmcid=\"PMC3578201\" pubmed-id=\"23550179\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00333\"/><usage unique-ip=\"78\" full-text=\"100\" pdf=\"22\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"39\" supp-data=\"1\" cited-by=\"0\"/></article><article id=\"PMC3579228\"><meta-data doi=\"10.7554/eLife.00488\" pmcid=\"PMC3579228\" pubmed-id=\"0\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00488\"/><usage unique-ip=\"12\" full-text=\"15\" pdf=\"0\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3582987\"><meta-data doi=\"10.7554/eLife.00593\" pmcid=\"PMC3582987\" pubmed-id=\"23467495\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00593\"/><usage unique-ip=\"26\" full-text=\"29\" pdf=\"7\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3582988\"><meta-data doi=\"10.7554/eLife.00400\" pmcid=\"PMC3582988\" pubmed-id=\"23467508\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00400\"/><usage unique-ip=\"53\" full-text=\"55\" pdf=\"14\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"23\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3583005\"><meta-data doi=\"10.7554/eLife.00348\" pmcid=\"PMC3583005\" pubmed-id=\"23467541\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00348\"/><usage unique-ip=\"96\" full-text=\"105\" pdf=\"22\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"27\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3589823\"><meta-data doi=\"10.7554/eLife.00615\" pmcid=\"PMC3589823\" pubmed-id=\"23470921\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00615\"/><usage unique-ip=\"22\" full-text=\"26\" pdf=\"2\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3589824\"><meta-data doi=\"10.7554/eLife.00337\" pmcid=\"PMC3589824\" pubmed-id=\"23471010\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00337\"/><usage unique-ip=\"46\" full-text=\"46\" pdf=\"11\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"5\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3589825\"><meta-data doi=\"10.7554/eLife.00327\" pmcid=\"PMC3589825\" pubmed-id=\"23471103\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00327\"/><usage unique-ip=\"59\" full-text=\"61\" pdf=\"15\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"25\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3591006\"><meta-data doi=\"10.7554/eLife.00133\" pmcid=\"PMC3591006\" pubmed-id=\"23482306\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00133\"/><usage unique-ip=\"68\" full-text=\"72\" pdf=\"22\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"32\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3591093\"><meta-data doi=\"10.7554/eLife.00218\" pmcid=\"PMC3591093\" pubmed-id=\"23483797\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00218\"/><usage unique-ip=\"48\" full-text=\"46\" pdf=\"11\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"23\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3591783\"><meta-data doi=\"10.7554/eLife.00190\" pmcid=\"PMC3591783\" pubmed-id=\"23482940\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00190\"/><usage unique-ip=\"42\" full-text=\"43\" pdf=\"8\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"8\" supp-data=\"2\" cited-by=\"0\"/></article><article id=\"PMC3592195\"><meta-data doi=\"10.7554/eLife.00577\" pmcid=\"PMC3592195\" pubmed-id=\"23538671\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00577\"/><usage unique-ip=\"46\" full-text=\"47\" pdf=\"7\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"10\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3594797\"><meta-data doi=\"10.7554/eLife.00646\" pmcid=\"PMC3594797\" pubmed-id=\"23538735\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00646\"/><usage unique-ip=\"18\" full-text=\"21\" pdf=\"0\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3601633\"><meta-data doi=\"10.7554/eLife.00638\" pmcid=\"PMC3601633\" pubmed-id=\"23538852\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00638\"/><usage unique-ip=\"39\" full-text=\"41\" pdf=\"7\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"19\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3601634\"><meta-data doi=\"10.7554/eLife.00641\" pmcid=\"PMC3601634\" pubmed-id=\"23539117\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00641\"/><usage unique-ip=\"62\" full-text=\"65\" pdf=\"14\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"16\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3601810\"><meta-data doi=\"10.7554/eLife.00336\" pmcid=\"PMC3601810\" pubmed-id=\"23539289\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00336\"/><usage unique-ip=\"98\" full-text=\"108\" pdf=\"24\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"56\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3601818\"><meta-data doi=\"10.7554/eLife.00378\" pmcid=\"PMC3601818\" pubmed-id=\"23539368\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00378\"/><usage unique-ip=\"78\" full-text=\"89\" pdf=\"21\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"18\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3601819\"><meta-data doi=\"10.7554/eLife.00354\" pmcid=\"PMC3601819\" pubmed-id=\"23539454\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00354\"/><usage unique-ip=\"103\" full-text=\"140\" pdf=\"27\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"36\" supp-data=\"4\" cited-by=\"0\"/></article><article id=\"PMC3602953\"><meta-data doi=\"10.7554/eLife.00605\" pmcid=\"PMC3602953\" pubmed-id=\"23538878\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00605\"/><usage unique-ip=\"65\" full-text=\"76\" pdf=\"17\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"18\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3602954\"><meta-data doi=\"10.7554/eLife.00312\" pmcid=\"PMC3602954\" pubmed-id=\"23538967\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00312\"/><usage unique-ip=\"158\" full-text=\"165\" pdf=\"58\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"70\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3608243\"><meta-data doi=\"10.7554/eLife.00648\" pmcid=\"PMC3608243\" pubmed-id=\"23539513\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00648\"/><usage unique-ip=\"71\" full-text=\"69\" pdf=\"16\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"20\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3608244\"><meta-data doi=\"10.7554/eLife.00642\" pmcid=\"PMC3608244\" pubmed-id=\"23539544\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00642\"/><usage unique-ip=\"40\" full-text=\"49\" pdf=\"7\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"0\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3608245\"><meta-data doi=\"10.7554/eLife.00625\" pmcid=\"PMC3608245\" pubmed-id=\"23539644\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00625\"/><usage unique-ip=\"46\" full-text=\"51\" pdf=\"16\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"7\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3608266\"><meta-data doi=\"10.7554/eLife.00260\" pmcid=\"PMC3608266\" pubmed-id=\"23538384\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00260\"/><usage unique-ip=\"103\" full-text=\"122\" pdf=\"45\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"39\" supp-data=\"6\" cited-by=\"0\"/></article><article id=\"PMC3610343\"><meta-data doi=\"10.7554/eLife.00269\" pmcid=\"PMC3610343\" pubmed-id=\"23543845\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00269\"/><usage unique-ip=\"57\" full-text=\"63\" pdf=\"10\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"9\" supp-data=\"2\" cited-by=\"0\"/></article><article id=\"PMC3614016\"><meta-data doi=\"10.7554/eLife.00278\" pmcid=\"PMC3614016\" pubmed-id=\"23577232\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00278\"/><usage unique-ip=\"39\" full-text=\"42\" pdf=\"7\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"16\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3614024\"><meta-data doi=\"10.7554/eLife.00655\" pmcid=\"PMC3614024\" pubmed-id=\"23577236\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00655\"/><usage unique-ip=\"27\" full-text=\"31\" pdf=\"5\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"2\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3614025\"><meta-data doi=\"10.7554/eLife.00435\" pmcid=\"PMC3614025\" pubmed-id=\"23577234\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00435\"/><usage unique-ip=\"100\" full-text=\"104\" pdf=\"32\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"62\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3614033\"><meta-data doi=\"10.7554/eLife.00367\" pmcid=\"PMC3614033\" pubmed-id=\"23577233\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00367\"/><usage unique-ip=\"49\" full-text=\"64\" pdf=\"15\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"9\" supp-data=\"8\" cited-by=\"0\"/></article><article id=\"PMC3614058\"><meta-data doi=\"10.7554/eLife.00639\" pmcid=\"PMC3614058\" pubmed-id=\"23577235\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00639\"/><usage unique-ip=\"46\" full-text=\"64\" pdf=\"8\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"2\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3622176\"><meta-data doi=\"10.7554/eLife.00692\" pmcid=\"PMC3622176\" pubmed-id=\"23580166\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00692\"/><usage unique-ip=\"27\" full-text=\"33\" pdf=\"5\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"5\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3622177\"><meta-data doi=\"10.7554/eLife.00444\" pmcid=\"PMC3622177\" pubmed-id=\"23580231\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00444\"/><usage unique-ip=\"60\" full-text=\"69\" pdf=\"22\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"26\" supp-data=\"4\" cited-by=\"0\"/></article><article id=\"PMC3622178\"><meta-data doi=\"10.7554/eLife.00426\" pmcid=\"PMC3622178\" pubmed-id=\"23580255\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00426\"/><usage unique-ip=\"63\" full-text=\"88\" pdf=\"18\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"29\" supp-data=\"5\" cited-by=\"0\"/></article><article id=\"PMC3622181\"><meta-data doi=\"10.7554/eLife.00499\" pmcid=\"PMC3622181\" pubmed-id=\"23580326\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00499\"/><usage unique-ip=\"80\" full-text=\"111\" pdf=\"37\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"22\" supp-data=\"0\" cited-by=\"1\"/></article><article id=\"PMC3622228\"><meta-data doi=\"10.7554/eLife.00659\" pmcid=\"PMC3622228\" pubmed-id=\"23580350\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00659\"/><usage unique-ip=\"37\" full-text=\"37\" pdf=\"10\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"11\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3622229\"><meta-data doi=\"10.7554/eLife.00663\" pmcid=\"PMC3622229\" pubmed-id=\"23580362\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00663\"/><usage unique-ip=\"46\" full-text=\"51\" pdf=\"15\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"11\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3628084\"><meta-data doi=\"10.7554/eLife.00415\" pmcid=\"PMC3628084\" pubmed-id=\"23599892\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00415\"/><usage unique-ip=\"40\" full-text=\"40\" pdf=\"10\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"14\" supp-data=\"0\" cited-by=\"1\"/></article><article id=\"PMC3628085\"><meta-data doi=\"10.7554/eLife.00458\" pmcid=\"PMC3628085\" pubmed-id=\"23599893\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00458\"/><usage unique-ip=\"120\" full-text=\"124\" pdf=\"36\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"43\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3628086\"><meta-data doi=\"10.7554/eLife.00362\" pmcid=\"PMC3628086\" pubmed-id=\"23599891\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00362\"/><usage unique-ip=\"91\" full-text=\"120\" pdf=\"24\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"64\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3628087\"><meta-data doi=\"10.7554/eLife.00534\" pmcid=\"PMC3628087\" pubmed-id=\"23599896\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00534\"/><usage unique-ip=\"72\" full-text=\"72\" pdf=\"16\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"19\" supp-data=\"1\" cited-by=\"0\"/></article><article id=\"PMC3628110\"><meta-data doi=\"10.7554/eLife.00482\" pmcid=\"PMC3628110\" pubmed-id=\"23599895\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00482\"/><usage unique-ip=\"44\" full-text=\"51\" pdf=\"1\" abstract=\"1\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"21\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3628404\"><meta-data doi=\"10.7554/eLife.00729\" pmcid=\"PMC3628404\" pubmed-id=\"23599898\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00729\"/><usage unique-ip=\"62\" full-text=\"100\" pdf=\"12\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"8\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3628405\"><meta-data doi=\"10.7554/eLife.00459\" pmcid=\"PMC3628405\" pubmed-id=\"23599894\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00459\"/><usage unique-ip=\"39\" full-text=\"42\" pdf=\"9\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"45\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3628440\"><meta-data doi=\"10.7554/eLife.00676\" pmcid=\"PMC3628440\" pubmed-id=\"23599897\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00676\"/><usage unique-ip=\"18\" full-text=\"23\" pdf=\"3\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"2\" supp-data=\"0\" cited-by=\"0\"/></article><article id=\"PMC3629793\"><meta-data doi=\"10.7554/eLife.00288\" pmcid=\"PMC3629793\" pubmed-id=\"23606943\" pub-year=\"2013\" volume=\"2\" issue=\"\" first-page=\"e00288\"/><usage unique-ip=\"35\" full-text=\"41\" pdf=\"4\" abstract=\"0\" scanned-summary=\"0\" scanned-page-browse=\"0\" figure=\"11\" supp-data=\"3\" cited-by=\"0\"/></article></articles></pmc-web-stat>",
                   "provider_raw_version": 1,
                   "max_event_date": "2013-04-30T23:59:59.999999",
                   "provider": "pmc",
                   "aliases": {
                       "pmid": [
                           "23066504",
                           "23066507",
                           "23066508",
                           "23066509",
                           "23066506",
                           "23066503"
                       ]
                   },
                   "type": "provider_data_dump",
                   "min_event_date": "2013-04-01T00:00:00",
                   "created": "2013-05-16T07:38:06.831119"
                }        

    def tearDown(self):
        teardown_postgres_for_unittests(self.db)


    def test_make_provider_batch_data(self):
        #make sure nothing there beforehand
        matching = ProviderBatchData.query.filter_by(provider="pmc").first()
        assert_equals(matching, None)

        new_batch_data = ProviderBatchData(**self.test_data)
        print new_batch_data

        # still not there
        matching = ProviderBatchData.query.filter_by(provider="pmc").first()
        assert_equals(matching, None)

        self.db.session.add(new_batch_data)
        self.db.session.commit()

        # and now poof there it is
        matching = ProviderBatchData.query.filter_by(provider="pmc").first()
        assert_equals(matching.provider, "pmc")

        assert_equals(matching.aliases, self.test_data["aliases"])

    def test_create_object_from_doc(self):
        new_object = provider_batch_data.create_objects_from_doc(self.old_doc)

        matching = ProviderBatchData.query.filter_by(provider="pmc").first()
        assert_equals(matching.provider, "pmc")

        assert_equals(matching.aliases, self.old_doc["aliases"])


########NEW FILE########
__FILENAME__ = test_tiredis
from nose.tools import raises, assert_equals, assert_items_equal, nottest
import redis
import json

from totalimpact import tiredis

SAMPLE_COLLECTION = {
    "_id": "kn5auf",
    "_rev": "8-69fdd2a34464f0ca9d02748ef057b1e8",
    "created": "2012-06-25T09:21:12.673503",
    "items": [] 
    }

class TestTiRedis():

    def setUp(self):
        # we're putting unittests for redis in their own db (number 8) so they can be deleted with abandon
        self.r = tiredis.from_url("redis://localhost:6379", db=8)
        self.r.flushdb()

    def test_from_url(self):
        self.r.set("test", "foo")
        assert_equals(self.r.get("test"), "foo")

    def test_init_currently_updating_status(self):
        self.r.init_currently_updating_status(["abcd", "efgh"], ["topsy", "wikipedia"])
        assert_items_equal(self.r.get_providers_currently_updating("abcd"), ["topsy", "wikipedia"])
        assert_items_equal(self.r.get_providers_currently_updating("efgh"), ["topsy", "wikipedia"])
        assert_equals(self.r.get_currently_updating("abcd", "wikipedia").values(), ["in queue"])

    def test_set_provider_started(self):
        self.r.init_currently_updating_status(["abcd"], ["topsy", "wikipedia"])
        self.r.set_provider_started("abcd", "wikipedia")
        assert_equals("queue" in self.r.get_currently_updating("abcd", "wikipedia"), False)
        assert_equals(self.r.get_currently_updating("abcd", "wikipedia").values(), ["started"])

    def test_set_provider_finished(self):
        assert_equals(self.r.get_num_providers_currently_updating("abcd"), 0)        
        self.r.init_currently_updating_status(["abcd"], ["topsy", "wikipedia"])
        assert_equals(self.r.get_num_providers_currently_updating("abcd"), 2)        
        self.r.set_provider_finished("abcd", "wikipedia")
        assert_equals(self.r.get_num_providers_currently_updating("abcd"), 1)
        print self.r.get_currently_updating("abcd", "topsy")
        assert_equals(self.r.get_currently_updating("abcd", "topsy").values(), ["in queue"])
        self.r.set_provider_finished("abcd", "topsy")
        assert_equals(self.r.get_num_providers_currently_updating("abcd"), 0)

    def test_add_to_alias_queue(self):
        self.r.add_to_alias_queue( [{"tiid":"abcd", "aliases_dict":{"doi":["10.123"]}}, 
                                    {"tiid":"efgh", "aliases_dict":{"doi":["10.456"]}}] )
        response = self.r.lpop("aliasqueue_high")
        expected = '{"tiid": "efgh", "alias_providers_already_run": [], "analytics_credentials": {}, "aliases_dict": {"doi": ["10.456"]}}'
        assert_equals(response, expected)
        response = self.r.lpop("aliasqueue_high")
        expected = '{"tiid": "abcd", "alias_providers_already_run": [], "analytics_credentials": {}, "aliases_dict": {"doi": ["10.123"]}}'
        assert_equals(response, expected)
        response = self.r.lpop("aliasqueue_high")
        expected = None
        assert_equals(response, expected)

    def test_memberitems_status(self):
        self.r.set_memberitems_status("abcd", 11)
        response = self.r.get_memberitems_status("abcd")
        assert_equals(response, 11)

    def test_confidence_interval_table(self):
        self.r.set_confidence_interval_table(100, 0.95, {"hi":"there"})
        response = self.r.get_confidence_interval_table(100, 0.95)
        assert_equals(response, {"hi":"there"})

    def test_reference_histogram_dict(self):
        self.r.set_reference_histogram_dict("article", "WoS", 2010, {"hi":"hist"})
        response = self.r.get_reference_histogram_dict("article", "WoS", 2010)
        assert_equals(response, {"hi":"hist"})

    def test_lookup_histogram_dict(self):
        self.r.set_reference_lookup_dict("article", "WoS", 2010, {"hi":"lookup"})
        response = self.r.get_reference_lookup_dict("article", "WoS", 2010)
        assert_equals(response, {"hi":"lookup"})



########NEW FILE########
__FILENAME__ = test_unicode_helpers
from nose.tools import raises, assert_equals, nottest

from totalimpact import unicode_helpers



class TestUnicodeHelpers():

    def setUp(self):
        pass

    def test_remove_nonprinting_characters(self):
        unicode_input = u"hi"
        response = unicode_helpers.remove_nonprinting_characters(unicode_input)
        expected = u"hi"
        assert_equals(response, expected)

    def test_remove_nonprinting_characters(self):
        unicode_input = '0000-0001-8907-4150\xe2\x80\x8e' # a nonprinting character at the end
        response = unicode_helpers.remove_nonprinting_characters(unicode_input)
        expected = "0000-0001-8907-4150"
        assert_equals(response, expected)

    def test_remove_nonprinting_characters_unicode_input(self):
        unicode_input = u'0000-0001-8907-4150\u200e'  # a nonprinting character at the end
        response = unicode_helpers.remove_nonprinting_characters(unicode_input)
        expected = u"0000-0001-8907-4150"
        assert_equals(response, expected)

########NEW FILE########
__FILENAME__ = test_updater
import json, os, Queue, datetime, copy

from totalimpact import app, db
from totalimpact import tiredis
from totalimpact import updater
from totalimpact import item as item_module
from nose.tools import raises, assert_items_equal, assert_equals, assert_greater, nottest
from test.utils import slow
from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests


class TestUpdater():
    def setUp(self):

        self.db = setup_postgres_for_unittests(db, app)

        # do the same thing for the redis db, set up the test redis database.  We're using DB Number 8
        self.r = tiredis.from_url("redis://localhost:6379", db=8)
        self.r.flushdb()
        now = datetime.datetime.utcnow()
        self.before = now - datetime.timedelta(days=2)
        self.last_week = now - datetime.timedelta(days=7)
        self.last_year = now - datetime.timedelta(days=370)

        # save basic item beforehand, and some additional items
        self.fake_item_doc = {
            "_id": "tiid1",
            "type": "item",
            "last_modified": now.isoformat(),
            "last_update_run": now.isoformat(),
            "aliases":{"doi":["10.7554/elife.1"]},
            "biblio": {"year":"2012"},
            "metrics": {}
        }
        self.fake_item_obj = item_module.create_objects_from_item_doc(self.fake_item_doc)        
        self.db.session.add(self.fake_item_obj)

        another_elife = copy.copy(self.fake_item_doc)
        another_elife["_id"] = "tiid2"
        another_elife["aliases"] = {"doi":["10.7554/ELIFE.2"]}
        another_elife["last_modified"] = self.before.isoformat()
        another_elife["last_update_run"] = self.before.isoformat()
        another_elife_obj = item_module.create_objects_from_item_doc(another_elife)        
        self.db.session.add(another_elife_obj)

        different_journal = copy.copy(self.fake_item_doc)
        different_journal["_id"] = "tiid3"
        different_journal["aliases"] = {"doi":["10.3897/zookeys.3"], "biblio":[{"year":1999}]}
        different_journal["last_modified"] = now.isoformat()
        different_journal["last_update_run"] = self.last_week.isoformat()
        different_journal_obj = item_module.create_objects_from_item_doc(different_journal)        
        self.db.session.add(different_journal_obj)

        different_journal2 = copy.copy(different_journal)
        different_journal2["_id"] = "tiid4"
        different_journal2["last_update_run"] = self.last_year.isoformat()
        different_journal_obj2 = item_module.create_objects_from_item_doc(different_journal2)        
        self.db.session.add(different_journal_obj2)

        self.db.session.commit()


    def tearDown(self):
        teardown_postgres_for_unittests(self.db)

    def test_get_tiids_not_updated_since(self):
        number_to_update = 10
        schedule = {"max_days_since_created": 7, "max_days_since_updated": 1, "exclude_old": True}      
        tiids = updater.get_tiids_not_updated_since(schedule, number_to_update)
        print tiids
        assert_equals(sorted(tiids), sorted(['tiid2']))

        schedule = {"max_days_since_created": 7, "max_days_since_updated": 1, "exclude_old": False}      
        tiids = updater.get_tiids_not_updated_since(schedule, number_to_update)
        print tiids
        assert_equals(sorted(tiids), sorted(['tiid2', 'tiid3', 'tiid4']))


    def test_gold_update(self):
        number_to_update = 10        
        tiids = updater.gold_update(number_to_update, self.r)
        print tiids
        assert_equals(sorted(tiids), sorted(['tiid2', 'tiid4']))


    def test_gold_update_sets_last_update_run(self):
        number_to_update = 10        
        now = datetime.datetime.utcnow().isoformat()
        tiids = updater.gold_update(number_to_update, self.r)
        item_obj = item_module.Item.query.get(tiids[0])  # can use this method because don't need metrics
        print now
        print item_obj.last_update_run

        assert_greater(item_obj.last_update_run.isoformat(), now)


########NEW FILE########
__FILENAME__ = test_views
import unittest, json, uuid
from copy import deepcopy
from urllib import quote_plus
import os
from nose.tools import assert_equals, nottest, assert_greater, assert_items_equal

from totalimpact import app, db, views, tiredis, api_user, collection, item as item_module
from totalimpact.providers.dryad import Dryad

from test.utils import setup_postgres_for_unittests, teardown_postgres_for_unittests, http


TEST_DRYAD_DOI = "10.5061/dryad.7898"
TEST_PLOS_DOI = "10.1371/journal.pone.0004803"
GOLD_MEMBER_ITEM_CONTENT = ["MEMBERITEM CONTENT"]
TEST_COLLECTION_ID = "TestCollectionId"
TEST_COLLECTION_TIID_LIST = ["tiid1", "tiid2"]
TEST_COLLECTION_TIID_LIST_MODIFIED = ["tiid1", "tiid_different"]

COLLECTION_SEED = json.loads("""{
    "id": "uuid-goes-here",
    "collection_name": "My Collection",
    "owner": "abcdef",
    "created": 1328569452.406,
    "last_modified": 1328569492.406,
    "alias_tiids": {"doi:123": "origtiid1", "github:frank":"origtiid2"}
}""")
COLLECTION_SEED_MODIFIED = deepcopy(COLLECTION_SEED)
COLLECTION_SEED_MODIFIED["alias_tiids"] = dict(zip(["doi:1", "doi:2"], TEST_COLLECTION_TIID_LIST_MODIFIED))

def MOCK_member_items(self, query_string, url=None, cache_enabled=True):
    return(GOLD_MEMBER_ITEM_CONTENT)

# ensures that all the functions in the views.py module will use a local db,
# which we can in turn use for these unit tests.
mydao = None
# do the same for redis, handing it local redis and instruction to use "DB 8" 
# to isolate unit testing
myredis = views.set_redis("redis://localhost:6379", db=8)

class ViewsTester(unittest.TestCase):

    def setUp(self):
        """
        This test item is a lightly-modified version of a real doc from our
        demo collection; it's available at http://total-impact-core.herokuapp.com/collection/kn5auf
        """
        test_item = '''
            {
            "_id": "1aff9dfebea711e1bdf912313d1a5e63",
            "_rev": "968-c7891982fca2ea41346a20b80c2b888d",
            "aliases": {
                "doi": [
                    "10.5061/dryad.j1fd7"
                ],
                "title": [
                    "Data from: Data archiving is a good use of research funds",
                    "data from: data archiving is a good  investment"
                ],
                "url": [
                    "http://datadryad.org/handle/10255/dryad.33537",
                    "http://hdl.handle.net/10255/dryad.33537"
                ]
            },
            "biblio": {
                "authors": "Piwowar, Vision, Whitlock, Piwowar, Vision, Whitlock, Piwowar, Vision, Whitlock",
                "genre": "dataset",
                "h1": "Data from: Data archiving is a good  investment",
                "repository": "Dryad Digital Repository",
                "title": "Data from: Data archiving is a good  investment",
                "year": "2011"
            },
            "created": "2012-06-25T09:21:11.960271",
            "currently_updating": false,
            "last_modified": "2012-11-18T04:57:40.539053",
            "metrics": {
                "delicious:bookmarks": {
                    "provenance_url": "http://www.delicious.com/url/4794ddb7a3e934ba23165af65fcfa9cd",
                    "static_meta": {
                        "description": "The number of bookmarks to this artifact (maximum=100).",
                        "display_name": "bookmarks",
                        "icon": "http://www.delicious.com/favicon.ico",
                        "provider": "Delicious",
                        "provider_url": "http://www.delicious.com/"
                    },
                    "values": {
                        "raw": 1,
                        "raw_history": {
                            "2012-06-23T09:21:16.027149": 1
                        }
                    }
                },
                "dryad:total_downloads": {
                    "provenance_url": "http://dx.doi.org/10.5061/dryad.j1fd7",
                    "static_meta": {
                        "description": "Dryad total downloads: combined number of downloads of the data package and data files",
                        "display_name": "total downloads",
                        "icon": "http:\\/\\/datadryad.org\\/favicon.ico",
                        "provider": "Dryad",
                        "provider_url": "http:\\/\\/www.datadryad.org\\/"
                    },
                    "values": {
                        "dryad": {
                            "CI95_lower": 91,
                            "CI95_upper": 98,
                            "estimate_lower": 96,
                            "estimate_upper": 96
                        },
                        "raw": 207,
                        "raw_history": {
                            "2012-06-25T09:21:16.027149": 132,
                            "2012-06-26T18:05:19.598432": 132,
                            "2012-06-26T20:10:16.858294": 132
                        }
                    }
                }
            },
            "type": "item"
        }
        '''

        self.test_api_user_meta = {    
                    'max_registered_items': 3, 
                    'planned_use': 'individual CV', 
                    'email': "test@example.com", 
                    'notes': '', 
                    'api_key_owner': 'Julia Smith', 
                    "example_url": "", 
                    "organization": "NASA",
                    "prefix": "NASA",
                }

        self.db = setup_postgres_for_unittests(db, app)

        item = item_module.create_objects_from_item_doc(json.loads(test_item))
        self.db.session.add(item)

        self.existing_api_user = api_user.ApiUser(**self.test_api_user_meta)
        self.existing_api_user.api_key = "validkey"  #override randomly assigned key
        self.db.session.add(self.existing_api_user)
        self.db.session.commit()


        # do the same thing for the redis db.  We're using DB 8 for unittests.
        self.r = tiredis.from_url("redis://localhost:6379", db=8)
        self.r.flushdb()

        #setup api test client
        self.app = app
        self.app.testing = True
        self.client = self.app.test_client()

        # Mock out relevant methods of the Dryad provider
        self.orig_Dryad_member_items = Dryad.member_items
        Dryad.member_items = MOCK_member_items

        self.aliases = [
            ["doi", "10.123"],
            ["doi", "10.124"],
            ["doi", "10.125"]
        ]


    def tearDown(self):
        teardown_postgres_for_unittests(self.db)
        Dryad.member_items = self.orig_Dryad_member_items


    def test_does_not_require_key_if_preversioned_url(self):
        resp = self.client.get("/")
        assert_equals(resp.status_code, 200)

    def test_forbidden_if_no_key_in_v1(self):
        resp = self.client.get("/v1/provider")
        assert_equals(resp.status_code, 403)

    def test_ok_if_registered_key_in_v1(self):
        resp = self.client.get("/v1/provider?key=validkey")
        assert_equals(resp.status_code, 200)

    def test_forbidden_if_unregistered_key_in_v1(self):
        resp = self.client.get("/v1/provider?key=invalidkey")
        assert_equals(resp.status_code, 403)

    def test_importer_post_bibtex(self): 
        bibtex_snippet = """@article{rogers2008affirming,
              title={Affirming Complexity:" White Teeth" and Cosmopolitanism},
              author={Rogers, K.},
              journal={Interdisciplinary Literary Studies},
              year={2008}
            }"""       
        response = self.client.post(
            '/v1/importer/bibtex' + "?key=validkey",
            data=json.dumps({"input": bibtex_snippet}),
            content_type="application/json"
        )
        print response
        print response.data
        assert_equals(response.status_code, 200)
        assert_equals(response.mimetype, "application/json")
        assert_equals(len(json.loads(response.data)), 1)

        tiid = json.loads(response.data)["products"].keys()[0]
        item = item_module.Item.from_tiid(tiid)
        for biblio in item.biblios:
            if biblio.biblio_name == "authors":
                assert_equals(biblio.biblio_value, "Rogers")


    def test_memberitems_get(self):        
        response = self.client.get('/v1/provider/dryad/memberitems/Otto%2C%20Sarah%20P.?method=sync&key=validkey')
        print response
        print response.data
        assert_equals(response.status_code, 200)
        assert_equals(json.loads(response.data)["memberitems"], GOLD_MEMBER_ITEM_CONTENT)
        assert_equals(response.mimetype, "application/json")

    def test_memberitems_get_with_nonprinting_character(self):        
        response = self.client.get(u'/v1/provider/dryad/memberitems/Otto\u200e%2C%20Sarah%20P.?method=sync&key=validkey')
        print response
        print response.data
        assert_equals(response.status_code, 200)
        assert_equals(json.loads(response.data)["memberitems"], GOLD_MEMBER_ITEM_CONTENT)
        assert_equals(response.mimetype, "application/json")

    def test_file_parsing(self):
        datadir = os.path.join(os.path.split(__file__)[0], "../../extras/sample_provider_pages/bibtex")
        path = os.path.join(datadir, "Vision.bib")
        bibtex_str = open(path, "r").read()

    def test_exists(self):
        resp = self.client.get("/v1/provider?key=validkey")
        assert resp

    def test_gets_delicious_static_meta(self):
        resp = self.client.get("/v1/provider?key=validkey")
        md = json.loads(resp.data)
        print md["delicious"]
        assert md["delicious"]['metrics']["bookmarks"]["description"]

    def test_item_post_unknown_tiid(self):
        response = self.client.post('/v1/item/doi/AnIdOfSomeKind/' + "?key=validkey")
        print response
        print response.data
        assert_equals(response.status_code, 201)  #Created
        assert_equals(json.loads(response.data), u'ok')

    def test_item_post_success(self):
        resp = self.client.post('/v1/item/doi/' + quote_plus(TEST_DRYAD_DOI) + "?key=validkey")
        tiid = json.loads(resp.data)

        response = self.client.get('/v1/item/doi/' + quote_plus(TEST_DRYAD_DOI) + "?key=validkey")
        assert_equals(response.status_code, 210) # 210 created, but not done updating...
        assert_equals(response.mimetype, "application/json")
        saved_item = json.loads(response.data)

        assert_equals([unicode(TEST_DRYAD_DOI)], saved_item["aliases"]["doi"])

    def test_tiid_get(self):
        response = self.client.post(
            '/v1/importer/dois' + "?key=validkey",
            data=json.dumps({"input": TEST_DRYAD_DOI}),
            content_type="application/json"
        )
        created_tiid = json.loads(response.data)["products"].keys()[0]
        print created_tiid

        response = self.client.get('/v1/tiid/doi/' + quote_plus(TEST_DRYAD_DOI) + "?key=validkey")
        print response.data
        found_tiid = json.loads(response.data)["tiid"]

        assert_equals(created_tiid, found_tiid)

    def test_item_get_missing_no_create_param_returns_404(self):
        url = '/v1/item/doi/' + quote_plus(TEST_DRYAD_DOI) + "?key=validkey"
        response = self.client.get(url)
        assert_equals(response.status_code, 404) # created but still updating

    def test_item_get_create_param_makes_new_item(self):
        url = '/v1/item/doi/' + quote_plus(TEST_DRYAD_DOI) + "?key=validkey&register=true"
        response = self.client.get(url)
        assert_equals(response.status_code, 210) # created and still updating
        item_info = json.loads(response.data)
        assert_equals(item_info["aliases"]["doi"][0], TEST_DRYAD_DOI)

    def test_v1_item_post_success(self):
        url = '/v1/item/doi/' + quote_plus(TEST_DRYAD_DOI) + "?key=validkey"
        response = self.client.post(url)
        assert_equals(response.status_code, 201)
        assert_equals(json.loads(response.data), "ok")

    def test_item_get_success_realid(self):
        # First put something in
        response = self.client.get('/v1/item/doi/' + quote_plus(TEST_DRYAD_DOI) + "?key=validkey")
        tiid = response.data
        print response
        print tiid

    def test_v1_item_get_success_realid(self):
        # First put something in
        url = '/v1/item/doi/' + quote_plus(TEST_DRYAD_DOI) + "?key=validkey"
        response_post = self.client.post(url)
        # now check response
        response_get = self.client.get(url)
        assert_equals(response_get.status_code, 210)
        expected = {u'created': u'2012-11-06T19:57:15.937961', u'_rev': u'1-05e5d8a964a0fe9af4284a2a7804815f', u'currently_updating': True, u'metrics': {}, u'last_modified': u'2012-11-06T19:57:15.937961', u'biblio': {u'genre': u'dataset'}, u'_id': u'jku42e6ogs8ghxbr7p390nz8', u'type': u'item', u'aliases': {u'doi': [u'10.5061/dryad.7898']}}
        response_data = json.loads(response_get.data)        
        assert_equals(response_data["aliases"], {u'doi': [TEST_DRYAD_DOI]})

    def test_item_post_unknown_namespace(self):
        response = self.client.post('/v1/item/AnUnknownNamespace/AnIdOfSomeKind/' + "?key=validkey")
        # cheerfully creates items whether we know their namespaces or not.
        assert_equals(response.status_code, 201)

    def test_item_nid_with_bad_character(self):
        url = '/v1/item/doi/10.5061/dryad.' + u'\u200b' + 'j1fd7?key=validkey'
        response_get = self.client.get(url)
        assert_equals(response_get.status_code, 200)

    def test_item_removes_history_by_default(self):
        url = '/v1/item/doi/10.5061/dryad.j1fd7?key=validkey'
        response = self.client.get(url)
        metrics = json.loads(response.data)["metrics"]
        assert_equals(metrics["dryad:total_downloads"]["values"]["raw"], 132)
        assert_equals(
            set(metrics["dryad:total_downloads"]["values"].keys()),
            set(["raw"]) # no raw_history
        )

    def test_item_include_history_param(self):
        url = '/v1/item/doi/10.5061/dryad.j1fd7?key=validkey&include_history=true'
        response = self.client.get(url)

        metrics = json.loads(response.data)["metrics"]
        print (metrics["dryad:total_downloads"])
        assert_equals(
            set(metrics["dryad:total_downloads"]["values"].keys()),
            set(["raw", "raw_history"])
        )

        assert_equals(metrics["dryad:total_downloads"]["values"]["raw_history"].values(), [132, 132, 132])


    def test_post_with_aliases_already_in_db(self):
        items = [
            ["doi", "10.123"],
            ["doi", "10.124"],
            ["doi", "10.125"]
        ]
        resp = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"aliases": items, "title":"mah collection"}),
            content_type="application/json"
        )
        coll = json.loads(resp.data)["collection"]

        new_items = [
            ["doi", "10.123"], # duplicate
            ["doi", "10.124"], # duplicate
            ["doi", "10.999"]  # new
        ]

        resp2 = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"aliases": new_items, "title": "mah_collection"}),
            content_type="application/json"
        )
        new_coll = json.loads(resp2.data)["collection"]

        collection_tiid_objects = collection.CollectionTiid.query.all()
        assert_equals(len(collection_tiid_objects), 6)
        assert_equals(len(set([obj.tiid for obj in collection_tiid_objects])), 6)


    def test_collection_post_new_collection(self):
        response = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"aliases": self.aliases, "title":"My Title"}),
            content_type="application/json")

        print response
        print response.data
        assert_equals(response.status_code, 201)  #Created
        assert_equals(response.mimetype, "application/json")
        response_loaded = json.loads(response.data)
        assert_equals(
                set(response_loaded.keys()),
                set(["collection"])
        )
        coll = response_loaded["collection"]
        assert_equals(len(coll["_id"]), 6)
        assert_equals(
            set(coll["alias_tiids"].keys()),
            set([":".join(alias) for alias in self.aliases])
        )

        collection_object = collection.Collection.query.filter_by(cid=coll["_id"]).first()
        assert_items_equal(collection_object.tiids, coll["alias_tiids"].values())
        assert_items_equal([added_item.alias_tuple for added_item in collection_object.added_items], [(unicode(a), unicode(b)) for (a, b) in self.aliases])


    def test_collection_post_new_from_tiids(self):
        tiids = ["123", "456"]
        response = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"tiids": tiids, "title":"My Title"}),
            content_type="application/json")

        print response
        print response.data
        assert_equals(response.status_code, 201)  #Created
        assert_equals(response.mimetype, "application/json")
        response_loaded = json.loads(response.data)
        assert_equals(
                set(response_loaded.keys()),
                set(["collection"])
        )
        coll = response_loaded["collection"]
        assert_equals(len(coll["_id"]), 6)
        assert_equals(coll["alias_tiids"].keys(), tiids)

        collection_object = collection.Collection.query.filter_by(cid=coll["_id"]).first()
        assert_items_equal(collection_object.tiids, tiids)
        assert_items_equal(collection_object.added_items, [])


    def test_collection_get_with_no_id(self):
        response = self.client.get('/v1/collection/' + "?key=validkey")
        assert_equals(response.status_code, 404)  #Not found

    def test_collection_get(self):
        response = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"aliases": self.aliases, "title":"mah collection"}),
            content_type="application/json"
        )
        collection = json.loads(response.data)["collection"]
        collection_id = collection["_id"]
        print collection_id

        resp = self.client.get('/v1/collection/'+collection_id + "?key=validkey")
        assert_equals(resp.status_code, 210)
        collection_data = json.loads(resp.data)
        print collection_data.keys()
        assert_equals(
            set(collection_data.keys()),
            {u'title',
             u'items',
             u'created',
             u'last_modified',
             u'alias_tiids',
             u'_id',
             u'type'}
        )
        assert_equals(len(collection_data["items"]), len(self.aliases))


    def test_get_csv(self):
        response = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"aliases": self.aliases, "title":"mah collection"}),
            content_type="application/json"
        )
        collection = json.loads(response.data)["collection"]
        collection_id = collection["_id"]

        resp = self.client.get('/collection/'+collection_id+'.csv')
        print resp
        rows = resp.data.split("\n")
        print rows
        assert_equals(len(rows), 5) # header plus 3 items plus csvDictWriter adds an extra line

    def test_collection_update_puts_items_on_alias_queue(self):
        items = [
            ["doi", "10.123"],
            ["doi", "10.124"],
            ["doi", "10.125"]
        ]
        resp = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"aliases": items, "title":"mah collection"}),
            content_type="application/json"
        )
        coll = json.loads(resp.data)["collection"]
        print coll
        cid = coll["_id"]

        resp = self.client.post(
            "/v1/collection/" + cid + "?key=validkey"
        )
        assert_equals(resp.data, "true")

        # test it is on the redis queue
        response_json = self.r.rpop("aliasqueue")
        print response_json

        response = json.loads(response_json)
        assert_equals(len(response), 3)
        assert_equals(response[1]["doi"][0][0:3], "10.")
        assert_equals(response[2], [])
        

    def test_delete_collection_item(self):
        # make a new collection
        response = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"aliases": self.aliases, "title":"mah collection"}),
            content_type="application/json"
        )
        resp = json.loads(response.data)
        coll =  resp["collection"]

        # delete an item.
        tiid_to_delete = coll["alias_tiids"]["doi:10.123"]

        collection_object = collection.Collection.query.filter_by(cid=coll["_id"]).first()
        assert(tiid_to_delete in collection_object.tiids)

        r = self.client.delete(
            "/v1/collection/{id}/items?api_admin_key={key}".format(
                id=coll["_id"], 
                key=os.getenv("API_KEY")),
            data=json.dumps({"tiids": [tiid_to_delete]}),
            content_type="application/json"
        )

        collection_object = collection.Collection.query.filter_by(cid=coll["_id"]).first()
        assert_equals(len(collection_object.tiids), 2)

        assert(tiid_to_delete not in collection_object.tiids)


    def test_add_collection_item_through_tiids(self):
        # make two items through an importer
        response = self.client.post(
            '/v1/importer/dois' + "?key=validkey",
            data=json.dumps({"input": TEST_DRYAD_DOI + "\n" + TEST_PLOS_DOI}),
            content_type="application/json"
        )
        created_tiids = json.loads(response.data)["products"].keys()
        print created_tiids

        # make a new collection using the first item
        response = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"tiids": [created_tiids[0]], "title":"My Title"}),
            content_type="application/json")

        coll = json.loads(response.data)["collection"]
        cid = coll["_id"]

        # now add the other item
        r = self.client.put(
            "/v1/collection/{id}/items?api_admin_key={key}".format(
                id=cid, 
                key=os.getenv("API_KEY")),
            data=json.dumps({"tiids": [created_tiids[1]]}),
            content_type="application/json"
        )

        changed_coll = collection.Collection.query.filter_by(cid=cid).first()
        print changed_coll

        # we added a new item
        print changed_coll.tiids
        assert_equals(changed_coll.tiids, created_tiids)


    def test_add_collection_item_through_aliases(self):        
        # make a new collection
        response = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"aliases": self.aliases, "title":"mah collection"}),
            content_type="application/json"
        )
        resp = json.loads(response.data)
        coll = resp["collection"]

        alias_list = []
        alias_list.append(["doi", "10.new"])

        r = self.client.put(
            "/v1/collection/{id}/items?api_admin_key={key}".format(
                id=coll["_id"], 
                key=os.getenv("API_KEY")),
            data=json.dumps({"aliases": alias_list}),
            content_type="application/json"
        )

        changed_coll = collection.Collection.query.filter_by(cid=coll["_id"]).first()
        print changed_coll

        # we added a new item
        assert_equals(len(changed_coll.tiids), 4)


    def test_change_collection_requires_key(self):

        # make a new collection
        response = self.client.post(
            '/v1/collection' + "?key=validkey",
            data=json.dumps({"aliases": self.aliases, "title":"mah collection"}),
            content_type="application/json"
        )
        resp = json.loads(response.data)
        coll =  resp["collection"]

        alias_list = []
        alias_list.append(["doi", "10.new"])

        # 403 Forbidden if wrong edit key
        r = self.client.put(
            "/v1/collection/{id}/items?api_admin_key={key}".format(
                id=coll["_id"], 
                key="wrong!"),
            data=json.dumps({"aliases": alias_list}),
            content_type="application/json"
        )
        assert_equals(r.status_code, 403)

        # 403 Bad Request if no edit key
        r = self.client.put(
            "/v1/collection/{id}/items".format(id=coll["_id"]),
            data=json.dumps({"aliases": alias_list}),
            content_type="application/json"
        )
        assert_equals(r.status_code, 403)

        # get the collection out the db and make sure nothing's changed
        changed_coll = collection.Collection.query.filter_by(cid=coll["_id"]).first()

        assert_equals(changed_coll.title, "mah collection")


    def test_tiid_get_tiids_for_multiple_known_aliases(self):
        # create two new items with the same plos alias
        first_plos_create_tiid_resp = self.client.post('/v1/item/doi/' +
                quote_plus(TEST_PLOS_DOI) + "?key=validkey")
        first_plos_create_tiid = json.loads(first_plos_create_tiid_resp.data)

        second_plos_create_tiid_resp = self.client.post('/v1/item/doi/' +
                quote_plus(TEST_PLOS_DOI) + "?key=validkey")
        second_plos_create_tiid = json.loads(second_plos_create_tiid_resp.data)

        # check that the tiid lists are the same
        assert_equals(first_plos_create_tiid, second_plos_create_tiid)


    def test_inbox(self):
        example_payload = {
               "headers": {
                   "To": "7be5eb5001593217143f@cloudmailin.net",
                   "From": "Google Scholar Alerts <scholaralerts-noreply@google.com>",
                   "Date": "Thu, 21 Feb 2013 20:00:13 +0000",
                   "Subject": "Confirm your Google Scholar Alert"
               },
               "plain": "Google received a request to start sending Scholar Alerts to  \n7be5eb5001593217143f@cloudmailin.net for the query:\nNew articles in Jonathan A. Eisen's profile\n\nClick to confirm this request:\nhttp://scholar.google.ca/scholar_alerts?update_op=confirm_alert&hl=en&alert_id=IMEzMffmofYJ&email_for_op=7be5eb5001593217143f%40cloudmailin.net\n\nClick to cancel this request:\nhttp://scholar.google.ca/scholar_alerts?view_op=cancel_alert_options&hl=en&alert_id=IMEzMffmofYJ&email_for_op=7be5eb5001593217143f%40cloudmailin.net\n\nThanks,\nThe Google Scholar Team",
            }

        response = self.client.post(
            "/v1/inbox?key=validkey",
            data=json.dumps(example_payload),
            content_type="application/json"
        )
        assert_equals(response.status_code, 200)
        assert_equals(json.loads(response.data), {u'subject': u'Confirm your Google Scholar Alert'})


    def test_new_api_user(self):
        # the api call needs the admin password
        self.test_api_user_meta["password"] = os.getenv("API_KEY")

        response = self.client.post(
            '/v1/key?key=validkey',
            data=json.dumps(self.test_api_user_meta),
            content_type="application/json"
        )
        print response.data
        resp_loaded = json.loads(response.data)
        assert_equals(resp_loaded["api_key"].split("-")[0], self.test_api_user_meta["prefix"].lower())


    def test_item_post_known_tiid(self):
        response = self.client.post('/v1/item/doi/IdThatAlreadyExists/' + "?key=validkey")
        print response
        print "here is the response data: " + response.data

        # FIXME should check and if already exists return 200
        # right now this makes a new item every time, creating many dups
        assert_equals(response.status_code, 201)
        assert_equals(json.loads(response.data), u'ok')






########NEW FILE########
__FILENAME__ = utils
from sqlalchemy.exc import OperationalError
from sqlalchemy.sql import text    

def slow(f):
    f.slow = True
    return f

def http(f):
    f.http = True
    return f

# from http://www.sqlalchemy.org/trac/wiki/UsageRecipes/DropEverything
# with a few changes

def drop_everything(db, app):
    from sqlalchemy.engine import reflection
    from sqlalchemy import create_engine
    from sqlalchemy.schema import (
        MetaData,
        Table,
        DropTable,
        ForeignKeyConstraint,
        DropConstraint,
        )

    conn = db.session()

    engine = create_engine(app.config['SQLALCHEMY_DATABASE_URI'])
    inspector = reflection.Inspector.from_engine(engine)

    # gather all data first before dropping anything.
    # some DBs lock after things have been dropped in 
    # a transaction.
    
    metadata = MetaData()

    tbs = []
    all_fks = []

    for table_name in inspector.get_table_names():
        fks = []
        for fk in inspector.get_foreign_keys(table_name):
            if not fk['name']:
                continue
            fks.append(
                ForeignKeyConstraint((),(),name=fk['name'])
                )
        t = Table(table_name,metadata,*fks)
        tbs.append(t)
        all_fks.extend(fks)

    conn.execute("""drop view if exists min_biblio""")

    for fkc in all_fks:
        conn.execute(DropConstraint(fkc))

    for table in tbs:
        conn.execute(DropTable(table))

    db.session.commit()    


def setup_postgres_for_unittests(db, app):

    if not "localhost" in app.config["SQLALCHEMY_DATABASE_URI"]:
        assert(False), "Not running this unittest because SQLALCHEMY_DATABASE_URI is not on localhost"

    result = db.session.execute("drop view if exists doaj_issn_lookup; drop table if exists doaj cascade")
    db.session.commit()

    drop_everything(db, app)

    db.create_all()

    from totalimpact import extra_schema 
    extra_schema.create_view_min_biblio()
    extra_schema.create_doaj_table()
    extra_schema.create_doaj_view()

    doaj_setup_sql = """
        INSERT INTO doaj(title, publisher, issn, eissn, "cc license") VALUES ('ZooKeys', 'Pensoft Publishers', '1313-2989', '1313-2970', 'by');
        INSERT INTO doaj(title, publisher, issn, eissn, "cc license") VALUES ('PLOS comp bio', 'PLOS', '1553-7358', '', 'by');
    """
    raw_sql = text(doaj_setup_sql)
    result = db.session.execute(raw_sql)
    return db


def teardown_postgres_for_unittests(db):
    db.session.close_all()

########NEW FILE########
__FILENAME__ = backend
#!/usr/bin/env python

import os, time, json, logging, threading, Queue, copy, sys, datetime
from collections import defaultdict

from totalimpact import tiredis, default_settings, db
from totalimpact import item as item_module
from totalimpact.providers.provider import ProviderFactory, ProviderError

logger = logging.getLogger('ti.backend')
logger.setLevel(logging.DEBUG)

thread_count = defaultdict(dict)




class RedisQueue(object):
    def __init__(self, queue_name, myredis):
        self.queue_name = queue_name
        self.myredis = myredis
        self.name = queue_name + "_queue"

    def push(self, message):
        message_json = json.dumps(message)
        # logger.info(u"{:20}: /biblio_print >>>PUSHING to redis {message_json}".format(
        #     self.name, message_json=message_json)) 
        queue_length = self.myredis.llen(self.queue_name)       
        # logger.info(u">>>PUSHING to redis queue {queue_name}, current length {queue_length}".format(
        #     queue_name=self.name, queue_length=queue_length)) 
        self.myredis.lpush(self.queue_name, message_json)

    @classmethod
    def pop(cls, redis_queues, myredis):
        message = None
        logger.debug(u"about to pop from redis".format())

        (popped_queue_name, message_json) = myredis.brpop([queue.queue_name for queue in redis_queues]) 
        if message_json:
            queue_length = myredis.llen(popped_queue_name)                   
            logger.debug(u"{:20}: <<<POPPED from redis, current length {queue_length}, now to parse message".format(
                popped_queue_name, queue_length=queue_length))
            try:
                # logger.debug(u"{:20}: <<<POPPED from redis: starts {message_json}".format(
                #     self.name, message_json=message_json[0:50]))        
                message = json.loads(message_json) 
            except (TypeError, KeyError):
                logger.info(u"{:20}: ERROR processing redis message {message_json}".format(
                    popped_queue_name, message_json=message_json))
        return message


class PythonQueue(object):
    def __init__(self, queue_name):
        self.queue_name = queue_name
        self.queue = Queue.Queue()

    def push(self, message):
        self.queue.put(copy.deepcopy(message))
        queue_length = self.queue.qsize()
        if queue_length >= 10:
            logger.info(u">>>PUSHING to python queue {queue_name}, current length approx {queue_length}".format(
                queue_name=self.queue_name, queue_length=queue_length)) 

        #logger.info(u"{:20}: >>>PUSHED".format(
        #        self.queue_name))

    def pop(self):
        try:
            # blocking pop
            message = copy.deepcopy(self.queue.get(block=True)) #maybe timeout isn't necessary
            self.queue.task_done()
            # queue_length = self.queue.qsize()
            # logger.info(u"<<<POPPED from python queue {queue_name}, current length approx {queue_length}".format(
            #     queue_name=self.queue_name, queue_length=queue_length)) 
        except Queue.Empty:
            logger.warning(u"{:20}: Queue.Empty... not expecting to get here".format(
               self.queue_name))            
            message = None
        return message


class Worker(object):
    def run_in_loop(self):
        while True:
            self.run()

    def spawn_and_loop(self):
        t = threading.Thread(target=self.run_in_loop, name=self.name+"_thread")
        t.daemon = True
        t.start()    

class ProviderWorker(Worker):
    def __init__(self, provider, polling_interval, alias_queues, provider_queue, wrapper, myredis):
        self.provider = provider
        self.provider_name = provider.provider_name
        self.polling_interval = polling_interval 
        self.provider_queue = provider_queue
        self.alias_queues = alias_queues
        self.wrapper = wrapper
        self.myredis = myredis
        self.name = self.provider_name+"_worker"


    # last variable is an artifact so it has same call signature as other callbacks
    def add_to_database_if_nonzero(self, 
            tiid, 
            new_content, 
            method_name, 
            analytics_credentials, 
            dummy_already_run=None):

        try:
            if new_content:
                # don't need item with metrics for this purpose, so don't bother getting metrics from db
                item_obj = item_module.Item.query.get(tiid)

                if item_obj:
                    if method_name=="aliases":
                        item_obj = item_module.add_aliases_to_item_object(new_content, item_obj)
                    elif method_name=="biblio":
                        updated_item_doc = item_module.update_item_with_new_biblio(new_content, item_obj, self.provider_name)
                    elif method_name=="metrics":
                        for metric_name in new_content:
                            item_obj = item_module.add_metric_to_item_object(metric_name, new_content[metric_name], item_obj)
                    else:
                        logger.warning(u"ack, supposed to save something i don't know about: " + str(new_content))
        finally:
            db.session.remove()

        # do this no matter what, but as last thing
        if method_name=="metrics":
            self.myredis.set_provider_finished(tiid, self.provider_name)
        return



    def add_to_alias_queue_and_database(self, 
                tiid, 
                aliases_dict, 
                method_name, 
                analytics_credentials, 
                alias_providers_already_run):

        self.add_to_database_if_nonzero(tiid, aliases_dict, method_name, analytics_credentials)

        alias_message = {
                "tiid": tiid, 
                "aliases_dict": aliases_dict,
                "analytics_credentials": analytics_credentials,
                "alias_providers_already_run": alias_providers_already_run
            }        

        logger.info(u"Adding to alias queue tiid:{tiid} for {provider_name}".format(
            tiid=tiid, 
            provider_name=self.provider_name))     

        # always push to highest priority queue if we're already going
        self.alias_queues["high"].push(alias_message)


    @classmethod
    def wrapper(cls, tiid, input_aliases_dict, provider, method_name, analytics_credentials, aliases_providers_run, callback):
        global thread_count

        #logger.info(u"{:20}: **Starting {tiid} {provider_name} {method_name} with {aliases}".format(
        #    "wrapper", tiid=tiid, provider_name=provider.provider_name, method_name=method_name, aliases=aliases))

        provider_name = provider.provider_name
        worker_name = provider_name+"_worker"

        input_alias_tuples = item_module.alias_tuples_from_dict(input_aliases_dict)
        method = getattr(provider, method_name)

        try:
            if provider.uses_analytics_credentials(method_name):
                method_response = method(input_alias_tuples, analytics_credentials=analytics_credentials)
            else:
                method_response = method(input_alias_tuples)
        except ProviderError:
            method_response = None
            logger.info(u"{:20}: **ProviderError {tiid} {method_name} {provider_name} ".format(
                worker_name, tiid=tiid, provider_name=provider_name.upper(), method_name=method_name.upper()))

        if method_name == "aliases":
            # update aliases to include the old ones too
            aliases_providers_run += [provider_name]
            if method_response:
                new_aliases_dict = item_module.alias_dict_from_tuples(method_response)
                new_canonical_aliases_dict = item_module.canonical_aliases(new_aliases_dict)
                response = item_module.merge_alias_dicts(new_canonical_aliases_dict, input_aliases_dict)
            else:
                response = input_aliases_dict
        else:
            response = method_response

        logger.info(u"{:20}: /biblio_print, RETURNED {tiid} {method_name} {provider_name} : {response}".format(
            worker_name, tiid=tiid, method_name=method_name.upper(), 
            provider_name=provider_name.upper(), response=response))

        callback(tiid, response, method_name, analytics_credentials, aliases_providers_run)

        try:
            del thread_count[provider_name][tiid+method_name]
        except KeyError:  # thread isn't there when we call wrapper in unit tests
            pass

        return response


    def run(self):
        global thread_count

        num_active_threads_for_this_provider = len(thread_count[self.provider.provider_name])

        if num_active_threads_for_this_provider >= self.provider.max_simultaneous_requests:
            logger.info(u"{provider} has {num_provider} threads, so not spawning another yet".format(
                num_provider=num_active_threads_for_this_provider, provider=self.provider.provider_name.upper()))
            time.sleep(self.polling_interval) # let the provider catch up
            return

        provider_message = self.provider_queue.pop()
        if provider_message:
            #logger.info(u"POPPED from queue for {provider}".format(
            #    provider=self.provider_name))
            tiid = provider_message["tiid"]
            aliases_dict = provider_message["aliases_dict"]
            method_name = provider_message["method_name"]
            analytics_credentials = provider_message["analytics_credentials"]
            alias_providers_already_run = provider_message["alias_providers_already_run"]

            if (method_name == "metrics") and self.provider.provides_metrics:
                self.myredis.set_provider_started(tiid, self.provider.provider_name)

            if method_name == "aliases":
                callback = self.add_to_alias_queue_and_database
            else:
                callback = self.add_to_database_if_nonzero

            #logger.info(u"BEFORE STARTING thread for {tiid} {method_name} {provider}".format(
            #    method_name=method_name.upper(), tiid=tiid, num=len(thread_count[self.provider.provider_name].keys()),
            #    provider=self.provider.provider_name.upper()))

            thread_count[self.provider.provider_name][tiid+method_name] = 1

            if num_active_threads_for_this_provider >= 10:
                logger.info(u"{num_total} total threads, {num_provider} threads for {provider}".format(
                    num_provider=num_active_threads_for_this_provider,
                    num_total=threading.active_count(),
                    provider=self.provider.provider_name.upper()))

            t = threading.Thread(target=ProviderWorker.wrapper, 
                args=(tiid, aliases_dict, self.provider, method_name, analytics_credentials, alias_providers_already_run, callback), 
                name=self.provider_name+"-"+method_name.upper()+"-"+tiid[0:4])
            t.start()
            return



class Backend(Worker):
    def __init__(self, alias_queues, provider_queues, myredis):
        self.alias_queues = alias_queues
        self.provider_queues = provider_queues
        self.myredis = myredis
        self.name = "Backend"

    @classmethod
    def sniffer(cls, item_aliases, aliases_providers_run, provider_config=default_settings.PROVIDERS):

        # default to nothing
        aliases_providers = []
        biblio_providers = []
        metrics_providers = []
        (genre, host) = item_module.decide_genre(item_aliases)

        all_metrics_providers = [provider.provider_name for provider in 
                        ProviderFactory.get_providers(provider_config, "metrics")]

        has_enough_alias_urls = ("url" in item_aliases)
        if has_enough_alias_urls:
            if ("doi" in item_aliases):
                has_enough_alias_urls = (len([url for url in item_aliases["url"] if url.startswith("http://dx.doi.org")]) > 0)

        if (genre == "article") and (host != "arxiv"):
            if not "mendeley" in aliases_providers_run:
                aliases_providers = ["mendeley"]
            elif not "crossref" in aliases_providers_run:
                aliases_providers = ["crossref"]  # do this before pubmed because might tease doi from url
            elif not "pubmed" in aliases_providers_run:
                aliases_providers = ["pubmed"]
            elif not "altmetric_com" in aliases_providers_run:
                aliases_providers = ["altmetric_com"]
            else:
                metrics_providers = all_metrics_providers
                biblio_providers = ["crossref", "pubmed", "mendeley", "webpage"]
        elif ("doi" in item_aliases) or (host == "arxiv"):
            if (set([host, "altmetric_com"]) == set(aliases_providers_run)):
                metrics_providers = all_metrics_providers
                biblio_providers = [host, "mendeley"]
            else:     
                if not "altmetric_com" in aliases_providers_run:
                    aliases_providers = ["altmetric_com"]
                else:
                    aliases_providers = [host]
        else:
            # relevant alias and biblio providers are always the same
            relevant_providers = [host]
            if relevant_providers == ["unknown"]:
                relevant_providers = ["webpage"]
            # if all the relevant providers have already run, then all the aliases are done
            # or if it already has urls
            if has_enough_alias_urls or (set(relevant_providers) == set(aliases_providers_run)):
                metrics_providers = all_metrics_providers
                biblio_providers = relevant_providers
            else:
                aliases_providers = relevant_providers

        return({
            "aliases":aliases_providers,
            "biblio":biblio_providers,
            "metrics":metrics_providers})


    def providers_too_busy(self, max_requests=50):
        global thread_count

        for provider_name in thread_count:
            if provider_name != "webpage":
                num_active_threads_for_this_provider = len(thread_count[provider_name])
                if num_active_threads_for_this_provider >= max_requests:
                    return provider_name
        return None

    def run(self):
        global thread_count

        # go through alias_queues, with highest priority first
        alias_message = RedisQueue.pop([self.alias_queues["high"], self.alias_queues["low"]], self.myredis)

        if alias_message:
            tiid = alias_message["tiid"]
            aliases_dict = alias_message["aliases_dict"]
            analytics_credentials = alias_message["analytics_credentials"]
            alias_providers_already_run = alias_message["alias_providers_already_run"]            

            relevant_provider_names = self.sniffer(aliases_dict, alias_providers_already_run)
            #logger.info(u"/biblio_print, backend for {tiid} sniffer got input {alias_dict}".format(
            #    tiid=tiid, alias_dict=alias_dict))
            logger.info(u"backend for {tiid} sniffer returned {providers}".format(
                tiid=tiid, providers=relevant_provider_names))

            # list out the method names so they are run in that priority, biblio before metrics
            for method_name in ["aliases", "biblio", "metrics"]:
                for provider_name in relevant_provider_names[method_name]:

                    provider_message = {
                        "tiid": tiid, 
                        "aliases_dict": aliases_dict, 
                        "method_name": method_name, 
                        "analytics_credentials": analytics_credentials,
                        "alias_providers_already_run": alias_providers_already_run}
                    try:
                        self.provider_queues[provider_name].push(provider_message)
                    except KeyError:
                        #removed a provider?
                        logger.warning(u"KeyError in backend for {tiid} on {provider_name}".format(
                            tiid=tiid, provider_name=provider_name))
        else:
            #time.sleep(0.1)  # is this necessary?
            pass



def main():

    mydao = None

    myredis = tiredis.from_url(os.getenv("REDIS_URL"))

    alias_queues = {
        "high": RedisQueue("aliasqueue_high", myredis), 
        "low": RedisQueue("aliasqueue_low", myredis)
        }
    # to clear alias_queue:
    #import redis, os
    #myredis = redis.from_url(os.getenv("REDIS_URL"))
    #myredis.delete("aliasqueue_high")


    polling_interval = 0.1   # how many seconds between polling to talk to provider
    provider_queues = {}
    providers = ProviderFactory.get_providers(default_settings.PROVIDERS)
    for provider in providers:
        provider_queues[provider.provider_name] = PythonQueue(provider.provider_name+"_queue")
        provider_worker = ProviderWorker(
            provider, 
            polling_interval, 
            alias_queues,
            provider_queues[provider.provider_name], 
            ProviderWorker.wrapper,
            myredis)
        provider_worker.spawn_and_loop()

    backend = Backend(alias_queues, provider_queues, myredis)
    try:
        backend.run_in_loop() # don't need to spawn this one
    except (KeyboardInterrupt, SystemExit): 
        # this approach is per http://stackoverflow.com/questions/2564137/python-how-to-terminate-a-thread-when-main-program-ends
        sys.exit()
 
if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = cache
import os
import sys
import pylibmc
import hashlib
import logging
import json
from cPickle import PicklingError

from totalimpact.utils import Retry

# set up logging
logger = logging.getLogger("ti.cache")

class CacheException(Exception):
    pass

class Cache(object):
    """ Maintains a cache of URL responses in memcached """

    def _build_hash_key(self, key):
        json_key = json.dumps(key)
        hash_key = hashlib.md5(json_key.encode("utf-8")).hexdigest()
        return hash_key

    def _get_memcached_client(self):
        servers = [os.environ.get('MEMCACHIER_SERVERS')]
        username=os.environ.get('MEMCACHIER_USERNAME')
        password=os.environ.get('MEMCACHIER_PASSWORD')
        if "localhost" in servers:
            username = None
            password = None
        mc = pylibmc.Client(
            servers=servers, 
            username=username,
            password=password,
            binary=True)
        return mc
 
    def __init__(self, max_cache_age=60*60):  #one hour
        self.max_cache_age = max_cache_age


    def flush_cache(self):
        #empties the cache
        mc = self._get_memcached_client()        
        mc.flush_all()

    @Retry(3, pylibmc.Error, 0.1)
    def get_cache_entry(self, key):
        """ Get an entry from the cache, returns None if not found """
        mc = self._get_memcached_client()
        hash_key = self._build_hash_key(key)
        response = mc.get(hash_key)
        return response

    @Retry(3, pylibmc.Error, 0.1)
    def set_cache_entry(self, key, data):
        """ Store a cache entry """

        #memcached will only store things up to 1MB as per http://sendapatch.se/projects/pylibmc/misc.html
        MAX_MEMCACHED_VALUE_SIZE = 1000*1000
        if sys.getsizeof(data["text"]) > MAX_MEMCACHED_VALUE_SIZE:
            logger.debug(u"Not caching because payload is too large")
            return None

        mc = self._get_memcached_client()
        hash_key = self._build_hash_key(key)
        try:
            set_response = mc.set(hash_key, data, time=self.max_cache_age)
            if not set_response:
                raise CacheException("Unable to store into Memcached. Make sure memcached server is running.")
        except PicklingError:
            # This happens when trying to cache a thread.lock object, for example.  Just don't cache.
            logger.debug(u"In set_cache_entry but got PicklingError")
            return None
        return set_response
  

########NEW FILE########
__FILENAME__ = collection
from werkzeug import generate_password_hash, check_password_hash
import shortuuid, string, random, datetime
import csv, StringIO, json, copy
from collections import OrderedDict, defaultdict
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm.exc import FlushError
from sqlalchemy.ext.hybrid import hybrid_property

from totalimpact import db
from totalimpact import item as item_module
from totalimpact.item import Alias, Item
from totalimpact import json_sqlalchemy
from totalimpact.providers.provider import ProviderFactory

# Master lock to ensure that only a single thread can write
# to the DB at one time to avoid document conflicts

import logging
logger = logging.getLogger('ti.collection')

# print out extra debugging
#logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)

def get_alias_strings(aliases):
    alias_strings = []
    for alias in aliases:
        alias = item_module.canonical_alias_tuple(alias)
        (namespace, nid) = alias
        try:
            alias_strings += [namespace+":"+nid]
        except TypeError:
            # jsonify the biblio dicts
            alias_strings += [namespace+":"+json.dumps(nid)]
    return alias_strings   

def add_items_to_collection_object(cid, tiids, alias_tuples=[]):
    logger.debug(u"in add_items_to_collection_object for {cid}".format(
        cid=cid))        

    collection_obj = Collection.query.filter_by(cid=cid).first()
    if not collection_obj:
        return None
    collection_obj.last_modified = datetime.datetime.utcnow()
    db.session.merge(collection_obj)

    for tiid in tiids:
        if tiid not in collection_obj.tiids:
            collection_obj.tiid_links += [CollectionTiid(tiid=tiid)]

    # for alias_tuple in alias_tuples:
    #     try:
    #         alias_tuple = item_module.canonical_alias_tuple(alias_tuple)
    #         #logger.info(u"added_aliases: {added_aliases}, this tuple: {alias_tuple}".format(
    #         #    added_aliases=collection_obj.added_aliases, 
    #         #    alias_tuple=alias_tuple))
    #         if alias_tuple not in collection_obj.added_aliases:
    #             collection_obj.added_items += [AddedItem(alias_tuple=alias_tuple)]
    #     except ValueError:
    #         logger.debug("could not separate alias tuple {alias_tuple}".format(
    #             alias_tuple=alias_tuple))            

    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in add_items_to_collection_object for {cid}, rolling back.  Message: {message}".format(
            cid=cid, 
            message=e.message)) 

    logger.debug("returning {collection_obj}".format(
        collection_obj=collection_obj))            

    return collection_obj



def add_items_to_collection(cid, aliases, analytics_credentials, myredis, mydao=None):
    logger.debug(u"in add_items_to_collection for {cid}".format(
        cid=cid))        

    tiids_aliases_map = item_module.create_tiids_from_aliases(aliases, analytics_credentials, myredis)

    logger.debug(u"in add_items_to_collection with {tiids_aliases_map}".format(
        tiids_aliases_map=tiids_aliases_map)) 

    tiids = tiids_aliases_map.keys()
    aliases = tiids_aliases_map.values()

    collection_obj = add_items_to_collection_object(cid, tiids, aliases)

    logger.debug(u"just finished add_items_to_collection for {cid}".format(
        cid=cid))        

    return collection_obj


def remove_items_from_collection(cid, tiids_to_delete, myredis, mydao=None):
    logger.debug(u"in remove_items_from_collection_object for {cid}".format(
        cid=cid))        

    collection_obj = Collection.query.filter_by(cid=cid).first()
    if not collection_obj:
        return None    
    collection_obj.last_modified = datetime.datetime.utcnow()
    db.session.merge(collection_obj)

    for coll_tiid in collection_obj.tiid_links:
        if coll_tiid.tiid in tiids_to_delete:
            collection_obj.tiid_links.remove(coll_tiid)

    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in remove_items_from_collection_object for {cid}, rolling back.  Message: {message}".format(
            cid=cid, 
            message=e.message))

    return collection_obj


def create_new_collection_from_tiids(cid, title, tiids, ip_address, refset_metadata):
    logger.debug(u"in create_new_collection_from_tiids for {cid}".format(
        cid=cid))        

    coll_doc, key = make(cid)
    if refset_metadata:
        coll_doc["refset_metadata"] = refset_metadata
    coll_doc["ip_address"] = ip_address
    coll_doc["title"] = title
    coll_doc["alias_tiids"] = dict(zip(tiids, tiids))

    collection_obj = create_objects_from_collection_doc(coll_doc)

    logger.info(u"saved new collection '{id}' with {num_items} items.".format(
            id=coll_doc["_id"],
            num_items=len(coll_doc["alias_tiids"])))

    logger.debug(json.dumps(coll_doc, sort_keys=True, indent=4))

    return (coll_doc, collection_obj)



def create_new_collection(cid, title, aliases, ip_address, refset_metadata, myredis, mydao):
    logger.debug(u"in create_new_collection for {cid}".format(
        cid=cid))        

    analytics_credentials = {}
    tiids_aliases_map = item_module.create_tiids_from_aliases(aliases, analytics_credentials, myredis)

    logger.debug(u"in add_items_to_collection with {tiids_aliases_map}".format(
        tiids_aliases_map=tiids_aliases_map)) 

    tiids = tiids_aliases_map.keys()
    aliases = tiids_aliases_map.values()

    coll_doc, key = make(cid)
    if refset_metadata:
        coll_doc["refset_metadata"] = refset_metadata
    coll_doc["ip_address"] = ip_address
    coll_doc["title"] = title
    alias_strings = get_alias_strings(aliases)
    coll_doc["alias_tiids"] = dict(zip(alias_strings, tiids))

    collection_obj = create_objects_from_collection_doc(coll_doc)

    logger.info(u"saved new collection '{id}' with {num_items} items.".format(
            id=coll_doc["_id"],
            num_items=len(coll_doc["alias_tiids"])))

    logger.debug(json.dumps(coll_doc, sort_keys=True, indent=4))

    return (coll_doc, collection_obj)


def create_objects_from_collection_doc(coll_doc):
    cid = coll_doc["_id"]
    
    logger.debug(u"in create_objects_from_collection_doc for {cid}".format(
        cid=coll_doc["_id"]))        

    new_coll_object = Collection.query.filter_by(cid=coll_doc["_id"]).first()
    if not new_coll_object:
        new_coll_object = Collection.create_from_old_doc(coll_doc)
    db.session.add(new_coll_object)    

    tiids = coll_doc["alias_tiids"].values()
    for tiid in tiids:
        if tiid not in new_coll_object.tiids:
            new_coll_object.tiid_links += [CollectionTiid(tiid=tiid)]

    logger.debug(u"new_tiid_objects for {cid} are {new_tiid_objects}".format(
        cid=new_coll_object.cid, 
        new_tiid_objects=new_coll_object.tiids))        

    # alias_strings = coll_doc["alias_tiids"].keys()
    # alias_tuples = [alias_string.split(":", 1) for alias_string in alias_strings]
    # for alias_tuple in alias_tuples:
    #     try:
    #         alias_tuple = item_module.canonical_alias_tuple(alias_tuple)
    #         if alias_tuple not in new_coll_object.added_aliases:
    #                 new_coll_object.added_items += [AddedItem(alias_tuple=alias_tuple, created=coll_doc["last_modified"])]
    #     except ValueError:
    #         logger.debug("could not separate alias tuple {alias_tuple}".format(
    #             alias_tuple=alias_tuple))

    # logger.debug(u"new_added_item_objects for {cid} are {new_added_item_objects}".format(
    #     cid=new_coll_object.cid, 
    #     new_added_item_objects=new_coll_object.added_aliases))      

    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in create_objects_from_collection_doc for {cid}, rolling back.  Message: {message}".format(
            cid=cid, 
            message=e.message))        

    return(new_coll_object)


def delete_collection(cid):
    coll_object = Collection.query.filter_by(cid=cid).first()
    db.session.delete(coll_object)
    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in delete_collection for {cid}, rolling back.  Message: {message}".format(
            cid=cid, 
            message=e.message))        
    return


# class AddedItem(db.Model):
#     id = db.Column(db.Integer, primary_key=True)
#     cid = db.Column(db.Text, db.ForeignKey('collection.cid'), index=True)
#     namespace = db.Column(db.Text)
#     nid = db.Column(json_sqlalchemy.JSONAlchemy(db.Text))
#     tiid = db.Column(db.Text)
#     created = db.Column(db.DateTime())

#     def __init__(self, **kwargs):
#         logger.debug(u"new AddedItem {kwargs}".format(
#             kwargs=kwargs))    
#         if "alias_tuple" in kwargs:
#             alias_tuple = item_module.canonical_alias_tuple(kwargs["alias_tuple"])
#             (namespace, nid) = alias_tuple
#             self.namespace = namespace
#             self.nid = nid                
#         if "created" in kwargs:
#             self.created = kwargs["created"]
#         else:   
#             self.created = datetime.datetime.utcnow()
#         super(AddedItem, self).__init__(**kwargs)

#     @hybrid_property
#     def alias_tuple(self):
#         return ((self.namespace, self.nid))

#     @alias_tuple.setter
#     def alias_tuple(self, alias_tuple):
#         try:
#             alias_tuple = item_module.canonical_alias_tuple(alias_tuple)
#             (namespace, nid) = alias_tuple
#         except ValueError:
#             logger.debug("could not separate alias tuple {alias_tuple}".format(
#                 alias_tuple=alias_tuple))
#             raise
#         self.namespace = namespace
#         self.nid = nid

#     def __repr__(self):
#         return '<AddedItem {collection}, {alias_tuple} {tiid}>'.format(
#             collection=self.collection, 
#             alias_tuple=self.alias_tuple,
#             tiid=self.tiid)

class CollectionTiid(db.Model):
    cid = db.Column(db.Text, db.ForeignKey('collection.cid'), primary_key=True, index=True)
    tiid = db.Column(db.Text, primary_key=True)

    def __init__(self, **kwargs):
        logger.debug(u"new CollectionTiid {kwargs}".format(
            kwargs=kwargs))                
        super(CollectionTiid, self).__init__(**kwargs)

    def __repr__(self):
        return '<CollectionTiid {collection} {tiid}>'.format(
            collection=self.collection, 
            tiid=self.tiid)


class Collection(db.Model):
    cid = db.Column(db.Text, primary_key=True)
    created = db.Column(db.DateTime())
    last_modified = db.Column(db.DateTime())
    ip_address = db.Column(db.Text)
    title = db.Column(db.Text)
    refset_metadata = db.Column(json_sqlalchemy.JSONAlchemy(db.Text))
    tiid_links = db.relationship('CollectionTiid', lazy='subquery', cascade="all, delete-orphan",
        backref=db.backref("collection", lazy="subquery"))
    # added_items = db.relationship('AddedItem', lazy='subquery', cascade="all, delete-orphan",
    #     backref=db.backref("collection", lazy="subquery"))

    def __init__(self, collection_id=None, **kwargs):
        logger.debug(u"new Collection {kwargs}".format(
            kwargs=kwargs))                

        if collection_id is None:
            collection_id = _make_id()
        self.cid = collection_id

        now = datetime.datetime.utcnow()
        if "created" in kwargs:
            self.created = kwargs["created"]
        else:   
            self.created = now

        if "last_modified" in kwargs:
            self.last_modified = kwargs["last_modified"]
        else:   
            self.last_modified = now

        super(Collection, self).__init__(**kwargs)

    @property
    def tiids(self):
        return [tiid_link.tiid for tiid_link in self.tiid_links]

    # @property
    # def added_aliases(self):
    #     return [added_item.alias_tuple for added_item in self.added_items]

    def __repr__(self):
        return '<Collection {cid}, {title}>'.format(
            cid=self.cid, 
            title=self.title)


    @classmethod
    def create_from_old_doc(cls, doc):
        doc_copy = copy.deepcopy(doc)
        doc_copy["cid"] = doc_copy["_id"]
        for key in doc_copy.keys():
            if key not in ["cid", "created", "last_modified", "ip_address", "title", "refset_metadata"]:
                del doc_copy[key]
        new_collection_object = Collection(**doc_copy)
        return new_collection_object




#delete when we move to postgres
def make(collection_id=None):
    shortuuid.set_alphabet('abcdefghijklmnopqrstuvwxyz1234567890')
    key = shortuuid.uuid()[0:10]

    if collection_id is None:
        collection_id = _make_id()

    now = datetime.datetime.utcnow().isoformat()
    collection = {}

    collection["_id"] = collection_id
    collection["created"] = now
    collection["last_modified"] = now
    collection["type"] = "collection"
    collection["owner"] = None
    collection["key"] = key  # using the hash was needless complexity...

    return collection, key


def _make_id(len=6):
    '''Make an id string.

    Currently uses only lowercase and digits for better say-ability. Six
    places gives us around 2B possible values.
    '''
    choices = string.ascii_lowercase + string.digits
    return ''.join(random.choice(choices) for x in range(len))

def get_titles(cids, mydao=None):
    ret = {}
    for cid in cids:
        coll = Collection.query.filter_by(cid=cid).first()
        ret[cid] = coll.title
    return ret


def get_collection_doc_from_object(collection_obj):
    collection_doc = {}

    collection_doc["_id"] = collection_obj.cid
    collection_doc["type"] = "collection"
    collection_doc["title"] = collection_obj.title
    collection_doc["created"] = collection_obj.created.isoformat()
    collection_doc["last_modified"] = collection_obj.last_modified.isoformat()
    # don't include ip_address in info for client
    collection_doc["alias_tiids"] = {}
    tiids = collection_obj.tiids
    for tiid in tiids:
        collection_doc["alias_tiids"][tiid] = tiid  

    return(collection_doc)

def get_collection_doc(cid):
    collection_obj = Collection.query.get(cid)
    if not collection_obj:
        return None
    return get_collection_doc_from_object(collection_obj)


def is_something_currently_updating(items_dict, myredis):
    something_currently_updating = False
    for tiid in items_dict:
        this_item_updating = item_module.is_currently_updating(tiid, myredis)
        something_currently_updating = something_currently_updating or this_item_updating
    return something_currently_updating


def get_items_for_client(tiids, myrefsets, myredis, most_recent_metric_date=None, most_recent_diff_metric_date=None):
    item_metric_dicts = get_readonly_item_metric_dicts(tiids, most_recent_metric_date, most_recent_diff_metric_date)

    dict_of_item_docs = {}
    for tiid in item_metric_dicts:
        try:
            item_doc_for_client = item_module.build_item_for_client(item_metric_dicts[tiid], myrefsets, myredis)
            dict_of_item_docs[tiid] = item_doc_for_client
        except (KeyError, TypeError, AttributeError):
            logger.info(u"Couldn't build item {tiid}".format(tiid=tiid))
            raise
    
    return dict_of_item_docs

def get_most_recent_metrics(tiids, most_recent_metric_date=None):
    # we use string concatination below because haven't figured out bind params yet
    # abort if anything suspicious in tiids
    for tiid in tiids:
        for e in tiid:
            if not e.isalnum():
                return {}

    if not most_recent_metric_date:
        most_recent_metric_date = datetime.datetime.utcnow().isoformat()

    tiid_string = ",".join(["'"+tiid+"'" for tiid in tiids])    
    metric_objects = item_module.Metric.query.from_statement("""
        WITH max_collect AS 
            (SELECT tiid, provider, metric_name, max(collected_date) AS collected_date
                FROM metric
                WHERE tiid in ({tiid_string})
                AND collected_date <= '{most_recent_metric_date}'::date + 1               
                GROUP BY tiid, provider, metric_name
                ORDER by tiid, provider)
            SELECT max_collect.*, m.raw_value, m.drilldown_url
                FROM metric m
                NATURAL JOIN max_collect""".format(
                    tiid_string=tiid_string, most_recent_metric_date=most_recent_metric_date)).all()

    # logger.debug("*** get_previous_metrics {most_recent_metric_date} metric_objects = {metric_objects}".format(
    #     most_recent_metric_date=most_recent_metric_date, metric_objects=metric_objects))

    return metric_objects


def get_previous_metrics(tiids, most_recent_diff_metric_date=None):
    # we use string concatination below because haven't figured out bind params yet
    # abort if anything suspicious in tiids
    for tiid in tiids:
        for e in tiid:
            if not e.isalnum():
                return {}

    if not most_recent_diff_metric_date:
        most_recent_diff_metric_date = (datetime.datetime.utcnow() - datetime.timedelta(days=7)).isoformat()

    tiid_string = ",".join(["'"+tiid+"'" for tiid in tiids])    
    metric_objects = item_module.Metric.query.from_statement("""
        WITH max_collect AS 
            (SELECT tiid, provider, metric_name, max(collected_date) AS collected_date
                FROM metric
                WHERE tiid in ({tiid_string})
                AND collected_date <= '{most_recent_diff_metric_date}'::date
                GROUP BY tiid, provider, metric_name
                ORDER by tiid, provider)
        SELECT max_collect.*, m.raw_value, m.drilldown_url
            FROM metric m
            NATURAL JOIN max_collect""".format(
                tiid_string=tiid_string, most_recent_diff_metric_date=most_recent_diff_metric_date)).all()

    # logger.debug("*** get_previous_metrics {most_recent_diff_metric_date} metric_objects = {metric_objects}".format(
    #     most_recent_diff_metric_date=most_recent_diff_metric_date, metric_objects=metric_objects))

    return metric_objects


def get_readonly_item_metric_dicts(tiids, most_recent_metric_date=None, most_recent_diff_metric_date=None):
    logger.info(u"in get_readonly_item_objects_with_metrics")

    item_objects = Item.query.filter(Item.tiid.in_(tiids)).all()
    items_by_tiid = {}
    metrics_summaries = {}
    for item_obj in item_objects:
        items_by_tiid[item_obj.tiid] = {
            "item_obj": item_obj, 
            "metrics_summaries": defaultdict(dict)}

    db.session.expunge_all()

    metric_objects_recent = get_most_recent_metrics(tiids, most_recent_metric_date)
    for metric_object in metric_objects_recent:
        items_by_tiid[metric_object.tiid]["metrics_summaries"][metric_object.fully_qualified_name]["most_recent"] = copy.copy(metric_object)

    metric_objects_7_days_ago = get_previous_metrics(tiids, most_recent_diff_metric_date)
    for metric_object in metric_objects_7_days_ago:
        items_by_tiid[metric_object.tiid]["metrics_summaries"][metric_object.fully_qualified_name]["7_days_ago"] = copy.copy(metric_object)

    return items_by_tiid


def get_collection_with_items_for_client(cid, myrefsets, myredis, mydao, include_history=False):
    collection_obj = Collection.query.get(cid)
    collection_doc = get_collection_doc_from_object(collection_obj)
    if not collection_doc:
        return (None, None)

    collection_doc["items"] = []
    tiids = collection_obj.tiids

    if tiids:
        item_metric_dicts = get_readonly_item_metric_dicts(tiids)

        for tiid in item_metric_dicts:
            #logger.info(u"got item {tiid} for {cid}".format(
            #    tiid=item_obj.tiid, cid=cid))
            try:
                item_for_client = item_module.build_item_for_client(item_metric_dicts[tiid], myrefsets, myredis)
            except (KeyError, TypeError, AttributeError):
                logger.info(u"Couldn't build item {tiid}, excluding it from the returned collection {cid}".format(
                    tiid=tiid, cid=cid))
                item_for_client = None
                raise
            if item_for_client:
                collection_doc["items"] += [item_for_client]
    
    something_currently_updating = False
    for item in collection_doc["items"]:
        item["currently_updating"] = item_module.is_currently_updating(item["_id"], myredis)
        something_currently_updating = something_currently_updating or item["currently_updating"]

    logger.debug(u"Got items for collection_doc %s" %cid)

    return (collection_doc, something_currently_updating)

def clean_value_for_csv(value_to_store):
    try:
        value_to_store = value_to_store.encode("utf-8").strip()
    except AttributeError:
        pass
    return value_to_store

def make_csv_rows(items):
    header_metric_names = []
    for item in items:
        header_metric_names += item["metrics"].keys()
    header_metric_names = sorted(list(set(header_metric_names)))

    header_alias_names = ["title", "doi"]

    # make header row
    header_list = ["tiid"] + header_alias_names + header_metric_names
    ordered_fieldnames = OrderedDict([(col, None) for col in header_list])

    # body rows
    rows = []
    for item in items:
        ordered_fieldnames = OrderedDict()
        ordered_fieldnames["tiid"] = item["_id"]
        for alias_name in header_alias_names:
            try:
                if alias_name=="title":
                    ordered_fieldnames[alias_name] = clean_value_for_csv(item['aliases']['biblio'][0]['title'])
                else:
                    ordered_fieldnames[alias_name] = clean_value_for_csv(item['aliases'][alias_name][0])
            except (AttributeError, KeyError):
                ordered_fieldnames[alias_name] = ""
        for metric_name in header_metric_names:
            try:
                raw_value = item['metrics'][metric_name]['values']['raw']
                ordered_fieldnames[metric_name] = clean_value_for_csv(raw_value)
            except (AttributeError, KeyError):
                ordered_fieldnames[metric_name] = ""
        rows += [ordered_fieldnames]
    return(ordered_fieldnames, rows)

def make_csv_stream(items):
    (header, rows) = make_csv_rows(items)

    mystream = StringIO.StringIO()
    dw = csv.DictWriter(mystream, delimiter=',', dialect=csv.excel, fieldnames=header)
    dw.writeheader()
    for row in rows:
        dw.writerow(row)
    contents = mystream.getvalue()
    mystream.close()
    return contents

def get_metric_value_lists(items):
    (ordered_fieldnames, rows) = make_csv_rows(items)
    metric_values = {}
    for metric_name in ProviderFactory.get_all_metric_names():
        if metric_name in ordered_fieldnames:
            if metric_name in ["tiid", "title", "doi"]:
                pass
            else:
                values = [row[metric_name] for row in rows]
                values = [value if value else 0 for value in values]
                # treat "Yes" as 1 for normalizaations
                values = [1 if value=="Yes" else value for value in values]
                metric_values[metric_name] = sorted(values, reverse=True)
        else:
            metric_values[metric_name] = [0 for row in rows]
    return metric_values

def get_metric_values_of_reference_sets(items):
    metric_value_lists = get_metric_value_lists(items)
    metrics_to_normalize = metric_value_lists.keys()
    for key in metrics_to_normalize:
        if ("plosalm" in key):
            del metric_value_lists[key]
        elif not isinstance(metric_value_lists[key][0], (int, float)):
            del metric_value_lists[key]
    return metric_value_lists  

def get_normalization_confidence_interval_ranges(metric_value_lists, confidence_interval_table):
    matches = {}
    response = {}
    for metric_name in metric_value_lists:
        metric_values = sorted(metric_value_lists[metric_name], reverse=False)
        if (len(confidence_interval_table) != len(metric_values)):
            logger.error(u"Was expecting normalization set to be {expected_len} but it is {actual_len}. Not loading.".format(
                expected_len=len(confidence_interval_table), actual_len=len(metric_values) ))
        matches[metric_name] = defaultdict(list)
        num_normalization_points = len(metric_values)
        for i in range(num_normalization_points):
            matches[metric_name][metric_values[i]] += [[(i*100)/num_normalization_points, confidence_interval_table[i][0], confidence_interval_table[i][1]]]

        response[metric_name] = {}
        for metric_value in matches[metric_name]:
            lowers = [lower for (est, lower, upper) in matches[metric_name][metric_value]]
            uppers = [upper for (est, lower, upper) in matches[metric_name][metric_value]]

            estimates = [est for (est, lower, upper) in matches[metric_name][metric_value]]

            response[metric_name][metric_value] = {
                "CI95_lower": min(lowers),
                "CI95_upper": max(uppers),
                "estimate_upper":  max(estimates),
                "estimate_lower": min(estimates)
                }

            # If ties, check and see if next higher metric value already has an estimate.
            # If not, assign it the estimate of the top range of the tied values, so 
            # that the metric_value+1 isn't conservatively assigned percentiles for the 
            # tie of the value below it
            if len(lowers) > 1:
                if not (metric_value+1 in response[metric_name]):
                    response[metric_name][metric_value+1] = {
                        "CI95_lower": max(lowers),
                        "CI95_upper": max(uppers),
                        "estimate_upper":  max(estimates),
                        "estimate_lower": max(estimates)
                        }


        # add a value that is one larger than the biggest value, hack the lower 95 bound to be a bit higher
        # than the 99th percentile
        largest_metric_value = max(matches[metric_name].keys())
        onehundredth_percentile_value = largest_metric_value + 1
        final_entry_in_conf_table = confidence_interval_table[-1]
        final_entry_CI95_lower = final_entry_in_conf_table[0]
        response[metric_name][onehundredth_percentile_value] = {
                "CI95_lower": final_entry_CI95_lower+1,
                "CI95_upper": 100,
                "estimate_upper": 100,
                "estimate_lower": 100
                }

    return response


def build_all_reference_lookups(myredis, mydao):
    # for expediency, assuming all reference collections are this size
    # risky assumption, but run with it for now!
    size_of_reference_collections = 100
    confidence_interval_level = 0.95
    percentiles = range(100)

    confidence_interval_table = myredis.get_confidence_interval_table(size_of_reference_collections, confidence_interval_level)
    if not confidence_interval_table:
        table_return = calc_confidence_interval_table(size_of_reference_collections, 
                confidence_interval_level=confidence_interval_level, 
                percentiles=percentiles)
        confidence_interval_table = table_return["lookup_table"]
        myredis.set_confidence_interval_table(size_of_reference_collections, 
                                                confidence_interval_level, 
                                                confidence_interval_table)        
        #print(json.dumps(confidence_interval_table, indent=4))

    logger.info(u"querying for reference_set_rows")

    reference_set_rows = Collection.query.filter(Collection.refset_metadata != None).all()

    #res = mydao.db.view("reference-sets/reference-sets", descending=True, include_docs=False, limits=100)
    #logger.info(u"Number rows = " + str(len(res.rows)))
    reference_lookup_dict = {"article": defaultdict(dict), "dataset": defaultdict(dict), "software": defaultdict(dict)}
    reference_histogram_dict = {"article": defaultdict(dict), "dataset": defaultdict(dict), "software": defaultdict(dict)}

    # randomize rows so that multiple gunicorn instances hit them in different orders
    randomized_rows = reference_set_rows
    random.shuffle(randomized_rows)
    if randomized_rows:
        for row in randomized_rows:
            try:
                #(cid, title) = row.key
                #refset_metadata = row.value
                cid = row.cid
                title = row.title
                refset_metadata = row.refset_metadata
                genre = refset_metadata["genre"]
                year = refset_metadata["year"]
                refset_name = refset_metadata["name"]
                refset_version = refset_metadata["version"]
                if refset_version < 0.1:
                    logger.error(u"Refset version too low for '%s', not loading its normalizations" %str(row.key))
                    continue
            except ValueError:
                logger.error(u"Normalization '%s' not formatted as expected, not loading its normalizations" %str(row.key))
                continue

            histogram = myredis.get_reference_histogram_dict(genre, refset_name, year)
            lookup = myredis.get_reference_lookup_dict(genre, refset_name, year)
            
            if histogram and lookup:
                logger.info(u"Loaded successfully from cache")
                reference_histogram_dict[genre][refset_name][year] = histogram
                reference_lookup_dict[genre][refset_name][year] = lookup
            else:
                logger.info(u"Not found in cache, so now building from items")
                if refset_name:
                    try:
                        # send it without reference sets because we are trying to load the reference sets here!
                        (coll_with_items, is_updating) = get_collection_with_items_for_client(cid, None, myredis, mydao)
                    except (LookupError, AttributeError):       
                        raise #not found

                    logger.info(u"Loading normalizations for %s" %coll_with_items["title"])

                    # hack for now to get big collections
                    normalization_numbers = get_metric_values_of_reference_sets(coll_with_items["items"])
                    reference_histogram_dict[genre][refset_name][year] = normalization_numbers

                    reference_lookup = get_normalization_confidence_interval_ranges(normalization_numbers, confidence_interval_table)
                    reference_lookup_dict[genre][refset_name][year] = reference_lookup

                    # save to redis
                    myredis.set_reference_histogram_dict(genre, refset_name, year, normalization_numbers)
                    myredis.set_reference_lookup_dict(genre, refset_name, year, reference_lookup)

    return(reference_lookup_dict, reference_histogram_dict)

# from http://userpages.umbc.edu/~rcampbel/Computers/Python/probstat.html
# also called binomial coefficient
def choose(n, k):
   accum = 1
   for m in range(1,k+1):
      accum = accum*(n-k+m)/m
   return accum

# from formula at http://www.milefoot.com/math/stat/ci-medians.htm
def probPercentile(p, n, i):
    prob = choose(n, i) * p**i * (1-p)**(n-i)
    return(prob)

def calc_confidence_interval_table(
        n,                              # sample size
        confidence_interval_level=0.95, # confidence interval threshold
        percentiles=range(8, 97, 2) # percentiles to calculate.  Median==50.
        ):
    percentile_lower_bound = [None for i in range(n)]
    percentile_upper_bound = [None for i in range(n)]
    limits = {}
    range_sum = {}
    for percentile in percentiles:
        #print(percentile)
        order_statistic_probs = {}
        for i in range(0, n+1):
            order_statistic_probs[i] = probPercentile(percentile*0.01, n, i)
        #print(order_statistic_probs)
        max_order_statistic_prob = [(i, order_statistic_probs[i]) 
            for i in order_statistic_probs 
                if order_statistic_probs[i]==max(order_statistic_probs.values())]
        lower_max_order_statistic_prob = min([i for (i, val) in max_order_statistic_prob])
        upper_max_order_statistic_prob = max([i for (i, val) in max_order_statistic_prob])
        for i in range(0, n/2):
            myrange = range(max(0, (lower_max_order_statistic_prob-i)), 
                            min(len(order_statistic_probs), (1+upper_max_order_statistic_prob+i)))
            range_sum[percentile] = sum([order_statistic_probs[j] for j in myrange])
            if range_sum[percentile] >= confidence_interval_level:
                #print range_sum[percentile]
                limits[percentile] = (min(myrange), 1+max(myrange))
                #print "from index %i to (but not including) %i" %(limits[percentile][0], limits[percentile][1])
                for i in myrange[0:-1]:
                    percentile_upper_bound[i] = percentile
                    if not percentile_lower_bound[i]:
                        percentile_lower_bound[i] = percentile
                break

    #for i in range(n-1, 0, -1):
    #    print (i+0.0)/n, ps_min[i], ps_max[i]
    return({"range_sum":range_sum, 
            "limits":limits, 
            "lookup_table":zip(percentile_lower_bound, percentile_upper_bound)})







########NEW FILE########
__FILENAME__ = default_settings
##################################################################
# Total Impact default configuration settings
# ALL KEYS HAVE TO BE UPPERCASE TO BE STORED IN APP SETTINGS
#

USER_AGENT = "ImpactStory/0.4.0" # User-Agent string to use on HTTP requests
VERSION = "cristhian" # version
PROXY = "" # used with  providers-test-proxy.py script in the extras directory
CACHE_ENABLED = True # Memcache server enabled

# List of desired providers and their configuration files
# Alias methods will be called in the order of this list
PROVIDERS = [
    # this is up here because it can produce dois
    ("pubmed", {}),

    # best biblio providers go here, in order with best first
    ("arxiv", {}),
    ("crossref", {}),
    ("dryad", {}),            
    ("figshare", {}),            
    ("github", {}),
    ("slideshare", {}),
    ("vimeo", {}),
    ("youtube", {}),

    # if-need-be biblio providers go here, in order with best first
    ("mendeley", {}),
    ("bibtex", {}),
    ("webpage", {}),

    # don't-have-biblio providers go here, alphabetical order
    ("altmetric_com", {}),    
    ("citeulike", {}),   
    ("delicious", {}),   
    ("plosalm", {}),
    ("plossearch", {}),
    ("scopus", {}),
    ("wikipedia", {}),
]


########NEW FILE########
__FILENAME__ = extra_schema
from totalimpact import views, db

def create_view_min_biblio():
    result = db.session.execute("""create or replace view min_biblio as (
                select 
                    a.tiid, 
                    a.provider, 
                    a.biblio_value as title, 
                    b.biblio_value as authors, 
                    c.biblio_value as journal, 
                    a.collected_date
                from biblio a
                join biblio b using (tiid, provider)
                join biblio c using (tiid, provider)
                where 
                a.biblio_name = 'title'
                and b.biblio_name = 'authors'
                and c.biblio_name = 'journal'
                )""")
    db.session.commit()


def create_doaj_table():
    doaj_setup_sql = """
        CREATE TABLE if not exists doaj (
            title varchar(250),
            publisher varchar(250),
            issn varchar(25) NOT NULL,
            eissn varchar(25),
            "cc license" varchar(25),
            PRIMARY KEY (issn)
        );"""    
    result = db.session.execute(doaj_setup_sql)
    db.session.commit()

def create_doaj_view():
    doaj_setup_sql = """
        CREATE or replace VIEW doaj_issn_lookup (issn) AS          
            SELECT replace((doaj.issn)::text, '-'::text, ''::text) AS issn FROM doaj
                UNION
            SELECT replace((doaj.eissn)::text, '-'::text, ''::text) AS issn FROM doaj;
        """    
    result = db.session.execute(doaj_setup_sql)
    db.session.commit()

create_view_min_biblio()    
create_doaj_table()    
create_doaj_view()    
########NEW FILE########
__FILENAME__ = fakes
import os, requests, json, datetime, time, sys, re, random, string
from time import sleep
from requests import Timeout
import logging

logger = logging.getLogger("ti.fakes")
logger.setLevel(logging.INFO)

requests_log = logging.getLogger("requests").setLevel(
    logging.WARNING) # Requests' logging is too noisy


# setup external services
webapp_url = "http://" + os.getenv("WEBAPP_ROOT")
api_url = "http://" + os.getenv("API_ROOT")

''' Test classes
*****************************************************************************'''

class Importer:
    '''Emulates a single importer on the create_collection page;

    feed it a provider name like "github" at instantiation, then run run a query
    like jasonpriem to get all the aliases associated with that account
    '''
    query = None

    def __init__(self, provider_name):
        self.provider_name = provider_name

    def get_aliases(self, query):
        query_url = "{api_url}/provider/{provider_name}/memberitems/{query}?method=sync".format(
            api_url=api_url,
            provider_name=self.provider_name,
            query=query
        )
        start = time.time()
        logger.info(u"getting aliases from the {provider} importer, using url '{url}'".format(
                provider=self.provider_name, url=query_url))
        r = requests.get(query_url)

        try:
            response = json.loads(r.text)
            aliases = response["memberitems"]
            logger.debug(u"got some aliases from the http call: " + str(aliases))
        except ValueError:
            logger.warning(u"{provider} importer returned no json for {query}".format(
                    provider=self.provider_name, query="query"))
            aliases = []

        # annoyingly, some providers return lists-as-IDs, which must be joined with a comma
        aliases = [(namespace, id) if isinstance(id, str)
                   else (namespace, ",".join(id)) for namespace, id in aliases]

        logger.info(u"{provider} importer got {num_aliases} aliases with username '{q}' in {elapsed} seconds.".format(
                provider=self.provider_name, num_aliases=len(aliases), q=query, elapsed=round(time.time() - start, 2)))

        return aliases


class ReportPage:
    def __init__(self, collection_id):
        self.collection_id = collection_id
        start = time.time()
        logger.info(u"loading the report page for collection '{collection_id}'.".format(
                collection_id=collection_id))
        request_url = "{webapp_url}/collection/{collection_id}".format(
            webapp_url=webapp_url,
            collection_id=collection_id
        )
        resp = requests.get(request_url)
        if resp.status_code == 200:
            self.collectionId = self._get_collectionId(resp.text)
            elapsed = time.time() - start
            logger.info(u"loaded the report page for '{collection_id}' in {elapsed} seconds.".format(
                    collection_id=collection_id, elapsed=elapsed))
        else:
            logger.warning(u"report page for '{collection_id}' failed to load! ({url})".format(
                    collection_id=collection_id, url=request_url))


    def _get_collectionId(self, text):
        """gets id of the collection to poll. in the real report page, this is done
        via a 'collectionId' javascript var that's placed there when the view constructs
        the page. this method parses the page to get them...it's brittle, though,
        since if the report page changes this breaks."""

        m = re.search('var reportId = "([^"]+)"', text)
        collectionId = m.group(1)
        return collectionId


    def poll(self, max_time=60):
        logger.info(u"polling collection '{collection_id}'".format(
                collection_id=self.collection_id))

        still_updating = True
        tries = 0
        start = time.time()
        while still_updating:
            url = api_url + "/collection/" + self.collectionId
            resp = requests.get(url, config={'verbose': None})
            try:
                items = json.loads(resp.text)["items"]
            except ValueError:
                items = []
                logger.warning(u"get '{url}' returned no json, only '{resp}') ".format(
                        url=url, resp=resp.text))

            tries += 1

            currently_updating_flags = [True for item in items if
                                        item["currently_updating"]]
            num_currently_updating = len(currently_updating_flags)
            num_finished_updating = len(items) - num_currently_updating

            logger.info(u"{num_done} of {num_total} items done updating after {tries} requests.".format(
                    num_done=num_finished_updating, num_total=len(items), tries=tries))
            logger.debug(u"got these items back: " + str(items))

            elapsed = time.time() - start
            if resp.status_code == 200:
                logger.info(u"collection '{id}' with {num_items} items finished updating in {elapsed} seconds.".format(
                        id=self.collection_id, num_items=len(items), elapsed=round(elapsed, 2)))
                return True
            elif elapsed > max_time:
                raise Exception(
                    "max polling time ({max} secs) exceeded for collection {id}. These items didn't update: {item_ids}".format(
                        max=max_time,
                        id=self.collection_id,
                        item_ids=", ".join([item["_id"] for item in items if
                                            item["currently_updating"]])))
                return False

            sleep(0.5)


class CreateCollectionPage:
    def __init__(self):
        self.reload()

    def reload(self):
        start = time.time()
        logger.info(u"loading the create-collection page")
        resp = requests.get(webapp_url + "/create")
        if resp.status_code == 200:
            elapsed = time.time() - start
            logger.info(u"loaded the create-collection page in {elapsed} seconds.".format(
                elapsed=elapsed))
        else:
            logger.warning(u"create-collection page failed to load!")
        self.aliases = []
        self.collection_name = "My collection"

    def set_collection_name(self, collection_name):
        self.collection_name = collection_name

    def enter_aliases_directly(self, aliases):
        self.aliases = self.aliases + aliases
        return self.aliases

    def get_aliases_with_importers(self, provider_name, query):
        importer = Importer(provider_name)
        aliases_from_this_importer = importer.get_aliases(query)
        self.aliases = self.aliases + aliases_from_this_importer
        return self.aliases

    def press_go_button(self):
        logger.info(u"user has pressed the 'go' button on the create-collection page.")
        if len(self.aliases) == 0:
            raise ValueError("Trying to create a collection with no aliases.")

        collection_id = self._create_collection()
        report_page = ReportPage(collection_id)
        report_page.poll()
        return collection_id


    def _create_collection(self):
        start = time.time()
        url = api_url + "/collection"
        collection_name = "[ti test] " + self.collection_name

        logger.info(u"creating collection with {num_aliases} tiids".format(
            num_aliases=len(self.aliases)
        ))
        logger.debug(u"creating collection with these aliases: " + str(self.aliases))

        resp = requests.post(
                url,
                data=json.dumps({
                    "aliases": self.aliases,
                    "title": collection_name
                }),
                headers={'Content-type': 'application/json'}
            )
        collection_id = json.loads(resp.text)["collection"]["_id"]

        logger.info(u"created collection '{id}' with {num_items} items in {elapsed} seconds.".format(
                id=collection_id, num_items=len(self.aliases), elapsed=round(time.time() - start, 2)))

        return collection_id

    def clean_db(self):
        pass


class IdSampler(object):
    def get_dois(self, num=1):
        start = time.time()
        dois = []
        url = "http://random.labs.crossref.org/dois?from=2000&count=" + str(num)
        logger.info(u"getting {num} random dois with IdSampler, using {url}".format(
                num=num, url=url))
        try:
            r = requests.get(url, timeout=10)
        except Timeout:
            logger.warning(u"the random doi service isn't working right now (timed out); sending back an empty list.")
            return dois

        if r.status_code == 200:
            try:
                dois = json.loads(r.text)
                logger.info(u"IdSampler got {count} random dois back in {elapsed} seconds".format(
                        count=len(dois), elapsed=round(time.time() - start, 2)))
                logger.debug(u"IdSampler got these dois back: " + str(dois))
            except ValueError:
                pass
        if not dois:
            logger.warning(u"the random doi service isn't working right now (got error code); sending back an empty list.")

        return dois

    def get_github_username(self):
        start = time.time()
        db_url = "http://total-impact.cloudant.com/github_usernames"
        rand_hex_string = hex(random.getrandbits(128))[
                          2:-1] # courtesy http://stackoverflow.com/a/976607/226013
        req_url = db_url + '/_all_docs?include_docs=true&limit=1&startkey="{startkey}"'.format(
            startkey=rand_hex_string
        )
        logger.info(u"getting a random github username with IdSampler, using {url}".format(
                url=req_url))
        r = requests.get(req_url)
        json_resp = json.loads(r.text)

        username = json_resp["rows"][0]["doc"]["actor"]
        logger.info(u"IdSampler got random github username '{username}' in {elapsed} seconds".format(
                username=username, elapsed=round(time.time() - start, 2)))

        return username


########NEW FILE########
__FILENAME__ = incoming_email
import datetime, json, re
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm.exc import FlushError
from totalimpact import db

import logging
logger = logging.getLogger('ti.incoming_email')

def save_incoming_email(payload):
    email = IncomingEmail(payload)
    email.log_if_google_scholar_notification_confirmation()
    email.log_if_google_scholar_new_articles()

    db.session.add(email)
    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in add_items_to_collection_object for {cid}, rolling back.  Message: {message}".format(
            cid=cid, 
            message=e.message))        

    return email


class IncomingEmail(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    created = db.Column(db.DateTime())
    payload = db.Column(db.Text)

    def __init__(self, payload):
        self.payload = json.dumps(payload)
        self.created = datetime.datetime.utcnow()
        super(IncomingEmail, self).__init__()

    @property
    def subject(self):
        payload = json.loads(self.payload)
        return payload["headers"]["Subject"]

    @property
    def email_body(self):
        payload = json.loads(self.payload)
        return payload["plain"]

    def __repr__(self):
        return '<IncomingEmail {id}, {created}, {payload_start}>'.format(
            id=self.id, 
            created=self.created, 
            payload_start=self.payload[0:100])

    def log_if_google_scholar_notification_confirmation(self):
        GOOGLE_SCHOLAR_CONFIRM_PATTERN = re.compile("""for the query:\nNew articles in (?P<name>.*)'s profile\n\nClick to confirm this request:\n(?P<url>.*)\n\n""")
        name = None
        url = None
        try:
            match = GOOGLE_SCHOLAR_CONFIRM_PATTERN.search(self.email_body)
            if match:
                url = match.group("url")
                name = match.group("name")
                logger.info(u"Google Scholar notification confirmation for {name} is at {url}".format(
                    name=name, url=url))
        except (KeyError, TypeError):
            pass
        return(name, url)

    def log_if_google_scholar_new_articles(self):
        GOOGLE_SCHOLAR_NEW_ARTICLES_PATTERN = re.compile("""Scholar Alert - (?P<name>.*) - new articles""")
        name = None
        try:
            match = GOOGLE_SCHOLAR_NEW_ARTICLES_PATTERN.search(self.subject)
            if match:
                name = match.group("name")
                logger.info(u"Just received Google Scholar alert: new articles for {name}, saved at {id}".format(
                    name=name, 
                    id=self.id))
        except (KeyError, TypeError):
            pass
        return(name)


########NEW FILE########
__FILENAME__ = item
from werkzeug import generate_password_hash, check_password_hash
import shortuuid, datetime, hashlib, threading, json, time, copy, re
from collections import defaultdict

from totalimpact.providers.provider import ProviderFactory
from totalimpact.providers.provider import ProviderTimeout, ProviderServerError
from totalimpact.providers.provider import normalize_alias
from totalimpact import unicode_helpers

from totalimpact import default_settings
from totalimpact.utils import Retry

from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm.exc import FlushError
from sqlalchemy.ext.hybrid import hybrid_property, hybrid_method
from sqlalchemy.sql import text    

from totalimpact import json_sqlalchemy, tiredis
from totalimpact import db


# Master lock to ensure that only a single thread can write
# to the DB at one time to avoid document conflicts

import logging
logger = logging.getLogger('ti.item')

# print out extra debugging
#logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)


all_static_meta = ProviderFactory.get_all_static_meta()



class NotAuthenticatedError(Exception):
    pass


def delete_item(tiid):
    item_object = Item.from_tiid(tiid)
    db.session.delete(item_object)
    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in delete_item for {tiid}, rolling back.  Message: {message}".format(
            tiid=tiid, 
            message=e.message))   


def create_metric_objects(old_style_metric_dict):
    new_metric_objects = []

    for full_metric_name in old_style_metric_dict:
        (provider, metric_name) = full_metric_name.split(":")
        metric_details = old_style_metric_dict[full_metric_name]
        new_style_metric_dict = {
            "metric_name": metric_name, 
            "provider": provider, 
            "drilldown_url": metric_details["provenance_url"]
        }

        for collected_date in metric_details["values"]["raw_history"]:
            new_style_metric_dict["collected_date"] = collected_date
            new_style_metric_dict["raw_value"] = metric_details["values"]["raw_history"][collected_date]
            metric_object = Metric(**new_style_metric_dict)
            new_metric_objects += [metric_object]    

    return new_metric_objects


def create_biblio_objects(list_of_old_style_biblio_dicts, provider=None, collected_date=datetime.datetime.utcnow()):
    new_biblio_objects = []

    provider_number = 0
    for biblio_dict in list_of_old_style_biblio_dicts:
        if not provider:
            provider_number += 1
            provider = "unknown" + str(provider_number)
        for biblio_name in biblio_dict:
            biblio_object = Biblio(biblio_name=biblio_name, 
                    biblio_value=biblio_dict[biblio_name], 
                    provider=provider, 
                    collected_date=collected_date)
            new_biblio_objects += [biblio_object]

    return new_biblio_objects


def create_alias_objects(old_style_alias_dict, collected_date=datetime.datetime.utcnow()):
    new_alias_objects = []
    alias_tuples = alias_tuples_from_dict(old_style_alias_dict)   
    for alias_tuple in alias_tuples:
        (namespace, nid) = alias_tuple
        if nid and namespace and (namespace != "biblio"):
            new_alias_objects += [Alias(alias_tuple=alias_tuple, collected_date=collected_date)]
    return new_alias_objects


def create_objects_from_item_doc(item_doc, skip_if_exists=False, commit=True):
    tiid = item_doc["_id"]

    # logger.debug(u"in create_objects_from_item_doc for {tiid}".format(
    #     tiid=item_doc["_id"]))        

    new_item_object = Item.from_tiid(item_doc["_id"])
    if new_item_object and skip_if_exists:
        return new_item_object
    else:
        new_item_object = Item.create_from_old_doc(item_doc)
    db.session.add(new_item_object)

    alias_dict = item_doc["aliases"]
    new_alias_objects = create_alias_objects(alias_dict, item_doc["last_modified"])
    new_item_object.aliases = new_alias_objects

    # biblio within aliases, skip just the biblio section
    if "biblio" in alias_dict:
        new_biblio_objects = create_biblio_objects(alias_dict["biblio"], provider=None, collected_date=item_doc["last_modified"]) 
        new_item_object.biblios = new_biblio_objects

    new_metric_objects = None
    if "metrics" in item_doc:
        new_metric_objects = create_metric_objects(item_doc["metrics"]) 
        for metric in new_metric_objects:
            metric.tiid = item_doc["_id"]
            db.session.add(metric)

    if commit:
        try:
            db.session.commit()
        except (IntegrityError, FlushError) as e:
            db.session.rollback()
            logger.warning(u"Fails Integrity check in create_objects_from_item_doc for {tiid}, rolling back.  Message: {message}".format(
                tiid=tiid, 
                message=e.message))   

    # have to set it because after the commit the metrics aren't set any more
    if new_metric_objects:
        new_item_object.metrics = new_metric_objects

    return new_item_object



class Metric(db.Model):
    tiid = db.Column(db.Text, db.ForeignKey('item.tiid'), primary_key=True, index=True)
    provider = db.Column(db.Text, primary_key=True)
    metric_name = db.Column(db.Text, primary_key=True)
    collected_date = db.Column(db.DateTime(), primary_key=True)
    raw_value = db.Column(json_sqlalchemy.JSONAlchemy(db.Text))
    drilldown_url = db.Column(db.Text)
    query_type = None

    def __init__(self, **kwargs):
        if "collected_date" in kwargs:
            self.collected_date = kwargs["collected_date"]
        else:
            self.collected_date = datetime.datetime.utcnow()
        if "query_type" in kwargs:
            self.query_type = kwargs["query_type"]
        super(Metric, self).__init__(**kwargs)

    @property
    def fully_qualified_name(self):
        return "{provider}:{metric_name}".format(
            provider=self.provider, metric_name=self.metric_name)

    def __repr__(self):
        return '<Metric {tiid} {provider}:{metric_name}={raw_value} on {collected_date} via {query_type}>'.format(
            provider=self.provider, 
            metric_name=self.metric_name, 
            raw_value=self.raw_value, 
            collected_date=self.collected_date, 
            query_type=self.query_type,
            tiid=self.tiid)


class Biblio(db.Model):
    tiid = db.Column(db.Text, db.ForeignKey('item.tiid'), primary_key=True, index=True)
    provider = db.Column(db.Text, primary_key=True)
    biblio_name = db.Column(db.Text, primary_key=True)
    biblio_value = db.Column(json_sqlalchemy.JSONAlchemy(db.Text))
    collected_date = db.Column(db.DateTime())

    def __init__(self, **kwargs):
        # logger.debug(u"new Biblio {kwargs}".format(
        #     kwargs=kwargs))                

        if "collected_date" in kwargs:
            self.collected_date = kwargs["collected_date"]
        else:   
            self.collected_date = datetime.datetime.utcnow()
        if not "provider" in kwargs:
            self.provider = "unknown"
           
        super(Biblio, self).__init__(**kwargs)

    def __repr__(self):
        return '<Biblio {biblio_name}, {item}>'.format(
            biblio_name=self.biblio_name, 
            item=self.item)

    @classmethod
    def filter_by_tiid(cls, tiid):
        response = cls.query.filter_by(tiid=tiid).all()
        return response

    @classmethod
    def as_dict_by_tiid(cls, tiid):
        response = {}
        biblio_elements = cls.query.filter_by(tiid=tiid).all()
        for biblio in biblio_elements:
            response[biblio.biblio_name] = biblio.biblio_value
        return response


class Alias(db.Model):
    tiid = db.Column(db.Text, db.ForeignKey('item.tiid'), primary_key=True, index=True)
    namespace = db.Column(db.Text, primary_key=True)
    nid = db.Column(db.Text, primary_key=True)
    collected_date = db.Column(db.DateTime())

    def __init__(self, **kwargs):
        # logger.debug(u"new Alias {kwargs}".format(
        #     kwargs=kwargs))                

        if "alias_tuple" in kwargs:
            alias_tuple = canonical_alias_tuple(kwargs["alias_tuple"])
            (namespace, nid) = alias_tuple
            self.namespace = namespace
            self.nid = nid                
        if "collected_date" in kwargs:
            self.collected_date = kwargs["collected_date"]
        else:   
            self.collected_date = datetime.datetime.utcnow()

        super(Alias, self).__init__(**kwargs)
        
    @hybrid_property
    def alias_tuple(self):
        return ((self.namespace, self.nid))

    @alias_tuple.setter
    def alias_tuple(self, alias_tuple):
        try:
            (namespace, nid) = alias_tuple
        except ValueError:
            logger.debug("could not separate alias tuple {alias_tuple}".format(
                alias_tuple=alias_tuple))
            raise
        self.namespace = namespace
        self.nid = nid        

    def __repr__(self):
        return '<Alias {item}, {alias_tuple}>'.format(
            item=self.item,
            alias_tuple=self.alias_tuple)

    @classmethod
    def filter_by_alias(cls, alias_tuple):
        alias_tuple = canonical_alias_tuple(alias_tuple)
        (namespace, nid) = alias_tuple
        response = cls.query.filter_by(namespace=namespace, nid=nid)
        return response


class Item(db.Model):
    tiid = db.Column(db.Text, primary_key=True)
    created = db.Column(db.DateTime())
    last_modified = db.Column(db.DateTime())
    last_update_run = db.Column(db.DateTime())
    aliases = db.relationship('Alias', lazy='subquery', cascade="all, delete-orphan",
        backref=db.backref("item", lazy="subquery"))
    biblios = db.relationship('Biblio', lazy='subquery', cascade="all, delete-orphan",
        backref=db.backref("item", lazy="subquery"))
    metrics = db.relationship('Metric', lazy='noload', cascade="all, delete-orphan",
        backref=db.backref("item", lazy="noload"))
    metrics_query = db.relationship('Metric', lazy='dynamic')

    def __init__(self, **kwargs):
        # logger.debug(u"new Item {kwargs}".format(
        #     kwargs=kwargs))                

        if "tiid" in kwargs:
            self.tiid = kwargs["tiid"]
        else:
            shortuuid.set_alphabet('abcdefghijklmnopqrstuvwxyz1234567890')
            self.tiid = shortuuid.uuid()[0:24]
       
        now = datetime.datetime.utcnow()
        if "created" in kwargs:
            self.created = kwargs["created"]
        else:   
            self.created = now
        if "last_modified" in kwargs:
            self.last_modified = kwargs["last_modified"]
        else:   
            self.last_modified = now
        if "last_update_run" in kwargs:
            self.last_update_run = kwargs["last_update_run"]
        else:   
            self.last_update_run = now

        super(Item, self).__init__(**kwargs)

    def __repr__(self):
        return '<Item {tiid}>'.format(
            tiid=self.tiid)

    @classmethod
    def from_tiid(cls, tiid, with_metrics=True):
        item = cls.query.get(tiid)
        if not item:
            return None
        if with_metrics:
            item.metrics = item.metrics_query.all()
        return item

    @property
    def alias_tuples(self):
        return [alias.alias_tuple for alias in self.aliases]

    @property
    def biblio_dict(self):
        response = {}
        for biblio in self.biblios:
            response[biblio.biblio_name] = biblio.biblio_value
        return response

    @property
    def biblio_dicts_per_provider(self):
        response = defaultdict(dict)
        for biblio in self.biblios:
            response[biblio.provider][biblio.biblio_name] = biblio.biblio_value
        return response        

    @property
    def publication_date(self):
        publication_date = None
        for biblio in self.biblios:
            if biblio.biblio_name == "date":
                publication_date = biblio.biblio_value
                continue
            if (biblio.biblio_name == "year") and biblio.biblio_value:
                publication_date = datetime.datetime(int(biblio.biblio_value), 12, 31)

        if not publication_date:
            publication_date = self.created
        return publication_date.isoformat()

    @hybrid_method
    def published_before(self, mydate):
        return (self.publication_date < mydate.isoformat())

    def has_user_provided_biblio(self):
        return any([biblio.provider=='user_provided' for biblio in self.biblios])

    def has_free_fulltext_url(self):
        return any([biblio.biblio_name=='free_fulltext_url' for biblio in self.biblios])

    @classmethod
    def create_from_old_doc(cls, doc):
        # logger.debug(u"in create_from_old_doc for {tiid}".format(
        #     tiid=doc["_id"]))

        doc_copy = copy.deepcopy(doc)
        doc_copy["tiid"] = doc_copy["_id"]
        for key in doc_copy.keys():
            if key not in ["tiid", "created", "last_modified", "last_update_run"]:
                del doc_copy[key]
        new_item_object = Item(**doc_copy)

        return new_item_object

    @property
    def biblio_dict(self):
        biblio_dict = {}
        for biblio_obj in self.biblios:
            if (biblio_obj.biblio_name not in biblio_dict) or (biblio_obj.provider == "user_provided"):
                    biblio_dict[biblio_obj.biblio_name] = biblio_obj.biblio_value    
        return biblio_dict

    def as_old_doc(self):
        # logger.debug(u"in as_old_doc for {tiid}".format(
        #     tiid=self.tiid))

        item_doc = {}
        item_doc["_id"] = self.tiid
        item_doc["last_modified"] = self.last_modified.isoformat()
        item_doc["created"] = self.created.isoformat()
        item_doc["last_update_run"] = self.last_update_run.isoformat()
        item_doc["type"] = "item"

        item_doc["biblio"] = self.biblio_dict

        item_doc["aliases"] = alias_dict_from_tuples(self.alias_tuples)
        if item_doc["biblio"]:
            item_doc["aliases"]["biblio"] = [item_doc["biblio"]]

        item_doc["metrics"] = {}
        for metric in self.metrics:
            metric_name = metric.provider + ":" + metric.metric_name
            metrics_method_response = (metric.raw_value, metric.drilldown_url)
            item_doc = add_metrics_data(metric_name, metrics_method_response, item_doc, metric.collected_date.isoformat())

        for full_metric_name in item_doc["metrics"]:
            most_recent_date_so_far = "1900"
            for this_date in item_doc["metrics"][full_metric_name]["values"]["raw_history"]:
                if this_date > most_recent_date_so_far:
                    most_recent_date_so_far = this_date
                    item_doc["metrics"][full_metric_name]["values"]["raw"] = item_doc["metrics"][full_metric_name]["values"]["raw_history"][this_date]

        return item_doc


def largest_value_that_is_less_than_or_equal_to(target, collection):
    collection_as_numbers = [(int(i), i) for i in collection if int(i) <= target]
    if collection_as_numbers:
        response = max(collection_as_numbers)[1]
    else:
        # the value is lower than anything we've seen before, so return lowest value
        response = min([(int(i), i) for i in collection])[1]
    return response


def clean_id(nid):
    try:
        nid = nid.strip(' "')
        nid = unicode_helpers.remove_nonprinting_characters(nid)
    except (TypeError, AttributeError):
        #isn't a string.  That's ok, might be biblio
        pass
    return(nid)

def get_item(tiid, myrefsets, myredis):
    item_obj = Item.from_tiid(tiid)

    if not item_obj:
        return None
    try:
        item_for_client = build_item_for_client(item_obj, myrefsets, myredis)
    except Exception, e:
        item_for_client = None
        logger.error(u"Exception %s: Skipping item, unable to build %s, %s" % (e.__repr__(), tiid, str(item_for_client)))
    return item_for_client



def diff_for_dict_metrics(previous_json, current_json):
    previous = json.loads(previous_json)
    current = json.loads(current_json)
    diff = []
    min_value_previous = min([entry["value"] for entry in previous])
    previous_dict = dict((entry["name"], entry["value"]) for entry in previous)
    for entry in current:
        if entry["name"] in previous_dict:
            previous_value = previous_dict[entry["name"]]
        else:
            previous_value = min_value_previous
        diff += [{"name": entry["name"], "value": entry["value"] - previous_value}]
    if max([entry["value"] for entry in diff]) == 0:
        return None
    else:
        return diff


def build_item_for_client(item_metrics_dict, myrefsets, myredis):
    item_obj = item_metrics_dict["item_obj"]
    metrics_summaries = item_metrics_dict["metrics_summaries"]
    item = item_obj.as_old_doc()

    # logger.debug(u"in build_item_for_client {tiid}".format(
    #     tiid=item["_id"]))

    try:
        (genre, host) = decide_genre(item['aliases'])
        item["biblio"]['genre'] = genre
    except (KeyError, TypeError):
        logger.error(u"Skipping item, unable to lookup aliases or biblio in %s" % str(item))
        return None

    try:
        if "authors" in item["biblio"]:
            del item["biblio"]["authors_literal"]
    except (KeyError, TypeError):
        pass    

    metrics = defaultdict(dict)

    for fully_qualified_metric_name in metrics_summaries:
        try:
            most_recent_metric_obj = metrics_summaries[fully_qualified_metric_name]["most_recent"]
            metric_name = fully_qualified_metric_name
            metrics[metric_name]["provenance_url"] = most_recent_metric_obj.drilldown_url
        except (KeyError, ValueError, AttributeError, TypeError):
            metric_name = None

        if metric_name and (metric_name in all_static_meta.keys()):  # make sure we still support this metrics type
            # add static data

            metrics[metric_name]["static_meta"] = all_static_meta[metric_name]  

            if most_recent_metric_obj:
                metrics[metric_name]["historical_values"] = {}
                raw = as_int_or_float_if_possible(most_recent_metric_obj.raw_value)
                if isinstance(raw, basestring):
                    try:
                        raw = json.loads(raw)
                    except ValueError:
                        pass

                metrics[metric_name]["historical_values"]["current"] = {
                        "collected_date": most_recent_metric_obj.collected_date.isoformat(),
                        "raw": raw
                        }

                metrics[metric_name]["values"] = {"raw": raw}
                earlier_metric_obj = None
                raw_diff = None

                try:
                    earlier_metric_obj = metrics_summaries[fully_qualified_metric_name]["7_days_ago"]
                    earlier_metric_raw = as_int_or_float_if_possible(earlier_metric_obj.raw_value)
                    metrics[metric_name]["historical_values"]["previous"] = {
                            "collected_date": earlier_metric_obj.collected_date.isoformat(),
                            "raw": earlier_metric_raw
                            }
                    raw_diff = raw - earlier_metric_raw
                except (KeyError, ValueError, AttributeError, TypeError):
                    # try:
                    #     raw_diff = diff_for_dict_metrics(earlier_metric_obj.raw_value, most_recent_metric_obj.raw_value)
                    # except (KeyError, ValueError, AttributeError, TypeError):
                    #     pass
                    metrics[metric_name]["historical_values"]["previous"] = {
                            "collected_date": None,
                            "raw": None
                            }

                try:
                    # need to round the dates because python difference returns number of full days between
                    rounded_recent_date = most_recent_metric_obj.collected_date.replace(hour = 0, minute = 0, second = 0, microsecond = 0)
                    rounded_earlier_date = earlier_metric_obj.collected_date.replace(hour = 0, minute = 0, second = 0, microsecond = 0)

                    raw_diff_days = (rounded_recent_date - rounded_earlier_date).days
                except (KeyError, ValueError, AttributeError, TypeError):
                    item_created_date = item_obj.created.replace(hour = 0, minute = 0, second = 0, microsecond = 0)
                    raw_diff_days = (rounded_recent_date - item_created_date).days

                    if raw_diff_days > 7:
                        raw_diff = raw
                        raw_diff_days = 7.2  # special value, assumes that things are updated once a week and this is first metric

                metrics[metric_name]["historical_values"]["diff"] = {
                    "days": raw_diff_days,
                    "raw": raw_diff
                    }
    
                try:
                    # add normalization values
                    # need year to calculate normalization below
                    year = int(item["biblio"]["year"])
                    if year < 2002:
                        year = 2002
                    normalized_values = get_normalized_values(genre, host, year, metric_name, raw, myrefsets)
                    metrics[metric_name]["values"].update(normalized_values)
                except (KeyError, ValueError, AttributeError):
                    #logger.error(u"No good year in biblio for item {tiid}, no normalization".format(
                    #    tiid=item["_id"]))
                    pass


    # ditch metrics we don't have static_meta for:

    item["metrics"] = {k:v for k, v in metrics.iteritems() if "static_meta" in v}
    item["currently_updating"] = is_currently_updating(item["_id"], myredis)

    return item

def as_int_or_float_if_possible(input_value):
    value = input_value
    try:
        value = int(input_value)
    except (ValueError, TypeError):
        try:
            value = float(input_value)
        except (ValueError, TypeError):
            pass
    return(value)




def add_metric_to_item_object(full_metric_name, metrics_method_response, item_obj):
    tiid = item_obj.tiid

    (metric_value, provenance_url) = metrics_method_response
    (provider, metric_name) = full_metric_name.split(":")

    new_style_metric_dict = {
        "tiid": tiid,
        "metric_name": metric_name, 
        "provider": provider, 
        "raw_value": as_int_or_float_if_possible(metric_value),
        "drilldown_url": provenance_url,
        "collected_date": datetime.datetime.utcnow()
    }    
    metric_object = Metric(**new_style_metric_dict)
    db.session.add(metric_object)

    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in add_metric_to_item_object for {tiid}, rolling back.  Message: {message}".format(
            tiid=tiid, 
            message=e.message)) 

    return item_obj




def add_aliases_to_item_object(aliases_dict, item_obj):
    logger.debug(u"in add_aliases_to_item_object for {tiid}".format(
        tiid=item_obj.tiid))        

    alias_objects = create_alias_objects(aliases_dict)

    for alias_obj in alias_objects:
        if not alias_obj.alias_tuple in item_obj.alias_tuples:
            item_obj.aliases.append(alias_obj)    

    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in add_aliases_to_item_object for {tiid}, rolling back.  Message: {message}".format(
            tiid=tiid, 
            message=e.message)) 
    return item_obj


def add_biblio(tiid, biblio_name, biblio_value, provider_name="user_provided", collected_date=datetime.datetime.utcnow()):

    logger.debug(u"in add_biblio for {tiid} {biblio_name}".format(
        tiid=tiid, biblio_name=biblio_name))

    biblio_object = Biblio.query.filter_by(tiid=tiid, provider=provider_name, biblio_name=biblio_name).first()
    if biblio_object:
        logger.debug(u"found a previous row in add_biblio for {tiid} {biblio_name}, so removing it".format(
            tiid=tiid, biblio_name=biblio_name))
        biblio_object.biblio_value = biblio_value
        biblio_object.collected_date = collected_date
    else:
        biblio_object = Biblio(tiid=tiid, 
                biblio_name=biblio_name, 
                biblio_value=biblio_value, 
                provider=provider_name, 
                collected_date=collected_date)
        db.session.add(biblio_object)

    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in add_biblio for {tiid}, rolling back.  Message: {message}".format(
            tiid=tiid, 
            message=e.message)) 

    logger.debug(u"finished saving add_biblio for {tiid} {biblio_name}".format(
        tiid=tiid, biblio_name=biblio_name))

    item_obj = Item.from_tiid(tiid)

    logger.debug(u"got object for add_biblio for {tiid} {biblio_name}".format(
        tiid=tiid, biblio_name=biblio_name))

    return item_obj



def add_biblio_to_item_object(new_biblio_dict, item_doc, provider_name):
    tiid = item_doc["_id"]
    item_obj = Item.from_tiid(tiid)

    logger.debug(u"in add_biblio_to_item_object for {tiid} {provider_name}, /biblio_print {new_biblio_dict}".format(
        tiid=tiid, 
        provider_name=provider_name,
        new_biblio_dict=new_biblio_dict))        

    new_biblio_objects = create_biblio_objects([new_biblio_dict], provider=provider_name)
    for new_biblio_obj in new_biblio_objects:
        if not Biblio.query.get((tiid, provider_name, new_biblio_obj.biblio_name)):
            item_obj.biblios += [new_biblio_obj]    

    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in add_biblio_to_item_object for {tiid}, rolling back.  Message: {message}".format(
            tiid=tiid, 
            message=e.message)) 
        
    return item_obj



def get_biblio_to_update(old_biblio, new_biblio):
    if not old_biblio:
        return new_biblio

    response = {}
    for biblio_name in new_biblio:
        if not biblio_name in old_biblio:
            response[biblio_name] = new_biblio[biblio_name]

        # a few things should get overwritten no matter what
        if (biblio_name=="title") and ("title" in old_biblio):
            if old_biblio["title"] == "AOP":
                response[biblio_name] = new_biblio[biblio_name]

        if (biblio_name in ["is_oa_journal", "oai_id", "free_fulltext_url"]):
            response[biblio_name] = new_biblio[biblio_name]

    return response



def update_item_with_new_biblio(new_biblio_dict, item_obj, provider_name=None):
    item_doc = item_obj.as_old_doc()

    # return None if no changes
    # don't change if biblio already there, except in special cases

    response = get_biblio_to_update(item_doc["biblio"], new_biblio_dict)
    if response:
        item_doc["biblio"] = response
        item_obj = add_biblio_to_item_object(new_biblio_dict, item_doc, provider_name=provider_name)
    return(item_obj)


def make():
    now = datetime.datetime.utcnow().isoformat()
    shortuuid.set_alphabet('abcdefghijklmnopqrstuvwxyz1234567890')

    item = {}
    item["_id"] = shortuuid.uuid()[0:24]
    item["aliases"] = {}
    item["biblio"] = {}
    item["last_modified"] = now
    item["created"] = now
    item["type"] = "item"
    return item


def clean_for_export(item, supplied_key=None, secret_key=None, override_export_clean=False):
    if not override_export_clean and supplied_key and (supplied_key==secret_key):
        return(item)

    # if still here, then need to remove sensitive data
    cleaned_item = copy.deepcopy(item)
    metrics = cleaned_item.setdefault("metrics", {})
    metric_names = metrics.keys()
    for metric_name in metric_names:
        if "scopus:" in metric_name:
            del cleaned_item["metrics"][metric_name]
        if "citeulike:" in metric_name:
            del cleaned_item["metrics"][metric_name]
    return cleaned_item


def decide_genre(alias_dict):
    # logger.debug(u"in decide_genre with {alias_dict}".format(
    #     alias_dict=alias_dict))        

    genre = "unknown"
    host = "unknown"

    '''Uses available aliases to decide the item's genre'''
    if "doi" in alias_dict:
        joined_doi_string = "".join(alias_dict["doi"])
        joined_doi_string = joined_doi_string.lower()
        if "10.5061/dryad." in joined_doi_string:
            genre = "dataset"
            host = "dryad"
        elif ".figshare." in joined_doi_string:
            host = "figshare"
            try:
                genre = alias_dict["biblio"][0]["genre"]
            except (KeyError, AttributeError):
                genre = "dataset"
        else:
            genre = "article"

    elif "pmid" in alias_dict:
        genre = "article"

    elif "arxiv" in alias_dict:
        genre = "article"
        host = "arxiv"

    elif "blog" in alias_dict:
        genre = "blog"
        host = "wordpresscom"

    elif "blog_post" in alias_dict:
        genre = "blog"
        host = "blog_post"

    elif "url" in alias_dict:
        joined_url_string = "".join(alias_dict["url"])
        joined_url_string = joined_url_string.lower()
        if "slideshare.net" in joined_url_string:
            genre = "slides"
            host = "slideshare"
        elif "github.com" in joined_url_string:
            genre = "software"
            host = "github"
        elif "twitter.com" in joined_url_string:
            if "/status/" in joined_url_string:
                genre = "twitter"
                host = "twitter_tweet"
            else:
                genre = "twitter"
                host = "twitter_account"
        elif "youtube.com" in joined_url_string:
            genre = "video"
            host = "youtube"
        elif "vimeo.com" in joined_url_string:
            genre = "video"
            host = "vimeo"
        else:
            genre = "webpage"

    # override if it came in with a genre, or call it an "article" if it has a journal
    if (host=="unknown" and ("biblio" in alias_dict)):
        for biblio_dict in alias_dict["biblio"]:
            if "genre" in biblio_dict and (biblio_dict["genre"] not in ["undefined", "other"]):
                genre = biblio_dict["genre"]
            elif ("journal" in biblio_dict) and biblio_dict["journal"]:  
                genre = "article"

    if "article" in genre:
        genre = "article"  #disregard whether journal article or conference article for now

    return (genre, host)


def canonical_alias_tuple(alias):
    (namespace, nid) = alias
    namespace = clean_id(namespace)
    nid = clean_id(nid)
    namespace = namespace.lower()
    if namespace=="doi":
        try:
            nid = nid.lower()
        except AttributeError:
            pass
    return(namespace, nid)

def canonical_aliases(orig_aliases_dict):
    # only put lowercase namespaces in items, and lowercase dois
    lowercase_aliases_dict = {}
    for orig_namespace in orig_aliases_dict:
        lowercase_namespace = clean_id(orig_namespace.lower())
        if lowercase_namespace == "doi":
            lowercase_aliases_dict[lowercase_namespace] = [clean_id(doi.lower()) for doi in orig_aliases_dict[orig_namespace]]
        else:
            lowercase_aliases_dict[lowercase_namespace] = [clean_id(nid) for nid in orig_aliases_dict[orig_namespace]]
    return lowercase_aliases_dict

def alias_tuples_from_dict(aliases_dict):
    """
    Convert from aliases dict we use in items, to a list of alias tuples.

    The providers need the tuples list, which look like this:
    [(doi, 10.123), (doi, 10.345), (pmid, 1234567)]
    """
    alias_tuples = []
    for ns, ids in aliases_dict.iteritems():
        if isinstance(ids, basestring): # it's a date, not a list of ids
            alias_tuples.append((ns, ids))
        else:
            for id in ids:
                alias_tuples.append((ns, id))
    return alias_tuples

def alias_dict_from_tuples(aliases_tuples):
    alias_dict = {}
    for (ns, ids) in aliases_tuples:
        if ns in alias_dict:
            alias_dict[ns] += [ids]
        else:
            alias_dict[ns] = [ids]
    return alias_dict

def merge_alias_dicts(aliases1, aliases2):
    #logger.debug(u"in MERGE ALIAS DICTS with %s and %s" %(aliases1, aliases2))
    merged_aliases = copy.deepcopy(aliases1)
    for ns, nid_list in aliases2.iteritems():
        for nid in nid_list:
            try:
                if not nid in merged_aliases[ns]:
                    merged_aliases[ns].append(nid)
            except KeyError: # no ids for that namespace yet. make it.
                merged_aliases[ns] = [nid]
    return merged_aliases

def get_metric_names(providers_config):
    full_metric_names = []
    providers = ProviderFactory.get_providers(providers_config)
    for provider in providers:
        metric_names = provider.metric_names()
        for metric_name in metric_names:
            full_metric_names.append(provider.provider_name + ':' + metric_name)
    return full_metric_names

def get_normalized_values(genre, host, year, metric_name, value, myrefsets):
    # Will be passed None as myrefsets type when loading items in reference collections :)

    if not myrefsets:
        return {}

    if host in ["dryad", "figshare"]:
        genre = "dataset"  #treat as dataset for the sake of normalization

    if genre not in myrefsets.keys():
        #logger.info(u"Genre {genre} not in refsets so give up".format(
        #    genre=genre))
        return {}

    # treat the f1000 "Yes" as a 1 for normalization
    if value=="Yes":
        value = 1

    response = {}
    for refsetname in myrefsets[genre]:
        # for nonarticles, use only the reference set type whose name matches the host (figshare, dryad, etc)
        if (genre != "article"):
            if (host != refsetname):
                continue  # skip this refset
        try:
            int_year = int(year)  #year is a number in the refset keys
            fencepost_values = myrefsets[genre][refsetname][int_year][metric_name].keys()
            myclosest = largest_value_that_is_less_than_or_equal_to(value, fencepost_values)
            response[refsetname] = myrefsets[genre][refsetname][int_year][metric_name][myclosest]
        except KeyError:
            #logger.info(u"No good lookup in %s %s %s for %s" %(genre, refsetname, year, metric_name))
            pass
        except ValueError:
            logger.error(u"Exception: no good lookup in %s %s %s for %s" %(genre, refsetname, year, metric_name))
            logger.debug(u"Value error calculating percentiles for %s %s %s for %s=%s" %(genre, refsetname, year, metric_name, str(value)))
            logger.debug(u"fencepost = {fencepost_values}".format(
                fencepost_values=fencepost_values))
            pass
            
    return response

def retrieve_items(tiids, myrefsets, myredis, mydao):
    something_currently_updating = False
    items = []
    for tiid in tiids:
        try:
            item = get_item(tiid, myrefsets, myredis)
        except (LookupError, AttributeError), e:
            logger.warning(u"Got an error looking up tiid '{tiid}'; error: {error}".format(
                    tiid=tiid, error=e.__repr__()))
            raise

        if not item:
            logger.warning(u"Looks like there's no item with tiid '{tiid}': ".format(
                    tiid=tiid))
            raise LookupError
            
        item["currently_updating"] = is_currently_updating(tiid, myredis)
        something_currently_updating = something_currently_updating or item["currently_updating"]

        items.append(item)
    return (items, something_currently_updating)

def is_currently_updating(tiid, myredis):
    num_providers_currently_updating = myredis.get_num_providers_currently_updating(tiid)
    currently_updating = num_providers_currently_updating > 0
    return currently_updating

   
def create_item(namespace, nid, myredis, mydao):
    logger.debug(u"In create_item with alias" + str((namespace, nid)))
    item_doc = make()
    namespace = clean_id(namespace)
    nid = clean_id(nid)
    item_doc["aliases"][namespace] = [nid]
    item_doc["aliases"] = canonical_aliases(item_doc["aliases"])

    item_obj = create_objects_from_item_doc(item_doc)

    logger.info(u"saved new collection '{tiid}'".format(
            tiid=item_doc["_id"]))

    logger.debug(json.dumps(item_doc, sort_keys=True, indent=4))

    analytics_credentials = {}
    start_item_update([{"tiid": item_doc["_id"], "aliases_dict":item_doc["aliases"]}], analytics_credentials, "low", myredis)

    logger.info(u"Created new item '{tiid}' with alias '{alias}'".format(
        tiid=item_doc["_id"],
        alias=str((namespace, nid))
    ))

    return item_doc["_id"]


def get_tiids_from_aliases(aliases):
    clean_aliases = [canonical_alias_tuple((ns, nid)) for (ns, nid) in aliases]
    aliases_tiid_mapping = {}

    for alias in clean_aliases:
        alias_key = alias
        tiid = None
        (ns, nid) = alias
        if (ns=="biblio"):
            alias_key = (ns, json.dumps(nid))
            tiid = get_tiid_by_biblio(nid)        
        else:
            alias_obj = Alias.query.filter_by(namespace=ns, nid=nid).first()
            try:
                tiid = alias_obj.tiid
                logger.debug(u"Found a tiid for {nid} in get_tiid_by_alias: {tiid}".format(
                    nid=nid, 
                    tiid=tiid))
            except AttributeError:
                pass
        aliases_tiid_mapping[alias_key] = tiid
    return aliases_tiid_mapping


# forgoes some checks for speed because only used for just-created items
def add_alias_to_new_item(alias_tuple, provider=None):
    item_obj = Item()
    (namespace, nid) = alias_tuple
    if namespace=="biblio":
        if not provider:
            provider = "unknown1"
        for biblio_name in nid:
                biblio_object = Biblio(biblio_name=biblio_name, 
                        biblio_value=nid[biblio_name], 
                        provider=provider)
                item_obj.biblios += [biblio_object]
    else:
        item_obj.aliases = [Alias(alias_tuple=alias_tuple)]
    return item_obj  


def create_tiids_from_aliases(aliases, analytics_credentials, myredis, provider=None):
    tiid_alias_mapping = {}
    clean_aliases = [canonical_alias_tuple((ns, nid)) for (ns, nid) in aliases]  
    dicts_to_update = []  

    logger.debug(u"in create_tiids_from_aliases, starting alias loop")

    for alias_tuple in clean_aliases:
        logger.debug(u"in create_tiids_from_aliases, with alias_tuple {alias_tuple}".format(
            alias_tuple=alias_tuple))
        item_obj = add_alias_to_new_item(alias_tuple, provider)
        tiid = item_obj.tiid
        db.session.add(item_obj)
        # logger.debug(u"in create_tiids_from_aliases, made item {item_obj}".format(
        #     item_obj=item_obj))

        tiid_alias_mapping[tiid] = alias_tuple
        dicts_to_update += [{"tiid":tiid, "aliases_dict": alias_dict_from_tuples([alias_tuple])}]

    logger.debug(u"in create_tiids_from_aliases, starting commit")
    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in create_tiids_from_aliases for {tiid}, rolling back.  Message: {message}".format(
            tiid=tiid, 
            message=e.message)) 

    # has to be after commits to database
    logger.debug(u"in create_tiids_from_aliases, starting start_item_update")
    start_item_update(dicts_to_update, analytics_credentials, "high", myredis)

    logger.debug(u"in create_tiids_from_aliases, finished")
    return tiid_alias_mapping


def get_items_from_tiids(tiids, with_metrics=True):
    items = []
    for tiid in tiids:
        item = Item.from_tiid(tiid, with_metrics)
        if item:
            items += [item]
        else:
            logger.warning(u"in get_items_from_tiids, no item found for tiid {tiid}".format(
                tiid=tiid))

    return items


def get_tiid_by_biblio(biblio_dict):
    try:
        raw_sql = text("""select tiid from min_biblio 
                                        where title=:title
                                        and authors=:authors
                                        and journal=:journal""")
        biblio_statement = db.session.execute(raw_sql, params={
            "title":'"'+biblio_dict["title"]+'"',
            "authors":'"'+biblio_dict["authors"]+'"',
            "journal":'"'+biblio_dict["journal"]+'"'
            })
        biblio = biblio_statement.first()
        db.session.commit()
        tiid = biblio.tiid
    except AttributeError:
        logger.error(u"AttributeError in get_tiid_by_biblio with {biblio_dict}".format(
            biblio_dict=biblio_dict))
        tiid = None

    return tiid

def get_tiid_by_alias(ns, nid, mydao=None):
    logger.debug(u"In get_tiid_by_alias with {ns}, {nid}".format(
        ns=ns, nid=nid))

    tiid = None
    if (ns=="biblio"):
        tiid = get_tiid_by_biblio(nid)
    else:
        # change input to lowercase etc
        (ns, nid) = canonical_alias_tuple((ns, nid))
        alias_obj = Alias.query.filter_by(namespace=ns, nid=nid).first()
        try:
            tiid = alias_obj.tiid
            logger.debug(u"Found a tiid for {nid} in get_tiid_by_alias: {tiid}".format(
                nid=nid, tiid=tiid))
        except AttributeError:
            pass

    if not tiid:
        logger.debug(u"no match for tiid for {nid}!".format(nid=nid))
    return tiid


def start_item_update(dicts_to_add, analytics_credentials, priority, myredis):
    # logger.debug(u"In start_item_update with {tiid}, priority {priority} /biblio_print {aliases_dict}".format(
    #     tiid=tiid, priority=priority, aliases_dict=aliases_dict))
    tiids = [d["tiid"] for d in dicts_to_add]
    myredis.init_currently_updating_status(tiids,
        ProviderFactory.providers_with_metrics(default_settings.PROVIDERS))
    myredis.add_to_alias_queue(dicts_to_add, analytics_credentials, priority)

def is_equivalent_alias_tuple_in_list(query_tuple, tuple_list):
    is_equivalent = (clean_alias_tuple_for_deduplication(query_tuple) in tuple_list)
    return is_equivalent

def clean_alias_tuple_for_deduplication(alias_tuple):
    (ns, nid) = alias_tuple
    if ns == "biblio":
        keys_to_compare = ["full_citation", "title", "authors", "journal", "year"]
        if not isinstance(nid, dict):
            nid = json.loads(nid)
        if "year" in nid:
            nid["year"] = str(nid["year"])
        biblio_dict_for_deduplication = dict([(k, v) for (k, v) in nid.iteritems() if k.lower() in keys_to_compare])

        biblios_as_string = json.dumps(biblio_dict_for_deduplication, sort_keys=True, indent=0, separators=(',', ':'))
        return ("biblio", biblios_as_string.lower())
    else:
        (ns, nid) = normalize_alias((ns, nid))
        try:
            cleaned_alias = (ns.lower(), nid.lower())
        except AttributeError:
            logger.debug(u"problem cleaning {alias_tuple}".format(
                alias_tuple=alias_tuple))
            cleaned_alias = alias_tuple
        return cleaned_alias


def alias_tuples_for_deduplication(item):
    alias_tuples = []
    if item.aliases:
        alias_tuples = [alias.alias_tuple for alias in item.aliases]
    biblio_dicts_per_provider = item.biblio_dicts_per_provider
    for provider in biblio_dicts_per_provider:
        alias_tuple = ("biblio", biblio_dicts_per_provider[provider])
        alias_tuples += [alias_tuple]
        # logger.debug(u"tiid={tiid}, for provider {provider} is a new alias {alias_tuple}".format(
        #     tiid=item.tiid, provider=provider, alias_tuple=alias_tuple))

    cleaned_tuples = [clean_alias_tuple_for_deduplication(alias_tuple) for alias_tuple in alias_tuples]
    cleaned_tuples = [alias_tuple for alias_tuple in cleaned_tuples if alias_tuple != ("biblio", '{}')]

    # logger.debug(u"tiid={tiid}, cleaned_tuples {cleaned_tuples}".format(
    #     tiid=item.tiid, cleaned_tuples=cleaned_tuples))
    return cleaned_tuples

def aliases_not_in_existing_tiids(retrieved_aliases, existing_tiids):
    new_aliases = []
    if not existing_tiids:
        return retrieved_aliases
    existing_items = Item.query.filter(Item.tiid.in_(existing_tiids)).all()

    aliases_from_all_items = []
    for item in existing_items:
        logger.debug(u"getting alias_tuples_for_deduplication for tiid={tiid}".format(
             tiid=item.tiid))
        aliases_from_all_items += alias_tuples_for_deduplication(item)

    for alias_tuple in retrieved_aliases:
        if is_equivalent_alias_tuple_in_list(alias_tuple, aliases_from_all_items):
            # logger.debug(u"already have alias {alias_tuple}".format(
            #     alias_tuple=alias_tuple))
            pass
        else:
            new_aliases += [alias_tuple]
            # logger.debug(u"is a new alias {alias_tuple}".format(
            #     alias_tuple=alias_tuple))
    return new_aliases


def build_duplicates_list(tiids):
    items = get_items_from_tiids(tiids, with_metrics=False)
    distinct_groups = defaultdict(list)
    duplication_list = {}
    for item in items:
        is_distinct_item = True

        alias_tuples = alias_tuples_for_deduplication(item)

        for alias in alias_tuples:

            if is_equivalent_alias_tuple_in_list(alias, duplication_list):
                # we already have one of the aliase
                distinct_item_id = duplication_list[clean_alias_tuple_for_deduplication(alias)] 
                is_distinct_item = False  

        if is_distinct_item:
            distinct_item_id = len(distinct_groups)
            for alias in alias_tuples:
                # we went through all the aliases and don't have any that match, so make a new entries
                duplication_list[clean_alias_tuple_for_deduplication(alias)] = distinct_item_id

        # whether distinct or not,
        # add this to the group, and add all its aliases too
        if item.created:
            created_date = item.created.isoformat()
        else:
            created_date = "1999-01-01T14:42:49.818393"   
        distinct_groups[distinct_item_id] += [{ "tiid":item.tiid, 
                                                "has_user_provided_biblio":item.has_user_provided_biblio(), 
                                                "has_free_fulltext_url":item.has_free_fulltext_url(), 
                                                "created":created_date
                                                }]

    distinct_groups_values = [group for group in distinct_groups.values() if group]
    return distinct_groups_values


########NEW FILE########
__FILENAME__ = json_sqlalchemy
# from https://gist.github.com/dbarnett/1730610

import json
import sqlalchemy
from sqlalchemy import String
from sqlalchemy.ext.mutable import Mutable

class JSONEncodedObj(sqlalchemy.types.TypeDecorator):
    """Represents an immutable structure as a json-encoded string."""

    impl = String

    def process_bind_param(self, value, dialect):
        if value is not None:
            value = json.dumps(value)
        return value

    def process_result_value(self, value, dialect):
        if value is not None:
            try:
                value = json.loads(value)
            except:
                return value
        return value

class MutationObj(Mutable):
    @classmethod
    def coerce(cls, key, value):
        if isinstance(value, dict) and not isinstance(value, MutationDict):
            return MutationDict.coerce(key, value)
        if isinstance(value, list) and not isinstance(value, MutationList):
            return MutationList.coerce(key, value)
        return value

    @classmethod
    def _listen_on_attribute(cls, attribute, coerce, parent_cls):
        key = attribute.key
        if parent_cls is not attribute.class_:
            return

        # rely on "propagate" here
        parent_cls = attribute.class_

        def load(state, *args):
            val = state.dict.get(key, None)
            if coerce:
                val = cls.coerce(key, val)
                state.dict[key] = val
            if isinstance(val, cls):
                val._parents[state.obj()] = key

        def set(target, value, oldvalue, initiator):
            if not isinstance(value, cls):
                value = cls.coerce(key, value)
            if isinstance(value, cls):
                value._parents[target.obj()] = key
            if isinstance(oldvalue, cls):
                oldvalue._parents.pop(target.obj(), None)
            return value

        def pickle(state, state_dict):
            val = state.dict.get(key, None)
            if isinstance(val, cls):
                if 'ext.mutable.values' not in state_dict:
                    state_dict['ext.mutable.values'] = []
                state_dict['ext.mutable.values'].append(val)

        def unpickle(state, state_dict):
            if 'ext.mutable.values' in state_dict:
                for val in state_dict['ext.mutable.values']:
                    val._parents[state.obj()] = key

        sqlalchemy.event.listen(parent_cls, 'load', load, raw=True, propagate=True)
        sqlalchemy.event.listen(parent_cls, 'refresh', load, raw=True, propagate=True)
        sqlalchemy.event.listen(attribute, 'set', set, raw=True, retval=True, propagate=True)
        sqlalchemy.event.listen(parent_cls, 'pickle', pickle, raw=True, propagate=True)
        sqlalchemy.event.listen(parent_cls, 'unpickle', unpickle, raw=True, propagate=True)

class MutationDict(MutationObj, dict):
    @classmethod
    def coerce(cls, key, value):
        """Convert plain dictionary to MutationDict"""
        self = MutationDict((k,MutationObj.coerce(key,v)) for (k,v) in value.items())
        self._key = key
        return self

    def __setitem__(self, key, value):
        dict.__setitem__(self, key, MutationObj.coerce(self._key, value))
        self.changed()

    def __delitem__(self, key):
        dict.__delitem__(self, key)
        self.changed()

class MutationList(MutationObj, list):
    @classmethod
    def coerce(cls, key, value):
        """Convert plain list to MutationList"""
        self = MutationList((MutationObj.coerce(key, v) for v in value))
        self._key = key
        return self

    def __setitem__(self, idx, value):
        list.__setitem__(self, idx, MutationObj.coerce(self._key, value))
        self.changed()

    def __setslice__(self, start, stop, values):
        list.__setslice__(self, start, stop, (MutationObj.coerce(self._key, v) for v in values))
        self.changed()

    def __delitem__(self, idx):
        list.__delitem__(self, idx)
        self.changed()

    def __delslice__(self, start, stop):
        list.__delslice__(self, start, stop)
        self.changed()

    def append(self, value):
        list.append(self, MutationObj.coerce(self._key, value))
        self.changed()

    def insert(self, idx, value):
        list.insert(self, idx, MutationObj.coerce(self._key, value))
        self.changed()

    def extend(self, values):
        list.extend(self, (MutationObj.coerce(self._key, v) for v in values))
        self.changed()

    def pop(self, *args, **kw):
        value = list.pop(self, *args, **kw)
        self.changed()
        return value

    def remove(self, value):
        list.remove(self, value)
        self.changed()

def JSONAlchemy(sqltype):
    """A type to encode/decode JSON on the fly

    sqltype is the string type for the underlying DB column.

    You can use it like:
    Column(JSONAlchemy(Text(600)))
    """
    class _JSONEncodedObj(JSONEncodedObj):
        impl = sqltype
    return MutationObj.as_mutable(_JSONEncodedObj)



########NEW FILE########
__FILENAME__ = models
from werkzeug import generate_password_hash, check_password_hash
import datetime, hashlib, threading, json, time, copy, re

from totalimpact.providers.provider import ProviderFactory
from totalimpact.providers.provider import ProviderTimeout, ProviderServerError
from totalimpact import default_settings
from totalimpact.utils import Retry

# Master lock to ensure that only a single thread can write
# to the DB at one time to avoid document conflicts

import logging
logger = logging.getLogger('ti.models')

# setup to remove control characters from received IDs
# from http://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python
control_chars = ''.join(map(unichr, range(0,32) + range(127,160)))
control_char_re = re.compile('[%s]' % re.escape(control_chars))

class NotAuthenticatedError(Exception):
    pass

class MemberItems():

    def __init__(self, provider, redis):
        self.provider = provider
        self.redis = redis

    def start_update(self, str):
        paginate_dict = self.provider.paginate(str)
        hash = hashlib.md5(str.encode('utf-8')).hexdigest()
        t = threading.Thread(target=self._update, 
                            args=(paginate_dict["pages"], paginate_dict["number_entries"], hash), 
                            name=hash[0:4]+"_memberitems_thread")
        t.daemon = True
        t.start()
        return hash


    def get_sync(self, query):
        ret = {}
        start = time.time()
        ret = {
            "memberitems": self.provider.member_items(query),
            "pages": 1,
            "complete": 1,
            "error": False
        }
        ret["number_entries"] = len(ret["memberitems"])

        logger.debug(u"got {number_finished_memberitems} synchronous memberitems for query '{query}' in {elapsed} seconds.".format(
            number_finished_memberitems=len(ret["memberitems"]),
            query=query,
            elapsed=round(time.time() - start, 2)
        ))
        return ret

    def get_async(self, query_hash):
        query_status = self.redis.get_memberitems_status(query_hash)
        start = time.time()

        if not query_status:
            query_status = {"memberitems": [], "pages": 1, "complete": 0, "error": False} # don't know number_entries yet

        logger.debug(u"have finished {number_finished_memberitems} of asynchronous memberitems for query hash '{query_hash}' in {elapsed} seconds.".format(
                number_finished_memberitems=len(query_status["memberitems"]),
                query_hash=query_hash,
                elapsed=round(time.time() - start, 2)
            ))

        return query_status


    @Retry(3, ProviderTimeout, 0.1)
    def _update(self, pages, number_entries, query_key):

        status = {
            "memberitems": [],
            "pages": len(pages),
            "complete": 0,
            "number_entries": number_entries,
            "error": False            
        }
        self.redis.set_memberitems_status(query_key, status)
        for page in pages:
            try:
                memberitems = self.provider.member_items(page)
                status["memberitems"].append(memberitems)
            except (ProviderTimeout, ProviderServerError):
                status["error"] = True
            status["complete"] += 1
            self.redis.set_memberitems_status(query_key, status)

        return True





########NEW FILE########
__FILENAME__ = altmetric_com
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import json, re, os, requests, socket
from operator import itemgetter

import logging
logger = logging.getLogger('ti.providers.altmetric_com')

class Altmetric_Com(Provider):  

    example_id = ("doi", "10.1101/gr.161315.113")

    url = "http://www.altmetric.com"
    descr = "We make article level metrics easy."
    aliases_url_template = 'http://api.altmetric.com/v1/fetch/%s?key=' + os.environ["ALTMETRIC_COM_KEY"]
    metrics_url_template_tweets = 'http://api.altmetric.com/v1/fetch/id/%s?key=' + os.environ["ALTMETRIC_COM_KEY"]
    metrics_url_other_metrics = 'http://api.altmetric.com/v1/citations/1y?key=' + os.environ["ALTMETRIC_COM_KEY"]
    provenance_url_template = 'http://www.altmetric.com/details.php?citation_id=%s&src=impactstory.org'

    static_meta_dict =  {
        "tweets": {
            "display_name": "Twitter tweets",
            "provider": "Altmetric.com",
            "provider_url": "http://twitter.com",
            "description": "Number of times the product has been tweeted",
            "icon": "https://twitter.com/favicon.ico",
        },
        "facebook_posts": {
            "display_name": "Facebook public posts",
            "provider": "Altmetric.com",
            "provider_url": "http://facebook.com",
            "description": "Number of posts mentioning the product on a public Facebook wall",
            "icon": "http://facebook.com/favicon.ico",
        },
        "gplus_posts": {
            "display_name": "Google+ posts",
            "provider": "Altmetric.com",
            "provider_url": "http://plus.google.com",
            "description": "Number of posts mentioning the product on Google+",
            "icon": "http://plus.google.com/favicon.ico",
        },
        "blog_posts": {
            "display_name": "blog posts",
            "provider": "Altmetric.com",
            "provider_url": "http://plus.google.com",
            "description": "Number of blog posts mentioning the product",
            "icon": "http://impactstory.org/static/img/blogs-icon.png",
        }                           
    }
    

    def __init__(self):
        super(Altmetric_Com, self).__init__()

    @property
    def provides_aliases(self):
        return True

    @property
    def provides_metrics(self):
        return True

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return namespace in ["doi"]

    def get_best_id(self, aliases):
        # return it with the id type as a prefix before / because that's how the altmetric.com api expects it
        aliases_dict = provider.alias_dict_from_tuples(aliases)
        if "altmetric_com" in aliases_dict:
            best_id = aliases_dict["altmetric_com"][0]
        else:
            best_id = None
        return(best_id)


    def provenance_url(self, metric_name, aliases):
        aliases_dict = provider.alias_dict_from_tuples(aliases)
        try:
            drilldown_url = self._get_templated_url(self.provenance_url_template, aliases_dict["altmetric_com"][0])
        except KeyError:
            drilldown_url = ""
        return drilldown_url


    def get_id_for_aliases(self, aliases):
        # return it with the id type as a prefix before / because that's how the altmetric.com api expects it
        aliases_dict = provider.alias_dict_from_tuples(aliases)
        if "doi" in aliases_dict:
            best_id = "doi/{id}".format(id=aliases_dict["doi"][0])
        elif "pmid" in aliases_dict:
            best_id = "pmid/{id}".format(id=aliases_dict["pmid"][0])
        elif "arxiv" in aliases_dict:
            best_id = "arxiv_id/{id}".format(id=aliases_dict["arxiv"][0])
        elif "altmetric_com" in aliases_dict:
            best_id = "altmetric_com/{id}".format(id=aliases_dict["altmetric_com"][0])
        else:
             best_id = None
        return(best_id)


    def aliases(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):            

        aliases_dict = provider.alias_dict_from_tuples(aliases)

        if "altmetric_com" in aliases_dict:
            return []  # nothing new to add

        nid = self.get_id_for_aliases(aliases)
        if not nid:
            return []

        new_aliases = self._get_aliases_for_id(nid, provider_url_template, cache_enabled)
        return new_aliases


    def _extract_aliases(self, page, id=None):
        dict_of_keylists = {"altmetric_com": ["altmetric_id"]}

        aliases_dict = provider._extract_from_json(page, dict_of_keylists)
        if aliases_dict:
            aliases_list = [("altmetric_com", str(aliases_dict["altmetric_com"]))]
        else:
            aliases_list = []
        return aliases_list


    def _extract_metrics_twitter(self, page, status_code=200, id=None):
        dict_of_keylists = {
            'altmetric_com:tweets' : ['counts', 'twitter', 'posts_count']
        }
        metrics_dict = provider._extract_from_json(page, dict_of_keylists)
        return metrics_dict


    def _extract_metrics_other_metrics(self, data, status_code=200, id=None):
        dict_of_keylists = {
            'altmetric_com:gplus_posts' : ['cited_by_gplus_count'],
            'altmetric_com:facebook_posts' : ['cited_by_fbwalls_count'],
            'altmetric_com:blog_posts' : ['cited_by_feeds_count']
        }
        entry = data["results"][0]
        metrics_dict = provider._extract_from_data_dict(entry, dict_of_keylists)
        return metrics_dict


    def get_metrics_for_other_metrics(self, 
            id, 
            provider_url_template=None, 
            cache_enabled=True):

        # self.logger.debug(u"%s getting metrics for %s" % (self.provider_name, id))

        headers = {u'content-type': u'application/x-www-form-urlencoded',
                    u'accept': u'application/json'}

        r = requests.post(self.metrics_url_other_metrics, 
                        data="citation_ids="+id, 
                        headers=headers)

        # extract the metrics
        try:
            data = r.json() 
            metrics_dict = self._extract_metrics_other_metrics(data)
        except socket.timeout, e:  # can apparently be thrown here
            self.logger.info(u"%s Provider timed out *after* GET in socket" %(self.provider_name))        
            raise ProviderTimeout("Provider timed out *after* GET in socket", e)        
        except (AttributeError, TypeError, ValueError):  # ValueError includes simplejson.decoder.JSONDecodeError
            # expected response if nothing found
            metrics_dict = {}

        return metrics_dict



    def metrics(self, 
            aliases,
            provider_url_template=None, 
            cache_enabled=True):

        id = self.get_best_id(aliases)

        # Only lookup metrics for items with appropriate ids
        if not id:
            #self.logger.debug(u"%s not checking metrics, no relevant alias" % (self.provider_name))
            return {}

        metrics = {}
        new_metrics = self.get_metrics_for_id(id, self.metrics_url_template_tweets, cache_enabled, 
                extract_metrics_method=self._extract_metrics_twitter)
        if new_metrics:
            metrics.update(new_metrics)
        new_metrics = self.get_metrics_for_other_metrics(id)
        if new_metrics:
            metrics.update(new_metrics)

        metrics_and_drilldown = {}
        for metric_name in metrics:
            drilldown_url = self.provenance_url(metric_name, aliases)
            metrics_and_drilldown[metric_name] = (metrics[metric_name], drilldown_url)

        return metrics_and_drilldown  





########NEW FILE########
__FILENAME__ = arxiv
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact.unicode_helpers import remove_nonprinting_characters

import os, re

import logging
logger = logging.getLogger('ti.providers.arxiv')

def clean_arxiv_id(arxiv_id):
    arxiv_id = remove_nonprinting_characters(arxiv_id)    
    arxiv_id = arxiv_id.lower().replace("arxiv:", "").replace("http://arxiv.org/abs/", "")
    return arxiv_id


class Arxiv(Provider):  

    example_id = ("arxiv", "1305.3328")

    url = "http://arxiv.org"
    descr = "arXiv is an e-print service in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance and statistics."
    biblio_url_template = "http://export.arxiv.org/api/query?id_list=%s"
    aliases_url_template = "http://arxiv.org/abs/%s"

    static_meta_dict = {}


    def __init__(self):
        super(Arxiv, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        if (namespace == "arxiv"):
            return True
        else:
            return False


    # overriding default because overriding aliases method
    @property
    def provides_aliases(self):
        return True

    # overriding default because overriding member items method
    @property
    def provides_members(self):
        return True


    # overriding because don't need to look up
    def member_items(self, 
            query_dict, 
            provider_url_template=None, 
            cache_enabled=True):

        if not self.provides_members:
            raise NotImplementedError()

        self.logger.debug(u"%s getting member_items for %s" % (self.provider_name, query_dict))

        arxiv_ids = query_dict.split("\n")
        aliases_tuples = [("arxiv", clean_arxiv_id(arxiv_id)) for arxiv_id in arxiv_ids if arxiv_id]

        return(aliases_tuples)


    # overriding
    def aliases(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):            

        arxiv_id = self.get_best_id(aliases)

        if not provider_url_template:
            provider_url_template = self.aliases_url_template
        new_alias = [("url", self._get_templated_url(provider_url_template, arxiv_id, "aliases"))]
        if new_alias in aliases:
            new_alias = []  #override because isn't new

        return new_alias


    def _extract_biblio(self, page, id=None):
        dict_of_keylists = {
            'title' : ['entry', 'title'],
            'date' : ['entry', 'published'],
        }
        biblio_dict = provider._extract_from_xml(page, dict_of_keylists)
        dom_authors = provider._find_all_in_xml(page, "name")

        try:
            authors = [author.firstChild.data for author in dom_authors]
            biblio_dict["authors"] = ", ".join([author.split(" ")[-1] for author in authors])
        except (AttributeError, TypeError):
            pass

        try:
            biblio_dict["year"] = biblio_dict["date"][0:4]
        except KeyError:
            pass

        biblio_dict["repository"] = "arXiv"
        biblio_dict["free_fulltext_url"] = self._get_templated_url(self.aliases_url_template, id, "aliases")

        return biblio_dict    
       



########NEW FILE########
__FILENAME__ = bibjson
from StringIO import StringIO
import json, re

from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderTimeout, ProviderServerError
from totalimpact import unicode_helpers 

import logging
logger = logging.getLogger('ti.providers.bibjson')


class Bibjson(Provider):  

    example_id = None

    url = ""
    descr = ""

    def _to_unicode(self, text):
        text = unicode_helpers.to_unicode_or_bust(text)
        return text


    def parse(self, bibjson_list):
        ret = []
        for bibjson_entry in bibjson_list:
            full_entry = bibjson_entry
            try:
                full_entry["authors"] = self._to_unicode(re.sub(", \d+", "", full_entry["marker"]))
            except (KeyError, AttributeError):
                full_entry["authors"] = ""

            try:
                full_entry["first_author"] = self._to_unicode(full_entry["marker"].split(",")[0])
            except (KeyError, AttributeError):
                full_entry["first_author"] = ""

            try:
                pages = full_entry["pages"]
                full_entry["first_page"] = pages.split("--")[0]
            except KeyError:
                full_entry["first_page"] = ""

            try:
                full_entry["title"] = full_entry["booktitle"]
            except (KeyError, AttributeError):
                pass

            ret.append(full_entry)

        return ret



    def member_items(self, bibjson_contents, cache_enabled=True):
        logger.debug(u"%20s getting member_items for bibjson" % (self.provider_name))

        parsed_bibjson = self.parse(bibjson_contents)

        aliases = [("biblio", entry) for entry in parsed_bibjson]

        return(aliases)

########NEW FILE########
__FILENAME__ = bibtex
from StringIO import StringIO
import json, re

from pybtex.database.input import bibtex
from pybtex.errors import enable_strict_mode, format_error
from pybtex.scanner import PybtexSyntaxError, PybtexError

from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderTimeout, ProviderServerError
from totalimpact import unicode_helpers 
from totalimpact.providers import bibtex_lookup

import logging
logger = logging.getLogger('ti.providers.bibtex')



def build_bibtex_to_unicode(unicode_to_bibtex):
    bibtex_to_unicode = {}
    for unicode_value in unicode_to_bibtex:
        bibtex = unicode_to_bibtex[unicode_value]
        bibtex = unicode(bibtex, "utf-8")
        bibtex = bibtex.strip()
        bibtex = bibtex.replace("\\", "")
        bibtex = bibtex.replace("{", "")
        bibtex = bibtex.replace("}", "")
        bibtex = "{"+bibtex+"}"
        bibtex_to_unicode[bibtex] = unicode_value
    return bibtex_to_unicode


class Bibtex(Provider):  

    example_id = None

    url = ""
    descr = ""

    def __init__(self):
        super(Bibtex, self).__init__()
        enable_strict_mode(True) #throw errors
        self.bibtex_to_unicode = build_bibtex_to_unicode(bibtex_lookup.unicode_to_latex)

    def _to_unicode(self, text):
        text = unicode_helpers.to_unicode_or_bust(text)
        if "{" in text:
            text = text.replace("\\", "")
            for i, j in self.bibtex_to_unicode.iteritems():
                text = text.replace(i, j)
        return text

    def _parse_bibtex_entries(self, entries):
        biblio_list = []
        for entry in entries:
            stream = StringIO(entry)
            parser = bibtex.Parser()
            try:
                biblio = parser.parse_stream(stream)
                biblio_list += [biblio]
            except (PybtexSyntaxError, PybtexError), error:
                error = error
                logger.error(format_error(error, prefix='BIBTEX_ERROR: '))
                #logger.error("BIBTEX_ERROR error input: '{entry}'".format(
                #    entry=entry))
                #raise ProviderContentMalformedError(error.message)
        return biblio_list

    def parse(self, bibtex_contents):
        ret = []
        cleaned_string = bibtex_contents.replace("\&", "").replace("%", "").strip()
        entries = ["@"+entry for entry in cleaned_string.split("@") if entry]
        biblio_list = self._parse_bibtex_entries(entries)

        for biblio in biblio_list:
            parsed = {}
            try:
                mykey = biblio.entries.keys()[0]
            except AttributeError:
                # doesn't seem to be a valid biblio object, so skip to the next one
                logger.info(u"%20s NO DOI because no entries attribute in %s" % (self.provider_name, biblio))
                continue

            try:
                parsed["journal"] = self._to_unicode(biblio.entries[mykey].fields["journal"])
            except KeyError:
                parsed["journal"] = ""


            try:
                lnames = [person.get_part_as_text("last") for person in biblio.entries[mykey].persons["author"]]
                parsed["first_author"] = self._to_unicode(lnames[0])
            except (KeyError, AttributeError):
                try:
                    parsed["first_author"] = self._to_unicode(biblio.entries[mykey].fields["author"][0].split(",")[0])
                except (KeyError, AttributeError):
                    parsed["first_author"] = ""

            try:
                lnames = [person.get_part_as_text("last") for person in biblio.entries[mykey].persons["author"]]
                parsed["authors"] = self._to_unicode(", ".join(lnames))
            except (KeyError, AttributeError):
                parsed["authors"] = ""

            try:
                parsed["number"] = biblio.entries[mykey].fields["number"]
            except KeyError:
                parsed["number"] = ""

            try:
                parsed["volume"] = biblio.entries[mykey].fields["volume"]
            except KeyError:
                parsed["volume"] = ""

            try:
                pages = biblio.entries[mykey].fields["pages"]
                parsed["first_page"] = pages.split("--")[0]
            except KeyError:
                parsed["first_page"] = ""

            try:
                year_string = biblio.entries[mykey].fields["year"].replace("{}", "")
                parsed["year"] = re.sub("\D", "", year_string)
            except KeyError:
                parsed["year"]  = ""

            try:
                parsed["title"] = self._to_unicode(biblio.entries[mykey].fields["title"])
            except KeyError:
                parsed["title"]  = ""

            #parsed["key"] = mykey

            ret.append(parsed)

        return ret



    def member_items(self, bibtex_contents, cache_enabled=True):
        logger.debug(u"%20s getting member_items for bibtex" % (self.provider_name))

        parsed_bibtex = self.parse(bibtex_contents)

        aliases = []
        for entry in parsed_bibtex:
            if ("journal" in entry) and "arXiv preprint" in entry["journal"]:
                arxiv_id = entry["journal"].replace("arXiv preprint", "")
                arxiv_id = arxiv_id.replace("arXiv:", "").strip()
                aliases += [("arxiv", arxiv_id)]
            else:                
                aliases += [("biblio", entry)]

        return(aliases)

########NEW FILE########
__FILENAME__ = bibtex_lookup
# from https://gist.github.com/jalavik/976294
# original XML at http://www.w3.org/Math/characters/unicode.xml
# XSL for conversion: https://gist.github.com/798546


unicode_to_latex = {
    u"\u0020": "\\space ",
    u"\u0023": "\\#",
    u"\u0024": "\\textdollar ",
    u"\u0025": "\\%",
    u"\u0026": "\\&amp;",
    u"\u0027": "\\textquotesingle ",
    u"\u002A": "\\ast ",
    u"\u005C": "\\textbackslash ",
    u"\u005E": "\\^{}",
    u"\u005F": "\\_",
    u"\u0060": "\\textasciigrave ",
    u"\u007B": "\\lbrace ",
    u"\u007C": "\\vert ",
    u"\u007D": "\\rbrace ",
    u"\u007E": "\\textasciitilde ",
    u"\u00A1": "\\textexclamdown ",
    u"\u00A2": "\\textcent ",
    u"\u00A3": "\\textsterling ",
    u"\u00A4": "\\textcurrency ",
    u"\u00A5": "\\textyen ",
    u"\u00A6": "\\textbrokenbar ",
    u"\u00A7": "\\textsection ",
    u"\u00A8": "\\textasciidieresis ",
    u"\u00A9": "\\textcopyright ",
    u"\u00AA": "\\textordfeminine ",
    u"\u00AB": "\\guillemotleft ",
    u"\u00AC": "\\lnot ",
    u"\u00AD": "\\-",
    u"\u00AE": "\\textregistered ",
    u"\u00AF": "\\textasciimacron ",
    u"\u00B0": "\\textdegree ",
    u"\u00B1": "\\pm ",
    u"\u00B2": "{^2}",
    u"\u00B3": "{^3}",
    u"\u00B4": "\\textasciiacute ",
    u"\u00B5": "\\mathrm{\\mu}",
    u"\u00B6": "\\textparagraph ",
    u"\u00B7": "\\cdot ",
    u"\u00B8": "\\c{}",
    u"\u00B9": "{^1}",
    u"\u00BA": "\\textordmasculine ",
    u"\u00BB": "\\guillemotright ",
    u"\u00BC": "\\textonequarter ",
    u"\u00BD": "\\textonehalf ",
    u"\u00BE": "\\textthreequarters ",
    u"\u00BF": "\\textquestiondown ",
    u"\u00C0": "\\`{A}",
    u"\u00C1": "\\'{A}",
    u"\u00C2": "\\^{A}",
    u"\u00C3": "\\~{A}",
    u"\u00C4": "\\\"{A}",
    u"\u00C5": "\\AA ",
    u"\u00C6": "\\AE ",
    u"\u00C7": "\\c{C}",
    u"\u00C8": "\\`{E}",
    u"\u00C9": "\\'{E}",
    u"\u00CA": "\\^{E}",
    u"\u00CB": "\\\"{E}",
    u"\u00CC": "\\`{I}",
    u"\u00CD": "\\'{I}",
    u"\u00CE": "\\^{I}",
    u"\u00CF": "\\\"{I}",
    u"\u00D0": "\\DH ",
    u"\u00D1": "\\~{N}",
    u"\u00D2": "\\`{O}",
    u"\u00D3": "\\'{O}",
    u"\u00D4": "\\^{O}",
    u"\u00D5": "\\~{O}",
    u"\u00D6": "\\\"{O}",
    u"\u00D7": "\\texttimes ",
    u"\u00D8": "\\O ",
    u"\u00D9": "\\`{U}",
    u"\u00DA": "\\'{U}",
    u"\u00DB": "\\^{U}",
    u"\u00DC": "\\\"{U}",
    u"\u00DD": "\\'{Y}",
    u"\u00DE": "\\TH ",
    u"\u00DF": "\\ss ",
    u"\u00E0": "\\`{a}",
    u"\u00E1": "\\'{a}",
    u"\u00E2": "\\^{a}",
    u"\u00E3": "\\~{a}",
    u"\u00E4": "\\\"{a}",
    u"\u00E5": "\\aa ",
    u"\u00E6": "\\ae ",
    u"\u00E7": "\\c{c}",
    u"\u00E8": "\\`{e}",
    u"\u00E9": "\\'{e}",
    u"\u00EA": "\\^{e}",
    u"\u00EB": "\\\"{e}",
    u"\u00EC": "\\`{\\i}",
    u"\u00ED": "\\'{\\i}",
    u"\u00EE": "\\^{\\i}",
    u"\u00EF": "\\\"{\\i}",
    u"\u00F0": "\\dh ",
    u"\u00F1": "\\~{n}",
    u"\u00F2": "\\`{o}",
    u"\u00F3": "\\'{o}",
    u"\u00F4": "\\^{o}",
    u"\u00F5": "\\~{o}",
    u"\u00F6": "\\\"{o}",
    u"\u00F7": "\\div ",
    u"\u00F8": "\\o ",
    u"\u00F9": "\\`{u}",
    u"\u00FA": "\\'{u}",
    u"\u00FB": "\\^{u}",
    u"\u00FC": "\\\"{u}",
    u"\u00FD": "\\'{y}",
    u"\u00FE": "\\th ",
    u"\u00FF": "\\\"{y}",
    u"\u0100": "\\={A}",
    u"\u0101": "\\={a}",
    u"\u0102": "\\u{A}",
    u"\u0103": "\\u{a}",
    u"\u0104": "\\k{A}",
    u"\u0105": "\\k{a}",
    u"\u0106": "\\'{C}",
    u"\u0107": "\\'{c}",
    u"\u0108": "\\^{C}",
    u"\u0109": "\\^{c}",
    u"\u010A": "\\.{C}",
    u"\u010B": "\\.{c}",
    u"\u010C": "\\v{C}",
    u"\u010D": "\\v{c}",
    u"\u010E": "\\v{D}",
    u"\u010F": "\\v{d}",
    u"\u0110": "\\DJ ",
    u"\u0111": "\\dj ",
    u"\u0112": "\\={E}",
    u"\u0113": "\\={e}",
    u"\u0114": "\\u{E}",
    u"\u0115": "\\u{e}",
    u"\u0116": "\\.{E}",
    u"\u0117": "\\.{e}",
    u"\u0118": "\\k{E}",
    u"\u0119": "\\k{e}",
    u"\u011A": "\\v{E}",
    u"\u011B": "\\v{e}",
    u"\u011C": "\\^{G}",
    u"\u011D": "\\^{g}",
    u"\u011E": "\\u{G}",
    u"\u011F": "\\u{g}",
    u"\u0120": "\\.{G}",
    u"\u0121": "\\.{g}",
    u"\u0122": "\\c{G}",
    u"\u0123": "\\c{g}",
    u"\u0124": "\\^{H}",
    u"\u0125": "\\^{h}",
    u"\u0126": "{\\fontencoding{LELA}\\selectfont\\char40}",
    u"\u0127": "\\Elzxh ",
    u"\u0128": "\\~{I}",
    u"\u0129": "\\~{\\i}",
    u"\u012A": "\\={I}",
    u"\u012B": "\\={\\i}",
    u"\u012C": "\\u{I}",
    u"\u012D": "\\u{\\i}",
    u"\u012E": "\\k{I}",
    u"\u012F": "\\k{i}",
    u"\u0130": "\\.{I}",
    u"\u0131": "\\i ",
    u"\u0132": "IJ",
    u"\u0133": "ij",
    u"\u0134": "\\^{J}",
    u"\u0135": "\\^{\\j}",
    u"\u0136": "\\c{K}",
    u"\u0137": "\\c{k}",
    u"\u0138": "{\\fontencoding{LELA}\\selectfont\\char91}",
    u"\u0139": "\\'{L}",
    u"\u013A": "\\'{l}",
    u"\u013B": "\\c{L}",
    u"\u013C": "\\c{l}",
    u"\u013D": "\\v{L}",
    u"\u013E": "\\v{l}",
    u"\u013F": "{\\fontencoding{LELA}\\selectfont\\char201}",
    u"\u0140": "{\\fontencoding{LELA}\\selectfont\\char202}",
    u"\u0141": "\\L ",
    u"\u0142": "\\l ",
    u"\u0143": "\\'{N}",
    u"\u0144": "\\'{n}",
    u"\u0145": "\\c{N}",
    u"\u0146": "\\c{n}",
    u"\u0147": "\\v{N}",
    u"\u0148": "\\v{n}",
    u"\u0149": "'n",
    u"\u014A": "\\NG ",
    u"\u014B": "\\ng ",
    u"\u014C": "\\={O}",
    u"\u014D": "\\={o}",
    u"\u014E": "\\u{O}",
    u"\u014F": "\\u{o}",
    u"\u0150": "\\H{O}",
    u"\u0151": "\\H{o}",
    u"\u0152": "\\OE ",
    u"\u0153": "\\oe ",
    u"\u0154": "\\'{R}",
    u"\u0155": "\\'{r}",
    u"\u0156": "\\c{R}",
    u"\u0157": "\\c{r}",
    u"\u0158": "\\v{R}",
    u"\u0159": "\\v{r}",
    u"\u015A": "\\'{S}",
    u"\u015B": "\\'{s}",
    u"\u015C": "\\^{S}",
    u"\u015D": "\\^{s}",
    u"\u015E": "\\c{S}",
    u"\u015F": "\\c{s}",
    u"\u0160": "\\v{S}",
    u"\u0161": "\\v{s}",
    u"\u0162": "\\c{T}",
    u"\u0163": "\\c{t}",
    u"\u0164": "\\v{T}",
    u"\u0165": "\\v{t}",
    u"\u0166": "{\\fontencoding{LELA}\\selectfont\\char47}",
    u"\u0167": "{\\fontencoding{LELA}\\selectfont\\char63}",
    u"\u0168": "\\~{U}",
    u"\u0169": "\\~{u}",
    u"\u016A": "\\={U}",
    u"\u016B": "\\={u}",
    u"\u016C": "\\u{U}",
    u"\u016D": "\\u{u}",
    u"\u016E": "\\r{U}",
    u"\u016F": "\\r{u}",
    u"\u0170": "\\H{U}",
    u"\u0171": "\\H{u}",
    u"\u0172": "\\k{U}",
    u"\u0173": "\\k{u}",
    u"\u0174": "\\^{W}",
    u"\u0175": "\\^{w}",
    u"\u0176": "\\^{Y}",
    u"\u0177": "\\^{y}",
    u"\u0178": "\\\"{Y}",
    u"\u0179": "\\'{Z}",
    u"\u017A": "\\'{z}",
    u"\u017B": "\\.{Z}",
    u"\u017C": "\\.{z}",
    u"\u017D": "\\v{Z}",
    u"\u017E": "\\v{z}",
    u"\u0195": "\\texthvlig ",
    u"\u019E": "\\textnrleg ",
    u"\u01AA": "\\eth ",
    u"\u01BA": "{\\fontencoding{LELA}\\selectfont\\char195}",
    u"\u01C2": "\\textdoublepipe ",
    u"\u01F5": "\\'{g}",
    u"\u0250": "\\Elztrna ",
    u"\u0252": "\\Elztrnsa ",
    u"\u0254": "\\Elzopeno ",
    u"\u0256": "\\Elzrtld ",
    u"\u0258": "{\\fontencoding{LEIP}\\selectfont\\char61}",
    u"\u0259": "\\Elzschwa ",
    u"\u025B": "\\varepsilon ",
    u"\u0263": "\\Elzpgamma ",
    u"\u0264": "\\Elzpbgam ",
    u"\u0265": "\\Elztrnh ",
    u"\u026C": "\\Elzbtdl ",
    u"\u026D": "\\Elzrtll ",
    u"\u026F": "\\Elztrnm ",
    u"\u0270": "\\Elztrnmlr ",
    u"\u0271": "\\Elzltlmr ",
    u"\u0272": "\\Elzltln ",
    u"\u0273": "\\Elzrtln ",
    u"\u0277": "\\Elzclomeg ",
    u"\u0278": "\\textphi ",
    u"\u0279": "\\Elztrnr ",
    u"\u027A": "\\Elztrnrl ",
    u"\u027B": "\\Elzrttrnr ",
    u"\u027C": "\\Elzrl ",
    u"\u027D": "\\Elzrtlr ",
    u"\u027E": "\\Elzfhr ",
    u"\u027F": "{\\fontencoding{LEIP}\\selectfont\\char202}",
    u"\u0282": "\\Elzrtls ",
    u"\u0283": "\\Elzesh ",
    u"\u0287": "\\Elztrnt ",
    u"\u0288": "\\Elzrtlt ",
    u"\u028A": "\\Elzpupsil ",
    u"\u028B": "\\Elzpscrv ",
    u"\u028C": "\\Elzinvv ",
    u"\u028D": "\\Elzinvw ",
    u"\u028E": "\\Elztrny ",
    u"\u0290": "\\Elzrtlz ",
    u"\u0292": "\\Elzyogh ",
    u"\u0294": "\\Elzglst ",
    u"\u0295": "\\Elzreglst ",
    u"\u0296": "\\Elzinglst ",
    u"\u029E": "\\textturnk ",
    u"\u02A4": "\\Elzdyogh ",
    u"\u02A7": "\\Elztesh ",
    u"\u02C7": "\\textasciicaron ",
    u"\u02C8": "\\Elzverts ",
    u"\u02CC": "\\Elzverti ",
    u"\u02D0": "\\Elzlmrk ",
    u"\u02D1": "\\Elzhlmrk ",
    u"\u02D2": "\\Elzsbrhr ",
    u"\u02D3": "\\Elzsblhr ",
    u"\u02D4": "\\Elzrais ",
    u"\u02D5": "\\Elzlow ",
    u"\u02D8": "\\textasciibreve ",
    u"\u02D9": "\\textperiodcentered ",
    u"\u02DA": "\\r{}",
    u"\u02DB": "\\k{}",
    u"\u02DC": "\\texttildelow ",
    u"\u02DD": "\\H{}",
    u"\u02E5": "\\tone{55}",
    u"\u02E6": "\\tone{44}",
    u"\u02E7": "\\tone{33}",
    u"\u02E8": "\\tone{22}",
    u"\u02E9": "\\tone{11}",
    u"\u0300": "\\`",
    u"\u0301": "\\'",
    u"\u0302": "\\^",
    u"\u0303": "\\~",
    u"\u0304": "\\=",
    u"\u0306": "\\u",
    u"\u0307": "\\.",
    u"\u0308": "\\\"",
    u"\u030A": "\\r",
    u"\u030B": "\\H",
    u"\u030C": "\\v",
    u"\u030F": "\\cyrchar\\C",
    u"\u0311": "{\\fontencoding{LECO}\\selectfont\\char177}",
    u"\u0318": "{\\fontencoding{LECO}\\selectfont\\char184}",
    u"\u0319": "{\\fontencoding{LECO}\\selectfont\\char185}",
    u"\u0321": "\\Elzpalh ",
    u"\u0322": "\\Elzrh ",
    u"\u0327": "\\c",
    u"\u0328": "\\k",
    u"\u032A": "\\Elzsbbrg ",
    u"\u032B": "{\\fontencoding{LECO}\\selectfont\\char203}",
    u"\u032F": "{\\fontencoding{LECO}\\selectfont\\char207}",
    u"\u0335": "\\Elzxl ",
    u"\u0336": "\\Elzbar ",
    u"\u0337": "{\\fontencoding{LECO}\\selectfont\\char215}",
    u"\u0338": "{\\fontencoding{LECO}\\selectfont\\char216}",
    u"\u033A": "{\\fontencoding{LECO}\\selectfont\\char218}",
    u"\u033B": "{\\fontencoding{LECO}\\selectfont\\char219}",
    u"\u033C": "{\\fontencoding{LECO}\\selectfont\\char220}",
    u"\u033D": "{\\fontencoding{LECO}\\selectfont\\char221}",
    u"\u0361": "{\\fontencoding{LECO}\\selectfont\\char225}",
    u"\u0386": "\\'{A}",
    u"\u0388": "\\'{E}",
    u"\u0389": "\\'{H}",
    u"\u038A": "\\'{}{I}",
    u"\u038C": "\\'{}O",
    u"\u038E": "\\mathrm{'Y}",
    u"\u038F": "\\mathrm{'\\Omega}",
    u"\u0390": "\\acute{\\ddot{\\iota}}",
    u"\u0391": "\\Alpha ",
    u"\u0392": "\\Beta ",
    u"\u0393": "\\Gamma ",
    u"\u0394": "\\Delta ",
    u"\u0395": "\\Epsilon ",
    u"\u0396": "\\Zeta ",
    u"\u0397": "\\Eta ",
    u"\u0398": "\\Theta ",
    u"\u0399": "\\Iota ",
    u"\u039A": "\\Kappa ",
    u"\u039B": "\\Lambda ",
    u"\u039E": "\\Xi ",
    u"\u03A0": "\\Pi ",
    u"\u03A1": "\\Rho ",
    u"\u03A3": "\\Sigma ",
    u"\u03A4": "\\Tau ",
    u"\u03A5": "\\Upsilon ",
    u"\u03A6": "\\Phi ",
    u"\u03A7": "\\Chi ",
    u"\u03A8": "\\Psi ",
    u"\u03A9": "\\Omega ",
    u"\u03AA": "\\mathrm{\\ddot{I}}",
    u"\u03AB": "\\mathrm{\\ddot{Y}}",
    u"\u03AC": "\\'{$\\alpha$}",
    u"\u03AD": "\\acute{\\epsilon}",
    u"\u03AE": "\\acute{\\eta}",
    u"\u03AF": "\\acute{\\iota}",
    u"\u03B0": "\\acute{\\ddot{\\upsilon}}",
    u"\u03B1": "\\alpha ",
    u"\u03B2": "\\beta ",
    u"\u03B3": "\\gamma ",
    u"\u03B4": "\\delta ",
    u"\u03B5": "\\epsilon ",
    u"\u03B6": "\\zeta ",
    u"\u03B7": "\\eta ",
    u"\u03B8": "\\texttheta ",
    u"\u03B9": "\\iota ",
    u"\u03BA": "\\kappa ",
    u"\u03BB": "\\lambda ",
    u"\u03BC": "\\mu ",
    u"\u03BD": "\\nu ",
    u"\u03BE": "\\xi ",
    u"\u03C0": "\\pi ",
    u"\u03C1": "\\rho ",
    u"\u03C2": "\\varsigma ",
    u"\u03C3": "\\sigma ",
    u"\u03C4": "\\tau ",
    u"\u03C5": "\\upsilon ",
    u"\u03C6": "\\varphi ",
    u"\u03C7": "\\chi ",
    u"\u03C8": "\\psi ",
    u"\u03C9": "\\omega ",
    u"\u03CA": "\\ddot{\\iota}",
    u"\u03CB": "\\ddot{\\upsilon}",
    u"\u03CC": "\\'{o}",
    u"\u03CD": "\\acute{\\upsilon}",
    u"\u03CE": "\\acute{\\omega}",
    u"\u03D0": "\\Pisymbol{ppi022}{87}",
    u"\u03D1": "\\textvartheta ",
    u"\u03D2": "\\Upsilon ",
    u"\u03D5": "\\phi ",
    u"\u03D6": "\\varpi ",
    u"\u03DA": "\\Stigma ",
    u"\u03DC": "\\Digamma ",
    u"\u03DD": "\\digamma ",
    u"\u03DE": "\\Koppa ",
    u"\u03E0": "\\Sampi ",
    u"\u03F0": "\\varkappa ",
    u"\u03F1": "\\varrho ",
    u"\u03F4": "\\textTheta ",
    u"\u03F6": "\\backepsilon ",
    u"\u0401": "\\cyrchar\\CYRYO ",
    u"\u0402": "\\cyrchar\\CYRDJE ",
    u"\u0403": "\\cyrchar{\\'\\CYRG}",
    u"\u0404": "\\cyrchar\\CYRIE ",
    u"\u0405": "\\cyrchar\\CYRDZE ",
    u"\u0406": "\\cyrchar\\CYRII ",
    u"\u0407": "\\cyrchar\\CYRYI ",
    u"\u0408": "\\cyrchar\\CYRJE ",
    u"\u0409": "\\cyrchar\\CYRLJE ",
    u"\u040A": "\\cyrchar\\CYRNJE ",
    u"\u040B": "\\cyrchar\\CYRTSHE ",
    u"\u040C": "\\cyrchar{\\'\\CYRK}",
    u"\u040E": "\\cyrchar\\CYRUSHRT ",
    u"\u040F": "\\cyrchar\\CYRDZHE ",
    u"\u0410": "\\cyrchar\\CYRA ",
    u"\u0411": "\\cyrchar\\CYRB ",
    u"\u0412": "\\cyrchar\\CYRV ",
    u"\u0413": "\\cyrchar\\CYRG ",
    u"\u0414": "\\cyrchar\\CYRD ",
    u"\u0415": "\\cyrchar\\CYRE ",
    u"\u0416": "\\cyrchar\\CYRZH ",
    u"\u0417": "\\cyrchar\\CYRZ ",
    u"\u0418": "\\cyrchar\\CYRI ",
    u"\u0419": "\\cyrchar\\CYRISHRT ",
    u"\u041A": "\\cyrchar\\CYRK ",
    u"\u041B": "\\cyrchar\\CYRL ",
    u"\u041C": "\\cyrchar\\CYRM ",
    u"\u041D": "\\cyrchar\\CYRN ",
    u"\u041E": "\\cyrchar\\CYRO ",
    u"\u041F": "\\cyrchar\\CYRP ",
    u"\u0420": "\\cyrchar\\CYRR ",
    u"\u0421": "\\cyrchar\\CYRS ",
    u"\u0422": "\\cyrchar\\CYRT ",
    u"\u0423": "\\cyrchar\\CYRU ",
    u"\u0424": "\\cyrchar\\CYRF ",
    u"\u0425": "\\cyrchar\\CYRH ",
    u"\u0426": "\\cyrchar\\CYRC ",
    u"\u0427": "\\cyrchar\\CYRCH ",
    u"\u0428": "\\cyrchar\\CYRSH ",
    u"\u0429": "\\cyrchar\\CYRSHCH ",
    u"\u042A": "\\cyrchar\\CYRHRDSN ",
    u"\u042B": "\\cyrchar\\CYRERY ",
    u"\u042C": "\\cyrchar\\CYRSFTSN ",
    u"\u042D": "\\cyrchar\\CYREREV ",
    u"\u042E": "\\cyrchar\\CYRYU ",
    u"\u042F": "\\cyrchar\\CYRYA ",
    u"\u0430": "\\cyrchar\\cyra ",
    u"\u0431": "\\cyrchar\\cyrb ",
    u"\u0432": "\\cyrchar\\cyrv ",
    u"\u0433": "\\cyrchar\\cyrg ",
    u"\u0434": "\\cyrchar\\cyrd ",
    u"\u0435": "\\cyrchar\\cyre ",
    u"\u0436": "\\cyrchar\\cyrzh ",
    u"\u0437": "\\cyrchar\\cyrz ",
    u"\u0438": "\\cyrchar\\cyri ",
    u"\u0439": "\\cyrchar\\cyrishrt ",
    u"\u043A": "\\cyrchar\\cyrk ",
    u"\u043B": "\\cyrchar\\cyrl ",
    u"\u043C": "\\cyrchar\\cyrm ",
    u"\u043D": "\\cyrchar\\cyrn ",
    u"\u043E": "\\cyrchar\\cyro ",
    u"\u043F": "\\cyrchar\\cyrp ",
    u"\u0440": "\\cyrchar\\cyrr ",
    u"\u0441": "\\cyrchar\\cyrs ",
    u"\u0442": "\\cyrchar\\cyrt ",
    u"\u0443": "\\cyrchar\\cyru ",
    u"\u0444": "\\cyrchar\\cyrf ",
    u"\u0445": "\\cyrchar\\cyrh ",
    u"\u0446": "\\cyrchar\\cyrc ",
    u"\u0447": "\\cyrchar\\cyrch ",
    u"\u0448": "\\cyrchar\\cyrsh ",
    u"\u0449": "\\cyrchar\\cyrshch ",
    u"\u044A": "\\cyrchar\\cyrhrdsn ",
    u"\u044B": "\\cyrchar\\cyrery ",
    u"\u044C": "\\cyrchar\\cyrsftsn ",
    u"\u044D": "\\cyrchar\\cyrerev ",
    u"\u044E": "\\cyrchar\\cyryu ",
    u"\u044F": "\\cyrchar\\cyrya ",
    u"\u0451": "\\cyrchar\\cyryo ",
    u"\u0452": "\\cyrchar\\cyrdje ",
    u"\u0453": "\\cyrchar{\\'\\cyrg}",
    u"\u0454": "\\cyrchar\\cyrie ",
    u"\u0455": "\\cyrchar\\cyrdze ",
    u"\u0456": "\\cyrchar\\cyrii ",
    u"\u0457": "\\cyrchar\\cyryi ",
    u"\u0458": "\\cyrchar\\cyrje ",
    u"\u0459": "\\cyrchar\\cyrlje ",
    u"\u045A": "\\cyrchar\\cyrnje ",
    u"\u045B": "\\cyrchar\\cyrtshe ",
    u"\u045C": "\\cyrchar{\\'\\cyrk}",
    u"\u045E": "\\cyrchar\\cyrushrt ",
    u"\u045F": "\\cyrchar\\cyrdzhe ",
    u"\u0460": "\\cyrchar\\CYROMEGA ",
    u"\u0461": "\\cyrchar\\cyromega ",
    u"\u0462": "\\cyrchar\\CYRYAT ",
    u"\u0464": "\\cyrchar\\CYRIOTE ",
    u"\u0465": "\\cyrchar\\cyriote ",
    u"\u0466": "\\cyrchar\\CYRLYUS ",
    u"\u0467": "\\cyrchar\\cyrlyus ",
    u"\u0468": "\\cyrchar\\CYRIOTLYUS ",
    u"\u0469": "\\cyrchar\\cyriotlyus ",
    u"\u046A": "\\cyrchar\\CYRBYUS ",
    u"\u046C": "\\cyrchar\\CYRIOTBYUS ",
    u"\u046D": "\\cyrchar\\cyriotbyus ",
    u"\u046E": "\\cyrchar\\CYRKSI ",
    u"\u046F": "\\cyrchar\\cyrksi ",
    u"\u0470": "\\cyrchar\\CYRPSI ",
    u"\u0471": "\\cyrchar\\cyrpsi ",
    u"\u0472": "\\cyrchar\\CYRFITA ",
    u"\u0474": "\\cyrchar\\CYRIZH ",
    u"\u0478": "\\cyrchar\\CYRUK ",
    u"\u0479": "\\cyrchar\\cyruk ",
    u"\u047A": "\\cyrchar\\CYROMEGARND ",
    u"\u047B": "\\cyrchar\\cyromegarnd ",
    u"\u047C": "\\cyrchar\\CYROMEGATITLO ",
    u"\u047D": "\\cyrchar\\cyromegatitlo ",
    u"\u047E": "\\cyrchar\\CYROT ",
    u"\u047F": "\\cyrchar\\cyrot ",
    u"\u0480": "\\cyrchar\\CYRKOPPA ",
    u"\u0481": "\\cyrchar\\cyrkoppa ",
    u"\u0482": "\\cyrchar\\cyrthousands ",
    u"\u0488": "\\cyrchar\\cyrhundredthousands ",
    u"\u0489": "\\cyrchar\\cyrmillions ",
    u"\u048C": "\\cyrchar\\CYRSEMISFTSN ",
    u"\u048D": "\\cyrchar\\cyrsemisftsn ",
    u"\u048E": "\\cyrchar\\CYRRTICK ",
    u"\u048F": "\\cyrchar\\cyrrtick ",
    u"\u0490": "\\cyrchar\\CYRGUP ",
    u"\u0491": "\\cyrchar\\cyrgup ",
    u"\u0492": "\\cyrchar\\CYRGHCRS ",
    u"\u0493": "\\cyrchar\\cyrghcrs ",
    u"\u0494": "\\cyrchar\\CYRGHK ",
    u"\u0495": "\\cyrchar\\cyrghk ",
    u"\u0496": "\\cyrchar\\CYRZHDSC ",
    u"\u0497": "\\cyrchar\\cyrzhdsc ",
    u"\u0498": "\\cyrchar\\CYRZDSC ",
    u"\u0499": "\\cyrchar\\cyrzdsc ",
    u"\u049A": "\\cyrchar\\CYRKDSC ",
    u"\u049B": "\\cyrchar\\cyrkdsc ",
    u"\u049C": "\\cyrchar\\CYRKVCRS ",
    u"\u049D": "\\cyrchar\\cyrkvcrs ",
    u"\u049E": "\\cyrchar\\CYRKHCRS ",
    u"\u049F": "\\cyrchar\\cyrkhcrs ",
    u"\u04A0": "\\cyrchar\\CYRKBEAK ",
    u"\u04A1": "\\cyrchar\\cyrkbeak ",
    u"\u04A2": "\\cyrchar\\CYRNDSC ",
    u"\u04A3": "\\cyrchar\\cyrndsc ",
    u"\u04A4": "\\cyrchar\\CYRNG ",
    u"\u04A5": "\\cyrchar\\cyrng ",
    u"\u04A6": "\\cyrchar\\CYRPHK ",
    u"\u04A7": "\\cyrchar\\cyrphk ",
    u"\u04A8": "\\cyrchar\\CYRABHHA ",
    u"\u04A9": "\\cyrchar\\cyrabhha ",
    u"\u04AA": "\\cyrchar\\CYRSDSC ",
    u"\u04AB": "\\cyrchar\\cyrsdsc ",
    u"\u04AC": "\\cyrchar\\CYRTDSC ",
    u"\u04AD": "\\cyrchar\\cyrtdsc ",
    u"\u04AE": "\\cyrchar\\CYRY ",
    u"\u04AF": "\\cyrchar\\cyry ",
    u"\u04B0": "\\cyrchar\\CYRYHCRS ",
    u"\u04B1": "\\cyrchar\\cyryhcrs ",
    u"\u04B2": "\\cyrchar\\CYRHDSC ",
    u"\u04B3": "\\cyrchar\\cyrhdsc ",
    u"\u04B4": "\\cyrchar\\CYRTETSE ",
    u"\u04B5": "\\cyrchar\\cyrtetse ",
    u"\u04B6": "\\cyrchar\\CYRCHRDSC ",
    u"\u04B7": "\\cyrchar\\cyrchrdsc ",
    u"\u04B8": "\\cyrchar\\CYRCHVCRS ",
    u"\u04B9": "\\cyrchar\\cyrchvcrs ",
    u"\u04BA": "\\cyrchar\\CYRSHHA ",
    u"\u04BB": "\\cyrchar\\cyrshha ",
    u"\u04BC": "\\cyrchar\\CYRABHCH ",
    u"\u04BD": "\\cyrchar\\cyrabhch ",
    u"\u04BE": "\\cyrchar\\CYRABHCHDSC ",
    u"\u04BF": "\\cyrchar\\cyrabhchdsc ",
    u"\u04C0": "\\cyrchar\\CYRpalochka ",
    u"\u04C3": "\\cyrchar\\CYRKHK ",
    u"\u04C4": "\\cyrchar\\cyrkhk ",
    u"\u04C7": "\\cyrchar\\CYRNHK ",
    u"\u04C8": "\\cyrchar\\cyrnhk ",
    u"\u04CB": "\\cyrchar\\CYRCHLDSC ",
    u"\u04CC": "\\cyrchar\\cyrchldsc ",
    u"\u04D4": "\\cyrchar\\CYRAE ",
    u"\u04D5": "\\cyrchar\\cyrae ",
    u"\u04D8": "\\cyrchar\\CYRSCHWA ",
    u"\u04D9": "\\cyrchar\\cyrschwa ",
    u"\u04E0": "\\cyrchar\\CYRABHDZE ",
    u"\u04E1": "\\cyrchar\\cyrabhdze ",
    u"\u04E8": "\\cyrchar\\CYROTLD ",
    u"\u04E9": "\\cyrchar\\cyrotld ",
    u"\u2002": "\\hspace{0.6em}",
    u"\u2003": "\\hspace{1em}",
    u"\u2004": "\\hspace{0.33em}",
    u"\u2005": "\\hspace{0.25em}",
    u"\u2006": "\\hspace{0.166em}",
    u"\u2007": "\\hphantom{0}",
    u"\u2008": "\\hphantom{,}",
    u"\u2009": "\\hspace{0.167em}",
    u"\u2009-0200A-0200A": "\\;",
    u"\u200A": "\\mkern1mu ",
    u"\u2013": "\\textendash ",
    u"\u2014": "\\textemdash ",
    u"\u2015": "\\rule{1em}{1pt}",
    u"\u2016": "\\Vert ",
    u"\u201B": "\\Elzreapos ",
    u"\u201C": "\\textquotedblleft ",
    u"\u201D": "\\textquotedblright ",
    u"\u201E": ",,",
    u"\u2020": "\\textdagger ",
    u"\u2021": "\\textdaggerdbl ",
    u"\u2022": "\\textbullet ",
    u"\u2025": "..",
    u"\u2026": "\\ldots ",
    u"\u2030": "\\textperthousand ",
    u"\u2031": "\\textpertenthousand ",
    u"\u2032": "{'}",
    u"\u2033": "{''}",
    u"\u2034": "{'''}",
    u"\u2035": "\\backprime ",
    u"\u2039": "\\guilsinglleft ",
    u"\u203A": "\\guilsinglright ",
    u"\u2057": "''''",
    u"\u205F": "\\mkern4mu ",
    u"\u2060": "\\nolinebreak ",
    u"\u20A7": "\\ensuremath{\\Elzpes}",
    u"\u20AC": "\\mbox{\\texteuro} ",
    u"\u20DB": "\\dddot ",
    u"\u20DC": "\\ddddot ",
    u"\u2102": "\\mathbb{C}",
    u"\u210A": "\\mathscr{g}",
    u"\u210B": "\\mathscr{H}",
    u"\u210C": "\\mathfrak{H}",
    u"\u210D": "\\mathbb{H}",
    u"\u210F": "\\hslash ",
    u"\u2110": "\\mathscr{I}",
    u"\u2111": "\\mathfrak{I}",
    u"\u2112": "\\mathscr{L}",
    u"\u2113": "\\mathscr{l}",
    u"\u2115": "\\mathbb{N}",
    u"\u2116": "\\cyrchar\\textnumero ",
    u"\u2118": "\\wp ",
    u"\u2119": "\\mathbb{P}",
    u"\u211A": "\\mathbb{Q}",
    u"\u211B": "\\mathscr{R}",
    u"\u211C": "\\mathfrak{R}",
    u"\u211D": "\\mathbb{R}",
    u"\u211E": "\\Elzxrat ",
    u"\u2122": "\\texttrademark ",
    u"\u2124": "\\mathbb{Z}",
    u"\u2126": "\\Omega ",
    u"\u2127": "\\mho ",
    u"\u2128": "\\mathfrak{Z}",
    u"\u2129": "\\ElsevierGlyph{2129}",
    u"\u212B": "\\AA ",
    u"\u212C": "\\mathscr{B}",
    u"\u212D": "\\mathfrak{C}",
    u"\u212F": "\\mathscr{e}",
    u"\u2130": "\\mathscr{E}",
    u"\u2131": "\\mathscr{F}",
    u"\u2133": "\\mathscr{M}",
    u"\u2134": "\\mathscr{o}",
    u"\u2135": "\\aleph ",
    u"\u2136": "\\beth ",
    u"\u2137": "\\gimel ",
    u"\u2138": "\\daleth ",
    u"\u2153": "\\textfrac{1}{3}",
    u"\u2154": "\\textfrac{2}{3}",
    u"\u2155": "\\textfrac{1}{5}",
    u"\u2156": "\\textfrac{2}{5}",
    u"\u2157": "\\textfrac{3}{5}",
    u"\u2158": "\\textfrac{4}{5}",
    u"\u2159": "\\textfrac{1}{6}",
    u"\u215A": "\\textfrac{5}{6}",
    u"\u215B": "\\textfrac{1}{8}",
    u"\u215C": "\\textfrac{3}{8}",
    u"\u215D": "\\textfrac{5}{8}",
    u"\u215E": "\\textfrac{7}{8}",
    u"\u2190": "\\leftarrow ",
    u"\u2191": "\\uparrow ",
    u"\u2192": "\\rightarrow ",
    u"\u2193": "\\downarrow ",
    u"\u2194": "\\leftrightarrow ",
    u"\u2195": "\\updownarrow ",
    u"\u2196": "\\nwarrow ",
    u"\u2197": "\\nearrow ",
    u"\u2198": "\\searrow ",
    u"\u2199": "\\swarrow ",
    u"\u219A": "\\nleftarrow ",
    u"\u219B": "\\nrightarrow ",
    u"\u219C": "\\arrowwaveright ",
    u"\u219D": "\\arrowwaveright ",
    u"\u219E": "\\twoheadleftarrow ",
    u"\u21A0": "\\twoheadrightarrow ",
    u"\u21A2": "\\leftarrowtail ",
    u"\u21A3": "\\rightarrowtail ",
    u"\u21A6": "\\mapsto ",
    u"\u21A9": "\\hookleftarrow ",
    u"\u21AA": "\\hookrightarrow ",
    u"\u21AB": "\\looparrowleft ",
    u"\u21AC": "\\looparrowright ",
    u"\u21AD": "\\leftrightsquigarrow ",
    u"\u21AE": "\\nleftrightarrow ",
    u"\u21B0": "\\Lsh ",
    u"\u21B1": "\\Rsh ",
    u"\u21B3": "\\ElsevierGlyph{21B3}",
    u"\u21B6": "\\curvearrowleft ",
    u"\u21B7": "\\curvearrowright ",
    u"\u21BA": "\\circlearrowleft ",
    u"\u21BB": "\\circlearrowright ",
    u"\u21BC": "\\leftharpoonup ",
    u"\u21BD": "\\leftharpoondown ",
    u"\u21BE": "\\upharpoonright ",
    u"\u21BF": "\\upharpoonleft ",
    u"\u21C0": "\\rightharpoonup ",
    u"\u21C1": "\\rightharpoondown ",
    u"\u21C2": "\\downharpoonright ",
    u"\u21C3": "\\downharpoonleft ",
    u"\u21C4": "\\rightleftarrows ",
    u"\u21C5": "\\dblarrowupdown ",
    u"\u21C6": "\\leftrightarrows ",
    u"\u21C7": "\\leftleftarrows ",
    u"\u21C8": "\\upuparrows ",
    u"\u21C9": "\\rightrightarrows ",
    u"\u21CA": "\\downdownarrows ",
    u"\u21CB": "\\leftrightharpoons ",
    u"\u21CC": "\\rightleftharpoons ",
    u"\u21CD": "\\nLeftarrow ",
    u"\u21CE": "\\nLeftrightarrow ",
    u"\u21CF": "\\nRightarrow ",
    u"\u21D0": "\\Leftarrow ",
    u"\u21D1": "\\Uparrow ",
    u"\u21D2": "\\Rightarrow ",
    u"\u21D3": "\\Downarrow ",
    u"\u21D4": "\\Leftrightarrow ",
    u"\u21D5": "\\Updownarrow ",
    u"\u21DA": "\\Lleftarrow ",
    u"\u21DB": "\\Rrightarrow ",
    u"\u21DD": "\\rightsquigarrow ",
    u"\u21F5": "\\DownArrowUpArrow ",
    u"\u2200": "\\forall ",
    u"\u2201": "\\complement ",
    u"\u2202": "\\partial ",
    u"\u2203": "\\exists ",
    u"\u2204": "\\nexists ",
    u"\u2205": "\\varnothing ",
    u"\u2207": "\\nabla ",
    u"\u2208": "\\in ",
    u"\u2209": "\\not\\in ",
    u"\u220B": "\\ni ",
    u"\u220C": "\\not\\ni ",
    u"\u220F": "\\prod ",
    u"\u2210": "\\coprod ",
    u"\u2211": "\\sum ",
    u"\u2213": "\\mp ",
    u"\u2214": "\\dotplus ",
    u"\u2216": "\\setminus ",
    u"\u2217": "{_\\ast}",
    u"\u2218": "\\circ ",
    u"\u2219": "\\bullet ",
    u"\u221A": "\\surd ",
    u"\u221D": "\\propto ",
    u"\u221E": "\\infty ",
    u"\u221F": "\\rightangle ",
    u"\u2220": "\\angle ",
    u"\u2221": "\\measuredangle ",
    u"\u2222": "\\sphericalangle ",
    u"\u2223": "\\mid ",
    u"\u2224": "\\nmid ",
    u"\u2225": "\\parallel ",
    u"\u2226": "\\nparallel ",
    u"\u2227": "\\wedge ",
    u"\u2228": "\\vee ",
    u"\u2229": "\\cap ",
    u"\u222A": "\\cup ",
    u"\u222B": "\\int ",
    u"\u222C": "\\int\\!\\int ",
    u"\u222D": "\\int\\!\\int\\!\\int ",
    u"\u222E": "\\oint ",
    u"\u222F": "\\surfintegral ",
    u"\u2230": "\\volintegral ",
    u"\u2231": "\\clwintegral ",
    u"\u2232": "\\ElsevierGlyph{2232}",
    u"\u2233": "\\ElsevierGlyph{2233}",
    u"\u2234": "\\therefore ",
    u"\u2235": "\\because ",
    u"\u2237": "\\Colon ",
    u"\u2238": "\\ElsevierGlyph{2238}",
    u"\u223A": "\\mathbin{{:}\\!\\!{-}\\!\\!{:}}",
    u"\u223B": "\\homothetic ",
    u"\u223C": "\\sim ",
    u"\u223D": "\\backsim ",
    u"\u223E": "\\lazysinv ",
    u"\u2240": "\\wr ",
    u"\u2241": "\\not\\sim ",
    u"\u2242": "\\ElsevierGlyph{2242}",
    u"\u2242-00338": "\\NotEqualTilde ",
    u"\u2243": "\\simeq ",
    u"\u2244": "\\not\\simeq ",
    u"\u2245": "\\cong ",
    u"\u2246": "\\approxnotequal ",
    u"\u2247": "\\not\\cong ",
    u"\u2248": "\\approx ",
    u"\u2249": "\\not\\approx ",
    u"\u224A": "\\approxeq ",
    u"\u224B": "\\tildetrpl ",
    u"\u224B-00338": "\\not\\apid ",
    u"\u224C": "\\allequal ",
    u"\u224D": "\\asymp ",
    u"\u224E": "\\Bumpeq ",
    u"\u224E-00338": "\\NotHumpDownHump ",
    u"\u224F": "\\bumpeq ",
    u"\u224F-00338": "\\NotHumpEqual ",
    u"\u2250": "\\doteq ",
    u"\u2250-00338": "\\not\\doteq",
    u"\u2251": "\\doteqdot ",
    u"\u2252": "\\fallingdotseq ",
    u"\u2253": "\\risingdotseq ",
    u"\u2254": ":=",
    u"\u2255": "=:",
    u"\u2256": "\\eqcirc ",
    u"\u2257": "\\circeq ",
    u"\u2259": "\\estimates ",
    u"\u225A": "\\ElsevierGlyph{225A}",
    u"\u225B": "\\starequal ",
    u"\u225C": "\\triangleq ",
    u"\u225F": "\\ElsevierGlyph{225F}",
    u"\u2260": "\\not =",
    u"\u2261": "\\equiv ",
    u"\u2262": "\\not\\equiv ",
    u"\u2264": "\\leq ",
    u"\u2265": "\\geq ",
    u"\u2266": "\\leqq ",
    u"\u2267": "\\geqq ",
    u"\u2268": "\\lneqq ",
    u"\u2268-0FE00": "\\lvertneqq ",
    u"\u2269": "\\gneqq ",
    u"\u2269-0FE00": "\\gvertneqq ",
    u"\u226A": "\\ll ",
    u"\u226A-00338": "\\NotLessLess ",
    u"\u226B": "\\gg ",
    u"\u226B-00338": "\\NotGreaterGreater ",
    u"\u226C": "\\between ",
    u"\u226D": "\\not\\kern-0.3em\\times ",
    u"\u226E": "\\not&lt;",
    u"\u226F": "\\not&gt;",
    u"\u2270": "\\not\\leq ",
    u"\u2271": "\\not\\geq ",
    u"\u2272": "\\lessequivlnt ",
    u"\u2273": "\\greaterequivlnt ",
    u"\u2274": "\\ElsevierGlyph{2274}",
    u"\u2275": "\\ElsevierGlyph{2275}",
    u"\u2276": "\\lessgtr ",
    u"\u2277": "\\gtrless ",
    u"\u2278": "\\notlessgreater ",
    u"\u2279": "\\notgreaterless ",
    u"\u227A": "\\prec ",
    u"\u227B": "\\succ ",
    u"\u227C": "\\preccurlyeq ",
    u"\u227D": "\\succcurlyeq ",
    u"\u227E": "\\precapprox ",
    u"\u227E-00338": "\\NotPrecedesTilde ",
    u"\u227F": "\\succapprox ",
    u"\u227F-00338": "\\NotSucceedsTilde ",
    u"\u2280": "\\not\\prec ",
    u"\u2281": "\\not\\succ ",
    u"\u2282": "\\subset ",
    u"\u2283": "\\supset ",
    u"\u2284": "\\not\\subset ",
    u"\u2285": "\\not\\supset ",
    u"\u2286": "\\subseteq ",
    u"\u2287": "\\supseteq ",
    u"\u2288": "\\not\\subseteq ",
    u"\u2289": "\\not\\supseteq ",
    u"\u228A": "\\subsetneq ",
    u"\u228A-0FE00": "\\varsubsetneqq ",
    u"\u228B": "\\supsetneq ",
    u"\u228B-0FE00": "\\varsupsetneq ",
    u"\u228E": "\\uplus ",
    u"\u228F": "\\sqsubset ",
    u"\u228F-00338": "\\NotSquareSubset ",
    u"\u2290": "\\sqsupset ",
    u"\u2290-00338": "\\NotSquareSuperset ",
    u"\u2291": "\\sqsubseteq ",
    u"\u2292": "\\sqsupseteq ",
    u"\u2293": "\\sqcap ",
    u"\u2294": "\\sqcup ",
    u"\u2295": "\\oplus ",
    u"\u2296": "\\ominus ",
    u"\u2297": "\\otimes ",
    u"\u2298": "\\oslash ",
    u"\u2299": "\\odot ",
    u"\u229A": "\\circledcirc ",
    u"\u229B": "\\circledast ",
    u"\u229D": "\\circleddash ",
    u"\u229E": "\\boxplus ",
    u"\u229F": "\\boxminus ",
    u"\u22A0": "\\boxtimes ",
    u"\u22A1": "\\boxdot ",
    u"\u22A2": "\\vdash ",
    u"\u22A3": "\\dashv ",
    u"\u22A4": "\\top ",
    u"\u22A5": "\\perp ",
    u"\u22A7": "\\truestate ",
    u"\u22A8": "\\forcesextra ",
    u"\u22A9": "\\Vdash ",
    u"\u22AA": "\\Vvdash ",
    u"\u22AB": "\\VDash ",
    u"\u22AC": "\\nvdash ",
    u"\u22AD": "\\nvDash ",
    u"\u22AE": "\\nVdash ",
    u"\u22AF": "\\nVDash ",
    u"\u22B2": "\\vartriangleleft ",
    u"\u22B3": "\\vartriangleright ",
    u"\u22B4": "\\trianglelefteq ",
    u"\u22B5": "\\trianglerighteq ",
    u"\u22B6": "\\original ",
    u"\u22B7": "\\image ",
    u"\u22B8": "\\multimap ",
    u"\u22B9": "\\hermitconjmatrix ",
    u"\u22BA": "\\intercal ",
    u"\u22BB": "\\veebar ",
    u"\u22BE": "\\rightanglearc ",
    u"\u22C0": "\\ElsevierGlyph{22C0}",
    u"\u22C1": "\\ElsevierGlyph{22C1}",
    u"\u22C2": "\\bigcap ",
    u"\u22C3": "\\bigcup ",
    u"\u22C4": "\\diamond ",
    u"\u22C5": "\\cdot ",
    u"\u22C6": "\\star ",
    u"\u22C7": "\\divideontimes ",
    u"\u22C8": "\\bowtie ",
    u"\u22C9": "\\ltimes ",
    u"\u22CA": "\\rtimes ",
    u"\u22CB": "\\leftthreetimes ",
    u"\u22CC": "\\rightthreetimes ",
    u"\u22CD": "\\backsimeq ",
    u"\u22CE": "\\curlyvee ",
    u"\u22CF": "\\curlywedge ",
    u"\u22D0": "\\Subset ",
    u"\u22D1": "\\Supset ",
    u"\u22D2": "\\Cap ",
    u"\u22D3": "\\Cup ",
    u"\u22D4": "\\pitchfork ",
    u"\u22D6": "\\lessdot ",
    u"\u22D7": "\\gtrdot ",
    u"\u22D8": "\\verymuchless ",
    u"\u22D9": "\\verymuchgreater ",
    u"\u22DA": "\\lesseqgtr ",
    u"\u22DB": "\\gtreqless ",
    u"\u22DE": "\\curlyeqprec ",
    u"\u22DF": "\\curlyeqsucc ",
    u"\u22E2": "\\not\\sqsubseteq ",
    u"\u22E3": "\\not\\sqsupseteq ",
    u"\u22E5": "\\Elzsqspne ",
    u"\u22E6": "\\lnsim ",
    u"\u22E7": "\\gnsim ",
    u"\u22E8": "\\precedesnotsimilar ",
    u"\u22E9": "\\succnsim ",
    u"\u22EA": "\\ntriangleleft ",
    u"\u22EB": "\\ntriangleright ",
    u"\u22EC": "\\ntrianglelefteq ",
    u"\u22ED": "\\ntrianglerighteq ",
    u"\u22EE": "\\vdots ",
    u"\u22EF": "\\cdots ",
    u"\u22F0": "\\upslopeellipsis ",
    u"\u22F1": "\\downslopeellipsis ",
    u"\u2305": "\\barwedge ",
    u"\u2306": "\\perspcorrespond ",
    u"\u2308": "\\lceil ",
    u"\u2309": "\\rceil ",
    u"\u230A": "\\lfloor ",
    u"\u230B": "\\rfloor ",
    u"\u2315": "\\recorder ",
    u"\u2316": "\\mathchar\"2208",
    u"\u231C": "\\ulcorner ",
    u"\u231D": "\\urcorner ",
    u"\u231E": "\\llcorner ",
    u"\u231F": "\\lrcorner ",
    u"\u2322": "\\frown ",
    u"\u2323": "\\smile ",
    u"\u2329": "\\langle ",
    u"\u232A": "\\rangle ",
    u"\u233D": "\\ElsevierGlyph{E838}",
    u"\u23A3": "\\Elzdlcorn ",
    u"\u23B0": "\\lmoustache ",
    u"\u23B1": "\\rmoustache ",
    u"\u2423": "\\textvisiblespace ",
    u"\u2460": "\\ding{172}",
    u"\u2461": "\\ding{173}",
    u"\u2462": "\\ding{174}",
    u"\u2463": "\\ding{175}",
    u"\u2464": "\\ding{176}",
    u"\u2465": "\\ding{177}",
    u"\u2466": "\\ding{178}",
    u"\u2467": "\\ding{179}",
    u"\u2468": "\\ding{180}",
    u"\u2469": "\\ding{181}",
    u"\u24C8": "\\circledS ",
    u"\u2506": "\\Elzdshfnc ",
    u"\u2519": "\\Elzsqfnw ",
    u"\u2571": "\\diagup ",
    u"\u25A0": "\\ding{110}",
    u"\u25A1": "\\square ",
    u"\u25AA": "\\blacksquare ",
    u"\u25AD": "\\fbox{~~}",
    u"\u25AF": "\\Elzvrecto ",
    u"\u25B1": "\\ElsevierGlyph{E381}",
    u"\u25B2": "\\ding{115}",
    u"\u25B3": "\\bigtriangleup ",
    u"\u25B4": "\\blacktriangle ",
    u"\u25B5": "\\vartriangle ",
    u"\u25B8": "\\blacktriangleright ",
    u"\u25B9": "\\triangleright ",
    u"\u25BC": "\\ding{116}",
    u"\u25BD": "\\bigtriangledown ",
    u"\u25BE": "\\blacktriangledown ",
    u"\u25BF": "\\triangledown ",
    u"\u25C2": "\\blacktriangleleft ",
    u"\u25C3": "\\triangleleft ",
    u"\u25C6": "\\ding{117}",
    u"\u25CA": "\\lozenge ",
    u"\u25CB": "\\bigcirc ",
    u"\u25CF": "\\ding{108}",
    u"\u25D0": "\\Elzcirfl ",
    u"\u25D1": "\\Elzcirfr ",
    u"\u25D2": "\\Elzcirfb ",
    u"\u25D7": "\\ding{119}",
    u"\u25D8": "\\Elzrvbull ",
    u"\u25E7": "\\Elzsqfl ",
    u"\u25E8": "\\Elzsqfr ",
    u"\u25EA": "\\Elzsqfse ",
    u"\u25EF": "\\bigcirc ",
    u"\u2605": "\\ding{72}",
    u"\u2606": "\\ding{73}",
    u"\u260E": "\\ding{37}",
    u"\u261B": "\\ding{42}",
    u"\u261E": "\\ding{43}",
    u"\u263E": "\\rightmoon ",
    u"\u263F": "\\mercury ",
    u"\u2640": "\\venus ",
    u"\u2642": "\\male ",
    u"\u2643": "\\jupiter ",
    u"\u2644": "\\saturn ",
    u"\u2645": "\\uranus ",
    u"\u2646": "\\neptune ",
    u"\u2647": "\\pluto ",
    u"\u2648": "\\aries ",
    u"\u2649": "\\taurus ",
    u"\u264A": "\\gemini ",
    u"\u264B": "\\cancer ",
    u"\u264C": "\\leo ",
    u"\u264D": "\\virgo ",
    u"\u264E": "\\libra ",
    u"\u264F": "\\scorpio ",
    u"\u2650": "\\sagittarius ",
    u"\u2651": "\\capricornus ",
    u"\u2652": "\\aquarius ",
    u"\u2653": "\\pisces ",
    u"\u2660": "\\ding{171}",
    u"\u2662": "\\diamond ",
    u"\u2663": "\\ding{168}",
    u"\u2665": "\\ding{170}",
    u"\u2666": "\\ding{169}",
    u"\u2669": "\\quarternote ",
    u"\u266A": "\\eighthnote ",
    u"\u266D": "\\flat ",
    u"\u266E": "\\natural ",
    u"\u266F": "\\sharp ",
    u"\u2701": "\\ding{33}",
    u"\u2702": "\\ding{34}",
    u"\u2703": "\\ding{35}",
    u"\u2704": "\\ding{36}",
    u"\u2706": "\\ding{38}",
    u"\u2707": "\\ding{39}",
    u"\u2708": "\\ding{40}",
    u"\u2709": "\\ding{41}",
    u"\u270C": "\\ding{44}",
    u"\u270D": "\\ding{45}",
    u"\u270E": "\\ding{46}",
    u"\u270F": "\\ding{47}",
    u"\u2710": "\\ding{48}",
    u"\u2711": "\\ding{49}",
    u"\u2712": "\\ding{50}",
    u"\u2713": "\\ding{51}",
    u"\u2714": "\\ding{52}",
    u"\u2715": "\\ding{53}",
    u"\u2716": "\\ding{54}",
    u"\u2717": "\\ding{55}",
    u"\u2718": "\\ding{56}",
    u"\u2719": "\\ding{57}",
    u"\u271A": "\\ding{58}",
    u"\u271B": "\\ding{59}",
    u"\u271C": "\\ding{60}",
    u"\u271D": "\\ding{61}",
    u"\u271E": "\\ding{62}",
    u"\u271F": "\\ding{63}",
    u"\u2720": "\\ding{64}",
    u"\u2721": "\\ding{65}",
    u"\u2722": "\\ding{66}",
    u"\u2723": "\\ding{67}",
    u"\u2724": "\\ding{68}",
    u"\u2725": "\\ding{69}",
    u"\u2726": "\\ding{70}",
    u"\u2727": "\\ding{71}",
    u"\u2729": "\\ding{73}",
    u"\u272A": "\\ding{74}",
    u"\u272B": "\\ding{75}",
    u"\u272C": "\\ding{76}",
    u"\u272D": "\\ding{77}",
    u"\u272E": "\\ding{78}",
    u"\u272F": "\\ding{79}",
    u"\u2730": "\\ding{80}",
    u"\u2731": "\\ding{81}",
    u"\u2732": "\\ding{82}",
    u"\u2733": "\\ding{83}",
    u"\u2734": "\\ding{84}",
    u"\u2735": "\\ding{85}",
    u"\u2736": "\\ding{86}",
    u"\u2737": "\\ding{87}",
    u"\u2738": "\\ding{88}",
    u"\u2739": "\\ding{89}",
    u"\u273A": "\\ding{90}",
    u"\u273B": "\\ding{91}",
    u"\u273C": "\\ding{92}",
    u"\u273D": "\\ding{93}",
    u"\u273E": "\\ding{94}",
    u"\u273F": "\\ding{95}",
    u"\u2740": "\\ding{96}",
    u"\u2741": "\\ding{97}",
    u"\u2742": "\\ding{98}",
    u"\u2743": "\\ding{99}",
    u"\u2744": "\\ding{100}",
    u"\u2745": "\\ding{101}",
    u"\u2746": "\\ding{102}",
    u"\u2747": "\\ding{103}",
    u"\u2748": "\\ding{104}",
    u"\u2749": "\\ding{105}",
    u"\u274A": "\\ding{106}",
    u"\u274B": "\\ding{107}",
    u"\u274D": "\\ding{109}",
    u"\u274F": "\\ding{111}",
    u"\u2750": "\\ding{112}",
    u"\u2751": "\\ding{113}",
    u"\u2752": "\\ding{114}",
    u"\u2756": "\\ding{118}",
    u"\u2758": "\\ding{120}",
    u"\u2759": "\\ding{121}",
    u"\u275A": "\\ding{122}",
    u"\u275B": "\\ding{123}",
    u"\u275C": "\\ding{124}",
    u"\u275D": "\\ding{125}",
    u"\u275E": "\\ding{126}",
    u"\u2761": "\\ding{161}",
    u"\u2762": "\\ding{162}",
    u"\u2763": "\\ding{163}",
    u"\u2764": "\\ding{164}",
    u"\u2765": "\\ding{165}",
    u"\u2766": "\\ding{166}",
    u"\u2767": "\\ding{167}",
    u"\u2776": "\\ding{182}",
    u"\u2777": "\\ding{183}",
    u"\u2778": "\\ding{184}",
    u"\u2779": "\\ding{185}",
    u"\u277A": "\\ding{186}",
    u"\u277B": "\\ding{187}",
    u"\u277C": "\\ding{188}",
    u"\u277D": "\\ding{189}",
    u"\u277E": "\\ding{190}",
    u"\u277F": "\\ding{191}",
    u"\u2780": "\\ding{192}",
    u"\u2781": "\\ding{193}",
    u"\u2782": "\\ding{194}",
    u"\u2783": "\\ding{195}",
    u"\u2784": "\\ding{196}",
    u"\u2785": "\\ding{197}",
    u"\u2786": "\\ding{198}",
    u"\u2787": "\\ding{199}",
    u"\u2788": "\\ding{200}",
    u"\u2789": "\\ding{201}",
    u"\u278A": "\\ding{202}",
    u"\u278B": "\\ding{203}",
    u"\u278C": "\\ding{204}",
    u"\u278D": "\\ding{205}",
    u"\u278E": "\\ding{206}",
    u"\u278F": "\\ding{207}",
    u"\u2790": "\\ding{208}",
    u"\u2791": "\\ding{209}",
    u"\u2792": "\\ding{210}",
    u"\u2793": "\\ding{211}",
    u"\u2794": "\\ding{212}",
    u"\u2798": "\\ding{216}",
    u"\u2799": "\\ding{217}",
    u"\u279A": "\\ding{218}",
    u"\u279B": "\\ding{219}",
    u"\u279C": "\\ding{220}",
    u"\u279D": "\\ding{221}",
    u"\u279E": "\\ding{222}",
    u"\u279F": "\\ding{223}",
    u"\u27A0": "\\ding{224}",
    u"\u27A1": "\\ding{225}",
    u"\u27A2": "\\ding{226}",
    u"\u27A3": "\\ding{227}",
    u"\u27A4": "\\ding{228}",
    u"\u27A5": "\\ding{229}",
    u"\u27A6": "\\ding{230}",
    u"\u27A7": "\\ding{231}",
    u"\u27A8": "\\ding{232}",
    u"\u27A9": "\\ding{233}",
    u"\u27AA": "\\ding{234}",
    u"\u27AB": "\\ding{235}",
    u"\u27AC": "\\ding{236}",
    u"\u27AD": "\\ding{237}",
    u"\u27AE": "\\ding{238}",
    u"\u27AF": "\\ding{239}",
    u"\u27B1": "\\ding{241}",
    u"\u27B2": "\\ding{242}",
    u"\u27B3": "\\ding{243}",
    u"\u27B4": "\\ding{244}",
    u"\u27B5": "\\ding{245}",
    u"\u27B6": "\\ding{246}",
    u"\u27B7": "\\ding{247}",
    u"\u27B8": "\\ding{248}",
    u"\u27B9": "\\ding{249}",
    u"\u27BA": "\\ding{250}",
    u"\u27BB": "\\ding{251}",
    u"\u27BC": "\\ding{252}",
    u"\u27BD": "\\ding{253}",
    u"\u27BE": "\\ding{254}",
    u"\u27F5": "\\longleftarrow ",
    u"\u27F6": "\\longrightarrow ",
    u"\u27F7": "\\longleftrightarrow ",
    u"\u27F8": "\\Longleftarrow ",
    u"\u27F9": "\\Longrightarrow ",
    u"\u27FA": "\\Longleftrightarrow ",
    u"\u27FC": "\\longmapsto ",
    u"\u27FF": "\\sim\\joinrel\\leadsto",
    u"\u2905": "\\ElsevierGlyph{E212}",
    u"\u2912": "\\UpArrowBar ",
    u"\u2913": "\\DownArrowBar ",
    u"\u2923": "\\ElsevierGlyph{E20C}",
    u"\u2924": "\\ElsevierGlyph{E20D}",
    u"\u2925": "\\ElsevierGlyph{E20B}",
    u"\u2926": "\\ElsevierGlyph{E20A}",
    u"\u2927": "\\ElsevierGlyph{E211}",
    u"\u2928": "\\ElsevierGlyph{E20E}",
    u"\u2929": "\\ElsevierGlyph{E20F}",
    u"\u292A": "\\ElsevierGlyph{E210}",
    u"\u2933": "\\ElsevierGlyph{E21C}",
    u"\u2933-00338": "\\ElsevierGlyph{E21D}",
    u"\u2936": "\\ElsevierGlyph{E21A}",
    u"\u2937": "\\ElsevierGlyph{E219}",
    u"\u2940": "\\Elolarr ",
    u"\u2941": "\\Elorarr ",
    u"\u2942": "\\ElzRlarr ",
    u"\u2944": "\\ElzrLarr ",
    u"\u2947": "\\Elzrarrx ",
    u"\u294E": "\\LeftRightVector ",
    u"\u294F": "\\RightUpDownVector ",
    u"\u2950": "\\DownLeftRightVector ",
    u"\u2951": "\\LeftUpDownVector ",
    u"\u2952": "\\LeftVectorBar ",
    u"\u2953": "\\RightVectorBar ",
    u"\u2954": "\\RightUpVectorBar ",
    u"\u2955": "\\RightDownVectorBar ",
    u"\u2956": "\\DownLeftVectorBar ",
    u"\u2957": "\\DownRightVectorBar ",
    u"\u2958": "\\LeftUpVectorBar ",
    u"\u2959": "\\LeftDownVectorBar ",
    u"\u295A": "\\LeftTeeVector ",
    u"\u295B": "\\RightTeeVector ",
    u"\u295C": "\\RightUpTeeVector ",
    u"\u295D": "\\RightDownTeeVector ",
    u"\u295E": "\\DownLeftTeeVector ",
    u"\u295F": "\\DownRightTeeVector ",
    u"\u2960": "\\LeftUpTeeVector ",
    u"\u2961": "\\LeftDownTeeVector ",
    u"\u296E": "\\UpEquilibrium ",
    u"\u296F": "\\ReverseUpEquilibrium ",
    u"\u2970": "\\RoundImplies ",
    u"\u297C": "\\ElsevierGlyph{E214}",
    u"\u297D": "\\ElsevierGlyph{E215}",
    u"\u2980": "\\Elztfnc ",
    u"\u2985": "\\ElsevierGlyph{3018}",
    u"\u2986": "\\Elroang ",
    u"\u2993": "&lt;\\kern-0.58em(",
    u"\u2994": "\\ElsevierGlyph{E291}",
    u"\u2999": "\\Elzddfnc ",
    u"\u299C": "\\Angle ",
    u"\u29A0": "\\Elzlpargt ",
    u"\u29B5": "\\ElsevierGlyph{E260}",
    u"\u29B6": "\\ElsevierGlyph{E61B}",
    u"\u29CA": "\\ElzLap ",
    u"\u29CB": "\\Elzdefas ",
    u"\u29CF": "\\LeftTriangleBar ",
    u"\u29CF-00338": "\\NotLeftTriangleBar ",
    u"\u29D0": "\\RightTriangleBar ",
    u"\u29D0-00338": "\\NotRightTriangleBar ",
    u"\u29DC": "\\ElsevierGlyph{E372}",
    u"\u29EB": "\\blacklozenge ",
    u"\u29F4": "\\RuleDelayed ",
    u"\u2A04": "\\Elxuplus ",
    u"\u2A05": "\\ElzThr ",
    u"\u2A06": "\\Elxsqcup ",
    u"\u2A07": "\\ElzInf ",
    u"\u2A08": "\\ElzSup ",
    u"\u2A0D": "\\ElzCint ",
    u"\u2A0F": "\\clockoint ",
    u"\u2A10": "\\ElsevierGlyph{E395}",
    u"\u2A16": "\\sqrint ",
    u"\u2A25": "\\ElsevierGlyph{E25A}",
    u"\u2A2A": "\\ElsevierGlyph{E25B}",
    u"\u2A2D": "\\ElsevierGlyph{E25C}",
    u"\u2A2E": "\\ElsevierGlyph{E25D}",
    u"\u2A2F": "\\ElzTimes ",
    u"\u2A34": "\\ElsevierGlyph{E25E}",
    u"\u2A35": "\\ElsevierGlyph{E25E}",
    u"\u2A3C": "\\ElsevierGlyph{E259}",
    u"\u2A3F": "\\amalg ",
    u"\u2A53": "\\ElzAnd ",
    u"\u2A54": "\\ElzOr ",
    u"\u2A55": "\\ElsevierGlyph{E36E}",
    u"\u2A56": "\\ElOr ",
    u"\u2A5E": "\\perspcorrespond ",
    u"\u2A5F": "\\Elzminhat ",
    u"\u2A63": "\\ElsevierGlyph{225A}",
    u"\u2A6E": "\\stackrel{*}{=}",
    u"\u2A75": "\\Equal ",
    u"\u2A7D": "\\leqslant ",
    u"\u2A7D-00338": "\\nleqslant ",
    u"\u2A7E": "\\geqslant ",
    u"\u2A7E-00338": "\\ngeqslant ",
    u"\u2A85": "\\lessapprox ",
    u"\u2A86": "\\gtrapprox ",
    u"\u2A87": "\\lneq ",
    u"\u2A88": "\\gneq ",
    u"\u2A89": "\\lnapprox ",
    u"\u2A8A": "\\gnapprox ",
    u"\u2A8B": "\\lesseqqgtr ",
    u"\u2A8C": "\\gtreqqless ",
    u"\u2A95": "\\eqslantless ",
    u"\u2A96": "\\eqslantgtr ",
    u"\u2A9D": "\\Pisymbol{ppi020}{117}",
    u"\u2A9E": "\\Pisymbol{ppi020}{105}",
    u"\u2AA1": "\\NestedLessLess ",
    u"\u2AA1-00338": "\\NotNestedLessLess ",
    u"\u2AA2": "\\NestedGreaterGreater ",
    u"\u2AA2-00338": "\\NotNestedGreaterGreater ",
    u"\u2AAF": "\\preceq ",
    u"\u2AAF-00338": "\\not\\preceq ",
    u"\u2AB0": "\\succeq ",
    u"\u2AB0-00338": "\\not\\succeq ",
    u"\u2AB5": "\\precneqq ",
    u"\u2AB6": "\\succneqq ",
    u"\u2AB7": "\\precapprox ",
    u"\u2AB8": "\\succapprox ",
    u"\u2AB9": "\\precnapprox ",
    u"\u2ABA": "\\succnapprox ",
    u"\u2AC5": "\\subseteqq ",
    u"\u2AC5-00338": "\\nsubseteqq ",
    u"\u2AC6": "\\supseteqq ",
    u"\u2AC6-00338": "\\nsupseteqq",
    u"\u2ACB": "\\subsetneqq ",
    u"\u2ACC": "\\supsetneqq ",
    u"\u2AEB": "\\ElsevierGlyph{E30D}",
    u"\u2AF6": "\\Elztdcol ",
    u"\u2AFD": "{{/}\\!\\!{/}}",
    u"\u2AFD-020E5": "{\\rlap{\\textbackslash}{{/}\\!\\!{/}}}",
    u"\u300A": "\\ElsevierGlyph{300A}",
    u"\u300B": "\\ElsevierGlyph{300B}",
    u"\u3018": "\\ElsevierGlyph{3018}",
    u"\u3019": "\\ElsevierGlyph{3019}",
    u"\u301A": "\\openbracketleft ",
    u"\u301B": "\\openbracketright ",
    u"\uFB00": "ff",
    u"\uFB01": "fi",
    u"\uFB02": "fl",
    u"\uFB03": "ffi",
    u"\uFB04": "ffl",
    u"\uD400": "\\mathbf{A}",
    u"\uD401": "\\mathbf{B}",
    u"\uD402": "\\mathbf{C}",
    u"\uD403": "\\mathbf{D}",
    u"\uD404": "\\mathbf{E}",
    u"\uD405": "\\mathbf{F}",
    u"\uD406": "\\mathbf{G}",
    u"\uD407": "\\mathbf{H}",
    u"\uD408": "\\mathbf{I}",
    u"\uD409": "\\mathbf{J}",
    u"\uD40A": "\\mathbf{K}",
    u"\uD40B": "\\mathbf{L}",
    u"\uD40C": "\\mathbf{M}",
    u"\uD40D": "\\mathbf{N}",
    u"\uD40E": "\\mathbf{O}",
    u"\uD40F": "\\mathbf{P}",
    u"\uD410": "\\mathbf{Q}",
    u"\uD411": "\\mathbf{R}",
    u"\uD412": "\\mathbf{S}",
    u"\uD413": "\\mathbf{T}",
    u"\uD414": "\\mathbf{U}",
    u"\uD415": "\\mathbf{V}",
    u"\uD416": "\\mathbf{W}",
    u"\uD417": "\\mathbf{X}",
    u"\uD418": "\\mathbf{Y}",
    u"\uD419": "\\mathbf{Z}",
    u"\uD41A": "\\mathbf{a}",
    u"\uD41B": "\\mathbf{b}",
    u"\uD41C": "\\mathbf{c}",
    u"\uD41D": "\\mathbf{d}",
    u"\uD41E": "\\mathbf{e}",
    u"\uD41F": "\\mathbf{f}",
    u"\uD420": "\\mathbf{g}",
    u"\uD421": "\\mathbf{h}",
    u"\uD422": "\\mathbf{i}",
    u"\uD423": "\\mathbf{j}",
    u"\uD424": "\\mathbf{k}",
    u"\uD425": "\\mathbf{l}",
    u"\uD426": "\\mathbf{m}",
    u"\uD427": "\\mathbf{n}",
    u"\uD428": "\\mathbf{o}",
    u"\uD429": "\\mathbf{p}",
    u"\uD42A": "\\mathbf{q}",
    u"\uD42B": "\\mathbf{r}",
    u"\uD42C": "\\mathbf{s}",
    u"\uD42D": "\\mathbf{t}",
    u"\uD42E": "\\mathbf{u}",
    u"\uD42F": "\\mathbf{v}",
    u"\uD430": "\\mathbf{w}",
    u"\uD431": "\\mathbf{x}",
    u"\uD432": "\\mathbf{y}",
    u"\uD433": "\\mathbf{z}",
    u"\uD434": "\\mathsl{A}",
    u"\uD435": "\\mathsl{B}",
    u"\uD436": "\\mathsl{C}",
    u"\uD437": "\\mathsl{D}",
    u"\uD438": "\\mathsl{E}",
    u"\uD439": "\\mathsl{F}",
    u"\uD43A": "\\mathsl{G}",
    u"\uD43B": "\\mathsl{H}",
    u"\uD43C": "\\mathsl{I}",
    u"\uD43D": "\\mathsl{J}",
    u"\uD43E": "\\mathsl{K}",
    u"\uD43F": "\\mathsl{L}",
    u"\uD440": "\\mathsl{M}",
    u"\uD441": "\\mathsl{N}",
    u"\uD442": "\\mathsl{O}",
    u"\uD443": "\\mathsl{P}",
    u"\uD444": "\\mathsl{Q}",
    u"\uD445": "\\mathsl{R}",
    u"\uD446": "\\mathsl{S}",
    u"\uD447": "\\mathsl{T}",
    u"\uD448": "\\mathsl{U}",
    u"\uD449": "\\mathsl{V}",
    u"\uD44A": "\\mathsl{W}",
    u"\uD44B": "\\mathsl{X}",
    u"\uD44C": "\\mathsl{Y}",
    u"\uD44D": "\\mathsl{Z}",
    u"\uD44E": "\\mathsl{a}",
    u"\uD44F": "\\mathsl{b}",
    u"\uD450": "\\mathsl{c}",
    u"\uD451": "\\mathsl{d}",
    u"\uD452": "\\mathsl{e}",
    u"\uD453": "\\mathsl{f}",
    u"\uD454": "\\mathsl{g}",
    u"\uD456": "\\mathsl{i}",
    u"\uD457": "\\mathsl{j}",
    u"\uD458": "\\mathsl{k}",
    u"\uD459": "\\mathsl{l}",
    u"\uD45A": "\\mathsl{m}",
    u"\uD45B": "\\mathsl{n}",
    u"\uD45C": "\\mathsl{o}",
    u"\uD45D": "\\mathsl{p}",
    u"\uD45E": "\\mathsl{q}",
    u"\uD45F": "\\mathsl{r}",
    u"\uD460": "\\mathsl{s}",
    u"\uD461": "\\mathsl{t}",
    u"\uD462": "\\mathsl{u}",
    u"\uD463": "\\mathsl{v}",
    u"\uD464": "\\mathsl{w}",
    u"\uD465": "\\mathsl{x}",
    u"\uD466": "\\mathsl{y}",
    u"\uD467": "\\mathsl{z}",
    u"\uD468": "\\mathbit{A}",
    u"\uD469": "\\mathbit{B}",
    u"\uD46A": "\\mathbit{C}",
    u"\uD46B": "\\mathbit{D}",
    u"\uD46C": "\\mathbit{E}",
    u"\uD46D": "\\mathbit{F}",
    u"\uD46E": "\\mathbit{G}",
    u"\uD46F": "\\mathbit{H}",
    u"\uD470": "\\mathbit{I}",
    u"\uD471": "\\mathbit{J}",
    u"\uD472": "\\mathbit{K}",
    u"\uD473": "\\mathbit{L}",
    u"\uD474": "\\mathbit{M}",
    u"\uD475": "\\mathbit{N}",
    u"\uD476": "\\mathbit{O}",
    u"\uD477": "\\mathbit{P}",
    u"\uD478": "\\mathbit{Q}",
    u"\uD479": "\\mathbit{R}",
    u"\uD47A": "\\mathbit{S}",
    u"\uD47B": "\\mathbit{T}",
    u"\uD47C": "\\mathbit{U}",
    u"\uD47D": "\\mathbit{V}",
    u"\uD47E": "\\mathbit{W}",
    u"\uD47F": "\\mathbit{X}",
    u"\uD480": "\\mathbit{Y}",
    u"\uD481": "\\mathbit{Z}",
    u"\uD482": "\\mathbit{a}",
    u"\uD483": "\\mathbit{b}",
    u"\uD484": "\\mathbit{c}",
    u"\uD485": "\\mathbit{d}",
    u"\uD486": "\\mathbit{e}",
    u"\uD487": "\\mathbit{f}",
    u"\uD488": "\\mathbit{g}",
    u"\uD489": "\\mathbit{h}",
    u"\uD48A": "\\mathbit{i}",
    u"\uD48B": "\\mathbit{j}",
    u"\uD48C": "\\mathbit{k}",
    u"\uD48D": "\\mathbit{l}",
    u"\uD48E": "\\mathbit{m}",
    u"\uD48F": "\\mathbit{n}",
    u"\uD490": "\\mathbit{o}",
    u"\uD491": "\\mathbit{p}",
    u"\uD492": "\\mathbit{q}",
    u"\uD493": "\\mathbit{r}",
    u"\uD494": "\\mathbit{s}",
    u"\uD495": "\\mathbit{t}",
    u"\uD496": "\\mathbit{u}",
    u"\uD497": "\\mathbit{v}",
    u"\uD498": "\\mathbit{w}",
    u"\uD499": "\\mathbit{x}",
    u"\uD49A": "\\mathbit{y}",
    u"\uD49B": "\\mathbit{z}",
    u"\uD49C": "\\mathscr{A}",
    u"\uD49E": "\\mathscr{C}",
    u"\uD49F": "\\mathscr{D}",
    u"\uD4A2": "\\mathscr{G}",
    u"\uD4A5": "\\mathscr{J}",
    u"\uD4A6": "\\mathscr{K}",
    u"\uD4A9": "\\mathscr{N}",
    u"\uD4AA": "\\mathscr{O}",
    u"\uD4AB": "\\mathscr{P}",
    u"\uD4AC": "\\mathscr{Q}",
    u"\uD4AE": "\\mathscr{S}",
    u"\uD4AF": "\\mathscr{T}",
    u"\uD4B0": "\\mathscr{U}",
    u"\uD4B1": "\\mathscr{V}",
    u"\uD4B2": "\\mathscr{W}",
    u"\uD4B3": "\\mathscr{X}",
    u"\uD4B4": "\\mathscr{Y}",
    u"\uD4B5": "\\mathscr{Z}",
    u"\uD4B6": "\\mathscr{a}",
    u"\uD4B7": "\\mathscr{b}",
    u"\uD4B8": "\\mathscr{c}",
    u"\uD4B9": "\\mathscr{d}",
    u"\uD4BB": "\\mathscr{f}",
    u"\uD4BD": "\\mathscr{h}",
    u"\uD4BE": "\\mathscr{i}",
    u"\uD4BF": "\\mathscr{j}",
    u"\uD4C0": "\\mathscr{k}",
    u"\uD4C1": "\\mathscr{l}",
    u"\uD4C2": "\\mathscr{m}",
    u"\uD4C3": "\\mathscr{n}",
    u"\uD4C5": "\\mathscr{p}",
    u"\uD4C6": "\\mathscr{q}",
    u"\uD4C7": "\\mathscr{r}",
    u"\uD4C8": "\\mathscr{s}",
    u"\uD4C9": "\\mathscr{t}",
    u"\uD4CA": "\\mathscr{u}",
    u"\uD4CB": "\\mathscr{v}",
    u"\uD4CC": "\\mathscr{w}",
    u"\uD4CD": "\\mathscr{x}",
    u"\uD4CE": "\\mathscr{y}",
    u"\uD4CF": "\\mathscr{z}",
    u"\uD4D0": "\\mathmit{A}",
    u"\uD4D1": "\\mathmit{B}",
    u"\uD4D2": "\\mathmit{C}",
    u"\uD4D3": "\\mathmit{D}",
    u"\uD4D4": "\\mathmit{E}",
    u"\uD4D5": "\\mathmit{F}",
    u"\uD4D6": "\\mathmit{G}",
    u"\uD4D7": "\\mathmit{H}",
    u"\uD4D8": "\\mathmit{I}",
    u"\uD4D9": "\\mathmit{J}",
    u"\uD4DA": "\\mathmit{K}",
    u"\uD4DB": "\\mathmit{L}",
    u"\uD4DC": "\\mathmit{M}",
    u"\uD4DD": "\\mathmit{N}",
    u"\uD4DE": "\\mathmit{O}",
    u"\uD4DF": "\\mathmit{P}",
    u"\uD4E0": "\\mathmit{Q}",
    u"\uD4E1": "\\mathmit{R}",
    u"\uD4E2": "\\mathmit{S}",
    u"\uD4E3": "\\mathmit{T}",
    u"\uD4E4": "\\mathmit{U}",
    u"\uD4E5": "\\mathmit{V}",
    u"\uD4E6": "\\mathmit{W}",
    u"\uD4E7": "\\mathmit{X}",
    u"\uD4E8": "\\mathmit{Y}",
    u"\uD4E9": "\\mathmit{Z}",
    u"\uD4EA": "\\mathmit{a}",
    u"\uD4EB": "\\mathmit{b}",
    u"\uD4EC": "\\mathmit{c}",
    u"\uD4ED": "\\mathmit{d}",
    u"\uD4EE": "\\mathmit{e}",
    u"\uD4EF": "\\mathmit{f}",
    u"\uD4F0": "\\mathmit{g}",
    u"\uD4F1": "\\mathmit{h}",
    u"\uD4F2": "\\mathmit{i}",
    u"\uD4F3": "\\mathmit{j}",
    u"\uD4F4": "\\mathmit{k}",
    u"\uD4F5": "\\mathmit{l}",
    u"\uD4F6": "\\mathmit{m}",
    u"\uD4F7": "\\mathmit{n}",
    u"\uD4F8": "\\mathmit{o}",
    u"\uD4F9": "\\mathmit{p}",
    u"\uD4FA": "\\mathmit{q}",
    u"\uD4FB": "\\mathmit{r}",
    u"\uD4FC": "\\mathmit{s}",
    u"\uD4FD": "\\mathmit{t}",
    u"\uD4FE": "\\mathmit{u}",
    u"\uD4FF": "\\mathmit{v}",
    u"\uD500": "\\mathmit{w}",
    u"\uD501": "\\mathmit{x}",
    u"\uD502": "\\mathmit{y}",
    u"\uD503": "\\mathmit{z}",
    u"\uD504": "\\mathfrak{A}",
    u"\uD505": "\\mathfrak{B}",
    u"\uD507": "\\mathfrak{D}",
    u"\uD508": "\\mathfrak{E}",
    u"\uD509": "\\mathfrak{F}",
    u"\uD50A": "\\mathfrak{G}",
    u"\uD50D": "\\mathfrak{J}",
    u"\uD50E": "\\mathfrak{K}",
    u"\uD50F": "\\mathfrak{L}",
    u"\uD510": "\\mathfrak{M}",
    u"\uD511": "\\mathfrak{N}",
    u"\uD512": "\\mathfrak{O}",
    u"\uD513": "\\mathfrak{P}",
    u"\uD514": "\\mathfrak{Q}",
    u"\uD516": "\\mathfrak{S}",
    u"\uD517": "\\mathfrak{T}",
    u"\uD518": "\\mathfrak{U}",
    u"\uD519": "\\mathfrak{V}",
    u"\uD51A": "\\mathfrak{W}",
    u"\uD51B": "\\mathfrak{X}",
    u"\uD51C": "\\mathfrak{Y}",
    u"\uD51E": "\\mathfrak{a}",
    u"\uD51F": "\\mathfrak{b}",
    u"\uD520": "\\mathfrak{c}",
    u"\uD521": "\\mathfrak{d}",
    u"\uD522": "\\mathfrak{e}",
    u"\uD523": "\\mathfrak{f}",
    u"\uD524": "\\mathfrak{g}",
    u"\uD525": "\\mathfrak{h}",
    u"\uD526": "\\mathfrak{i}",
    u"\uD527": "\\mathfrak{j}",
    u"\uD528": "\\mathfrak{k}",
    u"\uD529": "\\mathfrak{l}",
    u"\uD52A": "\\mathfrak{m}",
    u"\uD52B": "\\mathfrak{n}",
    u"\uD52C": "\\mathfrak{o}",
    u"\uD52D": "\\mathfrak{p}",
    u"\uD52E": "\\mathfrak{q}",
    u"\uD52F": "\\mathfrak{r}",
    u"\uD530": "\\mathfrak{s}",
    u"\uD531": "\\mathfrak{t}",
    u"\uD532": "\\mathfrak{u}",
    u"\uD533": "\\mathfrak{v}",
    u"\uD534": "\\mathfrak{w}",
    u"\uD535": "\\mathfrak{x}",
    u"\uD536": "\\mathfrak{y}",
    u"\uD537": "\\mathfrak{z}",
    u"\uD538": "\\mathbb{A}",
    u"\uD539": "\\mathbb{B}",
    u"\uD53B": "\\mathbb{D}",
    u"\uD53C": "\\mathbb{E}",
    u"\uD53D": "\\mathbb{F}",
    u"\uD53E": "\\mathbb{G}",
    u"\uD540": "\\mathbb{I}",
    u"\uD541": "\\mathbb{J}",
    u"\uD542": "\\mathbb{K}",
    u"\uD543": "\\mathbb{L}",
    u"\uD544": "\\mathbb{M}",
    u"\uD546": "\\mathbb{O}",
    u"\uD54A": "\\mathbb{S}",
    u"\uD54B": "\\mathbb{T}",
    u"\uD54C": "\\mathbb{U}",
    u"\uD54D": "\\mathbb{V}",
    u"\uD54E": "\\mathbb{W}",
    u"\uD54F": "\\mathbb{X}",
    u"\uD550": "\\mathbb{Y}",
    u"\uD552": "\\mathbb{a}",
    u"\uD553": "\\mathbb{b}",
    u"\uD554": "\\mathbb{c}",
    u"\uD555": "\\mathbb{d}",
    u"\uD556": "\\mathbb{e}",
    u"\uD557": "\\mathbb{f}",
    u"\uD558": "\\mathbb{g}",
    u"\uD559": "\\mathbb{h}",
    u"\uD55A": "\\mathbb{i}",
    u"\uD55B": "\\mathbb{j}",
    u"\uD55C": "\\mathbb{k}",
    u"\uD55D": "\\mathbb{l}",
    u"\uD55E": "\\mathbb{m}",
    u"\uD55F": "\\mathbb{n}",
    u"\uD560": "\\mathbb{o}",
    u"\uD561": "\\mathbb{p}",
    u"\uD562": "\\mathbb{q}",
    u"\uD563": "\\mathbb{r}",
    u"\uD564": "\\mathbb{s}",
    u"\uD565": "\\mathbb{t}",
    u"\uD566": "\\mathbb{u}",
    u"\uD567": "\\mathbb{v}",
    u"\uD568": "\\mathbb{w}",
    u"\uD569": "\\mathbb{x}",
    u"\uD56A": "\\mathbb{y}",
    u"\uD56B": "\\mathbb{z}",
    u"\uD56C": "\\mathslbb{A}",
    u"\uD56D": "\\mathslbb{B}",
    u"\uD56E": "\\mathslbb{C}",
    u"\uD56F": "\\mathslbb{D}",
    u"\uD570": "\\mathslbb{E}",
    u"\uD571": "\\mathslbb{F}",
    u"\uD572": "\\mathslbb{G}",
    u"\uD573": "\\mathslbb{H}",
    u"\uD574": "\\mathslbb{I}",
    u"\uD575": "\\mathslbb{J}",
    u"\uD576": "\\mathslbb{K}",
    u"\uD577": "\\mathslbb{L}",
    u"\uD578": "\\mathslbb{M}",
    u"\uD579": "\\mathslbb{N}",
    u"\uD57A": "\\mathslbb{O}",
    u"\uD57B": "\\mathslbb{P}",
    u"\uD57C": "\\mathslbb{Q}",
    u"\uD57D": "\\mathslbb{R}",
    u"\uD57E": "\\mathslbb{S}",
    u"\uD57F": "\\mathslbb{T}",
    u"\uD580": "\\mathslbb{U}",
    u"\uD581": "\\mathslbb{V}",
    u"\uD582": "\\mathslbb{W}",
    u"\uD583": "\\mathslbb{X}",
    u"\uD584": "\\mathslbb{Y}",
    u"\uD585": "\\mathslbb{Z}",
    u"\uD586": "\\mathslbb{a}",
    u"\uD587": "\\mathslbb{b}",
    u"\uD588": "\\mathslbb{c}",
    u"\uD589": "\\mathslbb{d}",
    u"\uD58A": "\\mathslbb{e}",
    u"\uD58B": "\\mathslbb{f}",
    u"\uD58C": "\\mathslbb{g}",
    u"\uD58D": "\\mathslbb{h}",
    u"\uD58E": "\\mathslbb{i}",
    u"\uD58F": "\\mathslbb{j}",
    u"\uD590": "\\mathslbb{k}",
    u"\uD591": "\\mathslbb{l}",
    u"\uD592": "\\mathslbb{m}",
    u"\uD593": "\\mathslbb{n}",
    u"\uD594": "\\mathslbb{o}",
    u"\uD595": "\\mathslbb{p}",
    u"\uD596": "\\mathslbb{q}",
    u"\uD597": "\\mathslbb{r}",
    u"\uD598": "\\mathslbb{s}",
    u"\uD599": "\\mathslbb{t}",
    u"\uD59A": "\\mathslbb{u}",
    u"\uD59B": "\\mathslbb{v}",
    u"\uD59C": "\\mathslbb{w}",
    u"\uD59D": "\\mathslbb{x}",
    u"\uD59E": "\\mathslbb{y}",
    u"\uD59F": "\\mathslbb{z}",
    u"\uD5A0": "\\mathsf{A}",
    u"\uD5A1": "\\mathsf{B}",
    u"\uD5A2": "\\mathsf{C}",
    u"\uD5A3": "\\mathsf{D}",
    u"\uD5A4": "\\mathsf{E}",
    u"\uD5A5": "\\mathsf{F}",
    u"\uD5A6": "\\mathsf{G}",
    u"\uD5A7": "\\mathsf{H}",
    u"\uD5A8": "\\mathsf{I}",
    u"\uD5A9": "\\mathsf{J}",
    u"\uD5AA": "\\mathsf{K}",
    u"\uD5AB": "\\mathsf{L}",
    u"\uD5AC": "\\mathsf{M}",
    u"\uD5AD": "\\mathsf{N}",
    u"\uD5AE": "\\mathsf{O}",
    u"\uD5AF": "\\mathsf{P}",
    u"\uD5B0": "\\mathsf{Q}",
    u"\uD5B1": "\\mathsf{R}",
    u"\uD5B2": "\\mathsf{S}",
    u"\uD5B3": "\\mathsf{T}",
    u"\uD5B4": "\\mathsf{U}",
    u"\uD5B5": "\\mathsf{V}",
    u"\uD5B6": "\\mathsf{W}",
    u"\uD5B7": "\\mathsf{X}",
    u"\uD5B8": "\\mathsf{Y}",
    u"\uD5B9": "\\mathsf{Z}",
    u"\uD5BA": "\\mathsf{a}",
    u"\uD5BB": "\\mathsf{b}",
    u"\uD5BC": "\\mathsf{c}",
    u"\uD5BD": "\\mathsf{d}",
    u"\uD5BE": "\\mathsf{e}",
    u"\uD5BF": "\\mathsf{f}",
    u"\uD5C0": "\\mathsf{g}",
    u"\uD5C1": "\\mathsf{h}",
    u"\uD5C2": "\\mathsf{i}",
    u"\uD5C3": "\\mathsf{j}",
    u"\uD5C4": "\\mathsf{k}",
    u"\uD5C5": "\\mathsf{l}",
    u"\uD5C6": "\\mathsf{m}",
    u"\uD5C7": "\\mathsf{n}",
    u"\uD5C8": "\\mathsf{o}",
    u"\uD5C9": "\\mathsf{p}",
    u"\uD5CA": "\\mathsf{q}",
    u"\uD5CB": "\\mathsf{r}",
    u"\uD5CC": "\\mathsf{s}",
    u"\uD5CD": "\\mathsf{t}",
    u"\uD5CE": "\\mathsf{u}",
    u"\uD5CF": "\\mathsf{v}",
    u"\uD5D0": "\\mathsf{w}",
    u"\uD5D1": "\\mathsf{x}",
    u"\uD5D2": "\\mathsf{y}",
    u"\uD5D3": "\\mathsf{z}",
    u"\uD5D4": "\\mathsfbf{A}",
    u"\uD5D5": "\\mathsfbf{B}",
    u"\uD5D6": "\\mathsfbf{C}",
    u"\uD5D7": "\\mathsfbf{D}",
    u"\uD5D8": "\\mathsfbf{E}",
    u"\uD5D9": "\\mathsfbf{F}",
    u"\uD5DA": "\\mathsfbf{G}",
    u"\uD5DB": "\\mathsfbf{H}",
    u"\uD5DC": "\\mathsfbf{I}",
    u"\uD5DD": "\\mathsfbf{J}",
    u"\uD5DE": "\\mathsfbf{K}",
    u"\uD5DF": "\\mathsfbf{L}",
    u"\uD5E0": "\\mathsfbf{M}",
    u"\uD5E1": "\\mathsfbf{N}",
    u"\uD5E2": "\\mathsfbf{O}",
    u"\uD5E3": "\\mathsfbf{P}",
    u"\uD5E4": "\\mathsfbf{Q}",
    u"\uD5E5": "\\mathsfbf{R}",
    u"\uD5E6": "\\mathsfbf{S}",
    u"\uD5E7": "\\mathsfbf{T}",
    u"\uD5E8": "\\mathsfbf{U}",
    u"\uD5E9": "\\mathsfbf{V}",
    u"\uD5EA": "\\mathsfbf{W}",
    u"\uD5EB": "\\mathsfbf{X}",
    u"\uD5EC": "\\mathsfbf{Y}",
    u"\uD5ED": "\\mathsfbf{Z}",
    u"\uD5EE": "\\mathsfbf{a}",
    u"\uD5EF": "\\mathsfbf{b}",
    u"\uD5F0": "\\mathsfbf{c}",
    u"\uD5F1": "\\mathsfbf{d}",
    u"\uD5F2": "\\mathsfbf{e}",
    u"\uD5F3": "\\mathsfbf{f}",
    u"\uD5F4": "\\mathsfbf{g}",
    u"\uD5F5": "\\mathsfbf{h}",
    u"\uD5F6": "\\mathsfbf{i}",
    u"\uD5F7": "\\mathsfbf{j}",
    u"\uD5F8": "\\mathsfbf{k}",
    u"\uD5F9": "\\mathsfbf{l}",
    u"\uD5FA": "\\mathsfbf{m}",
    u"\uD5FB": "\\mathsfbf{n}",
    u"\uD5FC": "\\mathsfbf{o}",
    u"\uD5FD": "\\mathsfbf{p}",
    u"\uD5FE": "\\mathsfbf{q}",
    u"\uD5FF": "\\mathsfbf{r}",
    u"\uD600": "\\mathsfbf{s}",
    u"\uD601": "\\mathsfbf{t}",
    u"\uD602": "\\mathsfbf{u}",
    u"\uD603": "\\mathsfbf{v}",
    u"\uD604": "\\mathsfbf{w}",
    u"\uD605": "\\mathsfbf{x}",
    u"\uD606": "\\mathsfbf{y}",
    u"\uD607": "\\mathsfbf{z}",
    u"\uD608": "\\mathsfsl{A}",
    u"\uD609": "\\mathsfsl{B}",
    u"\uD60A": "\\mathsfsl{C}",
    u"\uD60B": "\\mathsfsl{D}",
    u"\uD60C": "\\mathsfsl{E}",
    u"\uD60D": "\\mathsfsl{F}",
    u"\uD60E": "\\mathsfsl{G}",
    u"\uD60F": "\\mathsfsl{H}",
    u"\uD610": "\\mathsfsl{I}",
    u"\uD611": "\\mathsfsl{J}",
    u"\uD612": "\\mathsfsl{K}",
    u"\uD613": "\\mathsfsl{L}",
    u"\uD614": "\\mathsfsl{M}",
    u"\uD615": "\\mathsfsl{N}",
    u"\uD616": "\\mathsfsl{O}",
    u"\uD617": "\\mathsfsl{P}",
    u"\uD618": "\\mathsfsl{Q}",
    u"\uD619": "\\mathsfsl{R}",
    u"\uD61A": "\\mathsfsl{S}",
    u"\uD61B": "\\mathsfsl{T}",
    u"\uD61C": "\\mathsfsl{U}",
    u"\uD61D": "\\mathsfsl{V}",
    u"\uD61E": "\\mathsfsl{W}",
    u"\uD61F": "\\mathsfsl{X}",
    u"\uD620": "\\mathsfsl{Y}",
    u"\uD621": "\\mathsfsl{Z}",
    u"\uD622": "\\mathsfsl{a}",
    u"\uD623": "\\mathsfsl{b}",
    u"\uD624": "\\mathsfsl{c}",
    u"\uD625": "\\mathsfsl{d}",
    u"\uD626": "\\mathsfsl{e}",
    u"\uD627": "\\mathsfsl{f}",
    u"\uD628": "\\mathsfsl{g}",
    u"\uD629": "\\mathsfsl{h}",
    u"\uD62A": "\\mathsfsl{i}",
    u"\uD62B": "\\mathsfsl{j}",
    u"\uD62C": "\\mathsfsl{k}",
    u"\uD62D": "\\mathsfsl{l}",
    u"\uD62E": "\\mathsfsl{m}",
    u"\uD62F": "\\mathsfsl{n}",
    u"\uD630": "\\mathsfsl{o}",
    u"\uD631": "\\mathsfsl{p}",
    u"\uD632": "\\mathsfsl{q}",
    u"\uD633": "\\mathsfsl{r}",
    u"\uD634": "\\mathsfsl{s}",
    u"\uD635": "\\mathsfsl{t}",
    u"\uD636": "\\mathsfsl{u}",
    u"\uD637": "\\mathsfsl{v}",
    u"\uD638": "\\mathsfsl{w}",
    u"\uD639": "\\mathsfsl{x}",
    u"\uD63A": "\\mathsfsl{y}",
    u"\uD63B": "\\mathsfsl{z}",
    u"\uD63C": "\\mathsfbfsl{A}",
    u"\uD63D": "\\mathsfbfsl{B}",
    u"\uD63E": "\\mathsfbfsl{C}",
    u"\uD63F": "\\mathsfbfsl{D}",
    u"\uD640": "\\mathsfbfsl{E}",
    u"\uD641": "\\mathsfbfsl{F}",
    u"\uD642": "\\mathsfbfsl{G}",
    u"\uD643": "\\mathsfbfsl{H}",
    u"\uD644": "\\mathsfbfsl{I}",
    u"\uD645": "\\mathsfbfsl{J}",
    u"\uD646": "\\mathsfbfsl{K}",
    u"\uD647": "\\mathsfbfsl{L}",
    u"\uD648": "\\mathsfbfsl{M}",
    u"\uD649": "\\mathsfbfsl{N}",
    u"\uD64A": "\\mathsfbfsl{O}",
    u"\uD64B": "\\mathsfbfsl{P}",
    u"\uD64C": "\\mathsfbfsl{Q}",
    u"\uD64D": "\\mathsfbfsl{R}",
    u"\uD64E": "\\mathsfbfsl{S}",
    u"\uD64F": "\\mathsfbfsl{T}",
    u"\uD650": "\\mathsfbfsl{U}",
    u"\uD651": "\\mathsfbfsl{V}",
    u"\uD652": "\\mathsfbfsl{W}",
    u"\uD653": "\\mathsfbfsl{X}",
    u"\uD654": "\\mathsfbfsl{Y}",
    u"\uD655": "\\mathsfbfsl{Z}",
    u"\uD656": "\\mathsfbfsl{a}",
    u"\uD657": "\\mathsfbfsl{b}",
    u"\uD658": "\\mathsfbfsl{c}",
    u"\uD659": "\\mathsfbfsl{d}",
    u"\uD65A": "\\mathsfbfsl{e}",
    u"\uD65B": "\\mathsfbfsl{f}",
    u"\uD65C": "\\mathsfbfsl{g}",
    u"\uD65D": "\\mathsfbfsl{h}",
    u"\uD65E": "\\mathsfbfsl{i}",
    u"\uD65F": "\\mathsfbfsl{j}",
    u"\uD660": "\\mathsfbfsl{k}",
    u"\uD661": "\\mathsfbfsl{l}",
    u"\uD662": "\\mathsfbfsl{m}",
    u"\uD663": "\\mathsfbfsl{n}",
    u"\uD664": "\\mathsfbfsl{o}",
    u"\uD665": "\\mathsfbfsl{p}",
    u"\uD666": "\\mathsfbfsl{q}",
    u"\uD667": "\\mathsfbfsl{r}",
    u"\uD668": "\\mathsfbfsl{s}",
    u"\uD669": "\\mathsfbfsl{t}",
    u"\uD66A": "\\mathsfbfsl{u}",
    u"\uD66B": "\\mathsfbfsl{v}",
    u"\uD66C": "\\mathsfbfsl{w}",
    u"\uD66D": "\\mathsfbfsl{x}",
    u"\uD66E": "\\mathsfbfsl{y}",
    u"\uD66F": "\\mathsfbfsl{z}",
    u"\uD670": "\\mathtt{A}",
    u"\uD671": "\\mathtt{B}",
    u"\uD672": "\\mathtt{C}",
    u"\uD673": "\\mathtt{D}",
    u"\uD674": "\\mathtt{E}",
    u"\uD675": "\\mathtt{F}",
    u"\uD676": "\\mathtt{G}",
    u"\uD677": "\\mathtt{H}",
    u"\uD678": "\\mathtt{I}",
    u"\uD679": "\\mathtt{J}",
    u"\uD67A": "\\mathtt{K}",
    u"\uD67B": "\\mathtt{L}",
    u"\uD67C": "\\mathtt{M}",
    u"\uD67D": "\\mathtt{N}",
    u"\uD67E": "\\mathtt{O}",
    u"\uD67F": "\\mathtt{P}",
    u"\uD680": "\\mathtt{Q}",
    u"\uD681": "\\mathtt{R}",
    u"\uD682": "\\mathtt{S}",
    u"\uD683": "\\mathtt{T}",
    u"\uD684": "\\mathtt{U}",
    u"\uD685": "\\mathtt{V}",
    u"\uD686": "\\mathtt{W}",
    u"\uD687": "\\mathtt{X}",
    u"\uD688": "\\mathtt{Y}",
    u"\uD689": "\\mathtt{Z}",
    u"\uD68A": "\\mathtt{a}",
    u"\uD68B": "\\mathtt{b}",
    u"\uD68C": "\\mathtt{c}",
    u"\uD68D": "\\mathtt{d}",
    u"\uD68E": "\\mathtt{e}",
    u"\uD68F": "\\mathtt{f}",
    u"\uD690": "\\mathtt{g}",
    u"\uD691": "\\mathtt{h}",
    u"\uD692": "\\mathtt{i}",
    u"\uD693": "\\mathtt{j}",
    u"\uD694": "\\mathtt{k}",
    u"\uD695": "\\mathtt{l}",
    u"\uD696": "\\mathtt{m}",
    u"\uD697": "\\mathtt{n}",
    u"\uD698": "\\mathtt{o}",
    u"\uD699": "\\mathtt{p}",
    u"\uD69A": "\\mathtt{q}",
    u"\uD69B": "\\mathtt{r}",
    u"\uD69C": "\\mathtt{s}",
    u"\uD69D": "\\mathtt{t}",
    u"\uD69E": "\\mathtt{u}",
    u"\uD69F": "\\mathtt{v}",
    u"\uD6A0": "\\mathtt{w}",
    u"\uD6A1": "\\mathtt{x}",
    u"\uD6A2": "\\mathtt{y}",
    u"\uD6A3": "\\mathtt{z}",
    u"\uD6A8": "\\mathbf{\\Alpha}",
    u"\uD6A9": "\\mathbf{\\Beta}",
    u"\uD6AA": "\\mathbf{\\Gamma}",
    u"\uD6AB": "\\mathbf{\\Delta}",
    u"\uD6AC": "\\mathbf{\\Epsilon}",
    u"\uD6AD": "\\mathbf{\\Zeta}",
    u"\uD6AE": "\\mathbf{\\Eta}",
    u"\uD6AF": "\\mathbf{\\Theta}",
    u"\uD6B0": "\\mathbf{\\Iota}",
    u"\uD6B1": "\\mathbf{\\Kappa}",
    u"\uD6B2": "\\mathbf{\\Lambda}",
    u"\uD6B5": "\\mathbf{\\Xi}",
    u"\uD6B7": "\\mathbf{\\Pi}",
    u"\uD6B8": "\\mathbf{\\Rho}",
    u"\uD6B9": "\\mathbf{\\vartheta}",
    u"\uD6BA": "\\mathbf{\\Sigma}",
    u"\uD6BB": "\\mathbf{\\Tau}",
    u"\uD6BC": "\\mathbf{\\Upsilon}",
    u"\uD6BD": "\\mathbf{\\Phi}",
    u"\uD6BE": "\\mathbf{\\Chi}",
    u"\uD6BF": "\\mathbf{\\Psi}",
    u"\uD6C0": "\\mathbf{\\Omega}",
    u"\uD6C1": "\\mathbf{\\nabla}",
    u"\uD6C2": "\\mathbf{\\Alpha}",
    u"\uD6C3": "\\mathbf{\\Beta}",
    u"\uD6C4": "\\mathbf{\\Gamma}",
    u"\uD6C5": "\\mathbf{\\Delta}",
    u"\uD6C6": "\\mathbf{\\Epsilon}",
    u"\uD6C7": "\\mathbf{\\Zeta}",
    u"\uD6C8": "\\mathbf{\\Eta}",
    u"\uD6C9": "\\mathbf{\\theta}",
    u"\uD6CA": "\\mathbf{\\Iota}",
    u"\uD6CB": "\\mathbf{\\Kappa}",
    u"\uD6CC": "\\mathbf{\\Lambda}",
    u"\uD6CF": "\\mathbf{\\Xi}",
    u"\uD6D1": "\\mathbf{\\Pi}",
    u"\uD6D2": "\\mathbf{\\Rho}",
    u"\uD6D3": "\\mathbf{\\varsigma}",
    u"\uD6D4": "\\mathbf{\\Sigma}",
    u"\uD6D5": "\\mathbf{\\Tau}",
    u"\uD6D6": "\\mathbf{\\Upsilon}",
    u"\uD6D7": "\\mathbf{\\Phi}",
    u"\uD6D8": "\\mathbf{\\Chi}",
    u"\uD6D9": "\\mathbf{\\Psi}",
    u"\uD6DA": "\\mathbf{\\Omega}",
    u"\uD6DB": "\\partial ",
    u"\uD6DC": "\\in",
    u"\uD6DD": "\\mathbf{\\vartheta}",
    u"\uD6DE": "\\mathbf{\\varkappa}",
    u"\uD6DF": "\\mathbf{\\phi}",
    u"\uD6E0": "\\mathbf{\\varrho}",
    u"\uD6E1": "\\mathbf{\\varpi}",
    u"\uD6E2": "\\mathsl{\\Alpha}",
    u"\uD6E3": "\\mathsl{\\Beta}",
    u"\uD6E4": "\\mathsl{\\Gamma}",
    u"\uD6E5": "\\mathsl{\\Delta}",
    u"\uD6E6": "\\mathsl{\\Epsilon}",
    u"\uD6E7": "\\mathsl{\\Zeta}",
    u"\uD6E8": "\\mathsl{\\Eta}",
    u"\uD6E9": "\\mathsl{\\Theta}",
    u"\uD6EA": "\\mathsl{\\Iota}",
    u"\uD6EB": "\\mathsl{\\Kappa}",
    u"\uD6EC": "\\mathsl{\\Lambda}",
    u"\uD6EF": "\\mathsl{\\Xi}",
    u"\uD6F1": "\\mathsl{\\Pi}",
    u"\uD6F2": "\\mathsl{\\Rho}",
    u"\uD6F3": "\\mathsl{\\vartheta}",
    u"\uD6F4": "\\mathsl{\\Sigma}",
    u"\uD6F5": "\\mathsl{\\Tau}",
    u"\uD6F6": "\\mathsl{\\Upsilon}",
    u"\uD6F7": "\\mathsl{\\Phi}",
    u"\uD6F8": "\\mathsl{\\Chi}",
    u"\uD6F9": "\\mathsl{\\Psi}",
    u"\uD6FA": "\\mathsl{\\Omega}",
    u"\uD6FB": "\\mathsl{\\nabla}",
    u"\uD6FC": "\\mathsl{\\Alpha}",
    u"\uD6FD": "\\mathsl{\\Beta}",
    u"\uD6FE": "\\mathsl{\\Gamma}",
    u"\uD6FF": "\\mathsl{\\Delta}",
    u"\uD700": "\\mathsl{\\Epsilon}",
    u"\uD701": "\\mathsl{\\Zeta}",
    u"\uD702": "\\mathsl{\\Eta}",
    u"\uD703": "\\mathsl{\\Theta}",
    u"\uD704": "\\mathsl{\\Iota}",
    u"\uD705": "\\mathsl{\\Kappa}",
    u"\uD706": "\\mathsl{\\Lambda}",
    u"\uD709": "\\mathsl{\\Xi}",
    u"\uD70B": "\\mathsl{\\Pi}",
    u"\uD70C": "\\mathsl{\\Rho}",
    u"\uD70D": "\\mathsl{\\varsigma}",
    u"\uD70E": "\\mathsl{\\Sigma}",
    u"\uD70F": "\\mathsl{\\Tau}",
    u"\uD710": "\\mathsl{\\Upsilon}",
    u"\uD711": "\\mathsl{\\Phi}",
    u"\uD712": "\\mathsl{\\Chi}",
    u"\uD713": "\\mathsl{\\Psi}",
    u"\uD714": "\\mathsl{\\Omega}",
    u"\uD715": "\\partial ",
    u"\uD716": "\\in",
    u"\uD717": "\\mathsl{\\vartheta}",
    u"\uD718": "\\mathsl{\\varkappa}",
    u"\uD719": "\\mathsl{\\phi}",
    u"\uD71A": "\\mathsl{\\varrho}",
    u"\uD71B": "\\mathsl{\\varpi}",
    u"\uD71C": "\\mathbit{\\Alpha}",
    u"\uD71D": "\\mathbit{\\Beta}",
    u"\uD71E": "\\mathbit{\\Gamma}",
    u"\uD71F": "\\mathbit{\\Delta}",
    u"\uD720": "\\mathbit{\\Epsilon}",
    u"\uD721": "\\mathbit{\\Zeta}",
    u"\uD722": "\\mathbit{\\Eta}",
    u"\uD723": "\\mathbit{\\Theta}",
    u"\uD724": "\\mathbit{\\Iota}",
    u"\uD725": "\\mathbit{\\Kappa}",
    u"\uD726": "\\mathbit{\\Lambda}",
    u"\uD729": "\\mathbit{\\Xi}",
    u"\uD72B": "\\mathbit{\\Pi}",
    u"\uD72C": "\\mathbit{\\Rho}",
    u"\uD72D": "\\mathbit{O}",
    u"\uD72E": "\\mathbit{\\Sigma}",
    u"\uD72F": "\\mathbit{\\Tau}",
    u"\uD730": "\\mathbit{\\Upsilon}",
    u"\uD731": "\\mathbit{\\Phi}",
    u"\uD732": "\\mathbit{\\Chi}",
    u"\uD733": "\\mathbit{\\Psi}",
    u"\uD734": "\\mathbit{\\Omega}",
    u"\uD735": "\\mathbit{\\nabla}",
    u"\uD736": "\\mathbit{\\Alpha}",
    u"\uD737": "\\mathbit{\\Beta}",
    u"\uD738": "\\mathbit{\\Gamma}",
    u"\uD739": "\\mathbit{\\Delta}",
    u"\uD73A": "\\mathbit{\\Epsilon}",
    u"\uD73B": "\\mathbit{\\Zeta}",
    u"\uD73C": "\\mathbit{\\Eta}",
    u"\uD73D": "\\mathbit{\\Theta}",
    u"\uD73E": "\\mathbit{\\Iota}",
    u"\uD73F": "\\mathbit{\\Kappa}",
    u"\uD740": "\\mathbit{\\Lambda}",
    u"\uD743": "\\mathbit{\\Xi}",
    u"\uD745": "\\mathbit{\\Pi}",
    u"\uD746": "\\mathbit{\\Rho}",
    u"\uD747": "\\mathbit{\\varsigma}",
    u"\uD748": "\\mathbit{\\Sigma}",
    u"\uD749": "\\mathbit{\\Tau}",
    u"\uD74A": "\\mathbit{\\Upsilon}",
    u"\uD74B": "\\mathbit{\\Phi}",
    u"\uD74C": "\\mathbit{\\Chi}",
    u"\uD74D": "\\mathbit{\\Psi}",
    u"\uD74E": "\\mathbit{\\Omega}",
    u"\uD74F": "\\partial ",
    u"\uD750": "\\in",
    u"\uD751": "\\mathbit{\\vartheta}",
    u"\uD752": "\\mathbit{\\varkappa}",
    u"\uD753": "\\mathbit{\\phi}",
    u"\uD754": "\\mathbit{\\varrho}",
    u"\uD755": "\\mathbit{\\varpi}",
    u"\uD756": "\\mathsfbf{\\Alpha}",
    u"\uD757": "\\mathsfbf{\\Beta}",
    u"\uD758": "\\mathsfbf{\\Gamma}",
    u"\uD759": "\\mathsfbf{\\Delta}",
    u"\uD75A": "\\mathsfbf{\\Epsilon}",
    u"\uD75B": "\\mathsfbf{\\Zeta}",
    u"\uD75C": "\\mathsfbf{\\Eta}",
    u"\uD75D": "\\mathsfbf{\\Theta}",
    u"\uD75E": "\\mathsfbf{\\Iota}",
    u"\uD75F": "\\mathsfbf{\\Kappa}",
    u"\uD760": "\\mathsfbf{\\Lambda}",
    u"\uD763": "\\mathsfbf{\\Xi}",
    u"\uD765": "\\mathsfbf{\\Pi}",
    u"\uD766": "\\mathsfbf{\\Rho}",
    u"\uD767": "\\mathsfbf{\\vartheta}",
    u"\uD768": "\\mathsfbf{\\Sigma}",
    u"\uD769": "\\mathsfbf{\\Tau}",
    u"\uD76A": "\\mathsfbf{\\Upsilon}",
    u"\uD76B": "\\mathsfbf{\\Phi}",
    u"\uD76C": "\\mathsfbf{\\Chi}",
    u"\uD76D": "\\mathsfbf{\\Psi}",
    u"\uD76E": "\\mathsfbf{\\Omega}",
    u"\uD76F": "\\mathsfbf{\\nabla}",
    u"\uD770": "\\mathsfbf{\\Alpha}",
    u"\uD771": "\\mathsfbf{\\Beta}",
    u"\uD772": "\\mathsfbf{\\Gamma}",
    u"\uD773": "\\mathsfbf{\\Delta}",
    u"\uD774": "\\mathsfbf{\\Epsilon}",
    u"\uD775": "\\mathsfbf{\\Zeta}",
    u"\uD776": "\\mathsfbf{\\Eta}",
    u"\uD777": "\\mathsfbf{\\Theta}",
    u"\uD778": "\\mathsfbf{\\Iota}",
    u"\uD779": "\\mathsfbf{\\Kappa}",
    u"\uD77A": "\\mathsfbf{\\Lambda}",
    u"\uD77D": "\\mathsfbf{\\Xi}",
    u"\uD77F": "\\mathsfbf{\\Pi}",
    u"\uD780": "\\mathsfbf{\\Rho}",
    u"\uD781": "\\mathsfbf{\\varsigma}",
    u"\uD782": "\\mathsfbf{\\Sigma}",
    u"\uD783": "\\mathsfbf{\\Tau}",
    u"\uD784": "\\mathsfbf{\\Upsilon}",
    u"\uD785": "\\mathsfbf{\\Phi}",
    u"\uD786": "\\mathsfbf{\\Chi}",
    u"\uD787": "\\mathsfbf{\\Psi}",
    u"\uD788": "\\mathsfbf{\\Omega}",
    u"\uD789": "\\partial ",
    u"\uD78A": "\\in",
    u"\uD78B": "\\mathsfbf{\\vartheta}",
    u"\uD78C": "\\mathsfbf{\\varkappa}",
    u"\uD78D": "\\mathsfbf{\\phi}",
    u"\uD78E": "\\mathsfbf{\\varrho}",
    u"\uD78F": "\\mathsfbf{\\varpi}",
    u"\uD790": "\\mathsfbfsl{\\Alpha}",
    u"\uD791": "\\mathsfbfsl{\\Beta}",
    u"\uD792": "\\mathsfbfsl{\\Gamma}",
    u"\uD793": "\\mathsfbfsl{\\Delta}",
    u"\uD794": "\\mathsfbfsl{\\Epsilon}",
    u"\uD795": "\\mathsfbfsl{\\Zeta}",
    u"\uD796": "\\mathsfbfsl{\\Eta}",
    u"\uD797": "\\mathsfbfsl{\\vartheta}",
    u"\uD798": "\\mathsfbfsl{\\Iota}",
    u"\uD799": "\\mathsfbfsl{\\Kappa}",
    u"\uD79A": "\\mathsfbfsl{\\Lambda}",
    u"\uD79D": "\\mathsfbfsl{\\Xi}",
    u"\uD79F": "\\mathsfbfsl{\\Pi}",
    u"\uD7A0": "\\mathsfbfsl{\\Rho}",
    u"\uD7A1": "\\mathsfbfsl{\\vartheta}",
    u"\uD7A2": "\\mathsfbfsl{\\Sigma}",
    u"\uD7A3": "\\mathsfbfsl{\\Tau}",
    u"\uD7A4": "\\mathsfbfsl{\\Upsilon}",
    u"\uD7A5": "\\mathsfbfsl{\\Phi}",
    u"\uD7A6": "\\mathsfbfsl{\\Chi}",
    u"\uD7A7": "\\mathsfbfsl{\\Psi}",
    u"\uD7A8": "\\mathsfbfsl{\\Omega}",
    u"\uD7A9": "\\mathsfbfsl{\\nabla}",
    u"\uD7AA": "\\mathsfbfsl{\\Alpha}",
    u"\uD7AB": "\\mathsfbfsl{\\Beta}",
    u"\uD7AC": "\\mathsfbfsl{\\Gamma}",
    u"\uD7AD": "\\mathsfbfsl{\\Delta}",
    u"\uD7AE": "\\mathsfbfsl{\\Epsilon}",
    u"\uD7AF": "\\mathsfbfsl{\\Zeta}",
    u"\uD7B0": "\\mathsfbfsl{\\Eta}",
    u"\uD7B1": "\\mathsfbfsl{\\vartheta}",
    u"\uD7B2": "\\mathsfbfsl{\\Iota}",
    u"\uD7B3": "\\mathsfbfsl{\\Kappa}",
    u"\uD7B4": "\\mathsfbfsl{\\Lambda}",
    u"\uD7B7": "\\mathsfbfsl{\\Xi}",
    u"\uD7B9": "\\mathsfbfsl{\\Pi}",
    u"\uD7BA": "\\mathsfbfsl{\\Rho}",
    u"\uD7BB": "\\mathsfbfsl{\\varsigma}",
    u"\uD7BC": "\\mathsfbfsl{\\Sigma}",
    u"\uD7BD": "\\mathsfbfsl{\\Tau}",
    u"\uD7BE": "\\mathsfbfsl{\\Upsilon}",
    u"\uD7BF": "\\mathsfbfsl{\\Phi}",
    u"\uD7C0": "\\mathsfbfsl{\\Chi}",
    u"\uD7C1": "\\mathsfbfsl{\\Psi}",
    u"\uD7C2": "\\mathsfbfsl{\\Omega}",
    u"\uD7C3": "\\partial ",
    u"\uD7C4": "\\in",
    u"\uD7C5": "\\mathsfbfsl{\\vartheta}",
    u"\uD7C6": "\\mathsfbfsl{\\varkappa}",
    u"\uD7C7": "\\mathsfbfsl{\\phi}",
    u"\uD7C8": "\\mathsfbfsl{\\varrho}",
    u"\uD7C9": "\\mathsfbfsl{\\varpi}",
    u"\uD7CE": "\\mathbf{0}",
    u"\uD7CF": "\\mathbf{1}",
    u"\uD7D0": "\\mathbf{2}",
    u"\uD7D1": "\\mathbf{3}",
    u"\uD7D2": "\\mathbf{4}",
    u"\uD7D3": "\\mathbf{5}",
    u"\uD7D4": "\\mathbf{6}",
    u"\uD7D5": "\\mathbf{7}",
    u"\uD7D6": "\\mathbf{8}",
    u"\uD7D7": "\\mathbf{9}",
    u"\uD7D8": "\\mathbb{0}",
    u"\uD7D9": "\\mathbb{1}",
    u"\uD7DA": "\\mathbb{2}",
    u"\uD7DB": "\\mathbb{3}",
    u"\uD7DC": "\\mathbb{4}",
    u"\uD7DD": "\\mathbb{5}",
    u"\uD7DE": "\\mathbb{6}",
    u"\uD7DF": "\\mathbb{7}",
    u"\uD7E0": "\\mathbb{8}",
    u"\uD7E1": "\\mathbb{9}",
    u"\uD7E2": "\\mathsf{0}",
    u"\uD7E3": "\\mathsf{1}",
    u"\uD7E4": "\\mathsf{2}",
    u"\uD7E5": "\\mathsf{3}",
    u"\uD7E6": "\\mathsf{4}",
    u"\uD7E7": "\\mathsf{5}",
    u"\uD7E8": "\\mathsf{6}",
    u"\uD7E9": "\\mathsf{7}",
    u"\uD7EA": "\\mathsf{8}",
    u"\uD7EB": "\\mathsf{9}",
    u"\uD7EC": "\\mathsfbf{0}",
    u"\uD7ED": "\\mathsfbf{1}",
    u"\uD7EE": "\\mathsfbf{2}",
    u"\uD7EF": "\\mathsfbf{3}",
    u"\uD7F0": "\\mathsfbf{4}",
    u"\uD7F1": "\\mathsfbf{5}",
    u"\uD7F2": "\\mathsfbf{6}",
    u"\uD7F3": "\\mathsfbf{7}",
    u"\uD7F4": "\\mathsfbf{8}",
    u"\uD7F5": "\\mathsfbf{9}",
    u"\uD7F6": "\\mathtt{0}",
    u"\uD7F7": "\\mathtt{1}",
    u"\uD7F8": "\\mathtt{2}",
    u"\uD7F9": "\\mathtt{3}",
    u"\uD7FA": "\\mathtt{4}",
    u"\uD7FB": "\\mathtt{5}",
    u"\uD7FC": "\\mathtt{6}",
    u"\uD7FD": "\\mathtt{7}",
    u"\uD7FE": "\\mathtt{8}",
    u"\uD7FF": "\\mathtt{9}",
}

########NEW FILE########
__FILENAME__ = blog_post
from totalimpact.providers import provider
from totalimpact.providers import webpage
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderServerError

import json, os, re

import logging
logger = logging.getLogger('ti.providers.blog_post')

class Blog_Post(Provider):  

    example_id = ('blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com"}')
    metrics_url_template = "http://stats.wordpress.com/csv.php?api_key=%s&blog_uri=%s&table=views&days=-1&format=json&summarize=1&table=postviews&post_id=%s"
    biblio_url_template = "https://public-api.wordpress.com/rest/v1/sites/%s/?pretty=1"
    metrics_url_template_wordpress_site = "https://public-api.wordpress.com/rest/v1/sites/%s/?pretty=1"
    metrics_url_template_wordpress_post = "https://public-api.wordpress.com/rest/v1/sites/%s/posts/slug:%s?pretty=1"
    provenance_url_template = "%s"

    static_meta_dict = {}


    def __init__(self):
        super(Blog_Post, self).__init__()


    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return "blog_post" in namespace


    def get_best_id(self, aliases):
        aliases_dict = provider.alias_dict_from_tuples(aliases)

        # go through in preferred order
        for key in ["wordpress_blog_post", "blog_post", "url"]:
            if key in aliases_dict:
                nid = aliases_dict[key][0]
                return nid
        return None


    # overriding default because overriding aliases method
    @property
    def provides_aliases(self):
        return True

    # overriding default because overriding aliases method
    @property
    def provides_biblio(self):
        return True


    #override because need to break up id
    def _get_templated_url(self, template, nid, method=None):
        url = None
        if method=="biblio":
            nid = provider.strip_leading_http(nid).lower()
            url = template % (nid)
        elif method=="provenance":
            url = self.post_url_from_nid(nid)
        else:
            url = template % (nid)
        return(url)


    def post_url_from_nid(self, nid):
        try:
            return json.loads(nid)["post_url"]    
        except (KeyError, ValueError):
            return None

    def blog_url_from_nid(self, nid):
        try:
            return json.loads(nid)["blog_url"]    
        except (KeyError, ValueError):
            return None


    def wordpress_post_id_from_nid(self, nid):
        try:
            return json.loads(nid)["wordpress_post_id"]    
        except (KeyError, ValueError):
            return None


    # overriding
    def aliases(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):            

        aliases_dict = provider.alias_dict_from_tuples(aliases)
        new_aliases = []

        if "blog_post" in aliases_dict:
            nid = aliases_dict["blog_post"][0]
            post_url = self.post_url_from_nid(nid)

            # add url as alias if not already there
            new_alias = ("url", post_url)
            if new_alias not in aliases:
                new_aliases += [new_alias]

            # now add the wordpress alias info if it isn't already there
            if not "wordpress_blog_post" in aliases_dict:
                blog_url = provider.strip_leading_http(self.blog_url_from_nid(nid))
                wordpress_blog_api_url = self.metrics_url_template_wordpress_site % blog_url

                response = self.http_get(wordpress_blog_api_url)
                if "name" in response.text:
                    # it is a wordpress blog, so now get its wordpress post ID
                    if post_url.endswith("/"):
                        post_url = post_url[:-1]
                    post_end_slug = post_url.rsplit("/", 1)[1]

                    wordpress_post_api_url = self.metrics_url_template_wordpress_post %(blog_url, post_end_slug)
                    response = self.http_get(wordpress_post_api_url)
                    if "ID" in response.text:
                        wordpress_post_id = json.loads(response.text)["ID"]
                        nid_as_dict = json.loads(nid)
                        nid_as_dict.update({"wordpress_post_id": wordpress_post_id})
                        new_aliases += [("wordpress_blog_post", json.dumps(nid_as_dict))]

        return new_aliases



    # overriding
    def biblio(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True): 
        logger.info(u"calling webpage to handle aliases")

        nid = self.get_best_id(aliases)
        aliases_dict = provider.alias_dict_from_tuples(aliases)   
        nid = aliases_dict["blog_post"][0]
        post_url = self.post_url_from_nid(nid)
        blog_url = self.blog_url_from_nid(nid)

        biblio_dict = webpage.Webpage().biblio([("url", post_url)], provider_url_template, cache_enabled) 
        biblio_dict["url"] = post_url
        biblio_dict["account"] = provider.strip_leading_http(self.blog_url_from_nid(nid))
        if ("title" in biblio_dict) and ("|" in biblio_dict["title"]):
            (title, blog_title) = biblio_dict["title"].rsplit("|", 1)
            biblio_dict["title"] = title.strip()
            biblio_dict["blog_title"] = blog_title.strip()

        # try to get a response from wordpress.com
        url = self._get_templated_url(self.biblio_url_template, blog_url, "biblio")           
        response = self.http_get(url, cache_enabled=cache_enabled)
        if (response.status_code == 200) and ("name" in response.text):
            biblio_dict["hosting_platform"] = "wordpress.com"

        # in the future could get date posted from these sorts of calls:
        # https://public-api.wordpress.com/rest/v1/sites/blog.impactstory.org/posts/slug:link-your-figshare-and-impactstory-strips

        return biblio_dict




########NEW FILE########
__FILENAME__ = citeulike
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import simplejson
import re

import logging
logger = logging.getLogger('providers.citeulike')

class Citeulike(Provider):  

    example_id = ("doi", "10.1371/journal.pcbi.1000361")

    url = "http://www.citeulike.org/"
    descr = "CiteULike is a free service to help you to store, organise and share the scholarly papers you are reading."
    metrics_url_template = "http://www.citeulike.org/api/posts/for/doi/%s"
    provenance_url_template = "http://www.citeulike.org/doi/%s"

    static_meta_dict =  {
        "bookmarks": {
            "display_name": "bookmarks",
            "provider": "CiteULike",
            "provider_url": "http://www.citeulike.org/",
            "description": "Number of users who have bookmarked this item.",
            "icon": "http://citeulike.org/favicon.ico" ,
        }    
    }
    

    def __init__(self):
        super(Citeulike, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return("doi" == namespace)


    def _extract_metrics(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not provider._count_in_xml(page, 'posts'):
            raise ProviderContentMalformedError

        count = provider._count_in_xml(page, 'post')
        if count:
            metrics_dict = {'citeulike:bookmarks': count}
        else:
            metrics_dict = {}

        return metrics_dict




########NEW FILE########
__FILENAME__ = crossref
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderTimeout, ProviderServerError
from totalimpact.unicode_helpers import remove_nonprinting_characters
import itertools
import httplib, urllib, re

import logging
logger = logging.getLogger('ti.providers.crossref')

#!/usr/bin/env python

def clean_doi(input_doi):
    input_doi = remove_nonprinting_characters(input_doi)
    try:
        input_doi = input_doi.lower()
        if input_doi.startswith("http"):
            match = re.match("^https*://(dx\.)*doi.org/(10\..+)", input_doi)
            doi = match.group(2)
        elif "doi.org" in input_doi:
            match = re.match("^(dx\.)*doi.org/(10\..+)", input_doi)
            doi = match.group(2)
        elif input_doi.startswith("doi:"):
            match = re.match("^doi:(10\..+)", input_doi)
            doi = match.group(1)
        elif input_doi.startswith("10."):
            doi = input_doi
        elif "10." in input_doi:
            match = re.match(".*(10\.\d+.+)", input_doi, re.DOTALL)
            doi = match.group(1)
        else:
            doi = None
            try:
                logger.debug(u"MALFORMED DOI {input_doi}".format(
                    input_doi=input_doi))
            except:
                logger.debug(u"MALFORMED DOI, can't print doi")


    except AttributeError:
        doi = None

    return doi


class Crossref(Provider):  

    example_id = ("doi", "10.1371/journal.pcbi.1000361")
    url = "http://www.crossref.org/"
    descr = "An official Digital Object Identifier (DOI) Registration Agency of the International DOI Foundation."
    aliases_url_template = "http://dx.doi.org/%s"
    biblio_url_template = "http://dx.doi.org/%s"
    # example code to test 
    # curl -D - -L -H   "Accept: application/vnd.citationstyles.csl+json" "http://dx.doi.org/10.1021/np070361t" 

    def __init__(self):
        super(Crossref, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return("doi" == namespace)

    @property
    def provides_aliases(self):
         return True

    @property
    def provides_biblio(self):
         return True

    # overriding default because overriding member_items method
    @property
    def provides_members(self):
        return True

    # default method; providers can override
    def get_biblio_for_id(self, 
            id,
            provider_url_template=None, 
            cache_enabled=True):

        self.logger.debug(u"%s /biblio_print getting biblio for %s" % (self.provider_name, id))

        if not provider_url_template:
            provider_url_template = self.biblio_url_template
        url = provider_url_template % id

        self.logger.debug(u"%s /biblio_print _lookup_biblio_from_doi for %s" % (self.provider_name, id))
        biblio_dict = self._lookup_biblio_from_doi(id, url, cache_enabled)

        self.logger.debug(u"%s /biblio_print _lookup_issn_from_doi for %s" % (self.provider_name, id))
        biblio_dict.update(self._lookup_issn_from_doi(id, url, cache_enabled))

        self.logger.debug(u"%s /biblio_print free_fulltext_fragments for %s" % (self.provider_name, id))
        
        free_fulltext_fragments = ["/npre.", "/peerj.preprints"]
        if any(doi_fragment in id for doi_fragment in free_fulltext_fragments):
            biblio_dict["free_fulltext_url"] = url
        elif ("issn" in biblio_dict) and provider.is_issn_in_doaj(biblio_dict["issn"]):
            biblio_dict["free_fulltext_url"] = url

        return biblio_dict


    def _lookup_issn_from_doi(self, id, url, cache_enabled):
        # try to get a response from the data provider      
        response = self.http_get(url, 
            cache_enabled=cache_enabled, 
            allow_redirects=True,
            headers={"Accept": "application/json", "User-Agent": "impactstory.org"})

        if response.status_code != 200:
            self.logger.info(u"%s status_code=%i" 
                % (self.provider_name, response.status_code))            
            if response.status_code == 404: #not found
                return {}
            elif response.status_code == 403: #forbidden
                return {}
            elif (response.status_code == 406) or (response.status_code == 500): #this call isn't supported for datacite dois
                return {}
            elif ((response.status_code >= 300) and (response.status_code < 400)): #redirect
                return {}
            else:
                self._get_error(response.status_code, response)

        # extract the aliases
        try:
            biblio_dict = self._extract_biblio_issn(response.text, id)
        except (AttributeError, TypeError):
            biblio_dict = {}

        return biblio_dict


    def _extract_biblio_issn(self, page, id=None):
        dict_of_keylists = {
            'issn' : ['ISSN']
        }
        biblio_dict = provider._extract_from_json(page, dict_of_keylists)
        if not biblio_dict:
          return {}

        if "issn" in biblio_dict:
            biblio_dict["issn"] = biblio_dict["issn"][0].replace("-", "")

        return biblio_dict  


    def _lookup_biblio_from_doi(self, id, url, cache_enabled):
        # try to get a response from the data provider        
        response = self.http_get(url, 
            cache_enabled=cache_enabled, 
            allow_redirects=True,
            headers={"Accept": "application/vnd.citationstyles.csl+json", "User-Agent": "impactstory.org"})

        if response.status_code != 200:
            self.logger.info(u"%s status_code=%i" 
                % (self.provider_name, response.status_code))            
            if response.status_code == 404: #not found
                return {}
            elif response.status_code == 403: #forbidden
                return {}
            elif ((response.status_code >= 300) and (response.status_code < 400)): #redirect
                return {}
            else:
                self._get_error(response.status_code, response)

        # extract the aliases
        try:
            biblio_dict = self._extract_biblio(response.text, id)
        except (AttributeError, TypeError):
            biblio_dict = {}

        return biblio_dict


    def _extract_biblio(self, page, id=None):
        dict_of_keylists = {
            'title' : ['title'],
            'year' : ['issued'],
            'repository' : ['publisher'],
            'journal' : ['container-title'],
            'authors_literal' : ['author']
        }
        biblio_dict = provider._extract_from_json(page, dict_of_keylists)
        if not biblio_dict:
          return {}

        try:
            surname_list = [author["family"] for author in biblio_dict["authors_literal"]]
            if surname_list:
                biblio_dict["authors"] = u", ".join(surname_list)
                del biblio_dict["authors_literal"]
        except (IndexError, KeyError):
            try:
                literal_list = [author["literal"] for author in biblio_dict["authors_literal"]]
                if literal_list:
                    biblio_dict["authors_literal"] = u"; ".join(literal_list)
            except (IndexError, KeyError):
                pass

        try:
            if "year" in biblio_dict:
                if "raw" in biblio_dict["year"]:
                    biblio_dict["year"] = str(biblio_dict["year"]["raw"])
                elif "date-parts" in biblio_dict["year"]:
                    biblio_dict["year"] = str(biblio_dict["year"]["date-parts"][0][0])
                biblio_dict["year"] = re.sub("\D", "", biblio_dict["year"])
                if not biblio_dict["year"]:
                    del biblio_dict["year"]

        except IndexError:
            logger.info(u"/biblio_print could not parse year {biblio_dict}".format(
                biblio_dict=biblio_dict))
            del biblio_dict["year"]

        # replace many white spaces and \n with just one space
        try:
            biblio_dict["title"] = re.sub(u"\s+", u" ", biblio_dict["title"])
        except KeyError:
            pass

        return biblio_dict  


    #overriding default
    # if no doi, try to get doi from biblio
    # after that, if doi, get url and biblio
    def aliases(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):            

        aliases_dict = provider.alias_dict_from_tuples(aliases)

        doi = None
        new_aliases = []

        if "doi" in aliases_dict:
            doi = aliases_dict["doi"][0]
        else:
            if "url" in aliases_dict:
                for url in aliases_dict["url"]:
                    if url.startswith("http://dx.doi.org/"):
                        doi = url.replace("http://dx.doi.org/", "")
                        new_aliases += [("doi", doi)]

        if not doi:
            if "biblio" in aliases_dict:
                doi = self._lookup_doi_from_biblio(aliases_dict["biblio"][0], cache_enabled)
                if doi:
                    new_aliases += [("doi", doi)]   
                else:
                    if "url" in aliases_dict["biblio"][0]:
                        new_aliases += [("url", aliases_dict["biblio"][0]["url"])] 

        if not doi:
            # nothing else we can do 
            return new_aliases  #urls if we have them, otherwise empty list

        new_aliases += self._lookup_urls_from_doi(doi, provider_url_template, cache_enabled)
        
        # get uniques for things that are unhashable
        new_aliases_unique = [k for k,v in itertools.groupby(sorted(new_aliases))]
        return new_aliases_unique

    def _lookup_doi_from_biblio(self, biblio, cache_enabled):
        if not biblio:
            return []

        try:
            if (biblio["journal"] == ""):
                # need to have journal or can't look up with current api call
                logger.info(u"%20s /biblio_print NO DOI because no journal in %s" % (
                    self.provider_name, biblio))
                return []
            query_string =  (u"|%s|%s|%s|%s|%s|%s||%s|" % (
                biblio.get("journal", ""),
                biblio.get("first_author", biblio.get("authors", "").split(",")[0].strip()),
                biblio.get("volume", ""),
                biblio.get("number", ""),
                biblio.get("first_page", ""),
                biblio.get("year", ""),
                "ImpactStory"
                ))
        except KeyError:
            logger.info(u"%20s /biblio_print NO DOI because missing needed attribute in %s" % (
                self.provider_name, biblio))
            return []


        # for more info on crossref spec, see
        # http://ftp.crossref.org/02publishers/25query_spec.html
        url = "http://doi.crossref.org/servlet/query?pid=totalimpactdev@gmail.com&qdata=%s" % query_string

        try:
            logger.debug(u"%20s /biblio_print calling crossref at %s" % (self.provider_name, url))
            # doi-lookup call to crossref can take a while, give it a long timeout
            response = self.http_get(url, timeout=30, cache_enabled=cache_enabled)
        except ProviderTimeout:
            raise ProviderTimeout("CrossRef timeout")

        if response.status_code != 200:
            raise ProviderServerError("CrossRef status code was not 200")

        if not biblio["journal"].lower() in response.text.lower():
            raise ProviderServerError("CrossRef returned invalid text response")

        response_lines = response.text.split("\n")

        split_lines = [line.split("|") for line in response_lines if line]
        line_keys = [line[-2].strip() for line in split_lines]
        dois = [line[-1].strip() for line in split_lines]

        for key, doi in zip(line_keys, dois):
            if not doi:
                try:
                    logger.debug(u"%20s /biblio_print NO DOI from %s, %s" %(self.provider_name, biblio, key))
                except KeyError:
                    logger.debug(u"%20s /biblio_print NO DOI from %s, %s" %(self.provider_name, "", key))                    

        return doi


    # overriding default
    # gets url and biblio from doi
    def _lookup_urls_from_doi(self, 
            doi, 
            provider_url_template=None, 
            cache_enabled=True):

        self.logger.debug(u"%s getting aliases for %s" % (self.provider_name, doi))

        if not provider_url_template:
            provider_url_template = self.aliases_url_template
        # make it this way because don't want escaping
        doi_url = provider_url_template % doi

        # add url for http://dx.doi.org/ without escaping
        new_aliases = [("url", doi_url)]

        # add biblio
        biblio_dict = self.biblio([("doi", doi)])
        if biblio_dict:
            new_aliases += [("biblio", biblio_dict)]

        # try to get the redirect as well
        response = self.http_get(doi_url, cache_enabled=cache_enabled, allow_redirects=True)

        if response.status_code >= 400:
            self.logger.info(u"%s http_get status code=%i" 
                % (self.provider_name, response.status_code))
            #raise provider.ProviderServerError("doi resolve")
        else:
            try:  
                # url the doi resolved to     
                redirected_url = response.url
                # remove session stuff at the end
                url_to_store = redirected_url.split(";jsessionid")[0]
                new_aliases += [("url", url_to_store)]
            except (TypeError, AttributeError):
                pass

        return new_aliases


    # overriding because don't need to look up
    def member_items(self, 
            query_string, 
            provider_url_template=None, 
            cache_enabled=True):

        if not self.provides_members:
            raise NotImplementedError()

        self.logger.debug(u"%s getting member_items for %s" % (self.provider_name, query_string))

        doi_string = query_string.strip(" ")
        if not doi_string:
            return []
        dois = [clean_doi(doi) for doi in doi_string.split("\n")]
        aliases_tuples = [("doi", doi) for doi in dois if doi]

        return(aliases_tuples)



########NEW FILE########
__FILENAME__ = dataone
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import simplejson

import logging
logger = logging.getLogger('ti.providers.dataone')

class Dataone(Provider):  

    example_id = ("dataone", "esa.44.1")

    url = "http://www.dataone.org"
    descr = "Cyberinfrastructure for new innovative environmental science"
    biblio_url_template = "https://cn.dataone.org/cn/v1/resolve/%s"
    aliases_url_template = "https://cn.dataone.org/cn/v1/resolve/%s"

    def __init__(self):
        super(Dataone, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return("dataone" == namespace)

    #override because need to add "doi:" prefix when necessary
    def _get_templated_url(self, template, id, method=None):
        if id.startswith("10."):
            id = "doi:" + id
        url = template % id
        return(url)

    def _extract_biblio(self, redirect_page, id=None):
        redirect_dict_of_keylists = {
            'url' : ['url']
        }

        redirect_dict = provider._extract_from_xml(redirect_page, redirect_dict_of_keylists)

        logger.info(u"%20s WARNING, url= %s" 
                % (self.provider_name, redirect_dict["url"]))            

        # try to get a response from the data provider        
        response = self.http_get(redirect_dict["url"])

        if response.status_code != 200:
            logger.warning(u"%20s WARNING, status_code=%i getting %s" 
                % (self.provider_name, response.status_code, url))            
            self._get_error(response.status_code, response)
            return {}

        dict_of_keylists = {
            'title' : ['dataset', 'title'], 
            'published_date' : ['dataset', 'pubDate']
        }
        biblio_dict = provider._extract_from_xml(response.text, dict_of_keylists)

        return biblio_dict    
       
    def _extract_aliases(self, page, id=None):
        dict_of_keylists = {
            'url' : ['url']
        }

        aliases_dict = provider._extract_from_xml(page, dict_of_keylists)

        try:
            doi = provider.doi_from_url_string(aliases_dict["url"])
            if doi:
                aliases_dict["doi"] = doi
        except KeyError:
            pass

        if aliases_dict:
            aliases_list = [(namespace, nid) for (namespace, nid) in aliases_dict.iteritems()]
        else:
            aliases_list = []
        return aliases_list


########NEW FILE########
__FILENAME__ = delicious
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
import hashlib
import urllib

import logging
logger = logging.getLogger('ti.providers.delicious')

class Delicious(Provider):  

    example_id = ("url", "http://total-impact.org")
    metrics_url_template = "http://feeds.delicious.com/v2/json/url/%s?count=100"
    provenance_url_template = "http://delicious.com/link/%s"
    url = "http://www.delicious.com"
    descr = "Online social bookmarking service"
    static_meta_dict = {
        "bookmarks": {
            "display_name": "bookmarks",
            "provider": "Delicious",
            "provider_url": "http://www.delicious.com/",
            "description": "The number of bookmarks to this artifact (maximum=100).",
            "icon": "http://g.etfv.co/http://delicious.com"  #couldn't figure out link for local favicon
        }
    }

    def __init__(self):
        super(Delicious, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return("url" == namespace)

    def get_best_id(self, aliases):
        return self.get_relevant_alias_with_most_metrics("delicious:bookmarks", aliases)

    # overriding default because delicious needs md5 of url in template
    def _get_templated_url(self, template, id, method=None):
        try:
            id_unicode = unicode(id, "UTF-8")
        except TypeError:
            id_unicode = id
        id_utf8 = id_unicode.encode("UTF-8")
        md5_of_url = hashlib.md5(id_utf8).hexdigest()
        url = template % md5_of_url
        return(url)

    def _extract_metrics(self, page, status_code=200, id=None):
        metrics_dict = {}
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        data = provider._load_json(page)
        number_of_bookmarks = len(data)
        if number_of_bookmarks:
            metrics_dict = {
                'delicious:bookmarks' : number_of_bookmarks
            }

        return metrics_dict

    def provenance_url(self, metric_name, aliases):
        # Returns the same provenance url for all metrics
        id = self.get_best_id(aliases)

        if not id:
            return None

        # first we need to get a user, by looking up metrics again
        url = self._get_templated_url(self.provenance_url_template, id, "provenance")

        return url




########NEW FILE########
__FILENAME__ = dryad
from totalimpact.providers import provider
from totalimpact.providers import crossref
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderItemNotFoundError
from xml.dom import minidom 
from xml.parsers.expat import ExpatError
import re

import logging
logger = logging.getLogger('ti.providers.dryad')

class Dryad(Provider):  

    example_id = ("doi", "10.5061/dryad.7898")

    descr = "An international repository of data underlying peer-reviewed articles in the basic and applied biology."
    url = "http://www.datadryad.org"
    provenance_url_template = "http://dx.doi.org/%s"
    # No aliases_url_template because uses crossref
    # No biblio_url_template because uses crossref
    metrics_url_template = "http://dx.doi.org/%s"

    static_meta_dict = {
        "package_views": {
            "display_name": "package views",
            "provider": "Dryad",
            "provider_url": "http:\/\/www.datadryad.org\/",
            "description": "Dryad package views: number of views of the main package page",
            "icon": "http:\/\/datadryad.org\/favicon.ico",
        },
        "total_downloads": {
            "display_name": "total downloads",
            "provider": "Dryad",
            "provider_url": "http:\/\/www.datadryad.org\/",
            "description": "Dryad total downloads: combined number of downloads of the data package and data files",
            "icon": "http:\/\/datadryad.org\/favicon.ico",
        },
    }
        
    DRYAD_DOI_PATTERN = re.compile(r"(10\.5061/.*)")
    DRYAD_VIEWS_PACKAGE_PATTERN = re.compile("<th>Pageviews</th>\W*<td>(?P<views>\d+)</td>\W*</tr>", re.DOTALL)
    DRYAD_DOWNLOADS_PATTERN = re.compile("<tr>\W*<th>Downloaded</th>\W*<td>(?P<downloads>\d+) times</td>\W*</tr>", re.DOTALL)

    def __init__(self):
        super(Dryad, self).__init__()
        self.crossref = crossref.Crossref()
        
    def _is_dryad_doi(self, doi):
        response = self.DRYAD_DOI_PATTERN.search(doi)
        if response:
            return(True)
        else:
            return(False)

    def _get_dryad_doi(self, aliases):
        for doi in [nid for (namespace, nid) in aliases if namespace == 'doi']:
            if self._is_dryad_doi(doi):
                return doi
        return None

    def is_relevant_alias(self, alias):
        if not alias:
            return False
        (namespace, nid) = alias
        is_relevant = (namespace=="doi" and self._is_dryad_doi(nid))
        return is_relevant

    @property
    def provides_aliases(self):
         return True

    @property
    def provides_biblio(self):
         return True

    def aliases(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):  
        logger.info(u"calling crossref to handle aliases")
        return self.crossref.aliases(aliases, provider_url_template, cache_enabled)          

    def biblio(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):  
        logger.info(u"calling crossref to handle aliases")
        biblio = self.crossref.biblio(aliases, provider_url_template, cache_enabled)          

        # try to parse out last names, for now using most basic approach
        if not biblio:
            return {}
        if "authors_literal" in biblio:
            lnames = [author.split(u",")[0] for author in biblio["authors_literal"].split(u";")]
            biblio["authors"] = u",".join(lnames)
            del biblio["authors_literal"]
        return biblio

    def _extract_metrics(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 303:
                pass #this is ok
            elif status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if "Dryad" not in page:
            raise ProviderContentMalformedError

        metrics_dict = {}

        view_matches_package = self.DRYAD_VIEWS_PACKAGE_PATTERN.search(page)
        try:
            views = int(view_matches_package.group("views"))
            if views:
                metrics_dict["dryad:package_views"] = views
        except (ValueError, AttributeError):
            pass
        
        download_matches = self.DRYAD_DOWNLOADS_PATTERN.finditer(page)
        try:
            downloads = [int(download_match.group("downloads")) for download_match in download_matches]
            downloads_sum = sum(downloads)
            if downloads_sum:
                metrics_dict["dryad:total_downloads"] = downloads_sum
        except (ValueError, AttributeError):
            pass

        return metrics_dict


    def _get_named_arr_int_from_xml(self, xml, name, is_expected=True):
        """ Find the first node in the XML <arr> sections which are of node type <int>
            and return their text values. """
        identifiers = []
        arrs = self._get_named_arrs_from_xml(xml, name, is_expected)
        for arr in arrs:
            node = arr.getElementsByTagName('int')[0]
            identifiers.append(node.firstChild.nodeValue)
        return identifiers

    def _get_named_arr_str_from_xml(self, xml, name, is_expected=True):
        """ Find the first node in the XML <arr> sections which are of node type <str>
            and return their text values. """
        identifiers = []
        arrs = self._get_named_arrs_from_xml(xml, name, is_expected)

        for arr in arrs:
            node = arr.getElementsByTagName('str')[0]
            identifiers.append(node.firstChild.nodeValue)
        return identifiers

    def _get_named_arrs_from_xml(self, xml, name, is_expected=True):
        """ Find <arr> sections in the given xml document which have a
            match for the name attribute """
        try:
            doc = minidom.parseString(xml.encode('utf-8'))
        except ExpatError, e:
            raise ProviderContentMalformedError("Content parse provider supplied XML document")
        arrs = doc.getElementsByTagName('arr')
        matching_arrs = [elem for elem in arrs if elem.attributes['name'].value == name]
        if (is_expected and (len(matching_arrs) == 0)):
            raise ProviderContentMalformedError("Did not find expected number of matching arr blocks")

        return matching_arrs


########NEW FILE########
__FILENAME__ = figshare
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import re

import logging
logger = logging.getLogger('ti.providers.figshare')

class Figshare(Provider):  

    example_id = ("doi", "10.6084/m9.figshare.92393")

    url = "http://figshare.com"
    descr = "Make all of your research outputs sharable, citable and visible in the browser for free."
    biblio_url_template = "http://api.figshare.com/v1/articles/%s"
    aliases_url_template = "http://api.figshare.com/v1/articles/%s"
    metrics_url_template = "http://api.figshare.com/v1/articles/%s"
    provenance_url_template = "http://dx.doi.org/%s"
    member_items_url_template = "http://api.figshare.com/v1/authors/%s?page=%s"


    static_meta_dict = {
        "shares": {
            "display_name": "shares",
            "provider": "figshare",
            "provider_url": "http://figshare.com",
            "description": "The number of times this has been shared",
            "icon": "http://figshare.com/static/img/favicon.png",
        },
        "downloads": {
            "display_name": "downloads",
            "provider": "figshare",
            "provider_url": "http://figshare.com",
            "description": "The number of times this has been downloaded",
            "icon": "http://figshare.com/static/img/favicon.png",
            },
        "views": {
            "display_name": "views",
            "provider": "figshare",
            "provider_url": "http://figshare.com",
            "description": "The number of times this item has been viewed",
            "icon": "http://figshare.com/static/img/favicon.png",
            }
    }     

    def __init__(self):
        super(Figshare, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        is_figshare_doi = (namespace == "doi") and (".figshare." in nid.lower())
        return is_figshare_doi

    @property
    def provides_members(self):
         return True

    def get_figshare_userid_from_author_url(self, url):
        match = re.findall("figshare.com\/authors\/.*?\/(\d+)", url)
        return match[0]


    def _extract_aliases(self, page, id=None):
        dict_of_keylists = {"url": ["figshare_url"]}

        item = self._extract_figshare_record(page, id)
        aliases_dict = provider._extract_from_data_dict(item, dict_of_keylists)

        if aliases_dict:
            aliases_list = [(namespace, nid) for (namespace, nid) in aliases_dict.iteritems()]
        else:
            aliases_list = []
        return aliases_list


    def _extract_biblio(self, page, id=None):
        dict_of_keylists = {
            'title' : ['title'],
            'genre' : ['defined_type'],
            #'authors_literal' : ['authors'],
            'published_date' : ['published_date']
        }
        item = self._extract_figshare_record(page, id)
        biblio_dict = provider._extract_from_data_dict(item, dict_of_keylists)

        biblio_dict["repository"] = "figshare"
        
        try:
            biblio_dict["year"] = int(biblio_dict["published_date"][-4:])
        except (KeyError, TypeError):
            pass

        if "genre" in biblio_dict:
            genre = biblio_dict["genre"].lower()
            #override
            if genre in ["figure", "poster"]:
                genre = biblio_dict["genre"]
            elif genre == "presentation":
                genre = "slides"
            elif genre == "paper":
                genre = "article"
            elif genre == "media":
                genre = "video"   
            else:
                genre = "dataset"  #includes fileset 
            biblio_dict["genre"] = genre        

            if biblio_dict["genre"] == "article":
                biblio_dict["free_fulltext_url"] = self._get_templated_url(self.provenance_url_template, id, "provenance")

        # the authors data is messy, so just give up for now
        # if "authors_literal" in biblio_dict:
        #     surname_list = [author["last_name"] for author in biblio_dict["authors_literal"]]
        #     if surname_list:
        #         biblio_dict["authors"] = ", ".join(surname_list)
        #         del biblio_dict["authors_literal"]

        return biblio_dict   


    def _extract_figshare_record(self, page, id):
        data = provider._load_json(page)
        if not data:
            return {}
        item = data["items"][0]
        if str(item["article_id"]) in id:
            return item
        else:
            return {}


    def _extract_metrics(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        dict_of_keylists = {
            'figshare:shares' : ['shares'],
            'figshare:downloads' : ['downloads'],
            'figshare:views' : ['views']
        }
        item = self._extract_figshare_record(page, id)
        metrics_dict = provider._extract_from_data_dict(item, dict_of_keylists)
        return metrics_dict


    def _extract_members(self, page, query_string=None): 
        data = provider._load_json(page)        
        dois = [item["DOI"].replace("http://dx.doi.org/", "") for item in data["items"]]
        doi_aliases = [("doi", doi) for doi in dois]
        return(doi_aliases)


    # default method; providers can override
    def member_items(self, 
            account_name,
            provider_url_template=None, 
            cache_enabled=True):

        if not self.provides_members:
            raise NotImplementedError()

        self.logger.debug(u"%s getting member_items for %s" % (self.provider_name, account_name))

        if not provider_url_template:
            provider_url_template = self.member_items_url_template

        figshare_userid = self.get_figshare_userid_from_author_url(account_name)
        next_page = 1
        members = []
        while next_page:

            url = provider_url_template % (figshare_userid, next_page)
            
            # try to get a response from the data provider  
            response = self.http_get(url, cache_enabled=cache_enabled)

            if response.status_code != 200:
                self.logger.info(u"%s status_code=%i" 
                    % (self.provider_name, response.status_code))            
                if response.status_code == 404:
                    raise ProviderItemNotFoundError
                elif response.status_code == 303: #redirect
                    pass                
                else:
                    self._get_error(response.status_code, response)

            # extract the member ids
            number_of_items_per_page = 10 #figshare default
            try:
                page = response.text
                data = provider._load_json(page)
                if data["items_found"] > next_page*number_of_items_per_page:
                    next_page += 1
                else:
                    next_page = None
                members += self._extract_members(page, account_name)
            except (AttributeError, TypeError):
                next_page = None

        return(members)



########NEW FILE########
__FILENAME__ = github
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import os, re

import logging
logger = logging.getLogger('ti.providers.github')

class Github(Provider):  

    example_id = ("github", "egonw,cdk")

    url = "http://github.com"
    descr = "A social, online repository for open-source software."
    member_items_url_template = "https://api.github.com/users/%s/repos?client_id=" + os.environ["GITHUB_CLIENT_ID"] + "&client_secret=" + os.environ["GITHUB_CLIENT_SECRET"]
    biblio_url_template = "https://api.github.com/repos/%s/%s?client_id=" + os.environ["GITHUB_CLIENT_ID"] + "&client_secret=" + os.environ["GITHUB_CLIENT_SECRET"]
    aliases_url_template = "https://api.github.com/repos/%s/%s?client_id=" + os.environ["GITHUB_CLIENT_ID"] + "&client_secret=" + os.environ["GITHUB_CLIENT_SECRET"]
    metrics_url_template = "https://api.github.com/repos/%s/%s?client_id=" + os.environ["GITHUB_CLIENT_ID"] + "&client_secret=" + os.environ["GITHUB_CLIENT_SECRET"]
    repo_url_template = "https://github.com/%s/%s"

    provenance_url_templates = {
        "github:stars" : "https://github.com/%s/%s/stargazers",
        "github:forks" : "https://github.com/%s/%s/network/members"
        }

    static_meta_dict = {
        "stars": {
            "display_name": "stars",
            "provider": "GitHub",
            "provider_url": "http://github.com",
            "description": "The number of people who have given the GitHub repository a star",
            "icon": "https://github.com/fluidicon.png",
        },
        "forks": {
            "display_name": "forks",
            "provider": "GitHub",
            "provider_url": "http://github.com",
            "description": "The number of people who have forked the GitHub repository",
            "icon": "https://github.com/fluidicon.png",
            }
    }     

    def __init__(self):
        super(Github, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        if (("url" == namespace) and ("github.com/" in nid)):
            return True
        elif (namespace == "github"):  # deprecate github namespace after /v1
            return True
        else:
            return False

    #override because need to break up id
    def _get_templated_url(self, template, nid, method=None):
        url = None
        if method=="members":
            url = template % nid
        else:
            if "http" in nid:
                (host, username, repo_name) = nid.rsplit("/", 2)
            else:
                (username, repo_name) = nid.split(",")  # deprecate github namespace after /v1
            url = template % (username, repo_name)

        return(url)

    def _extract_members(self, page, account_name): 
        members = []
        # add repositories from account
        data = provider._load_json(page)
        hits = [hit["name"] for hit in data]
        members += [("url", self.repo_url_template %(account_name, hit)) for hit in list(set(hits))]

        return(members)


    def _extract_biblio(self, page, id=None):
        dict_of_keylists = {
            'title' : ['name'],
            'description' : ['description'],
            'owner' : ['owner', 'login'],
            'url' : ['svn_url'],
            'last_push_date' : ['pushed_at'],
            'create_date' : ['created_at']
        }
        biblio_dict = provider._extract_from_json(page, dict_of_keylists)
        try:
            biblio_dict["year"] = biblio_dict["create_date"][0:4]
        except KeyError:
            pass

        return biblio_dict    
       
    def _extract_aliases(self, page, id=None):
        dict_of_keylists = {"url": ["svn_url"], 
                            "title" : ["name"]}

        aliases_dict = provider._extract_from_json(page, dict_of_keylists)
        if aliases_dict:
            aliases_list = [(namespace, nid) for (namespace, nid) in aliases_dict.iteritems()]
        else:
            aliases_list = []
        return aliases_list


    def _extract_metrics(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not "forks_count" in page:
            raise ProviderContentMalformedError

        dict_of_keylists = {
            'github:stars' : ['watchers'],
            'github:forks' : ['forks']
        }

        metrics_dict = provider._extract_from_json(page, dict_of_keylists)

        return metrics_dict


    # overriding default because different provenance url for each metric
    def provenance_url(self, metric_name, aliases):
        nid = self.get_best_id(aliases)
        if not nid:
            return None
        provenance_url = self._get_templated_url(self.provenance_url_templates[metric_name], nid, "provenance")
        return provenance_url

########NEW FILE########
__FILENAME__ = mendeley
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderAuthenticationError
from totalimpact import tiredis
from totalimpact.utils import Retry

import simplejson, urllib, os, string, itertools
import requests
import requests.auth
import redis
from urllib import urlencode
from urlparse import parse_qs, urlsplit, urlunsplit

import logging
logger = logging.getLogger('ti.providers.mendeley')
shared_redis = tiredis.from_url(os.getenv("REDIS_URL"), db=0)


### NOTE:  TO GET MENDELEY PROVIDER WORKING
# you need to bootstrap the access and refresh tokens into REDIS_URL from production redis.
# instructions on how to do that are in repo total-impact-deploy/instructions.md


# from https://gist.github.com/jalperin/8b3367b65012291fe23f
def get_token():
    client_auth = requests.auth.HTTPBasicAuth(os.getenv("MENDELEY_OAUTH2_CLIENT_ID"), os.getenv("MENDELEY_OAUTH2_SECRET"))
    post_data = {"grant_type": "authorization_code",
                 "code": os.getenv("MENDELEY_OAUTH2_GENERAGED_CODE"),
                 "redirect_uri": "http://impactstory.org"}
    token_url = 'https://api-oauth2.mendeley.com/oauth/token'
    response = requests.post(token_url,
                             auth=client_auth,
                             data=post_data)
    token_json = response.json()
    return token_json

def renew_token(access_token, refresh_token):
    logger.debug(u"Mendeley: renewing access token")
    client_auth = requests.auth.HTTPBasicAuth(os.getenv("MENDELEY_OAUTH2_CLIENT_ID"), os.getenv("MENDELEY_OAUTH2_SECRET"))
    headers = {"Authorization": "bearer " + access_token}
    post_data = {"grant_type": "refresh_token",
                 "refresh_token": refresh_token}
    token_url = 'https://api-oauth2.mendeley.com/oauth/token'
    response = requests.post(token_url,
                             auth=client_auth,
                             data=post_data)
    token_json = response.json()
    return token_json

def store_access_cred(token_response):
    access_token = token_response["access_token"]
    shared_redis.set("MENDELEY_OAUTH2_ACCESS_TOKEN", access_token)
    refresh_token = token_response["refresh_token"]
    shared_redis.set("MENDELEY_OAUTH2_REFRESH_TOKEN", refresh_token)
    return (access_token, refresh_token)

def get_access_token(force_renew=False):
    access_token = shared_redis.get("MENDELEY_OAUTH2_ACCESS_TOKEN")
    if force_renew:
        previous_refresh_token = shared_redis.get("MENDELEY_OAUTH2_REFRESH_TOKEN")
        token_response = renew_token(access_token, previous_refresh_token)
        (access_token, refresh_token) = store_access_cred(token_response)
    elif not access_token:
        token_response = get_token()
        (access_token, refresh_token) = store_access_cred(token_response)
    return access_token


# from http://stackoverflow.com/questions/4293460/how-to-add-custom-parameters-to-an-url-query-string-with-python
def set_query_parameter(url, param_name, param_value):
    """Given a URL, set or replace a query parameter and return the
    modified URL.

    >>> set_query_parameter('http://example.com?foo=bar&biz=baz', 'foo', 'stuff')
    'http://example.com?foo=stuff&biz=baz'

    """
    scheme, netloc, path, query_string, fragment = urlsplit(url)
    query_params = parse_qs(query_string)

    query_params[param_name] = [param_value]
    new_query_string = urlencode(query_params, doseq=True)

    return urlunsplit((scheme, netloc, path, new_query_string, fragment))



class Mendeley(Provider):  

    example_id = ("doi", "10.1371/journal.pcbi.1000361")

    url = "http://www.mendeley.com"
    descr = " A research management tool for desktop and web."
    uuid_from_title_template = 'https://api-oauth2.mendeley.com/oapi/documents/search/"%s"/'
    metrics_from_uuid_template = "https://api-oauth2.mendeley.com/oapi/documents/details/%s"
    metrics_from_doi_template = "https://api-oauth2.mendeley.com/oapi/documents/details/%s?type=doi"
    metrics_from_pmid_template = "https://api-oauth2.mendeley.com/oapi/documents/details/%s?type=pmid"
    metrics_from_arxiv_template = "https://api-oauth2.mendeley.com/oapi/documents/details/%s?type=arxiv"
    aliases_url_template = uuid_from_title_template
    biblio_url_template = metrics_from_doi_template
    doi_url_template = "http://dx.doi.org/%s"

    static_meta_dict = {
        "readers": {
            "display_name": "readers",
            "provider": "Mendeley",
            "provider_url": "http://www.mendeley.com/",
            "description": "The number of readers who have added the article to their libraries",
            "icon": "http://www.mendeley.com/favicon.ico",
        },    
        "groups": {
            "display_name": "groups",
            "provider": "Mendeley",
            "provider_url": "http://www.mendeley.com/",
            "description": "The number of groups who have added the article to their libraries",
            "icon": "http://www.mendeley.com/favicon.ico",
        },
        "discipline": {
            "display_name": "discipline, top 3 percentages",
            "provider": "Mendeley",
            "provider_url": "http://www.mendeley.com/",
            "description": "Percent of readers by discipline, for top three disciplines (csv, api only)",
            "icon": "http://www.mendeley.com/favicon.ico",
        },   
        "career_stage": {
            "display_name": "career stage, top 3 percentages",
            "provider": "Mendeley",
            "provider_url": "http://www.mendeley.com/",
            "description": "Percent of readers by career stage, for top three career stages (csv, api only)",
            "icon": "http://www.mendeley.com/favicon.ico",
        },   
        "country": {
            "display_name": "country, top 3 percentages",
            "provider": "Mendeley",
            "provider_url": "http://www.mendeley.com/",
            "description": "Percent of readers by country, for top three countries (csv, api only)",
            "icon": "http://www.mendeley.com/favicon.ico",
        }
    }


    def __init__(self):
        super(Mendeley, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        # right now restricted to doi because we check the title lookup matches doi
        ## to keep precision high.  Later could experiment with opening this up.
        relevant = (namespace=="doi")
        return(relevant)

    def _extract_metrics(self, page, status_code=200, id=None):
        if not "identifiers" in page:
            raise ProviderContentMalformedError()

        dict_of_keylists = {"mendeley:readers": ["stats", "readers"], 
                            "mendeley:discipline": ["stats", "discipline"],
                            "mendeley:career_stage": ["stats", "status"],
                            "mendeley:country": ["stats", "country"],
                            "mendeley:groups" : ["groups"]}

        metrics_dict = provider._extract_from_json(page, dict_of_keylists)

        # get count of groups
        try:
            metrics_dict["mendeley:groups"] = len(metrics_dict["mendeley:groups"])
        except (TypeError, KeyError):
            # don't add null or zero metrics
            pass

        return metrics_dict


    def _extract_provenance_url(self, page, status_code=200, id=None):
        data = provider._load_json(page)
        try:
            provenance_url = data['mendeley_url']
        except KeyError:
            provenance_url = ""
        return provenance_url        

    @Retry(2, ProviderAuthenticationError, 0.1)
    def _get_page(self, url, cache_enabled=True):
        url_with_access_token = set_query_parameter(url, "access_token", get_access_token())
        response = self.http_get(url_with_access_token, cache_enabled=cache_enabled)
        if response.status_code != 200:
            if response.status_code == 404:
                return None
            elif response.status_code == 401:
                logger.debug(u"got status 401 so going to try renewing access token")
                get_access_token(force_renew=True)
                raise ProviderAuthenticationError("not authenticated")
            else:
                raise(self._get_error(response.status_code, response))
        return response.text
         
    def _get_uuid_lookup_page(self, title, cache_enabled=True):
        uuid_from_title_url = self.uuid_from_title_template % (urllib.quote(title.encode("utf-8")))
        page = self._get_page(uuid_from_title_url, cache_enabled)
        if not page:
            raise ProviderContentMalformedError()            
        if not "documents" in page:
            raise ProviderContentMalformedError()
        return page

    def _get_metrics_lookup_page(self, template, id, cache_enabled=True):
        double_encoded_id = urllib.quote(urllib.quote(id, safe=""), safe="")
        metrics_url = template %(double_encoded_id)
        page = self._get_page(metrics_url, cache_enabled)
        if page:
            if not "identifiers" in page:
                page = None
        return page

    @classmethod
    def remove_punctuation(cls, input):
        # from http://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python
        no_punc = input
        if input:
            no_punc = "".join(e for e in input if (e.isalnum() or e.isspace()))
        return no_punc

    def _get_uuid_from_title(self, aliases_dict, page):
        data = provider._load_json(page)
        try:
            doi = aliases_dict["doi"][0]
        except KeyError:
            doi = None

        try:
            biblio = aliases_dict["biblio"][0]
        except KeyError:
            biblio = None

        for mendeley_record in data["documents"]:
            if doi and (mendeley_record["doi"] == doi):
                uuid = mendeley_record["uuid"]
                return {"uuid": uuid}
            else:
                # more complicated.  Try to match title and year.
                try:
                    mendeley_title = self.remove_punctuation(mendeley_record["title"]).lower()
                    aliases_title = self.remove_punctuation(biblio["title"]).lower()
                except (TypeError, KeyError, AttributeError):
                    logger.warning(u"Mendeley: NO TITLES for aliases, skipping")
                    continue  # nothing to see here.  Skip to next record

                try:
                    if (len(str(biblio["year"])) != 4):
                        logger.warning(u"Mendeley: NO YEAR for aliases, skipping")
                        continue
                except (TypeError, KeyError, AttributeError):
                    logger.warning(u"Mendeley: NO YEAR for aliases, skipping")
                    continue  # nothing to see here.  Skip to next record

                if (mendeley_title == aliases_title):
                    if (str(mendeley_record["year"]) == str(biblio["year"])):

                        # check if author name in common. if not, yell, but continue anyway
                        first_mendeley_surname = mendeley_record["authors"][0]["surname"]
                        has_matching_authors = first_mendeley_surname.lower() in biblio["authors"].lower()
                        if not has_matching_authors:
                            logger.warning(u"Mendeley: NO MATCHING AUTHORS between %s and %s" %(
                                first_mendeley_surname, biblio["authors"]))
                        # but return it anyway
                        response = {}
                        for id_type in ["uuid", "mendeley_url", "doi", "pmid"]:
                            try:
                                if mendeley_record[id_type]:
                                    if id_type == "mendeley_url":
                                        response["url"] = mendeley_record[id_type]
                                    else:
                                        response[id_type] = mendeley_record[id_type]
                            except KeyError:
                                pass
                        return response
                    else:
                        logger.debug(u"Mendeley: years don't match %s and %s" %(
                            str(mendeley_record["year"]), str(biblio["year"])))
                else:
                    logger.debug(u"Mendeley: titles don't match /biblio_print %s and %s" %(
                        self.remove_punctuation(mendeley_record["title"]), self.remove_punctuation(biblio["title"])))
        # no joy
        return None

    def _get_metrics_and_drilldown_from_metrics_page(self, page):
        metrics_dict = self._extract_metrics(page)
        metrics_and_drilldown = {}
        for metric_name in metrics_dict:
            drilldown_url = self._extract_provenance_url(page)
            metrics_and_drilldown[metric_name] = (metrics_dict[metric_name], drilldown_url)
        return metrics_and_drilldown  


    def metrics(self, 
            aliases,
            provider_url_template=None, # ignore this because multiple url steps
            cache_enabled=True):

        # Only lookup metrics for items with appropriate ids
        from totalimpact import item
        aliases_dict = item.alias_dict_from_tuples(aliases)

        metrics_page = None    
        # try lookup by doi
        try:
            metrics_page = self._get_metrics_lookup_page(self.metrics_from_doi_template, aliases_dict["doi"][0], cache_enabled)
        except KeyError:
            pass
        # try lookup by pmid
        if not metrics_page:
            try:
                metrics_page = self._get_metrics_lookup_page(self.metrics_from_pmid_template, aliases_dict["pmid"][0], cache_enabled)
            except KeyError:
                pass
        # try lookup by arxiv
        if not metrics_page:
            try:
                metrics_page = self._get_metrics_lookup_page(self.metrics_from_arxiv_template, aliases_dict["arxiv"][0], cache_enabled)
            except KeyError:
                pass
        # try lookup by title
        if not metrics_page:
            try:
                page = self._get_uuid_lookup_page(aliases_dict["biblio"][0]["title"], cache_enabled)
                if page:
                    uuid = self._get_uuid_from_title(aliases_dict, page)["uuid"]
                    if uuid:
                        logger.debug(u"Mendeley: uuid is %s for %s" %(uuid, aliases_dict["biblio"][0]["title"]))
                        metrics_page = self._get_metrics_lookup_page(self.metrics_from_uuid_template, uuid)
                    else:
                        logger.debug(u"Mendeley: couldn't find uuid for %s" %(aliases_dict["biblio"][0]["title"]))
            except (KeyError, TypeError):
                pass
        # give up!
        if not metrics_page:
            return {}

        metrics_and_drilldown = self._get_metrics_and_drilldown_from_metrics_page(metrics_page)

        return metrics_and_drilldown

    def aliases(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):            

        aliases_dict = provider.alias_dict_from_tuples(aliases)

        if not "biblio" in aliases_dict:
            return []
        if ("doi" in aliases_dict) or ("pmid" in aliases_dict):
            # have better sources, leave them to it.
            return []

        new_aliases = []
        for alias in aliases_dict["biblio"]:
            new_aliases += self._get_aliases_for_id(alias, provider_url_template, cache_enabled)
        
        # get uniques for things that are unhashable
        new_aliases_unique = [k for k,v in itertools.groupby(sorted(new_aliases))]

        return new_aliases_unique

    def _get_aliases_for_id(self, 
            biblio, 
            provider_url_template=None, 
            cache_enabled=True):

        self.logger.debug(u"%s getting aliases for %s" % (self.provider_name, str(biblio)))

        if not provider_url_template:
            provider_url_template = self.aliases_url_template

        page = self._get_uuid_lookup_page(biblio["title"], cache_enabled)

        try:       
            new_aliases = self._extract_aliases(page, biblio)
        except (TypeError, AttributeError):
            self.logger.debug(u"Error.  returning with no new aliases")
            new_aliases = []

        return new_aliases

    def _extract_aliases(self, page, biblio):
        mendeley_ids = self._get_uuid_from_title({"doi":[None], "biblio":[biblio]}, page)
        if mendeley_ids:
            aliases_list = [(namespace, nid) for (namespace, nid) in mendeley_ids.iteritems()]
        else:
            aliases_list = []
        return aliases_list


    def get_biblio_for_id(self, 
            id,
            provider_url_template=None, 
            cache_enabled=True):

        self.logger.debug(u"%s getting biblio for %s" % (self.provider_name, id))

        if not provider_url_template:
            provider_url_template = self.biblio_url_template
        page = self._get_metrics_lookup_page(provider_url_template, id, cache_enabled)
        
        # extract the aliases
        try:
            biblio_dict = self._extract_biblio(page, id)
        except (AttributeError, TypeError):
            biblio_dict = {}

        if biblio_dict and ("is_oa_journal" in biblio_dict) and (biblio_dict["is_oa_journal"]=='True'):
            biblio_dict["free_fulltext_url"] = self.doi_url_template %id
        elif biblio_dict and ("issn" in biblio_dict) and provider.is_issn_in_doaj(biblio_dict["issn"]):
            biblio_dict["free_fulltext_url"] = self.doi_url_template %id

        return biblio_dict


    def _extract_biblio(self, page, id=None):
        biblio_dict = {}

        dict_of_keylists = {
            'issn' : ['identifiers', 'issn'],
            'oai_id' : ['identifiers', 'oai_id']
        }
        biblio_dict = provider._extract_from_json(page, dict_of_keylists, include_falses=False)
        if "issn" in biblio_dict:
            biblio_dict["issn"] = biblio_dict["issn"].replace("-", "")

        dict_of_keylists = {
            'is_oa_journal' : ['oa_journal']
        }
        biblio_dict.update(provider._extract_from_json(page, dict_of_keylists, include_falses=True))
        if biblio_dict and "is_oa_journal" in biblio_dict:
            biblio_dict["is_oa_journal"] = str(biblio_dict["is_oa_journal"]) # cast boolean to string
        return biblio_dict 


########NEW FILE########
__FILENAME__ = orcid
import re, json
from totalimpact.providers import bibtex
from totalimpact.providers import provider
from totalimpact.providers import crossref
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderItemNotFoundError

import logging
logger = logging.getLogger('ti.providers.orcid')

class Orcid(Provider):  
    descr = "Connecting research and researchers"
    url = "http://www.orcid.org"
    member_items_url_template = "http://pub.orcid.org/%s/orcid-works"
        
    def __init__(self):
        self.bibtex_parser = bibtex.Bibtex()
        super(Orcid, self).__init__()
        
    def _parse_orcid_work(self, work):
        if not work:
            return {}

        biblio = {}
        logger.debug(u"%20s parsing orcid work" % (self.provider_name))

        try:
            if work["work-citation"]["work-citation-type"].lower()=="bibtex":
                biblio = self.bibtex_parser.parse(work["work-citation"]["citation"])[0]
        except (KeyError, TypeError, IndexError):
            logger.debug(u"%20s error getting work citation type" % (self.provider_name))
            pass

        try:
            biblio["year"] = work["publication-date"]["year"]["value"]
            biblio["year"] = re.sub("\D", "", biblio["year"])           
        except (KeyError, TypeError, IndexError):
            biblio["year"]  = ""

        try:
            biblio["title"] = work["work-title"]["title"]["value"]
        except (KeyError, TypeError, IndexError):
            biblio["title"]  = ""

        try:
            biblio["journal"] = work["work-title"]["subtitle"]["value"]
        except (KeyError, TypeError, IndexError):
            biblio["journal"]  = ""

        try:
            biblio["url"] = work["url"]["value"]
            if biblio["url"].startswith("http://www.scopus.com/inward"):
                del biblio["url"]
        except (KeyError, TypeError, IndexError):
            if "url" in biblio:
                del biblio["genre"]

        if not "authors" in biblio:
            biblio["authors"]  = ""

        try:
            if work["work-external-identifiers"]["work-external-identifier"][0]['work-external-identifier-type'] == "ISBN":
                biblio["isbn"] = work["work-external-identifiers"]["work-external-identifier"][0]["work-external-identifier-id"]['value']
        except (KeyError, TypeError, IndexError):
            pass

        try:
            biblio["genre"] = work["work-type"].lower().replace("_", " ")
            if biblio["genre"] == "undefined":
                del biblio["genre"]
        except (KeyError, TypeError, IndexError):
            pass

        try:
            biblio["full_citation"] = work["work-citation"]["citation"]
            biblio["full_citation_type"] = work["work-citation"]["work-citation-type"].lower()
        except (KeyError, TypeError, IndexError):
            pass

        return biblio

    def _extract_members(self, page, query_string=None):
        if 'orcid-profile' not in page:
            raise ProviderContentMalformedError("Content does not contain expected text")

        data = provider._load_json(page)
        members = []
        try:
            orcid_works = data["orcid-profile"]["orcid-activities"]["orcid-works"]["orcid-work"]
        except KeyError:
            return []

        for work in orcid_works:
            new_member = None
            try:
                ids = work["work-external-identifiers"]["work-external-identifier"]

                for myid in ids:
                    if myid['work-external-identifier-type'] == "DOI":
                        doi = myid['work-external-identifier-id']['value']
                        doi = crossref.clean_doi(doi)
                        if doi:
                            new_member = ("doi", doi)
                    if myid['work-external-identifier-type'] == "PMID":
                        new_member = ("pmid", myid['work-external-identifier-id']['value'])
            except KeyError:
                pass

            if not new_member:
                logger.info(u"no external identifiers, try saving whole citation for {orcid}".format(
                    orcid=query_string))
                biblio = self._parse_orcid_work(work)
                new_member = ("biblio", biblio)

            if new_member:
                members += [new_member]    

        if not members:
            raise ProviderItemNotFoundError

        logger.info(u"returning {n} members for {orcid}".format(
            n=len(members), orcid=query_string))

        return(members)

    def member_items(self, 
            query_string, 
            provider_url_template=None, 
            cache_enabled=True):

        logger.debug(u"%20s getting member_items for %s" % (self.provider_name, query_string))

        if not provider_url_template:
            provider_url_template = self.member_items_url_template

        query_string = query_string.replace("http://orcid.org/", "")
        url = self._get_templated_url(provider_url_template, query_string, "members")
        headers = {}
        headers["accept"] = "application/json"

        # try to get a response from the data provider  
        # cache FALSE for now because people probably changing ORCIDs a lot
        response = self.http_get(url, headers=headers, cache_enabled=False) 

        if response.status_code != 200:
            self.logger.info(u"%s status_code=%i" 
                % (self.provider_name, response.status_code))            
            if response.status_code == 404:
                raise ProviderItemNotFoundError
            elif response.status_code == 303: #redirect
                pass                
            else:
                self._get_error(response.status_code, response)

        # extract the member ids
        try:
            members = self._extract_members(response.text, query_string)
        except (AttributeError, TypeError):
            members = []

        return(members)

########NEW FILE########
__FILENAME__ = plosalm
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import simplejson, os, re, urllib

import logging
logger = logging.getLogger('ti.providers.plosalm')

class Plosalm(Provider):  

    example_id = ("doi", "10.1371/journal.pcbi.1000361")

    url = "http://www.plos.org/"
    descr = "PLOS article level metrics."
    metrics_url_template = "http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key=" + os.environ["PLOS_KEY_V3"]
    provenance_url_template = "http://dx.doi.org/%s"

    PLOS_ICON = "http://www.plos.org/wp-content/themes/plos_new/favicon.ico"

    static_meta_dict =  {
        "html_views": {
            "display_name": "html views",
            "provider": "PLOS",
            "provider_url": "http://www.plos.org/",
            "description": "the number of views of the HTML article on PLOS",
            "icon": PLOS_ICON,
        },    
        "pdf_views": {
            "display_name": "pdf views",
            "provider": "PLOS",
            "provider_url": "http://www.plos.org/",
            "description": "the number of downloads of the PDF from PLOS",
            "icon": PLOS_ICON,
        }  
    }
    

    def __init__(self):
        super(Plosalm, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        relevant = (("doi" == namespace) and ("10.1371/" in nid))
        return(relevant)

    def _extract_metrics(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not "sources" in page:
            raise ProviderContentMalformedError

        json_response = provider._load_json(page)
        this_article = json_response[0]["sources"][0]["metrics"]

        dict_of_keylists = {
            'plosalm:html_views' : ['html'],
            'plosalm:pdf_views' : ['pdf']
        }

        metrics_dict = provider._extract_from_data_dict(this_article, dict_of_keylists)

        return metrics_dict



########NEW FILE########
__FILENAME__ = plossearch
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import simplejson, re, os, urllib

import logging
logger = logging.getLogger('ti.providers.plossearch')

class Plossearch(Provider):  

    example_id = ("doi", "10.5061/dryad.c2b53")

    url = "http://www.plos.org/"
    descr = "PLoS article level metrics."
    metrics_url_template = 'http://api.plos.org/search?q="%s"&api_key=' + os.environ["PLOS_KEY_V3"]
    provenance_url_template = 'http://www.plosone.org/search/advanced?queryTerm=&unformattedQuery=everything:"%s"'

    static_meta_dict =  {
        "mentions": {
            "display_name": "mentions",
            "provider": "PLOS",
            "provider_url": "http://www.plos.org/",
            "description": "the number of times the research product was mentioned in the full-text of PLOS papers",
            "icon": "http://www.plos.org/wp-content/themes/plos_new/favicon.ico" ,
        }
    } 

    def __init__(self):
        super(Plossearch, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        if namespace in ["url", "doi"]:
            for host in ["dryad", "figshare", "github", "youtube", "vimeo", "arxiv"]:
                if host in nid:
                    return True
        return False

    def get_best_id(self, aliases):
        return self.get_relevant_alias_with_most_metrics("plossearch:mentions", aliases)

    def _extract_metrics(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not "<response>" in page:
            raise ProviderContentMalformedError

        count = provider._count_in_xml(page, 'doc')
        if count:
            metrics_dict = {'plossearch:mentions': count}
        else:
            metrics_dict = {}

        return metrics_dict

    # need to override to url encode for metrics
    def _get_templated_url(self, template, id, method=None):
        id = re.sub('^http(s?)://', '', id)
        try:
            id = urllib.quote(id, safe="")
        except KeyError:  # thrown if bad characters
            pass
        url = template % id
        return(url)


########NEW FILE########
__FILENAME__ = pmc
import hashlib, simplejson, os, collections

from totalimpact import db
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact.provider_batch_data import ProviderBatchData

import logging
logger = logging.getLogger('ti.providers.pmc')

batch_data = None

class Pmc(Provider):  

    example_id = ("pmid", "23066504")
    provenance_url_template = None
    url = "http://www.ncbi.nlm.nih.gov/pmc"
    descr = "a free archive of biomedical and life sciences journal literature at the NIH/NLM"
    static_meta_dict = {
        "pdf_downloads": {
            "display_name": "PDF downloads",
            "provider": "PMC",
            "provider_url": "http://www.ncbi.nlm.nih.gov/pmc",
            "description": "Number of times the PDF has been downloaded from PMC",
            "icon": "http://www.ncbi.nlm.nih.gov/favicon.ico"
        },
        "abstract_views": {
            "display_name": "abstract views",
            "provider": "PMC",
            "provider_url": "http://www.ncbi.nlm.nih.gov/pmc",
            "description": "Number of times the abstract has been viewed on PMC",
            "icon": "http://www.ncbi.nlm.nih.gov/favicon.ico"
        },        
        "fulltext_views": {
            "display_name": "fulltext views",
            "provider": "PMC",
            "provider_url": "http://www.ncbi.nlm.nih.gov/pmc",
            "description": "Number of times the full text has been viewed on PMC",
            "icon": "http://www.ncbi.nlm.nih.gov/favicon.ico"
        },        
        "unique_ip_views": {
            "display_name": "unique IP views",
            "provider": "PMC",
            "provider_url": "http://www.ncbi.nlm.nih.gov/pmc",
            "description": "Number of unique IP addresses that have viewed this on PMC",
            "icon": "http://www.ncbi.nlm.nih.gov/favicon.ico"
        },         
        "figure_views": {
            "display_name": "figure views",
            "provider": "PMC",
            "provider_url": "http://www.ncbi.nlm.nih.gov/pmc",
            "description": "Number of times the figures have been viewed on PMC",
            "icon": "http://www.ncbi.nlm.nih.gov/favicon.ico"
        },        
        "suppdata_views": {
            "display_name": "suppdata views",
            "provider": "PMC",
            "provider_url": "http://www.ncbi.nlm.nih.gov/pmc",
            "description": "Number of times the supplementary data has been viewed on PMC",
            "icon": "http://www.ncbi.nlm.nih.gov/favicon.ico"
        }               
    }

    def __init__(self):
        super(Pmc, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return("pmid" == namespace)

    def build_batch_data_dict(self):
        logger.info(u"Building batch data for PMC")
        batch_data = collections.defaultdict(list)

        matches = ProviderBatchData.query.filter_by(provider="pmc").all()
        for provider_batch_data_obj in matches:
            for nid in provider_batch_data_obj.aliases["pmid"]:
                pmid_alias = ("pmid", nid)
                batch_data[pmid_alias] += [{"raw": provider_batch_data_obj.raw, 
                                            "max_event_date":provider_batch_data_obj.max_event_date}]

        logger.info(u"Finished building batch data for PMC: {n} rows".format(n=len(batch_data)))

        return batch_data

    def has_applicable_batch_data(self, namespace, nid):
        has_applicable_batch_data = False

        matches = ProviderBatchData.query.filter_by(provider="pmc").all()
        for provider_batch_data_obj in matches:
            if nid in provider_batch_data_obj.aliases[namespace]:
                has_applicable_batch_data = True

        return has_applicable_batch_data

    def _extract_metrics(self, page, status_code=200, id=None): 
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if "<pmc-web-stat>" not in page:
            raise ProviderContentMalformedError

        (doc, lookup_function) = provider._get_doc_from_xml(page)
        if not doc:
            return {}
        try:
            articles = doc.getElementsByTagName("article")
            for article in articles:
                print article
                metrics_dict = {}            
                meta_data = article.getElementsByTagName("meta-data")[0]
                pmid = meta_data.getAttribute("pubmed-id")
                if id == pmid:
                    metrics = article.getElementsByTagName("usage")[0]
                    
                    pdf_downloads = int(metrics.getAttribute("pdf"))
                    if pdf_downloads:
                        metrics_dict.update({'pmc:pdf_downloads': pdf_downloads})

                    abstract_views = int(metrics.getAttribute("abstract"))
                    if abstract_views:
                        metrics_dict.update({'pmc:abstract_views': abstract_views})

                    fulltext_views = int(metrics.getAttribute("full-text"))
                    if fulltext_views:
                        metrics_dict.update({'pmc:fulltext_views': fulltext_views})

                    unique_ip_views = int(metrics.getAttribute("unique-ip"))
                    if unique_ip_views:
                        metrics_dict.update({'pmc:unique_ip_views': unique_ip_views})

                    figure_views = int(metrics.getAttribute("figure"))
                    if figure_views:
                        metrics_dict.update({'pmc:figure_views': figure_views})

                    suppdata_views = int(metrics.getAttribute("supp-data"))
                    if suppdata_views:
                        metrics_dict.update({'pmc:suppdata_views': suppdata_views})

                    return metrics_dict

        except (KeyError, IndexError, TypeError):
            pass

        return {}

    def _get_metrics_and_drilldown(self, pages, pmid):
        metrics_dict = {}
        for page in pages:
            one_month_metrics_dict = self._extract_metrics(page, id=pmid)
            print one_month_metrics_dict
            for metric in one_month_metrics_dict:
                try:
                    metrics_dict[metric] += one_month_metrics_dict[metric]
                except KeyError:
                    metrics_dict[metric] = one_month_metrics_dict[metric]
        metrics_and_drilldown = {}
        for metric_name in metrics_dict:
            drilldown_url = ""
            metrics_and_drilldown[metric_name] = (metrics_dict[metric_name], drilldown_url)
        return metrics_and_drilldown

    def metrics(self, 
            aliases,
            provider_url_template=None, # ignore this because multiple url steps
            cache_enabled=True):

        # if haven't loaded batch_data, return no metrics
        global batch_data
        if not batch_data:
            batch_data = self.build_batch_data_dict()
            pass

        metrics_and_drilldown = {}

        # Only lookup metrics for items with appropriate ids
        from totalimpact import item
        aliases_dict = item.alias_dict_from_tuples(aliases)
        try:
            pmid = aliases_dict["pmid"][0]
        except KeyError:
            return {}
            
        pmid_alias = ("pmid", pmid)
        page = ""

        if pmid_alias in batch_data:
            pages = [page["raw"] for page in batch_data[pmid_alias]]
        if page:
            metrics_and_drilldown = self._get_metrics_and_drilldown(pages, pmid)

        return metrics_and_drilldown



########NEW FILE########
__FILENAME__ = provider
 # -*- coding: utf-8 -*-  # need this line because test utf-8 strings later

from totalimpact import cache as cache_module
from totalimpact import providers
from totalimpact import default_settings
from totalimpact import utils
from totalimpact import app
from totalimpact import db
from totalimpact.unicode_helpers import remove_nonprinting_characters

import requests, os, time, threading, sys, traceback, importlib, urllib, logging, itertools
import simplejson
import BeautifulSoup
import socket
import analytics
import re
from xml.dom import minidom 
from xml.parsers.expat import ExpatError
from sqlalchemy.sql import text    

logger = logging.getLogger("ti.provider")

# Requests' logging is too noisy
requests_log = logging.getLogger("requests").setLevel(logging.WARNING) 


class CachedResponse:
    def __init__(self, cache_data):
        self.status_code = cache_data['status_code']
        self.url = cache_data['url']
        self.text = cache_data['text']

def get_page_from_cache(url, headers, allow_redirects, cache):
    cache_key = headers.copy()
    cache_key.update({"url":url, "allow_redirects":allow_redirects})

    cache_data = cache.get_cache_entry(cache_key)
    # use it if it was a 200, otherwise go get it again
    if cache_data and (cache_data['status_code'] == 200):
        logger.debug(u"returning from cache: %s" %(url))
        return CachedResponse(cache_data)
    return None

def store_page_in_cache(url, headers, allow_redirects, response, cache):
    cache_key = headers.copy()
    cache_key.update({"url":url, "allow_redirects":allow_redirects})

    cache_data = {
        'text':             response.text, 
        'status_code':      response.status_code, 
        'url':              response.url}
    cache.set_cache_entry(cache_key, cache_data)

def is_doi(nid):
    nid = nid.lower()
    if nid.startswith("doi:") or nid.startswith("10.") or "doi.org/" in nid:
        return True
    return False

def is_pmid(nid):
    if nid.startswith("pmid") or (len(nid)>2 and len(nid)<=8 and re.search("\d+", nid)):
        return True
    return False

def is_url(nid):
    if nid.lower().startswith("http://") or nid.lower().startswith("https://"):
        return True
    return False

def is_arxiv(nid):
    if nid.lower().startswith("arxiv:") or "arxiv.org/" in nid:
        return True
    return False

def normalize_alias(alias):
    (ns, nid) = alias
    if ns == "biblio":
        return (ns, nid)

    nid = remove_nonprinting_characters(nid)
    nid = nid.strip()  # also remove spaces
    if is_doi(nid):
        nid = providers.crossref.clean_doi(nid)
    elif is_pmid(nid):
        nid = providers.pubmed.clean_pmid(nid)
    elif is_arxiv(nid):
        nid = providers.arxiv.clean_arxiv_id(nid)
    elif is_url(nid):
        nid = providers.webpage.clean_url(nid)

    return (ns, nid)


def get_aliases_from_product_id_strings(product_id_strings):
    aliases = []
    for nid in product_id_strings:
        nid = remove_nonprinting_characters(nid)
        nid = nid.strip()  # also remove spaces
        if is_doi(nid):
            aliases += providers.crossref.Crossref().member_items(nid)
        elif is_pmid(nid):
            aliases += providers.pubmed.Pubmed().member_items(nid)
        elif is_arxiv(nid):
            aliases += providers.arxiv.Arxiv().member_items(nid)
        elif is_url(nid):
            aliases += providers.webpage.Webpage().member_items(nid)
    print aliases
    return aliases


def import_products(provider_name, import_input):
    if provider_name in ["bibtex", "product_id_strings"]:
        logger.debug(u"in import_products with {provider_name}".format(
            provider_name=provider_name))
    else:
        logger.debug(u"in import_products with {provider_name}: {import_input}".format(
            provider_name=provider_name, import_input=import_input))

    aliases = []

    # pull in standard items, if we were passed any of these
    if provider_name=="product_id_strings":
        aliases = get_aliases_from_product_id_strings(import_input["product_id_strings"])
    elif provider_name=="bibtex":
        provider = ProviderFactory.get_provider("bibtex")
        aliases = provider.member_items(import_input["bibtex"])
    else:
        try:
            provider = ProviderFactory.get_provider(provider_name)
            aliases = provider.member_items(import_input["account_name"])
        except ImportError:
            pass

    return(aliases)


def is_issn_in_doaj(issn):
    raw_sql = text("""SELECT issn from doaj_issn_lookup where issn=:issn""")
    result = db.session.execute(raw_sql, params={
         "issn": issn
        })
    first_result = result.first()
    is_in_doaj = first_result != None
    db.session.remove()
    return is_in_doaj

class ProviderFactory(object):

    @classmethod
    def get_provider(cls, provider_name):
        provider_module = importlib.import_module('totalimpact.providers.'+provider_name)
        provider = getattr(provider_module, provider_name.title())
        instance = provider()
        return instance

    @classmethod
    def get_providers(cls, config_providers, filter_by=None):
        """ config is the application configuration """
        providers = []
        for provider_name, v in config_providers:
            try:
                prov = ProviderFactory.get_provider(provider_name)
                prov.provider_name = provider_name
                providers.append(prov)

                if filter_by is not None:
                    if not getattr(prov, "provides_"+filter_by):
                        providers.pop()

            except ProviderConfigurationError:
                logger.error(u"Unable to configure provider ... skipping " + str(v))
        return providers

    @classmethod
    def providers_with_metrics(cls, config_providers):
        providers = cls.get_providers(config_providers)
        providers_with_metrics = []
        for provider in providers:
            if provider.provides_metrics:
                providers_with_metrics += [provider.provider_name]
        return providers_with_metrics

    @classmethod
    def get_all_static_meta(cls, config_providers=default_settings.PROVIDERS):
        # this is now duplicating get_all_metadata below; not high refactoring priority, though.
        all_static_meta = {}
        providers = cls.get_providers(config_providers)
        for provider in providers:
            if provider.provides_metrics:
                for metric_name in provider.static_meta_dict:
                    full_metric_name = provider.provider_name + ":" + metric_name
                    all_static_meta[full_metric_name] = provider.static_meta_dict[metric_name]
        return(all_static_meta)

    @classmethod
    def get_all_metric_names(cls, config_providers=default_settings.PROVIDERS):
        all_static_meta = cls.get_all_static_meta(config_providers)
        metric_names = all_static_meta.keys()
        return(metric_names)

    @classmethod
    def get_all_metadata(cls, config_providers=default_settings.PROVIDERS):
        ret = {}
        providers = cls.get_providers(config_providers)
        for provider in providers:
            provider_data = {}
            provider_data["provides_metrics"] = provider.provides_metrics
            provider_data["provides_aliases"] = provider.provides_aliases

            try:
                provider_data["url"] = provider.url
            except AttributeError:
                pass

            try:
                provider_data["descr"] = provider.descr
            except AttributeError:
                pass

            try:
                provider_data["metrics"] = provider.static_meta_dict
            except AttributeError:
                pass

            provider_name = provider.__class__.__name__.lower()

            ret[provider_name] = provider_data

        return ret


        
class Provider(object):

    def __init__(self, 
            max_cache_duration=60*60,  # one hour 
            max_retries=0, 
            tool_email="mytotalimpact@gmail.com"): 
        # FIXME change email to team@impactstory.org after registering it with crossref
    
        self.max_cache_duration = max_cache_duration
        self.max_retries = max_retries
        self.tool_email = tool_email
        self.provider_name = self.__class__.__name__.lower()
        self.max_simultaneous_requests = 20  # max simultaneous requests, used by backend        
        self.logger = logging.getLogger("ti.providers." + self.provider_name)

    def __repr__(self):
        return "Provider(%s)" % self.provider_name
    
    # API Methods
    # These should be filled in by each Provider implementing this signature

    # default method; providers can override
    def _get_error(self, status_code, response=None):
        try:
            headers = response.headers
        except AttributeError:
            headers = {}
        try:
            text = response.text
        except (AttributeError, TypeError):
            text = ""       

        if response:
            url = response.url
        else:
            url = None

        # analytics.track("CORE", "Received error response from Provider", {
        #     "provider": self.provider_name, 
        #     "url": url,
        #     "text": text,
        #     "status_code": status_code
        #     })

        if status_code >= 500:
            error = ProviderServerError(response)
            self.logger.info(u"%s ProviderServerError status code=%i, %s, %s" 
                % (self.provider_name, status_code, text, str(headers)))
        else:
            error = ProviderClientError(response)
            self.logger.info(u"%s ProviderClientError status code=%i, %s, %s" 
                % (self.provider_name, status_code, text, str(headers)))

        raise(error)
        return error

    def _get_templated_url(self, template, id, method=None):
        try:
            id_unicode = unicode(id, "UTF-8")
        except TypeError:
            id_unicode = id
        id_utf8 = id_unicode.encode("UTF-8")

        substitute_id = id_utf8
        if template != "%s":
           substitute_id = urllib.quote(id_utf8)

        url = template % substitute_id
        return(url)

    def relevant_aliases(self, aliases):
        filtered = [alias for alias in aliases 
                        if self.is_relevant_alias(alias)]
        #self.logger.debug(u"%s relevant_aliases are %s given %s" % (self.provider_name, str(filtered), str(aliases)))

        return filtered

    def get_best_id(self, aliases):
        filtered = self.relevant_aliases(aliases)
        if filtered:
            alias = filtered[0]
            (namespace, nid) = alias
        else:
            nid = None
        return(nid)

    @property
    def provides_members(self):
         return ("_extract_members" in dir(self))

    @property
    def provides_aliases(self):
         return ("_extract_aliases" in dir(self))

    @property
    def provides_biblio(self):
         return ("_extract_biblio" in dir(self))

    @property
    def provides_metrics(self):
         return ("_extract_metrics" in dir(self))

    @property
    def provides_static_meta(self):
         return ("static_meta_dict" in dir(self))

    # default method; providers that use analytics credentials should override
    def uses_analytics_credentials(self, method_name):
         return False

    # default method; providers can override    
    def metric_names(self):
        try:
            metric_names = self.static_meta_dict.keys()
        except AttributeError:
            metric_names = []
        return(metric_names)

    # default method; providers can override    
    def static_meta(self, metric_name):
        if not self.provides_static_meta:
            raise NotImplementedError()

        return(self.static_meta_dict[metric_name])
        

    # default method; providers can override    
    def provenance_url(self, metric_name, aliases):
        # Returns the same provenance url for all metrics
        id = self.get_best_id(aliases)

        if id:
            provenance_url = self._get_templated_url(self.provenance_url_template, id, "provenance")
        else:
            provenance_url = None

        return provenance_url

    # default method; providers can override
    def member_items(self, 
            query_string, 
            provider_url_template=None, 
            cache_enabled=True):

        if not self.provides_members:
            raise NotImplementedError()

        self.logger.debug(u"%s getting member_items for %s" % (self.provider_name, query_string))

        if not provider_url_template:
            provider_url_template = self.member_items_url_template

        url = self._get_templated_url(provider_url_template, query_string, "members")
        if not url:
            return []

        # try to get a response from the data provider  
        response = self.http_get(url, cache_enabled=cache_enabled)

        if response.status_code != 200:
            self.logger.info(u"%s status_code=%i" 
                % (self.provider_name, response.status_code))            
            if response.status_code == 404:
                raise ProviderItemNotFoundError
            elif response.status_code == 303: #redirect
                pass                
            else:
                self._get_error(response.status_code, response)
        page = response.text

        # extract the member ids
        try:
            members = self._extract_members(page, query_string)
        except (AttributeError, TypeError):
            members = []

        return(members)

    # default method; providers can override
    def biblio(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True):

        if not self.provides_biblio:
            raise NotImplementedError()

        id = self.get_best_id(aliases)

        # Only lookup biblio for items with appropriate ids
        if not id:
            #self.logger.debug(u"%s not checking biblio, no relevant alias" % (self.provider_name))
            return None

        if not provider_url_template:
            provider_url_template = self.biblio_url_template

        return self.get_biblio_for_id(id, provider_url_template, cache_enabled)


    # default method; providers can override
    def get_biblio_for_id(self, 
            id,
            provider_url_template=None, 
            cache_enabled=True):

        if not self.provides_biblio:
            return {}

        self.logger.debug(u"%s getting biblio for %s" % (self.provider_name, id))

        if not provider_url_template:
            provider_url_template = self.biblio_url_template
        url = self._get_templated_url(provider_url_template, id, "biblio")

        # try to get a response from the data provider        
        response = self.http_get(url, cache_enabled=cache_enabled)

        if response.status_code != 200:
            self.logger.info(u"%s status_code=%i" 
                % (self.provider_name, response.status_code))            
            if response.status_code == 404: #not found
                return {}
            elif response.status_code == 403: #forbidden
                return {}
            elif ((response.status_code >= 300) and (response.status_code < 400)): #redirect
                return {}
            else:
                self._get_error(response.status_code, response)
        
        # extract the aliases
        try:
            biblio_dict = self._extract_biblio(response.text, id)
        except (AttributeError, TypeError):
            biblio_dict = {}

        return biblio_dict

    # default method; providers can override
    def aliases(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):            

        if not self.provides_aliases:
            #raise NotImplementedError()
            return []

        # Get a list of relevant aliases
        relevant_aliases = self.relevant_aliases(aliases)

        if not relevant_aliases:
            #self.logger.debug(u"%s not checking aliases, no relevant alias" % (self.provider_name))
            return []

        new_aliases = []
        for alias in relevant_aliases:
            (namespace, nid) = alias
            new_aliases += self._get_aliases_for_id(nid, provider_url_template, cache_enabled)
        
        # get uniques for things that are unhashable
        new_aliases_unique = [k for k,v in itertools.groupby(sorted(new_aliases))]

        return new_aliases_unique

    # default method; providers can override
    def _get_aliases_for_id(self, 
            id, 
            provider_url_template=None, 
            cache_enabled=True):

        if not self.provides_aliases:
            return []

        self.logger.debug(u"%s getting aliases for %s" % (self.provider_name, id))

        if not provider_url_template:
            provider_url_template = self.aliases_url_template
        url = self._get_templated_url(provider_url_template, id, "aliases")

        # try to get a response from the data provider                
        response = self.http_get(url, cache_enabled=cache_enabled)
        
        if response.status_code != 200:
            self.logger.info(u"%s status_code=%i" 
                % (self.provider_name, response.status_code))            
            if response.status_code == 404:
                return []
            elif response.status_code == 403:  #forbidden
                return []
            elif response.status_code == 303: #redirect
                pass                
            else:
                self._get_error(response.status_code, response)

        try:       
            new_aliases = self._extract_aliases(response.text, id)
        except (TypeError, AttributeError):
            new_aliases = []

        return new_aliases


    # default method; providers can override
    def metrics(self, 
            aliases,
            provider_url_template=None, 
            cache_enabled=True):

        if not self.provides_metrics:
            raise NotImplementedError()

        id = self.get_best_id(aliases)

        # Only lookup metrics for items with appropriate ids
        if not id:
            #self.logger.debug(u"%s not checking metrics, no relevant alias" % (self.provider_name))
            return {}

        if not provider_url_template:
            provider_url_template = self.metrics_url_template

        metrics = self.get_metrics_for_id(id, provider_url_template, cache_enabled)
        metrics_and_drilldown = {}
        for metric_name in metrics:
            drilldown_url = self.provenance_url(metric_name, aliases)
            metrics_and_drilldown[metric_name] = (metrics[metric_name], drilldown_url)

        return metrics_and_drilldown  


    # default method; providers can override
    def get_metrics_for_id(self, 
            id, 
            provider_url_template=None, 
            cache_enabled=True, 
            url_override=None,
            extract_metrics_method=None):

        if not self.provides_metrics:
            return {}

        if not extract_metrics_method:
            extract_metrics_method = self._extract_metrics

        # self.logger.debug(u"%s getting metrics for %s" % (self.provider_name, id))

        if url_override:
            url = url_override
        else:
            if not provider_url_template:
                provider_url_template = self.metrics_url_template
            url = self._get_templated_url(provider_url_template, id, "metrics")

        if not url:
            return {}

        # try to get a response from the data provider
        response = self.http_get(url, cache_enabled=cache_enabled, allow_redirects=True)

        #self.logger.debug(u"%s get_metrics_for_id response.status_code %i" % (self.provider_name, response.status_code))
        
        # extract the metrics
        try:
            metrics_dict = extract_metrics_method(response.text, response.status_code, id=id)
        except (requests.exceptions.Timeout, socket.timeout) as e:  # can apparently be thrown here
            self.logger.info(u"%s Provider timed out *after* GET in socket" %(self.provider_name))        
            raise ProviderTimeout("Provider timed out *after* GET in socket", e)        
        except (AttributeError, TypeError):  # throws type error if response.text is none
            metrics_dict = {}

        return metrics_dict

    # ideally would aggregate all tweets from all urls.  
    # the problem is this requires multiple drill-down links, which is troubling for UI at the moment
    # for now, look up all the alias urls and use metrics for url that is most tweeted
    def get_relevant_alias_with_most_metrics(self, metric_name, aliases, provider_url_template=None, cache_enabled=None):
        url_with_biggest_so_far = None
        biggest_so_far = 0
        url_aliases = []

        # also try adding a trailing slash to all of them, and a non trailing slash
        for (namespace, url) in self.relevant_aliases(aliases):
            if url.endswith("/"):
                url_aliases += [("url", url), ("url", re.sub("\/$", "", url))]
            else: 
                url_aliases += [("url", url), ("url", url+u"/")]

        for url_alias in url_aliases:
            (namespace, url) = url_alias
            metrics = self.get_metrics_for_id(url, provider_url_template, cache_enabled)
            if metric_name in metrics:
                if (metrics[metric_name] > biggest_so_far):
                    logger.debug(u"{new_url} has higher metrics than {prev_highest}".format(
                        new_url=url, prev_highest=url_with_biggest_so_far))
                    url_with_biggest_so_far = url
                    biggest_so_far = metrics[metric_name]
        return(url_with_biggest_so_far)
       

    # Core methods
    # These should be consistent for all providers
    
    def http_get(self, url, headers={}, timeout=20, cache_enabled=True, allow_redirects=False):
        """ Returns a requests.models.Response object or raises exception
            on failure. Will cache requests to the same URL. """

        headers["User-Agent"] = app.config["USER_AGENT"]

        # use the cache if the config parameter is set and the arg allows it
        use_cache = app.config["CACHE_ENABLED"] and cache_enabled
        if use_cache:
            cache = cache_module.Cache(self.max_cache_duration)
            cached_response = get_page_from_cache(url, headers, allow_redirects, cache)
            if cached_response:
                return cached_response
            
        try:
            # analytics.track("CORE", "Sent GET to Provider", {"provider": self.provider_name, "url": url}, 
            #     context={ "providers": { 'Mixpanel': False } })
            try:
                self.logger.info(u"{provider_name} LIVE GET on {url}".format(
                    provider_name=self.provider_name, url=url))
            except UnicodeDecodeError:
                self.logger.info(u"{provider_name} LIVE GET on an url that throws UnicodeDecodeError".format(
                    provider_name=self.provider_name))

            r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=allow_redirects, verify=False)
            if r and not r.encoding:
                r.encoding = "utf-8"     
            if r and use_cache:
                store_page_in_cache(url, headers, allow_redirects, r, cache)

        except (requests.exceptions.Timeout, socket.timeout) as e:
            self.logger.info(u"{provider_name} provider timed out on GET on {url}".format(
                provider_name=self.provider_name, url=url))
            # analytics.track("CORE", "Received no response from Provider (timeout)", 
            #     {"provider": self.provider_name, "url": url})
            raise ProviderTimeout("Provider timed out during GET on " + url, e)

        except requests.exceptions.RequestException as e:
            self.logger.info(u"{provider_name} RequestException on GET on {url}".format(
                provider_name=self.provider_name, url=url))
            # analytics.track("CORE", "Received RequestException from Provider", 
            #     {"provider": self.provider_name, "url": url})
            raise ProviderHttpError("RequestException during GET on: " + url, e)

        return r


    def http_get_multiple(self, urls, headers={}, timeout=20, cache_enabled=True, allow_redirects=False, num_concurrent_requests=False):
        """ Returns a requests.models.Response object or raises exception
            on failure. Will cache requests to the same URL. """

        headers["User-Agent"] = app.config["USER_AGENT"]

        # use the cache if the config parameter is set and the arg allows it
        use_cache = app.config["CACHE_ENABLED"] and cache_enabled
        if use_cache:
            cache = cache_module.Cache(self.max_cache_duration)

        responses = {}
        for url in urls:
            responses[url] = None
            if use_cache:
                cached_response = get_page_from_cache(url, headers, allow_redirects, cache)
                if cached_response:
                    responses[url] = cached_response

        uncached_urls = [url for url in responses if not responses[url]]

        # replace the loop below with requests made in parallel, ideally!
        fresh_responses = []
        fresh_responses_dict = {}
        for u in uncached_urls:
            fresh_responses += [self.http_get(u, headers=headers, timeout=timeout, allow_redirects=allow_redirects)]

        if fresh_responses:
            fresh_responses_dict = dict(zip(uncached_urls, fresh_responses))
            if use_cache:
                for url in fresh_responses_dict:
                    r = fresh_responses_dict[url]
                    if r and not r.encoding:
                        r.encoding = "utf-8"     
                    store_page_in_cache(url, headers, allow_redirects, r, cache)
        responses.update(fresh_responses_dict)
        return responses


class ProviderError(Exception):
    def __init__(self, message="", inner=None):
        self._message = message  # naming it self.message raises DepreciationWarning
        self.inner = inner
        # NOTE: experimental
        self.stack = traceback.format_stack()[:-1]
        
    # DeprecationWarning: BaseException.message has been deprecated 
    #   as of Python 2.6 so implement property here
    @property
    def message(self): 
        return (self._message)

    def log(self):
        msg = " " + self.message + " " if self.message is not None and self.message != "" else ""
        wraps = "(inner exception: " + repr(self.inner) + ")"
        return self.__class__.__name__ + ":" + msg + wraps

    def __str__(self):
        return repr(self._message)


class ProviderClientError(ProviderError):
    def __init__(self, response, message="", inner=None):
        super(ProviderClientError, self).__init__(message, inner)
        self.response = response

class ProviderServerError(ProviderError):
    def __init__(self, response, message="", inner=None):
        super(ProviderServerError, self).__init__(message, inner)
        self.response = response

class ProviderConfigurationError(ProviderError):
    pass

class ProviderTimeout(ProviderServerError):
    pass

class ProviderHttpError(ProviderError):
    pass

class ProviderContentMalformedError(ProviderClientError):
    pass

class ProviderItemNotFoundError(ProviderClientError):
    pass
    
class ProviderValidationFailedError(ProviderClientError):
    pass

class ProviderRateLimitError(ProviderClientError):
    pass

class ProviderAuthenticationError(ProviderClientError):
    pass

def _load_json(page):
    try:
        data = simplejson.loads(page) 
    except simplejson.JSONDecodeError, e:
        logger.error(u"%s json decode fail on '%s'" %("_load_json", e.msg))
        raise ProviderContentMalformedError
    return(data)

def _lookup_json(data, keylist):
    for mykey in keylist:
        try:
            data = data[mykey]
        except (KeyError, TypeError):
            return None
    return(data)

def _extract_from_data_dict(data, dict_of_keylists, include_falses=False):
    return_dict = {}
    if dict_of_keylists:
        for (metric, keylist) in dict_of_keylists.iteritems():
            value = _lookup_json(data, keylist)

            # unless include_falses, only set metrics for non-zero and non-null metrics
            if include_falses or (value and (value != "0")):
                return_dict[metric] = value
    return return_dict


def _extract_from_json(page, dict_of_keylists, include_falses=False):
    data = _load_json(page)
    if not data:
        return {}
    return_dict = _extract_from_data_dict(data, dict_of_keylists, include_falses)
    return return_dict

def _get_doc_from_xml(page):
    try:
        try:
            doc = minidom.parseString(page.strip().encode('utf-8'))
        except UnicodeDecodeError:
            doc = minidom.parseString(page.strip())            
        lookup_function = _lookup_xml_from_dom
    except ExpatError, e:
        doc = BeautifulSoup.BeautifulStoneSoup(page) 
        lookup_function = _lookup_xml_from_soup

    if not doc:
        raise ProviderContentMalformedError
    return (doc, lookup_function)

def _count_in_xml(page, mykey): 
    doc_list = _find_all_in_xml(page, mykey)
    if not doc_list:
        return 0
    count = len(doc_list)
    return(count)

def _find_all_in_xml(page, mykey):  
    (doc, lookup_function) = _get_doc_from_xml(page)  
    if not doc:
        return None
    try:
        doc_list = doc.getElementsByTagName(mykey)
    except (KeyError, IndexError, TypeError):
        return None
            
    return(doc_list)


def _lookup_xml_from_dom(doc, keylist): 
    response = None   
    for mykey in keylist:
        if not doc:
            return None

        try:
            doc_list = doc.getElementsByTagName(mykey)
            # just takes the first one for now
            doc = doc_list[0]
        except (KeyError, IndexError):
            return None
            
    if doc:    
        try:
            response = doc.firstChild.data
        except AttributeError:
            return None
    try:
        response = int(response)
    except ValueError:
        pass
    return(response)

def _lookup_xml_from_soup(soup, keylist):    
    smaller_bowl_of_soup = soup
    for mykey in keylist:
        if not smaller_bowl_of_soup:
            return None
        try:
            # BeautifulSoup forces all keys to lowercase
            smaller_bowl_of_soup = smaller_bowl_of_soup.find(mykey.lower())
        except KeyError:
            return None
            
    if smaller_bowl_of_soup: 
        response = smaller_bowl_of_soup.text
    else:
        response = None

    try:
        response = int(response)
    except (ValueError, TypeError):
        pass

    return(response)

def _metrics_dict_as_ints(metrics_dict):
    metrics_dict_ints = {key:int(val) for (key, val) in metrics_dict.items()}
    return metrics_dict_ints

def _extract_from_xml(page, dict_of_keylists):
    (doc, lookup_function) = _get_doc_from_xml(page)
    return_dict = {}
    if dict_of_keylists:
        for (metric, keylist) in dict_of_keylists.iteritems():
            value = lookup_function(doc, keylist)

            # only set metrics for non-zero and non-null metrics
            if value:
                try:
                    value = value.strip()  #strip spaces if any
                except AttributeError:
                    pass
                return_dict[metric] = value

    return return_dict

# given a url that has a doi embedded in it, return the doi
def doi_from_url_string(url):
    logger.info(u"%s parsing url %s" %("doi_from_url_string", url))

    result = re.findall("(10\.\d+.[0-9a-wA-W_/\.\-%]+)" , url, re.DOTALL)
    try:
        doi = urllib.unquote(result[0])
    except IndexError:
        doi = None

    return(doi)

def alias_dict_from_tuples(aliases_tuples):
    alias_dict = {}
    for (ns, ids) in aliases_tuples:
        if ns in alias_dict:
            alias_dict[ns] += [ids]
        else:
            alias_dict[ns] = [ids]
    return alias_dict
    
def strip_leading_http(url):
    response = re.sub(u"^https*://", "", url)
    return response


########NEW FILE########
__FILENAME__ = pubmed
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact.unicode_helpers import remove_nonprinting_characters

import simplejson, urllib, os, itertools, datetime, re
from StringIO import StringIO
from lxml import etree

import logging
logger = logging.getLogger('ti.providers.pubmed')

def clean_pmid(pmid):
    pmid = remove_nonprinting_characters(pmid)
    pmid = pmid.lower().replace("pmid:", "")
    return pmid


class Pubmed(Provider):  

    example_id = ("pmid", "22855908")

    url = "http://pubmed.gov"
    descr = "PubMed comprises more than 21 million citations for biomedical literature"
    provenance_url_pmc_citations_template = "http://www.ncbi.nlm.nih.gov/pubmed?linkname=pubmed_pubmed_citedin&from_uid=%s"
    provenance_url_pmc_citations_filtered_template = "http://www.ncbi.nlm.nih.gov/pubmed?term=%s&cmd=DetailsSearch"
    provenance_url_f1000_template = "http://f1000.com/pubmed/%s"

    metrics_url_template = None # have specific metrics urls instead
    metrics_pmc_citations_url_template = "http://www.pubmedcentral.nih.gov/utils/entrez2pmcciting.cgi?view=xml&id=%s&email=team@total-impact.org&tool=total-impact"
    metrics_pmc_filter_url_template = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=(%s)&email=team@total-impact.org&tool=total-impact"
    elink_url = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=%s&cmd=llinks&email=team@total-impact.org&tool=total-impact"
    metrics_f1000_url_template = elink_url

    aliases_from_doi_url_template = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=%s&email=team@total-impact.org&tool=total-impact" 
    aliases_from_pmid_url_template = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=%s&retmode=xml&email=team@total-impact.org&tool=total-impact" 

    aliases_pubmed_url_template = "http://www.ncbi.nlm.nih.gov/pubmed/%s"

    biblio_url_efetch_template = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=%s&retmode=xml&email=team@total-impact.org&tool=total-impact" 
    biblio_url_elink_template = elink_url

    pmc_article_template = "http://www.ncbi.nlm.nih.gov/pmc/articles/%s"

    static_meta_dict = {
        "pmc_citations": {
            "display_name": "citations",
            "provider": "PubMed Central",
            "provider_url": "http://pubmed.gov",
            "description": "The number of citations by papers in PubMed Central",
            "icon": "http://www.ncbi.nlm.nih.gov/favicon.ico"
        }, 
        "pmc_citations_reviews": {
            "display_name": "citations: reviews",
            "provider": "PubMed Central",
            "provider_url": "http://pubmed.gov",
            "description": "The number of citations by review papers in PubMed Central",
            "icon": "http://www.ncbi.nlm.nih.gov/favicon.ico"
        }, 
        "pmc_citations_editorials": {
            "display_name": "citations: editorials",
            "provider": "PubMed Central",
            "provider_url": "http://pubmed.gov",
            "description": "The number of citations by editorials papers in PubMed Central",
            "icon": "http://www.ncbi.nlm.nih.gov/favicon.ico"
        },            
        "f1000": {
            "display_name": "reviewed",
            "provider": "F1000",
            "provider_url": "http://f1000.com",
            "description": "The article has been reviewed by F1000",
            "icon": "http://f1000.com/1371136042516/images/favicons/favicon-F1000.ico"
        }            
    }


    def __init__(self):
        super(Pubmed, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        relevant = (namespace=="pmid")
        return(relevant)

    # overriding default because overriding aliases method
    @property
    def provides_aliases(self):
        return True

    # overriding default because overriding biblio method
    @property
    def provides_biblio(self):
        return True

    # overriding default because overriding metrics method
    @property
    def provides_metrics(self):
        return True

    # overriding default because overriding member_items method
    @property
    def provides_members(self):
        return True

    def _extract_biblio_efetch(self, page, id=None):
        if "ArticleDate" in page:
            dict_of_keylists = {"year": ["PubmedArticleSet", "MedlineCitation", "Article", "ArticleDate", "Year"], 
                                "month": ["PubmedArticleSet", "MedlineCitation", "Article", "ArticleDate", "Month"],
                                "day": ["PubmedArticleSet", "MedlineCitation", "Article", "ArticleDate", "Day"],
                                "title": ["PubmedArticleSet", "MedlineCitation", "Article", "ArticleTitle"],
                                "issn": ["PubmedArticleSet", "MedlineCitation", "Article", "Journal", "ISSN"],
                                "journal": ["PubmedArticleSet", "MedlineCitation", "Article", "Journal", "Title"],
                                }
        else:
            dict_of_keylists = {"year": ["PubmedArticleSet", "MedlineCitation", "Article", "PubDate", "Year"], 
                                "month": ["PubmedArticleSet", "MedlineCitation", "Article", "PubDate", "Month"],
                                "day": ["PubmedArticleSet", "MedlineCitation", "Article", "PubDate", "Day"],
                                "title": ["PubmedArticleSet", "MedlineCitation", "Article", "ArticleTitle"],
                                "issn": ["PubmedArticleSet", "MedlineCitation", "Article", "Journal", "ISSN"],
                                "journal": ["PubmedArticleSet", "MedlineCitation", "Article", "Journal", "Title"],
                                }            
        biblio_dict = provider._extract_from_xml(page, dict_of_keylists)
        dom_authors = provider._find_all_in_xml(page, "LastName")
        try:
            biblio_dict["authors"] = ", ".join([author.firstChild.data for author in dom_authors])
        except (AttributeError, TypeError):
            pass

        try:
            biblio_dict["issn"] = biblio_dict["issn"].replace("-", "")
        except (AttributeError, KeyError):
            pass

        try:
            datetime_published = datetime.datetime(year=biblio_dict["year"], 
                                                    month=biblio_dict["month"], 
                                                    day=biblio_dict["day"])
            biblio_dict["date"] = datetime_published.isoformat()
            biblio_dict["year"] = re.sub("\D", "", str(biblio_dict["year"]))
            del biblio_dict["month"]
            del biblio_dict["day"]
        except (AttributeError, TypeError, KeyError):
            logger.debug(u"%20s don't have full date information %s" % (self.provider_name, id))
            pass

        try:
            biblio_dict["year"] = str(biblio_dict["year"])
        except (KeyError):
            pass

        return biblio_dict  


    def _extract_biblio_elink(self, page, id=None):

        biblio_dict = {}
        if not page:
            return biblio_dict
            
        try:
            tree = etree.parse(StringIO(page))
            obj_urls = tree.xpath("//ObjUrl[Category='Full Text Sources' and Attribute='free resource']/Url")
            if obj_urls:
                biblio_dict = {"free_fulltext_url": obj_urls[0].text}
        except etree.XMLSyntaxError:
            self.logger.warning(u"{provider_name} _extract_biblio_elink XMLSyntaxError on {id}".format(
                provider_name=self.provider_name, id=id))

        return biblio_dict 


    def biblio(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True):

        aliases_dict = provider.alias_dict_from_tuples(aliases)
        if not "pmid" in aliases_dict:
            return None
        id = aliases_dict["pmid"][0]

        self.logger.debug(u"%s getting biblio for %s" % (self.provider_name, id))
        biblio_dict = {}

        efetch_url = self._get_templated_url(self.biblio_url_efetch_template, id, "biblio")
        efetch_page = self._get_eutils_page(efetch_url, id, cache_enabled=cache_enabled)
        biblio_dict.update(self._extract_biblio_efetch(efetch_page, id))

        elink_url = self._get_templated_url(self.biblio_url_elink_template, id, "biblio")
        elink_page = self._get_eutils_page(elink_url, id, cache_enabled=cache_enabled)
        biblio_dict.update(self._extract_biblio_elink(elink_page, id))

        if "pmc" in aliases_dict:
            biblio_dict["free_fulltext_url"] = self.pmc_article_template % aliases_dict["pmc"][0]
        elif ("issn" in biblio_dict) and provider.is_issn_in_doaj(biblio_dict["issn"]):
            biblio_dict["free_fulltext_url"] = self.aliases_pubmed_url_template %id

        return biblio_dict


    def _extract_aliases_from_doi(self, page, doi):
        dict_of_keylists = {"pmid": ["eSearchResult", "IdList", "Id"], 
                            "QueryTranslation": ["eSearchResult", "QueryTranslation"]}

        aliases_dict = provider._extract_from_xml(page, dict_of_keylists)
        aliases_list = []
        if aliases_dict:
            if aliases_dict["QueryTranslation"] == (doi + "[All Fields]"):
                aliases_list = [("pmid", str(aliases_dict["pmid"]))]
        return aliases_list


    def _extract_aliases_from_pmid(self, page, pmid):
        (doc, lookup_function) = provider._get_doc_from_xml(page)
        doi = None
        pmc = None
        try:
            articleidlist = doc.getElementsByTagName("ArticleIdList")[0]
            for articleid in articleidlist.getElementsByTagName("ArticleId"):
                if (articleid.getAttribute("IdType") == u"doi"):
                    doi = articleid.firstChild.data
                if (articleid.getAttribute("IdType") == u"pmc"):
                    pmc = articleid.firstChild.data

            if not doi:
                #give it another try, in another part of the xml 
                # see http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=23682040&retmode=xml&email=team@total-impact.org&tool=total-impact
                article = doc.getElementsByTagName("Article")[0]
                for elocationid in article.getElementsByTagName("ELocationID"):
                    if (elocationid.getAttribute("EIdType") == u"doi"):
                        if (elocationid.getAttribute("ValidYN") == u"Y"):
                            doi = elocationid.firstChild.data

        except (IndexError, TypeError):
            pass

        #sometimes no doi, or PMID has a doi-fragment in the doi field:
        aliases_list = []
        if doi:
            if "10." in doi:  
                aliases_list += [("doi", doi), ("url", "http://dx.doi.org/"+doi)]
        if pmc:
            aliases_list += [("pmc", pmc), ("url", "http://www.ncbi.nlm.nih.gov/pmc/articles/"+pmc)]

        return aliases_list

    def _get_eutils_page(self, url, id, cache_enabled=True):
        logger.debug(u"%20s getting eutils page for %s" % (self.provider_name, id))

        response = self.http_get(url, cache_enabled=cache_enabled)
        if response.status_code != 200:
            logger.warning(u"%20s WARNING, status_code=%i getting %s" 
                % (self.provider_name, response.status_code, url))            
            if response.status_code == 404:
                return ""
            if response.status_code == 414:  #Request-URI Too Large
                return ""
            else:
                self._get_error(response.status_code, response)
        return response.text

    # overriding so can look up in both directions
    def aliases(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):            

        new_aliases = []

        for alias in aliases:
            (namespace, nid) = alias
            if (namespace == "doi"):
                aliases_from_doi_url = self.aliases_from_doi_url_template %nid
                page = self._get_eutils_page(aliases_from_doi_url, nid, cache_enabled)
                if page:
                    new_aliases += self._extract_aliases_from_doi(page, nid)
            if (namespace == "pmid"):
                # look up doi and other things on pubmed page
                aliases_from_pmid_url = self.aliases_from_pmid_url_template %nid
                page = self._get_eutils_page(aliases_from_pmid_url, nid, cache_enabled)
                if page:
                    new_aliases += self._extract_aliases_from_pmid(page, nid)
                    biblio = self._extract_biblio_efetch(page, nid)
                    if biblio:
                        new_aliases += [("biblio", biblio)]
                # also, add link to paper on pubmed
                new_aliases += [("url", self.aliases_pubmed_url_template %nid)] 

        # get uniques for things that are unhashable
        new_aliases_unique = [k for k,v in itertools.groupby(sorted(new_aliases))]

        return new_aliases_unique

    def _filter(self, id, citing_pmcids, filter_ptype):
        pmcids_string = " OR ".join(["PMC"+pmcid for pmcid in citing_pmcids])
        query_string = filter_ptype + "[ptyp] AND (" + pmcids_string + ")"
        pmcid_filter_url = self.metrics_pmc_filter_url_template %query_string
        page = self._get_eutils_page(pmcid_filter_url, id)
        (doc, lookup_function) = provider._get_doc_from_xml(page)  
        try:    
            id_docs = doc.getElementsByTagName("Id")
            pmids = [id_doc.firstChild.data for id_doc in id_docs]
        except TypeError:
            logger.warning(u"%20s no Id xml tags for %s" % (self.provider_name, id))
            pmids = []
        return pmids

    def _check_reviewed_by_f1000(self, id, cache_enabled):
        metrics_f1000_url = self.metrics_f1000_url_template %id
        page = self._get_eutils_page(metrics_f1000_url, id)
        f1000_url = "http://f1000.com/pubmed/%s" %id
        if (f1000_url in page):
            reviewed_by_f1000 = "Yes"
        else:
            reviewed_by_f1000 = 0
        return reviewed_by_f1000

    # override because multiple pages to get
    def get_metrics_for_id(self, 
            id, 
            provider_url_template=None, 
            cache_enabled=True):

        # logger.debug(u"%20s getting metrics for %s" % (self.provider_name, id))
        metrics_dict = {}

        reviewed_by_f1000 = self._check_reviewed_by_f1000(id, cache_enabled)
        if reviewed_by_f1000:
            metrics_dict["pubmed:f1000"] = reviewed_by_f1000

        # they sometimes contain duples, so uniquify
        citing_pmcids = list(set(self._get_citing_pmcids(id, cache_enabled)))
        if (citing_pmcids):
            metrics_dict["pubmed:pmc_citations"] = len(citing_pmcids)
    
            number_review_pmids = len(self._filter(id, citing_pmcids, "review"))
            if number_review_pmids:
                metrics_dict["pubmed:pmc_citations_reviews"] = number_review_pmids
    
            number_editorial_pmids = len(self._filter(id, citing_pmcids, "editorial"))
            if number_editorial_pmids:
                metrics_dict["pubmed:pmc_citations_editorials"] = number_editorial_pmids
        # check for f1000

        return metrics_dict

    def _extract_citing_pmcids(self, page):
        if (not "PubMedToPMCcitingformSET" in page):
            raise ProviderContentMalformedError()
        dict_of_keylists = {"pubmed:pmc_citations": ["PubMedToPMCcitingformSET", "REFORM"]}
        (doc, lookup_function) = provider._get_doc_from_xml(page)
        try:
            pmcid_doms = doc.getElementsByTagName("PMCID")
            pmcids = [pmcid_dom.firstChild.data for pmcid_dom in pmcid_doms]
        except TypeError:
            logger.warning(u"%20s no PMCID xml tags for %s" % (self.provider_name, id))            
            pmcids = []
        return pmcids

    # documentation for pubmedtopmcciting: http://www.pubmedcentral.nih.gov/utils/entrez2pmcciting.cgi
    # could take multiple PMC IDs
    def _get_citing_pmcids(self, id, cache_enabled=True):
        pmc_citations_url = self.metrics_pmc_citations_url_template %id
        page = self._get_eutils_page(pmc_citations_url, id, cache_enabled)
        pmcids = self._extract_citing_pmcids(page)
        return pmcids

    def provenance_url(self, metric_name, aliases):
        id = self.get_best_id(aliases)     
        if not id:
            # not relevant to Pubmed
            return None

        url = None
        if (metric_name == "pubmed:pmc_citations"):
            url = self._get_templated_url(self.provenance_url_pmc_citations_template, id, "provenance")

        elif (metric_name == "pubmed:f1000"):
            url = self._get_templated_url(self.provenance_url_f1000_template, id, "provenance")

        elif (metric_name == "pubmed:pmc_citations_reviews"):
            citing_pmcids = self._get_citing_pmcids(id)
            filtered_pmids = self._filter(id, citing_pmcids, "review")
            pmids_string = " OR ".join([pmid for pmid in filtered_pmids])
            url = self._get_templated_url(self.provenance_url_pmc_citations_filtered_template, 
                    urllib.quote(pmids_string), "provenance")

        elif (metric_name == "pubmed:pmc_citations_editorials"):
            citing_pmcids = self._get_citing_pmcids(id)
            filtered_pmids = self._filter(id,citing_pmcids, "editorial")
            pmids_string = " OR ".join([pmid for pmid in filtered_pmids])
            url = self._get_templated_url(self.provenance_url_pmc_citations_filtered_template, 
                    urllib.quote(pmids_string), "provenance")

        return url


    # overriding because don't need to look up
    def member_items(self, 
            query_string, 
            provider_url_template=None, 
            cache_enabled=True):

        if not self.provides_members:
            raise NotImplementedError()

        self.logger.debug(u"%s getting member_items for %s" % (self.provider_name, query_string))

        pmids = re.findall("\d+", query_string)
        aliases_tuples = [("pmid", clean_pmid(pmid)) for pmid in pmids if pmid]

        return(aliases_tuples)




########NEW FILE########
__FILENAME__ = scienceseeker
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
import hashlib
import simplejson

import logging
logger = logging.getLogger('ti.providers.scienceseeker')

class Scienceseeker(Provider):  

    example_id = ("doi", "10.1016/j.cbpa.2010.06.169")
    metrics_url_template = "http://scienceseeker.org/search/default/?type=post&filter0=citation&modifier0=id-all&value0=%s"
    provenance_url_template = "http://scienceseeker.org/posts/?type=post&filter0=citation&modifier0=id-all&value0=%s"
    url = "http://www.scienceseeker.org"
    descr = "Science news from science newsmakers"
    static_meta_dict = {
        "blog_posts": {
            "display_name": "blog posts",
            "provider": "Science Seeker",
            "provider_url": "http://www.scienceseeker.org",
            "description": "The number of blog posts that cite this item.",
            "icon": "http://scienceseeker.org/images/misc/favicon.ico"
        }
    }

    def __init__(self):
        super(Scienceseeker, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return("doi" == namespace)

    def _extract_metrics(self, page, status_code=200, id=None): 
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if ("subjectseeker" not in page) and ("Recent Posts" not in page):
            raise ProviderContentMalformedError

        (doc, lookup_function) = provider._get_doc_from_xml(page)  
        if not doc:
            return {}
        try:
            feed_doc = doc.getElementsByTagName("feed")
            entry_docs = feed_doc[0].getElementsByTagName("entry")
            number_blog_posts = len(entry_docs)
        except (KeyError, IndexError, TypeError):
            return {}

        if number_blog_posts:
            metrics_dict = {'scienceseeker:blog_posts': number_blog_posts}
        else:
            metrics_dict = {}
        return metrics_dict




########NEW FILE########
__FILENAME__ = scopus
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderServerError

import simplejson, re, os, random, string, urllib

import logging
logger = logging.getLogger('ti.providers.scopus')

class Scopus(Provider):  

    example_id = ("doi", "10.1371/journal.pone.0000308")

    url = "http://www.info.sciverse.com/scopus/about"
    descr = "The world's largest abstract and citation database of peer-reviewed literature."
    # template urls below because they need a freshly-minted random string
    metrics_url_template = None
    provenance_url_template = "http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b&scp=%s"

    static_meta_dict =  { 
        "citations": {
            "display_name": "citations",
            "provider": "Scopus",
            "provider_url": "http://www.info.sciverse.com/scopus/about",
            "description": "Number of times the item has been cited",
            "icon": "http://www.info.sciverse.com/sites/all/themes/sciverse/favicon.ico" ,
        }
    }
    

    def __init__(self):
        super(Scopus, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return (namespace in ["doi", "biblio"])


    def _get_json(self, fullpage):
        try:
            # extract json from inside the first and last parens
            # from http://codereview.stackexchange.com/questions/2561/converting-jsonp-to-json-is-this-regex-correct
            page = fullpage[ fullpage.index("(")+1 : fullpage.rindex(")") ]
        except (AttributeError, ValueError):
            raise ProviderContentMalformedError()

        data = provider._load_json(page)
        return(data)



    def _extract_metrics(self, record, status_code=200, id=None):
        try:
            citations = int(record["citedby-count"])    
        except (KeyError, TypeError, ValueError):
            return {}

        if citations:
            metrics_dict = {"scopus:citations": citations}
        else:
            metrics_dict = {}                    
        return metrics_dict

    def _extract_provenance_url(self, record, status_code=200, id=None):
        try:
            api_url = record["prism:url"] 
            match = re.findall("scopus_id:([\dA-Z]+)", api_url)
            scopus_id = match[0]
            provenance_url = self._get_templated_url(self.provenance_url_template, scopus_id)
        except (KeyError, TypeError):
            provenance_url = ""
        return provenance_url

    def _get_page(self, url, headers={}):
        response = self.http_get(url, headers=headers, timeout=30)
        if response.status_code != 200:
            if response.status_code == 404:
                return None
            else:
                raise(self._get_error(response.status_code, response))
        page = response.text
        if not page:
            raise ProviderContentMalformedError()
        return page

    def _extract_relevant_record(self, fullpage, id):
        data = provider._load_json(fullpage)
        response = None
        try:
            response = data["search-results"]["entry"][0]
        except (KeyError, ValueError):
            # not in Scopus database
            return None
        return response

    def _get_scopus_page(self, url):
        headers = {}
        headers["accept"] = "application/json"

        try:
            page = self._get_page(url, headers)
        except ProviderServerError:
            logger.info(u"error getting page with id {url}".format(url=url))
            return None

        if not page:
            logger.info(u"empty page with id {url}".format(url=url))
            return None
        if "Result set was empty" in page:
            #logger.warning(u"empty result set with doi {url}".format(url=url))
            return None
        return page


    def _get_relevant_record_with_doi(self, doi):
        # pick a new random string so don't time out.  Unfort, url now can't cache.
        random_string = "".join(random.sample(string.letters, 10))

        # this is how scopus wants us to escape special characters
        # http://help.scopus.com/Content/h_specialchars.htm
        # what scopus considers a special character and can appear in a DOI is a little unknown
        # but definitely includes parens.
        doi = doi.replace("(", "{(}")
        doi = doi.replace(")", "{)}")
        url_template = "https://api.elsevier.com/content/search/index:SCOPUS?query=DOI({doi})&field=citedby-count&apiKey="+os.environ["SCOPUS_KEY"]+"&insttoken="+os.environ["SCOPUS_INSTTOKEN"]
        url = url_template.format(doi=doi)

        page = self._get_scopus_page(url)

        if not page:
            return None  # empty result set

        relevant_record = self._extract_relevant_record(page, id)
        return relevant_record


    def _get_scopus_url(self, biblio_dict):
        url_template_one_journal = "https://api.elsevier.com/content/search/index:SCOPUS?query=AUTHOR-NAME({first_author})%20AND%20TITLE({title})%20AND%20SRCTITLE({journal})&field=citedby-count&apiKey="+os.environ["SCOPUS_KEY"]+"&insttoken="+os.environ["SCOPUS_INSTTOKEN"]            
        url_template_two_journals = "https://api.elsevier.com/content/search/index:SCOPUS?query=AUTHOR-NAME({first_author})%20AND%20TITLE({title})%20AND%20(SRCTITLE({journal1})%20OR%20SRCTITLE({journal2}))&field=citedby-count&apiKey="+os.environ["SCOPUS_KEY"]+"&insttoken="+os.environ["SCOPUS_INSTTOKEN"]
        alt_journal_names = {"BMJ": "British Medical Journal"}

        title = biblio_dict["title"].replace("(", "{(}").replace(")", "{)}")
        journal = biblio_dict["journal"].replace("(", "{(}").replace(")", "{)}")

        if journal in alt_journal_names.keys():
            journal1 = journal
            journal2 = alt_journal_names[journal]
            url = url_template_two_journals.format(
                    first_author=urllib.quote(biblio_dict["first_author"]), 
                    title=urllib.quote(title), 
                    journal1=urllib.quote(journal1), 
                    journal2=urllib.quote(journal2))
        else:
            url = url_template_one_journal.format(
                    first_author=urllib.quote(biblio_dict["first_author"]), 
                    title=urllib.quote(title), 
                    journal=urllib.quote(journal))
        return url



    def _get_relevant_record_with_biblio(self, biblio_dict):
        try:        
            if not "first_author" in biblio_dict:
                biblio_dict["first_author"] = biblio_dict["authors"].split(" ")[0]
            if not biblio_dict["first_author"]:
                return None
            if not biblio_dict["title"] and biblio_dict["journal"] and biblio_dict["first_author"]:
                logger.debug("not enough info in _get_relevant_record_with_biblio to look up citations")
            url = self._get_scopus_url(biblio_dict)

        except KeyError:
            logger.debug("tried _get_relevant_record_with_biblio but leaving because KeyError")
            return None

        page = self._get_scopus_page(url)

        if not page:
            return None  # empty result set

        relevant_record = self._extract_relevant_record(page, biblio_dict)
        return relevant_record


    def _get_metrics_and_drilldown_from_metrics_page(self, provider_url_template, namespace, id):
        relevant_record = None
        if namespace=="doi":
            relevant_record = self._get_relevant_record_with_doi(id)
        elif namespace=="biblio":
            relevant_record = self._get_relevant_record_with_biblio(id)

        if not relevant_record:
            logger.info(u"no scopus page with id {id}".format(id=id))
            return {}

        metrics_dict = self._extract_metrics(relevant_record)
        
        metrics_and_drilldown = {}
        for metric_name in metrics_dict:
            drilldown_url = self._extract_provenance_url(relevant_record)
            metrics_and_drilldown[metric_name] = (metrics_dict[metric_name], drilldown_url)
        return metrics_and_drilldown  


    def get_best_alias(self, aliases_dict):
        for namespace in ["doi", "biblio"]:
            if namespace in aliases_dict:
                return (namespace, aliases_dict[namespace][0])
        return (None, None)

    # custom, because uses doi if available, else biblio
    def metrics(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True):

        aliases_dict = provider.alias_dict_from_tuples(aliases)

        metrics_and_drilldown = {}
        if "doi" in aliases_dict:
            nid = aliases_dict["doi"][0]
            metrics_and_drilldown = self._get_metrics_and_drilldown_from_metrics_page(provider_url_template, 
                    namespace="doi", 
                    id=nid)
        if not metrics_and_drilldown and "biblio" in aliases_dict:
            nid = aliases_dict["biblio"][0]
            metrics_and_drilldown = self._get_metrics_and_drilldown_from_metrics_page(provider_url_template, 
                    namespace="biblio", 
                    id=nid)

        return metrics_and_drilldown

########NEW FILE########
__FILENAME__ = slideshare
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError, ProviderItemNotFoundError, ProviderRateLimitError

import simplejson, urllib, time, hashlib, re, os
from xml.dom import minidom 
from xml.parsers.expat import ExpatError

import logging
logger = logging.getLogger('ti.providers.slideshare')

class Slideshare(Provider):  

    example_id = ("url", "http://www.slideshare.net/cavlec")
    url = "http://www.slideshare.net/"
    descr = "The best way to share presentations, documents and professional videos."

    member_items_url_template = "https://www.slideshare.net/api/2/get_slideshows_by_user?api_key=" + os.environ["SLIDESHARE_KEY"] + "&detailed=1&ts=%s&hash=%s&username_for=%s"
    everything_url_template = "https://www.slideshare.net/api/2/get_slideshow?api_key=" + os.environ["SLIDESHARE_KEY"] + "&detailed=1&ts=%s&hash=%s&slideshow_url=%s"
    biblio_url_template = everything_url_template
    aliases_url_template = everything_url_template
    metrics_url_template = everything_url_template
    provenance_url_template = "%s"

    sanity_check_re = re.compile("<User")

    static_meta_dict = {
        "downloads": {
            "display_name": "downloads",
            "provider": "SlideShare",
            "provider_url": "http://www.slideshare.net/",
            "description": "The number of times the presentation has been downloaded",
            "icon": "http://www.slideshare.net/favicon.ico" ,
        },    
        "favorites": {
            "display_name": "favorites",
            "provider": "SlideShare",
            "provider_url": "http://www.slideshare.net/",
            "description": "The number of times the presentation has been favorited",
            "icon": "http://www.slideshare.net/favicon.ico" ,
        },    
        "comments": {
            "display_name": "comments",
            "provider": "SlideShare",
            "provider_url": "http://www.slideshare.net/",
            "description": "The number of comments the presentation has received",
            "icon": "http://www.slideshare.net/favicon.ico" ,
        },    
        "views": {
            "display_name": "views",
            "provider": "SlideShare",
            "provider_url": "http://www.slideshare.net/",
            "description": "The number of times the presentation has been viewed",
            "icon": "http://www.slideshare.net/favicon.ico" ,
        }    
    }


    def __init__(self):
        super(Slideshare, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        relevant = ((namespace=="url")and ("slideshare.net" in nid))
        return(relevant)

    #override because need to break up id
    def _get_templated_url(self, template, id, method=None):
        if method=="provenance":
            return template %id
            
        ts = time.time()
        hash_combo = hashlib.sha1(os.environ["SLIDESHARE_SECRET"] + str(ts)).hexdigest()
        complete_url = template %(ts, hash_combo, id)
        return(complete_url)

    def _sanity_check_page(self, page):
        if not self.sanity_check_re.search(page):
            if ("User Not Found" in page):
                raise ProviderItemNotFoundError
            elif ("Account Exceeded Daily Limit" in page):
                logger.info(u"Exceeded api limit for provider {provider}".format(
                    provider=self.provider_name))
                raise ProviderRateLimitError("Exceeded api limit for provider {provider}".format(
                    provider=self.provider_name))
            else:
                raise ProviderContentMalformedError
        return True

    def _extract_members(self, page, query_string): 
        try:
            doc = minidom.parseString(page.strip().encode('utf-8'))
        except ExpatError:
            raise ProviderContentMalformedError

        self._sanity_check_page(page)

        urls = doc.getElementsByTagName("URL")
        members = [("url", url.firstChild.data) for url in list(set(urls))]
        return(members)

    def _extract_biblio(self, page, id=None):
        dict_of_keylists = {
            'title' : ['Slideshow', 'Title'],
            'username' : ['Slideshow', 'Username'],
            'created' : ['Slideshow', 'Created'],
        }
        self._sanity_check_page(page)

        biblio_dict = provider._extract_from_xml(page, dict_of_keylists)
        biblio_dict["repository"] = "Slideshare"
        return biblio_dict    
       
    def _extract_aliases(self, page, id=None):
        dict_of_keylists = {
            'title' : ['Slideshow', 'Title']
        }
        self._sanity_check_page(page)

        aliases_dict = provider._extract_from_xml(page, dict_of_keylists)
        if aliases_dict:
            aliases_list = [(namespace, nid) for (namespace, nid) in aliases_dict.iteritems()]
        else:
            aliases_list = []
        return aliases_list


    def _extract_metrics(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))
        self._sanity_check_page(page)

        dict_of_keylists = {
            'slideshare:downloads' : ['Slideshow', 'NumDownloads'],
            'slideshare:views' : ['Slideshow', 'NumViews'],
            'slideshare:comments' : ['Slideshow', 'NumComments'],
            'slideshare:favorites' : ['Slideshow', 'NumFavorites'],
        }

        metrics_dict = provider._extract_from_xml(page, dict_of_keylists)
        for mykey in metrics_dict:
            metrics_dict[mykey] = int( metrics_dict[mykey])
        return metrics_dict


########NEW FILE########
__FILENAME__ = topsy
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import json, re, os
from operator import itemgetter

import logging
logger = logging.getLogger('ti.providers.topsy')

class Topsy(Provider):  

    example_id = ("url", "http://total-impact.org")

    url = "http://www.topsy.com/"
    descr = "Real-time search for the social web, <a href='http://topsy.com'><img src='http://cdn.topsy.com/img/powered.png'/></a>"
    metrics_url_template_general = 'http://otter.topsy.com/stats.json?url=%s&window=a&apikey=' + os.environ["TOPSY_KEY"]
    metrics_url_template_site = 'http://otter.topsy.com/search.json?q=site:%s&window=a&page=1&perpage=100&apikey=' + os.environ["TOPSY_KEY"]

    provenance_url_template_general = 'http://topsy.com/trackback?url=%s&window=a'
    provenance_url_template_site = 'http://topsy.com/s?q=site%%3A%s&window=a'  #escape % with %%

    top_tweeted_url_templates = {
        "site": 'http://otter.topsy.com/search.json?q=site:%s&window=a&type=link&page=%s&perpage=100&apikey=' + os.environ["TOPSY_KEY"],
        "twitter_account": 'http://otter.topsy.com/search.json?q=@%s&window=a&type=tweet&page=%s&perpage=100&apikey=' + os.environ["TOPSY_KEY"],
        "tweets_about": 'http://otter.topsy.com/search.json?q=@%s&window=a&type=tweet&page=%s&perpage=100&apikey=' + os.environ["TOPSY_KEY"]
    }

    static_meta_dict =  {
        "tweets": {
            "display_name": "tweets",
            "provider": "Topsy",
            "provider_url": "http://www.topsy.com/",
            "description": "Number of times the item has been tweeted",
            "icon": "http://twitter.com/phoenix/favicon.ico",
        },    
        "influential_tweets": {
            "display_name": "influential tweets",
            "provider": "Topsy",
            "provider_url": "http://www.topsy.com/",
            "description": "Number of times the item has been tweeted by influential tweeters",
            "icon": "http://twitter.com/phoenix/favicon.ico" ,
        }
    }
    

    def __init__(self):
        super(Topsy, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return namespace in ["blog", "url"]

    @property
    def provides_metrics(self):
         return True

    def get_site_id_for_template(self, aliases):
        for alias in aliases:
            (namespace, nid) = alias
            if ("blog"==namespace):
                blog_url_for_template = provider.strip_leading_http(nid).lower()
                return blog_url_for_template
        return None

    def get_best_id(self, aliases):
        nid = self.get_site_id_for_template(aliases)
        if nid:
            return nid
        else:
            metrics_url_template = self.metrics_url_template_general
            return self.get_relevant_alias_with_most_metrics("topsy:tweets", aliases, metrics_url_template)


    def provenance_url(self, metric_name, aliases):
        nid = self.get_site_id_for_template(aliases)
        if nid:
            provenance_url_template = self.provenance_url_template_site
        else:
            provenance_url_template = self.provenance_url_template_general
            metrics_url_template = self.metrics_url_template_general
            nid = self.get_relevant_alias_with_most_metrics("topsy:tweets", aliases, metrics_url_template)

        drilldown_url = self._get_templated_url(provenance_url_template, nid)
        return drilldown_url


    def metrics(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True):

        metrics_and_drilldown = {}
        nid = self.get_site_id_for_template(aliases)
        if nid:
            metrics_url_template = self.metrics_url_template_site
        else:
            metrics_url_template = self.metrics_url_template_general
            nid = self.get_relevant_alias_with_most_metrics("topsy:tweets", aliases, metrics_url_template, cache_enabled)

        if not nid:
            return {}
            
        metrics = self.get_metrics_for_id(nid, metrics_url_template, cache_enabled)
        metrics_and_drilldown = {}
        for metric_name in metrics:
            drilldown_url = self.provenance_url(metric_name, aliases)
            metrics_and_drilldown[metric_name] = (metrics[metric_name], drilldown_url)

        return metrics_and_drilldown 


    def _extract_metrics(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        metrics_dict = {}

        if "hits" in page:
            data = provider._load_json(page)
            hits = [post["hits"] for post in data["response"]["list"]]
            if hits:
                sum_of_hits = sum(hits)
                metrics_dict["topsy:tweets"] = sum_of_hits
        else:
            dict_of_keylists = {
                'topsy:tweets' : ['response', 'all'],
                'topsy:influential_tweets' : ['response', 'influential']
            }
            metrics_dict = provider._extract_from_json(page, dict_of_keylists)

        return metrics_dict


    def get_url_from_entry(self, query, entry, query_type):
        if query_type=="site":
            if query.lower() in entry["url"].lower():
                return entry["url"]
            elif entry["url_expansions"] and ("topsy_expanded_url" in entry["url_expansions"][0].lower()):
                return entry["url_expansions"][0]["topsy_expanded_url"]
        elif query_type=="twitter_account":
            if "/{query}/".format(query=query.lower()) in entry["url"].lower():
                return entry["url"]
            else:
                return None
        elif query_type=="tweets_about":
            if "/{query}/".format(query=query.lower()) not in entry["url"].lower():
                return entry["url"]
            else:
                return None
        return entry["url"]


    def top_tweeted_urls(self, query, query_type="site", number_to_return=10, pages=5):

        if query_type == "site":
            query = re.sub("http(s?)://", "", query.lower())
        elif query_type in ["twitter_account", "tweets_about"]:
            query = query.replace("@", "")

        template_url = self.top_tweeted_url_templates[query_type]
        urls = [template_url % (query, i) for i in range(1, pages+1)]
        responses = self.http_get_multiple(urls)
        tweeted_entries = [] 
        for url in responses:
            tweeted_entries += provider._load_json(responses[url].text)["response"]["list"]
        sorted_list = sorted(tweeted_entries, key=itemgetter('hits'), reverse=True) 

        top_tweeted_urls = [] #needs to be ordered

        for entry in sorted_list:
            url = self.get_url_from_entry(query, entry, query_type)
            if url and (url not in top_tweeted_urls):
                top_tweeted_urls.append(url)    
        return(top_tweeted_urls[0:number_to_return])


########NEW FILE########
__FILENAME__ = twitter_account
from birdy.twitter import AppClient, TwitterApiError
import os, re

from totalimpact.providers import provider
from totalimpact.providers import topsy
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import logging
logger = logging.getLogger('ti.providers.twitter_account')

class Twitter_Account(Provider):  

    example_id = ("url", "http://twitter.com/jasonpriem")

    url = "http://twitter.com"
    descr = "Social networking and microblogging service."
    member_items_url_template = "http://twitter.com/%s"
    provenance_url_templates = {
        "twitter_account:followers": "https://twitter.com/%s/followers",
        "twitter_account:lists": "https://twitter.com/%s/memberships"
        }

    static_meta_dict = {
        "followers": {
            "display_name": "followers",
            "provider": "Twitter",
            "provider_url": "http://twitter.com",
            "description": "The number of people following this Twitter account",
            "icon": "https://twitter.com/favicon.ico"
        },
        "lists": {
            "display_name": "lists",
            "provider": "Twitter",
            "provider_url": "http://twitter.com",
            "description": "The number of people who have included this Twitter account in a Twitter list",
            "icon": "https://twitter.com/favicon.ico"
            }
    }     

    def __init__(self):
        super(Twitter_Account, self).__init__()
        self.client = AppClient(os.getenv("TWITTER_CONSUMER_KEY"), 
                            os.getenv("TWITTER_CONSUMER_SECRET"),
                            os.getenv("TWITTER_ACCESS_TOKEN"))

    # overriding default because overriding member_items method
    # @property
    # def provides_members(self):
    #     return True

    @property
    def provides_biblio(self):
         return True

    @property
    def provides_metrics(self):
         return True

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        try:
            nid = nid.lower()
        except AttributeError:
            return False 
        if (namespace == "url"):
            if ("twitter.com" in nid) and ("/status/" not in nid):
                return True
        return False


    def screen_name(self, nid):
        #regex from http://stackoverflow.com/questions/4424179/how-to-validate-a-twitter-username-using-regex
        match = re.findall("twitter.com/([A-Za-z0-9_]{1,15}$)", nid)
        return match[0]


    # def member_items(self, 
    #         query_string, 
    #         provider_url_template=None, 
    #         cache_enabled=True):

    #     twitter_username = query_string.replace("@", "")
    #     url = self._get_templated_url(self.member_items_url_template, twitter_username, "members")
    #     members = [("url", url)]

    #     # import top tweets
    #     for tweet_url in topsy.Topsy().top_tweeted_urls(twitter_username, "twitter_account", number_to_return=10):
    #         members += [("url", tweet_url)] 

    #     return(members)


    def get_account_data(self, aliases):
        nid = self.get_best_id(aliases)
        if not nid:
            return None

        try:
            screen_name = self.screen_name(nid)
            r = self.client.api.users.show.get(screen_name=screen_name)
        except (IndexError, TwitterApiError):
            return None

        return r.data



    def biblio(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True):

        biblio_dict = {}
        biblio_dict["repository"] = "Twitter"

        data = self.get_account_data(aliases)
        if not data:
            return biblio_dict

        biblio_dict["title"] = u"@{screen_name}".format(
            screen_name=data["screen_name"])
        biblio_dict["authors"] = data["name"]
        biblio_dict["description"] = data["description"]
        biblio_dict["created_at"] = data["created_at"]
        twitter_username = data["screen_name"].replace("@", "")
        biblio_dict["url"] = u"http://twitter.com/{twitter_username}".format(
            twitter_username=data["screen_name"].replace("@", ""))

        biblio_dict["is_account"] = True  # special key to tell webapp to render as genre heading
        biblio_dict["account"] = u"@{screen_name}".format(
            screen_name=data["screen_name"])

        return biblio_dict
  


    def metrics(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True):

        data = self.get_account_data(aliases)
        if not data:
            return {}

        dict_of_keylists = {
            'twitter_account:followers' : ['followers_count'],
            'twitter_account:lists' : ['listed_count']
        }

        metrics_dict = {}
        for field in dict_of_keylists:
            metric_value = data[dict_of_keylists[field][0]]
            if metric_value:
                metrics_dict[field] = metric_value

        metrics_and_drilldown = {}
        for metric_name in metrics_dict:
            drilldown_url = self.provenance_url(metric_name, aliases)
            metrics_and_drilldown[metric_name] = (metrics_dict[metric_name], drilldown_url)

        return metrics_and_drilldown  


    # overriding default because different provenance url for each metric
    def provenance_url(self, metric_name, aliases):
        nid = self.get_best_id(aliases)
        if not nid:
            return None
        screen_name = self.screen_name(nid)
        provenance_url = self._get_templated_url(self.provenance_url_templates[metric_name], screen_name, "provenance")
        return provenance_url



########NEW FILE########
__FILENAME__ = twitter_tweet
import os, re, datetime

from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import logging
logger = logging.getLogger('ti.providers.twitter_tweet')

class Twitter_Tweet(Provider):  

    example_id = ("url", "http://twitter.com/jasonpriem")

    biblio_template_url = "https://api.twitter.com/1/statuses/oembed.json?id=%s&hide_media=1&hide_thread=1&maxwidth=650"
  

    def __init__(self):
        super(Twitter_Tweet, self).__init__()


    @property
    def provides_biblio(self):
         return True


    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        try:
            nid = nid.lower()
        except AttributeError:
            return False 
        if (namespace == "url"):
            if ("twitter.com" in nid) and ("/status/" in nid):
                return True
        return False


    def screen_name(self, tweet_url):
        #regex from http://stackoverflow.com/questions/4424179/how-to-validate-a-twitter-username-using-regex
        match = re.findall("twitter.com/([A-Za-z0-9_]{1,15})/", tweet_url)
        return match[0]


    def tweet_id(self, tweet_url):
        match = re.findall("twitter.com/.*/status/(\d+)", tweet_url)
        return match[0]



    # default method; providers can override
    def biblio(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True):

        tweet_url = self.get_best_id(aliases)
        biblio_embed_url = self.biblio_template_url % (self.tweet_id(tweet_url))
        response = self.http_get(biblio_embed_url)
        data = provider._load_json(response.text)

        biblio_dict = {}
        biblio_dict["repository"] = "Twitter"
        biblio_dict["url"] = tweet_url

        if not data:
            return biblio_dict

        biblio_dict["title"] = u"@{screen_name}".format(screen_name=self.screen_name(tweet_url))
        biblio_dict["authors"] = data["author_name"]
        biblio_dict["embed"] = data["html"]
        biblio_dict["embed_url"] = biblio_embed_url
        biblio_dict["account"] = u"@{screen_name}".format(screen_name=self.screen_name(tweet_url))
        try:
            tweet_match = re.findall(u'<p>(.*?)</p>.*statuses/\d+">(.*?)</a></blockquote>', biblio_dict["embed"])
            biblio_dict["tweet_text"] = tweet_match[0][0]
            biblio_dict["date"] = datetime.datetime.strptime(tweet_match[0][1], "%B %d, %Y").isoformat()
            biblio_dict["year"] = biblio_dict["date"][0:4]
        except (AttributeError):
            logger.debug("couldn't parse tweet embed {embed}".format(
                embed=biblio_dict["embed"]))

        return biblio_dict
  


########NEW FILE########
__FILENAME__ = vimeo
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import simplejson, os, re

import logging
logger = logging.getLogger('ti.providers.vimeo')

class Vimeo(Provider):  

    example_id = ("url", "http://vimeo.com/48605764")

    url = "http://vimeo.com"
    descr = "Vimeo: Your videos belong here."
    biblio_url_template = "http://vimeo.com/api/v2/video/%s.json"
    aliases_url_template = "http://vimeo.com/api/v2/video/%s.json"
    metrics_url_template = "http://vimeo.com/api/v2/video/%s.json"
    provenance_url_template = "http://vimeo.com/%s"

    static_meta_dict = {
        "plays": {
            "display_name": "plays",
            "provider": "Vimeo",
            "provider_url": "http://vimeo.com",
            "description": "The number of people who have played the video",
            "icon": "https://secure-a.vimeocdn.com/images_v6/favicon_32.ico",
        },
        "likes": {
            "display_name": "likes",
            "provider": "Vimeo",
            "provider_url": "http://vimeo.com",
            "description": "The number of people who have 'liked' the video",
            "icon": "https://secure-a.vimeocdn.com/images_v6/favicon_32.ico",
            },
        "comments": {
            "display_name": "comments",
            "provider": "Vimeo",
            "provider_url": "http://vimeo.com",
            "description": "The number of comments on a video",
            "icon": "https://secure-a.vimeocdn.com/images_v6/favicon_32.ico",
            }
    }     

    def __init__(self):
        super(Vimeo, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        if (("url" == namespace) and ("vimeo.com/" in nid)):
            return True
        else:
            return False

    def _get_video_id(self, video_url):
        try:
            nid_as_vimeo_id = re.findall("vimeo.com\/(\d+)", video_url)[0]
        except IndexError:
            raise ProviderContentMalformedError("No recognizable vimeo id")
        return nid_as_vimeo_id

    #override because need to break up id
    def _get_templated_url(self, template, nid_as_video_url, method=None):
        nid_as_video_id = self._get_video_id(nid_as_video_url)
        url = template % (nid_as_video_id)
        return(url)

    def _extract_biblio(self, page, id=None):

        json_response = provider._load_json(page)
        this_video_json = json_response[0]

        dict_of_keylists = {
            'title':        ['title'],
            'authors':      ['user_name'],
            'published_date': ['upload_date'],
            'url':          ['url']
        }

        biblio_dict = provider._extract_from_data_dict(this_video_json, dict_of_keylists)

        try:
            biblio_dict["year"] = biblio_dict["published_date"][0:4]
        except KeyError:
            pass

        biblio_dict["repository"] = "Vimeo"

        return biblio_dict    


    def _extract_metrics(self, page, status_code=200, id=None):        
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not "user_id" in page:
            raise ProviderContentMalformedError

        json_response = provider._load_json(page)
        this_video_json = json_response[0]

        dict_of_keylists = {
            'vimeo:plays' : ['stats_number_of_plays'],
            'vimeo:likes' : ['stats_number_of_likes'],
            'vimeo:comments' : ['stats_number_of_comments']
        }

        metrics_dict = provider._extract_from_data_dict(this_video_json, dict_of_keylists)

        return metrics_dict

########NEW FILE########
__FILENAME__ = webpage
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from totalimpact import unicode_helpers
from totalimpact.unicode_helpers import remove_nonprinting_characters

import lxml.html
import lxml.etree
import re

import logging
logger = logging.getLogger('ti.providers.webpage')


def clean_url(input_url):
    url = remove_nonprinting_characters(input_url)
    return url


class Webpage(Provider):  

    example_id = ("url", "http://total-impact.org/")

    biblio_url_template = "%s"
    provenance_url_template = "%s"
    descr = "Information scraped from webpages by ImpactStory"
    url = "http://impactstory.org"


    def __init__(self):
        super(Webpage, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        is_relevant = (namespace in ["url", "biblio"])
        return(is_relevant)

    # overriding default because overriding member_items method
    @property
    def provides_members(self):
        return True


    # copy biblio from aliases into item["biblio"] if no better source
    def biblio(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True):

        biblio = {}
        aliases_dict = provider.alias_dict_from_tuples(aliases)
        if "biblio" in aliases_dict:
            biblio = aliases_dict["biblio"][0]
        elif "url" in aliases_dict:
            url = aliases_dict["url"][0]
            if url and ("http://www.scopus.com/inward" not in url):
                if not provider_url_template:
                    provider_url_template = self.biblio_url_template
                biblio = self.get_biblio_for_id(url, provider_url_template, cache_enabled)

        return biblio


    # override because webpage doesn't throw timeouts, just get biblio if easy
    def get_biblio_for_id(self, 
            id,
            provider_url_template=None, 
            cache_enabled=True):

        logger.debug(u"%20s getting biblio for %s" % (self.provider_name, id))

        if not provider_url_template:
            provider_url_template = self.biblio_url_template
        url = self._get_templated_url(provider_url_template, id, "biblio")

        # try to get a response from the data provider 
        try:      
            response = self.http_get(url, cache_enabled=cache_enabled, allow_redirects=True)
        except provider.ProviderTimeout:
            logger.info(u"%20s ProviderTimeout getting %s so giving up on webpage biblio" 
                % (self.provider_name, id))
            return {}
        except provider.ProviderHttpError:
            logger.info(u"%20s ProviderHttpError getting %s so giving up on webpage biblio" 
                % (self.provider_name, id))
            return {}

        if response.status_code != 200:
            logger.info(u"%20s status_code=%i getting %s so giving up on webpage biblio" 
                % (self.provider_name, response.status_code, id))            
            return {}
        
        # extract the aliases
        try:
            biblio_dict = self._extract_biblio(response.text, id)
        except TypeError:  #sometimes has a response but no text in it
            return {}

        return biblio_dict

    # use lxml because is html
    def _extract_biblio(self, page, id=None):
        biblio_dict = {}

        if not page:
            return biblio_dict
        
        unicode_page = unicode_helpers.to_unicode_or_bust(page)
        try:
            parsed_html = lxml.html.document_fromstring(unicode_page)

            try:
                response = parsed_html.find(".//title").text
                if response and response.strip():
                    biblio_dict["title"] = response.strip()
            except AttributeError:
                pass

            try:
                response = parsed_html.find(".//h1").text
                if response and response.strip():
                    biblio_dict["h1"] = response.strip()
            except AttributeError:
                pass            

        # throws ParserError when document is empty        
        except (ValueError, lxml.etree.ParserError):
            logger.warning(u"%20s couldn't parse %s so giving up on webpage biblio" 
                            % (self.provider_name, id)) 
            try:
                response = re.search("<title>(.+?)</title>", unicode_page).group(1)
                response.replace("\n", "")
                response.replace("\r", "")
                if response:
                    biblio_dict["title"] = response.strip()
            except AttributeError:
                pass
        return biblio_dict    

    # overriding because don't need to look up
    def member_items(self, 
            query_string, 
            provider_url_template=None, 
            cache_enabled=True):

        if not self.provides_members:
            raise NotImplementedError()

        self.logger.debug(u"%s getting member_items for %s" % (self.provider_name, query_string))

        url_string = query_string.strip(" ")
        urls = [clean_url(url) for url in url_string.split("\n")]
        aliases_tuples = [("url", url) for url in urls if url]

        return(aliases_tuples)


########NEW FILE########
__FILENAME__ = wikipedia
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from xml.dom import minidom
from xml.parsers.expat import ExpatError

import logging
logger = logging.getLogger('ti.providers.wikipedia')

class Wikipedia(Provider):  
    """ Gets numbers of citations for a DOI document from wikipedia using
        the Wikipedia search interface.
    """

    example_id = ("doi", "10.1371/journal.pcbi.1000361")

    provenance_url_template = 'http://en.wikipedia.org/wiki/Special:Search?search="%s"&go=Go'
    metrics_url_template = 'http://en.wikipedia.org/w/api.php?action=query&list=search&srprop=timestamp&format=xml&srsearch="%s"'

    url = "http://www.wikipedia.org/"
    descr = "The free encyclopedia that anyone can edit."
    static_meta_dict = {
        "mentions": {
            "display_name": "mentions",
            "provider": "Wikipedia",
            "provider_url": "http://www.wikipedia.org/",
            "description": "The number of Wikipedia articles that mentioned this object.",
            "icon": "http://wikipedia.org/favicon.ico",
        }
    }


    def __init__(self):
        super(Wikipedia, self).__init__()

    def is_relevant_alias(self, alias):
        if not alias:
            return False
        (namespace, nid) = alias
        is_relevant = (namespace=="doi")
        return is_relevant

    def _extract_metrics(self, page, status_code=200, id=None):
        #logger.info(u"_extract_metrics with %s, %i,\n%s\n" % (id, status_code, page))

        if status_code != 200:
            if (status_code == 404):
                return {}
            else:
                raise(self._get_error(status_code))

        (doc, lookup_function) = provider._get_doc_from_xml(page)

        try:
            searchinfo = doc.getElementsByTagName('searchinfo')
            totalhits = int(searchinfo[0].attributes['totalhits'].value)
        except (TypeError, IndexError):
            raise ProviderContentMalformedError("No searchinfo in response document")

        if totalhits:
            metrics_dict = {"wikipedia:mentions": totalhits}
        else:
            metrics_dict = {}
        #logger.info(u"_extract_metrics returns metrics_dict %s" % (str(metrics_dict)))

        return metrics_dict
            


########NEW FILE########
__FILENAME__ = wordpresscom
from totalimpact.providers import provider
from totalimpact.providers import topsy
from totalimpact.providers import webpage
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import json, os, re

import logging
logger = logging.getLogger('ti.providers.wordpresscom')

class Wordpresscom(Provider):  

    example_id = ("blog", "http://blog.impactstory.org")

    url = "http://wordpress.com"
    descr = "A blog web hosting service provider owned by Automattic, and powered by the open source WordPress software."
    biblio_url_template = "https://public-api.wordpress.com/rest/v1/sites/%s/?pretty=1"
    aliases_url_template = "https://public-api.wordpress.com/rest/v1/sites/%s/?pretty=1"
    metrics_url_template_public = "https://public-api.wordpress.com/rest/v1/sites/%s/?pretty=1"
    metrics_url_template_comments = "https://public-api.wordpress.com/rest/v1/sites/%s/comments?pretty=1"
    metrics_url_template_wordpress_blog_views = "http://stats.wordpress.com/csv.php?api_key=%s&blog_uri=%s&table=views&days=-1&format=json&summarize=1"
    metrics_url_template_wordpress_post_views = "http://stats.wordpress.com/csv.php?api_key=%s&blog_uri=%s&table=views&days=-1&format=json&summarize=1&table=postviews&post_id=%s"
    metrics_url_template_wordpress_post_comments = "https://public-api.wordpress.com/rest/v1/sites/%s/posts/%s?pretty=1"
    provenance_url_template = "%s"

    static_meta_dict = {
        "subscribers": {
            "display_name": "subscribers",
            "provider": "WordPress.com",
            "provider_url": "http://wordpress.com",
            "description": "The number of people who receive emails about new posts on this blog",
            "icon": "https://wordpress.com/favicon.ico",
        },
        "views": {
            "display_name": "views",
            "provider": "WordPress.com",
            "provider_url": "http://wordpress.com",
            "description": "The number of times a blog post has been viewed",
            "icon": "https://wordpress.com/favicon.ico",
        },
        "comments": {
            "display_name": "comments",
            "provider": "WordPress.com",
            "provider_url": "http://wordpress.com",
            "description": "The number of comments on a blog post",
            "icon": "https://wordpress.com/favicon.ico",
        }
    }     

    def __init__(self):
        super(Wordpresscom, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        return namespace in ["blog", "blog_post"]

    # overriding default because overriding member_items method
    @property
    def provides_members(self):
        return True

    # overriding default because overriding aliases method
    @property
    def provides_aliases(self):
        return True

    # overriding default because overriding biblio method
    @property
    def provides_biblio(self):
        return True

    # overriding default because overriding metrics method
    @property
    def provides_metrics(self):
        return True

    # overriding
    def uses_analytics_credentials(self, method_name):
        if method_name == "metrics":
            return True
        else:
            return False



    #override because need to break strip http
    def _get_templated_url(self, template, nid, method=None):
        if method in ["metrics", "biblio", "aliases"]:
            nid = provider.strip_leading_http(nid).lower()
        url = template % (nid)
        return(url)


    # overriding
    def member_items(self, 
            input_dict, 
            provider_url_template=None, 
            cache_enabled=True):

        members = []

        if "blogUrl" in input_dict:
            blog_url = input_dict["blogUrl"]
        else:
            blog_url = None

        if blog_url:
            members += [("blog", blog_url)]

            # import top blog posts
            for post_url in topsy.Topsy().top_tweeted_urls(blog_url, number_to_return=10):
                blog_post_nid = {   
                        "post_url": post_url, 
                        "blog_url": blog_url
                        }
                members += [("blog_post", json.dumps(blog_post_nid))] 


        # handle individual blog posts
        if "blog_post_urls" in input_dict:
            members_as_webpages = webpage.Webpage().member_items(input_dict["blog_post_urls"])
            for (url_namespace, post_url) in members_as_webpages:
                if blog_url:
                    blog_url_for_blog_post_urls = blog_url
                else:
                    blog_url_for_blog_post_urls = "http://"+provider.strip_leading_http(post_url).split("/", 1)[0]
                blog_post_nid = {   
                        "post_url": post_url, 
                        "blog_url": blog_url_for_blog_post_urls 
                        }
                members += [("blog_post", json.dumps(blog_post_nid))] 

        return (members)
  

    # overriding
    def aliases(self, 
            aliases, 
            provider_url_template=None,
            cache_enabled=True):            

        new_aliases = []
        if not provider_url_template:
            provider_url_template = self.aliases_url_template

        for alias in aliases:
            (namespace, nid) = alias
            if namespace=="blog":
                new_alias = ("url", nid)
                if new_alias not in aliases:
                    new_aliases += [new_alias]

                url = self._get_templated_url(provider_url_template, nid, "aliases")

                # try to get a response from the data provider        
                response = self.http_get(url, cache_enabled=cache_enabled)

                if (response.status_code == 200) and ("ID" in response.text):
                    dict_of_keylists = {
                            'wordpress_blog_id' : ['ID']
                        }
                    aliases_dict = provider._extract_from_json(response.text, dict_of_keylists)
                    new_alias = ("wordpress_blog_id", str(aliases_dict["wordpress_blog_id"]))
                    if new_alias not in aliases:
                        new_aliases += [new_alias]

        return new_aliases


    def biblio(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True):

        aliases_dict = provider.alias_dict_from_tuples(aliases)
        if "blog" in aliases_dict:
            id = aliases_dict["blog"][0]

        # Only lookup biblio for items with appropriate ids
        if not id:
            #self.logger.debug(u"%s not checking biblio, no relevant alias" % (self.provider_name))
            return None

        if not provider_url_template:
            provider_url_template = self.biblio_url_template

        self.logger.debug(u"%s getting biblio for %s" % (self.provider_name, id))

        # set up stuff that is true for all blogs, wordpress and not
        biblio_dict = {}
        biblio_dict["url"] = id
        biblio_dict["account"] = provider.strip_leading_http(id)
        biblio_dict["is_account"] = True  # special key to tell webapp to render as genre heading

        # now add things that are true just for wordpress blogs

        if not provider_url_template:
            provider_url_template = self.biblio_url_template
        url = self._get_templated_url(provider_url_template, id, "biblio")

        # try to get a response from the data provider        
        response = self.http_get(url, cache_enabled=cache_enabled)

        if (response.status_code == 200) and ("name" in response.text):
            biblio_dict["hosting_platform"] = "wordpress.com"
            try:
                biblio_dict.update(self._extract_biblio(response.text, id))
            except (AttributeError, TypeError):
                pass

        return biblio_dict


    def _extract_biblio(self, page, id=None):
        dict_of_keylists = {
            'title' : ['name'],
            'description' : ['description']
        }
        biblio_dict = provider._extract_from_json(page, dict_of_keylists)
        return biblio_dict   


    def wordpress_post_id_from_nid(self, nid):
        try:
            return json.loads(nid)["wordpress_post_id"]    
        except (KeyError, ValueError):
            return None

    def blog_url_from_nid(self, nid):
        try:
            return json.loads(nid)["blog_url"]    
        except (KeyError, ValueError):
            return None


    # default method; providers can override    
    def provenance_url(self, metric_name, aliases):
        aliases_dict = provider.alias_dict_from_tuples(aliases)
        if "url" in aliases_dict:
            return aliases_dict["url"][0]
        else:
            return self.get_best_id(aliases)


    def metrics(self, 
            aliases,
            provider_url_template=None,
            cache_enabled=True, 
            analytics_credentials=None):

        metrics = {}

        aliases_dict = provider.alias_dict_from_tuples(aliases)
        if "blog" in aliases_dict:
            blog_url = aliases_dict["blog"][0]

            url_override = self.metrics_url_template_public % (provider.strip_leading_http(blog_url).lower())

            new_metrics = self.get_metrics_for_id(blog_url,
                                cache_enabled=cache_enabled, 
                                extract_metrics_method=self._extract_metrics_subscribers,
                                url_override=url_override)
            metrics.update(new_metrics)

        if "wordpress_blog_id" in aliases_dict:
            wordpress_blog_id = aliases_dict["wordpress_blog_id"][0]

            url_override = self.metrics_url_template_comments % wordpress_blog_id

            new_metrics = self.get_metrics_for_id(blog_url,
                                cache_enabled=cache_enabled, 
                                extract_metrics_method=self._extract_metrics_blog_comments,
                                url_override=url_override)
            metrics.update(new_metrics)


        if ("blog" in aliases_dict) and analytics_credentials:
            blog_url = aliases_dict["blog"][0]
            api_key = analytics_credentials["wordpress_api_key"]

            url_override = self.metrics_url_template_wordpress_blog_views % (api_key, provider.strip_leading_http(blog_url).lower())

            new_metrics = self.get_metrics_for_id(blog_url,
                                cache_enabled=cache_enabled, 
                                extract_metrics_method=self._extract_metrics_blog_views,
                                url_override=url_override)

            metrics.update(new_metrics)

        if ("wordpress_blog_post" in aliases_dict):
            nid = aliases_dict["wordpress_blog_post"][0]
            post_id = self.wordpress_post_id_from_nid(nid)
            blog_url = self.blog_url_from_nid(nid)

            url_override = self.metrics_url_template_wordpress_post_comments % (provider.strip_leading_http(blog_url).lower(), post_id)
            new_metrics = self.get_metrics_for_id(post_id,
                                cache_enabled=cache_enabled, 
                                extract_metrics_method=self._extract_metrics_post_comments,
                                url_override=url_override)
            metrics.update(new_metrics)

            if analytics_credentials:
                api_key = analytics_credentials["wordpress_api_key"]

                url_override = self.metrics_url_template_wordpress_post_views % (api_key, provider.strip_leading_http(blog_url).lower(), post_id)
                new_metrics = self.get_metrics_for_id(blog_url,
                                    cache_enabled=cache_enabled, 
                                    extract_metrics_method=self._extract_metrics_blog_views,
                                    url_override=url_override)
                metrics.update(new_metrics)


        metrics_and_drilldown = {}
        for metric_name in metrics:
            drilldown_url = self.provenance_url(metric_name, aliases)
            metrics_and_drilldown[metric_name] = (metrics[metric_name], drilldown_url)

        return metrics_and_drilldown 



    def _extract_metrics_subscribers(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not "is_private" in page:
            raise ProviderContentMalformedError

        dict_of_keylists = {
            'wordpresscom:subscribers' : ['subscribers_count']
        }

        metrics_dict = provider._extract_from_json(page, dict_of_keylists)
        return metrics_dict


    def _extract_metrics_blog_views(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not "views" in page:
            raise ProviderContentMalformedError

        dict_of_keylists = {
            'wordpresscom:views' : ['views']
        }

        metrics_dict = provider._extract_from_json(page, dict_of_keylists)        
        return metrics_dict


    def _extract_metrics_blog_comments(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not "found" in page:
            raise ProviderContentMalformedError

        dict_of_keylists = {
            'wordpresscom:comments' : ['found']
        }

        metrics_dict = provider._extract_from_json(page, dict_of_keylists)        
        return metrics_dict


    def _extract_metrics_post_comments(self, page, status_code=200, id=None):
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not "comment_count" in page:
            raise ProviderContentMalformedError

        dict_of_keylists = {
            'wordpresscom:comments' : ['comment_count']
        }

        metrics_dict = provider._extract_from_json(page, dict_of_keylists)        
        return metrics_dict


########NEW FILE########
__FILENAME__ = youtube
from totalimpact.providers import provider
from totalimpact.providers.provider import Provider, ProviderContentMalformedError

import simplejson, os, re

import logging
logger = logging.getLogger('ti.providers.youtube')

class Youtube(Provider):  

    example_id = ("youtube", "http://www.youtube.com/watch?v=d39DL4ed754")

    url = "http://youtube.com"
    descr = "YouTube allows billions of people to discover, watch and share originally-created videos"
    biblio_url_template = "https://www.googleapis.com/youtube/v3/videos?id=%s&part=snippet,statistics&key=" + os.environ["YOUTUBE_KEY"]
    aliases_url_template = "https://www.googleapis.com/youtube/v3/videos?id=%s&part=snippet,statistics&key=" + os.environ["YOUTUBE_KEY"]
    metrics_url_template = "https://www.googleapis.com/youtube/v3/videos?id=%s&part=snippet,statistics&key=" + os.environ["YOUTUBE_KEY"]
    provenance_url_template = "http://www.youtube.com/watch?v=%s"

    static_meta_dict = {
        "views": {
            "display_name": "views",
            "provider": "YouTube",
            "provider_url": "http://youtube.com",
            "description": "The number of people who have viewed the video",
            "icon": "http://www.youtube.com/favicon.ico",
        },
        "likes": {
            "display_name": "likes",
            "provider": "YouTube",
            "provider_url": "http://youtube.com",
            "description": "The number of people who have 'liked' the video",
            "icon": "http://www.youtube.com/favicon.ico",
            },
        "dislikes": {
            "display_name": "dislikes",
            "provider": "YouTube",
            "provider_url": "http://youtube.com",
            "description": "The number of people who have who have 'disliked' the video",
            "icon": "http://www.youtube.com/favicon.ico",
            },
        "favorites": {
            "display_name": "favorites",
            "provider": "YouTube",
            "provider_url": "http://youtube.com",
            "description": "The number of people who have marked the video as a favorite",
            "icon": "http://www.youtube.com/favicon.ico",
            },
        "comments": {
            "display_name": "comments",
            "provider": "YouTube",
            "provider_url": "http://youtube.com",
            "description": "The number of comments on a video",
            "icon": "http://www.youtube.com/favicon.ico",
            }
    }     

    def __init__(self):
        super(Youtube, self).__init__()

    def is_relevant_alias(self, alias):
        (namespace, nid) = alias
        if (("url" == namespace) and ("youtube.com/" in nid)):
            nid_as_youtube_url = self._get_video_id(nid)
            if nid_as_youtube_url:
                return True
        return False

    def _get_video_id(self, video_url):
        match = re.findall("watch.*[\?|&]v=([\dA-Za-z_\-]+)", video_url)
        try:
            nid_as_youtube_url = match[0]
        except IndexError:
            nid_as_youtube_url = None
            logging.error(u"couldn't get video_id for {video_url}".format(
                video_url=video_url))
        return nid_as_youtube_url

    #override because need to break up id
    def _get_templated_url(self, template, nid_as_youtube_url, method=None):
        nid_as_video_id = self._get_video_id(nid_as_youtube_url)
        if not nid_as_video_id:
            raise ProviderContentMalformedError
        url = template % (nid_as_video_id)
        return(url)

    def _extract_biblio(self, page, id=None):

        if not "snippet" in page:
            raise ProviderContentMalformedError

        json_response = provider._load_json(page)
        this_video_json = json_response["items"][0]

        dict_of_keylists = {
            'title': ['snippet', 'title'],
            'channel_title': ['snippet', 'channelTitle'],
            'published_date': ['snippet', 'publishedAt']
        }

        biblio_dict = provider._extract_from_data_dict(this_video_json, dict_of_keylists)

        try:
            biblio_dict["year"] = biblio_dict["published_date"][0:4]
        except KeyError:
            pass

        biblio_dict["url"] = id
        biblio_dict["repository"] = "YouTube"

        return biblio_dict    


    def _extract_metrics(self, page, status_code=200, id=None):        
        if status_code != 200:
            if status_code == 404:
                return {}
            else:
                raise(self._get_error(status_code))

        if not "snippet" in page:
            raise ProviderContentMalformedError

        json_response = provider._load_json(page)
        this_video_json = json_response["items"][0]

        dict_of_keylists = {
            'youtube:views' : ['statistics', 'viewCount'],
            'youtube:likes' : ['statistics', 'likeCount'],
            'youtube:dislikes' : ['statistics', 'dislikeCount'],
            'youtube:favorites' : ['statistics', 'favoriteCount'],
            'youtube:comments' : ['statistics', 'commentCount'],
        }

        metrics_dict = provider._extract_from_data_dict(this_video_json, dict_of_keylists)

        metrics_dict = provider._metrics_dict_as_ints(metrics_dict)

        return metrics_dict

########NEW FILE########
__FILENAME__ = provider_batch_data
import datetime, json, re, copy
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm.exc import FlushError
from totalimpact import db
from totalimpact import json_sqlalchemy

import logging
logger = logging.getLogger('ti.provider_batch_data')


class ProviderBatchData(db.Model):
    provider = db.Column(db.Text, primary_key=True)
    min_event_date = db.Column(db.DateTime(), primary_key=True)
    max_event_date = db.Column(db.DateTime())
    raw = db.Column(db.Text)
    aliases = db.Column(json_sqlalchemy.JSONAlchemy(db.Text))
    provider_raw_version = db.Column(db.Numeric)
    created = db.Column(db.DateTime())

    def __init__(self, **kwargs):
        self.created = datetime.datetime.utcnow()
        super(ProviderBatchData, self).__init__(**kwargs)

    def __repr__(self):
        return '<ProviderBatchData {provider}, {min_event_date}, {len_aliases} aliases>'.format(
            provider=self.provider, 
            min_event_date=self.min_event_date, 
            len_aliases=sum([len(self.aliases[namespace]) for namespace in self.aliases]))

def create_objects_from_doc(doc):
    logger.debug(u"in create_objects_from_doc for {id}".format(
        id=doc["_id"]))        

    new_object = ProviderBatchData.query.filter_by(
        provider=doc["provider"], 
        min_event_date=doc["min_event_date"]).first()
    if not new_object:
        new_dict = copy.deepcopy(doc)
        del new_dict["_id"]
        if "_rev" in new_dict:
            del new_dict["_rev"]
        del new_dict["type"]
        new_object = ProviderBatchData(**new_dict)
    db.session.add(new_object)

    try:
        db.session.commit()
    except (IntegrityError, FlushError) as e:
        db.session.rollback()
        logger.warning(u"Fails Integrity check in create_objects_from_doc for {new_object}, rolling back.  Message: {message}".format(
            new_object=new_object, 
            message=e.message))  

    return new_object



########NEW FILE########
__FILENAME__ = testers
from totalimpact import fakes
import logging, time, random, string, datetime

# setup logging
class ContextFilter(logging.Filter):
    def filter(self, record):
        record.msg = "test '{name}': {msg}".format(
            name=self.test_name,
            msg=record.msg
        )
        return True

logger = logging.getLogger("ti.testers")

class CollectionTester(object):

    def test(self, method):
        start = time.time()
        interaction_name = ''.join(random.choice(string.ascii_lowercase) for x in range(5))

        # all log messages will have the name of the test.
        f = ContextFilter()
        f.test_name = interaction_name
        logger.addFilter(f)

        logger.info(u"{classname}.{action_type}('{interaction_name}') starting now".format(
            classname=self.__class__.__name__,
            action_type=method,
            interaction_name=interaction_name
        ))

        try:
            error_str = None
            result = getattr(self, method)(interaction_name)
        except Exception, e:
            error_str = e.__repr__()
            logger.exception(u"{classname}.{method}('{interaction_name}') threw an error: '{error_str}'".format(
                classname=self.__class__.__name__,
                method=method,
                interaction_name=interaction_name,
                error_str=error_str
            ))
            result = None

        end = time.time()
        elapsed = end - start
        logger.info(u"{classname}.{method}('{interaction_name}') finished in {elapsed} seconds.".format(
            classname=self.__class__.__name__,
            method=method,
            interaction_name=interaction_name,
            elapsed=round(elapsed, 2)
        ))

        # this is a dumb way to do the times; should be using time objects, not stamps
        report = {
            "start": datetime.datetime.fromtimestamp(start).strftime('%m-%d %H:%M:%S'),
            "end": datetime.datetime.fromtimestamp(end).strftime('%m-%d %H:%M:%S'),
            "elapsed": round(elapsed, 2),
            "action": "collection." + method,
            "name": interaction_name,
            "result":result,
            "error_str": error_str
        }
        logger.info(u"{classname}.{method}('{interaction_name}') finished. Here's the report: {report}".format(
            classname=self.__class__.__name__,
            method=method,
            interaction_name=interaction_name,
            report=str(report)
        ))
        return report


    def create(self, interaction_name):
        ''' Imitates a user creating and viewing a collection.

        Should be run before commits. Is also run regularly on the production
        server. Would be better to walk through the actual pages with a headless
        browser, but they are so heavy on js, that seems very hard. Soo we use
        the fake pages to imitate the AJAX calls the js pages make.
        '''

        logger.debug(u"in the 'create' method now.")
        ccp = fakes.CreateCollectionPage()

        sampler = fakes.IdSampler()
        ccp.enter_aliases_directly([["doi", x] for x in sampler.get_dois(5)])
        ccp.get_aliases_with_importers("github", sampler.get_github_username())
        # include a paper known to be in the DB: it is in the official sample collection        
        ccp.enter_aliases_directly([["doi", "10.1186/1471-2148-9-37"]])
        logger.info(u"all aliases in collection {aliases}".format(aliases=str(ccp.aliases)))

        ccp.set_collection_name(interaction_name)

        return ccp.press_go_button()

    def read(self, interaction_name, collection_name="kn5auf"):
        '''Imitates a user viewing the sample collection.

        This method is useful for testing, and should be run before commits.
        However, in production we use StillAlive to actually load and check the
        report page using a headless browswer, which is better than this
        simulation.
        '''
        logger.debug(u"in the 'read' method now.")
        report_page = fakes.ReportPage(collection_name)
        result = report_page.poll()
        return result

    def update(self, interaction_name, collection_name="kn5auf"):
        '''Imitates a user updating a collection

        Not implemented yet because isn't as common or important.
        '''

        pass

    def delete(self, interaction_name):
        '''Imitates a user updating a collection

        Listed for CRUD completeness, but don't think we need this.
        '''
        pass

########NEW FILE########
__FILENAME__ = tiredis
import redis, logging, json, datetime, os, iso8601
from collections import defaultdict

from totalimpact.providers.provider import ProviderFactory


logger = logging.getLogger("ti.tiredis")

def from_url(url, db=0):
    r = redis.from_url(url, db)
    return r

def set_hash_value(self, key, hash_key, value, time_to_expire, pipe=None):
    if not pipe:
        pipe = self
    json_value = json.dumps(value)
    pipe.hset(key, hash_key, json_value)
    pipe.expire(key, time_to_expire)

def get_hash_value(self, key, hash_key):
    try:
        json_value = self.hget(key, hash_key)
        value = json.loads(json_value)
    except TypeError:
        value = None
    return value

def get_all_hash_values(self, key):
    return self.hgetall(key)


def delete_hash_key(self, key, hash_key):
    return self.hdel(key, hash_key)


# don't use in production
def clear_currently_updating_status(self):
    # delete currently updating things, to start fresh
    currently_updating_keys = self.keys("currently_updating:*")
    for key in currently_updating_keys:
        self.delete(key)

def set_currently_updating(self, tiid, provider_name, value, pipe=None):
    if not pipe:
        pipe = self

    key = "currently_updating:{tiid}".format(
        tiid=tiid)
    expire = 60*60*24  # for a day    
    pipe.set_hash_value(key, provider_name, value, expire, pipe)

def get_currently_updating(self, tiid, provider_name):
    key = "currently_updating:{tiid}".format(
        tiid=tiid)
    return self.get_hash_value(key, provider_name)

def delete_currently_updating(self, tiid, provider_name):
    key = "currently_updating:{tiid}".format(
        tiid=tiid)
    return self.delete_hash_key(key, provider_name)


def init_currently_updating_status(self, tiids, providers):
    pipe = self.pipeline()    

    for tiid in tiids:
        # logger.debug(u"set_all_providers for '{tiid}'.".format(
        #     tiid=tiid))
        now = datetime.datetime.utcnow().isoformat()
        for provider_name in providers:
            currently_updating_status = {now: "in queue"}
            self.set_currently_updating(tiid, provider_name, currently_updating_status, pipe)
    pipe.execute()


def set_provider_started(self, item_id, provider_name):
    now = datetime.datetime.utcnow().isoformat()
    currently_updating_status = {now: "started"}
    self.set_currently_updating(item_id, provider_name, currently_updating_status)
    # logger.info(u"set_provider_started for %s %s" % (
    #     item_id, provider_name))


def set_provider_finished(self, item_id, provider_name):
    self.delete_currently_updating(item_id, provider_name)
    # logger.info(u"set_provider_finished for {tiid} {provider_name}".format(
    #     tiid=item_id, provider_name=provider_name))


def get_providers_currently_updating(self, item_id):
    key = "currently_updating:{tiid}".format(
        tiid=item_id)
    providers_currently_updating = self.get_all_hash_values(key)
    return providers_currently_updating


def get_num_providers_currently_updating(self, item_id):
    providers_currently_updating = self.get_providers_currently_updating(item_id)
    num_currently_updating = 0
    if not providers_currently_updating:
        # logger.info(u"In get_num_providers_currently_updating, no providers_currently_updating for {tiid}".format(
        #     tiid=item_id))
        pass
    for provider in providers_currently_updating:
        value = json.loads(providers_currently_updating[provider])
        last_update_time = iso8601.parse_date(value.keys()[0])
        now = datetime.datetime.utcnow()
        # from http://stackoverflow.com/questions/796008/cant-subtract-offset-naive-and-offset-aware-datetimes
        elapsed = now - last_update_time.replace(tzinfo=None)
        if elapsed < datetime.timedelta(hours=0, minutes=5):
            num_currently_updating += 1 
            # logger.warning(u"In get_num_providers_currently_updating, elapsed time still short, set currently_updating=True for {tiid}{provider}".format(
            #     tiid=item_id, provider=provider))
        else:
            # logger.warning(u"In get_num_providers_currently_updating, elapsed time is too long, set currently_updating=False for {tiid}{provider}".format(
            #     tiid=item_id, provider=provider))
            pass

    return num_currently_updating


def add_to_alias_queue(self, dicts_to_add, analytics_credentials={}, priority="high", alias_providers_already_run=[]):
    queue_name = "aliasqueue_" + priority
    pipe = self.pipeline()    
    for message_dict in dicts_to_add:
        message = json.dumps({
                "tiid": message_dict["tiid"], 
                "aliases_dict": message_dict["aliases_dict"],
                "analytics_credentials": analytics_credentials,
                "alias_providers_already_run": alias_providers_already_run
            })
        logger.debug(u"Adding to alias_queue {queue_name}: /biblio_print {message}".format(
            queue_name=queue_name, message=message))
        pipe.lpush(queue_name, message)
    pipe.execute()
    queue_length = self.llen(queue_name)       
    logger.info(u">>>PUSHING to redis queue {queue_name}, current length {queue_length}".format(
        queue_name=queue_name, queue_length=queue_length)) 



def set_value(self, key, value, time_to_expire):
    json_value = json.dumps(value)
    self.set(key, json_value)
    self.expire(key, time_to_expire)

def get_value(self, key):
    try:
        json_value = self.get(key)
        value = json.loads(json_value)
    except TypeError:
        value = None
    return value



def set_memberitems_status(self, memberitems_key, query_status):
    key = "memberitems:"+memberitems_key 
    expire = 60*60*24  # for a day    
    self.set_value(key, query_status, expire)

def get_memberitems_status(self, memberitems_key):
    key = "memberitems:"+memberitems_key 
    value = self.get_value(key)
    return value

def set_confidence_interval_table(self, size, level, table):
    key = "confidence_interval_table:{size},{level}".format(
        size=size, level=level)
    expire = 60*60*24*7  # for a week
    self.set_value(key, table, expire)

def get_confidence_interval_table(self, size, level):
    key = "confidence_interval_table:{size},{level}".format(
        size=size, level=level)
    value = self.get_value(key)
    return value

def set_reference_histogram_dict(self, genre, refset_name, year, table):
    key = "refset_histogram:{genre},{refset_name},{year}".format(
        genre=genre, refset_name=refset_name, year=year)
    expire = 60*60*24  # for a day    
    self.set_value(key, table, expire)

def get_reference_histogram_dict(self, genre, refset_name, year):
    key = "refset_histogram:{genre},{refset_name},{year}".format(
        genre=genre, refset_name=refset_name, year=year)
    value = self.get_value(key)
    return value

def set_reference_lookup_dict(self, genre, refset_name, year, table):
    key = "refset_lookup:{genre},{refset_name},{year}".format(
        genre=genre, refset_name=refset_name, year=year)
    expire = 60*60*24  # for a day    
    self.set_value(key, table, expire)

def get_reference_lookup_dict(self, genre, refset_name, year):
    key = "refset_lookup:{genre},{refset_name},{year}".format(
        genre=genre, refset_name=refset_name, year=year)
    value = self.get_value(key)
    return value



redis.Redis.get_hash_value = get_hash_value
redis.Redis.get_all_hash_values = get_all_hash_values
redis.Redis.set_hash_value = set_hash_value
redis.Redis.delete_hash_key = delete_hash_key

redis.Redis.set_value = set_value
redis.Redis.get_value = get_value
redis.Redis.set_currently_updating = set_currently_updating
redis.Redis.get_currently_updating = get_currently_updating
redis.Redis.delete_currently_updating = delete_currently_updating
redis.Redis.clear_currently_updating_status = clear_currently_updating_status
redis.Redis.init_currently_updating_status = init_currently_updating_status
redis.Redis.set_provider_started = set_provider_started
redis.Redis.set_provider_finished = set_provider_finished
redis.Redis.get_providers_currently_updating = get_providers_currently_updating
redis.Redis.get_num_providers_currently_updating = get_num_providers_currently_updating
redis.Redis.add_to_alias_queue = add_to_alias_queue
redis.Redis.set_memberitems_status = set_memberitems_status
redis.Redis.get_memberitems_status = get_memberitems_status
redis.Redis.set_confidence_interval_table = set_confidence_interval_table
redis.Redis.get_confidence_interval_table = get_confidence_interval_table
redis.Redis.set_reference_histogram_dict = set_reference_histogram_dict
redis.Redis.get_reference_histogram_dict = get_reference_histogram_dict
redis.Redis.set_reference_lookup_dict = set_reference_lookup_dict
redis.Redis.get_reference_lookup_dict = get_reference_lookup_dict



########NEW FILE########
__FILENAME__ = unicode_helpers
import unicodedata
import logging

logger = logging.getLogger('ti.unicode_helpers')

#from http://farmdev.com/talks/unicode/
def to_unicode_or_bust(obj, encoding='utf-8'):
    if isinstance(obj, basestring):
        if not isinstance(obj, unicode):
            obj = unicode(obj, encoding)
    return obj


def printing_character_or_space(c):
    is_printing_character_or_space = True
    # see http://www.fileformat.info/info/unicode/category/index.htm
    char_classes_to_remove = ["C", "M", "Z"]
    if unicodedata.category(c)[0] in char_classes_to_remove:
        if c != " ":
            is_printing_character_or_space = False
    return is_printing_character_or_space


def remove_nonprinting_characters(input, encoding='utf-8'):
    input_was_unicode = True
    if isinstance(input, basestring):
        if not isinstance(input, unicode):
            input_was_unicode = False

    unicode_input = to_unicode_or_bust(input)


    response = u''.join(c for c in unicode_input if printing_character_or_space(c))

    if not input_was_unicode:
        response = response.encode(encoding)
        
    return response


########NEW FILE########
__FILENAME__ = updater
#!/usr/bin/env python
import argparse
import logging, os, sys, random, datetime, time, collections
import requests
from sqlalchemy.sql import text    

from totalimpact import tiredis
from totalimpact import app, db
from totalimpact import item as item_module

logger = logging.getLogger('ti.updater')
logger.setLevel(logging.DEBUG)

# run in heroku by a) commiting, b) pushing to heroku, and c) running
# heroku run python totalimpact/updater.py


def update_by_tiids(all_tiids, myredis):
    for tiid in all_tiids:
        item_obj = item_module.Item.query.get(tiid)  # can use this method because don't need metrics
        item_doc = item_obj.as_old_doc()
        item_module.start_item_update([{"tiid": item_doc["_id"], "aliases_dict":item_doc["aliases"]}], {}, "low", myredis)
    return all_tiids


def set_last_update_run(all_tiids):
    now = datetime.datetime.utcnow().isoformat()
    for tiid in all_tiids:
        item_obj = item_module.Item.query.get(tiid)  # can use this method because don't need metrics
        try:
            item_obj.last_update_run = now
            db.session.add(item_obj)
        except AttributeError:
            logger.warning(u"no object found for tiid {tiid} when updating last_update_run".format(
                tiid=tiid))

    db.session.commit()
    return all_tiids


# create table registered_tiid as (select tiid, api_key
# from alias, registered_item
# where alias.nid=registered_item.nid and alias.namespace=registered_item.namespace
# )

def get_registered_tiids_not_updated_since(number_to_update, now=datetime.datetime.utcnow()):

    raw_sql = text("""SELECT tiid FROM item i
        WHERE last_update_run < now()::date - 7
        and tiid in 
        (select tiid
        from registered_tiid where api_key not in ('vanwijikc233acaa'))
        ORDER BY last_update_run DESC
        LIMIT :number_to_update""")

    result = db.session.execute(raw_sql, params={
        "number_to_update": number_to_update
        })
    tiids = [row["tiid"] for row in result]

    return tiids

def get_nids_for_altmetric_com_to_update(number_to_update):

    raw_sql = text("""SELECT i.tiid, namespace, nid, last_update_run 
        FROM item i, alias a, user_tiids_valid_users u
        WHERE last_update_run < now()::date - 2
        AND i.tiid = a.tiid
        AND i.tiid = u.tiid
        AND namespace in ('doi', 'arxiv', 'pmid')
        ORDER BY last_update_run ASC
        LIMIT :number_to_update""")

    result = db.session.execute(raw_sql, params={
        "number_to_update": number_to_update
        })
    #tiids = [row["tiid"] for row in result]

    return result.fetchall()


def get_altmetric_ids_from_nids(nids):
    nid_string = u"|".join(nids)
    #print "nid_string", nid_string

    headers = {u'content-type': u'application/x-www-form-urlencoded', 
                u'accept': u'application/json'}
    try:
        r = requests.post(u"http://api.altmetric.com/v1/translate?key=" + os.getenv("ALTMETRIC_COM_KEY"), 
                            data=u"ids="+nid_string, 
                            headers=headers)
        altmetric_ids_dict = r.json()
    except UnicodeEncodeError:
        print "UnicodeEncodeError"
        altmetric_ids_dict = []

    nids_by_altmetric_id = dict((str(altmetric_ids_dict[nid]), nid) for nid in altmetric_ids_dict)
    return nids_by_altmetric_id


def altmetric_com_ids_to_update(altmetric_ids):
    altmetric_ids_string = u",".join(altmetric_ids)
    #print "altmetric_ids_string", altmetric_ids_string

    headers = {u'content-type': u'application/x-www-form-urlencoded',
                u'accept': u'application/json'}

    r = requests.post(u"http://api.altmetric.com/v1/citations/1y?key=" + os.getenv("ALTMETRIC_COM_KEY"), 
                        data=u"citation_ids="+altmetric_ids_string, 
                        headers=headers)
    try:
        data = r.json()
        ids_with_changes = [str(entry["altmetric_id"]) for entry in data["results"]]    
    except ValueError:
        # says "Not Found" not in JSON if nothing found
        ids_with_changes = []
    return ids_with_changes


def tiids_from_altmetric_ids(altmetric_ids, nids_by_altmetric_id, tiids_by_nids):
    #print "altmetric_ids", altmetric_ids
    nids = [nids_by_altmetric_id[id] for id in altmetric_ids]
    tiids_nested = [tiids_by_nids[nid] for nid in nids]
    #print "tiids_nested", tiids_nested
    tiids = [tiid for inner_list in tiids_nested for tiid in inner_list]
    #print "tiids", tiids
    return tiids


def altmetric_com_update(number_to_update, myredis):
    candidate_tiid_rows = get_nids_for_altmetric_com_to_update(number_to_update)
    tiids_by_nids = collections.defaultdict(list)
    for row in candidate_tiid_rows:
        tiids_by_nids[row["nid"]] += [row["tiid"]]
    print "tiids_by_nids", tiids_by_nids
    all_tiids = [row["tiid"] for row in candidate_tiid_rows]
    #print "all_tiids", all_tiids

    nids = tiids_by_nids.keys()
    if not nids:
        logger.info("no items to update")
        return []
    #print "nids", nids
    nids_by_altmetric_id = get_altmetric_ids_from_nids(nids)
    print "nids_by_altmetric_id", nids_by_altmetric_id

    altmetric_ids = nids_by_altmetric_id.keys()
    #print "altmetric_ids", altmetric_ids

    if altmetric_ids:
        tiids_with_altmetric_ids = tiids_from_altmetric_ids(altmetric_ids, nids_by_altmetric_id, tiids_by_nids)
        updated_tiids = update_by_tiids(tiids_with_altmetric_ids, myredis)

        # altmetric_ids_with_changes = altmetric_com_ids_to_update(altmetric_ids, nids_by_altmetric_id, tiids_by_nids)
        # tiids_with_changes = tiids_from_altmetric_ids(altmetric_ids_with_changes)
        # updated_tiids = update_by_tiids(tiids_with_changes, myredis)


    if all_tiids:
        set_last_update_run(all_tiids)
    return({"len_candidate_tiid_rows":len(candidate_tiid_rows), "len_altmetric_ids":len(altmetric_ids)})


def gold_update(number_to_update, myredis, now=datetime.datetime.utcnow()):
    tiids = get_registered_tiids_not_updated_since(number_to_update, now)
    updated_tiids = update_by_tiids(tiids, myredis)
    return updated_tiids


def main(action_type, number_to_update=35, number_loops=400, seconds_wait=10):
    #35 every 10 minutes is 35*6perhour*24hours=5040 per day

    redis_url = os.getenv("REDIS_URL")

    myredis = tiredis.from_url(redis_url)
    print u"running " + action_type

    try:
        if action_type == "gold_update":
            tiids = gold_update(number_to_update, myredis)
        elif action_type == "altmetric_com":
            for i in range(0, number_loops):
                print "another loop of altmetric_com_update", i
                resp = altmetric_com_update(number_to_update, myredis)
                print "updating", resp["len_altmetric_ids"]
                print "sleeping to pause, for", seconds_wait * resp["len_altmetric_ids"], "seconds"
                time.sleep(seconds_wait * resp["len_altmetric_ids"])
                if not resp["len_candidate_tiid_rows"]:
                    raise SystemExit
    except (KeyboardInterrupt, SystemExit): 
        # this approach is per http://stackoverflow.com/questions/2564137/python-how-to-terminate-a-thread-when-main-program-ends
        sys.exit()
 
if __name__ == "__main__":

    # get args from the command line:
    parser = argparse.ArgumentParser(description="Run periodic metrics updating from the command line")
    parser.add_argument("action_type", type=str, help="The action to test; available actions are 'gold_update' (that's all right now)")
    parser.add_argument('--number_to_update', default='35', type=int, help="Number to update.")
    parser.add_argument('--number_loops', default='1', type=int, help="Number loops.")
    parser.add_argument('--seconds_wait', default='10', type=int, help="Number seconds wait per altmetrics_id.")
    args = vars(parser.parse_args())
    print args
    print "updater.py starting."
    main(args["action_type"], args["number_to_update"], number_loops=args["number_loops"], seconds_wait=args["seconds_wait"])



########NEW FILE########
__FILENAME__ = utils
import time
import logging

logger = logging.getLogger('ti.utils')

class Retry(object):
    default_exceptions = (Exception,)
    def __init__(self, tries, exceptions=None, delay=0):
        """
        Decorator for retrying a function if exception occurs

        tries -- num tries
        exceptions -- exceptions to catch
        delay -- wait between retries
        from http://peter-hoffmann.com/2010/retry-decorator-python.html
        """
        self.tries = tries
        if exceptions is None:
            exceptions = Retry.default_exceptions
        self.exceptions =  exceptions
        self.delay = delay

    def __call__(self, f):
        def fn(*args, **kwargs):
            exception = None
            for _ in range(self.tries):
                try:
                    return f(*args, **kwargs)
                except self.exceptions, e:
                    logger.debug(u"Retry: "+e.__repr__())
                    time.sleep(self.delay)
                    exception = e

            #if no success after tries, could raise last exception here...

            logger.debug(u"Tried mightily, but giving up after {tries} '{exception_type}' exceptions.".format(
                tries=self.tries,
                exception_type=e.__repr__()
            ))

            return False # fail silently...
        return fn


########NEW FILE########
__FILENAME__ = views
from flask import json, request, abort, make_response, g
from flask import render_template
import sys, os
import datetime, re, copy
from werkzeug.security import check_password_hash
from collections import defaultdict
import redis
import analytics
import requests

from totalimpact import app, tiredis, collection, incoming_email
from totalimpact import item as item_module
from totalimpact.models import MemberItems, NotAuthenticatedError
from totalimpact.providers import provider as provider_module
from totalimpact.providers.provider import ProviderFactory, ProviderItemNotFoundError, ProviderError, ProviderServerError, ProviderTimeout
from totalimpact import unicode_helpers
from totalimpact import default_settings
import logging


logger = logging.getLogger("ti.views")
logger.setLevel(logging.DEBUG)

mydao = None
myredis = tiredis.from_url(os.getenv("REDIS_URL"), db=0)  # main app is on DB 0

logger.debug(u"Building reference sets")
myrefsets = None
myrefsets_histograms = None
try:
    (myrefsets, myrefsets_histograms) = collection.build_all_reference_lookups(myredis, mydao)
    logger.debug(u"Reference sets dict has %i keys" %len(myrefsets.keys()))
except (LookupError, AttributeError), e:
    logger.error(u"Exception %s: Unable to load reference sets" % (e.__repr__()))

def set_db(url, db):
    """useful for unit testing, where you want to use a local database
    """
    global mydao 
    mydao = None
    return mydao

def set_redis(url, db):
    """useful for unit testing, where you want to use a local database
    """
    global myredis 
    myredis = tiredis.from_url(url, db)
    return myredis


def is_valid_key(key):
    internal_keys = ["yourkey", "samplekey", "item-report-page", "api-docs", os.getenv("API_KEY").lower(), os.getenv("API_ADMIN_KEY").lower()]
    is_valid_internal_key = key.lower() in internal_keys
    return is_valid_internal_key

def check_key():
    if "/v1/" in request.url:
        api_key = request.values.get('key', '')
        if not api_key:
            api_key = request.args.get("api_admin_key", "")
        if not is_valid_key(api_key):
            abort_custom(403, "You must include key=YOURKEY in your query.  Contact team@impactstory.org for a valid api key.")
    return # if success don't return any content


def abort_custom(status_code, msg):
    body_dict = {
        "HTTP_status_code": status_code,
        "message": msg,
        "error": True
    }
    if request.args.get("callback"):
        status_code = 200  # JSONP can't deal with actual errors, it needs something back
        resp_string = "{callback_name}( {resp} )".format(
            callback_name=request.args.get("callback"),
            resp=json.dumps(body_dict)
        )
    else:
        resp_string = json.dumps(body_dict, sort_keys=True, indent=4)

    resp = make_response(resp_string, status_code)
    resp.mimetype = "application/json"
    abort(resp)

def check_mimetype():
    g.return_as_html = False
    if request.path.endswith(".html"):
        g.return_as_html = True
        request.path = request.path.replace(".html", "")



@app.before_request
def before_request():
    check_key()
    check_mimetype()


@app.after_request
def add_crossdomain_header(resp):
    #support CORS    
    resp.headers['Access-Control-Allow-Origin'] = "*"
    resp.headers['Access-Control-Allow-Methods'] = "POST, GET, OPTIONS, PUT, DELETE"
    resp.headers['Access-Control-Allow-Headers'] = "origin, content-type, accept, x-requested-with"
    return resp

@app.after_request
def set_mimetype_and_encoding(resp):
    resp.headers.add("Content-Encoding", "UTF-8")

    if "csv" in resp.mimetype:
        return resp
    if "static" in request.path:
        return resp

    try:
        if g.return_as_html:
            logger.info(u"rendering output through debug_api.html template")
            resp.mimetype = "text/html"
            return make_response(render_template(
                'debug_api.html',
                data=resp.data))
    except AttributeError:
        pass

    # logger.info(u"rendering output as json")
    resp.mimetype = "application/json"
    return resp


# adding a simple route to confirm working API
@app.route('/')
@app.route('/v1')
def hello():
    msg = {
        "hello": "world",
        "message": "Congratulations! You have found the ImpactStory API.",
        "more-info": "http://impactstory.org/api-docs",
        "contact": "team@impactstory.org",
        "version": app.config["VERSION"]
    }
    resp = make_response(json.dumps(msg, sort_keys=True, indent=4), 200)
    return resp


@app.route('/v1/item/<namespace>/<path:nid>', methods=['POST'])
def item_namespace_post(namespace, nid):
    abort_custom(410, "no longer supported")


@app.route('/v1/item/<tiid>', methods=['GET'])
def get_item_from_tiid(tiid, format=None, include_history=False, callback_name=None):
    try:
        item = item_module.get_item(tiid, myrefsets, myredis)
    except (LookupError, AttributeError):
        abort_custom(404, "item does not exist")

    if not item:
        abort_custom(404, "item does not exist")

    if item_module.is_currently_updating(tiid, myredis):
        response_code = 210 # not complete yet
        item["currently_updating"] = True
    else:
        response_code = 200
        item["currently_updating"] = False

    api_key = request.args.get("key", None)
    clean_item = item_module.clean_for_export(item, api_key, os.getenv("API_ADMIN_KEY"))
    clean_item["HTTP_status_code"] = response_code  # hack for clients who can't read real response codes

    resp_string = json.dumps(clean_item, sort_keys=True, indent=4)
    if callback_name is not None:
        resp_string = callback_name + '(' + resp_string + ')'

    resp = make_response(resp_string, response_code)

    return resp

@app.route('/v1/tiid/<namespace>/<path:nid>', methods=['GET'])
def get_tiid_from_namespace_nid(namespace, nid):
    tiid = item_module.get_tiid_by_alias(namespace, nid)
    if not tiid:
        abort_custom(404, "alias not in database")
    return make_response(json.dumps({"tiid": tiid}, sort_keys=True, indent=4), 200)


@app.route('/v1/item/<namespace>/<path:nid>', methods=['GET'])
def get_item_from_namespace_nid(namespace, nid, format=None, include_history=False):
    abort_custom(410, "no longer supported")



@app.route('/v1/provider', methods=['GET'])
def provider():
    ret = ProviderFactory.get_all_metadata()
    resp = make_response(json.dumps(ret, sort_keys=True, indent=4), 200)

    return resp



def format_into_products_dict(tiids_aliases_map):
    products_dict = {}
    for tiid in tiids_aliases_map:
        (ns, nid) = tiids_aliases_map[tiid]
        products_dict[tiid] = {"aliases": {ns: [nid]}}
    return products_dict



@app.route("/v1/importer/<provider_name>", methods=['POST'])
def importer_post(provider_name):
    try:
        analytics_credentials = request.json["analytics_credentials"]
        del request.json["analytics_credentials"]
    except KeyError:
        analytics_credentials = {}

    try:
        existing_tiids = request.json["existing_tiids"]
        del request.json["existing_tiids"]
    except KeyError:
        existing_tiids = []

    try:
        retrieved_aliases = provider_module.import_products(provider_name, request.json)
    except ImportError:
        abort_custom(404, "an importer for provider '{provider_name}' is not found".format(
            provider_name=provider_name))        
    except ProviderItemNotFoundError:
        abort_custom(404, "item not found")
    except ProviderItemNotFoundError:
        abort_custom(404, "item not found")
    except (ProviderTimeout, ProviderServerError):
        abort_custom(503, "timeout error, might be transient")
    except ProviderError:
        abort(500, "internal error from provider")

    new_aliases = item_module.aliases_not_in_existing_tiids(retrieved_aliases, existing_tiids)
    tiids_aliases_map = item_module.create_tiids_from_aliases(new_aliases, analytics_credentials, myredis, provider_name)
    logger.debug(u"in provider_importer_get with {tiids_aliases_map}".format(
        tiids_aliases_map=tiids_aliases_map))

    products_dict = format_into_products_dict(tiids_aliases_map)

    resp = make_response(json.dumps({"products": products_dict}, sort_keys=True, indent=4), 200)
    return resp


def abort_if_fails_collection_edit_auth(request):
    if request.args.get("api_admin_key"):
        supplied_key = request.args.get("api_admin_key", "")
        if os.getenv("API_KEY") == supplied_key:  #remove this once webapp sends admin_api_key
            return True
        if os.getenv("API_ADMIN_KEY") == supplied_key:
            return True
    abort_custom(403, "This collection has no update key; it can't be changed.")


def get_alias_strings(aliases):
    alias_strings = []
    for (namespace, nid) in aliases:
        namespace = item_module.clean_id(namespace)
        nid = item_module.clean_id(nid)
        try:
            alias_strings += [namespace+":"+nid]
        except TypeError:
            # jsonify the biblio dicts
            alias_strings += [namespace+":"+json.dumps(nid)]
    return alias_strings   



'''
GET /collection/:collection_ID
returns a collection object and the items
'''
@app.route('/v1/collection/<cid>', methods=['GET'])
@app.route('/v1/collection/<cid>.<format>', methods=['GET'])
def collection_get(cid='', format="json", include_history=False):
    logger.info(u"in collection_get".format(cid=cid))

    if (request.args.get("include_items") in ["0", "false", "False"]):
        coll = collection.get_collection_doc(cid)
        if not coll:
            abort_custom(404, "collection not found")

        # except if format is csv.  can't do that.
        if format == "csv":
            abort_custom(405, "csv method not supported for not include_items")
        else:
            response_code = 200
            resp = make_response(json.dumps(coll, sort_keys=True, indent=4),
                                 response_code)
    else:
        include_history = (request.args.get("include_history", 0) in ["1", "true", "True"])
        (coll_with_items, something_currently_updating) = collection.get_collection_with_items_for_client(cid, myrefsets, myredis, mydao, include_history)

        # return success if all reporting is complete for all items    
        if something_currently_updating:
            response_code = 210 # update is not complete yet
        else:
            response_code = 200

        if format == "csv":
            # remove scopus before exporting to csv, so don't add magic keep-scopus keys to clean method
            clean_items = [item_module.clean_for_export(item) for item in coll_with_items["items"]]
            csv = collection.make_csv_stream(clean_items)
            resp = make_response(csv, response_code)
            resp.mimetype = "text/csv;charset=UTF-8"
            resp.headers.add("Content-Disposition",
                             "attachment; filename=impactstory-{cid}.csv".format(
                                cid=cid))
            resp.headers.add("Content-Encoding",
                             "UTF-8")
        else:

            secret_key = os.getenv("API_ADMIN_KEY") 
            if request.args.get("api_admin_key"):
                supplied_key = request.args.get("api_admin_key", "")
            else:
                supplied_key = request.args.get("key", "")

            clean_if_necessary_items = [item_module.clean_for_export(item, supplied_key, secret_key)
                for item in coll_with_items["items"]]

            coll_with_items["items"] = clean_if_necessary_items
            resp = make_response(json.dumps(coll_with_items, sort_keys=True, indent=4),
                                 response_code)
    return resp


@app.route('/v1/collection/<cid>/items', methods=['POST'])
def delete_and_put_helper(cid=""):
    """
    Lets browsers who can't do PUT or DELETE fake it with a POST
    """
    http_method = request.args.get("http_method", "")
    if http_method.lower() == "delete":
        return remove_items_from_collection(cid)
    elif http_method.lower() == "put":
        return add_items_to_collection(cid)
    else:
        abort_custom(404, "You must specify a valid HTTP method (POST or PUT) with the"
                   " http_method argument.")


@app.route('/v1/collection/<cid>/items', methods=['DELETE'])
def remove_items_from_collection(cid=""):
    """
    Deletes items from a collection
    """
    abort_if_fails_collection_edit_auth(request)

    try:
        collection_object = collection.remove_items_from_collection(
            cid=cid, 
            tiids_to_delete=request.json["tiids"], 
            myredis=myredis, 
            mydao=mydao)
    except (AttributeError, TypeError, KeyError) as e:
        # we got missing or improperly formated data.
        logger.error(u"DELETE /collection/{id}/items threw an error: '{error_str}'. input: {json}.".format(
                id=cid,
                error_str=e,
                json=request.json))
        abort_custom(500, "Error deleting items from collection")

    (coll_doc, is_updating) = collection.get_collection_with_items_for_client(cid, myrefsets, myredis, mydao, include_history=False)
    resp = make_response(json.dumps(coll_doc, sort_keys=True, indent=4), 200)
    return resp


@app.route("/v1/collection/<cid>/items", methods=["PUT"])
def add_items_to_collection(cid=""):
    """
    Adds new items to a collection.
    """

    abort_if_fails_collection_edit_auth(request)

    try:
        if "tiids" in request.json:
            collection_object = collection.add_items_to_collection_object(
                    cid=cid, 
                    tiids=request.json["tiids"], 
                    alias_tuples=None)
        else:
            #to be depricated
            collection_object = collection.add_items_to_collection(
                cid=cid, 
                aliases=request.json["aliases"], 
                analytics_credentials={},
                myredis=myredis)
    except (AttributeError, TypeError) as e:
        # we got missing or improperly formated data.
        logger.error(u"PUT /collection/{id}/items threw an error: '{error_str}'. input: {json}.".format(
                id=cid,
                error_str=e,
                json=request.json))
        abort_custom(500, "Error adding items to collection")

    (coll_doc, is_updating) = collection.get_collection_with_items_for_client(cid, myrefsets, myredis, mydao, include_history=False)

    resp = make_response(json.dumps(coll_doc, sort_keys=True, indent=4), 200)

    return resp



def refresh_from_tiids(tiids, analytics_credentials, priority, myredis):
    item_objects = item_module.Item.query.filter(item_module.Item.tiid.in_(tiids)).all()
    dicts_to_refresh = []  

    for item_obj in item_objects:
        try:
            tiid = item_obj.tiid
            alias_dict = item_module.alias_dict_from_tuples(item_obj.alias_tuples)       
            dicts_to_refresh += [{"tiid":tiid, "aliases_dict": alias_dict}]
        except AttributeError:
            logger.debug(u"couldn't find tiid {tiid} so not refreshing its metrics".format(
                tiid=tiid))

    item_module.start_item_update(dicts_to_refresh, analytics_credentials, priority, myredis)
    return tiids


""" Refreshes all the items from tiids
    Depricate this one
"""
@app.route("/v1/products/<tiids_string>", methods=["POST"])
# not officially supported in api
def products_refresh_post_inline(tiids_string):
    tiids = tiids_string.split(",")
    try:
        analytics_credentials = request.json["analytics_credentials"]
    except KeyError:
        analytics_credentials = {}

    try:
        priority = request.json["priority"]
    except KeyError:
        priority = "high"

    refresh_from_tiids(tiids, analytics_credentials, priority, myredis)
    resp = make_response("true", 200)
    return resp


# refreshes items from tiids list in body of POST
@app.route('/v1/products/refresh', methods=['POST'])
def products_refresh_post():
    logger.debug(u"in products_refresh_post with tiids")
    tiids = request.json["tiids"]
    try:
        analytics_credentials = request.json["analytics_credentials"]
    except KeyError:
        analytics_credentials = {}

    try:
        priority = request.json["priority"]
    except KeyError:
        priority = "high"

    refresh_from_tiids(tiids, analytics_credentials, priority, myredis)
    resp = make_response("true", 200)    
    return resp


# sends back duplicate groups from tiids list in body of POST
@app.route('/v1/products/duplicates', methods=['POST'])
def products_duplicates_post():
    logger.debug(u"in products_duplicates_post with tiids")
    tiids = request.json["tiids"]
    duplicates_list = item_module.build_duplicates_list(tiids)
    resp = make_response(json.dumps({"duplicates_list": duplicates_list}, sort_keys=True, indent=4), 200)   
    return resp


""" Refreshes all the items in a given collection.
    Still useful for refsets!
"""
@app.route("/v1/collection/<cid>", methods=["POST"])
# not officially supported in api, though still useful for refsets
def collection_metrics_refresh(cid=""):
    # first, get the tiids in this collection:
    try:
        coll_doc = collection.get_collection_doc(cid)
        tiids = coll_doc["alias_tiids"].values()
    except (TypeError, AttributeError):
        logger.exception(u"couldn't get tiids in POST collection '{cid}'".format(
            cid=cid
        ))
        abort_custom(500, "Error doing collection_update")

    refresh_from_tiids(tiids, {}, "low", myredis)

    resp = make_response("true", 200)
    return resp



@app.route("/v1/collection/<cid>", methods=["DELETE"])
def delete_collection(cid=None):
    abort_custom(501, "Deleting collections is not currently supported.")



# creates products from aliases or returns items from tiids
@app.route('/v1/products', methods=['POST'])
@app.route('/v1/products.<format>', methods=['POST'])
def products_post(format="json"):
    if "aliases" in request.json:
        analytics_credentials = {}
        tiids_aliases_map = item_module.create_tiids_from_aliases(request.json["aliases"], analytics_credentials, myredis)
        products_dict = format_into_products_dict(tiids_aliases_map)
        response = make_response(json.dumps({"products": products_dict}, sort_keys=True, indent=4), 200)
        return response
    elif "tiids" in request.json:
        # overloading post for get because tiids string gets long
        logger.debug(u"in products_post with tiids, so getting products to return")
        tiids = request.json["tiids"]
        tiids_string = ",".join(tiids)
        try:
            most_recent_metric_date = request.json["most_recent_metric_date"]
            most_recent_diff_metric_date = request.json["most_recent_diff_metric_date"]
        except KeyError:
            most_recent_metric_date = None
            most_recent_diff_metric_date = None

        return products_get(tiids_string, format, most_recent_metric_date, most_recent_diff_metric_date)
    else:
        abort_custom(400, "bad arguments")


def cleaned_items(tiids, myredis, override_export_clean=False, most_recent_metric_date=None, most_recent_diff_metric_date=None):
    items_dict = collection.get_items_for_client(tiids, myrefsets, myredis, most_recent_metric_date, most_recent_diff_metric_date)

    secret_key = os.getenv("API_ADMIN_KEY")
    supplied_key = request.args.get("api_admin_key", "")
    cleaned_items_dict = {}
    for tiid in items_dict:
        cleaned_items_dict[tiid] = item_module.clean_for_export(items_dict[tiid], supplied_key, secret_key, override_export_clean)
    return cleaned_items_dict


# returns a product from a tiid
@app.route('/v1/product/<tiid>', methods=['GET'])
def single_product_get(tiid):
    cleaned_items_dict = cleaned_items([tiid], myredis)
    try:
        single_item = cleaned_items_dict[tiid]
    except TypeError:
        abort_custom(404, "No product found with that tiid")

    response_code = 200
    if collection.is_something_currently_updating(cleaned_items_dict, myredis):
        response_code = 210 # update is not complete yet

    resp = make_response(json.dumps(single_item, sort_keys=True, indent=4),
                         response_code)

    return resp


# returns products from tiids
@app.route('/v1/products/<tiids_string>', methods=['GET'])
@app.route('/v1/products.<format>/<tiids_string>', methods=['GET'])
def products_get(tiids_string, format="json", most_recent_metric_date=None, most_recent_diff_metric_date=None):
    tiids = tiids_string.split(",")
    override_export_clean = (format=="csv")
    cleaned_items_dict = cleaned_items(tiids, myredis, override_export_clean, most_recent_metric_date, most_recent_diff_metric_date)

    response_code = 200
    if collection.is_something_currently_updating(cleaned_items_dict, myredis):
        response_code = 210 # update is not complete yet

    if format == "csv":
        csv = collection.make_csv_stream(cleaned_items_dict.values())
        resp = make_response(csv, response_code)
        resp.mimetype = "text/csv;charset=UTF-8"
        resp.headers.add("Content-Encoding", "UTF-8")
    else:
        resp = make_response(json.dumps({"products": cleaned_items_dict}, sort_keys=True, indent=4),
                             response_code)

    return resp




# creates a collection from aliases or tiids
@app.route('/v1/collection', methods=['POST'])
def collection_create():
    """
    POST /collection
    creates new collection
    """
    response_code = None
    try:
        cid = request.args.get("collection_id", collection._make_id())
        if "tiids" in request.json:
            (coll_doc, collection_object) = collection.create_new_collection_from_tiids(
                cid=cid, 
                title=request.json.get("title", "my collection"), 
                tiids=request.json.get("tiids"), 
                ip_address=request.remote_addr, 
                refset_metadata=request.json.get("refset_metadata", None))
        else:
            # to be depricated
            (coll_doc, collection_object) = collection.create_new_collection(
                cid=cid, 
                title=request.json.get("title", "my collection"), 
                aliases=request.json["aliases"], 
                ip_address=request.remote_addr, 
                refset_metadata=request.json.get("refset_metadata", None), 
                myredis=myredis, 
                mydao=mydao)
    except (AttributeError, TypeError):
        # we got missing or improperly formated data.
        logger.error(u"we got missing or improperly formated data: '{cid}' with {json}.".format(
                cid=cid,
                json=str(request.json)))
        abort_custom(404, "Missing arguments.")

    response_code = 201 # Created
    resp = make_response(json.dumps({"collection":coll_doc},
            sort_keys=True, indent=4), response_code)
    return resp


@app.route("/v1/product/<tiid>/biblio", methods=["PATCH"])
def product_biblio_modify(tiid):
    data = request.json
    for biblio_field_name in data:
        item = item_module.add_biblio(tiid, biblio_field_name, data[biblio_field_name])
    response = {"product": item.as_old_doc()}
    return make_response(json.dumps(response, indent=4), 200)



# for internal use only
@app.route('/test/collection/<action_type>', methods=['GET'])
def tests_interactions(action_type=''):
    logger.info(u"getting test/collection/" + action_type)

    report = myredis.hgetall("test.collection." + action_type)
    report["url"] = "http://{root}/collection/{collection_id}".format(
        root=os.getenv("WEBAPP_ROOT"),
        collection_id=report["result"]
    )

    return render_template(
        'interaction_test_report.html',
        report=report
    )
       


@app.route("/v1/collections/<cids>")
def get_collection_titles(cids=''):
    from time import sleep
    sleep(1)
    cids_arr = cids.split(",")
    coll_info = collection.get_titles(cids_arr, mydao)
    resp = make_response(json.dumps(coll_info, indent=4), 200)
    return resp


@app.route("/v1/collections/reference-sets")
def reference_sets():
    resp = make_response(json.dumps(myrefsets, indent=4), 200)
    return resp

@app.route("/v1/collections/reference-sets-histograms")
def reference_sets_histograms():
    rows = []
    header_added = False
    for genre in myrefsets_histograms:
        for refset in myrefsets_histograms[genre]:
            for year in myrefsets_histograms[genre][refset]:
                if not header_added:
                    first_metric_name = myrefsets_histograms[genre][refset][year].keys()[0]
                    data_labels = [str(i)+"th" for i in range(len(myrefsets_histograms[genre][refset][year][first_metric_name]))]
                    header = ",".join(["genre", "refset", "year", "metric_name"] + data_labels)
                    rows.append(header)
                    header_added = True
                for metric_name in myrefsets_histograms[genre][refset][year]:
                    metadata = [genre, refset, str(year), metric_name]
                    metrics = [str(i) for i in myrefsets_histograms[genre][refset][year][metric_name]]
                    rows.append(",".join(metadata+metrics))
    resp = make_response("\n".join(rows), 200)
    # Do we want it to pop up to save?  kinda nice to just see it in browser
    #resp.mimetype = "text/csv;charset=UTF-8"
    #resp.headers.add("Content-Disposition", "attachment; filename=refsets.csv")
    #resp.headers.add("Content-Encoding", "UTF-8")
    return resp

@app.route("/collections/reference-sets-medians")
def reference_sets_medians():
    refset_medians = defaultdict(dict)
    for genre in myrefsets_histograms:
        if not genre in refset_medians:
            refset_medians[genre] = {}
        for refset in myrefsets_histograms[genre]:
            if not refset in refset_medians[genre]:
                refset_medians[genre][refset] = {}
            for year in myrefsets_histograms[genre][refset]:
                if not year in refset_medians[genre][refset]:
                    refset_medians[genre][refset][year] = {}
                for metric_name in myrefsets_histograms[genre][refset][year]:
                    median = myrefsets_histograms[genre][refset][year][metric_name][50]
                    refset_medians[genre][refset][year][metric_name] = median
    resp = make_response(json.dumps(refset_medians, indent=4), 200)

    return resp


# route to receive email
@app.route('/v1/inbox', methods=["POST"])
def inbox():
    payload = request.json
    email = incoming_email.save_incoming_email(payload)
    logger.info(u"You've got mail. Subject: {subject}".format(
        subject=email.subject))
    resp = make_response(json.dumps({"subject":email.subject}, sort_keys=True, indent=4), 200)
    return resp



########NEW FILE########

__FILENAME__ = arch
# arch.py
# Manipulating the machine architecture string.
#
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals

def _invert(dct):
    return {v: k for k in dct for v in dct[k]}

_BASEARCH_MAP = _invert({
    'aarch64' : ('aarch64',),
    'alpha'   : ('alpha', 'alphaev4', 'alphaev45', 'alphaev5', 'alphaev56',
                 'alphaev6', 'alphaev67', 'alphaev68', 'alphaev7', 'alphapca56'),
    'arm'     : ('armv5tejl', 'armv5tel', 'armv6l', 'armv7l'),
    'armhfp'  : ('armv7hl', 'armv7hnl'),
    'i386'    : ('i386', 'athlon', 'geode', 'i386', 'i486', 'i586', 'i686'),
    'ia64'    : ('ia64',),
    'noarch'  : ('noarch',),
    'ppc'     : ('ppc',),
    'ppc64'   : ('ppc64', 'ppc64iseries', 'ppc64p7', 'ppc64pseries'),
    'ppc64le' : ('ppc64le',),
    's390'    : ('s390',),
    's390x'   : ('s390x',),
    'sh3'     : ('sh3',),
    'sh4'     : ('sh4', 'sh4a'),
    'sparc'   : ('sparc', 'sparc64', 'sparc64v', 'sparcv8', 'sparcv9',
                 'sparcv9v'),
    'x86_64'  : ('x86_64', 'amd64', 'ia32e'),
})

def basearch(arch):
    return _BASEARCH_MAP[arch]

########NEW FILE########
__FILENAME__ = base
# Copyright 2005 Duke University
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

"""
Supplies the Base class.
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals
from dnf import const, query, sack
from dnf.i18n import _, P_, ucd
from dnf.yum import config
from dnf.yum import history
from dnf.yum import misc
from dnf.yum import rpmsack
from dnf.yum.config import ParsingError, ConfigParser
from dnf.yum.parser import ConfigPreProcessor
from functools import reduce

import dnf.arch
import dnf.callback
import dnf.comps
import dnf.conf
import dnf.drpm
import dnf.exceptions
import dnf.goal
import dnf.history
import dnf.lock
import dnf.logging
import dnf.persistor
import dnf.plugin
import dnf.repo
import dnf.repodict
import dnf.rpmUtils.connection
import dnf.rpmUtils.transaction
import dnf.subject
import dnf.transaction
import dnf.util
import dnf.yum.rpmtrans
import functools
import glob
import hawkey
import io
import logging
import os
import operator
import rpm
import signal
import time

class Base(object):
    def __init__(self):
        # :api
        self._closed = False
        self._conf = config.YumConf()
        self._goal = None
        self._persistor = None
        self._sack = None
        self._transaction = None
        self._ts = None
        self._comps = None
        self._history = None
        self._tempfiles = []
        self.ds_callback = dnf.callback.Depsolve()
        self.group_persistor = None
        self.logger = logging.getLogger("dnf")
        self.logging = dnf.logging.Logging()
        self._repos = dnf.repodict.RepoDict()
        self.repo_setopts = {} # since we have to use repo_setopts in base and
                               # not in cli - set it up as empty so no one
                               # trips over it later

        self.rpm_probfilter = set([rpm.RPMPROB_FILTER_OLDPACKAGE])
        self.plugins = dnf.plugin.Plugins()

        arch = hawkey.detect_arch()
        self._conf.yumvar['arch'] = arch
        self._conf.yumvar['basearch'] = dnf.arch.basearch(arch)
        self._conf.yumvar_update_from_env()

    def __enter__(self):
        return self

    def __exit__(self, *exc_args):
        self.close()

    def _add_repo_to_sack(self, name):
        hrepo = hawkey.Repo(name)
        repo = self.repos[name]
        try:
            repo.load()
        except dnf.exceptions.RepoError as e:
            if repo.skip_if_unavailable is False:
                raise
            self.logger.warning(_("%s, disabling."), e)
            repo.disable()
            return
        hrepo.repomd_fn = repo.repomd_fn
        hrepo.primary_fn = repo.primary_fn
        hrepo.filelists_fn = repo.filelists_fn
        hrepo.cost = repo.cost
        if repo.presto_fn:
            hrepo.presto_fn = repo.presto_fn
        else:
            self.logger.debug("not found deltainfo for: %s" % repo.name)
        repo.hawkey_repo = hrepo
        self._sack.load_yum_repo(hrepo, build_cache=True, load_filelists=True,
                                 load_presto=repo.deltarpm)

    def _setup_excludes(self):
        disabled = set(self.conf.disable_excludes)
        if 'all' in disabled:
            return
        if 'main' not in disabled:
            for excl in self.conf.exclude:
                subj = dnf.subject.Subject(excl)
                pkgs = subj.get_best_query(self.sack)
                self.sack.add_excludes(pkgs)
        for r in self.repos.iter_enabled():
            if r.id in disabled:
                continue
            for excl in r.exclude:
                pkgs = self.sack.query().filter(reponame=r.id).\
                    filter_autoglob(name=excl)
                self.sack.add_excludes(pkgs)

    def _store_persistent_data(self):
        def check_expired(repo):
            try:
                exp_remaining = repo.metadata_expire_in()[1]
                return False if exp_remaining is None else exp_remaining <= 0
            except dnf.exceptions.MetadataError:
                return False

        if self._persistor:
            expired = [r.id for r in self.repos.iter_enabled()
                       if check_expired(r)]
            self._persistor.set_expired_repos(expired)

        if self.group_persistor:
            self.group_persistor.save()

    @property
    def comps(self):
        # :api
        return self._comps

    @property
    def conf(self):
        # :api
        return self._conf

    @property
    def repos(self):
        return self._repos

    @repos.deleter
    def repos(self):
        self._repos = None

    @property
    @dnf.util.lazyattr("_rpmconn")
    def rpmconn(self):
        return dnf.rpmUtils.connection.RpmConnection(self.conf.installroot)

    @property
    def sack(self):
        # :api
        return self._sack

    @property
    def transaction(self):
        return self._transaction

    def activate_persistor(self):
        self._persistor = dnf.persistor.RepoPersistor(self.conf.cachedir)

    def fill_sack(self, load_system_repo=True, load_available_repos=True):
        """Prepare the Sack and the Goal objects. :api."""
        start = time.time()
        self._sack = sack.build_sack(self)
        with dnf.lock.metadata_cache_lock:
            if load_system_repo is not False:
                try:
                    self._sack.load_system_repo(build_cache=True)
                except IOError:
                    if load_system_repo != 'auto':
                        raise
            if load_available_repos:
                for r in self.repos.iter_enabled():
                    self._add_repo_to_sack(r.id)
        conf = self.conf
        self._sack.configure(conf.installonlypkgs, conf.installonly_limit)
        self.logger.debug('hawkey sack setup time: %0.3f' %
                                  (time.time() - start))
        self._setup_excludes()
        self._goal = dnf.goal.Goal(self._sack)
        return self._sack

    @property
    @dnf.util.lazyattr("_yumdb")
    def yumdb(self):
        db_path = os.path.normpath(self.conf.persistdir + '/yumdb')
        return rpmsack.AdditionalPkgDB(db_path)

    def close(self):
        """Close all potential handles. :api

        Typically the handles are to data sources and sinks.

        """

        if self._closed:
            return
        self.logger.log(dnf.logging.SUBDEBUG, 'Cleaning up.')
        self._closed = True
        # Do not trigger the lazy creation:
        if self._history is not None:
            self.history.close()
        self._store_persistent_data()
        self.closeRpmDB()

    def read_repos(self, repofn):
        """Read in repositories from a config .repo file."""

        confpp_obj = ConfigPreProcessor(repofn, vars=self.conf.yumvar)
        parser = ConfigParser()
        try:
            parser.readfp(confpp_obj)
        except ParsingError as e:
            msg = str(e)
            raise dnf.exceptions.ConfigError(msg)

        # Check sections in the .repo file that was just slurped up
        for section in parser.sections():

            if section in ['main', 'installed']:
                continue

            # Check the repo.id against the valid chars
            invalid = dnf.repo.repo_id_invalid(section)
            if invalid is not None:
                self.logger.warning("Bad id for repo: %s, byte = %s %d" %
                                    (section, section[invalid], invalid))
                continue

            try:
                thisrepo = self.readRepoConfig(parser, section)
            except (dnf.exceptions.RepoError, dnf.exceptions.ConfigError) as e:
                self.logger.warning(e)
                continue
            else:
                thisrepo.repofile = repofn

            if thisrepo.id in self.repo_setopts:
                for opt in self.repo_setopts[thisrepo.id].items:
                    if not hasattr(thisrepo, opt):
                        msg = "Repo %s did not have a %s attr. before setopt"
                        self.logger.warning(msg % (thisrepo.id, opt))
                    setattr(thisrepo, opt, getattr(self.repo_setopts[thisrepo.id],
                                                   opt))

            # Got our list of repo objects, add them to the repos
            # collection
            try:
                self.repos.add(thisrepo)
            except dnf.exceptions.ConfigError as e:
                self.logger.warning(e)

    def read_all_repos(self):
        """Read repositories from the main conf file and from .repo files."""
        # :api
        # Get the repos from the main yum.conf file
        self.read_repos(self.conf.config_file_path)

        # Read .repo files from directories specified by conf.reposdir
        for repofn in (repofn for reposdir in self.conf.reposdir
                       for repofn in sorted(glob.glob('%s/*.repo' % reposdir))):
            try:
                self.read_repos(repofn)
            except dnf.exceptions.ConfigError:
                self.logger.warning(_("Warning: failed loading '%s', skipping."),
                                    repofn)

    def readRepoConfig(self, parser, section):
        """Parse an INI file section for a repository.

        :param parser: :class:`ConfigParser` or similar object to read
           INI file values from
        :param section: INI file section to read
        :return: :class:`dnf.repo.Repo` instance
        """
        repo = dnf.repo.Repo(section, self.conf.cachedir)
        try:
            repo.populate(parser, section, self.conf)
        except ValueError as e:
            msg = _('Repository %r: Error parsing config: %s' % (section, e))
            raise dnf.exceptions.ConfigError(msg)

        # Ensure that the repo name is set
        if not repo.name:
            repo.name = section
            self.logger.error(_('Repository %r is missing name in configuration, '
                    'using id') % section)
        repo.name = ucd(repo.name)

        repo.yumvar.update(self.conf.yumvar)
        repo.cfg = parser

        return repo

    def reset(self, sack=False, repos=False, goal=False):
        """Make the Base object forget about various things. :api"""
        if sack:
            self._sack = None
        if repos:
            self._repos = dnf.repodict.RepoDict()
        if goal:
            self._goal = None
            if self._sack is not None:
                self._goal = dnf.goal.Goal(self._sack)

    def closeRpmDB(self):
        """Closes down the instances of rpmdb that could be open."""
        del self.ts
        self._comps = None

    _TS_FLAGS_TO_RPM = {'noscripts': rpm.RPMTRANS_FLAG_NOSCRIPTS,
                        'notriggers': rpm.RPMTRANS_FLAG_NOTRIGGERS,
                        'nodocs': rpm.RPMTRANS_FLAG_NODOCS,
                        'test': rpm.RPMTRANS_FLAG_TEST,
                        'justdb': rpm.RPMTRANS_FLAG_JUSTDB,
                        'nocontexts': rpm.RPMTRANS_FLAG_NOCONTEXTS,
                        'nocrypto' : rpm.RPMTRANS_FLAG_NOFILEDIGEST}
    _TS_VSFLAGS_TO_RPM = {'nocrypto' : rpm._RPMVSF_NOSIGNATURES |
                          rpm._RPMVSF_NODIGESTS}

    @property
    def ts(self):
        """Set up the RPM transaction set that will be used for all the work."""
        if self._ts is not None:
            return self._ts
        self._ts = dnf.rpmUtils.transaction.TransactionWrapper(
            self.conf.installroot)
        self._ts.setFlags(0) # reset everything.
        for flag in self.conf.tsflags:
            rpm_flag = self._TS_FLAGS_TO_RPM.get(flag)
            if rpm_flag is None:
                self.logger.critical(_('Invalid tsflag in config file: %s'), flag)
                continue
            self._ts.addTsFlag(rpm_flag)
            vs_flag = self._TS_VSFLAGS_TO_RPM.get(flag)
            if vs_flag is not None:
                self._ts.pushVSFlags(vs_flag)

        probfilter = reduce(operator.or_, self.rpm_probfilter, 0)
        self._ts.setProbFilter(probfilter)
        return self._ts

    @ts.deleter
    def ts(self):
        """Releases the RPM transaction set. """
        if self._ts is None:
            return
        self._ts.close()
        del self._ts
        self._ts = None

    def _activate_group_persistor(self):
        return dnf.persistor.GroupPersistor(self.conf.persistdir)

    def read_comps(self):
        """Create the groups object to access the comps metadata. :api"""
        group_st = time.time()
        self.group_persistor = self._activate_group_persistor()
        self._comps = dnf.comps.Comps()

        self.logger.log(dnf.logging.SUBDEBUG, 'Getting group metadata')
        for repo in self.repos.iter_enabled():
            if not repo.enablegroups:
                continue
            if not repo.metadata:
                continue
            comps_fn = repo.metadata.comps_fn
            if comps_fn is None:
                continue

            self.logger.log(dnf.logging.SUBDEBUG,
                            'Adding group file from repository: %s', repo.id)
            if repo.md_only_cached:
                decompressed = misc.calculate_repo_gen_dest(comps_fn,
                                                            'groups.xml')
                if not os.path.exists(decompressed):
                    # root privileges are needed for comps decompression
                    continue
            else:
                decompressed = misc.repo_gen_decompress(comps_fn, 'groups.xml')

            try:
                self._comps.add_from_xml_filename(decompressed)
            except dnf.exceptions.CompsError as e:
                msg = _('Failed to add groups file for repository: %s - %s')
                self.logger.critical(msg % (repo.id, e))


        self.logger.debug('group time: %0.3f' % (time.time() - group_st))
        return self._comps

    def _getHistory(self):
        """auto create the history object that to access/append the transaction
           history information. """
        if self._history is None:
            db_path = self.conf.persistdir + "/history"
            releasever = self.conf.yumvar['releasever']
            self._history = history.YumHistory(db_path, self.yumdb,
                                               root=self.conf.installroot,
                                               releasever=releasever)
        return self._history

    history = property(fget=lambda self: self._getHistory(),
                       fset=lambda self, value: setattr(self, "_history", value),
                       fdel=lambda self: setattr(self, "_history", None),
                       doc="Yum History Object")

    def _goal2transaction(self, goal):
        ts = dnf.transaction.Transaction()
        all_obsoleted = set(goal.list_obsoleted())

        for pkg in goal.list_downgrades():
            obs = goal.obsoleted_by_package(pkg)
            downgraded = obs[0]
            self.ds_callback.pkg_added(downgraded, 'dd')
            self.ds_callback.pkg_added(pkg, 'd')
            ts.add_downgrade(pkg, downgraded, obs[1:])
        for pkg in goal.list_reinstalls():
            self.ds_callback.pkg_added(pkg, 'r')
            obs = goal.obsoleted_by_package(pkg)
            reinstalled = obs[0]
            ts.add_reinstall(pkg, reinstalled, obs[1:])
        for pkg in goal.list_installs():
            self.ds_callback.pkg_added(pkg, 'i')
            obs = goal.obsoleted_by_package(pkg)
            ts.add_install(pkg, obs, goal.get_reason(pkg))
            cb = lambda pkg: self.ds_callback.pkg_added(pkg, 'od')
            dnf.util.mapall(cb, obs)
        for pkg in goal.list_upgrades():
            group_fn = functools.partial(operator.contains, all_obsoleted)
            obs, upgraded = dnf.util.group_by_filter(
                group_fn, goal.obsoleted_by_package(pkg))
            cb = lambda pkg: self.ds_callback.pkg_added(pkg, 'od')
            dnf.util.mapall(cb, obs)
            if pkg.name in self.conf.installonlypkgs:
                ts.add_install(pkg, obs)
            else:
                ts.add_upgrade(pkg, upgraded[0], obs)
                cb = lambda pkg: self.ds_callback.pkg_added(pkg, 'ud')
                dnf.util.mapall(cb, upgraded)
            self.ds_callback.pkg_added(pkg, 'u')
        for pkg in goal.list_erasures():
            self.ds_callback.pkg_added(pkg, 'e')
            ts.add_erase(pkg)
        return ts

    def _query_matches_installed(self, q):
        """ See what packages in the query match packages (also in older
            versions, but always same architecture) that are already installed.

            Unlike in case of _sltr_matches_installed(), it is practical here to
            know even the packages in the original query that can still be
            installed.
        """
        inst = q.installed()
        inst_per_arch = inst.na_dict()
        avail_per_arch = q.latest().available().na_dict()
        avail_l = []
        inst_l = []
        for na in avail_per_arch:
            if na in inst_per_arch:
                inst_l.append(inst_per_arch[na][0])
            else:
                avail_l.extend(avail_per_arch[na])
        return inst_l, avail_l

    def _sltr_matches_installed(self, sltr):
        """ See if sltr matches a patches that is (in older version or different
            architecture perhaps) already installed.
        """
        inst = self.sack.query().installed()
        inst = inst.filter(pkg=sltr.matches())
        return list(inst)

    def iter_userinstalled(self):
        """Get iterator over the packages installed by the user."""
        return (pkg for pkg in self.sack.query().installed()
                if self.yumdb.get_package(pkg).get('reason') == 'user' and
                   self.yumdb.get_package(pkg).get('from_repo') != 'anakonda')

    def push_userinstalled(self, goal):
        msg = _('--> Finding unneeded leftover dependencies')
        self.logger.debug(msg)
        for pkg in self.sack.query().installed().run():
            yumdb_info = self.yumdb.get_package(pkg)
            reason = 'user'
            try:
                reason = yumdb_info.reason
            except AttributeError:
                pass
            if reason != 'dep':
                goal.userinstalled(pkg)

    def run_hawkey_goal(self, goal, allow_erasing):
        ret = goal.run(allow_uninstall=allow_erasing, force_best=self.conf.best)
        if self.conf.debug_solver:
            goal.write_debugdata('./debugdata')
        return ret

    def resolve(self, allow_erasing=False):
        """Build the transaction set. :api"""
        exc = None

        ds_st = time.time()
        self.ds_callback.start()
        goal = self._goal
        if goal.req_has_erase():
            self.push_userinstalled(goal)
        if not self.run_hawkey_goal(goal, allow_erasing):
            if self.conf.debuglevel >= 6:
                goal.log_decisions()
            exc = dnf.exceptions.DepsolveError('. '.join(goal.problems))
        else:
            self._transaction = self._goal2transaction(goal)

        self.ds_callback.end()
        self.logger.debug('Depsolve time: %0.3f' % (time.time() - ds_st))

        got_transaction = self._transaction is not None and \
                          len(self._transaction) > 0
        if got_transaction:
            msg = self._transaction.rpm_limitations()
            if msg:
                exc = dnf.exceptions.Error(msg)

        if exc is not None:
            raise exc
        if not got_transaction:
            persistor = self.group_persistor
            if persistor:
                persistor.commit()
        return got_transaction

    def do_transaction(self, display=None):
        # :api
        # save our ds_callback out
        dscb = self.ds_callback
        self.ds_callback = None
        self.transaction.populate_rpm_ts(self.ts)

        rcd_st = time.time()
        self.logger.info(_('Running transaction check'))
        msgs = self._run_rpm_check()
        if msgs:
            rpmlib_only = True
            for msg in msgs:
                if msg.startswith('rpmlib('):
                    continue
                rpmlib_only = False
            if rpmlib_only:
                print(_("ERROR You need to upgrade rpm to handle:"))
            else:
                print(_('ERROR with transaction check vs depsolve:'))

            for msg in msgs:
                print(i18n.to_utf8(msg))

            if rpmlib_only:
                return 1, [_('RPM needs to be upgraded')]
            raise dnf.exceptions.TransactionCheckError(
                'transaction not consistent with package database')

        self.logger.info(_('Transaction check succeeded.'))
        self.logger.debug('Transaction check time: %0.3f' %
                          (time.time() - rcd_st))

        tt_st = time.time()
        self.logger.info(_('Running transaction test'))
        if not self.conf.diskspacecheck:
            self.rpm_probfilter.add(rpm.RPMPROB_FILTER_DISKSPACE)

        self.ts.order() # order the transaction
        self.ts.clean() # release memory not needed beyond this point

        testcb = dnf.yum.rpmtrans.RPMTransaction(self, test=True)
        tserrors = self.ts.test(testcb)
        del testcb

        if len(tserrors) > 0:
            errstring = _('Transaction check error:\n')
            for descr in tserrors:
                errstring += '  %s\n' % ucd(descr)

            raise dnf.exceptions.Error(errstring + '\n' + \
                 self.errorSummary(errstring))

        self.logger.info(_('Transaction test succeeded.'))
        self.logger.debug('Transaction test time: %0.3f' % (time.time() - tt_st))

        # unset the sigquit handler
        sigquit = signal.signal(signal.SIGQUIT, signal.SIG_DFL)
        ts_st = time.time()

        # put back our depcheck callback
        self.ds_callback = dscb
        # setup our rpm ts callback
        if display is None:
            cb = dnf.yum.rpmtrans.RPMTransaction(self)
        else:
            cb = dnf.yum.rpmtrans.RPMTransaction(self, display=display)
        if self.conf.debuglevel < 2:
            cb.display.output = False

        self.logger.info(_('Running transaction'))
        return_code = self.runTransaction(cb=cb)
        if return_code == 0 and self.group_persistor:
            self.group_persistor.commit()

        self.logger.debug('Transaction time: %0.3f' % (time.time() - ts_st))
        # put back the sigquit handler
        signal.signal(signal.SIGQUIT, sigquit)

        return return_code, None

    def _record_history(self):
        return self.conf.history_record and \
            not self.ts.isTsFlagSet(rpm.RPMTRANS_FLAG_TEST)

    @dnf.lock.rpmdb_lock.decorator
    def runTransaction(self, cb):
        """Perform the transaction.

        :param cb: an rpm callback object to use in the transaction
        :return: a :class:`misc.GenericHolder` containing
           information about the results of the transaction
        :raises: :class:`dnf.exceptions.YumRPMTransError` if there is a
           transaction cannot be completed
        """
        if self._record_history():
            using_pkgs_pats = list(self.conf.history_record_packages)
            installed_query = self.sack.query().installed()
            using_pkgs = installed_query.filter(name=using_pkgs_pats).run()
            rpmdbv = self.sack.rpmdb_version(self.yumdb)
            lastdbv = self.history.last()
            if lastdbv is not None:
                lastdbv = lastdbv.end_rpmdbversion

            if lastdbv is None or rpmdbv != lastdbv:
                self.logger.debug("RPMDB altered outside of DNF.")

            cmdline = None
            if hasattr(self, 'args') and self.args:
                cmdline = ' '.join(self.args)
            elif hasattr(self, 'cmds') and self.cmds:
                cmdline = ' '.join(self.cmds)

            self.history.beg(rpmdbv, using_pkgs, list(self.transaction),
                             [], [], cmdline)
            # write out our config and repo data to additional history info
            self._store_config_in_history()

        if self.conf.reset_nice:
            onice = os.nice(0)
            if onice:
                try:
                    os.nice(-onice)
                except:
                    onice = 0

        self.logger.debug('runTransaction: rpm transaction start.')
        errors = self.ts.run(cb.callback, '')
        self.logger.debug('runTransaction: rpm transaction over.')
        # ts.run() exit codes are, hmm, "creative": None means all ok, empty
        # list means some errors happened in the transaction and non-empty
        # list that there were errors preventing the ts from starting...
        if self.conf.reset_nice:
            try:
                os.nice(onice)
            except:
                pass

        return_code = 0
        if errors is None:
            pass
        elif len(errors) == 0:
            # this is a particularly tricky case happening also when rpm failed
            # to obtain the transaction lock. We can only try to see if a
            # particular element failed and if not, decide that is the
            # case.
            if len([el for el in self.ts if el.Failed()]) > 0:
                errstring = _('Warning: scriptlet or other non-fatal errors '
                              'occurred during transaction.')
                self.logger.debug(errstring)
                return_code = 1
            else:
                self.logger.critical(_("Transaction couldn't start (no root?)"))
                msg = _('Could not run transaction.')
                raise dnf.exceptions.YumRPMTransError(msg=msg, errors=[])
        else:
            if self._record_history():
                herrors = [ucd(x) for x in errors]
                self.history.end(rpmdbv, 2, errors=herrors)


            self.logger.critical(_("Transaction couldn't start:"))
            for e in errors:
                self.logger.critical(e[0]) # should this be 'to_unicoded'?
            msg = _("Could not run transaction.")
            raise dnf.exceptions.YumRPMTransError(msg=msg, errors=errors)

        for i in ('ts_all_fn', 'ts_done_fn'):
            if hasattr(cb, i):
                fn = getattr(cb, i)
                try:
                    misc.unlink_f(fn)
                except (IOError, OSError) as e:
                    msg = _('Failed to remove transaction file %s')
                    self.logger.critical(msg, fn)

        # sync up what just happened versus what is in the rpmdb
        if not self.ts.isTsFlagSet(rpm.RPMTRANS_FLAG_TEST):
            self.verify_transaction(return_code, cb.verify_tsi_package)

        if (not self.conf.keepcache and
            not self.ts.isTsFlagSet(rpm.RPMTRANS_FLAG_TEST)):
            self.clean_used_packages()

        return return_code

    def verify_transaction(self, return_code, verify_pkg_cb=None):
        """Check that the transaction did what was expected, and
        propagate external yumdb information.  Output error messages
        if the transaction did not do what was expected.

        :param resultobject: the :class:`misc.GenericHolder`
           object returned from the :func:`runTransaction` call that
           ran the transaction
        :param txmbr_cb: the callback for the rpm transaction members
        """
        # check to see that the rpmdb and the transaction roughly matches
        # push package object metadata outside of rpmdb into yumdb
        # delete old yumdb metadata entries

        # for each pkg in the transaction
        # if it is an install - see that the pkg is installed
        # if it is a remove - see that the pkg is no longer installed, provided
        #    that there is not also an install of this pkg in the transaction
        #    (reinstall)
        # for any kind of install add from_repo to the yumdb, and the cmdline
        # and the install reason

        total = self.transaction.total_package_count()
        def display_banner(pkg, count):
            count += 1
            if verify_pkg_cb is not None:
                verify_pkg_cb(pkg, count, total)
            return count

        vt_st = time.time()
        count = 0
        # the rpmdb has changed by now. hawkey doesn't support dropping a repo
        # yet we have to check what packages are in now: build a transient sack
        # with only rpmdb in it. In the future when RPM Python bindings can tell
        # us if a particular transaction element failed or not we can skip this
        # completely.
        rpmdb_sack = sack.rpmdb_sack(self)

        for tsi in self._transaction:
            rpo = tsi.installed
            if rpo is None:
                continue

            installed = rpmdb_sack.query().installed().nevra(
                rpo.name, rpo.evr, rpo.arch)
            if len(installed) < 1:
                self.logger.critical(_('%s was supposed to be installed' \
                                           ' but is not!' % rpo))
                count = display_banner(rpo, count)
                continue
            po = installed[0]
            count = display_banner(rpo, count)
            yumdb_info = self.yumdb.get_package(po)
            yumdb_info.from_repo = rpo.repoid

            yumdb_info.reason = tsi.propagated_reason(self.yumdb)
            yumdb_info.releasever = self.conf.yumvar['releasever']
            if hasattr(self, 'args') and self.args:
                yumdb_info.command_line = ' '.join(self.args)
            elif hasattr(self, 'cmds') and self.cmds:
                yumdb_info.command_line = ' '.join(self.cmds)
            csum = rpo.returnIdSum()
            if csum is not None:
                yumdb_info.checksum_type = str(csum[0])
                yumdb_info.checksum_data = csum[1]

            if rpo.from_cmdline:
                try:
                    st = os.stat(rpo.localPkg())
                    lp_ctime = str(int(st.st_ctime))
                    lp_mtime = str(int(st.st_mtime))
                    yumdb_info.from_repo_revision = lp_ctime
                    yumdb_info.from_repo_timestamp = lp_mtime
                except Exception:
                    pass
            elif hasattr(rpo.repo, 'repoXML'):
                md = rpo.repo.repoXML
                if md and md.revision is not None:
                    yumdb_info.from_repo_revision = str(md.revision)
                if md:
                    yumdb_info.from_repo_timestamp = str(md.timestamp)

            loginuid = misc.getloginuid()
            if tsi.op_type in (dnf.transaction.DOWNGRADE,
                               dnf.transaction.REINSTALL,
                               dnf.transaction.UPGRADE):
                opo = tsi.erased
                opo_yumdb_info = self.yumdb.get_package(opo)
                if 'installed_by' in opo_yumdb_info:
                    yumdb_info.installed_by = opo_yumdb_info.installed_by
                if loginuid is not None:
                    yumdb_info.changed_by = str(loginuid)
            elif loginuid is not None:
                yumdb_info.installed_by = str(loginuid)

            if self.conf.history_record:
                self.history.sync_alldb(po)

        just_installed = self.sack.query().\
            filter(pkg=self.transaction.install_set)
        for rpo in self.transaction.remove_set:
            installed = rpmdb_sack.query().installed().nevra(
                rpo.name, rpo.evr, rpo.arch)
            if len(installed) > 0:
                if not len(just_installed.filter(arch=rpo.arch, name=rpo.name,
                                                 evr=rpo.evr)):
                    msg = _('%s was supposed to be removed but is not!')
                    self.logger.critical(msg % rpo)
                    count = display_banner(rpo, count)
                    continue
            count = display_banner(rpo, count)
            yumdb_item = self.yumdb.get_package(po=rpo)
            yumdb_item.clean()

        if self._record_history():
            rpmdbv = rpmdb_sack.rpmdb_version(self.yumdb)
            self.history.end(rpmdbv, return_code)
        self.logger.debug('VerifyTransaction time: %0.3f' % (time.time() - vt_st))

    def download_packages(self, pkglist, progress=None, callback_total=None):
        """Download the packages specified by the given list of packages. :api

        `pkglist` is a list of packages to download, `progress` is an optional
         DownloadProgress instance, `callback_total` an optional callback to
         output messages about the download operation.

        """

        # select and sort packages to download
        if progress is None:
            progress = dnf.callback.NullDownloadProgress()

        drpm = dnf.drpm.DeltaInfo(self.sack.query().installed(), progress)
        remote_pkgs = [po for po in pkglist
                       if not (po.from_cmdline or po.repo.local)]
        payloads = [dnf.repo.pkg2payload(pkg, progress, drpm.delta_factory,
                                         dnf.repo.RPMPayload)
                    for pkg in remote_pkgs]

        beg_download = time.time()
        remote_size = sum(pload.download_size for pload in payloads)
        progress.start(len(payloads), remote_size)
        errors = dnf.repo.download_payloads(payloads, drpm)
        if errors.irrecoverable:
            raise dnf.exceptions.DownloadError(errors.irrecoverable)

        saving = dnf.repo.update_saving((0, 0), payloads, errors.recoverable)

        if errors.recoverable:
            msg = dnf.exceptions.DownloadError.errmap2str(errors.recoverable)
            self.logger.info(msg)

            remaining_pkgs = [pkg for pkg in errors.recoverable]
            payloads = [dnf.repo.pkg2payload(pkg, progress, dnf.repo.RPMPayload)
                        for pkg in remaining_pkgs]
            remaining_size = sum(pload.download_size for pload in payloads)
            progress.start(len(payloads), remaining_size)

            errors = dnf.repo.download_payloads(payloads, drpm)
            assert not errors.recoverable
            if errors.irrecoverable:
                raise dnf.exceptions.DownloadError(errors.irrecoverable)

            remote_pkgs.extend(remaining_pkgs)
            remote_size += remaining_size
            saving = dnf.repo.update_saving(saving, payloads, {})

        if callback_total is not None:
            callback_total(remote_pkgs, remote_size, beg_download)

        (real, full) = saving
        if real != full:
            msg = _("Delta RPMs reduced %.1f MB of updates to %.1f MB "
                    "(%d.1%% saved)")
            percent = 100 - real / full * 100
            self.logger.info(msg, full / 1024**2, real / 1024**2, percent)

    def add_remote_rpm(self, path):
        # :api
        self.sack.create_cmdline_repo()
        if not os.path.exists(path) and '://' in path:
            # download remote rpm to a tempfile
            path = dnf.util.urlopen(path, suffix='.rpm', delete=False).name
            self._tempfiles.append(path)
        return self.sack.add_cmdline_package(path)

    def sigCheckPkg(self, po):
        """Verify the GPG signature of the given package object.

        :param po: the package object to verify the signature of
        :return: (result, error_string)
           where result is::

              0 = GPG signature verifies ok or verification is not required.
              1 = GPG verification failed but installation of the right GPG key
                    might help.
              2 = Fatal GPG verification error, give up.
        """
        if po.from_cmdline:
            check = self.conf.localpkg_gpgcheck
            hasgpgkey = 0
        else:
            repo = self.repos[po.repoid]
            check = repo.gpgcheck
            hasgpgkey = not not repo.gpgkey

        if check:
            ts = self.rpmconn.readonly_ts
            sigresult = dnf.rpmUtils.miscutils.checkSig(ts, po.localPkg())
            localfn = os.path.basename(po.localPkg())

            if sigresult == 0:
                result = 0
                msg = ''

            elif sigresult == 1:
                if hasgpgkey:
                    result = 1
                else:
                    result = 2
                msg = _('Public key for %s is not installed') % localfn

            elif sigresult == 2:
                result = 2
                msg = _('Problem opening package %s') % localfn

            elif sigresult == 3:
                if hasgpgkey:
                    result = 1
                else:
                    result = 2
                result = 1
                msg = _('Public key for %s is not trusted') % localfn

            elif sigresult == 4:
                result = 2
                msg = _('Package %s is not signed') % localfn

        else:
            result = 0
            msg = ''

        return result, msg

    def clean_used_packages(self):
        """Delete the header and package files used in the
        transaction from the yum cache.
        """
        filelist = self._tempfiles
        for pkg in self.transaction.install_set:
            if pkg is None:
                continue
            if pkg.from_system or pkg.from_cmdline:
                continue

            # make sure it's not a local file
            repo = self.repos[pkg.repoid]
            for u in repo.baseurl:
                if u.startswith("file:"):
                    break
            else:
                filelist.append(pkg.localPkg())

        # now remove them
        for fn in filelist:
            if not os.path.exists(fn):
                continue
            try:
                misc.unlink_f(fn)
            except OSError:
                self.logger.warning(_('Cannot remove %s'), fn)
                continue
            else:
                self.logger.log(dnf.logging.SUBDEBUG,
                    _('%s removed'), fn)

    def cleanPackages(self):
        """Delete the package files from the yum cache."""

        exts = ['rpm']
        return self._cleanFiles(exts, 'pkgdir', 'package')

    def clean_binary_cache(self):
        """ Delete the binary cache files from the DNF cache.

            IOW, clean up the .solv and .solvx hawkey cache files.
        """
        files = [os.path.join(self.conf.cachedir,
                              hawkey.SYSTEM_REPO_NAME + ".solv")]
        for repo in self.repos.iter_enabled():
            basename = os.path.join(self.conf.cachedir, repo.id)
            files.append(basename + ".solv")
            files.append(basename + "-filenames.solvx")
        files = [f for f in files if os.access(f, os.F_OK)]

        return self._cleanFilelist('dbcache', files)

    def cleanMetadata(self):
        """Delete the metadata files from the yum cache."""

        exts = ['xml.gz', 'xml', 'cachecookie', 'mirrorlist', 'asc',
                'xml.bz2', 'xml.xz']
        # Metalink is also here, but is a *.xml file
        return self._cleanFiles(exts, 'cachedir', 'metadata')

    def cleanExpireCache(self):
        """Delete the local data saying when the metadata and mirror
           lists were downloaded for each repository."""

        for repo in self.repos.iter_enabled():
            repo.md_expire_cache()
        return 0, [_('The enabled repos were expired')]

    def cleanRpmDB(self):
        """Delete any cached data from the local rpmdb."""

        cachedir = self.conf.persistdir + "/rpmdb-indexes/"
        if not os.path.exists(cachedir):
            filelist = []
        else:
            filelist = misc.getFileList(cachedir, '', [])
        return self._cleanFilelist('rpmdb', filelist)

    def _cleanFiles(self, exts, pathattr, filetype):
        filelist = []
        for ext in exts:
            for repo in self.repos.iter_enabled():
                if repo.local:
                    continue
                path = getattr(repo, pathattr)
                if os.path.exists(path) and os.path.isdir(path):
                    filelist = misc.getFileList(path, ext, filelist)
        return self._cleanFilelist(filetype, filelist)

    def _cleanFilelist(self, filetype, filelist):
        removed = 0
        for item in filelist:
            try:
                misc.unlink_f(item)
            except OSError:
                self.logger.critical(_('Cannot remove %s file %s'),
                                     filetype, item)
                continue
            else:
                self.logger.log(dnf.logging.SUBDEBUG,
                    _('%s file %s removed'), filetype, item)
                removed += 1
        msg = P_('%d %s file removed', '%d %s files removed', removed)
        msg %= (removed, filetype)
        return 0, [msg]

    def doPackageLists(self, pkgnarrow='all', patterns=None, showdups=None,
                       ignore_case=False, reponame=None):
        """Return a :class:`misc.GenericHolder` containing
        lists of package objects.  The contents of the lists are
        specified in various ways by the arguments.

        :param pkgnarrow: a string specifying which types of packages
           lists to produces, such as updates, installed, available,
           etc.
        :param patterns: a list of names or wildcards specifying
           packages to list
        :param showdups: whether to include duplicate packages in the
           lists
        :param ignore_case: whether to ignore case when searching by
           package names
        :param reponame: limit packages list to the given repository
        :return: a :class:`misc.GenericHolder` instance with the
           following lists defined::

             available = list of packageObjects
             installed = list of packageObjects
             upgrades = tuples of packageObjects (updating, installed)
             extras = list of packageObjects
             obsoletes = tuples of packageObjects (obsoleting, installed)
             recent = list of packageObjects
        """
        if showdups is None:
            showdups = self.conf.showdupesfromrepos
        if patterns is None:
            return self._list_pattern(
                pkgnarrow, patterns, showdups, ignore_case, reponame)

        assert not dnf.util.is_string_type(patterns)
        list_fn = functools.partial(
            self._list_pattern, pkgnarrow, showdups=showdups,
            ignore_case=ignore_case, reponame=reponame)
        if patterns is None or len(patterns) == 0:
            return list_fn(None)
        yghs = map(list_fn, patterns)
        return reduce(lambda a, b: a.merge_lists(b), yghs)

    def _list_pattern(self, pkgnarrow, pattern, showdups, ignore_case,
                      reponame=None):
        def is_from_repo(package):
            """Test whether given package originates from the repository."""
            if reponame is None:
                return True
            return self.yumdb.get_package(package).get('from_repo') == reponame

        def pkgs_from_repo(packages):
            """Filter out the packages which do not originate from the repo."""
            return (package for package in packages if is_from_repo(package))

        def query_for_repo(query):
            """Filter out the packages which do not originate from the repo."""
            if reponame is None:
                return query
            return query.filter(reponame=reponame)

        ygh = misc.GenericHolder(iter=pkgnarrow)

        installed = []
        available = []
        reinstall_available = []
        old_available = []
        updates = []
        obsoletes = []
        obsoletesTuples = []
        recent = []
        extras = []

        # do the initial pre-selection
        q = self.sack.query()
        if pattern is not None:
            subj = dnf.subject.Subject(pattern, ignore_case=ignore_case)
            q = subj.get_greedy_query(self.sack)

        # list all packages - those installed and available:
        if pkgnarrow == 'all':
            dinst = {}
            ndinst = {} # Newest versions by name.arch
            for po in q.installed():
                dinst[po.pkgtup] = po
                if showdups:
                    continue
                key = (po.name, po.arch)
                if key not in ndinst or po > ndinst[key]:
                    ndinst[key] = po
            installed = list(pkgs_from_repo(dinst.values()))

            avail = query_for_repo(q)
            if not showdups:
                avail = avail.latest()
            for pkg in avail:
                if showdups:
                    if pkg.pkgtup in dinst:
                        reinstall_available.append(pkg)
                    else:
                        available.append(pkg)
                else:
                    key = (pkg.name, pkg.arch)
                    if pkg.pkgtup in dinst:
                        reinstall_available.append(pkg)
                    elif key not in ndinst or pkg.evr_gt(ndinst[key]):
                        available.append(pkg)
                    else:
                        old_available.append(pkg)

        # produce the updates list of tuples
        elif pkgnarrow == 'upgrades':
            updates = query_for_repo(q).upgrades().run()

        # installed only
        elif pkgnarrow == 'installed':
            installed = list(pkgs_from_repo(q.installed()))

        # available in a repository
        elif pkgnarrow == 'available':
            if showdups:
                avail = query_for_repo(q).available()
                installed_dict = q.installed().na_dict()
                for avail_pkg in avail:
                    key = (avail_pkg.name, avail_pkg.arch)
                    installed_pkgs = installed_dict.get(key, [])
                    same_ver = [pkg for pkg in installed_pkgs
                                if pkg.evr == avail_pkg.evr]
                    if len(same_ver) > 0:
                        reinstall_available.append(avail_pkg)
                    else:
                        available.append(avail_pkg)
            else:
                # we will only look at the latest versions of packages:
                available_dict = query_for_repo(q).available().latest().na_dict()
                installed_dict = q.installed().latest().na_dict()
                for (name, arch) in available_dict:
                    avail_pkg = available_dict[(name, arch)][0]
                    inst_pkg = installed_dict.get((name, arch), [None])[0]
                    if not inst_pkg or avail_pkg.evr_gt(inst_pkg):
                        available.append(avail_pkg)
                    elif avail_pkg.evr_eq(inst_pkg):
                        reinstall_available.append(avail_pkg)
                    else:
                        old_available.append(avail_pkg)

        # not in a repo but installed
        elif pkgnarrow == 'extras':
            # anything installed but not in a repo is an extra
            avail_dict = q.available().pkgtup_dict()
            inst_dict = q.installed().pkgtup_dict()
            for pkgtup, pkgs in inst_dict.items():
                if pkgtup not in avail_dict:
                    extras.extend(pkg for pkg in pkgs if is_from_repo(pkg))

        # obsoleting packages (and what they obsolete)
        elif pkgnarrow == 'obsoletes':
            self.conf.obsoletes = 1
            inst = q.installed()
            obsoletes = query_for_repo(self.sack.query()).filter(obsoletes=inst)
            obsoletesTuples = []
            for new in obsoletes:
                obsoleted_reldeps = new.obsoletes
                obsoletesTuples.extend([(new, old) for old in
                                        inst.filter(provides=obsoleted_reldeps)])

        # packages recently added to the repositories
        elif pkgnarrow == 'recent':
            now = time.time()
            recentlimit = now-(self.conf.recent*86400)
            if showdups:
                avail = q.available()
            else:
                avail = q.latest()

            for po in query_for_repo(avail):
                if int(po.buildtime) > recentlimit:
                    recent.append(po)

        ygh.installed = installed
        ygh.available = available
        ygh.reinstall_available = reinstall_available
        ygh.old_available = old_available
        ygh.updates = updates
        ygh.obsoletes = obsoletes
        ygh.obsoletesTuples = obsoletesTuples
        ygh.recent = recent
        ygh.extras = extras

        return ygh

    def findDeps(self, pkgs):
        """Return the dependencies for a given package object list, as well
        as possible solutions for those dependencies.

        :param pkgs: a list of package objects
        :return: the dependencies as a dictionary of dictionaries:
           packageobject = [reqs] = [list of satisfying pkgs]
        """
        results = {}

        for pkg in pkgs:
            results[pkg] = {}
            reqs = pkg.requires
            reqs.sort()
            pkgresults = results[pkg] # shorthand so we don't have to do the
                                      # double bracket thing

            for req in reqs:
                (r, f, v) = req
                if r.startswith('rpmlib('):
                    continue

                satisfiers = []

                for po in self.whatProvides(r, f, v):
                    satisfiers.append(po)

                pkgresults[req] = satisfiers

        return results

    def _add_comps_trans(self, trans):
        cnt = 0
        attr_fn = ((trans.install, self._goal.install),
                   (trans.upgrade, self._goal.upgrade),
                   (trans.remove, self._goal.erase))

        for (attr, fn) in attr_fn:
            for it in attr:
                if not self.sack.query().filter(name=it):
                    # a comps item that doesn't refer to anything real
                    continue
                sltr = dnf.selector.Selector(self.sack)
                sltr.set(name=it)
                fn(select=sltr)
                cnt += 1

        self._goal.group_members.update(trans.install)
        return cnt

    def build_comps_solver(self):
        def reason_fn(pkgname):
            q = self.sack.query().installed().filter(name=pkgname)
            if not q:
                return None
            try:
                return self.yumdb.get_package(q[0]).reason
            except AttributeError:
                return 'unknown'

        return dnf.comps.Solver(self.group_persistor, reason_fn)

    def environment_install(self, env, types, exclude=None):
        solver = self.build_comps_solver()
        types = self._translate_comps_pkg_types(types)
        trans = solver.environment_install(env, types, exclude or set())
        return self._add_comps_trans(trans)

    def environment_remove(self, env):
        solver = self.build_comps_solver()
        trans = solver.environment_remove(env)
        return self._add_comps_trans(trans)

    _COMPS_TRANSLATION = {
        'default'   : dnf.comps.DEFAULT,
        'mandatory' : dnf.comps.MANDATORY,
        'optional'  : dnf.comps.OPTIONAL
    }

    @staticmethod
    def _translate_comps_pkg_types(pkg_types):
        ret = 0
        for (name, enum) in Base._COMPS_TRANSLATION.items():
            if name in pkg_types:
                ret |= enum
        return ret

    def group_install(self, grp, pkg_types, exclude=None):
        # :api
        solver = self.build_comps_solver()
        pkg_types = self._translate_comps_pkg_types(pkg_types)
        trans = solver.group_install(grp, pkg_types, exclude)
        self.logger.debug("Adding packages from group '%s': %s",
                          grp.id, trans.install)
        return self._add_comps_trans(trans)

    def group_remove(self, grp):
        # :api
        solver = self.build_comps_solver()
        trans = solver.group_remove(grp)
        return self._add_comps_trans(trans)

    def group_upgrade(self, grp):
        # :api
        solver = self.build_comps_solver()
        trans = solver.group_upgrade(grp)
        return self._add_comps_trans(trans)

    def select_group(self, group, pkg_types=const.GROUP_PACKAGE_TYPES):
        """Mark all the packages in the given group to be installed. :api

        :param group: the group containing the packages to mark for installation
        :return: number of transaction members added to the transaction set

        """

        # :deprecated in 0.4.18, eligible for dropping after 2014-06-20 AND no
        # sooner than in 0.4.21
        msg = "dnf.Base.select_group() is deprecated. Use group_install()."
        dnf.logging.depr(msg)

        if group.selected:
            return 0
        group.selected = True

        pkgs = []
        if 'mandatory' in pkg_types:
            pkgs.extend(group.mandatory_packages)
        if 'default' in pkg_types:
            pkgs.extend(group.default_packages)
        if 'optional' in pkg_types:
            pkgs.extend(group.optional_packages)

        inst_set = set([pkg.name for pkg in self.sack.query().installed()])
        adding_msg = _('Adding package %s from group %s')
        cnt = 0
        for pkg in pkgs:
            self.logger.debug(adding_msg % (pkg.name, group.id))
            if pkg.name in inst_set:
                continue
            inst_set.add(pkg.name)
            current_cnt = self.install_groupie(pkg.name, inst_set)
            cnt += current_cnt

        if cnt == 0:
            msg = _('Warning: Group %s does not have any packages.')
            self.logger.warning(msg % group.id)
        return cnt

    def gpgKeyCheck(self):
        """Checks for the presence of GPG keys in the rpmdb.

        :return: 0 if there are no GPG keys in the rpmdb, and 1 if
           there are keys
        """
        gpgkeyschecked = self.conf.cachedir + '/.gpgkeyschecked.yum'
        if os.path.exists(gpgkeyschecked):
            return 1

        installroot = self.conf.installroot
        myts = dnf.rpmUtils.transaction.initReadOnlyTransaction(root=installroot)
        myts.pushVSFlags(~(rpm._RPMVSF_NOSIGNATURES|rpm._RPMVSF_NODIGESTS))
        idx = myts.dbMatch('name', 'gpg-pubkey')
        keys = len(idx)
        del idx
        del myts

        if keys == 0:
            return 0
        else:
            mydir = os.path.dirname(gpgkeyschecked)
            if not os.path.exists(mydir):
                os.makedirs(mydir)

            fo = open(gpgkeyschecked, 'w')
            fo.close()
            del fo
            return 1

    def install(self, pkg_spec, reponame=None):
        """Mark package(s) given by pkg_spec and reponame for installation.:api
        """

        def msg_installed(pkg):
            name = ucd(pkg)
            msg = _('Package %s is already installed, skipping.') % name
            self.logger.warning(msg)

        subj = dnf.subject.Subject(pkg_spec)
        if self.conf.multilib_policy == "all" or subj.filename_pattern or \
           subj.is_arch_specified(self.sack):
            q = subj.get_best_query(self.sack)
            if reponame is not None:
                q = q.filter(reponame=reponame)
            already_inst, available = self._query_matches_installed(q)
            for i in already_inst:
                msg_installed(i)
            for a in available:
                self._goal.install(a)
            return len(available)
        elif self.conf.multilib_policy == "best":
            sltr = subj.get_best_selector(self.sack)
            if not sltr:
                raise dnf.exceptions.MarkingError('no package matched', pkg_spec)
            if reponame is not None:
                sltr = sltr.set(reponame=reponame)
            already_inst = self._sltr_matches_installed(sltr)
            if already_inst:
                msg_installed(already_inst[0])
            self._goal.install(select=sltr)
            return 1
        return 0

    def install_groupie(self, pkg_name, inst_set):
        """Installs a group member package by name. """
        forms = [hawkey.FORM_NAME]
        subj = dnf.subject.Subject(pkg_name)
        if self.conf.multilib_policy == "all":
            q = subj.get_best_query(self.sack, with_provides=False, forms=forms)
            for pkg in q:
                self._goal.install(pkg)
            return len(q)
        elif self.conf.multilib_policy == "best":
            sltr = subj.get_best_selector(self.sack, forms=forms)
            if sltr:
                self._goal.install(select=sltr)
                return 1
        return 0

    def package_downgrade(self, pkg):
        # :api
        if pkg.from_system:
            msg = 'downgrade_package() for an installed package.'
            raise NotImplementedError(msg)

        installed = sorted(self.sack.query().installed().filter(name=pkg.name))
        if len(installed) > 0 and installed[0] > pkg:
            self._goal.install(pkg)
            self._goal.erase(installed[0])
            return 2
        return 0

    def package_install(self, pkg):
        # :api
        self._goal.install(pkg)
        return 1

    def package_remove(self, pkg):
        self._goal.erase(pkg)
        return 1

    def package_upgrade(self, pkg):
        # :api
        if pkg.from_system:
            msg = 'upgrade_package() for an installed package.'
            raise NotImplementedError(msg)

        self._goal.upgrade_to(pkg)
        return 1

    def upgrade(self, pkg_spec, reponame=None):
        # :api
        sltr = dnf.subject.Subject(pkg_spec).get_best_selector(self.sack)
        if sltr:
            if reponame is not None:
                sltr = sltr.set(reponame=reponame)

            prev_count = self._goal.req_length()
            self._goal.upgrade(select=sltr)
            if self._goal.req_length() - prev_count:
                return 1

        raise dnf.exceptions.MarkingError('no package matched', pkg_spec)

    def upgrade_all(self, reponame=None):
        # :api
        if reponame is None:
            self._goal.upgrade_all()
        else:
            try:
                self.upgrade('*', reponame)
            except dnf.exceptions.MarkingError:
                pass
        return 1

    def upgrade_to(self, pkg_spec, reponame=None):
        forms = [hawkey.FORM_NEVRA, hawkey.FORM_NEVR]
        sltr = dnf.subject.Subject(pkg_spec).get_best_selector(self.sack,
                                                               forms=forms)
        if sltr:
            if reponame is not None:
                sltr = sltr.set(reponame=reponame)

            prev_count = self._goal.req_length()
            self._goal.upgrade_to(select=sltr)
            if self._goal.req_length() - prev_count:
                return 1

        return 0

    def distro_sync(self, pkg_spec=None):
        if pkg_spec is None:
            self._goal.distupgrade_all()
        else:
            sltr = dnf.subject.Subject(pkg_spec).get_best_selector(self.sack)
            if not sltr:
                self.logger.info(_('No package %s installed.'), pkg_spec)
                return 0
            self._goal.distupgrade(select=sltr)
        return 1

    def remove(self, pkg_spec, reponame=None):
        """Mark the specified package for removal. #:api """

        matches = dnf.subject.Subject(pkg_spec).get_best_query(self.sack)
        installed = [
            pkg for pkg in matches.installed()
            if reponame is None or
               self.yumdb.get_package(pkg).get('from_repo') == reponame]
        if not installed:
            raise dnf.exceptions.PackagesNotInstalledError('no package matched',
                                                           pkg_spec)

        clean_deps = self.conf.clean_requirements_on_remove
        for pkg in installed:
            self._goal.erase(pkg, clean_deps=clean_deps)
        return len(installed)

    def reinstall(self, pkg_spec, old_reponame=None, new_reponame=None,
                  new_reponame_neq=None, remove_na=False):
        self._add_reinstall_rpm_probfilters()
        subj = dnf.subject.Subject(pkg_spec)
        q = subj.get_best_query(self.sack)
        installed_pkgs = [
            pkg for pkg in q.installed()
            if old_reponame is None or
               self.yumdb.get_package(pkg).get('from_repo') == old_reponame]

        available_q = q.available()
        if new_reponame is not None:
            available_q = available_q.filter(reponame=new_reponame)
        if new_reponame_neq is not None:
            available_q = available_q.filter(reponame__neq=new_reponame_neq)
        available_nevra2pkg = query.per_nevra_dict(available_q)

        if not installed_pkgs:
            raise dnf.exceptions.PackagesNotInstalledError(
                'no package matched', pkg_spec, available_nevra2pkg.values())

        cnt = 0
        clean_deps = self.conf.clean_requirements_on_remove
        for installed_pkg in installed_pkgs:
            try:
                available_pkg = available_nevra2pkg[str(installed_pkg)]
            except KeyError:
                if not remove_na:
                    continue
                self._goal.erase(installed_pkg, clean_deps=clean_deps)
            else:
                self._goal.install(available_pkg)
            cnt += 1

        if cnt == 0:
            raise dnf.exceptions.PackagesNotAvailableError(
                'no package matched', pkg_spec, installed_pkgs)

        return cnt

    def downgrade(self, pkg_spec):
        """Mark a package to be downgraded. :api

        This is equivalent to first removing the currently installed package,
        and then installing an older version.

        """
        subj = dnf.subject.Subject(pkg_spec)
        q = subj.get_best_query(self.sack)
        installed = sorted(q.installed())
        installed_pkg = dnf.util.first(installed)
        if installed_pkg is None:
            available_pkgs = q.available()
            if available_pkgs:
                raise dnf.exceptions.PackagesNotInstalledError(
                    'no package matched', pkg_spec, available_pkgs)
            raise dnf.exceptions.PackageNotFoundError('no package matched',
                                                      pkg_spec)

        arch = installed_pkg.arch
        q = self.sack.query().filter(name=installed_pkg.name, arch=arch)
        avail = [pkg for pkg in q.downgrades() if pkg < installed_pkg]
        avail_pkg = dnf.util.first(sorted(avail, reverse=True))
        if avail_pkg is None:
            return 0

        self._goal.install(avail_pkg)
        return 1

    def provides(self, provides_spec):
        providers = query.by_provides(self.sack, provides_spec)
        if providers:
            return providers
        if any(map(dnf.util.is_glob_pattern, provides_spec)):
            return self.sack.query().filter(file__glob=provides_spec)
        return self.sack.query().filter(file=provides_spec)

    def history_redo(self, transaction,
                     force_reinstall=False, force_changed_removal=False):
        """Repeat the transaction represented by the given
        :class:`history.YumHistoryTransaction` object.

        :param transaction: a
           :class:`history.YumHistoryTransaction` object
           representing the transaction to be repeated
        :param force_reinstall: bool - do we want to reinstall anything that was
           installed/updated/downgraded/etc.
        :param force_changed_removal: bool - do we want to force remove anything
           that was downgraded or upgraded.
        :return: whether the transaction was repeated successfully
        """
        # NOTE: This is somewhat basic atm. ... see comment in undo.
        #  Also note that redo doesn't force install Dep-Install packages,
        # which is probably what is wanted the majority of the time.

        old_conf_obs = self.conf.obsoletes
        self.conf.obsoletes = False
        done = False
        for pkg in transaction.trans_data:
            if pkg.state == 'Reinstall':
                if self.reinstall(pkgtup=pkg.pkgtup):
                    done = True
        for pkg in transaction.trans_data:
            if pkg.state == 'Downgrade':
                if force_reinstall and self.rpmdb.searchPkgTuple(pkg.pkgtup):
                    if self.reinstall(pkgtup=pkg.pkgtup):
                        done = True
                    continue

                try:
                    if self.downgrade(pkgtup=pkg.pkgtup):
                        done = True
                except dnf.exceptions.Error:
                    # :dead
                    self.logger.critical(_('Failed to downgrade: %s'), pkg)
        for pkg in transaction.trans_data:
            if force_changed_removal and pkg.state == 'Downgraded':
                if self.tsInfo.getMembers(pkg.pkgtup):
                    continue
                if self.remove(pkgtup=pkg.pkgtup, silence_warnings=True):
                    done = True
        for pkg in transaction.trans_data:
            if pkg.state == 'Update':
                if force_reinstall and self.rpmdb.searchPkgTuple(pkg.pkgtup):
                    if self.reinstall(pkgtup=pkg.pkgtup):
                        done = True
                    continue

                if self.upgrade(pkgtup=pkg.pkgtup):
                    done = True
                else:
                    self.logger.critical(_('Failed to upgrade: %s'), pkg)
        for pkg in transaction.trans_data:
            if force_changed_removal and pkg.state == 'Updated':
                if self.tsInfo.getMembers(pkg.pkgtup):
                    continue
                if self.remove(pkgtup=pkg.pkgtup, silence_warnings=True):
                    done = True
        for pkg in transaction.trans_data:
            if pkg.state in ('Install', 'True-Install', 'Obsoleting'):
                if force_reinstall and self.rpmdb.searchPkgTuple(pkg.pkgtup):
                    if self.reinstall(pkgtup=pkg.pkgtup):
                        done = True
                    continue

                if self.install(pkgtup=pkg.pkgtup):
                    done = True
        for pkg in transaction.trans_data:
            if pkg.state == 'Erase':
                if self.remove(pkgtup=pkg.pkgtup):
                    done = True
        self.conf.obsoletes = old_conf_obs
        return done

    def history_undo_operations(self, operations):
        """Undo the operations on packages by their NEVRAs.

        :param operations: a NEVRAOperations to be undone
        :return: (exit_code, [ errors ])

        exit_code is::

            0 = we're done, exit
            1 = we've errored, exit with error string
            2 = we've got work yet to do, onto the next stage
        """

        def handle_downgrade(new_nevra, old_nevra, obsoleted_nevras):
            """Handle a downgraded package."""
            news = self.sack.query().installed().nevra(new_nevra)
            if not news:
                raise dnf.exceptions.PackagesNotInstalledError(
                    'no package matched', new_nevra)
            olds = self.sack.query().available().nevra(old_nevra)
            if not olds:
                raise dnf.exceptions.PackagesNotAvailableError(
                    'no package matched', old_nevra)
            assert len(news) == 1
            self._transaction.add_upgrade(dnf.util.first(olds), news[0], None)
            for obsoleted_nevra in obsoleted_nevras:
                handle_erase(obsoleted_nevra)

        def handle_erase(old_nevra):
            """Handle an erased package."""
            pkgs = self.sack.query().available().nevra(old_nevra)
            if not pkgs:
                raise dnf.exceptions.PackagesNotAvailableError(
                    'no package matched', old_nevra)
            self._transaction.add_install(dnf.util.first(pkgs), None, 'history')

        def handle_install(new_nevra, obsoleted_nevras):
            """Handle an installed package."""
            pkgs = self.sack.query().installed().nevra(new_nevra)
            if not pkgs:
                raise dnf.exceptions.PackagesNotInstalledError(
                    'no package matched', new_nevra)
            assert len(pkgs) == 1
            self._transaction.add_erase(pkgs[0])
            for obsoleted_nevra in obsoleted_nevras:
                handle_erase(obsoleted_nevra)

        def handle_reinstall(new_nevra, old_nevra, obsoleted_nevras):
            """Handle a reinstalled package."""
            news = self.sack.query().installed().nevra(new_nevra)
            if not news:
                raise dnf.exceptions.PackagesNotInstalledError(
                    'no package matched', new_nevra)
            olds = self.sack.query().available().nevra(old_nevra)
            if not olds:
                raise dnf.exceptions.PackagesNotAvailableError(
                    'no package matched', old_nevra)
            obsoleteds = []
            for nevra in obsoleted_nevras:
                obsoleteds_ = self.sack.query().installed().nevra(nevra)
                if obsoleteds_:
                    assert len(obsoleteds_) == 1
                    obsoleteds.append(obsoleteds_[0])
            assert len(news) == 1
            self._add_reinstall_rpm_probfilters()
            self._transaction.add_reinstall(dnf.util.first(olds), news[0],
                                            obsoleteds)

        def handle_upgrade(new_nevra, old_nevra, obsoleted_nevras):
            """Handle an upgraded package."""
            news = self.sack.query().installed().nevra(new_nevra)
            if not news:
                raise dnf.exceptions.PackagesNotInstalledError(
                    'no package matched', new_nevra)
            olds = self.sack.query().available().nevra(old_nevra)
            if not olds:
                raise dnf.exceptions.PackagesNotAvailableError(
                    'no package matched', old_nevra)
            assert len(news) == 1
            self._transaction.add_downgrade(dnf.util.first(olds), news[0], None)
            for obsoleted_nevra in obsoleted_nevras:
                handle_erase(obsoleted_nevra)

        # Build the transaction directly, because the depsolve is not needed.
        self._transaction = dnf.transaction.Transaction()
        for state, nevra, replaced_nevra, obsoleted_nevras in operations:
            if state == 'Install':
                assert not replaced_nevra
                handle_install(nevra, obsoleted_nevras)
            elif state == 'Erase':
                assert not replaced_nevra and not obsoleted_nevras
                handle_erase(nevra)
            elif state == 'Reinstall':
                handle_reinstall(nevra, replaced_nevra, obsoleted_nevras)
            elif state == 'Downgrade':
                handle_downgrade(nevra, replaced_nevra, obsoleted_nevras)
            elif state == 'Update':
                handle_upgrade(nevra, replaced_nevra, obsoleted_nevras)
            else:
                assert False

    def _retrievePublicKey(self, keyurl, repo=None, getSig=True):
        """
        Retrieve a key file
        @param keyurl: url to the key to retrieve
        Returns a list of dicts with all the keyinfo
        """

        msg = _('Retrieving key from %s') % keyurl
        self.logger.info(msg)

        # Go get the GPG key from the given URL
        try:
            # If we have a repo, use the proxy etc. configuration for it.
            with dnf.util.urlopen(keyurl, repo) as fh:
                rawkey = fh.read()

        except IOError as e:
            raise dnf.exceptions.Error(_('GPG key retrieval failed: ') %
                                       ucd(e))

        # check for a .asc file accompanying it - that's our gpg sig on the key
        # suck it down and do the check
        sigfile = None
        valid_sig = False
        if getSig and repo and repo.gpgcakey:
            self.getCAKeyForRepo(repo, callback=repo.confirm_func)
            try:
                sigfile = dnf.util.urlopen(keyurl + '.asc', repo)

            except IOError as e:
                sigfile = None

            if sigfile:
                if not misc.valid_detached_sig(sigfile,
                                    io.StringIO(rawkey), repo.gpgcadir):
                    #if we decide we want to check, even though the sig failed
                    # here is where we would do that
                    raise dnf.exceptions.Error(
                        _('GPG key signature on key %s does not match '
                          'CA Key for repo: %s') % (keyurl, repo.id))
                else:
                    msg = _('GPG key signature verified against CA Key(s)')
                    self.logger.info(msg)
                    valid_sig = True

        # Parse the key
        try:
            keys_info = misc.getgpgkeyinfo(rawkey, multiple=True)
        except ValueError as e:
            raise dnf.exceptions.Error(_('Invalid GPG Key from %s: %s') %
                                      (keyurl, ucd(e)))
        keys = []
        for keyinfo in keys_info:
            thiskey = {}
            for info in ('keyid', 'timestamp', 'userid',
                         'fingerprint', 'raw_key'):
                if info not in keyinfo:
                    msg = _('GPG key parsing failed: key does not have value %s')
                    raise dnf.exceptions.Error(msg % info)
                thiskey[info] = keyinfo[info]
            thiskey['hexkeyid'] = misc.keyIdToRPMVer(keyinfo['keyid']).upper()
            thiskey['valid_sig'] = valid_sig
            thiskey['has_sig'] = bool(sigfile)
            keys.append(thiskey)

        return keys

    def _log_key_import(self, info, keyurl, keytype='GPG'):
        msg = None
        fname = dnf.util.strip_prefix(keyurl, "file://")
        if fname:
            pkgs = self.sack.query().filter(file=fname)
            if pkgs:
                pkg = pkgs[0]
                msg = (_('Importing %s key 0x%s:\n'
                         ' Userid     : "%s"\n'
                         ' Fingerprint: %s\n'
                         ' Package    : %s (%s)\n'
                         ' From       : %s') %
                       (keytype, info['hexkeyid'], ucd(info['userid']),
                        misc.gpgkey_fingerprint_ascii(info),
                        pkg, pkg.reponame, fname))
        if msg is None:
            msg = (_('Importing %s key 0x%s:\n'
                     ' Userid     : "%s"\n'
                     ' Fingerprint: %s\n'
                     ' From       : %s') %
                   (keytype, info['hexkeyid'], ucd(info['userid']),
                    misc.gpgkey_fingerprint_ascii(info),
                    keyurl.replace("file://", "")))
        self.logger.critical("%s", msg)

    def getKeyForPackage(self, po, askcb=None, fullaskcb=None):
        """Retrieve a key for a package. If needed, use the given
        callback to prompt whether the key should be imported.

        :param po: the package object to retrieve the key of
        :param askcb: Callback function to use to ask permission to
           import a key.  The arguments *askck* should take are the
           package object, the userid of the key, and the keyid
        :param fullaskcb: Callback function to use to ask permission to
           import a key.  This differs from *askcb* in that it gets
           passed a dictionary so that we can expand the values passed.
        :raises: :class:`dnf.exceptions.Error` if there are errors
           retrieving the keys
        """
        repo = self.repos[po.repoid]
        keyurls = repo.gpgkey
        key_installed = False

        def _prov_key_data(msg):
            msg += _('\n\n\n'
                     ' Failing package is: %s\n'
                     ' GPG Keys are configured as: %s\n'
                     ) % (po, ", ".join(repo.gpgkey))
            return msg

        user_cb_fail = False
        for keyurl in keyurls:
            keys = self._retrievePublicKey(keyurl, repo)

            for info in keys:
                ts = self.rpmconn.readonly_ts
                # Check if key is already installed
                if misc.keyInstalled(ts, info['keyid'], info['timestamp']) >= 0:
                    msg = _('GPG key at %s (0x%s) is already installed')
                    self.logger.info(msg, keyurl, info['hexkeyid'])
                    continue

                if repo.gpgcakey and info['has_sig'] and info['valid_sig']:
                    key_installed = True
                else:
                    # Try installing/updating GPG key
                    self._log_key_import(info, keyurl)
                    rc = False
                    if self.conf.assumeno:
                        rc = False
                    elif self.conf.assumeyes:
                        rc = True

                    # grab the .sig/.asc for the keyurl, if it exists if it
                    # does check the signature on the key if it is signed by
                    # one of our ca-keys for this repo or the global one then
                    # rc = True else ask as normal.

                    elif fullaskcb:
                        rc = fullaskcb({"po": po, "userid": info['userid'],
                                        "hexkeyid": info['hexkeyid'],
                                        "keyurl": keyurl,
                                        "fingerprint": info['fingerprint'],
                                        "timestamp": info['timestamp']})
                    elif askcb:
                        rc = askcb(po, info['userid'], info['hexkeyid'])

                    if not rc:
                        user_cb_fail = True
                        continue

                # Import the key
                result = ts.pgpImportPubkey(misc.procgpgkey(info['raw_key']))
                if result != 0:
                    msg = _('Key import failed (code %d)') % result
                    raise dnf.exceptions.Error(_prov_key_data(msg))
                self.logger.info(_('Key imported successfully'))
                key_installed = True

        if not key_installed and user_cb_fail:
            raise dnf.exceptions.Error(_("Didn't install any keys"))

        if not key_installed:
            msg = _('The GPG keys listed for the "%s" repository are ' \
                  'already installed but they are not correct for this ' \
                  'package.\n' \
                  'Check that the correct key URLs are configured for ' \
                  'this repository.') % repo.name
            raise dnf.exceptions.Error(_prov_key_data(msg))

        # Check if the newly installed keys helped
        result, errmsg = self.sigCheckPkg(po)
        if result != 0:
            msg = _("Import of key(s) didn't help, wrong key(s)?")
            self.logger.info(msg)
            errmsg = ucd(errmsg)
            raise dnf.exceptions.Error(_prov_key_data(errmsg))

    def _getAnyKeyForRepo(self, repo, destdir, keyurl_list, is_cakey=False,
                          callback=None):
        """
        Retrieve a key for a repository If needed, prompt for if the key should
        be imported using callback

        @param repo: Repository object to retrieve the key of.
        @param destdir: destination of the gpg pub ring
        @param keyurl_list: list of urls for gpg keys
        @param is_cakey: bool - are we pulling in a ca key or not
        @param callback: Callback function to use for asking for permission to
                         import a key. This is verification, but also "choice".
                         Takes a dictionary of key info.
        """

        key_installed = False

        def _prov_key_data(msg):
            cakeytxt = _("No")
            if is_cakey:
                cakeytxt = _("Yes")
            msg += _('\n\n\n'
                     ' CA Key: %s\n'
                     ' Failing repo is: %s\n'
                     ' GPG Keys are configured as: %s\n'
                     ) % (cakeytxt, repo, ", ".join(keyurl_list))
            return msg

        user_cb_fail = False
        for keyurl in keyurl_list:
            keys = self._retrievePublicKey(keyurl, repo, getSig=not is_cakey)
            for info in keys:
                # Check if key is already installed
                keyids = misc.return_keyids_from_pubring(destdir)
                if hex(int(info['keyid']))[2:-1].upper() in keyids:
                    msg = _('GPG key at %s (0x%s) is already imported')
                    self.logger.info(msg, keyurl, info['hexkeyid'])
                    key_installed = True
                    continue
                # Try installing/updating GPG key
                if is_cakey:
                    # know where the 'imported_cakeys' file is
                    ikf = self.conf._repos_persistdir + '/imported_cakeys'
                    keytype = 'CA'
                    cakeys = []
                    try:
                        cakeys_d = open(ikf, 'r').read()
                        cakeys = cakeys_d.split('\n')
                    except (IOError, OSError):
                        pass
                    if str(info['hexkeyid']) in cakeys:
                        key_installed = True
                else:
                    keytype = 'GPG'
                    if repo.gpgcakey and info['has_sig'] and info['valid_sig']:
                        key_installed = True

                if not key_installed:
                    self._log_key_import(info, keyurl, keytype)
                    rc = False
                    if self.conf.assumeno:
                        rc = False
                    elif self.conf.assumeyes:
                        rc = True

                    elif callback:
                        rc = callback({"repo": repo, "userid": info['userid'],
                                       "hexkeyid": info['hexkeyid'],
                                       "keyurl": keyurl,
                                       "fingerprint": info['fingerprint'],
                                       "timestamp": info['timestamp']})


                    if not rc:
                        user_cb_fail = True
                        continue

                # Import the key
                result = misc.import_key_to_pubring(info['raw_key'],
                                                    info['hexkeyid'],
                                                    gpgdir=destdir)
                if not result:
                    msg = _('Key %s import failed') % info['hexkeyid']
                    raise dnf.exceptions.Error(_prov_key_data(msg))
                self.logger.info(_('Key imported successfully'))
                key_installed = True
                # write out the key id to imported_cakeys in the repos basedir
                if is_cakey and key_installed:
                    if info['hexkeyid'] not in cakeys:
                        ikfo = open(ikf, 'a')
                        try:
                            ikfo.write(info['hexkeyid']+'\n')
                            ikfo.flush()
                            ikfo.close()
                        except (IOError, OSError):
                            # not-critical
                            pass

        if not key_installed and user_cb_fail:
            msg = _("Didn't install any keys for repo %s") % repo
            raise dnf.exceptions.Error(_prov_key_data(msg))

        if not key_installed:
            msg = \
                  _('The GPG keys listed for the "%s" repository are ' \
                  'already installed but they are not correct.\n' \
                  'Check that the correct key URLs are configured for ' \
                  'this repository.') % (repo.name)
            raise dnf.exceptions.Error(_prov_key_data(msg))

    def getCAKeyForRepo(self, repo, callback=None):
        """Retrieve a key for a repository.  If needed, use the given
        callback to prompt whether the key should be imported.

        :param repo: repository object to retrieve the key of
        :param callback: callback function to use for asking for
           verification of key information
        """
        self._getAnyKeyForRepo(repo, repo.gpgcadir, repo.gpgcakey, is_cakey=True,
                               callback=callback)

    def _run_rpm_check(self):
        results = []
        self.ts.check()
        for prob in self.ts.problems():
            #  Newer rpm (4.8.0+) has problem objects, older have just strings.
            #  Should probably move to using the new objects, when we can. For
            # now just be compatible.
            results.append(ucd(prob))

        return results

    def _store_config_in_history(self):
        self.history.write_addon_data('config-main', self.conf.dump())
        myrepos = ''
        for repo in self.repos.iter_enabled():
            myrepos += repo.dump()
            myrepos += '\n'
        self.history.write_addon_data('config-repos', myrepos)

    def _add_reinstall_rpm_probfilters(self):
        self.rpm_probfilter.add(rpm.RPMPROB_FILTER_REPLACEPKG)

########NEW FILE########
__FILENAME__ = callback
# callbacks.py
# Abstract interfaces to communicate progress on tasks.
#
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
import dnf.yum.rpmtrans

STATUS_OK = None # :api
STATUS_FAILED = 1 # :api
STATUS_ALREADY_EXISTS = 2 # :api
STATUS_MIRROR = 3  # :api
STATUS_DRPM = 4    # :api


class Payload(object):
    # :api

    def __init__(self, progress):
        self.progress = progress

    def __str__(self):
        """Nice, human-readable representation. :api"""
        pass

    @property
    def download_size(self):
        """Total size of the download. :api"""
        pass


class DownloadProgress(object):
    # :api

    def end(self, payload, status, msg):
        """Communicate the information that `payload` has finished downloading.

        :api, `status` is a constant denoting the type of outcome, `err_msg` is an
        error message in case the outcome was an error.

        """
        pass

    def message(self, msg):
        pass

    def progress(self, payload, done):
        """Update the progress display. :api

        `payload` is the payload this call reports progress for, `done` is how
        many bytes of this payload are already downloaded.

        """

        pass

    def start(self, total_files, total_size):
        """Start new progress metering. :api

        `total_files` the number of files that will be downloaded,
        `total_size` total size of all files.

        """

        pass

class NullDownloadProgress(DownloadProgress):
    pass

class Depsolve(object):
    def start(self):
        pass

    def pkg_added(self, pkg, mode):
        pass

    def end(self):
        pass

# alias for RPM transaction callback that logs things to a file
LoggingTransactionDisplay = dnf.yum.rpmtrans.LoggingTransactionDisplay

########NEW FILE########
__FILENAME__ = cli
# Copyright 2005 Duke University
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
#
# Written by Seth Vidal

"""
Command line interface yum class and related.
"""

from __future__ import print_function
from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.cli import CliError
from dnf.i18n import ucd, _, P_

import dnf
import dnf.cli.commands
import dnf.cli.commands.autoerase
import dnf.cli.commands.downgrade
import dnf.cli.commands.group
import dnf.cli.commands.install
import dnf.cli.commands.reinstall
import dnf.cli.commands.search
import dnf.cli.commands.upgrade
import dnf.cli.commands.distrosync
import dnf.cli.demand
import dnf.cli.option_parser
import dnf.conf
import dnf.const
import dnf.exceptions
import dnf.cli.format
import dnf.logging
import dnf.plugin
import dnf.persistor
import dnf.sack
import dnf.util
import dnf.yum.config
import dnf.yum.misc
import dnf.yum.parser
import hawkey
import logging
import operator
import os
from . import output
import random
import re
import signal
import sys
import time


def _add_pkg_simple_list_lens(data, pkg, indent=''):
    """ Get the length of each pkg's column. Add that to data.
        This "knows" about simpleList and printVer. """
    na = len(pkg.name) + 1 + len(pkg.arch) + len(indent)
    ver = len(pkg.evr)
    rid = len(pkg.reponame)
    for (d, v) in (('na', na), ('ver', ver), ('rid', rid)):
        data[d].setdefault(v, 0)
        data[d][v] += 1

def _list_cmd_calc_columns(output, ypl):
    """ Work out the dynamic size of the columns to pass to fmtColumns. """
    data = {'na' : {}, 'ver' : {}, 'rid' : {}}
    for lst in (ypl.installed, ypl.available, ypl.extras,
                ypl.updates, ypl.recent):
        for pkg in lst:
            _add_pkg_simple_list_lens(data, pkg)
    if len(ypl.obsoletes) > 0:
        for (npkg, opkg) in ypl.obsoletesTuples:
            _add_pkg_simple_list_lens(data, npkg)
            _add_pkg_simple_list_lens(data, opkg, indent=" " * 4)

    data = [data['na'], data['ver'], data['rid']]
    columns = output.calcColumns(data, remainder_column=1)
    return (-columns[0], -columns[1], -columns[2])

def sigquit(signum, frame):
    """SIGQUIT handler for the yum cli.  This function will print an
    error message and exit the program.

    :param signum: unused
    :param frame: unused
    """
    print("Quit signal sent - exiting immediately", file=sys.stderr)
    sys.exit(1)

def print_versions(pkgs, base, output):
    def sm_ui_time(x):
        return time.strftime("%Y-%m-%d %H:%M", time.gmtime(x))
    def sm_ui_date(x): # For changelogs, there is no time
        return time.strftime("%Y-%m-%d", time.gmtime(x))

    rpmdb_sack = dnf.sack.rpmdb_sack(base)
    done = False
    for pkg in rpmdb_sack.query().installed().filter(name=pkgs):
        if done:
            print("")
        done = True
        if pkg.epoch == '0':
            ver = '%s-%s.%s' % (pkg.version, pkg.release, pkg.arch)
        else:
            ver = '%s:%s-%s.%s' % (pkg.epoch,
                                   pkg.version, pkg.release, pkg.arch)
        name = "%s%s%s" % (output.term.MODE['bold'], pkg.name,
                           output.term.MODE['normal'])
        print(_("  Installed: %s-%s at %s") %(name, ver,
                                              sm_ui_time(pkg.installtime)))
        print(_("  Built    : %s at %s") % (pkg.packager if pkg.packager else "",
                                            sm_ui_time(pkg.buildtime)))
        # :hawkey, no changelist information yet
        # print(_("  Committed: %s at %s") % (pkg.committer,
        #                                    sm_ui_date(pkg.committime)))

class BaseCli(dnf.Base):
    """This is the base class for yum cli."""

    def __init__(self):
        # handle sigquit early on
        signal.signal(signal.SIGQUIT, sigquit)
        dnf.Base.__init__(self)
        self.output = output.Output(self, self.conf)
        self.logger = logging.getLogger("dnf")

    def errorSummary(self, errstring):
        """Parse the error string for 'interesting' errors which can
        be grouped, such as disk space issues.

        :param errstring: the error string
        :return: a string containing a summary of the errors
        """
        summary = ''
        # do disk space report first
        p = re.compile('needs (\d+)MB on the (\S+) filesystem')
        disk = {}
        for m in p.finditer(errstring):
            if m.group(2) not in disk:
                disk[m.group(2)] = int(m.group(1))
            if disk[m.group(2)] < int(m.group(1)):
                disk[m.group(2)] = int(m.group(1))

        if disk:
            summary += _('Disk Requirements:\n')
            for k in disk:
                summary += P_('  At least %dMB more space needed on the %s filesystem.\n', '  At least %dMB more space needed on the %s filesystem.\n', disk[k]) % (disk[k], k)

        # TODO: simplify the dependency errors?

        # Fixup the summary
        summary = _('Error Summary\n-------------\n') + summary

        return summary

    def do_transaction(self):
        """Take care of package downloading, checking, user
        confirmation and actually running the transaction.

        :return: a numeric return code, and optionally a list of
           errors.  A negative return code indicates that errors
           occurred in the pre-transaction checks
        """
        # make sure there's something to do
        if len(self._transaction) == 0:
            msg = _('Trying to run the transaction but nothing to do. Exiting.')
            self.logger.info(msg)
            return -1, None

        lsts = self.output.list_transaction(self.transaction)
        self.logger.info(lsts)
        # Check which packages have to be downloaded
        downloadpkgs = []
        rmpkgs = []
        stuff_to_download = False
        install_only = True
        for tsi in self._transaction:
            installed = tsi.installed
            if installed is not None:
                stuff_to_download = True
                downloadpkgs.append(installed)
            erased = tsi.erased
            if erased is not None:
                install_only = False
                rmpkgs.append(erased)

        # Close the connection to the rpmdb so that rpm doesn't hold the SIGINT
        # handler during the downloads.
        del self.ts

        # report the total download size to the user
        if not stuff_to_download:
            self.output.reportRemoveSize(rmpkgs)
        else:
            self.output.reportDownloadSize(downloadpkgs, install_only)

        # confirm with user
        if self._promptWanted():
            if self.conf.assumeno or not self.output.userconfirm():
                self.logger.info(_('Exiting on user Command'))
                return -1, None


        if downloadpkgs:
            self.logger.info(_('Downloading Packages:'))
        try:
            total_cb = self.output.download_callback_total_cb
            self.download_packages(downloadpkgs, self.output.progress, total_cb)
        except dnf.exceptions.DownloadError as e:
            specific = dnf.cli.format.indent_block(str(e))
            errstring = _('Error downloading packages:\n%s') % specific
            raise dnf.exceptions.Error(errstring)

        # Check GPG signatures
        if self.gpgsigcheck(downloadpkgs) != 0:
            return -1, None

        display = output.CliTransactionDisplay()
        return_code, resultmsgs = super(BaseCli, self).do_transaction(display)
        if return_code == 0:
            msg = self.output.post_transaction_output(self.transaction)
            self.logger.info(msg)
        return return_code, resultmsgs

    def gpgsigcheck(self, pkgs):
        """Perform GPG signature verification on the given packages,
        installing keys if possible.

        :param pkgs: a list of package objects to verify the GPG
           signatures of
        :return: non-zero if execution should stop due to an error
        :raises: Will raise :class:`Error` if there's a problem
        """
        for po in pkgs:
            result, errmsg = self.sigCheckPkg(po)

            if result == 0:
                # Verified ok, or verify not req'd
                continue

            elif result == 1:
                ay = self.conf.assumeyes and not self.conf.assumeno
                if not sys.stdin.isatty() and not ay:
                    raise dnf.exceptions.Error(_('Refusing to automatically import keys when running ' \
                            'unattended.\nUse "-y" to override.'))

                # the callback here expects to be able to take options which
                # userconfirm really doesn't... so fake it
                fn = lambda x, y, z: self.output.userconfirm()
                self.getKeyForPackage(po, fn)

            else:
                # Fatal error
                raise dnf.exceptions.Error(errmsg)

        return 0

    def check_updates(self, patterns=(), reponame=None, print_=True):
        """Check updates matching given *patterns* in selected repository."""
        ypl = self.returnPkgLists('upgrades', patterns, reponame=reponame)
        if self.conf.obsoletes or self.conf.verbose:
            typl = self.returnPkgLists('obsoletes', patterns, reponame=reponame)
            ypl.obsoletes = typl.obsoletes
            ypl.obsoletesTuples = typl.obsoletesTuples

        if print_:
            columns = _list_cmd_calc_columns(self.output, ypl)
            if len(ypl.updates) > 0:
                local_pkgs = {}
                highlight = self.output.term.MODE['bold']
                if highlight:
                    # Do the local/remote split we get in "yum updates"
                    for po in sorted(ypl.updates):
                        local = po.localPkg()
                        if os.path.exists(local) and po.verifyLocalPkg():
                            local_pkgs[(po.name, po.arch)] = po

                cul = self.conf.color_update_local
                cur = self.conf.color_update_remote
                self.output.listPkgs(ypl.updates, '', outputType='list',
                              highlight_na=local_pkgs, columns=columns,
                              highlight_modes={'=' : cul, 'not in' : cur})
            if len(ypl.obsoletes) > 0:
                print(_('Obsoleting Packages'))
                # The tuple is (newPkg, oldPkg) ... so sort by new
                for obtup in sorted(ypl.obsoletesTuples,
                                    key=operator.itemgetter(0)):
                    self.output.updatesObsoletesList(obtup, 'obsoletes',
                                                     columns=columns)

        return ypl.updates or ypl.obsoletes

    def upgrade_userlist_to(self, userlist, reponame=None):
        oldcount = self._goal.req_length()
        for l in userlist:
            self.upgrade_to(l, reponame)
        cnt = self._goal.req_length() - oldcount
        if cnt <= 0:
            raise dnf.exceptions.Error(_('No packages marked for upgrade.'))

    def distro_sync_userlist(self, userlist):
        """ Upgrade or downgrade packages to match the latest versions available
            in the enabled repositories.

            :return: (exit_code, [ errors ])

            exit_code is::
                0 = we're done, exit
                1 = we've errored, exit with error string
                2 = we've got work yet to do, onto the next stage
        """
        oldcount = self._goal.req_length()
        if len(userlist) == 0:
            self.distro_sync()
        else:
            for pkg_spec in userlist:
                self.distro_sync(pkg_spec)

        cnt = self._goal.req_length() - oldcount
        if cnt <= 0 and not self._goal.req_has_distupgrade_all():
            msg = _('No packages marked for distribution synchronization.')
            raise dnf.exceptions.Error(msg)

    def downgradePkgs(self, userlist):
        """Attempt to take the user specified list of packages or
        wildcards and downgrade them. If a complete version number if
        specified, attempt to downgrade them to the specified version

        :param userlist: a list of names or wildcards specifying
           packages to downgrade
        :return: (exit_code, [ errors ])

        exit_code is::

            0 = we're done, exit
            1 = we've errored, exit with error string
            2 = we've got work yet to do, onto the next stage
        """

        oldcount = self._goal.req_length()

        for arg in userlist:
            if arg.endswith('.rpm'):
                pkg = self.add_remote_rpm(arg)
                self.package_downgrade(pkg)
                continue # it was something on disk and it ended in rpm
                         # no matter what we don't go looking at repos

            try:
                self.downgrade(arg)
            except dnf.exceptions.PackageNotFoundError as err:
                msg = _('No package %s%s%s available.')
                self.logger.info(msg, self.output.term.MODE['bold'], arg,
                                 self.output.term.MODE['normal'])
            except dnf.exceptions.PackagesNotInstalledError as err:
                for pkg in err.packages:
                    self.logger.info(_('No match for available package: %s'), pkg)
            except dnf.exceptions.MarkingError:
                assert False
        cnt = self._goal.req_length() - oldcount
        if cnt <= 0:
            raise dnf.exceptions.Error(_('Nothing to do.'))

    def output_packages(self, basecmd, pkgnarrow='all', patterns=(), reponame=None):
        """Output selection *pkgnarrow* of packages matching *patterns* and *repoid*."""
        try:
            highlight = self.output.term.MODE['bold']
            ypl = self.returnPkgLists(
                pkgnarrow, patterns, installed_available=highlight, reponame=reponame)
        except dnf.exceptions.Error as e:
            return 1, [str(e)]
        else:
            update_pkgs = {}
            inst_pkgs = {}
            local_pkgs = {}

            columns = None
            if basecmd == 'list':
                # Dynamically size the columns
                columns = _list_cmd_calc_columns(self.output, ypl)

            if highlight and ypl.installed:
                #  If we have installed and available lists, then do the
                # highlighting for the installed packages so you can see what's
                # available to update, an extra, or newer than what we have.
                for pkg in (ypl.hidden_available +
                            ypl.reinstall_available +
                            ypl.old_available):
                    key = (pkg.name, pkg.arch)
                    if key not in update_pkgs or pkg > update_pkgs[key]:
                        update_pkgs[key] = pkg

            if highlight and ypl.available:
                #  If we have installed and available lists, then do the
                # highlighting for the available packages so you can see what's
                # available to install vs. update vs. old.
                for pkg in ypl.hidden_installed:
                    key = (pkg.name, pkg.arch)
                    if key not in inst_pkgs or pkg > inst_pkgs[key]:
                        inst_pkgs[key] = pkg

            if highlight and ypl.updates:
                # Do the local/remote split we get in "yum updates"
                for po in sorted(ypl.updates):
                    if po.reponame != hawkey.SYSTEM_REPO_NAME:
                        local_pkgs[(po.name, po.arch)] = po

            # Output the packages:
            clio = self.conf.color_list_installed_older
            clin = self.conf.color_list_installed_newer
            clir = self.conf.color_list_installed_reinstall
            clie = self.conf.color_list_installed_extra
            rip = self.output.listPkgs(ypl.installed, _('Installed Packages'), basecmd,
                                highlight_na=update_pkgs, columns=columns,
                                highlight_modes={'>' : clio, '<' : clin,
                                                 '=' : clir, 'not in' : clie})
            clau = self.conf.color_list_available_upgrade
            clad = self.conf.color_list_available_downgrade
            clar = self.conf.color_list_available_reinstall
            clai = self.conf.color_list_available_install
            rap = self.output.listPkgs(ypl.available, _('Available Packages'), basecmd,
                                highlight_na=inst_pkgs, columns=columns,
                                highlight_modes={'<' : clau, '>' : clad,
                                                 '=' : clar, 'not in' : clai})
            rep = self.output.listPkgs(ypl.extras, _('Extra Packages'), basecmd,
                                columns=columns)
            cul = self.conf.color_update_local
            cur = self.conf.color_update_remote
            rup = self.output.listPkgs(ypl.updates, _('Upgraded Packages'), basecmd,
                                highlight_na=local_pkgs, columns=columns,
                                highlight_modes={'=' : cul, 'not in' : cur})

            # XXX put this into the ListCommand at some point
            if len(ypl.obsoletes) > 0 and basecmd == 'list':
            # if we've looked up obsolete lists and it's a list request
                rop = [0, '']
                print(_('Obsoleting Packages'))
                for obtup in sorted(ypl.obsoletesTuples,
                                    key=operator.itemgetter(0)):
                    self.output.updatesObsoletesList(obtup, 'obsoletes',
                                                     columns=columns)
            else:
                rop = self.output.listPkgs(ypl.obsoletes, _('Obsoleting Packages'),
                                    basecmd, columns=columns)
            rrap = self.output.listPkgs(ypl.recent, _('Recently Added Packages'),
                                 basecmd, columns=columns)
            if len(patterns) and \
               rrap[0] and rop[0] and rup[0] and rep[0] and rap[0] and rip[0]:
                raise dnf.exceptions.Error(_('No matching Packages to list'))

    def returnPkgLists(self, pkgnarrow='all', patterns=None,
                       installed_available=False, reponame=None):
        """Return a :class:`dnf.yum.misc.GenericHolder` object containing
        lists of package objects that match the given names or wildcards.

        :param pkgnarrow: a string specifying which types of packages
           lists to produce, such as updates, installed, available, etc.
        :param patterns: a list of names or wildcards specifying
           packages to list
        :param installed_available: whether the available package list
           is present as .hidden_available when doing all, available,
           or installed
        :param reponame: limit packages list to the given repository

        :return: a :class:`dnf.yum.misc.GenericHolder` instance with the
           following lists defined::

             available = list of packageObjects
             installed = list of packageObjects
             upgrades = tuples of packageObjects (updating, installed)
             extras = list of packageObjects
             obsoletes = tuples of packageObjects (obsoleting, installed)
             recent = list of packageObjects
        """

        done_hidden_available = False
        done_hidden_installed = False
        if installed_available and pkgnarrow == 'installed':
            done_hidden_available = True
            pkgnarrow = 'all'
        elif installed_available and pkgnarrow == 'available':
            done_hidden_installed = True
            pkgnarrow = 'all'

        ypl = self.doPackageLists(
            pkgnarrow, patterns, ignore_case=True, reponame=reponame)
        if self.conf.showdupesfromrepos:
            ypl.available += ypl.reinstall_available

        if installed_available:
            ypl.hidden_available = ypl.available
            ypl.hidden_installed = ypl.installed
        if done_hidden_available:
            ypl.available = []
        if done_hidden_installed:
            ypl.installed = []
        return ypl

    def deplist(self, args):
        """Print out a formatted list of dependencies for a list of
        packages.  This is a cli wrapper method for
        :class:`dnf.yum.base.Base.findDeps`.

        :param args: a list of names or wildcards specifying packages
           that should have their dependenices printed
        :return: (exit_code, [ errors ])

        exit_code is::

            0 = we're done, exit
            1 = we've errored, exit with error string
            2 = we've got work yet to do, onto the next stage
        """
        # :dead
        pkgs = []
        for arg in args:
            if arg.endswith('.rpm'):
                # :hawkey
                # thispkg = dnf.yum.packages.YumUrlPackage(self, self.ts, arg)
                thispkg = None
                pkgs.append(thispkg)
            elif self.conf.showdupesfromrepos:
                pkgs.extend(self.pkgSack.returnPackages(patterns=[arg]))
            else:
                try:
                    pkgs.extend(self.pkgSack.returnNewestByName(patterns=[arg]))
                except dnf.exceptions.Error:
                    pass

        results = self.findDeps(pkgs)
        self.output.depListOutput(results)

    def provides(self, args):
        """Print out a list of packages that provide the given file or
        feature.  This a cli wrapper to the provides methods in the
        rpmdb and pkgsack.

        :param args: the name of a file or feature to search for
        :return: (exit_code, [ errors ])

        exit_code is::

            0 = we're done, exit
            1 = we've errored, exit with error string
            2 = we've got work yet to do, onto the next stage
        """
        # always in showdups mode
        old_sdup = self.conf.showdupesfromrepos
        self.conf.showdupesfromrepos = True

        matches = []
        for spec in args:
            matches.extend(super(BaseCli, self). provides(spec))
        for pkg in matches:
            self.output.matchcallback_verbose(pkg, [], args)
        self.conf.showdupesfromrepos = old_sdup

        if not matches:
            raise dnf.exceptions.Error(_('No Matches found'))

    def cleanCli(self, userlist):
        """Remove data from the yum cache directory.  What data is
        removed depends on the options supplied by the user.

        :param userlist: a list of options.  The following are valid
           options::

             expire-cache = Eliminate the local data saying when the
               metadata and mirror lists were downloaded for each
               repository.
             packages = Eliminate any cached packages
             metadata = Eliminate all of the files which yum uses to
               determine the remote availability of packages
             dbcache = Eliminate the sqlite cache used for faster
               access to metadata
             rpmdb = Eliminate any cached datat from the local rpmdb
             all = do all of the above
        :return: (exit_code, [ errors ])

        exit_code is::

            0 = we're done, exit
            1 = we've errored, exit with error string
            2 = we've got work yet to do, onto the next stage
        """
        pkgcode = xmlcode = dbcode = expccode = 0
        pkgresults = xmlresults = dbresults = expcresults = []
        msg = self.output.fmtKeyValFill(_('Cleaning repos: '),
                        ' '.join([ x.id for x in self.repos.iter_enabled()]))
        self.logger.info(msg)
        if 'all' in userlist:
            self.logger.info(_('Cleaning up Everything'))
            pkgcode, pkgresults = self.cleanPackages()
            xmlcode, xmlresults = self.cleanMetadata()
            dbcode, dbresults = self.clean_binary_cache()
            rpmcode, rpmresults = self.cleanRpmDB()

            code = pkgcode + xmlcode + dbcode + rpmcode
            results = (pkgresults + xmlresults + dbresults +
                       rpmresults)
            for msg in results:
                self.logger.debug(msg)
            return code, []

        if 'packages' in userlist:
            self.logger.debug(_('Cleaning up Packages'))
            pkgcode, pkgresults = self.cleanPackages()
        if 'metadata' in userlist:
            self.logger.debug(_('Cleaning up xml metadata'))
            xmlcode, xmlresults = self.cleanMetadata()
        if 'dbcache' in userlist or 'metadata' in userlist:
            self.logger.debug(_('Cleaning up database cache'))
            dbcode, dbresults =  self.clean_binary_cache()
        if 'expire-cache' in userlist or 'metadata' in userlist:
            self.logger.debug(_('Cleaning up expire-cache metadata'))
            expccode, expcresults = self.cleanExpireCache()
        if 'rpmdb' in userlist:
            self.logger.debug(_('Cleaning up cached rpmdb data'))
            expccode, expcresults = self.cleanRpmDB()

        results = pkgresults + xmlresults + dbresults + expcresults
        for msg in results:
            self.logger.info( msg)
        code = pkgcode + xmlcode + dbcode + expccode
        if code:
            raise dnf.exceptions.Error('Error cleaning up.')

    def _promptWanted(self):
        # shortcut for the always-off/always-on options
        if self.conf.assumeyes and not self.conf.assumeno:
            return False
        if self.conf.alwaysprompt:
            return True

        # prompt if:
        #  package was added to fill a dependency
        #  package is being removed
        #  package wasn't explicitly given on the command line
        for txmbr in self.tsInfo.getMembers():
            if txmbr.isDep or \
                   txmbr.name not in self.extcmds:
                return True

        # otherwise, don't prompt
        return False

    @staticmethod
    def transaction_id_or_offset(extcmd):
        """Convert user input to a transaction ID or an offset from the end."""
        try:
            offset_str, = re.match('^last(-\d+)?$', extcmd).groups()
        except AttributeError:  # extcmd does not match the regex.
            id_ = int(extcmd)
            if id_ < 0:
                # Negative return values are reserved for offsets.
                raise ValueError('bad transaction ID given: %s' % extcmd)
            return id_
        else:
            # Was extcmd 'last-N' or just 'last'?
            offset = int(offset_str) if offset_str else 0
            # Return offsets as negative numbers, where -1 means the last
            # transaction as when indexing sequences.
            return offset - 1

    def _history_get_transactions(self, extcmds):
        if not extcmds:
            self.logger.critical(_('No transaction ID given'))
            return None

        tids = []
        last = None
        for extcmd in extcmds:
            try:
                id_or_offset = self.transaction_id_or_offset(extcmd)
            except ValueError:
                self.logger.critical(_('Bad transaction ID given'))
                return None

            if id_or_offset < 0:
                if last is None:
                    cto = False
                    last = self.history.last(complete_transactions_only=cto)
                    if last is None:
                        self.logger.critical(_('Bad transaction ID given'))
                        return None
                tids.append(str(last.tid + id_or_offset + 1))
            else:
                tids.append(str(id_or_offset))

        old = self.history.old(tids)
        if not old:
            self.logger.critical(_('Not found given transaction ID'))
            return None
        return old

    def history_get_transaction(self, extcmds):
        old = self._history_get_transactions(extcmds)
        if old is None:
            return None
        if len(old) > 1:
            self.logger.critical(_('Found more than one transaction ID!'))
        return old[0]

    def history_rollback_transaction(self, extcmd):
        """Rollback given transaction."""
        old = self.history_get_transaction((extcmd,))
        if old is None:
            return 1, ['Failed history rollback, no transaction']
        last = self.history.last()
        if last is None:
            return 1, ['Failed history rollback, no last?']
        if old.tid == last.tid:
            return 0, ['Rollback to current, nothing to do']

        mobj = None
        for tid in self.history.old(list(range(old.tid + 1, last.tid + 1))):
            if tid.altered_lt_rpmdb:
                self.logger.warning(_('Transaction history is incomplete, before %u.'), tid.tid)
            elif tid.altered_gt_rpmdb:
                self.logger.warning(_('Transaction history is incomplete, after %u.'), tid.tid)

            if mobj is None:
                mobj = dnf.yum.history.YumMergedHistoryTransaction(tid)
            else:
                mobj.merge(tid)

        tm = time.ctime(old.beg_timestamp)
        print("Rollback to transaction %u, from %s" % (old.tid, tm))
        print(self.output.fmtKeyValFill("  Undoing the following transactions: ",
                                      ", ".join((str(x) for x in mobj.tid))))
        self.output.historyInfoCmdPkgsAltered(mobj)  # :todo

        history = dnf.history.open_history(self.history)  # :todo
        operations = dnf.history.NEVRAOperations()
        for id_ in range(old.tid + 1, last.tid + 1):
            operations += history.transaction_nevra_ops(id_)

        hibeg = self.output.term.MODE['bold']
        hiend = self.output.term.MODE['normal']
        try:
            self.history_undo_operations(operations)
        except dnf.exceptions.PackagesNotInstalledError as err:
            self.logger.info(_('No package %s%s%s installed.'),
                             hibeg, ucd(err.pkg_spec), hiend)
            return 1, ['A transaction cannot be undone']
        except dnf.exceptions.PackagesNotAvailableError as err:
            self.logger.info(_('No package %s%s%s available.'),
                             hibeg, ucd(err.pkg_spec), hiend)
            return 1, ['A transaction cannot be undone']
        except dnf.exceptions.MarkingError:
            assert False
        else:
            return 2, ["Rollback to transaction %u" % (old.tid,)]

    def history_undo_transaction(self, extcmd):
        """Undo given transaction."""
        old = self.history_get_transaction((extcmd,))
        if old is None:
            return 1, ['Failed history undo']

        tm = time.ctime(old.beg_timestamp)
        print("Undoing transaction %u, from %s" % (old.tid, tm))
        self.output.historyInfoCmdPkgsAltered(old)  # :todo

        history = dnf.history.open_history(self.history)  # :todo

        hibeg = self.output.term.MODE['bold']
        hiend = self.output.term.MODE['normal']
        try:
            self.history_undo_operations(history.transaction_nevra_ops(old.tid))
        except dnf.exceptions.PackagesNotInstalledError as err:
            self.logger.info(_('No package %s%s%s installed.'),
                             hibeg, ucd(err.pkg_spec), hiend)
            return 1, ['An operation cannot be undone']
        except dnf.exceptions.PackagesNotAvailableError as err:
            self.logger.info(_('No package %s%s%s available.'),
                             hibeg, ucd(err.pkg_spec), hiend)
            return 1, ['An operation cannot be undone']
        except dnf.exceptions.MarkingError:
            assert False
        else:
            return 2, ["Undoing transaction %u" % (old.tid,)]

class Cli(object):
    def __init__(self, base):
        self._system_cachedir = None
        self.demands = dnf.cli.demand.DemandSheet() #:cli
        self.logger = logging.getLogger("dnf")
        self.command = None
        self.base = base
        self.cli_commands = {}
        self.nogpgcheck = False

        self.register_command(dnf.cli.commands.autoerase.AutoeraseCommand)
        self.register_command(dnf.cli.commands.install.InstallCommand)
        self.register_command(dnf.cli.commands.upgrade.UpgradeCommand)
        self.register_command(dnf.cli.commands.UpgradeToCommand)
        self.register_command(dnf.cli.commands.InfoCommand)
        self.register_command(dnf.cli.commands.ListCommand)
        self.register_command(dnf.cli.commands.EraseCommand)
        self.register_command(dnf.cli.commands.group.GroupCommand)
        self.register_command(dnf.cli.commands.MakeCacheCommand)
        self.register_command(dnf.cli.commands.CleanCommand)
        self.register_command(dnf.cli.commands.ProvidesCommand)
        self.register_command(dnf.cli.commands.CheckUpdateCommand)
        self.register_command(dnf.cli.commands.search.SearchCommand)
        self.register_command(dnf.cli.commands.RepoListCommand)
        self.register_command(dnf.cli.commands.RepoPkgsCommand)
        self.register_command(dnf.cli.commands.HelpCommand)
        self.register_command(dnf.cli.commands.reinstall.ReinstallCommand)
        self.register_command(dnf.cli.commands.downgrade.DowngradeCommand)
        self.register_command(dnf.cli.commands.HistoryCommand)
        self.register_command(dnf.cli.commands.distrosync.DistroSyncCommand)

    def _configure_cachedir(self):
        # perform the CLI-specific cachedir tricks
        conf = self.base.conf
        suffix = dnf.yum.parser.varReplace(dnf.const.CACHEDIR_SUFFIX, conf.yumvar)
        cli_cache = dnf.conf.CliCache(conf.cachedir, suffix)
        conf.cachedir = cli_cache.cachedir
        self._system_cachedir = cli_cache.system_cachedir
        self.logger.debug("cachedir: %s", conf.cachedir)

    def _configure_repos(self, opts):
        self.base.read_all_repos()
        # Process repo enables and disables in order
        try:
            for (repo, operation) in opts.repos_ed:
                repolist = self.base.repos.get_matching(repo)
                if not repolist:
                    msg = _("Unknown repo: '%s'")
                    raise dnf.exceptions.RepoError(msg % repo)
                if operation == "enable":
                    repolist.enable()
                else:
                    repolist.disable()
        except dnf.exceptions.ConfigError as e:
            self.logger.critical(e)
            self.print_usage()
            sys.exit(1)

        if self.nogpgcheck:
            for repo in self.base.repos.values():
                repo.gpgcheck = False
                repo.repo_gpgcheck = False

        for rid in self.base._persistor.get_expired_repos():
            repo = self.base.repos.get(rid)
            if repo:
                repo.md_expire_cache()

        if opts.cacheonly:
            for repo in self.base.repos.values():
                repo.basecachedir = self._system_cachedir
                repo.md_only_cached = True

        # setup the progress bars/callbacks
        (bar, self.base.ds_callback) = self.base.output.setup_progress_callbacks()
        self.base.repos.all().set_progress_bar(bar)
        confirm_func = self.base.output._cli_confirm_gpg_key_import
        self.base.repos.all().confirm_func = confirm_func

    def _root_and_conffile(self, installroot, conffile):
        """After the first parse of the cmdline options, find initial values for
        installroot and conffile.

        :return: installroot and conffile strings
        """
        # If the conf file is inside the  installroot - use that.
        # otherwise look for it in the normal root
        if installroot and conffile:
            abs_fn = os.path.join(installroot, conffile)
            if os.access(abs_fn, os.R_OK):
                conffile = abs_fn
        elif installroot:
            conffile = dnf.const.CONF_FILENAME
            abs_fn = os.path.join(installroot, conffile[1:])
            if os.access(abs_fn, os.R_OK):
                conffile = abs_fn
        if installroot is None:
            installroot = '/'
        if conffile is None:
            conffile = dnf.const.CONF_FILENAME
        return installroot, conffile

    def _parse_commands(self):
        """Read :attr:`self.cmds` and parse them out to make sure that
        the requested base command and argument makes any sense at
        all.  This function will also set :attr:`self.base.basecmd` and
        :attr:`self.extcmds`.
        """
        self.logger.debug('dnf version: %s', dnf.const.VERSION)
        self.logger.log(dnf.logging.SUBDEBUG,
                        'Command: %s', self.cmdstring)
        self.logger.log(dnf.logging.SUBDEBUG,
                        'Installroot: %s', self.base.conf.installroot)
        if len(self.base.conf.commands) == 0 and len(self.base.cmds) < 1:
            self.base.cmds = self.base.conf.commands
        else:
            self.base.conf.commands = self.base.cmds
        if len(self.base.cmds) < 1:
            self.logger.critical(_('You need to give some command'))
            self.print_usage()
            raise CliError

        basecmd = self.base.cmds[0] # our base command
        command_cls = self.cli_commands.get(basecmd)
        if command_cls is None:
            self.logger.critical(_('No such command: %s. Please use %s --help'),
                                  basecmd, sys.argv[0])
            raise CliError
        self.command = command_cls(self)

        (base, ext) = self.command.canonical(self.base.cmds)
        self.base.basecmd, self.base.extcmds = (base, ext)
        self.logger.log(dnf.logging.SUBDEBUG, 'Base command: %s', base)
        self.logger.log(dnf.logging.SUBDEBUG, 'Extra commands: %s', ext)

    def _parse_setopts(self, setopts):
        """parse the setopts list handed to us and saves the results as
           repo_setopts and main_setopts in the base object"""

        repoopts = {}
        mainopts = dnf.yum.misc.GenericHolder()
        mainopts.items = []

        bad_setopt_tm = []
        bad_setopt_ne = []

        for item in setopts:
            vals = item.split('=')
            if len(vals) > 2:
                bad_setopt_tm.append(item)
                continue
            if len(vals) < 2:
                bad_setopt_ne.append(item)
                continue
            k,v = vals
            period = k.find('.')
            if period != -1:
                repo = k[:period]
                k = k[period+1:]
                if repo not in repoopts:
                    repoopts[repo] = dnf.yum.misc.GenericHolder()
                    repoopts[repo].items = []
                setattr(repoopts[repo], k, v)
                repoopts[repo].items.append(k)
            else:
                setattr(mainopts, k, v)
                mainopts.items.append(k)

        self.main_setopts = mainopts
        self.base.repo_setopts = repoopts

        return bad_setopt_tm, bad_setopt_ne

    def _get_first_config(self, opts):
        config_args = ['noplugins', 'version', "quiet", "verbose", 'conffile',
                       'debuglevel', 'errorlevel', 'installroot', 'releasever',
                       'setopt']
        in_dict = opts.__dict__
        return {k: in_dict[k] for k in in_dict if k in config_args}

    def configure(self, args):
        """Parse command line arguments, and set up :attr:`self.base.conf` and
        :attr:`self.cmds`, as well as logger objects in base instance.

        :param args: a list of command line arguments
        """
        self.optparser = dnf.cli.option_parser.OptionParser()
        opts, cmds = self.optparser.parse_known_args(args)

        # Just print out the version if that's what the user wanted
        if opts.version:
            print(dnf.const.VERSION)
            opts.quiet = True
            opts.verbose = False

        # go through all the setopts and set the global ones
        bad_setopt_tm, bad_setopt_ne = self._parse_setopts(opts.setopts)

        if self.main_setopts:
            for opt in self.main_setopts.items:
                setattr(opts, opt, getattr(self.main_setopts, opt))

        # get the install root to use
        self.optparser._checkAbsInstallRoot(opts.installroot)
        (root, opts.conffile) = self._root_and_conffile(opts.installroot,
                                                        opts.conffile)
        # the conffile is solid now
        assert(opts.conffile is not None)
        if opts.quiet:
            opts.debuglevel = 0
        if opts.verbose:
            opts.debuglevel = opts.errorlevel = dnf.const.VERBOSE_LEVEL

        # Read up configuration options and initialize plugins
        overrides = self.optparser._non_nones2dict(self._get_first_config(opts))
        releasever = opts.releasever
        try:
            self.read_conf_file(opts.conffile, root, releasever, overrides)

            # now set all the non-first-start opts from main from our setopts
            if self.main_setopts:
                for opt in self.main_setopts.items:
                    if not hasattr(self.base.conf, opt):
                        msg ="Main config did not have a %s attr. before setopt"
                        self.logger.warning(msg % opt)
                    setattr(self.base.conf, opt, getattr(self.main_setopts, opt))

        except (dnf.exceptions.ConfigError, ValueError) as e:
            self.logger.critical(_('Config error: %s'), e)
            sys.exit(1)
        except IOError as e:
            e = '%s: %s' % (ucd(e.args[1]), repr(e.filename))
            self.logger.critical(_('Config error: %s'), e)
            sys.exit(1)
        for item in bad_setopt_tm:
            msg = "Setopt argument has multiple values: %s"
            self.logger.warning(msg % item)
        for item in bad_setopt_ne:
            msg = "Setopt argument has no value: %s"
            self.logger.warning(msg % item)

        self.optparser.configure_from_options(opts, self.base.conf, self.demands,
                                              self.base.output)
        self.base.cmds = cmds

        if opts.version:
            opts.quiet = True
            opts.verbose = False
        if opts.quiet:
            opts.debuglevel = 0
        if opts.verbose:
            opts.debuglevel = opts.errorlevel = dnf.const.VERBOSE_LEVEL
        self.nogpgcheck = opts.nogpgcheck

        # the configuration reading phase is now concluded, finish the init
        self._configure_cachedir()
        # with cachedir in place we can configure stuff depending on it:
        self.base.activate_persistor()
        self._configure_repos(opts)

        if opts.version:
            print_versions(self.base.conf.history_record_packages, self.base,
                           self.base.output)
            sys.exit(0)

        if opts.sleeptime is not None:
            sleeptime = random.randrange(opts.sleeptime*60)
        else:
            sleeptime = 0

        # store the main commands & summaries, before plugins are loaded
        self.optparser.add_commands(self.cli_commands, 'main')
        if self.base.conf.plugins:
            self.base.plugins.load(self.base.conf.pluginpath, opts.disableplugins)
        self.base.plugins.run_init(self.base, self)
        # store the plugin commands & summaries
        self.optparser.add_commands(self.cli_commands,'plugin')

        # build the usage info and put it into the optparser.
        self.optparser.usage = self.optparser.get_usage()

        # show help if the user requests it
        # this is done here, because we first have the full
        # usage info after the plugins are loaded.
        if opts.help:
            self.optparser.print_help()
            sys.exit(0)

        # save our original args out
        self.base.args = args
        # save out as a nice command string
        self.cmdstring = dnf.const.PROGRAM_NAME + ' '
        for arg in self.base.args:
            self.cmdstring += '%s ' % arg

        try:
            self._parse_commands() # before we return check over the base command
                                  # + args make sure they match/make sense
        except CliError:
            sys.exit(1)
        self.command.configure(self.base.extcmds)

        if opts.debugsolver:
            self.base.conf.debug_solver = True

        self.base.plugins.run_config()
        # run the sleep - if it's unchanged then it won't matter
        time.sleep(sleeptime)

    def check(self):
        """Make sure the command line and options make sense."""
        self.command.doCheck(self.base.basecmd, self.base.extcmds)

    def read_conf_file(self, path=None, root="/", releasever=None,
                       overrides=None):
        conf_st = time.time()
        conf = self.base.conf
        conf.installroot = root
        conf.read(path)
        conf.releasever = releasever
        conf.yumvar_update_from_etc()

        if overrides is not None:
            conf.override(overrides)

        conf.logdir = dnf.yum.config.logdir_fit(conf.logdir)
        for opt in ('cachedir', 'logdir', 'persistdir'):
            conf.prepend_installroot(opt)
            conf._var_replace(opt)

        self.base.logging.setup_from_dnf_conf(conf)

        # repos are ver/arch specific so add $basearch/$releasever
        yumvar = conf.yumvar
        conf._repos_persistdir = os.path.normpath(
            '%s/repos/%s/%s/' % (conf.persistdir,
                                 yumvar.get('basearch', '$basearch'),
                                 yumvar.get('releasever', '$releasever')))
        self.logger.debug('Config time: %0.3f' % (time.time() - conf_st))
        return conf

    def register_command(self, command_cls):
        """Register a Command. :api"""
        for name in command_cls.aliases:
            if name in self.cli_commands:
                raise dnf.exceptions.ConfigError(_('Command "%s" already defined') % name)
            self.cli_commands[name] = command_cls

    def run(self):
        """Call the base command, and pass it the extended commands or
           arguments.

        :return: (exit_code, [ errors ])

        exit_code is::

            0 = we're done, exit
            1 = we've errored, exit with error string
            2 = we've got work yet to do, onto the next stage
        """
        if self.demands.refresh_metadata:
            self.base.cleanCli('expire-cache')
        if self.demands.sack_activation:
            lar = self.demands.available_repos
            self.base.fill_sack(load_system_repo='auto',
                                load_available_repos=lar)
            self.base.plugins.run_sack()
        return self.command.run(self.base.extcmds)

    def print_usage(self):
        return self.optparser.print_usage()

########NEW FILE########
__FILENAME__ = autoerase
# autoerase.py
# Autoerase CLI command.
#
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from .. import commands

import hawkey


class AutoeraseCommand(commands.Command):

    aliases = ('autoerase',)

    def configure(self, _):
        demands = self.cli.demands
        demands.available_repos = True
        demands.resolving = True
        demands.root_user = True
        demands.sack_activation = True

    def run(self, extcmds):
        base = self.base
        sack = base.sack
        goal = hawkey.Goal(sack)

        base.push_userinstalled(goal)

        solved = goal.run()
        if base.conf.debug_solver:
            goal.write_debugdata('./debugdata-autoerase')
        assert solved

        for pkg in goal.list_unneeded():
            base.package_remove(pkg)

########NEW FILE########
__FILENAME__ = distrosync
# distrosync.py
# distro-sync CLI command.
#
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from dnf.cli import commands
from dnf.i18n import _


class DistroSyncCommand(commands.Command):
    """A class containing methods needed by the cli to execute the
    distro-synch command.
    """

    aliases = ('distro-sync', 'distribution-synchronization')
    activate_sack = True
    resolve = True
    summary = _("Synchronize installed packages to the latest available versions")
    usage = '[%s...]' % _('PACKAGE')
    writes_rpmdb = True

    def doCheck(self, basecmd, extcmds):
        """Verify that conditions are met so that this command can run.
        These include that the program is being run by the root user,
        and that there are enabled repositories with gpg keys.

        :param basecmd: the name of the command
        :param extcmds: the command line arguments passed to *basecmd*
        """
        commands.checkGPGKey(self.base, self.cli)
        commands.checkEnabledRepo(self.base, extcmds)

    def run(self, extcmds):
        return self.base.distro_sync_userlist(extcmds)

########NEW FILE########
__FILENAME__ = downgrade
# downgrade.py
# Downgrade CLI command.
#
# Copyright (C) 2014 Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from .. import commands
from dnf.i18n import _

class DowngradeCommand(commands.Command):
    """A class containing methods needed by the cli to execute the
    downgrade command.
    """

    activate_sack = True
    aliases = ('downgrade',)
    resolve = True
    summary = _("downgrade a package")
    usage = "%s..."  % _('PACKAGE')
    writes_rpmdb = True

    def doCheck(self, basecmd, extcmds):
        """Verify that conditions are met so that this command can
        run.  These include that the program is being run by the root
        user, that there are enabled repositories with gpg keys, and
        that this command is called with appropriate arguments.


        :param basecmd: the name of the command
        :param extcmds: the command line arguments passed to *basecmd*
        """
        commands.checkGPGKey(self.base, self.cli)
        commands.checkPackageArg(self.cli, basecmd, extcmds)
        commands.checkEnabledRepo(self.base, extcmds)

    def run(self, extcmds):
        return self.base.downgradePkgs(extcmds)

########NEW FILE########
__FILENAME__ = group
# group.py
# Group CLI command.
#
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from .. import commands
from dnf.i18n import _, ucd

import dnf.cli
import dnf.exceptions
import dnf.util
import logging
import operator

logger = logging.getLogger("dnf")

def _ensure_grp_arg(cli, basecmd, extcmds):
    """Verify that *extcmds* contains the name of at least one group for
    *basecmd* to act on.

    :param base: a :class:`dnf.Base` object.
    :param basecmd: the name of the command being checked for
    :param extcmds: a list of arguments passed to *basecmd*
    :raises: :class:`cli.CliError`
    """
    if len(extcmds) == 0:
        cli.logger.critical(_('Error: Need a group or list of groups'))
        commands.err_mini_usage(cli, basecmd)
        raise dnf.cli.CliError


class CompsQuery(object):

    AVAILABLE = 1
    INSTALLED = 2

    ENVIRONMENTS = 1
    GROUPS = 2

    def __init__(self, comps, prst, kinds, status):
        self.comps = comps
        self.prst = prst
        self.kinds = kinds
        self.status = status

    def _get(self, items, persistence_fn):
        lst = []
        for it in items:
            installed = persistence_fn(it.id).installed
            if self.status & self.INSTALLED and installed:
                lst.append(it)
            if self.status & self.AVAILABLE and not installed:
                lst.append(it)
        return lst

    def get(self, *patterns):
        res = dnf.util.Bunch()
        res.environments = []
        res.groups = []
        for pat in patterns:
            envs = grps = None
            if self.kinds & self.ENVIRONMENTS:
                envs = self._get(self.comps.environments_by_pattern(pat),
                                 self.prst.environment)
                res.environments.extend(envs)
            if self.kinds & self.GROUPS:
                grps = self._get(self.comps.groups_by_pattern(pat),
                                 self.prst.group)
                res.groups.extend(grps)
            if not envs and not grps:
                msg = _("No relevant match for the specified '%s'.")
                msg = msg % ucd(pat)
                raise dnf.cli.CliError(msg)
        return res


class GroupCommand(commands.Command):
    """ Single sub-command interface for most groups interaction. """

    direct_commands = {'grouplist'    : 'list',
                       'groupinstall' : 'install',
                       'groupupdate'  : 'install',
                       'groupremove'  : 'remove',
                       'grouperase'   : 'remove',
                       'groupinfo'    : 'info'}
    aliases = ('group', 'groups') + tuple(direct_commands.keys())
    summary = _("Display, or use, the groups information")
    usage = "[list|info|summary|install|upgrade|remove|mark] [%s]" % _('GROUP')

    _CMD_ALIASES = {'update'     : 'upgrade',
                    'erase'      : 'remove'}
    _MARK_CMDS = ('install', 'remove')


    @staticmethod
    def _grp_cmd(extcmds):
        return extcmds[0], extcmds[1:]

    @staticmethod
    def _split_extcmds(extcmds):
        if extcmds[0] == 'with-optional':
            types = tuple(dnf.const.GROUP_PACKAGE_TYPES + ('optional',))
            return types, extcmds[1:]
        return dnf.const.GROUP_PACKAGE_TYPES, extcmds

    @classmethod
    def canonical(cls, command_list):
        first = command_list[0]
        rest = command_list[1:]

        cmd = cls.direct_commands.get(first)
        if cmd is None:
            cmd = 'summary'
            if rest:
                cmd = rest.pop(0)
        cmd = cls._CMD_ALIASES.get(cmd, cmd)

        rest.insert(0, cmd)
        return ('groups', rest)

    def __init__(self, cli):
        super(GroupCommand, self).__init__(cli)

    def _assert_comps(self):
        msg = _('No group data available for configured repositories.')
        if not len(self.base.comps):
            raise dnf.exceptions.CompsError(msg)

    def _environment_lists(self, patterns):
        def installed_pred(env):
            return self.base.group_persistor.environment(env.id).installed

        self._assert_comps()
        if patterns is None:
            envs = self.base.comps.environments
        else:
            envs = self.base.comps.environments_by_pattern(",".join(patterns))

        available, installed = dnf.util.partition(installed_pred, envs)

        sort_fn = operator.attrgetter('ui_name')
        return sorted(installed, key=sort_fn), sorted(available, key=sort_fn)

    def _group_lists(self, uservisible, patterns):
        def installed_pred(group):
            return self.base.group_persistor.group(group.id).installed
        installed = []
        available = []

        self._assert_comps()

        if patterns is None:
            grps = self.base.comps.groups
        else:
            grps = self.base.comps.groups_by_pattern(",".join(patterns))
        for grp in grps:
            tgt_list = available
            if installed_pred(grp):
                tgt_list = installed
            if not uservisible or grp.uservisible:
                tgt_list.append(grp)

        sort_fn = operator.attrgetter('ui_name')
        return sorted(installed, key=sort_fn), sorted(available, key=sort_fn)

    def _grp_setup(self):
        try:
            comps = self.base.read_comps()
        except dnf.exceptions.Error as e:
            return 1, [str(e)]
        if not comps:
            return 1, [_('No Groups Available in any repository')]

    def _info(self, userlist):
        for strng in userlist:
            group_matched = False
            for group in self.base.comps.groups_by_pattern(strng):
                self.output.displayPkgsInGroups(group)
                group_matched = True

            if not group_matched:
                logger.error(_('Warning: Group %s does not exist.'), strng)

        return 0, []

    def _install(self, extcmds):
        types, patterns = self._split_extcmds(extcmds)
        q = CompsQuery(self.base.comps, self.base.group_persistor,
                       CompsQuery.ENVIRONMENTS | CompsQuery.GROUPS,
                       CompsQuery.AVAILABLE)
        res = q.get(*patterns)
        for env in res.environments:
            self.base.environment_install(env, types)
        for grp in res.groups:
            self.base.group_install(grp, types)

    def _list(self, userlist):
        uservisible = 1

        if len(userlist) > 0:
            if userlist[0] == 'hidden':
                uservisible = 0
                userlist.pop(0)
        if not userlist:
            userlist = None # Match everything...

        env_inst, env_avail = self._environment_lists(userlist)
        installed, available = self._group_lists(uservisible, userlist)

        if not any([env_inst, env_avail, installed, available]):
            logger.error(_('Warning: No groups match: %s'),
                              ", ".join(userlist))
            return 0, []

        def _out_grp(sect, group):
            if not done:
                logger.info(sect)
            msg = '   %s' % group.ui_name
            if self.base.conf.verbose:
                msg += ' (%s)' % group.id
            if group.lang_only:
                msg += ' [%s]' % group.lang_only
            logger.info('%s', msg)

        def _out_env(sect, envs):
            if envs:
                logger.info(sect)
            for e in envs:
                msg = '   %s' % e.ui_name
                if self.base.conf.verbose:
                    msg += ' (%s)' % e.id
                logger.info(msg)

        _out_env(_('Available environment groups:'), env_avail)
        _out_env(_('Installed environment groups:'), env_inst)

        done = False
        for group in installed:
            if group.lang_only:
                continue
            _out_grp(_('Installed groups:'), group)
            done = True

        done = False
        for group in installed:
            if not group.lang_only:
                continue
            _out_grp(_('Installed language groups:'), group)
            done = True

        done = False
        for group in available:
            if group.lang_only:
                continue
            _out_grp(_('Available groups:'), group)
            done = True

        done = False
        for group in available:
            if not group.lang_only:
                continue
            _out_grp(_('Available language groups:'), group)
            done = True

        return 0, []

    def _mark_install(self, patterns):
        persistor = self.base.group_persistor
        q = CompsQuery(self.base.comps, persistor,
                       CompsQuery.GROUPS | CompsQuery.ENVIRONMENTS,
                       CompsQuery.AVAILABLE)
        solver = self.base.build_comps_solver()
        res = q.get(*patterns)
        types = dnf.comps.DEFAULT | dnf.comps.MANDATORY | dnf.comps.OPTIONAL
        for env in res.environments:
            solver.environment_install(env, types, None)
        if res.environments:
            logger.info(_('Environments marked installed: %s'),
                        ','.join([g.ui_name for g in res.environments]))
        for grp in res.groups:
            solver.group_install(grp, types, None)
        if res.groups:
            logger.info(_('Groups marked installed: %s'),
                        ','.join([g.ui_name for g in res.groups]))
        persistor.commit()

    def _mark_remove(self, patterns):
        persistor = self.base.group_persistor
        q = CompsQuery(self.base.comps, persistor,
                       CompsQuery.GROUPS | CompsQuery.ENVIRONMENTS,
                       CompsQuery.INSTALLED)
        solver = self.base.build_comps_solver()
        res = q.get(*patterns)
        for env in res.environments:
            solver.environment_remove(env)
        if res.environments:
            logger.info(_('Environments marked removed: %s'),
                        ','.join([g.ui_name for g in res.environments]))
        for grp in res.groups:
            solver.group_remove(grp)
        if res.groups:
            logger.info(_('Groups marked removed: %s'),
                        ','.join([g.ui_name for g in res.groups]))
        persistor.commit()

    def _mark_subcmd(self, extcmds):
        if extcmds[0] in self._MARK_CMDS:
            return extcmds[0], extcmds[1:]
        return 'install', extcmds

    def _remove(self, patterns):
        q = CompsQuery(self.base.comps, self.base.group_persistor,
                       CompsQuery.ENVIRONMENTS | CompsQuery.GROUPS,
                       CompsQuery.INSTALLED)
        res = q.get(*patterns)

        for env in res.environments:
            self.base.environment_remove(env)
        for grp in res.groups:
            self.base.group_remove(grp)

    def _summary(self, userlist):
        uservisible = 1
        if len(userlist) > 0:
            if userlist[0] == 'hidden':
                uservisible = 0
                userlist.pop(0)
        if not userlist:
            userlist = None # Match everything...

        installed, available = self._group_lists(uservisible, userlist)

        def _out_grp(sect, num):
            if not num:
                return
            logger.info('%s %u', sect, num)
        done = 0
        for group in installed:
            if group.lang_only:
                continue
            done += 1
        _out_grp(_('Installed Groups:'), done)

        done = 0
        for group in installed:
            if not group.lang_only:
                continue
            done += 1
        _out_grp(_('Installed Language Groups:'), done)

        done = False
        for group in available:
            if group.lang_only:
                continue
            done += 1
        _out_grp(_('Available Groups:'), done)

        done = False
        for group in available:
            if not group.lang_only:
                continue
            done += 1
        _out_grp(_('Available Language Groups:'), done)

        return 0, []

    def _upgrade(self, patterns):
        q = CompsQuery(self.base.comps, self.base.group_persistor,
                       CompsQuery.GROUPS, CompsQuery.INSTALLED)
        res = q.get(*patterns)
        cnt = 0
        for grp in res.groups:
            cnt += self.base.group_upgrade(grp)
        if not cnt:
            msg = _('No packages marked for upgrade.')
            raise dnf.cli.CliError(msg)

    def configure(self, extcmds):
        cmd = extcmds[0]
        demands = self.cli.demands
        demands.available_repos = True
        demands.sack_activation = True
        if cmd in ('install', 'mark', 'remove', 'upgrade'):
            demands.root_user = True

    def doCheck(self, basecmd, extcmds):
        """Verify that conditions are met so that this command can run.
        The exact conditions checked will vary depending on the
        subcommand that is being called.

        :param basecmd: the name of the command
        :param extcmds: the command line arguments passed to *basecmd*
        """
        cmd, extcmds = self._grp_cmd(extcmds)

        commands.checkEnabledRepo(self.base)

        if cmd in ('install', 'remove', 'mark', 'info'):
            _ensure_grp_arg(self.cli, cmd, extcmds)

        if cmd in ('install', 'upgrade'):
            commands.checkGPGKey(self.base, self.cli)

        cmds = ('list', 'info', 'remove', 'install', 'upgrade', 'summary', 'mark')
        if cmd not in cmds:
            logger.critical(_('Invalid groups sub-command, use: %s.'),
                                 ", ".join(cmds))
            raise dnf.cli.CliError

    def run(self, extcmds):
        cmd, extcmds = self._grp_cmd(extcmds)

        self._grp_setup()

        if cmd == 'summary':
            return self._summary(extcmds)
        if cmd == 'list':
            return self._list(extcmds)
        if cmd == 'info':
            return self._info(extcmds)
        if cmd == 'mark':
            (subcmd, extcmds) = self._mark_subcmd(extcmds)
            if subcmd == 'remove':
                return self._mark_remove(extcmds)
            else:
                assert subcmd == 'install'
                return self._mark_install(extcmds)

        self.cli.demands.resolving = True
        if cmd == 'install':
            return self._install(extcmds)
        if cmd == 'upgrade':
            return self._upgrade(extcmds)
        if cmd == 'remove':
            return self._remove(extcmds)

########NEW FILE########
__FILENAME__ = install
# install.py
# Install CLI command.
#
# Copyright (C) 2014 Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from .. import commands
from dnf.i18n import _

import dnf.exceptions
import functools
import operator

class InstallCommand(commands.Command):
    """A class containing methods needed by the cli to execute the
    install command.
    """

    aliases = ('install',)
    activate_sack = True
    resolve = True
    summary = _("Install a package or packages on your system")
    usage = "%s..." % _('PACKAGE')
    writes_rpmdb = True

    def doCheck(self, basecmd, extcmds):
        """Verify that conditions are met so that this command can run.
        These include that the program is being run by the root user,
        that there are enabled repositories with gpg keys, and that
        this command is called with appropriate arguments.

        :param basecmd: the name of the command
        :param extcmds: the command line arguments passed to *basecmd*
        """
        commands.checkGPGKey(self.base, self.cli)
        commands.checkPackageArg(self.cli, basecmd, extcmds)
        commands.checkEnabledRepo(self.base, extcmds)

    @staticmethod
    def parse_extcmds(extcmds):
        """Parse command arguments."""
        pkg_specs, grp_specs, filenames = [], [], []
        for argument in extcmds:
            if argument.endswith('.rpm'):
                filenames.append(argument)
            elif argument.startswith('@'):
                grp_specs.append(argument[1:])
            else:
                pkg_specs.append(argument)
        return pkg_specs, grp_specs, filenames

    def run(self, extcmds):
        pkg_specs, grp_specs, filenames = self.parse_extcmds(extcmds)

        # Install files.
        local_pkgs = map(self.base.add_remote_rpm, filenames)
        results = map(self.base.package_install, local_pkgs)
        done = functools.reduce(operator.or_, results, False)

        # Install groups.
        if grp_specs:
            self.base.read_comps()
        cnt = 0
        for spec in grp_specs:
            group = self.base.comps.group_by_pattern(spec)
            if group is None:
                msg = _("Warning: Group '%s' does not exist.")
                self.base.logger.error(msg, dnf.i18n.ucd(spec))
                continue
            cnt += self.base.group_install(group, dnf.const.GROUP_PACKAGE_TYPES)
        if grp_specs and not cnt:
            msg = _('No packages in any requested group available '\
                    'to install or upgrade.')
            raise dnf.exceptions.Error(msg)
        elif cnt:
            done = True

        # Install packages.
        for pkg_spec in pkg_specs:
            try:
                self.base.install(pkg_spec)
            except dnf.exceptions.MarkingError:
                msg = _('No package %s%s%s available.')
                self.base.logger.info(
                    msg, self.base.output.term.MODE['bold'], pkg_spec,
                    self.base.output.term.MODE['normal'])
            else:
                done = True

        if not done:
            raise dnf.exceptions.Error(_('Nothing to do.'))

########NEW FILE########
__FILENAME__ = reinstall
# reinstall.py
# Reinstall CLI command.
#
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from .. import commands
from dnf.i18n import _

import dnf.exceptions
import functools
import operator

class ReinstallCommand(commands.Command):
    """A class containing s needed by the cli to execute the
    reinstall command.
    """

    activate_sack = True
    aliases = ('reinstall',)
    resolve = True
    summary = _("reinstall a package")
    usage = "%s..." % _('PACKAGE')
    writes_rpmdb = True

    def doCheck(self, basecmd, extcmds):
        """Verify that conditions are met so that this command can
        run.  These include that the program is being run by the root
        user, that there are enabled repositories with gpg keys, and
        that this command is called with appropriate arguments.


        :param basecmd: the name of the command
        :param extcmds: the command line arguments passed to *basecmd*
        """
        commands.checkGPGKey(self.base, self.cli)
        commands.checkPackageArg(self.cli, basecmd, extcmds)
        commands.checkEnabledRepo(self.base, extcmds)

    @staticmethod
    def parse_extcmds(extcmds):
        """Parse command arguments."""
        pkg_specs, filenames = [], []
        for argument in extcmds:
            if argument.endswith('.rpm'):
                filenames.append(argument)
            else:
                pkg_specs.append(argument)
        return pkg_specs, filenames

    def run(self, extcmds):
        pkg_specs, filenames = self.parse_extcmds(extcmds)

        # Reinstall files.
        local_pkgs = map(self.base.add_remote_rpm, filenames)
        results = map(self.base.package_install, local_pkgs)
        done = functools.reduce(operator.or_, results, False)
        self.base._add_reinstall_rpm_probfilters()

        # Reinstall packages.
        for pkg_spec in pkg_specs:
            try:
                self.base.reinstall(pkg_spec)
            except dnf.exceptions.PackagesNotInstalledError:
                self.base.logger.info(_('No match for argument: %s'),
                                      pkg_spec)
            except dnf.exceptions.PackagesNotAvailableError as err:
                for pkg in err.packages:
                    xmsg = ''
                    yumdb_info = self.base.yumdb.get_package(pkg)
                    if 'from_repo' in yumdb_info:
                        xmsg = _(' (from %s)') % yumdb_info.from_repo
                    msg = _('Installed package %s%s%s%s not available.')
                    self.base.logger.info(
                        msg, self.base.output.term.MODE['bold'], pkg,
                        self.base.output.term.MODE['normal'], xmsg)
            except dnf.exceptions.MarkingError:
                assert False, 'Only the above marking errors are expected.'
            else:
                done = True

        if not done:
            raise dnf.exceptions.Error(_('Nothing to do.'))

########NEW FILE########
__FILENAME__ = search
# search.py
# Search CLI command.
#
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals
from .. import commands
from dnf.i18n import ucd, _

import dnf.i18n
import dnf.match_counter
import dnf.util
import hawkey


class SearchCommand(commands.Command):
    """A class containing methods needed by the cli to execute the
    search command.
    """

    aliases = ('search',)
    summary = _('Search package details for the given string')
    usage = _('QUERY_STRING')

    def _search(self, args):
        """Search for simple text tags in a package object."""

        def _print_match_section(text, keys):
            # Print them in the order they were passed
            used_keys = [arg for arg in args if arg in keys]
            formatted = self.base.output.fmtSection(text % ", ".join(used_keys))
            print(ucd(formatted))

        # prepare the input
        search_all = False
        if len(args) > 1 and args[0] == 'all':
            args.pop(0)
            search_all = True

        counter = dnf.match_counter.MatchCounter()
        for arg in args:
            self._search_counted(counter, 'name', arg)
            self._search_counted(counter, 'summary', arg)

        section_text = _('N/S Matched: %s')
        if search_all or counter.total() == 0:
            section_text = _('Matched: %s')
            for arg in args:
                self._search_counted(counter, 'description', arg)
                self._search_counted(counter, 'url', arg)

        matched_needles = None
        limit = None
        if not self.base.conf.showdupesfromrepos:
            limit = self.base.sack.query().filter(pkg=counter.keys()).latest()
        for pkg in counter.sorted(reverse=True, limit_to=limit):
            if matched_needles != counter.matched_needles(pkg):
                matched_needles = counter.matched_needles(pkg)
                _print_match_section(section_text, matched_needles)
            self.base.output.matchcallback(pkg, counter.matched_haystacks(pkg),
                                           args)

        if len(counter) == 0:
            self.base.logger.warning(_('Warning: No matches found for: %s'), arg)
            raise dnf.exceptions.Error(_('No Matches found'))

    def _search_counted(self, counter, attr, needle):
        fdict = {'%s__substr' % attr : needle}
        if dnf.util.is_glob_pattern(needle):
            fdict = {'%s__glob' % attr : needle}
        q = self.base.sack.query().filter(hawkey.ICASE, **fdict)
        for pkg in q.run():
            counter.add(pkg, attr, needle)
        return counter

    def configure(self, _):
        demands = self.cli.demands
        demands.available_repos = True
        demands.sack_activation = True

    def doCheck(self, basecmd, extcmds):
        commands.checkItemArg(self.cli, basecmd, extcmds)

    def run(self, extcmds):
        self.base.logger.debug(_('Searching Packages: '))
        return self._search(extcmds)

########NEW FILE########
__FILENAME__ = upgrade
# upgrade.py
# Upgrade CLI command.
#
# Copyright (C) 2014 Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from .. import commands
from dnf.i18n import _

import dnf.exceptions
import functools
import operator

class UpgradeCommand(commands.Command):
    """A class containing methods needed by the cli to execute the
    update command.
    """
    aliases = ('upgrade', 'update')
    activate_sack = True
    resolve = True
    summary = _("Upgrade a package or packages on your system")
    usage = "[%s...]" % _('PACKAGE')
    writes_rpmdb = True

    def doCheck(self, basecmd, extcmds):
        """Verify that conditions are met so that this command can run.

        These include that there are enabled repositories with gpg
        keys, and that this command is being run by the root user.

        :param basecmd: the name of the command
        :param extcmds: the command line arguments passed to *basecmd*
        """
        commands.checkGPGKey(self.base, self.cli)
        commands.checkEnabledRepo(self.base, extcmds)

    @staticmethod
    def parse_extcmds(extcmds):
        """Parse command arguments."""
        pkg_specs, filenames = [], []
        for argument in extcmds:
            if argument.endswith('.rpm'):
                filenames.append(argument)
            else:
                pkg_specs.append(argument)
        return pkg_specs, filenames

    def run(self, extcmds):
        pkg_specs, filenames = self.parse_extcmds(extcmds)

        if not pkg_specs and not filenames:
            # Update all packages.
            self.base.upgrade_all()
            done = True
        else:
            # Update files.
            local_pkgs = map(self.base.add_remote_rpm, filenames)
            results = map(self.base.package_upgrade, local_pkgs)
            done = functools.reduce(operator.or_, results, False)

            # Update packages.
            for pkg_spec in pkg_specs:
                try:
                    self.base.upgrade(pkg_spec)
                except dnf.exceptions.MarkingError:
                    self.base.logger.info(_('No match for argument: %s'),
                                          pkg_spec)
                else:
                    done = True

        if not done:
            raise dnf.exceptions.Error(_('No packages marked for upgrade.'))

########NEW FILE########
__FILENAME__ = demand
# demand.py
# Demand sheet and related classes.
#
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals

class _BoolDefault(object):
    def __init__(self, default):
        self.default = default
        self._storing_name = '__%s%x' % (self.__class__.__name__, id(self))

    def __get__(self, obj, objtype=None):
        objdict = obj.__dict__
        if self._storing_name in objdict:
            return objdict[self._storing_name]
        return self.default

    def __set__(self, obj, val):
        objdict = obj.__dict__
        if self._storing_name in objdict:
            current_val = objdict[self._storing_name]
            if current_val != val:
                raise AttributeError('Demand already set.')
        objdict[self._storing_name] = val

class DemandSheet(object):
    """Collection of demands that different CLI parts have on other parts. :api"""

    # :api...
    allow_erasing = _BoolDefault(False)
    available_repos = _BoolDefault(False)
    resolving = _BoolDefault(False)
    root_user = _BoolDefault(False)
    sack_activation = _BoolDefault(False)
    success_exit_status = 0

    refresh_metadata = _BoolDefault(False)

########NEW FILE########
__FILENAME__ = format
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.

from __future__ import unicode_literals
from dnf.pycomp import long

def format_number(number, SI=0, space=' '):
    """Return a human-readable metric-like string representation
    of a number.

    :param number: the number to be converted to a human-readable form
    :param SI: If is 0, this function will use the convention
       that 1 kilobyte = 1024 bytes, otherwise, the convention
       that 1 kilobyte = 1000 bytes will be used
    :param space: string that will be placed between the number
       and the SI prefix
    :return: a human-readable metric-like string representation of
       *number*
    """

    # copied from from urlgrabber.progress
    symbols = [ ' ', # (none)
                'k', # kilo
                'M', # mega
                'G', # giga
                'T', # tera
                'P', # peta
                'E', # exa
                'Z', # zetta
                'Y'] # yotta

    if SI: step = 1000.0
    else: step = 1024.0

    thresh = 999
    depth = 0
    max_depth = len(symbols) - 1

    if number is None:
        number = 0.0

    # we want numbers between 0 and thresh, but don't exceed the length
    # of our list.  In that event, the formatting will be screwed up,
    # but it'll still show the right number.
    while number > thresh and depth < max_depth:
        depth  = depth + 1
        number = number / step

    if isinstance(number, int) or isinstance(number, long):
        format = '%i%s%s'
    elif number < 9.95:
        # must use 9.95 for proper sizing.  For example, 9.99 will be
        # rounded to 10.0 with the .1f format string (which is too long)
        format = '%.1f%s%s'
    else:
        format = '%.0f%s%s'

    return(format % (float(number or 0), space, symbols[depth]))

def format_time(seconds, use_hours=0):
    """Return a human-readable string representation of a number
    of seconds.  The string will show seconds, minutes, and
    optionally hours.

    :param seconds: the number of seconds to convert to a
       human-readable form
    :param use_hours: If use_hours is 0, the representation will
       be in minutes and seconds. Otherwise, it will be in hours,
       minutes, and seconds
    :return: a human-readable string representation of *seconds*
    """

    # copied from from urlgrabber.progress
    if seconds is None or seconds < 0:
        if use_hours: return '--:--:--'
        else:         return '--:--'
    elif seconds == float('inf'):
        return 'Infinite'
    else:
        seconds = int(seconds)
        minutes = seconds // 60
        seconds = seconds % 60
        if use_hours:
            hours = minutes // 60
            minutes = minutes % 60
            return '%02i:%02i:%02i' % (hours, minutes, seconds)
        else:
            return '%02i:%02i' % (minutes, seconds)

def indent_block(s):
    return '\n'.join('  ' + s for s in s.splitlines())

########NEW FILE########
__FILENAME__ = main
# Copyright 2005 Duke University
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

"""
Entrance point for the yum command line interface.
"""

from __future__ import print_function
from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.i18n import ucd

import sys


def suppress_keyboard_interrupt_message():
    """Prevent unsightly KeyboardInterrupt tracebacks.

    Nothing will be printed to the terminal after an uncaught
    :class:`exceptions.KeyboardInterrupt`.

    """
    old_excepthook = sys.excepthook

    def new_hook(type, value, traceback):
        if type != KeyboardInterrupt:
            old_excepthook(type, value, traceback)
        else:
            pass

    sys.excepthook = new_hook


# do this ASAP to prevent tracebacks after ^C during imports
suppress_keyboard_interrupt_message()

from dnf.cli.utils import show_lock_owner
from dnf.i18n import _

import dnf.cli.cli
import dnf.exceptions
import dnf.i18n
import dnf.logging
import dnf.util
import errno
import logging
import os
import os.path

logger = logging.getLogger("dnf")


def ex_IOError(e):
    logger.log(dnf.logging.SUBDEBUG, '', exc_info=True)
    logger.critical(ucd(e))
    return 1


def ex_Error(e):
    if e.value is not None:
        logger.critical(_('Error: %s'), ucd(e))
    return 1


def main(args):
    try:
        with dnf.cli.cli.BaseCli() as base:
            return _main(base, args)
    except dnf.exceptions.ProcessLockError as e:
        logger.critical(e.value)
        show_lock_owner(e.pid, logger)
        return 1
    except dnf.exceptions.LockError as e:
        logger.critical(e.value)
        return 1
    except IOError as e:
        return ex_IOError(e)
    except KeyboardInterrupt as e:
        print(_("Terminated."), file=sys.stderr)
        return 1
    return 0


def _main(base, args):
    """Run the dnf program from a command line interface."""

    dnf.i18n.setup_locale()
    dnf.i18n.setup_stdout()

    # our core object for the cli
    base.logging.presetup()
    cli = dnf.cli.cli.Cli(base)

    # do our cli parsing and config file setup
    # also sanity check the things being passed on the cli
    try:
        cli.configure(map(ucd, args))
        cli.check()
    except dnf.exceptions.LockError:
        raise
    except dnf.exceptions.Error as e:
        return ex_Error(e)
    except (IOError, OSError) as e:
        return ex_IOError(e)

    # Try to open the current directory to see if we have
    # read and execute access. If not, chdir to /
    try:
        f = open(".")
    except IOError as e:
        if e.errno == errno.EACCES:
            logger.critical(_('No read/execute access in current directory, moving to /'))
            os.chdir("/")
    else:
        f.close()

    try:
        cli.run()
    except dnf.exceptions.LockError:
        raise
    except dnf.exceptions.Error as e:
        return ex_Error(e)
    except (IOError, OSError) as e:
        return ex_IOError(e)

    if cli.demands.resolving:
        ret = resolving(cli, base)
        if ret:
            return ret

    return cli.demands.success_exit_status


def resolving(cli, base):
    """Perform the depsolve, download and RPM transaction stage."""

    if base.transaction is None:
        try:
            got_transaction = base.resolve(cli.demands.allow_erasing)
        except dnf.exceptions.Error as e:
            logger.critical(_('Error: %s'), e)
            return 1

        logger.info(_('Dependencies resolved.'))
    else:
        got_transaction = len(base.transaction)

    # Act on the depsolve result
    if not got_transaction:
        logger.info(_('Nothing to do.'))
        return 0

    # Run the transaction
    try:
        return_code, resultmsgs = base.do_transaction()
    except dnf.exceptions.LockError:
        raise
    except dnf.exceptions.TransactionCheckError as err:
        return_code, resultmsgs = 1, cli.command.get_error_output(err)
    except dnf.exceptions.Error as e:
        return ex_Error(e)
    except IOError as e:
        return ex_IOError(e)

    # rpm ts.check() failed.
    if resultmsgs:
        for msg in resultmsgs:
            logger.critical("%s", msg)
    elif return_code < 0:
        return_code = 1 # Means the pre-transaction checks failed...
        #  This includes:
        # . No packages.
        # . Hitting N at the prompt.
        # . GPG check failures.
    else:
        base.plugins.run_transaction()
        logger.info(_('Complete!'))

    return return_code


def user_main(args, exit_code=False):
    """Call one of the multiple main() functions based on environment variables.

    :param args: command line arguments passed into yum
    :param exit_code: if *exit_code* is True, this function will exit
       python with its exit code when it has finished executing.
       Otherwise, it will return its exit code.
    :return: the exit code from dnf.yum execution
    """

    errcode = main(args)
    if exit_code:
        sys.exit(errcode)
    return errcode


if __name__ == "__main__":
    user_main(sys.argv[1:], exit_code=True)

########NEW FILE########
__FILENAME__ = option_parser
# optparse.py
# CLI options parser.
#
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
from dnf.i18n import _

import argparse
import dnf.exceptions
import dnf.yum.misc
import logging
import sys

logger = logging.getLogger("dnf")

class OptionParser(argparse.ArgumentParser):
    """Subclass that makes some minor tweaks to make ArgumentParser do things the
    "yum way".
    """

    def __init__(self, **kwargs):
        argparse.ArgumentParser.__init__(self, add_help=False, **kwargs)
        self._cmd_usage = {} # names, summary for dnf commands, to build usage
        self._cmd_groups = set() # cmd groups added (main, plugin)
        self._addYumBasicOptions()

    def error(self, msg):
        """Output an error message, and exit the program.  This method
        is overridden so that error output goes to the logger.

        :param msg: the error message to output
        """
        self.print_usage()
        logger.critical(_("Command line error: %s"), msg)
        sys.exit(1)

    @staticmethod
    def _splitArg(seq):
        """ Split all strings in seq, at "," and whitespace.
            Returns a new list. """
        ret = []
        for arg in seq:
            ret.extend(arg.replace(",", " ").split())
        return ret

    @staticmethod
    def _non_nones2dict(in_dct):
        dct = {k: in_dct[k] for k in in_dct
               if in_dct[k] is not None
               if in_dct[k] != []}
        return dct

    def configure_from_options(self, opts, conf, demands, output):
        """Configure parts of CLI from the opts. """

        try:
            # config file is parsed and moving us forward
            # set some things in it.
            if opts.best:
                conf.best = opts.best

            # Handle remaining options
            if opts.allowerasing:
                demands.allow_erasing = opts.allowerasing

            if opts.assumeyes:
                conf.assumeyes = 1
            if opts.assumeno:
                conf.assumeno = 1

            if opts.disableplugins:
                opts.disableplugins = self._splitArg(opts.disableplugins)

            if opts.obsoletes:
                conf.obsoletes = 1

            if opts.installroot:
                self._checkAbsInstallRoot(opts.installroot)
                conf.installroot = opts.installroot
            if opts.noplugins:
                conf.plugins = False

            if opts.showdupesfromrepos:
                conf.showdupesfromrepos = True

            demands.refresh_metadata = opts.refresh_metadata

            if opts.color not in (None, 'auto', 'always', 'never',
                                  'tty', 'if-tty', 'yes', 'no', 'on', 'off'):
                raise ValueError(_("--color takes one of: auto, always, never"))
            elif opts.color is None:
                if conf.color != 'auto':
                    output.term.reinit(color=conf.color)
            else:
                _remap = {'tty' : 'auto', 'if-tty' : 'auto',
                          '1' : 'always', 'true' : 'always',
                          'yes' : 'always', 'on' : 'always',
                          '0' : 'always', 'false' : 'always',
                          'no' : 'never', 'off' : 'never'}
                opts.color = _remap.get(opts.color, opts.color)
                if opts.color != 'auto':
                    output.term.reinit(color=opts.color)

            if opts.disableexcludes:
                disable_excludes = self._splitArg(opts.disableexcludes)
            else:
                disable_excludes = []
            conf.disable_excludes = disable_excludes

            for exclude in self._splitArg(opts.exclude):
                try:
                    excludelist = conf.exclude
                    excludelist.append(exclude)
                    conf.exclude = excludelist
                except dnf.exceptions.ConfigError as e:
                    logger.critical(e)
                    self.print_help()
                    sys.exit(1)

            if opts.rpmverbosity is not None:
                conf.rpmverbosity = opts.rpmverbosity

        except ValueError as e:
            logger.critical(_('Options Error: %s'), e)
            self.print_help()
            sys.exit(1)

    @staticmethod
    def _checkAbsInstallRoot(installroot):
        if not installroot:
            return
        if installroot[0] == '/':
            return
        # We have a relative installroot ... haha
        logger.critical(_('--installroot must be an absolute path: %s'),
                             installroot)
        sys.exit(1)

    class _RepoCallback(argparse.Action):
        def __call__(self, parser, namespace, values, opt_str):
            operation = 'disable' if opt_str == '--disablerepo' else 'enable'
            l = getattr(namespace, self.dest)
            l.append((values, operation))

    def _addYumBasicOptions(self):
        # All defaults need to be a None, so we can always tell whether the user
        # has set something or whether we are getting a default.
        self.conflict_handler = "resolve"
        self.conflict_handler = "error"

        self.add_argument('--allowerasing', action='store_true', default=None,
                           help=_('allow erasing of installed packages to '
                                  'resolve dependencies'))
        self.add_argument("-b", "--best", action="store_true", default=None,
                           help=_("try the best available package versions in "
                                  "transactions."))
        self.add_argument("-C", "--cacheonly", dest="cacheonly",
                           action="store_true", default=None,
                           help=_("run entirely from system cache, "
                                  "don't update cache"))
        self.add_argument("-c", "--config", dest="conffile",
                           default=None, metavar='[config file]',
                           help=_("config file location"))
        self.add_argument("-R", "--randomwait", dest="sleeptime", type=int,
                           default=None, metavar='[minutes]',
                           help=_("maximum command wait time"))
        self.add_argument("-d", "--debuglevel", dest="debuglevel",
                           metavar='[debug level]', default=None,
                           help=_("debugging output level"), type=int)
        self.add_argument("--debugsolver",
                           action="store_true", default=None,
                           help=_("dumps detailed solving results into files"))
        self.add_argument("--showduplicates", dest="showdupesfromrepos",
                           action="store_true", default=None,
                           help=_("show duplicates, in repos, "
                                  "in list/search commands"))
        self.add_argument("-e", "--errorlevel", default=None, type=int,
                           help=_("error output level"))
        self.add_argument("--rpmverbosity", default=None,
                           help=_("debugging output level for rpm"),
                           metavar='[debug level name]')
        self.add_argument("-q", "--quiet", dest="quiet", action="store_true",
                           default=None, help=_("quiet operation"))
        self.add_argument("-v", "--verbose", action="store_true",
                           default=None, help=_("verbose operation"))
        self.add_argument("-y", "--assumeyes", action="store_true",
                           default=None, help=_("answer yes for all questions"))
        self.add_argument("--assumeno", action="store_true",
                           default=None, help=_("answer no for all questions"))
        self.add_argument("--version", action="store_true", default=None,
                           help=_("show Yum version and exit"))
        self.add_argument("--installroot", help=_("set install root"),
                           metavar='[path]')
        self.add_argument("--enablerepo", action=self._RepoCallback,
                           dest='repos_ed', default=[],
                           metavar='[repo]')
        self.add_argument("--disablerepo", action=self._RepoCallback,
                           dest='repos_ed', default=[],
                           metavar='[repo]')
        self.add_argument("-x", "--exclude", default=[], action="append",
                           help=_("exclude packages by name or glob"),
                           metavar='[package]')
        self.add_argument("--disableexcludes", default=[], action="append",
                          help=_("disable excludes"),
                          metavar='[repo]')
        self.add_argument("--obsoletes", action="store_true", default=None,
                          help=_("enable obsoletes processing during upgrades"))
        self.add_argument("--noplugins", action="store_true", default=None,
                           help=_("disable all plugins"))
        self.add_argument("--nogpgcheck", action="store_true", default=None,
                          help=_("disable gpg signature checking"))
        self.add_argument("--disableplugin", dest="disableplugins", default=[],
                           action="append",
                           help=_("disable plugins by name"),
                           metavar='[plugin]')
        self.add_argument("--color", dest="color", default=None,
                          help=_("control whether color is used"))
        self.add_argument("--releasever", default=None,
                           help=_("override the value of $releasever in config"
                                  " and repo files"))
        self.add_argument("--setopt", dest="setopts", default=[],
                           action="append",
                           help=_("set arbitrary config and repo options"))
        self.add_argument("--refresh", dest="refresh_metadata",
                          action="store_true")
        # we add our own help option, so we can control that help is not shown
        # automatic when we do the .parse_known_args(args)
        # but first after plugins are loaded.
        self.add_argument('-h', '--help', action="store_true", help="show help")

    def _add_cmd_usage(self, cmd, group):
        """ store usage info about a single dnf command."""
        summary = cmd.summary
        name = cmd.aliases[0]
        if not name in self._cmd_usage:
            self._cmd_usage[name] = (group, summary)
            self._cmd_groups.add(group)

    def add_commands(self, cli_cmds, group):
        """ store name & summary for dnf commands

        The stored information is used build usage information
        grouped by build-in & plugin commands.
        """
        commands = dnf.yum.misc.unique([x for x in cli_cmds.values()])
        for cmd in commands:
            self._add_cmd_usage(cmd, group)

    def get_usage(self):
        """ get the usage infomation to show the user. """
        desc = {'main': _('List of Main Commands'),
                'plugin': _('List of Plugin Commands')}
        name = dnf.const.PROGRAM_NAME
        usage = '%s [options] COMMAND\n' % name
        for grp in ['main', 'plugin']:
            if not grp in self._cmd_groups:  # dont add plugin usage, if we dont have plugins
                continue
            usage += "\n%s\n\n" % desc[grp]
            for name in sorted(self._cmd_usage.keys()):
                group, summary = self._cmd_usage[name]
                if group == grp:
                    usage += "%-25s %s\n" % (name, summary)
        return usage

########NEW FILE########
__FILENAME__ = output
# Copyright 2005 Duke University
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

"""Handle actual output from the cli."""

from __future__ import print_function
from __future__ import unicode_literals
import itertools
import operator
import sys
import time
import logging
import pwd
import re
import textwrap

import dnf.callback
import dnf.cli.progress
import dnf.conf
from dnf.i18n import _, P_, ucd, fill_exact_width
from dnf.yum.misc import prco_tuple_to_string
import dnf.yum.misc

from dnf.yum.rpmtrans import LoggingTransactionDisplay
import dnf.yum.packages

import dnf.i18n
import dnf.yum.history
import dnf.transaction
import dnf.util

from dnf.cli.format import format_number, format_time
from dnf.cli.term import _term_width
from dnf.pycomp import xrange, basestring, is_py3bytes, long

import locale
import hawkey

try:
    assert max(2, 4) == 4
except:
    # Python-2.4.x doesn't have min/max ... *sigh*
    def min(x, *args):
        for y in args:
            if x > y: x = y
        return x
    def max(x, *args):
        for y in args:
            if x < y: x = y
        return x

def _make_lists(transaction):
    b = dnf.util.Bunch()
    for t in ('downgraded', 'erased', 'installed', 'reinstalled', 'upgraded'):
        b[t] = []
    for tsi in transaction:
        if tsi.op_type == dnf.transaction.DOWNGRADE:
            b.downgraded.append(tsi)
        elif tsi.op_type == dnf.transaction.ERASE:
            b.erased.append(tsi)
        elif tsi.op_type == dnf.transaction.INSTALL:
            b.installed.append(tsi)
        elif tsi.op_type == dnf.transaction.REINSTALL:
            b.reinstalled.append(tsi)
        elif tsi.op_type == dnf.transaction.UPGRADE:
            b.upgraded.append(tsi)
    return b

_ACTIVE_DCT = {
    dnf.transaction.DOWNGRADE : operator.attrgetter('installed'),
    dnf.transaction.ERASE : operator.attrgetter('erased'),
    dnf.transaction.INSTALL : operator.attrgetter('installed'),
    dnf.transaction.REINSTALL : operator.attrgetter('installed'),
    dnf.transaction.UPGRADE : operator.attrgetter('installed'),
    }
def _active_pkg(tsi):
    """Return the package from tsi that takes the active role in the transaction.
    """
    return _ACTIVE_DCT[tsi.op_type](tsi)

class Term(object):
    """A class to provide some terminal "UI" helpers based on curses."""

    # From initial search for "terminfo and python" got:
    # http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/475116
    # ...it's probably not copyrightable, but if so ASPN says:
    #
    #  Except where otherwise noted, recipes in the Python Cookbook are
    # published under the Python license.

    __enabled = True

    columns = property(lambda self: _term_width())

    __cap_names = {
        'underline' : 'smul',
        'reverse' : 'rev',
        'normal' : 'sgr0',
        }

    __colors = {
        'black' : 0,
        'blue' : 1,
        'green' : 2,
        'cyan' : 3,
        'red' : 4,
        'magenta' : 5,
        'yellow' : 6,
        'white' : 7
        }
    __ansi_colors = {
        'black' : 0,
        'red' : 1,
        'green' : 2,
        'yellow' : 3,
        'blue' : 4,
        'magenta' : 5,
        'cyan' : 6,
        'white' : 7
        }
    __ansi_forced_MODE = {
        'bold' : '\x1b[1m',
        'blink' : '\x1b[5m',
        'dim' : '',
        'reverse' : '\x1b[7m',
        'underline' : '\x1b[4m',
        'normal' : '\x1b(B\x1b[m'
        }
    __ansi_forced_FG_COLOR = {
        'black' : '\x1b[30m',
        'red' : '\x1b[31m',
        'green' : '\x1b[32m',
        'yellow' : '\x1b[33m',
        'blue' : '\x1b[34m',
        'magenta' : '\x1b[35m',
        'cyan' : '\x1b[36m',
        'white' : '\x1b[37m'
        }
    __ansi_forced_BG_COLOR = {
        'black' : '\x1b[40m',
        'red' : '\x1b[41m',
        'green' : '\x1b[42m',
        'yellow' : '\x1b[43m',
        'blue' : '\x1b[44m',
        'magenta' : '\x1b[45m',
        'cyan' : '\x1b[46m',
        'white' : '\x1b[47m'
        }

    def __forced_init(self):
        self.MODE = self.__ansi_forced_MODE
        self.FG_COLOR = self.__ansi_forced_FG_COLOR
        self.BG_COLOR = self.__ansi_forced_BG_COLOR

    def reinit(self, term_stream=None, color='auto'):
        """Reinitializes the :class:`Term`.

        :param term_stream:  the terminal stream that the
           :class:`Term` should be initialized to use.  If
           *term_stream* is not given, :attr:`sys.stdout` is used.
        :param color: when to colorize output.  Valid values are
           'always', 'auto', and 'never'.  'always' will use ANSI codes
           to always colorize output, 'auto' will decide whether do
           colorize depending on the terminal, and 'never' will never
           colorize.
        """
        self.__enabled = True
        self.lines = 24

        if color == 'always':
            self.__forced_init()
            return

        # Output modes:
        self.MODE = {
            'bold' : '',
            'blink' : '',
            'dim' : '',
            'reverse' : '',
            'underline' : '',
            'normal' : ''
            }

        # Colours
        self.FG_COLOR = {
            'black' : '',
            'blue' : '',
            'green' : '',
            'cyan' : '',
            'red' : '',
            'magenta' : '',
            'yellow' : '',
            'white' : ''
            }

        self.BG_COLOR = {
            'black' : '',
            'blue' : '',
            'green' : '',
            'cyan' : '',
            'red' : '',
            'magenta' : '',
            'yellow' : '',
            'white' : ''
            }

        if color == 'never':
            self.__enabled = False
            return
        assert color == 'auto'

        # Curses isn't available on all platforms
        try:
            import curses
        except:
            self.__enabled = False
            return

        # If the stream isn't a tty, then assume it has no capabilities.
        if not term_stream:
            term_stream = sys.stdout
        if not term_stream.isatty():
            self.__enabled = False
            return

        # Check the terminal type.  If we fail, then assume that the
        # terminal has no capabilities.
        try:
            curses.setupterm(fd=term_stream.fileno())
        except:
            self.__enabled = False
            return
        self._ctigetstr = curses.tigetstr

        self.lines = curses.tigetnum('lines')

        # Look up string capabilities.
        for cap_name in self.MODE:
            mode = cap_name
            if cap_name in self.__cap_names:
                cap_name = self.__cap_names[cap_name]
            self.MODE[mode] = self._tigetstr(cap_name)

        # Colors
        set_fg = self._tigetstr('setf').encode('utf-8')
        if set_fg:
            for (color, val) in self.__colors.items():
                self.FG_COLOR[color] = curses.tparm(set_fg, val).decode() or ''
        set_fg_ansi = self._tigetstr('setaf').encode('utf-8')
        if set_fg_ansi:
            for (color, val) in self.__ansi_colors.items():
                fg_color = curses.tparm(set_fg_ansi, val).decode() or ''
                self.FG_COLOR[color] = fg_color
        set_bg = self._tigetstr('setb').encode('utf-8')
        if set_bg:
            for (color, val) in self.__colors.items():
                self.BG_COLOR[color] = curses.tparm(set_bg, val).decode() or ''
        set_bg_ansi = self._tigetstr('setab').encode('utf-8')
        if set_bg_ansi:
            for (color, val) in self.__ansi_colors.items():
                bg_color = curses.tparm(set_bg_ansi, val).decode() or ''
                self.BG_COLOR[color] = bg_color

    def __init__(self, term_stream=None, color='auto'):
        self.reinit(term_stream, color)

    def _tigetstr(self, cap_name):
        # String capabilities can include "delays" of the form "$<2>".
        # For any modern terminal, we should be able to just ignore
        # these, so strip them out.
        cap = self._ctigetstr(cap_name) or ''
        if is_py3bytes(cap):
            cap = cap.decode()
        return re.sub(r'\$<\d+>[/*]?', '', cap)

    def sub(self, haystack, beg, end, needles, escape=None, ignore_case=False):
        """Search the string *haystack* for all occurrences of any
        string in the list *needles*.  Prefix each occurrence with
        *beg*, and postfix each occurrence with *end*, then return the
        modified string.  For example::

           >>> yt = Term()
           >>> yt.sub('spam and eggs', 'x', 'z', ['and'])
           'spam xandz eggs'

        This is particularly useful for emphasizing certain words
        in output: for example, calling :func:`sub` with *beg* =
        MODE['bold'] and *end* = MODE['normal'] will return a string
        that when printed to the terminal will appear to be *haystack*
        with each occurrence of the strings in *needles* in bold
        face.  Note, however, that the :func:`sub_mode`,
        :func:`sub_bold`, :func:`sub_fg`, and :func:`sub_bg` methods
        provide convenient ways to access this same emphasizing functionality.

        :param haystack: the string to be modified
        :param beg: the string to be prefixed onto matches
        :param end: the string to be postfixed onto matches
        :param needles: a list of strings to add the prefixes and
           postfixes to
        :param escape: a function that accepts a string and returns
           the same string with problematic characters escaped.  By
           default, :func:`re.escape` is used.
        :param ignore_case: whether case should be ignored when
           searching for matches
        :return: *haystack* with *beg* prefixing, and *end*
          postfixing, occurrences of the strings in *needles*
        """
        if not self.__enabled:
            return haystack

        if not escape:
            escape = re.escape

        render = lambda match: beg + match.group() + end
        for needle in needles:
            pat = escape(needle)
            if ignore_case:
                pat = re.template(pat, re.I)
            haystack = re.sub(pat, render, haystack)
        return haystack
    def sub_norm(self, haystack, beg, needles, **kwds):
        """Search the string *haystack* for all occurrences of any
        string in the list *needles*.  Prefix each occurrence with
        *beg*, and postfix each occurrence with self.MODE['normal'],
        then return the modified string.  If *beg* is an ANSI escape
        code, such as given by self.MODE['bold'], this method will
        return *haystack* with the formatting given by the code only
        applied to the strings in *needles*.

        :param haystack: the string to be modified
        :param beg: the string to be prefixed onto matches
        :param end: the string to be postfixed onto matches
        :param needles: a list of strings to add the prefixes and
           postfixes to
        :return: *haystack* with *beg* prefixing, and self.MODE['normal']
          postfixing, occurrences of the strings in *needles*
        """
        return self.sub(haystack, beg, self.MODE['normal'], needles, **kwds)

    def sub_mode(self, haystack, mode, needles, **kwds):
        """Search the string *haystack* for all occurrences of any
        string in the list *needles*.  Prefix each occurrence with
        self.MODE[*mode*], and postfix each occurrence with
        self.MODE['normal'], then return the modified string.  This
        will return a string that when printed to the terminal will
        appear to be *haystack* with each occurrence of the strings in
        *needles* in the given *mode*.

        :param haystack: the string to be modified
        :param mode: the mode to set the matches to be in.  Valid
           values are given by self.MODE.keys().
        :param needles: a list of strings to add the prefixes and
           postfixes to
        :return: *haystack* with self.MODE[*mode*] prefixing, and
          self.MODE['normal'] postfixing, occurrences of the strings
          in *needles*
        """
        return self.sub_norm(haystack, self.MODE[mode], needles, **kwds)

    def sub_bold(self, haystack, needles, **kwds):
        """Search the string *haystack* for all occurrences of any
        string in the list *needles*.  Prefix each occurrence with
        self.MODE['bold'], and postfix each occurrence with
        self.MODE['normal'], then return the modified string.  This
        will return a string that when printed to the terminal will
        appear to be *haystack* with each occurrence of the strings in
        *needles* in bold face.

        :param haystack: the string to be modified
        :param needles: a list of strings to add the prefixes and
           postfixes to
        :return: *haystack* with self.MODE['bold'] prefixing, and
          self.MODE['normal'] postfixing, occurrences of the strings
          in *needles*
        """
        return self.sub_mode(haystack, 'bold', needles, **kwds)

    def sub_fg(self, haystack, color, needles, **kwds):
        """Search the string *haystack* for all occurrences of any
        string in the list *needles*.  Prefix each occurrence with
        self.FG_COLOR[*color*], and postfix each occurrence with
        self.MODE['normal'], then return the modified string.  This
        will return a string that when printed to the terminal will
        appear to be *haystack* with each occurrence of the strings in
        *needles* in the given color.

        :param haystack: the string to be modified
        :param color: the color to set the matches to be in.  Valid
           values are given by self.FG_COLOR.keys().
        :param needles: a list of strings to add the prefixes and
           postfixes to
        :return: *haystack* with self.FG_COLOR[*color*] prefixing, and
          self.MODE['normal'] postfixing, occurrences of the strings
          in *needles*
        """
        return self.sub_norm(haystack, self.FG_COLOR[color], needles, **kwds)

    def sub_bg(self, haystack, color, needles, **kwds):
        """Search the string *haystack* for all occurrences of any
        string in the list *needles*.  Prefix each occurrence with
        self.BG_COLOR[*color*], and postfix each occurrence with
        self.MODE['normal'], then return the modified string.  This
        will return a string that when printed to the terminal will
        appear to be *haystack* with each occurrence of the strings in
        *needles* highlighted in the given background color.

        :param haystack: the string to be modified
        :param color: the background color to set the matches to be in.  Valid
           values are given by self.BG_COLOR.keys().
        :param needles: a list of strings to add the prefixes and
           postfixes to
        :return: *haystack* with self.BG_COLOR[*color*] prefixing, and
          self.MODE['normal'] postfixing, occurrences of the strings
          in *needles*
        """
        return self.sub_norm(haystack, self.BG_COLOR[color], needles, **kwds)

class Output(object):
    """Main output class for the yum command line."""

    GRP_PACKAGE_INDENT = ' ' * 3

    def __init__(self, base, conf):
        self.conf = conf
        self.base = base
        self.logger = logging.getLogger("dnf")
        self.term = Term()
        self.progress = None

    def _highlight(self, highlight):
        hibeg = ''
        hiend = ''
        if not highlight:
            pass
        elif not isinstance(highlight, basestring) or highlight == 'bold':
            hibeg = self.term.MODE['bold']
        elif highlight == 'normal':
            pass # Minor opt.
        else:
            # Turn a string into a specific output: colour, bold, etc.
            for high in highlight.replace(',', ' ').split():
                if False: pass
                elif high == 'normal':
                    hibeg = ''
                elif high in self.term.MODE:
                    hibeg += self.term.MODE[high]
                elif high in self.term.FG_COLOR:
                    hibeg += self.term.FG_COLOR[high]
                elif (high.startswith('fg:') and
                      high[3:] in self.term.FG_COLOR):
                    hibeg += self.term.FG_COLOR[high[3:]]
                elif (high.startswith('bg:') and
                      high[3:] in self.term.BG_COLOR):
                    hibeg += self.term.BG_COLOR[high[3:]]

        if hibeg:
            hiend = self.term.MODE['normal']
        return (hibeg, hiend)

    def _sub_highlight(self, haystack, highlight, needles, **kwds):
        hibeg, hiend = self._highlight(highlight)
        return self.term.sub(haystack, hibeg, hiend, needles, **kwds)

    @staticmethod
    def _calc_columns_spaces_helps(current, data_tups, left):
        """ Spaces left on the current field will help how many pkgs? """
        ret = 0
        for tup in data_tups:
            if left < (tup[0] - current):
                break
            ret += tup[1]
        return ret

    @property
    def history(self):
        return self.base.history

    @property
    def sack(self):
        return self.base.sack

    @property
    def yumdb(self):
        return self.base.yumdb

    def calcColumns(self, data, columns=None, remainder_column=0,
                    total_width=None, indent=''):
        """Dynamically calculate the widths of the columns that the
        fields in data should be placed into for output.

        :param data: a list of dictionaries that represent the data to
           be output.  Each dictionary in the list corresponds to a
           column of output. The keys of the dictionary are the
           lengths of the items to be output, and the value associated
           with a key is the number of items of that length.
        :param columns: a list containing the minimum amount of space
           that must be allocated for each row. This can be used to
           ensure that there is space available in a column if, for
           example, the actual lengths of the items being output
           cannot be given in *data*
        :param remainder_column: number of the column to receive a few
           extra spaces that may remain after other allocation has
           taken place
        :param total_width: the total width of the output.
           self.term.columns is used by default
        :param indent: string that will be prefixed to a line of
           output to create e.g. an indent
        :return: a list of the widths of the columns that the fields
           in data should be placed into for output
        """
        if total_width is None:
            total_width = self.term.columns

        cols = len(data)
        # Convert the data to ascending list of tuples, (field_length, pkgs)
        pdata = data
        data  = [None] * cols # Don't modify the passed in data
        for d in range(0, cols):
            data[d] = sorted(pdata[d].items())

        #  We start allocating 1 char to everything but the last column, and a
        # space between each (again, except for the last column). Because
        # at worst we are better with:
        # |one two three|
        # | four        |
        # ...than:
        # |one two three|
        # |            f|
        # |our          |
        # ...the later being what we get if we pre-allocate the last column, and
        # thus. the space, due to "three" overflowing it's column by 2 chars.
        if columns is None:
            columns = [1] * (cols - 1)
            columns.append(0)

        total_width -= (sum(columns) + (cols - 1) + len(indent))
        if not columns[-1]:
            total_width += 1
        while total_width > 0:
            # Find which field all the spaces left will help best
            helps = 0
            val   = 0
            for d in xrange(0, cols):
                thelps = self._calc_columns_spaces_helps(columns[d], data[d],
                                                         total_width)
                if not thelps:
                    continue
                #  We prefer to overflow: the last column, and then earlier
                # columns. This is so that in the best case (just overflow the
                # last) ... grep still "works", and then we make it prettier.
                if helps and (d == (cols - 1)) and (thelps / 2) < helps:
                    continue
                if thelps < helps:
                    continue
                helps = thelps
                val   = d

            #  If we found a column to expand, move up to the next level with
            # that column and start again with any remaining space.
            if helps:
                diff = data[val].pop(0)[0] - columns[val]
                if not columns[val] and (val == (cols - 1)):
                    #  If we are going from 0 => N on the last column, take 1
                    # for the space before the column.
                    total_width  -= 1
                columns[val] += diff
                total_width  -= diff
                continue

            overflowed_columns = 0
            for d in xrange(0, cols):
                if not data[d]:
                    continue
                overflowed_columns += 1
            if overflowed_columns:
                #  Split the remaining spaces among each overflowed column
                # equally
                norm = total_width // overflowed_columns
                for d in xrange(0, cols):
                    if not data[d]:
                        continue
                    columns[d] += norm
                    total_width -= norm

            #  Split the remaining spaces among each column equally, except the
            # last one. And put the rest into the remainder column
            cols -= 1
            norm = total_width // cols
            for d in xrange(0, cols):
                columns[d] += norm
            columns[remainder_column] += total_width - (cols * norm)
            total_width = 0

        return columns

    @staticmethod
    def _fmt_column_align_width(width):
        if width < 0:
            return (u"-", -width)
        return (u"", width)

    def _col_data(self, col_data):
        assert len(col_data) == 2 or len(col_data) == 3
        if len(col_data) == 2:
            (val, width) = col_data
            hibeg = hiend = ''
        if len(col_data) == 3:
            (val, width, highlight) = col_data
            (hibeg, hiend) = self._highlight(highlight)
        return (ucd(val), width, hibeg, hiend)

    def fmtColumns(self, columns, msg=u'', end=u''):
        """Return a row of data formatted into a string for output.
        Items can overflow their columns.

        :param columns: a list of tuples containing the data to
           output.  Each tuple contains first the item to be output,
           then the amount of space allocated for the column, and then
           optionally a type of highlighting for the item
        :param msg: a string to begin the line of output with
        :param end: a string to end the line of output with
        :param text_width: a function to find the width of the items
           in the columns.  This defaults to utf8 but can be changed
           to len() if you know it'll be fine
        :return: a row of data formatted into a string for output
        """
        total_width = len(msg)
        data = []
        for col_data in columns[:-1]:
            (val, width, hibeg, hiend) = self._col_data(col_data)

            if not width: # Don't count this column, invisible text
                msg += u"%s"
                data.append(val)
                continue

            (align, width) = self._fmt_column_align_width(width)
            val_width = len(val)
            if val_width <= width:
                #  Don't use utf8_width_fill() because it sucks performance
                # wise for 1,000s of rows. Also allows us to use len(), when
                # we can.
                msg += u"%s%s%s%s "
                if (align == u'-'):
                    data.extend([hibeg, val, " " * (width - val_width), hiend])
                else:
                    data.extend([hibeg, " " * (width - val_width), val, hiend])
            else:
                msg += u"%s%s%s\n" + " " * (total_width + width + 1)
                data.extend([hibeg, val, hiend])
            total_width += width
            total_width += 1
        (val, width, hibeg, hiend) = self._col_data(columns[-1])
        (align, width) = self._fmt_column_align_width(width)
        val = ("%s%" + align + "*s%s") % (hibeg, width, val, hiend)
        msg += u"%%s%s" % end
        data.append(val)
        return msg % tuple(data)

    def simpleList(self, pkg, ui_overflow=False, indent='', highlight=False,
                   columns=None):
        """Print a package as a line.

        :param pkg: the package to be printed
        :param ui_overflow: unused
        :param indent: string to be prefixed onto the line to provide
           e.g. an indent
        :param highlight: highlighting options for the name of the
           package
        :param colums: tuple containing the space allocated for each
           column of output.  The columns are the package name, version,
           and repository
        """
        if columns is None:
            columns = (-40, -22, -16) # Old default
        na = '%s%s.%s' % (indent, pkg.name, pkg.arch)
        hi_cols = [highlight, 'normal', 'normal']
        columns = list(zip((na, pkg.evr, pkg.reponame), columns, hi_cols))
        print(self.fmtColumns(columns))

    def simpleEnvraList(self, pkg, ui_overflow=False,
                        indent='', highlight=False, columns=None):
        """Print a package as a line, with the package itself in envra
        format so it can be passed to list/install/etc.

        :param pkg: the package to be printed
        :param ui_overflow: unused
        :param indent: string to be prefixed onto the line to provide
           e.g. an indent
        :param highlight: highlighting options for the name of the
           package
        :param colums: tuple containing the space allocated for each
           column of output.  The columns the are the package envra and
           repository
        """
        if columns is None:
            columns = (-63, -16) # Old default
        envra = '%s%s' % (indent, ucd(pkg))
        hi_cols = [highlight, 'normal', 'normal']
        rid = pkg.ui_from_repo
        columns = list(zip((envra, rid), columns, hi_cols))
        print(self.fmtColumns(columns))

    def simple_name_list(self, pkg):
        """Print a package as a line containing its name."""
        print(ucd(pkg.name))

    def fmtKeyValFill(self, key, val):
        """Return a key value pair in the common two column output
        format.

        :param key: the key to be formatted
        :param val: the value associated with *key*
        :return: the key value pair formatted in two columns for output
        """
        keylen = len(key)
        cols = self.term.columns
        nxt = ' ' * (keylen - 2) + ': '
        if not val:
            # textwrap.fill in case of empty val returns empty string
            return key
        val = ucd(val)
        ret = textwrap.fill(val, width=cols, initial_indent=key,
                            subsequent_indent=nxt)
        if ret.count("\n") > 1 and keylen > (cols // 3):
            # If it's big, redo it again with a smaller subsequent off
            ret = textwrap.fill(val, width=cols, initial_indent=key,
                                subsequent_indent='     ...: ')
        return ret

    def fmtSection(self, name, fill='='):
        """Format and return a section header.  The format of the
        header is a line with *name* centred, and *fill* repeated on
        either side to fill an entire line on the terminal.

        :param name: the name of the section
        :param fill: the character to repeat on either side of *name*
          to fill an entire line.  *fill* must be a single character.
        :return: a string formatted to be a section header
        """
        name = ucd(name)
        cols = self.term.columns - 2
        name_len = len(name)
        if name_len >= (cols - 4):
            beg = end = fill * 2
        else:
            beg = fill * ((cols - name_len) // 2)
            end = fill * (cols - name_len - len(beg))

        return "%s %s %s" % (beg, name, end)

    def infoOutput(self, pkg, highlight=False):
        """Print information about the given package.

        :param pkg: the package to print information about
        :param hightlight: highlighting options for the name of the
           package
        """
        (hibeg, hiend) = self._highlight(highlight)
        yumdb_info = self.yumdb.get_package(pkg) if pkg.from_system else {}
        print(_("Name        : %s%s%s") % (hibeg, pkg.name, hiend))
        print(_("Arch        : %s") % pkg.arch)
        if pkg.e != "0":
            print(_("Epoch       : %s") % pkg.e)
        print(_("Version     : %s") % pkg.v)
        print(_("Release     : %s") % pkg.r)
        print(_("Size        : %s") % format_number(float(pkg.size)))
        print(_("Repo        : %s") % pkg.repoid)
        if 'from_repo' in yumdb_info:
            print(_("From repo   : %s") % yumdb_info.from_repo)
        if self.conf.verbose:
            # :hawkey does not support changelog information
            # print(_("Committer   : %s") % ucd(pkg.committer))
            # print(_("Committime  : %s") % time.ctime(pkg.committime))
            print(_("Buildtime   : %s") % time.ctime(pkg.buildtime))
            if pkg.installtime > 0:
                print(_("Install time: %s") % time.ctime(pkg.installtime))
            if pkg.repoid == 'installed':
                uid = None
                if 'installed_by' in pkg.yumdb_info:
                    try:
                        uid = int(pkg.yumdb_info.installed_by)
                    except ValueError: # In case int() fails
                        uid = None
                print(_("Installed by: %s") % self._pwd_ui_username(uid))
                uid = None
                if 'changed_by' in pkg.yumdb_info:
                    try:
                        uid = int(pkg.yumdb_info.changed_by)
                    except ValueError: # In case int() fails
                        uid = None
                print(_("Changed by  : %s") % self._pwd_ui_username(uid))
        print(self.fmtKeyValFill(_("Summary     : "), pkg.summary or ""))
        if pkg.url:
            print(_("URL         : %s") % ucd(pkg.url))
        print(self.fmtKeyValFill(_("License     : "), pkg.license))
        print(self.fmtKeyValFill(_("Description : "),
              pkg.description or ""))
        print("")

    def updatesObsoletesList(self, uotup, changetype, columns=None):
        """Print a simple string that explains the relationship
        between the members of an update or obsoletes tuple.

        :param uotup: an update or obsoletes tuple.  The first member
           is the new package, and the second member is the old
           package
        :param changetype: a string indicating what the change between
           the packages is, e.g. 'updates' or 'obsoletes'
        :param columns: a tuple containing information about how to
           format the columns of output.  The absolute value of each
           number in the tuple indicates how much space has been
           allocated for the corresponding column.  If the number is
           negative, the text in the column will be left justified,
           and if it is positive, the text will be right justified.
           The columns of output are the package name, version, and repository
        """
        (changePkg, instPkg) = uotup

        if columns is not None:
            # New style, output all info. for both old/new with old indented
            chi = self.conf.color_update_remote
            if changePkg.reponame != hawkey.SYSTEM_REPO_NAME:
                chi = self.conf.color_update_local
            self.simpleList(changePkg, columns=columns, highlight=chi)
            self.simpleList(instPkg,   columns=columns, indent=' ' * 4,
                            highlight=self.conf.color_update_installed)
            return

        # Old style
        c_compact = changePkg.compactPrint()
        i_compact = '%s.%s' % (instPkg.name, instPkg.arch)
        c_repo = changePkg.repoid
        print('%-35.35s [%.12s] %.10s %-20.20s' % (c_compact, c_repo, changetype, i_compact))

    def listPkgs(self, lst, description, outputType, highlight_na={},
                 columns=None, highlight_modes={}):
        """Prints information about the given list of packages.

        :param lst: a list of packages to print information about
        :param description: string describing what the list of
           packages contains, e.g. 'Available Packages'
        :param outputType: The type of information to be printed.
           Current options::

              'list' - simple pkg list
              'info' - similar to rpm -qi output
              'name' - simple name list
        :param highlight_na: a dictionary containing information about
              packages that should be highlighted in the output.  The
              dictionary keys are (name, arch) tuples for the package,
              and the associated values are the package objects
              themselves.
        :param columns: a tuple containing information about how to
           format the columns of output.  The absolute value of each
           number in the tuple indicates how much space has been
           allocated for the corresponding column.  If the number is
           negative, the text in the column will be left justified,
           and if it is positive, the text will be right justified.
           The columns of output are the package name, version, and
           repository
        :param highlight_modes: dictionary containing information
              about to highlight the packages in *highlight_na*.
              *highlight_modes* should contain the following keys::

                 'not_in' - highlighting used for packages not in *highlight_na*
                 '=' - highlighting used when the package versions are equal
                 '<' - highlighting used when the package has a lower version number
                 '>' - highlighting used when the package has a higher version number
        :return: (exit_code, [errors])

        exit_code is::

            0 = we're done, exit
            1 = we've errored, exit with error string
        """
        if outputType in ['list', 'info', 'name']:
            thingslisted = 0
            if len(lst) > 0:
                thingslisted = 1
                print('%s' % description)
                for pkg in sorted(lst):
                    key = (pkg.name, pkg.arch)
                    highlight = False
                    if False: pass
                    elif key not in highlight_na:
                        highlight = highlight_modes.get('not in', 'normal')
                    elif pkg.evr_eq(highlight_na[key]):
                        highlight = highlight_modes.get('=', 'normal')
                    elif pkg.evr_lt(highlight_na[key]):
                        highlight = highlight_modes.get('>', 'bold')
                    else:
                        highlight = highlight_modes.get('<', 'normal')

                    if outputType == 'list':
                        self.simpleList(pkg, ui_overflow=True,
                                        highlight=highlight, columns=columns)
                    elif outputType == 'info':
                        self.infoOutput(pkg, highlight=highlight)
                    elif outputType == 'name':
                        self.simple_name_list(pkg)
                    else:
                        pass

            if thingslisted == 0:
                return 1, ['No packages to list']
            return 0, []

    def userconfirm(self):
        """Get a yes or no from the user, and default to No

        :return: True if the user selects yes, and False if the user
           selects no
        """
        yui = (ucd(_('y')), ucd(_('yes')))
        nui = (ucd(_('n')), ucd(_('no')))
        aui = yui + nui
        while True:
            msg = _('Is this ok [y/N]: ')
            choice = ''
            if self.conf.defaultyes:
                msg = _('Is this ok [Y/n]: ')
            try:
                choice = dnf.i18n.ucd_input(msg)
            except EOFError:
                pass
            except KeyboardInterrupt:
                choice = nui[0]
            choice = ucd(choice).lower()
            if len(choice) == 0:
                choice = yui[0] if self.conf.defaultyes else nui[0]
            if choice in aui:
                break

            # If the English one letter names don't mix with the translated
            # letters, allow them too:
            if u'y' == choice and u'y' not in aui:
                choice = yui[0]
                break
            if u'n' == choice and u'n' not in aui:
                choice = nui[0]
                break

        if choice in yui:
            return True
        return False

    def _cli_confirm_gpg_key_import(self, keydict):
        # FIXME what should we be printing here?
        return self.userconfirm()

    def _pkgs2name_dict(self, sections):
        installed = self.sack.query().installed().name_dict()
        available = self.sack.query().available().name_dict()

        d = {}
        for pkg_name in itertools.chain(*list(zip(*sections))[1]):
            if pkg_name in installed:
                d[pkg_name] = installed[pkg_name][0]
            elif pkg_name in available:
                d[pkg_name] = available[pkg_name][0]
        return d

    def _pkgs2col_lengths(self, sections, name_dict):
        nevra_lengths = {}
        repo_lengths = {}
        for pkg_name in itertools.chain(*list(zip(*sections))[1]):
            pkg = name_dict.get(pkg_name)
            if pkg is None:
                continue
            nevra_l = len(ucd(pkg)) + len(self.GRP_PACKAGE_INDENT)
            repo_l = len(ucd(pkg.reponame))
            nevra_lengths[nevra_l] = nevra_lengths.get(nevra_l, 0) + 1
            repo_lengths[repo_l] = repo_lengths.get(repo_l, 0) + 1
        return (nevra_lengths, repo_lengths)

    def _display_packages(self, pkg_names):
        for name in pkg_names:
            print('%s%s' % (self.GRP_PACKAGE_INDENT, name))

    def _display_packages_verbose(self, pkg_names, name_dict, columns):
        for name in pkg_names:
            pkg = name_dict[name]
            highlight = False
            if not pkg.from_system:
                highlight = self.conf.color_list_available_install
            self.simpleEnvraList(pkg, ui_overflow=True,
                                 indent=self.GRP_PACKAGE_INDENT,
                                 highlight=highlight,
                                 columns=columns)

    def displayPkgsInGroups(self, group):
        """Output information about the packages in a given group

        :param group: a Group object to output information about
        """
        def names(packages):
            return sorted(pkg.name for pkg in packages)
        print(_('\nGroup: %s') % group.ui_name)

        verbose = self.conf.verbose
        if verbose:
            print(_(' Group-Id: %s') % ucd(group.id))
        if group.ui_description:
            print(_(' Description: %s') % ucd(group.ui_description) or "")
        if group.lang_only:
            print(_(' Language: %s') % group.lang_only)

        sections = (
            (_(' Mandatory Packages:'), names(group.mandatory_packages)),
            (_(' Default Packages:'), names(group.default_packages)),
            (_(' Optional Packages:'), names(group.optional_packages)),
            (_(' Conditional Packages:'), names(group.conditional_packages)))
        if verbose:
            name_dict = self._pkgs2name_dict(sections)
            col_lengths = self._pkgs2col_lengths(sections, name_dict)
            columns = self.calcColumns(col_lengths)
            columns = (-columns[0], -columns[1])
            for (section_name, packages) in sections:
                if len(packages) < 1:
                    continue
                print(section_name)
                self._display_packages_verbose(packages, name_dict, columns)
        else:
            for (section_name, packages) in sections:
                if len(packages) < 1:
                    continue
                print(section_name)
                self._display_packages(packages)

    def depListOutput(self, results):
        """Format and output a list of findDeps results

        :param results: a list of package dependency information as
           returned by findDeps
        """
        verb = self.conf.verbose
        for pkg in sorted(results):
            print(_("package: %s") % pkg.compactPrint())
            if len(results[pkg]) == 0:
                print(_("  No dependencies for this package"))
                continue

            for req in sorted(results[pkg]):
                reqlist = results[pkg][req]
                print(_("  dependency: %s") % prco_tuple_to_string(req))
                if not reqlist:
                    print(_("   Unsatisfied dependency"))
                    continue

                seen = {}
                for po in reversed(sorted(reqlist)):
                    key = (po.name, po.arch)
                    if not verb and key in seen:
                        continue
                    seen[key] = po
                    print("   provider: %s" % po.compactPrint())

    def matchcallback(self, po, values, matchfor=None, verbose=None,
                      highlight=None):
        """Output search/provides type callback matches.

        :param po: the package object that matched the search
        :param values: the information associated with *po* that
           matched the search
        :param matchfor: a list of strings to be highlighted in the
           output
        :param verbose: whether to output extra verbose information
        :param highlight: highlighting options for the highlighted matches
        """
        if self.conf.showdupesfromrepos:
            msg = '%s : ' % po
        else:
            msg = '%s.%s : ' % (po.name, po.arch)
        msg = self.fmtKeyValFill(msg, po.summary or "")
        if matchfor:
            if highlight is None:
                highlight = self.conf.color_search_match
            msg = self._sub_highlight(msg, highlight, matchfor,ignore_case=True)
        print(msg)

        if verbose is None:
            verbose = self.conf.verbose
        if not verbose:
            return

        print(_("Repo        : %s") % po.ui_from_repo)
        done = False
        for item in dnf.yum.misc.unique(values):
            if po.name == item or po.summary == item:
                continue # Skip double name/summary printing

            if not done:
                print(_('Matched from:'))
                done = True
            can_overflow = True
            if False: pass
            elif po.description == item:
                key = _("Description : ")
                item = ucd(item)
            elif po.url == item:
                key = _("URL         : %s")
                can_overflow = False
            elif po.license == item:
                key = _("License     : %s")
                can_overflow = False
            elif item.startswith("/"):
                key = _("Filename    : %s")
                item = ucd(item) or ""
                can_overflow = False
            else:
                key = _("Other       : ")

            if matchfor:
                item = self._sub_highlight(item, highlight, matchfor,
                                           ignore_case=True)
            if can_overflow:
                print(self.fmtKeyValFill(key, ucd(item)))
            else:
                print(key % item)
        print()

    def matchcallback_verbose(self, po, values, matchfor=None):
        """Output search/provides type callback matches.  This will
        output more information than :func:`matchcallback`.

        :param po: the package object that matched the search
        :param values: the information associated with *po* that
           matched the search
        :param matchfor: a list of strings to be highlighted in the
           output
        """
        return self.matchcallback(po, values, matchfor, verbose=True)

    def reportDownloadSize(self, packages, installonly=False):
        """Report the total download size for a set of packages

        :param packages: a list of package objects
        :param installonly: whether the transaction consists only of installations
        """
        totsize = 0
        locsize = 0
        insize  = 0
        error = False
        for pkg in packages:
            # Just to be on the safe side, if for some reason getting
            # the package size fails, log the error and don't report download
            # size
            try:
                size = int(pkg.size)
                totsize += size
                try:
                    if pkg.verifyLocalPkg():
                        locsize += size
                except:
                    pass

                if not installonly:
                    continue

                try:
                    size = int(pkg.installsize)
                except:
                    pass
                insize += size
            except:
                error = True
                self.logger.error(_('There was an error calculating total download size'))
                break

        if (not error):
            if locsize:
                self.logger.info(_("Total size: %s"),
                                        format_number(totsize))
            if locsize != totsize:
                self.logger.info(_("Total download size: %s"),
                                        format_number(totsize - locsize))
            if installonly:
                self.logger.info(
                                        _("Installed size: %s"),
                                        format_number(insize))

    def reportRemoveSize(self, packages):
        """Report the total size of packages being removed.

        :param packages: a list of package objects
        """
        totsize = 0
        error = False
        for pkg in packages:
            # Just to be on the safe side, if for some reason getting
            # the package size fails, log the error and don't report download
            # size
            try:
                size = pkg.size
                totsize += size
            except:
                error = True
                self.logger.error(_('There was an error calculating installed size'))
                break
        if (not error):
            self.logger.info(
                                    _("Installed size: %s"),
                                    format_number(totsize))

    def list_transaction(self, transaction):
        """Return a string representation of the transaction in an
        easy-to-read format.
        """
        list_bunch = _make_lists(transaction)
        pkglist_lines = []
        data  = {'n' : {}, 'v' : {}, 'r' : {}}
        a_wid = 0 # Arch can't get "that big" ... so always use the max.

        def _add_line(lines, data, a_wid, po, obsoletes=[]):
            (n, a, e, v, r) = po.pkgtup
            evr = po.evr
            repoid = po.reponame
            size = format_number(po.size)

            if a is None: # gpgkeys are weird
                a = 'noarch'

            # none, partial, full?
            if po.from_system:
                hi = self.conf.color_update_installed
            elif po.from_cmdline:
                hi = self.conf.color_update_local
            else:
                hi = self.conf.color_update_remote
            lines.append((n, a, evr, repoid, size, obsoletes, hi))
            #  Create a dict of field_length => number of packages, for
            # each field.
            for (d, v) in (("n",len(n)), ("v",len(evr)), ("r",len(repoid))):
                data[d].setdefault(v, 0)
                data[d][v] += 1
            a_wid = max(a_wid, len(a))
            return a_wid

        for (action, pkglist) in [(_('Installing'), list_bunch.installed),
                                  (_('Upgrading'), list_bunch.upgraded),
                                  (_('Reinstalling'), list_bunch.reinstalled),
                                  (_('Removing'), list_bunch.erased),
                                  (_('Downgrading'), list_bunch.downgraded)]:
            lines = []
            for tsi in pkglist:
                active = _active_pkg(tsi)
                a_wid = _add_line(lines, data, a_wid, active, tsi.obsoleted)

            pkglist_lines.append((action, lines))

        if not data['n']:
            return u''
        else:
            data    = [data['n'],    {}, data['v'], data['r'], {}]
            columns = [1,         a_wid,         1,         1,  5]
            columns = self.calcColumns(data, indent="  ", columns=columns,
                                       remainder_column=2)
            (n_wid, a_wid, v_wid, r_wid, s_wid) = columns
            assert s_wid == 5

            out = [u"""
%s
%s
%s
""" % ('=' * self.term.columns,
       self.fmtColumns(((_('Package'), -n_wid), (_('Arch'), -a_wid),
                        (_('Version'), -v_wid), (_('Repository'), -r_wid),
                        (_('Size'), s_wid)), u" "),
       '=' * self.term.columns)]

        for (action, lines) in pkglist_lines:
            if lines:
                totalmsg = u"%s:\n" % action
            for (n, a, evr, repoid, size, obsoletes, hi) in lines:
                columns = ((n,   -n_wid, hi), (a,      -a_wid),
                           (evr, -v_wid), (repoid, -r_wid), (size, s_wid))
                msg = self.fmtColumns(columns, u" ", u"\n")
                hibeg, hiend = self._highlight(self.conf.color_update_installed)
                for obspo in sorted(obsoletes):
                    appended = _('     replacing  %s%s%s.%s %s\n')
                    appended %= (hibeg, obspo.name, hiend, obspo.arch, obspo.evr)
                    msg += appended
                totalmsg = totalmsg + msg

            if lines:
                out.append(totalmsg)

        out.append(_("""
Transaction Summary
%s
""") % ('=' * self.term.columns))
        summary_data =  (
            (_('Install'), len(list_bunch.installed), 0),
            (_('Upgrade'), len(list_bunch.upgraded), 0),
            (_('Remove'), len(list_bunch.erased), 0),
            (_('Downgrade'), len(list_bunch.downgraded), 0))
        max_msg_action   = 0
        max_msg_count    = 0
        max_msg_pkgs     = 0
        max_msg_depcount = 0
        for action, count, depcount in summary_data:
            if not count and not depcount:
                continue

            msg_pkgs = P_('Package', 'Packages', count)
            len_msg_action = len(action)
            len_msg_count = len(str(count))
            len_msg_pkgs = len(msg_pkgs)

            if depcount:
                len_msg_depcount = len(str(depcount))
            else:
                len_msg_depcount = 0

            max_msg_action   = max(len_msg_action,   max_msg_action)
            max_msg_count    = max(len_msg_count,    max_msg_count)
            max_msg_pkgs     = max(len_msg_pkgs,     max_msg_pkgs)
            max_msg_depcount = max(len_msg_depcount, max_msg_depcount)

        for action, count, depcount in summary_data:
            msg_pkgs = P_('Package', 'Packages', count)
            if depcount:
                msg_deppkgs = P_('Dependent package', 'Dependent packages',
                                 depcount)
                action_msg = "%-*s" % (max_msg_action, action)
                if count:
                    msg = '%s  %*d %s (+%*d %s)\n'
                    out.append(msg % (action_msg,
                                      max_msg_count, count,
                                      "%-*s" % (max_msg_pkgs, msg_pkgs),
                                      max_msg_depcount, depcount, msg_deppkgs))
                else:
                    msg = '%s  %s  ( %*d %s)\n'
                    out.append(msg % (action_msg,
                                      (max_msg_count + max_msg_pkgs) * ' ',
                                      max_msg_depcount, depcount, msg_deppkgs))
            elif count:
                msg = '%s  %*d %s\n'
                out.append(msg % ("%-*s" % (max_msg_action, action),
                                  max_msg_count, count, msg_pkgs))
        return ''.join(out)

    def post_transaction_output(self, transaction):
        """Returns a human-readable summary of the results of the
        transaction.

        :return: a string containing a human-readable summary of the
           results of the transaction
        """
        #  Works a bit like calcColumns, but we never overflow a column we just
        # have a dynamic number of columns.
        def _fits_in_cols(msgs, num):
            """ Work out how many columns we can use to display stuff, in
                the post trans output. """
            if len(msgs) < num:
                return []

            left = self.term.columns - ((num - 1) + 2)
            if left <= 0:
                return []

            col_lens = [0] * num
            col = 0
            for msg in msgs:
                if len(msg) > col_lens[col]:
                    diff = (len(msg) - col_lens[col])
                    if left <= diff:
                        return []
                    left -= diff
                    col_lens[col] = len(msg)
                col += 1
                col %= len(col_lens)

            for col in range(len(col_lens)):
                col_lens[col] += left // num
                col_lens[col] *= -1
            return col_lens

        out = ''
        list_bunch = _make_lists(transaction)

        for (action, tsis) in [(_('Reinstalled'), list_bunch.reinstalled),
                               (_('Removed'), list_bunch.erased),
                               (_('Installed'), list_bunch.installed),
                               (_('Upgraded'), list_bunch.upgraded),
                               (_('Downgraded'), list_bunch.downgraded)]:
            if not tsis:
                continue
            msgs = []
            out += '\n%s:\n' % action
            for pkg in [tsi.active for tsi in tsis]:
                (n, a, e, v, r) = pkg.pkgtup
                evr = pkg.evr
                msg = "%s.%s %s" % (n, a, evr)
                msgs.append(msg)
            for num in (8, 7, 6, 5, 4, 3, 2):
                cols = _fits_in_cols(msgs, num)
                if cols:
                    break
            if not cols:
                cols = [-(self.term.columns - 2)]
            while msgs:
                current_msgs = msgs[:len(cols)]
                out += '  '
                out += self.fmtColumns(list(zip(current_msgs, cols)), end=u'\n')
                msgs = msgs[len(cols):]

        return out

    def setup_progress_callbacks(self):
        """Set up the progress callbacks and various
           output bars based on debug level.
        """
        progressbar = None
        if self.conf.debuglevel >= 2 and sys.stdout.isatty():
            progressbar = dnf.cli.progress.MultiFileProgressMeter(fo=sys.stdout)
            self.progress = dnf.cli.progress.MultiFileProgressMeter(fo=sys.stdout)

        # setup our depsolve progress callback
        return (progressbar, DepSolveProgressCallBack())

    def download_callback_total_cb(self, remote_pkgs, remote_size,
                                   download_start_timestamp):
        """Outputs summary information about the download process.

        :param remote_pkgs: a list of package objects that were downloaded
        :param remote_size: the total amount of information that was
           downloaded, in bytes
        :param download_start_timestamp: the time when the download
           process started, in seconds since the epoch
        """
        if len(remote_pkgs) <= 1:
            return

        width = _term_width()
        self.logger.info("-" * width)
        dl_time = max(0.01, time.time() - download_start_timestamp)
        msg = ' %5sB/s | %5sB %9s     ' % (
            format_number(remote_size // dl_time),
            format_number(remote_size),
            format_time(dl_time))
        msg = "%-*s" % (width - len(msg), _("Total")) + msg
        self.logger.info(msg)

    def _history_uiactions(self, hpkgs):
        actions = set()
        count = 0
        for hpkg in hpkgs:
            st = hpkg.state
            if st == 'True-Install':
                st = 'Install'
            if st == 'Dep-Install': # Mask these at the higher levels
                st = 'Install'
            if st == 'Obsoleted': #  This is just a UI tweak, as we can't have
                                  # just one but we need to count them all.
                st = 'Obsoleting'
            if st in ('Install', 'Update', 'Erase', 'Reinstall', 'Downgrade',
                      'Obsoleting'):
                actions.add(st)
                count += 1
        assert len(actions) <= 6
        if len(actions) > 1:
            large2small = {'Install'      : _('I'),
                           'Obsoleting'   : _('O'),
                           'Erase'        : _('E'),
                           'Reinstall'    : _('R'),
                           'Downgrade'    : _('D'),
                           'Update'       : _('U'),
                           }
            return count, ", ".join([large2small[x] for x in sorted(actions)])

        # So empty transactions work, although that "shouldn't" really happen
        return count, "".join(list(actions))

    def _pwd_ui_username(self, uid, limit=None):
        if isinstance(uid, list):
            return [self._pwd_ui_username(u, limit) for u in uid]

        # loginuid is set to      -1 (0xFFFF_FFFF) on init, in newer kernels.
        # loginuid is set to INT_MAX (0x7FFF_FFFF) on init, in older kernels.
        if uid is None or uid in (0xFFFFFFFF, 0x7FFFFFFF):
            loginid = _("<unset>")
            name = _("System") + " " + loginid
            if limit is not None and len(name) > limit:
                name = loginid
            return ucd(name)

        def _safe_split_0(text, *args):
            """ Split gives us a [0] for everything _but_ '', this function
                returns '' in that case. """
            ret = text.split(*args)
            if not ret:
                return ''
            return ret[0]

        try:
            user = pwd.getpwuid(uid)
            fullname = _safe_split_0(user.pw_gecos, ';', 2)
            name = "%s <%s>" % (fullname, user.pw_name)
            if limit is not None and len(name) > limit:
                name = "%s ... <%s>" % (_safe_split_0(fullname), user.pw_name)
                if len(name) > limit:
                    name = "<%s>" % user.pw_name
            return ucd(name)
        except KeyError:
            return ucd(uid)

    @staticmethod
    def _historyRangeRTIDs(old, tid):
        ''' Convert a user "TID" string of 2..4 into: (2, 4). '''
        def str2int(x):
            try:
                if x == 'last' or x.startswith('last-'):
                    tid = old.tid
                    if x.startswith('last-'):
                        off = int(x[len('last-'):])
                        if off <= 0:
                            int("z")
                        tid -= off
                    return tid
                return int(x)
            except ValueError:
                return None

        if '..' not in tid:
            return None
        btid, etid = tid.split('..', 2)
        btid = str2int(btid)
        if btid > old.tid:
            return None
        elif btid <= 0:
            return None
        etid = str2int(etid)
        if etid > old.tid:
            return None

        if btid is None or etid is None:
            return None

        # Have a range ... do a "merged" transaction.
        if btid > etid:
            btid, etid = etid, btid
        return (btid, etid)

    def _historyRangeTIDs(self, rtids):
        ''' Convert a list of ranged tid typles into all the tids needed, Eg.
            [(2,4), (6,8)] == [2, 3, 4, 6, 7, 8]. '''
        tids = set()
        last_end = -1 # This just makes displaying it easier...
        for mtid in sorted(rtids):
            if mtid[0] < last_end:
                self.logger.warn(_('Skipping merged transaction %d to %d, as it overlaps' % (mtid[0], mtid[1])))
                continue # Don't do overlapping
            last_end = mtid[1]
            for num in range(mtid[0], mtid[1] + 1):
                tids.add(num)
        return tids

    def _history_list_transactions(self, extcmds):
        old = self.history.last()
        if old is None:
            self.logger.critical(_('No transactions'))
            return None, None

        tids = set()
        pats = []
        usertids = extcmds[1:]
        printall = False
        if usertids:
            printall = True
            if usertids[0] == 'all':
                usertids.pop(0)
        for tid in usertids:
            try:
                int(tid)
                tids.add(tid)
            except ValueError:
                rtid = self._historyRangeRTIDs(old, tid)
                if rtid:
                    tids.update(self._historyRangeTIDs([rtid]))
                    continue
                pats.append(tid)
        if pats:
            tids.update(self.history.search(pats))

        if not tids and usertids:
            self.logger.critical(_('Bad transaction IDs, or package(s), given'))
            return None, None
        return tids, printall

    def historyListCmd(self, extcmds):
        """Output a list of information about the history of yum
        transactions.

        :param extcmds: list of extra command line arguments
        :return: (exit_code, [errors])

        exit_code is::

            0 = we're done, exit
            1 = we've errored, exit with error string
        """
        tids, printall = self._history_list_transactions(extcmds)
        if tids is None:
            return 1, ['Failed history list']

        limit = 20
        if printall:
            limit = None

        old_tids = self.history.old(tids, limit=limit)
        done = 0
        if self.conf.history_list_view == 'users':
            uids = [1,2]
        elif self.conf.history_list_view == 'commands':
            uids = [1]
        else:
            assert self.conf.history_list_view == 'single-user-commands'
            uids = set()
            blanks = 0
            for old in old_tids:
                if not printall and done >= limit:
                    break

                done += 1
                if old.cmdline is None:
                    blanks += 1
                uids.add(old.loginuid)
            if len(uids) == 1 and blanks > (done // 2):
                uids.add('blah')

        fmt = "%s | %s | %s | %s | %s"
        if len(uids) == 1:
            name = _("Command line")
        else:
            name = _("Login user")
        print(fmt % (fill_exact_width(_("ID"), 6),
                     fill_exact_width(name, 24),
                     fill_exact_width(_("Date and time"), 6),
                     fill_exact_width(_("Action(s)"), 6),
                     fill_exact_width(_("Altered"), 6)))
        print("-" * 79)
        fmt = "%6u | %s | %-16.16s | %s | %4u"
        done = 0
        for old in old_tids:
            if not printall and done >= limit:
                break

            done += 1
            if len(uids) == 1:
                name = old.cmdline or ''
            else:
                name = self._pwd_ui_username(old.loginuid, 24)
            tm = time.strftime("%Y-%m-%d %H:%M",
                               time.localtime(old.beg_timestamp))
            num, uiacts = self._history_uiactions(old.trans_data)
            name = fill_exact_width(name, 24)
            uiacts = fill_exact_width(uiacts, 14)
            rmark = lmark = ' '
            if old.return_code is None:
                rmark = lmark = '*'
            elif old.return_code:
                rmark = lmark = '#'
                # We don't check .errors, because return_code will be non-0
            elif old.output:
                rmark = lmark = 'E'
            elif old.rpmdb_problems:
                rmark = lmark = 'P'
            elif old.trans_skip:
                rmark = lmark = 's'
            if old.altered_lt_rpmdb:
                rmark = '<'
            if old.altered_gt_rpmdb:
                lmark = '>'
            print(fmt % (old.tid, name, tm, uiacts, num), "%s%s" % (lmark,rmark))

    def historyInfoCmd(self, extcmds):
        """Output information about a transaction in history

        :param extcmds: list of extra command line arguments
        :return: (exit_code, [errors])

        exit_code is::

            0 = we're done, exit
            1 = we've errored, exit with error string
        """
        tids = set()
        mtids = set()
        pats = []
        old = self.history.last()
        if old is None:
            self.logger.critical(_('No transactions'))
            return 1, ['Failed history info']

        for tid in extcmds[1:]:
            if self._historyRangeRTIDs(old, tid):
                # Have a range ... do a "merged" transaction.
                mtids.add(self._historyRangeRTIDs(old, tid))
                continue

            try:
                id_or_offset = self.base.transaction_id_or_offset(tid)
            except ValueError:
                # A package pattern.
                pats.append(tid)
                continue

            # A transaction ID or an offset from the last transaction ID.
            tids.add(id_or_offset if id_or_offset >= 0 else
                     old.tid + id_or_offset + 1)
        if pats:
            tids.update(self.history.search(pats))
        utids = tids.copy()
        if mtids:
            mtids = sorted(mtids)
            tids.update(self._historyRangeTIDs(mtids))

        if not tids and len(extcmds) < 2:
            old = self.history.last(complete_transactions_only=False)
            if old is not None:
                tids.add(old.tid)
                utids.add(old.tid)

        if not tids:
            self.logger.critical(_('No transaction ID, or package, given'))
            return 1, ['Failed history info']

        lastdbv = self.history.last()
        if lastdbv is not None:
            lasttid = lastdbv.tid
            lastdbv = lastdbv.end_rpmdbversion

        done = False
        bmtid, emtid = -1, -1
        mobj = None
        if mtids:
            bmtid, emtid = mtids.pop(0)
        for tid in self.history.old(tids):
            if lastdbv is not None and tid.tid == lasttid:
                #  If this is the last transaction, is good and it doesn't
                # match the current rpmdb ... then mark it as bad.
                rpmdbv  = self.sack.rpmdb_version(self.yumdb)
                if lastdbv != rpmdbv:
                    tid.altered_gt_rpmdb = True
            lastdbv = None

            if tid.tid >= bmtid and tid.tid <= emtid:
                if mobj is None:
                    mobj = dnf.yum.history.YumMergedHistoryTransaction(tid)
                else:
                    mobj.merge(tid)
            elif mobj is not None:
                if done:
                    print("-" * 79)
                done = True

                self._historyInfoCmd(mobj)
                mobj = None
                if mtids:
                    bmtid, emtid = mtids.pop(0)
                    if tid.tid >= bmtid and tid.tid <= emtid:
                        mobj = dnf.yum.history.YumMergedHistoryTransaction(tid)

            if tid.tid in utids:
                if done:
                    print("-" * 79)
                done = True

                self._historyInfoCmd(tid, pats)

        if mobj is not None:
            if done:
                print("-" * 79)

            self._historyInfoCmd(mobj)

    def _hpkg2from_repo(self, hpkg):
        """ Given a pkg, find the ipkg.ui_from_repo."""
        if 'from_repo' in hpkg.yumdb_info:
            return hpkg.ui_from_repo
        return "(unknown)"

    def _historyInfoCmd(self, old, pats=[]):
        name = self._pwd_ui_username(old.loginuid)

        _pkg_states_installed = {'i' : _('Installed'), 'e' : _('Erased'),
                                 'o' : _('Upgraded'), 'n' : _('Downgraded')}
        _pkg_states_available = {'i' : _('Installed'), 'e' : _('Not installed'),
                                 'o' : _('Older'), 'n' : _('Newer')}
        maxlen = max([len(x) for x in (list(_pkg_states_installed.values()) +
                                       list(_pkg_states_available.values()))])
        _pkg_states_installed['maxlen'] = maxlen
        _pkg_states_available['maxlen'] = maxlen
        def _simple_pkg(pkg, prefix_len, was_installed=False, highlight=False,
                        pkg_max_len=0, show_repo=True):
            prefix = " " * prefix_len
            if was_installed:
                _pkg_states = _pkg_states_installed
            else:
                _pkg_states = _pkg_states_available
            state  = _pkg_states['i']
            ipkgs = self.sack.query().installed().filter(name=hpkg.name).run()
            ipkgs.sort()
            if not ipkgs:
                state  = _pkg_states['e']
            elif hpkg.pkgtup in (ipkg.pkgtup for ipkg in ipkgs):
                pass
            elif ipkgs[-1] > hpkg:
                state  = _pkg_states['o']
            elif ipkgs[0] < hpkg:
                state  = _pkg_states['n']
            else:
                assert False, "Impossible, installed not newer and not older"
            if highlight:
                (hibeg, hiend) = self._highlight('bold')
            else:
                (hibeg, hiend) = self._highlight('normal')
            state = "%-*s" % (_pkg_states['maxlen'], state)
            ui_repo = ''
            if show_repo:
                ui_repo = self._hpkg2from_repo(hpkg)
            print("%s%s%s%s %-*s %s" % (prefix, hibeg, state, hiend,
                                        pkg_max_len, hpkg, ui_repo))

        if isinstance(old.tid, list):
            print(_("Transaction ID :"), "%u..%u" % (old.tid[0], old.tid[-1]))
        else:
            print(_("Transaction ID :"), old.tid)
        begtm = time.ctime(old.beg_timestamp)
        print(_("Begin time     :"), begtm)
        if old.beg_rpmdbversion is not None:
            if old.altered_lt_rpmdb:
                print(_("Begin rpmdb    :"), old.beg_rpmdbversion, "**")
            else:
                print(_("Begin rpmdb    :"), old.beg_rpmdbversion)
        if old.end_timestamp is not None:
            endtm = time.ctime(old.end_timestamp)
            endtms = endtm.split()
            if begtm.startswith(endtms[0]): # Chop uninteresting prefix
                begtms = begtm.split()
                sofar = 0
                for i in range(len(endtms)):
                    if i > len(begtms):
                        break
                    if begtms[i] != endtms[i]:
                        break
                    sofar += len(begtms[i]) + 1
                endtm = (' ' * sofar) + endtm[sofar:]
            diff = old.end_timestamp - old.beg_timestamp
            if diff < 5 * 60:
                diff = _("(%u seconds)") % diff
            elif diff < 5 * 60 * 60:
                diff = _("(%u minutes)") % (diff // 60)
            elif diff < 5 * 60 * 60 * 24:
                diff = _("(%u hours)") % (diff // (60 * 60))
            else:
                diff = _("(%u days)") % (diff // (60 * 60 * 24))
            print(_("End time       :"), endtm, diff)
        if old.end_rpmdbversion is not None:
            if old.altered_gt_rpmdb:
                print(_("End rpmdb      :"), old.end_rpmdbversion, "**")
            else:
                print(_("End rpmdb      :"), old.end_rpmdbversion)
        if isinstance(name, list):
            for name in name:
                print(_("User           :"), name)
        else:
            print(_("User           :"), name)
        if isinstance(old.return_code, list):
            codes = old.return_code
            if codes[0] is None:
                print(_("Return-Code    :"), "**", _("Aborted"), "**")
                codes = codes[1:]
            if codes:
                print(_("Return-Code    :"), _("Failures:"), ", ".join(codes))
        elif old.return_code is None:
            print(_("Return-Code    :"), "**", _("Aborted"), "**")
        elif old.return_code:
            print(_("Return-Code    :"), _("Failure:"), old.return_code)
        else:
            print(_("Return-Code    :"), _("Success"))

        if old.cmdline is not None:
            if isinstance(old.cmdline, list):
                for cmdline in old.cmdline:
                    print(_("Command Line   :"), cmdline)
            else:
                print(_("Command Line   :"), old.cmdline)

        if not isinstance(old.tid, list):
            addon_info = self.history.return_addon_data(old.tid)

            # for the ones we create by default - don't display them as there
            default_addons = set(['config-main', 'config-repos'])
            non_default = set(addon_info).difference(default_addons)
            if len(non_default) > 0:
                print(_("Additional non-default information stored: %d") % \
                          len(non_default))

        if old.trans_with:
            # This is _possible_, but not common
            print(_("Transaction performed with:"))
            pkg_max_len = max((len(str(hpkg)) for hpkg in old.trans_with))
        for hpkg in old.trans_with:
            _simple_pkg(hpkg, 4, was_installed=True, pkg_max_len=pkg_max_len)
        print(_("Packages Altered:"))
        self.historyInfoCmdPkgsAltered(old, pats)

        if old.trans_skip:
            print(_("Packages Skipped:"))
            pkg_max_len = max((len(str(hpkg)) for hpkg in old.trans_skip))
        for hpkg in old.trans_skip:
            #  Don't show the repo. here because we can't store it as they were,
            # by definition, not installed.
            _simple_pkg(hpkg, 4, pkg_max_len=pkg_max_len, show_repo=False)

        if old.rpmdb_problems:
            print(_("Rpmdb Problems:"))
        for prob in old.rpmdb_problems:
            key = "%s%s: " % (" " * 4, prob.problem)
            print(self.fmtKeyValFill(key, prob.text))
            if prob.packages:
                pkg_max_len = max((len(str(hpkg)) for hpkg in prob.packages))
            for hpkg in prob.packages:
                _simple_pkg(hpkg, 8, was_installed=True, highlight=hpkg.main,
                            pkg_max_len=pkg_max_len)

        if old.output:
            print(_("Scriptlet output:"))
            num = 0
            for line in old.output:
                num += 1
                print("%4d" % num, line)
        if old.errors:
            print(_("Errors:"))
            num = 0
            for line in old.errors:
                num += 1
                print("%4d" % num, line)

    _history_state2uistate = {'True-Install' : _('Install'),
                              'Install'      : _('Install'),
                              'Dep-Install'  : _('Dep-Install'),
                              'Obsoleted'    : _('Obsoleted'),
                              'Obsoleting'   : _('Obsoleting'),
                              'Erase'        : _('Erase'),
                              'Reinstall'    : _('Reinstall'),
                              'Downgrade'    : _('Downgrade'),
                              'Downgraded'   : _('Downgraded'),
                              'Update'       : _('Upgrade'),
                              'Updated'      : _('Upgraded'),
                              }
    def historyInfoCmdPkgsAltered(self, old, pats=[]):
        """Print information about how packages are altered in a transaction.

        :param old: the :class:`history.YumHistoryTransaction` to
           print information about
        :param pats: a list of patterns.  Packages that match a patten
           in *pats* will be highlighted in the output
        """
        last = None
        #  Note that these don't use _simple_pkg() because we are showing what
        # happened to them in the transaction ... not the difference between the
        # version in the transaction and now.
        all_uistates = self._history_state2uistate
        maxlen = 0
        pkg_max_len = 0
        for hpkg in old.trans_data:
            uistate = all_uistates.get(hpkg.state, hpkg.state)
            if maxlen < len(uistate):
                maxlen = len(uistate)
            if pkg_max_len < len(str(hpkg)):
                pkg_max_len = len(str(hpkg))

        for hpkg in old.trans_data:
            prefix = " " * 4
            if not hpkg.done:
                prefix = " ** "

            highlight = 'normal'
            if pats:
                x,m,u = dnf.yum.packages.parsePackages([hpkg], pats)
                if x or m:
                    highlight = 'bold'
            (hibeg, hiend) = self._highlight(highlight)

            # To chop the name off we need nevra strings, str(pkg) gives envra
            # so we have to do it by hand ... *sigh*.
            cn = hpkg.ui_nevra

            uistate = all_uistates.get(hpkg.state, hpkg.state)
            uistate = "%-*s" % (maxlen, uistate)
            # Should probably use columns here...
            if False: pass
            elif (last is not None and
                  last.state == 'Updated' and last.name == hpkg.name and
                  hpkg.state == 'Update'):
                ln = len(hpkg.name) + 1
                cn = (" " * ln) + cn[ln:]
            elif (last is not None and
                  last.state == 'Downgrade' and last.name == hpkg.name and
                  hpkg.state == 'Downgraded'):
                ln = len(hpkg.name) + 1
                cn = (" " * ln) + cn[ln:]
            else:
                last = None
                if hpkg.state in ('Updated', 'Downgrade'):
                    last = hpkg
            print("%s%s%s%s %-*s %s" % (prefix, hibeg, uistate, hiend,
                                        pkg_max_len, cn,
                                        self._hpkg2from_repo(hpkg)))

    def historySummaryCmd(self, extcmds):
        """Print a summary of transactions in history.

        :param extcmds: list of extra command line arguments
        """
        tids, printall = self._history_list_transactions(extcmds)
        if tids is None:
            return 1, ['Failed history info']

        fmt = "%s | %s | %s | %s"
        print(fmt % (fill_exact_width(_("Login user"), 26),
                     fill_exact_width(_("Time"), 19),
                     fill_exact_width(_("Action(s)"), 16),
                     fill_exact_width(_("Altered"), 8)))
        print("-" * 79)
        fmt = "%s | %s | %s | %8u"
        data = {'day' : {}, 'week' : {},
                'fortnight' : {}, 'quarter' : {}, 'half' : {},
                'year' : {}, 'all' : {}}
        for old in self.history.old(tids):
            name = self._pwd_ui_username(old.loginuid, 26)
            period = 'all'
            now = time.time()
            if False: pass
            elif old.beg_timestamp > (now - (24 * 60 * 60)):
                period = 'day'
            elif old.beg_timestamp > (now - (24 * 60 * 60 *  7)):
                period = 'week'
            elif old.beg_timestamp > (now - (24 * 60 * 60 * 14)):
                period = 'fortnight'
            elif old.beg_timestamp > (now - (24 * 60 * 60 *  7 * 13)):
                period = 'quarter'
            elif old.beg_timestamp > (now - (24 * 60 * 60 *  7 * 26)):
                period = 'half'
            elif old.beg_timestamp > (now - (24 * 60 * 60 * 365)):
                period = 'year'
            data[period].setdefault(name, []).append(old)
        _period2user = {'day'       : _("Last day"),
                        'week'      : _("Last week"),
                        'fortnight' : _("Last 2 weeks"), # US default :p
                        'quarter'   : _("Last 3 months"),
                        'half'      : _("Last 6 months"),
                        'year'      : _("Last year"),
                        'all'       : _("Over a year ago")}
        done = 0
        for period in ('day', 'week', 'fortnight', 'quarter', 'half', 'year',
                       'all'):
            if not data[period]:
                continue
            for name in sorted(data[period]):
                if not printall and done > 19:
                    break
                done += 1

                hpkgs = []
                for old in data[period][name]:
                    hpkgs.extend(old.trans_data)
                count, uiacts = self._history_uiactions(hpkgs)
                uperiod = _period2user[period]
                # Should probably use columns here, esp. for uiacts?
                print(fmt % (fill_exact_width(name, 26),
                             fill_exact_width(uperiod, 19),
                             fill_exact_width(uiacts, 16), count))

    def historyAddonInfoCmd(self, extcmds):
        """Print addon information about transaction in history.

        :param extcmds: list of extra command line arguments
        """
        tid = None
        if len(extcmds) > 1:
            tid = extcmds[1]
            if tid == 'last':
                tid = None
        if tid is not None:
            try:
                int(tid)
            except ValueError:
                self.logger.critical(_('Bad transaction ID given'))
                return 1, ['Failed history addon-info']

        if tid is not None:
            old = self.history.old(tids=[tid])
        else:
            old = [self.history.last(complete_transactions_only=False)]
            if old[0] is None:
                self.logger.critical(_('No transaction ID, or package, given'))
                return 1, ['Failed history addon-info']

        if not old:
            self.logger.critical(_('No Transaction %s found') % tid)
            return 1, ['Failed history addon-info']

        hist_data = old[0]
        addon_info = self.history.return_addon_data(hist_data.tid)
        if len(extcmds) <= 2:
            print(_("Transaction ID:"), hist_data.tid)
            print(_('Available additional history information:'))
            for itemname in self.history.return_addon_data(hist_data.tid):
                print('  %s' % itemname)
            print('')

            return 0, ['history addon-info']

        for item in extcmds[2:]:
            if item in addon_info:
                self.logger.info('%s:', item)
                print(self.history.return_addon_data(hist_data.tid, item), end='')
                self.logger.info('')
            else:
                print(_('%s: No additional data found by this name') % item)
            self.logger.info('')

    def historyPackageListCmd(self, extcmds):
        """Print a list of information about transactions from history
        that involve the given package or packages.

        :param extcmds: list of extra command line arguments
        """
        tids = self.history.search(extcmds)
        limit = None
        if extcmds and not tids:
            self.logger.critical(_('Bad transaction IDs, or package(s), given'))
            return 1, ['Failed history packages-list']
        if not tids:
            limit = 20

        all_uistates = self._history_state2uistate

        fmt = "%s | %s | %s"
        # REALLY Needs to use columns!
        print(fmt % (fill_exact_width(_("ID"), 6),
                     fill_exact_width(_("Action(s)"), 14),
                     fill_exact_width(_("Package"), 53)))
        print("-" * 79)
        fmt = "%6u | %s | %-50s"
        num = 0
        for old in self.history.old(tids, limit=limit):
            if limit is not None and num and (num +len(old.trans_data)) > limit:
                break
            last = None

            # Copy and paste from list ... uh.
            rmark = lmark = ' '
            if old.return_code is None:
                rmark = lmark = '*'
            elif old.return_code:
                rmark = lmark = '#'
                # We don't check .errors, because return_code will be non-0
            elif old.output:
                rmark = lmark = 'E'
            elif old.rpmdb_problems:
                rmark = lmark = 'P'
            elif old.trans_skip:
                rmark = lmark = 's'
            if old.altered_lt_rpmdb:
                rmark = '<'
            if old.altered_gt_rpmdb:
                lmark = '>'

            for hpkg in old.trans_data: # Find a pkg to go with each cmd...
                if limit is None:
                    x,m,u = dnf.yum.packages.parsePackages([hpkg], extcmds)
                    if not x and not m:
                        continue

                uistate = all_uistates.get(hpkg.state, hpkg.state)
                uistate = "%-*s" % (14, uistate)

                #  To chop the name off we need nevra strings, str(pkg) gives
                # envra so we have to do it by hand ... *sigh*.
                cn = hpkg.ui_nevra

                # Should probably use columns here...
                if False: pass
                elif (last is not None and
                      last.state == 'Updated' and last.name == hpkg.name and
                      hpkg.state == 'Update'):
                    ln = len(hpkg.name) + 1
                    cn = (" " * ln) + cn[ln:]
                elif (last is not None and
                      last.state == 'Downgrade' and last.name == hpkg.name and
                      hpkg.state == 'Downgraded'):
                    ln = len(hpkg.name) + 1
                    cn = (" " * ln) + cn[ln:]
                else:
                    last = None
                    if hpkg.state in ('Updated', 'Downgrade'):
                        last = hpkg

                num += 1
                print(fmt % (old.tid, uistate, cn), "%s%s" % (lmark,rmark))

    def historyPackageInfoCmd(self, extcmds):
        """Print information about packages in history transactions.

        :param extcmds: list of extra command line arguments
        """
        tids = self.history.search(extcmds)
        limit = None
        if extcmds and not tids:
            self.logger.critical(_('Bad transaction IDs, or package(s), given'))
            return 1, ['Failed history packages-info']
        if not tids:
            limit = 20

        all_uistates = self._history_state2uistate

        num = 0
        for old in self.history.old(tids, limit=limit):
            if limit is not None and num and (num +len(old.trans_data)) > limit:
                break

            for hpkg in old.trans_data: # Find a pkg to go with each cmd...
                if limit is None:
                    x,m,u = dnf.yum.packages.parsePackages([hpkg], extcmds)
                    if not x and not m:
                        continue

                uistate = all_uistates.get(hpkg.state, hpkg.state)
                if num:
                    print("")
                print(_("Transaction ID :"), old.tid)
                tm = time.ctime(old.beg_timestamp)
                print(_("Begin time     :"), tm)
                print(_("Package        :"), hpkg.ui_nevra)
                print(_("State          :"), uistate)
                if hpkg.size is not None:
                    num = int(hpkg.size)
                    print(_("Size           :"), locale.format("%d", num, True))
                if hpkg.buildhost is not None:
                    print(_("Build host     :"), hpkg.buildhost)
                if hpkg.buildtime is not None:
                    tm = time.ctime(int(hpkg.buildtime))
                    print(_("Build time     :"), tm)
                if hpkg.packager is not None:
                    print(_("Packager       :"), hpkg.packager)
                if hpkg.vendor is not None:
                    print(_("Vendor         :"), hpkg.vendor)
                if hpkg.license is not None:
                    print(_("License        :"), hpkg.license)
                if hpkg.url is not None:
                    print(_("URL            :"), hpkg.url)
                if hpkg.sourcerpm is not None:
                    print(_("Source RPM     :"), hpkg.sourcerpm)
                if hpkg.committime is not None:
                    tm = time.ctime(int(hpkg.committime))
                    print(_("Commit Time    :"), tm)
                if hpkg.committer is not None:
                    print(_("Committer      :"), hpkg.committer)
                if hpkg.yumdb_info.reason is not None:
                    print(_("Reason         :"), hpkg.yumdb_info.reason)
                if hpkg.yumdb_info.command_line is not None:
                    print(_("Command Line   :"), hpkg.yumdb_info.command_line)
                if hpkg.yumdb_info.from_repo is not None:
                    print(_("From repo      :"), hpkg.yumdb_info.from_repo)
                if hpkg.yumdb_info.installed_by is not None:
                    uid = int(hpkg.yumdb_info.installed_by)
                    name = self._pwd_ui_username(uid)
                    print(_("Installed by   :"), name)
                if hpkg.yumdb_info.changed_by is not None:
                    uid = int(hpkg.yumdb_info.changed_by)
                    name = self._pwd_ui_username(uid)
                    print(_("Changed by     :"), name)

                num += 1

class DepSolveProgressCallBack(dnf.callback.Depsolve):
    """A class to provide text output callback functions for Dependency Solver callback."""

    def __init__(self):
        """requires yum-cli log and errorlog functions as arguments"""
        self.logger = logging.getLogger("dnf")
        self.loops = 0

    def pkg_added(self, pkg, mode):
        """Print information about a package being added to the
        transaction set.

        :param pkgtup: tuple containing the package name, arch,
           version, and repository
        :param mode: a short string indicating why the package is
           being added to the transaction set.

        Valid current values for *mode* are::

           i = the package will be installed
           u = the package will be an update
           e = the package will be erased
           r = the package will be reinstalled
           d = the package will be a downgrade
           o = the package will be obsoleting another package
           ud = the package will be updated
           od = the package will be obsoleted
        """
        modedict = { 'i': _('installed'),
                     'u': _('an upgrade'),
                     'e': _('erased'),
                     'r': _('reinstalled'),
                     'd': _('a downgrade'),
                     'o': _('obsoleting'),
                     'ud': _('upgraded'),
                     'od': _('obsoleted'),
                     'dd': _('downgraded')}
        (n, a, evr) = (pkg.name, pkg.arch, pkg.evr)
        modeterm = modedict[mode]
        self.logger.debug(_('---> Package %s.%s %s will be %s'), n, a, evr,
                          modeterm)

    def start(self):
        """Perform setup at the beginning of the dependency solving
        process.
        """
        self.logger.debug(_('--> Starting dependency resolution'))
        self.loops += 1

    def end(self):
        """Output a message stating that dependency resolution has finished."""
        self.logger.debug(_('--> Finished dependency resolution'))

class CliTransactionDisplay(LoggingTransactionDisplay):
    """A Yum specific callback class for RPM operations."""

    width = property(lambda self: _term_width())

    def __init__(self):
        super(CliTransactionDisplay, self).__init__()
        self.lastmsg = ""
        self.lastpackage = None # name of last package we looked at
        self.output = True

        # for a progress bar
        self.mark = "#"
        self.marks = 22

    def event(self, package, action, te_current, te_total, ts_current, ts_total):
        """Output information about an rpm operation.  This may
        include a text progress bar.

        :param package: the package involved in the event
        :param action: the type of action that is taking place.  Valid
           values are given by
           :func:`rpmtrans.LoggingTransactionDisplay.action.keys()`
        :param te_current: a number representing the amount of work
           already done in the current transaction
        :param te_total: a number representing the total amount of work
           to be done in the current transaction
        :param ts_current: the number of the current transaction in
           transaction set
        :param ts_total: the total number of transactions in the
           transaction set
        """
        process = self.action.get(action)
        if process is None:
            return

        wid1 = self._max_action_width()

        pkgname = str(package)
        self.lastpackage = package
        if te_total == 0:
            percent = 0
        else:
            percent = (te_current*long(100))//te_total
        self._out_event(te_current, te_total, ts_current, ts_total,
                        percent, process, pkgname, wid1)

    def _max_action_width(self):
        if not hasattr(self, '_max_action_wid_cache'):
            wid1 = 0
            for val in self.action.values():
                wid_val = len(val)
                if wid1 < wid_val:
                    wid1 = wid_val
            self._max_action_wid_cache = wid1
        wid1 = self._max_action_wid_cache
        return wid1

    def _out_event(self, te_current, te_total, ts_current, ts_total,
                   percent, process, pkgname, wid1):
        if self.output and (sys.stdout.isatty() or te_current == te_total):
            (fmt, wid1, wid2) = self._makefmt(percent, ts_current, ts_total,
                                              progress=sys.stdout.isatty(),
                                              pkgname=pkgname, wid1=wid1)
            pkgname = ucd(pkgname)
            msg = fmt % ("%-*.*s" % (wid1, wid1, process),
                         "%-*.*s" % (wid2, wid2, pkgname))
            if msg != self.lastmsg:
                sys.stdout.write(msg)
                sys.stdout.flush()
                self.lastmsg = msg
            if te_current == te_total:
                print(" ")

    def scriptout(self, msgs):
        """Print messages originating from a package script.

        :param msgs: the messages coming from the script
        """
        if msgs:
            sys.stdout.write(ucd(msgs))
            sys.stdout.flush()

    def _makefmt(self, percent, ts_current, ts_total, progress = True,
                 pkgname=None, wid1=15):
        l = len(str(ts_total))
        size = "%s.%s" % (l, l)
        fmt_done = "%" + size + "s/%" + size + "s"
        done = fmt_done % (ts_current, ts_total)

        #  This should probably use TerminLine, but we don't want to dep. on
        # that. So we kind do an ok job by hand ... at least it's dynamic now.
        if pkgname is None:
            pnl = 22
        else:
            pnl = len(pkgname)

        overhead  = (2 * l) + 2 # Length of done, above
        overhead +=  2+ wid1 +2 # Length of beginning ("  " action " :")
        overhead +=  1          # Space between pn and done
        overhead +=  2          # Ends for progress
        overhead +=  1          # Space for end
        width = self.width
        if width < overhead:
            width = overhead    # Give up
        width -= overhead
        if pnl > width // 2:
            pnl = width // 2

        marks = self.width - (overhead + pnl)
        width = "%s.%s" % (marks, marks)
        fmt_bar = "[%-" + width + "s]"
        # pnl = str(28 + marks + 1)
        full_pnl = pnl + marks + 1

        if progress and percent == 100: # Don't chop pkg name on 100%
            fmt = "\r  %s: %s   " + done
            wid2 = full_pnl
        elif progress:
            bar = fmt_bar % (self.mark * int(marks * (percent / 100.0)), )
            fmt = "\r  %s: %s " + bar + " " + done
            wid2 = pnl
        elif percent == 100:
            fmt = "  %s: %s   " + done
            wid2 = full_pnl
        else:
            bar = fmt_bar % (self.mark * marks, )
            fmt = "  %s: %s " + bar + " " + done
            wid2 = pnl
        return fmt, wid1, wid2

    def verify_tsi_package(self, pkg, count, total):
        percent = 100
        process = _('Verifying')
        wid1 = max(len(process), self._max_action_width())
        self._out_event(100, 100, count, total, percent, process, str(pkg), wid1)

def progressbar(current, total, name=None):
    """Output the current status to the terminal using a simple
    text progress bar consisting of 50 # marks.

    :param current: a number representing the amount of work
       already done
    :param total: a number representing the total amount of work
       to be done
    :param name: a name to label the progress bar with
    """
    """simple progress bar 50 # marks"""

    mark = '#'
    if not sys.stdout.isatty():
        return

    if current == 0:
        percent = 0
    else:
        if total != 0:
            percent = float(current) / total
        else:
            percent = 0

    width = _term_width()

    if name is None and current == total:
        name = '-'

    end = ' %d/%d' % (current, total)
    width -= len(end) + 1
    if width < 0:
        width = 0
    if name is None:
        width -= 2
        if width < 0:
            width = 0
        hashbar = mark * int(width * percent)
        output = '\r[%-*s]%s' % (width, hashbar, end)
    elif current == total: # Don't chop name on 100%
        output = '\r%s%s' % (fill_exact_width(name, width), end)
    else:
        width -= 4
        if width < 0:
            width = 0
        nwid = width // 2
        if nwid > len(name):
            nwid = len(name)
        width -= nwid
        hashbar = mark * int(width * percent)
        output = '\r%s: [%-*s]%s' % (fill_exact_width(name, nwid), width,
                                     hashbar, end)

    if current <= total:
        sys.stdout.write(output)

    if current == total:
        sys.stdout.write('\n')

    sys.stdout.flush()

########NEW FILE########
__FILENAME__ = progress
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.

from __future__ import unicode_literals
from dnf.cli.format import format_number, format_time
from dnf.cli.term import _term_width
from time import time

import sys
import dnf.callback

class MultiFileProgressMeter(dnf.callback.DownloadProgress):
    """Multi-file download progress meter"""

    STATUS_2_STR = {
        dnf.callback.STATUS_FAILED : 'FAILED',
        dnf.callback.STATUS_ALREADY_EXISTS : 'SKIPPED',
        dnf.callback.STATUS_MIRROR : 'MIRROR',
        dnf.callback.STATUS_DRPM : 'DRPM',
    }

    def __init__(self, fo=sys.stderr, update_period=0.3, tick_period=1.0, rate_average=5.0):
        """Creates a new progress meter instance

        update_period -- how often to update the progress bar
        tick_period -- how fast to cycle through concurrent downloads
        rate_average -- time constant for average speed calculation
        """
        self.fo = fo
        self.update_period = update_period
        self.tick_period = tick_period
        self.rate_average = rate_average

    def message(self, msg):
        self.fo.write(msg)
        self.fo.flush()

    def start(self, total_files, total_size):
        self.total_files = total_files
        self.total_size = total_size

        # download state
        self.done_files = 0
        self.done_size = 0
        self.state = {}
        self.active = []

        # rate averaging
        self.last_time = 0
        self.last_size = 0
        self.rate = None

    def progress(self, payload, done):
        now = time()
        text = str(payload)
        total = int(payload.download_size)
        done = int(done)

        # update done_size
        if text not in self.state:
            self.state[text] = now, 0
            self.active.append(text)
        start, old = self.state[text]
        self.state[text] = start, done
        self.done_size += done - old

        # update screen if enough time has elapsed
        if now - self.last_time > self.update_period:
            if total > self.total_size:
                self.total_size = total
            self._update(now)

    def _update(self, now):
        if self.last_time:
            delta_time = now - self.last_time
            delta_size = self.done_size - self.last_size
            if delta_time > 0 and delta_size > 0:
                # update the average rate
                rate = delta_size / delta_time
                if self.rate is not None:
                    weight = min(delta_time/self.rate_average, 1)
                    rate = rate*weight + self.rate*(1 - weight)
                self.rate = rate
        self.last_time = now
        self.last_size = self.done_size

        # pick one of the active downloads
        text = self.active[int(now/self.tick_period) % len(self.active)]
        if self.total_files > 1:
            n = '%d' % (self.done_files + 1)
            if len(self.active) > 1:
                n += '-%d' % (self.done_files + len(self.active))
            text = '(%s/%d): %s' % (n, self.total_files, text)

        # average rate, total done size, estimated remaining time
        msg = ' %5sB/s | %5sB %9s ETA\r' % (
            format_number(self.rate) if self.rate else '---  ',
            format_number(self.done_size),
            format_time((self.total_size - self.done_size) / self.rate) if self.rate else '--:--')
        left = _term_width() - len(msg)
        bl = (left - 7)//2
        if bl > 8:
            # use part of the remaining space for progress bar
            pct = self.done_size*100 // self.total_size
            n, p = divmod(self.done_size*bl*2 // self.total_size, 2)
            bar = '='*n + '-'*p
            msg = '%3d%% [%-*s]%s' % (pct, bl, bar, msg)
            left -= bl + 7
        self.fo.write('%-*.*s%s' % (left, left, text, msg))
        self.fo.flush()

    def end(self, payload, status, err_msg):
        start = now = time()
        text = str(payload)
        size = int(payload.download_size)

        # update state
        if status in (dnf.callback.STATUS_MIRROR, dnf.callback.STATUS_DRPM):
            pass
        elif text in self.state:
            start, done = self.state.pop(text)
            self.active.remove(text)
            size -= done
            self.done_files += 1
            self.done_size += size
        elif status == dnf.callback.STATUS_ALREADY_EXISTS:
            self.done_files += 1
            self.done_size += size

        if status:
            # the error message, no trimming
            msg = '[%s] %s: ' % (self.STATUS_2_STR[status], text)
            left = _term_width() - len(msg) - 1
            msg = '%s%-*s\n' % (msg, left, err_msg)
        else:
            if self.total_files > 1:
                text = '(%d/%d): %s' % (self.done_files, self.total_files, text)

            # average rate, file size, download time
            tm = max(now - start, 0.001)
            msg = ' %5sB/s | %5sB %9s    \n' % (
                format_number(float(done) / tm),
                format_number(done),
                format_time(tm))
            left = _term_width() - len(msg)
            msg = '%-*.*s%s' % (left, left, text, msg)
        self.fo.write(msg)
        self.fo.flush()

        # now there's a blank line. fill it if possible.
        if self.active:
            self._update(now)

########NEW FILE########
__FILENAME__ = term
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from __future__ import unicode_literals
import termios, fcntl, struct

def _term_width(fd=1):
    """ Get the real terminal width """
    # Code from http://mail.python.org/pipermail/python-list/2000-May/033365.html
    try:
        buf = 'abcdefgh'
        buf = fcntl.ioctl(fd, termios.TIOCGWINSZ, buf)
        ret = struct.unpack(b'hhhh', buf)[1]
        if ret == 0:
            return 80
        if ret < 20:
            return 20
        return ret
    except IOError:
        return 80


########NEW FILE########
__FILENAME__ = utils
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

"""Various utility functions, and a utility class."""

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.i18n import _
import os
import time

from .cli import *

from dnf.cli.format import format_number

try:
    _USER_HZ = os.sysconf(os.sysconf_names['SC_CLK_TCK'])
except (AttributeError, KeyError):
    # Huh, non-Unix platform? Or just really old?
    _USER_HZ = 100

def jiffies_to_seconds(jiffies):
    """Convert a number of jiffies to seconds. How many jiffies are in a second
    is system-dependent, e.g. 100 jiffies = 1 second is common.

    :param jiffies: a number of jiffies
    :return: the equivalent number of seconds
    """
    return int(jiffies) / _USER_HZ

def seconds_to_ui_time(seconds):
    """Return a human-readable string representation of the length of
    a time interval given in seconds.

    :param seconds: the length of the time interval in seconds
    :return: a human-readable string representation of the length of
      the time interval
    """
    if seconds >= 60 * 60 * 24:
        return "%d day(s) %d:%02d:%02d" % (seconds // (60 * 60 * 24),
                                           (seconds // (60 * 60)) % 24,
                                           (seconds // 60) % 60,
                                           seconds % 60)
    if seconds >= 60 * 60:
        return "%d:%02d:%02d" % (seconds // (60 * 60), (seconds // 60) % 60,
                                 (seconds % 60))
    return "%02d:%02d" % ((seconds // 60), seconds % 60)

def get_process_info(pid):
    """Return information about a process taken from
    /proc/*pid*/status, /proc/stat/, and /proc/*pid*/stat.

    :param pid: the process id number
    :return: a dictionary containing information about the process
    """
    if not pid:
        return

    try:
        pid = int(pid)
    except ValueError as e:
        return

    # Maybe true if /proc isn't mounted, or not Linux ... or something.
    if (not os.path.exists("/proc/%d/status" % pid) or
        not os.path.exists("/proc/stat") or
        not os.path.exists("/proc/%d/stat" % pid)):
        return

    ps = {}
    for line in open("/proc/%d/status" % pid):
        if line[-1] != '\n':
            continue
        data = line[:-1].split(':\t', 1)
        if len(data) < 2:
            continue
        if data[1].endswith(' kB'):
            data[1] = data[1][:-3]
        ps[data[0].strip().lower()] = data[1].strip()
    if 'vmrss' not in ps:
        return
    if 'vmsize' not in ps:
        return
    boot_time = None
    for line in open("/proc/stat"):
        if line.startswith("btime "):
            boot_time = int(line[len("btime "):-1])
            break
    if boot_time is None:
        return
    ps_stat = open("/proc/%d/stat" % pid).read().split()
    ps['utime'] = jiffies_to_seconds(ps_stat[13])
    ps['stime'] = jiffies_to_seconds(ps_stat[14])
    ps['cutime'] = jiffies_to_seconds(ps_stat[15])
    ps['cstime'] = jiffies_to_seconds(ps_stat[16])
    ps['start_time'] = boot_time + jiffies_to_seconds(ps_stat[21])
    ps['state'] = {'R' : _('Running'),
                   'S' : _('Sleeping'),
                   'D' : _('Uninterruptible'),
                   'Z' : _('Zombie'),
                   'T' : _('Traced/Stopped')
                   }.get(ps_stat[2], _('Unknown'))

    return ps

def show_lock_owner(pid, logger):
    """Output information about another process that is holding the
    yum lock.

    :param pid: the process id number of the process holding the yum
       lock
    :param logger: the logger to output the information to
    :return: a dictionary containing information about the process.
       This is the same as the dictionary returned by
       :func:`get_process_info`.
    """
    ps = get_process_info(pid)
    if not ps:
        return None

    # This yumBackend isn't very friendly, so...
    msg = _('  The application with PID %d is: %s')
    if ps['name'] == 'yumBackend.py':
        nmsg = msg % (pid, 'PackageKit')
    else:
        nmsg = msg % (pid, ps['name'])

    logger.critical("%s", nmsg)
    logger.critical(_("    Memory : %5s RSS (%5sB VSZ)") %
                    (format_number(int(ps['vmrss']) * 1024),
                     format_number(int(ps['vmsize']) * 1024)))

    ago = seconds_to_ui_time(int(time.time()) - ps['start_time'])
    logger.critical(_("    Started: %s - %s ago") %
                    (time.ctime(ps['start_time']), ago))
    logger.critical(_("    State  : %s") % ps['state'])

    return ps

########NEW FILE########
__FILENAME__ = comps
# comps.py
# Interface to libcomps.
#
# Copyright (C) 2013-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import print_function
from __future__ import unicode_literals
from dnf.exceptions import CompsError
from dnf.i18n import _
from functools import reduce

import dnf.i18n
import dnf.util
import fnmatch
import gettext
import libcomps
import locale
import operator
import re

# :api :binformat
CONDITIONAL = 1
DEFAULT     = 2
MANDATORY   = 4
OPTIONAL    = 8

ALL_TYPES = CONDITIONAL | DEFAULT | MANDATORY | OPTIONAL

def _internal_comps_length(comps):
    collections = (comps.categories, comps.groups, comps.environments)
    return reduce(operator.__add__, map(len, collections))

def _first_if_iterable(seq):
    if seq is None:
        return None
    return dnf.util.first(seq)

def _by_pattern(pattern, case_sensitive, sqn):
    """Return items from sqn matching either exactly or glob-wise."""

    pattern = dnf.i18n.ucd(pattern)
    exact = {g for g in sqn if g.name == pattern or g.id == pattern}
    if exact:
        return exact

    if case_sensitive:
        match = re.compile(fnmatch.translate(pattern)).match
    else:
        match = re.compile(fnmatch.translate(pattern), flags=re.I).match

    return {g for g in sqn if match(g.name) or match(g.id)}

class _Langs(object):

    """Get all usable abbreviations for the current language."""

    def __init__(self):
        self.last_locale = None
        self.cache = None

    @staticmethod
    def _dotted_locale_str():
        lcl = locale.getlocale(locale.LC_MESSAGES)
        if lcl == (None, None):
            return 'C'
        return'.'.join(lcl)

    def get(self):
        current_locale = self._dotted_locale_str()
        if self.last_locale == current_locale:
            return self.cache

        self.cache = []
        locales = [current_locale]
        if current_locale != 'C':
            locales.append('C')
        for l in locales:
            for nlang in gettext._expand_lang(l):
                if nlang not in self.cache:
                    self.cache.append(nlang)

        self.last_locale = current_locale
        return self.cache

class Forwarder(object):
    def __init__(self, iobj, langs):
        self._i = iobj
        self._langs = langs

    def __getattr__(self, name):
        return getattr(self._i, name)

    def _ui_text(self, default, dct):
        for l in self._langs.get():
            t = dct.get(l)
            if t is not None:
                return t
        return default

    @property
    def ui_description(self):
        return self._ui_text(self.desc, self.desc_by_lang)

    @property
    def ui_name(self):
        return self._ui_text(self.name, self.name_by_lang)

class Category(Forwarder):
    # :api
    pass

class Environment(Forwarder):
    # :api

    def __init__(self, iobj, langs, group_factory):
        super(Environment, self).__init__(iobj, langs)
        self._group_factory = group_factory

    def groups_iter(self):
        for grp_id in self.group_ids:
            grp = self._group_factory(grp_id.name)
            if grp is None:
                msg = "no group '%s' from environment '%s'"
                raise ValueError(msg % (grp_id.name, self.id))
            yield grp

class Group(Forwarder):
    # :api
    def __init__(self, iobj, langs, pkg_factory):
        super(Group, self).__init__(iobj, langs)
        self._pkg_factory = pkg_factory
        self.selected = False

    def _packages_of_type(self, type_):
        return [pkg for pkg in self.packages if pkg.type == type_]

    @property
    def conditional_packages(self):
        return self._packages_of_type(libcomps.PACKAGE_TYPE_CONDITIONAL)

    @property
    def default_packages(self):
        return self._packages_of_type(libcomps.PACKAGE_TYPE_DEFAULT)

    def packages_iter(self):
        # :api
        return map(self._pkg_factory, self.packages)

    @property
    def mandatory_packages(self):
        return self._packages_of_type(libcomps.PACKAGE_TYPE_MANDATORY)

    @property
    def optional_packages(self):
        return self._packages_of_type(libcomps.PACKAGE_TYPE_OPTIONAL)

    @property
    def visible(self):
        return self._i.uservisible

class Package(Forwarder):
    """Represents comps package data. :api"""

    _OPT_MAP = {
        libcomps.PACKAGE_TYPE_CONDITIONAL : CONDITIONAL,
        libcomps.PACKAGE_TYPE_DEFAULT     : DEFAULT,
        libcomps.PACKAGE_TYPE_MANDATORY   : MANDATORY,
        libcomps.PACKAGE_TYPE_OPTIONAL    : OPTIONAL,
    }

    def __init__(self, ipkg):
        self._i = ipkg

    @property
    def name(self):
        # :api
        return self._i.name

    @property
    def option_type(self):
        # :api
        return self._OPT_MAP[self.type]

class Comps(object):
    # :api

    def __init__(self):
        self._i = libcomps.Comps()
        self._langs = _Langs()

    def __len__(self):
        return _internal_comps_length(self._i)

    def _build_category(self, icategory):
        return Category(icategory, self._langs)

    def _build_environment(self, ienvironment):
        return Environment(ienvironment, self._langs, self.group_by_id)

    def _build_group(self, igroup):
        return Group(igroup, self._langs, self._build_package)

    def _build_package(self, ipkg):
        return Package(ipkg)

    def add_from_xml_filename(self, fn):
        comps = libcomps.Comps()
        ret = comps.fromxml_f(fn)
        if ret == -1:
            errors = comps.get_last_parse_errors()
            raise CompsError(' '.join(errors))
        self._i = self._i + comps

    @property
    def categories(self):
        # :api
        return list(self.categories_iter())

    def category_by_pattern(self, pattern, case_sensitive=False):
        # :api
        cats = self.categories_by_pattern(pattern, case_sensitive)
        return _first_if_iterable(cats)

    def categories_by_pattern(self, pattern, case_sensitive=False):
        # :api
        return _by_pattern(pattern, case_sensitive, self.categories)

    def categories_iter(self):
        # :api
        return (self._build_category(c) for c in self._i.categories)

    @property
    def environments(self):
        # :api
        return list(self.environments_iter())

    def environment_by_pattern(self, pattern, case_sensitive=False):
        # :api
        envs = self.environments_by_pattern(pattern, case_sensitive)
        return _first_if_iterable(envs)

    def environments_by_pattern(self, pattern, case_sensitive=False):
        # :api
        return _by_pattern(pattern, case_sensitive, self.environments)

    def environments_iter(self):
        # :api
        return (self._build_environment(e) for e in self._i.environments)

    @property
    def groups(self):
        # :api
        return list(self.groups_iter())

    def group_by_id(self, id_):
        return dnf.util.first(g for g in self.groups_iter() if g.id == id_)

    def group_by_pattern(self, pattern, case_sensitive=False):
        # :api
        grps = self.groups_by_pattern(pattern, case_sensitive)
        return _first_if_iterable(grps)

    def groups_by_pattern(self, pattern, case_sensitive=False):
        # :api
        return _by_pattern(pattern, case_sensitive, self.groups)

    def groups_iter(self):
        # :api
        return (self._build_group(g) for g in self._i.groups)


class TransactionBunch(object):
    def __init__(self):
        self.install = set()
        self.remove = set()
        self.upgrade = set()

    def __iadd__(self, other):
        self.install.update(other.install)
        self.upgrade.update(other.upgrade)
        self.remove = (self.remove | other.remove) - self.install - self.upgrade
        return self


class Solver(object):
    def __init__(self, persistor, reason_fn):
        self.persistor = persistor
        self._reason_fn = reason_fn

    @staticmethod
    def _full_group_set(env):
        return {grp.id for grp in env.groups_iter()}

    @staticmethod
    def _full_package_set(grp):
        return {pkg.name for pkg in grp.mandatory_packages +
                grp.default_packages + grp.optional_packages}

    @staticmethod
    def _pkgs_of_type(group, pkg_types, exclude):
        pkgs = set()
        if pkg_types & MANDATORY:
            pkgs.update(pkg.name for pkg in group.mandatory_packages)
        if pkg_types & DEFAULT:
            pkgs.update(pkg.name for pkg in group.default_packages
                        if pkg.name not in exclude)
        if pkg_types & OPTIONAL:
            pkgs.update(pkg.name for pkg in group.optional_packages
                        if pkg.name not in exclude)
        return pkgs

    def _removable_pkg(self, pkg_name):
        prst = self.persistor
        count = 0
        if self._reason_fn(pkg_name) != 'group':
            return False
        for id_ in prst.groups:
            p_grp = prst.group(id_)
            count += sum(1 for pkg in p_grp.full_list if pkg == pkg_name)
        return count < 2

    def _removable_grp(self, grp_name):
        prst = self.persistor
        count = 0
        for id_ in prst.environments:
            p_env = prst.environment(id_)
            count += sum(1 for grp in p_env.full_list if grp == grp_name)
        return count < 2

    def environment_install(self, env, pkg_types, exclude):
        p_env = self.persistor.environment(env.id)
        if p_env.installed:
            raise CompsError(_("Environment '%s' is already installed.") %
                             env.ui_name)

        p_env.grp_types = CONDITIONAL | DEFAULT | MANDATORY | OPTIONAL
        exclude = set() if exclude is None else set(exclude)
        p_env.pkg_exclude.extend(exclude)
        p_env.pkg_types = pkg_types
        p_env.full_list.extend(self._full_group_set(env))

        trans = TransactionBunch()
        for grp in env.groups_iter():
            try:
                trans += self.group_install(grp, pkg_types, exclude)
            except dnf.exceptions.CompsError:
                pass
        return trans

    def environment_remove(self, env):
        p_env = self.persistor.environment(env.id)
        if not p_env.installed:
            raise CompsError(_("Environment '%s' is not installed.") %
                             env.ui_name)

        trans = TransactionBunch()
        group_names = set(p_env.full_list)

        for grp in env.groups_iter():
            if grp.id not in group_names:
                continue
            if not self._removable_grp(grp.id):
                continue
            trans += self.group_remove(grp)

        del p_env.full_list[:]
        del p_env.pkg_exclude[:]
        p_env.grp_types = 0
        p_env.pkg_types = 0
        return trans

    def environment_upgrade(self, env):
        p_env = self.persistor.environment(env.id)
        if not p_env.installed:
            raise CompsError(_("Environment '%s' is not installed.") %
                             env.ui_name)

        old_set = set(p_env.full_list)
        new_set = self._full_group_set(env)
        pkg_types = p_env.pkg_types
        exclude = p_env.pkg_exclude

        trans = TransactionBunch()
        for grp in env.groups_iter():
            if grp.id in old_set:
                # upgrade
                try:
                    trans += self.group_upgrade(grp)
                except dnf.exceptions.CompsError:
                    # might no longer be installed
                    pass
            else:
                # install
                trans += self.group_install(grp, pkg_types, exclude)
        return trans

    def group_install(self, group, pkg_types, exclude):
        p_grp = self.persistor.group(group.id)
        if p_grp.installed:
            raise CompsError(_("Group '%s' is already installed.") %
                             group.ui_name)

        exclude = set() if exclude is None else set(exclude)
        p_grp.pkg_exclude.extend(exclude)
        p_grp.pkg_types = pkg_types
        p_grp.full_list.extend(self._full_package_set(group))

        trans = TransactionBunch()
        trans.install = self._pkgs_of_type(group, pkg_types, exclude)
        return trans

    def group_remove(self, group):
        p_grp = self.persistor.group(group.id)
        if not p_grp.installed:
            raise CompsError(_("Group '%s' not installed.") %
                             group.ui_name)

        trans = TransactionBunch()
        exclude = p_grp.pkg_exclude
        trans.remove = {pkg for pkg in p_grp.full_list
                        if pkg not in exclude and self._removable_pkg(pkg)}
        p_grp.pkg_types = 0
        del p_grp.full_list[:]
        del p_grp.pkg_exclude[:]
        return trans

    def group_upgrade(self, group):
        p_grp = self.persistor.group(group.id)
        if not p_grp.installed:
            raise CompsError(_("Group '%s' not installed.") %
                             group.ui_name)
        exclude = set(p_grp.pkg_exclude)
        old_set = set(p_grp.full_list)
        new_set = self._pkgs_of_type(group, p_grp.pkg_types, exclude)
        del p_grp.full_list[:]
        p_grp.full_list.extend(self._full_package_set(group))

        trans = TransactionBunch()
        trans.install = new_set - old_set
        trans.remove = old_set - new_set
        trans.upgrade = old_set - trans.remove
        return trans

########NEW FILE########
__FILENAME__ = conf
# conf.py
# dnf configuration classes.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#


"""
The configuration classes and routines in yum are splattered over too many
places, hard to change and debug. The new structure here will replace that. Its
goal is to:

* accept configuration options from all three sources (the main config file,
  repo config files, command line switches)
* handle all the logic of storing those and producing related values.
* returning configuration values.
* optionally: asserting no value is overridden once it has been applied
  somewhere (e.g. do not let a new repo be initialized with different global
  cache path than an already existing one).

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.i18n import ucd, _
import dnf.const
import dnf.yum.config
import os.path
from . import util
from .yum import misc

Conf = dnf.yum.config.YumConf

class CliCache(object):
    def __init__(self, prefix, suffix):
        # set from the client, at most once:
        self.prefix = prefix
        self.suffix = suffix
        # internal:
        self._ready = False
        self._cachedir = None
        self._system_cachedir = None

    def _make_ready(self):
        if self._ready:
            return

        self._ready = True
        self._system_cachedir = self._retdir(self.prefix)
        if util.am_i_root():
            self._cachedir = self._system_cachedir
        else:
            try:
                user_prefix = misc.getCacheDir()
                self._cachedir = self._retdir(user_prefix)
            except (IOError, OSError) as e:
                self.logger.critical(_('Could not set cachedir: %s'), ucd(e))

    def _retdir(self, dir):
        return os.path.join(dir, self.suffix)

    @property
    def cachedir(self):
        self._make_ready()
        return self._cachedir

    @property
    def system_cachedir(self):
        self._make_ready()
        return self._system_cachedir

########NEW FILE########
__FILENAME__ = drpm
# drpm.py
# Delta RPM support
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from binascii import hexlify
from dnf.yum.misc import unlink_f
from dnf.i18n import _

import dnf.callback
import dnf.repo
import hawkey
import librepo
import logging
import os

MAX_PERCENTAGE = 50
APPLYDELTA = '/usr/bin/applydeltarpm'

logger = logging.getLogger("dnf")


class DeltaPayload(dnf.repo.PackagePayload):
    def __init__(self, delta_info, delta, pkg, progress):
        super(DeltaPayload, self).__init__(pkg, progress)
        self.delta_info = delta_info
        self.delta = delta

    def __str__(self):
        return os.path.basename(self.delta.location)

    def _end_cb(self, cbdata, lr_status, msg):
        super(DeltaPayload, self)._end_cb(cbdata, lr_status, msg)
        if lr_status != librepo.TRANSFER_ERROR:
            self.delta_info.enqueue(self)

    def _target_params(self):
        delta = self.delta
        ctype, csum = delta.chksum
        ctype = hawkey.chksum_name(ctype)
        chksum = hexlify(csum).decode()

        ctype_code = getattr(librepo, ctype.upper(), librepo.CHECKSUM_UNKNOWN)
        if ctype_code == librepo.CHECKSUM_UNKNOWN:
            logger.warn(_("unsupported checksum type: %s") % ctype)

        return {
            'relative_url' : delta.location,
            'checksum_type' : ctype_code,
            'checksum' : chksum,
            'expectedsize' : delta.downloadsize,
            'base_url' : delta.baseurl,
        }

    @property
    def download_size(self):
        return self.delta.downloadsize

    @property
    def full_size(self):
        return self.pkg.downloadsize

    def localPkg(self):
        location = self.delta.location
        return os.path.join(self.pkg.repo.pkgdir, os.path.basename(location))


class DeltaInfo(object):
    def __init__(self, query, progress):
        '''A delta lookup and rebuild context
           query -- installed packages to use when looking up deltas
           progress -- progress obj to display finished delta rebuilds
        '''
        deltarpm = 0
        if os.access(APPLYDELTA, os.X_OK):
            try:
                deltarpm = os.sysconf('SC_NPROCESSORS_ONLN')
            except:
                deltarpm = 4
        self.deltarpm = deltarpm
        self.query = query
        self.progress = progress

        self.queue = []
        self.jobs = {}
        self.err = {}

    def delta_factory(self, po, progress):
        '''Turn a po to Delta RPM po, if possible'''
        if not po.repo.deltarpm or not self.deltarpm:
            # drpm disabled
            return None
        if os.path.exists(po.localPkg()):
            # already there
            return None

        best = po.size * MAX_PERCENTAGE / 100
        best_delta = None
        for ipo in self.query.filter(name=po.name, arch=po.arch):
            delta = po.get_delta_from_evr(ipo.evr)
            if delta and delta.downloadsize < best:
                best = delta.downloadsize
                best_delta = delta
        if best_delta:
           return DeltaPayload(self, best_delta, po, progress)
        return None

    def job_done(self, pid, code):
        # handle a finished delta rebuild
        logger.debug('drpm: %d: return code: %d, %d', pid, code >> 8, code & 0xff)

        pload = self.jobs.pop(pid)
        pkg = pload.pkg
        if code != 0:
            unlink_f(pload.pkg.localPkg())
            self.err[pkg] = [_('Delta RPM rebuild failed')]
        elif not pload.pkg.verifyLocalPkg():
            self.err[pkg] = [_('Checksum of the delta-rebuilt RPM failed')]
        else:
            os.unlink(pload.localPkg())
            self.progress.end(pload, dnf.callback.STATUS_DRPM, 'done')

    def start_job(self, pload):
        # spawn a delta rebuild job
        spawn_args = [APPLYDELTA, APPLYDELTA,
                      '-a', pload.pkg.arch,
                      pload.localPkg(), pload.pkg.localPkg()]
        pid = os.spawnl(os.P_NOWAIT, *spawn_args)
        logger.debug('drpm: spawned %d: %s', pid, ' '.join(spawn_args[1:]))
        self.jobs[pid] = pload

    def enqueue(self, pload):
        # process finished jobs, start new ones
        while self.jobs:
            pid, code = os.waitpid(-1, os.WNOHANG)
            if not pid:
                break
            self.job_done(pid, code)
        self.queue.append(pload)
        while len(self.jobs) < self.deltarpm:
            self.start_job(self.queue.pop(0))
            if not self.queue:
                break

    def wait(self):
        '''Wait until all jobs have finished'''
        while self.jobs:
            pid, code = os.wait()
            self.job_done(pid, code)
            if self.queue:
                self.start_job(self.queue.pop(0))

########NEW FILE########
__FILENAME__ = exceptions
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
# Copyright 2004 Duke University

"""
Core DNF Errors.
"""

from __future__ import unicode_literals
from dnf.i18n import ucd

class Error(Exception):
    """Base Error. All other Errors thrown by DNF should inherit from this.

    :api

    """
    def __init__(self, value=None):
        Exception.__init__(self)
        self.value = ucd(value)

    def __str__(self):
        return "%s" %(self.value,)

    def __unicode__(self):
        return '%s' % self.value

class DeprecationWarning(DeprecationWarning):
    # :api
    pass

class CompsError(Error):
    pass

class YumRPMTransError(Error):
    def __init__(self, msg, errors):
        Error.__init__(self, msg)
        self.errors = errors

class LockError(Error):
    pass

class ProcessLockError(LockError):
    def __init__(self, value, pid):
        super(ProcessLockError, self).__init__(value)
        self.pid = pid

    def __reduce__(self):
        """Pickling support."""
        return (ProcessLockError, (self.value, self.pid))

class ThreadLockError(LockError):
    pass

class RepoError(Error):
    # :api
    pass

class ConfigError(Error):
    pass

class DepsolveError(Error):
    # :api
    pass

class DownloadError(Error):
    # :api
    def __init__(self, errmap):
        self.errmap = errmap

    @staticmethod
    def errmap2str(errmap):
        errstrings = []
        for key in errmap:
            for error in errmap[key]:
                msg = '%s: %s' % (key, error) if key else '%s' % error
                errstrings.append(msg)
        return '\n'.join(errstrings)

    def __str__(self):
        return self.errmap2str(self.errmap)

    def __unicode__(self):
        return ucd(self)

class MetadataError(Error):
    pass

class MiscError(Error):
    pass

class MarkingError(Error):
    # :api

    def __init__(self, value=None, pkg_spec=None):
        """Initialize the marking error instance."""
        super(MarkingError, self).__init__(value)
        self.pkg_spec = pkg_spec

class PackageNotFoundError(MarkingError):
    pass

class PackagesNotInstalledError(MarkingError):
    def __init__(self, value=None, pkg_spec=None, packages=[]):
        super(PackagesNotInstalledError, self).__init__(value, pkg_spec)
        self.packages = packages

class PackagesNotAvailableError(MarkingError):
    def __init__(self, value=None, pkg_spec=None, packages=[]):
        super(PackagesNotAvailableError, self).__init__(value, pkg_spec)
        self.packages = packages

class TransactionCheckError(Error):
    pass

########NEW FILE########
__FILENAME__ = goal
# goal.py
# Customized hawkey.Goal
#
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
import hawkey


class Goal(hawkey.Goal):
    def __init__(self, sack):
        super(Goal, self).__init__(sack)
        self.group_members = set()

    def get_reason(self, pkg):
        code = super(Goal, self).get_reason(pkg)
        if code == hawkey.REASON_DEP:
            return 'dep'
        if code == hawkey.REASON_USER:
            if pkg.name in self.group_members:
                return 'group'
            return 'user'
        raise ValueError('Unknown reason: %d' % reason)

########NEW FILE########
__FILENAME__ = history
# history.py
# Interfaces to the history of transactions.
#
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

"""Interfaces to the history of transactions."""

from __future__ import absolute_import
from __future__ import unicode_literals
from collections import defaultdict, Container, Iterable, Sized
from dnf.util import is_exhausted, split_by
from dnf.yum.history import YumHistory

INSTALLING_STATES = {'Install', 'Reinstall', 'Update', 'Downgrade'}

PRIMARY_STATES = {'Install', 'Erase', 'Reinstall', 'Downgrade', 'Update'}

REMOVING_STATES = {'Erase', 'Reinstalled', 'Updated', 'Downgraded', 'Obsoleted'}

STATE2OPPOSITE = {'Install': 'Erase',
                  'Erase': 'Install',
                  'Reinstall': 'Reinstall',
                  'Update': 'Downgrade',
                  'Downgrade': 'Update'}

STATE2COMPLEMENT = {'Reinstall': 'Reinstalled',
                    'Reinstalled': 'Reinstall',
                    'Update': 'Updated',
                    'Updated': 'Update',
                    'Downgrade': 'Downgraded',
                    'Downgraded': 'Downgrade'}

def open_history(database):
    """Open a history of transactions."""
    if isinstance(database, YumHistory):
        return _HistoryWrapper(database)
    else:
        raise TypeError("unsupported database type: %s" % type(database))

class _HistoryWrapper(object):
    """Transactions history interface on top of an YumHistory."""

    def __init__(self, yum_history):
        """Initialize a wrapper instance."""
        object.__init__(self)
        self._history = yum_history

    def __enter__(self):
        """Enter the runtime context."""
        return self

    def __exit__(self, _exc_type, _exc_val, _exc_tb):
        """Exit the runtime context."""
        self.close()
        return False

    def close(self):
        """Close the history."""
        self._history.close()

    def has_transaction(self, id_):
        """Test whether a transaction with given ID is stored."""
        return bool(self._history.old((str(id_),)))

    def last_transaction_id(self):
        """Get ID of the last stored transaction."""
        last_tx = self._history.last(complete_transactions_only=False)
        return last_tx.tid if last_tx else None

    def transaction_nevra_ops(self, id_):
        """Get operations on packages (by their NEVRAs) in the transaction."""
        if not self.has_transaction(id_):
            raise ValueError('no transaction with given ID: %d' % id_)

        hpkgs = self._history._old_data_pkgs(str(id_), sort=False)

        # Split history to history packages representing transaction items.
        items_hpkgs = split_by(hpkgs, lambda hpkg: hpkg.state in PRIMARY_STATES)

        # First item should be empty if the first state is valid.
        empty_item_hpkgs = next(items_hpkgs)
        assert not empty_item_hpkgs  # is empty

        # Return the operations.
        operations = NEVRAOperations()
        for item_hpkgs in items_hpkgs:
            obsoleted_nevras = []
            obsoleting_nevra = None
            replaced_nevra, replaced_state = None, None

            # It is easier to traverse the packages in the reversed order.
            reversed_it = reversed(tuple(item_hpkgs))
            hpkg = next(reversed_it)

            while hpkg.state == 'Obsoleted':  # Read obsoleted packages.
                obsoleted_nevras.append(hpkg.nevra)
                hpkg = next(reversed_it)
            if obsoleted_nevras:  # Read obsoleting package.
                assert hpkg.state == 'Obsoleting'
                obsoleting_nevra = hpkg.nevra
                hpkg = next(reversed_it)
            if hpkg.state in {'Reinstalled', 'Downgraded', 'Updated'}:  # Replaced.
                replaced_nevra, replaced_state = hpkg.nevra, hpkg.state
                hpkg = next(reversed_it)
            assert is_exhausted(reversed_it)
            assert not obsoleting_nevra or obsoleting_nevra == hpkg.nevra
            assert not replaced_state or replaced_state == STATE2COMPLEMENT[hpkg.state]

            operations.add(hpkg.state, hpkg.nevra, replaced_nevra, obsoleted_nevras)
        return operations

class NEVRAOperations(Sized, Iterable, Container):
    """Mutable container of operations on packages by their NEVRAs."""

    def __init__(self):
        """Initialize a wrapper instance."""
        self._nevra2primary_state = {}
        self._replaced_by = {}
        self._obsoleted_by = defaultdict(set)

    def __add__(self, other):
        """Compute the sum of *self* and the *other* one."""
        result = NEVRAOperations()
        for state, nevra, replaced_nevra, obsoleted_nevras in self:
            result.add(state, nevra, replaced_nevra, obsoleted_nevras)
        for state, nevra, replaced_nevra, obsoleted_nevras in other:
            result.add(state, nevra, replaced_nevra, obsoleted_nevras)
        return result

    def __contains__(self, operation):
        """Test whether the *operation* is in *self*."""
        try:
            state, nevra, replaced, obsoleted = operation
        except ValueError:
            return False
        try:
            state_ = self._nevra2primary_state[nevra]
        except KeyError:
            return False
        if state_ != state:
            return False
        replaced_ = self._replaced_by.get(nevra, None)
        if replaced_ != replaced:
            return False
        obsoleted_ = self._obsoleted_by[nevra]
        return set(obsoleted_) == set(obsoleted)

    def __eq__(self, other):
        """Test whether *self* is equal to the *other* one."""
        if self is other:
            return True
        if type(self) is not type(other):
            return False
        if len(self) != len(other):
            return False
        return all(operation in other for operation in self)

    def __iter__(self):
        """Get iterator over the contained operations."""
        return (
            (state, nevra, self._replaced_by.get(nevra, None), self._obsoleted_by[nevra])
            for nevra, state in self._nevra2primary_state.items())

    def __len__(self):
        """Compute the number of contained operations."""
        return len(self._nevra2primary_state.items())

    def __ne__(self, other):
        """Test whether *self* is not equal to the *other* one."""
        return not self == other

    def _add_erase(self, old_nevra):
        """Add new erase of the *old_nevra*."""
        state = self._state(old_nevra, None)
        if state is None:
            self._set_primary_state(old_nevra, 'Erase')
        else:
            if state in REMOVING_STATES:
                raise ValueError('NEVRA was already removed: %s' % old_nevra)
            elif state in INSTALLING_STATES:
                self._unset_primary_state(old_nevra)
            else:
                assert False

    def _add_install(self, new_nevra):
        """Add new install of the *new_nevra*."""
        state = self._state(new_nevra, None)
        if state is None:
            self._set_primary_state(new_nevra, 'Install')
        else:
            if state in INSTALLING_STATES:
                raise ValueError('NEVRA was already installed: %s' % new_nevra)
            elif state in REMOVING_STATES:
                self._set_primary_state(new_nevra, 'Reinstall', new_nevra)
            else:
                assert False

    def _add_obsoleted(self, obsoleting_nevra, obsoleted_nevra):
        """Add new *obsoleted_nevra* obsoleted by the *obsoleting_nevra*."""
        state = self._state(obsoleted_nevra, None)
        if state in {None, 'Obsoleted'}:
            self._set_obsoleted_state(obsoleting_nevra, obsoleted_nevra)
        elif state in REMOVING_STATES:
            assert state != 'Obsoleted'
            raise ValueError('NEVRA was already removed: %s' % obsoleted_nevra)
        elif state in INSTALLING_STATES:
            self._unset_primary_state(obsoleted_nevra)

    def _add_replacement(self, state, new_nevra, old_nevra):
        """Add new *new_nevra* replacing the *old_nevra* using the *state*."""
        assert state in {'Reinstall', 'Update', 'Downgrade'}

        old_state, new_state = self._state(old_nevra, None), self._state(new_nevra, None)
        if old_state is None and new_state is None:
            self._set_primary_state(new_nevra, state, old_nevra)
        elif old_state in REMOVING_STATES:
            raise ValueError('NEVRA was already removed: %s' % old_nevra)
        elif new_state in INSTALLING_STATES:
            if old_nevra != new_nevra:
                raise ValueError('NEVRA was already installed: %s' % new_nevra)
            # Following applies only for reinstallation of the NEVRA by the
            # same NEVRA. Do nothing.
            assert state == 'Reinstall'
        elif old_state == 'Reinstall' and new_state is None:
            # If a reinstall precedes, replace it by the new replacement but
            # use the replaced NEVRA of the reinstall as the new replaced NEVRA.
            self._combine_replacements(old_nevra, new_nevra, state)
        elif old_state == STATE2OPPOSITE[state] and new_state is None:
            # Following does not apply for for reinstalls. If the opposite
            # state precedes, replace it by an erase&install because it is
            # not clear, whether the result is an update or a downgrade.
            assert state != 'Reinstall'
            self._combine_replacements(old_nevra, new_nevra, 'Install')
        elif old_state == STATE2OPPOSITE[state] and self._replaced_by[old_nevra] == new_nevra:
            # If the opposite state with same both NEVRAs (but swapped)
            # precedes, replace it by a reinstall of the new replacing NEVRA
            # and an erase of the new replaced NEVRA.
            assert new_state == (STATE2COMPLEMENT[old_state]
                                 if old_nevra != new_nevra else 'Reinstall')
            self._combine_replacements(old_nevra, new_nevra, 'Reinstall')
        elif old_state == 'Install' and new_state == 'Erase':
            # If a manual replacement (erase&install) precedes, reinstall the
            # new replacing NEVRA and forget the new replacement.
            self._unset_primary_state(old_nevra)
            self._set_primary_state(new_nevra, 'Reinstall', new_nevra)
        elif old_state in INSTALLING_STATES and new_state is None:  # Remaining old states.
            # If the old_state does not match any previous conditions and if
            # it is a primary state, replace the old replacing NEVRA by the
            # new one.
            self._combine_replacements(old_nevra, new_nevra, old_state)
        elif old_state is None and new_state in REMOVING_STATES:
            # If the new replacing NEVRA was removed, reinstall it, install
            # the old replacing NEVRA and remove the new replaced NEVRA.
            try:
                replacement_nevra = self._replaces(new_nevra)
            except ValueError:
                pass
            else:
                self._set_primary_state(replacement_nevra, 'Install')
            self._set_primary_state(old_nevra, 'Erase')
            self._set_primary_state(new_nevra, 'Reinstall', new_nevra)
        else:
            assert False

    def _combine_replacements(self, old_nevra, new_nevra, state):
        """Combine the *old_nevra* operation with the *new_nevra* into the *state*."""
        replaced_nevra = None if state == 'Install' else self._replaced_by[old_nevra]
        self._unset_primary_state(old_nevra)
        self._set_primary_state(new_nevra, state, replaced_nevra)

    def _replaces(self, old_nevra, default=False):
        """Get the NEVRA replacing the *old_nevra*."""
        replacements = iter(self._replaced_by.items())
        for key, value in replacements:
            if value == old_nevra:
                assert all(val != old_nevra for val, _key in replacements)
                return key

        if default is not False:
            return default
        raise ValueError('no replacement for NEVRA: %s' % old_nevra)

    def _set_obsoleted_state(self, obsoleting_nevra, obsoleted_nevra):
        """Set the *obsoleting_nevra* as an obsoleting and the *obsoleted_nevra* as an obsoleted."""
        self._nevra2primary_state.pop(obsoleted_nevra, None)
        for replacement_nevra, replaced_nevra in self._replaced_by.items():
            if replaced_nevra == obsoleted_nevra:
                del self._replaced_by[replacement_nevra]
        self._obsoleted_by[obsoleting_nevra].add(obsoleted_nevra)

    def _set_primary_state(self, nevra, state, replaced_nevra=None):
        """Set the *nevra* in the *state* and the *replaced_nevra* as replaced."""
        self._nevra2primary_state[nevra] = state
        for replacement_nevra, replaced_nevra_ in list(self._replaced_by.items()):
            if replaced_nevra_ == nevra:
                del self._replaced_by[replacement_nevra]
        for obsoleted_nevras in self._obsoleted_by.values():
            obsoleted_nevras.discard(nevra)

        if replaced_nevra is not None:
            if replaced_nevra != nevra:
                self._nevra2primary_state.pop(replaced_nevra, None)
            for replacement_nevra, replaced_nevra_ in self._replaced_by.items():
                if replaced_nevra_ == replaced_nevra:
                    del self._replaced_by[replacement_nevra]
            self._replaced_by[nevra] = replaced_nevra
            for obsoleted_nevras in self._obsoleted_by.values():
                obsoleted_nevras.discard(replaced_nevra)

    def _state(self, nevra, default=False):
        """Get the state of the *nevra*."""
        try:
            state = self._nevra2primary_state[nevra]
        except KeyError:
            pass
        else:
            assert all(nevra not in obsoleted for obsoleted in self._obsoleted_by.values())
            assert ((state == 'Reinstall' and self._replaced_by[nevra] == nevra) or
                    self._replaces(nevra, None) is None)
            return state

        try:
            replacement_nevra = self._replaces(nevra)
        except ValueError:
            pass
        else:
            assert all(nevra not in obsoleted for obsoleted in self._obsoleted_by.values())
            replacement_state = self._state(replacement_nevra)
            return STATE2COMPLEMENT[replacement_state]

        if any(nevra in obsoleted for obsoleted in self._obsoleted_by.values()):
            return 'Obsoleted'

        if default is not False:
            return default

        raise ValueError('no state of NEVRA: %s' % nevra)

    def _unset_primary_state(self, nevra):
        """Unset primary state of the *nevra*."""
        state = self._nevra2primary_state.pop(nevra)
        assert ((state == 'Reinstall' and self._replaced_by[nevra] == nevra) or
                self._replaces(nevra, None) is None)
        assert all(nevra not in obsoleted for obsoleted in self._obsoleted_by.values())
        try:
            replaced_nevra = self._replaced_by[nevra]
        except KeyError:
            assert state == 'Install'
        else:
            self._set_primary_state(replaced_nevra, 'Erase')
        for obsoleted_nevra in self._obsoleted_by.pop(nevra, ()):
            self._set_primary_state(obsoleted_nevra, 'Erase')

    def add(self, state, nevra, replaced_nevra=None, obsoleted_nevras=()):
        """Add new *nevra* in the *state* replacing and obsoleting other NEVRAs."""
        if state == 'Install':
            if replaced_nevra:
                raise ValueError('Install cannot replace anything: %s'
                                 % replaced_nevra)
            self._add_install(nevra)
        elif state == 'Erase':
            if replaced_nevra or obsoleted_nevras:
                raise ValueError('Erase cannot replace/obsolete anything: %s' %
                                 replaced_nevra or obsoleted_nevras)
            self._add_erase(nevra)
        elif state in {'Reinstall', 'Downgrade', 'Update'}:
            self._add_replacement(state, nevra, replaced_nevra)
        else:
            raise ValueError('unknown operation: %s' % state)

        for obsoleted_nevra in obsoleted_nevras:
            self._add_obsoleted(nevra, obsoleted_nevra)

########NEW FILE########
__FILENAME__ = i18n
# i18n.py
#
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import print_function
from __future__ import unicode_literals
from dnf.pycomp import PY3, is_py3bytes, unicode, setlocale
import gettext
import locale
import os
import sys

"""
Centralize i18n stuff here. Must be unittested.
"""

class UnicodeStream(object):
    def __init__(self, stream, encoding):
        self.stream = stream
        self.encoding = encoding

    def write(self, s):
        if not isinstance(s, str):
            s = (s.decode(self.encoding, 'replace') if PY3 else
                 s.encode(self.encoding, 'replace'))
        self.stream.write(s)

    def __getattr__(self, name):
        return getattr(self.stream, name)

def _full_ucd_support(encoding):
    """Return true if encoding can express any Unicode character.

    Even if an encoding can express all accented letters in the given language,
    we can't generally settle for it in DNF since sometimes we output special
    characters like the registered trademark symbol (U+00AE) and surprisingly
    many national non-unicode encodings, including e.g. ASCII and ISO-8859-2,
    don't contain it.

    """
    if encoding is None:
        return False
    lower = encoding.lower()
    if lower.startswith('utf-') or lower.startswith('utf_'):
        return True
    return False

def _guess_encoding():
    """ Take the best shot at the current system's string encoding. """
    return locale.getpreferredencoding()

def setup_locale():
    try:
        setlocale(locale.LC_ALL, '')
        # set time to C so that we output sane things in the logs (#433091)
        setlocale(locale.LC_TIME, 'C')
    except locale.Error as e:
        # default to C locale if we get a failure.
        print('Failed to set locale, defaulting to C', file=sys.stderr)
        os.environ['LC_ALL'] = 'C'
        setlocale(locale.LC_ALL, 'C')

def setup_stdout():
    """ Check that stdout is of suitable encoding and handle the situation if
        not.

        Returns True if stdout was of suitable encoding already and no changes
        were needed.
    """
    stdout = sys.stdout
    try:
        encoding = stdout.encoding
    except AttributeError:
        encoding = None
    if not _full_ucd_support(encoding):
        sys.stdout = UnicodeStream(stdout, _guess_encoding())
        return False
    return True

def ucd_input(ucstring):
    """ Take input from user.

        What the raw_input() built-in does, but encode the prompt first
        (raw_input() won't check sys.stdout.encoding as e.g. print does, see
        test_i18n.TestInput.test_assumption()).
    """
    if not isinstance(ucstring, unicode):
        raise TypeError("input() accepts Unicode strings")
    if PY3:
        return input(ucstring)
    enc = sys.stdout.encoding if sys.stdout.encoding else 'utf8'
    s = ucstring.encode(enc, 'strict')
    return raw_input(s)

def ucd(obj):
    """ Like the builtin unicode() but tries to use a reasonable encoding. """
    if PY3:
        if is_py3bytes(obj):
            return str(obj, _guess_encoding())
        elif isinstance(obj, str):
            return obj
        return str(obj)
    else:
        if isinstance(obj, unicode):
            return obj
        if hasattr(obj, '__unicode__'):
            # see the doc for the unicode() built-in. The logic here is: if obj
            # implements __unicode__, let it take a crack at it, but handle the
            # situation if it fails:
            try:
                return unicode(obj)
            except UnicodeError:
                pass
        return unicode(str(obj), _guess_encoding())


def fill_exact_width(msg, width):
    return "%-*.*s" % (width, width, msg)


# setup translation
t = gettext.translation('dnf', fallback=True)
_ = t.ugettext
P_ = t.ungettext

########NEW FILE########
__FILENAME__ = lock
# lock.py
# DNF Locking Subsystem.
#
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
import dnf.const
from dnf.exceptions import ProcessLockError, ThreadLockError
import dnf.util
import os
import threading

class ProcessLock(object):
    def __init__(self, name):
        self.name = name
        self.thread_lock = threading.RLock()
        self.count = 0

    def _lock_thread(self):
        if not self.thread_lock.acquire(blocking=False):
            msg = '%s already locked by a different thread' % self.name
            raise ThreadLockError(msg)
        self.count += 1

    def _read_lock(self):
        with open(self._target, 'r') as f:
            return int(f.readline())

    @property
    @dnf.util.lazyattr('_tgt')
    def _target(self):
        fn = 'dnf-%s-lock.pid' % self.name
        if dnf.util.am_i_root():
            return os.path.join(dnf.const.RUNDIR, fn)
        user_run_dir = dnf.util.user_run_dir()
        dnf.util.ensure_dir(user_run_dir)
        return os.path.join(user_run_dir, fn)

    def _try_lock(self):
        pid = str(os.getpid()).encode('utf-8')
        try:
            fd = os.open(self._target, os.O_CREAT | os.O_WRONLY | os.O_EXCL, 0o644)
            os.write(fd, pid)
            os.close(fd)
            return True
        except OSError:
            return False

    def _unlock_thread(self):
        self.count -= 1
        self.thread_lock.release()

    def __enter__(self):
        self._lock_thread()
        if self._try_lock():
            return
        pid = self._read_lock()
        if pid == os.getpid():
            # already locked by this process
            return
        if not os.access('/proc/%d/stat' % pid, os.F_OK):
            # locked by a dead process
            os.unlink(self._target)
            if self._try_lock():
                return
        self._unlock_thread()
        msg = '%s already locked by %d' % (self.name, pid)
        raise ProcessLockError(msg, pid)

    def __exit__(self, *exc_args):
        if self.count == 1:
            os.unlink(self._target)
        self._unlock_thread()

    def decorator(self, fn):
        def wrapped(*args, **kwargs):
            with self:
                return fn(*args, **kwargs)
        return wrapped

metadata_cache_lock = ProcessLock('metadata-cache')
rpmdb_lock = ProcessLock('package-cache-lock')

########NEW FILE########
__FILENAME__ = logging
# logging.py
# DNF Logging Subsystem.
#
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
import dnf.exceptions
import dnf.const
import dnf.util
import logging
import os
import sys
import warnings

# :api loggers are: 'dnf', 'dnf.plugin', 'dnf.rpm'

SUPERCRITICAL = 100 # do not use this for logging
CRITICAL      = logging.CRITICAL
ERROR         = logging.ERROR
WARNING       = logging.WARNING
INFO          = logging.INFO
DEBUG         = logging.DEBUG
SUBDEBUG      = 6

def only_once(fn):
    """Method decorator turning the method into noop on second or later calls."""
    def noop(*args, **kwargs):
        pass
    def swan_song(self, *args, **kwargs):
        fn(self, *args, **kwargs)
        setattr(self, fn.__name__, noop)
    return swan_song

class MaxLevelFilter(object):
    def __init__(self, max_level):
        self.max_level = max_level

    def filter(self, record):
        if record.levelno >= self.max_level:
            return 0
        return 1

_VERBOSE_VAL_MAPPING = {
    0 : SUPERCRITICAL,
    1 : logging.INFO,
    2 : logging.INFO, # the default
    3 : logging.DEBUG,
    4 : logging.DEBUG,
    5 : logging.DEBUG,
    6 : logging.DEBUG, # verbose value
    }

def _cfg_verbose_val2level(cfg_errval):
    assert(0 <= cfg_errval <= 10)
    return _VERBOSE_VAL_MAPPING.get(cfg_errval, SUBDEBUG)

# Both the DNF default and the verbose default are WARNING. Note that ERROR has
# no specific level.
_ERR_VAL_MAPPING = {
    0 : SUPERCRITICAL,
    1 : logging.CRITICAL
    }

def _cfg_err_val2level(cfg_errval):
    assert(0 <= cfg_errval <= 10)
    return _ERR_VAL_MAPPING.get(cfg_errval, logging.WARNING)

def _create_filehandler(logfile):
    if not os.path.exists(logfile):
        dnf.util.ensure_dir(os.path.dirname(logfile))
        dnf.util.touch(logfile)
        # By default, make logfiles readable by the user (so the reporting ABRT
        # user can attach root logfiles).
        os.chmod(logfile, 0o644)
    handler = logging.FileHandler(logfile)
    formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s",
                                  "%b %d %H:%M:%S")
    handler.setFormatter(formatter)
    return handler

def _paint_mark(logger):
    logger.log(INFO, dnf.const.LOG_MARKER)

def depr(msg):
    warnings.warn(msg, dnf.exceptions.DeprecationWarning, 2)

class Logging(object):
    def __init__(self):
        self.stdout_handler = self.stderr_handler = None

    @only_once
    def presetup(self):
        logging.addLevelName(SUBDEBUG, "SUBDEBUG")
        logger_dnf = logging.getLogger("dnf")
        logger_dnf.setLevel(SUBDEBUG)

        # setup stdout
        stdout = logging.StreamHandler(sys.stdout)
        stdout.setLevel(INFO)
        stdout.addFilter(MaxLevelFilter(logging.WARNING))
        logger_dnf.addHandler(stdout)
        self.stdout_handler = stdout

        # setup stderr
        stderr = logging.StreamHandler(sys.stderr)
        stderr.setLevel(WARNING)
        logger_dnf.addHandler(stderr)
        self.stderr_handler = stderr

    @only_once
    def setup(self, verbose_level, error_level, logdir):
        self.presetup()
        logger_dnf = logging.getLogger("dnf")

        # setup file logger
        logfile = os.path.join(logdir, dnf.const.LOG)
        handler = _create_filehandler(logfile)
        logger_dnf.addHandler(handler)
        # temporarily turn off stdout/stderr handlers:
        self.stdout_handler.setLevel(SUPERCRITICAL)
        self.stderr_handler.setLevel(SUPERCRITICAL)
        # put the marker in the file now:
        _paint_mark(logger_dnf)
        # bring std handlers to the preferred level
        self.stdout_handler.setLevel(verbose_level)
        self.stderr_handler.setLevel(error_level)

        # setup Python warnings
        logging.captureWarnings(True)
        logger_warnings = logging.getLogger("py.warnings")
        logger_warnings.addHandler(self.stderr_handler)
        logger_warnings.addHandler(handler)

        # setup RPM callbacks logger
        logger_rpm = logging.getLogger("dnf.rpm")
        logger_rpm.propagate = False
        logger_rpm.setLevel(SUBDEBUG)
        logfile = os.path.join(logdir, dnf.const.LOG_RPM)
        handler = _create_filehandler(logfile)
        logger_rpm.addHandler(handler)
        _paint_mark(logger_rpm)

    def setup_from_dnf_conf(self, conf):
        verbose_level_r = _cfg_verbose_val2level(conf.debuglevel)
        error_level_r = _cfg_err_val2level(conf.errorlevel)
        logdir = conf.logdir
        return self.setup(verbose_level_r, error_level_r, logdir)

########NEW FILE########
__FILENAME__ = match_counter
# match_counter.py
# Implements class MatchCounter.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals
from functools import reduce

WEIGHTS = {
    'name'		: 7,
    'summary'		: 4,
    'description'	: 2,
    'url'		: 1,
    }


def _canonize_string_set(sset, length):
    """ Ordered sset with empty strings prepended. """
    current = len(sset)
    l = [''] * (length - current) + sorted(sset)
    return l


class MatchCounter(dict):
    """Map packages to which of their attributes matched in a search against
    what values.

    The mapping is: ``package -> [(key, needle), ... ]``.

    """

    @staticmethod
    def _eval_weights(pkg, matches):
        # how much is each match worth and return their sum:
        def weight(match):
            key = match[0]
            needle = match[1]
            haystack = getattr(pkg, key)
            coef = 2 if haystack == needle else 1
            return coef * WEIGHTS[key]

        return sum(map(weight, matches))

    @staticmethod
    def _eval_distance(pkg, matches):
        dist = 0
        for (key, needle) in matches:
            haystack = getattr(pkg, key)
            dist += len(haystack) - len(needle)
        return dist

    def _key_func(self):
        """Get the key function used for sorting matches.

        It is not enough to only look at the matches and order them by the sum
        of their weighted hits. In case this number is the same we have to
        ensure that the same matched needles are next to each other in the
        result.

        Returned function is:
        pkg -> (weights_sum, canonized_needles_set, -distance)

        """
        max_length = self._max_needles()
        def get_key(pkg):
            return (self._eval_weights(pkg, self[pkg]),
                    _canonize_string_set(self.matched_needles(pkg), max_length),
                    -self._eval_distance(pkg, self[pkg]))
        return get_key

    def _max_needles(self):
        """Return the max count of needles of all packages."""
        if self:
            return max(len(self.matched_needles(pkg)) for pkg in self)
        return 0

    def add(self, pkg, key, needle):
        self.setdefault(pkg, []).append((key, needle))

    def dump(self):
        for pkg in self:
            print('%s\t%s' % (pkg, self[pkg]))

    def matched_haystacks(self, pkg):
        return set(getattr(pkg, m[0]) for m in self[pkg])

    def matched_keys(self, pkg):
        return set(m[0] for m in self[pkg])

    def matched_needles(self, pkg):
        return set(m[1] for m in self[pkg])

    def sorted(self, reverse=False, limit_to=None):
        keys = limit_to if limit_to else self.keys()
        return sorted(keys, key=self._key_func(), reverse=reverse)

    def total(self):
        return reduce(lambda total, pkg: total + len(self[pkg]), self, 0)

########NEW FILE########
__FILENAME__ = package
# package.py
# Module defining the dnf.Package class.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

""" Contains the dnf.Package class. """

from __future__ import absolute_import
from __future__ import unicode_literals

import binascii
import dnf.rpmUtils.miscutils
import dnf.yum.misc
import hawkey
import logging
import os

logger = logging.getLogger("dnf")

class Package(hawkey.Package):
    """ Represents a package. """
    def __init__(self, initobject, base):
        super(Package, self).__init__(initobject)
        self.base = base
        self._chksum = None
        self._repo = None
        self._size = None

    @property
    def chksum(self):
        if self._chksum:
            return self._chksum
        if self.from_cmdline:
            chksum_type = dnf.yum.misc.get_default_chksum_type()
            chksum_val = dnf.yum.misc.checksum(chksum_type, self.location)
            return (hawkey.chksum_type(chksum_type),
                    binascii.unhexlify(chksum_val))
        return super(Package, self).chksum

    @chksum.setter
    def chksum(self, val):
        self._chksum = val

    @property
    def from_cmdline(self):
        return self.reponame == hawkey.CMDLINE_REPO_NAME

    @property
    def from_system(self):
        return self.reponame == hawkey.SYSTEM_REPO_NAME

    @property
    def header(self):
        rpmfile = self.localPkg()
        if not os.path.exists(rpmfile):
            raise RuntimeError("Rpm file does not exist: '%s'", rpmfile)
        return dnf.rpmUtils.miscutils.headerFromFilename(rpmfile)

    @property
    def size(self):
        if self._size:
            return self._size
        return super(Package, self).size

    @size.setter
    def size(self, val):
        self._size = val

    @property
    def pkgid(self):
        try:
            (_, chksum) = self.hdr_chksum
            return binascii.hexlify(chksum)
        except AttributeError:
            return None

    @property # yum compatibility attribute
    def idx(self):
        """ Always type it to int, rpm bindings expect it like that. """
        return int(self.rpmdbid)

    @property # yum compatibility attribute
    def repoid(self):
        return self.reponame

    @property # yum compatibility attribute
    def pkgtup(self):
        return (self.name, self.arch, str(self.e), self.v, self.r)

    @property # yum compatibility attribute
    def repo(self):
        if self._repo:
            return self._repo
        return self.base.repos[self.reponame]

    @repo.setter
    def repo(self, val):
        self._repo = val

    @property # yum compatibility attribute
    def relativepath(self):
        return self.location

    @property # yum compatibility attribute
    def a(self):
        return self.arch

    @property # yum compatibility attribute
    def e(self):
        return self.epoch

    @property # yum compatibility attribute
    def v(self):
        return self.version

    @property # yum compatibility attribute
    def r(self):
        return self.release

    @property # yum compatibility attribute
    def ui_from_repo(self):
        return self.reponame

    # yum compatibility method
    def evr_eq(self, pkg):
        return self.evr_cmp(pkg) == 0

    # yum compatibility method
    def evr_gt(self, pkg):
        return self.evr_cmp(pkg) > 0

    # yum compatibility method
    def evr_lt(self, pkg):
        return self.evr_cmp(pkg) < 0

    # yum compatibility method
    def getDiscNum(self):
        return self.medianr

    # yum compatibility method
    def localPkg(self):
        """ Package's location in the filesystem.

            For packages in remote repo returns where the package will be/has
            been downloaded.
        """
        if self.from_cmdline:
            return self.location

        loc = self.location
        if not self.repo.local:
            loc = os.path.basename(loc)
        return os.path.join(self.repo.pkgdir, loc)

    # yum compatibility method
    def returnIdSum(self):
        """ Return the chksum type and chksum string how the legacy yum expects
            it.
        """
        (chksum_type, chksum) = self.chksum
        return (hawkey.chksum_name(chksum_type), binascii.hexlify(chksum).decode())

    # yum compatibility method
    def verifyLocalPkg(self):
        if self.from_system:
            raise ValueError("Can not verify an installed package.")
        if self.from_cmdline:
            return True # local package always verifies against itself
        (chksum_type, chksum) = self.returnIdSum()
        real_sum = dnf.yum.misc.checksum(chksum_type, self.localPkg(),
                                         datasize=self.size)
        if real_sum != chksum:
            logger.debug('%s: %s check failed: %s vs %s' %
                         (self, chksum_type, real_sum, chksum))
            return False
        return True

########NEW FILE########
__FILENAME__ = persistor
# persistor.py
# Persistence data container.
#
# Copyright (C) 2013-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

# The current implementation is storing to files in persistdir. Do not depend on
# specific files existing, instead use the persistor API. The underlying
# implementation can change, e.g. for one general file with a serialized dict of
# data etc.

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.i18n import _

import collections
import dbm
import dnf.util
import errno
import hashlib
import json
import logging
import os

logger = logging.getLogger("dnf")

def _clone_dct(dct):
    cln = {}
    for (k, v) in dct.items():
        if isinstance(v, list):
            cln[k] = v[:]
        elif isinstance(v, dict):
            cln[k] = _clone_dct(v)
        else:
            cln[k] = v
    return cln

class ClonableDict(collections.MutableMapping):
    """A dict with list values that can be cloned.

    This wraps around an ordinary dict (which only gives a shallow copy).

    """

    def __init__(self, dct):
        self.dct = dct

    def __delitem__(self, key):
        del self.dct[key]

    def __getitem__(self, key):
        return self.dct[key]

    def __iter__(self):
        return iter(self.dct)

    def __len__(self):
        return len(self.dct)

    def __setitem__(self, key, val):
        self.dct[key] = val

    @classmethod
    def wrap_dict(cls, dct):
        groups = cls(dct)
        return groups

    def clone(self):
        cls = self.__class__
        return cls.wrap_dict(_clone_dct(self.dct))


class _PersistMember(object):
    DEFAULTS = ClonableDict({
        'full_list' : [],
        'grp_types' : 0,
        'pkg_exclude' : [],
        'pkg_types' : 0,
    })

    @staticmethod
    def default():
        return _PersistMember.DEFAULTS.clone().dct

    def __init__(self, param_dct):
        self.param_dct = param_dct

    @property
    def pkg_exclude(self):
        return self.param_dct['pkg_exclude']

    @property
    def full_list(self):
        return self.param_dct['full_list']

    @property
    def installed(self):
        return self.grp_types | self.pkg_types != 0

    @property
    def grp_types(self):
        return self.param_dct['grp_types']

    @grp_types.setter
    def grp_types(self, val):
        self.param_dct['grp_types'] = val

    @property
    def pkg_types(self):
        return self.param_dct['pkg_types']

    @pkg_types.setter
    def pkg_types(self, val):
        self.param_dct['pkg_types'] = val


class GroupPersistor(object):

    @staticmethod
    def _empty_db():
        return ClonableDict({
            'ENVIRONMENTS' : {},
            'GROUPS' : {},
            'meta' : {'version' : '0.5.0'}
        })

    def __init__(self, persistdir):
        self._commit = False
        self._dbfile = os.path.join(persistdir, 'groups.json')
        self.db = None
        self._original = None
        self._load()
        self._ensure_sanity()

    def _access(self, subdict, id_):
        subdict = self.db[subdict]
        dct = subdict.get(id_)
        if dct is None:
            dct = _PersistMember.default()
            subdict[id_] = dct

        return _PersistMember(dct)

    def _ensure_sanity(self):
        """Make sure the input db is valid."""
        if 'GROUPS' in self.db and 'ENVIRONMENTS' in self.db:
            return
        logger.warning(_('Invalid groups database, clearing.'))
        self.db = self._empty_db()

    def _load(self):
        self.db = self._empty_db()
        try:
            with open(self._dbfile) as db:
                content = db.read()
                self.db = ClonableDict.wrap_dict(json.loads(content))
                self._original = self.db.clone()
                self._migrate()
        except IOError as e:
            if e.errno != errno.ENOENT:
                raise

    def _migrate(self):
        try:
            version = self.db['meta']['version']
        except KeyError:
            msg = _('Unsupported installed groups database found, resetting.')
            logger.warning(msg)
            self.db = self._empty_db()
            version = self.db['meta']['version']
        logger.debug('group persistor md version: %s', version)

    def _prune_db(self):
        for members_dct in (self.db['ENVIRONMENTS'], self.db['GROUPS']):
            del_list = []
            for (id_, memb) in members_dct.items():
                if not _PersistMember(memb).installed:
                    del_list.append(id_)
            for id_ in del_list:
                del members_dct[id_]

    def commit(self):
        self._commit = True

    def environment(self, id_):
        return self._access('ENVIRONMENTS', id_)

    @property
    def environments(self):
        return self.db['ENVIRONMENTS']

    def group(self, id_):
        return self._access('GROUPS', id_)

    @property
    def groups(self):
        return self.db['GROUPS']

    def save(self):
        if not self._commit:
            return False
        self._prune_db()
        if self.db == self._original:
            return False
        logger.debug('group persistor: saving.')
        with open(self._dbfile, 'w') as db:
            json.dump(self.db.dct, db)
        self._commit = False
        return True


class RepoPersistor(object):
    """Persistent data kept for repositories.

    Is arch/releasever specific and stores to cachedir.

    """

    def __init__(self, cachedir):
        self.cachedir = cachedir

    def _check_json_db(self):
        json_path = os.path.join(self.cachedir, "expired_repos.json")
        if not os.path.isfile(json_path):
            # inicialize new db
            dnf.util.ensure_dir(self.cachedir)
            self._write_json_data(json_path, [])

    def _get_expired_from_json(self):
        json_path = os.path.join(self.cachedir, "expired_repos.json")
        f = open(json_path, 'r')
        content = f.read()
        f.close()
        if content == "":
            data = []
            logger.warning(_("%s is empty file"), "expired_repos.json")
            self._write_json_data(json_path, data)
        else:
            data = json.loads(content)
        return set(data)

    def _write_json_data(self, path, expired_repos):
        f = open(path, 'w')
        json.dump(expired_repos, f)
        f.close()

    @property
    def _last_makecache_path(self):
        return os.path.join(self.cachedir, "last_makecache")

    def get_expired_repos(self):
        self._check_json_db()
        return self._get_expired_from_json()

    def reset_last_makecache(self):
        try:
            dnf.util.touch(self._last_makecache_path)
            return True
        except IOError:
            logger.info("Failed storing last makecache time.")
            return False

    def set_expired_repos(self, expired_iterable):
        self._check_json_db()
        json_path = os.path.join(self.cachedir, "expired_repos.json")
        self._write_json_data(json_path, list(set(expired_iterable)))

    def since_last_makecache(self):
        try:
            return int(dnf.util.file_age(self._last_makecache_path))
        except OSError:
            logger.info("Failed determining last makecache time.")
            return None

########NEW FILE########
__FILENAME__ = plugin
# plugin.py
# The interface for building DNF plugins.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from dnf.i18n import _
import dnf.logging
import dnf.pycomp
import dnf.util
import fnmatch
import glob
import importlib
import iniparse.compat
import logging
import operator
import os
import sys

logger = logging.getLogger('dnf')

DYNAMIC_PACKAGE = 'dnf.plugin.dynamic'

class Plugin(object):
    """The base class custom plugins must derive from. #:api"""

    name = '<invalid>'

    @staticmethod
    def read_config(conf, name):
        # :api
        fn = '%s/%s.conf' % (conf.pluginconfpath, name)
        parser = iniparse.compat.ConfigParser()
        parser.read(fn)
        return parser

    def __init__(self, base, cli):
        # :api
        pass

    def config(self):
        # :api
        pass

    def sack(self):
        # :api
        pass

    def transaction(self):
        # :api
        pass

class Plugins(object):
    def __init__(self):
        self.plugin_cls = []
        self.plugins = []

    def _caller(method):
        def fn(self):
            dnf.util.mapall(operator.methodcaller(method), self.plugins)
        return fn

    def load(self, paths, skips):
        """Dynamically load relevant plugin modules."""

        if DYNAMIC_PACKAGE in sys.modules:
            raise RuntimeError("load_plugins() called twice")
        sys.modules[DYNAMIC_PACKAGE] = package = dnf.pycomp.ModuleType(DYNAMIC_PACKAGE)
        package.__path__ = []

        files = iter_py_files(paths, skips)
        import_modules(package, files)
        self.plugin_cls = plugin_classes()[:]
        if len(self.plugin_cls) > 0:
            names = [plugin.name for plugin in self.plugin_cls]
            logger.debug('Loaded plugins: %s', ', '.join(names))

    run_config = _caller('config')

    def run_init(self, base, cli=None):
        for p_cls in self.plugin_cls:
            plugin = p_cls(base, cli)
            self.plugins.append(plugin)

    run_sack = _caller('sack')
    run_transaction = _caller('transaction')

    def unload(self):
        del sys.modules[DYNAMIC_PACKAGE]

def plugin_classes():
    return Plugin.__subclasses__()

def import_modules(package, py_files):
    for fn in py_files:
        path, module = os.path.split(fn)
        package.__path__.append(path)
        (module, ext) = os.path.splitext(module)
        name = '%s.%s' % (package.__name__, module)
        try:
            module = importlib.import_module(name)
        except Exception as e:
            logger.error(_('Failed loading plugin: %s'), module)
            logger.log(dnf.logging.SUBDEBUG, '', exc_info=True)

def iter_py_files(paths, skips):
    for p in paths:
        for fn in glob.glob('%s/*.py' % p):
            (name, _) = os.path.splitext(os.path.basename(fn))
            if any(fnmatch.fnmatch(name, pattern) for pattern in skips):
                 continue
            yield fn

########NEW FILE########
__FILENAME__ = pycomp
# pycomp.py
# Python 2 and Python 3 compatibility module
#
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from gettext import NullTranslations
from sys import version_info

import itertools
import locale
import types

if version_info.major >= 3:
    PY3 = True

    # functions renamed in py3
    basestring = unicode = str
    filterfalse = itertools.filterfalse
    long = int
    NullTranslations.ugettext = NullTranslations.gettext
    NullTranslations.ungettext = NullTranslations.ngettext
    xrange = range
    raw_input = input
    from io import StringIO
    to_ord = lambda i: i

    # string helpers
    def is_py2str_py3bytes(o):
        return isinstance(o, bytes)
    def is_py3bytes(o):
        return isinstance(o, bytes)

    # functions that don't take unicode arguments in py2
    ModuleType = lambda m: types.ModuleType(m)
    def setlocale(category, loc=None):
        locale.setlocale(category, loc)
    def write_to_file(f, content):
        f.write(content)

else:
    PY3 = False

    # functions renamed in py3
    from __builtin__ import unicode, basestring, long, xrange, raw_input
    from StringIO import StringIO
    filterfalse = itertools.ifilterfalse
    to_ord = lambda i: ord(i)

    # string helpers
    def is_py2str_py3bytes(o):
        return isinstance(o, str)
    def is_py3bytes(o):
        return False

    # functions that don't take unicode arguments in py2
    ModuleType = lambda m: types.ModuleType(m.encode('utf-8'))
    def setlocale(category, loc=None):
        locale.setlocale(category, loc.encode('utf-8'))
    def write_to_file(f, content):
        f.write(content.encode('utf-8'))

########NEW FILE########
__FILENAME__ = query
# query.py
# Implements Query.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
import functools
import hawkey
import dnf.exceptions
import dnf.selector
import dnf.util

from dnf.pycomp import basestring

def is_nevra(pattern):
    try:
        hawkey.split_nevra(pattern)
    except hawkey.ValueException:
        return False
    return True

class Query(hawkey.Query):
    # :api
    # :api also includes hawkey.Query.filter

    def available(self):
        # :api
        return self.filter(reponame__neq=hawkey.SYSTEM_REPO_NAME)

    def downgrades(self):
        # :api
        return self.filter(downgrades=True)

    def filter_autoglob(self, *args, **kwargs):
        nargs = {}
        for (key, value) in kwargs.items():
            if dnf.util.is_glob_pattern(value):
                nargs[key + "__glob"] = value
            else:
                nargs[key] = value
        return self.filter(*args, **nargs)

    def installed(self):
        # :api
        return self.filter(reponame=hawkey.SYSTEM_REPO_NAME)

    def latest(self):
        # :api
        return self.filter(latest_per_arch=True)

    def upgrades(self):
        # :api
        return self.filter(upgrades=True)

    def name_dict(self):
        d = {}
        for pkg in self:
            d.setdefault(pkg.name, []).append(pkg)
        return d

    def na_dict(self):
        d = {}
        for pkg in self.run():
            key = (pkg.name, pkg.arch)
            d.setdefault(key, []).append(pkg)
        return d

    def pkgtup_dict(self):
        return per_pkgtup_dict(self.run())

    def nevra(self, *args):
        args_len = len(args)
        if args_len == 3:
            return self.filter(name=args[0], evr=args[1], arch=args[2])
        if args_len == 1:
            nevra = hawkey.split_nevra(args[0])
        elif args_len == 5:
            nevra = args
        else:
            raise TypeError("nevra() takes 1, 3 or 5 str params")
        return self.filter(
            name=nevra.name, epoch=nevra.epoch, version=nevra.version,
            release=nevra.release, arch=nevra.arch)


def by_provides(sack, patterns, ignore_case=False, get_query=False):
    if isinstance(patterns, basestring):
        patterns = [patterns]
    try:
        reldeps = list(map(functools.partial(hawkey.Reldep, sack), patterns))
    except hawkey.ValueException:
        return sack.query().filter(empty=True)
    q = sack.query()
    flags = []
    if ignore_case:
        flags.append(hawkey.ICASE)
    q.filterm(*flags, provides=reldeps)
    if get_query:
        return q
    return q.run()

def per_pkgtup_dict(pkg_list):
    d = {}
    for pkg in pkg_list:
        d.setdefault(pkg.pkgtup, []).append(pkg)
    return d

def per_nevra_dict(pkg_list):
    return {str(pkg):pkg for pkg in pkg_list}

########NEW FILE########
__FILENAME__ = repo
# repo.py
# DNF Repository objects.
#
# Copyright (C) 2013-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.i18n import ucd, _

import dnf.callback
import dnf.const
import dnf.exceptions
import dnf.logging
import dnf.util
import dnf.yum.config
import dnf.yum.misc
import functools
import logging
import librepo
import operator
import os
import shutil
import string
import time
import types

_METADATA_RELATIVE_DIR = "repodata"
_METALINK_FILENAME = "metalink.xml"
_MIRRORLIST_FILENAME = "mirrorlist"
_RECOGNIZED_CHKSUMS = ['sha512', 'sha256']

logger = logging.getLogger("dnf")

def repo_id_invalid(repo_id):
    """Return index of an invalid character in the repo ID (if present). :api"""
    allowed_chars = ''.join((string.ascii_letters, string.digits, '-_.:'))
    invalids = (index for index, char in enumerate(repo_id)
                if char not in allowed_chars)
    return dnf.util.first(invalids)

def _metalink_path(dirname):
    return os.path.join(dirname, _METALINK_FILENAME)

def _mirrorlist_path(dirname):
    return os.path.join(dirname, _MIRRORLIST_FILENAME)

def _subst2tuples(subst_dct):
    return [(k, v) for (k, v) in subst_dct.items()]

def pkg2payload(pkg, progress, *factories):
    for fn in factories:
        pload = fn(pkg, progress)
        if pload is not None:
            return pload
    raise ValueError('no matching payload factory for %s' % pkg)

class _DownloadErrors(object):
    def __init__(self):
        self.fatal = None
        self._irrecoverable = {}
        self._recoverable = {}

    @property
    def irrecoverable(self):
        if self._irrecoverable:
            return self._irrecoverable
        if self.fatal:
            return {'': [self.fatal]}
        return {}

    @property
    def recoverable(self):
        return self._recoverable

    @recoverable.setter
    def recoverable(self, new_dct):
        self._recoverable = new_dct

def download_payloads(payloads, drpm):
    # download packages
    drpm.err.clear()
    targets = [pload.librepo_target() for pload in payloads]
    errs = _DownloadErrors()
    try:
        librepo.download_packages(targets, failfast=True)
    except librepo.LibrepoException as e:
        errs.fatal = e.args[1] or '<unspecified librepo error>'
    drpm.wait()

    # process downloading errors
    errs.recoverable = drpm.err.copy()
    for tgt in targets:
        err = tgt.err
        if err is None:
            continue
        if err == 'Already downloaded' or err.startswith('Not finished'):
            continue
        payload = tgt.cbdata
        pkg = payload.pkg
        errs.irrecoverable[pkg] = [err]

    return errs

def update_saving(saving, payloads, errs):
    real, full = saving
    for pload in payloads:
        pkg = pload.pkg
        if pkg in errs:
            real += pload.download_size
            continue
        real += pload.download_size
        full += pload.full_size
    return real, full

class _Handle(librepo.Handle):
    def __init__(self, gpgcheck, max_mirror_tries):
        super(_Handle, self).__init__()
        self.gpgcheck = gpgcheck
        self.maxmirrortries = max_mirror_tries
        self.interruptible = True
        self.repotype = librepo.LR_YUMREPO
        self.useragent = dnf.const.USER_AGENT
        self.yumdlist = ["primary", "filelists", "prestodelta", "group_gz"]

    def __str__(self):
        return '_Handle: metalnk: %s, mlist: %s, urls %s.' % \
            (self.metalinkurl, self.mirrorlisturl, self.urls)

    @classmethod
    def new_local(cls, subst_dct, gpgcheck, max_mirror_tries, cachedir):
        h = cls(gpgcheck, max_mirror_tries)
        h.varsub = _subst2tuples(subst_dct)
        h.destdir = cachedir
        h.urls = [cachedir]
        h.local = True
        if os.access(h.metalink_path, os.R_OK):
            h.mirrorlist = h.metalink_path
        elif os.access(h.mirrorlist_path, os.R_OK):
            h.mirrorlist = h.mirrorlist_path
        return h

    @property
    def metadata_dir(self):
        return os.path.join(self.destdir, _METADATA_RELATIVE_DIR)

    @property
    def metalink_path(self):
        return _metalink_path(self.destdir)

    @property
    def mirrorlist_path(self):
        return _mirrorlist_path(self.destdir)

class Metadata(object):
    def __init__(self, res, handle):
        self.expired = False
        self.fresh = False # :api
        self.repo_dct = res.yum_repo
        self.repomd_dct = res.yum_repomd
        self._mirrors = handle.mirrors[:]

    @property
    def age(self):
        return self.file_age('primary')

    @property
    def comps_fn(self):
        return self.repo_dct.get("group_gz") or self.repo_dct.get("group")

    @property
    def content_tags(self):
        return self.repomd_dct.get('content_tags')

    @property
    def distro_tags(self):
        pairs = self.repomd_dct.get('distro_tags', [])
        return {k:v for (k, v) in pairs}

    def file_age(self, what):
        return time.time() - self.file_timestamp(what)

    def file_timestamp(self, what):
        try:
            return dnf.util.file_timestamp(self.repo_dct[what])
        except OSError as e:
            raise dnf.exceptions.MetadataError(ucd(e))

    @property
    def filelists_fn(self):
        return self.repo_dct.get('filelists')

    @property
    def mirrors(self):
        return self._mirrors

    @property
    def md_timestamp(self):
        """Gets the highest timestamp of all metadata types."""
        timestamps = [content.get('timestamp')
                      for (_, content) in self.repomd_dct.items()
                      if isinstance(content, dict)]
        return max(timestamps)

    @property
    def presto_fn(self):
        return self.repo_dct.get('prestodelta')

    @property
    def primary_fn(self):
        return self.repo_dct.get('primary')

    def reset_age(self):
        dnf.util.touch(self.primary_fn, no_create=True)

    @property
    def repomd_fn(self):
        return self.repo_dct.get('repomd')

    @property
    def revision(self):
        return self.repomd_dct.get('revision')

    @property
    def timestamp(self):
        return self.file_timestamp('primary')

class PackagePayload(dnf.callback.Payload):
    def __init__(self, pkg, progress):
        super(PackagePayload, self).__init__(progress)
        self.pkg = pkg

    @dnf.util.log_method_call(functools.partial(logger.log, dnf.logging.SUBDEBUG))
    def _end_cb(self, cbdata, lr_status, msg):
        """End callback to librepo operation."""
        status = dnf.callback.STATUS_FAILED
        if msg is None:
            status = dnf.callback.STATUS_OK
        elif msg.startswith('Not finished'):
            return
        elif lr_status == librepo.TRANSFER_ALREADYEXISTS:
            status = dnf.callback.STATUS_ALREADY_EXISTS

        self.progress.end(self, status, msg)

    @dnf.util.log_method_call(functools.partial(logger.log, dnf.logging.SUBDEBUG))
    def _mirrorfail_cb(self, cbdata, err, url):
        self.progress.end(self, dnf.callback.STATUS_MIRROR, err)

    def _progress_cb(self, cbdata, total, done):
        self.progress.progress(self, done)

    @property
    def error(self):
        """Error obtaining the Payload."""
        pass

    @property
    def full_size(self):
        return self.download_size

    def librepo_target(self):
        pkg = self.pkg
        pkgdir = pkg.repo.pkgdir
        dnf.util.ensure_dir(pkgdir)

        target_dct = {
            'handle' : pkg.repo.get_handle(),
            'dest' : pkgdir,
            'resume' : True,
            'cbdata' : self,
            'progresscb' : self._progress_cb,
            'endcb' : self._end_cb,
            'mirrorfailurecb' : self._mirrorfail_cb,
        }
        target_dct.update(self._target_params())

        return librepo.PackageTarget(**target_dct)


class RPMPayload(PackagePayload):

    def __str__(self):
        return os.path.basename(self.pkg.location)

    def _target_params(self):
        pkg = self.pkg
        ctype, csum = pkg.returnIdSum()
        ctype_code = getattr(librepo, ctype.upper(), librepo.CHECKSUM_UNKNOWN)
        if ctype_code == librepo.CHECKSUM_UNKNOWN:
            logger.warn(_("unsupported checksum type: %s"), ctype)

        return {
            'relative_url' : pkg.location,
            'checksum_type' : ctype_code,
            'checksum' : csum,
            'expectedsize' : pkg.downloadsize,
            'base_url' : pkg.baseurl,
        }

    @property
    def download_size(self):
        """Total size of the download."""
        return self.pkg.downloadsize


class MDPayload(dnf.callback.Payload):

    def __str__(self):
        return self._text

    def _progress_cb(self, cbdata, total, done):
        self._download_size = total
        self.progress.progress(self, done)

    def _fastestmirror_cb(self, cbdata, stage, data):
        if stage == librepo.FMSTAGE_DETECTION:
            # pinging mirrors, this might take a while
            msg = 'determining the fastest mirror (%d hosts).. ' % data
            self.fm_running = True
        elif stage == librepo.FMSTAGE_STATUS and self.fm_running:
            # done.. report but ignore any errors
            msg = 'error: %s\n' % data if data else 'done.\n'
        else:
            return
        self.progress.message(msg)

    @property
    def download_size(self):
        return self._download_size

    @property
    def progress(self):
        return self._progress

    @progress.setter
    def progress(self, progress):
        if progress is None:
            progress = dnf.callback.NullDownloadProgress()
        self._progress = progress

    def start(self, text):
        self._text = text
        self._download_size = 0
        self.progress.start(1, 1)

    def end(self):
        self._download_size = 0
        self.progress.end(self, None, None)

SYNC_TRY_CACHE = 1
SYNC_EXPIRED = 2    # consider the current cache expired, no matter its real age
SYNC_ONLY_CACHE = 3 # use the local cache, even if it's expired, never download.

class Repo(dnf.yum.config.RepoConf):
    # :api
    DEFAULT_SYNC = SYNC_TRY_CACHE

    def __init__(self, id_, basecachedir):
        # :api
        super(Repo, self).__init__()
        self._pkgdir = None
        self._md_pload = MDPayload(dnf.callback.NullDownloadProgress())
        self.id = id_ # :api
        self.basecachedir = basecachedir
        self.metadata = None # :api
        self.sync_strategy = self.DEFAULT_SYNC
        self.yumvar = {} # empty dict of yumvariables for $string replacement
        self.max_mirror_tries = 0 # try them all
        self._handle = None

    def __repr__(self):
        return "<%s %s>" % (self.__class__.__name__, self.id)

    def _exc2msg(self, librepo_exception):
        exc_msg = librepo_exception.args[1]
        msg = _("Failed to synchronize cache for repo '%s': %s") % \
              (self.id, exc_msg)
        return msg

    def _handle_load(self, handle):
        if handle.progresscb:
            self._md_pload.start(self.name)
        result = handle.perform()
        if handle.progresscb:
            self._md_pload.end()
        return Metadata(result, handle)

    def _handle_new_local(self, destdir):
        return _Handle.new_local(self.yumvar, self.repo_gpgcheck,
                                 self.max_mirror_tries, destdir)

    def _handle_new_remote(self, destdir, mirror_setup=True):
        h = _Handle(self.repo_gpgcheck, self.max_mirror_tries)
        h.varsub = _subst2tuples(self.yumvar)
        h.destdir = destdir

        # setup mirror URLs
        mirrorlist = self.metalink or self.mirrorlist
        if mirrorlist:
            if mirror_setup:
                h.setopt(librepo.LRO_MIRRORLIST, mirrorlist)
                h.setopt(librepo.LRO_FASTESTMIRROR, self.fastestmirror)
                h.setopt(librepo.LRO_FASTESTMIRRORCACHE,
                         os.path.join(self.basecachedir, 'fastestmirror.cache'))
            else:
                # use already resolved mirror list
                h.setopt(librepo.LRO_URLS, self.metadata.mirrors)
        elif self.baseurl:
            h.setopt(librepo.LRO_URLS, self.baseurl)
        else:
            msg = 'Cannot find a valid baseurl for repo: %s' % self.id
            raise dnf.exceptions.RepoError(msg)

        # setup download progress
        h.progresscb = self._md_pload._progress_cb
        self._md_pload.fm_running = False
        h.fastestmirrorcb = self._md_pload._fastestmirror_cb

        # apply repo options
        h.maxspeed = self.throttle if type(self.throttle) is int \
                     else int(self.bandwidth * self.throttle)
        h.proxy = self.proxy
        h.sslverifypeer = h.sslverifyhost = self.sslverify

        return h

    def _handle_new_pkg_download(self):
        return self._handle_new_remote(self.pkgdir, mirror_setup=False)

    def get_handle(self):
        """Returns a librepo handle, set as per the repo options

        Note that destdir is None, and the handle is cached.
        """
        if not self._handle:
            self._handle = self._handle_new_remote(None)
        return self._handle

    @property
    def local(self):
        if self.metalink or self.mirrorlist:
            return False
        if self.baseurl[0].startswith('file://'):
            return True
        return False

    def _replace_metadata(self, handle):
        dnf.util.ensure_dir(self.cachedir)
        dnf.util.rm_rf(self.metadata_dir)
        dnf.util.rm_rf(self.metalink_path)
        dnf.util.rm_rf(self.mirrorlist_path)
        shutil.move(handle.metadata_dir, self.metadata_dir)
        if handle.metalink:
            shutil.move(handle.metalink_path, self.metalink_path)
        elif handle.mirrorlist:
            shutil.move(handle.mirrorlist_path, self.mirrorlist_path)

    def _reset_metadata_expired(self):
        self.metadata.expired = self.metadata.age >= self.metadata_expire
        if self.metadata_expire == -1:
            self.metadata.expired = False

    def _try_cache(self):
        """Tries to load metadata from the local cache.

        Correctly sets self.metadata.expired.

        Returns True if we got any (even expired) metadata locally.

        """
        assert self.metadata is None
        handle = self._handle_new_local(self.cachedir)
        try:
            self.metadata = self._handle_load(handle)
        except (librepo.LibrepoException, IOError):
            return False
        if self.sync_strategy == SYNC_EXPIRED:
            # we shouldn't exit earlier as reviving needs self.metadata
            self.metadata.expired = True
            return False

        self._reset_metadata_expired()
        return True

    def _try_revive(self):
        """Use metalink to check whether our metadata are still current."""
        if not self.metadata:
            return False
        if not self.metalink:
            return False
        repomd_fn = self.metadata.repo_dct['repomd']
        with dnf.util.tmpdir() as tmpdir, open(repomd_fn) as repomd:
            handle = self._handle_new_remote(tmpdir)
            handle.fetchmirrors = True
            handle.perform()
            if handle.metalink is None:
                logger.debug("reviving: repo '%s' skipped, no metalink.", self.id)
                return False
            hashes = handle.metalink['hashes']
            hashes = [hsh_val for hsh_val in hashes
                      if hsh_val[0] in _RECOGNIZED_CHKSUMS]
            if len(hashes) < 1:
                logger.debug("reviving: repo '%s' skipped, no usable hash.",
                             self.id)
                return False
            algos = list(map(operator.itemgetter(0), hashes))
            chksums = dnf.yum.misc.Checksums(algos,
                                             ignore_missing=True,
                                             ignore_none=True)
            chksums.read(repomd, -1)
            digests = chksums.hexdigests()
            for (algo, digest) in hashes:
                if digests[algo] != digest:
                    logger.debug("reviving: failed for '%s', mismatched %s sum.",
                                 self.id, algo)
                    return False
        logger.debug("reviving: '%s' can be revived.", self.id)
        return True

    @property
    def cachedir(self):
        return os.path.join(self.basecachedir, self.id)

    _REPOCONF_ATTRS = set(dir(dnf.yum.config.RepoConf))
    def dump(self):
        """Return a string representing configuration of this repo."""
        output = '[%s]\n' % self.id
        for attr in dir(self):
            # exclude all vars which are not opts
            if attr not in self._REPOCONF_ATTRS:
                continue
            if attr.startswith('_'):
                continue

            res = getattr(self, attr)
            if isinstance(res, types.MethodType):
                continue
            if not res and type(res) not in (type(False), type(0)):
                res = ''
            if isinstance(res, list):
                res = ',\n   '.join(res)
            output = output + '%s = %s\n' % (attr, res)

        return output

    def disable(self):
        # :api
        self.enabled = False

    def enable(self):
        # :api
        self.enabled = True

    @property
    def filelists_fn(self):
        return self.metadata.filelists_fn

    def load(self):
        """Load the metadata for this repo. :api

        Depending on the configuration and the age and consistence of data
        available on the disk cache, either loads the metadata from the cache or
        downloads them from the mirror, baseurl or metalink.

        This method will by default not try to refresh already loaded data if
        called repeatedly.

        Returns True if this call to load() caused a fresh metadata download.

        """
        if self.metadata or self._try_cache():
            if self.sync_strategy == SYNC_ONLY_CACHE or not self.metadata.expired:
                logger.debug('repo: using cache for: %s', self.id)
                return False
        if self.sync_strategy == SYNC_ONLY_CACHE:
            msg = "Cache-only enabled but no cache for '%s'" % self.id
            raise dnf.exceptions.RepoError(msg)
        try:
            if self._try_revive():
                # the expired metadata still reflect the origin:
                self.metadata.reset_age()
                self.sync_strategy = SYNC_TRY_CACHE
                self.metadata.expired = False
                return True

            with dnf.util.tmpdir() as tmpdir:
                handle = self._handle_new_remote(tmpdir)
                msg = 'repo: downloading from remote: %s, %s'
                logger.log(dnf.logging.SUBDEBUG, msg, self.id, handle)
                self._handle_load(handle)
                # override old md with the new ones:
                self._replace_metadata(handle)

            # get md from the cache now:
            handle = self._handle_new_local(self.cachedir)
            self.metadata = self._handle_load(handle)
            self.metadata.fresh = True
        except librepo.LibrepoException as e:
            self.metadata = None
            raise dnf.exceptions.RepoError(self._exc2msg(e))
        self.sync_strategy = SYNC_TRY_CACHE
        return True

    @property
    def metadata_dir(self):
        return os.path.join(self.cachedir, _METADATA_RELATIVE_DIR)

    @property
    def metalink_path(self):
        return _metalink_path(self.cachedir)

    @property
    def mirrorlist_path(self):
        return _mirrorlist_path(self.cachedir)

    def metadata_expire_in(self):
        """Get the number of seconds after which the cached metadata will expire.

        Returns a tuple, boolean whether there even is cached metadata and the
        number of seconds it will expire in. Negative number means the metadata
        has expired already, None that it never expires.

        """
        if not self.metadata:
            self._try_cache()
        if self.metadata:
            if self.metadata_expire == -1:
                return True, None
            expiration = self.metadata_expire - self.metadata.age
            if self.metadata.expired:
                expiration = min(0, expiration)
            return True, expiration
        return False, 0

    def md_expire_cache(self):
        """Mark whatever is in the current cache expired.

        This repo instance will alway try to fetch a fresh metadata after this
        method is called.

        """
        if self.metadata:
            self.metadata.expired = True
        self.sync_strategy = SYNC_EXPIRED

    def md_try_cache(self):
        """Use cache for metadata if possible, sync otherwise."""
        self.sync_strategy = SYNC_TRY_CACHE

    @property
    def md_only_cached(self):
        return self.sync_strategy == SYNC_ONLY_CACHE

    @md_only_cached.setter
    def md_only_cached(self, val):
        """Force using only the metadata the repo has in the local cache."""
        if val:
            self.sync_strategy = SYNC_ONLY_CACHE
        else:
            self.sync_strategy = SYNC_TRY_CACHE

    @property
    def presto_fn(self):
        return self.metadata.presto_fn

    @property
    def pkgdir(self):
        # :api
        if self.local:
            return dnf.util.strip_prefix(self.baseurl[0], 'file://')
        if self._pkgdir is not None:
            return self._pkgdir
        return os.path.join(self.cachedir, 'packages')

    @pkgdir.setter
    def pkgdir(self, val):
        # :api
        self._pkgdir = val

    @property
    def primary_fn(self):
        return self.metadata.primary_fn

    @property
    def repomd_fn(self):
        return self.metadata.repomd_fn

    def set_progress_bar(self, progress):
        # :api
        self._md_pload.progress = progress

    def valid(self):
        if len(self.baseurl) == 0 and not self.metalink and not self.mirrorlist:
            return "Repository %s has no mirror or baseurl set." % self.id
        return None

########NEW FILE########
__FILENAME__ = repodict
# repodict.py
# Managing repo configuration in DNF.
#
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
import fnmatch
import dnf.util
from dnf.exceptions import ConfigError, RepoError

class MultiCallList(list):
    def __init__(self, iterable):
        self.extend(iterable)

    def __getattr__(self, what):
        def fn(*args, **kwargs):
            def call_what(v):
                method = getattr(v, what)
                return method(*args, **kwargs)
            return list(map(call_what, self))
        return fn

    def __setattr__(self, what, val):
        def setter(item):
            setattr(item, what, val)
        return list(map(setter, self))

class RepoDict(dict):
    # :api
    def add(self, repo):
        # :api
        id_ = repo.id
        if id_ in self:
            msg = 'Repository %s is listed more than once in the configuration'
            raise ConfigError(msg % id_)
        msg = repo.valid()
        if msg:
            raise ConfigError(msg)
        self[id_] = repo

    def all(self):
        # :api
        return MultiCallList(self.values())

    def any_enabled(self):
        return not dnf.util.empty(self.iter_enabled())

    def enabled(self):
        return [r for r in self.values() if r.enabled]

    def get_matching(self, key):
        # :api
        if dnf.util.is_glob_pattern(key):
            l = [self[k] for k in self if fnmatch.fnmatch(k, key)]
            return MultiCallList(l)
        repo = self.get(key, None)
        if repo is None:
            return MultiCallList([])
        return MultiCallList([repo])

    def iter_enabled(self):
        # :api
        return (r for r in self.values() if r.enabled)

########NEW FILE########
__FILENAME__ = connection
# connection.py
# Maintain RPMDB connections.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals

from .transaction import initReadOnlyTransaction
import dnf.util

class RpmConnection(object):
    def __init__(self, root):
        self.root = root

    @property
    @dnf.util.lazyattr("_readonly_ts")
    def readonly_ts(self):
        return initReadOnlyTransaction(self.root)

########NEW FILE########
__FILENAME__ = error
# error.py
# RpmUtilsError
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

class RpmUtilsError(Exception):
    pass

########NEW FILE########
__FILENAME__ = miscutils
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
# Copyright 2003 Duke University

from __future__ import print_function, absolute_import
from __future__ import unicode_literals
import dnf.pycomp
import rpm
import gzip
import os
import sys
import locale
import signal

from .error import RpmUtilsError
from . import transaction

def compareEVR(first, second):
    # return 1: a is newer than b
    # 0: a and b are the same version
    # -1: b is newer than a
    (e1, v1, r1) = first
    (e2, v2, r2) = second
    if e1 is None:
        e1 = '0'
    else:
        e1 = str(e1)
    v1 = str(v1)
    r1 = str(r1)
    if e2 is None:
        e2 = '0'
    else:
        e2 = str(e2)
    v2 = str(v2)
    r2 = str(r2)
    #print('%s, %s, %s vs %s, %s, %s' % (e1, v1, r1, e2, v2, r2))
    rc = rpm.labelCompare((e1, v1, r1), (e2, v2, r2))
    #print('%s, %s, %s vs %s, %s, %s = %s' % (e1, v1, r1, e2, v2, r2, rc))
    return rc

def checkSig(ts, package):
    """Takes a transaction set and a package, check it's sigs,
    return 0 if they are all fine
    return 1 if the gpg key can't be found
    return 2 if the header is in someway damaged
    return 3 if the key is not trusted
    return 4 if the pkg is not gpg or pgp signed"""

    value = 0
    currentflags = ts.setVSFlags(0)
    fdno = os.open(package, os.O_RDONLY)
    try:
        hdr = ts.hdrFromFdno(fdno)
    except rpm.error as e:
        if str(e) == "public key not availaiable":
            value = 1
        if str(e) == "public key not available":
            value = 1
        if str(e) == "public key not trusted":
            value = 3
        if str(e) == "error reading package header":
            value = 2
    else:
        error, siginfo = getSigInfo(hdr)
        if error == 101:
            os.close(fdno)
            del hdr
            value = 4
        else:
            del hdr

    try:
        os.close(fdno)
    except OSError as e: # if we're not opened, don't scream about it
        pass

    ts.setVSFlags(currentflags) # put things back like they were before
    return value

def getSigInfo(hdr):
    """checks signature from an hdr hand back signature information and/or
       an error code"""

    dnf.pycomp.setlocale(locale.LC_ALL, 'C')
    string = '%|DSAHEADER?{%{DSAHEADER:pgpsig}}:{%|RSAHEADER?{%{RSAHEADER:pgpsig}}:{%|SIGGPG?{%{SIGGPG:pgpsig}}:{%|SIGPGP?{%{SIGPGP:pgpsig}}:{(none)}|}|}|}|'
    siginfo = hdr.sprintf(string)
    if siginfo != '(none)':
        error = 0
        sigtype, sigdate, sigid = siginfo.split(',')
    else:
        error = 101
        sigtype = 'MD5'
        sigdate = 'None'
        sigid = 'None'

    infotuple = (sigtype, sigdate, sigid)
    return error, infotuple

###########
# Title: Remove duplicates from a sequence
# Submitter: Tim Peters
# From: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/52560

def unique(s):
    """Return a list of the elements in s, but without duplicates.

    For example, unique([1,2,3,1,2,3]) is some permutation of [1,2,3],
    unique("abcabc") some permutation of ["a", "b", "c"], and
    unique(([1, 2], [2, 3], [1, 2])) some permutation of
    [[2, 3], [1, 2]].

    For best speed, all sequence elements should be hashable.  Then
    unique() will usually work in linear time.

    If not possible, the sequence elements should enjoy a total
    ordering, and if list(s).sort() doesn't raise TypeError it's
    assumed that they do enjoy a total ordering.  Then unique() will
    usually work in O(N*log2(N)) time.

    If that's not possible either, the sequence elements must support
    equality-testing.  Then unique() will usually work in quadratic
    time.
    """

    n = len(s)
    if n == 0:
        return []

    # Try using a dict first, as that's the fastest and will usually
    # work.  If it doesn't work, it will usually fail quickly, so it
    # usually doesn't cost much to *try* it.  It requires that all the
    # sequence elements be hashable, and support equality comparison.
    u = {}
    try:
        for x in s:
            u[x] = 1
    except TypeError:
        del u  # move on to the next method
    else:
        return list(u.keys())

    # We can't hash all the elements.  Second fastest is to sort,
    # which brings the equal elements together; then duplicates are
    # easy to weed out in a single pass.
    # NOTE:  Python's list.sort() was designed to be efficient in the
    # presence of many duplicate elements.  This isn't true of all
    # sort functions in all languages or libraries, so this approach
    # is more effective in Python than it may be elsewhere.
    try:
        t = list(s)
        t.sort()
    except TypeError:
        del t  # move on to the next method
    else:
        assert n > 0
        last = t[0]
        lasti = i = 1
        while i < n:
            if t[i] != last:
                t[lasti] = last = t[i]
                lasti += 1
            i += 1
        return t[:lasti]

    # Brute force is all that's left.
    u = []
    for x in s:
        if x not in u:
            u.append(x)
    return u


def splitFilename(filename):
    """
    Pass in a standard style rpm fullname

    Return a name, version, release, epoch, arch, e.g.::
        foo-1.0-1.i386.rpm returns foo, 1.0, 1, i386
        1:bar-9-123a.ia64.rpm returns bar, 9, 123a, 1, ia64
    """

    if filename[-4:] == '.rpm':
        filename = filename[:-4]

    archIndex = filename.rfind('.')
    arch = filename[archIndex+1:]

    relIndex = filename[:archIndex].rfind('-')
    rel = filename[relIndex+1:archIndex]

    verIndex = filename[:relIndex].rfind('-')
    ver = filename[verIndex+1:relIndex]

    epochIndex = filename.find(':')
    if epochIndex == -1:
        epoch = ''
    else:
        epoch = filename[:epochIndex]

    name = filename[epochIndex + 1:verIndex]
    return name, ver, rel, epoch, arch


def rpm2cpio(fdno, out=sys.stdout, bufsize=2048):
    """Performs roughly the equivalent of rpm2cpio(8).
       Reads the package from fdno, and dumps the cpio payload to out,
       using bufsize as the buffer size."""
    ts = transaction.initReadOnlyTransaction()
    hdr = ts.hdrFromFdno(fdno)
    del ts

    compr = hdr[rpm.RPMTAG_PAYLOADCOMPRESSOR] or 'gzip'
    #XXX FIXME
    #if compr == 'bzip2':
        # TODO: someone implement me!
    #el
    if compr != 'gzip':
        raise RpmUtilsError('Unsupported payload compressor: "%s"' % compr)
    f = gzip.GzipFile(None, 'rb', None, os.fdopen(fdno, 'rb', bufsize))
    while 1:
        tmp = f.read(bufsize)
        if tmp == "": break
        out.write(tmp)
    f.close()

def hdrFromPackage(ts, package):
    """hand back the rpm header or raise an Error if the pkg is fubar"""
    try:
        fdno = os.open(package, os.O_RDONLY)
    except OSError as e:
        raise RpmUtilsError('Unable to open file')

    # XXX: We should start a readonly ts here, so we don't get the options
    # from the other one (sig checking, etc)
    try:
        hdr = ts.hdrFromFdno(fdno)
    except rpm.error as e:
        os.close(fdno)
        msg = "RPM error opening package '%s': %s"
        raise RpmUtilsError(msg % (package, str(e)))
    if type(hdr) != rpm.hdr:
        os.close(fdno)
        raise RpmUtilsError("RPM Error opening Package (type)")

    os.close(fdno)
    return hdr

def headerFromFilename(filename):
    ts = transaction.initReadOnlyTransaction()
    hdr = hdrFromPackage(ts, filename)
    return hdr

def checkSignals():
    if hasattr(rpm, "checkSignals") and hasattr(rpm, 'signalsCaught'):
        if rpm.signalsCaught([signal.SIGINT,
                              signal.SIGTERM,
                              signal.SIGPIPE,
                              signal.SIGQUIT,
                              signal.SIGHUP]):
            sys.exit(1)


########NEW FILE########
__FILENAME__ = transaction
#
# Client code for Update Agent
# Copyright (c) 1999-2002 Red Hat, Inc.  Distributed under GPL.
#
#         Adrian Likins <alikins@redhat.com>
# Some Edits by Seth Vidal <skvidal@phy.duke.edu>
#
# a couple of classes wrapping up transactions so that we
#    can share transactions instead of creating new ones all over
#

from __future__ import absolute_import
from __future__ import unicode_literals
import rpm

read_ts = None
ts = None

# wrapper/proxy class for rpm.Transaction so we can
# instrument it, etc easily
class TransactionWrapper(object):
    def __init__(self, root='/'):
        self.ts = rpm.TransactionSet(root)
        self._methods = ['check',
                         'order',
                         'addErase',
                         'addInstall',
                         'run',
                         'pgpImportPubkey',
                         'pgpPrtPkts',
                         'problems',
                         'setFlags',
                         'setVSFlags',
                         'setProbFilter',
                         'hdrFromFdno',
                         'next',
                         'clean']
        self.tsflags = []
        self.open = True

    def __del__(self):
        # Automatically close the rpm transaction when the reference is lost
        self.close()

    def close(self):
        if self.open:
            self.ts.closeDB()
            self.ts = None
            self.open = False

    def dbMatch(self, *args, **kwds):
        if 'patterns' in kwds:
            patterns = kwds.pop('patterns')
        else:
            patterns = []

        mi = self.ts.dbMatch(*args, **kwds)
        for (tag, tp, pat) in patterns:
            mi.pattern(tag, tp, pat)
        return mi

    def __getattr__(self, attr):
        if attr in self._methods:
            return self.getMethod(attr)
        else:
            raise AttributeError(attr)

    def __iter__(self):
        return self.ts

    def getMethod(self, method):
        # in theory, we can override this with
        # profile/etc info
        return getattr(self.ts, method)

    # push/pop methods so we dont lose the previous
    # set value, and we can potentiall debug a bit
    # easier
    def pushVSFlags(self, flags):
        self.tsflags.append(flags)
        self.ts.setVSFlags(self.tsflags[-1])

    def addTsFlag(self, flag):
        curflags = self.ts.setFlags(0)
        self.ts.setFlags(curflags | flag)

    def getTsFlags(self):
        curflags = self.ts.setFlags(0)
        self.ts.setFlags(curflags)
        return curflags

    def isTsFlagSet(self, flag):
        val = self.getTsFlags()
        return bool(flag & val)

    def setScriptFd(self, fd):
        self.ts.scriptFd = fd.fileno()

    def test(self, cb, conf={}):
        """tests the ts we've setup, takes a callback function and a conf dict
           for flags and what not"""

        origflags = self.getTsFlags()
        self.addTsFlag(rpm.RPMTRANS_FLAG_TEST)
        # FIXME GARBAGE - remove once this is reimplemented elsehwere
        # KEEPING FOR API COMPLIANCE ONLY
        if conf.get('diskspacecheck') == 0:
            self.ts.setProbFilter(rpm.RPMPROB_FILTER_DISKSPACE)
        tserrors = self.ts.run(cb.callback, '')
        self.ts.setFlags(origflags)

        reserrors = []
        if tserrors:
            for (descr, (etype, mount, need)) in tserrors:
                reserrors.append(descr)

        return reserrors

def initReadOnlyTransaction(root='/'):
    read_ts =  TransactionWrapper(root=root)
    read_ts.pushVSFlags((rpm._RPMVSF_NOSIGNATURES|rpm._RPMVSF_NODIGESTS))
    return read_ts

########NEW FILE########
__FILENAME__ = sack
# sack.py
# The dnf.Sack class, derived from hawkey.Sack
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
import dnf.util
import dnf.yum.misc
import hawkey
import logging
import sys
import dnf.package
import dnf.query
from dnf.pycomp import basestring

class SackVersion(object):
    def __init__(self):
        self._num = 0
        self._chksum = dnf.yum.misc.Checksums(['sha1'])

    def __str__(self):
        return "%u:%s" % (self._num, self._chksum.hexdigest())

    def __eq__(self, other):
        if other is None: return False
        if isinstance(other, basestring):
            return str(self) == other
        if self._num != other._num: return False
        if self._chksum.digest() != other._chksum.digest(): return False
        return True

    def __ne__(self, other):
        return not (self == other)

    def update(self, pkg, csum):
        self._num += 1
        self._chksum.update(str(pkg))
        if csum is not None:
            self._chksum.update(csum[0])
            self._chksum.update(csum[1])

class Sack(hawkey.Sack):
    def __init__(self, *args, **kwargs):
        super(Sack, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger("dnf")

    def configure(self, installonly=None, installonly_limit=0):
        if installonly:
            self.installonly = installonly
        self.installonly_limit = installonly_limit

    def query(self):
        """Factory function returning a DNF Query. :api"""
        return dnf.query.Query(self)

    def rpmdb_version(self, yumdb):
        pkgs = self.query().installed().run()
        main = SackVersion()
        for pkg in pkgs:
            ydbi = yumdb.get_package(pkg)
            csum = None
            if 'checksum_type' in ydbi and 'checksum_data' in ydbi:
                csum = (ydbi.checksum_type, ydbi.checksum_data)
            main.update(pkg, csum)
        return main

def build_sack(base):
    cachedir = base.conf.cachedir
    # create the dir ourselves so we have the permissions under control:
    dnf.util.ensure_dir(cachedir)
    return Sack(pkgcls=dnf.package.Package, pkginitval=base,
                cachedir=cachedir,
                rootdir=base.conf.installroot)

def rpmdb_sack(base):
    sack = build_sack(base)
    sack.load_system_repo()
    return sack

########NEW FILE########
__FILENAME__ = selector
# selector.py
# DNF specific hawkey.Selector handling.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals

import hawkey
import dnf.util

class Selector(hawkey.Selector):
    # :api
    def set_autoglob(self, **kwargs):
        nargs = {}
        for (key, value) in kwargs.items():
            if dnf.util.is_glob_pattern(value):
                nargs[key + "__glob"] = value
            else:
                nargs[key] = value
        return self.set(**nargs)

########NEW FILE########
__FILENAME__ = subject
# subject.py
# Implements Subject.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals
from dnf.util import first, is_glob_pattern

import dnf.selector
import hawkey

class Subject(object):
    # :api

    def __init__(self, pkg_spec, ignore_case=False):
        self.subj = hawkey.Subject(pkg_spec) # internal subject
        self.icase = ignore_case

    def _nevra_to_filters(self, query, nevra):
        nevra_attrs = [("name", True, True), ("epoch", False, False),
                       ("version", True, False), ("release", False, False),
                       ("arch", True, False)]

        for (name, check_glob, add_flags) in nevra_attrs:
            attr = getattr(nevra, name)
            flags = []
            if attr:
                if add_flags:
                    flags = self._query_flags
                if check_glob:
                    query = query.filter_autoglob(*flags, **{name: attr})
                else:
                    query.filterm(*flags, **{name: attr})

        return query

    @staticmethod
    def _nevra_to_selector(sltr, nevra):
        if nevra.name is not None:
            sltr.set_autoglob(name=nevra.name)
        if nevra.version is not None:
            evr = nevra.version
            if nevra.epoch is not None and nevra.epoch > 0:
                evr = "%d:%s" % (nevra.epoch, evr)
            if nevra.release is None:
                sltr.set(version=evr)
            else:
                evr = "%s-%s" % (evr, nevra.release)
                sltr.set(evr=evr)
        if nevra.arch is not None:
            sltr.set(arch=nevra.arch)
        return sltr

    @property
    def _query_flags(self):
        flags = []
        if self.icase:
            flags.append(hawkey.ICASE)
        return flags

    @property
    def filename_pattern(self):
        return self.subj.pattern.find('/') != -1

    @property
    def pattern(self):
        return self.subj.pattern

    def is_arch_specified(self, sack):
        nevra = first(self.subj.nevra_possibilities_real(sack, allow_globs=True))
        if nevra and nevra.arch:
            return is_glob_pattern(nevra.arch)
        return False

    def get_greedy_query(self, sack):
        pkgs = set()
        for nevra in self.subj.nevra_possibilities_real(sack, allow_globs=True,
                                                        icase=self.icase):
            q = self._nevra_to_filters(sack.query(), nevra)
            pkgs |= set(q.run())

        for rd in self.subj.reldep_possibilities_real(sack, icase=self.icase):
            q = sack.query().filter(provides=rd)
            pkgs |= set(q.run())

        q = sack.query().filter(*self._query_flags,
                                nevra__glob=self.subj.pattern)
        pkgs |= set(q.run())

        return sack.query().filter(pkg=pkgs)

    def get_best_query(self, sack, with_provides=True, forms=None):
        # :api
        pat = self.subj.pattern
        if self.filename_pattern:
            return sack.query().filter_autoglob(file=pat)

        kwargs = {'allow_globs' : True,
                  'icase'	: self.icase}
        if forms:
            kwargs['form'] = forms
        nevra = first(self.subj.nevra_possibilities_real(sack, **kwargs))
        if nevra:
            return self._nevra_to_filters(sack.query(), nevra)

        if with_provides:
            reldeps = self.subj.reldep_possibilities_real(sack, icase=self.icase)
            reldep = first(reldeps)
            if reldep:
                return sack.query().filter(provides=reldep)
        return sack.query().filter(empty=True)

    def get_best_selector(self, sack, forms=None):
        # :api
        kwargs = {'allow_globs' : True}
        if forms:
            kwargs['form'] = forms
        nevra = first(self.subj.nevra_possibilities_real(sack, **kwargs))
        if nevra:
            return self._nevra_to_selector(dnf.selector.Selector(sack), nevra)

        reldep = first(self.subj.reldep_possibilities_real(sack))
        if reldep:
            dep = str(reldep)
            return dnf.selector.Selector(sack).set(provides=dep)
        return None

########NEW FILE########
__FILENAME__ = transaction
# transaction.py
# Managing the transaction to be passed to RPM.
#
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.i18n import _
import logging
import operator
from functools import reduce

DOWNGRADE = 1
ERASE     = 2
INSTALL   = 3
REINSTALL = 4
UPGRADE   = 5

class TransactionItem(object):
    __slots__ = ('op_type', 'installed', 'erased', 'obsoleted', 'reason')

    def __init__(self, op_type, installed=None, erased=None, obsoleted=None,
                 reason='unknown'):
        self.op_type = op_type
        self.installed = installed
        self.erased = erased
        self.obsoleted = list() if obsoleted is None else obsoleted
        self.reason = reason # reason for it to be in the transaction set

    @property
    def active(self):
        return self.installed if self.installed is not None else self.erased

    @property
    def active_history_state(self):
        return (self.installed_history_state if self.installed is not None
                else self.erased_history_state)

    @property
    def erased_history_state(self):
        return self._HISTORY_ERASE_STATES[self.op_type]

    _HISTORY_INSTALLED_STATES = {
        DOWNGRADE : 'Downgrade',
        INSTALL   : 'Install',
        REINSTALL : 'Reinstall',
        UPGRADE   : 'Update'
        }

    _HISTORY_ERASE_STATES = {
        DOWNGRADE : 'Downgraded',
        ERASE     : 'Erase',
        REINSTALL : 'Reinstalled',
        UPGRADE   : 'Updated'
        }

    def history_iterator(self):
        if self.installed is not None:
            yield(self.installed, self.installed_history_state)
        if self.erased is not None:
            yield(self.erased, self.erased_history_state)
        if self.obsoleted:
            yield(self.installed, self.obsoleting_history_state)
        for obs in self.obsoleted:
            yield(obs, self.obsoleted_history_state)

    @property
    def installed_history_state(self):
        return self._HISTORY_INSTALLED_STATES[self.op_type]

    def installs(self):
        return [] if self.installed is None else [self.installed]

    @property
    def obsoleted_history_state(self):
        return 'Obsoleted'

    @property
    def obsoleting_history_state(self):
        return 'Obsoleting'

    def propagated_reason(self, yumdb):
        if self.reason == 'user':
            return self.reason
        if self.op_type in [DOWNGRADE, REINSTALL, UPGRADE]:
            previously = yumdb.get_package(self.erased).get('reason')
            if previously:
                return previously
        return self.reason

    def removes(self):
        l =  [] if self.erased is None else [self.erased]
        return l + self.obsoleted

class Transaction(object):
    # :api

    def __init__(self):
        # :api
        self._tsis = []
        self.logger = logging.getLogger("dnf")

    def __iter__(self):
        return iter(self._tsis)

    def __len__(self):
        return len(self._tsis)

    def _items2set(self, extracting_fn):
        lists = map(extracting_fn, self._tsis)
        sets = map(set, lists)
        return reduce(operator.or_, sets, set())

    def add_downgrade(self, new, downgraded, obsoleted):
        # :api
        tsi = TransactionItem(DOWNGRADE, new, downgraded, obsoleted)
        self._tsis.append(tsi)

    def add_erase(self, erased):
        # :api
        tsi = TransactionItem(ERASE, erased=erased)
        self._tsis.append(tsi)

    def add_install(self, new, obsoleted, reason='unknown'):
        # :api
        tsi = TransactionItem(INSTALL, new, obsoleted=obsoleted,
                              reason=reason)
        self._tsis.append(tsi)

    def add_reinstall(self, new, reinstalled, obsoleted):
        # :api
        tsi = TransactionItem(REINSTALL, new, reinstalled, obsoleted)
        self._tsis.append(tsi)

    def add_upgrade(self, upgrade, upgraded, obsoleted):
        # :api
        tsi = TransactionItem(UPGRADE, upgrade, upgraded, obsoleted)
        self._tsis.append(tsi)

    def get_items(self, op_type):
        return [tsi for tsi in self._tsis if tsi.op_type == op_type]

    @property
    def install_set(self):
        fn = operator.methodcaller('installs')
        return self._items2set(fn)

    def populate_rpm_ts(self, ts):
        """Populate the RPM transaction set."""

        for tsi in self._tsis:
            if tsi.op_type == DOWNGRADE:
                ts.addErase(tsi.erased.idx)
                hdr = tsi.installed.header
                ts.addInstall(hdr, tsi, 'i')
                self.logger.debug("populate_rpm_ts: downgrade: %s/%s" %
                                  (tsi.installed, tsi.erased))
            elif tsi.op_type == ERASE:
                ts.addErase(tsi.erased.idx)
                self.logger.debug("populate_rpm_ts: erase: %s" % tsi.erased)
            elif tsi.op_type == INSTALL:
                hdr = tsi.installed.header
                if tsi.obsoleted:
                    ts.addInstall(hdr, tsi, 'u')
                    msg = "populate_rpm_ts: install: %s promoted to upgrade."
                    self.logger.debug(msg % tsi.installed)
                else:
                    ts.addInstall(hdr, tsi, 'i')
                    self.logger.debug("populate_rpm_ts: install: %s" %
                                      tsi.installed)
            elif tsi.op_type == REINSTALL:
                ts.addErase(tsi.erased.idx)
                hdr = tsi.installed.header
                ts.addInstall(hdr, tsi, 'i')
                self.logger.debug("populate_rpm_ts: reinstall: %s" %
                                  tsi.erased)
            elif tsi.op_type == UPGRADE:
                hdr = tsi.installed.header
                ts.addInstall(hdr, tsi, 'u')
                self.logger.debug("populate_rpm_ts: upgrade: %s" % tsi.installed)
        return ts

    @property
    def remove_set(self):
        fn = operator.methodcaller('removes')
        return self._items2set(fn)

    def rpm_limitations(self):
        """ Ensures all the members can be passed to rpm as they are to pefrom
            the transaction.
        """
        src_installs = [pkg for pkg in self.install_set if pkg.arch == 'src']
        if len(src_installs):
            return _("Will not install a source rpm package (%s).") % \
                src_installs[0]
        return None

    def total_package_count(self):
        return len(self.install_set | self.remove_set)

########NEW FILE########
__FILENAME__ = util
# util.py
# Basic dnf utils.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import print_function
from __future__ import absolute_import
from __future__ import unicode_literals

from .pycomp import PY3, basestring
from functools import reduce

import dnf.const
import dnf.pycomp
import itertools
import librepo
import os
import shutil
import subprocess
import tempfile
import time

"""DNF Utilities.

Generally these are not a part of the public DNF API.

"""

def am_i_root():
    return os.geteuid() == 0

def ensure_dir(dname):
    if os.path.exists(dname):
        if not os.path.isdir(dname):
            raise IOError("%s is not a directory" % dname)
    else:
        os.makedirs(dname, mode=0o755)

def empty(iterable):
    try:
        l = len(iterable)
    except TypeError:
        l = len(list(iterable))
    return l == 0

def first(iterable):
    """Returns the first item from an iterable or None if it has no elements."""
    it = iter(iterable)
    try:
        return next(it)
    except StopIteration:
        return None

def file_age(fn):
    return time.time() - file_timestamp(fn)

def file_timestamp(fn):
    return os.stat(fn).st_mtime

def group_by_filter(fn, iterable):
    def splitter(acc, item):
        acc[not bool(fn(item))].append(item)
        return acc
    return reduce(splitter, iterable, ([], []))

def insert_if(item, iterable, condition):
    """Insert an item into an iterable by a condition."""
    for original_item in iterable:
        if condition(original_item):
            yield item
        yield original_item

def is_exhausted(iterator):
    """Test whether an iterator is exhausted."""
    try:
        next(iterator)
    except StopIteration:
        return True
    else:
        return False

def is_glob_pattern(pattern):
    return set(pattern) & set("*[?")

def is_string_type(obj):
    return isinstance(obj, basestring)

def lazyattr(attrname):
    """Decorator to get lazy attribute initialization.

    Composes with @property. Force reinitialization by deleting the <attrname>.
    """
    def get_decorated(fn):
        def cached_getter(obj):
            try:
                return getattr(obj, attrname)
            except AttributeError:
                val = fn(obj)
                setattr(obj, attrname, val)
                return val
        return cached_getter
    return get_decorated

def log_method_call(log_call):
    def wrapper(fn):
        def new_func(*args, **kwargs):
            name = '%s.%s' % (args[0].__class__.__name__, fn.__name__)
            log_call('Call: %s: %s, %s', name, args[1:], kwargs)
            return fn(*args, **kwargs)
        return new_func
    return wrapper

def mapall(fn, *seq):
    """Like functools.map(), but return a list instead of an iterator.

    This means all side effects of fn take place even without iterating the
    result.

    """
    return list(map(fn, *seq))

def on_ac_power():
    """Decide whether we are on line power.

    Returns True if we are on line power, False if not, None if it can not be
    decided.

    """
    try:
        ret = subprocess.call('/usr/bin/on_ac_power')
        return not ret
    except OSError:
        return None

def partition(pred, iterable):
    """Use a predicate to partition entries into false entries and true entries.

    Credit: Python library itertools' documentation.

    """
    t1, t2 = itertools.tee(iterable)
    return dnf.pycomp.filterfalse(pred, t1), filter(pred, t2)

def rm_rf(path):
    try:
        shutil.rmtree(path)
    except OSError:
        pass

def split_by(iterable, condition):
    """Split an iterable into tuples by a condition.

    Inserts a separator before each item which meets the condition and then
    cuts the iterable by these separators.

    """
    separator = object()  # A unique object.
    # Create a function returning tuple of objects before the separator.
    def next_subsequence(it):
        return tuple(itertools.takewhile(lambda e: e != separator, it))

    # Mark each place where the condition is met by the separator.
    marked = insert_if(separator, iterable, condition)

    # The 1st subsequence may be empty if the 1st item meets the condition.
    yield next_subsequence(marked)

    while True:
        subsequence = next_subsequence(marked)
        if not subsequence:
            break
        yield subsequence

def strip_prefix(s, prefix):
    if s.startswith(prefix):
        return s[len(prefix):]
    return None

def timed(fn):
    """Decorator, prints out the ms a function took to complete.

    Used for debugging.

    """
    def decorated(*args, **kwargs):
        start = time.time()
        retval = fn(*args, **kwargs)
        length = time.time() - start
        print("%s took %.02f ms" % (fn.__name__, length * 1000))
        return retval
    return decorated

def touch(path, no_create=False):
    """Create an empty file if it doesn't exist or bump it's timestamps.

    If no_create is True only bumps the timestamps.
    """
    if no_create or os.access(path, os.F_OK):
        return os.utime(path, None)
    with open(path, 'a'):
        pass

def user_run_dir():
    uid = str(os.getuid())
    return os.path.join(dnf.const.USER_RUNDIR, uid, dnf.const.PROGRAM_NAME)

class tmpdir(object):
    def __init__(self):
        prefix = '%s-' % dnf.const.PREFIX
        self.path = tempfile.mkdtemp(prefix=prefix)

    def __enter__(self):
        return self.path

    def __exit__(self, exc_type, exc_value, traceback):
        rm_rf(self.path)

class Bunch(dict):
    """Dictionary with attribute accessing syntax.

    In DNF, prefer using this over dnf.yum.misc.GenericHolder.

    Credit: Alex Martelli, Doug Hudgeon

    """
    def __init__(self, *args, **kwds):
         super(Bunch, self).__init__(*args, **kwds)
         self.__dict__ = self

    def __hash__(self):
        return id(self)

default_handle = librepo.Handle()
default_handle.useragent = dnf.const.USER_AGENT

def urlopen(absurl, repo=None, **kwargs):
    """Open the specified absolute url, return a file object.

    repo -- Use this repo-specific config (proxies, certs)
    kwargs -- These are passed to TemporaryFile
    """
    if PY3:
        kwargs['mode'] = 'w+'
        kwargs['encoding'] = 'utf-8'
    fo = tempfile.NamedTemporaryFile(**kwargs)
    handle = default_handle
    if repo:
        handle = repo.get_handle()
    try:
        librepo.download_url(absurl, fo.fileno(), handle)
    except librepo.LibrepoException as e:
        raise IOError(e.args[1])
    fo.seek(0)
    return fo

########NEW FILE########
__FILENAME__ = config
# Copyright 2002 Duke University
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

"""
Configuration parser and default values for yum.
"""

from __future__ import absolute_import
from __future__ import unicode_literals

from . import misc
from .misc import read_in_items_from_dot_dir
from .parser import ConfigPreProcessor, varReplace
from dnf.pycomp import is_py3bytes, basestring
from iniparse import INIConfig
from iniparse.compat import NoSectionError, NoOptionError, ParsingError
from iniparse.compat import RawConfigParser as ConfigParser

import copy
import dnf.const
import dnf.exceptions
import dnf.rpmUtils.transaction
import dnf.util
import os
import rpm
import shlex
import sys
import types

try:
    import urlparse
except ImportError:
    import urllib.parse as urlparse

class Option(object):
    """
    This class handles a single Yum configuration file option. Create
    subclasses for each type of supported configuration option.
    Python descriptor foo (__get__ and __set__) is used to make option
    definition easy and concise.
    """

    def __init__(self, default=None, parse_default=False):
        self._setattrname()
        self.inherit = False
        if parse_default:
            default = self.parse(default)
        self.default = default

    def _setattrname(self):
        """Calculate the internal attribute name used to store option state in
        configuration instances.
        """
        self._attrname = '__opt%d' % id(self)
        self._attrname_deleted = '__opt%d_deleted' % id(self)

    def __get__(self, obj, objtype):
        """Called when the option is read (via the descriptor protocol).

        :param obj: The configuration instance to modify.
        :param objtype: The type of the config instance (not used).
        :return: The parsed option value or the default value if the value
           wasn't set in the configuration file.
        """
        if getattr(obj, self._attrname_deleted, None) is True:
            raise RuntimeError("Option is no longer a part of the conf.")
        if obj is None:
            return self

        return getattr(obj, self._attrname, None)

    def __set__(self, obj, value):
        """Called when the option is set (via the descriptor protocol).

        :param obj: The configuration instance to modify.
        :param value: The value to set the option to.
        """
        # Only try to parse if it's a string
        if isinstance(value, basestring):
            try:
                value = self.parse(value)
            except ValueError as e:
                raise ValueError('Error parsing "%s = %r": %s' % (self._optname,
                                                                 value, str(e)))
        elif isinstance(value, int):
            try:
                value = self.parse_int(value)
            except ValueError as e:
                raise ValueError('Error parsing "%s = %r": %s' % (self._optname,
                                                                 value, str(e)))

        setattr(obj, self._attrname, value)

    def __delete__(self, obj):
        setattr(obj, self._attrname_deleted, True)

    def setup(self, obj, name):
        """Initialise the option for a config instance.
        This must be called before the option can be set or retrieved.

        :param obj: :class:`BaseConfig` (or subclass) instance.
        :param name: Name of the option.
        """
        self._optname = name
        setattr(obj, self._attrname, copy.copy(self.default))

    def clone(self):
        """Return a safe copy of this :class:`Option` instance.

        :return: a safe copy of this :class:`Option` instance
        """
        new = copy.copy(self)
        new._setattrname()
        return new

    def parse(self, s):
        """Parse the string value to the :class:`Option`'s native value.

        :param s: raw string value to parse
        :return: validated native value
        :raise: ValueError if there was a problem parsing the string.
           Subclasses should override this
        """
        return s

    def parse_int(self, n):
        """Parse `n`, ensuring it is a suitable integer value.

        Return parsed value or raise ValueError if it is not suitable.

        """

        return n

    def tostring(self, value):
        """Convert the :class:`Option`'s native value to a string value.  This
        does the opposite of the :func:`parse` method above.
        Subclasses should override this.

        :param value: native option value
        :return: string representation of input
        """
        return str(value)

def Inherit(option_obj):
    """Clone an :class:`Option` instance for the purposes of inheritance. The returned
    instance has all the same properties as the input :class:`Option` and shares items
    such as the default value. Use this to avoid redefinition of reused
    options.

    :param option_obj: :class:`Option` instance to inherit
    :return: New :class:`Option` instance inherited from the input
    """
    new_option = option_obj.clone()
    new_option.inherit = True
    return new_option

class ListOption(Option):
    """An option containing a list of strings."""

    def __init__(self, default=None, parse_default=False):
        if default is None:
            default = []
        super(ListOption, self).__init__(default, parse_default)

    def parse(self, s):
        """Convert a string from the config file to a workable list, parses
        globdir: paths as foo.d-style dirs.

        :param s: The string to be converted to a list. Commas and
           whitespace are used as separators for the list
        :return: *s* converted to a list
        """
        # we need to allow for the '\n[whitespace]' continuation - easier
        # to sub the \n with a space and then read the lines
        s = s.replace('\n', ' ')
        s = s.replace(',', ' ')
        results = []
        for item in s.split():
            if item.startswith('glob:'):
                thisglob = item.replace('glob:', '')
                results.extend(read_in_items_from_dot_dir(thisglob))
                continue
            results.append(item)

        return results

    def tostring(self, value):
        """Convert a list of to a string value.  This does the
        opposite of the :func:`parse` method above.

        :param value: a list of values
        :return: string representation of input
        """
        return '\n '.join(value)

class UrlOption(Option):
    """This option handles lists of URLs with validation of the URL
    scheme.
    """

    def __init__(self, default=None, schemes=('http', 'ftp', 'file', 'https'),
            allow_none=False):
        super(UrlOption, self).__init__(default)
        self.schemes = schemes
        self.allow_none = allow_none

    def parse(self, url):
        """Parse a url to make sure that it is valid, and in a scheme
        that can be used.

        :param url: a string containing the url to parse
        :return: *url* if it is valid
        :raises: :class:`ValueError` if there is an error parsing the url
        """
        url = url.strip()

        # Handle the "_none_" special case
        if url.lower() == '_none_':
            if self.allow_none:
                return None
            else:
                raise ValueError('"_none_" is not a valid value')

        # Check that scheme is valid
        (s,b,p,q,f,o) = urlparse.urlparse(url)
        if s not in self.schemes:
            raise ValueError('URL must be %s not "%s"' % (self._schemelist(), s))

        return url

    def _schemelist(self):
        '''Return a user friendly list of the allowed schemes
        '''
        if len(self.schemes) < 1:
            return 'empty'
        elif len(self.schemes) == 1:
            return self.schemes[0]
        else:
            return '%s or %s' % (', '.join(self.schemes[:-1]), self.schemes[-1])

class UrlListOption(ListOption):
    """Option for handling lists of URLs with validation of the URL
    scheme.
    """
    def __init__(self, default=None, schemes=('http', 'ftp', 'file', 'https'),
                 parse_default=False):
        super(UrlListOption, self).__init__(default, parse_default)

        # Hold a UrlOption instance to assist with parsing
        self._urloption = UrlOption(schemes=schemes)

    def parse(self, s):
        """Parse a string containing multiple urls into a list, and
        ensure that they are in a scheme that can be used.

        :param s: the string to parse
        :return: a list of strings containing the urls in *s*
        :raises: :class:`ValueError` if there is an error parsing the urls
        """
        out = []
        s = s.replace('\n', ' ')
        s = s.replace(',', ' ')
        items = [ item.replace(' ', '%20') for item in shlex.split(s) ]
        tmp = []
        for item in items:
            if item.startswith('glob:'):
                thisglob = item.replace('glob:', '')
                tmp.extend(read_in_items_from_dot_dir(thisglob))
                continue
            tmp.append(item)

        for url in super(UrlListOption, self).parse(' '.join(tmp)):
            out.append(self._urloption.parse(url))
        return out


class IntOption(Option):
    """An option representing an integer value."""

    def __init__(self, default=None, range_min=None, range_max=None):
        super(IntOption, self).__init__(default)
        self._range_min = range_min
        self._range_max = range_max

    def parse(self, s):
        """Parse a string containing an integer.

        :param s: the string to parse
        :return: the integer in *s*
        :raises: :class:`ValueError` if there is an error parsing the
           integer
        """
        try:
            val = int(s)
        except (ValueError, TypeError) as e:
            raise ValueError('invalid integer value')
        return self.parse_int(val)

    def parse_int(self, n):
        if self._range_max is not None and n > self._range_max:
            raise ValueError('Out of range integer value.')
        if self._range_min is not None and n < self._range_min:
            raise ValueError('Out of range integer value.')
        return n

class PositiveIntOption(IntOption):
    """An option representing a positive integer value, where 0 can
    have a special representation.
    """
    def __init__(self, default=None, range_min=0, range_max=None,
                 names_of_0=None):
        super(PositiveIntOption, self).__init__(default, range_min, range_max)
        self._names0 = names_of_0

    def parse(self, s):
        """Parse a string containing a positive integer, where 0 can
           have a special representation.

        :param s: the string to parse
        :return: the integer in *s*
        :raises: :class:`ValueError` if there is an error parsing the
           integer
        """
        if s in self._names0:
            return 0
        return super(PositiveIntOption, self).parse(s)

class SecondsOption(Option):
    """An option representing an integer value of seconds, or a human
    readable variation specifying days, hours, minutes or seconds
    until something happens. Works like :class:`BytesOption`.  Note
    that due to historical president -1 means "never", so this accepts
    that and allows the word never, too.

    Valid inputs: 100, 1.5m, 90s, 1.2d, 1d, 0xF, 0.1, -1, never.
    Invalid inputs: -10, -0.1, 45.6Z, 1d6h, 1day, 1y.

    Return value will always be an integer
    """
    MULTS = {'d': 60 * 60 * 24, 'h' : 60 * 60, 'm' : 60, 's': 1}

    def parse(self, s):
        """Parse a string containing an integer value of seconds, or a human
        readable variation specifying days, hours, minutes or seconds
        until something happens. Works like :class:`BytesOption`.  Note
        that due to historical president -1 means "never", so this accepts
        that and allows the word never, too.

        Valid inputs: 100, 1.5m, 90s, 1.2d, 1d, 0xF, 0.1, -1, never.
        Invalid inputs: -10, -0.1, 45.6Z, 1d6h, 1day, 1y.

        :param s: the string to parse
        :return: an integer representing the number of seconds
           specified by *s*
        :raises: :class:`ValueError` if there is an error parsing the string
        """
        if len(s) < 1:
            raise ValueError("no value specified")

        if s == "-1" or s == "never": # Special cache timeout, meaning never
            return -1
        if s[-1].isalpha():
            n = s[:-1]
            unit = s[-1].lower()
            mult = self.MULTS.get(unit, None)
            if not mult:
                raise ValueError("unknown unit '%s'" % unit)
        else:
            n = s
            mult = 1

        try:
            n = float(n)
        except (ValueError, TypeError) as e:
            raise ValueError('invalid value')

        if n < 0:
            raise ValueError("seconds value may not be negative")

        return int(n * mult)

class BoolOption(Option):
    """An option representing a boolean value.  The value can be one
    of 0, 1, yes, no, true, or false.
    """

    def parse(self, s):
        """Parse a string containing a boolean value.  1, yes, and
        true will evaluate to True; and 0, no, and false will evaluate
        to False.  Case is ignored.

        :param s: the string containing the boolean value
        :return: the boolean value contained in *s*
        :raises: :class:`ValueError` if there is an error in parsing
           the boolean value
        """
        s = s.lower()
        if s in ('0', 'no', 'false'):
            return False
        elif s in ('1', 'yes', 'true'):
            return True
        else:
            raise ValueError('invalid boolean value')

    def tostring(self, value):
        """Convert a boolean value to a string value.  This does the
        opposite of the :func:`parse` method above.

        :param value: the boolean value to convert
        :return: a string representation of *value*
        """
        if value:
            return "1"
        else:
            return "0"

class FloatOption(Option):
    """An option representing a numeric float value."""

    def parse(self, s):
        """Parse a string containing a numeric float value.

        :param s: a string containing a numeric float value to parse
        :return: the numeric float value contained in *s*
        :raises: :class:`ValueError` if there is an error parsing
           float value
        """
        try:
            return float(s.strip())
        except (ValueError, TypeError):
            raise ValueError('invalid float value')

class SelectionOption(Option):
    """Handles string values where only specific values are
    allowed.
    """
    def __init__(self, default=None, allowed=(), mapper={}):
        super(SelectionOption, self).__init__(default)
        self._allowed = allowed
        self._mapper  = mapper

    def parse(self, s):
        """Parse a string for specific values.

        :param s: the string to parse
        :return: *s* if it contains a valid value
        :raises: :class:`ValueError` if there is an error parsing the values
        """
        if s in self._mapper:
            s = self._mapper[s]
        if s not in self._allowed:
            raise ValueError('"%s" is not an allowed value' % s)
        return s

class CaselessSelectionOption(SelectionOption):
    """Mainly for compatibility with :class:`BoolOption`, works like
    :class:`SelectionOption` but lowers input case.
    """
    def parse(self, s):
        """Parse a string for specific values.

        :param s: the string to parse
        :return: *s* if it contains a valid value
        :raises: :class:`ValueError` if there is an error parsing the values
        """
        return super(CaselessSelectionOption, self).parse(s.lower())

class BytesOption(Option):
    """An option representing a value in bytes. The value may be given
    in bytes, kilobytes, megabytes, or gigabytes.
    """
    # Multipliers for unit symbols
    MULTS = {
        'k': 1024,
        'm': 1024*1024,
        'g': 1024*1024*1024,
    }

    def parse(self, s):
        """Parse a friendly bandwidth option to bytes.  The input
        should be a string containing a (possibly floating point)
        number followed by an optional single character unit. Valid
        units are 'k', 'M', 'G'. Case is ignored. The convention that
        1k = 1024 bytes is used.

        Valid inputs: 100, 123M, 45.6k, 12.4G, 100K, 786.3, 0.
        Invalid inputs: -10, -0.1, 45.6L, 123Mb.

        :param s: the string to parse
        :return: the number of bytes represented by *s*
        :raises: :class:`ValueError` if the option can't be parsed
        """
        if len(s) < 1:
            raise ValueError("no value specified")

        if s[-1].isalpha():
            n = s[:-1]
            unit = s[-1].lower()
            mult = self.MULTS.get(unit, None)
            if not mult:
                raise ValueError("unknown unit '%s'" % unit)
        else:
            n = s
            mult = 1

        try:
            n = float(n)
        except ValueError:
            raise ValueError("couldn't convert '%s' to number" % n)

        if n < 0:
            raise ValueError("bytes value may not be negative")

        return int(n * mult)

class ThrottleOption(BytesOption):
    """An option representing a bandwidth throttle value. See
    :func:`parse` for acceptable input values.
    """

    def parse(self, s):
        """Get a throttle option. Input may either be a percentage or
        a "friendly bandwidth value" as accepted by the
        :class:`BytesOption`.

        Valid inputs: 100, 50%, 80.5%, 123M, 45.6k, 12.4G, 100K, 786.0, 0.
        Invalid inputs: 100.1%, -4%, -500.

        :param s: the string to parse
        :return: the bandwidth represented by *s*. The return value
           will be an int if a bandwidth value was specified, and a
           float if a percentage was given
        :raises: :class:`ValueError` if input can't be parsed
        """
        if len(s) < 1:
            raise ValueError("no value specified")

        if s[-1] == '%':
            n = s[:-1]
            try:
                n = float(n)
            except ValueError:
                raise ValueError("couldn't convert '%s' to number" % n)
            if n < 0 or n > 100:
                raise ValueError("percentage is out of range")
            return n / 100.0
        else:
            return BytesOption.parse(self, s)

class BaseConfig(object):
    """Base class for storing configuration definitions.

    Subclass when creating your own definitions.

    """

    def __init__(self):
        self._section = None

        for name in self.iterkeys():
            option = self.optionobj(name)
            option.setup(self, name)

    def __str__(self):
        out = []
        out.append('[%s]' % self._section)
        for name, value in self.iteritems():
            out.append('%s: %r' % (name, value))
        return '\n'.join(out)

    def override(self, ovr_dict):
        """Override config values with those from ovr_dict.

        Do nothing about the keys that are not options.
        """
        for (ovr_opt, ovr_val) in ovr_dict.items():
            opt = self.optionobj(ovr_opt, exceptions=False)
            if opt is not None:
                setattr(self, ovr_opt, ovr_val)

    def populate(self, parser, section, parent=None):
        """Set option values from an INI file section.

        :param parser: :class:`ConfigParser` instance (or subclass)
        :param section: INI file section to read use
        :param parent: Optional parent :class:`BaseConfig` (or
            subclass) instance to use when doing option value
            inheritance
        """
        self.cfg = parser
        self._section = section

        if parser.has_section(section):
            opts = set(parser.options(section))
        else:
            opts = set()
        for name in self.iterkeys():
            option = self.optionobj(name)
            value = None
            if name in opts:
                value = parser.get(section, name)
            else:
                # No matching option in this section, try inheriting
                if parent and option.inherit:
                    value = getattr(parent, name)

            if value is not None:
                setattr(self, name, value)

    def optionobj(cls, name, exceptions=True):
        """Return the :class:`Option` instance for the given name.

        :param cls: the class to return the :class:`Option` instance from
        :param name: the name of the :class:`Option` instance to return
        :param exceptions: defines what action to take if the
           specified :class:`Option` instance does not exist. If *exceptions* is
           True, a :class:`KeyError` will be raised. If *exceptions*
           is False, None will be returned
        :return: the :class:`Option` instance specified by *name*, or None if
           it does not exist and *exceptions* is False
        :raises: :class:`KeyError` if the specified :class:`Option` does not
           exist, and *exceptions* is True
        """
        obj = getattr(cls, name, None)
        if isinstance(obj, Option):
            return obj
        elif exceptions:
            raise KeyError
        else:
            return None
    optionobj = classmethod(optionobj)

    def isoption(cls, name):
        """Return True if the given name refers to a defined option.

        :param cls: the class to find the option in
        :param name: the name of the option to search for
        :return: whether *name* specifies a defined option
        """
        return cls.optionobj(name, exceptions=False) is not None
    isoption = classmethod(isoption)

    def iterkeys(self):
        """Yield the names of all defined options in the instance."""

        for name in dir(self):
            if self.isoption(name):
                yield name

    def iteritems(self):
        """Yield (name, value) pairs for every option in the
        instance. The value returned is the parsed, validated option
        value.
        """
        # Use dir() so that we see inherited options too
        for name in self.iterkeys():
            yield (name, getattr(self, name))

    def write(self, fileobj, section=None, always=()):
        """Write out the configuration to a file-like object.

        :param fileobj: File-like object to write to
        :param section: Section name to use. If not specified, the section name
            used during parsing will be used
        :param always: A sequence of option names to always write out.
            Options not listed here will only be written out if they are at
            non-default values. Set to None to dump out all options
        """
        # Write section heading
        if section is None:
            if self._section is None:
                raise ValueError("not populated, don't know section")
            section = self._section

        # Updated the ConfigParser with the changed values
        cfgOptions = self.cfg.options(section)
        for name,value in self.iteritems():
            option = self.optionobj(name)
            if always is None or name in always or option.default != value or name in cfgOptions :
                self.cfg.set(section,name, option.tostring(value))
        # write the updated ConfigParser to the fileobj.
        self.cfg.write(fileobj)

class YumConf(BaseConfig):
    """Configuration option definitions for yum.conf's [main] section."""

    debuglevel = IntOption(2, 0, 10) # :api
    errorlevel = IntOption(2, 0, 10)

    distroverpkg = Option('redhat-release')
    installroot = Option('/') # :api
    config_file_path = Option(dnf.const.CONF_FILENAME) # :api
    plugins = BoolOption(True)
    pluginpath = ListOption([dnf.const.PLUGINPATH]) # :api
    pluginconfpath = ListOption(['/etc/dnf/plugins'])
    gaftonmode = BoolOption(False)
    syslog_ident = Option()
    syslog_facility = Option('LOG_USER')
    syslog_device = Option('/dev/log')
    persistdir = Option(dnf.const.PERSISTDIR) # :api

    def __init__(self):
        super(YumConf, self).__init__()
        self.yumvar = {}

    def _var_replace(self, option):
        path = getattr(self, option)
        setattr(self, option, varReplace(path, self.yumvar))

    def prepend_installroot(self, option):
        # :api
        path = getattr(self, option)
        path = path.lstrip('/')
        setattr(self, option, os.path.join(self.installroot, path))

    @property
    def releasever(self):
        # :api
        return self.yumvar.get('releasever')

    @releasever.setter
    def releasever(self, val):
        # :api
        if val is None:
            val = _getsysver(self.installroot,
                             self.distroverpkg)
        self.yumvar['releasever'] = val

    def yumvar_update_from_env(self):
        for num in range(0, 10):
            env = 'YUM%d' % num
            val = os.environ.get(env, '')
            if val:
                self.yumvar[env.lower()] = val

    def yumvar_update_from_etc(self):
        try:
            dir_fsvars = self.installroot + "/etc/yum/vars/"
            fsvars = os.listdir(dir_fsvars)
        except OSError:
            fsvars = []
        for fsvar in fsvars:
            if os.path.islink(dir_fsvars + fsvar):
                continue
            try:
                val = open(dir_fsvars + fsvar).readline()
                if val and val[-1] == '\n':
                    val = val[:-1]
            except (OSError, IOError):
                continue
            self.yumvar[fsvar] = val

    retries = PositiveIntOption(10, names_of_0=["<forever>"])
    recent = IntOption(7, range_min=0)
    reset_nice = BoolOption(True)

    cachedir = Option(dnf.const.SYSTEM_CACHEDIR) # :api

    keepcache = BoolOption(False)
    logdir = Option('/var/log') # :api
    reposdir = ListOption(['/etc/yum/repos.d', '/etc/yum.repos.d']) # :api

    debug_solver = BoolOption(False)

    commands = ListOption()
    exclude = ListOption()
    failovermethod = Option('roundrobin')
    fastestmirror = BoolOption(False)
    proxy = UrlOption(schemes=('http', 'ftp', 'https'), allow_none=True)
    proxy_username = Option()
    proxy_password = Option()
    username = Option()
    password = Option()
    installonlypkgs = ListOption(dnf.const.INSTALLONLYPKGS)
    # NOTE: If you set this to 2, then because it keeps the current kernel it
    # means if you ever install an "old" kernel it'll get rid of the newest one
    # so you probably want to use 3 as a minimum ... if you turn it on.
    installonly_limit = PositiveIntOption(0, range_min=2, # :api
                                          names_of_0=["0", "<off>"])
    tsflags = ListOption() # :api

    assumeyes = BoolOption(False)
    assumeno  = BoolOption(False)
    defaultyes = BoolOption(False)
    alwaysprompt = BoolOption(True)
    diskspacecheck = BoolOption(True)
    keepalive = BoolOption(True)
    # FIXME: rename gpgcheck to pkgs_gpgcheck
    gpgcheck = BoolOption(False)
    repo_gpgcheck = BoolOption(False)
    localpkg_gpgcheck = BoolOption(False)
    obsoletes = BoolOption(True)
    showdupesfromrepos = BoolOption(False)
    enabled = BoolOption(True)
    enablegroups = BoolOption(True)
    enable_group_conditionals = BoolOption(True)
    groupremove_leaf_only = BoolOption(False)

    timeout = FloatOption(30.0) # FIXME: Should use variation of SecondsOption

    bandwidth = BytesOption(0)
    throttle = ThrottleOption(0)
    ip_resolve = CaselessSelectionOption(
            allowed = ('ipv4', 'ipv6', 'whatever'),
            mapper  = {'4': 'ipv4', '6': 'ipv6'})

    http_caching = SelectionOption('all', ('none', 'packages', 'all'))
    metadata_expire = SecondsOption(60 * 60 * 48)    # 48 hours
    metadata_timer_sync = SecondsOption(60 * 60 * 3) #  3 hours
    disable_excludes = ListOption()
    multilib_policy = SelectionOption('best', ('best', 'all')) # :api
    best = BoolOption(False) # :api
    bugtracker_url = Option(dnf.const.BUGTRACKER)

    color = SelectionOption('auto', ('auto', 'never', 'always'),
                            mapper={'on' : 'always', 'yes' : 'always',
                                    '1' : 'always', 'true' : 'always',
                                    'off' : 'never', 'no' : 'never',
                                    '0' : 'never', 'false' : 'never',
                                    'tty' : 'auto', 'if-tty' : 'auto'})
    color_list_installed_older = Option('bold')
    color_list_installed_newer = Option('bold,yellow')
    color_list_installed_reinstall = Option('normal')
    color_list_installed_extra = Option('bold,red')

    color_list_available_upgrade = Option('bold,blue')
    color_list_available_downgrade = Option('dim,cyan')
    color_list_available_reinstall = Option('bold,underline,green')
    color_list_available_install = Option('normal')

    color_update_installed = Option('normal')
    color_update_local     = Option('bold')
    color_update_remote    = Option('normal')

    color_search_match = Option('bold')

    sslcacert = Option()
    sslverify = BoolOption(True)
    sslclientcert = Option()
    sslclientkey = Option()
    deltarpm = BoolOption(False)

    history_record = BoolOption(True)
    history_record_packages = ListOption(['dnf', 'rpm'])

    rpmverbosity = Option('info')

    protected_packages = ListOption("yum, glob:/etc/yum/protected.d/*.conf",
                                    parse_default=True)
    protected_multilib = BoolOption(True)
    exit_on_lock = BoolOption(False)

    loadts_ignoremissing = BoolOption(False)
    loadts_ignorerpm = BoolOption(False)

    clean_requirements_on_remove = BoolOption(False)

    history_list_view = SelectionOption('single-user-commands',
                                        ('single-user-commands', 'users',
                                         'commands'),
                                     mapper={'cmds'          : 'commands',
                                             'default' :'single-user-commands'})

    def dump(self):
        """Return a string representing the values of all the
        configuration options.

        :return: a string representing the values of all the
           configuration options
        """
        output = '[main]\n'
        # we exclude all vars which start with _ or are in this list:
        excluded_vars = ('cfg', 'yumvar', 'disable_excludes', 'config_file_path')
        for attr in dir(self):
            if attr.startswith('_'):
                continue
            if attr in excluded_vars:
                continue
            try:
                res = getattr(self, attr)
            except RuntimeError:
                output += "(%s deleted)\n" % attr
                continue
            if isinstance(res, types.MethodType):
                continue
            if not res and type(res) not in (type(False), type(0)):
                res = ''
            if isinstance(res, list):
                res = ',\n   '.join(res)
            output = output + '%s = %s\n' % (attr, res)

        return output

    def read(self, filename=None):
        # :api
        if filename is None:
            filename = self.config_file_path
        parser = ConfigParser()
        config_pp = ConfigPreProcessor(filename)
        try:
            parser.readfp(config_pp)
        except ParsingError as e:
            raise dnf.exceptions.ConfigError("Parsing file failed: %s" % e)
        self.populate(parser, 'main')

        # update to where we read the file from
        self.config_file_path = filename
        # not really a config value:

    @property
    def verbose(self):
        return self.debuglevel >= dnf.const.VERBOSE_LEVEL

class RepoConf(BaseConfig):
    """Option definitions for repository INI file sections."""

    __cached_keys = set()
    def iterkeys(self):
        """Yield the names of all defined options in the instance."""

        ck = self.__cached_keys
        if not isinstance(self, RepoConf):
            ck = set()
        if not ck:
            ck.update(list(BaseConfig.iterkeys(self)))

        for name in self.__cached_keys:
            yield name

    name = Option() # :api
    enabled = Inherit(YumConf.enabled)
    baseurl = UrlListOption() # :api
    mirrorlist = UrlOption() # :api
    metalink   = UrlOption() # :api
    mediaid = Option()
    gpgkey = UrlListOption()
    gpgcakey = UrlListOption()
    exclude = ListOption()
    includepkgs = ListOption()

    fastestmirror = Inherit(YumConf.fastestmirror)
    proxy = Inherit(YumConf.proxy)
    proxy_username = Inherit(YumConf.proxy_username)
    proxy_password = Inherit(YumConf.proxy_password)
    retries = Inherit(YumConf.retries)
    failovermethod = Inherit(YumConf.failovermethod)
    username = Inherit(YumConf.username)
    password = Inherit(YumConf.password)

    # FIXME: rename gpgcheck to pkgs_gpgcheck
    gpgcheck = Inherit(YumConf.gpgcheck)
    repo_gpgcheck = Inherit(YumConf.repo_gpgcheck)
    keepalive = Inherit(YumConf.keepalive)
    enablegroups = Inherit(YumConf.enablegroups)

    bandwidth = Inherit(YumConf.bandwidth)
    throttle = Inherit(YumConf.throttle)
    timeout = Inherit(YumConf.timeout)
    ip_resolve = Inherit(YumConf.ip_resolve)

    http_caching = Inherit(YumConf.http_caching)
    metadata_expire = Inherit(YumConf.metadata_expire)
    cost = IntOption(1000)

    sslcacert = Inherit(YumConf.sslcacert)
    sslverify = Inherit(YumConf.sslverify) # :api
    sslclientcert = Inherit(YumConf.sslclientcert)
    sslclientkey = Inherit(YumConf.sslclientkey)
    deltarpm = Inherit(YumConf.deltarpm)

    skip_if_unavailable = BoolOption(True)

class VersionGroupConf(BaseConfig):
    """Option definitions for version groups."""

    pkglist = ListOption()
    run_with_packages = BoolOption(False)

def readVersionGroupsConfig(configfile="/etc/yum/version-groups.conf"):
    """Parse the configuration file for version groups.

    :param configfile: the configuration file to read
    :return: a dictionary containing the parsed options
    """

    parser = ConfigParser()
    confpp_obj = ConfigPreProcessor(configfile)
    try:
        parser.readfp(confpp_obj)
    except ParsingError as e:
        raise dnf.exceptions.ConfigError("Parsing file failed: %s" % e)
    ret = {}
    for section in parser.sections():
        ret[section] = VersionGroupConf()
        ret[section].populate(parser, section)
    return ret


def getOption(conf, section, name, option):
    """Convenience function to retrieve a parsed and converted value from a
    :class:`ConfigParser`.

    :param conf: ConfigParser instance or similar
    :param section: Section name
    :param name: :class:`Option` name
    :param option: :class:`Option` instance to use for conversion
    :return: The parsed value or default if value was not present
    :raises: :class:`ValueError` if the option could not be parsed
    """
    try:
        val = conf.get(section, name)
    except (NoSectionError, NoOptionError):
        return option.default
    return option.parse(val)

def _getsysver(installroot, distroverpkg):
    '''Calculate the release version for the system.

    @param installroot: The value of the installroot option.
    @param distroverpkg: The value of the distroverpkg option.
    @return: The release version as a string (eg. '4' for FC4)
    '''
    ts = dnf.rpmUtils.transaction.initReadOnlyTransaction(root=installroot)
    ts.pushVSFlags(~(rpm._RPMVSF_NOSIGNATURES|rpm._RPMVSF_NODIGESTS))
    try:
        idx = ts.dbMatch('provides', distroverpkg)
    except TypeError as e:
        # This is code for "cannot open rpmdb"
        # this is for pep 352 compliance on python 2.6 and above :(
        if sys.hexversion < 0x02050000:
            if hasattr(e,'message'):
                raise dnf.exceptions.Error("Error: " + str(e.message))
            else:
                raise dnf.exceptions.Error("Error: " + str(e))
        raise dnf.exceptions.Error("Error: " + str(e))
    except rpm.error as e:
        # This is the "new" code for "cannot open rpmdb", 4.8.0 ish
        raise dnf.exceptions.Error("Error: " + str(e))
    # we're going to take the first one - if there is more than one of these
    # then the user needs a beating
    if len(idx) == 0:
        releasever = '$releasever'
    else:
        try:
            hdr = next(idx)
        except StopIteration:
            raise dnf.exceptions.Error("Error: rpmdb failed release provides. Try: rpm --rebuilddb")
        releasever = hdr['version']
        if is_py3bytes(releasever):
            releasever = str(releasever, "utf-8")
        del hdr
    del idx
    del ts
    return releasever

def writeRawRepoFile(repo,only=None):
    """Write changes in a repo object back to a .repo file.

    :param repo: the Repo Object to write back out
    :param only: list of attributes to work on. If *only* is None, all
       options will be written out
    """
    ini = INIConfig(open(repo.repofile))
    # b/c repoids can have $values in them we need to map both ways to figure
    # out which one is which
    section_id = repo.id
    if repo.id not in ini._sections:
        for sect in ini._sections.keys():
            if varReplace(sect, repo.yumvar) == repo.id:
                section_id = sect

    # Updated the ConfigParser with the changed values
    cfgOptions = repo.cfg.options(repo.id)
    for name,value in repo.iteritems():
        if value is None: # Proxy
            continue

        if only is not None and name not in only:
            continue

        option = repo.optionobj(name)
        ovalue = option.tostring(value)
        #  If the value is the same, but just interpreted ... when we don't want
        # to keep the interpreted values.
        if (name in ini[section_id] and
            ovalue == varReplace(ini[section_id][name], repo.yumvar)):
            ovalue = ini[section_id][name]

        if name not in cfgOptions and option.default == value:
            continue

        ini[section_id][name] = ovalue
    fp = open(repo.repofile,"w")
    fp.write(str(ini))
    fp.close()

def logdir_fit(current_logdir):
    return current_logdir if dnf.util.am_i_root() else misc.getCacheDir()

########NEW FILE########
__FILENAME__ = history
# Copyright (C) 2009, 2012-2013  Red Hat, Inc.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
#
# James Antill <james@fedoraproject.org>

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.i18n import _, ucd
import time
import os, os.path
import glob

from .sqlutils import sqlite, executeSQL, sql_esc_glob
from . import misc as misc
import dnf.arch
import dnf.exceptions
import dnf.rpmUtils.miscutils
import dnf.i18n
import functools


#  Cut over for when we should just give up and load everything.
#  The main problem here is not so much SQLite dying (although that happens
# at large values: http://sqlite.org/limits.html#max_variable_number) but that
# but SQLite going really slow when it gets medium sized values (much slower
# than just loading everything and filtering it in python).
PATTERNS_MAX = 8
#  We have another value here because name is indexed and sqlite is _much_
# faster even at large numbers of patterns.
PATTERNS_INDEXED_MAX = 128

def _setupHistorySearchSQL(patterns=None, ignore_case=False):
    """Setup need_full and patterns for _yieldSQLDataList, also see if
       we can get away with just using searchNames(). """

    if patterns is None:
        patterns = []

    fields = ['name', 'sql_nameArch', 'sql_nameVerRelArch',
              'sql_nameVer', 'sql_nameVerRel',
              'sql_envra', 'sql_nevra']
    need_full = False
    for pat in patterns:
        if misc.re_full_search_needed(pat):
            need_full = True
            break

    pat_max = PATTERNS_MAX
    if not need_full:
        fields = ['name']
        pat_max = PATTERNS_INDEXED_MAX
    if len(patterns) > pat_max:
        patterns = []
    if ignore_case:
        patterns = sql_esc_glob(patterns)
    else:
        tmp = []
        need_glob = False
        for pat in patterns:
            if misc.re_glob(pat):
                tmp.append((pat, 'glob'))
                need_glob = True
            else:
                tmp.append((pat, '='))
        if not need_full and not need_glob and patterns:
            return (need_full, patterns, fields, True)
        patterns = tmp
    return (need_full, patterns, fields, False)

class _YumHistPackageYumDB(object):
    """ Class to pretend to be yumdb_info for history packages. """

    def __init__(self, pkg):
        self._pkg = pkg

    _valid_yumdb_keys = set(["command_line",
                             "from_repo", "from_repo_revision",
                             "from_repo_timestamp",
                             "installed_by", "changed_by",
                             "reason", "releasever"])
    def __getattr__(self, attr):
        """ Load yumdb attributes from the history sqlite. """
        pkg = self._pkg
        if attr.startswith('_'):
            raise AttributeError("%s has no yum attribute %s" % (pkg, attr))

        if attr not in self._valid_yumdb_keys:
            raise AttributeError("%s has no yum attribute %s" % (pkg, attr))

        val = pkg._history._load_yumdb_key(pkg, attr)
        if False and val is None:
            raise AttributeError("%s has no yum attribute %s" % (pkg, attr))

        if val is None:
            return None

        val = str(val) or ""
        setattr(self, attr, val)

        return val

    def __contains__(self, attr):
        #  This is faster than __iter__ and it makes things fail in a much more
        # obvious way in weird FS corruption cases like: BZ 593436
        x = self.get(attr)
        return x is not None

    def get(self, attr, default=None):
        """retrieve an add'l data obj"""

        try:
            res = getattr(self, attr)
        except AttributeError:
            return default
        return res

@functools.total_ordering
class YumHistoryPackage(object):

    def __init__(self, name, arch, epoch, version, release, checksum=None,
                 history=None):
        self.name    = name
        self.version = version
        self.release = release
        self.epoch   = epoch
        self.arch    = arch
        self.pkgtup = (self.name, self.arch,
                       self.epoch, self.version, self.release)
        if checksum is None:
            self._checksums = [] # (type, checksum, id(0,1)
        else:
            chk = checksum.split(':')
            self._checksums = [(chk[0], chk[1], 1)] # (type, checksum, id(0,1))
        self.repoid = "<history>"

        self._history = history
        self.yumdb_info = _YumHistPackageYumDB(self)

    _valid_rpmdb_keys = set(["buildtime", "buildhost",
                             "license", "packager",
                             "size", "sourcerpm", "url", "vendor",
                             # ?
                             "committer", "committime"])

    def __le__(self, other):
        """Test whether the *self* is less than or equal to the *other*."""
        ret = self.verCMP(other)
        if ret != 0:
            return ret < 0  # less or grater

        if self.arch != other.arch:
            return self.arch < other.arch  # less or greater

        try:
            self_repoid, other_repoid = self.repoid, other.repoid
        except AttributeError:
            return True  # equal

        if self_repoid == other_repoid:
            return True  # equal

        # We want 'installed' to appear over 'abcd' and 'xyz', so boost that
        if self_repoid == 'installed':
            return False  # greater
        if other_repoid == 'installed':
            return True  # less

        return self_repoid < other_repoid  # less or grater

    @staticmethod
    def __comparePoEVR(po1, po2):
        """
        Compare two Package or PackageEVR objects.
        """
        (e1, v1, r1) = (po1.epoch, po1.version, po1.release)
        (e2, v2, r2) = (po2.epoch, po2.version, po2.release)
        return dnf.rpmUtils.miscutils.compareEVR((e1, v1, r1), (e2, v2, r2))

    def __eq__(self, other):
        """ Compare packages for yes/no equality, includes everything in the
            UI package comparison. """
        if not other:
            return False
        if not hasattr(other, 'pkgtup') or self.pkgtup != other.pkgtup:
            return False
        if hasattr(self, 'repoid') and hasattr(other, 'repoid'):
            if self.repoid != other.repoid:
                return False
        return True

    def __getattr__(self, attr):
        """ Load rpmdb attributes from the history sqlite. """
        if attr.startswith('_'):
            raise AttributeError("%s has no attribute %s" % (self, attr))

        if attr not in self._valid_rpmdb_keys:
            raise AttributeError("%s has no attribute %s" % (self, attr))

        val = self._history._load_rpmdb_key(self, attr)
        if False and val is None:
            raise AttributeError("%s has no attribute %s" % (self, attr))

        if val is None:
            return None

        val = str(val) or ""
        setattr(self, attr, val)

        return val

    def __ne__(self, other):
        return not (self == other)

    def __repr__(self):
        return ("<%s : %s (%s)>" %
                (self.__class__.__name__, str(self), hex(id(self))))

    def __str__(self):
        return self.ui_envra

    @property
    def envra(self):
        return ('%s:%s-%s-%s.%s' %
                (self.epoch, self.name, self.version, self.release, self.arch))

    @property
    def nevra(self):
        return ('%s-%s:%s-%s.%s' %
                (self.name, self.epoch, self.version, self.release, self.arch))

    @property
    def nvra(self):
        return ('%s-%s-%s.%s' %
                (self.name, self.version, self.release, self.arch))

    def returnIdSum(self):
        for (csumtype, csum, csumid) in self._checksums:
            if csumid:
                return (csumtype, csum)

    @property
    def ui_envra(self):
        if self.epoch == '0':
            return self.nvra
        else:
            return self.envra

    def _ui_from_repo(self):
        """ This reports the repo the package is from, we integrate YUMDB info.
            for RPM packages so a package from "fedora" that is installed has a
            ui_from_repo of "@fedora". Note that, esp. with the --releasever
            option, "fedora" or "rawhide" isn't authoritive.
            So we also check against the current releasever and if it is
            different we also print the YUMDB releasever. This means that
            installing from F12 fedora, while running F12, would report as
            "@fedora/13". """
        if 'from_repo' in self.yumdb_info:
            self._history.releasever
            end = ''
            if (self._history.releasever is not None and
                'releasever' in self.yumdb_info and
                self.yumdb_info.releasever != self._history.releasever):
                end = '/' + self.yumdb_info.releasever
            return '@' + self.yumdb_info.from_repo + end
        return self.repoid
    ui_from_repo = property(fget=lambda self: self._ui_from_repo())

    @property
    def ui_nevra(self):
        if self.epoch == '0':
            return self.nvra
        else:
            return self.nevra

    def verCMP(self, other):
        """ Compare package to another one, only rpm-version ordering. """
        if not other:
            return 1
        if self.name != other.name:
            return -1 if self.name < other.name else +1
        return self.__comparePoEVR(self, other)


class YumHistoryPackageState(YumHistoryPackage):
    def __init__(self, name,arch, epoch,version,release, state, checksum=None,
                 history=None):
        YumHistoryPackage.__init__(self, name,arch, epoch,version,release,
                                   checksum, history)
        self.done  = None
        self.state = state


class YumHistoryRpmdbProblem(object):
    """ Class representing an rpmdb problem that existed at the time of the
        transaction. """

    def __init__(self, history, rpid, problem, text):
        self._history = history

        self.rpid = rpid
        self.problem = problem
        self.text = text

        self._loaded_P = None

    def __cmp__(self, other):
        if other is None:
            return 1
        ret = cmp(self.problem, other.problem)
        if ret: return -ret
        ret = cmp(self.rpid, other.rpid)
        return ret

    def _getProbPkgs(self):
        if self._loaded_P is None:
            self._loaded_P = sorted(self._history._old_prob_pkgs(self.rpid))
        return self._loaded_P

    packages = property(fget=lambda self: self._getProbPkgs())


class YumHistoryTransaction(object):
    """ Holder for a history transaction. """

    def __init__(self, history, row):
        self._history = history

        self.tid              = row[0]
        self.beg_timestamp    = row[1]
        self.beg_rpmdbversion = row[2]
        self.end_timestamp    = row[3]
        self.end_rpmdbversion = row[4]
        self.loginuid         = row[5]
        self.return_code      = row[6]

        self._loaded_TW = None
        self._loaded_TD = None
        self._loaded_TS = None

        self._loaded_PROB = None

        self._have_loaded_CMD = False # cmdline can validly be None
        self._loaded_CMD = None

        self._loaded_ER = None
        self._loaded_OT = None

        self.altered_lt_rpmdb = None
        self.altered_gt_rpmdb = None

    def __cmp__(self, other):
        if other is None:
            return 1
        ret = cmp(self.beg_timestamp, other.beg_timestamp)
        if ret: return -ret
        ret = cmp(self.end_timestamp, other.end_timestamp)
        if ret: return ret
        ret = cmp(self.tid, other.tid)
        return -ret

    def _getTransWith(self):
        if self._loaded_TW is None:
            self._loaded_TW = sorted(self._history._old_with_pkgs(self.tid))
        return self._loaded_TW
    def _getTransData(self):
        if self._loaded_TD is None:
            self._loaded_TD = sorted(self._history._old_data_pkgs(self.tid))
        return self._loaded_TD
    def _getTransSkip(self):
        if self._loaded_TS is None:
            self._loaded_TS = sorted(self._history._old_skip_pkgs(self.tid))
        return self._loaded_TS

    trans_with = property(fget=lambda self: self._getTransWith())
    trans_data = property(fget=lambda self: self._getTransData())
    trans_skip = property(fget=lambda self: self._getTransSkip())

    def _getProblems(self):
        if self._loaded_PROB is None:
            self._loaded_PROB = sorted(self._history._old_problems(self.tid))
        return self._loaded_PROB

    rpmdb_problems = property(fget=lambda self: self._getProblems())

    def _getCmdline(self):
        if not self._have_loaded_CMD:
            self._have_loaded_CMD = True
            self._loaded_CMD = self._history._old_cmdline(self.tid)
        return self._loaded_CMD

    cmdline = property(fget=lambda self: self._getCmdline())

    def _getErrors(self):
        if self._loaded_ER is None:
            self._loaded_ER = self._history._load_errors(self.tid)
        return self._loaded_ER
    def _getOutput(self):
        if self._loaded_OT is None:
            self._loaded_OT = self._history._load_output(self.tid)
        return self._loaded_OT

    errors     = property(fget=lambda self: self._getErrors())
    output     = property(fget=lambda self: self._getOutput())

class YumMergedHistoryTransaction(YumHistoryTransaction):
    def __init__(self, obj):
        self._merged_tids = set([obj.tid])
        self._merged_objs = [obj]

        self.beg_timestamp    = obj.beg_timestamp
        self.beg_rpmdbversion = obj.beg_rpmdbversion
        self.end_timestamp    = obj.end_timestamp
        self.end_rpmdbversion = obj.end_rpmdbversion

        self._loaded_TW = None
        self._loaded_TD = None
        #  Hack, this is difficult ... not sure if we want to list everything
        # that was skipped. Just those things which were skipped and then not
        # updated later ... or nothing. Nothing is much easier.
        self._loaded_TS = []

        self._loaded_PROB = None

        self._have_loaded_CMD = False # cmdline can validly be None
        self._loaded_CMD = None

        self._loaded_ER = None
        self._loaded_OT = None

        self.altered_lt_rpmdb = None
        self.altered_gt_rpmdb = None

    def _getAllTids(self):
        return sorted(self._merged_tids)
    tid         = property(fget=lambda self: self._getAllTids())

    def _getLoginUIDs(self):
        ret = set((tid.loginuid for tid in self._merged_objs))
        if len(ret) == 1:
            return list(ret)[0]
        return sorted(ret)
    loginuid    = property(fget=lambda self: self._getLoginUIDs())

    def _getReturnCodes(self):
        ret_codes = set((tid.return_code for tid in self._merged_objs))
        if len(ret_codes) == 1 and 0 in ret_codes:
            return 0
        if 0 in ret_codes:
            ret_codes.remove(0)
        return sorted(ret_codes)
    return_code = property(fget=lambda self: self._getReturnCodes())

    def _getTransWith(self):
        ret = []
        filt = set()
        for obj in self._merged_objs:
            for pkg in obj.trans_with:
                if pkg.pkgtup in filt:
                    continue
                filt.add(pkg.pkgtup)
                ret.append(pkg)
        return sorted(ret)

    # This is the real tricky bit, we want to "merge" so that:
    #     pkgA-1 => pkgA-2
    #     pkgA-2 => pkgA-3
    #     pkgB-1 => pkgB-2
    #     pkgB-2 => pkgB-1
    # ...becomes:
    #     pkgA-1 => pkgA-3
    #     pkgB-1 => pkgB-1 (reinstall)
    # ...note that we just give up if "impossible" things happen, Eg.
    #     pkgA-1 => pkgA-2
    #     pkgA-4 => pkgA-5
    @staticmethod
    def _p2sk(pkg, state=None):
        """ Take a pkg and return the key for it's state lookup. """
        if state is None:
            state = pkg.state
        #  Arch is needed so multilib. works, dito. basearch() -- (so .i586
        # => .i686 moves are seen)
        return (pkg.name, dnf.arch.basearch(pkg.arch), state)

    @staticmethod
    def _list2dict(pkgs):
        pkgtup2pkg   = {}
        pkgstate2pkg = {}
        for pkg in pkgs:
            key = YumMergedHistoryTransaction._p2sk(pkg)
            pkgtup2pkg[pkg.pkgtup] = pkg
            pkgstate2pkg[key]      = pkg
        return pkgtup2pkg, pkgstate2pkg
    @staticmethod
    def _conv_pkg_state(pkg, state):
        npkg = YumHistoryPackageState(pkg.name, pkg.arch,
                                      pkg.epoch,pkg.version,pkg.release, state,
                                      history=pkg._history)
        npkg._checksums = pkg._checksums
        npkg.done = pkg.done
        if npkg.state in dnf.history.INSTALLING_STATES:
            npkg.state_installed = True
        if npkg.state in dnf.history.REMOVING_STATES:
            npkg.state_installed = False
        return npkg
    @staticmethod
    def _get_pkg(sk, pkgstate2pkg):
        if type(sk) != type((0,1)):
            sk = YumMergedHistoryTransaction._p2sk(sk)
        if sk not in pkgstate2pkg:
            return None
        return pkgstate2pkg[sk]
    def _move_pkg(self, sk, nstate, pkgtup2pkg, pkgstate2pkg):
        xpkg = self._get_pkg(sk, pkgstate2pkg)
        if xpkg is None:
            return
        del pkgstate2pkg[self._p2sk(xpkg)]
        xpkg = self._conv_pkg_state(xpkg, nstate)
        pkgtup2pkg[xpkg.pkgtup] = xpkg
        pkgstate2pkg[self._p2sk(xpkg)] = xpkg

    def _getTransData(self):
        def _get_pkg_f(sk):
            return self._get_pkg(sk, fpkgstate2pkg)
        def _get_pkg_n(sk):
            return self._get_pkg(sk, npkgstate2pkg)
        def _move_pkg_f(sk, nstate):
            self._move_pkg(sk, nstate, fpkgtup2pkg, fpkgstate2pkg)
        def _move_pkg_n(sk, nstate):
            self._move_pkg(sk, nstate, npkgtup2pkg, npkgstate2pkg)
        def _del1_n(pkg):
            del npkgtup2pkg[pkg.pkgtup]
            key = self._p2sk(pkg)
            if key in npkgstate2pkg: # For broken rpmdbv's and installonly
                del npkgstate2pkg[key]
        def _del1_f(pkg):
            del fpkgtup2pkg[pkg.pkgtup]
            key = self._p2sk(pkg)
            if key in fpkgstate2pkg: # For broken rpmdbv's and installonly
                del fpkgstate2pkg[key]
        def _del2(fpkg, npkg):
            assert fpkg.pkgtup == npkg.pkgtup
            _del1_f(fpkg)
            _del1_n(npkg)
        fpkgtup2pkg   = {}
        fpkgstate2pkg = {}
        #  We need to go from oldest to newest here, so we can see what happened
        # in the correct chronological order.
        for obj in self._merged_objs:
            npkgtup2pkg, npkgstate2pkg = self._list2dict(obj.trans_data)

            # Handle Erase => Install, as update/reinstall/downgrade
            for key in list(fpkgstate2pkg.keys()):
                (name, arch, state) = key
                if state not in  ('Obsoleted', 'Erase'):
                    continue
                fpkg = fpkgstate2pkg[key]
                for xstate in ('Install', 'True-Install', 'Dep-Install',
                               'Obsoleting'):
                    npkg = _get_pkg_n(self._p2sk(fpkg, xstate))
                    if npkg is not None:
                        break
                else:
                    continue

                if False: pass
                elif fpkg > npkg:
                    _move_pkg_f(fpkg, 'Downgraded')
                    if xstate != 'Obsoleting':
                        _move_pkg_n(npkg, 'Downgrade')
                elif fpkg < npkg:
                    _move_pkg_f(fpkg, 'Updated')
                    if xstate != 'Obsoleting':
                        _move_pkg_n(npkg, 'Update')
                else:
                    _del1_f(fpkg)
                    if xstate != 'Obsoleting':
                        _move_pkg_n(npkg, 'Reinstall')

            sametups = set(npkgtup2pkg.keys()).intersection(fpkgtup2pkg.keys())
            for pkgtup in sametups:
                if pkgtup not in fpkgtup2pkg or pkgtup not in npkgtup2pkg:
                    continue
                fpkg = fpkgtup2pkg[pkgtup]
                npkg = npkgtup2pkg[pkgtup]
                if False: pass
                elif fpkg.state == 'Reinstall':
                    if npkg.state in ('Reinstall', 'Erase', 'Obsoleted',
                                      'Downgraded', 'Updated'):
                        _del1_f(fpkg)
                elif fpkg.state in ('Obsoleted', 'Erase'):
                    #  Should be covered by above loop which deals with
                    # all goood state changes.
                    good_states = ('Install', 'True-Install', 'Dep-Install',
                                   'Obsoleting')
                    assert npkg.state not in good_states

                elif fpkg.state in ('Install', 'True-Install', 'Dep-Install'):
                    if False: pass
                    elif npkg.state in ('Erase', 'Obsoleted'):
                        _del2(fpkg, npkg)
                    elif npkg.state == 'Updated':
                        _del2(fpkg, npkg)
                        #  Move '*Install' state along to newer pkg. (not for
                        # obsoletes).
                        _move_pkg_n(self._p2sk(fpkg, 'Update'), fpkg.state)
                    elif npkg.state == 'Downgraded':
                        _del2(fpkg, npkg)
                        #  Move '*Install' state along to newer pkg. (not for
                        # obsoletes).
                        _move_pkg_n(self._p2sk(fpkg, 'Downgrade'), fpkg.state)

                elif fpkg.state in ('Downgrade', 'Update', 'Obsoleting'):
                    if False: pass
                    elif npkg.state == 'Reinstall':
                        _del1_n(npkg)
                    elif npkg.state in ('Erase', 'Obsoleted'):
                        _del2(fpkg, npkg)

                        # Move 'Erase'/'Obsoleted' state to orig. pkg.
                        _move_pkg_f(self._p2sk(fpkg, 'Updated'),    npkg.state)
                        _move_pkg_f(self._p2sk(fpkg, 'Downgraded'), npkg.state)

                    elif npkg.state in ('Downgraded', 'Updated'):
                        xfpkg = _get_pkg_f(self._p2sk(fpkg, 'Updated'))
                        if xfpkg is None:
                            xfpkg = _get_pkg_f(self._p2sk(fpkg, 'Downgraded'))
                        if xfpkg is None:
                            if fpkg.state != 'Obsoleting':
                                continue
                            # Was an Install*/Reinstall with Obsoletes
                            xfpkg = fpkg
                        xnpkg = _get_pkg_n(self._p2sk(npkg, 'Update'))
                        if xnpkg is None:
                            xnpkg = _get_pkg_n(self._p2sk(npkg, 'Downgrade'))
                        if xnpkg is None:
                            xnpkg = _get_pkg_n(self._p2sk(npkg, 'Obsoleting'))
                        if xnpkg is None:
                            continue

                        #  Now we have 4 pkgs, f1, f2, n1, n2, and 3 pkgtups
                        # f2.pkgtup == n1.pkgtup. So we need to find out if
                        # f1 => n2 is an Update or a Downgrade.
                        _del2(fpkg, npkg)
                        if xfpkg == xnpkg:
                            nfstate = 'Reinstall'
                            if 'Obsoleting' in (fpkg.state, xnpkg.state):
                                nfstate = 'Obsoleting'
                            if xfpkg != fpkg:
                                _move_pkg_f(xfpkg, nfstate)
                            _del1_n(xnpkg)
                        elif xfpkg < xnpkg:
                            # Update...
                            nfstate = 'Updated'
                            nnstate = 'Update'
                            if 'Obsoleting' in (fpkg.state, xnpkg.state):
                                nnstate = 'Obsoleting'
                            if xfpkg != fpkg:
                                _move_pkg_f(xfpkg, nfstate)
                            _move_pkg_n(xnpkg, nnstate)
                        else:
                            # Downgrade...
                            nfstate = 'Downgraded'
                            nnstate = 'Downgrade'
                            if 'Obsoleting' in (fpkg.state, xnpkg.state):
                                nnstate = 'Obsoleting'
                            if xfpkg != fpkg:
                                _move_pkg_f(xfpkg, nfstate)
                            _move_pkg_n(xnpkg, nnstate)

            for x in npkgtup2pkg:
                fpkgtup2pkg[x] = npkgtup2pkg[x]
            for x in npkgstate2pkg:
                fpkgstate2pkg[x] = npkgstate2pkg[x]
        return sorted(fpkgtup2pkg.values())

    def _getProblems(self):
        probs = set()
        for tid in self._merged_objs:
            for prob in tid.rpmdb_problems:
                probs.add(prob)
        return sorted(probs)

    def _getCmdline(self):
        cmdlines = []
        for tid in self._merged_objs:
            if not tid.cmdline:
                continue
            if cmdlines and cmdlines[-1] == tid.cmdline:
                continue
            cmdlines.append(tid.cmdline)
        if not cmdlines:
            return None
        return cmdlines

    def _getErrors(self):
        ret = []
        for obj in self._merged_objs:
            ret.extend(obj.errors)
        return ret
    def _getOutput(self):
        ret = []
        for obj in self._merged_objs:
            ret.extend(obj.output)
        return ret

    def merge(self, obj):
        if obj.tid in self._merged_tids:
            return # Already done, signal an error?

        self._merged_tids.add(obj.tid)
        self._merged_objs.append(obj)
        # Oldest first...
        self._merged_objs.sort(reverse=True)

        if self.beg_timestamp > obj.beg_timestamp:
            self.beg_timestamp    = obj.beg_timestamp
            self.beg_rpmdbversion = obj.beg_rpmdbversion
        if self.end_timestamp < obj.end_timestamp:
            self.end_timestamp    = obj.end_timestamp
            self.end_rpmdbversion = obj.end_rpmdbversion


class YumHistory(object):
    """ API for accessing the history sqlite data. """

    def __init__(self, db_path, yumdb, root='/', releasever=None):
        self._conn = None

        self.conf = misc.GenericHolder()
        if not os.path.normpath(db_path).startswith(root):
            self.conf.db_path  = os.path.normpath(root + '/' + db_path)
        else:
            self.conf.db_path = os.path.normpath('/' + db_path)
        self.conf.writable = False
        self.conf.readable = True
        self.yumdb = yumdb

        self.releasever = releasever

        if not os.path.exists(self.conf.db_path):
            try:
                os.makedirs(self.conf.db_path)
            except (IOError, OSError) as e:
                error = dnf.i18n.ucd(e)
                msg = _("Unable to initialize yumdb history: %s") % error
                raise dnf.exceptions.Error(msg)
            else:
                self.conf.writable = True
        else:
            if os.access(self.conf.db_path, os.W_OK):
                self.conf.writable = True

        DBs = glob.glob('%s/history-*-*-*.sqlite' % self.conf.db_path)
        self._db_file = None
        for d in reversed(sorted(DBs)):
            fname = os.path.basename(d)
            fname = fname[len("history-"):-len(".sqlite")]
            pieces = fname.split('-', 4)
            if len(pieces) != 3:
                continue
            try:
                for piece in pieces:
                    int(piece)
            except ValueError:
                continue

            self._db_date = '%s-%s-%s' % (pieces[0], pieces[1], pieces[2])
            self._db_file = d
            break

        if self._db_file is None:
            self._create_db_file()

        # make an addon path for where we're going to stick
        # random additional history info - probably from plugins and what-not
        self.conf.addon_path = self.conf.db_path + '/' + self._db_date
        if not os.path.exists(self.conf.addon_path):
            try:
                os.makedirs(self.conf.addon_path)
            except (IOError, OSError) as e:
                # some sort of useful thing here? A warning?
                return
        else:
            if os.access(self.conf.addon_path, os.W_OK):
                self.conf.writable = True


    def __del__(self):
        self.close()

    def _get_cursor(self):
        if self._conn is None:
            if not self.conf.readable:
                return None

            try:
                self._conn = sqlite.connect(self._db_file)
            except (sqlite.OperationalError, sqlite.DatabaseError):
                self.conf.readable = False
                return None

            #  Note that this is required due to changing the history DB in the
            # callback for removed txmbrs ... which happens inside the chroot,
            # as against all our other access which is outside the chroot. So
            # we need sqlite to not open the journal.
            #  In theory this sucks, as history could be shared. In reality
            # it's deep yum stuff and there should only be one yum.
            executeSQL(self._conn.cursor(), "PRAGMA locking_mode = EXCLUSIVE")

        return self._conn.cursor()
    def _commit(self):
        return self._conn.commit()
    def _rollback(self):
        return self._conn.rollback()

    def close(self):
        if self._conn is not None:
            self._conn.close()
            self._conn = None

    def _pkgtup2pid(self, pkgtup, checksum=None, create=True):
        cur = self._get_cursor()
        executeSQL(cur, """SELECT pkgtupid, checksum FROM pkgtups
                           WHERE name=? AND arch=? AND
                                 epoch=? AND version=? AND release=?""", pkgtup)
        for sql_pkgtupid, sql_checksum in cur:
            if checksum is None and sql_checksum is None:
                return sql_pkgtupid
            if checksum is None:
                continue
            if sql_checksum is None:
                continue
            if checksum == sql_checksum:
                return sql_pkgtupid

        if not create:
            return None

        pkgtup = map(ucd, pkgtup)
        (n,a,e,v,r) = pkgtup
        if checksum is not None:
            res = executeSQL(cur,
                             """INSERT INTO pkgtups
                                (name, arch, epoch, version, release, checksum)
                                VALUES (?, ?, ?, ?, ?, ?)""", (n,a,e,v,r,
                                                               checksum))
        else:
            res = executeSQL(cur,
                             """INSERT INTO pkgtups
                                (name, arch, epoch, version, release)
                                VALUES (?, ?, ?, ?, ?)""", (n,a,e,v,r))
        return cur.lastrowid
    def _apkg2pid(self, po, create=True):
        csum = po.returnIdSum()
        if csum is not None:
            csum = "%s:%s" % (str(csum[0]), str(csum[1]))
        return self._pkgtup2pid(po.pkgtup, csum, create)
    def _ipkg2pid(self, po, create=True):
        csum = None
        yumdb = self.yumdb.get_package(po)
        if 'checksum_type' in yumdb and 'checksum_data' in yumdb:
            csum = "%s:%s" % (yumdb.checksum_type, yumdb.checksum_data)
        return self._pkgtup2pid(po.pkgtup, csum, create)
    def _hpkg2pid(self, po, create=False):
        return self._apkg2pid(po, create)

    def pkg2pid(self, po, create=True):
        if isinstance(po, YumHistoryPackage):
            return self._hpkg2pid(po, create)
        if po.from_system:
            return self._ipkg2pid(po, create)
        return self._apkg2pid(po, create)

    def trans_with_pid(self, pid):
        cur = self._get_cursor()
        if cur is None:
            return None
        res = executeSQL(cur,
                         """INSERT INTO trans_with_pkgs
                         (tid, pkgtupid)
                         VALUES (?, ?)""", (self._tid, pid))
        return cur.lastrowid

    def trans_skip_pid(self, pid):
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_2():
            return None

        res = executeSQL(cur,
                         """INSERT INTO trans_skip_pkgs
                         (tid, pkgtupid)
                         VALUES (?, ?)""", (self._tid, pid))
        return cur.lastrowid

    def trans_data_pid_beg(self, pid, state):
        assert state is not None
        if not hasattr(self, '_tid') or state is None:
            return # Not configured to run
        cur = self._get_cursor()
        if cur is None:
            return # Should never happen, due to above
        res = executeSQL(cur,
                         """INSERT INTO trans_data_pkgs
                         (tid, pkgtupid, state)
                         VALUES (?, ?, ?)""", (self._tid, pid, state))
        return cur.lastrowid
    def trans_data_pid_end(self, pid, state):
        # State can be none here, Eg. TS_FAILED from rpmtrans
        if not hasattr(self, '_tid') or state is None:
            return # Not configured to run

        cur = self._get_cursor()
        if cur is None:
            return # Should never happen, due to above
        res = executeSQL(cur,
                         """UPDATE trans_data_pkgs SET done = ?
                         WHERE tid = ? AND pkgtupid = ? AND state = ?
                         """, ('TRUE', self._tid, pid, state))
        self._commit()

    def _trans_rpmdb_problem(self, problem):
        if not hasattr(self, '_tid'):
            return # Not configured to run
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_2():
            return None
        # str(problem) doesn't work if problem contains unicode(),
        uproblem = ucd(problem)
        res = executeSQL(cur,
                         """INSERT INTO trans_rpmdb_problems
                         (tid, problem, msg)
                         VALUES (?, ?, ?)""", (self._tid,
                                               problem.problem,
                                               uproblem))
        rpid = cur.lastrowid

        if not rpid:
            return rpid

        pkgs = {}
        pkg = problem.pkg
        pkgs[pkg.pkgtup] = pkg
        if problem.problem == 'conflicts':
            for pkg in problem.conflicts:
                pkgs[pkg.pkgtup] = pkg
        if problem.problem == 'duplicates':
            pkgs[problem.duplicate.pkgtup] = problem.duplicate

        for pkg in pkgs.values():
            pid = self.pkg2pid(pkg)
            if pkg.pkgtup == problem.pkg.pkgtup:
                main = 'TRUE'
            else:
                main = 'FALSE'
            res = executeSQL(cur,
                             """INSERT INTO trans_prob_pkgs
                             (rpid, pkgtupid, main)
                             VALUES (?, ?, ?)""", (rpid, pid, main))

        return rpid

    def _trans_cmdline(self, cmdline):
        if not hasattr(self, '_tid'):
            return # Not configured to run
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_2():
            return None
        res = executeSQL(cur,
                         """INSERT INTO trans_cmdline
                         (tid, cmdline)
                         VALUES (?, ?)""", (self._tid, ucd(cmdline)))
        return cur.lastrowid

    def beg(self, rpmdb_version, using_pkgs, tsis, skip_packages=[],
            rpmdb_problems=[], cmdline=None):
        cur = self._get_cursor()
        if cur is None:
            return
        res = executeSQL(cur,
                         """INSERT INTO trans_beg
                            (timestamp, rpmdb_version, loginuid)
                            VALUES (?, ?, ?)""", (int(time.time()),
                                                    str(rpmdb_version),
                                                    misc.getloginuid()))
        self._tid = cur.lastrowid

        for pkg in using_pkgs:
            pid = self._ipkg2pid(pkg)
            self.trans_with_pid(pid)

        for tsi in tsis:
            for (pkg, state) in tsi.history_iterator():
                pid   = self.pkg2pid(pkg)
                self.trans_data_pid_beg(pid, state)

        for pkg in skip_packages:
            pid   = self.pkg2pid(pkg)
            self.trans_skip_pid(pid)

        for problem in rpmdb_problems:
            self._trans_rpmdb_problem(problem)

        if cmdline:
            self._trans_cmdline(cmdline)

        self._commit()

    def _log_errors(self, errors):
        cur = self._get_cursor()
        if cur is None:
            return
        for error in errors:
            error = ucd(error)
            executeSQL(cur,
                       """INSERT INTO trans_error
                          (tid, msg) VALUES (?, ?)""", (self._tid, error))
        self._commit()

    def log_scriptlet_output(self, msg):
        if msg is None or not hasattr(self, '_tid'):
            return # Not configured to run

        cur = self._get_cursor()
        if cur is None:
            return # Should never happen, due to above
        for error in msg.splitlines():
            error = ucd(error)
            executeSQL(cur,
                       """INSERT INTO trans_script_stdout
                          (tid, line) VALUES (?, ?)""", (self._tid, error))
        self._commit()

    def _load_errors(self, tid):
        cur = self._get_cursor()
        executeSQL(cur,
                   """SELECT msg FROM trans_error
                      WHERE tid = ?
                      ORDER BY mid ASC""", (tid,))
        ret = []
        for row in cur:
            ret.append(row[0])
        return ret

    def _load_output(self, tid):
        cur = self._get_cursor()
        executeSQL(cur,
                   """SELECT line FROM trans_script_stdout
                      WHERE tid = ?
                      ORDER BY lid ASC""", (tid,))
        ret = []
        for row in cur:
            ret.append(row[0])
        return ret

    def end(self, rpmdb_version, return_code, errors=None):
        assert return_code or not errors
        if not hasattr(self, '_tid'):
            return # Failed at beg() time
        cur = self._get_cursor()
        if cur is None:
            return # Should never happen, due to above
        res = executeSQL(cur,
                         """INSERT INTO trans_end
                            (tid, timestamp, rpmdb_version, return_code)
                            VALUES (?, ?, ?, ?)""", (self._tid,int(time.time()),
                                                     str(rpmdb_version),
                                                     return_code))
        self._commit()
        if not return_code:
            #  Simple hack, if the transaction finished. Note that this
            # catches the erase cases (as we still don't get pkgtups for them),
            # Eg. Updated elements.
            executeSQL(cur,
                       """UPDATE trans_data_pkgs SET done = ?
                          WHERE tid = ?""", ('TRUE', self._tid,))
            self._commit()
        if errors is not None:
            self._log_errors(errors)
        del self._tid

    def write_addon_data(self, dataname, data):
        """append data to an arbitrary-named file in the history
           addon_path/transaction id location,
           returns True if write succeeded, False if not"""

        if not hasattr(self, '_tid'):
            # maybe we should raise an exception or a warning here?
            return False

        if not dataname:
            return False

        if not data:
            return False

        # make sure the tid dir exists
        tid_dir = self.conf.addon_path + '/' + str(self._tid)

        if self.conf.writable and not os.path.exists(tid_dir):
            try:
                os.makedirs(tid_dir, mode=0o700)
            except (IOError, OSError) as e:
                # emit a warning/raise an exception?
                return False

        # cleanup dataname
        safename = dataname.replace('/', '_')
        data_fn = tid_dir + '/' + safename
        try:
            # open file in append
            fo = open(data_fn, 'wb+')
            # write data
            fo.write(data.encode('utf-8'))
            # flush data
            fo.flush()
            fo.close()
        except (IOError, OSError) as e:
            return False
        # return
        return True

    def return_addon_data(self, tid, item=None):
        hist_and_tid = self.conf.addon_path + '/' + str(tid) + '/'
        addon_info = glob.glob(hist_and_tid + '*')
        addon_names = [ i.replace(hist_and_tid, '') for i in addon_info ]
        if not item:
            return addon_names

        if item not in addon_names:
            # XXX history needs SOME kind of exception, or warning, I think?
            return None

        fo = open(hist_and_tid + item, 'r')
        data = fo.read()
        fo.close()
        return data

    def _old_with_pkgs(self, tid):
        cur = self._get_cursor()
        executeSQL(cur,
                   """SELECT name, arch, epoch, version, release, checksum
                      FROM trans_with_pkgs JOIN pkgtups USING(pkgtupid)
                      WHERE tid = ?
                      ORDER BY name ASC, epoch ASC""", (tid,))
        ret = []
        for row in cur:
            obj = YumHistoryPackage(row[0],row[1],row[2],row[3],row[4], row[5],
                                    history=self)
            ret.append(obj)
        return ret
    def _old_data_pkgs(self, tid, sort=True):
        cur = self._get_cursor()
        sql = """SELECT name, arch, epoch, version, release,
                        checksum, done, state
                 FROM trans_data_pkgs JOIN pkgtups USING(pkgtupid)
                 WHERE tid = ?"""
        if sort:
            sql = " ".join((sql, "ORDER BY name ASC, epoch ASC, state DESC"))
        executeSQL(cur, sql, (tid,))
        ret = []
        for row in cur:
            obj = YumHistoryPackageState(row[0],row[1],row[2],row[3],row[4],
                                         row[7], row[5], history=self)
            obj.done     = row[6] == 'TRUE'
            obj.state_installed = None
            if obj.state in dnf.history.INSTALLING_STATES:
                obj.state_installed = True
            if obj.state in dnf.history.REMOVING_STATES:
                obj.state_installed = False
            ret.append(obj)
        return ret
    def _old_skip_pkgs(self, tid):
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_2():
            return []
        executeSQL(cur,
                   """SELECT name, arch, epoch, version, release, checksum
                      FROM trans_skip_pkgs JOIN pkgtups USING(pkgtupid)
                      WHERE tid = ?
                      ORDER BY name ASC, epoch ASC""", (tid,))
        ret = []
        for row in cur:
            obj = YumHistoryPackage(row[0],row[1],row[2],row[3],row[4], row[5],
                                    history=self)
            ret.append(obj)
        return ret
    def _old_prob_pkgs(self, rpid):
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_2():
            return []
        executeSQL(cur,
                   """SELECT name, arch, epoch, version, release, checksum, main
                      FROM trans_prob_pkgs JOIN pkgtups USING(pkgtupid)
                      WHERE rpid = ?
                      ORDER BY name ASC, epoch ASC""", (rpid,))
        ret = []
        for row in cur:
            obj = YumHistoryPackage(row[0],row[1],row[2],row[3],row[4], row[5],
                                    history=self)
            obj.main = row[6] == 'TRUE'
            ret.append(obj)
        return ret

    def _old_problems(self, tid):
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_2():
            return []
        executeSQL(cur,
                   """SELECT rpid, problem, msg
                      FROM trans_rpmdb_problems
                      WHERE tid = ?
                      ORDER BY problem ASC, rpid ASC""", (tid,))
        ret = []
        for row in cur:
            obj = YumHistoryRpmdbProblem(self, row[0], row[1], row[2])
            ret.append(obj)
        return ret

    def _old_cmdline(self, tid):
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_2():
            return None
        executeSQL(cur,
                   """SELECT cmdline
                      FROM trans_cmdline
                      WHERE tid = ?""", (tid,))
        ret = []
        for row in cur:
            return row[0]
        return None

    def old(self, tids=[], limit=None, complete_transactions_only=False):
        """ Return a list of the last transactions, note that this includes
            partial transactions (ones without an end transaction). """
        cur = self._get_cursor()
        if cur is None:
            return []
        sql =  """SELECT tid,
                         trans_beg.timestamp AS beg_ts,
                         trans_beg.rpmdb_version AS beg_rv,
                         trans_end.timestamp AS end_ts,
                         trans_end.rpmdb_version AS end_rv,
                         loginuid, return_code
                  FROM trans_beg JOIN trans_end USING(tid)"""
        # NOTE: sqlite doesn't do OUTER JOINs ... *sigh*. So we have to do it
        #       ourself.
        if not complete_transactions_only:
            sql =  """SELECT tid,
                             trans_beg.timestamp AS beg_ts,
                             trans_beg.rpmdb_version AS beg_rv,
                             NULL, NULL,
                             loginuid, NULL
                      FROM trans_beg"""
        params = None
        if tids and len(tids) <= PATTERNS_INDEXED_MAX:
            params = tids = list(set(tids))
            sql += " WHERE tid IN (%s)" % ", ".join(['?'] * len(tids))
        #  This relies on the fact that the PRIMARY KEY in sqlite will always
        # increase with each transaction. In theory we can use:
        # ORDER BY beg_ts DESC ... except sometimes people do installs with a
        # system clock that is very broken, and using that screws them forever.
        sql += " ORDER BY tid DESC"
        if limit is not None:
            sql += " LIMIT " + str(limit)
        executeSQL(cur, sql, params)
        ret = []
        tid2obj = {}
        for row in cur:
            if tids and len(tids) > PATTERNS_INDEXED_MAX:
                if row[0] not in tids:
                    continue
            obj = YumHistoryTransaction(self, row)
            tid2obj[row[0]] = obj
            ret.append(obj)

        sql =  """SELECT tid,
                         trans_end.timestamp AS end_ts,
                         trans_end.rpmdb_version AS end_rv,
                         return_code
                  FROM trans_end"""
        params = list(tid2obj.keys())
        if len(params) > PATTERNS_INDEXED_MAX:
            executeSQL(cur, sql)
        else:
            sql += " WHERE tid IN (%s)" % ", ".join(['?'] * len(params))
            executeSQL(cur, sql, params)
        for row in cur:
            if row[0] not in tid2obj:
                continue
            tid2obj[row[0]].end_timestamp    = row[1]
            tid2obj[row[0]].end_rpmdbversion = row[2]
            tid2obj[row[0]].return_code      = row[3]

        # Go through backwards, and see if the rpmdb versions match
        las = None
        for obj in reversed(ret):
            cur_rv = obj.beg_rpmdbversion
            las_rv = None
            if las is not None:
                las_rv = las.end_rpmdbversion
            if las_rv is None or cur_rv is None or (las.tid + 1) != obj.tid:
                pass
            elif las_rv != cur_rv:
                obj.altered_lt_rpmdb = True
                las.altered_gt_rpmdb = True
            else:
                obj.altered_lt_rpmdb = False
                las.altered_gt_rpmdb = False
            las = obj

        return ret

    def last(self, complete_transactions_only=True):
        """ This is the last full transaction. So any incomplete transactions
            do not count, by default. """
        ret = self.old([], 1, complete_transactions_only)
        if not ret:
            return None
        assert len(ret) == 1
        return ret[0]

    def _load_anydb_key(self, pkg, db, attr):
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_3():
            return None

        pid = self.pkg2pid(pkg, create=False)
        if pid is None:
            return None

        sql = """SELECT %(db)sdb_val FROM pkg_%(db)sdb
                  WHERE pkgtupid=? and %(db)sdb_key=? """ % {'db' : db}
        executeSQL(cur, sql, (pid, attr))
        for row in cur:
            return row[0]

        return None

    def _load_rpmdb_key(self, pkg, attr):
        return self._load_anydb_key(pkg, "rpm", attr)
    def _load_yumdb_key(self, pkg, attr):
        return self._load_anydb_key(pkg, "yum", attr)

    def _save_anydb_key(self, pkg, db, attr, val):
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_3():
            return None

        pid = self.pkg2pid(pkg, create=False)
        if pid is None:
            return None

        sql = """INSERT INTO pkg_%(db)sdb (pkgtupid, %(db)sdb_key, %(db)sdb_val)
                        VALUES (?, ?, ?)""" % {'db' : db}
        executeSQL(cur, sql, (pid, attr, ucd(val)))

        return cur.lastrowid

    def _save_rpmdb(self, ipkg):
        """ Save all the data for rpmdb for this installed pkg, assumes
            there is no data currently. """
        for attr in YumHistoryPackage._valid_rpmdb_keys:
            val = getattr(ipkg, attr, None)
            if val is None:
                continue
            if not self._save_anydb_key(ipkg, "rpm", attr, val):
                return False
        return True

    def _save_yumdb(self, ipkg):
        """ Save all the data for yumdb for this installed pkg, assumes
            there is no data currently. """
        yumdb_info = self.yumdb.get_package(ipkg)
        for attr in _YumHistPackageYumDB._valid_yumdb_keys:
            val = yumdb_info.get(attr)
            if val is None:
                continue
            if not self._save_anydb_key(ipkg, "yum", attr, val):
                return False
        return True

    def _wipe_anydb(self, pkg, db):
        """ Delete all the data for rpmdb/yumdb for this installed pkg. """
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_3():
            return False

        pid = self.pkg2pid(pkg, create=False)
        if pid is None:
            return False

        sql = """DELETE FROM pkg_%(db)sdb WHERE pkgtupid=?""" % {'db' : db}
        executeSQL(cur, sql, (pid,))

        return True

    def sync_alldb(self, ipkg):
        """ Sync. all the data for rpmdb/yumdb for this installed pkg. """
        if not self._wipe_anydb(ipkg, "rpm"):
            return False
        if not (self._wipe_anydb(ipkg, "yum") and
                self._save_rpmdb(ipkg) and
                self._save_yumdb(ipkg)):
            self._rollback()
            return False

        self._commit()
        return True

    def _pkg_stats(self):
        """ Some stats about packages in the DB. """

        ret = {'nevrac' : 0,
               'nevra'  : 0,
               'nevr'   : 0,
               'na'     : 0,
               'rpmdb'  : 0,
               'yumdb'  : 0,
               }
        cur = self._get_cursor()
        if cur is None or not self._update_db_file_3():
            return False

        data = (('nevrac', "COUNT(*)",                      "pkgtups"),
                ('na',     "COUNT(DISTINCT(name || arch))", "pkgtups"),
                ('nevra',"COUNT(DISTINCT(name||version||epoch||release||arch))",
                 "pkgtups"),
                ('nevr',   "COUNT(DISTINCT(name||version||epoch||release))",
                 "pkgtups"),
                ('rpmdb',  "COUNT(DISTINCT(pkgtupid))", "pkg_rpmdb"),
                ('yumdb',  "COUNT(DISTINCT(pkgtupid))", "pkg_yumdb"))

        for key, bsql, esql in data:
            executeSQL(cur, "SELECT %s FROM %s" % (bsql, esql))
            for row in cur:
                ret[key] = row[0]
        return ret

    def _yieldSQLDataList(self, patterns, fields, ignore_case):
        """Yields all the package data for the given params. """

        cur = self._get_cursor()
        qsql = _FULL_PARSE_QUERY_BEG

        pat_sqls = []
        pat_data = []
        for (pattern, rest) in patterns:
            for field in fields:
                if ignore_case:
                    pat_sqls.append("%s LIKE ?%s" % (field, rest))
                else:
                    pat_sqls.append("%s %s ?" % (field, rest))
                pat_data.append(pattern)
        assert pat_sqls

        qsql += " OR ".join(pat_sqls)
        executeSQL(cur, qsql, pat_data)
        for x in cur:
            yield x

    def search(self, patterns, ignore_case=True):
        """ Search for history transactions which contain specified
            packages al. la. "yum list". Returns transaction ids. """
        # Search packages ... kind of sucks that it's search not list, pkglist?

        cur = self._get_cursor()
        if cur is None:
            return set()

        data = _setupHistorySearchSQL(patterns, ignore_case)
        (need_full, npatterns, fields, names) = data

        ret = []
        pkgtupids = set()

        if npatterns:
            for row in self._yieldSQLDataList(npatterns, fields, ignore_case):
                pkgtupids.add(row[0])
        else:
            # Too many patterns, *sigh*
            pat_max = PATTERNS_MAX
            if not need_full:
                pat_max = PATTERNS_INDEXED_MAX
            for npatterns in misc.seq_max_split(patterns, pat_max):
                data = _setupHistorySearchSQL(npatterns, ignore_case)
                (need_full, nps, fields, names) = data
                assert nps
                for row in self._yieldSQLDataList(nps, fields, ignore_case):
                    pkgtupids.add(row[0])

        sql =  """SELECT tid FROM trans_data_pkgs WHERE pkgtupid IN """
        sql += "(%s)" % ",".join(['?'] * len(pkgtupids))
        params = list(pkgtupids)
        tids = set()
        if len(params) > PATTERNS_INDEXED_MAX:
            executeSQL(cur, """SELECT tid FROM trans_data_pkgs""")
            for row in cur:
                if row[0] in params:
                    tids.add(row[0])
            return tids
        if not params:
            return tids
        executeSQL(cur, sql, params)
        for row in cur:
            tids.add(row[0])
        return tids

    _update_ops_3 = ['''\
\
 CREATE TABLE pkg_rpmdb (
     pkgtupid INTEGER NOT NULL REFERENCES pkgtups,
     rpmdb_key TEXT NOT NULL,
     rpmdb_val TEXT NOT NULL);
''', '''\
 CREATE INDEX i_pkgkey_rpmdb ON pkg_rpmdb (pkgtupid, rpmdb_key);
''', '''\
 CREATE TABLE pkg_yumdb (
     pkgtupid INTEGER NOT NULL REFERENCES pkgtups,
     yumdb_key TEXT NOT NULL,
     yumdb_val TEXT NOT NULL);
''', '''\
 CREATE INDEX i_pkgkey_yumdb ON pkg_yumdb (pkgtupid, yumdb_key);
''']

# pylint: disable-msg=E0203
    def _update_db_file_3(self):
        """ Update to version 3 of history, rpmdb/yumdb data. """
        if not self._update_db_file_2():
            return False

        if hasattr(self, '_cached_updated_3'):
            return self._cached_updated_3

        cur = self._get_cursor()
        if cur is None:
            return False

        executeSQL(cur, "PRAGMA table_info(pkg_yumdb)")
        #  If we get anything, we're fine. There might be a better way of
        # saying "anything" but this works.
        for ob in cur:
            break
        else:
            for op in self._update_ops_3:
                cur.execute(op)
            self._commit()
        self._cached_updated_3 = True
        return True

    _update_ops_2 = ['''\
\
 CREATE TABLE trans_skip_pkgs (
     tid INTEGER NOT NULL REFERENCES trans_beg,
     pkgtupid INTEGER NOT NULL REFERENCES pkgtups);
''', '''\
\
 CREATE TABLE trans_cmdline (
     tid INTEGER NOT NULL REFERENCES trans_beg,
     cmdline TEXT NOT NULL);
''', '''\
\
 CREATE TABLE trans_rpmdb_problems (
     rpid INTEGER PRIMARY KEY,
     tid INTEGER NOT NULL REFERENCES trans_beg,
     problem TEXT NOT NULL, msg TEXT NOT NULL);
''', '''\
\
 CREATE TABLE trans_prob_pkgs (
     rpid INTEGER NOT NULL REFERENCES trans_rpmdb_problems,
     pkgtupid INTEGER NOT NULL REFERENCES pkgtups,
     main BOOL NOT NULL DEFAULT FALSE);
''', '''\
\
 CREATE VIEW vtrans_data_pkgs AS
     SELECT tid,name,epoch,version,release,arch,pkgtupid,
            state,done,
            name || '-' || epoch || ':' ||
            version || '-' || release || '.' || arch AS nevra
     FROM trans_data_pkgs JOIN pkgtups USING(pkgtupid)
     ORDER BY name;
''', '''\
\
 CREATE VIEW vtrans_with_pkgs AS
     SELECT tid,name,epoch,version,release,arch,pkgtupid,
            name || '-' || epoch || ':' ||
            version || '-' || release || '.' || arch AS nevra
     FROM trans_with_pkgs JOIN pkgtups USING(pkgtupid)
     ORDER BY name;
''', '''\
\
 CREATE VIEW vtrans_skip_pkgs AS
     SELECT tid,name,epoch,version,release,arch,pkgtupid,
            name || '-' || epoch || ':' ||
            version || '-' || release || '.' || arch AS nevra
     FROM trans_skip_pkgs JOIN pkgtups USING(pkgtupid)
     ORDER BY name;
''', # NOTE: Old versions of sqlite don't like the normal way to do the next
     #       view. So we do it with the select. It's for debugging only, so
     #       no big deal.
'''\
\
 CREATE VIEW vtrans_prob_pkgs2 AS
     SELECT tid,rpid,name,epoch,version,release,arch,pkgtups.pkgtupid,
            main,problem,msg,
            name || '-' || epoch || ':' ||
            version || '-' || release || '.' || arch AS nevra
     FROM (SELECT * FROM trans_prob_pkgs,trans_rpmdb_problems WHERE
           trans_prob_pkgs.rpid=trans_rpmdb_problems.rpid)
           JOIN pkgtups USING(pkgtupid)
     ORDER BY name;
''']

    def _update_db_file_2(self):
        """ Update to version 2 of history, includes trans_skip_pkgs. """
        if not self.conf.writable:
            return False

        if hasattr(self, '_cached_updated_2'):
            return self._cached_updated_2

        cur = self._get_cursor()
        if cur is None:
            return False

        executeSQL(cur, "PRAGMA table_info(trans_skip_pkgs)")
        #  If we get anything, we're fine. There might be a better way of
        # saying "anything" but this works.
        for ob in cur:
            break
        else:
            for op in self._update_ops_2:
                cur.execute(op)
            self._commit()
        self._cached_updated_2 = True
        return True

# pylint: enable-msg=E0203

    def _create_db_file(self):
        """ Create a new history DB file, populating tables etc. """

        self._db_date = time.strftime('%Y-%m-%d')
        _db_file = '%s/%s-%s.%s' % (self.conf.db_path,
                                    'history',
                                    self._db_date,
                                    'sqlite')
        if self._db_file == _db_file:
            os.rename(_db_file, _db_file + '.old')
            # Just in case ... move the journal file too.
            if os.path.exists(_db_file + '-journal'):
                os.rename(_db_file  + '-journal', _db_file + '-journal.old')
        self._db_file = _db_file

        if self.conf.writable and not os.path.exists(self._db_file):
            # make them default to 0600 - sysadmin can change it later
            # if they want
            fo = os.open(self._db_file, os.O_CREAT, 0o600)
            os.close(fo)

        cur = self._get_cursor()
        if cur is None:
            raise IOError(_("Can not create history database at '%s'.") % \
                          self._db_file)

        ops = ['''\
 CREATE TABLE trans_beg (
     tid INTEGER PRIMARY KEY,
     timestamp INTEGER NOT NULL, rpmdb_version TEXT NOT NULL,
     loginuid INTEGER);
''', '''\
 CREATE TABLE trans_end (
     tid INTEGER PRIMARY KEY REFERENCES trans_beg,
     timestamp INTEGER NOT NULL, rpmdb_version TEXT NOT NULL,
     return_code INTEGER NOT NULL);
''', '''\
\
 CREATE TABLE trans_with_pkgs (
     tid INTEGER NOT NULL REFERENCES trans_beg,
     pkgtupid INTEGER NOT NULL REFERENCES pkgtups);
''', '''\
\
 CREATE TABLE trans_error (
     mid INTEGER PRIMARY KEY,
     tid INTEGER NOT NULL REFERENCES trans_beg,
     msg TEXT NOT NULL);
''', '''\
 CREATE TABLE trans_script_stdout (
     lid INTEGER PRIMARY KEY,
     tid INTEGER NOT NULL REFERENCES trans_beg,
     line TEXT NOT NULL);
''', '''\
\
 CREATE TABLE trans_data_pkgs (
     tid INTEGER NOT NULL REFERENCES trans_beg,
     pkgtupid INTEGER NOT NULL REFERENCES pkgtups,
     done BOOL NOT NULL DEFAULT FALSE, state TEXT NOT NULL);
''', '''\
\
 CREATE TABLE pkgtups (
     pkgtupid INTEGER PRIMARY KEY,     name TEXT NOT NULL, arch TEXT NOT NULL,
     epoch TEXT NOT NULL, version TEXT NOT NULL, release TEXT NOT NULL,
     checksum TEXT);
''', '''\
 CREATE INDEX i_pkgtup_naevr ON pkgtups (name, arch, epoch, version, release);
''']
        for op in ops:
            cur.execute(op)
        for op in self._update_ops_2:
            cur.execute(op)
        for op in self._update_ops_3:
            cur.execute(op)
        self._commit()

_FULL_PARSE_QUERY_BEG = """
SELECT pkgtupid,name,epoch,version,release,arch,
  name || "." || arch AS sql_nameArch,
  name || "-" || version || "-" || release || "." || arch AS sql_nameVerRelArch,
  name || "-" || version AS sql_nameVer,
  name || "-" || version || "-" || release AS sql_nameVerRel,
  epoch || ":" || name || "-" || version || "-" || release || "." || arch AS sql_envra,
  name || "-" || epoch || ":" || version || "-" || release || "." || arch AS sql_nevra
  FROM pkgtups
  WHERE
"""

########NEW FILE########
__FILENAME__ = misc
# misc.py
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

"""
Assorted utility functions for yum.
"""

from __future__ import print_function, absolute_import
from __future__ import unicode_literals
from dnf.exceptions import MiscError
from dnf.pycomp import basestring, unicode, long
import os
import os.path
from io import StringIO
import base64
import binascii
import struct
import re
import errno
import dnf.exceptions
from . import pgpmsg
import tempfile
import glob
import pwd
import bz2
import gzip
import shutil
_available_compression = ['gz', 'bz2']
try:
    import lzma
    _available_compression.append('xz')
except ImportError:
    lzma = None

from stat import *
try:
    import gpgme
    import gpgme.editutil
except ImportError:
    gpgme = None

import hashlib
_available_checksums = set(['md5', 'sha1', 'sha256', 'sha384', 'sha512'])
_default_checksums = ['sha256']

import dnf.i18n
import dnf.const

_re_compiled_glob_match = None
def re_glob(s):
    """ Tests if a string is a shell wildcard. """
    # TODO/FIXME maybe consider checking if it is a stringsType before going on - otherwise
    # returning None
    global _re_compiled_glob_match
    if _re_compiled_glob_match is None:
        _re_compiled_glob_match = re.compile('[*?]|\[.+\]').search
    return _re_compiled_glob_match(s)

_re_compiled_full_match = None
def re_full_search_needed(s):
    """ Tests if a string needs a full nevra match, instead of just name. """
    global _re_compiled_full_match
    if _re_compiled_full_match is None:
        # A glob, or a "." or "-" separator, followed by something (the ".")
        one = re.compile('.*([-.*?]|\[.+\]).').match
        # Any epoch, for envra
        two = re.compile('[0-9]+:').match
        _re_compiled_full_match = (one, two)
    for rec in _re_compiled_full_match:
        if rec(s):
            return True
    return False

###########
# Title: Remove duplicates from a sequence
# Submitter: Tim Peters
# From: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/52560
def unique(s):
    """Return a list of the elements in s, but without duplicates.

    For example, unique([1,2,3,1,2,3]) is some permutation of [1,2,3],
    unique("abcabc") some permutation of ["a", "b", "c"], and
    unique(([1, 2], [2, 3], [1, 2])) some permutation of
    [[2, 3], [1, 2]].

    For best speed, all sequence elements should be hashable.  Then
    unique() will usually work in linear time.

    If not possible, the sequence elements should enjoy a total
    ordering, and if list(s).sort() doesn't raise TypeError it's
    assumed that they do enjoy a total ordering.  Then unique() will
    usually work in O(N*log2(N)) time.

    If that's not possible either, the sequence elements must support
    equality-testing.  Then unique() will usually work in quadratic
    time.
    """

    n = len(s)
    if n == 0:
        return []

    # Try using a set first, as that's the fastest and will usually
    # work.  If it doesn't work, it will usually fail quickly, so it
    # usually doesn't cost much to *try* it.  It requires that all the
    # sequence elements be hashable, and support equality comparison.
    try:
        u = set(s)
    except TypeError:
        pass
    else:
        return list(u)

    # We can't hash all the elements.  Second fastest is to sort,
    # which brings the equal elements together; then duplicates are
    # easy to weed out in a single pass.
    # NOTE:  Python's list.sort() was designed to be efficient in the
    # presence of many duplicate elements.  This isn't true of all
    # sort functions in all languages or libraries, so this approach
    # is more effective in Python than it may be elsewhere.
    try:
        t = list(s)
        t.sort()
    except TypeError:
        del t  # move on to the next method
    else:
        assert n > 0
        last = t[0]
        lasti = i = 1
        while i < n:
            if t[i] != last:
                t[lasti] = last = t[i]
                lasti += 1
            i += 1
        return t[:lasti]

    # Brute force is all that's left.
    u = []
    for x in s:
        if x not in u:
            u.append(x)
    return u

class Checksums(object):
    """ Generate checksum(s), on given pieces of data. Producing the
        Length and the result(s) when complete. """

    def __init__(self, checksums=None, ignore_missing=False, ignore_none=False):
        if checksums is None:
            checksums = _default_checksums
        self._sumalgos = []
        self._sumtypes = []
        self._len = 0

        done = set()
        for sumtype in checksums:
            if sumtype == 'sha':
                sumtype = 'sha1'
            if sumtype in done:
                continue

            if sumtype in _available_checksums:
                sumalgo = hashlib.new(sumtype)
            elif ignore_missing:
                continue
            else:
                raise MiscError('Error Checksumming, bad checksum type %s' % sumtype)
            done.add(sumtype)
            self._sumtypes.append(sumtype)
            self._sumalgos.append(sumalgo)
        if not done and not ignore_none:
            raise MiscError('Error Checksumming, no valid checksum type')

    def __len__(self):
        return self._len

    # Note that len(x) is assert limited to INT_MAX, which is 2GB on i686.
    length = property(fget=lambda self: self._len)

    def update(self, data):
        self._len += len(data)
        for sumalgo in self._sumalgos:
            data = data.encode('utf-8') if isinstance(data, unicode) else data
            sumalgo.update(data)

    def read(self, fo, size=2**16):
        data = fo.read(size)
        self.update(data)
        return data

    def hexdigests(self):
        ret = {}
        for sumtype, sumdata in zip(self._sumtypes, self._sumalgos):
            ret[sumtype] = sumdata.hexdigest()
        return ret

    def hexdigest(self, checksum=None):
        if checksum is None:
            if not self._sumtypes:
                return None
            checksum = self._sumtypes[0]
        if checksum == 'sha':
            checksum = 'sha1'
        return self.hexdigests()[checksum]

    def digests(self):
        ret = {}
        for sumtype, sumdata in zip(self._sumtypes, self._sumalgos):
            ret[sumtype] = sumdata.digest()
        return ret

    def digest(self, checksum=None):
        if checksum is None:
            if not self._sumtypes:
                return None
            checksum = self._sumtypes[0]
        if checksum == 'sha':
            checksum = 'sha1'
        return self.digests()[checksum]

def get_default_chksum_type():
    return _default_checksums[0]

def checksum(sumtype, file, CHUNK=2**16, datasize=None):
    """takes filename, hand back Checksum of it
       sumtype = md5 or sha/sha1/sha256/sha512 (note sha == sha1)
       filename = /path/to/file
       CHUNK=65536 by default"""

    # chunking brazenly lifted from Ryan Tomayko

    if isinstance(file, basestring):
        try:
            with open(file, 'rb', CHUNK) as fo:
                return checksum(sumtype, fo, CHUNK, datasize)
        except (IOError, OSError) as e:
            raise MiscError('Error opening file for checksum: %s' % file)

    try:
        # assumes file is a file-like-object
        data = Checksums([sumtype])
        while data.read(file, CHUNK):
            if datasize is not None and data.length > datasize:
                break

        # This screws up the length, but that shouldn't matter. We only care
        # if this checksum == what we expect.
        if datasize is not None and datasize != data.length:
            return '!%u!%s' % (datasize, data.hexdigest(sumtype))

        return data.hexdigest(sumtype)
    except (IOError, OSError) as e:
        raise MiscError('Error reading file for checksum: %s' % file)

def getFileList(path, ext, filelist):
    """Return all files in path matching ext, store them in filelist,
       recurse dirs return list object"""

    extlen = len(ext)
    try:
        dir_list = os.listdir(path)
    except OSError as e:
        raise MiscError(('Error accessing directory %s, %s') % (path, e))

    for d in dir_list:
        if os.path.isdir(path + '/' + d):
            filelist = getFileList(path + '/' + d, ext, filelist)
        else:
            if not ext or d[-extlen:].lower() == '%s' % (ext):
                newpath = os.path.normpath(path + '/' + d)
                filelist.append(newpath)

    return filelist

class GenericHolder(object):
    """Generic Holder class used to hold other objects of known types
       It exists purely to be able to do object.somestuff, object.someotherstuff
       or object[key] and pass object to another function that will
       understand it"""

    def __init__(self, iter=None):
        self.__iter = iter

    def __iter__(self):
        if self.__iter is not None:
            return iter(self[self.__iter])

    def __getitem__(self, item):
        if hasattr(self, item):
            return getattr(self, item)
        else:
            raise KeyError(item)

    def all_lists(self):
        """Return a dictionary of all lists."""
        return {key: list_ for key, list_ in vars(self).items()
                if type(list_) is list}

    def merge_lists(self, other):
        """ Concatenate the list attributes from 'other' to ours. """
        for (key, val) in other.all_lists().items():
            vars(self).setdefault(key, []).extend(val)
        return self

def procgpgkey(rawkey):
    '''Convert ASCII armoured GPG key to binary
    '''
    # TODO: CRC checking? (will RPM do this anyway?)

    # Normalise newlines
    rawkey = re.sub('\r\n?', '\n', rawkey)

    # Extract block
    block = StringIO()
    inblock = 0
    pastheaders = 0
    for line in rawkey.split('\n'):
        if line.startswith('-----BEGIN PGP PUBLIC KEY BLOCK-----'):
            inblock = 1
        elif inblock and line.strip() == '':
            pastheaders = 1
        elif inblock and line.startswith('-----END PGP PUBLIC KEY BLOCK-----'):
            # Hit the end of the block, get out
            break
        elif pastheaders and line.startswith('='):
            # Hit the CRC line, don't include this and stop
            break
        elif pastheaders:
            block.write(line+'\n')

    # Decode and return
    return base64.decodestring(block.getvalue().encode('utf-8'))

def gpgkey_fingerprint_ascii(info, chop=4):
    ''' Given a key_info data from getgpgkeyinfo(), return an ascii
    fingerprint. Chop every 4 ascii values, as that is what GPG does. '''
    # First "duh" ... it's a method...
    fp = info['fingerprint']()
    fp = binascii.hexlify(fp).decode()
    if chop:
        fp = [fp[i:i+chop] for i in range(0, len(fp), chop)]
        fp = " ".join(fp)
    return fp

def getgpgkeyinfo(rawkey, multiple=False):
    '''Return a dict of info for the given ASCII armoured key text

    Returned dict will have the following keys: 'userid', 'keyid', 'timestamp'

    Will raise ValueError if there was a problem decoding the key.
    '''
    # Catch all exceptions as there can be quite a variety raised by this call
    key_info_objs = []
    try:
        keys = pgpmsg.decode_multiple_keys(rawkey)
    except Exception as e:
        raise ValueError(str(e))
    if len(keys) == 0:
        raise ValueError('No key found in given key data')

    for key in keys:
        keyid_blob = key.public_key.key_id()

        info = {
            'userid': key.user_id,
            'keyid': struct.unpack(b'>Q', keyid_blob)[0],
            'timestamp': key.public_key.timestamp,
            'fingerprint' : key.public_key.fingerprint,
            'raw_key' : key.raw_key,
            'has_sig' : False,
            'valid_sig': False,
        }

        # Retrieve the timestamp from the matching signature packet
        # (this is what RPM appears to do)
        for userid in key.user_ids[0]:
            if not isinstance(userid, pgpmsg.signature):
                continue

            if userid.key_id() == keyid_blob:
                # Get the creation time sub-packet if available
                if hasattr(userid, 'hashed_subpaks'):
                    tspkt = \
                        userid.get_hashed_subpak(pgpmsg.SIG_SUB_TYPE_CREATE_TIME)
                    if tspkt != None:
                        info['timestamp'] = int(tspkt[1])
                        break
        key_info_objs.append(info)
    if multiple:
        return key_info_objs
    else:
        return key_info_objs[0]


def keyIdToRPMVer(keyid):
    '''Convert an integer representing a GPG key ID to the hex version string
    used by RPM
    '''
    return "%08x" % (keyid & long(0xffffffff))


def keyInstalled(ts, keyid, timestamp):
    '''
    Return if the GPG key described by the given keyid and timestamp are
    installed in the rpmdb.

    The keyid and timestamp should both be passed as integers.
    The ts is an rpm transaction set object

    Return values:
        - -1      key is not installed
        - 0       key with matching ID and timestamp is installed
        - 1       key with matching ID is installed but has a older timestamp
        - 2       key with matching ID is installed but has a newer timestamp

    No effort is made to handle duplicates. The first matching keyid is used to
    calculate the return result.
    '''
    # Convert key id to 'RPM' form
    keyid = keyIdToRPMVer(keyid)

    # Search
    for hdr in ts.dbMatch('name', 'gpg-pubkey'):
        if hdr['version'] == keyid:
            installedts = int(hdr['release'], 16)
            if installedts == timestamp:
                return 0
            elif installedts < timestamp:
                return 1
            else:
                return 2

    return -1

def import_key_to_pubring(rawkey, keyid, cachedir=None, gpgdir=None, make_ro_copy=True):
    # FIXME - cachedir can be removed from this method when we break api
    if gpgme is None:
        return False

    if not gpgdir:
        gpgdir = '%s/gpgdir' % cachedir

    if not os.path.exists(gpgdir):
        os.makedirs(gpgdir)

    key_fo = StringIO(rawkey)
    os.environ['GNUPGHOME'] = gpgdir
    # import the key
    ctx = gpgme.Context()
    fp = open(os.path.join(gpgdir, 'gpg.conf'), 'wb')
    fp.write('')
    fp.close()
    ctx.import_(key_fo)
    key_fo.close()
    # ultimately trust the key or pygpgme is definitionally stupid
    k = ctx.get_key(keyid)
    gpgme.editutil.edit_trust(ctx, k, gpgme.VALIDITY_ULTIMATE)

    if make_ro_copy:

        rodir = gpgdir + '-ro'
        if not os.path.exists(rodir):
            os.makedirs(rodir, mode=0o755)
            for f in glob.glob(gpgdir + '/*'):
                basename = os.path.basename(f)
                ro_f = rodir + '/' + basename
                shutil.copy(f, ro_f)
                os.chmod(ro_f, 0o755)
            fp = open(rodir + '/gpg.conf', 'w', 0o755)
            # yes it is this stupid, why do you ask?
            opts="""lock-never
no-auto-check-trustdb
trust-model direct
no-expensive-trust-checks
no-permission-warning
preserve-permissions
"""
            fp.write(opts)
            fp.close()


    return True

def return_keyids_from_pubring(gpgdir):
    if gpgme is None or not os.path.exists(gpgdir):
        return []

    os.environ['GNUPGHOME'] = gpgdir
    ctx = gpgme.Context()
    keyids = []
    for k in ctx.keylist():
        for subkey in k.subkeys:
            if subkey.can_sign:
                keyids.append(subkey.keyid)

    return keyids

def valid_detached_sig(sig_file, signed_file, gpghome=None):
    """takes signature , file that was signed and an optional gpghomedir"""

    if gpgme is None:
        return False

    if gpghome:
        if not os.path.exists(gpghome):
            return False
        os.environ['GNUPGHOME'] = gpghome

    if hasattr(sig_file, 'read'):
        sig = sig_file
    else:
        sig = open(sig_file, 'r')
    if hasattr(signed_file, 'read'):
        signed_text = signed_file
    else:
        signed_text = open(signed_file, 'r')
    plaintext = None
    ctx = gpgme.Context()

    try:
        sigs = ctx.verify(sig, signed_text, plaintext)
    except gpgme.GpgmeError as e:
        return False
    else:
        if not sigs:
            return False
        # is there ever a case where we care about a sig beyond the first one?
        thissig = sigs[0]
        if not thissig:
            return False

        if thissig.validity in (gpgme.VALIDITY_FULL, gpgme.VALIDITY_MARGINAL,
                                gpgme.VALIDITY_ULTIMATE):
            return True

    return False

def getCacheDir():
    """return a path to a valid and safe cachedir - only used when not running
       as root or when --tempcache is set"""

    uid = os.geteuid()
    try:
        usertup = pwd.getpwuid(uid)
        username = usertup[0]
    except KeyError:
        return None # if it returns None then, well, it's bollocksed

    # check for /var/tmp/dnf-username-* -
    prefix = '%s-%s-' % (dnf.const.PREFIX, username)
    dirpath = '%s/%s*' % (dnf.const.TMPDIR, prefix)
    cachedirs = sorted(glob.glob(dirpath))
    for thisdir in cachedirs:
        stats = os.lstat(thisdir)
        if S_ISDIR(stats[0]) and S_IMODE(stats[0]) == 448 and stats[4] == uid:
            return thisdir

    # make the dir (tempfile.mkdtemp())
    cachedir = tempfile.mkdtemp(prefix=prefix, dir=dnf.const.TMPDIR)
    return cachedir

def sortPkgObj(pkg1 ,pkg2):
    """sorts a list of yum package objects by name"""
    if pkg1.name > pkg2.name:
        return 1
    elif pkg1.name == pkg2.name:
        return 0
    else:
        return -1

def version_tuple_to_string(evrTuple):
    """
    Convert a tuple representing a package version to a string.

    @param evrTuple: A 3-tuple of epoch, version, and release.

    Return the string representation of evrTuple.
    """
    (e, v, r) = evrTuple
    s = ""

    if e not in [0, '0', None]:
        s += '%s:' % e
    if v is not None:
        s += '%s' % v
    if r is not None:
        s += '-%s' % r
    return s

def prco_tuple_to_string(prcoTuple):
    """returns a text string of the prco from the tuple format"""

    (name, flag, evr) = prcoTuple
    flags = {'GT':'>', 'GE':'>=', 'EQ':'=', 'LT':'<', 'LE':'<='}
    if flag is None:
        return name

    return '%s %s %s' % (name, flags[flag], version_tuple_to_string(evr))

def _decompress_chunked(source, dest, ztype):

    if ztype not in _available_compression:
        msg = "%s compression not available" % ztype
        raise dnf.exceptions.MiscError(msg)

    if ztype == 'bz2':
        s_fn = bz2.BZ2File(source, 'r')
    elif ztype == 'xz':
        s_fn = lzma.LZMAFile(source, 'r')
    elif ztype == 'gz':
        s_fn = gzip.GzipFile(source, 'r')


    destination = open(dest, 'wb')

    while True:
        try:
            data = s_fn.read(1024000)
        except IOError:
            break

        if not data: break

        try:
            destination.write(data)
        except (OSError, IOError) as e:
            msg = "Error writing to file %s: %s" % (dest, str(e))
            raise dnf.exceptions.MiscError(msg)

    destination.close()
    s_fn.close()

def bunzipFile(source,dest):
    """ Extract the bzipped contents of source to dest. """
    _decompress_chunked(source, dest, ztype='bz2')

def seq_max_split(seq, max_entries):
    """ Given a seq, split into a list of lists of length max_entries each. """
    ret = []
    num = len(seq)
    seq = list(seq) # Trying to use a set/etc. here is bad
    beg = 0
    while num > max_entries:
        end = beg + max_entries
        ret.append(seq[beg:end])
        beg += max_entries
        num -= max_entries
    ret.append(seq[beg:])
    return ret

def unlink_f(filename):
    """ Call os.unlink, but don't die if the file isn't there. This is the main
        difference between "rm -f" and plain "rm". """
    try:
        os.unlink(filename)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise

def stat_f(filename, ignore_EACCES=False):
    """ Call os.stat(), but don't die if the file isn't there. Returns None. """
    try:
        return os.stat(filename)
    except OSError as e:
        if e.errno in (errno.ENOENT, errno.ENOTDIR):
            return None
        if ignore_EACCES and e.errno == errno.EACCES:
            return None
        raise

def _getloginuid():
    """ Get the audit-uid/login-uid, if available. None is returned if there
        was a problem. Note that no caching is done here. """
    #  We might normally call audit.audit_getloginuid(), except that requires
    # importing all of the audit module. And it doesn't work anyway: BZ 518721
    try:
        with open("/proc/self/loginuid") as fo:
            data = fo.read()
            return int(data)
    except (IOError, ValueError):
        return None

_cached_getloginuid = None
def getloginuid():
    """ Get the audit-uid/login-uid, if available. None is returned if there
        was a problem. The value is cached, so you don't have to save it. """
    global _cached_getloginuid
    if _cached_getloginuid is None:
        _cached_getloginuid = _getloginuid()
    return _cached_getloginuid


# ---------- i18n ----------
import locale

def get_my_lang_code():
    try:
        mylang = locale.getlocale(locale.LC_MESSAGES)
    except ValueError as e:
        # This is RHEL-5 python crack, Eg. en_IN can't be parsed properly
        mylang = (None, None)
    if mylang == (None, None): # odd :)
        mylang = 'C'
    else:
        mylang = '.'.join(mylang)

    return mylang

def return_running_pids():
    """return list of running processids, excluding this one"""
    mypid = os.getpid()
    pids = []
    for fn in glob.glob('/proc/[0123456789]*'):
        if mypid == os.path.basename(fn):
            continue
        pids.append(os.path.basename(fn))
    return pids

def decompress(filename, dest=None, fn_only=False, check_timestamps=False):
    """take a filename and decompress it into the same relative location.
       if the file is not compressed just return the file"""

    out = dest
    if not dest:
        out = filename

    if filename.endswith('.gz'):
        ztype='gz'
        if not dest:
            out = filename.replace('.gz', '')

    elif filename.endswith('.bz') or filename.endswith('.bz2'):
        ztype='bz2'
        if not dest:
            if filename.endswith('.bz'):
                out = filename.replace('.bz','')
            else:
                out = filename.replace('.bz2', '')

    elif filename.endswith('.xz'):
        ztype='xz'
        if not dest:
            out = filename.replace('.xz', '')

    else:
        out = filename # returning the same file since it is not compressed
        ztype = None

    if ztype and not fn_only:
        if check_timestamps:
            fi = stat_f(filename)
            fo = stat_f(out)
            if fi and fo and fo.st_mtime == fi.st_mtime:
                return out

        _decompress_chunked(filename, out, ztype)
        if check_timestamps and fi:
            os.utime(out, (fi.st_mtime, fi.st_mtime))

    return out

def calculate_repo_gen_dest(filename, generated_name):
    dest = os.path.dirname(filename)
    dest += '/gen'
    if not os.path.exists(dest):
        os.makedirs(dest, mode=0o755)
    return dest + '/' + generated_name

def repo_gen_decompress(filename, generated_name, cached=False):
    """ This is a wrapper around decompress, where we work out a cached
        generated name, and use check_timestamps. filename _must_ be from
        a repo. and generated_name is the type of the file. """

    dest = calculate_repo_gen_dest(filename, generated_name)
    return decompress(filename, dest=dest, check_timestamps=True,fn_only=cached)

def read_in_items_from_dot_dir(thisglob, line_as_list=True):
    """ Takes a glob of a dir (like /etc/foo.d/\*.foo) returns a list of all the
       lines in all the files matching that glob, ignores comments and blank
       lines, optional paramater 'line_as_list tells whether to treat each line
       as a space or comma-separated list, defaults to True.
    """
    results = []
    for fname in glob.glob(thisglob):
        with open(fname) as f:
            for line in f:
                if re.match('\s*(#|$)', line):
                    continue
                line = line.rstrip() # no more trailing \n's
                line = line.lstrip() # be nice
                if not line:
                    continue
                if line_as_list:
                    line = line.replace('\n', ' ')
                    line = line.replace(',', ' ')
                    results.extend(line.split())
                    continue
                results.append(line)
    return results

########NEW FILE########
__FILENAME__ = packages
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
# Copyright 2004 Duke University
# Written by Seth Vidal <skvidal at phy.duke.edu>

"""
Classes and functions dealing with rpm package representations.
"""

from __future__ import absolute_import
from __future__ import unicode_literals
from . import misc
import re
import fnmatch

def buildPkgRefDict(pkgs, casematch=True):
    """take a list of pkg objects and return a dict the contains all the possible
       naming conventions for them eg: for (name,i386,0,1,1)
       dict[name] = (name, i386, 0, 1, 1)
       dict[name.i386] = (name, i386, 0, 1, 1)
       dict[name-1-1.i386] = (name, i386, 0, 1, 1)
       dict[name-1] = (name, i386, 0, 1, 1)
       dict[name-1-1] = (name, i386, 0, 1, 1)
       dict[0:name-1-1.i386] = (name, i386, 0, 1, 1)
       dict[name-0:1-1.i386] = (name, i386, 0, 1, 1)
       """
    pkgdict = {}
    for pkg in pkgs:
        (n, a, e, v, r) = pkg.pkgtup
        if not casematch:
            n = n.lower()
            a = a.lower()
            e = e.lower()
            v = v.lower()
            r = r.lower()
        name = n
        nameArch = '%s.%s' % (n, a)
        nameVerRelArch = '%s-%s-%s.%s' % (n, v, r, a)
        nameVer = '%s-%s' % (n, v)
        nameVerRel = '%s-%s-%s' % (n, v, r)
        envra = '%s:%s-%s-%s.%s' % (e, n, v, r, a)
        nevra = '%s-%s:%s-%s.%s' % (n, e, v, r, a)
        for item in [name, nameArch, nameVerRelArch, nameVer, nameVerRel, envra, nevra]:
            if item not in pkgdict:
                pkgdict[item] = []
            pkgdict[item].append(pkg)

    return pkgdict

def parsePackages(pkgs, usercommands, casematch=0,
                  unique='repo-epoch-name-version-release-arch'):
    """matches up the user request versus a pkg list:
       for installs/updates available pkgs should be the 'others list'
       for removes it should be the installed list of pkgs
       takes an optional casematch option to determine if case should be matched
       exactly. Defaults to not matching."""

    pkgdict = buildPkgRefDict(pkgs, bool(casematch))
    exactmatch = []
    matched = []
    unmatched = []
    for command in usercommands:
        if not casematch:
            command = command.lower()
        if command in pkgdict:
            exactmatch.extend(pkgdict[command])
            del pkgdict[command]
        else:
            # anything we couldn't find a match for
            # could mean it's not there, could mean it's a wildcard
            if misc.re_glob(command):
                trylist = pkgdict.keys()
                # command and pkgdict are already lowered if not casematch
                # so case sensitive is always fine
                restring = fnmatch.translate(command)
                regex = re.compile(restring)
                foundit = 0
                for item in trylist:
                    if regex.match(item):
                        matched.extend(pkgdict[item])
                        del pkgdict[item]
                        foundit = 1

                if not foundit:
                    unmatched.append(command)

            else:
                unmatched.append(command)

    unmatched = misc.unique(unmatched)
    if unique == 'repo-epoch-name-version-release-arch': # pkg.__hash__
        matched    = misc.unique(matched)
        exactmatch = misc.unique(exactmatch)
    elif unique == 'repo-pkgkey': # So we get all pkg entries from a repo
        def pkgunique(pkgs):
            u = {}
            for pkg in pkgs:
                mark = "%s%s" % (pkg.repo.id, pkg.pkgKey)
                u[mark] = pkg
            return list(u.values())
        matched    = pkgunique(matched)
        exactmatch = pkgunique(exactmatch)
    else:
        raise ValueError("Bad value for unique: %s" % unique)
    return exactmatch, matched, unmatched

########NEW FILE########
__FILENAME__ = parser
# parser.py
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import print_function
from __future__ import unicode_literals
import re
try:
    import urlparse
except ImportError:
	import urllib.parse as urlparse
import os.path

import dnf.exceptions
import dnf.util

_KEYCRE = re.compile(r"\$(\w+)")

def varReplace(raw, vars):
    '''Perform variable replacement

    :param raw: String to perform substitution on.
    :param vars: Dictionary of variables to replace. Key is variable name
      (without $ prefix). Value is replacement string.
    :return: Input raw string with substituted values.
    '''

    done = []                      # Completed chunks to return

    while raw:
        m = _KEYCRE.search(raw)
        if not m:
            done.append(raw)
            break

        # Determine replacement value (if unknown variable then preserve
        # original)
        varname = m.group(1).lower()
        replacement = vars.get(varname, m.group())

        start, end = m.span()
        done.append(raw[:start])    # Keep stuff leading up to token
        done.append(replacement)    # Append replacement value
        raw = raw[end:]             # Continue with remainder of string

    return ''.join(done)

class ConfigPreProcessor(object):
    """
    ConfigParser Include Pre-Processor

    File-like Object capable of pre-processing include= lines for
    a ConfigParser.

    The readline function expands lines matching include=(url)
    into lines from the url specified. Includes may occur in
    included files as well.

    Suggested Usage::
        cfg = ConfigParser.ConfigParser()
        fileobj = confpp( fileorurl )
        cfg.readfp(fileobj)
    """


    def __init__(self, configfile, vars=None):
        # put the vars away in a helpful place
        self._vars = vars

        # used to track the current ini-section
        self._section = None

        # set some file-like object attributes for ConfigParser
        # these just make confpp look more like a real file object.
        self.mode = 'r'

        # first make configfile a url even if it points to
        # a local file
        scheme = urlparse.urlparse(configfile)[0]
        if scheme == '':
            # check it to make sure it's not a relative file url
            if configfile[0] != '/':
                configfile = os.getcwd() + '/' + configfile
            url = 'file://' + configfile
        else:
            url = configfile

        # these are used to maintain the include stack and check
        # for recursive/duplicate includes
        self._incstack = []
        self._alreadyincluded = []

        # _pushfile will return None if he couldn't open the file
        fo = self._pushfile( url )
        if fo is None:
            raise dnf.exceptions.ConfigError('Error accessing file: %s' % url)

    def readline( self, size=0 ):
        """
        Implementation of File-Like Object readline function. This should be
        the only function called by ConfigParser according to the python docs.
        We maintain a stack of real FLOs and delegate readline calls to the
        FLO on top of the stack. When EOF occurs on the topmost FLO, it is
        popped off the stack and the next FLO takes over. include= lines
        found anywhere cause a new FLO to be opened and pushed onto the top
        of the stack. Finally, we return EOF when the bottom-most (configfile
        arg to __init__) FLO returns EOF.

        Very Technical Pseudo Code::

            def confpp.readline() [this is called by ConfigParser]
                open configfile, push on stack
                while stack has some stuff on it
                    line = readline from file on top of stack
                    pop and continue if line is EOF
                    if line starts with 'include=' then
                        error if file is recursive or duplicate
                        otherwise open file, push on stack
                        continue
                    else
                        return line

                return EOF
        """

        # set line to EOF initially.
        line=''
        while len(self._incstack) > 0:
            # peek at the file like object on top of the stack
            fo = self._incstack[-1]
            line = fo.readline()
            if len(line) > 0:
                m = re.match( r'\s*include\s*=\s*(?P<url>.*)', line )
                if m:
                    url = m.group('url')
                    if len(url) == 0:
                        raise dnf.exceptions.ConfigError('Error parsing config %s: include must specify file to include.' % (self.name))
                    else:
                        # whooohoo a valid include line.. push it on the stack
                        fo = self._pushfile( url )
                else:
                    # check if the current line starts a new section
                    secmatch = re.match( r'\s*\[(?P<section>.*)\]', line )
                    if secmatch:
                        self._section = secmatch.group('section')
                    # line didn't match include=, just return it as is
                    # for the ConfigParser
                    break
            else:
                # the current file returned EOF, pop it off the stack.
                self._popfile()

        # if the section is prefixed by a space then it is breaks iniparser/configparser
        # so fix it
        broken_sec_match = re.match(r'\s+\[(?P<section>.*)\]', line)
        if broken_sec_match:
            line = line.lstrip()
        # at this point we have a line from the topmost file on the stack
        # or EOF if the stack is empty
        if self._vars:
            return varReplace(line, self._vars)
        return line


    def _absurl( self, url ):
        """
        Returns an absolute url for the (possibly) relative
        url specified. The base url used to resolve the
        missing bits of url is the url of the file currently
        being included (i.e. the top of the stack).
        """

        if len(self._incstack) == 0:
            # it's the initial config file. No base url to resolve against.
            return url
        else:
            return urlparse.urljoin( self.geturl(), url )

    def _pushfile( self, url ):
        """
        Opens the url specified, pushes it on the stack, and
        returns a file like object. Returns None if the url
        has previously been included.
        If the file can not be opened this function exits.
        """

        # absolutize this url using the including files url
        # as a base url.
        absurl = self._absurl(url)

        # get the current section to add it to the included
        # url's name.
        includetuple = (absurl, self._section)
        # check if this has previously been included.
        if self._isalreadyincluded(includetuple):
            return None
        try:
            fo = dnf.util.urlopen(absurl)
        except IOError as e:
            fo = None
        if fo is not None:
            self.name = absurl
            self._incstack.append( fo )
            self._alreadyincluded.append(includetuple)
        else:
            fn = dnf.util.strip_prefix(absurl, 'file://')
            msg = "Can not read configuration: %s" % fn if fn else absurl
            raise dnf.exceptions.ConfigError(msg)

        return fo

    def _popfile( self ):
        """
        Pop a file off the stack signaling completion of including that file.
        """
        fo = self._incstack.pop()
        fo.close()
        if len(self._incstack) > 0:
            self.name = self._incstack[-1].geturl()
        else:
            self.name = None


    def _isalreadyincluded( self, tuple ):
        """
        Checks if the tuple describes an include that was already done.
        This does not necessarily have to be recursive
        """
        for etuple in self._alreadyincluded:
            if etuple == tuple: return 1
        return 0


    def geturl(self): return self.name

########NEW FILE########
__FILENAME__ = pgpmsg
# Copyright (C) 2003,2005,2009  Jens B. Jorgensen <jbj1@ultraemail.net>
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from __future__ import print_function
from __future__ import unicode_literals
import struct, time, io, base64, types
from dnf.pycomp import long, to_ord

#  We use this so that we can work on python-2.4 and python-2.6, and thus.
# use import md5/import sha on the older one and import hashlib on the newer.
#  Stupid deprecation warnings.

# pylint: disable-msg=W0108
# Ignore :W0108: *Lambda may not be necessary*


try:
    import hashlib
except ImportError:
    # Python-2.4.z ... gah!
    import sha
    import md5
    class hashlib:

        @staticmethod
        def new(algo):
            if algo == 'md5':
                return md5.new()
            if algo == 'sha1':
                return sha.new()
            raise ValueError("Bad checksum type")

debug = None

# Cypher Type Byte
# bits 7,6 of the CTB say what kind it is
# we only have reserved defined
CTB_76_NORMAL = 0x80
CTB_76_NEW = 0xc0
CTB_76_MASK = 0xc0

# CTB packet type, bits 5,4,3,2
CTB_PKTV2_MASK = 0x3c    # 1111 - mask for this field
CTB_PKT_MASK = 0x3f      # 111111 - all the lower bits

CTB_PKT_PK_ENC = 1       # 0001 - public-key encrypted session packet
CTB_PKT_SIG = 2          # 0010 - signature packet
CTB_PKT_SK_ENC = 3       # 0011 - symmetric-key encrypted session packet
CTB_PKT_OP_SIG = 4       # 0100 - one-pass signature packet
CTB_PKT_SK_CERT = 5      # 0101 - secret-key certificate packet
CTB_PKT_PK_CERT = 6      # 0110 - public-key certificate packet
CTB_PKT_SK_SUB = 7       # 0111 - secret-key subkey packet
CTB_PKT_COMPRESSED = 8   # 1000 - compressed data packet
CTB_PKT_ENC = 9          # 1001 - symmetric-key encrypted data packet
CTB_PKT_MARKER = 10      # 1010 - marker packet
CTB_PKT_LIT = 11         # 1011 - literal data packet
CTB_PKT_TRUST = 12       # 1100 - trust packet
CTB_PKT_USER_ID = 13     # 1101 - user id packet
CTB_PKT_PK_SUB = 14      # 1110 - public subkey packet
CTB_PKT_USER_ATTR = 17   # 10001 - user attribute packet
CTB_PKT_SYM_ENC_INT = 18 # 10010 - symmetric encrypted integrity packet
CTB_PKT_MOD_DETECT = 19  # 10011 - modification detection code packet

ctb_pkt_to_str = {
    CTB_PKT_PK_ENC : 'public-key encrypted session packet',
    CTB_PKT_SIG : 'signature packet',
    CTB_PKT_SK_ENC : 'symmetric-key encrypted session packet',
    CTB_PKT_OP_SIG : 'one-pass signature packet',
    CTB_PKT_SK_CERT : 'secret-key certificate packet',
    CTB_PKT_PK_CERT : 'public-key certificate packet',
    CTB_PKT_SK_SUB : 'secret-key subkey packet',
    CTB_PKT_COMPRESSED : 'compressed data packet',
    CTB_PKT_ENC : 'symmetric-key encrypted data packet',
    CTB_PKT_MARKER : 'marker packet',
    CTB_PKT_LIT : 'literal data packet',
    CTB_PKT_TRUST : 'trust packet',
    CTB_PKT_USER_ID : 'user id packet',
    CTB_PKT_PK_SUB : 'public subkey packet',
    CTB_PKT_USER_ATTR : 'user attribute packet',
    CTB_PKT_SYM_ENC_INT : 'symmetric encrypted integrity packet',
    CTB_PKT_MOD_DETECT : 'modification detection code packet'
}


# CTB packet-length
CTB_PKT_LEN_MASK = 0x3   # 11 - mask

CTB_PKT_LEN_1 = 0        # 00 - 1 byte
CTB_PKT_LEN_2 = 1        # 01 - 2 bytes
CTB_PKT_LEN_4 = 2        # 10 - 4 bytes
CTB_PKT_LEN_UNDEF = 3    # 11 - no packet length supplied

# Algorithms

# Public Key Algorithms
ALGO_PK_RSA_ENC_OR_SIGN = 1        # RSA (Encrypt or Sign)
ALGO_PK_RSA_ENC_ONLY = 2           # RSA Encrypt-Only
ALGO_PK_RSA_SIGN_ONLY = 3          # RSA Sign-Only
ALGO_PK_ELGAMAL_ENC_ONLY = 16      # Elgamal (Encrypt-Only)
ALGO_PK_DSA = 17                   # DSA (Digital Signature Standard)
ALGO_PK_ELLIPTIC_CURVE = 18        # Elliptic Curve
ALGO_PK_ECDSA = 19                 # ECDSA
ALGO_PK_ELGAMAL_ENC_OR_SIGN = 20   # Elgamal (Encrypt or Sign)
ALGO_PK_DH = 21                    # Diffie-Hellman

algo_pk_to_str = {
    ALGO_PK_RSA_ENC_OR_SIGN : 'RSA (Encrypt or Sign)',
    ALGO_PK_RSA_ENC_ONLY : 'RSA Encrypt-Only',
    ALGO_PK_RSA_SIGN_ONLY : 'RSA Sign-Only',
    ALGO_PK_ELGAMAL_ENC_ONLY : 'Elgamal Encrypt-Only',
    ALGO_PK_DSA : 'DSA (Digital Signature Standard)',
    ALGO_PK_ELLIPTIC_CURVE : 'Elliptic Curve',
    ALGO_PK_ECDSA : 'ECDSA',
    ALGO_PK_ELGAMAL_ENC_OR_SIGN : 'Elgamal (Encrypt or Sign)',
    ALGO_PK_DH : 'Diffie-Hellman'
}

# Symmetric Key Algorithms
ALGO_SK_PLAIN = 0 # Plaintext or unencrypted data
ALGO_SK_IDEA = 1 # IDEA
ALGO_SK_3DES = 2 # Triple-DES
ALGO_SK_CAST5 = 3 # CAST5
ALGO_SK_BLOWFISH = 4 # Blowfish
ALGO_SK_SAFER_SK128 = 5 # SAFER-SK128
ALGO_SK_DES_SK = 6 # DES/SK
ALGO_SK_AES_128 = 7 # AES 128-bit
ALGO_SK_AES_192 = 8 # AES 192-bit
ALGO_SK_AES_256 = 9 # AES 256-bit
ALGO_SK_TWOFISH_256 = 10 # Twofish 256

algo_sk_to_str = {
    ALGO_SK_PLAIN : 'Plaintext or unencrypted data',
    ALGO_SK_IDEA : 'IDEA',
    ALGO_SK_3DES : 'Triple-DES',
    ALGO_SK_CAST5 : 'CAST5',
    ALGO_SK_BLOWFISH : 'Blowfish',
    ALGO_SK_SAFER_SK128 : 'SAFER-SK128',
    ALGO_SK_DES_SK : 'DES/SK',
    ALGO_SK_AES_128 : 'AES 128-bit',
    ALGO_SK_AES_192 : 'AES 192-bit',
    ALGO_SK_AES_256 : 'AES 256-bit',
    ALGO_SK_TWOFISH_256 : 'Twofish 256-bit'
}

# Compression Algorithms
ALGO_COMP_UNCOMP = 0 # Uncompressed
ALGO_COMP_ZIP = 1    # ZIP
ALGO_COMP_ZLIB = 2   # ZLIB
ALGO_COMP_BZIP2 = 3  # BZip2

algo_comp_to_str = {
    ALGO_COMP_UNCOMP : 'Uncompressed',
    ALGO_COMP_ZIP : 'ZIP',
    ALGO_COMP_ZLIB : 'ZLIB',
    ALGO_COMP_BZIP2 : 'BZip2'
}

# Hash Algorithms
ALGO_HASH_MD5 = 1                  # MD5
ALGO_HASH_SHA1 = 2                 # SHA1
ALGO_HASH_RIPEMD160 = 3            # RIPEMD160
ALGO_HASH_SHA_DBL = 4              # double-width SHA
ALGO_HASH_MD2 = 5                  # MD2
ALGO_HASH_TIGER192 = 6             # TIGER192
ALGO_HASH_HAVAL_5_160 = 7          # HAVAL-5-160
ALGO_HASH_SHA256 = 8               # SHA256
ALGO_HASH_SHA384 = 9               # SHA384
ALGO_HASH_SHA512 = 10              # SHA512
ALGO_HASH_SHA224 = 11              # SHA224

algo_hash_to_str = {
    ALGO_HASH_MD5 : 'MD5',
    ALGO_HASH_SHA1 : 'SHA1',
    ALGO_HASH_RIPEMD160 : 'RIPEMD160',
    ALGO_HASH_SHA_DBL : 'double-width SHA',
    ALGO_HASH_MD2 : 'MD2',
    ALGO_HASH_TIGER192 : 'TIGER192',
    ALGO_HASH_HAVAL_5_160 : 'HAVAL-5-160',
    ALGO_HASH_SHA256 : 'SHA256',
    ALGO_HASH_SHA384 : 'SHA384',
    ALGO_HASH_SHA512 : 'SHA512',
    ALGO_HASH_SHA224 : 'SHA224'
}

# Signature types
SIG_TYPE_DOCUMENT = 0x00           # document signature, binary image
SIG_TYPE_DOCUMENT_CANON = 0x01     # document signature, canonical text
SIG_TYPE_STANDALONE = 0x02         # signature over just subpackets
SIG_TYPE_PK_USER_GEN = 0x10        # public key packet and user ID packet, generic certification
SIG_TYPE_PK_USER_PER = 0x11        # public key packet and user ID packet, persona
SIG_TYPE_PK_USER_CAS = 0x12        # public key packet and user ID packet, casual certification
SIG_TYPE_PK_USER_POS = 0x13        # public key packet and user ID packet, positive certification
SIG_TYPE_SUBKEY_BIND = 0x18        # subkey binding
SIG_TYPE_KEY = 0x1F                # key signature
SIG_TYPE_KEY_REVOKE = 0x20      # key revocation
SIG_TYPE_SUBKEY_REVOKE = 0x28   # subkey revocation
SIG_TYPE_CERT_REVOKE = 0x30     # certificate revocation
SIG_TYPE_TIMESTAMP = 0x40       # timestamp

sig_type_to_str = {
    SIG_TYPE_DOCUMENT : 'document signature, binary image',
    SIG_TYPE_DOCUMENT_CANON : 'document signature, canonical text',
    SIG_TYPE_STANDALONE : 'signature over just subpackets',
    SIG_TYPE_PK_USER_GEN : 'public key packet and user ID packet, generic certification',
    SIG_TYPE_PK_USER_PER : 'public key packet and user ID packet, persona',
    SIG_TYPE_PK_USER_CAS : 'public key packet and user ID packet, casual certification',
    SIG_TYPE_PK_USER_POS : 'public key packet and user ID packet, positive certification',
    SIG_TYPE_SUBKEY_BIND : 'subkey binding',
    SIG_TYPE_KEY : 'key signature',
    SIG_TYPE_KEY_REVOKE : 'key revocation',
    SIG_TYPE_SUBKEY_REVOKE : 'subkey revocation',
    SIG_TYPE_CERT_REVOKE : 'certificate revocation',
    SIG_TYPE_TIMESTAMP : 'timestamp'
}

# Signature sub-packet types
SIG_SUB_TYPE_CREATE_TIME = 2        # signature creation time
SIG_SUB_TYPE_EXPIRE_TIME = 3        # signature expiration time
SIG_SUB_TYPE_EXPORT_CERT = 4        # exportable certification
SIG_SUB_TYPE_TRUST_SIG = 5          # trust signature
SIG_SUB_TYPE_REGEXP = 6             # regular expression
SIG_SUB_TYPE_REVOCABLE = 7          # revocable
SIG_SUB_TYPE_KEY_EXPIRE = 9         # key expiration time
SIG_SUB_TYPE_PLACEHOLDER = 10       # placeholder for backward compatibility
SIG_SUB_TYPE_PREF_SYMM_ALGO = 11    # preferred symmetric algorithms
SIG_SUB_TYPE_REVOKE_KEY = 12        # revocation key
SIG_SUB_TYPE_ISSUER_KEY_ID = 16     # issuer key ID
SIG_SUB_TYPE_NOTATION = 20          # notation data
SIG_SUB_TYPE_PREF_HASH_ALGO = 21    # preferred hash algorithms
SIG_SUB_TYPE_PREF_COMP_ALGO = 22    # preferred compression algorithms
SIG_SUB_TYPE_KEY_SRV_PREF = 23      # key server preferences
SIG_SUB_TYPE_PREF_KEY_SRVR = 24     # preferred key server
SIG_SUB_TYPE_PRIM_USER_ID = 25      # primary user id
SIG_SUB_TYPE_POLICY_URI = 26        # policy URI
SIG_SUB_TYPE_KEY_FLAGS = 27         # key flags
SIG_SUB_TYPE_SGNR_USER_ID = 28      # signer's user id
SIG_SUB_TYPE_REVOKE_REASON = 29     # reason for revocation
SIG_SUB_TYPE_FEATURES = 30          # features
SIG_SUB_TYPE_SIG_TARGET = 31        # signature target
SIG_SUB_TYPE_EMBEDDED_SIG = 32      # embedded signature

sig_sub_type_to_str = {
    SIG_SUB_TYPE_CREATE_TIME : 'signature creation time',
    SIG_SUB_TYPE_EXPIRE_TIME : 'signature expiration time',
    SIG_SUB_TYPE_EXPORT_CERT : 'exportable certification',
    SIG_SUB_TYPE_TRUST_SIG : 'trust signature',
    SIG_SUB_TYPE_REGEXP : 'regular expression',
    SIG_SUB_TYPE_REVOCABLE : 'revocable',
    SIG_SUB_TYPE_KEY_EXPIRE : 'key expiration time',
    SIG_SUB_TYPE_PLACEHOLDER : 'placeholder for backward compatibility',
    SIG_SUB_TYPE_PREF_SYMM_ALGO : 'preferred symmetric algorithms',
    SIG_SUB_TYPE_REVOKE_KEY : 'revocation key',
    SIG_SUB_TYPE_ISSUER_KEY_ID : 'issuer key ID',
    SIG_SUB_TYPE_NOTATION : 'notation data',
    SIG_SUB_TYPE_PREF_HASH_ALGO : 'preferred hash algorithms',
    SIG_SUB_TYPE_PREF_COMP_ALGO : 'preferred compression algorithms',
    SIG_SUB_TYPE_KEY_SRV_PREF : 'key server preferences',
    SIG_SUB_TYPE_PREF_KEY_SRVR : 'preferred key server',
    SIG_SUB_TYPE_PRIM_USER_ID : 'primary user id',
    SIG_SUB_TYPE_POLICY_URI : 'policy URI',
    SIG_SUB_TYPE_KEY_FLAGS : 'key flags',
    SIG_SUB_TYPE_SGNR_USER_ID : "signer's user id",
    SIG_SUB_TYPE_REVOKE_REASON : 'reason for revocation',
    SIG_SUB_TYPE_FEATURES : 'features',
    SIG_SUB_TYPE_SIG_TARGET : 'signature target',
    SIG_SUB_TYPE_EMBEDDED_SIG : 'embedded signature'
}

# in a signature subpacket there may be a revocation reason, these codes indicate
# the reason
REVOKE_REASON_NONE = 0              # No reason specified
REVOKE_REASON_SUPER = 0x01          # Key is superceded
REVOKE_REASON_COMPR = 0x02          # Key has been compromised
REVOKE_REASON_NOT_USED = 0x03       # Key is no longer used
REVOKE_REASON_ID_INVALID = 0x20     # user id information is no longer valid

revoke_reason_to_str = {
    REVOKE_REASON_NONE : 'No reason specified',
    REVOKE_REASON_SUPER : 'Key is superceded',
    REVOKE_REASON_COMPR : 'Key has been compromised',
    REVOKE_REASON_NOT_USED : 'Key is no longer used',
    REVOKE_REASON_ID_INVALID : 'user id information is no longer valid'
}

# These flags are used in a 'key flags' signature subpacket
KEY_FLAGS1_MAY_CERTIFY = 0x01 # This key may be used to certify other keys
KEY_FLAGS1_MAY_SIGN = 0x02 # This key may be used to sign data
KEY_FLAGS1_MAY_ENC_COMM = 0x04 # This key may be used to encrypt communications
KEY_FLAGS1_MAY_ENC_STRG = 0x08 # This key may be used to encrypt storage
KEY_FLAGS1_PRIV_MAYBE_SPLIT = 0x10 # Private component have be split through secret-sharing mech.
KEY_FLAGS1_GROUP = 0x80 # Private component may be among group

# A revocation key subpacket has these class values
REVOKE_KEY_CLASS_MAND = 0x80 # this bit must always be set
REVOKE_KEY_CLASS_SENS = 0x40 # sensitive

# Features may be indicated in a signature hashed subpacket
PGP_FEATURE_1_MOD_DETECT = 0x01 # Modification detection

pgp_feature_to_str = {
    PGP_FEATURE_1_MOD_DETECT : 'Modification Detection'
}

def get_whole_number(msg, idx, numlen) :
    """get_whole_number(msg, idx, numlen)
extracts a "whole number" field of length numlen from msg at index idx
returns (<whole number>, new_idx) where the whole number is a long integer
and new_idx is the index of the next element in the message"""
    n = long(0)
    while numlen > 0 :
        b = (struct.unpack(b"B", msg[idx:idx+1]))[0]
        n = n * long(256) + long(b)
        idx = idx + 1
        numlen = numlen - 1
    return (n, idx)

def get_whole_int(msg, idx, numlen) :
    """get_whole_int(msg, idx, numlen)
same as get_whole_number but returns the number as an int for convenience"""
    n, idx = get_whole_number(msg, idx, numlen)
    return int(n), idx

def pack_long(l) :
    """pack_long(l)
    returns big-endian representation of unsigned long integer"""
    arr = []
    while l > 0 :
        arr.insert(0, struct.pack(b"B", l & 0xff))
        l >>= 8
    return b''.join(arr)

def pack_mpi(l) :
    """pack_mpi(l)
    returns the PGP Multi-Precision Integer representation of unsigned long integer"""
    s = pack_long(l)
    # the len is the number of bits, counting only from the MSB,
    # so we need to account for that
    bits = (len(s) - 1) * 8
    if len(s) > 0 :
        n = to_ord(s[0])
        while n != 0 :
            bits += 1
            n >>= 1
    else :
        bits = 0 # otherwise bits == -8
    return struct.pack(b">H", bits) + s

def get_sig_subpak_len(msg, idx) :
    """get_sig_subpak_len(msg, idx)
extracts a signature subpacket length field
returns (subpak_len, new_idx)"""
    plen, idx = get_whole_int(msg, idx, 1)
    if plen < 192 :
        return plen, idx
    if plen < 255 :
        plen2, idx = get_whole_int(msg, idx, 1)
        return ((plen - 192) << 8) + plen2 + 192, idx
    return get_whole_int(msg, idx, 4)

def get_n_mpi(msg, idx) :
    """get_mpi(msg, idx)
    extracts a multi-precision integer field from the message msg at index idx
    returns (n, <mpi>, new_idx) where the mpi is a long integer and new_idx is
    the index of the next element in the message and n is the number of bits of
    precision in <mpi>"""
    ln, idx = get_whole_int(msg, idx, 2)
    return (ln,) + get_whole_number(msg, idx, (ln+7)//8)

def get_mpi(msg, idx) :
    """get_mpi(msg, idx)
extracts a multi-precision integer field from the message msg at index idx
returns (<mpi>, new_idx) where the mpi is a long integer and new_idx is
the index of the next element in the message"""
    l = get_n_mpi(msg, idx)
    return (l[1], l[2])

def str_to_hex(s) :
    return ''.join(map(lambda x : hex(ord(x))[2:].zfill(2), list(s)))

def duration_to_str(s) :
    if s == 0 :
        return 'never'
    secs = s % 60
    s = s // 60
    mins = s % 60
    s = s // 60
    hrs = s % 60
    s = s // 24
    days = s
    return '%d days %02d:%02d:%02d' % (days, hrs, mins, secs)

def map_to_str(m, vals) :
    slist = []
    # change to a list if it's a single value
    if not isinstance(vals, list) and not isinstance(vals, tuple):
        vals = list((vals,))
    for i in vals :
        if i in m :
            slist.append(m[i])
        else :
            slist.append('unknown(' + str(i) + ')')
    return ', '.join(slist)

class pgp_packet(object) :
    def __init__(self) :
        self.pkt_typ = None

    def __str__(self) :
        return map_to_str(ctb_pkt_to_str, self.pkt_typ)

class public_key(pgp_packet) :
    def __init__(self) :
        pgp_packet.__init__(self)
        self.version = None
        self.pk_algo = None
        self.key_size = 0
        self.fingerprint_ = None # we cache this upon calculation

    def fingerprint(self) :
        # return cached value if we have it
        if self.fingerprint_ :
            return self.fingerprint_

        # otherwise calculate it now and cache it
        # v3 and v4 are calculated differently
        if self.version == 3 :
            h = hashlib.new('md5')
            h.update(pack_long(self.pk_rsa_mod))
            h.update(pack_long(self.pk_rsa_exp))
            self.fingerprint_ = h.digest()
        elif self.version == 4 :
            # we hash what would be the whole PGP message containing
            # the pgp certificate
            h = hashlib.new('sha1')
            h.update(b'\x99')
            # we need to has the length of the packet as well
            buf = self.serialize()
            h.update(struct.pack(b">H", len(buf)))
            h.update(buf)
            self.fingerprint_ = h.digest()
        else :
            raise RuntimeError("unknown public key version %d" % self.version)
        return self.fingerprint_

    def key_id(self) :
        if self.version == 3 :
            return pack_long(self.pk_rsa_mod & long(0xffffffffffffffff))
        elif self.version == 4 :
            return self.fingerprint()[-8:]

    def serialize(self) :
        chunks = []
        if self.version == 3 :
            chunks.append(struct.pack(b'>BIHB', self.version, int(self.timestamp), self.validity, self.pk_algo))
            chunks.append(pack_mpi(self.pk_rsa_mod))
            chunks.append(pack_mpi(self.pk_rsa_exp))
        elif self.version == 4 :
            chunks.append(struct.pack(b'>BIB', self.version, int(self.timestamp), self.pk_algo))
            if self.pk_algo == ALGO_PK_RSA_ENC_OR_SIGN or self.pk_algo == ALGO_PK_RSA_SIGN_ONLY :
                chunks.append(pack_mpi(self.pk_rsa_mod))
                chunks.append(pack_mpi(self.pk_rsa_exp))
            elif self.pk_algo == ALGO_PK_DSA :
                chunks.append(pack_mpi(self.pk_dsa_prime_p))
                chunks.append(pack_mpi(self.pk_dsa_grp_ord_q))
                chunks.append(pack_mpi(self.pk_dsa_grp_gen_g))
                chunks.append(pack_mpi(self.pk_dsa_pub_key))
            elif self.pk_algo == ALGO_PK_ELGAMAL_ENC_OR_SIGN or self.pk_algo == ALGO_PK_ELGAMAL_ENC_ONLY :
                chunks.append(pack_mpi(self.pk_elgamal_prime_p))
                chunks.append(pack_mpi(self.pk_elgamal_grp_gen_g))
                chunks.append(pack_mpi(self.pk_elgamal_pub_key))
            else :
                raise RuntimeError("unknown public key algorithm %d" % (self.pk_algo))
        return b''.join(chunks)

    def deserialize(self, msg, idx, pkt_len) :
        idx_save = idx
        self.version, idx = get_whole_int(msg, idx, 1)
        if self.version != 2 and self.version != 3 and self.version != 4 :
            raise RuntimeError('unknown public key packet version %d at %d' % (self.version, idx_save))
        if self.version == 2 : # map v2 into v3 for coding simplicity since they're structurally the same
            self.version = 3
        self.timestamp, idx = get_whole_number(msg, idx, 4)
        self.timestamp = float(self.timestamp)
        if self.version == 3 :
            self.validity, idx = get_whole_number(msg, idx, 2)
        self.pk_algo, idx = get_whole_int(msg, idx, 1)
        if self.pk_algo == ALGO_PK_RSA_ENC_OR_SIGN or self.pk_algo == ALGO_PK_RSA_SIGN_ONLY :
            self.key_size, self.pk_rsa_mod, idx = get_n_mpi(msg, idx)
            self.pk_rsa_exp, idx = get_mpi(msg, idx)
        elif self.pk_algo == ALGO_PK_DSA :
            l1, self.pk_dsa_prime_p, idx = get_n_mpi(msg, idx)
            self.pk_dsa_grp_ord_q, idx = get_mpi(msg, idx)
            self.pk_dsa_grp_gen_g, idx = get_mpi(msg, idx)
            l2, self.pk_dsa_pub_key, idx = get_n_mpi(msg, idx)
            self.key_size = l1 + l2
        elif self.pk_algo == ALGO_PK_ELGAMAL_ENC_OR_SIGN or self.pk_algo == ALGO_PK_ELGAMAL_ENC_ONLY :
            self.key_size, self.pk_elgamal_prime_p, idx = get_n_mpi(msg, idx)
            self.pk_elgamal_grp_gen_g, idx = get_mpi(msg, idx)
            self.pk_elgamal_pub_key, idx = get_mpi(msg, idx)
        else :
            raise RuntimeError("unknown public key algorithm %d at %d" % (self.pk_algo, idx_save))

    def __str__(self) :
        sio = io.StringIO()
        sio.write(pgp_packet.__str__(self) + "\n")
        sio.write("version: " + str(self.version) + "\n")
        sio.write("timestamp: " + time.ctime(self.timestamp) + "\n")
        if self.version == 3 :
            sio.write("validity: " + time.ctime(self.timestamp + self.validity * 24 * 60 * 60) + "\n")
        sio.write("pubkey algo: " + algo_pk_to_str[self.pk_algo] + "\n")
        if self.pk_algo == ALGO_PK_RSA_ENC_OR_SIGN or self.pk_algo == ALGO_PK_RSA_SIGN_ONLY :
            sio.write("pk_rsa_mod: " + hex(self.pk_rsa_mod) + "\n")
            sio.write("pk_rsa_exp: " + hex(self.pk_rsa_exp) + "\n")
        elif self.pk_algo == ALGO_PK_DSA :
            sio.write("pk_dsa_prime_p: " + hex(self.pk_dsa_prime_p) + "\n")
            sio.write("pk_dsa_grp_ord_q: " + hex(self.pk_dsa_grp_ord_q) + "\n")
            sio.write("pk_dsa_grp_gen_g: " + hex(self.pk_dsa_grp_gen_g) + "\n")
            sio.write("pk_dsa_pub_key: " + hex(self.pk_dsa_pub_key) + "\n")
        elif self.pk_algo == ALGO_PK_ELGAMAL_ENC_OR_SIGN or self.pk_algo == ALGO_PK_ELGAMAL_ENC_ONLY :
            sio.write("pk_elgamal_prime_p: " + hex(self.pk_elgamal_prime_p) + "\n")
            sio.write("pk_elgamal_grp_gen_g: " + hex(self.pk_elgamal_grp_gen_g) + "\n")
            sio.write("pk_elgamal_pub_key: " + hex(self.pk_elgamal_pub_key) + "\n")
        return sio.getvalue()

class user_id(pgp_packet) :
    def __init__(self) :
        pgp_packet.__init__(self)
        self.id = None

    def deserialize(self, msg, idx, pkt_len) :
        self.id = msg[idx:idx + pkt_len]

    def __str__(self) :
        return pgp_packet.__str__(self) + "\n" + "id: " + self.id + "\n"

class user_attribute(pgp_packet) :
    def __init__(self) :
        pgp_packet.__init__(self)
        self.sub_type = None
        self.data = None

    def deserialize(self, msg, idx, pkt_len) :
        self.sub_type, idx = get_whole_int(msg, idx, 1)
        pkt_len = pkt_len - 1
        self.data = msg[idx:idx + pkt_len]

    def __str__(self) :
        return pgp_packet.__str__(self) + "\n" + "sub_type: " + str(self.sub_type) + "\ndata: " + str_to_hex(self.data)

class signature(pgp_packet) :
    def __init__(self) :
        pgp_packet.__init__(self)
        self.version = None
        self.sig_type = None
        self.pk_algo = None
        self.hash_algo = None
        self.hash_frag = None

    def key_id(self) :
        if self.version == 3 :
            return self.key_id_
        else :
            i = self.get_hashed_subpak(SIG_SUB_TYPE_ISSUER_KEY_ID)
            if i :
                return i[1]
            i = self.get_unhashed_subpak(SIG_SUB_TYPE_ISSUER_KEY_ID)
            if i :
                return i[1]
            return None

    def creation_time(self) :
        if self.version == 3 :
            return self.timestamp
        else :
            i = self.get_hashed_subpak(SIG_SUB_TYPE_CREATE_TIME)
            return i[1]

    def expiration(self) :
        if self.version != 4 :
            raise ValueError('v3 signatures don\'t have expirations')
        i = self.get_hashed_subpak(SIG_SUB_TYPE_KEY_EXPIRE)
        if i :
            return i[1]
        return 0 # if not present then it never expires

    def get_hashed_subpak(self, typ) :
        for i in self.hashed_subpaks :
            if i[0] == typ :
                return i
        return None

    def get_unhashed_subpak(self, typ) :
        for i in self.unhashed_subpaks :
            if i[0] == typ :
                return i
        return None

    def deserialize_subpacket(self, msg, idx) :
        sublen, idx = get_sig_subpak_len(msg, idx)
        subtype, idx = get_whole_int(msg, idx, 1)
        if subtype == SIG_SUB_TYPE_CREATE_TIME : # key creation time
            tm, idx = get_whole_number(msg, idx, 4)
            return (subtype, float(tm)), idx
        if subtype == SIG_SUB_TYPE_EXPIRE_TIME or subtype == SIG_SUB_TYPE_KEY_EXPIRE :
            s, idx = get_whole_int(msg, idx, 4)
            return (subtype, s), idx
        if subtype == SIG_SUB_TYPE_EXPORT_CERT or subtype == SIG_SUB_TYPE_REVOCABLE :
            bool, idx = get_whole_int(msg, idx, 1)
            return (subtype, bool), idx
        if subtype == SIG_SUB_TYPE_TRUST_SIG : # trust signature
            trust_lvl, idx = get_whole_int(msg, idx, 1)
            trust_amt, idx = get_whole_int(msg, idx, 1)
            return (subtype, trust_lvl, trust_amt), idx
        if subtype == SIG_SUB_TYPE_REGEXP : # regular expression
            expr = msg[idx:idx+sublen-1]
            idx = idx + sublen - 1
            return (subtype, expr), idx
        if subtype == SIG_SUB_TYPE_PREF_SYMM_ALGO or subtype == SIG_SUB_TYPE_PREF_HASH_ALGO or subtype == SIG_SUB_TYPE_PREF_COMP_ALGO or subtype == SIG_SUB_TYPE_KEY_FLAGS :
            algo_list = [to_ord(x) for x in list(msg[idx:idx+sublen-1])]
            idx = idx + sublen - 1
            return (subtype, algo_list), idx
        if subtype == SIG_SUB_TYPE_REVOKE_KEY : # revocation key
            cls, idx = get_whole_int(msg, idx, 1)
            algo, idx = get_whole_int(msg, idx, 1)
            fprint = msg[idx:idx+20]
            idx = idx + 20
            return (subtype, cls, algo, fprint), idx
        if subtype == SIG_SUB_TYPE_ISSUER_KEY_ID : # issuer key ID
            k_id = msg[idx:idx+8]
            idx = idx + 8
            return (subtype, k_id), idx
        if subtype == SIG_SUB_TYPE_NOTATION : # notation data
            flg1, idx = get_whole_int(msg, idx, 1)
            flg2, idx = get_whole_int(msg, idx, 1)
            flg3, idx = get_whole_int(msg, idx, 1)
            flg4, idx = get_whole_int(msg, idx, 1)
            name_len, idx = get_whole_int(msg, idx, 2)
            val_len, idx = get_whole_int(msg, idx, 2)
            nam = msg[idx:idx+name_len]
            idx = idx + name_len
            val = msg[idx:idx+val_len]
            idx = idx + val_len
            return (subtype, flg1, flg2, flg3, flg4, nam, val), idx
        if subtype == SIG_SUB_TYPE_KEY_SRV_PREF : # key server preferences
            prefs = [ to_ord(x) for x in msg[idx:idx+sublen-1] ]
            idx = idx + sublen - 1
            return (subtype, prefs), idx
        if subtype == SIG_SUB_TYPE_PREF_KEY_SRVR : # preferred key server
            url = msg[idx:idx+sublen-1]
            idx = idx + sublen - 1
            return (subtype, url), idx
        if subtype == SIG_SUB_TYPE_PRIM_USER_ID : # primary user id
            bool, idx = get_whole_int(msg, idx, 1)
            return (subtype, bool), idx
        if subtype == SIG_SUB_TYPE_POLICY_URI : # policy URI
            uri = msg[idx:idx+sublen-1]
            idx = idx + sublen - 1
            return (subtype, uri), idx
        if subtype == SIG_SUB_TYPE_SGNR_USER_ID : # signer's user id
            signer_id = msg[idx:idx+sublen-1]
            idx = idx + sublen - 1
            return (subtype, signer_id), idx
        if subtype == SIG_SUB_TYPE_REVOKE_REASON : # reason for revocation
            rev_code, idx = get_whole_int(msg, idx, 1)
            reas_len = sublen - 2
            reas = msg[idx:idx+reas_len]
            idx = idx + reas_len
            return (subtype, rev_code, reas), idx
        if subtype == SIG_SUB_TYPE_FEATURES : # features
            sublen = sublen - 1
            l = [subtype]
            while sublen > 0 :
                oct, idx = get_whole_int(msg, idx, 1)
                l.append(oct)
                sublen = sublen - 1
            return tuple(l), idx
        if subtype == SIG_SUB_TYPE_SIG_TARGET : # signature target
            public_key_algo, idx = get_whole_int(msg, idx, 1)
            hash_algo, idx = get_whole_int(msg, idx, 1)
            hash = msg[idx:idx+sublen-3]
            idx = idx + sublen - 3
            return (subtype, public_key_algo, hash_algo, hash), idx
        if subtype == SIG_SUB_TYPE_EMBEDDED_SIG : # embedded signature
            # don't do anything fancy, just the raw bits
            dat = msg[idx:idx+sublen-1]
            idx = idx + sublen - 1
            return (subtype, dat), idx

        # otherwise the subpacket is an unknown type, so we just pack the data in it
        dat = msg[idx:idx+sublen-1]
        idx = idx + sublen - 1
        return (subtype, dat), idx

    def is_primary_user_id(self) :
        """is_primary_user_id()
        returns true if this signature contains a primary user id subpacket with value true"""
        for i in self.hashed_subpaks :
            if i[0] == SIG_SUB_TYPE_PRIM_USER_ID :
                return i[1]
        return 0

    def subpacket_to_str(self, sp) :
        if sp[0] == SIG_SUB_TYPE_CREATE_TIME : # signature creation time
            return 'creation time: ' + time.ctime(sp[1])
        if sp[0] == SIG_SUB_TYPE_EXPIRE_TIME : # signature expiration time
            return 'signature expires: ' + duration_to_str(sp[1])
        if sp[0] == SIG_SUB_TYPE_EXPORT_CERT : # exportable certification
            if sp[1] :
                return 'signature exportable: TRUE'
            else :
                return 'signature exportable: FALSE'
        if sp[0] == SIG_SUB_TYPE_TRUST_SIG : # trust signature
            if sp[1] == 0 :
                return 'trust: ordinary'
            if sp[1] == 1 :
                return 'trust: introducer (%d)' % sp[2]
            if sp[1] == 2 :
                return 'trust: meta-introducer (%d)' % sp[2]
            return 'trust: %d %d' % (sp[1], sp[2])
        if sp[0] == SIG_SUB_TYPE_REGEXP : # regular expression
            return 'regexp: ' + sp[1]
        if sp[0] == SIG_SUB_TYPE_REVOCABLE : # revocable
            if sp[1] :
                return 'signature revocable: TRUE'
            else :
                return 'signature revocable: FALSE'
        if sp[0] == SIG_SUB_TYPE_KEY_EXPIRE : # key expiration time
            return 'key expires: ' + duration_to_str(sp[1])
        if sp[0] == SIG_SUB_TYPE_PREF_SYMM_ALGO : # preferred symmetric algorithms
            return 'preferred symmetric algorithms: ' + map_to_str(algo_sk_to_str, sp[1])
        if sp[0] == SIG_SUB_TYPE_REVOKE_KEY : # revocation key
            s = 'revocation key: '
            if sp[1] & REVOKE_KEY_CLASS_SENS :
                s = s + '(sensitive) '
            return s + map_to_str(algo_pk_to_str, sp[2]) + ' ' + str_to_hex(sp[3])
        if sp[0] == SIG_SUB_TYPE_ISSUER_KEY_ID : # issuer key ID
            return 'issuer key id: ' + str_to_hex(sp[1])
        if sp[0] == SIG_SUB_TYPE_NOTATION : # notation data
            return 'notation: flags(%d, %d, %d, %d) name(%s) value(%s)' % sp[1:]
        if sp[0] == SIG_SUB_TYPE_PREF_HASH_ALGO : # preferred hash algorithms
            return 'preferred hash algorithms: ' + map_to_str(algo_hash_to_str, sp[1])
        if sp[0] == SIG_SUB_TYPE_PREF_COMP_ALGO : # preferred compression algorithms
            return 'preferred compression algorithms: ' + map_to_str(algo_comp_to_str, sp[1])
        if sp[0] == SIG_SUB_TYPE_KEY_SRV_PREF : # key server preferences
            s = 'key server preferences: '
            prefs = []
            if sp[1][0] & 0x80 :
                prefs.append('No-modify')
            return s + ', '.join(prefs)
        if sp[0] == SIG_SUB_TYPE_PREF_KEY_SRVR : # preferred key server
            return 'preferred key server: %s' % sp[1]
        if sp[0] == SIG_SUB_TYPE_PRIM_USER_ID : # primary user id
            if sp[1] :
                return 'is primary user id'
            else :
                return 'is not primary user id'
        if sp[0] == SIG_SUB_TYPE_POLICY_URI : # policy URL
            return 'policy url: %s' % sp[1]
        if sp[0] == SIG_SUB_TYPE_KEY_FLAGS : # key flags
            flags = []
            flgs1 = 0
            if len(sp[1]) >= 1 :
                flgs1 = sp[1][0]
            if flgs1 & KEY_FLAGS1_MAY_CERTIFY :
                flags.append('may certify other keys')
            if flgs1 & KEY_FLAGS1_MAY_SIGN :
                flags.append('may sign data')
            if flgs1 & KEY_FLAGS1_MAY_ENC_COMM :
                flags.append('may encrypt communications')
            if flgs1 & KEY_FLAGS1_MAY_ENC_STRG :
                flags.append('may encrypt storage')
            if flgs1 & KEY_FLAGS1_PRIV_MAYBE_SPLIT :
                flags.append('private component may have been secret-sharing split')
            if flgs1 & KEY_FLAGS1_GROUP :
                flags.append('group key')
            return 'key flags: ' + ', '.join(flags)
        if sp[0] == SIG_SUB_TYPE_SGNR_USER_ID : # signer's user id
            return 'signer id: ' + sp[1]
        if sp[0] == SIG_SUB_TYPE_REVOKE_REASON : # reason for revocation
            reas = revoke_reason_to_str.get(sp[1], '')
            return 'reason for revocation: %s, %s' % (reas, sp[2])
        if sp[0] == SIG_SUB_TYPE_FEATURES : # features
            features = []
            if len(sp) > 1 :
                val = sp[1]
                if val & PGP_FEATURE_1_MOD_DETECT :
                    features.append('Modification Detection')
                val = val & ~PGP_FEATURE_1_MOD_DETECT
                if val != 0 :
                    features.append('[0]=0x%x' % val)
            for i in range(2, len(sp)) :
                features.append('[%d]=0x%x' % (i-1,sp[i]))
            return 'features: ' + ', '.join(features)
        # this means we don't know what the thing is so we just have raw data
        return 'unknown(%d): %s' % (sp[0], str_to_hex(sp[1]))

    def deserialize(self, msg, idx, pkt_len) :
        self.version, idx = get_whole_int(msg, idx, 1)
        if self.version == 2 :
            self.version = 3
        if self.version == 3 :
            hash_len, idx = get_whole_number(msg, idx, 1)
            self.sig_type, idx = get_whole_int(msg, idx, 1)
            self.timestamp, idx = get_whole_number(msg, idx, 4)
            self.timestamp = float(self.timestamp)
            self.key_id_ = msg[idx:idx+8]
            idx = idx + 8
            self.pk_algo, idx = get_whole_int(msg, idx, 1)
            self.hash_algo, idx = get_whole_int(msg, idx, 1)
        elif self.version == 4:
            self.sig_type, idx = get_whole_int(msg, idx, 1)
            self.pk_algo, idx = get_whole_int(msg, idx, 1)
            self.hash_algo, idx = get_whole_int(msg, idx, 1)
            sub_paks_len, idx = get_whole_int(msg, idx, 2)
            sub_paks_end = idx + sub_paks_len
            self.hashed_subpaks = []
            while idx < sub_paks_end :
                sp, idx = self.deserialize_subpacket(msg, idx)
                self.hashed_subpaks.append(sp)
            sub_paks_len, idx = get_whole_int(msg, idx, 2)
            sub_paks_end = idx + sub_paks_len
            self.unhashed_subpaks = []
            while idx < sub_paks_end :
                sp, idx = self.deserialize_subpacket(msg, idx)
                self.unhashed_subpaks.append(sp)
        else :
            raise RuntimeError('unknown signature packet version %d at %d' % (self.version, idx))
        self.hash_frag, idx = get_whole_number(msg, idx, 2)
        if self.pk_algo == ALGO_PK_RSA_ENC_OR_SIGN or self.pk_algo == ALGO_PK_RSA_SIGN_ONLY :
            self.rsa_sig, idx = get_mpi(msg, idx)
        elif self.pk_algo == ALGO_PK_DSA :
            self.dsa_sig_r, idx = get_mpi(msg, idx)
            self.dsa_sig_s, idx = get_mpi(msg, idx)
        else :
            raise RuntimeError('unknown public-key algorithm (%d) in signature at %d' % (self.pk_algo, idx))
        return idx

    def __str__(self) :
        sio = io.StringIO()
        sio.write(pgp_packet.__str__(self) + "\n")
        sio.write("version: " + str(self.version) + "\n")
        sio.write("type: " + sig_type_to_str[self.sig_type] + "\n")
        if self.version == 3 :
            sio.write("timestamp: " + time.ctime(self.timestamp) + "\n")
            sio.write("key_id: " + str_to_hex(self.key_id_) + "\n")
        elif self.version == 4 :
            sio.write("hashed subpackets:\n")
            for i in self.hashed_subpaks :
                sio.write("    " + self.subpacket_to_str(i) + "\n")
            sio.write("unhashed subpackets:\n")
            for i in self.unhashed_subpaks :
                sio.write("    " + self.subpacket_to_str(i) + "\n")
        sio.write("hash_algo: " + algo_hash_to_str[self.hash_algo] + "\n")
        sio.write("hash_frag: " + hex(self.hash_frag) + "\n")
        if self.pk_algo == ALGO_PK_RSA_ENC_OR_SIGN or self.pk_algo == ALGO_PK_RSA_SIGN_ONLY :
            sio.write("pk_algo: RSA\n")
            sio.write("rsa_sig: " + hex(self.rsa_sig) + "\n")
        elif self.pk_algo == ALGO_PK_DSA :
            sio.write("pk_algo: DSA\n")
            sio.write("dsa_sig_r: " + hex(self.dsa_sig_r) + "\n")
            sio.write("dsa_sig_s: " + hex(self.dsa_sig_s) + "\n")
        return sio.getvalue()

#
# This class encapsulates an openpgp public "certificate", which is formed in a message as
# a series of PGP packets of certain types in certain orders
#

class pgp_certificate(object):
    def __init__(self) :
        self.version = None
        self.public_key = None
        self.revocations = []
        self.user_ids = []
        self.primary_user_id = -1 # index of the primary user id

    def __str__(self) :
        sio = io.StringIO()
        sio.write("PGP Public Key Certificate v%d\n" % self.version)
        sio.write("Cert ID: %s\n" % str_to_hex(self.public_key.key_id()))
        sio.write("Primary ID: %s\n" % self.user_id)
        sio.write(str(self.public_key))
        for uid in self.user_ids :
            sio.write(str(uid[0]))
            for sig in uid[1:] :
                sio.write("   " + str(sig))
        if hasattr(self, 'user_attrs') :
            for uattr in self.user_attrs :
                sio.write(' ')
                sio.write(str(uattr[0]))
                for sig in uattr[1:] :
                    sio.write("   " + str(sig))
        return sio.getvalue()

    def get_user_id(self):
        # take the LAST one in the list, not first
        # they appear to be ordered FIFO from the key and that means if you
        # added a key later then it won't show the one you expect
        return self.user_ids[self.primary_user_id][0].id

    user_id = property(get_user_id)

    def expiration(self) :
        if self.version == 3 :
            if self.public_key.validity == 0 :
                return 0
            return self.public_key.timestamp + self.public_key.validity * 24 * 60 * 60
        else : # self.version == 4
            # this is a bit more complex, we need to find the signature on the
            # key and get its expiration
            u_id = self.user_ids[0]
            for i in u_id[1:] :
                if i.sig_type == SIG_TYPE_PK_USER_GEN :
                    exp = i.expiration()
                    if exp == 0 :
                        return 0
                    return self.public_key.timestamp + exp
            return 0

    def key_size(self) :
        return 0

    def load(self, pkts) :
        """load(pkts)
Initialize the pgp_certificate with a list of OpenPGP packets. The list of packets will
be scanned to make sure they are valid for a pgp certificate."""


        # each certificate should begin with a public key packet
        if pkts[0].pkt_typ != CTB_PKT_PK_CERT :
            raise ValueError('first PGP packet should be a public-key packet, not %s' % map_to_str(ctb_pkt_to_str, pkts[0].pkt_typ))

        # all versions have a public key although in a v4 cert the main key is only
        # used for signing, never encryption
        self.public_key = pkts[0]

        # ok, then what's the version
        self.version = self.public_key.version

        # now the behavior splits a little depending on the version
        if self.version == 3 :
            pkt_idx = 1

            # zero or more revocations
            while pkts[pkt_idx].pkt_typ == CTB_PKT_SIG :
                if pkts[pkt_idx].version != 3 :
                    raise ValueError('version 3 cert has version %d signature' % pkts[pkt_idx].version)
                if pkts[pkt_idx].sig_type != SIG_TYPE_KEY_REVOKE :
                    raise ValueError('v3 cert revocation sig has type %s' % map_to_str(sig_type_to_str, pkts[pkt_idx].sig_type))

                # ok, well at least the type is good, we'll assume the cert is
                # revoked
                self.revocations.append(pkts[pkt_idx])

                # increment the pkt_idx to go to the next one
                pkt_idx = pkt_idx + 1

            # the following packets are User ID, Signature pairs
            while pkt_idx < len(pkts) :
                # this packet is supposed to be a user id
                if pkts[pkt_idx].pkt_typ != CTB_PKT_USER_ID :
                    if len(self.user_ids) == 0 :
                        raise ValueError('pgp packet %d is not user id, is %s' % (pkt_idx, map_to_str(ctb_pkt_to_str, pkts[pkt_idx].pkt_typ)))
                    else :
                        break

                user_id = [pkts[pkt_idx]]
                pkt_idx = pkt_idx + 1
                is_revoked = 0
                is_primary_user_id = 0

                # there may be a sequence of signatures following the user id which
                # bind it to the key
                while pkt_idx < len(pkts) and pkts[pkt_idx].pkt_typ == CTB_PKT_SIG :
                    if pkts[pkt_idx].sig_type not in (SIG_TYPE_PK_USER_GEN, SIG_TYPE_PK_USER_PER, SIG_TYPE_PK_USER_CAS, SIG_TYPE_PK_USER_POS, SIG_TYPE_CERT_REVOKE) :
                        raise ValueError('signature %d doesn\'t bind user_id to key, is %s' % (pkt_idx, map_to_str(sig_type_to_str, pkts[pkt_idx].sig_typ)))

                    user_id.append(pkts[pkt_idx])

                    pkt_idx = pkt_idx + 1

                # append the user ID and signature(s) onto a list
                self.user_ids.append(user_id)

        else : # self.version == 4
            pkt_idx = 1
            self.direct_key_sigs = []
            self.subkeys = []
            self.rvkd_subkeys = []
            self.user_attrs = []

            cert_id = self.public_key.key_id()

            # second packet could be a revocation (or a direct key self signature)
            while pkt_idx < len(pkts) and pkts[pkt_idx].pkt_typ == CTB_PKT_SIG :
                if pkts[pkt_idx].version != 4 :
                    raise ValueError('version 4 cert has version %d signature' % pkts[pkt_idx].version)
                if pkts[pkt_idx].sig_type == SIG_TYPE_KEY_REVOKE :
                    self.revocations.append(pkts[pkt_idx])
                elif pkts[pkt_idx].sig_type == SIG_TYPE_KEY :
                    self.direct_key_sigs.append(pkts[pkt_idx])
                else :
                    raise ValueError('v4 cert signature has type %s, supposed to be revocation signature or direct key signature' % map_to_str(sig_type_to_str, pkts[pkt_idx].sig_type))

                # increment the pkt_idx to go to the next one
                pkt_idx = pkt_idx + 1

            # the following packets are:
            # User ID, signature... sets or
            # subkey, signature... sets or
            # user attribute, signature... sets
            prim_user_id_sig_time = 0

            while pkt_idx < len(pkts) :
                # this packet is supposed to be a user id
                if pkts[pkt_idx].pkt_typ == CTB_PKT_USER_ID :
                    user_id = [pkts[pkt_idx]]
                    is_revoked = 0
                    is_primary_user_id = 0

                    pkt_idx = pkt_idx + 1

                    # there may be a sequence of signatures following the user id which
                    # bind it to the key
                    while pkt_idx < len(pkts) and pkts[pkt_idx].pkt_typ == CTB_PKT_SIG :
                        if pkts[pkt_idx].sig_type not in (SIG_TYPE_PK_USER_GEN, SIG_TYPE_PK_USER_PER, SIG_TYPE_PK_USER_CAS, SIG_TYPE_PK_USER_POS, SIG_TYPE_CERT_REVOKE) :
                            raise ValueError('signature %d doesn\'t bind user_id to key, is %s' % (pkt_idx, map_to_str(sig_type_to_str, pkts[pkt_idx].sig_type)))
                        user_id.append(pkts[pkt_idx])

                        # is this the primary user id?
                        if pkts[pkt_idx].key_id() == cert_id :
                            if pkts[pkt_idx].is_primary_user_id() :
                                ct = pkts[pkt_idx].creation_time()
                                if ct > prim_user_id_sig_time :
                                    self.primary_user_id = len(self.user_ids)
                                    prim_user_id_sig_time = ct

                        pkt_idx = pkt_idx + 1

                    # append the user ID and signature(s) onto the list
                    self.user_ids.append(user_id)

                # this packet is supposed to be a user id
                elif pkts[pkt_idx].pkt_typ == CTB_PKT_USER_ATTR :
                    user_attr = [pkts[pkt_idx]]
                    is_revoked = 0

                    pkt_idx = pkt_idx + 1

                    # there may be a sequence of signatures following the user id which
                    # bind it to the key
                    while pkt_idx < len(pkts) and pkts[pkt_idx].pkt_typ == CTB_PKT_SIG :
                        if pkts[pkt_idx].sig_type not in (SIG_TYPE_PK_USER_GEN, SIG_TYPE_PK_USER_PER, SIG_TYPE_PK_USER_CAS, SIG_TYPE_PK_USER_POS, SIG_TYPE_CERT_REVOKE) :
                            raise ValueError('signature %d doesn\'t bind user_attr to key, is %s' % (pkt_idx, map_to_str(sig_type_to_str, pkts[pkt_idx].sig_type)))
                        user_attr.append(pkts[pkt_idx])
                        pkt_idx = pkt_idx + 1

                    # append the user ID and signature(s) onto the list
                    self.user_attrs.append(user_attr)

                elif pkts[pkt_idx].pkt_typ == CTB_PKT_PK_SUB :
                    # collect this list of subkey + signature [ + revocation ]
                    subkey = [pkts[pkt_idx]]
                    pkt_idx = pkt_idx + 1
                    is_revoked = 0

                    # there must be one signature following the subkey that binds it to the main key
                    if pkt_idx >= len(pkts) :
                        raise ValueError('subkey at index %d was not followed by a signature' % (pkt_idx-1))
                    if pkts[pkt_idx].pkt_typ != CTB_PKT_SIG or pkts[pkt_idx].sig_type != SIG_TYPE_SUBKEY_BIND :
                        raise ValueError('signature %d doesn\'t bind subkey to key, type is %s' % (pkt_idx, map_to_str(sig_type_to_str, pkts[pkt_idx].sig_typ)))
                    subkey.append(pkts[pkt_idx])

                    pkt_idx = pkt_idx + 1

                    # there may optionally be a revocation
                    if pkt_idx < len(pkts) and pkts[pkt_idx].pkt_typ == CTB_PKT_SIG and pkts[pkt_idx].sig_type == SIG_TYPE_SUBKEY_REVOKE :
                        is_revoked = 1
                        subkey.append(pkts[pkt_idx])
                        pkt_idx = pkt_idx + 1

                    # append the user ID and signature(s) onto the list
                    if is_revoked :
                        self.rvkd_subkeys.append(subkey)
                    else :
                        self.subkeys.append(subkey)
                elif pkts[pkt_idx].pkt_typ == CTB_PKT_SIG :

                    # ok, well at least the type is good, we'll assume the cert is
                    # revoked
                    self.revocations.append(pkts[pkt_idx])

                    # increment the pkt_idx to go to the next one
                    pkt_idx = pkt_idx + 1


                else :
                    break

        # did we get all the things we needed?
        #if not self.user_id :
        # just take the first valid user id we encountered then
        if len(self.user_ids) == 0 :
            raise ValueError('no user id packet was present in the cert %s' % str_to_hex(self.public_key.key_id()))
        return pkt_idx


def get_ctb(msg, idx) :
    """get_ctb(msg, idx)
extracts a the "cypher type bit" information from message msg at index idx
returns (type, len, new_idx) where type is the enumerated type of the packet,
len is the length of the packet, and new_idx is the index of the next element
in the message"""
    b, idx = get_whole_int(msg, idx, 1)
    if (b & CTB_76_MASK) == CTB_76_NORMAL :
        n_len = 0 # undefined length
        if (b & CTB_PKT_LEN_MASK) == CTB_PKT_LEN_1 :
            n_len = 1
        if (b & CTB_PKT_LEN_MASK) == CTB_PKT_LEN_2 :
            n_len = 2
        if (b & CTB_PKT_LEN_MASK) == CTB_PKT_LEN_4 :
            n_len = 4
        if (b & CTB_PKT_LEN_MASK) == CTB_PKT_LEN_UNDEF :
            n_len = 0
        pkt_len = 0
        if n_len > 0 :
            pkt_len, idx = get_whole_int(msg, idx, n_len)
        return (b & CTB_PKTV2_MASK) >> 2, pkt_len, idx
    elif (b & CTB_76_MASK) == CTB_76_NEW :
        plen, idx = get_whole_int(msg, idx, 1)
        if plen < 192 :
            return b & CTB_PKT_MASK, plen, idx
        if plen < 224 :
            plen2, idx = get_whole_int(msg, idx, 1)
            return b & CTB_PKT_MASK, ((plen - 192) << 8) + plen2 + 192, idx
        if plen == 255 :
            plen, idx = get_whole_int(msg, idx, 4)
            return b & CTB_PKT_MASK, plen, idx
        else :
            raise Exception('partial message bodies are not supported by this version (%d)').with_traceback(b)
    else :
        raise Exception("unknown (not \"normal\") cypher type bit %d at byte %d" % (b, idx))

def crc24(msg) :
    crc24_init = 0xb704ce
    crc24_poly = 0x1864cfb

    crc = crc24_init
    for i in list(msg) :
        crc = crc ^ (to_ord(i) << 16)
        for j in range(0, 8) :
            crc = crc << 1
            if crc & 0x1000000 :
                crc = crc ^ crc24_poly
    return crc & 0xffffff

def decode(msg) :
    # each message is a sequence of packets so we go through the message
    # and generate a list of packets and return that
    pkt_list = []
    idx = 0
    msg_len = len(msg)
    while idx < msg_len :
        pkt_typ, pkt_len, idx = get_ctb(msg, idx)
        pkt = None
        if pkt_typ == CTB_PKT_PK_CERT or pkt_typ == CTB_PKT_PK_SUB :
            pkt = public_key()

        elif pkt_typ == CTB_PKT_USER_ID :
            pkt = user_id()

        elif pkt_typ == CTB_PKT_SIG :
            pkt = signature()

        elif pkt_typ == CTB_PKT_USER_ATTR :
            pkt = user_attribute()

        if pkt :
            pkt.pkt_typ = pkt_typ
            pkt.deserialize(msg, idx, pkt_len)
            if debug :
                debug.write(pkt.__str__() + "\n")
        else :
            raise ValueError('unexpected pgp packet type %s at %d' % (map_to_str(ctb_pkt_to_str, pkt_typ), idx))

        pkt_list.append(pkt)

        idx = idx + pkt_len
    return pkt_list

def decode_msg(msg, multi=False) :
    """decode_msg(msg) ==> list of OpenPGP "packet" objects
Takes an ascii-armored PGP block and returns a list of objects each of which
corresponds to a PGP "packets".

A PGP message is a series of packets. You need to understand how packets are
to be combined together in order to know what to do with them. For example
a PGP "certificate" includes a public key, user id(s), and signature.
"""
    # first we'll break the block up into lines and trim each line of any
    # carriage return chars
    pgpkey_lines = map(lambda x : x.rstrip(), msg.split('\n'))

    # check out block
    in_block = 0
    in_data = 0

    block_buf = io.StringIO()
    for l in pgpkey_lines :
        if not in_block :
            if l == '-----BEGIN PGP PUBLIC KEY BLOCK-----' :
                in_block = 1
            continue

        # are we at the actual data yet?
        if not in_data :
            if len(l) == 0 :
                in_data = 1
            continue

        # are we at the checksum line?
        if l and l[0] == '=' :
            # get the checksum number
            csum = base64.decodestring(l[1:5].encode('utf-8'))
            i = 0
            csum, i = get_whole_number(csum, i, 3)

            # convert the base64 cert data to binary data
            cert_msg = base64.decodestring(
                block_buf.getvalue().encode('utf-8'))
            block_buf.close()

            # check the checksum
            if csum != crc24(cert_msg) :
                raise Exception('bad checksum on pgp message')

            # ok, the sum looks ok so we'll actually decode the thing
            pkt_list = decode(cert_msg)
            # turn it into a real cert
            cert_list = []
            while len(pkt_list) > 0 :
                cert = pgp_certificate()
                cert.raw_key = msg
                pkt_idx = cert.load(pkt_list)
                cert_list.append(cert)
                pkt_list[0:pkt_idx] = []
            if not multi:
                if not cert_list:
                    return None
                return cert_list[0]
            return cert_list

        # add the data to our buffer then
        block_buf.write(l)

    if not multi:
        return None
    return []

def decode_multiple_keys(msg):
    #ditto of above - but handling multiple certs/keys per file
    certs = []

    pgpkey_lines = map(lambda x : x.rstrip(), msg.split('\n'))
    in_block = 0
    block = ''
    for l in pgpkey_lines :
        if not in_block :
            if l == '-----BEGIN PGP PUBLIC KEY BLOCK-----' :
                in_block = 1
                block += '%s\n' % l
                continue

        block += '%s\n' % l
        if l == '-----END PGP PUBLIC KEY BLOCK-----':
            in_block = 0
            thesecerts = decode_msg(block, multi=True)
            if thesecerts:
                certs.extend(thesecerts)
            block = ''
            continue
    return certs

########NEW FILE########
__FILENAME__ = rpmsack
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from __future__ import absolute_import
from __future__ import unicode_literals
from . import misc
import dnf.pycomp
import glob
import os
import rpm


# For returnPackages(patterns=)
flags = {"GT": rpm.RPMSENSE_GREATER,
         "GE": rpm.RPMSENSE_EQUAL | rpm.RPMSENSE_GREATER,
         "LT": rpm.RPMSENSE_LESS,
         "LE": rpm.RPMSENSE_LESS | rpm.RPMSENSE_EQUAL,
         "EQ": rpm.RPMSENSE_EQUAL,
         None: 0 }

def _open_no_umask(*args):
    """ Annoying people like to set umask's for root, which screws everything
        up for user readable stuff. """
    oumask = os.umask(0o22)
    try:
        ret = open(*args)
    finally:
        os.umask(oumask)

    return ret

def _makedirs_no_umask(*args):
    """ Annoying people like to set umask's for root, which screws everything
        up for user readable stuff. """
    oumask = os.umask(0o22)
    try:
        ret = os.makedirs(*args)
    finally:
        os.umask(oumask)

    return ret

def _iopen(*args):
    """ IOError wrapper BS for open, stupid exceptions. """
    try:
        ret = open(*args)
    except IOError as e:
        return None, e
    return ret, None

def _sanitize(path):
    return path.replace('/', '').replace('~', '')

class AdditionalPkgDB(object):
    """ Accesses additional package data rpmdb is unable to store.

        Previously known as yumdb.
    """
    # dir: <persistdir>/yumdb
    # pkgs stored in name[0]/pkgid-name-ver-rel-arch dirs
    # dirs have files per piece of info we're keeping, e.g. repoid, install
    # reason, status, etc.

    def __init__(self, db_path):
        self.conf = misc.GenericHolder()
        self.conf.db_path = db_path
        self.conf.writable = False

        self._packages = {} # pkgid = dir
        if not os.path.exists(self.conf.db_path):
            try:
                _makedirs_no_umask(self.conf.db_path)
                self.conf.writable = True
            except (IOError, OSError) as e:
                # some sort of useful thing here? A warning?
                pass
        else:
            if os.access(self.conf.db_path, os.W_OK):
                self.conf.writable = True
        self.yumdb_cache = {'attr' : {}}

    def _get_dir_name(self, pkgtup, pkgid):
        if pkgid in self._packages:
            return self._packages[pkgid]
        (n, a, e, v,r) = pkgtup
        n = _sanitize(n)
        str_pkgid = pkgid
        if pkgid is None:
            str_pkgid = '<nopkgid>'
        elif dnf.pycomp.is_py2str_py3bytes(pkgid):
            str_pkgid = pkgid.decode()
        thisdir = '%s/%s/%s-%s-%s-%s-%s' % (self.conf.db_path,
                                            n[0], str_pkgid, n, v, r, a)
        self._packages[pkgid] = thisdir
        return thisdir

    def get_package(self, po=None, pkgtup=None, pkgid=None):
        """Return an RPMDBAdditionalDataPackage Object for this package"""
        if po:
            thisdir = self._get_dir_name(po.pkgtup, po.pkgid)
        elif pkgtup and pkgid:
            thisdir = self._get_dir_name(pkgtup, pkgid)
        else:
            raise ValueError("Missing arguments.")

        return RPMDBAdditionalDataPackage(self.conf, thisdir,
                                          yumdb_cache=self.yumdb_cache)

class RPMDBAdditionalDataPackage(object):

    # We do auto hardlink on these attributes
    _auto_hardlink_attrs = set(['checksum_type', 'reason',
                                'installed_by', 'changed_by',
                                'from_repo', 'from_repo_revision',
                                'from_repo_timestamp', 'releasever',
                                'command_line'])

    def __init__(self, conf, pkgdir, yumdb_cache=None):
        self._conf = conf
        self._mydir = pkgdir

        self._read_cached_data = {}

        #  'from_repo' is the most often requested piece of data, and is often
        # the same for a huge number of packages. So we use hardlinks to share
        # data, and try to optimize for that.
        #  It's useful for other keys too (installed_by/changed_by/reason/etc.)
        # so we make it generic.
        self._yumdb_cache = yumdb_cache

    def _auto_cache(self, attr, value, fn, info=None):
        """ Create caches for the attr. We have a per. object read cache so at
            worst we only have to read a single attr once. Then we expand that
            with (dev, ino) cache, so hardlink data can be read once for
            multiple packages. """
        self._read_cached_data[attr] = value
        if self._yumdb_cache is None:
            return

        nlinks = 1
        if info is not None:
            nlinks = info.st_nlink
        if nlinks <= 1 and attr not in self._auto_hardlink_attrs:
            return

        if value in self._yumdb_cache['attr']:
            sinfo = self._yumdb_cache['attr'][value][1]
            if info is not None and sinfo is not None:
                if (info.st_dev, info.st_ino) == (sinfo.st_dev, sinfo.st_ino):
                    self._yumdb_cache['attr'][value][2].add(fn)
                    self._yumdb_cache[fn] = value
                    return
            if self._yumdb_cache['attr'][value][0] >= nlinks:
                # We already have a better cache file.
                return

        self._yumdb_cache['attr'][value] = (nlinks, info, set([fn]))
        self._yumdb_cache[fn]            = value

    def _unlink_yumdb_cache(self, fn):
        """ Remove old values from the link cache. """
        if fn in self._yumdb_cache:
            ovalue = self._yumdb_cache[fn]
            if ovalue in self._yumdb_cache['attr']:
                self._yumdb_cache['attr'][ovalue][2].discard(fn)
                if not self._yumdb_cache['attr'][ovalue][2]:
                    del self._yumdb_cache['attr'][ovalue]
            del self._yumdb_cache[fn]

    def _link_yumdb_cache(self, fn, value):
        """ If we have a matching yumdb cache, link() to it instead of having
            to open()+write(). """
        if self._yumdb_cache is None:
            return False

        self._unlink_yumdb_cache(fn)

        if value not in self._yumdb_cache['attr']:
            return False

        assert self._yumdb_cache['attr'][value][2]
        try:
            lfn = next(iter(self._yumdb_cache['attr'][value][2]))
            misc.unlink_f(fn + '.tmp')
            os.link(lfn, fn + '.tmp')
            os.rename(fn + '.tmp', fn)
        except:
            return False

        self._yumdb_cache['attr'][value][2].add(fn)
        self._yumdb_cache[fn] = value

        return True

    def _attr2fn(self, attr):
        """ Given an attribute, return the filename. """
        return os.path.normpath(self._mydir + '/' + attr)

    def _write(self, attr, value):
        # check for self._conf.writable before going on?
        if not os.path.exists(self._mydir):
            _makedirs_no_umask(self._mydir)

        attr = _sanitize(attr)
        if attr in self._read_cached_data:
            del self._read_cached_data[attr]
        fn = self._attr2fn(attr)

        if attr.endswith('.tmp'):
            raise AttributeError("Cannot set attribute %s on %s" % (attr, self))

        # Auto hardlink some of the attrs...
        if self._link_yumdb_cache(fn, value):
            return

        # Default write()+rename()... hardlink -c can still help.
        misc.unlink_f(fn + '.tmp')

        fo = _open_no_umask(fn + '.tmp', 'w')
        try:
            dnf.pycomp.write_to_file(fo, value)
        except (OSError, IOError) as e:
            raise AttributeError("Cannot set attribute %s on %s" % (attr, self))

        fo.flush()
        fo.close()
        del fo
        os.rename(fn +  '.tmp', fn) # even works on ext4 now!:o

        self._auto_cache(attr, value, fn)

    def _read(self, attr):
        attr = _sanitize(attr)

        if attr in self._read_cached_data:
            return self._read_cached_data[attr]
        fn = self._attr2fn(attr)

        if attr.endswith('.tmp'):
            raise AttributeError("%s has no attribute %s" % (self, attr))

        info = misc.stat_f(fn, ignore_EACCES=True)
        if info is None:
            raise AttributeError("%s has no attribute %s" % (self, attr))

        if info.st_nlink > 1 and self._yumdb_cache is not None:
            key = (info.st_dev, info.st_ino)
            if key in self._yumdb_cache:
                self._auto_cache(attr, self._yumdb_cache[key], fn, info)
                return self._read_cached_data[attr]

        fo, e = _iopen(fn)
        if fo is None: # This really sucks, don't do that.
            return '<E:%d>' % e.errno
        value = fo.read()
        fo.close()
        del fo

        if info.st_nlink > 1 and self._yumdb_cache is not None:
            self._yumdb_cache[key] = value
        self._auto_cache(attr, value, fn, info)

        return value

    def _delete(self, attr):
        """remove the attribute file"""

        attr = _sanitize(attr)
        fn = self._attr2fn(attr)
        if attr in self._read_cached_data:
            del self._read_cached_data[attr]
        self._unlink_yumdb_cache(fn)
        if os.path.exists(fn):
            try:
                os.unlink(fn)
            except (IOError, OSError):
                raise AttributeError("Cannot delete attribute %s on %s " % (attr, self))

    def __getattr__(self, attr):
        return self._read(attr)

    def __setattr__(self, attr, value):
        if not attr.startswith('_'):
            self._write(attr, value)
        else:
            object.__setattr__(self, attr, value)

    def __delattr__(self, attr):
        if not attr.startswith('_'):
            self._delete(attr)
        else:
            object.__delattr__(self, attr)

    def __contains__(self, attr):
        #  This is faster than __iter__ and it makes things fail in a much more
        # obvious way in weird FS corruption cases like: BZ 593436
        x = self.get(attr)
        return x is not None

    def __iter__(self, show_hidden=False):
        for item in self._read_cached_data:
            yield item
        for item in glob.glob(self._mydir + '/*'):
            item = item[(len(self._mydir) + 1):]
            if item in self._read_cached_data:
                continue
            if not show_hidden and item.endswith('.tmp'):
                continue
            yield item

    def clean(self):
        # purge out everything
        for item in self.__iter__(show_hidden=True):
            self._delete(item)
        try:
            os.rmdir(self._mydir)
        except OSError:
            pass

    def get(self, attr, default=None):
        """retrieve an add'l data obj"""

        try:
            res = self._read(attr)
        except AttributeError:
            return default
        return res

########NEW FILE########
__FILENAME__ = rpmtrans
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
# Copyright 2005 Duke University
# Parts Copyright 2007 Red Hat, Inc

from __future__ import print_function, absolute_import
from __future__ import unicode_literals
from dnf.i18n import _
from dnf.pycomp import basestring
import dnf.transaction
import rpm
import os
import fcntl
import time
import logging
import sys
from . import misc
import tempfile
import collections

# transaction set states
TS_UPDATE = 10
TS_INSTALL = 20
TS_ERASE = 40
TS_OBSOLETED = 50
TS_OBSOLETING = 60
TS_AVAILABLE = 70
TS_UPDATED = 90
TS_FAILED = 100

TS_INSTALL_STATES = [TS_INSTALL, TS_UPDATE, TS_OBSOLETING]
TS_REMOVE_STATES = [TS_ERASE, TS_OBSOLETED, TS_UPDATED]

class TransactionDisplay(object):
    # per-package events
    PKG_CLEANUP   = 1
    PKG_DOWNGRADE = 2
    PKG_ERASE     = 3
    PKG_INSTALL   = 4
    PKG_OBSOLETE  = 5
    PKG_REINSTALL = 6
    PKG_UPGRADE   = 7

    # transaction-wide events
    TRANS_POST      = 10

    ACTION_FROM_OP_TYPE = {
        dnf.transaction.DOWNGRADE : PKG_DOWNGRADE,
        dnf.transaction.ERASE     : PKG_ERASE,
        dnf.transaction.INSTALL   : PKG_INSTALL,
        dnf.transaction.REINSTALL : PKG_REINSTALL,
        dnf.transaction.UPGRADE   : PKG_UPGRADE
        }

    def __init__(self):
        pass

    def event(self, package, action, te_current, te_total, ts_current, ts_total):
        """
        @param package: A yum package object or simple string of a package name
        @param action: A constant transaction set state
        @param te_current: current number of bytes processed in the transaction
                           element being processed
        @param te_total: total number of bytes in the transaction element being
                         processed
        @param ts_current: number of processes completed in whole transaction
        @param ts_total: total number of processes in the transaction.
        """
        # this is where a progress bar would be called

        pass

    def scriptout(self, msgs):
        """msgs is the messages that were output (if any)."""
        pass

    def errorlog(self, msg):
        """takes a simple error msg string"""
        print(msg, file=sys.stderr)

    def filelog(self, package, action):
        # check package object type - if it is a string - just output it
        """package is the same as in event() - a package object or simple string
           action is also the same as in event()"""
        pass

    def verify_tsi_package(self, pkg, count, total):
        pass

class LoggingTransactionDisplay(TransactionDisplay):
    '''
    Base class for a RPMTransaction display callback class
    '''
    def __init__(self):
        super(LoggingTransactionDisplay, self).__init__()
        self.action = {self.PKG_CLEANUP   : _('Cleanup'),
                       self.PKG_DOWNGRADE : _('Downgrading'),
                       self.PKG_ERASE     : _('Erasing'),
                       self.PKG_INSTALL   : _('Installing'),
                       self.PKG_OBSOLETE  : _('Obsoleting'),
                       self.PKG_REINSTALL : _('Reinstalling'),
                       self.PKG_UPGRADE   : _('Upgrading')}
        self.fileaction = {self.PKG_CLEANUP   : 'Cleanup',
                           self.PKG_DOWNGRADE : 'Downgraded',
                           self.PKG_ERASE     : 'Erased',
                           self.PKG_INSTALL   : 'Installed',
                           self.PKG_OBSOLETE  : 'Obsoleted',
                           self.PKG_REINSTALL : 'Reinstalled',
                           self.PKG_UPGRADE   :  'Upgraded'}
        self.logger = logging.getLogger("dnf.rpm")

    def errorlog(self, msg):
        super(LoggingTransactionDisplay, self).errorlog(msg)
        self.logger.error(msg)

    def filelog(self, package, action):
        # If the action is not in the fileaction list then dump it as a string
        # hurky but, sadly, not much else
        process = self.fileaction[action]
        if process is None:
            return
        msg = '%s: %s' % (process, package)
        self.logger.info(msg)

class RPMTransaction(object):
    def __init__(self, base, test=False, display=TransactionDisplay):
        self.display = display
        if isinstance(display, collections.Callable):
            self.display = display()
        self.base = base # base yum object b/c we need so much
        self.test = test # are we a test?
        self.trans_running = False
        self.fd = None
        self.total_actions = 0
        self.total_installed = 0
        self.complete_actions = 0
        self.installed_pkg_names = set()
        self.total_removed = 0
        self.logger = logging.getLogger("dnf.rpm")
        self.filelog = False

        self._setupOutputLogging(base.conf.rpmverbosity)
        self._ts_done = None

    def _fdSetCloseOnExec(self, fd):
        """ Set the close on exec. flag for a filedescriptor. """
        flag = fcntl.FD_CLOEXEC
        current_flags = fcntl.fcntl(fd, fcntl.F_GETFD)
        if current_flags & flag:
            return
        fcntl.fcntl(fd, fcntl.F_SETFD, current_flags | flag)

    def _setupOutputLogging(self, rpmverbosity="info"):
        # UGLY... set up the transaction to record output from scriptlets
        io_r = tempfile.NamedTemporaryFile()
        self._readpipe = io_r
        self._writepipe = open(io_r.name, 'w+b')
        self.base.ts.setScriptFd(self._writepipe)
        rpmverbosity = {'critical' : 'crit',
                        'emergency' : 'emerg',
                        'error' : 'err',
                        'information' : 'info',
                        'warn' : 'warning'}.get(rpmverbosity, rpmverbosity)
        rpmverbosity = 'RPMLOG_' + rpmverbosity.upper()
        if not hasattr(rpm, rpmverbosity):
            rpmverbosity = 'RPMLOG_INFO'
        rpm.setVerbosity(getattr(rpm, rpmverbosity))
        rpm.setLogFile(self._writepipe)

    def _shutdownOutputLogging(self):
        # reset rpm bits from reording output
        rpm.setVerbosity(rpm.RPMLOG_NOTICE)
        rpm.setLogFile(sys.stderr)
        try:
            self._writepipe.close()
        except:
            pass

    def _scriptOutput(self):
        try:
            out = self._readpipe.read()
            if not out:
                return None
            return out
        except IOError:
            pass

    def _scriptout(self):
        msgs = self._scriptOutput()
        self.display.scriptout(msgs)
        self.base.history.log_scriptlet_output(msgs)

    def __del__(self):
        self._shutdownOutputLogging()

    def _extract_cbkey(self, cbkey):
        if isinstance(cbkey, dnf.transaction.TransactionItem):
            return self._extract_tsi_cbkey(cbkey)
        else:
            return self._extract_str_cbkey(cbkey)

    @staticmethod
    def _extract_tsi_cbkey(tsi):
        assert(isinstance(tsi, dnf.transaction.TransactionItem))
        return tsi.active, tsi.active_history_state, tsi

    def _extract_str_cbkey(self, name):
        assert(isinstance(name, basestring))
        obsoleted = obsoleted_state = obsoleted_tsi = None
        for tsi in self.base.transaction:
            # only walk the tsis once. prefer finding an erase over an obsoleted
            # package:
            if tsi.erased is not None and tsi.erased.name == name:
                return tsi.erased, tsi.erased_history_state, tsi
            for o in tsi.obsoleted:
                if o.name == name:
                    obsoleted = o
                    obsoleted_state = tsi.obsoleted_history_state
                    obsoleted_tsi = tsi
        return obsoleted, obsoleted_state, obsoleted_tsi

    def _fn_rm_installroot(self, filename):
        """ Remove the installroot from the filename. """
        # to handle us being inside a chroot at this point
        # we hand back the right path to those 'outside' of the chroot() calls
        # but we're using the right path inside.
        if self.base.conf.installroot == '/':
            return filename

        return filename.replace(os.path.normpath(self.base.conf.installroot),'')

    def ts_done_open(self):
        """ Open the transaction done file, must be started outside the
            chroot. """

        if self.test:
            return False
        if self._ts_done is not None:
            return True

        self.ts_done_fn = '%s/transaction-done.%s' % (self.base.conf.persistdir,
                                                      self._ts_time)
        ts_done_fn = self._fn_rm_installroot(self.ts_done_fn)

        try:
            self._ts_done = open(ts_done_fn, 'w')
        except (IOError, OSError) as e:
            self.display.errorlog('could not open ts_done file: %s' % e)
            self._ts_done = None
            return False
        self._fdSetCloseOnExec(self._ts_done.fileno())
        return True

    def ts_done_write(self, msg):
        """ Write some data to the transaction done file. """
        if self._ts_done is None:
            return

        try:
            self._ts_done.write(msg)
            self._ts_done.flush()
        except (IOError, OSError) as e:
            #  Having incomplete transactions is probably worse than having
            # nothing.
            self.display.errorlog('could not write to ts_done file: %s' % e)
            self._ts_done = None
            misc.unlink_f(self.ts_done_fn)

    def ts_done(self, package, action):
        """writes out the portions of the transaction which have completed"""

        if not self.ts_done_open(): return

        # walk back through self._te_tuples
        # make sure the package and the action make some kind of sense
        # write it out and pop(0) from the list

        # make sure we have a list to work from
        if len(self._te_tuples) == 0:
            # if we don't then this is pretrans or postrans or a trigger
            # either way we have to respond correctly so just return and don't
            # emit anything
            return

        (t,e,n,v,r,a) = self._te_tuples[0] # what we should be on

        # make sure we're in the right action state
        msg = 'ts_done state is %s %s should be %s %s' % (package, action, t, n)
        if action in TS_REMOVE_STATES:
            if t != 'erase':
                self.display.filelog(package, msg)
        if action in TS_INSTALL_STATES:
            if t != 'install':
                self.display.filelog(package, msg)

        # check the pkg name out to make sure it matches
        if isinstance(package, basestring):
            name = package
        else:
            name = package.name

        if n != name:
            msg = 'ts_done name in te is %s should be %s' % (n, package)
            self.display.filelog(package, msg)

        # hope springs eternal that this isn't wrong
        msg = '%s %s:%s-%s-%s.%s\n' % (t,e,n,v,r,a)

        self.ts_done_write(msg)
        self._te_tuples.pop(0)

    def ts_all(self):
        """write out what our transaction will do"""

        # save the transaction elements into a list so we can run across them
        if not hasattr(self, '_te_tuples'):
            self._te_tuples = []

        for te in self.base.ts:
            n = te.N()
            a = te.A()
            v = te.V()
            r = te.R()
            e = te.E()
            if e is None:
                e = '0'
            if te.Type() == 1:
                t = 'install'
            elif te.Type() == 2:
                t = 'erase'
            else:
                t = te.Type()

            # save this in a list
            self._te_tuples.append((t,e,n,v,r,a))

        # write to a file
        self._ts_time = time.strftime('%Y-%m-%d.%H:%M.%S')
        tsfn = '%s/transaction-all.%s' % (self.base.conf.persistdir, self._ts_time)
        self.ts_all_fn = tsfn
        tsfn = self._fn_rm_installroot(tsfn)

        try:
            if not os.path.exists(os.path.dirname(tsfn)):
                os.makedirs(os.path.dirname(tsfn)) # make the dir,
            fo = open(tsfn, 'w')
        except (IOError, OSError) as e:
            self.display.errorlog('could not open ts_all file: %s' % e)
            self._ts_done = None
            return

        try:
            for (t,e,n,v,r,a) in self._te_tuples:
                msg = "%s %s:%s-%s-%s.%s\n" % (t,e,n,v,r,a)
                fo.write(msg)
            fo.flush()
            fo.close()
        except (IOError, OSError) as e:
            #  Having incomplete transactions is probably worse than having
            # nothing.
            self.display.errorlog('could not write to ts_all file: %s' % e)
            misc.unlink_f(tsfn)
            self._ts_done = None

    def callback( self, what, bytes, total, h, user ):
        if what == rpm.RPMCALLBACK_TRANS_START:
            self._transStart( bytes, total, h )
        elif what == rpm.RPMCALLBACK_TRANS_PROGRESS:
            self._transProgress( bytes, total, h )
        elif what == rpm.RPMCALLBACK_TRANS_STOP:
            self._transStop( bytes, total, h )
        elif what == rpm.RPMCALLBACK_INST_OPEN_FILE:
            return self._instOpenFile( bytes, total, h )
        elif what == rpm.RPMCALLBACK_INST_CLOSE_FILE:
            self._instCloseFile(  bytes, total, h )
        elif what == rpm.RPMCALLBACK_INST_PROGRESS:
            self._instProgress( bytes, total, h )
        elif what == rpm.RPMCALLBACK_UNINST_START:
            self._unInstStart( bytes, total, h )
        elif what == rpm.RPMCALLBACK_UNINST_PROGRESS:
            self._unInstProgress( bytes, total, h )
        elif what == rpm.RPMCALLBACK_UNINST_STOP:
            self._unInstStop( bytes, total, h )
        elif what == rpm.RPMCALLBACK_CPIO_ERROR:
            self._cpioError(bytes, total, h)
        elif what == rpm.RPMCALLBACK_UNPACK_ERROR:
            self._unpackError(bytes, total, h)
        elif what == rpm.RPMCALLBACK_SCRIPT_ERROR:
            self._scriptError(bytes, total, h)
        elif what == rpm.RPMCALLBACK_SCRIPT_START:
            self._scriptStart(bytes, total, h)
        elif what == rpm.RPMCALLBACK_SCRIPT_STOP:
            self._scriptStop(bytes, total, h);


    def _transStart(self, bytes, total, h):
        self.total_actions = total
        if self.test: return
        self.trans_running = True
        self.ts_all() # write out what transaction will do
        self.ts_done_open()

    def _transProgress(self, bytes, total, h):
        pass

    def _transStop(self, bytes, total, h):
        if self._ts_done is not None:
            self._ts_done.close()

    def _instOpenFile(self, bytes, total, h):
        self.lastmsg = None
        pkg, _, _ = self._extract_tsi_cbkey(h)
        rpmloc = pkg.localPkg()
        try:
            self.fd = open(rpmloc)
        except IOError as e:
            self.display.errorlog("Error: Cannot open file %s: %s" % (rpmloc, e))
        else:
            if self.trans_running:
                self.total_installed += 1
                self.complete_actions += 1
                self.installed_pkg_names.add(pkg.name)
            return self.fd.fileno()

    def _instCloseFile(self, bytes, total, h):
        pkg, state, tsi = self._extract_tsi_cbkey(h)
        self.fd.close()
        self.fd = None

        if self.test or not self.trans_running:
            return

        action = TransactionDisplay.ACTION_FROM_OP_TYPE[tsi.op_type]
        self.display.filelog(pkg, action)
        self._scriptout()
        pid = self.base.history.pkg2pid(pkg)
        self.base.history.trans_data_pid_end(pid, state)
        # :dead
        # self.ts_done(txmbr.po, txmbr.output_state)

        if self.complete_actions == self.total_actions:
            # RPM doesn't explicitly report when post-trans phase starts
            action = TransactionDisplay.TRANS_POST
            self.display.event(None, action, None, None, None, None)

    def _instProgress(self, bytes, total, h):
        pkg, _, tsi = self._extract_tsi_cbkey(h)
        action = TransactionDisplay.ACTION_FROM_OP_TYPE[tsi.op_type]
        self.display.event(pkg, action, bytes, total, self.complete_actions,
                           self.total_actions)

    def _unInstStart(self, bytes, total, h):
        pass

    def _unInstProgress(self, bytes, total, h):
        pass

    def _unInstStop(self, bytes, total, h):
        pkg, state, _ = self._extract_str_cbkey(h)
        self.total_removed += 1
        self.complete_actions += 1
        if state == 'Obsoleted':
            action = TransactionDisplay.PKG_OBSOLETE
        elif state == 'Updated':
            action = TransactionDisplay.PKG_CLEANUP
        else:
            action = TransactionDisplay.PKG_ERASE
        self.display.filelog(pkg, action)
        self.display.event(pkg, action, 100, 100, self.complete_actions,
                           self.total_actions)

        if self.test:
            return

        if state is not None:
            self._scriptout()

            #  Note that we are currently inside the chroot, which makes
            # sqlite panic when it tries to open it's journal file.
            # So let's have some "fun" and workaround that:
            _do_chroot = False
            if _do_chroot and self.base.conf.installroot != '/':
                os.chroot(".")
            pid   = self.base.history.pkg2pid(pkg)
            self.base.history.trans_data_pid_end(pid, state)
            if _do_chroot and self.base.conf.installroot != '/':
                os.chroot(self.base.conf.installroot)
            # :dead
            # self.ts_done(txmbr.po, txmbr.output_state)
        else:
            self._scriptout()
            # :dead
            # self.ts_done(name, action)

    def _cpioError(self, bytes, total, h):
        # In the case of a remove, we only have a name, not a tsi:
        pkg, _, _ = self._extract_cbkey(h)
        msg = "Error in cpio payload of rpm package %s" % pkg
        self.display.errorlog(msg)

    def _unpackError(self, bytes, total, h):
        pkg, _, _ = self._extract_cbkey(h)
        msg = "Error unpacking rpm package %s" % pkg
        self.display.errorlog(msg)

    def _scriptError(self, bytes, total, h):
        # "bytes" carries the failed scriptlet tag,
        # "total" carries fatal/non-fatal status
        scriptlet_name = rpm.tagnames.get(bytes, "<unknown>")

        pkg, _, _ = self._extract_cbkey(h)
        name = pkg.name

        if total:
            msg = ("Error in %s scriptlet in rpm package %s" %
                   (scriptlet_name, name))
        else:
            msg = ("Non-fatal %s scriptlet failure in rpm package %s" %
                   (scriptlet_name, name))
        self.display.errorlog(msg)

    def _scriptStart(self, bytes, total, h):
        pass

    def _scriptStop(self, bytes, total, h):
        self._scriptout()

    def verify_tsi_package(self, pkg, count, total):
        self.display.verify_tsi_package(pkg, count, total)

########NEW FILE########
__FILENAME__ = sqlutils
# This program is free software; you can redistribute it and/or modify
# it under the terms of version 2 of the GNU General Public License
# as published by the Free Software Foundation
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
# Copyright 2005 Duke University

"""
utility functions to handle differences in pysqlite versions
These are from Wichert Akkerman <wichert@deephackmode.org>'s python-dhm
http://www.wiggy.net/code/python-dhm
"""

from __future__ import unicode_literals

try:
    import sqlite3 as sqlite
except ImportError:
    import sqlite

class TokenizeError(Exception):
    """Tokenizer error class"""
    pass

def Tokenize(str, whitespace=" \t\r\n", quotes="\"", escapes="\\"):
    """String tokenizer

    This function tokenizes a string while taking quotation and
    escaping into account.

      >>> import dhm.strtools
      >>> dhm.strtools.Tokenize("this is a test")
      ['this', 'is', 'a', 'test']
      >>> dhm.strtools.Tokenize("this \"is a\" test")
      ['this', 'is a', 'test']
      >>> dhm.strtools.Tokenize("this \\\"is\\\" a test")
      ['this', '"is"', 'a', 'test']
      >>> dhm.strtools.Tokenize("this \"is a test")
      Traceback (most recent call last):
        File "<stdin>", line 1, in ?
        File "/usr/local/lib/python2.2/site-packages/dhm/strtools.py", line 80, in Tokenize
          raise TokenizeError, "Unexpected end of string in quoted text"
      dhm.strtools.TokenizeError: Unexecpted end of string in quoted text

    @param        str: string to tokenize
    @type         str: string
    @param whitespace: whitespace characters seperating tokens
    @type  whitespace: string
    @param     quotes: legal quoting characters
    @type      quotes: string
    @param    escapes: characters which can escape quoting characters
    @type     escapes: string
    @return: list of tokens
    @rtype:  sequence of strings
    """
    (buffer, tokens, curtoken, quote)=(str, [], None, None)

    try:
        while buffer:
            if buffer[0]==quote:
                quote=None
            elif (quote==None) and (buffer[0] in quotes):
                quote=buffer[0]
            elif buffer[0] in whitespace:
                if quote!=None:
                    curtoken+=buffer[0]
                else:
                    tokens.append(curtoken)
                    curtoken=None
                    while buffer[1] in whitespace:
                        buffer=buffer[1:]
            elif buffer[0] in escapes:
                if curtoken==None:
                    curtoken=buffer[1]
                else:
                    curtoken+=buffer[1]
                buffer=buffer[1:]
            else:
                if curtoken==None:
                    curtoken=buffer[0]
                else:
                    curtoken+=buffer[0]

            buffer=buffer[1:]
    except IndexError:
        raise TokenizeError("Unexpected end of string")
    
    if quote:
        raise TokenizeError("Unexpected end of string in quoted text")

    if curtoken!=None:
        tokens.append(curtoken)

    return tokens


def QmarkToPyformat(query, params):
    """Convert from qmark to pyformat parameter style.

    The python DB-API 2.0 specifies four different possible parameter
    styles that can be used by drivers. This function converts from the
    qmark style to pyformat style.

    @param  query: SQL query to transform
    @type   query: string
    @param params: arguments to query
    @type  params: sequence of strings
    @return: converted query and parameters
    @rtype:  tuple with the new command and a dictionary of arguments
    """
    tokens=Tokenize(query, quotes="'")
    output=[]
    count=1
    for token in tokens:
        if token.endswith("?"):
            output.append(token[:-1] + "%%(param%d)s" % count)
            count+=1
        elif token.endswith("?,") or token.endswith("?)"):
            ntoken = token[:-2] + "%%(param%d)s" % count
            ntoken += token[-1]
            output.append(ntoken)
            count+=1
        else:
            output.append(token)

    dict={}
    count=1
    for param in params:
        dict["param%d" % count]=param
        count+=1
    
    return (" ".join(output), dict)


def executeSQLPyFormat(cursor, query, params=None):
    """
    Execute a python < 2.5 (external sqlite module) style query.

    @param cursor: A sqlite cursor
    @param query: The query to execute
    @param params: An optional list of parameters to the query
    """
    if params is None:
        return cursor.execute(query)

    # Leading whitespace confuses QmarkToPyformat()
    query = query.strip()
    (q, p) = QmarkToPyformat(query, params)
    return cursor.execute(q, p)

def executeSQLQmark(cursor, query, params=None):
    """
    Execute a python 2.5 (sqlite3) style query.

    @param cursor: A sqlite cursor
    @param query: The query to execute
    @param params: An optional list of parameters to the query
    """
    if params is None:
        return cursor.execute(query)
    
    return cursor.execute(query, params)

if sqlite.version_info[0] > 1:
    executeSQL = executeSQLQmark
else:
    executeSQL = executeSQLPyFormat


def sql_esc(pattern):
    """ Apply SQLite escaping, if needed. Returns pattern and esc. """
    esc = ''
    if "_" in pattern or "%" in pattern:
        esc = ' ESCAPE "!"'
        pattern = pattern.replace("!", "!!")
        pattern = pattern.replace("%", "!%")
        pattern = pattern.replace("_", "!_")
    return (pattern, esc)

def sql_esc_glob(patterns):
    """ Converts patterns to SQL LIKE format, if required (or gives up if
        not possible). """
    ret = []
    for pattern in patterns:
        if '[' in pattern: # LIKE only has % and _, so [abc] can't be done.
            return []      # So Load everything

        # Convert to SQL LIKE format
        (pattern, esc) = sql_esc(pattern)
        pattern = pattern.replace("*", "%")
        pattern = pattern.replace("?", "_")
        ret.append((pattern, esc))
    return ret

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Hawkey documentation build configuration file, created by
# sphinx-quickstart on Tue Aug  7 11:06:24 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import os
import re
import sys

_dirname = os.path.dirname(__file__)

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.

sys.path.insert(0, _dirname)

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.

extensions = ['rhbug']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'DNF'
copyright = u'2012-2014, Red Hat, Licensed under GPLv2+'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.

def version_readout():
    def submatch(sub, in_str):
        pat = re.compile('SET\(DNF_%s "(\d+)"\)' % sub)
        return pat.match(in_str).group(1)

    fn = os.path.join(_dirname, '../VERSION.cmake')
    with open(fn) as f:
        lines = f.readlines()

    major = submatch('MAJOR', lines[0])
    minor = submatch('MINOR', lines[1])
    patch = submatch('PATCH', lines[2])
    return (major, minor, patch)

version = '%s.%s.%s' % version_readout()
# The full version, including alpha/beta/rc tags.
release = '%s-1' % version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
# html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'dnfdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'dnf.tex', u'DNF Documentation',
   u'Ale Kozumplk', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('command_ref', 'dnf', u'DNF Command Reference',
     [u'Ale Kozumplk'], 8),
    ('conf_ref', 'dnf.conf', u'DNF Configuration Reference',
     [u'Ale Kozumplk'], 8)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'dnf', u'DNF Documentation',
   u'Ale Kozumplk', 'DNF', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

rst_prolog = """
.. default-domain:: py
.. _DNF: https://github.com/akozumpl/dnf/
.. _hawkey: http://akozumpl.github.io/hawkey/
.. _Yum: http://yum.baseurl.org/
.. _bugzilla: https://bugzilla.redhat.com/enter_bug.cgi?product=Fedora&component=dnf
"""

########NEW FILE########
__FILENAME__ = rhbug
# rhbug.py
# rhbug Sphinx extension.
#
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from docutils import nodes

try:
    import bugzilla
except ImportError:
    bugzilla = None
import json
import os

class Summary(object):
    def __init__(self, cache_fn):
        self.cache_fn = cache_fn

    def __call__(self, bug_id):
        bug_id = int(bug_id)
        summary = self._from_cache(bug_id)
        if summary is not None:
            return summary
        summary = self._from_bugzilla(bug_id)
        self._store_in_cache(bug_id, summary)
        return summary

    def _from_bugzilla(self, bug_id):
        if bugzilla is None:
            return ''
        rhbz = bugzilla.RHBugzilla(url="https://bugzilla.redhat.com/xmlrpc.cgi")
        query = rhbz.build_query(bug_id=bug_id)
        bug = rhbz.query(query)[0]
        return bug.summary

    def _from_cache(self, bug_id):
        try:
            with open(self.cache_fn, 'r') as json_file:
                cache = json.load(json_file)
                summary = [entry[1] for entry in cache if entry[0] == bug_id]
                return summary[0]
        except (IOError, IndexError):
            return None

    def _store_in_cache(self, bug_id, summary):
        if bugzilla is None:
            return
        try:
            with open(self.cache_fn, 'r') as json_file:
                cache = json.load(json_file)
        except IOError:
            cache = []
        cache.append((bug_id, summary))
        with open(self.cache_fn, 'w') as json_file:
            json.dump(cache, json_file, indent=4)

def RhBug_role(role, rawtext, text, lineno, inliner, options={}, content=[]):
    source = inliner.document.settings._source
    summaries_fn = '%s/summaries_cache' % os.path.dirname(source)
    summary = Summary(summaries_fn)(text)
    link_name = 'Bug %s - %s' % (text, summary)
    url = 'https://bugzilla.redhat.com/show_bug.cgi?id=%s' % text
    node = nodes.reference(rawtext, link_name, refuri=url)
    return [node], []

def setup(app):
    app.add_role('rhbug', RhBug_role)

########NEW FILE########
__FILENAME__ = test_autoerase
# Copyright (C) 2014 Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from tests import support

import dnf.cli.commands.autoerase as autoerase


class AutoEraseCommandTest(support.ResultTestCase):

    def test_run(self):
        base = support.MockBase()
        q = base.sack.query()
        pkgs = list(q.filter(name='librita')) + list(q.filter(name='pepper'))
        yumdb = base.yumdb
        for pkg in pkgs:
            yumdb.get_package(pkg).reason = 'dep'

        cli = base.mock_cli()
        cmd = autoerase.AutoeraseCommand(cli)
        cmd.run([])
        inst, rem = self.installed_removed(base)
        self.assertEmpty(inst)
        removed = ('librita-1-1.i686', 'librita-1-1.x86_64', 'pepper-20-0.x86_64')
        self.assertItemsEqual((map(str, rem)), removed)

########NEW FILE########
__FILENAME__ = test_group
# Copyright (C) 2014 Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from tests import support

import dnf.cli.commands.group as group
import dnf.comps

def names(items):
    return (it.name for it in items)

class GroupCommandStaticTest(support.TestCase):

    def test_canonical(self):
        cmd = group.GroupCommand(None)
        (basecmd, extcmds) = cmd.canonical(['grouplist', 'crack'])
        self.assertEqual(basecmd, 'groups')
        self.assertEqual(extcmds, ['list', 'crack'])

        (_, extcmds) = cmd.canonical(['groups'])
        self.assertEqual(extcmds, ['summary'])

        (_, extcmds) = cmd.canonical(['group', 'info', 'crack'])
        self.assertEqual(extcmds, ['info', 'crack'])

        (_, extcmds) = cmd.canonical(['group', 'update', 'crack'])
        self.assertEqual(extcmds, ['upgrade', 'crack'])

    def test_split_extcmds(self):
        split = group.GroupCommand._split_extcmds(['with-optional', 'crack'])
        self.assertEqual(split, (('mandatory', 'default', 'optional'), ['crack']))
        split = group.GroupCommand._split_extcmds(['crack'])
        self.assertEqual(split, (('mandatory', 'default'), ['crack']))


class GroupCommandTest(support.TestCase):
    def setUp(self):
        base = support.MockBase("main")
        base.read_mock_comps()
        base.init_sack()
        self.cmd = group.GroupCommand(base.mock_cli())

    def test_environment_list(self):
        env_inst, env_avail = self.cmd._environment_lists(['sugar*'])
        self.assertLength(env_inst, 1)
        self.assertLength(env_avail, 0)
        self.assertEqual(env_inst[0].name, 'Sugar Desktop Environment')


class CompsQueryTest(support.TestCase):

    def setUp(self):
        (self.comps, self.prst) = support.mock_comps(True)

    def test_all(self):
        status_all = group.CompsQuery.AVAILABLE | group.CompsQuery.INSTALLED
        kinds_all = group.CompsQuery.ENVIRONMENTS | group.CompsQuery.GROUPS
        q = group.CompsQuery(self.comps, self.prst, kinds_all, status_all)

        res = q.get('sugar*', '*er*')
        self.assertItemsEqual(names(res.environments),
                              ('Sugar Desktop Environment',))
        self.assertItemsEqual(names(res.groups), ("Pepper's", 'Solid Ground'))

    def test_err(self):
        q = group.CompsQuery(self.comps, self.prst, group.CompsQuery.ENVIRONMENTS,
                             group.CompsQuery.AVAILABLE)
        with self.assertRaises(dnf.cli.CliError):
            q.get('*er*')

    def test_installed(self):
        q = group.CompsQuery(self.comps, self.prst, group.CompsQuery.GROUPS,
                             group.CompsQuery.INSTALLED)
        res = q.get('somerset')
        self.assertEmpty(res.environments)
        self.assertItemsEqual(names(res.groups), ('Solid Ground',))

########NEW FILE########
__FILENAME__ = test_search
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from tests import support
from tests import mock

import dnf.cli.commands.search as search
import dnf.match_counter
import dnf.pycomp

class SearchCountedTest(support.TestCase):
    def setUp(self):
        base = support.MockBase("main")
        self.cmd = search.SearchCommand(base.mock_cli())

    def test_search_counted(self):
        counter = dnf.match_counter.MatchCounter()
        self.cmd._search_counted(counter, 'summary', 'ation')
        self.assertEqual(len(counter), 2)
        haystacks = set()
        for pkg in counter:
            haystacks.update(counter.matched_haystacks(pkg))
        self.assertItemsEqual(haystacks, ["It's an invitation.",
                                          "Make a reservation."])

    def test_search_counted_glob(self):
        counter = dnf.match_counter.MatchCounter()
        self.cmd._search_counted(counter, 'summary', '*invit*')
        self.assertEqual(len(counter), 1)

class SearchTest(support.TestCase):
    def setUp(self):
        self.base = support.MockBase("search")
        self.base.output = mock.MagicMock()
        self.base.output.fmtSection = lambda str: str
        self.cmd = search.SearchCommand(self.base.mock_cli())

    def patched_search(self, *args, **kwargs):
        with support.patch_std_streams() as (stdout, _):
            self.cmd._search(*args, **kwargs)
            call_args = self.base.output.matchcallback.call_args_list
            pkgs = [c[0][0] for c in call_args]
            return (stdout.getvalue(), pkgs)

    def test_search(self):
        (_, pkgs) = self.patched_search(['lotus'])
        pkg_names = list(map(str, pkgs))
        self.assertIn('lotus-3-16.i686', pkg_names)
        self.assertIn('lotus-3-16.x86_64', pkg_names)

    @mock.patch('dnf.cli.commands.search._',
                dnf.pycomp.NullTranslations().ugettext)
    def test_search_caseness(self):
        (stdout, pkgs) = self.patched_search(['LOTUS'])
        self.assertEqual(stdout, 'N/S Matched: LOTUS\n')
        pkg_names = map(str, pkgs)
        self.assertIn('lotus-3-16.i686', pkg_names)
        self.assertIn('lotus-3-16.x86_64', pkg_names)

########NEW FILE########
__FILENAME__ = test_demand
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.

from __future__ import absolute_import
from __future__ import unicode_literals

import dnf.cli.demand
import tests.support

class DemandTest(tests.support.TestCase):

    def test_bool_default(self):
        demands = dnf.cli.demand.DemandSheet()
        demands.resolving = True
        self.assertTrue(demands.resolving)
        demands.resolving = True
        self.assertTrue(demands.resolving)
        with self.assertRaises(AttributeError):
            demands.resolving = False

    def test_default(self):
        demands = dnf.cli.demand.DemandSheet()
        self.assertFalse(demands.resolving)
        self.assertFalse(demands.sack_activation)
        self.assertFalse(demands.root_user)
        self.assertEqual(demands.success_exit_status, 0)

    def test_independence(self):
        d1 = dnf.cli.demand.DemandSheet()
        d1.resolving = True
        d2 = dnf.cli.demand.DemandSheet()
        self.assertTrue(d1.resolving)
        self.assertFalse(d2.resolving)

########NEW FILE########
__FILENAME__ = test_option_parser
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.cli.option_parser import OptionParser
from tests import support
from tests.support import mock

import argparse
import dnf.cli.commands
import dnf.pycomp
import dnf.util


class OptionParserTest(support.TestCase):
    def test_parse(self):
        parser = OptionParser()
        opts, cmds = parser.parse_known_args(['update', '--nogpgcheck'])
        self.assertEqual(cmds, ['update'])
        self.assertTrue(opts.nogpgcheck)
        self.assertIsNone(opts.color)

    def test_configure_from_options(self):
        parser = OptionParser()
        opts, _ = parser.parse_known_args(['update', '-y', '--allowerasing'])
        conf = dnf.util.Bunch()
        conf.color = 'auto'
        demands = dnf.util.Bunch()
        parser.configure_from_options(opts, conf, demands, None)
        self.assertTrue(demands.allow_erasing)
        self.assertTrue(conf.assumeyes)

    def test_non_nones2dict(self):
        parser = OptionParser()
        values = parser.parse_args(args=['-y'])
        self.assertIsInstance(values, argparse.Namespace)
        dct = parser._non_nones2dict(values.__dict__)
        self.assertTrue(dct['assumeyes'])


class MyTestCommand(dnf.cli.commands.Command):

    aliases = ["test-cmd"]
    summary = 'summary'
    usage = 'usage'

    def __init__(self, cli):
        dnf.cli.commands.Command.__init__(self, cli)


class MyTestCommand2(dnf.cli.commands.Command):

    aliases = ["test-cmd2"]
    summary = 'summary2'
    usage = 'usage2'

    def __init__(self, cli):
        dnf.cli.commands.Command.__init__(self, cli)


class OptionParserAddCmdTest(support.TestCase):

    def setUp(self):
        self.cli_commands = {}
        self.parser = OptionParser()
        self.parser._ = dnf.pycomp.NullTranslations().ugettext
        self.cli = mock.Mock()

    def _register_command(self, command_cls):
        """ helper for simulate dnf.cli.cli.Cli.register_Command()"""
        for name in command_cls.aliases:
            self.cli_commands[name] = command_cls

    def test_add_commands(self):
        cmd = MyTestCommand(self.cli)
        self._register_command(cmd)
        self.parser.add_commands(self.cli_commands, "main")
        name = cmd.aliases[0]
        self.assertTrue(name in self.parser._cmd_usage)
        group, summary = self.parser._cmd_usage[name]
        self.assertEqual(group, 'main')
        self.assertEqual(summary, cmd.summary)
        self.assertEqual(self.parser._cmd_groups, set(['main']))

    def test_add_commands_only_once(self):
        cmd = MyTestCommand(self.cli)
        self._register_command(cmd)
        self.parser.add_commands(self.cli_commands, "main")
        cmd = MyTestCommand(self.cli)
        self._register_command(cmd)
        self.parser.add_commands(self.cli_commands, "plugin")
        self.assertEqual(len(self.parser._cmd_usage.keys()), 1)
        self.assertEqual(self.parser._cmd_groups, set(['main']))

    def test_cmd_groups(self):
        cmd = MyTestCommand(self.cli)
        self._register_command(cmd)
        self.parser.add_commands(self.cli_commands, "main")
        cmd = MyTestCommand2(self.cli)
        self._register_command(cmd)
        self.parser.add_commands(self.cli_commands, "plugin")
        self.assertEqual(len(self.parser._cmd_groups), 2)
        self.assertEqual(self.parser._cmd_groups, set(['main', 'plugin']))

    def test_help_option_set(self):
        opts, cmds = self.parser.parse_known_args(['-h'])
        self.assertTrue(opts.help)

    def test_help_option_notset(self):
        opts, cmds = self.parser.parse_known_args(['foo', 'bar'])
        self.assertFalse(opts.help)

    def test_get_usage(self):
        output = [
            u'dnf [options] COMMAND',
            u'',
            u'List of Main Commands',
            u'',
            u'test-cmd                  summary',
            u'',
            u'List of Plugin Commands',
            u'',
            u'test-cmd2                 summary2',
            u'']
        cmd = MyTestCommand(self.cli)
        self._register_command(cmd)
        self.parser.add_commands(self.cli_commands, "main")
        cmd2 = MyTestCommand2(self.cli)
        self._register_command(cmd2)
        self.parser.add_commands(self.cli_commands, "plugin")
        self.assertEqual(len(self.parser._cmd_usage.keys()), 2)
        usage = self.parser.get_usage().split('\n')
        self.assertEqual(usage, output)

########NEW FILE########
__FILENAME__ = mock
# mock.py
# Test tools for mocking and patching.
# Copyright (C) 2007-2012 Michael Foord & the mock team
# E-mail: fuzzyman AT voidspace DOT org DOT uk

# mock 1.0.1
# http://www.voidspace.org.uk/python/mock/

# Released subject to the BSD License
# Please see http://www.voidspace.org.uk/python/license.shtml

__all__ = (
    'Mock',
    'MagicMock',
    'patch',
    'sentinel',
    'DEFAULT',
    'ANY',
    'call',
    'create_autospec',
    'FILTER_DIR',
    'NonCallableMock',
    'NonCallableMagicMock',
    'mock_open',
    'PropertyMock',
)


__version__ = '1.0.1'


import pprint
import sys

try:
    import inspect
except ImportError:
    # for alternative platforms that
    # may not have inspect
    inspect = None

try:
    from functools import wraps as original_wraps
except ImportError:
    # Python 2.4 compatibility
    def wraps(original):
        def inner(f):
            f.__name__ = original.__name__
            f.__doc__ = original.__doc__
            f.__module__ = original.__module__
            wrapped = getattr(original, '__wrapped__', original)
            f.__wrapped__ = wrapped
            return f
        return inner
else:
    if sys.version_info[:2] >= (3, 2):
        wraps = original_wraps
    else:
        def wraps(func):
            def inner(f):
                f = original_wraps(func)(f)
                wrapped = getattr(func, '__wrapped__', func)
                f.__wrapped__ = wrapped
                return f
            return inner

try:
    unicode
except NameError:
    # Python 3
    basestring = unicode = str

try:
    long
except NameError:
    # Python 3
    long = int

try:
    BaseException
except NameError:
    # Python 2.4 compatibility
    BaseException = Exception

try:
    next
except NameError:
    def next(obj):
        return obj.next()


BaseExceptions = (BaseException,)
if 'java' in sys.platform:
    # jython
    import java
    BaseExceptions = (BaseException, java.lang.Throwable)

try:
    _isidentifier = str.isidentifier
except AttributeError:
    # Python 2.X
    import keyword
    import re
    regex = re.compile(r'^[a-z_][a-z0-9_]*$', re.I)
    def _isidentifier(string):
        if string in keyword.kwlist:
            return False
        return regex.match(string)


inPy3k = sys.version_info[0] == 3

# Needed to work around Python 3 bug where use of "super" interferes with
# defining __class__ as a descriptor
_super = super

self = 'im_self'
builtin = '__builtin__'
if inPy3k:
    self = '__self__'
    builtin = 'builtins'

FILTER_DIR = True


def _is_instance_mock(obj):
    # can't use isinstance on Mock objects because they override __class__
    # The base class for all mocks is NonCallableMock
    return issubclass(type(obj), NonCallableMock)


def _is_exception(obj):
    return (
        isinstance(obj, BaseExceptions) or
        isinstance(obj, ClassTypes) and issubclass(obj, BaseExceptions)
    )


class _slotted(object):
    __slots__ = ['a']


DescriptorTypes = (
    type(_slotted.a),
    property,
)


def _getsignature(func, skipfirst, instance=False):
    if inspect is None:
        raise ImportError('inspect module not available')

    if isinstance(func, ClassTypes) and not instance:
        try:
            func = func.__init__
        except AttributeError:
            return
        skipfirst = True
    elif not isinstance(func, FunctionTypes):
        # for classes where instance is True we end up here too
        try:
            func = func.__call__
        except AttributeError:
            return

    if inPy3k:
        try:
            argspec = inspect.getfullargspec(func)
        except TypeError:
            # C function / method, possibly inherited object().__init__
            return
        regargs, varargs, varkw, defaults, kwonly, kwonlydef, ann = argspec
    else:
        try:
            regargs, varargs, varkwargs, defaults = inspect.getargspec(func)
        except TypeError:
            # C function / method, possibly inherited object().__init__
            return

    # instance methods and classmethods need to lose the self argument
    if getattr(func, self, None) is not None:
        regargs = regargs[1:]
    if skipfirst:
        # this condition and the above one are never both True - why?
        regargs = regargs[1:]

    if inPy3k:
        signature = inspect.formatargspec(
            regargs, varargs, varkw, defaults,
            kwonly, kwonlydef, ann, formatvalue=lambda value: "")
    else:
        signature = inspect.formatargspec(
            regargs, varargs, varkwargs, defaults,
            formatvalue=lambda value: "")
    return signature[1:-1], func


def _check_signature(func, mock, skipfirst, instance=False):
    if not _callable(func):
        return

    result = _getsignature(func, skipfirst, instance)
    if result is None:
        return
    signature, func = result

    # can't use self because "self" is common as an argument name
    # unfortunately even not in the first place
    src = "lambda _mock_self, %s: None" % signature
    checksig = eval(src, {})
    _copy_func_details(func, checksig)
    type(mock)._mock_check_sig = checksig


def _copy_func_details(func, funcopy):
    funcopy.__name__ = func.__name__
    funcopy.__doc__ = func.__doc__
    #funcopy.__dict__.update(func.__dict__)
    funcopy.__module__ = func.__module__
    if not inPy3k:
        funcopy.func_defaults = func.func_defaults
        return
    funcopy.__defaults__ = func.__defaults__
    funcopy.__kwdefaults__ = func.__kwdefaults__


def _callable(obj):
    if isinstance(obj, ClassTypes):
        return True
    if getattr(obj, '__call__', None) is not None:
        return True
    return False


def _is_list(obj):
    # checks for list or tuples
    # XXXX badly named!
    return type(obj) in (list, tuple)


def _instance_callable(obj):
    """Given an object, return True if the object is callable.
    For classes, return True if instances would be callable."""
    if not isinstance(obj, ClassTypes):
        # already an instance
        return getattr(obj, '__call__', None) is not None

    klass = obj
    # uses __bases__ instead of __mro__ so that we work with old style classes
    if klass.__dict__.get('__call__') is not None:
        return True

    for base in klass.__bases__:
        if _instance_callable(base):
            return True
    return False


def _set_signature(mock, original, instance=False):
    # creates a function with signature (*args, **kwargs) that delegates to a
    # mock. It still does signature checking by calling a lambda with the same
    # signature as the original.
    if not _callable(original):
        return

    skipfirst = isinstance(original, ClassTypes)
    result = _getsignature(original, skipfirst, instance)
    if result is None:
        # was a C function (e.g. object().__init__ ) that can't be mocked
        return

    signature, func = result

    src = "lambda %s: None" % signature
    checksig = eval(src, {})
    _copy_func_details(func, checksig)

    name = original.__name__
    if not _isidentifier(name):
        name = 'funcopy'
    context = {'_checksig_': checksig, 'mock': mock}
    src = """def %s(*args, **kwargs):
    _checksig_(*args, **kwargs)
    return mock(*args, **kwargs)""" % name
    exec (src, context)
    funcopy = context[name]
    _setup_func(funcopy, mock)
    return funcopy


def _setup_func(funcopy, mock):
    funcopy.mock = mock

    # can't use isinstance with mocks
    if not _is_instance_mock(mock):
        return

    def assert_called_with(*args, **kwargs):
        return mock.assert_called_with(*args, **kwargs)
    def assert_called_once_with(*args, **kwargs):
        return mock.assert_called_once_with(*args, **kwargs)
    def assert_has_calls(*args, **kwargs):
        return mock.assert_has_calls(*args, **kwargs)
    def assert_any_call(*args, **kwargs):
        return mock.assert_any_call(*args, **kwargs)
    def reset_mock():
        funcopy.method_calls = _CallList()
        funcopy.mock_calls = _CallList()
        mock.reset_mock()
        ret = funcopy.return_value
        if _is_instance_mock(ret) and not ret is mock:
            ret.reset_mock()

    funcopy.called = False
    funcopy.call_count = 0
    funcopy.call_args = None
    funcopy.call_args_list = _CallList()
    funcopy.method_calls = _CallList()
    funcopy.mock_calls = _CallList()

    funcopy.return_value = mock.return_value
    funcopy.side_effect = mock.side_effect
    funcopy._mock_children = mock._mock_children

    funcopy.assert_called_with = assert_called_with
    funcopy.assert_called_once_with = assert_called_once_with
    funcopy.assert_has_calls = assert_has_calls
    funcopy.assert_any_call = assert_any_call
    funcopy.reset_mock = reset_mock

    mock._mock_delegate = funcopy


def _is_magic(name):
    return '__%s__' % name[2:-2] == name


class _SentinelObject(object):
    "A unique, named, sentinel object."
    def __init__(self, name):
        self.name = name

    def __repr__(self):
        return 'sentinel.%s' % self.name


class _Sentinel(object):
    """Access attributes to return a named object, usable as a sentinel."""
    def __init__(self):
        self._sentinels = {}

    def __getattr__(self, name):
        if name == '__bases__':
            # Without this help(mock) raises an exception
            raise AttributeError
        return self._sentinels.setdefault(name, _SentinelObject(name))


sentinel = _Sentinel()

DEFAULT = sentinel.DEFAULT
_missing = sentinel.MISSING
_deleted = sentinel.DELETED


class OldStyleClass:
    pass
ClassType = type(OldStyleClass)


def _copy(value):
    if type(value) in (dict, list, tuple, set):
        return type(value)(value)
    return value


ClassTypes = (type,)
if not inPy3k:
    ClassTypes = (type, ClassType)

_allowed_names = set(
    [
        'return_value', '_mock_return_value', 'side_effect',
        '_mock_side_effect', '_mock_parent', '_mock_new_parent',
        '_mock_name', '_mock_new_name'
    ]
)


def _delegating_property(name):
    _allowed_names.add(name)
    _the_name = '_mock_' + name
    def _get(self, name=name, _the_name=_the_name):
        sig = self._mock_delegate
        if sig is None:
            return getattr(self, _the_name)
        return getattr(sig, name)
    def _set(self, value, name=name, _the_name=_the_name):
        sig = self._mock_delegate
        if sig is None:
            self.__dict__[_the_name] = value
        else:
            setattr(sig, name, value)

    return property(_get, _set)



class _CallList(list):

    def __contains__(self, value):
        if not isinstance(value, list):
            return list.__contains__(self, value)
        len_value = len(value)
        len_self = len(self)
        if len_value > len_self:
            return False

        for i in range(0, len_self - len_value + 1):
            sub_list = self[i:i+len_value]
            if sub_list == value:
                return True
        return False

    def __repr__(self):
        return pprint.pformat(list(self))


def _check_and_set_parent(parent, value, name, new_name):
    if not _is_instance_mock(value):
        return False
    if ((value._mock_name or value._mock_new_name) or
        (value._mock_parent is not None) or
        (value._mock_new_parent is not None)):
        return False

    _parent = parent
    while _parent is not None:
        # setting a mock (value) as a child or return value of itself
        # should not modify the mock
        if _parent is value:
            return False
        _parent = _parent._mock_new_parent

    if new_name:
        value._mock_new_parent = parent
        value._mock_new_name = new_name
    if name:
        value._mock_parent = parent
        value._mock_name = name
    return True



class Base(object):
    _mock_return_value = DEFAULT
    _mock_side_effect = None
    def __init__(self, *args, **kwargs):
        pass



class NonCallableMock(Base):
    """A non-callable version of `Mock`"""

    def __new__(cls, *args, **kw):
        # every instance has its own class
        # so we can create magic methods on the
        # class without stomping on other mocks
        new = type(cls.__name__, (cls,), {'__doc__': cls.__doc__})
        instance = object.__new__(new)
        return instance


    def __init__(
            self, spec=None, wraps=None, name=None, spec_set=None,
            parent=None, _spec_state=None, _new_name='', _new_parent=None,
            **kwargs
        ):
        if _new_parent is None:
            _new_parent = parent

        __dict__ = self.__dict__
        __dict__['_mock_parent'] = parent
        __dict__['_mock_name'] = name
        __dict__['_mock_new_name'] = _new_name
        __dict__['_mock_new_parent'] = _new_parent

        if spec_set is not None:
            spec = spec_set
            spec_set = True

        self._mock_add_spec(spec, spec_set)

        __dict__['_mock_children'] = {}
        __dict__['_mock_wraps'] = wraps
        __dict__['_mock_delegate'] = None

        __dict__['_mock_called'] = False
        __dict__['_mock_call_args'] = None
        __dict__['_mock_call_count'] = 0
        __dict__['_mock_call_args_list'] = _CallList()
        __dict__['_mock_mock_calls'] = _CallList()

        __dict__['method_calls'] = _CallList()

        if kwargs:
            self.configure_mock(**kwargs)

        _super(NonCallableMock, self).__init__(
            spec, wraps, name, spec_set, parent,
            _spec_state
        )


    def attach_mock(self, mock, attribute):
        """
        Attach a mock as an attribute of this one, replacing its name and
        parent. Calls to the attached mock will be recorded in the
        `method_calls` and `mock_calls` attributes of this one."""
        mock._mock_parent = None
        mock._mock_new_parent = None
        mock._mock_name = ''
        mock._mock_new_name = None

        setattr(self, attribute, mock)


    def mock_add_spec(self, spec, spec_set=False):
        """Add a spec to a mock. `spec` can either be an object or a
        list of strings. Only attributes on the `spec` can be fetched as
        attributes from the mock.

        If `spec_set` is True then only attributes on the spec can be set."""
        self._mock_add_spec(spec, spec_set)


    def _mock_add_spec(self, spec, spec_set):
        _spec_class = None

        if spec is not None and not _is_list(spec):
            if isinstance(spec, ClassTypes):
                _spec_class = spec
            else:
                _spec_class = _get_class(spec)

            spec = dir(spec)

        __dict__ = self.__dict__
        __dict__['_spec_class'] = _spec_class
        __dict__['_spec_set'] = spec_set
        __dict__['_mock_methods'] = spec


    def __get_return_value(self):
        ret = self._mock_return_value
        if self._mock_delegate is not None:
            ret = self._mock_delegate.return_value

        if ret is DEFAULT:
            ret = self._get_child_mock(
                _new_parent=self, _new_name='()'
            )
            self.return_value = ret
        return ret


    def __set_return_value(self, value):
        if self._mock_delegate is not None:
            self._mock_delegate.return_value = value
        else:
            self._mock_return_value = value
            _check_and_set_parent(self, value, None, '()')

    __return_value_doc = "The value to be returned when the mock is called."
    return_value = property(__get_return_value, __set_return_value,
                            __return_value_doc)


    @property
    def __class__(self):
        if self._spec_class is None:
            return type(self)
        return self._spec_class

    called = _delegating_property('called')
    call_count = _delegating_property('call_count')
    call_args = _delegating_property('call_args')
    call_args_list = _delegating_property('call_args_list')
    mock_calls = _delegating_property('mock_calls')


    def __get_side_effect(self):
        sig = self._mock_delegate
        if sig is None:
            return self._mock_side_effect
        return sig.side_effect

    def __set_side_effect(self, value):
        value = _try_iter(value)
        sig = self._mock_delegate
        if sig is None:
            self._mock_side_effect = value
        else:
            sig.side_effect = value

    side_effect = property(__get_side_effect, __set_side_effect)


    def reset_mock(self):
        "Restore the mock object to its initial state."
        self.called = False
        self.call_args = None
        self.call_count = 0
        self.mock_calls = _CallList()
        self.call_args_list = _CallList()
        self.method_calls = _CallList()

        for child in self._mock_children.itervalues():
            if isinstance(child, _SpecState):
                continue
            child.reset_mock()

        ret = self._mock_return_value
        if _is_instance_mock(ret) and ret is not self:
            ret.reset_mock()


    def configure_mock(self, **kwargs):
        """Set attributes on the mock through keyword arguments.

        Attributes plus return values and side effects can be set on child
        mocks using standard dot notation and unpacking a dictionary in the
        method call:

        >>> attrs = {'method.return_value': 3, 'other.side_effect': KeyError}
        >>> mock.configure_mock(**attrs)"""
        for arg, val in sorted(kwargs.items(),
                               # we sort on the number of dots so that
                               # attributes are set before we set attributes on
                               # attributes
                               key=lambda entry: entry[0].count('.')):
            args = arg.split('.')
            final = args.pop()
            obj = self
            for entry in args:
                obj = getattr(obj, entry)
            setattr(obj, final, val)


    def __getattr__(self, name):
        if name == '_mock_methods':
            raise AttributeError(name)
        elif self._mock_methods is not None:
            if name not in self._mock_methods or name in _all_magics:
                raise AttributeError("Mock object has no attribute %r" % name)
        elif _is_magic(name):
            raise AttributeError(name)

        result = self._mock_children.get(name)
        if result is _deleted:
            raise AttributeError(name)
        elif result is None:
            wraps = None
            if self._mock_wraps is not None:
                # XXXX should we get the attribute without triggering code
                # execution?
                wraps = getattr(self._mock_wraps, name)

            result = self._get_child_mock(
                parent=self, name=name, wraps=wraps, _new_name=name,
                _new_parent=self
            )
            self._mock_children[name]  = result

        elif isinstance(result, _SpecState):
            result = create_autospec(
                result.spec, result.spec_set, result.instance,
                result.parent, result.name
            )
            self._mock_children[name]  = result

        return result


    def __repr__(self):
        _name_list = [self._mock_new_name]
        _parent = self._mock_new_parent
        last = self

        dot = '.'
        if _name_list == ['()']:
            dot = ''
        seen = set()
        while _parent is not None:
            last = _parent

            _name_list.append(_parent._mock_new_name + dot)
            dot = '.'
            if _parent._mock_new_name == '()':
                dot = ''

            _parent = _parent._mock_new_parent

            # use ids here so as not to call __hash__ on the mocks
            if id(_parent) in seen:
                break
            seen.add(id(_parent))

        _name_list = list(reversed(_name_list))
        _first = last._mock_name or 'mock'
        if len(_name_list) > 1:
            if _name_list[1] not in ('()', '().'):
                _first += '.'
        _name_list[0] = _first
        name = ''.join(_name_list)

        name_string = ''
        if name not in ('mock', 'mock.'):
            name_string = ' name=%r' % name

        spec_string = ''
        if self._spec_class is not None:
            spec_string = ' spec=%r'
            if self._spec_set:
                spec_string = ' spec_set=%r'
            spec_string = spec_string % self._spec_class.__name__
        return "<%s%s%s id='%s'>" % (
            type(self).__name__,
            name_string,
            spec_string,
            id(self)
        )


    def __dir__(self):
        """Filter the output of `dir(mock)` to only useful members.
        XXXX
        """
        extras = self._mock_methods or []
        from_type = dir(type(self))
        from_dict = list(self.__dict__)

        if FILTER_DIR:
            from_type = [e for e in from_type if not e.startswith('_')]
            from_dict = [e for e in from_dict if not e.startswith('_') or
                         _is_magic(e)]
        return sorted(set(extras + from_type + from_dict +
                          list(self._mock_children)))


    def __setattr__(self, name, value):
        if name in _allowed_names:
            # property setters go through here
            return object.__setattr__(self, name, value)
        elif (self._spec_set and self._mock_methods is not None and
            name not in self._mock_methods and
            name not in self.__dict__):
            raise AttributeError("Mock object has no attribute '%s'" % name)
        elif name in _unsupported_magics:
            msg = 'Attempting to set unsupported magic method %r.' % name
            raise AttributeError(msg)
        elif name in _all_magics:
            if self._mock_methods is not None and name not in self._mock_methods:
                raise AttributeError("Mock object has no attribute '%s'" % name)

            if not _is_instance_mock(value):
                setattr(type(self), name, _get_method(name, value))
                original = value
                value = lambda *args, **kw: original(self, *args, **kw)
            else:
                # only set _new_name and not name so that mock_calls is tracked
                # but not method calls
                _check_and_set_parent(self, value, None, name)
                setattr(type(self), name, value)
                self._mock_children[name] = value
        elif name == '__class__':
            self._spec_class = value
            return
        else:
            if _check_and_set_parent(self, value, name, name):
                self._mock_children[name] = value
        return object.__setattr__(self, name, value)


    def __delattr__(self, name):
        if name in _all_magics and name in type(self).__dict__:
            delattr(type(self), name)
            if name not in self.__dict__:
                # for magic methods that are still MagicProxy objects and
                # not set on the instance itself
                return

        if name in self.__dict__:
            object.__delattr__(self, name)

        obj = self._mock_children.get(name, _missing)
        if obj is _deleted:
            raise AttributeError(name)
        if obj is not _missing:
            del self._mock_children[name]
        self._mock_children[name] = _deleted



    def _format_mock_call_signature(self, args, kwargs):
        name = self._mock_name or 'mock'
        return _format_call_signature(name, args, kwargs)


    def _format_mock_failure_message(self, args, kwargs):
        message = 'Expected call: %s\nActual call: %s'
        expected_string = self._format_mock_call_signature(args, kwargs)
        call_args = self.call_args
        if len(call_args) == 3:
            call_args = call_args[1:]
        actual_string = self._format_mock_call_signature(*call_args)
        return message % (expected_string, actual_string)


    def assert_called_with(_mock_self, *args, **kwargs):
        """assert that the mock was called with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        self = _mock_self
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            raise AssertionError('Expected call: %s\nNot called' % (expected,))

        if self.call_args != (args, kwargs):
            msg = self._format_mock_failure_message(args, kwargs)
            raise AssertionError(msg)


    def assert_called_once_with(_mock_self, *args, **kwargs):
        """assert that the mock was called exactly once and with the specified
        arguments."""
        self = _mock_self
        if not self.call_count == 1:
            msg = ("Expected to be called once. Called %s times." %
                   self.call_count)
            raise AssertionError(msg)
        return self.assert_called_with(*args, **kwargs)


    def assert_has_calls(self, calls, any_order=False):
        """assert the mock has been called with the specified calls.
        The `mock_calls` list is checked for the calls.

        If `any_order` is False (the default) then the calls must be
        sequential. There can be extra calls before or after the
        specified calls.

        If `any_order` is True then the calls can be in any order, but
        they must all appear in `mock_calls`."""
        if not any_order:
            if calls not in self.mock_calls:
                raise AssertionError(
                    'Calls not found.\nExpected: %r\n'
                    'Actual: %r' % (calls, self.mock_calls)
                )
            return

        all_calls = list(self.mock_calls)

        not_found = []
        for kall in calls:
            try:
                all_calls.remove(kall)
            except ValueError:
                not_found.append(kall)
        if not_found:
            raise AssertionError(
                '%r not all found in call list' % (tuple(not_found),)
            )


    def assert_any_call(self, *args, **kwargs):
        """assert the mock has been called with the specified arguments.

        The assert passes if the mock has *ever* been called, unlike
        `assert_called_with` and `assert_called_once_with` that only pass if
        the call is the most recent one."""
        kall = call(*args, **kwargs)
        if kall not in self.call_args_list:
            expected_string = self._format_mock_call_signature(args, kwargs)
            raise AssertionError(
                '%s call not found' % expected_string
            )


    def _get_child_mock(self, **kw):
        """Create the child mocks for attributes and return value.
        By default child mocks will be the same type as the parent.
        Subclasses of Mock may want to override this to customize the way
        child mocks are made.

        For non-callable mocks the callable variant will be used (rather than
        any custom subclass)."""
        _type = type(self)
        if not issubclass(_type, CallableMixin):
            if issubclass(_type, NonCallableMagicMock):
                klass = MagicMock
            elif issubclass(_type, NonCallableMock) :
                klass = Mock
        else:
            klass = _type.__mro__[1]
        return klass(**kw)



def _try_iter(obj):
    if obj is None:
        return obj
    if _is_exception(obj):
        return obj
    if _callable(obj):
        return obj
    try:
        return iter(obj)
    except TypeError:
        # XXXX backwards compatibility
        # but this will blow up on first call - so maybe we should fail early?
        return obj



class CallableMixin(Base):

    def __init__(self, spec=None, side_effect=None, return_value=DEFAULT,
                 wraps=None, name=None, spec_set=None, parent=None,
                 _spec_state=None, _new_name='', _new_parent=None, **kwargs):
        self.__dict__['_mock_return_value'] = return_value

        _super(CallableMixin, self).__init__(
            spec, wraps, name, spec_set, parent,
            _spec_state, _new_name, _new_parent, **kwargs
        )

        self.side_effect = side_effect


    def _mock_check_sig(self, *args, **kwargs):
        # stub method that can be replaced with one with a specific signature
        pass


    def __call__(_mock_self, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        _mock_self._mock_check_sig(*args, **kwargs)
        return _mock_self._mock_call(*args, **kwargs)


    def _mock_call(_mock_self, *args, **kwargs):
        self = _mock_self
        self.called = True
        self.call_count += 1
        self.call_args = _Call((args, kwargs), two=True)
        self.call_args_list.append(_Call((args, kwargs), two=True))

        _new_name = self._mock_new_name
        _new_parent = self._mock_new_parent
        self.mock_calls.append(_Call(('', args, kwargs)))

        seen = set()
        skip_next_dot = _new_name == '()'
        do_method_calls = self._mock_parent is not None
        name = self._mock_name
        while _new_parent is not None:
            this_mock_call = _Call((_new_name, args, kwargs))
            if _new_parent._mock_new_name:
                dot = '.'
                if skip_next_dot:
                    dot = ''

                skip_next_dot = False
                if _new_parent._mock_new_name == '()':
                    skip_next_dot = True

                _new_name = _new_parent._mock_new_name + dot + _new_name

            if do_method_calls:
                if _new_name == name:
                    this_method_call = this_mock_call
                else:
                    this_method_call = _Call((name, args, kwargs))
                _new_parent.method_calls.append(this_method_call)

                do_method_calls = _new_parent._mock_parent is not None
                if do_method_calls:
                    name = _new_parent._mock_name + '.' + name

            _new_parent.mock_calls.append(this_mock_call)
            _new_parent = _new_parent._mock_new_parent

            # use ids here so as not to call __hash__ on the mocks
            _new_parent_id = id(_new_parent)
            if _new_parent_id in seen:
                break
            seen.add(_new_parent_id)

        ret_val = DEFAULT
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect

            if not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
                return result

            ret_val = effect(*args, **kwargs)
            if ret_val is DEFAULT:
                ret_val = self.return_value

        if (self._mock_wraps is not None and
             self._mock_return_value is DEFAULT):
            return self._mock_wraps(*args, **kwargs)
        if ret_val is DEFAULT:
            ret_val = self.return_value
        return ret_val



class Mock(CallableMixin, NonCallableMock):
    """
    Create a new `Mock` object. `Mock` takes several optional arguments
    that specify the behaviour of the Mock object:

    * `spec`: This can be either a list of strings or an existing object (a
      class or instance) that acts as the specification for the mock object. If
      you pass in an object then a list of strings is formed by calling dir on
      the object (excluding unsupported magic attributes and methods). Accessing
      any attribute not in this list will raise an `AttributeError`.

      If `spec` is an object (rather than a list of strings) then
      `mock.__class__` returns the class of the spec object. This allows mocks
      to pass `isinstance` tests.

    * `spec_set`: A stricter variant of `spec`. If used, attempting to *set*
      or get an attribute on the mock that isn't on the object passed as
      `spec_set` will raise an `AttributeError`.

    * `side_effect`: A function to be called whenever the Mock is called. See
      the `side_effect` attribute. Useful for raising exceptions or
      dynamically changing return values. The function is called with the same
      arguments as the mock, and unless it returns `DEFAULT`, the return
      value of this function is used as the return value.

      Alternatively `side_effect` can be an exception class or instance. In
      this case the exception will be raised when the mock is called.

      If `side_effect` is an iterable then each call to the mock will return
      the next value from the iterable. If any of the members of the iterable
      are exceptions they will be raised instead of returned.

    * `return_value`: The value returned when the mock is called. By default
      this is a new Mock (created on first access). See the
      `return_value` attribute.

    * `wraps`: Item for the mock object to wrap. If `wraps` is not None then
      calling the Mock will pass the call through to the wrapped object
      (returning the real result). Attribute access on the mock will return a
      Mock object that wraps the corresponding attribute of the wrapped object
      (so attempting to access an attribute that doesn't exist will raise an
      `AttributeError`).

      If the mock has an explicit `return_value` set then calls are not passed
      to the wrapped object and the `return_value` is returned instead.

    * `name`: If the mock has a name then it will be used in the repr of the
      mock. This can be useful for debugging. The name is propagated to child
      mocks.

    Mocks can also be called with arbitrary keyword arguments. These will be
    used to set attributes on the mock after it is created.
    """



def _dot_lookup(thing, comp, import_path):
    try:
        return getattr(thing, comp)
    except AttributeError:
        __import__(import_path)
        return getattr(thing, comp)


def _importer(target):
    components = target.split('.')
    import_path = components.pop(0)
    thing = __import__(import_path)

    for comp in components:
        import_path += ".%s" % comp
        thing = _dot_lookup(thing, comp, import_path)
    return thing


def _is_started(patcher):
    # XXXX horrible
    return hasattr(patcher, 'is_local')


class _patch(object):

    attribute_name = None
    _active_patches = set()

    def __init__(
            self, getter, attribute, new, spec, create,
            spec_set, autospec, new_callable, kwargs
        ):
        if new_callable is not None:
            if new is not DEFAULT:
                raise ValueError(
                    "Cannot use 'new' and 'new_callable' together"
                )
            if autospec is not None:
                raise ValueError(
                    "Cannot use 'autospec' and 'new_callable' together"
                )

        self.getter = getter
        self.attribute = attribute
        self.new = new
        self.new_callable = new_callable
        self.spec = spec
        self.create = create
        self.has_local = False
        self.spec_set = spec_set
        self.autospec = autospec
        self.kwargs = kwargs
        self.additional_patchers = []


    def copy(self):
        patcher = _patch(
            self.getter, self.attribute, self.new, self.spec,
            self.create, self.spec_set,
            self.autospec, self.new_callable, self.kwargs
        )
        patcher.attribute_name = self.attribute_name
        patcher.additional_patchers = [
            p.copy() for p in self.additional_patchers
        ]
        return patcher


    def __call__(self, func):
        if isinstance(func, ClassTypes):
            return self.decorate_class(func)
        return self.decorate_callable(func)


    def decorate_class(self, klass):
        for attr in dir(klass):
            if not attr.startswith(patch.TEST_PREFIX):
                continue

            attr_value = getattr(klass, attr)
            if not hasattr(attr_value, "__call__"):
                continue

            patcher = self.copy()
            setattr(klass, attr, patcher(attr_value))
        return klass


    def decorate_callable(self, func):
        if hasattr(func, 'patchings'):
            func.patchings.append(self)
            return func

        @wraps(func)
        def patched(*args, **keywargs):
            # don't use a with here (backwards compatibility with Python 2.4)
            extra_args = []
            entered_patchers = []

            # can't use try...except...finally because of Python 2.4
            # compatibility
            exc_info = tuple()
            try:
                try:
                    for patching in patched.patchings:
                        arg = patching.__enter__()
                        entered_patchers.append(patching)
                        if patching.attribute_name is not None:
                            keywargs.update(arg)
                        elif patching.new is DEFAULT:
                            extra_args.append(arg)

                    args += tuple(extra_args)
                    return func(*args, **keywargs)
                except:
                    if (patching not in entered_patchers and
                        _is_started(patching)):
                        # the patcher may have been started, but an exception
                        # raised whilst entering one of its additional_patchers
                        entered_patchers.append(patching)
                    # Pass the exception to __exit__
                    exc_info = sys.exc_info()
                    # re-raise the exception
                    raise
            finally:
                for patching in reversed(entered_patchers):
                    patching.__exit__(*exc_info)

        patched.patchings = [self]
        if hasattr(func, 'func_code'):
            # not in Python 3
            patched.compat_co_firstlineno = getattr(
                func, "compat_co_firstlineno",
                func.func_code.co_firstlineno
            )
        return patched


    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if not self.create and original is DEFAULT:
            raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
        return original, local


    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()

        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None

        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")

        original, local = self.get_original()

        if new is DEFAULT and autospec is None:
            inherit = False
            if spec is True:
                # set spec to the object we are replacing
                spec = original
                if spec_set is True:
                    spec_set = original
                    spec = None
            elif spec is not None:
                if spec_set is True:
                    spec_set = spec
                    spec = None
            elif spec_set is True:
                spec_set = original

            if spec is not None or spec_set is not None:
                if original is DEFAULT:
                    raise TypeError("Can't use 'spec' with create=True")
                if isinstance(original, ClassTypes):
                    # If we're patching out a class and there is a spec
                    inherit = True

            Klass = MagicMock
            _kwargs = {}
            if new_callable is not None:
                Klass = new_callable
            elif spec is not None or spec_set is not None:
                this_spec = spec
                if spec_set is not None:
                    this_spec = spec_set
                if _is_list(this_spec):
                    not_callable = '__call__' not in this_spec
                else:
                    not_callable = not _callable(this_spec)
                if not_callable:
                    Klass = NonCallableMagicMock

            if spec is not None:
                _kwargs['spec'] = spec
            if spec_set is not None:
                _kwargs['spec_set'] = spec_set

            # add a name to mocks
            if (isinstance(Klass, type) and
                issubclass(Klass, NonCallableMock) and self.attribute):
                _kwargs['name'] = self.attribute

            _kwargs.update(kwargs)
            new = Klass(**_kwargs)

            if inherit and _is_instance_mock(new):
                # we can only tell if the instance should be callable if the
                # spec is not a list
                this_spec = spec
                if spec_set is not None:
                    this_spec = spec_set
                if (not _is_list(this_spec) and not
                    _instance_callable(this_spec)):
                    Klass = NonCallableMagicMock

                _kwargs.pop('name')
                new.return_value = Klass(_new_parent=new, _new_name='()',
                                         **_kwargs)
        elif autospec is not None:
            # spec is ignored, new *must* be default, spec_set is treated
            # as a boolean. Should we check spec is not None and that spec_set
            # is a bool?
            if new is not DEFAULT:
                raise TypeError(
                    "autospec creates the mock for you. Can't specify "
                    "autospec and new."
                )
            if original is DEFAULT:
                raise TypeError("Can't use 'autospec' with create=True")
            spec_set = bool(spec_set)
            if autospec is True:
                autospec = original

            new = create_autospec(autospec, spec_set=spec_set,
                                  _name=self.attribute, **kwargs)
        elif kwargs:
            # can't set keyword args when we aren't creating the mock
            # XXXX If new is a Mock we could call new.configure_mock(**kwargs)
            raise TypeError("Can't pass kwargs to a mock we aren't creating")

        new_attr = new

        self.temp_original = original
        self.is_local = local
        setattr(self.target, self.attribute, new_attr)
        if self.attribute_name is not None:
            extra_args = {}
            if self.new is DEFAULT:
                extra_args[self.attribute_name] =  new
            for patching in self.additional_patchers:
                arg = patching.__enter__()
                if patching.new is DEFAULT:
                    extra_args.update(arg)
            return extra_args

        return new


    def __exit__(self, *exc_info):
        """Undo the patch."""
        if not _is_started(self):
            raise RuntimeError('stop called on unstarted patcher')

        if self.is_local and self.temp_original is not DEFAULT:
            setattr(self.target, self.attribute, self.temp_original)
        else:
            delattr(self.target, self.attribute)
            if not self.create and not hasattr(self.target, self.attribute):
                # needed for proxy objects like django settings
                setattr(self.target, self.attribute, self.temp_original)

        del self.temp_original
        del self.is_local
        del self.target
        for patcher in reversed(self.additional_patchers):
            if _is_started(patcher):
                patcher.__exit__(*exc_info)


    def start(self):
        """Activate a patch, returning any created mock."""
        result = self.__enter__()
        self._active_patches.add(self)
        return result


    def stop(self):
        """Stop an active patch."""
        self._active_patches.discard(self)
        return self.__exit__()



def _get_target(target):
    try:
        target, attribute = target.rsplit('.', 1)
    except (TypeError, ValueError):
        raise TypeError("Need a valid target to patch. You supplied: %r" %
                        (target,))
    getter = lambda: _importer(target)
    return getter, attribute


def _patch_object(
        target, attribute, new=DEFAULT, spec=None,
        create=False, spec_set=None, autospec=None,
        new_callable=None, **kwargs
    ):
    """
    patch.object(target, attribute, new=DEFAULT, spec=None, create=False,
                 spec_set=None, autospec=None, new_callable=None, **kwargs)

    patch the named member (`attribute`) on an object (`target`) with a mock
    object.

    `patch.object` can be used as a decorator, class decorator or a context
    manager. Arguments `new`, `spec`, `create`, `spec_set`,
    `autospec` and `new_callable` have the same meaning as for `patch`. Like
    `patch`, `patch.object` takes arbitrary keyword arguments for configuring
    the mock object it creates.

    When used as a class decorator `patch.object` honours `patch.TEST_PREFIX`
    for choosing which methods to wrap.
    """
    getter = lambda: target
    return _patch(
        getter, attribute, new, spec, create,
        spec_set, autospec, new_callable, kwargs
    )


def _patch_multiple(target, spec=None, create=False, spec_set=None,
                    autospec=None, new_callable=None, **kwargs):
    """Perform multiple patches in a single call. It takes the object to be
    patched (either as an object or a string to fetch the object by importing)
    and keyword arguments for the patches::

        with patch.multiple(settings, FIRST_PATCH='one', SECOND_PATCH='two'):
            ...

    Use `DEFAULT` as the value if you want `patch.multiple` to create
    mocks for you. In this case the created mocks are passed into a decorated
    function by keyword, and a dictionary is returned when `patch.multiple` is
    used as a context manager.

    `patch.multiple` can be used as a decorator, class decorator or a context
    manager. The arguments `spec`, `spec_set`, `create`,
    `autospec` and `new_callable` have the same meaning as for `patch`. These
    arguments will be applied to *all* patches done by `patch.multiple`.

    When used as a class decorator `patch.multiple` honours `patch.TEST_PREFIX`
    for choosing which methods to wrap.
    """
    if type(target) in (unicode, str):
        getter = lambda: _importer(target)
    else:
        getter = lambda: target

    if not kwargs:
        raise ValueError(
            'Must supply at least one keyword argument with patch.multiple'
        )
    # need to wrap in a list for python 3, where items is a view
    items = list(kwargs.items())
    attribute, new = items[0]
    patcher = _patch(
        getter, attribute, new, spec, create, spec_set,
        autospec, new_callable, {}
    )
    patcher.attribute_name = attribute
    for attribute, new in items[1:]:
        this_patcher = _patch(
            getter, attribute, new, spec, create, spec_set,
            autospec, new_callable, {}
        )
        this_patcher.attribute_name = attribute
        patcher.additional_patchers.append(this_patcher)
    return patcher


def patch(
        target, new=DEFAULT, spec=None, create=False,
        spec_set=None, autospec=None, new_callable=None, **kwargs
    ):
    """
    `patch` acts as a function decorator, class decorator or a context
    manager. Inside the body of the function or with statement, the `target`
    is patched with a `new` object. When the function/with statement exits
    the patch is undone.

    If `new` is omitted, then the target is replaced with a
    `MagicMock`. If `patch` is used as a decorator and `new` is
    omitted, the created mock is passed in as an extra argument to the
    decorated function. If `patch` is used as a context manager the created
    mock is returned by the context manager.

    `target` should be a string in the form `'package.module.ClassName'`. The
    `target` is imported and the specified object replaced with the `new`
    object, so the `target` must be importable from the environment you are
    calling `patch` from. The target is imported when the decorated function
    is executed, not at decoration time.

    The `spec` and `spec_set` keyword arguments are passed to the `MagicMock`
    if patch is creating one for you.

    In addition you can pass `spec=True` or `spec_set=True`, which causes
    patch to pass in the object being mocked as the spec/spec_set object.

    `new_callable` allows you to specify a different class, or callable object,
    that will be called to create the `new` object. By default `MagicMock` is
    used.

    A more powerful form of `spec` is `autospec`. If you set `autospec=True`
    then the mock with be created with a spec from the object being replaced.
    All attributes of the mock will also have the spec of the corresponding
    attribute of the object being replaced. Methods and functions being
    mocked will have their arguments checked and will raise a `TypeError` if
    they are called with the wrong signature. For mocks replacing a class,
    their return value (the 'instance') will have the same spec as the class.

    Instead of `autospec=True` you can pass `autospec=some_object` to use an
    arbitrary object as the spec instead of the one being replaced.

    By default `patch` will fail to replace attributes that don't exist. If
    you pass in `create=True`, and the attribute doesn't exist, patch will
    create the attribute for you when the patched function is called, and
    delete it again afterwards. This is useful for writing tests against
    attributes that your production code creates at runtime. It is off by by
    default because it can be dangerous. With it switched on you can write
    passing tests against APIs that don't actually exist!

    Patch can be used as a `TestCase` class decorator. It works by
    decorating each test method in the class. This reduces the boilerplate
    code when your test methods share a common patchings set. `patch` finds
    tests by looking for method names that start with `patch.TEST_PREFIX`.
    By default this is `test`, which matches the way `unittest` finds tests.
    You can specify an alternative prefix by setting `patch.TEST_PREFIX`.

    Patch can be used as a context manager, with the with statement. Here the
    patching applies to the indented block after the with statement. If you
    use "as" then the patched object will be bound to the name after the
    "as"; very useful if `patch` is creating a mock object for you.

    `patch` takes arbitrary keyword arguments. These will be passed to
    the `Mock` (or `new_callable`) on construction.

    `patch.dict(...)`, `patch.multiple(...)` and `patch.object(...)` are
    available for alternate use-cases.
    """
    getter, attribute = _get_target(target)
    return _patch(
        getter, attribute, new, spec, create,
        spec_set, autospec, new_callable, kwargs
    )


class _patch_dict(object):
    """
    Patch a dictionary, or dictionary like object, and restore the dictionary
    to its original state after the test.

    `in_dict` can be a dictionary or a mapping like container. If it is a
    mapping then it must at least support getting, setting and deleting items
    plus iterating over keys.

    `in_dict` can also be a string specifying the name of the dictionary, which
    will then be fetched by importing it.

    `values` can be a dictionary of values to set in the dictionary. `values`
    can also be an iterable of `(key, value)` pairs.

    If `clear` is True then the dictionary will be cleared before the new
    values are set.

    `patch.dict` can also be called with arbitrary keyword arguments to set
    values in the dictionary::

        with patch.dict('sys.modules', mymodule=Mock(), other_module=Mock()):
            ...

    `patch.dict` can be used as a context manager, decorator or class
    decorator. When used as a class decorator `patch.dict` honours
    `patch.TEST_PREFIX` for choosing which methods to wrap.
    """

    def __init__(self, in_dict, values=(), clear=False, **kwargs):
        if isinstance(in_dict, basestring):
            in_dict = _importer(in_dict)
        self.in_dict = in_dict
        # support any argument supported by dict(...) constructor
        self.values = dict(values)
        self.values.update(kwargs)
        self.clear = clear
        self._original = None


    def __call__(self, f):
        if isinstance(f, ClassTypes):
            return self.decorate_class(f)
        @wraps(f)
        def _inner(*args, **kw):
            self._patch_dict()
            try:
                return f(*args, **kw)
            finally:
                self._unpatch_dict()

        return _inner


    def decorate_class(self, klass):
        for attr in dir(klass):
            attr_value = getattr(klass, attr)
            if (attr.startswith(patch.TEST_PREFIX) and
                 hasattr(attr_value, "__call__")):
                decorator = _patch_dict(self.in_dict, self.values, self.clear)
                decorated = decorator(attr_value)
                setattr(klass, attr, decorated)
        return klass


    def __enter__(self):
        """Patch the dict."""
        self._patch_dict()


    def _patch_dict(self):
        values = self.values
        in_dict = self.in_dict
        clear = self.clear

        try:
            original = in_dict.copy()
        except AttributeError:
            # dict like object with no copy method
            # must support iteration over keys
            original = {}
            for key in in_dict:
                original[key] = in_dict[key]
        self._original = original

        if clear:
            _clear_dict(in_dict)

        try:
            in_dict.update(values)
        except AttributeError:
            # dict like object with no update method
            for key in values:
                in_dict[key] = values[key]


    def _unpatch_dict(self):
        in_dict = self.in_dict
        original = self._original

        _clear_dict(in_dict)

        try:
            in_dict.update(original)
        except AttributeError:
            for key in original:
                in_dict[key] = original[key]


    def __exit__(self, *args):
        """Unpatch the dict."""
        self._unpatch_dict()
        return False

    start = __enter__
    stop = __exit__


def _clear_dict(in_dict):
    try:
        in_dict.clear()
    except AttributeError:
        keys = list(in_dict)
        for key in keys:
            del in_dict[key]


def _patch_stopall():
    """Stop all active patches."""
    for patch in list(_patch._active_patches):
        patch.stop()


patch.object = _patch_object
patch.dict = _patch_dict
patch.multiple = _patch_multiple
patch.stopall = _patch_stopall
patch.TEST_PREFIX = 'test'

magic_methods = (
    "lt le gt ge eq ne "
    "getitem setitem delitem "
    "len contains iter "
    "hash str sizeof "
    "enter exit "
    "divmod neg pos abs invert "
    "complex int float index "
    "trunc floor ceil "
)

numerics = "add sub mul div floordiv mod lshift rshift and xor or pow "
inplace = ' '.join('i%s' % n for n in numerics.split())
right = ' '.join('r%s' % n for n in numerics.split())
extra = ''
if inPy3k:
    extra = 'bool next '
else:
    extra = 'unicode long nonzero oct hex truediv rtruediv '

# not including __prepare__, __instancecheck__, __subclasscheck__
# (as they are metaclass methods)
# __del__ is not supported at all as it causes problems if it exists

_non_defaults = set('__%s__' % method for method in [
    'cmp', 'getslice', 'setslice', 'coerce', 'subclasses',
    'format', 'get', 'set', 'delete', 'reversed',
    'missing', 'reduce', 'reduce_ex', 'getinitargs',
    'getnewargs', 'getstate', 'setstate', 'getformat',
    'setformat', 'repr', 'dir'
])


def _get_method(name, func):
    "Turns a callable object (like a mock) into a real function"
    def method(self, *args, **kw):
        return func(self, *args, **kw)
    method.__name__ = name
    return method


_magics = set(
    '__%s__' % method for method in
    ' '.join([magic_methods, numerics, inplace, right, extra]).split()
)

_all_magics = _magics | _non_defaults

_unsupported_magics = set([
    '__getattr__', '__setattr__',
    '__init__', '__new__', '__prepare__'
    '__instancecheck__', '__subclasscheck__',
    '__del__'
])

_calculate_return_value = {
    '__hash__': lambda self: object.__hash__(self),
    '__str__': lambda self: object.__str__(self),
    '__sizeof__': lambda self: object.__sizeof__(self),
    '__unicode__': lambda self: unicode(object.__str__(self)),
}

_return_values = {
    '__lt__': NotImplemented,
    '__gt__': NotImplemented,
    '__le__': NotImplemented,
    '__ge__': NotImplemented,
    '__int__': 1,
    '__contains__': False,
    '__len__': 0,
    '__exit__': False,
    '__complex__': 1j,
    '__float__': 1.0,
    '__bool__': True,
    '__nonzero__': True,
    '__oct__': '1',
    '__hex__': '0x1',
    '__long__': long(1),
    '__index__': 1,
}


def _get_eq(self):
    def __eq__(other):
        ret_val = self.__eq__._mock_return_value
        if ret_val is not DEFAULT:
            return ret_val
        return self is other
    return __eq__

def _get_ne(self):
    def __ne__(other):
        if self.__ne__._mock_return_value is not DEFAULT:
            return DEFAULT
        return self is not other
    return __ne__

def _get_iter(self):
    def __iter__():
        ret_val = self.__iter__._mock_return_value
        if ret_val is DEFAULT:
            return iter([])
        # if ret_val was already an iterator, then calling iter on it should
        # return the iterator unchanged
        return iter(ret_val)
    return __iter__

_side_effect_methods = {
    '__eq__': _get_eq,
    '__ne__': _get_ne,
    '__iter__': _get_iter,
}



def _set_return_value(mock, method, name):
    fixed = _return_values.get(name, DEFAULT)
    if fixed is not DEFAULT:
        method.return_value = fixed
        return

    return_calulator = _calculate_return_value.get(name)
    if return_calulator is not None:
        try:
            return_value = return_calulator(mock)
        except AttributeError:
            # XXXX why do we return AttributeError here?
            #      set it as a side_effect instead?
            return_value = AttributeError(name)
        method.return_value = return_value
        return

    side_effector = _side_effect_methods.get(name)
    if side_effector is not None:
        method.side_effect = side_effector(mock)



class MagicMixin(object):
    def __init__(self, *args, **kw):
        _super(MagicMixin, self).__init__(*args, **kw)
        self._mock_set_magics()


    def _mock_set_magics(self):
        these_magics = _magics

        if self._mock_methods is not None:
            these_magics = _magics.intersection(self._mock_methods)

            remove_magics = set()
            remove_magics = _magics - these_magics

            for entry in remove_magics:
                if entry in type(self).__dict__:
                    # remove unneeded magic methods
                    delattr(self, entry)

        # don't overwrite existing attributes if called a second time
        these_magics = these_magics - set(type(self).__dict__)

        _type = type(self)
        for entry in these_magics:
            setattr(_type, entry, MagicProxy(entry, self))



class NonCallableMagicMock(MagicMixin, NonCallableMock):
    """A version of `MagicMock` that isn't callable."""
    def mock_add_spec(self, spec, spec_set=False):
        """Add a spec to a mock. `spec` can either be an object or a
        list of strings. Only attributes on the `spec` can be fetched as
        attributes from the mock.

        If `spec_set` is True then only attributes on the spec can be set."""
        self._mock_add_spec(spec, spec_set)
        self._mock_set_magics()



class MagicMock(MagicMixin, Mock):
    """
    MagicMock is a subclass of Mock with default implementations
    of most of the magic methods. You can use MagicMock without having to
    configure the magic methods yourself.

    If you use the `spec` or `spec_set` arguments then *only* magic
    methods that exist in the spec will be created.

    Attributes and the return value of a `MagicMock` will also be `MagicMocks`.
    """
    def mock_add_spec(self, spec, spec_set=False):
        """Add a spec to a mock. `spec` can either be an object or a
        list of strings. Only attributes on the `spec` can be fetched as
        attributes from the mock.

        If `spec_set` is True then only attributes on the spec can be set."""
        self._mock_add_spec(spec, spec_set)
        self._mock_set_magics()



class MagicProxy(object):
    def __init__(self, name, parent):
        self.name = name
        self.parent = parent

    def __call__(self, *args, **kwargs):
        m = self.create_mock()
        return m(*args, **kwargs)

    def create_mock(self):
        entry = self.name
        parent = self.parent
        m = parent._get_child_mock(name=entry, _new_name=entry,
                                   _new_parent=parent)
        setattr(parent, entry, m)
        _set_return_value(parent, m, entry)
        return m

    def __get__(self, obj, _type=None):
        return self.create_mock()



class _ANY(object):
    "A helper object that compares equal to everything."

    def __eq__(self, other):
        return True

    def __ne__(self, other):
        return False

    def __repr__(self):
        return '<ANY>'

ANY = _ANY()



def _format_call_signature(name, args, kwargs):
    message = '%s(%%s)' % name
    formatted_args = ''
    args_string = ', '.join([repr(arg) for arg in args])
    kwargs_string = ', '.join([
        '%s=%r' % (key, value) for key, value in kwargs.items()
    ])
    if args_string:
        formatted_args = args_string
    if kwargs_string:
        if formatted_args:
            formatted_args += ', '
        formatted_args += kwargs_string

    return message % formatted_args



class _Call(tuple):
    """
    A tuple for holding the results of a call to a mock, either in the form
    `(args, kwargs)` or `(name, args, kwargs)`.

    If args or kwargs are empty then a call tuple will compare equal to
    a tuple without those values. This makes comparisons less verbose::

        _Call(('name', (), {})) == ('name',)
        _Call(('name', (1,), {})) == ('name', (1,))
        _Call(((), {'a': 'b'})) == ({'a': 'b'},)

    The `_Call` object provides a useful shortcut for comparing with call::

        _Call(((1, 2), {'a': 3})) == call(1, 2, a=3)
        _Call(('foo', (1, 2), {'a': 3})) == call.foo(1, 2, a=3)

    If the _Call has no name then it will match any name.
    """
    def __new__(cls, value=(), name=None, parent=None, two=False,
                from_kall=True):
        name = ''
        args = ()
        kwargs = {}
        _len = len(value)
        if _len == 3:
            name, args, kwargs = value
        elif _len == 2:
            first, second = value
            if isinstance(first, basestring):
                name = first
                if isinstance(second, tuple):
                    args = second
                else:
                    kwargs = second
            else:
                args, kwargs = first, second
        elif _len == 1:
            value, = value
            if isinstance(value, basestring):
                name = value
            elif isinstance(value, tuple):
                args = value
            else:
                kwargs = value

        if two:
            return tuple.__new__(cls, (args, kwargs))

        return tuple.__new__(cls, (name, args, kwargs))


    def __init__(self, value=(), name=None, parent=None, two=False,
                 from_kall=True):
        self.name = name
        self.parent = parent
        self.from_kall = from_kall


    def __eq__(self, other):
        if other is ANY:
            return True
        try:
            len_other = len(other)
        except TypeError:
            return False

        self_name = ''
        if len(self) == 2:
            self_args, self_kwargs = self
        else:
            self_name, self_args, self_kwargs = self

        other_name = ''
        if len_other == 0:
            other_args, other_kwargs = (), {}
        elif len_other == 3:
            other_name, other_args, other_kwargs = other
        elif len_other == 1:
            value, = other
            if isinstance(value, tuple):
                other_args = value
                other_kwargs = {}
            elif isinstance(value, basestring):
                other_name = value
                other_args, other_kwargs = (), {}
            else:
                other_args = ()
                other_kwargs = value
        else:
            # len 2
            # could be (name, args) or (name, kwargs) or (args, kwargs)
            first, second = other
            if isinstance(first, basestring):
                other_name = first
                if isinstance(second, tuple):
                    other_args, other_kwargs = second, {}
                else:
                    other_args, other_kwargs = (), second
            else:
                other_args, other_kwargs = first, second

        if self_name and other_name != self_name:
            return False

        # this order is important for ANY to work!
        return (other_args, other_kwargs) == (self_args, self_kwargs)


    def __ne__(self, other):
        return not self.__eq__(other)


    def __call__(self, *args, **kwargs):
        if self.name is None:
            return _Call(('', args, kwargs), name='()')

        name = self.name + '()'
        return _Call((self.name, args, kwargs), name=name, parent=self)


    def __getattr__(self, attr):
        if self.name is None:
            return _Call(name=attr, from_kall=False)
        name = '%s.%s' % (self.name, attr)
        return _Call(name=name, parent=self, from_kall=False)


    def __repr__(self):
        if not self.from_kall:
            name = self.name or 'call'
            if name.startswith('()'):
                name = 'call%s' % name
            return name

        if len(self) == 2:
            name = 'call'
            args, kwargs = self
        else:
            name, args, kwargs = self
            if not name:
                name = 'call'
            elif not name.startswith('()'):
                name = 'call.%s' % name
            else:
                name = 'call%s' % name
        return _format_call_signature(name, args, kwargs)


    def call_list(self):
        """For a call object that represents multiple calls, `call_list`
        returns a list of all the intermediate calls as well as the
        final call."""
        vals = []
        thing = self
        while thing is not None:
            if thing.from_kall:
                vals.append(thing)
            thing = thing.parent
        return _CallList(reversed(vals))


call = _Call(from_kall=False)



def create_autospec(spec, spec_set=False, instance=False, _parent=None,
                    _name=None, **kwargs):
    """Create a mock object using another object as a spec. Attributes on the
    mock will use the corresponding attribute on the `spec` object as their
    spec.

    Functions or methods being mocked will have their arguments checked
    to check that they are called with the correct signature.

    If `spec_set` is True then attempting to set attributes that don't exist
    on the spec object will raise an `AttributeError`.

    If a class is used as a spec then the return value of the mock (the
    instance of the class) will have the same spec. You can use a class as the
    spec for an instance object by passing `instance=True`. The returned mock
    will only be callable if instances of the mock are callable.

    `create_autospec` also takes arbitrary keyword arguments that are passed to
    the constructor of the created mock."""
    if _is_list(spec):
        # can't pass a list instance to the mock constructor as it will be
        # interpreted as a list of strings
        spec = type(spec)

    is_type = isinstance(spec, ClassTypes)

    _kwargs = {'spec': spec}
    if spec_set:
        _kwargs = {'spec_set': spec}
    elif spec is None:
        # None we mock with a normal mock without a spec
        _kwargs = {}

    _kwargs.update(kwargs)

    Klass = MagicMock
    if type(spec) in DescriptorTypes:
        # descriptors don't have a spec
        # because we don't know what type they return
        _kwargs = {}
    elif not _callable(spec):
        Klass = NonCallableMagicMock
    elif is_type and instance and not _instance_callable(spec):
        Klass = NonCallableMagicMock

    _new_name = _name
    if _parent is None:
        # for a top level object no _new_name should be set
        _new_name = ''

    mock = Klass(parent=_parent, _new_parent=_parent, _new_name=_new_name,
                 name=_name, **_kwargs)

    if isinstance(spec, FunctionTypes):
        # should only happen at the top level because we don't
        # recurse for functions
        mock = _set_signature(mock, spec)
    else:
        _check_signature(spec, mock, is_type, instance)

    if _parent is not None and not instance:
        _parent._mock_children[_name] = mock

    if is_type and not instance and 'return_value' not in kwargs:
        mock.return_value = create_autospec(spec, spec_set, instance=True,
                                            _name='()', _parent=mock)

    for entry in dir(spec):
        if _is_magic(entry):
            # MagicMock already does the useful magic methods for us
            continue

        if isinstance(spec, FunctionTypes) and entry in FunctionAttributes:
            # allow a mock to actually be a function
            continue

        # XXXX do we need a better way of getting attributes without
        # triggering code execution (?) Probably not - we need the actual
        # object to mock it so we would rather trigger a property than mock
        # the property descriptor. Likewise we want to mock out dynamically
        # provided attributes.
        # XXXX what about attributes that raise exceptions other than
        # AttributeError on being fetched?
        # we could be resilient against it, or catch and propagate the
        # exception when the attribute is fetched from the mock
        try:
            original = getattr(spec, entry)
        except AttributeError:
            continue

        kwargs = {'spec': original}
        if spec_set:
            kwargs = {'spec_set': original}

        if not isinstance(original, FunctionTypes):
            new = _SpecState(original, spec_set, mock, entry, instance)
            mock._mock_children[entry] = new
        else:
            parent = mock
            if isinstance(spec, FunctionTypes):
                parent = mock.mock

            new = MagicMock(parent=parent, name=entry, _new_name=entry,
                            _new_parent=parent, **kwargs)
            mock._mock_children[entry] = new
            skipfirst = _must_skip(spec, entry, is_type)
            _check_signature(original, new, skipfirst=skipfirst)

        # so functions created with _set_signature become instance attributes,
        # *plus* their underlying mock exists in _mock_children of the parent
        # mock. Adding to _mock_children may be unnecessary where we are also
        # setting as an instance attribute?
        if isinstance(new, FunctionTypes):
            setattr(mock, entry, new)

    return mock


def _must_skip(spec, entry, is_type):
    if not isinstance(spec, ClassTypes):
        if entry in getattr(spec, '__dict__', {}):
            # instance attribute - shouldn't skip
            return False
        spec = spec.__class__
    if not hasattr(spec, '__mro__'):
        # old style class: can't have descriptors anyway
        return is_type

    for klass in spec.__mro__:
        result = klass.__dict__.get(entry, DEFAULT)
        if result is DEFAULT:
            continue
        if isinstance(result, (staticmethod, classmethod)):
            return False
        return is_type

    # shouldn't get here unless function is a dynamically provided attribute
    # XXXX untested behaviour
    return is_type


def _get_class(obj):
    try:
        return obj.__class__
    except AttributeError:
        # in Python 2, _sre.SRE_Pattern objects have no __class__
        return type(obj)


class _SpecState(object):

    def __init__(self, spec, spec_set=False, parent=None,
                 name=None, ids=None, instance=False):
        self.spec = spec
        self.ids = ids
        self.spec_set = spec_set
        self.parent = parent
        self.instance = instance
        self.name = name


FunctionTypes = (
    # python function
    type(create_autospec),
    # instance method
    type(ANY.__eq__),
    # unbound method
    type(_ANY.__eq__),
)

FunctionAttributes = set([
    'func_closure',
    'func_code',
    'func_defaults',
    'func_dict',
    'func_doc',
    'func_globals',
    'func_name',
])


file_spec = None


def mock_open(mock=None, read_data=''):
    """
    A helper function to create a mock to replace the use of `open`. It works
    for `open` called directly or used as a context manager.

    The `mock` argument is the mock object to configure. If `None` (the
    default) then a `MagicMock` will be created for you, with the API limited
    to methods or attributes available on standard file handles.

    `read_data` is a string for the `read` method of the file handle to return.
    This is an empty string by default.
    """
    global file_spec
    if file_spec is None:
        # set on first use
        if inPy3k:
            import _io
            file_spec = list(set(dir(_io.TextIOWrapper)).union(set(dir(_io.BytesIO))))
        else:
            file_spec = file

    if mock is None:
        mock = MagicMock(name='open', spec=open)

    handle = MagicMock(spec=file_spec)
    handle.write.return_value = None
    handle.__enter__.return_value = handle
    handle.read.return_value = read_data

    mock.return_value = handle
    return mock


class PropertyMock(Mock):
    """
    A mock intended to be used as a property, or other descriptor, on a class.
    `PropertyMock` provides `__get__` and `__set__` methods so you can specify
    a return value when it is fetched.

    Fetching a `PropertyMock` instance from an object calls the mock, with
    no args. Setting it calls the mock with the value being set.
    """
    def _get_child_mock(self, **kwargs):
        return MagicMock(**kwargs)

    def __get__(self, obj, obj_type):
        return self()
    def __set__(self, obj, val):
        self(val)

########NEW FILE########
__FILENAME__ = lucky
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
import dnf.plugin

class LuckyPlugin(dnf.plugin.Plugin):

    name = 'lucky'

    def __init__(self, *args):
        self._config = False

    def config(self):
        self._config = True

########NEW FILE########
__FILENAME__ = support
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from functools import reduce
from sys import version_info as python_version

import contextlib
import dnf
import dnf.cli.cli
import dnf.cli.demand
import dnf.comps
import dnf.exceptions
import dnf.goal
import dnf.package
import dnf.persistor
import dnf.pycomp
import dnf.repo
import dnf.sack
import hawkey
import hawkey.test
import itertools
import logging
import os
import re
import unittest

if dnf.pycomp.PY3:
    from unittest import mock
else:
    from tests import mock

skip = unittest.skip

TRACEBACK_RE = re.compile(
    '(Traceback \(most recent call last\):\n'
    '(?:  File "[^"\n]+", line \d+, in \w+\n'
    '(?:    .+\n)?)+'
    '\S.*\n)')
REASONS = {
    'hole'      : 'group',
    'pepper'    : 'group',
    'right'     : 'dep',
    'tour'      : 'group',
    'trampoline': 'group',
}
RPMDB_CHECKSUM = '5ff5337cff3fcdcee31760ab6478c9a7c784c0b2'
TOTAL_RPMDB_COUNT = 5
SYSTEM_NSOLVABLES = TOTAL_RPMDB_COUNT
MAIN_NSOLVABLES = 9
UPDATES_NSOLVABLES = 4
AVAILABLE_NSOLVABLES = MAIN_NSOLVABLES + UPDATES_NSOLVABLES
TOTAL_GROUPS = 3
TOTAL_NSOLVABLES = SYSTEM_NSOLVABLES + AVAILABLE_NSOLVABLES

# testing infrastructure

def dnf_toplevel():
    return os.path.normpath(os.path.join(__file__, "../../"))

def repo(reponame):
    return os.path.join(repo_dir(), reponame)

def repo_dir():
    this_dir=os.path.dirname(__file__)
    return os.path.join(this_dir, "repos")

COMPS_PATH = os.path.join(repo_dir(), "main_comps.xml")
NONEXISTENT_FILE = os.path.join(dnf_toplevel(), "does-not/exist")
TOUR_44_PKG_PATH = os.path.join(repo_dir(), "rpm/tour-4-4.noarch.rpm")
TOUR_50_PKG_PATH = os.path.join(repo_dir(), "rpm/tour-5-0.noarch.rpm")
TOUR_51_PKG_PATH = os.path.join(repo_dir(), "rpm/tour-5-1.noarch.rpm")
USER_RUNDIR = '/tmp/dnf-user-rundir'

# often used query

def installed_but(sack, *args):
    q = sack.query().filter(reponame__eq=hawkey.SYSTEM_REPO_NAME)
    return reduce(lambda query, name: query.filter(name__neq=name), args, q)

# patching the stdout

@contextlib.contextmanager
def patch_std_streams():
    with mock.patch('sys.stdout', new_callable=dnf.pycomp.StringIO) as stdout, \
            mock.patch('sys.stderr', new_callable=dnf.pycomp.StringIO) as stderr:
        yield (stdout, stderr)

@contextlib.contextmanager
def wiretap_logs(logger_name, level, stream):
    """Record *logger_name* logs of at least *level* into the *stream*."""
    logger = logging.getLogger(logger_name)

    orig_level = logger.level
    logger.setLevel(level)

    handler = logging.StreamHandler(stream)
    logger.addHandler(handler)

    try:
        yield stream
    finally:
        logger.removeHandler(handler)
        logger.setLevel(orig_level)

# mock objects

def mock_comps(seed_persistor):
    comps = dnf.comps.Comps()
    comps.add_from_xml_filename(COMPS_PATH)

    persistor = MockGroupPersistor()
    if seed_persistor:
        p_env = persistor.environment('sugar-desktop-environment')
        p_env.grp_types = dnf.comps.ALL_TYPES
        p_env.pkg_types = dnf.comps.ALL_TYPES
        p_env.full_list.extend(('Peppers', 'somerset'))
        p_pep = persistor.group('Peppers')
        p_pep.pkg_types = dnf.comps.MANDATORY
        p_pep.full_list.extend(('hole', 'lotus'))
        p_som = persistor.group('somerset')
        p_som.pkg_types = dnf.comps.MANDATORY
        p_som.full_list.extend(('pepper', 'trampoline', 'lotus'))

    return comps, persistor

class _BaseStubMixin(object):
    """A reusable class for creating `dnf.Base` stubs.

    See also: hawkey/test/python/__init__.py.

    Note that currently the used TestSack has always architecture set to
    "x86_64". This is to get the same behavior when running unit tests on
    different arches.

    """
    def __init__(self, *extra_repos):
        super(_BaseStubMixin, self).__init__()
        for r in extra_repos:
            repo = MockRepo(r, None)
            repo.enable()
            self._repos.add(repo)

        self._conf = FakeConf()
        self._persistor = FakePersistor()
        self._yumdb = MockYumDB()
        self.ds_callback = mock.Mock()

    @property
    def sack(self):
        if self._sack:
            return self._sack
        return self.init_sack()

    def _activate_group_persistor(self):
        return MockGroupPersistor()

    def build_comps_solver(self):
        return dnf.comps.Solver(self.group_persistor, REASONS.get)

    def activate_persistor(self):
        pass

    def init_sack(self):
        # Create the Sack, tell it how to build packages, passing in the Package
        # class and a Base reference.
        self._sack = TestSack(repo_dir(), self)
        self._sack.load_system_repo()
        for repo in self.repos.iter_enabled():
            fn = "%s.repo" % repo.id
            self._sack.load_test_repo(repo.id, fn)

        self._sack.configure(self.conf.installonlypkgs)
        self._goal = dnf.goal.Goal(self._sack)
        return self._sack

    def close(self):
        pass

    def mock_cli(self):
        stream = dnf.pycomp.StringIO()
        logger = logging.getLogger('test')
        logger.setLevel(logging.DEBUG)
        logger.addHandler(logging.StreamHandler(stream))
        return mock.Mock(base=self, log_stream=stream, logger=logger,
                         nogpgcheck=True, demands=dnf.cli.demand.DemandSheet())

    def read_mock_comps(self, seed_persistor=True):
        self._comps, self.group_persistor = mock_comps(seed_persistor)
        return self._comps

    def read_all_repos(self):
        pass


class BaseCliStub(_BaseStubMixin, dnf.cli.cli.BaseCli):
    """A class mocking `dnf.cli.cli.BaseCli`."""

    def __init__(self, *extra_repos):
        """Initialize the base."""
        super(BaseCliStub, self).__init__(*extra_repos)
        self.output.term = MockTerminal()

class HistoryStub(dnf.yum.history.YumHistory):
    """Stub of dnf.yum.history.YumHistory for easier testing."""

    def __init__(self):
        """Initialize a stub instance."""
        self.old_data_pkgs = {}

    def _old_data_pkgs(self, tid, sort=True):
        """Get packages of a transaction."""
        if sort:
            raise NotImplementedError('sorting not implemented yet')
        return self.old_data_pkgs.get(tid, ())[:]

    def close(self):
        """Close the history."""
        pass

    def old(self, tids=[], limit=None, *_args, **_kwargs):
        """Get transactions with given IDs."""
        create = lambda tid: dnf.yum.history.YumHistoryTransaction(self,
            (int(tid), 0, '0:685cc4ac4ce31b9190df1604a96a3c62a3100c35',
             1, '1:685cc4ac4ce31b9190df1604a96a3c62a3100c36', 0, 0))

        sorted_all_tids = sorted(self.old_data_pkgs.keys(), reverse=True)

        trxs = (create(tid) for tid in tids or sorted_all_tids
                if tid in self.old_data_pkgs)
        limited = trxs if limit is None else itertools.islice(trxs, limit)
        return tuple(limited)

class MockOutput(object):
    _cli_confirm_gpg_key_import = None

    def __init__(self):
        self.term = MockTerminal()

    def setup_progress_callbacks(self):
        return (None, None)

class MockPackage(object):
    def __init__(self, nevra, repo=None):
        self.baseurl = None
        self.chksum = (None, None)
        self.downloadsize = None
        self.header = None
        self.location = '%s.rpm' % nevra
        self.repo = repo
        self.reponame = None if repo is None else repo.id
        self.str = nevra
        (self.name, self.epoch, self.version, self.release, self.arch) = \
            hawkey.split_nevra(nevra)
        self.evr = '%(epoch)d:%(version)s-%(release)s' % vars(self)
        self.pkgtup = (self.name, self.arch, str(self.epoch), self.version,
                       self.release)

    def __str__(self):
        return self.str

    def localPkg(self):
        return os.path.join(self.repo.pkgdir, os.path.basename(self.location))

    def returnIdSum(self):
        return self.chksum

class MockRepo(dnf.repo.Repo):
    def valid(self):
        return None

class MockTerminal(object):
    def __init__(self):
        self.MODE = {'bold'   : '', 'normal' : ''}
        self.columns = 80
        self.reinit = mock.Mock()

class TestSack(hawkey.test.TestSackMixin, dnf.sack.Sack):
    def __init__(self, repo_dir, base):
        hawkey.test.TestSackMixin.__init__(self, repo_dir)
        dnf.sack.Sack.__init__(self,
                               arch=hawkey.test.FIXED_ARCH,
                               pkgcls=dnf.package.Package,
                               pkginitval=base,
                               make_cache_dir=True)

class MockBase(_BaseStubMixin, dnf.Base):
    """A class mocking `dnf.Base`."""

def mock_sack(*extra_repos):
    return MockBase(*extra_repos).sack

class MockYumDB(mock.Mock):
    def __init__(self):
        super(mock.Mock, self).__init__()
        self.db = {}

    def get_package(self, po):
        return self.db.setdefault(str(po), mock.Mock())

    def assertLength(self, length):
        assert(len(self.db) == length)

class RPMDBAdditionalDataPackageStub(dnf.yum.rpmsack.RPMDBAdditionalDataPackage):

    """A class mocking `dnf.yum.rpmsack.RPMDBAdditionalDataPackage`."""

    def __init__(self):
        """Initialize the data."""
        super(RPMDBAdditionalDataPackageStub, self).__init__(None, None, None)

    def __iter__(self, show_hidden=False):
        """Return a new iterator over the data."""
        for item in self._read_cached_data:
            yield item

    def _attr2fn(self, attribute):
        """Convert given *attribute* to a filename."""
        raise NotImplementedError('the method is not supported')

    def _delete(self, attribute):
        """Delete the *attribute* value."""
        try:
            del self._read_cached_data[attribute]
        except KeyError:
            raise AttributeError("Cannot delete attribute %s on %s " %
                                 (attribute, self))

    def _read(self, attribute):
        """Read the *attribute* value."""
        if attribute in self._read_cached_data:
            return self._read_cached_data[attribute]
        raise AttributeError("%s has no attribute %s" % (self, attribute))

    def _write(self, attribute, value):
        """Write the *attribute* value."""
        self._auto_cache(attribute, value, None)

    def clean(self):
        """Purge out everything."""
        for item in self.__iter__(show_hidden=True):
            self._delete(item)

# mock object taken from testbase.py in yum/test:
class FakeConf(object):
    def __init__(self):
        self.assumeyes = None
        self.best = False
        self.cachedir = dnf.const.TMPDIR
        self.clean_requirements_on_remove = False
        self.color = 'never'
        self.color_update_installed = 'normal'
        self.color_update_remote = 'normal'
        self.color_list_available_downgrade = 'dim'
        self.color_list_available_install = 'normal'
        self.color_list_available_reinstall = 'bold'
        self.color_list_available_upgrade = 'bold'
        self.color_list_installed_extra = 'bold'
        self.color_list_installed_newer = 'bold'
        self.color_list_installed_older = 'bold'
        self.color_list_installed_reinstall = 'normal'
        self.color_update_local = 'bold'
        self.commands = []
        self.debug_solver = False
        self.debuglevel = 8
        self.defaultyes = False
        self.disable_excludes = []
        self.exclude = []
        self.groupremove_leaf_only = False
        self.history_record = False
        self.installonly_limit = 0
        self.installonlypkgs = ['kernel']
        self.installroot = '/'
        self.multilib_policy = 'best'
        self.obsoletes = True
        self.persistdir = '/should-not-exist-bad-test/persist'
        self.plugins = False
        self.protected_multilib = False
        self.protected_packages = []
        self.showdupesfromrepos = False
        self.tsflags = []
        self.verbose = False
        self.yumvar = {'releasever' : 'Fedora69'}

class FakePersistor(object):
    def get_expired_repos(self):
        return set()

    def reset_last_makecache(self):
        pass

    def since_last_makecache(self):
        return None

class MockGroupPersistor(dnf.persistor.GroupPersistor):
    """Empty persistor that doesn't need any I/O."""
    def __init__(self):
        self.db = self._empty_db()

# object matchers for asserts

class ObjectMatcher(object):
    """Class allowing partial matching of objects."""

    def __init__(self, type_=None, attrs=None):
        """Initialize a matcher instance."""
        self._type = type_
        self._attrs = attrs

    def __eq__(self, other):
        """Test whether this object is equal to the *other* one."""
        if self._type is not None:
            if type(other) is not self._type:
                return False

        if self._attrs:
            for attr, value in self._attrs.items():
                if value != getattr(other, attr):
                    return False
        return True

    def __ne__(self, other):
        """Test whether this object is not equal to the *other* one."""
        return not self == other

    def __repr__(self):
        """Compute the "official" string representation of this object."""
        args_strs = []

        if self._type is not None:
            args_strs.append('type_=%s' % repr(self._type))

        if self._attrs:
            attrs_str = ', '.join('%s: %s' % (str(attr), repr(value))
                                  for attr, value in self._attrs.items())
            args_strs.append('attrs={%s}' % attrs_str)

        return '%s(%s)' % (type(self).__name__, ", ".join(args_strs))

# test cases

if python_version.major < 3:
    class PycompTestCase(unittest.TestCase):
        pass
else:
    class PycompTestCase(unittest.TestCase):
        def assertItemsEqual(self, item1, item2):
            super().assertCountEqual(item1, item2)

class TestCase(PycompTestCase):
    def assertEmpty(self, collection):
        return self.assertEqual(len(collection), 0)

    def assertFile(self, path):
        """Assert the given path is a file."""
        return self.assertTrue(os.path.isfile(path))

    def assertLength(self, collection, length):
        return self.assertEqual(len(collection), length)

    def assertPathDoesNotExist(self, path):
        return self.assertFalse(os.access(path, os.F_OK))

    def assertStartsWith(self, string, what):
        return self.assertTrue(string.startswith(what))

    def assertTracebackIn(self, end, string):
        """Test that a traceback ending with line *end* is in the *string*."""
        traces = (match.group() for match in TRACEBACK_RE.finditer(string))
        self.assertTrue(any(trace.endswith(end) for trace in traces))

class ResultTestCase(TestCase):

    allow_erasing = False

    def _get_installed(self, base):
        try:
            base.resolve(self.allow_erasing)
        except dnf.exceptions.DepsolveError:
            self.fail()

        installed = set(base.sack.query().installed())
        for r in base._transaction.remove_set:
            installed.remove(r)
        installed.update(base._transaction.install_set)
        return installed

    def assertResult(self, base, pkgs):
        """Check whether the system contains the given pkgs.

        pkgs must be present. Any other pkgs result in an error. Pkgs are
        present if they are in the rpmdb and are not REMOVEd or they are
        INSTALLed.
        """

        self.assertItemsEqual(self._get_installed(base), pkgs)

    def installed_removed(self, base):
        try:
            base.resolve(self.allow_erasing)
        except dnf.exceptions.DepsolveError:
            self.fail()

        installed = base._transaction.install_set
        removed = base._transaction.remove_set
        return installed, removed

########NEW FILE########
__FILENAME__ = test_api
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
from dnf.pycomp import unicode
from tests.support import PycompTestCase
import dnf

class APITest(PycompTestCase):
    def test_base(self):
        self.assertIsInstance(dnf.Base, type)

    def test_conf(self):
        base = dnf.Base()
        self.assertIsInstance(base.conf.installroot, unicode)
        # reasonable default
        self.assertEqual(base.conf.installroot, '/')
        # assignable
        dnf.conf.installroot = '/mnt/rootimage'

########NEW FILE########
__FILENAME__ = test_arch
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals

import dnf.arch
import tests.support


class ArchTest(tests.support.TestCase):

    def test_basearch(self):
        fn = dnf.arch.basearch
        self.assertEqual(fn('armv7hl'), 'armhfp')
        self.assertEqual(fn('i686'), 'i386')
        self.assertEqual(fn('noarch'), 'noarch')
        self.assertEqual(fn('ppc64iseries'), 'ppc64')
        self.assertEqual(fn('sparc64v'), 'sparc')
        self.assertEqual(fn('x86_64'), 'x86_64')

########NEW FILE########
__FILENAME__ = test_base
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock
from tests.support import PycompTestCase

import binascii
import dnf
import dnf.const
import dnf.exceptions
import dnf.package
import dnf.subject
import dnf.transaction
import hawkey
import os
import rpm

class BaseTest(support.TestCase):
    def test_instance(self):
        base = dnf.Base()

    def test_push_userinstalled(self):
        base = support.MockBase()
        # setup:
        base.conf.clean_requirements_on_remove = True
        goal = mock.Mock(spec=["userinstalled"])
        for pkg in base.sack.query().installed():
            base.yumdb.get_package(pkg).reason = 'dep'
        pkg1 = base.sack.query().installed().filter(name="pepper")[0]
        base.yumdb.get_package(pkg1).reason = "user"
        pkg2 = base.sack.query().installed().filter(name="hole")[0]
        base.yumdb.get_package(pkg2).reason = "unknown"

        # test:
        base.push_userinstalled(goal)
        calls = [c[0][0].name for c in goal.userinstalled.call_args_list]
        self.assertItemsEqual(calls, ('hole', 'pepper'))

    def test_reset(self):
        base = support.MockBase('main')
        base.reset(sack=True, repos=False)
        self.assertIsNone(base._sack)
        self.assertLength(base.repos, 1)

    @mock.patch('dnf.rpmUtils.transaction.TransactionWrapper')
    def test_ts(self, mock_ts):
        base = dnf.Base()
        self.assertEqual(base._ts, None)
        ts = base.ts
        # check the setup is correct
        ts.setFlags.call_args.assert_called_with(0)
        flags = ts.setProbFilter.call_args[0][0]
        self.assertTrue(flags & rpm.RPMPROB_FILTER_OLDPACKAGE)
        self.assertFalse(flags & rpm.RPMPROB_FILTER_REPLACEPKG)
        # check file conflicts are reported:
        self.assertFalse(flags & rpm.RPMPROB_FILTER_REPLACENEWFILES)
        # check we can close the connection
        del base.ts
        self.assertEqual(base._ts, None)
        ts.close.assert_called_once_with()

    def test_iter_userinstalled(self):
        """Test iter_userinstalled with a package installed by the user."""
        base = dnf.Base()
        base._sack = support.mock_sack('main')
        base._yumdb = support.MockYumDB()
        pkg, = base.sack.query().installed().filter(name='pepper')
        base.yumdb.get_package(pkg).get = {'reason': 'user', 'from_repo': 'main'}.get

        iterator = base.iter_userinstalled()

        self.assertEqual(next(iterator), pkg)
        self.assertRaises(StopIteration, next, iterator)

    def test_iter_userinstalled_badfromrepo(self):
        """Test iter_userinstalled with a package installed from a bad repository."""
        base = dnf.Base()
        base._sack = support.mock_sack('main')
        base._yumdb = support.MockYumDB()

        pkg, = base.sack.query().installed().filter(name='pepper')
        base.yumdb.get_package(pkg).get = {'reason': 'user', 'from_repo': 'anakonda'}.get

        iterator = base.iter_userinstalled()

        self.assertRaises(StopIteration, next, iterator)

    def test_iter_userinstalled_badreason(self):
        """Test iter_userinstalled with a package installed for a wrong reason."""
        base = dnf.Base()
        base._sack = support.mock_sack('main')
        base._yumdb = support.MockYumDB()

        pkg, = base.sack.query().installed().filter(name='pepper')
        base.yumdb.get_package(pkg).get = {'reason': 'dep', 'from_repo': 'main'}.get

        iterator = base.iter_userinstalled()

        self.assertRaises(StopIteration, next, iterator)

    def test_translate_comps_pkg_types(self):
        base = dnf.Base()
        num = base._translate_comps_pkg_types(('mandatory', 'optional'))
        self.assertEqual(num, 12)

class MockBaseTest(PycompTestCase):
    """Test the Base methods that need a Sack."""

    def setUp(self):
        self.base = support.MockBase("main")

    def test_add_remote_rpm(self):
        pkg = self.base.add_remote_rpm(support.TOUR_50_PKG_PATH)
        self.assertIsInstance(pkg, dnf.package.Package)
        self.assertEqual(pkg.name, 'tour')

class BuildTransactionTest(support.TestCase):
    def test_resolve(self):
        base = support.MockBase("updates")
        base.upgrade("pepper")
        self.assertTrue(base.resolve())
        base.ds_callback.assert_has_calls([mock.call.start(),
                                           mock.call.pkg_added(mock.ANY, 'ud'),
                                           mock.call.pkg_added(mock.ANY, 'u')])
        self.assertLength(base.transaction, 1)

# verify transaction test helpers
HASH = "68e9ded8ea25137c964a638f12e9987c"
def mock_sack_fn():
    return (lambda base: support.TestSack(support.repo_dir(), base))

@property
def ret_pkgid(self):
    return self.name

class VerifyTransactionTest(PycompTestCase):
    def setUp(self):
        self.base = support.MockBase("main")
        self.base._transaction = dnf.transaction.Transaction()

    @mock.patch('dnf.sack.build_sack', new_callable=mock_sack_fn)
    @mock.patch('dnf.package.Package.pkgid', ret_pkgid) # neutralize @property
    def test_verify_transaction(self, unused_build_sack):
        # we don't simulate the transaction itself here, just "install" what is
        # already there and "remove" what is not.
        new_pkg = self.base.sack.query().available().filter(name="pepper")[1]
        new_pkg.chksum = (hawkey.CHKSUM_MD5, binascii.unhexlify(HASH))
        new_pkg.repo = mock.Mock()
        removed_pkg = self.base.sack.query().available().filter(
            name="mrkite")[0]

        self.base.transaction.add_install(new_pkg, [])
        self.base.transaction.add_erase(removed_pkg)
        self.base.verify_transaction(0)
        # mock is designed so this returns the exact same mock object it did
        # during the method call:
        yumdb_info = self.base.yumdb.get_package(new_pkg)
        self.assertEqual(yumdb_info.from_repo, 'main')
        self.assertEqual(yumdb_info.reason, 'unknown')
        self.assertEqual(yumdb_info.releasever, 'Fedora69')
        self.assertEqual(yumdb_info.checksum_type, 'md5')
        self.assertEqual(yumdb_info.checksum_data, HASH)
        self.base.yumdb.assertLength(2)

class InstallReasonTest(support.ResultTestCase):
    def setUp(self):
        self.base = support.MockBase("main")

    def test_reason(self):
        self.base.install("mrkite")
        self.base.resolve()
        new_pkgs = self.base._transaction.get_items(dnf.transaction.INSTALL)
        pkg_reasons = [(tsi.installed.name, tsi.reason) for tsi in new_pkgs]
        self.assertItemsEqual([("mrkite", "user"), ("trampoline", "dep")],
                              pkg_reasons)

class InstalledMatchingTest(support.ResultTestCase):
    def setUp(self):
        self.base = support.MockBase("main")
        self.sack = self.base.sack

    def test_query_matching(self):
        subj = dnf.subject.Subject("pepper")
        query = subj.get_best_query(self.sack)
        inst, avail = self.base._query_matches_installed(query)
        self.assertItemsEqual(['pepper-20-0.x86_64'], map(str, inst))
        self.assertItemsEqual(['pepper-20-0.src'], map(str, avail))

    def test_selector_matching(self):
        subj = dnf.subject.Subject("pepper")
        sltr = subj.get_best_selector(self.sack)
        inst = self.base._sltr_matches_installed(sltr)
        self.assertItemsEqual(['pepper-20-0.x86_64'], map(str, inst))

class CleanTest(PycompTestCase):
    def test_clean_binary_cache(self):
        base = support.MockBase("main")
        with mock.patch('os.access', return_value=True) as access,\
                mock.patch.object(base, "_cleanFilelist") as _:
            base.clean_binary_cache()
        self.assertEqual(len(access.call_args_list), 3)
        fname = access.call_args_list[0][0][0]
        assert(fname.startswith(dnf.const.TMPDIR))
        assert(fname.endswith(hawkey.SYSTEM_REPO_NAME + '.solv'))
        fname = access.call_args_list[1][0][0]
        assert(fname.endswith('main.solv'))
        fname = access.call_args_list[2][0][0]
        assert(fname.endswith('main-filenames.solvx'))

    def test_clean_files_local(self):
        """Do not delete files from a local repo."""
        base = support.MockBase("main")
        repo = base.repos['main']
        repo.baseurl = ['file:///dnf-bad-test']
        repo.basecachedir = '/tmp/dnf-bad-test'
        with mock.patch.object(base, "_cleanFilelist") as cf_mock,\
             mock.patch('os.path.exists', return_value=True) as exists_mock:
            base._cleanFiles(['rpm'], 'pkgdir', 'package')
        # local repo is not even checked for directory existence:
        self.assertIsNone(exists_mock.call_args)

class CompsTest(support.TestCase):
    # Also see test_comps.py

    # prevent creating the gen/ directory:
    @mock.patch('dnf.yum.misc.repo_gen_decompress', lambda x, y: x)
    def test_read_comps(self):
        base = support.MockBase("main")
        base.repos['main'].metadata = mock.Mock(comps_fn=support.COMPS_PATH)
        base.read_comps()
        groups = base.comps.groups
        self.assertLength(groups, support.TOTAL_GROUPS)

    def test_read_comps_disabled(self):
        base = support.MockBase("main")
        base.repos['main'].enablegroups = False
        self.assertEmpty(base.read_comps())

class Goal2TransactionTest(support.TestCase):
    def test_upgrade(self):
        base = support.MockBase("main", "updates")
        base.upgrade("hole")
        goal = base._goal
        self.assertTrue(base.run_hawkey_goal(goal, allow_erasing=False))
        ts = base._goal2transaction(goal)
        self.assertLength(ts._tsis, 1)
        tsi = ts._tsis[0]
        self.assertItemsEqual(map(str, tsi.installs()), ('hole-2-1.x86_64',))
        self.assertItemsEqual(map(str, tsi.removes()),
                              ('hole-1-1.x86_64', 'tour-5-0.noarch'))

########NEW FILE########
__FILENAME__ = test_cli
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from argparse import Namespace
from tests import support
from tests.support import PycompTestCase
from tests.support import mock

import dnf.cli.cli
import dnf.conf
import dnf.goal
import dnf.repo
import dnf.repodict
import os
import unittest

VERSIONS_OUTPUT="""\
  Installed: pepper-0:20-0.x86_64 at 1970-01-01 00:00
  Built    :  at 1970-01-01 00:00

  Installed: tour-0:5-0.noarch at 1970-01-01 00:00
  Built    :  at 1970-01-01 00:00
"""

class VersionStringTest(PycompTestCase):
    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    def test_print_versions(self):
        base = support.MockBase()
        output = support.MockOutput()
        with mock.patch('sys.stdout') as stdout,\
                mock.patch('dnf.sack.rpmdb_sack', return_value=base.sack):
            dnf.cli.cli.print_versions(['pepper', 'tour'], base, output)
        written = ''.join([mc[1][0] for mc in stdout.method_calls
                           if mc[0] == 'write'])
        self.assertEqual(written, VERSIONS_OUTPUT)

class BaseCliTest(support.ResultTestCase):
    def setUp(self):
        self._base = dnf.cli.cli.BaseCli()
        self._base._sack = support.mock_sack('main', 'updates')
        self._base._goal = dnf.goal.Goal(self._base.sack)

        main_repo = support.MockRepo('main', None)
        main_repo.metadata = mock.Mock(comps_fn=support.COMPS_PATH)
        main_repo.enable()
        self._base.repos.add(main_repo)

        self._base.logger = mock.create_autospec(self._base.logger)
        self._base.output.term = support.MockTerminal()
        self._base.downgrade = mock.Mock(wraps=self._base.downgrade)

    @mock.patch('dnf.cli.cli.P_', dnf.pycomp.NullTranslations().ungettext)
    def test_downgradePkgs(self):
        self._base.downgradePkgs(('tour',))

        self.assertEqual(self._base.downgrade.mock_calls, [mock.call('tour')])
        self.assertEqual(self._base.logger.mock_calls, [])

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    def test_downgradePkgs_notfound(self):
        with self.assertRaises(dnf.exceptions.Error) as ctx:
            self._base.downgradePkgs(('non-existent',))
        self.assertEqual(str(ctx.exception), 'Nothing to do.')

        self.assertEqual(self._base.downgrade.mock_calls, [mock.call('non-existent')])
        self.assertEqual(self._base.logger.mock_calls,
                         [mock.call.info('No package %s%s%s available.', '', 'non-existent', '')])

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    def test_downgradePkgs_notinstalled(self):
        pkg = support.ObjectMatcher(dnf.package.Package, {'name': 'lotus'})

        with self.assertRaises(dnf.exceptions.Error) as ctx:
            self._base.downgradePkgs(('lotus',))
        self.assertEqual(str(ctx.exception), 'Nothing to do.')

        self.assertEqual(self._base.downgrade.mock_calls, [mock.call('lotus')])
        self.assertEqual(self._base.logger.mock_calls,
                         [mock.call.info('No match for available package: %s', pkg)] * 2)

    def test_transaction_id_or_offset_bad(self):
        """Test transaction_id_or_offset with a bad input."""
        self.assertRaises(ValueError,
                          dnf.cli.cli.BaseCli.transaction_id_or_offset, 'bad')

    def test_transaction_id_or_offset_last(self):
        """Test transaction_id_or_offset with the zero offset."""
        id_or_offset = dnf.cli.cli.BaseCli.transaction_id_or_offset('last')
        self.assertEqual(id_or_offset, -1)

    def test_transaction_id_or_offset_negativeid(self):
        """Test transaction_id_or_offset with a negative ID."""
        self.assertRaises(ValueError,
                          dnf.cli.cli.BaseCli.transaction_id_or_offset, '-1')

    def test_transaction_id_or_offset_offset(self):
        """Test transaction_id_or_offset with an offset."""
        id_or_offset = dnf.cli.cli.BaseCli.transaction_id_or_offset('last-1')
        self.assertEqual(id_or_offset, -2)

    def test_transaction_id_or_offset_positiveid(self):
        """Test transaction_id_or_offset with a positive ID."""
        id_or_offset = dnf.cli.cli.BaseCli.transaction_id_or_offset('1')
        self.assertEqual(id_or_offset, 1)

@mock.patch('dnf.cli.cli.Cli.read_conf_file')
class CliTest(PycompTestCase):
    def setUp(self):
        self.base = support.MockBase("main")
        self.base.output = support.MockOutput()
        self.cli = dnf.cli.cli.Cli(self.base)

    def test_knows_upgrade(self, read_conf_file):
        upgrade = self.cli.cli_commands['upgrade']
        update = self.cli.cli_commands['update']
        self.assertIs(upgrade, update)

    def test_simple(self, read_conf_file):
        self.assertFalse(self.base.conf.assumeyes)
        self.cli.configure(['update', '-y'])
        self.assertTrue(self.base.conf.assumeyes)

    def test_opt_between_cmds(self,read_conf_file):
        self.cli.configure(args=['install', 'pkg1', '-y', 'pkg2'])
        self.assertTrue(self.base.conf.assumeyes)
        self.assertEqual(self.base.basecmd, "install")
        self.assertEqual(self.base.extcmds, ["pkg1", "pkg2"])

    def test_configure_repos(self, read_conf_file):
        opts = Namespace()
        opts.repos_ed = [('*', 'disable'), ('comb', 'enable')]
        opts.cacheonly = True
        self.base._repos = dnf.repodict.RepoDict()
        self.base._repos.add(support.MockRepo('one', None))
        self.base._repos.add(support.MockRepo('two', None))
        self.base._repos.add(support.MockRepo('comb', None))
        self.cli.nogpgcheck = True
        self.cli._configure_repos(opts)
        self.assertFalse(self.base.repos['one'].enabled)
        self.assertFalse(self.base.repos['two'].enabled)
        self.assertTrue(self.base.repos['comb'].enabled)
        self.assertFalse(self.base.repos["comb"].gpgcheck)
        self.assertFalse(self.base.repos["comb"].repo_gpgcheck)
        self.assertEqual(self.base.repos["comb"].sync_strategy,
                         dnf.repo.SYNC_ONLY_CACHE)

    def test_configure_repos_expired(self, read_conf_file):
        """Ensure that --cacheonly beats the expired status."""
        opts = Namespace()
        opts.repos_ed = []
        opts.cacheonly = True

        pers = self.base._persistor
        pers.get_expired_repos = mock.Mock(return_value=('one',))
        self.base._repos = dnf.repodict.RepoDict()
        self.base._repos.add(support.MockRepo('one', None))
        self.cli._configure_repos(opts)
        self.assertEqual(self.base.repos['one'].sync_strategy,
                         dnf.repo.SYNC_ONLY_CACHE)

@mock.patch('dnf.logging.Logging.setup', new=mock.MagicMock)
class ConfigureTest(PycompTestCase):
    def setUp(self):
        self.base = support.MockBase("main")
        self.base._conf = dnf.conf.Conf()
        self.base.output = support.MockOutput()
        self.base.plugins = mock.Mock()
        self.cli = dnf.cli.cli.Cli(self.base)
        self.cli.command = mock.Mock()
        self.conffile = os.path.join(support.dnf_toplevel(), "etc/dnf/dnf.conf")

    def test_configure(self):
        """ Test Cli.configure.

            For now just see that the method runs.
        """
        self.cli.configure(['update', '-c', self.conffile])
        self.assertEqual(self.cli.cmdstring, "dnf update -c %s " % self.conffile)

    def test_configure_verbose(self):
        self.cli.configure(['-v', 'update', '-c', self.conffile])
        self.assertEqual(self.cli.cmdstring, "dnf -v update -c %s " %
                         self.conffile)
        self.assertEqual(self.base.conf.debuglevel, 6)
        self.assertEqual(self.base.conf.errorlevel, 6)

    @mock.patch('dnf.cli.cli.Cli.read_conf_file')
    @mock.patch('dnf.cli.cli.Cli._parse_commands', new=mock.MagicMock)
    def test_installroot_explicit(self, read_conf_file):
        self.cli.base.basecmd = 'update'
        self.cli.base.extcmds = []

        self.cli.configure(['--installroot', '/roots/dnf', 'update'])
        read_conf_file.assert_called_with('/etc/dnf/dnf.conf', '/roots/dnf', None,
                                          {'conffile': '/etc/dnf/dnf.conf',
                                           'installroot': '/roots/dnf'})

    @mock.patch('dnf.cli.cli.Cli.read_conf_file')
    @mock.patch('dnf.cli.cli.Cli._parse_commands', new=mock.MagicMock)
    def test_installroot_with_etc(self, read_conf_file):
        """Test that conffile is detected in a new installroot."""
        self.cli.base.basecmd = 'update'
        self.cli.base.extcmds = []

        tlv = support.dnf_toplevel()
        self.cli.configure(['--installroot', tlv, 'update'])
        read_conf_file.assert_called_with(
            '%s/etc/dnf/dnf.conf' % tlv, tlv, None,
            {'conffile': '%s/etc/dnf/dnf.conf' % tlv,
             'installroot': tlv})

    def test_installroot_configurable(self):
        """Test that conffile is detected in a new installroot."""
        self.cli.base.basecmd = 'update'

        conf = os.path.join(support.dnf_toplevel(), "tests/etc/installroot.conf")
        self.cli.configure(['-c', conf, '--releasever', '17', 'update'])
        self.assertEqual(self.base.conf.installroot, '/roots/dnf')

########NEW FILE########
__FILENAME__ = test_cli_format
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.cli.format import format_time, format_number

import dnf.cli.format
import tests.support

class FormatTest(tests.support.TestCase):
    def test_format_time(self):
        self.assertEquals(format_time(None), '--:--')
        self.assertEquals(format_time(-1), '--:--')
        self.assertEquals(format_time(12*60+34), '12:34')
        self.assertEquals(format_time(12*3600+34*60+56), '754:56')
        self.assertEquals(format_time(12*3600+34*60+56, use_hours=True), '12:34:56')

    def test_format_number(self):
        self.assertEquals(format_number(None), '0.0  ')
        self.assertEquals(format_number(-1), '-1  ')
        self.assertEquals(format_number(1.0), '1.0  ')
        self.assertEquals(format_number(999.0), '999  ')
        self.assertEquals(format_number(1000.0), '1.0 k')
        self.assertEquals(format_number(1 << 20), '1.0 M')
        self.assertEquals(format_number(1 << 30), '1.0 G')
        self.assertEquals(format_number(1e6, SI=1), '1.0 M')
        self.assertEquals(format_number(1e9, SI=1), '1.0 G')

    def test_indent_block(self):
        s = 'big\nbrown\nbag'
        out = dnf.cli.format.indent_block(s)
        self.assertEqual(out, '  big\n  brown\n  bag')

########NEW FILE########
__FILENAME__ = test_cli_progress
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.

from __future__ import absolute_import
from __future__ import unicode_literals
from tests.support import mock

import dnf.callback
import dnf.cli.progress
import dnf.pycomp
import tests.support
import time

class MockStdout(dnf.pycomp.StringIO):
    def visible_lines(self):
        lines = self.lines()
        last = len(lines) - 1
        return [l[:-1] for (i, l) in enumerate(lines)
                if l.endswith('\n') or i == last]

    def lines(self):
        return self.getvalue().splitlines(True)

class FakePayload(object):
    def __init__(self, string, size):
        self.string = string
        self.size = size

    def __str__(self):
        return self.string

    @property
    def download_size(self):
        return self.size

class ProgressTest(tests.support.TestCase):
    def test_single(self):
        now = 1379406823.9
        fo = MockStdout()
        with mock.patch('dnf.cli.progress._term_width', return_value=60), \
             mock.patch('dnf.cli.progress.time', lambda: now):

            p = dnf.cli.progress.MultiFileProgressMeter(fo)
            pload = FakePayload('dummy-text', 5)
            p.start(1, 1)
            for i in range(6):
                now += 1.0
                p.progress(pload, i)
            p.end(pload, None, None)

        self.assertEquals(fo.lines(), [
            'dummy-text  0% [          ] ---  B/s |   0  B     --:-- ETA\r',
            'dummy-text 20% [==        ] 1.0  B/s |   1  B     00:04 ETA\r',
            'dummy-text 40% [====      ] 1.0  B/s |   2  B     00:03 ETA\r',
            'dummy-text 60% [======    ] 1.0  B/s |   3  B     00:02 ETA\r',
            'dummy-text 80% [========  ] 1.0  B/s |   4  B     00:01 ETA\r',
            'dummy-text100% [==========] 1.0  B/s |   5  B     00:00 ETA\r',
            'dummy-text                  1.0  B/s |   5  B     00:05    \n'])

    def test_mirror(self):
        fo = MockStdout()
        p = dnf.cli.progress.MultiFileProgressMeter(fo, update_period=-1)
        p.start(1, 5)
        pload = FakePayload('foo', 5.0)
        now = 1379406823.9

        with mock.patch('dnf.cli.progress._term_width', return_value=60), \
             mock.patch('dnf.cli.progress.time', lambda: now):
            p.progress(pload, 3)
            p.end(pload, dnf.callback.STATUS_MIRROR, 'Timeout.')
            p.progress(pload, 4)
        self.assertEqual(fo.visible_lines(), [
            '[MIRROR] foo: Timeout.                                     ',
            'foo        80% [========  ] ---  B/s |   4  B     --:-- ETA'])

    _REFERENCE_TAB = [
        ['(1-2/2): f  0% [          ] ---  B/s |   0  B     --:-- ETA'],
        ['(1-2/2): b 10% [=         ] 2.2  B/s |   3  B     00:12 ETA'],
        ['(1-2/2): f 20% [==        ] 2.4  B/s |   6  B     00:10 ETA'],
        ['(1-2/2): b 30% [===       ] 2.5  B/s |   9  B     00:08 ETA'],
        ['(1-2/2): f 40% [====      ] 2.6  B/s |  12  B     00:06 ETA'],
        ['(1-2/2): b 50% [=====     ] 2.7  B/s |  15  B     00:05 ETA'],
        ['(1-2/2): f 60% [======    ] 2.8  B/s |  18  B     00:04 ETA'],
        ['(1-2/2): b 70% [=======   ] 2.8  B/s |  21  B     00:03 ETA'],
        ['(1-2/2): f 80% [========  ] 2.9  B/s |  24  B     00:02 ETA'],
        ['(1-2/2): b 90% [========= ] 2.9  B/s |  27  B     00:01 ETA'],
        ['(1/2): foo                  1.0  B/s |  10  B     00:10    ',
         '(2/2): bar100% [==========] 2.9  B/s |  30  B     00:00 ETA']]

    def test_multi(self):
        now = 1379406823.9
        fo = MockStdout()
        with mock.patch('dnf.cli.progress._term_width', return_value=60), \
             mock.patch('dnf.cli.progress.time', lambda: now):

            p = dnf.cli.progress.MultiFileProgressMeter(fo)
            p.start(2, 30)
            pload1 = FakePayload('foo', 10.0)
            pload2 = FakePayload('bar', 20.0)
            for i in range(11):
                p.progress(pload1, float(i))
                if i == 10:
                    p.end(pload1, None, None)
                now += 0.5

                p.progress(pload2, float(i*2))
                self.assertEquals(self._REFERENCE_TAB[i], fo.visible_lines())
                if i == 10:
                    p.end(pload2, dnf.callback.STATUS_FAILED, 'some error')
                now += 0.5

        # check "end" events
        self.assertEqual(fo.visible_lines(), [
            '(1/2): foo                  1.0  B/s |  10  B     00:10    ',
            '[FAILED] bar: some error                                   '])
        self.assertTrue(2.0 < p.rate < 4.0)

    @mock.patch('dnf.cli.progress._term_width', return_value=40)
    def test_skip(self, mock_term_width):
        fo = MockStdout()
        p = dnf.cli.progress.MultiFileProgressMeter(fo)
        p.start(2, 30)
        pload1 = FakePayload('club', 20.0)
        p.end(pload1, dnf.callback.STATUS_ALREADY_EXISTS, 'already got')
        self.assertEqual(p.done_files, 1)
        self.assertEqual(p.done_size, pload1.size)
        self.assertEqual(fo.getvalue(),
                         '[SKIPPED] club: already got            \n')

########NEW FILE########
__FILENAME__ = test_commands
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf.cli.commands
import dnf.cli.commands.group
import dnf.cli.commands.install
import dnf.cli.commands.reinstall
import dnf.cli.commands.upgrade
import dnf.repo
import itertools
import logging
import unittest

class CommandsCliTest(support.TestCase):
    def setUp(self):
        self.base = support.MockBase()
        self.cli = self.base.mock_cli()

    def test_erase_configure(self):
        erase_cmd = dnf.cli.commands.EraseCommand(self.cli)
        erase_cmd.configure([])
        self.assertTrue(self.cli.demands.allow_erasing)

    @mock.patch('dnf.cli.commands._', dnf.pycomp.NullTranslations().ugettext)
    def test_history_get_error_output_rollback_transactioncheckerror(self):
        """Test get_error_output with the history rollback and a TransactionCheckError."""
        cmd = dnf.cli.commands.HistoryCommand(self.cli)
        self.base.basecmd = 'history'
        self.base.extcmds = ('rollback', '1')

        lines = cmd.get_error_output(dnf.exceptions.TransactionCheckError())

        self.assertEqual(
            lines,
            ('Cannot rollback transaction 1, doing so would result in an '
             'inconsistent package database.',))

    @mock.patch('dnf.cli.commands._', dnf.pycomp.NullTranslations().ugettext)
    def test_history_get_error_output_undo_transactioncheckerror(self):
        """Test get_error_output with the history undo and a TransactionCheckError."""
        cmd = dnf.cli.commands.HistoryCommand(self.cli)
        self.base.basecmd = 'history'
        self.base.extcmds = ('undo', '1')

        lines = cmd.get_error_output(dnf.exceptions.TransactionCheckError())

        self.assertEqual(
            lines,
            ('Cannot undo transaction 1, doing so would result in an '
             'inconsistent package database.',))

    @staticmethod
    @mock.patch('dnf.Base.fill_sack')
    def _do_makecache(cmd, fill_sack):
        return cmd.run(['timer'])

    def assertLastInfo(self, cmd, msg):
        self.assertEqual(cmd.base.logger.info.mock_calls[-1],
                         mock.call(msg))

    @mock.patch('dnf.cli.commands._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.util.on_ac_power', return_value=True)
    def test_makecache_timer(self, _on_ac_power):
        cmd = dnf.cli.commands.MakeCacheCommand(self.cli)
        cmd.base.logger = mock.create_autospec(cmd.base.logger)

        self.base.conf.metadata_timer_sync = 0
        self.assertFalse(self._do_makecache(cmd))
        self.assertLastInfo(cmd, u'Metadata timer caching disabled.')

        self.base.conf.metadata_timer_sync = 5 # resync after 5 seconds
        self.base._persistor.since_last_makecache = mock.Mock(return_value=3)
        self.assertFalse(self._do_makecache(cmd))
        self.assertLastInfo(cmd, u'Metadata cache refreshed recently.')

        self.base._persistor.since_last_makecache = mock.Mock(return_value=10)
        self.base._sack = 'nonempty'
        r = support.MockRepo("glimpse", None)
        self.base.repos.add(r)

        # regular case 1: metadata is already expired:
        r.metadata_expire_in = mock.Mock(return_value=(False, 0))
        r.sync_strategy = dnf.repo.SYNC_TRY_CACHE
        self.assertTrue(self._do_makecache(cmd))
        self.assertLastInfo(cmd, u'Metadata cache created.')
        self.assertEqual(r.sync_strategy, dnf.repo.SYNC_EXPIRED)

        # regular case 2: metadata is cached and will expire later than
        # metadata_timer_sync:
        r.metadata_expire_in = mock.Mock(return_value=(True, 100))
        r.sync_strategy = dnf.repo.SYNC_TRY_CACHE
        self.assertTrue(self._do_makecache(cmd))
        self.assertLastInfo(cmd, u'Metadata cache created.')
        self.assertEqual(r.sync_strategy, dnf.repo.SYNC_TRY_CACHE)

        # regular case 3: metadata is cached but will eqpire before
        # metadata_timer_sync:
        r.metadata_expire_in = mock.Mock(return_value=(True, 4))
        r.sync_strategy = dnf.repo.SYNC_TRY_CACHE
        self.assertTrue(self._do_makecache(cmd))
        self.assertLastInfo(cmd, u'Metadata cache created.')
        self.assertEqual(r.sync_strategy, dnf.repo.SYNC_EXPIRED)

    @mock.patch('dnf.cli.commands._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.util.on_ac_power', return_value=False)
    def test_makecache_timer_battery(self, _on_ac_power):
        cmd = dnf.cli.commands.MakeCacheCommand(self.cli)
        cmd.base.logger = mock.create_autospec(cmd.base.logger)
        self.base.conf.metadata_timer_sync = 5

        self.assertFalse(self._do_makecache(cmd))
        msg = u'Metadata timer caching disabled when running on a battery.'
        self.assertLastInfo(cmd, msg)

    @mock.patch('dnf.cli.commands._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.util.on_ac_power', return_value=None)
    def test_makecache_timer_battery2(self, _on_ac_power):
        cmd = dnf.cli.commands.MakeCacheCommand(self.cli)
        self.base.conf.metadata_timer_sync = 5
        self.assertTrue(self._do_makecache(cmd))

class CommandTest(support.TestCase):
    def test_canonical(self):
        cmd = dnf.cli.commands.upgrade.UpgradeCommand(None)
        (base, ext) = cmd.canonical(['update', 'cracker', 'filling'])
        self.assertEqual(base, 'upgrade')
        self.assertEqual(ext, ['cracker', 'filling'])

class EraseCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.EraseCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(EraseCommandTest, self).setUp()
        base = support.BaseCliStub()
        base.init_sack()
        self.cmd = dnf.cli.commands.EraseCommand(base.mock_cli())

    def test_run(self):
        """Test whether the package is installed."""
        self.cmd.run(['pepper'])

        self.assertResult(
            self.cmd.base,
            self.cmd.base.sack.query().installed().filter(name__neq='pepper'))

    @mock.patch('dnf.cli.commands._', dnf.pycomp.NullTranslations().ugettext)
    def test_run_notfound(self):
        """Test whether it fails if the package cannot be found."""
        stdout = dnf.pycomp.StringIO()

        with support.wiretap_logs('dnf', logging.INFO, stdout):
            self.assertRaises(dnf.exceptions.Error, self.cmd.run, ['non-existent'])

        self.assertEqual(stdout.getvalue(),
                         'No match for argument: non-existent\n')
        self.assertResult(self.cmd.base, self.cmd.base.sack.query().installed())

class InstallCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.install.InstallCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(InstallCommandTest, self).setUp()
        base = support.BaseCliStub('main')
        base.repos['main'].metadata = mock.Mock(comps_fn=support.COMPS_PATH)
        base.init_sack()
        self._cmd = dnf.cli.commands.install.InstallCommand(base.mock_cli())

    def test_configure(self):
        cli = self._cmd.cli
        self._cmd.configure([])
        self.assertFalse(cli.demands.allow_erasing)
        self.assertTrue(cli.demands.sack_activation)

    def test_run_group(self):
        """Test whether a group is installed."""
        self._cmd.run(['@Solid Ground'])

        base = self._cmd.cli.base
        self.assertResult(base, itertools.chain(
              base.sack.query().installed(),
              dnf.subject.Subject('trampoline').get_best_query(base.sack)))

    @mock.patch('dnf.cli.commands.install._',
                dnf.pycomp.NullTranslations().ugettext)
    def test_run_group_notfound(self):
        """Test whether it fails if the group cannot be found."""
        stdout = dnf.pycomp.StringIO()

        with support.wiretap_logs('dnf', logging.INFO, stdout):
            self.assertRaises(dnf.exceptions.Error,
                              self._cmd.run, ['@non-existent'])

        self.assertEqual(stdout.getvalue(),
                         "Warning: Group 'non-existent' does not exist.\n")
        self.assertResult(self._cmd.cli.base,
                          self._cmd.cli.base.sack.query().installed())

    def test_run_package(self):
        """Test whether a package is installed."""
        self._cmd.run(['lotus'])

        base = self._cmd.cli.base
        self.assertResult(base, itertools.chain(
              base.sack.query().installed(),
              dnf.subject.Subject('lotus.x86_64').get_best_query(base.sack)))

    @mock.patch('dnf.cli.commands.install._',
                dnf.pycomp.NullTranslations().ugettext)
    def test_run_package_notfound(self):
        """Test whether it fails if the package cannot be found."""
        stdout = dnf.pycomp.StringIO()

        with support.wiretap_logs('dnf', logging.INFO, stdout):
            self.assertRaises(dnf.exceptions.Error,
                              self._cmd.run, ['non-existent'])

        self.assertEqual(stdout.getvalue(),
                         'No package non-existent available.\n')
        self.assertResult(self._cmd.cli.base,
                          self._cmd.cli.base.sack.query().installed())

class ReinstallCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.ReinstallCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(ReinstallCommandTest, self).setUp()
        base = support.BaseCliStub('main')
        base.init_sack()
        self._cmd = dnf.cli.commands.reinstall.ReinstallCommand(base.mock_cli())

    def test_run(self):
        """Test whether the package is installed."""
        self._cmd.run(['pepper'])

        base = self._cmd.cli.base
        self.assertResult(base, itertools.chain(
            base.sack.query().installed().filter(name__neq='pepper'),
            dnf.subject.Subject('pepper.x86_64').get_best_query(base.sack)
            .available()))

    @mock.patch('dnf.cli.commands.reinstall._',
                dnf.pycomp.NullTranslations().ugettext)
    def test_run_notinstalled(self):
        """Test whether it fails if the package is not installed."""
        stdout = dnf.pycomp.StringIO()

        with support.wiretap_logs('dnf', logging.INFO, stdout):
            self.assertRaises(dnf.exceptions.Error, self._cmd.run, ['lotus'])

        self.assertEqual(stdout.getvalue(), 'No match for argument: lotus\n')
        self.assertResult(self._cmd.cli.base,
                          self._cmd.cli.base.sack.query().installed())

    @mock.patch('dnf.cli.commands.reinstall._',
                dnf.pycomp.NullTranslations().ugettext)
    def test_run_notavailable(self):
        """Test whether it fails if the package is not available."""
        base = self._cmd.cli.base
        holes_query = dnf.subject.Subject('hole').get_best_query(base.sack)
        for pkg in holes_query.installed():
            self._cmd.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self._cmd.base.yumdb.get_package(pkg).from_repo = 'unknown'
        stdout = dnf.pycomp.StringIO()

        with support.wiretap_logs('dnf', logging.INFO, stdout):
            self.assertRaises(dnf.exceptions.Error, self._cmd.run, ['hole'])

        self.assertEqual(
            stdout.getvalue(),
            'Installed package hole-1-1.x86_64 (from unknown) not available.\n')
        self.assertResult(base, base.sack.query().installed())

class RepoPkgsCommandTest(unittest.TestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsCommandTest, self).setUp()
        cli = support.BaseCliStub().mock_cli()
        self.cmd = dnf.cli.commands.RepoPkgsCommand(cli)

    def test_configure_badargs(self):
        """Test whether the method does not fail even in case of wrong args."""
        self.cmd.configure([])

class RepoPkgsCheckUpdateSubCommandTest(unittest.TestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.CheckUpdateSubCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsCheckUpdateSubCommandTest, self).setUp()
        cli = support.BaseCliStub('main', 'updates', 'third_party').mock_cli()
        self._cmd = dnf.cli.commands.RepoPkgsCommand.CheckUpdateSubCommand(cli)
        self.cli = cli

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    def test(self):
        """Test whether only upgrades in the repository are listed."""
        with support.patch_std_streams() as (stdout, _):
            self._cmd.run('updates', [])

        self.assertEqual(
            stdout.getvalue(),
            u'\n'
            u'hole.x86_64                               1-2'
            u'                            updates\n'
            u'hole.x86_64                               2-1'
            u'                            updates\n'
            u'pepper.x86_64                             20-1'
            u'                           updates\n'
            u'Obsoleting Packages\n'
            u'hole.i686                                 2-1'
            u'                            updates\n'
            u'    tour.noarch                           5-0'
            u'                            @System\n'
            u'hole.x86_64                               2-1'
            u'                            updates\n'
            u'    tour.noarch                           5-0'
            u'                            @System\n')
        self.assertEqual(self.cli.demands.success_exit_status, 100)

    def test_not_found(self):
        """Test whether exit code differs if updates are not found."""
        self._cmd.run('main', [])
        self.assertNotEqual(self.cli.demands.success_exit_status, 100)

class RepoPkgsInfoSubCommandTest(unittest.TestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.InfoSubCommand`` class."""

    AVAILABLE_TITLE = u'Available Packages\n'

    HOLE_I686_INFO = (u'Name        : hole\n'
                      u'Arch        : i686\n'
                      u'Epoch       : 0\n'
                      u'Version     : 2\n'
                      u'Release     : 1\n'
                      u'Size        : 0.0  \n'
                      u'Repo        : updates\n'
                      u'Summary     : \n'
                      u'License     : \n'
                      u'Description : \n'
                      u'\n')

    HOLE_X86_64_INFO = (u'Name        : hole\n'
                        u'Arch        : x86_64\n'
                        u'Epoch       : 0\n'
                        u'Version     : 2\n'
                        u'Release     : 1\n'
                        u'Size        : 0.0  \n'
                        u'Repo        : updates\n'
                        u'Summary     : \n'
                        u'License     : \n'
                        u'Description : \n\n')

    INSTALLED_TITLE = u'Installed Packages\n'

    PEPPER_SYSTEM_INFO = (u'Name        : pepper\n'
                          u'Arch        : x86_64\n'
                          u'Epoch       : 0\n'
                          u'Version     : 20\n'
                          u'Release     : 0\n'
                          u'Size        : 0.0  \n'
                          u'Repo        : @System\n'
                          u'From repo   : main\n'
                          u'Summary     : \n'
                          u'License     : \n'
                          u'Description : \n\n')

    PEPPER_UPDATES_INFO = (u'Name        : pepper\n'
                           u'Arch        : x86_64\n'
                           u'Epoch       : 0\n'
                           u'Version     : 20\n'
                           u'Release     : 1\n'
                           u'Size        : 0.0  \n'
                           u'Repo        : updates\n'
                           u'Summary     : \n'
                           u'License     : \n'
                           u'Description : \n\n')

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsInfoSubCommandTest, self).setUp()
        base = support.BaseCliStub('main', 'updates', 'third_party')
        base.conf.recent = 7
        self._cmd = dnf.cli.commands.RepoPkgsCommand.InfoSubCommand(
                        base.mock_cli())

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    def test_info_all(self):
        """Test whether only packages related to the repository are listed."""
        for pkg in self._cmd.base.sack.query().installed().filter(name='pepper'):
            self._cmd.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self._cmd.base.yumdb.get_package(pkg).from_repo = 'main'

        with support.patch_std_streams() as (stdout, _):
            self._cmd.run('main', ['all', '*p*'])

        self.assertEqual(
            stdout.getvalue(),
            ''.join((
                self.INSTALLED_TITLE,
                self.PEPPER_SYSTEM_INFO,
                self.AVAILABLE_TITLE,
                u'Name        : pepper\n'
                u'Arch        : src\n'
                u'Epoch       : 0\n'
                u'Version     : 20\n'
                u'Release     : 0\n'
                u'Size        : 0.0  \n'
                u'Repo        : main\n'
                u'Summary     : \n'
                u'License     : \n'
                u'Description : \n'
                u'\n',
                u'Name        : trampoline\n'
                u'Arch        : noarch\n'
                u'Epoch       : 0\n'
                u'Version     : 2.1\n'
                u'Release     : 1\n'
                u'Size        : 0.0  \n'
                u'Repo        : main\n'
                u'Summary     : \n'
                u'License     : \n'
                u'Description : \n'
                u'\n')))

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    def test_info_available(self):
        """Test whether only packages in the repository are listed."""
        with support.patch_std_streams() as (stdout, _):
            self._cmd.run('updates', ['available'])

        self.assertEqual(
            stdout.getvalue(),
            ''.join((
                self.AVAILABLE_TITLE,
                self.HOLE_I686_INFO,
                self.HOLE_X86_64_INFO,
                self.PEPPER_UPDATES_INFO)))

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    def test_info_extras(self):
        """Test whether only extras installed from the repository are listed."""
        for pkg in self._cmd.base.sack.query().installed().filter(name='tour'):
            self._cmd.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self._cmd.base.yumdb.get_package(pkg).from_repo = 'unknown'

        with support.patch_std_streams() as (stdout, _):
            self._cmd.run('unknown', ['extras'])

        self.assertEqual(
            stdout.getvalue(),
            u'Extra Packages\n'
            u'Name        : tour\n'
            u'Arch        : noarch\n'
            u'Epoch       : 0\n'
            u'Version     : 5\n'
            u'Release     : 0\n'
            u'Size        : 0.0  \n'
            u'Repo        : @System\n'
            u'From repo   : unknown\n'
            u'Summary     : \n'
            u'License     : \n'
            u'Description : \n\n')

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    def test_info_installed(self):
        """Test whether only packages installed from the repository are listed."""
        for pkg in self._cmd.base.sack.query().installed().filter(name='pepper'):
            self._cmd.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self._cmd.base.yumdb.get_package(pkg).from_repo = 'main'

        with support.patch_std_streams() as (stdout, _):
            self._cmd.run('main', ['installed'])

        self.assertEqual(
            stdout.getvalue(),
            ''.join((self.INSTALLED_TITLE, self.PEPPER_SYSTEM_INFO)))

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    def test_info_obsoletes(self):
        """Test whether only obsoletes in the repository are listed."""
        with support.patch_std_streams() as (stdout, _):
            self._cmd.run('updates', ['obsoletes'])

        self.assertEqual(
            stdout.getvalue(),
            ''.join((
                u'Obsoleting Packages\n',
                self.HOLE_I686_INFO,
                self.HOLE_X86_64_INFO)))

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    def test_info_recent(self):
        """Test whether only packages in the repository are listed."""
        with mock.patch('time.time', return_value=0), \
                support.patch_std_streams() as (stdout, _):
            self._cmd.run('updates', ['recent'])

        self.assertEqual(
            stdout.getvalue(),
            ''.join((
                u'Recently Added Packages\n',
                self.HOLE_I686_INFO,
                self.HOLE_X86_64_INFO,
                self.PEPPER_UPDATES_INFO)))

    @mock.patch('dnf.cli.cli._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    def test_info_upgrades(self):
        """Test whether only upgrades in the repository are listed."""
        with support.patch_std_streams() as (stdout, _):
            self._cmd.run('updates', ['upgrades'])

        self.assertEqual(
            stdout.getvalue(),
            ''.join((
                u'Upgraded Packages\n'
                u'Name        : hole\n'
                u'Arch        : x86_64\n'
                u'Epoch       : 0\n'
                u'Version     : 1\n'
                u'Release     : 2\n'
                u'Size        : 0.0  \n'
                u'Repo        : updates\n'
                u'Summary     : \n'
                u'License     : \n'
                u'Description : \n'
                u'\n',
                self.HOLE_X86_64_INFO,
                self.PEPPER_UPDATES_INFO)))

class RepoPkgsInstallSubCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.InstallSubCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsInstallSubCommandTest, self).setUp()
        base = support.BaseCliStub('main', 'third_party')
        base.repos['main'].metadata = mock.Mock(comps_fn=support.COMPS_PATH)
        base.repos['third_party'].enablegroups = False
        base.init_sack()
        self.cmd = dnf.cli.commands.RepoPkgsCommand.InstallSubCommand(
                       base.mock_cli())

    def test_all(self):
        """Test whether all packages from the repository are installed."""
        self.cmd.run('third_party', [])

        self.assertResult(self.cmd.base, itertools.chain(
            self.cmd.base.sack.query().installed().filter(name__neq='hole'),
            self.cmd.base.sack.query().available().filter(reponame='third_party',
                                                          arch='x86_64')))

class RepoPkgsMoveToSubCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.MoveToSubCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsMoveToSubCommandTest, self).setUp()
        base = support.BaseCliStub('distro', 'main')
        base.init_sack()
        self.cmd = dnf.cli.commands.RepoPkgsCommand.MoveToSubCommand(
                       base.mock_cli())

    def test_all(self):
        """Test whether only packages in the repository are installed."""
        self.cmd.run('distro', [])

        self.assertResult(self.cmd.base, itertools.chain(
            self.cmd.base.sack.query().installed().filter(name__neq='tour'),
            dnf.subject.Subject('tour-5-0').get_best_query(self.cmd.base.sack)
            .available()))

class RepoPkgsReinstallOldSubCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.ReinstallOldSubCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsReinstallOldSubCommandTest, self).setUp()
        base = support.BaseCliStub('main')
        base.init_sack()
        self.cmd = dnf.cli.commands.RepoPkgsCommand.ReinstallOldSubCommand(
                       base.mock_cli())

    def test_all(self):
        """Test whether all packages from the repository are reinstalled."""
        for pkg in self.cmd.base.sack.query().installed():
            reponame = 'main' if pkg.name != 'pepper' else 'non-main'
            self.cmd.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self.cmd.base.yumdb.get_package(pkg).from_repo = reponame

        self.cmd.run('main', [])

        self.assertResult(self.cmd.base, itertools.chain(
              self.cmd.base.sack.query().installed().filter(name__neq='librita'),
              dnf.subject.Subject('librita.i686').get_best_query(self.cmd.base.sack)
              .installed(),
              dnf.subject.Subject('librita').get_best_query(self.cmd.base.sack)
              .available()))

class RepoPkgsReinstallSubCommandTest(unittest.TestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.ReinstallSubCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsReinstallSubCommandTest, self).setUp()
        base = support.BaseCliStub('main')
        self.cmd = dnf.cli.commands.RepoPkgsCommand.ReinstallSubCommand(
                       base.mock_cli())

        self.mock = mock.Mock()
        old_run_patcher = mock.patch(
            'dnf.cli.commands.RepoPkgsCommand.ReinstallOldSubCommand.run',
            self.mock.reinstall_old_run)
        move_run_patcher = mock.patch(
            'dnf.cli.commands.RepoPkgsCommand.MoveToSubCommand.run',
            self.mock.move_to_run)

        old_run_patcher.start()
        self.addCleanup(old_run_patcher.stop)
        move_run_patcher.start()
        self.addCleanup(move_run_patcher.stop)

    def test_all_fails(self):
        """Test whether it fails if everything fails."""
        self.mock.reinstall_old_run.side_effect = dnf.exceptions.Error('test')
        self.mock.move_to_run.side_effect = dnf.exceptions.Error('test')

        self.assertRaises(dnf.exceptions.Error, self.cmd.run, 'main', [])

        self.assertEqual(self.mock.mock_calls,
                         [mock.call.reinstall_old_run('main', []),
                          mock.call.move_to_run('main', [])])

    def test_all_moveto(self):
        """Test whether reinstall-old is called first and move-to next."""
        self.mock.reinstall_old_run.side_effect = dnf.exceptions.Error('test')

        self.cmd.run('main', [])

        self.assertEqual(self.mock.mock_calls,
                         [mock.call.reinstall_old_run('main', []),
                          mock.call.move_to_run('main', [])])

    def test_all_reinstallold(self):
        """Test whether only reinstall-old is called."""
        self.cmd.run('main', [])

        self.assertEqual(self.mock.mock_calls,
                         [mock.call.reinstall_old_run('main', [])])

class RepoPkgsRemoveOrReinstallSubCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.RemoveOrReinstallSubCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsRemoveOrReinstallSubCommandTest, self).setUp()
        base = support.BaseCliStub('distro')
        base.init_sack()
        self.cmd = dnf.cli.commands.RepoPkgsCommand.RemoveOrReinstallSubCommand(
                       base.mock_cli())

    def test_all_not_installed(self):
        """Test whether it fails if no package is installed from the repository."""
        self.assertRaises(dnf.exceptions.Error, self.cmd.run, 'non-distro', [])

        self.assertResult(self.cmd.base, self.cmd.base.sack.query().installed())

    def test_all_reinstall(self):
        """Test whether all packages from the repository are reinstalled."""
        for pkg in self.cmd.base.sack.query().installed():
            reponame = 'distro' if pkg.name != 'tour' else 'non-distro'
            self.cmd.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self.cmd.base.yumdb.get_package(pkg).from_repo = reponame

        self.cmd.run('non-distro', [])

        self.assertResult(self.cmd.base, itertools.chain(
              self.cmd.base.sack.query().installed().filter(name__neq='tour'),
              dnf.subject.Subject('tour').get_best_query(self.cmd.base.sack)
              .available()))

    def test_all_remove(self):
        """Test whether all packages from the repository are removed."""
        for pkg in self.cmd.base.sack.query().installed():
            reponame = 'distro' if pkg.name != 'hole' else 'non-distro'
            self.cmd.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self.cmd.base.yumdb.get_package(pkg).from_repo = reponame

        self.cmd.run('non-distro', [])

        self.assertResult(
            self.cmd.base,
            self.cmd.base.sack.query().installed().filter(name__neq='hole'))

class RepoPkgsRemoveSubCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.RemoveSubCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsRemoveSubCommandTest, self).setUp()
        base = support.BaseCliStub('main')
        base.init_sack()
        self.cmd = dnf.cli.commands.RepoPkgsCommand.RemoveSubCommand(
                       base.mock_cli())

    def test_all(self):
        """Test whether only packages from the repository are removed."""
        for pkg in self.cmd.base.sack.query().installed():
            reponame = 'main' if pkg.name == 'pepper' else 'non-main'
            self.cmd.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self.cmd.base.yumdb.get_package(pkg).from_repo = reponame

        self.cmd.run('main', [])

        self.assertResult(
            self.cmd.base,
            self.cmd.base.sack.query().installed().filter(name__neq='pepper'))

class RepoPkgsUpgradeSubCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.UpgradeSubCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsUpgradeSubCommandTest, self).setUp()
        base = support.BaseCliStub('updates', 'third_party')
        base.init_sack()
        self.cmd = dnf.cli.commands.RepoPkgsCommand.UpgradeSubCommand(
                        base.mock_cli())

    def test_all(self):
        """Test whether all packages from the repository are installed."""
        self.cmd.run('third_party', [])

        self.assertResult(self.cmd.base, itertools.chain(
            self.cmd.base.sack.query().installed().filter(name__neq='hole'),
            self.cmd.base.sack.query().upgrades().filter(reponame='third_party',
                                                         arch='x86_64')))

class RepoPkgsUpgradeToSubCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.RepoPkgsCommand.UpgradeToSubCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(RepoPkgsUpgradeToSubCommandTest, self).setUp()
        base = support.BaseCliStub('updates', 'third_party')
        base.init_sack()
        self.cmd = dnf.cli.commands.RepoPkgsCommand.UpgradeToSubCommand(
                       base.mock_cli())

    def test_all(self):
        """Test whether the package from the repository is installed."""
        self.cmd.run('updates', ['hole-1-2'])

        self.assertResult(self.cmd.base, itertools.chain(
            self.cmd.base.sack.query().installed().filter(name__neq='hole'),
            dnf.subject.Subject('hole-1-2.x86_64').get_best_query(self.cmd.base.sack)
            .filter(reponame='updates')))

class UpgradeCommandTest(support.ResultTestCase):

    """Tests of ``dnf.cli.commands.upgrade.UpgradeCommand`` class."""

    def setUp(self):
        """Prepare the test fixture."""
        super(UpgradeCommandTest, self).setUp()
        base = support.BaseCliStub('updates')
        base.init_sack()
        self.cmd = dnf.cli.commands.upgrade.UpgradeCommand(base.mock_cli())

    def test_run(self):
        """Test whether a package is updated."""
        self.cmd.run(['pepper'])

        self.assertResult(self.cmd.base, itertools.chain(
            self.cmd.base.sack.query().installed().filter(name__neq='pepper'),
            self.cmd.base.sack.query().upgrades().filter(name='pepper')))

    @mock.patch('dnf.cli.commands.upgrade._',
                dnf.pycomp.NullTranslations().ugettext)
    def test_updatePkgs_notfound(self):
        """Test whether it fails if the package cannot be found."""
        stdout = dnf.pycomp.StringIO()

        with support.wiretap_logs('dnf', logging.INFO, stdout):
            self.assertRaises(dnf.exceptions.Error,
                              self.cmd.run, ['non-existent'])

        self.assertEqual(stdout.getvalue(),
                         'No match for argument: non-existent\n')
        self.assertResult(self.cmd.base, self.cmd.base.sack.query().installed())

########NEW FILE########
__FILENAME__ = test_comps
# -*- coding: utf-8 -*-
#
# Copyright (C) 2013-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf.comps
import dnf.exceptions
import dnf.persistor
import dnf.util
import libcomps
import operator

TRANSLATION=u"""Tato skupina zahrnuje nejmen monou mnoinu balk. Je vhodn napklad na instalace malch router nebo firewall."""

class LangsTest(support.TestCase):
    @mock.patch('locale.getlocale', return_value=('cs_CZ', 'UTF-8'))
    def test_get(self, _unused):
        langs = dnf.comps._Langs().get()
        self.assertEqual(langs, ['cs_CZ.UTF-8', 'cs_CZ', 'cs.UTF-8', 'cs', 'C'])

class CompsTest(support.TestCase):
    def setUp(self):
        comps = dnf.comps.Comps()
        comps.add_from_xml_filename(support.COMPS_PATH)
        self.comps = comps

    def test_by_pattern(self):
        comps = self.comps
        self.assertLength(comps.groups_by_pattern('Base'), 1)
        self.assertLength(comps.groups_by_pattern('*'), support.TOTAL_GROUPS)
        self.assertLength(comps.groups_by_pattern('Solid*'), 1)

        group = dnf.util.first(comps.groups_by_pattern('Base'))
        self.assertIsInstance(group, dnf.comps.Group)

    def test_environments(self):
        env = self.comps.environments[0]
        self.assertEqual(env.name_by_lang['cs'], u'Prosted Sugar')
        self.assertEqual(env.desc_by_lang['de'],

                         u'Eine Software-Spielwiese zum Lernen des Lernens.')
        self.assertItemsEqual((id_.name for id_ in env.group_ids),
                              ('somerset', 'Peppers'))
        self.assertItemsEqual((id_.default for id_ in env.group_ids),
                              (True, False))
        self.assertItemsEqual((id_.name for id_ in env.option_ids),
                              ('base',))

        self.assertTrue(all(isinstance(grp, dnf.comps.Group)
                            for grp in env.groups_iter()))

    def test_groups(self):
        g = self.comps.group_by_pattern('base')
        self.assertTrue(g.visible)
        g = self.comps.group_by_pattern('somerset')
        self.assertFalse(g.visible)

    def test_group_packages(self):
        g = self.comps.group_by_pattern('base')
        self.assertItemsEqual(map(operator.attrgetter('name'), g.packages_iter()),
                              ('tour', 'pepper'))

    def test_iteration(self):
        comps = self.comps
        self.assertEqual([g.name for g in comps.groups_iter()],
                         ['Base', 'Solid Ground', "Pepper's"])
        self.assertEqual([c.name for c in comps.categories_iter()],
                         ['Base System'])
        g = dnf.util.first(comps.groups_iter())
        self.assertEqual(g.desc_by_lang['cs'], TRANSLATION)

    def test_packages(self):
        comps = self.comps
        group = dnf.util.first(comps.groups_iter())
        self.assertSequenceEqual([pkg.name for pkg in group.packages],
                                 (u'pepper', u'tour'))
        self.assertSequenceEqual([pkg.name for pkg in group.mandatory_packages],
                                 (u'pepper', u'tour'))

    def test_size(self):
        comps = self.comps
        self.assertLength(comps, 5)
        self.assertLength(comps.groups, support.TOTAL_GROUPS)
        self.assertLength(comps.categories, 1)
        self.assertLength(comps.environments, 1)

    @mock.patch('locale.getlocale', return_value=('cs_CZ', 'UTF-8'))
    def test_ui_name(self, _unused):
        comps = self.comps
        group = dnf.util.first(comps.groups_by_pattern('base'))
        self.assertEqual(group.ui_name, u'Kritick cesta (Zklad)')

    @mock.patch('locale.getlocale', return_value=('cs_CZ', 'UTF-8'))
    def test_ui_desc(self, _unused):
        comps = self.comps
        env = dnf.util.first(comps.environments_by_pattern('sugar-*'))
        self.assertEqual(env.ui_description, u'Software pro vuku o vyuovn.')

class PackageTest(support.TestCase):
    def test_instance(self):
        lc_pkg = libcomps.Package('weather', libcomps.PACKAGE_TYPE_OPTIONAL)
        pkg = dnf.comps.Package(lc_pkg)
        self.assertEqual(pkg.name, 'weather')
        self.assertEqual(pkg.option_type, dnf.comps.OPTIONAL)

class TestTransactionBunch(support.TestCase):

    def test_adding(self):
        t1 = dnf.comps.TransactionBunch()
        t1.install = {'right'}
        t1.upgrade = {'tour'}
        t1.remove = {'pepper'}
        t2 = dnf.comps.TransactionBunch()
        t2.install = {'pepper'}
        t2.upgrade = {'right'}
        t1 += t2
        self.assertItemsEqual(t1.install, ('right', 'pepper'))
        self.assertItemsEqual(t1.upgrade, ('tour', 'right'))
        self.assertEmpty(t1.remove)


class SolverTestMixin(object):

    def setUp(self):
        comps = dnf.comps.Comps()
        comps.add_from_xml_filename(support.COMPS_PATH)
        self.comps = comps
        self.persistor = support.MockGroupPersistor()
        self.solver = dnf.comps.Solver(self.persistor, support.REASONS.get)


class SolverGroupTest(SolverTestMixin, support.TestCase):

    def test_install(self):
        grp = self.comps.group_by_pattern('base')
        trans = self.solver.group_install(grp, dnf.comps.MANDATORY, ['right'])
        self.assertLength(trans.install, 2)
        p_grp = self.persistor.group('base')
        self.assertItemsEqual(p_grp.full_list, ['pepper', 'tour'])
        self.assertItemsEqual(p_grp.pkg_exclude, ['right'])
        self.assertEqual(p_grp.pkg_types, dnf.comps.MANDATORY)

    def test_removable_pkg(self):
        p_grp1 = self.persistor.group('base')
        p_grp2 = self.persistor.group('tune')
        p_grp1.full_list.extend(('pepper', 'tour', 'right'))
        p_grp2.full_list.append('tour')
        self.assertTrue(self.solver._removable_pkg('pepper'))
        # right's reason is "dep"
        self.assertFalse(self.solver._removable_pkg('right'))
        # tour appears in more than one group
        self.assertFalse(self.solver._removable_pkg('tour'))

    def test_remove(self):
        # setup of the "current state"
        p_grp = self.persistor.group('base')
        p_grp.pkg_types = dnf.comps.MANDATORY
        p_grp.full_list.extend(('pepper', 'tour'))
        p_grp2 = self.persistor.group('tune')
        p_grp2.full_list.append('pepper')

        grp = self.comps.group_by_pattern('base')
        trans = self.solver.group_remove(grp)
        self.assertFalse(p_grp.installed)
        self.assertItemsEqual(trans.remove, ('tour',))

    def test_upgrade(self):
        # setup of the "current state"
        p_grp = self.persistor.group('base')
        p_grp.pkg_types = dnf.comps.MANDATORY
        p_grp.full_list.extend(('pepper', 'handerson'))

        grp = self.comps.group_by_pattern('base')
        trans = self.solver.group_upgrade(grp)
        self.assertItemsEqual(trans.install, ('tour',))
        self.assertItemsEqual(trans.remove, ('handerson',))
        self.assertItemsEqual(trans.upgrade, ('pepper',))
        self.assertItemsEqual(p_grp.full_list, ('tour', 'pepper'))


class SolverEnvironmentTest(SolverTestMixin, support.TestCase):

    def _install(self, env):
        return self.solver.environment_install(env, dnf.comps.MANDATORY,
                                               ('lotus',))

    def test_install(self):
        env = self.comps.environment_by_pattern('sugar-desktop-environment')
        trans = self._install(env)
        self.assertItemsEqual(trans.install, ('pepper', 'trampoline', 'hole',
                                              'lotus'))
        sugar = self.persistor.environment('sugar-desktop-environment')
        self.assertItemsEqual(sugar.full_list, ('Peppers', 'somerset'))
        somerset = self.persistor.group('somerset')
        self.assertTrue(somerset.installed)
        self.assertEqual(somerset.pkg_types, dnf.comps.MANDATORY)
        self.assertItemsEqual(somerset.pkg_exclude, ('lotus',))
        base = self.persistor.group('somerset')
        self.assertTrue(base.installed)

    def test_remove(self):
        env = self.comps.environment_by_pattern('sugar-desktop-environment')
        self._install(env)
        trans = self.solver.environment_remove(env)

        p_env = self.persistor.environment('sugar-desktop-environment')
        self.assertItemsEqual(trans.remove, ('pepper', 'trampoline', 'hole'))
        self.assertFalse(p_env.grp_types)
        self.assertFalse(p_env.pkg_types)

    def test_upgrade(self):
        """Upgrade environment, the one group it knows is no longer installed."""
        p_env = self.persistor.environment('sugar-desktop-environment')
        p_env.full_list.extend(['somerset'])
        p_env.grp_types = dnf.comps.ALL_TYPES
        p_env.pkg_types = dnf.comps.ALL_TYPES

        env = self.comps.environment_by_pattern('sugar-desktop-environment')
        trans = self.solver.environment_upgrade(env)
        self.assertItemsEqual(trans.install, ('hole', 'lotus'))
        self.assertEmpty(trans.upgrade)

########NEW FILE########
__FILENAME__ = test_config
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
from dnf.conf import CliCache
from dnf.yum.config import Option, BaseConfig, YumConf
from tests.support import PycompTestCase
from tests.support import mock

import unittest

class OptionTest(unittest.TestCase):
    class Cfg(BaseConfig):
        a_setting = Option("roundabout")

    def test_delete(self):
        cfg = self.Cfg()
        self.assertEqual(cfg.a_setting, "roundabout")
        del cfg.a_setting
        try:
            cfg.a_setting
        except RuntimeError as e:
            pass
        else:
            self.fail("option should be deleted now.")

class CacheTest(PycompTestCase):
     @mock.patch('dnf.util.am_i_root', return_value=True)
     def test_root(self, unused_am_i_root):
         cache = CliCache('/var/lib/spinning', 'i286/20')
         self.assertEqual(cache.system_cachedir, '/var/lib/spinning/i286/20')
         self.assertEqual(cache.cachedir, '/var/lib/spinning/i286/20')

     @mock.patch('dnf.yum.misc.getCacheDir', return_value="/notmp/dnf-walr-yeAH")
     @mock.patch('dnf.util.am_i_root', return_value=False)
     def test_noroot(self, fn_root, fn_getcachedir):
         cache = CliCache('/var/lib/spinning', 'i286/20')
         self.assertEqual(fn_getcachedir.call_count, 0)
         self.assertEqual(cache.cachedir, '/notmp/dnf-walr-yeAH/i286/20')
         self.assertEqual(fn_getcachedir.call_count, 1)

         # the cachedirs are cached now, getCacheDir is not called again:
         self.assertEqual(cache.cachedir, '/notmp/dnf-walr-yeAH/i286/20')
         self.assertEqual(fn_getcachedir.call_count, 1)

class YumConfTest(PycompTestCase):
    def test_bugtracker(self):
        conf = YumConf()
        self.assertEqual(conf.bugtracker_url,
                         "https://bugzilla.redhat.com/enter_bug.cgi" +
                         "?product=Fedora&component=dnf")

    def test_overrides(self):
        conf = YumConf()
        self.assertFalse(conf.assumeyes)
        self.assertFalse(conf.assumeno)
        self.assertEqual(conf.color_list_installed_older, 'bold')

        override = {'assumeyes': True,
                    'color_list_installed_older': 'timid'}
        conf.override(override)
        self.assertTrue(conf.assumeyes)
        self.assertFalse(conf.assumeno) # no change
        self.assertEqual(conf.color_list_installed_older, 'timid')

    def test_prepend_installroot(self):
        conf = YumConf()
        conf.installroot = '/mnt/root'
        conf.prepend_installroot('persistdir')
        self.assertEqual(conf.persistdir, '/mnt/root/var/lib/dnf')

    def test_ranges(self):
        conf = YumConf()
        with self.assertRaises(ValueError):
            conf.debuglevel = 11

########NEW FILE########
__FILENAME__ = test_distsync
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf.goal
import rpm

class DistroSyncAll(support.ResultTestCase):
    def setUp(self):
        self.base = support.MockBase("distro")
        self.sack = self.base.sack

    def test_distro_sync_all(self):
        self.base.distro_sync()
        self.assertIn(rpm.RPMPROB_FILTER_OLDPACKAGE, self.base.rpm_probfilter)
        packages = support.installed_but(self.sack, "pepper", "librita").run()
        q = self.sack.query().available().filter(name=["pepper", "librita"])
        packages.extend(q)
        self.assertResult(self.base, packages)


class DistroSync(support.ResultTestCase):
    def setUp(self):
        self._base = support.BaseCliStub()
        self._base._sack = support.mock_sack('main', 'updates')
        self._base._goal = dnf.goal.Goal(self._base.sack)
        self._base.logger = mock.create_autospec(self._base.logger)

    def test_distro_sync(self):
        installed = self._get_installed(self._base)
        original_pkg = list(filter(lambda p: p.name == "hole", installed))
        self._base.distro_sync_userlist(('bla', 'hole'))
        obsolete_pkg = list(filter(lambda p: p.name == "tour", installed))

        # check from log package name that is not installed
        self.assertEqual(self._base.logger.mock_calls[1][1][1], 'bla')

        installed2 = self._get_installed(self._base)
        updated_pkg = list(filter(lambda p: p.name == "hole", installed2))
        self.assertLength(updated_pkg, 1)
        self.assertLength(original_pkg, 1)
        self.assertLength(updated_pkg, 1)

        # holy pkg upgraded from version 1 to 2 and obsoletes tour
        self.assertEqual(original_pkg[0].version, "1")
        self.assertEqual(updated_pkg[0].version, "2")
        installed.remove(original_pkg[0])
        installed.remove(obsolete_pkg[0])
        installed2.remove(updated_pkg[0])
        self.assertEqual(installed, installed2)

########NEW FILE########
__FILENAME__ = test_downgrade
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf
import dnf.goal
import rpm

class DowngradeTest(support.ResultTestCase):

    @mock.patch('dnf.rpmUtils.transaction.TransactionWrapper')
    def test_package_downgrade(self, ts):
        base = support.MockBase()

        pkg = base.add_remote_rpm(support.TOUR_44_PKG_PATH)
        cnt = base.package_downgrade(pkg)
        base.ts.setProbFilter.assert_called_with(
            rpm.RPMPROB_FILTER_OLDPACKAGE)
        self.assertGreater(cnt, 0)
        (installed, removed) = self.installed_removed(base)
        self.assertItemsEqual(map(str, installed), ("tour-4-4.noarch", ))
        self.assertItemsEqual(map(str, removed), ("tour-5-0.noarch", ))

    def test_downgrade(self):
        base = support.MockBase("main")
        sack = base.sack
        cnt = base.downgrade("tour")
        self.assertGreater(cnt, 0)

        new_pkg = sack.query().available().filter(name="tour")[0]
        self.assertEqual(new_pkg.evr, "4.6-1")
        new_set = support.installed_but(sack, "tour") + [new_pkg]
        self.assertResult(base, new_set)

    def test_downgrade2(self):
        b = support.MockBase("old_versions")
        ret = b.downgrade("tour")
        installed, removed = self.installed_removed(b)
        self.assertItemsEqual(map(str, installed), ['tour-4.9-1.noarch'])
        self.assertItemsEqual(map(str, removed), ['tour-5-0.noarch'])

class DowngradeTest2(support.TestCase):

    def setUp(self):
        self._base = dnf.Base()
        self._base._sack = support.mock_sack('main')
        self._base._goal = self._goal = mock.create_autospec(dnf.goal.Goal)

    def test_downgrade_pkgnevra(self):
        """ Downgrade should handle full NEVRAs. """
        pkg = support.ObjectMatcher(dnf.package.Package, {'name': 'tour'})

        downgraded_count = self._base.downgrade('tour-0:5-0.noarch')

        self.assertEqual(self._goal.mock_calls, [mock.call.install(pkg)])
        self.assertEqual(downgraded_count, 1)

    def test_downgrade_notinstalled(self):
        pkg = support.ObjectMatcher(dnf.package.Package, {'name': 'lotus'})

        with self.assertRaises(dnf.exceptions.PackagesNotInstalledError) as context:
            self._base.downgrade('lotus')
        self.assertEqual(context.exception.pkg_spec, 'lotus')
        self.assertEqual(tuple(context.exception.packages), (pkg,) * 2)
        self.assertEqual(self._goal.mock_calls, [])

    def test_downgrade_notfound(self):
        with self.assertRaises(dnf.exceptions.PackageNotFoundError) as context:
            self._base.downgrade('non-existent')
        self.assertEqual(context.exception.pkg_spec, 'non-existent')
        self.assertEqual(self._goal.mock_calls, [])

    def test_downgrade_nodowngrade(self):
        downgraded_count = self._base.downgrade('pepper')

        self.assertEqual(self._goal.mock_calls, [])
        self.assertEqual(downgraded_count, 0)

########NEW FILE########
__FILENAME__ = test_drpm
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
from tests import support, mock
from dnf.yum.misc import unlink_f
from dnf.util import Bunch

import dnf.drpm
import dnf.exceptions
import hawkey

PACKAGE = 'tour-5-1.noarch'

class DrpmTest(support.TestCase):
    def __init__(self, *args):
        support.TestCase.__init__(self, *args)
        self.base = support.MockBase()
        self.sack = self.base.sack

        # load the testing repo
        repo = support.MockRepo('drpm', '/tmp/dnf-cache')
        self.base.repos[repo.id] = repo
        repo.baseurl = ['file://%s/%s' % (support.repo_dir(), repo.id)]
        repo.load()

        # add it to sack
        hrepo = hawkey.Repo(repo.id)
        hrepo.repomd_fn = repo.repomd_fn
        hrepo.primary_fn = repo.primary_fn
        hrepo.filelists_fn = repo.filelists_fn
        hrepo.presto_fn = repo.presto_fn
        self.sack.load_yum_repo(hrepo, load_filelists=True, load_presto=True)

    def setUp(self):
        # find the newest 'tour' package available
        self.pkg = max(self.base.sack.query().available().filter(name='tour'))
        self.assertEqual(str(self.pkg), PACKAGE)

        # pretend it's remote and not cached
        self.pkg.repo.__class__.local = False
        self.pkg.localPkg = lambda: '/tmp/%s.rpm' % PACKAGE
        unlink_f(self.pkg.localPkg())

    def tearDown(self):
        # don't break other tests
        del self.pkg.repo.__class__.local

    def test_delta(self):
        # there should be a delta from 5-0 to 5-1
        self.assertTrue(self.pkg.get_delta_from_evr('5-0'))

    def download(self, errors=None, err={}):
        # utility function, calls Base.download_packages()
        # and returns the list of relative URLs it used.
        urls = []

        def dlp(targets, failfast):
            target, = targets
            self.assertEqual(target.__class__.__name__, 'PackageTarget')
            self.assertTrue(failfast)
            urls.append(target.relative_url)
            err = errors and errors.pop(0)
            if err:
                # PackageTarget.err is not writable
                targets[0] = Bunch(cbdata=target.cbdata, err=err)

        with mock.patch('librepo.download_packages', dlp):
            try:
                self.base.download_packages([self.pkg])
            except dnf.exceptions.DownloadError as e:
                pass
        return urls

    def test_simple_download(self):
        self.assertEquals(self.download(), [PACKAGE +'.rpm'])

    def test_drpm_download(self):
        # the testing drpm is about 150% of the target..
        self.pkg.repo.deltarpm = 1
        dnf.drpm.APPLYDELTA = '/bin/true'
        with mock.patch('dnf.drpm.MAX_PERCENTAGE', 50):
            self.assertEquals(self.download(), ['tour-5-1.noarch.rpm'])
        with mock.patch('dnf.drpm.MAX_PERCENTAGE', 200):
            self.assertEquals(self.download(), ['drpms/tour-5-1.noarch.drpm'])

########NEW FILE########
__FILENAME__ = test_exceptions
# test_exceptions.py
#
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals

import dnf.exceptions
import tests.support

JAY_ERR = """jay-3.x86_64: Can not download."""

class DownloadErrorTest(tests.support.TestCase):

    def test_str(self):
        exc = dnf.exceptions.DownloadError(errmap={
            'jay-3.x86_64'  : ['Can not download.']})
        self.assertEqual(str(exc), JAY_ERR)

        exc = dnf.exceptions.DownloadError(errmap={'' : ['Epic fatal.']})
        self.assertEqual(str(exc), 'Epic fatal.')

########NEW FILE########
__FILENAME__ = test_goal
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
import dnf.goal
import dnf.selector
import tests.support


class GoalTest(tests.support.TestCase):
    def setUp(self):
        base = tests.support.MockBase('main')
        self.sack = base.sack

    def test_get_reason(self):
        sltr = dnf.selector.Selector(self.sack)
        sltr.set(name='mrkite')
        grp_sltr = dnf.selector.Selector(self.sack)
        grp_sltr.set(name='lotus')

        goal = dnf.goal.Goal(self.sack)
        goal.install(select=sltr)
        goal.install(select=grp_sltr)
        goal.group_members.add('lotus')
        goal.run()
        installs = goal.list_installs()
        mrkite = [pkg for pkg in installs if pkg.name == 'mrkite'][0]
        lotus = [pkg for pkg in installs if pkg.name == 'lotus'][0]
        trampoline = [pkg for pkg in installs if pkg.name == 'trampoline'][0]
        self.assertEqual(goal.get_reason(lotus), 'group')
        self.assertEqual(goal.get_reason(mrkite), 'user')
        self.assertEqual(goal.get_reason(trampoline), 'dep')

########NEW FILE########
__FILENAME__ = test_groups
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support

import dnf.comps
import dnf.util
import operator
import warnings

class EmptyPersistorTest(support.ResultTestCase):
    """Test group operations with empty persistor."""

    def setUp(self):
        self.base = support.MockBase('main')
        self.base.read_mock_comps(False)
        self.base.init_sack()

    def test_group_install_exclude(self):
        comps = self.base.comps
        grp = comps.group_by_pattern('somerset')
        cnt = self.base.group_install(grp, ('optional',), exclude=('lotus',))
        self.assertEqual(cnt, 0)

    def test_add_comps_trans(self):
        trans = dnf.comps.TransactionBunch()
        trans.install.add('trampoline')
        self.assertGreater(self.base._add_comps_trans(trans), 0)
        self.assertIn('trampoline', self.base._goal.group_members)
        (installed, removed) = self.installed_removed(self.base)
        self.assertItemsEqual(map(str, installed), ('trampoline-2.1-1.noarch',))
        self.assertEmpty(removed)

        trans = dnf.comps.TransactionBunch()
        trans.install.add('waltz')
        self.assertEqual(self.base._add_comps_trans(trans), 0)


class PresetPersistorTest(support.ResultTestCase):
    """Test group operations with some data in the persistor."""

    def setUp(self):
        self.base = support.MockBase("main")
        self.base.read_mock_comps()
        self.base.init_sack()

    def test_environment_remove(self):
        comps = self.base.comps
        env = comps.environment_by_pattern("sugar-desktop-environment")
        self.assertGreater(self.base.environment_remove(env), 0)
        prst = self.base.group_persistor
        p_env = prst.environment(env.id)
        self.assertFalse(p_env.installed)
        peppers = prst.group('Peppers')
        somerset = prst.group('somerset')
        self.assertFalse(peppers.installed)
        self.assertFalse(somerset.installed)

    def test_install(self):
        comps = self.base.comps
        grp = dnf.util.first(comps.groups_by_pattern("Solid Ground"))
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            self.assertEqual(self.base.select_group(grp), 1)
        inst, removed = self.installed_removed(self.base)
        self.assertItemsEqual([pkg.name for pkg in inst], ("trampoline",))
        self.assertLength(removed, 0)

    def test_group_install(self):
        prst = self.base.group_persistor
        grp = self.base.comps.group_by_pattern('Base')
        p_grp = prst.group('base')
        self.assertFalse(p_grp.installed)

        self.assertEqual(self.base.group_install(grp, ('mandatory',)), 2)
        inst, removed = self.installed_removed(self.base)
        self.assertEmpty(inst)
        self.assertEmpty(removed)
        self.assertTrue(p_grp.installed)

    def test_group_remove(self):
        prst = self.base.group_persistor
        grp = self.base.comps.group_by_pattern('somerset')
        p_grp = prst.group('somerset')

        self.assertGreater(self.base.group_remove(grp), 0)
        inst, removed = self.installed_removed(self.base)
        self.assertEmpty(inst)
        self.assertItemsEqual([pkg.name for pkg in removed], ('pepper',))
        self.assertFalse(p_grp.installed)


class EnvironmentInstallTest(support.ResultTestCase):
    def setUp(self):
        """Set up a test where sugar is considered not installed."""
        self.base = support.MockBase("main")
        self.base.init_sack()
        self.base.read_mock_comps()
        self.prst = self.base.group_persistor
        p_env = self.prst.environment('sugar-desktop-environment')
        p_env.pkg_types = 0
        p_env.grp_types = 0
        del p_env.full_list[:]
        p_grp = self.prst.group('somerset')
        p_grp.pkg_types = 0
        del p_grp.full_list[:]

    def test_environment_install(self):
        comps = self.base.comps
        env = comps.environment_by_pattern("sugar-desktop-environment")
        self.base.environment_install(env, ('mandatory',))
        installed, _ = self.installed_removed(self.base)
        self.assertItemsEqual(map(operator.attrgetter('name'), installed),
                              ('trampoline',))

        p_env = self.prst.environment('sugar-desktop-environment')
        self.assertItemsEqual(p_env.full_list, ('somerset', 'Peppers'))
        self.assertTrue(p_env.installed)

        peppers = self.prst.group('Peppers')
        somerset = self.prst.group('somerset')
        self.assertTrue(all((peppers.installed, somerset.installed)))

########NEW FILE########
__FILENAME__ = test_history
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import PycompTestCase
from tests.support import mock

import dnf.history
import dnf.yum.history

class TestedHistory(dnf.yum.history.YumHistory):
    @mock.patch("os.path.exists", return_value=True)
    def __init__(self, unused_exists):
        self._db_date = "1962-07-12"
        super(TestedHistory, self).__init__(support.NONEXISTENT_FILE, mock.Mock())

    def _create_db_file(self):
        return None

class History(PycompTestCase):
    def setUp(self):
        self.base = support.MockBase("main")
        self.sack = self.base.sack
        self.history = TestedHistory()

    def pkgtup2pid_test(self):
        """ Check pkg2pid() correctly delegates to _*2pid()s. """
        hpkg = dnf.yum.history.YumHistoryPackage("n", "a", "e", "v", "r")
        with mock.patch.object(self.history, "_hpkg2pid") as hpkg2pid:
            self.history.pkg2pid(hpkg)
            hpkg2pid.assert_called_with(hpkg, True)

        ipkg = self.sack.query().installed().filter(name="pepper")[0]
        with mock.patch.object(self.history, "_ipkg2pid") as ipkg2pid:
            self.history.pkg2pid(ipkg)
            ipkg2pid.assert_called_with(ipkg, True)

        apkg = self.sack.query().available().filter(name="lotus")[0]
        with mock.patch.object(self.history, "_apkg2pid") as apkg2pid:
            self.history.pkg2pid(apkg)
            apkg2pid.assert_called_with(apkg, True)

class HistoryWrapperTest(support.TestCase):
    """Unit tests of dnf.history._HistoryWrapper."""

    def _create_wrapper(self, yum_history):
        """Create new instance of _HistoryWrapper."""
        wrapper = dnf.history.open_history(yum_history)
        assert isinstance(wrapper, dnf.history._HistoryWrapper)
        return wrapper

    def test_context_manager(self):
        """Test whether _HistoryWrapper can be used as a context manager."""
        yum_history = mock.create_autospec(dnf.yum.history.YumHistory)
        history = self._create_wrapper(yum_history)

        with history as instance:
            pass

        self.assertIs(instance, history)
        self.assertEqual(yum_history.close.mock_calls, [mock.call()])

    def test_close(self):
        """Test close."""
        yum_history = mock.create_autospec(dnf.yum.history.YumHistory)
        history = self._create_wrapper(yum_history)

        history.close()

        self.assertEqual(yum_history.close.mock_calls, [mock.call()])

    def test_has_transaction_absent(self):
        """Test has_transaction without any transaction."""
        with self._create_wrapper(support.HistoryStub()) as history:
            present = history.has_transaction(1)

        self.assertFalse(present)

    def test_has_transaction_present(self):
        """Test has_transaction with a transaction present."""
        yum_history = support.HistoryStub()
        yum_history.old_data_pkgs['1'] = (
            dnf.yum.history.YumHistoryPackageState(
                'lotus', 'x86_64', '0', '3', '16', 'Erase',
                history=yum_history),)

        with self._create_wrapper(yum_history) as history:
            present = history.has_transaction(1)

        self.assertTrue(present)

    def test_last_transaction_id(self):
        """Test last_transaction_id with some transactions."""
        yum_history = support.HistoryStub()
        yum_history.old_data_pkgs['1'] = (
            dnf.yum.history.YumHistoryPackageState(
                'lotus', 'x86_64', '0', '3', '16', 'Erase',
                history=yum_history),)
        yum_history.old_data_pkgs['2'] = (
            dnf.yum.history.YumHistoryPackageState(
                'pepper', 'x86_64', '0', '20', '0', 'Install',
                history=yum_history),)

        with self._create_wrapper(yum_history) as history:
            id_ = history.last_transaction_id()

        self.assertEqual(id_, 2)

    def test_last_transaction_id_notransaction(self):
        """Test last_transaction_id without any transaction."""
        with self._create_wrapper(support.HistoryStub()) as history:
            id_ = history.last_transaction_id()

        self.assertIsNone(id_)

    def test_transaction_nevra_ops_notransaction(self):
        """Test transaction_nevra_ops without any transaction."""
        with self._create_wrapper(support.HistoryStub()) as history:
            self.assertRaises(ValueError, history.transaction_nevra_ops, 0)

    def test_transaction_nevra_ops_update(self):
        """Test transaction_nevra_ops with a downgrade operation."""
        yum_history = support.HistoryStub()
        yum_history.old_data_pkgs['1'] = (
            dnf.yum.history.YumHistoryPackageState(
                'tour', 'noarch', '0', '4.8', '1', 'Update',
                history=yum_history),
            dnf.yum.history.YumHistoryPackageState(
                'tour', 'noarch', '0', '4.6', '1', 'Updated',
                history=yum_history),
            dnf.yum.history.YumHistoryPackageState(
                'tour', 'noarch', '0', '4.8', '1', 'Obsoleting',
                history=yum_history),
            dnf.yum.history.YumHistoryPackageState(
                'lotus', 'x86_64', '0', '3', '16', 'Obsoleted',
                history=yum_history))
        expected_ops = dnf.history.NEVRAOperations()
        expected_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch', ('lotus-0:3-16.x86_64',))

        with self._create_wrapper(yum_history) as history:
            result_ops = history.transaction_nevra_ops(1)

        self.assertItemsEqual(result_ops, expected_ops)

class NEVRAOperationsTest(support.TestCase):
    """Unit tests of dnf.history.NEVRAOperations."""

    def test_add_erase_installed(self):
        """Test add with an erasure of NEVRA which was installed before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Install', 'tour-0:4.6-1.noarch', obsoleted_nevras=('lotus-0:3-16.x86_64',))
        ops.add('Erase', 'tour-0:4.6-1.noarch')

        self.assertItemsEqual(
            ops,
            (('Erase', 'lotus-0:3-16.x86_64', None, set()),))

    def test_add_erase_removed(self):
        """Test add with an erasure of NEVRA which was removed before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Erase', 'tour-0:4.6-1.noarch')

        self.assertRaises(
            ValueError,
            ops.add, 'Erase', 'tour-0:4.6-1.noarch')

    def test_add_install_installed(self):
        """Test add with two installs of the same NEVRA."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Install', 'tour-0:4.6-1.noarch')

        self.assertRaises(
            ValueError,
            ops.add, 'Install', 'tour-0:4.6-1.noarch')

    def test_add_install_removed(self):
        """Test add with an install of NEVRA which was removed before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Erase', 'tour-0:4.6-1.noarch')
        ops.add('Install', 'tour-0:4.6-1.noarch')

        self.assertItemsEqual(
            ops,
            (('Reinstall', 'tour-0:4.6-1.noarch', 'tour-0:4.6-1.noarch', set()),))

    def test_add_obsoleted_installed(self):
        """Test add with an obsoleted NEVRA which was installed before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Install', 'lotus-0:3-16.x86_64')
        ops.add('Install', 'tour-0:4.6-1.noarch', obsoleted_nevras=('lotus-0:3-16.x86_64',))

        self.assertItemsEqual(
            ops,
            (('Install', 'tour-0:4.6-1.noarch', None, set()),))

    def test_add_obsoleted_obsoleted(self):
        """Test add with an obsoleted NEVRA which was obsoleted before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Install', 'tour-0:4.6-1.noarch', obsoleted_nevras=('lotus-0:3-16.x86_64', 'mrkite-0:2-0.x86_64'))
        ops.add('Install', 'pepper-0:20-0.x86_64', obsoleted_nevras=('lotus-0:3-16.x86_64', 'librita-0:1-1.x86_64'))

        self.assertItemsEqual(
            ops,
            (('Install', 'tour-0:4.6-1.noarch', None, {'lotus-0:3-16.x86_64', 'mrkite-0:2-0.x86_64'}),
             ('Install', 'pepper-0:20-0.x86_64', None, {'lotus-0:3-16.x86_64', 'librita-0:1-1.x86_64'})))

    def test_add_obsoleted_removed(self):
        """Test add with an obsoleted NEVRA which was removed before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Erase', 'lotus-0:3-16.x86_64')

        self.assertRaises(
            ValueError,
            ops.add, 'Install', 'tour-0:4.6-1.noarch', obsoleted_nevras=('lotus-0:3-16.x86_64',))

    def test_add_reinstall_installed(self):
        """Test add with a reinstall of NEVRA which was installed before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Install', 'tour-0:4.6-1.noarch')
        ops.add('Reinstall', 'tour-0:4.6-1.noarch', 'tour-0:4.6-1.noarch')

        self.assertItemsEqual(
            ops,
            (('Install', 'tour-0:4.6-1.noarch', None, set()),))

    def test_add_replace_installed(self):
        """Test add with a replacing NEVRA which was installed before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Install', 'tour-0:4.8-1.noarch')

        self.assertRaises(
            ValueError,
            ops.add, 'Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

    def test_add_replace_opposite(self):
        """Test add with a replacement which was done before, but swapped."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Downgrade', 'tour-0:4.6-1.noarch', 'tour-0:4.8-1.noarch')
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        self.assertItemsEqual(
            ops,
            (('Reinstall', 'tour-0:4.8-1.noarch', 'tour-0:4.8-1.noarch', set()),))

    def test_add_replace_opposite_manual(self):
        """Test add with a manual replacement which was done before, but swapped."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Erase', 'tour-0:4.8-1.noarch')
        ops.add('Install', 'tour-0:4.6-1.noarch')
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        self.assertItemsEqual(
            ops,
            (('Reinstall', 'tour-0:4.8-1.noarch', 'tour-0:4.8-1.noarch', set()),))

    def test_add_replace_removed(self):
        """Test add with a replacing NEVRA which was removed before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Erase', 'tour-0:4.8-1.noarch')
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        self.assertItemsEqual(
            ops,
            (('Reinstall', 'tour-0:4.8-1.noarch', 'tour-0:4.8-1.noarch', set()),
             ('Erase', 'tour-0:4.6-1.noarch', None, set())))

    def test_add_replaced_opposite(self):
        """Test add with a replaced NEVRA which replaced a NEVRA before in the opposite direction."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Downgrade', 'tour-0:4.6-1.noarch', 'tour-0:4.9-1.noarch')
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        self.assertItemsEqual(
            ops,
            (('Erase', 'tour-0:4.9-1.noarch', None, set()),
             ('Install', 'tour-0:4.8-1.noarch', None, set())))

    def test_add_replaced_removed(self):
        """Test add with a replaced NEVRA which was removed before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Erase', 'tour-0:4.6-1.noarch')

        self.assertRaises(
            ValueError,
            ops.add, 'Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

    def test_add_replaced_reinstall(self):
        """Test add with a replaced NEVRA which was reinstalled before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Reinstall', 'tour-0:4.6-1.noarch', 'tour-0:4.6-1.noarch')
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        self.assertItemsEqual(
            ops,
            (('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch', set()),))

    def test_add_replaced_replacement(self):
        """Test add with a replaced NEVRA which replaced a NEVRA before."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')
        ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch')

        self.assertItemsEqual(
            ops,
            (('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.6-1.noarch', set()),))

    def test_addition(self):
        """Test addition of two instances."""
        left_ops = dnf.history.NEVRAOperations()
        left_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')
        right_ops = dnf.history.NEVRAOperations()
        right_ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch')
        expected_ops = dnf.history.NEVRAOperations()
        expected_ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.6-1.noarch')

        result_ops = left_ops + right_ops

        self.assertEqual(result_ops, expected_ops)

    def test_addition_inplace(self):
        """Test in-place addition of two instances."""
        left_ops = dnf.history.NEVRAOperations()
        left_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')
        right_ops = dnf.history.NEVRAOperations()
        right_ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch')
        expected_ops = dnf.history.NEVRAOperations()
        expected_ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.6-1.noarch')

        left_ops += right_ops

        self.assertEqual(left_ops, expected_ops)

    def test_equality(self):
        """Test equality of two equal instances."""
        left_ops = dnf.history.NEVRAOperations()
        left_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')
        right_ops = dnf.history.NEVRAOperations()
        right_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        is_equal = left_ops == right_ops

        self.assertTrue(is_equal)

    def test_equality_differentcontent(self):
        """Test equality of two instances with different contents."""
        left_ops = dnf.history.NEVRAOperations()
        left_ops.add('Downgrade', 'tour-0:4.6-1.noarch', 'tour-0:4.8-1.noarch')
        right_ops = dnf.history.NEVRAOperations()
        right_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        is_equal = left_ops == right_ops

        self.assertFalse(is_equal)

    def test_equality_differentlength(self):
        """Test equality of two instances with different lengths."""
        left_ops = dnf.history.NEVRAOperations()
        right_ops = dnf.history.NEVRAOperations()
        right_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        is_equal = left_ops == right_ops

        self.assertFalse(is_equal)

    def test_equality_differenttype(self):
        """Test equality of an instance and an object of a different type."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        is_equal = ops == 'tour-0:4.8-1.noarch'

        self.assertFalse(is_equal)

    def test_equality_identity(self):
        """Test equality of the same instance."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        is_equal = ops == ops

        self.assertTrue(is_equal)

    def test_inequality(self):
        """Test inequality of two different instances."""
        left_ops = dnf.history.NEVRAOperations()
        left_ops.add('Downgrade', 'tour-0:4.6-1.noarch', 'tour-0:4.8-1.noarch')
        right_ops = dnf.history.NEVRAOperations()
        right_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        is_inequal = left_ops != right_ops

        self.assertTrue(is_inequal)

    def test_inequality_equal(self):
        """Test inequality of two equal instances."""
        left_ops = dnf.history.NEVRAOperations()
        left_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')
        right_ops = dnf.history.NEVRAOperations()
        right_ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        is_inequal = left_ops != right_ops

        self.assertFalse(is_inequal)

    def test_iterator(self):
        """Test iterator of an instance."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        iterator = iter(ops)

        self.assertEqual(
            next(iterator),
            ('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch', set()))
        self.assertRaises(StopIteration, next, iterator)

    def test_length(self):
        """Test length of an instance."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.8-1.noarch', 'tour-0:4.6-1.noarch')

        length = len(ops)

        self.assertEqual(length, 1)

    def test_membership(self):
        """Test membership of a contained operation."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch')

        is_in = ('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch', ()) in ops

        self.assertTrue(is_in)

    def test_membership_differentnevra(self):
        """Test membership of an operation with different (replacing) NEVRA."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch')

        is_in = ('Update', 'pepper-0:20-0.x86_64', 'tour-0:4.8-1.noarch', ()) in ops

        self.assertFalse(is_in)

    def test_membership_differentobsoleted(self):
        """Test membership of an operation with different obsoleted NEVRAs."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch')

        is_in = ('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch', ('pepper-0:20-0.x86_64',)) in ops

        self.assertFalse(is_in)

    def test_membership_differentreplaced(self):
        """Test membership of an operation with different replaced NEVRA."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch')

        is_in = ('Update', 'tour-0:4.9-1.noarch', 'pepper-0:20-0.x86_64', ()) in ops

        self.assertFalse(is_in)

    def test_membership_differentstate(self):
        """Test membership of an operation with different state."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch')

        is_in = ('Downgrade', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch', ()) in ops

        self.assertFalse(is_in)

    def test_membership_differenttype(self):
        """Test membership of an object of a different type."""
        ops = dnf.history.NEVRAOperations()
        ops.add('Update', 'tour-0:4.9-1.noarch', 'tour-0:4.8-1.noarch')

        is_in = 'tour-0:4.9-1.noarch' in ops

        self.assertFalse(is_in)

########NEW FILE########
__FILENAME__ = test_history_undo
# test_history_undo.py
# Tests of the history undo command.
#
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

"""Tests of the history undo command."""

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf import Base
from dnf.exceptions import PackagesNotAvailableError, PackagesNotInstalledError
from dnf.history import NEVRAOperations
from dnf.package import Package
from dnf.transaction import (ERASE, DOWNGRADE, INSTALL, REINSTALL,
                             TransactionItem, UPGRADE)
from hawkey import split_nevra
from tests.support import mock_sack, ObjectMatcher
from unittest import TestCase

class BaseTest(TestCase):
    """Unit tests of dnf.Base."""

    def _create_item_matcher(self, op_type, installed=None, erased=None,
                             obsoleted=[], reason='unknown'):
        """Create a new instance of dnf.transaction.TransactionItem matcher."""
        attrs = {'op_type': op_type,
                 'installed': self._create_package_matcher(installed)
                              if installed else installed,
                 'erased': self._create_package_matcher(erased)
                           if erased else erased,
                 'obsoleted': [self._create_package_matcher(nevra)
                               for nevra in obsoleted],
                 'reason': reason}
        return ObjectMatcher(TransactionItem, attrs)

    def _create_package_matcher(self, nevra_str):
        """Create a new instance of dnf.package.Package matcher."""
        nevra = split_nevra(nevra_str)
        attrs = {'name': nevra.name,
                 'epoch': nevra.epoch,
                 'version': nevra.version,
                 'release': nevra.release,
                 'arch': nevra.arch}
        return ObjectMatcher(Package, attrs)

    def setUp(self):
        """Prepare the test fixture."""
        self._base = Base()
        self._base._sack = mock_sack('main', 'updates')

    def test_history_undo_operations_downgrade(self):
        """Test history_undo_operations with a downgrade."""
        operations = NEVRAOperations()
        operations.add('Downgrade', 'pepper-20-0.x86_64', 'pepper-20-1.x86_64', ('lotus-3-16.x86_64',))

        with self._base:
            self._base.history_undo_operations(operations)

        transaction_it = iter(self._base.transaction)
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                             UPGRADE, installed='pepper-20-1.x86_64',
                             erased='pepper-20-0.x86_64'))
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                            INSTALL, installed='lotus-3-16.x86_64',
                            reason='history'))
        self.assertRaises(StopIteration, next, transaction_it)

    def test_history_undo_operations_downgrade_notavailable(self):
        """Test history_undo_operations with an unavailable downgrade."""
        operations = NEVRAOperations()
        operations.add('Downgrade', 'pepper-20-0.x86_64', 'pepper-20-2.x86_64')

        with self._base, self.assertRaises(PackagesNotAvailableError) as context:
            self._base.history_undo_operations(operations)

        self.assertEqual(context.exception.pkg_spec, 'pepper-20-2.x86_64')

    def test_history_undo_operations_downgrade_notinstalled(self):
        """Test history_undo_operations with a not installed downgrade."""
        operations = NEVRAOperations()
        operations.add('Downgrade', 'lotus-3-0.x86_64', 'lotus-3-16.x86_64')

        with self._base, self.assertRaises(PackagesNotInstalledError) as context:
            self._base.history_undo_operations(operations)

        self.assertEqual(context.exception.pkg_spec, 'lotus-3-0.x86_64')

    def test_history_undo_operations_erase(self):
        """Test history_undo_operations with an erase."""
        operations = NEVRAOperations()
        operations.add('Erase', 'lotus-3-16.x86_64')

        with self._base:
            self._base.history_undo_operations(operations)

        transaction_it = iter(self._base.transaction)
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                             INSTALL, installed='lotus-3-16.x86_64',
                             reason='history'))
        self.assertRaises(StopIteration, next, transaction_it)

    def test_history_undo_operations_erase_twoavailable(self):
        """Test history_undo_operations with an erase available in two repos."""
        base = Base()
        base._sack = mock_sack('main', 'search')
        operations = NEVRAOperations()
        operations.add('Erase', 'lotus-3-16.x86_64')

        with base:
            base.history_undo_operations(operations)

        transaction_it = iter(base.transaction)
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                             INSTALL, installed='lotus-3-16.x86_64',
                             reason='history'))
        self.assertRaises(StopIteration, next, transaction_it)

    def test_history_undo_operations_erase_notavailable(self):
        """Test history_undo_operations with an unavailable erase."""
        operations = NEVRAOperations()
        operations.add('Erase', 'hole-1-1.x86_64')

        with self._base, self.assertRaises(PackagesNotAvailableError) as context:
            self._base.history_undo_operations(operations)

        self.assertEqual(context.exception.pkg_spec, 'hole-1-1.x86_64')

    def test_history_undo_operations_install(self):
        """Test history_undo_operations with an install."""
        operations = NEVRAOperations()
        operations.add('Install', 'pepper-20-0.x86_64', obsoleted_nevras=('lotus-3-16.x86_64',))

        with self._base:
            self._base.history_undo_operations(operations)

        transaction_it = iter(self._base.transaction)
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                             ERASE, erased='pepper-20-0.x86_64'))
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                             INSTALL, installed='lotus-3-16.x86_64',
                             reason='history'))
        self.assertRaises(StopIteration, next, transaction_it)

    def test_history_undo_operations_install_notinstalled(self):
        """Test history_undo_operations with a not installed install."""
        operations = NEVRAOperations()
        operations.add('Install', 'mrkite-2-0.x86_64')

        with self._base, self.assertRaises(PackagesNotInstalledError) as context:
            self._base.history_undo_operations(operations)

        self.assertEqual(context.exception.pkg_spec, 'mrkite-2-0.x86_64')

    def test_history_undo_operations_reinstall(self):
        """Test history_undo_operations with a reinstall."""
        operations = NEVRAOperations()
        operations.add('Reinstall', 'pepper-20-0.x86_64', 'pepper-20-0.x86_64', ('hole-1-1.x86_64',))

        with self._base:
            self._base.history_undo_operations(operations)

        transaction_it = iter(self._base.transaction)
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                             REINSTALL, installed='pepper-20-0.x86_64',
                             erased='pepper-20-0.x86_64',
                             obsoleted=('hole-1-1.x86_64',)))
        self.assertRaises(StopIteration, next, transaction_it)

    def test_history_undo_operations_reinstall_notavailable(self):
        """Test history_undo_operations with an unvailable reinstall."""
        operations = NEVRAOperations()
        operations.add('Reinstall', 'mrkite-2-0.x86_64', 'mrkite-2-0.x86_64')

        with self._base, self.assertRaises(PackagesNotInstalledError) as context:
            self._base.history_undo_operations(operations)

        self.assertEqual(context.exception.pkg_spec, 'mrkite-2-0.x86_64')

    def test_history_undo_operations_reinstall_notinstalled(self):
        """Test history_undo_operations with a not installed reinstall."""
        operations = NEVRAOperations()
        operations.add('Reinstall', 'hole-1-1.x86_64', 'hole-1-1.x86_64')

        with self._base, self.assertRaises(PackagesNotAvailableError) as context:
            self._base.history_undo_operations(operations)

        self.assertEqual(context.exception.pkg_spec, 'hole-1-1.x86_64')

    def test_history_undo_operations_reinstall_notinstalled_obsoleted(self):
        """Test history_undo_operations with a not installed obsoleted of a reinstall."""
        operations = NEVRAOperations()
        operations.add('Reinstall', 'pepper-20-0.x86_64', 'pepper-20-0.x86_64', ('lotus-3-16.x86_64',))

        with self._base:
            self._base.history_undo_operations(operations)

        transaction_it = iter(self._base.transaction)
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                             REINSTALL, installed='pepper-20-0.x86_64',
                             erased='pepper-20-0.x86_64', obsoleted=()))
        self.assertRaises(StopIteration, next, transaction_it)

    def test_history_undo_operations_update(self):
        """Test history_undo_operations with an update."""
        operations = NEVRAOperations()
        operations.add('Update', 'tour-5-0.noarch', 'tour-4.6-1.noarch', ('lotus-3-16.x86_64',))

        with self._base:
            self._base.history_undo_operations(operations)

        transaction_it = iter(self._base.transaction)
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                             DOWNGRADE, installed='tour-4.6-1.noarch',
                             erased='tour-5-0.noarch'))
        self.assertEqual(next(transaction_it),
                         self._create_item_matcher(
                             INSTALL, installed='lotus-3-16.x86_64',
                             reason='history'))
        self.assertRaises(StopIteration, next, transaction_it)

    def test_history_undo_operations_update_notavailable(self):
        """Test history_undo_operations with an unavailable update."""
        operations = NEVRAOperations()
        operations.add('Update', 'tour-5-0.noarch', 'tour-4.6-2.noarch')

        with self._base, self.assertRaises(PackagesNotAvailableError) as context:
            self._base.history_undo_operations(operations)

        self.assertEqual(context.exception.pkg_spec, 'tour-4.6-2.noarch')

    def test_history_undo_operations_update_notinstalled(self):
        """Test history_undo_operations with a not installed update."""
        operations = NEVRAOperations()
        operations.add('Update', 'lotus-4-0.x86_64', 'lotus-3-16.x86_64')

        with self._base, self.assertRaises(PackagesNotInstalledError) as context:
            self._base.history_undo_operations(operations)

        self.assertEqual(context.exception.pkg_spec, 'lotus-4-0.x86_64')

########NEW FILE########
__FILENAME__ = test_i18n
# -*- coding: utf-8 -*-
#
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.pycomp import PY3
from tests.support import PycompTestCase
from tests.support import mock

import unittest
import dnf.i18n
import sys

UC_TEXT          = u'ka' # means 'Width' in Czech
UC_TEXT_OSERROR  = u'Soubor ji existuje' # 'File already exists'
STR_TEXT_OSERROR = 'Soubor ji existuje'

@mock.patch('locale.setlocale')
class TestLocale(PycompTestCase):
    def test_setup_locale(self, mock_setlocale):
        dnf.i18n.setup_locale()
        self.assertTrue(2 <= mock_setlocale.call_count <= 3)

class TestStdout(PycompTestCase):
    def test_setup_stdout(self):
        # No stdout output can be seen when sys.stdout is patched, debug msgs,
        # etc. included.
        with mock.patch('sys.stdout', spec=('write',)):
            retval = dnf.i18n.setup_stdout()
            self.assertFalse(retval)
        with mock.patch('sys.stdout') as mock_stdout:
            mock_stdout.encoding = None
            retval = dnf.i18n.setup_stdout()
            self.assertFalse(retval)
        with mock.patch('sys.stdout') as mock_stdout:
            mock_stdout.encoding = 'UTF-8'
            retval = dnf.i18n.setup_stdout()
            self.assertTrue(retval)
        with mock.patch('sys.stdout') as mock_stdout:
            mock_stdout.encoding = 'ISO-8859-2'
            retval = dnf.i18n.setup_stdout()
            self.assertFalse(retval)

    def test_stream(self):
        fileobj = dnf.pycomp.StringIO()
        stream = dnf.i18n.UnicodeStream(fileobj, "ISO-8859-2")
        stream.write(UC_TEXT)
        output = fileobj.getvalue()
        self.assertEqual(output, u'\u0160\xed\u0159ka' if PY3 else b'\xa9\xed\xf8ka')
        self.assertEqual(len(output), len(UC_TEXT))

class TestInput(PycompTestCase):
    @unittest.skipIf(PY3, "builtin input accepts unicode and bytes")
    def test_assumption(self):
        """ Test that raw_input() always fails on a unicode string with accented
            characters. If this is not the case we might not need i18n.input()
            as a raw_input() wrapper.
         """
        if sys.stdout.isatty():
            # Only works when stdout is a terminal (and not captured in some
            # way, for instance when nosetests is run without the -s switch).
            self.assertRaises(UnicodeEncodeError, raw_input, UC_TEXT)

    @unittest.skipIf(PY3, "in python3 there's no conversion in dnf.i18n.input")
    @mock.patch('sys.stdout')
    @mock.patch('__builtin__.raw_input', lambda x: x)
    def test_input(self, stdout):
        stdout.encoding = None
        s = dnf.i18n.ucd_input(UC_TEXT)
        self.assertEqual(s, UC_TEXT.encode('utf8'))

        stdout.encoding = 'iso-8859-2'
        s = dnf.i18n.ucd_input(UC_TEXT)
        self.assertEqual(s, UC_TEXT.encode('iso-8859-2'))

        self.assertRaises(TypeError, dnf.i18n.ucd_input, b"string")

class TestConversion(PycompTestCase):
    @mock.patch('dnf.i18n._guess_encoding', return_value='utf-8')
    def test_ucd(self, _unused):
        s = UC_TEXT.encode('utf8')
        # the assumption is this string can't be simply converted back to
        # unicode:
        u = dnf.i18n.ucd(s)
        self.assertEqual(u, UC_TEXT)
        # test a sample OSError, typically constructed with an error code and a
        # utf-8 encoded string:
        obj = OSError(17, 'Soubor ji existuje')
        expected = u"[Errno 17] %s" % UC_TEXT_OSERROR
        self.assertEqual(dnf.i18n.ucd(obj), expected)
        # ucd() should return unicode unmodified
        self.assertEqual(dnf.i18n.ucd(expected), expected)

########NEW FILE########
__FILENAME__ = test_install
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
import dnf.exceptions
import itertools

class InstallMultilib(support.ResultTestCase):
    def setUp(self):
        self.base = support.MockBase('main', 'third_party')
        self.base.conf.multilib_policy = "all"

    def test_not_available(self):
        """ Installing a nonexistent package is a void operation. """
        cnt = self.base.install("not-available")
        self.assertEqual(cnt, 0)
        installed_pkgs = self.base.sack.query().installed().run()
        self.assertResult(self.base, installed_pkgs)

    def test_install(self):
        """ Simple install. """
        self.base.install("mrkite")
        available = self.base.sack.query().available()
        expected = available.filter(name=["mrkite", "trampoline"]).run()
        # ensure sanity of the test (otherwise it would pass no matter what):
        self.assertEqual(len(expected), 2)
        new_set = self.base.sack.query().installed() + expected
        self.assertResult(self.base, new_set)

    def test_install_by_provides(self):
        """ Test the package to be installed can be specified by provide. """
        self.base.install("henry(the_horse)")
        self.assertGreater(self.base._goal.req_length(), 0)

    def test_install_by_filename(self):
        self.base.install("/usr/lib64/liblot*")
        inst, _ = self.installed_removed(self.base)
        self.assertItemsEqual(map(str, inst), ['lotus-3-16.x86_64'])

    def test_install_nevra(self):
        self.base.install("lotus-3-16.i686")
        lotus, = dnf.subject.Subject('lotus-3-16.i686').get_best_query(self.base.sack)
        new_set = self.base.sack.query().installed() + [lotus]
        self.assertResult(self.base, new_set)

    def test_install_reponame(self):
        """Test whether packages are filtered by the reponame."""
        result = itertools.chain(
            self.base.sack.query().installed(),
            dnf.subject.Subject('lotus-3-16.i686').get_best_query(self.base.sack),
            dnf.subject.Subject('lotus-3-16.x86_64').get_best_query(self.base.sack))

        self.base.install('lotus', reponame='main')
        self.assertResult(self.base, result)

        assert dnf.subject.Subject('lotus-3-17.i686').get_best_query(self.base.sack), \
               ('the base must contain packages a package in another repo '
                'which matches the pattern but is preferred, otherwise the '
                'test makes no sense')

    @support.mock.patch('dnf.transaction._', dnf.pycomp.NullTranslations().ugettext)
    def test_install_src_fails(self):
        self.base.install("pepper-20-0.src")
        re = 'Will not install a source rpm'
        self.assertRaisesRegexp(dnf.exceptions.Error, re,
                                self.base.resolve)

class MultilibAllMainRepo(support.ResultTestCase):
    def setUp(self):
        self.base = support.MockBase("main")
        self.installed = self.base.sack.query().installed().run()
        self.base.conf.multilib_policy = "all"

    def test_install(self):
        """ Installing a package existing in multiple architectures attempts
            installing all of them.
        """
        cnt = self.base.install("lotus")
        self.assertEqual(cnt, 2)
        q = self.base.sack.query().available().filter(name="lotus")
        new_set = self.installed + q.run()
        self.assertResult(self.base, new_set)

class MultilibBest(support.ResultTestCase):
    def setUp(self):
        self.base = support.MockBase('main', 'third_party')
        self.installed = self.base.sack.query().installed().run()
        self.assertEqual(self.base.conf.multilib_policy, "best")

    def test_non_existent(self):
        """ Installing a nonexistent package is a void operation. """
        with self.assertRaises(dnf.exceptions.MarkingError) as context:
            self.base.install('not-available')
        self.assertEqual(context.exception.pkg_spec, 'not-available')
        installed_pkgs = self.base.sack.query().installed().run()
        self.assertResult(self.base, installed_pkgs)

    def test_not_available(self):
        """ Installing a unavailable package is a void operation. """
        cnt = self.base.install("hole")
        self.assertEqual(cnt, 1)
        installed_pkgs = self.base.sack.query().installed().run()
        self.assertResult(self.base, installed_pkgs)

    def test_install(self):
        """ Installing a package existing in multiple architectures only
            installs the one for our arch.
        """
        cnt = self.base.install("lotus")
        self.assertEqual(cnt, 1)

        new_package, = dnf.subject.Subject('lotus-3-17.x86_64').get_best_query(self.base.sack)
        new_set = self.installed + [new_package]
        self.assertResult(self.base, new_set)

    def test_install_by_provides(self):
        """ Test the package to be installed can be specified by provide. """
        self.base.install("henry(the_horse)")
        self.assertGreater(self.base._goal.req_length(), 0)
        trampoline = self.base.sack.query().available().filter(
            name="trampoline")
        new_set = self.installed + trampoline.run()
        self.assertResult(self.base, new_set)

    def test_install_by_cmp_provides(self):
        """Test the package to be installed can be specified by as provide with
        "<=>".
        """
        self.base.install('splendid > 2.0')
        self.assertGreater(self.base._goal.req_length(), 0)
        trampoline = self.base.sack.query().available().filter(
            name="trampoline")
        new_set = self.installed + trampoline.run()
        self.assertResult(self.base, new_set)

    def test_install_by_filename_glob(self):
        self.base.install("/*/be/there")
        (installed, _) = self.installed_removed(self.base)
        self.assertItemsEqual(map(str, installed), ('trampoline-2.1-1.noarch',))

        self.base.reset(goal=True)
        self.base.install("*/there")
        (installed, _) = self.installed_removed(self.base)
        self.assertItemsEqual(map(str, installed), ('trampoline-2.1-1.noarch',))

    def test_install_glob(self):
        self.base.install("mrkite*")
        q = self.base.sack.query().available().filter(name="mrkite*")
        new_set = self.installed + q.run()
        installed, removed = self.installed_removed(self.base)
        self.assertItemsEqual(map(str, installed),
                              ['mrkite-2-0.x86_64',
                               'mrkite-k-h-1-1.x86_64',
                               'trampoline-2.1-1.noarch'])

    def test_install_glob_arch(self):
        self.base.install("lotus.*6*")
        installed, removed = self.installed_removed(self.base)
        self.assertItemsEqual(map(str, installed),
                              ['lotus-3-17.i686',
                               'lotus-3-17.x86_64'])

    def test_install_reponame(self):
        """Test whether packages are filtered by the reponame."""
        result = itertools.chain(
            self.base.sack.query().installed(),
            dnf.subject.Subject('lotus-3-16.x86_64').get_best_query(self.base.sack))

        self.base.install('lotus', reponame='main')
        self.assertResult(self.base, result)

        assert dnf.subject.Subject('lotus-3-17.i686').get_best_query(self.base.sack), \
               ('the base must contain packages a package in another repo '
                'which matches the pattern but is preferred, otherwise the '
                'test makes no sense')

########NEW FILE########
__FILENAME__ = test_libcomps
# -*- coding: utf-8 -*-
#
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from tests import support

import libcomps

class LibcompsTest(support.TestCase):

    """Sanity tests of the Libcomps library."""

    def test_environment_parse(self):
        xml = """\
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE comps PUBLIC "-//Red Hat, Inc.//DTD Comps info//EN" "comps.dtd">
<comps>
  <group>
   <id>somerset</id>
   <default>true</default>
   <uservisible>true</uservisible>
   <display_order>1024</display_order>
   <name>Solid Ground</name>
   <description>--</description>
    <packagelist>
      <packagereq type="mandatory">pepper</packagereq>
      <packagereq type="mandatory">trampoline</packagereq>
    </packagelist>
  </group>
  <environment>
    <id>minimal</id>
    <name>Min install</name>
    <description>Basic functionality.</description>
    <display_order>5</display_order>
    <grouplist>
      <groupid>somerset</groupid>
    </grouplist>
  </environment>
</comps>
"""
        comps = libcomps.Comps()
        ret = comps.fromxml_str(xml)
        self.assertGreaterEqual(ret, 0)

    def test_segv(self):
        c1 = libcomps.Comps()
        c2 = libcomps.Comps()
        c2.fromxml_f(support.COMPS_PATH)
        c = c1 + c2 # sigsegved here

    def test_segv2(self):
        c1 = libcomps.Comps()
        c1.fromxml_f(support.COMPS_PATH)

        c2 = libcomps.Comps()
        c2.fromxml_f(support.COMPS_PATH)

        c = c1 + c2
        x = c.groups[0].packages[0].name

########NEW FILE########
__FILENAME__ = test_list
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
import itertools
import unittest

class List(support.TestCase):
    def test_doPackageLists_reponame(self):
        """Test whether packages are filtered by the reponame."""
        reponame = 'main'
        base = support.MockBase(reponame)
        lists = base.doPackageLists(reponame=reponame)

        pkgs = itertools.chain.from_iterable(lists.all_lists().values())
        self.assertItemsEqual({pkg.reponame for pkg in pkgs}, {reponame})

        assert len(set(pkg.reponame for pkg in base.sack.query())) > 1, \
               ('the base must contain packages from multiple repos, '
                'otherwise the test makes no sense')

    def test_list_installed(self):
        base = support.MockBase()
        ypl = base.doPackageLists('installed')
        self.assertEqual(len(ypl.installed), support.TOTAL_RPMDB_COUNT)

    def test_list_installed_reponame(self):
        """Test whether only packages installed from the repository are listed."""
        base = support.MockBase()
        expected = base.sack.query().installed().filter(name={'pepper',
                                                              'librita'})
        for pkg in expected:
            base.yumdb.db[str(pkg)] = {'from_repo': 'main'}

        lists = base.doPackageLists('installed', reponame='main')

        self.assertItemsEqual(lists.installed, expected)

    def test_list_updates(self):
        base = support.MockBase("updates", "main")
        ypl = base.doPackageLists('upgrades')
        self.assertEqual(len(ypl.updates), support.UPDATES_NSOLVABLES - 1)
        pkg = ypl.updates[0]
        self.assertEqual(pkg.name, "hole")
        ypl = base.doPackageLists('upgrades', ["pepper"])
        self.assertEqual(len(ypl.updates), 1)
        ypl = base.doPackageLists('upgrades', ["mrkite"])
        self.assertEqual(len(ypl.updates), 0)

        ypl = base.doPackageLists('upgrades', ["hole"])
        self.assertEqual(len(ypl.updates), 2)

    def test_lists_multiple(self):
        base = support.MockBase('updates', "main")
        ypl = base.doPackageLists('upgrades', ['pepper', 'hole'])
        self.assertLength(ypl.updates, 3)

class TestListAllRepos(support.TestCase):
    def setUp(self):
        self.base = support.MockBase("main", "updates")
        self.base.conf.multilib_policy = "all"

    def test_list_pattern(self):
        ypl = self.base.doPackageLists('all', ['hole'])
        self.assertLength(ypl.installed, 1)
        self.assertLength(ypl.available, 2)

    def test_list_pattern_arch(self):
        ypl = self.base.doPackageLists('all', ['hole.x86_64'])
        self.assertLength(ypl.installed, 1)
        self.assertLength(ypl.available, 1)

    def test_list_pattern_nevra(self):
        ypl = self.base.doPackageLists('all', ['*u*6*'])
        self.assertLength(ypl.available, 3)

    def test_list_available(self):
        ypl = self.base.doPackageLists('available', ['hole'], showdups=False)
        self.assertItemsEqual(map(str, ypl.available), ('hole-2-1.i686',
                                                        'hole-2-1.x86_64'))

        ypl = self.base.doPackageLists('available', ['hole'], showdups=True)
        self.assertItemsEqual(map(str, ypl.available), ('hole-2-1.i686',
                                                        'hole-2-1.x86_64',
                                                        'hole-1-2.x86_64'))

########NEW FILE########
__FILENAME__ = test_lock
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

"""Unit test dnf.lock module.

Locking is very hard to cover reasonably with a unit test, this is more or less
just a sanity check.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.exceptions import ProcessLockError, ThreadLockError
from tests.support import mock

import dnf.lock
import dnf.util
import multiprocessing
try:
    import queue
except ImportError:
    import Queue as queue
import tests.support
import threading

class ConcurrencyMixin(object):
    def __init__(self, lock):
        self.lock = lock

    def run(self):
        try:
            with self.lock:
                pass
        except (ProcessLockError, ThreadLockError) as e:
            self.queue.put(e)

class OtherThread(ConcurrencyMixin, threading.Thread):
    def __init__(self, lock):
        ConcurrencyMixin.__init__(self, lock)
        threading.Thread.__init__(self)
        self.queue = queue.Queue(1)

class OtherProcess(ConcurrencyMixin, multiprocessing.Process):
    def __init__(self, lock):
        ConcurrencyMixin.__init__(self, lock)
        multiprocessing.Process.__init__(self)
        self.queue = multiprocessing.Queue(1)

@mock.patch('dnf.const.USER_RUNDIR', tests.support.USER_RUNDIR)
class ProcessLockTest(tests.support.TestCase):
    @classmethod
    def tearDownClass(cls):
        dnf.util.rm_rf(tests.support.USER_RUNDIR)

    def test_simple(self):
        l1 = dnf.lock.ProcessLock("unit-test")
        target = l1._target
        with l1:
            self.assertFile(target)
        self.assertPathDoesNotExist(target)

    def test_reentrance(self):
        l1 = dnf.lock.ProcessLock("unit-test")
        with l1:
            with l1:
                pass

    def test_another_process(self):
        l1 = dnf.lock.ProcessLock("unit-test")
        process = OtherProcess(l1)
        with l1:
            process.start()
            process.join()
        self.assertIsInstance(process.queue.get(), ProcessLockError)

    def test_another_thread(self):
        l1 = dnf.lock.ProcessLock("unit-test")
        thread = OtherThread(l1)
        with l1:
            thread.start()
            thread.join()
        self.assertIsInstance(thread.queue.get(), ThreadLockError)

    def test_decorator(self):
        l1 = dnf.lock.ProcessLock("unit-test")

        @l1.decorator
        def decorated():
            self.assertEqual(l1.count, 1)

        decorated()

########NEW FILE########
__FILENAME__ = test_logging
# Copyright (C) 2013-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf.const
import dnf.logging
import logging
import collections
import operator
import os
import tempfile

LogfileEntry = collections.namedtuple('LogfileEntry', ('date', 'time', 'message'))

def _split_logfile_entry(entry):
    record = entry.split(' ')
    # strip the trailing '\n' from the message:
    message = ' '.join(record[4:])[:-1]
    return LogfileEntry(date=' '.join(record[0:2]),
                        time=record[2], message=message)

def drop_all_handlers():
    for logger_name in ('dnf', 'dnf.rpm'):
        logger = logging.getLogger(logger_name)
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)

class TestLogging(support.TestCase):
    """Tests the logging mechanisms in DNF.

    If it causes a problem in the future that loggers are singletons that don't
    get torn down between tests, look at logging.Manager internals.

    """

    def setUp(self):
        self.logdir = tempfile.mkdtemp(prefix="dnf-logtest-")
        self.logging = dnf.logging.Logging()

    def tearDown(self):
        drop_all_handlers()
        dnf.util.rm_rf(self.logdir)

    @staticmethod
    def _bench(logger):
        logger.debug(u"d")
        logger.info(u"i")
        logger.warning(u"w")
        logger.error(u"e")

    def test_level_conversion(self):
        self.assertRaises(AssertionError, dnf.logging._cfg_verbose_val2level, 11)
        self.assertEqual(dnf.logging._cfg_verbose_val2level(0),
                         dnf.logging.SUPERCRITICAL)
        self.assertEqual(dnf.logging._cfg_verbose_val2level(7),
                         dnf.logging.SUBDEBUG)

    def test_setup(self):
        logger = logging.getLogger("dnf")
        with support.patch_std_streams() as (stdout, stderr):
            self.logging.setup(logging.INFO, logging.ERROR, self.logdir)
            self._bench(logger)
        self.assertEqual("i\n", stdout.getvalue())
        self.assertEqual("e\n", stderr.getvalue())

    def test_setup_verbose(self):
        logger = logging.getLogger("dnf")
        with support.patch_std_streams() as (stdout, stderr):
            self.logging.setup(logging.DEBUG, logging.WARNING, self.logdir)
            self._bench(logger)
        self.assertEqual("d\ni\n", stdout.getvalue())
        self.assertEqual("w\ne\n", stderr.getvalue())

    @mock.patch('dnf.logging.Logging.setup')
    def test_setup_from_dnf_conf(self, setup_m):
        conf = mock.Mock(debuglevel=2, errorlevel=2, logdir=self.logdir)
        self.logging.setup_from_dnf_conf(conf)
        self.assertEqual(setup_m.call_args, mock.call(dnf.logging.INFO,
                                                      dnf.logging.WARNING,
                                                      self.logdir))
        conf = mock.Mock(debuglevel=6, errorlevel=6, logdir=self.logdir)
        self.logging.setup_from_dnf_conf(conf)
        self.assertEqual(setup_m.call_args, mock.call(dnf.logging.DEBUG,
                                                      dnf.logging.WARNING,
                                                      self.logdir))

    def test_file_logging(self):
        # log nothing to the console:
        self.logging.setup(dnf.logging.SUPERCRITICAL, dnf.logging.SUPERCRITICAL,
                          self.logdir)
        logger = logging.getLogger("dnf")
        with support.patch_std_streams() as (stdout, stderr):
            logger.info("i")
            logger.critical("c")
        self.assertEqual(stdout.getvalue(), '')
        self.assertEqual(stderr.getvalue(), '')
        # yet the file should contain both the entries:
        logfile = os.path.join(self.logdir, "dnf.log")
        self.assertFile(logfile)
        with open(logfile) as f:
            msgs =  map(operator.attrgetter("message"),
                        map(_split_logfile_entry, f.readlines()))
        self.assertSequenceEqual(list(msgs), [dnf.const.LOG_MARKER, 'i', 'c'])

    def test_rpm_logging(self):
        # log everything to the console:
        self.logging.setup(dnf.logging.SUBDEBUG, dnf.logging.SUBDEBUG,
                          self.logdir)
        logger = logging.getLogger("dnf.rpm")
        with support.patch_std_streams() as (stdout, stderr):
            logger.info('rpm transaction happens.')
        # rpm logger never outputs to the console:
        self.assertEqual(stdout.getvalue(), "")
        self.assertEqual(stderr.getvalue(), "")
        logfile = os.path.join(self.logdir, "dnf.rpm.log")
        self.assertFile(logfile)
        with open(logfile) as f:
            msgs =  map(operator.attrgetter("message"),
                        map(_split_logfile_entry, f.readlines()))
        self.assertSequenceEqual(list(msgs), [dnf.const.LOG_MARKER,
                                        'rpm transaction happens.'])

    def test_setup_only_once(self):
        logger = logging.getLogger("dnf")
        self.assertLength(logger.handlers, 0)
        self.logging.setup(dnf.logging.SUBDEBUG, dnf.logging.SUBDEBUG,
                           self.logdir)
        cnt = len(logger.handlers)
        self.assertGreater(cnt, 0)
        self.logging.setup(dnf.logging.SUBDEBUG, dnf.logging.SUBDEBUG,
                           self.logdir)
        # no new handlers
        self.assertEqual(cnt, len(logger.handlers))

########NEW FILE########
__FILENAME__ = test_main
# Copyright (C) 2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

"""Tests of the CLI entry point."""

from __future__ import unicode_literals
import dnf.cli.main
import dnf.logging
import dnf.pycomp
import tests.support

class MainTest(tests.support.TestCase):
    """Tests the ``dnf.cli.main`` module."""

    def test_ex_IOError_logs_traceback(self):
        """Test whether the traceback is logged if an error is raised."""

        lvl = dnf.logging.SUBDEBUG
        out = dnf.pycomp.StringIO()

        with tests.support.wiretap_logs('dnf', lvl, out):
            try:
                raise OSError('test_ex_IOError_logs_traceback')
            except OSError as e:
                dnf.cli.main.ex_IOError(e)
        self.assertTracebackIn('OSError: test_ex_IOError_logs_traceback\n',
                               out.getvalue())

########NEW FILE########
__FILENAME__ = test_match_counter
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf.match_counter

class PackageStub(support.MockPackage):
    @classmethod
    def several(cls, count):
        for _ in range(count):
            yield cls()

    def __init__(self, nevra='nevra-1-1.noarch', summary='summary'):
        super(PackageStub, self).__init__(nevra)
        self.summary = summary
        self.url = ''
        self.description = ''


class MatchCounterTest(support.TestCase):
    def test_canonize_string_set(self):
        a = ['f', 'p']
        b = ['p']
        self.assertLess(dnf.match_counter._canonize_string_set(b, 2),
                        dnf.match_counter._canonize_string_set(a, 2))

    def test_matched(self):
        pkg = support.MockPackage("humbert-1-1.noarch")
        pkg.url = url = "http://humbert.com"
        pkg.summary = summary = "Glimpses of an incomparably more poignant bliss."
        counter = dnf.match_counter.MatchCounter()
        counter.add(pkg, 'summary', 'poignant')
        counter.add(pkg, 'url', 'humbert')
        counter.add(pkg, 'summary', 'humbert')
        self.assertItemsEqual(counter.matched_needles(pkg),
                              ['humbert', 'poignant'])
        self.assertItemsEqual(counter.matched_keys(pkg), ['url', 'summary'])
        self.assertItemsEqual(counter.matched_haystacks(pkg), [url, summary])

    @mock.patch('dnf.match_counter.MatchCounter._eval_distance', return_value=0)
    def test_sorted(self, _):
        counter = dnf.match_counter.MatchCounter()
        self.assertEqual(counter.sorted(), [])

        counter = dnf.match_counter.MatchCounter()
        pkg1, pkg2, pkg3 = PackageStub().several(3)
        counter.add(pkg1, 'name', '')
        counter.add(pkg2, 'summary', '')
        self.assertEqual(counter.sorted(), [pkg2, pkg1])

        counter.add(pkg3, 'url', '')
        self.assertEqual(counter.sorted(), [pkg3, pkg2, pkg1])
        self.assertEqual(counter.sorted(reverse=True), [pkg1, pkg2, pkg3])

    @mock.patch('dnf.match_counter.MatchCounter._eval_distance', return_value=0)
    def test_sorted_with_needles(self, _):
        # the same needles should be listed together:
        counter = dnf.match_counter.MatchCounter()
        pkg1, pkg2, pkg3, pkg4 = PackageStub().several(4)
        counter.add(pkg1, 'summary', 'grin')
        counter.add(pkg2, 'summary', 'foolish')
        counter.add(pkg3, 'summary', 'grin')
        counter.add(pkg4, 'summary', 'grin')

        srt = counter.sorted()
        self.assertIn(srt.index(pkg2), (0, 3))

        # more unique needles is more than less unique needles:
        counter = dnf.match_counter.MatchCounter()
        counter.add(pkg1, 'summary', 'a')
        counter.add(pkg1, 'summary', 'b')
        counter.add(pkg2, 'summary', 'b')
        counter.add(pkg2, 'summary', 'b')

        self.assertSequenceEqual(counter.sorted(), (pkg2, pkg1))

    @mock.patch('dnf.match_counter.MatchCounter._eval_distance', return_value=0)
    def test_sorted_limit(self, _):
        counter = dnf.match_counter.MatchCounter()
        pkg1, pkg2, pkg3 = PackageStub().several(3)
        counter.add(pkg1, 'name', '')
        counter.add(pkg2, 'url', '')
        counter.add(pkg3, 'description', '')
        self.assertSequenceEqual(counter.sorted(limit_to=[pkg1, pkg2]),
                                 (pkg2, pkg1))

    @mock.patch('dnf.match_counter.MatchCounter._eval_distance', return_value=0)
    def test_sorted_exact_match(self, _):
        """Exactly matching the name beats name and summary non-exact match."""
        counter = dnf.match_counter.MatchCounter()
        pkg1 = PackageStub('wednesday-1-1.noarch', 'morning')
        pkg2 = PackageStub('wednesdaymorning-1-1.noarch', "5 o'clock")
        counter.add(pkg1, 'name', 'wednesday')
        counter.add(pkg2, 'name', 'wednesday')
        counter.add(pkg2, 'summary', 'clock')
        self.assertSequenceEqual(counter.sorted(), (pkg2, pkg1))

    def test_total(self):
        counter = dnf.match_counter.MatchCounter()
        counter.add(3, 'summary', 'humbert')
        counter.add(3, 'url', 'humbert')
        counter.add(20, 'summary', 'humbert')
        self.assertEqual(len(counter), 2)
        self.assertEqual(counter.total(), 3)

    def test_distance(self):
        pkg2 = support.MockPackage('rust-and-stardust-1-2.x86_64')
        pkg1 = support.MockPackage('rust-1-3.x86_64')
        counter = dnf.match_counter.MatchCounter()
        counter.add(pkg1, 'name', 'rust')
        counter.add(pkg2, 'name', 'rust')
        # 'rust-and-stardust' is a worse match for 'rust' than 'rust' itself
        self.assertSequenceEqual([x.name for x in counter.sorted()],
                                 ['rust-and-stardust', 'rust'])

########NEW FILE########
__FILENAME__ = test_misc
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
import dnf.yum.misc
from tests.support import PycompTestCase

class TestGenericHolder(PycompTestCase):
    def test_merge_lists(self):
        gh = dnf.yum.misc.GenericHolder()
        gh2 = dnf.yum.misc.GenericHolder()
        gh.l = ["lucy", "in", "the"]
        gh2.l = ["sky"]
        gh2.l2 = ["with", "diamonds"]
        gh.merge_lists(gh2)
        self.assertEqual(gh.l, ["lucy", "in", "the", "sky"])
        self.assertEqual(gh.l2, ["with", "diamonds"])

########NEW FILE########
__FILENAME__ = test_output
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf.cli.output
import dnf.transaction
import io
import unittest

INFOOUTPUT_OUTPUT="""\
Name        : tour
Arch        : noarch
Epoch       : 0
Version     : 5
Release     : 0
Size        : 0.0  
Repo        : None
Summary     : A summary of the package.
URL         : http://example.com
License     : GPL+
Description : 

"""

LIST_TRANSACTION_OUTPUT=u"""
================================================================================
 Package           Arch              Version           Repository          Size
================================================================================
Upgrading:
 pepper            x86_64            20-1              updates              0  
     replacing  hole.x86_64 1-1

Transaction Summary
================================================================================
Upgrade  1 Package
"""

class OutputTest(support.TestCase):
    @staticmethod
    def _keyboard_interrupt(*ignored):
        raise KeyboardInterrupt

    @staticmethod
    def _eof_error(*ignored):
        raise EOFError

    def setUp(self):
        self.base = support.MockBase('updates')
        self.output = dnf.cli.output.Output(self.base, self.base.conf)

    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output.P_', dnf.pycomp.NullTranslations().ungettext)
    @mock.patch('dnf.cli.output._term_width', return_value=80)
    def test_list_transaction(self, _term_width):
        sack = self.base.sack
        q = sack.query().filter(name='pepper')
        i = q.installed()[0]
        u = q.available()[0]
        obs = sack.query().filter(name='hole').installed()[0]

        transaction = dnf.transaction.Transaction()
        transaction.add_upgrade(u, i, [obs])
        self.assertEqual(self.output.list_transaction(transaction),
                         LIST_TRANSACTION_OUTPUT)

    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.i18n.ucd_input')
    def test_userconfirm(self, input_fnc):
        # with defaultyes==False
        input_fnc.return_value = 'y'
        self.assertTrue(self.output.userconfirm())
        self.assertEqual(input_fnc.call_args, mock.call(u'Is this ok [y/N]: '))

        input_fnc.return_value = 'n'
        self.assertFalse(self.output.userconfirm())

        input_fnc.return_value = ''
        self.assertFalse(self.output.userconfirm())

        input_fnc.side_effect = self._keyboard_interrupt
        input_fnc.return_value = 'y'
        self.assertFalse(self.output.userconfirm())

        input_fnc.side_effect = self._eof_error
        self.assertFalse(self.output.userconfirm())

        # with defaultyes==True
        self.output.conf.defaultyes = True
        input_fnc.side_effect = None
        input_fnc.return_value = ''
        self.assertTrue(self.output.userconfirm())

        input_fnc.side_effect = self._keyboard_interrupt
        input_fnc.return_value = ''
        self.assertFalse(self.output.userconfirm())

        input_fnc.side_effect = self._eof_error
        self.assertTrue(self.output.userconfirm())

    def _to_unicode_mock(str):
        return {'y': 'a', 'yes': 'ano', 'n': 'e', 'no': 'ee'}.get(str, str)

    @mock.patch('dnf.cli.output._', _to_unicode_mock)
    @mock.patch('dnf.i18n.ucd_input')
    def test_userconfirm_translated(self, input_fnc):
        input_fnc.return_value = 'ee'
        self.assertFalse(self.output.userconfirm())

        input_fnc.return_value = 'ano'
        self.assertTrue(self.output.userconfirm())

    class _InputGenerator(object):
        INPUT=['haha', 'dada', 'n']

        def __init__(self):
            self.called = 0

        def __call__(self, msg):
            ret = self.INPUT[self.called]
            self.called += 1
            return ret

    def test_userconfirm_bad_input(self):
        input_fnc = self._InputGenerator()
        with mock.patch('dnf.i18n.ucd_input', input_fnc):
            self.assertFalse(self.output.userconfirm())
        self.assertEqual(input_fnc.called, 3)

    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    def test_infoOutput_with_none_description(self):
        pkg = support.MockPackage('tour-5-0.noarch')
        pkg.from_system = False
        pkg.size = 0
        pkg.pkgid = None
        pkg.repoid = None
        pkg.e = pkg.epoch
        pkg.v = pkg.version
        pkg.r = pkg.release
        pkg.summary = 'A summary of the package.'
        pkg.url = 'http://example.com'
        pkg.license = 'GPL+'
        pkg.description = None

        with mock.patch('sys.stdout') as stdout:
            self.output.infoOutput(pkg)
        written = ''.join([mc[1][0] for mc in stdout.method_calls
                          if mc[0] == 'write'])
        self.assertEqual(written, INFOOUTPUT_OUTPUT)

PKGS_IN_GROUPS_OUTPUT = u"""\

Group: Pepper's
 Mandatory Packages:
   hole
   lotus
"""

PKGS_IN_GROUPS_VERBOSE_OUTPUT = u"""\

Group: Pepper's
 Group-Id: Peppers
 Mandatory Packages:
   hole-1-1.x86_64                                                       @System
   lotus-3-16.i686                                                       main   
"""

class GroupOutputTest(unittest.TestCase):
    def setUp(self):
        base = support.MockBase('main')
        base.read_mock_comps()
        output = dnf.cli.output.Output(base, base.conf)

        self.base = base
        self.output = output

    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output._term_width', return_value=80)
    def test_group_info(self, _term_width):
        group = self.base.comps.group_by_pattern('Peppers')
        with support.patch_std_streams() as (stdout, stderr):
            self.output.displayPkgsInGroups(group)
        self.assertEqual(stdout.getvalue(), PKGS_IN_GROUPS_OUTPUT)

    @mock.patch('dnf.cli.output._', dnf.pycomp.NullTranslations().ugettext)
    @mock.patch('dnf.cli.output._term_width', return_value=80)
    def test_group_verbose_info(self, _term_width):
        group = self.base.comps.group_by_pattern('Peppers')
        self.output.conf.verbose = True
        with support.patch_std_streams() as (stdout, stderr):
            self.output.displayPkgsInGroups(group)
        self.assertEqual(stdout.getvalue(), PKGS_IN_GROUPS_VERBOSE_OUTPUT)

class TermTest(unittest.TestCase):

    """Tests of ```dnf.cli.output.Term``` class."""

    def test_mode_tty(self):
        """Test whether all modes are properly set if the stream is a tty.

        It also ensures that all the values are unicode strings.

        """
        tty = mock.create_autospec(io.IOBase)
        tty.isatty.return_value = True

        tigetstr = lambda name: '<cap_%(name)s>' % locals()
        with mock.patch('curses.tigetstr', autospec=True, side_effect=tigetstr):
            term = dnf.cli.output.Term(tty)

        self.assertEqual(term.MODE,
                         {u'blink': tigetstr(u'blink'),
                          u'bold': tigetstr(u'bold'),
                          u'dim': tigetstr(u'dim'),
                          u'normal': tigetstr(u'sgr0'),
                          u'reverse': tigetstr(u'rev'),
                          u'underline': tigetstr(u'smul')})

    def test_mode_tty_incapable(self):
        """Test whether modes correct if the stream is an incapable tty.

        It also ensures that all the values are unicode strings.

        """
        tty = mock.create_autospec(io.IOBase)
        tty.isatty.return_value = True

        with mock.patch('curses.tigetstr', autospec=True, return_value=None):
            term = dnf.cli.output.Term(tty)

        self.assertEqual(term.MODE,
                         {u'blink': u'',
                          u'bold': u'',
                          u'dim': u'',
                          u'normal': u'',
                          u'reverse': u'',
                          u'underline': u''})

    def test_mode_nontty(self):
        """Test whether all modes are properly set if the stream is not a tty.

        It also ensures that all the values are unicode strings.

        """
        nontty = mock.create_autospec(io.IOBase)
        nontty.isatty.return_value = False

        term = dnf.cli.output.Term(nontty)

        self.assertEqual(term.MODE,
                         {u'blink': u'',
                          u'bold': u'',
                          u'dim': u'',
                          u'normal': u'',
                          u'reverse': u'',
                          u'underline': u''})

########NEW FILE########
__FILENAME__ = test_package
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import hawkey
import binascii
from tests.support import PycompTestCase
from dnf.pycomp import long

TOUR_MD5 = binascii.unhexlify("68e9ded8ea25137c964a638f12e9987c")
TOUR_SHA256 = binascii.unhexlify("ce77c1e5694b037b6687cf0ab812ca60431ec0b65116abbb7b82684f0b092d62")
TOUR_WRONG_MD5 = binascii.unhexlify("ffe9ded8ea25137c964a638f12e9987c")
TOUR_SIZE = 2317

class PackageTest(PycompTestCase):
    def setUp(self):
        base = support.MockBase("main")
        self.sack = base.sack
        self.pkg = self.sack.query().available().filter(name="pepper")[1]

    def test_from_cmdline(self):
        self.sack.create_cmdline_repo()
        local_pkg = self.sack.add_cmdline_package(support.TOUR_44_PKG_PATH)
        self.assertTrue(local_pkg.from_cmdline)
        self.assertFalse(self.pkg.from_cmdline)

    def test_from_system(self):
        pkg = self.sack.query().installed().filter(name="pepper")[0]
        self.assertTrue(pkg.from_system)
        self.assertFalse(self.pkg.from_system)

    @mock.patch("dnf.package.Package.rpmdbid", long(3))
    def test_idx(self):
        """ pkg.idx is an int. """
        pkg = self.sack.query().installed().filter(name="pepper")[0]
        self.assertEqual(type(pkg.idx), int)

    def test_pkgtup(self):
        self.assertEqual(self.pkg.pkgtup, ('pepper', 'x86_64', '0', '20', '0'))

    @mock.patch("dnf.package.Package.location", 'f/foo.rpm')
    def test_localPkg(self):
        self.pkg.repo.basecachedir = '/cachedir'
        self.pkg.repo.baseurl = ['file:///mnt/cd']
        self.assertTrue(self.pkg.repo.local)
        self.assertEquals(self.pkg.localPkg(), '/mnt/cd/f/foo.rpm')
        self.pkg.repo.baseurl = ['http://remote']
        self.assertFalse(self.pkg.repo.local)
        self.assertEquals(self.pkg.localPkg(), '/cachedir/main/packages/foo.rpm')

    def test_verify(self):
        with mock.patch.object(self.pkg, 'localPkg',
                               return_value=support.TOUR_44_PKG_PATH):
            self.pkg.chksum = (hawkey.CHKSUM_MD5, TOUR_MD5)
            self.pkg.size = TOUR_SIZE
            self.assertTrue(self.pkg.verifyLocalPkg())
            self.pkg.chksum = (hawkey.CHKSUM_MD5, TOUR_WRONG_MD5)
            self.assertFalse(self.pkg.verifyLocalPkg())

    def test_return_id_sum(self):
        self.pkg.chksum = (hawkey.CHKSUM_MD5, TOUR_MD5)
        self.assertEqual(self.pkg.returnIdSum(),
                         ('md5', '68e9ded8ea25137c964a638f12e9987c'))

    def test_verify_local(self):
        self.sack.create_cmdline_repo()
        local_pkg = self.sack.add_cmdline_package(support.TOUR_44_PKG_PATH)
        self.assertEqual(local_pkg.reponame, hawkey.CMDLINE_REPO_NAME)
        self.assertTrue(local_pkg.verifyLocalPkg())

    def test_chksum_local(self):
        self.sack.create_cmdline_repo()
        local_pkg = self.sack.add_cmdline_package(support.TOUR_44_PKG_PATH)
        chksum = local_pkg.chksum
        self.assertEqual(chksum[0], hawkey.CHKSUM_SHA256)
        self.assertEqual(chksum[1], TOUR_SHA256)

    def test_verify_installed(self):
        pkg = self.sack.query().installed().filter(name="pepper")[0]
        self.assertRaises(ValueError, pkg.verifyLocalPkg)

########NEW FILE########
__FILENAME__ = test_persistor
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals

import dnf.comps
import dnf.persistor
import dnf.pycomp
import tempfile
import tests.support

IDS = set(['one', 'two', 'three'])

class ClonableDictTest(tests.support.TestCase):
    def test_clone(self):
        g = dnf.persistor.ClonableDict({})
        g['base'] = ['pepper', 'tour']
        g_c = g.clone()
        self.assertEqual(g, g_c)
        g_c['base'].append('magical')
        self.assertNotEqual(g, g_c)


class GroupPersistorTest(tests.support.TestCase):
    def setUp(self):
        self.persistdir = tempfile.mkdtemp(prefix="dnf-groupprst-test.0.0.5")
        self.prst = dnf.persistor.GroupPersistor(self.persistdir)

    def tearDown(self):
        dnf.util.rm_rf(self.persistdir)

    def test_default(self):
        """Default items are empty."""
        grp = self.prst.group('pepper')
        self.assertEmpty(grp.full_list)
        self.assertEquals(grp.pkg_types, 0)

    def test_prune_db(self):
        prst = self.prst
        grp = prst.group('pepper')
        prst._prune_db()
        self.assertEmpty(prst.db['GROUPS'])

        grp = prst.group('pepper')
        grp.pkg_types = dnf.comps.MANDATORY
        prst._prune_db()
        self.assertLength(prst.db['GROUPS'], 1)

    def test_saving(self):
        prst = self.prst
        grp = prst.group('pepper')
        grp.full_list.extend(['pepper', 'tour'])
        grp.pkg_types = dnf.comps.DEFAULT | dnf.comps.OPTIONAL
        prst.commit()
        self.assertTrue(prst.save())

        prst = dnf.persistor.GroupPersistor(self.persistdir)
        grp = prst.group('pepper')
        self.assertEqual(grp.full_list, ['pepper', 'tour'])
        self.assertEqual(grp.pkg_types, dnf.comps.DEFAULT | dnf.comps.OPTIONAL)

    def test_version(self):
        version = self.prst.db['meta']['version']
        self.assertIsInstance(version, dnf.pycomp.unicode)


class RepoPersistorTest(tests.support.TestCase):
    def setUp(self):
        self.persistdir = tempfile.mkdtemp(prefix="dnf-repoprst-test-")
        self.prst = dnf.persistor.RepoPersistor(self.persistdir)

    def tearDown(self):
        dnf.util.rm_rf(self.persistdir)

    def test_expired_repos(self):
        self.assertLength(self.prst.get_expired_repos(), 0)
        self.prst.set_expired_repos(IDS)
        self.assertEqual(self.prst.get_expired_repos(), IDS)

        prst = dnf.persistor.RepoPersistor(self.persistdir)
        self.assertEqual(prst.get_expired_repos(), IDS)

########NEW FILE########
__FILENAME__ = test_plugin
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals

import dnf.logging
import dnf.plugin
import dnf.pycomp
import tests.support

PLUGINS = "%s/tests/plugins" % tests.support.dnf_toplevel()

class PluginTest(tests.support.TestCase):
    def setUp(self):
        self.plugins = dnf.plugin.Plugins()
        self.plugins.load([PLUGINS], ())

    def tearDown(self):
        self.plugins.unload()

    def test_load(self):
        self.assertLength(self.plugins.plugin_cls, 1)
        cls = self.plugins.plugin_cls[0]
        assert(issubclass(cls, dnf.plugin.Plugin))
        self.assertEqual(cls.name, 'lucky')

    def test_runs(self):
        self.assertLength(self.plugins.plugins, 0)
        self.plugins.run_init(None, None)
        self.assertLength(self.plugins.plugins, 1)
        self.plugins.run_config()
        lucky = self.plugins.plugins[0]
        self.assertTrue(lucky._config)

    def test_config(self):
        base = tests.support.MockBase()
        base.conf.pluginconfpath = PLUGINS
        self.plugins.run_init(base, None)
        lucky = self.plugins.plugins[0]
        conf = lucky.read_config(base.conf, 'lucky')
        self.assertTrue(conf.getboolean('main', 'enabled'))
        self.assertEqual(conf.get('main', 'wanted'), '/to/be/haunted')

class PluginSkipsTest(tests.support.TestCase):
    def test_skip(self):
        self.plugins = dnf.plugin.Plugins()
        self.plugins.load([PLUGINS], ('luck*',))
        self.assertLength(self.plugins.plugin_cls, 0)

    def tearDown(self):
        self.plugins.unload()

class PluginNonExistentTest(tests.support.TestCase):

    """Tests with a non-existent plugin."""

    def test_logs_traceback(self):
        """Test whether the traceback is logged if a plugin cannot be imported."""
        package = dnf.pycomp.ModuleType('testpkg')
        package.__path__ = []
        stream = dnf.pycomp.StringIO()

        with tests.support.wiretap_logs('dnf', dnf.logging.SUBDEBUG, stream):
            dnf.plugin.import_modules(package, ('nonexistent.py',))

        end = ('ImportError: No module named \'testpkg\'\n' if dnf.pycomp.PY3
               else 'ImportError: No module named testpkg.nonexistent\n')
        self.assertTracebackIn(end, stream.getvalue())

########NEW FILE########
__FILENAME__ = test_provides
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support

class ProvidesTest(support.TestCase):
    def setUp(self):
        self.base = support.MockBase("main")

    def test_file(self):
        self.assertLength(self.base.provides("*ais*smile"), 1)
        self.assertLength(self.base.provides("/raised/smile"), 1)

    def test_name(self):
        self.assertLength(self.base.provides("henry(the_horse)"), 1)
        self.assertLength(self.base.provides("lotus"), 2)

########NEW FILE########
__FILENAME__ = test_queries
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
import dnf.query
import dnf.subject
from tests.support import PycompTestCase

class QueriesTest(support.TestCase):
    def test_duplicities(self):
        sack = support.mock_sack()
        pepper = sack.query().installed().filter(name="pepper")
        # make sure 'pepper' package exists:
        self.assertEqual(len(pepper), 1)
        # we shouldn't see it more than once with a tricky query below:
        res = sack.query().installed().filter(name=["pep*", "*per"])
        res_set = set(res)
        self.assertEqual(len(res), len(res_set))

    def test_by_file(self):
        # check sanity first:
        sack = support.mock_sack()
        q = sack.query().filter(file__eq="/raised/smile")
        self.assertEqual(len(q.run()), 1)
        pkg = q.result[0]

    def test_by_repo(self):
        sack = support.mock_sack("updates", "main")
        pkgs = sack.query().filter(reponame__eq="updates")
        self.assertEqual(len(pkgs), support.UPDATES_NSOLVABLES)
        pkgs = sack.query().filter(reponame__eq="main")
        self.assertEqual(len(pkgs), support.MAIN_NSOLVABLES)

    def test_installed_exact(self):
        sack = support.mock_sack()
        pkgs = sack.query().installed().nevra("tour-4.9-0.noarch")
        self.assertEqual(len(pkgs), 0)
        pkgs = sack.query().installed().nevra("tour-5-0.x86_64")
        self.assertEqual(len(pkgs), 0)
        pkgs = sack.query().installed().nevra("tour-5-0.noarch")
        self.assertEqual(len(pkgs), 1)

class SubjectTest(support.TestCase):
    def setUp(self):
        self.sack = support.mock_sack("main", "updates")

    def test_wrong_name(self):
        subj = dnf.subject.Subject("call-his-wife-in")
        self.assertLength(subj.get_best_query(self.sack), 0)

    def test_query_composing(self):
        q = dnf.subject.Subject("librita").get_best_query(self.sack)
        q = q.filter(arch="i686")
        self.assertEqual(str(q[0]), "librita-1-1.i686")

    def test_icase_name(self):
        subj = dnf.subject.Subject("PEpper", ignore_case=True)
        q = subj.get_best_query(self.sack)
        self.assertLength(q, 4)

    def test_get_best_selector(self):
        s = dnf.subject.Subject("pepper-20-0.x86_64").get_best_selector(self.sack)
        self.assertIsNotNone(s)

    def test_best_selector_for_version(self):
        sltr = dnf.subject.Subject("hole-2").get_best_selector(self.sack)
        self.assertItemsEqual(map(str, sltr.matches()),
                              ['hole-2-1.x86_64', 'hole-2-1.i686'])

    def test_with_confusing_dashes(self):
        sltr = dnf.subject.Subject("mrkite-k-h").get_best_selector(self.sack)
        self.assertLength(sltr.matches(), 1)
        sltr = dnf.subject.Subject("mrkite-k-h.x86_64").\
            get_best_selector(self.sack)
        self.assertLength(sltr.matches(), 1)

class DictsTest(PycompTestCase):
    def test_per_nevra_dict(self):
        sack = support.mock_sack("main")
        pkgs = sack.query().filter(name="lotus")
        dct = dnf.query.per_nevra_dict(pkgs)
        self.assertItemsEqual(dct.keys(),
                              ["lotus-3-16.x86_64", "lotus-3-16.i686"])
        self.assertItemsEqual(dct.values(), pkgs)

########NEW FILE########
__FILENAME__ = test_reinstall
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
import dnf
import itertools

class Reinstall(support.ResultTestCase):
    def setUp(self):
        self.base = support.MockBase('main')
        self.base.conf.multilib_policy = 'all'
        self.sack = self.base.sack

    def test_reinstall(self):
        cnt = self.base.reinstall('pepper')
        self.assertEqual(cnt, 1)
        new_set = support.installed_but(self.sack, "pepper")
        available_query = self.sack.query().available()
        new_set += list(available_query.nevra("pepper-20-0.x86_64"))
        self.assertResult(self.base, new_set)

    def test_reinstall_new_reponame_available(self):
        """Test whether it installs packages only from the repository."""
        reinstalled_count = self.base.reinstall('librita', new_reponame='main')

        self.assertEqual(reinstalled_count, 1)
        self.assertResult(self.base, itertools.chain(
            self.sack.query().installed().filter(name__neq='librita'),
            dnf.subject.Subject('librita.i686').get_best_query(self.sack).installed(),
            dnf.subject.Subject('librita').get_best_query(self.sack).available()))

    def test_reinstall_new_reponame_notavailable(self):
        """Test whether it installs packages only from the repository."""
        self.assertRaises(
            dnf.exceptions.PackagesNotAvailableError,
            self.base.reinstall, 'librita', new_reponame='non-main')

    def test_reinstall_new_reponame_neq_available(self):
        """Test whether it installs only packages not from the repository."""
        reinstalled_count = self.base.reinstall('librita', new_reponame_neq='non-main')

        self.assertEqual(reinstalled_count, 1)
        self.assertResult(self.base, itertools.chain(
            self.sack.query().installed().filter(name__neq='librita'),
            dnf.subject.Subject('librita.i686').get_best_query(self.sack).installed(),
            dnf.subject.Subject('librita').get_best_query(self.sack).available()))

    def test_reinstall_new_reponame_neq_notavailable(self):
        """Test whether it installs only packages not from the repository."""
        self.assertRaises(
            dnf.exceptions.PackagesNotAvailableError,
            self.base.reinstall, 'librita', new_reponame_neq='main')

    def test_reinstall_notfound(self):
        """Test whether it fails if the package does not exist."""
        with self.assertRaises(dnf.exceptions.PackagesNotInstalledError) as ctx:
            self.base.reinstall('non-existent')

        self.assertEqual(ctx.exception.pkg_spec, 'non-existent')
        self.assertResult(self.base, self.sack.query().installed())

    def test_reinstall_notinstalled(self):
        """Test whether it fails if the package is not installed."""
        with self.assertRaises(dnf.exceptions.PackagesNotInstalledError) as ctx:
            self.base.reinstall('lotus')

        self.assertEqual(ctx.exception.pkg_spec, 'lotus')
        self.assertResult(self.base, self.sack.query().installed())

    def test_reinstall_notavailable(self):
        """Test whether it fails if the package is not available."""
        with self.assertRaises(dnf.exceptions.PackagesNotAvailableError) as ctx:
            self.base.reinstall('hole')

        self.assertEqual(ctx.exception.pkg_spec, 'hole')
        self.assertItemsEqual(
            ctx.exception.packages,
            dnf.subject.Subject('hole').get_best_query(self.sack).installed())
        self.assertResult(self.base, self.sack.query().installed())

    def test_reinstall_notavailable_available(self):
        """Test whether it does not fail if some packages are available and some not."""
        reinstalled_count = self.base.reinstall('librita')

        self.assertEqual(reinstalled_count, 1)
        self.assertResult(self.base, itertools.chain(
            self.sack.query().installed().filter(name__neq='librita'),
            dnf.subject.Subject('librita.i686').get_best_query(self.sack).installed(),
            dnf.subject.Subject('librita').get_best_query(self.sack).available()))

    def test_reinstall_old_reponame_installed(self):
        """Test whether it reinstalls packages only from the repository."""
        for pkg in self.sack.query().installed().filter(name='librita'):
            self.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self.base.yumdb.get_package(pkg).from_repo = 'main'

        reinstalled_count = self.base.reinstall('librita', old_reponame='main')

        self.assertEqual(reinstalled_count, 1)
        self.assertResult(self.base, itertools.chain(
            self.sack.query().installed().filter(name__neq='librita'),
            dnf.subject.Subject('librita.i686').get_best_query(self.sack).installed(),
            dnf.subject.Subject('librita').get_best_query(self.sack).available()))

    def test_reinstall_old_reponame_notinstalled(self):
        """Test whether it reinstalls packages only from the repository."""
        self.assertRaises(
            dnf.exceptions.PackagesNotInstalledError,
            self.base.reinstall, 'librita', old_reponame='non-main')

    def test_reinstall_remove_notavailable(self):
        """Test whether it removes the package which is not available."""
        self.base.reinstall('hole', remove_na=True)

        self.assertResult(
            self.base,
            self.sack.query().installed().filter(name__neq='hole'))

########NEW FILE########
__FILENAME__ = test_remove
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
import dnf.cli.commands
import itertools

class Remove(support.ResultTestCase):
    def setUp(self):
        self.base = support.MockBase()
        self.allow_erasing = True

    def test_not_installed(self):
        """ Removing a not-installed package is a void operation. """
        with self.assertRaises(dnf.exceptions.PackagesNotInstalledError) as context:
            self.base.remove('mrkite')
        self.assertEqual(context.exception.pkg_spec, 'mrkite')
        installed_pkgs = self.base.sack.query().installed().run()
        self.assertResult(self.base, installed_pkgs)

    def test_remove(self):
        """ Simple remove. """
        ret = self.base.remove("pepper")
        self.assertResult(self.base,
                          support.installed_but(self.base.sack, "pepper"))

    def test_remove_depended(self):
        """ Remove a lib that some other package depends on. """
        ret = self.base.remove("librita")
        # we should end up with nothing in this case:
        new_set = support.installed_but(self.base.sack, "librita", "pepper")
        self.assertResult(self.base, new_set)

    def test_remove_nevra(self):
        ret = self.base.remove("pepper-20-0.x86_64")
        pepper = self.base.sack.query().installed().filter(name="pepper")
        (installed, removed) = self.installed_removed(self.base)
        self.assertLength(installed, 0)
        self.assertItemsEqual(removed, pepper.run())

    def test_remove_glob(self):
        """ Test that weird input combinations with globs work. """
        ret = self.base.remove("*.i686")
        self.assertEqual(ret, 1)

    def test_remove_provides(self):
        """Remove uses provides too."""
        self.assertEqual(1, self.base.remove('parking'))

    def test_reponame(self):
        """Test whether only packages from the repository are uninstalled."""
        pkg_subj = dnf.subject.Subject('librita.x86_64')
        for pkg in pkg_subj.get_best_query(self.base.sack).installed():
            self.base.yumdb.db[str(pkg)] = support.RPMDBAdditionalDataPackageStub()
            self.base.yumdb.get_package(pkg).from_repo = 'main'

        self.base.remove('librita', 'main')
        self.assertResult(self.base, itertools.chain(
              self.base.sack.query().installed().filter(name__neq='librita'),
              dnf.subject.Subject('librita.i686').get_best_query(self.base.sack)
              .installed()))

########NEW FILE########
__FILENAME__ = test_repo
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from dnf.i18n import ucd
from tests import support
from tests.support import mock

import dnf.callback
import dnf.drpm
import dnf.repo
import dnf.util
import dnf.exceptions
import iniparse.compat
import io
import librepo
import os
import tempfile
import unittest

REPOS = "%s/tests/repos" % support.dnf_toplevel()
BASEURL = "file://%s/rpm" % REPOS
TOUR_CHKSUM = """\
ce77c1e5694b037b6687cf0ab812ca60431ec0b65116abbb7b82684f0b092d62"""

class RepoIdInvalidTest(unittest.TestCase):

    def test(self):
        """Test repo_id_invalid with a good repo ID."""
        index = dnf.repo.repo_id_invalid('R_e-p.o:i3d')
        self.assertIsNone(index)

    def test_invalid(self):
        """Test repo_id_invalid with a repo ID with an invalid character."""
        index = dnf.repo.repo_id_invalid('R_e-p.o/:i3d')
        self.assertEqual(index, 7)

class RepoTestMixin(object):
    """Test the logic of dnf.repo.Repo.

    There is one cache directory for the entire TestCase, but each individual
    test cleans up the cache after itself.

    We only test sync from a local dir. Testing all sorts of remote downloads
    from mirrorlists etc. is up to librepo.

    """
    TMP_CACHEDIR = None

    @classmethod
    def setUpClass(cls):
         cls.TMP_CACHEDIR = tempfile.mkdtemp(prefix="dnf-repotest-")

    @classmethod
    def tearDownClass(cls):
        dnf.util.rm_rf(cls.TMP_CACHEDIR)

    def build_repo(self, id_, name=None):
        repo = dnf.repo.Repo(id_, self.TMP_CACHEDIR)
        repo.baseurl = [BASEURL]
        repo.name = id_ if name is None else name
        return repo

    def tearDown(self):
        repo_path = os.path.join(self.TMP_CACHEDIR, "r")
        dnf.util.rm_rf(repo_path)

class HandleTest(support.TestCase):
    def test_useragent(self):
        h = dnf.repo._Handle(False, 0)
        self.assertTrue(h.useragent.startswith("dnf/"))
        self.assertEqual(h.maxmirrortries, 0)

    def test_substs(self):
        subst_dct = {'version': '69'}
        h = dnf.repo._Handle.new_local(subst_dct, False, 1, '/')
        self.assertItemsEqual(h.varsub, [('version', '69'),])

class MetadataTest(support.TestCase):
    def setUp(self):
        result = mock.Mock(spec=['yum_repo', 'yum_repomd'])
        result.yum_repo = {'primary': support.NONEXISTENT_FILE}
        handle = mock.Mock(spec=['mirrors'])
        handle.mirrors = []
        self.md = dnf.repo.Metadata(result, handle)

    def test_file_timestamp(self):
        self.assertRaises(dnf.exceptions.MetadataError,
                          self.md.file_timestamp, 'primary')

class RepoTest(RepoTestMixin, support.TestCase):
    """Test the logic of dnf.repo.Repo.

    There is one cache directory for the entire TestCase, but each individual
    test cleans up the cache after itself.

    We only test sync from a local dir. Testing all sorts of remote downloads
    from mirrorlists etc. is up to librepo.

    """

    def setUp(self):
        self.repo = self.build_repo('r', 'r for riot')

    def test_cachedir(self):
        self.assertEqual(self.repo.cachedir,
                         os.path.join(self.TMP_CACHEDIR, self.repo.id))

    def test_dump(self):
        dump = self.repo.dump()
        f = io.StringIO(ucd(dump))
        parser = iniparse.compat.ConfigParser()
        parser.readfp(f)
        self.assertIn('r', parser.sections())
        opts = parser.options('r')
        self.assertIn('bandwidth', opts)
        self.assertIn('gpgkey', opts)
        self.assertEqual(parser.get('r', 'timeout'), '30.0')

    def test_cost(self):
        """Test the cost is passed down to the hawkey repo instance."""
        repo2 = dnf.repo.Repo("r2", self.TMP_CACHEDIR)
        repo2.baseurl = [BASEURL]
        repo2.name = "r2 repo"
        self.repo.cost = 500
        repo2.cost = 700

        base = support.MockBase()
        base.init_sack()
        base.repos.add(self.repo)
        base.repos.add(repo2)
        base._add_repo_to_sack('r')
        base._add_repo_to_sack('r2')
        self.assertEqual(500, self.repo.hawkey_repo.cost)
        self.assertEqual(700, repo2.hawkey_repo.cost)

    def test_expire_cache(self):
        self.repo.load()
        # the second time we only hit the cache:
        del self.repo
        self.repo = dnf.repo.Repo("r", self.TMP_CACHEDIR)
        self.repo.baseurl = [BASEURL]
        self.repo.md_expire_cache()
        self.assertTrue(self.repo.load())

    def test_gpgcheck(self):
        self.repo.gpgcheck = True
        self.assertTrue(self.repo.load())

    @mock.patch('dnf.repo.Repo.local', False)
    def test_keep_old_pgks(self):
        dnf.util.ensure_dir(self.repo.pkgdir)
        survivor = os.path.join(self.repo.pkgdir, "survivor")
        dnf.util.touch(survivor)
        # syncing a repo shouldn't clear the pkgdir
        self.repo.load()
        self.assertFile(survivor)

    def test_load_twice(self):
        self.assertTrue(self.repo.load())
        # the second time we only hit the cache:
        del self.repo
        self.repo = dnf.repo.Repo("r", self.TMP_CACHEDIR)
        self.assertFalse(self.repo.load())
        self.assertIsNotNone(self.repo.metadata)

    def test_load(self):
        repo = self.repo
        self.assertIsNone(repo.metadata)
        self.assertTrue(repo.load())
        self.assertIsNotNone(repo.metadata)
        repomd = os.path.join(self.TMP_CACHEDIR, "r/repodata/repomd.xml")
        self.assertTrue(os.path.isfile(repomd))
        self.assertTrue(repo.metadata.fresh)

    def test_load_badconf(self):
        self.repo.baseurl = []
        self.assertRaises(dnf.exceptions.RepoError, self.repo.load)

    def test_metadata_expire_in(self):
        repo = self.repo
        self.assertEqual(repo.metadata_expire_in(), (False, 0))
        repo.load()
        (has, time) = repo.metadata_expire_in()
        self.assertTrue(has)
        self.assertGreater(time, 0)

        repo.metadata_expire = 'never'
        self.assertEqual(repo.metadata_expire_in(), (True, None))

    def test_md_only_cached(self):
        self.repo.md_only_cached = True
        self.assertRaises(dnf.exceptions.RepoError, self.repo.load)
        self.repo.md_try_cache()
        self.repo.load()
        del self.repo
        self.setUp() # get a new repo
        self.repo.md_only_cached = True
        self.assertFalse(self.repo.load())
        self.assertFalse(self.repo.metadata.fresh)

        # try again with a quickly expiring cache
        del self.repo
        self.setUp()
        self.repo.metadata_expire = 0
        self.repo.md_only_cached = True
        self.assertFalse(self.repo.load())

    def test_pkgdir(self):
        self.assertRegexpMatches(self.repo.pkgdir, '/.*tests/repos/rpm')
        self.repo.mirrorlist = 'http://anything'
        self.assertTrue(self.repo.pkgdir.startswith(self.TMP_CACHEDIR))

    def test_progress_cb(self):
        m = mock.Mock()
        self.repo.set_progress_bar(m)
        self.repo.load()
        self.assertTrue(m.start.called)
        self.assertTrue(m.progress.called)
        self.assertTrue(m.end.called)

    @mock.patch('librepo.Handle.setopt')
    def test_repo_gpgcheck(self, setopt):
        """Test repo_gpgcheck option works."""
        self.repo.repo_gpgcheck = False
        handle = self.repo._handle_new_remote("/bag")
        setopt.assert_any_call(librepo.LRO_GPGCHECK, False)

        self.repo.repo_gpgcheck = True
        handle = self.repo._handle_new_remote("/bag")
        setopt.assert_any_call(librepo.LRO_GPGCHECK, True)

    def test_reset_metadata_expired(self):
        repo = self.repo
        repo.load()
        repo.metadata_expire = 0
        repo._reset_metadata_expired()
        self.assertTrue(repo.metadata.expired)
        repo.metadata_expire = 'never'
        repo._reset_metadata_expired()
        self.assertFalse(repo.metadata.expired)

    def test_valid(self):
        self.assertIsNone(self.repo.valid())

        repo = dnf.repo.Repo('r', None)
        self.assertRegexpMatches(repo.valid(), 'no mirror or baseurl')

    def test_handle_new_pkg_download(self):
        """Ensure mirrors are never resolved for package download."""
        repo = self.repo
        repo.mirrorlist = 'http://anything'
        repo.metadata = mock.Mock()
        repo.metadata.mirrors = ['resolved']
        h = repo._handle_new_pkg_download()
        self.assertIsNone(h.mirrorlist)

    def test_throttle(self):
        self.repo.throttle = '50%'
        self.repo.bandwidth = '10M'
        self.assertEquals(self.repo.throttle, 0.5)
        self.assertEquals(self.repo.bandwidth, 10 << 20)
        opts = {}
        with mock.patch('librepo.Handle.setopt', opts.__setitem__):
            self.repo.get_handle()
        self.assertEquals(opts[librepo.LRO_MAXSPEED], 5 << 20)

class LocalRepoTest(support.TestCase):
    def setUp(self):
        # directly loads the repo as created by createrepo
        self.repo = dnf.repo.Repo("rpm", REPOS)
        self.repo.name = "r for riot"

    def test_mirrors(self):
        self.repo.md_only_cached = True
        self.assertFalse(self.repo.load()) # got a cache
        self.assertLength(self.repo.metadata.mirrors, 4)
        self.assertEqual(self.repo.metadata.mirrors[0], 'http://many/x86_64')

    @mock.patch.object(dnf.repo.Metadata, 'reset_age')
    @mock.patch('dnf.repo.Repo._handle_new_remote')
    def test_reviving(self, new_remote_m, reset_age_m):
        self.repo.md_expire_cache()
        self.repo.metalink = 'http://meh'
        remote_handle_m = new_remote_m()
        remote_handle_m.metalink = \
            {'hashes': [('md5', 'fcf04ce803b3e15cbef6ea6f12ed4533'),
                        ('sha1', '3731498f6b7b96316590205a4d7a2add484471e0'),
                        ('sha256', '4394be16de62563321f6ea9604513a8a2f6b9ab67898bbed218feeca8e6a7180'),
                        ('sha512', 'e583eeb91874954b24a376176a087462403e518563f9cb3bdc4f7eae792e8d15ac488bc6d3fb632bbf0ac6cf58bf769e94e9773df6605616a28cf2c00adf8e14')]}
        self.assertTrue(self.repo.load())
        self.assertTrue(remote_handle_m.fetchmirrors)
        self.assertEqual(self.repo.sync_strategy, dnf.repo.SYNC_TRY_CACHE)
        self.assertFalse(self.repo.metadata.expired)
        reset_age_m.assert_called()

    @mock.patch.object(dnf.repo.Metadata, 'reset_age')
    @mock.patch('dnf.repo.Repo._handle_new_remote')
    def test_reviving_lame_hashes(self, new_remote_m, reset_age_m):
        self.repo.md_expire_cache()
        self.repo.metalink = 'http://meh'
        new_remote_m().metalink = \
            {'hashes': [('md5', 'fcf04ce803b3e15cbef6ea6f12ed4533'),
                        ('sha1', '3731498f6b7b96316590205a4d7a2add484471e0')]}
        self.repo._try_cache()
        self.assertFalse(self.repo._try_revive())

    @mock.patch.object(dnf.repo.Metadata, 'reset_age')
    @mock.patch('dnf.repo.Repo._handle_new_remote')
    def test_reviving_mismatched_hashes(self, new_remote_m, reset_age_m):
        self.repo.md_expire_cache()
        self.repo.metalink = 'http://meh'
        new_remote_m().metalink = \
            {'hashes': [('sha256', '4394be16de62563321f6ea9604513a8a2f6b9ab67898bbed218feeca8e6a7180'),
                        ('sha512', 'obviousfail')]}
        # can not do the entire load() here, it would run on after try_revive()
        # failed.
        self.repo._try_cache()
        self.assertFalse(self.repo._try_revive())

    @mock.patch('dnf.repo.Repo._handle_new_remote')
    def test_reviving_404(self, new_remote_m):
        self.repo.md_expire_cache()
        self.repo.metalink = 'http://meh'
        exc = librepo.LibrepoException(10, 'Error HTTP/FTP status code: 404', 404)
        new_remote_m().perform = mock.Mock(side_effect=exc)
        self.assertRaises(dnf.exceptions.RepoError, self.repo.load)

class DownloadPayloadsTest(RepoTestMixin, support.TestCase):

    def test_drpm_error(self):
        def wait(self):
            self.err['step'] = ['right']

        drpm = dnf.drpm.DeltaInfo(None, None)
        with mock.patch('dnf.drpm.DeltaInfo.wait', wait):
            errs = dnf.repo.download_payloads([], drpm)
        self.assertEqual(errs.recoverable, {'step' : ['right']})
        self.assertEmpty(errs.irrecoverable)

    def test_empty_transaction(self):
        drpm = dnf.drpm.DeltaInfo(None, None)
        errs = dnf.repo.download_payloads([], drpm)
        self.assertEmpty(errs.recoverable)
        self.assertEmpty(errs.irrecoverable)

    def test_fatal_error(self):
        def raiser(targets, failfast):
            raise librepo.LibrepoException(10, 'hit', 'before')

        drpm = dnf.drpm.DeltaInfo(None, None)
        with mock.patch('librepo.download_packages', side_effect=raiser):
            errs = dnf.repo.download_payloads([], drpm)
        self.assertEqual(errs.irrecoverable, {'' : ['hit']})
        self.assertEmpty(errs.recoverable)

    # twist Repo to think it's remote:
    @mock.patch('dnf.repo.Repo.local', False)
    def test_remote_download(self):
        progress = dnf.callback.NullDownloadProgress()
        repo = self.build_repo('r', 'r for riot')
        pkg = support.MockPackage("tour-4-4.noarch", repo=repo)
        pkg.downloadsize = 2317
        pkg.chksum = ('sha256', TOUR_CHKSUM)

        pload = dnf.repo.RPMPayload(pkg, progress)
        drpm = dnf.drpm.DeltaInfo(None, None)
        errs = dnf.repo.download_payloads([pload], drpm)
        self.assertEmpty(errs.recoverable)
        self.assertEmpty(errs.irrecoverable)
        path = os.path.join(self.TMP_CACHEDIR, 'r/packages/tour-4-4.noarch.rpm')
        self.assertFile(path)


class MDPayloadTest(unittest.TestCase):
    def test_null_progress(self):
        """MDPayload always has some progress attribute."""
        pload = dnf.repo.MDPayload(None)
        pload.start('roll up')
        self.assertIsNotNone(pload.progress)


class SavingTest(unittest.TestCase):
    def test_update_saving(self):
        progress = dnf.callback.NullDownloadProgress()

        pkg = support.MockPackage("tour-4-4.noarch")
        pkg.downloadsize = 5
        pload1 = dnf.repo.RPMPayload(pkg, progress)
        pkg = support.MockPackage("magical-4-4.noarch")
        pkg.downloadsize = 8
        pload2 = dnf.drpm.DeltaPayload(None, mock.Mock(downloadsize=5), pkg,
                                       progress)
        saving = (5, 10)
        saving = dnf.repo.update_saving(saving, (pload1, pload2), {})
        self.assertEqual(saving, (15, 23))

    def test_update_saving_with_err(self):
        progress = dnf.callback.NullDownloadProgress()

        pkg = support.MockPackage("magical-4-4.noarch")
        pkg.downloadsize = 8
        pload = dnf.drpm.DeltaPayload(None, mock.Mock(downloadsize=5), pkg,
                                      progress)
        saving = (5, 10)
        saving = dnf.repo.update_saving(saving, (pload,), {pkg:'failed'})
        self.assertEqual(saving, (10, 10))

########NEW FILE########
__FILENAME__ = test_repodict
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
from tests import support
from tests.support import PycompTestCase
from tests.support import mock

import dnf.repodict
import operator

class TestMultiCall(PycompTestCase):
    def test_multi_call(self):
        l = dnf.repodict.MultiCallList(["one", "two", "three"])
        self.assertEqual(l.upper(), ["ONE", "TWO", "THREE"])
        self.assertEqual(l.pop(), "three")

    def test_assignment(self):
        o1 = mock.Mock(x=3)
        o2 = mock.Mock(x=5)
        l = dnf.repodict.MultiCallList([o1, o2])
        l.x = 5
        self.assertEqual([5, 5], list(map(operator.attrgetter('x'), [o1, o2])))

class TestRepoDict(PycompTestCase):
    def setUp(self):
        self.x  = support.MockRepo('x', None)
        self.xx = support.MockRepo('xx', None)
        self.y  = support.MockRepo('y', None)
        self.z  = support.MockRepo('z', None)

        self.repos = dnf.repodict.RepoDict()
        self.repos.add(self.x)
        self.repos.add(self.xx)
        self.repos.add(self.y)
        self.repos.add(self.z)
        self.full_set = {self.x, self.xx, self.y, self.z}

    def test_any_enabled(self):
        self.assertTrue(self.repos.any_enabled())
        self.repos.get_matching("*").disable()
        self.assertFalse(self.repos.any_enabled())

    def test_enabled(self):
        self.assertSequenceEqual(self.repos.enabled(),
                                 list(self.repos.iter_enabled()))

    def test_get_matching(self):
        self.assertEqual(self.repos['x'], self.x)
        self.assertItemsEqual(self.repos.get_matching('*'), self.full_set)
        self.assertItemsEqual(self.repos.get_matching('y'), {self.y})
        self.assertItemsEqual(self.repos.get_matching('x*'), {self.x, self.xx})

        self.assertItemsEqual(self.repos.get_matching('nope'), [])

    def test_iter_enabled(self):
        self.assertItemsEqual(self.repos.iter_enabled(), self.full_set)
        self.repos.get_matching('x*').disable()
        self.assertItemsEqual(self.repos.iter_enabled(), {self.y, self.z})

    def test_all(self):
        self.assertItemsEqual(self.repos.all(), self.full_set)

########NEW FILE########
__FILENAME__ = test_rpmconnection
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import unicode_literals
from dnf.rpmUtils.connection import RpmConnection
import inspect
from tests.support import PycompTestCase

class TestConnection(PycompTestCase):
    def test_sanity(self):
        rpm = RpmConnection('/')
        ts = rpm.readonly_ts
        self.assertTrue(inspect.isbuiltin(ts.clean))

########NEW FILE########
__FILENAME__ = test_sack
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf.exceptions
import dnf.repo
import dnf.sack

class SackTest(support.TestCase):
    def test_rpmdb_version(self):
        base = support.MockBase()
        sack = base.sack
        yumdb = mock.MagicMock()
        version = base.sack.rpmdb_version(yumdb)
        self.assertEqual(version._num, support.TOTAL_RPMDB_COUNT)
        self.assertEqual(version._chksum.hexdigest(), support.RPMDB_CHECKSUM)

    def test_setup_excludes(self):
        base = support.MockBase()
        base.conf.exclude=['pepper']
        base._setup_excludes()
        peppers = base.sack.query().filter(name='pepper').run()
        self.assertLength(peppers, 0)

        base = support.MockBase()
        base.conf.disable_excludes = ['all']
        base.conf.exclude=['pepper']
        base._setup_excludes()
        peppers = base.sack.query().filter(name='pepper').run()
        self.assertLength(peppers, 1)

        base = support.MockBase('main')
        base.repos['main'].exclude=['pepp*']
        base._setup_excludes()
        peppers = base.sack.query().filter(name='pepper', reponame='main')
        self.assertLength(peppers, 0)

    def test_add_repo_to_sack(self):
        def raiser():
            raise dnf.exceptions.RepoError()

        base = support.MockBase()
        r = support.MockRepo('bag', None)
        r.enable()
        base._repos.add(r)
        r.load = mock.Mock(side_effect=raiser)
        r.skip_if_unavailable = False
        self.assertRaises(dnf.exceptions.RepoError,
                          base._add_repo_to_sack, "bag")
        self.assertTrue(r.enabled)
        r.skip_if_unavailable = True
        base._add_repo_to_sack("bag")
        self.assertFalse(r.enabled)

########NEW FILE########
__FILENAME__ = test_sanity
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
import os.path
import sys
from tests.support import PycompTestCase

class Sanity(PycompTestCase):
    def test_sanity(self):
        assert(os.access(support.repo("@System.repo"), os.R_OK))
        sack = support.mock_sack()
        assert(sack)
        self.assertEqual(len(sack), support.SYSTEM_NSOLVABLES)

        sack2 = support.MockBase("main", "updates").sack
        self.assertEqual(len(sack2), support.TOTAL_NSOLVABLES)

    def test_toplevel(self):
        self.assertIn(support.dnf_toplevel(), sys.path)

########NEW FILE########
__FILENAME__ = test_selector
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import PycompTestCase
from tests.support import mock

import dnf.selector

class SelectorTest(PycompTestCase):
    def setUp(self):
        self.sack = support.mock_sack()

    def test_set_autoglob(self):
        sltr = dnf.selector.Selector(self.sack)
        with mock.patch.object(sltr, 'set') as mock_set:
            sltr.set_autoglob(learn="how", to="play*")
            mock_set.assert_called_with(learn="how", to__glob="play*")

########NEW FILE########
__FILENAME__ = test_transaction
# Copyright (C) 2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests.support import mock

import dnf.goal
import dnf.repo
import dnf.transaction
import rpm
import tests.support

class TransactionItemTest(tests.support.TestCase):
    def test_active_history_state_erase(self):
        """Test active_history_state with the erase op_type."""
        tsi = dnf.transaction.TransactionItem(
            dnf.transaction.ERASE, erased='old')

        history_state = tsi.active_history_state

        self.assertEqual(history_state, 'Erase')

    def test_active_history_state_install(self):
        """Test active_history_state with the install op_type."""
        tsi = dnf.transaction.TransactionItem(
            dnf.transaction.INSTALL, installed='new', obsoleted=['o1', 'o2'])

        history_state = tsi.active_history_state

        self.assertEqual(history_state, 'Install')

    def test_creating(self):
        tsi = dnf.transaction.TransactionItem(dnf.transaction.UPGRADE, 'new',
                                              'old', ['o1', 'o2', 'o3'])
        self.assertEqual(tsi.installed, 'new')
        self.assertEqual(tsi.erased, 'old')
        self.assertItemsEqual(tsi.obsoleted, ('o1', 'o2', 'o3'))

        tsi = dnf.transaction.TransactionItem(dnf.transaction.ERASE,
                                              erased='old')
        self.assertEqual(tsi.installed, None)
        self.assertEqual(tsi.erased, 'old')
        self.assertItemsEqual(tsi.obsoleted, ())

    def test_history_iterator_reinstall(self):
        """Test history_iterator with the reinstall op_type."""
        tsi = dnf.transaction.TransactionItem(dnf.transaction.REINSTALL, 'new',
                                              'old', ['o1', 'o2', 'o3'])
        self.assertItemsEqual(tsi.history_iterator(),
                              [('new', 'Reinstall'), ('old', 'Reinstalled'),
                               ('new', 'Obsoleting'), ('o1', 'Obsoleted'),
                               ('o2', 'Obsoleted'), ('o3', 'Obsoleted')])

    def test_history_iterator_upgrade(self):
        """Test history_iterator with the upgrade op_type."""
        tsi = dnf.transaction.TransactionItem(dnf.transaction.UPGRADE, 'new',
                                              'old', ['o1', 'o2', 'o3'])
        self.assertItemsEqual(tsi.history_iterator(),
                              [('new', 'Update'), ('old', 'Updated'),
                               ('new', 'Obsoleting'), ('o1', 'Obsoleted'),
                               ('o2', 'Obsoleted'), ('o3', 'Obsoleted')])

    def test_propagated_reason(self):
        TI = dnf.transaction.TransactionItem
        yumdb = mock.Mock()
        yumdb.get_package().get = lambda s: 'dep'

        tsi = TI(dnf.transaction.INSTALL, installed='i1', reason='user')
        self.assertEqual(tsi.propagated_reason(yumdb), 'user')
        tsi = TI(dnf.transaction.UPGRADE, installed='u1', erased='r1')
        self.assertEqual(tsi.propagated_reason(yumdb), 'dep')
        tsi = TI(dnf.transaction.DOWNGRADE, installed='d1', erased='r2')
        self.assertEqual(tsi.propagated_reason(yumdb), 'dep')

        # test the call can survive if no reason is known:
        yumdb = mock.Mock()
        yumdb.get_package().get = lambda s: None
        self.assertEqual(tsi.propagated_reason(yumdb), 'unknown')

    def test_removes(self):
        tsi = dnf.transaction.TransactionItem(dnf.transaction.UPGRADE, 'new',
                                              'old', ['o1', 'o2', 'o3'])
        self.assertItemsEqual(tsi.removes(), ('old', 'o1', 'o2', 'o3'))

class TransactionTest(tests.support.TestCase):
    def setUp(self):
        self.ts = dnf.transaction.Transaction()
        self.ts.add_install('i1', ['o1', 'o2', 'o3'])
        self.ts.add_upgrade('u1', 'r1', ['o4'])
        self.ts.add_upgrade('u2', 'r2', [])
        self.ts.add_downgrade('d1', 'r3', [])

    def test_get_items(self):
        self.assertLength(self.ts.get_items(dnf.transaction.ERASE), 0)
        self.assertLength(self.ts.get_items(dnf.transaction.UPGRADE), 2)

    def test_iter(self):
        self.assertLength(list(self.ts), 4)
        self.assertIsInstance(next(iter(self.ts)),
                              dnf.transaction.TransactionItem)

    def test_length(self):
        self.assertLength(self.ts, 4)

    def test_sets(self):
        self.assertItemsEqual(self.ts.install_set, ('i1', 'u1', 'u2', 'd1'))
        self.assertItemsEqual(self.ts.remove_set,
                              ('o1', 'o2', 'o3', 'o4', 'r1', 'r2', 'r3'))

    def test_total_package_count(self):
        self.assertEqual(self.ts.total_package_count(), 11)

class RPMLimitationsTest(tests.support.TestCase):
    def test_rpm_limitations(self):
        ts = dnf.transaction.Transaction()
        pkg = tests.support.MockPackage('anyway-2-0.src')
        ts.add_install(pkg, [])
        msg = ts.rpm_limitations()
        self.assertIsNot(msg, None)

class PopulateTSTest(tests.support.TestCase):
    def test_populate_rpm_ts(self):
        ts = dnf.transaction.Transaction()
        repo = dnf.repo.Repo('r', '/tmp')

        inst = tests.support.MockPackage("ago-20.0-1.x86_64.fc69", repo)
        upg = tests.support.MockPackage("billy-1.2-1.x86_64.fc69", repo)
        old = tests.support.MockPackage("billy-1.1-1.x86_64.fc69", repo)
        ts.add_install(inst, [])
        ts.add_upgrade(upg, old, [])
        rpm_ts = ts.populate_rpm_ts(mock.Mock())
        rpm_ts.assert_has_calls([mock.call.addInstall(None, ts._tsis[0], 'i'),
                                 mock.call.addInstall(None, ts._tsis[1], 'u')])

class RPMProbFilters(tests.support.TestCase):

    @mock.patch('dnf.rpmUtils.transaction.TransactionWrapper')
    def test_filters_install(self, mock_ts):
        self.base = tests.support.BaseCliStub()
        self.base._sack = tests.support.mock_sack('main', 'search')
        self.base._goal = dnf.goal.Goal(self.base.sack)
        self.base.install("lotus")
        ts = self.base.ts
        ts.setProbFilter.assert_called_with(rpm.RPMPROB_FILTER_OLDPACKAGE)

    @mock.patch('dnf.rpmUtils.transaction.TransactionWrapper')
    def test_filters_downgrade(self, ts):
        self.base = tests.support.BaseCliStub()
        self.base._sack = tests.support.mock_sack('main', 'old_versions')
        self.base._goal = dnf.goal.Goal(self.base.sack)
        self.base.downgrade("tour")
        ts = self.base.ts
        ts.setProbFilter.assert_called_with(rpm.RPMPROB_FILTER_OLDPACKAGE)

    @mock.patch('dnf.rpmUtils.transaction.TransactionWrapper')
    def test_filters_reinstall(self, ts):
        self.base = tests.support.BaseCliStub()
        self.base._sack = tests.support.mock_sack('main')
        self.base._goal = dnf.goal.Goal(self.base.sack)
        self.base.reinstall("librita")
        expected = rpm.RPMPROB_FILTER_REPLACEPKG |\
                   rpm.rpm.RPMPROB_FILTER_OLDPACKAGE
        self.base.ts.setProbFilter.assert_called_with(expected)

########NEW FILE########
__FILENAME__ = test_update
# Copyright (C) 2012-2014  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf
import dnf.goal
import dnf.util
import itertools
import tests.test_repo

class Update(support.ResultTestCase):
    def test_update(self):
        """ Simple update. """
        base = support.MockBase("updates")
        ret = base.upgrade("pepper")
        new_versions = base.sack.query().upgrades().filter(name="pepper")
        other_installed = base.sack.query().installed().filter(name__neq="pepper")
        expected = other_installed.run() + new_versions.run()
        self.assertResult(base, expected)

    def test_update_not_found(self):
        base = dnf.Base()
        base._sack = support.mock_sack('updates')
        base._goal = goal = mock.create_autospec(dnf.goal.Goal)

        with self.assertRaises(dnf.exceptions.MarkingError) as context:
            base.upgrade('non-existent')
        self.assertEqual(context.exception.pkg_spec, 'non-existent')
        self.assertEqual(goal.mock_calls, [])

    def test_update_not_installed(self):
        """ Updating an uninstalled package is a void operation. """
        base = support.MockBase("main")
        # no "mrkite" installed:
        base.upgrade("mrkite")
        self.assertResult(base, base.sack.query().installed().run())

    def test_update_all(self):
        """ Update all you can. """
        base = support.MockBase("main", "updates")
        sack = base.sack
        base.upgrade_all()
        expected = support.installed_but(sack, "pepper", "hole", "tour") + \
            list(sack.query().available().nevra("pepper-20-1.x86_64")) + \
            list(sack.query().available().nevra("hole-2-1.x86_64"))
        self.assertResult(base, expected)

    def test_upgrade_all_reponame(self):
        """Test whether only packages in selected repo are upgraded."""
        base = support.MockBase('updates', 'third_party')
        base.init_sack()

        base.upgrade_all('third_party')

        self.assertResult(base, itertools.chain(
            base.sack.query().installed().filter(name__neq='hole'),
            base.sack.query().upgrades().filter(reponame='third_party')))

    def test_upgrade_to_package(self):
        base = support.MockBase()
        pkg = base.add_remote_rpm(support.TOUR_51_PKG_PATH)
        cnt = base.package_upgrade(pkg)
        self.assertEqual(cnt, 1)
        new_pkg = base.sack.query().available().filter(name="tour")[0]
        new_set = support.installed_but(base.sack, "tour") + [new_pkg]
        self.assertResult(base, new_set)

    def test_update_arches(self):
        base = support.MockBase("main", "updates")
        base.upgrade("hole")
        installed, removed = self.installed_removed(base)
        self.assertItemsEqual(map(str, installed), ['hole-2-1.x86_64'])
        self.assertItemsEqual(map(str, removed),
                              ['hole-1-1.x86_64', 'tour-5-0.noarch'])

    def test_upgrade_reponame(self):
        """Test whether only packages in selected repo are upgraded."""
        base = support.MockBase('updates', 'broken_deps')
        base.logger = mock.Mock()

        base.upgrade('*e*', 'broken_deps')

        installed, removed = self.installed_removed(base)
        # Sack contains two upgrades with the same version. Because of that
        # test whether the installed package is one of those packages.
        self.assertLength(installed, 1)
        self.assertIn(
            dnf.util.first(installed),
            base.sack.query().upgrades().filter(name='pepper'))
        self.assertItemsEqual(
            removed,
            base.sack.query().installed().filter(name='pepper'))
        assert dnf.subject.Subject('*e*').get_best_query(base.sack).upgrades().filter(name__neq='pepper', reponame__neq='broken_deps'), \
               ('in another repo, there must be another update matching the '
                'pattern, otherwise the test makes no sense')

    def test_upgrade_reponame_not_in_repo(self):
        """Test whether no packages are upgraded if bad repo is selected."""
        base = support.MockBase('updates', 'broken_deps')

        with self.assertRaises(dnf.exceptions.MarkingError) as context:
            base.upgrade('hole', 'broken_deps')

        self.assertEqual(context.exception.pkg_spec, 'hole')
        self.assertResult(base, base.sack.query().installed())
        assert dnf.subject.Subject('hole').get_best_query(base.sack).upgrades().filter(reponame__neq='broken_deps'), \
               ('in another repo, there must be an update matching the '
                'pattern, otherwise the test makes no sense')

class SkipBroken(support.ResultTestCase):
    def setUp(self):
        self.base = support.MockBase("broken_deps")
        self.sack = self.base.sack

    def test_upgrade_all(self):
        """ upgrade() without parameters upgrade everything it can that has its
            deps in trim. Broken packages are silently skipped.
        """
        self.base.upgrade_all()
        new_set = support.installed_but(self.sack, "pepper").run()
        new_set.extend(self.sack.query().available().nevra("pepper-20-1.x86_64"))
        self.assertResult(self.base, new_set)

class CostUpdate(tests.test_repo.RepoTestMixin, support.ResultTestCase):
    def test_cost(self):
        """Test the repo costs are respected."""
        r1 = self.build_repo('r1')
        r2 = self.build_repo('r2')
        r1.cost = 500
        r2.cost = 700

        base = support.MockBase()
        base.init_sack()
        base.repos.add(r1)
        base.repos.add(r2)
        base._add_repo_to_sack('r1')
        base._add_repo_to_sack('r2')
        base.upgrade("tour")
        (installed, _) = self.installed_removed(base)
        self.assertEqual('r1', dnf.util.first(installed).reponame)

########NEW FILE########
__FILENAME__ = test_upgrade_to
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
import dnf
import itertools

class UpgradeTo(support.ResultTestCase):
    def test_upgrade_to(self):
        base = support.MockBase("main", "updates")
        sack = base.sack
        base.upgrade_to("pepper-20-1.x86_64")
        new_set = support.installed_but(sack, "pepper").run()
        q = sack.query().available().nevra("pepper-20-1.x86_64")
        new_set.extend(q)
        self.assertResult(base, new_set)

    def test_upgrade_to_reponame(self):
        """Test whether only packages in selected repo are used."""
        base = support.MockBase('updates', 'third_party')
        base.init_sack()

        base.upgrade_to('hole-1-2.x86_64', 'updates')

        self.assertResult(base, itertools.chain(
            base.sack.query().installed().filter(name__neq='hole'),
            dnf.subject.Subject('hole-1-2.x86_64').get_best_query(base.sack).filter(reponame='updates')))

    def test_upgrade_to_reponame_not_in_repo(self):
        """Test whether no packages are upgraded if bad repo is selected."""
        base = support.MockBase('main', 'updates')
        base.init_sack()

        base.upgrade_to('hole-1-2.x86_64', 'main')

        self.assertResult(base, base.sack.query().installed())

########NEW FILE########
__FILENAME__ = test_util
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf.util

class Slow(object):
    def __init__(self, val):
        self._val = val
        self.computed = 0

    def new_val(self, val):
        self._val = val
        del self._square1
        del self._square2

    @dnf.util.lazyattr("_square1")
    def square1(self):
        self.computed += 1
        return self._val * self._val

    @property
    @dnf.util.lazyattr("_square2")
    def square2(self):
        self.computed += 1
        return self._val * self._val

class Util(support.TestCase):
    def test_am_i_root(self):
        with mock.patch('os.geteuid', return_value=1001):
            self.assertFalse(dnf.util.am_i_root())
        with mock.patch('os.geteuid', return_value=0):
            assert(dnf.util.am_i_root())

    def test_bunch(self):
        b = dnf.util.Bunch()
        self.assertRaises(AttributeError, lambda: b.more)
        b.garden = 'weeds'
        self.assertEqual(b['garden'], 'weeds')
        b['digging'] = 4
        self.assertEqual(b.digging, 4)

    def test_empty(self):
        self.assertTrue(dnf.util.empty(()))
        self.assertFalse(dnf.util.empty([1,2,3]))
        self.assertTrue(dnf.util.empty((x for x in [])))
        self.assertTrue(dnf.util.empty(iter([])))
        self.assertFalse(dnf.util.empty((x for x in [2, 3])))

    def test_file_timestamp(self):
        stat = mock.Mock()
        stat.st_mtime = 123
        with mock.patch('os.stat', return_value=stat):
            self.assertEqual(dnf.util.file_timestamp("/yeah"), 123)
        self.assertRaises(OSError, dnf.util.file_timestamp, "/does/not/ex1st")

    def test_first(self):
        self.assertEqual(dnf.util.first([5, 4, 3]), 5)
        ge = (x for x in range(5, 8))
        self.assertEqual(dnf.util.first(ge), 5)
        self.assertEqual(dnf.util.first([]), None)

        def generator():
            if False:
                yield 10
        self.assertEqual(dnf.util.first(generator()), None)

    def test_group_by_filter(self):
        self.assertEqual(dnf.util.group_by_filter(lambda x: x % 2, range(5)),
                         ([1, 3], [0, 2, 4]))
        self.assertEqual(dnf.util.group_by_filter(lambda x: x, range(5)),
                         ([1, 2, 3, 4], [0]))

    def test_insert_if(self):
        """Test insert_if with sometimes fulfilled condition."""
        item = object()
        iterable = range(4)
        condition = lambda item: item % 2 == 0

        iterator = dnf.util.insert_if(item, iterable, condition)

        self.assertEqual(next(iterator), item)
        self.assertEqual(next(iterator), 0)
        self.assertEqual(next(iterator), 1)
        self.assertEqual(next(iterator), item)
        self.assertEqual(next(iterator), 2)
        self.assertEqual(next(iterator), 3)
        self.assertRaises(StopIteration, next, iterator)

    def test_is_exhausted_true(self):
        """Test is_exhausted with an iterator which is exhausted."""
        iterator = iter(())

        result = dnf.util.is_exhausted(iterator)

        self.assertTrue(result)

    def test_is_exhausted_false(self):
        """Test is_exhausted with an iterator which is not exhausted."""
        iterator = iter((1,))

        result = dnf.util.is_exhausted(iterator)

        self.assertFalse(result)

    def test_is_glob_pattern(self):
        assert(dnf.util.is_glob_pattern("all*.ext"))
        assert(dnf.util.is_glob_pattern("all?.ext"))
        assert(not dnf.util.is_glob_pattern("not.ext"))

    def test_lazyattr(self):
        slow = Slow(12)

        self.assertEqual(slow.computed, 0)
        self.assertEqual(slow.square1(), 144)
        self.assertEqual(slow.computed, 1)
        self.assertEqual(slow.square1(), 144)
        self.assertEqual(slow.square1(), 144)
        self.assertEqual(slow.computed, 1)

        self.assertEqual(slow.square2, 144)
        self.assertEqual(slow.computed, 2)
        self.assertEqual(slow.square2, 144)
        self.assertEqual(slow.computed, 2)

        slow.new_val(13)
        self.assertEqual(slow.square1(), 169)
        self.assertEqual(slow.square2, 169)
        self.assertEqual(slow.computed, 4)

    def test_mapall(self):
        l = [1, 2, 3]
        out = dnf.util.mapall(lambda n: 2 * n, l)
        self.assertIsInstance(out, list)
        self.assertEqual(out, [2, 4, 6])

    def test_partition(self):
        l = list(range(6))
        smaller, larger = dnf.util.partition(lambda i: i > 4, l)
        self.assertItemsEqual(smaller, (0, 1, 2, 3, 4))
        self.assertItemsEqual(larger, (5,))

    def test_split_by(self):
        """Test split_by with sometimes fulfilled condition."""
        iterable = range(7)
        condition = lambda item: item % 3 == 0

        iterator = dnf.util.split_by(iterable, condition)

        self.assertEqual(next(iterator), ())
        self.assertEqual(next(iterator), (0, 1, 2))
        self.assertEqual(next(iterator), (3, 4, 5))
        self.assertEqual(next(iterator), (6,))
        self.assertRaises(StopIteration, next, iterator)

    def test_split_by_empty(self):
        """Test split with empty iterable."""
        iterable = []
        condition = lambda item: item % 3 == 0

        iterator = dnf.util.split_by(iterable, condition)

        self.assertEqual(next(iterator), ())
        self.assertRaises(StopIteration, next, iterator)

    def test_strip_prefix(self):
        self.assertIsNone(dnf.util.strip_prefix("razorblade", "blade"))
        self.assertEqual(dnf.util.strip_prefix("razorblade", "razor"), "blade")

    def test_touch(self):
        self.assertRaises(OSError, dnf.util.touch,
                          support.NONEXISTENT_FILE, no_create=True)

########NEW FILE########
__FILENAME__ = test_yumdb
# Copyright (C) 2012-2013  Red Hat, Inc.
#
# This copyrighted material is made available to anyone wishing to use,
# modify, copy, or redistribute it subject to the terms and conditions of
# the GNU General Public License v.2, or (at your option) any later version.
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY expressed or implied, including the implied warranties of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.  You should have received a copy of the
# GNU General Public License along with this program; if not, write to the
# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the
# source code or documentation are not subject to the GNU General Public
# License and may only be used or replicated with the express permission of
# Red Hat, Inc.
#

from __future__ import absolute_import
from __future__ import unicode_literals
from tests import support
from tests.support import mock

import dnf.yum.rpmsack
import unittest


@mock.patch('os.path.exists', return_value=True)
class TestAdditionalPkgDB(unittest.TestCase):
    def _instantiate(self, base):
        path = base.conf.persistdir + '/yumdb'
        return dnf.yum.rpmsack.AdditionalPkgDB(path)

    def test_get_dir(self, mock_exists):
        base = support.MockBase()
        pkgdb = self._instantiate(base)
        pkg = base.sack.query().installed().filter(name="pepper")[0]
        expected = '%s/yumdb/p/bad9-pepper-20-0-x86_64' % base.conf.persistdir

        directory = pkgdb._get_dir_name(pkg.pkgtup, b'bad9')
        self.assertEqual(expected, directory)
        directory = pkgdb._get_dir_name(pkg.pkgtup, 'bad9')
        self.assertEqual(expected, directory)
        directory = pkgdb._get_dir_name(pkg.pkgtup, None)
        self.assertEqual('%s/yumdb/p/<nopkgid>-pepper-20-0-x86_64' %
                         base.conf.persistdir, directory)

########NEW FILE########

__FILENAME__ = accounts
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Accounts controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import get_host_record, host_title_maker, host_a_maker, create_hostfilter_query
from skaldship.passwords import process_password_file, process_cracked_file, process_mass_password, insert_or_update_acct
import re
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir

@auth.requires_login()
def index():
    return dict()

##-------------------------------------------------------------------------
## accounts
##-------------------------------------------------------------------------

@auth.requires_signature()
@auth.requires_login()
def compromise():
    """Toggle compromised true/false"""
    compr_count = 0
    uncompr_count = 0
    if request.vars.has_key('ids'):
        for z in request.vars.ids.split('|'):
            if z is not '':
                flag = db.t_accounts[z].f_compromised
                if flag:
                    db.t_accounts[z].f_compromised = False
                    uncompr_count += 1
                else:
                    db.t_accounts[z].f_compromised = False
                    compr_count += 1
        db.commit()
        cache.ram.clear('accounts_list')
        response.flash = '%s compromised / %s uncompromised' % (compr_count, uncompr_count)
        response.js = 'accounttable.fnReloadAjax();'
    return

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    if request.vars.has_key('ids'):
        for z in request.vars.ids.split('|'):
            if z is not '' or z is not None:
                db(db.t_accounts.id == z).delete()
                count += 1
        db.commit()
        msg = '%s Account(s) deleted' % (count)
        cache.ram.clear('accounts_list')
        response.js = 'accounttable.fnReloadAjax();'
    else:
        msg = "No Account IDs sent for deletion"
    response.flash = msg
    return dict(msg=msg)

@auth.requires_login()
def add():
    if request.vars.has_key('id'):
        host_id = db.t_hosts[request.vars.id] or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    else:
        host_id = None

    if host_id:
        # grab services for a host
        services = db(db.t_services.f_hosts_id == host_id.id).select(cache=(cache.ram,30))
        svc_set = []
        for svc in services:
            svc_set.append([svc.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[svc.f_hosts_id]), svc.f_proto, svc.f_number)])
        db.t_accounts.f_services_id.requires = IS_IN_SET(svc_set)
        form=crud.create(db.t_accounts,message='Account added',next=URL('accounts_create', vars={'id': host_id.id}))
        db.t_accounts.f_services_id.requires = None
    else:
        form=crud.create(db.t_accounts, next='edit/[id]',message="Account added")
    cache.ram.clear('accounts_list')
    response.title = "%s :: Add Account" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def view():
    record = db.t_accounts(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Account record not found')}))
    form=crud.read(db.t_accounts,record)
    return dict(form=form)

@auth.requires_login()
def edit():
    record = db.t_accounts(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Account record not found')}))
    service = db(db.t_services.id == record.f_services_id).select().first()
    services = db(db.t_services.f_hosts_id == service.f_hosts_id).select()
    svc_set = []
    for svc in services:
        svc_set.append([svc.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[svc.f_hosts_id]), svc.f_proto, svc.f_number)])
    db.t_accounts.f_services_id.requires = IS_IN_SET(svc_set)
    form=crud.update(db.t_accounts,record,next='edit/[id]',
                     ondelete=lambda form: redirect(URL('list')))
    db.t_accounts.f_services_id.requires = None
    hosttitle = "%s :: %s/%s" % (host_title_maker(db.t_hosts[service.f_hosts_id]), service.f_proto, service.f_number)
    response.title = "%s :: Update Account :: %s :: %s" % (settings.title, record.f_username, hosttitle)
    return dict(form=form)

@auth.requires_login()
def accounts_grid():
    response.title = "%s :: Accounts" % (settings.title)
    if session.hostfilter is None:
        session.hostfilter = [(None, None), False]

    query = (db.t_accounts.id > 0)  & (db.t_accounts.f_services_id== db.t_services.id)
    query = create_hostfilter_query(session.hostfilter, query, 't_services')
    columns = [
        db.t_hosts.f_ipv4, db.t_hosts.f_hostname, db.t_services.f_proto, db.t_services.f_number,
        db.t_accounts.f_fullname, db.t_accounts.f_domain, db.t_accounts.f_password,
        db.t_accounts.f_hash1_type, db.t_accounts.f_hash1, db.t_accounts.f_hash2_type,
        db.t_accounts.f_hash2, db.t_accounts.f_uid, db.t_accounts.f_gid,
        db.t_accounts.f_level, db.t_accounts.f_active, db.t_accounts.f_lockout,
        db.t_accounts.f_duration, db.t_accounts.f_source, db.t_accounts.f_message,
        db.t_accounts.f_description, db.t_accounts.id,
    ]
    rows = SQLFORM.grid(query, columns, deletable=True, selectable=True, details=False, field_id=db.t_accounts.id)

    return dict(rows=rows)

@auth.requires_login()
def csv():
    """
    Download account data in CSV format
    """
    import cStringIO
    q = (db.t_accounts.f_services_id==db.t_services.id) & (db.t_services.f_hosts_id==db.t_hosts.id)
    if request.vars.has_key('type'):
        q &= ((db.t_accounts.f_hash1_type == request.vars.type) | (db.t_accounts.f_hash1_type == request.vars.type))
    if request.vars.has_key('username'):
        q &= (db.t_accounts.f_username == request.vars.username)

    accts = db(q).select(
        db.t_hosts.f_ipv4, db.t_services.f_proto, db.t_services.f_number,
        db.t_accounts.f_services_id, db.t_accounts.id, db.t_accounts.f_username,
        db.t_accounts.f_password, db.t_accounts.f_compromised,
        db.t_accounts.f_hash1, db.t_accounts.f_hash1_type,
        db.t_accounts.f_hash2, db.t_accounts.f_hash2_type,
        db.t_accounts.f_uid, db.t_accounts.f_gid, db.t_accounts.f_level,
        db.t_accounts.f_source, db.t_accounts.f_message, db.t_accounts.f_description
    )
    s = cStringIO.StringIO()
    accts.export_to_csv_file(s)
    return s.getvalue()

@auth.requires_login()
def list():
    response.title = "%s :: Accounts" % (settings.title)
    # if no filter is set then we blank it out
    if session.hostfilter is None:
        session.hostfilter = [(None, None), False]

    if request.extension == 'json':
        query = (db.t_accounts.id > 0) & (db.t_accounts.f_services_id== db.t_services.id)
        query = create_hostfilter_query(session.hostfilter, query, 't_services')
        if request.vars.hash_type is not None:
            query &= ((db.t_accounts.f_hash1_type == request.vars.hash_type) | (db.t_accounts.f_hash2_type == request.vars.hash_type))

        if request.vars.has_key('iDisplayStart'):
            start = int(request.vars.iDisplayStart)
        else:
            start = 0
        if request.vars.has_key('iDisplayLength'):
            if request.vars.iDisplayLength == '-1':
                limit = db(query).count()
            else:
                limit = start + int(request.vars.iDisplayLength)
        else:
            limit = int(auth.user.f_show_size)

        srch_data = request.vars.get('sSearch')
        if srch_data:
            # sSearch global search box

            # parse the search into fields (port:num proto:tcp etc)
            srch_vals = [
                ["port", db.t_services.f_number],
                ["proto", db.t_services.f_proto],
                ["user", db.t_accounts.f_username],
                ["name", db.t_accounts.f_fullname],
                ["domain", db.t_accounts.f_domain],
                ["hash", db.t_accounts.f_hash1],
                ["hash1", db.t_accounts.f_hash1],
                ["hash2", db.t_accounts.f_hash2],
                ["htype", db.t_accounts.f_hash1_type],
                ["uid", db.t_accounts.f_uid],
                ["gid", db.t_accounts.f_gid],
                ["level", db.t_accounts.f_level],
                ["source", db.t_accounts.f_source],
                ["desc", db.t_accounts.f_description],
                ["msg", db.t_accounts.f_message],
                ["ip", db.t_hosts.f_ipv4],
                ["ipv4", db.t_hosts.f_ipv4],
                ["ipv6", db.t_hosts.f_ipv6],
                ["hostname", db.t_hosts.f_hostname],
            ]

            parsed = False
            for val in srch_vals:
                srch_str = "%s:(?P<f>\w+)" % val[0]
                srch_res = re.findall(srch_str, srch_data)
                for res in srch_res:
                    parsed = True
                    if val[0] in ['source', 'desc', 'hostname']:
                        query &= (val[1].upper().contains(res.upper()))
                    else:
                        query &= (val[1].upper() == res.upper())

            if not parsed:
                query &= db.t_accounts.f_username.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_accounts.f_password.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_accounts.f_fullname.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_accounts.f_domain.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_accounts.f_hash1.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_accounts.f_hash2.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_accounts.f_source.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_accounts.f_message.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_accounts.f_description.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_hosts.f_ipv4.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_hosts.f_ipv6.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_hosts.f_hostname.like("%%%s%%" % request.vars.sSearch)

            #total_count = db.t_vulndata.id.count()
        if request.vars.iSortingCols == '1':
            # sorting by a column - this is a little trippy because tuples start at 0
            # and datatables starts at 1 so we have to subtract 1 from iSortCol_0
            cols = (
                db.t_accounts.f_compromised,
                db.t_hosts.f_ipv4,
                db.t_services.f_number,
                db.t_accounts.f_username,
                db.t_accounts.f_fullname,
                db.t_accounts.f_domain,
                db.t_accounts.f_password,
                db.t_accounts.f_hash1_type,
                db.t_accounts.f_hash1,
                db.t_accounts.f_hash2_type,
                db.t_accounts.f_hash2,
                db.t_accounts.f_uid,
                db.t_accounts.f_gid,
                db.t_accounts.f_level,
                db.t_accounts.f_active,
                db.t_accounts.f_lockout,
                db.t_accounts.f_source,
                db.t_accounts.f_message,
                db.t_accounts.f_description
            )

            orderby = cols[int(request.vars.iSortCol_0) ]
            if request.vars.sSortDir_0 == 'asc':
                rows=db(query).select(
                    db.t_accounts.ALL,
                    db.t_hosts.id,
                    db.t_hosts.f_ipv4,
                    db.t_hosts.f_ipv6,
                    db.t_hosts.f_hostname,
                    db.t_services.f_proto,
                    db.t_services.f_number,
                    orderby=orderby,
                    limitby=(start, limit),
                    cache=(cache.with_prefix(cache.ram, "accounts_list"), 180))
            else:
                rows=db(query).select(
                    db.t_accounts.ALL,
                    db.t_hosts.id,
                    db.t_hosts.f_ipv4,
                    db.t_hosts.f_ipv6,
                    db.t_hosts.f_hostname,
                    db.t_services.f_proto,
                    db.t_services.f_number,
                    orderby=~orderby,
                    limitby=(start, limit),
                    cache=(cache.with_prefix(cache.ram, "accounts_list"), 180))
        else:
            rows=db(query).select(
                db.t_accounts.ALL,
                db.t_hosts.id,
                db.t_hosts.f_ipv4,
                db.t_hosts.f_ipv6,
                db.t_hosts.f_hostname,
                db.t_services.f_proto,
                db.t_services.f_number,
                limitby=(start,limit),
                cache=(cache.with_prefix(cache.ram, "accounts_list"), 180))

        #rows=db(q).select(
        #    db.t_accounts.ALL,
        #    db.t_hosts.id,
        #    db.t_hosts.f_ipv4,
        #    db.t_hosts.f_ipv6,
        #    db.t_hosts.f_hostname,
        #    db.t_services.f_proto,
        #    db.t_services.f_number,
        #    #cache=(cache.ram,60)
        #)

        aaData = []
        # datatable formatting is specific
        for r in rows:
            atxt = {}
            if r.t_accounts.f_compromised == True:
                atxt['0'] = '<div class="acct_compromised" name="row_id" id="%s"><span class="icon-check"></span></div>' % r.t_accounts.id
            else:
                atxt['0'] = '<div class="acct_uncompromised" name="row_id" id="%s"/>' % r.t_accounts.id

            #svc = db.t_services[r.f_services_id]

            atxt['1'] = host_a_maker(r.t_hosts).xml()
            atxt['2'] = "%s/%s" % (r.t_services.f_proto, r.t_services.f_number)
            atxt['3'] = DIV(A(I(_class="icon-pencil", _style="display: inline-block;"),
                               _target="accounts_update_%s" % (r.t_accounts.id), \
                               _href=URL('edit.html', args=r.t_accounts.id), \
                               ),
                             A("%s" % (r.t_accounts.f_username), \
                               _target="_blank", _id="username", \
                               _href=URL("by_username", vars={'username':r.t_accounts.f_username}, extension="html"), \
                               )
                             ).xml()
            atxt['4'] = r.t_accounts.f_fullname
            atxt['5'] = r.t_accounts.f_domain
            atxt['6'] = r.t_accounts.f_password
            atxt['7'] = r.t_accounts.f_hash1_type
            atxt['8'] = r.t_accounts.f_hash1
            atxt['9'] = r.t_accounts.f_hash2_type
            atxt['10'] = r.t_accounts.f_hash2
            atxt['11'] = r.t_accounts.f_uid
            atxt['12'] = r.t_accounts.f_gid
            atxt['13'] = r.t_accounts.f_level
            atxt['14'] = r.t_accounts.f_active
            atxt['15'] = r.t_accounts.f_source
            atxt['16'] = r.t_accounts.f_message
            atxt['17'] = r.t_accounts.f_description
            atxt['DT_RowId'] = str(r.t_accounts.id)

            aaData.append(atxt)

        result = {
            'sEcho': request.vars.sEcho,
            'iTotalDisplayRecords': db(query).count(),
            'iTotalRecords': db(db.t_accounts).count(),
            'aaData': aaData,
            'query': db._lastsql,
        }

        return result

    rows=db(db.t_accounts).select(db.t_accounts.f_hash1_type, groupby=db.t_accounts.f_hash1_type, cache=(cache.ram, 60))
    hash_types=[]
    for r in rows:
        #if r.f_hash2_type is not None:
        #    hash_types.append("%s/%s" % (r.f_hash1_type, r.f_hash2_type))
        #else:
            hash_types.append(r.f_hash1_type)

    form = TABLE(THEAD(TR(TH(T('C'), _width="1%"),
                          TH(T('Host')),
                          TH(T('Port')),
                          TH(T('Username')),
                          TH(T('Fullname')),
                          TH(T('Domain')),
                          TH(T('Password')),
                          TH(T('Hash 1 Type')),
                          TH(T('Hash 1')),
                          TH(T('Hash 2 Type')),
                          TH(T('Hash 2')),
                          TH(T('UID')),
                          TH(T('GID')),
                          TH(T('Level')),
                          TH(T('Active')),
                          TH(T('Source')),
                          TH(T('Message')),
                          TH(T('Description')),
                          )  ),
                 _class="datatable",
                 _id="accounttable",
                 _style="width:100%")

    add = AddModal(
        db.t_accounts, 'Add', 'Add', 'Add Account',
        #fields=[],
        cmd='accounttable.fnReloadAjax();'
    )
    services = db(db.t_services.f_hosts_id > 0).select(cache=(cache.ram,30))

    svc_set = []
    for svc in services:
        svc_set.append([svc.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[svc.f_hosts_id]), svc.f_proto, svc.f_number)])
    db.t_accounts.f_services_id.requires = IS_IN_SET(svc_set)
    db.t_accounts.id.comment = add.create()

    return dict(form=form, hash_types=hash_types, add=add)

@auth.requires_login()
def check_john_pot():
    """
    Checks all passwords against the john.pot file specified by the
    user. Any hash found in the database has their cleartext assigned.

    Any entry with f_password not None is skipped.

    For NTLM it first checks NTLM. If not found it will split the
    LM into two and look up each side.

    NOTE: This does NOT call john the ripper, it only loads the POT file.

    XXX: This needs to be debugged/tested.. It was a 3am code writing binge!
    """
    import os

    if request.extension == "load":
        buttons=[]
    else:
        buttons=['submit']

    known_paths = []
    known_paths.append([0, None])
    known_paths.append([1, '/opt/metasploit_pro/apps/pro/engine/config/john.pot'])
    known_paths.append([2, '/root/.john/john.pot'])

    form = SQLFORM.factory(
        Field('potfile', 'upload', uploadfolder=os.path.join(request.folder, settings.password_upload_dir), label=T('POT File')),
        Field('common_paths', 'string', default="0", requires=IS_IN_SET(known_paths), label=T('Known paths')),
        Field('other_file', 'string', label=T('John.pot File Location')),
        buttons=buttons, _action=URL('accounts', 'check_john_pot'), _id='john_pot',
    )

    if form.errors:
        response.flash = 'Error in form'
        return TABLE(*[TR(k, v) for k, v in form.errors.items()])
    elif form.accepts(request.vars, session):
        if form.vars.potfile:
            potfile = os.path.join(request.folder, settings.password_upload_dir, form.vars.potfile)
        elif form.vars.other_file:
            potfile = request.vars.other_file
        else:
            potfile = known_paths[int(form.vars.common_paths)][1]

        from skaldship.jtr import JohnPot, ntpwchk

        try:
            logger.info("Loading %s ..." % (potfile))
            potdata = JohnPot()
            potdata.load(potfile)
        except Exception, e:
            response.flash = "Error loading %s: %s" % (potfile, e)
            return dict(form=form)

        # Clear out any uncracked LM hashes:
        # db(db.t_accounts.f_password.contains("???????")).update(f_password=None, f_compromised=False)

        query = (db.t_accounts.f_hash1 <> None) & (db.t_accounts.f_password == None)
        query |= (db.t_accounts.f_password.contains('???????'))
        uncracked = db(query).select(cache=(cache.ram, 60))
        update_count = 0

        for acct in uncracked:
            # check for no password!
            if acct.f_hash1.upper() == 'AAD3B435B51404EEAAD3B435B51404EE' and acct.f_hash2.upper() == '31D6CFE0D16AE931B73C59D7E0C089C0':
                acct.update_record(f_password='', f_compromised=True)
                logger.info("Found blank LM/NTLM for %s" % (acct.f_username))
                db.commit()
                update_count += 1
                continue

            if acct.f_hash1_type == "LM":
                # we may have a LANMAN, look up the NTLM first
                pw = potdata.search("$NT$%s" % (acct.f_hash2.upper()))
                if pw is not None:
                    acct.update_record(f_password=pw, f_compromised=True)
                    logger.info("Found NTLM for %s: %s" % (acct.f_username, pw))
                    db.commit()
                    update_count += 1
                    continue
                else:
                    # skip over no password and NTLM not found
                    if acct.f_hash1 == "NO PASSWORD*********************":
                        continue
                    # No NTLM, lets split up the LM and append the $LM$
                    # and upper case the hash
                    pwhash = acct.f_hash1.upper()
                    if len(pwhash) != 32:
                        logger.warning("Error: You say you have an LM hash but it's not 32 characters: %s" % (acct.f_hash1))
                        continue
                    pw1 = potdata.search("$LM$%s" % (pwhash[0:16]))
                    if pw1 is None:
                        pw1 = "???????"
                    pw2 = potdata.search("$LM$%s" % (pwhash[16:]))
                    if pw2 is None:
                        pw2 = "???????"
                    pw = "%s%s" % (pw1, pw2)
                    msg = "Found LM for %s: %s" % (acct.f_username, pw)
                    if pw != "??????????????":
                        if "???????" in pw:
                            acct.update_record(f_password=pw, f_compromised=False)
                        else:
                            # case permute the password and check the NTLM
                            try:
                                (status, newpw) = ntpwchk(pw, acct.f_hash1, acct.f_hash2)
                            except Exception:
                                status = None
                            if status:
                                pw = newpw
                                acct.update_record(f_password=pw, f_compromised=True)
                                msg = "Permuted NTLM for %s: %s" % (acct.f_username, pw)
                        db.commit()
                        logger.info(msg)
                        update_count += 1

            else:
                # lookup everything else
                pw = potdata.search(acct.f_hash1)

                if pw is not None:
                    acct.update_record(f_password=pw, f_compromised=True)
                    db.commit()
                    logger.info("Found password for %s: %s" % (acct.f_username, pw))
                    update_count += 1

        response.flash = "%s accounts updated with passwords" % (update_count)
        cache.ram.clear('accounts_list')

    response.title = "%s :: Process john.pot File" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def by_host():
    """
    Returns a list of services + serviceinfo based upon an host identifier
    (id, ipv4, ipv6)
    """
    record = get_host_record(request.args(0))
    if record is None:
        redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    response.title = "%s :: Accounts for %s" % (settings.title, host_title_maker(record))
    query = (db.t_services.f_hosts_id == record.id)

    if request.extension == "json":
        aaData = []
        rows = db(query).select(db.t_accounts.ALL, db.t_services.ALL, left=db.t_services.on(db.t_accounts.f_services_id==db.t_services.id))
        for r in rows:
            if r.t_accounts.f_compromised == True:
                comprdiv = '<div class="acct_compromised" name="row_id" id="%s"><span class="icon-check"></span></div>' % r.t_accounts.id
            else:
                comprdiv = '<div class="acct_uncompromised" name="row_id" id="%s"/>' % r.t_accounts.id

            aaData.append({
                '0': A("edit", _target="accounts_update_%s" % (r.t_accounts.id), _href=URL('accounts', 'edit', args=[r.t_accounts.id], extension='html')).xml(),
                '1': comprdiv,
                '2': A("%s/%s" % (r.t_services.f_proto, r.t_services.f_number), _target="services_edit_%s" % (r.t_services.id), _href=URL('services', 'edit', args=[r.t_services.id], extension='html')).xml(),
                '3': A(r.t_accounts.f_username, _target="accounts_username_%s" % (r.t_accounts.f_username), _href=URL('accounts', 'by_username', vars={'username': r.t_accounts.f_username}, extension='html')).xml(),
                '4': r.t_accounts.f_fullname,
                '5': r.t_accounts.f_password,
                '6': r.t_accounts.f_hash1_type,
                '7': r.t_accounts.f_hash1,
                '8': r.t_accounts.f_hash2_type,
                '9': r.t_accounts.f_hash2,
                '10': r.t_accounts.f_uid,
                '11': r.t_accounts.f_gid,
                '12': r.t_accounts.f_lockout,
                '13': r.t_accounts.f_duration,
                '14': r.t_accounts.f_source,
                '15': r.t_accounts.f_level,
                '16': r.t_accounts.f_description,
                'DT_RowId': r.t_accounts.id,
            } )

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }

        return result

    form = TABLE(THEAD(TR(TH(T(''), _width="5%"),
                          TH(T('Compr'), _width="5%"),
                          TH(T('Port')),
                          TH(T('Username')),
                          TH(T('Fullname')),
                          TH(T('Password')),
                          TH(T('Hash 1 Type')),
                          TH(T('Hash 1')),
                          TH(T('Hash 2 Type')),
                          TH(T('Hash 2')),
                          TH(T('UID')),
                          TH(T('GID')),
                          TH(T('Lockout')),
                          TH(T('Duration')),
                          TH(T('Source')),
                          TH(T('Level')),
                          TH(T('Description')),
                          )  ),
                 _class="datatable",
                 _id="accounttable",
                 _style="width:100%")

    add = AddModal(
        db.t_accounts, 'Add', 'Add', 'Add Account',
        #fields=[],
        cmd='accounttable.fnReloadAjax();'
    )
    services = db(db.t_services.f_hosts_id == record.id).select(cache=(cache.ram,30))
    svc_set = []
    for svc in services:
        svc_set.append([svc.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[svc.f_hosts_id]), svc.f_proto, svc.f_number)])
    db.t_accounts.f_services_id.requires = IS_IN_SET(svc_set)
    db.t_accounts.id.comment = add.create()

    return dict(form=form, host=record, add=add)

@auth.requires_login()
def by_username():

    if request.vars.get('username', 'None') != 'None':
        record = None
        response.title = "%s :: Account entries for %s" % (settings.title, request.vars.username)
        query = (db.t_accounts.f_username.like(request.vars.username))
    else:
        query = (db.t_accounts.active)

    aaData = []
    rows = db(query).select(db.t_accounts.ALL, db.t_hosts.ALL, db.t_services.ALL, left=(db.t_services.on(db.t_accounts.f_services_id==db.t_services.id), db.t_hosts.on(db.t_hosts.id == db.t_services.f_hosts_id)))
    for r in rows:
        atxt=[]
        hostrec=db.t_hosts[r.t_hosts.id]
        atxt.append(TD(A("edit", _target="accounts_update_%s" % (r.t_accounts.id), _href=URL('accounts', 'edit', args=[r.t_accounts.id], extension='html'))))
        atxt.append(TD(INPUT(_name="id", _value=str(r.t_accounts.id), _type="checkbox")))
        atxt.append(TD(A("%s" % (host_title_maker(hostrec)), _target="host_detail_%s" % (r.t_hosts.id), _href=URL('hosts', 'detail', args=[r.t_hosts.id]))))
        atxt.append(TD(A("%s/%s" % (r.t_services.f_proto, r.t_services.f_number), _target="services_edit_%s" % (r.t_services.id), _href=URL('services', 'edit',args=[r.t_services.id], extension='html'))))
        atxt.append(TD(r.t_accounts.f_username))
        atxt.append(TD(r.t_accounts.f_fullname))
        atxt.append(TD(r.t_accounts.f_password))
        atxt.append(TD(r.t_accounts.f_compromised))
        atxt.append(TD(r.t_accounts.f_hash1_type))
        atxt.append(TD(r.t_accounts.f_hash1))
        atxt.append(TD(r.t_accounts.f_hash2_type))
        atxt.append(TD(r.t_accounts.f_hash2))
        atxt.append(TD(r.t_accounts.f_uid))
        atxt.append(TD(r.t_accounts.f_gid))
        atxt.append(TD(r.t_accounts.f_lockout))
        atxt.append(TD(r.t_accounts.f_duration))
        atxt.append(TD(r.t_accounts.f_source))
        atxt.append(TD(r.t_accounts.f_level))
        atxt.append(TD(r.t_accounts.f_description))

        aaData.append(TR(atxt))

    form = TABLE(THEAD(TR(TH(T('ID'), _width="5%"),
                          TH(T(''), _width="2%"),
                          TH(T('Host')),
                          TH(T('Port')),
                          TH(T('Username')),
                          TH(T('Fullname')),
                          TH(T('Password')),
                          TH(T('Compromised')),
                          TH(T('Hash 1 Type')),
                          TH(T('Hash 1')),
                          TH(T('Hash 2 Type')),
                          TH(T('Hash 2')),
                          TH(T('UID')),
                          TH(T('GID')),
                          TH(T('Lockout')),
                          TH(T('Duration')),
                          TH(T('Source')),
                          TH(T('Level')),
                          TH(T('Description')),
                          )  ),
                 TBODY(aaData),
                 _class="datatable",
                 _id="accounttable",
                 _style="width:100%")

    return dict(form=form)

@auth.requires_login()
def duplicate_on_hosts():
    """
    Find duplicate accounts on multiple services per host
    """
    query = (db.t_accounts.f_services_id == db.t_services.id)
    query &= (db.t_services.f_hosts_id == db.t_hosts.id)

    columns = [
        db.t_hosts.f_ipv4, db.t_hosts.f_hostname, db.t_services.f_proto, db.t_services.f_number,
        db.t_accounts.f_username, db.t_accounts.f_domain, db.t_accounts.f_compromised,
        db.t_accounts.f_password, db.t_accounts.f_hash1_type, db.t_accounts.f_hash1,
        db.t_accounts.f_hash2_type, db.t_accounts.f_hash2, db.t_accounts.f_uid,
        db.t_accounts.f_gid, db.t_accounts.f_level, db.t_accounts.f_source,
        db.t_accounts.f_message, db.t_accounts.f_description, db.t_accounts.id
    ]
    rows = SQLFORM.grid(query, columns, deletable=True, selectable=True, details=False, field_id=db.t_accounts.id)
    #rows = SQLFORM.smartgrid(db.t_hosts, linked_tables = [ db.t_services, db.t_accounts ])
    return dict(rows=rows)

@auth.requires_login()
def import_file():
    """
    Import and parse password file into t_accounts
    """
    import os
    from skaldship.general import check_datadir
    check_datadir(request.folder)

    # Service_id is primary, host_id is secondary, if none then list
    # all the services
    svc_set = []
    url=URL('accounts', 'import_file')
    if request.vars.has_key('service_id'):
        try:
            record = db.t_services[request.vars.service_id]
            svc_set.append((record.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[record.f_hosts_id]), record.f_proto, record.f_number)))
            url = URL('accounts', 'import_file', vars={'service_id':request.vars.service_id})
        except:
            pass
    elif request.vars.has_key('host_id'):
        try:
            host_record = get_host_record(request.vars.host_id)
            svc_records = db(db.t_services.f_hosts_id == host_record.id).select(cache=(cache.ram, 30))
            url = URL('accounts', 'import_file', vars={'host_id':request.vars.host_id})
            for record in svc_records:
                svc_set.append((record.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[record.f_hosts_id]), record.f_proto, record.f_number)))
        except:
            pass

    if len(svc_set) == 0:
        # all services
        svc_records = db(db.t_services).select(cache=(cache.ram,30))
        svc_set = []
        for record in svc_records:
            svc_set.append((record.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[record.f_hosts_id]), record.f_proto, record.f_number)))

    if request.extension == "load":
        buttons=[]
    else:
        buttons=['submit']

    form = SQLFORM.factory(
        Field('f_service', 'string', label=T('Host / Service'), requires=IS_IN_SET(svc_set), default=svc_set[0][0]),
        Field('f_filename', 'upload', uploadfolder=os.path.join(request.folder, settings.password_upload_dir), label=T('Password file')),
        Field('f_type', 'string', label=T('File type'), default='PWDUMP', requires=IS_IN_SET(settings.password_file_types)),
        Field('f_source', 'string', label=T('Source (if necessary)')),
        Field('f_add_to_evidence', 'boolean', label=T('Add Evidence')),
        Field('f_taskit', type='boolean', default=True, label=T('Run in background task')),
        buttons=buttons, _action=url, _id='accounts_import_form'
    )

    resp_text = ""
    accounts_added = []
    accounts_updated = []
    if form.errors:
        response.flash = 'Error in form'
        return TABLE(*[TR(k, v) for k, v in form.errors.items()])
    elif form.accepts(request.vars, session):
        if form.vars.f_filename is not None:
            orig_filename = request.vars.f_filename.filename
        filename = os.path.join(request.folder, settings.password_upload_dir, form.vars.f_filename)
        if form.vars.f_taskit:
            task = scheduler.queue_task(
                accounts_import_file,
                pvars=dict(
                    filename=filename,
                    service=form.vars.f_service,
                    f_type=form.vars.f_type,
                    f_source=form.vars.f_source
                ),
                group_name=settings.scheduler_group_name,
                sync_output=5,
                timeout=300    # 5 minutes
            )
            if task.id:
                resp_text = "Submitted file for processing: %s" % (A("task " + str(task.id), _href=URL(c='tasks', f='status', args=task.id)).xml())
            else:
                resp_text = "Error submitting job: %s" % (task.errors)
        else:
            logger.info("Processing password file: %s" % (filename))
            account_data = process_password_file(
                pw_file=filename,
                file_type=request.vars.f_type,
                source=request.vars.f_source
            )
            resp_text = insert_or_update_acct(form.vars.f_service, account_data)
            logger.info(resp_text)

        if form.vars.f_add_to_evidence is True:
            # add the password file to evidence
            try:
                pwdata = open(filename, "r").readlines()
            except Exception, e:
                logger.error("Error opening %s: %s" % (filename, e))

            db.t_evidence.insert( f_hosts_id = db.t_services[form.vars.f_service].f_hosts_id,
                                  f_type = 'Password File',
                                  f_text = form.vars.f_type,
                                  f_filename = orig_filename,
                                  f_evidence = form.vars.f_filename,
                                  f_data = pwdata)
            db.commit()

    response.flash = resp_text
    response.title = "%s :: Import Password File" % (settings.title)

    if request.extension == "json":
        response.js = "accounttable.fnReloadAjax();"
        return dict()
    else:
        return dict(form=form)

@auth.requires_login()
def update_hashes_by_file():
    """
    Upload and parse a list of cracked hashes
    Supporting password file formats:
       JTR PWDUMP
       JTR Shadow
       Hash:Password
       Password:Hash
    """
    import os
    from skaldship.general import check_datadir
    check_datadir(request.folder)

    if request.extension == "load":
        buttons=[]
    else:
        buttons=['submit']

    pw_set = ('JTR PWDUMP', 'JTR Shadow', 'Hash:Password', 'Password:Hash')

    form = SQLFORM.factory(
        Field('f_filename', 'upload', uploadfolder=os.path.join(request.folder, settings.password_upload_dir), label=T('Password file')),
        Field('f_type', 'string', label=T('File type'), default='PWDUMP', requires=IS_IN_SET(pw_set)),
        Field('f_message', 'string', label=T('Message to add')),
        buttons=buttons, _action=URL('accounts', 'update_hashes_by_file'), _id='accounts_update_hashes_by_file',
    )

    resp_text = ""
    accounts_added = []
    accounts_updated = []
    if request.vars.f_filename is not None:
        orig_filename = request.vars.f_filename.filename
    if form.errors:
        response.flash = 'Error in form'
        return TABLE(*[TR(k, v) for k, v in form.errors.items()])
    elif form.accepts(request.vars, session):
        filename = os.path.join(request.folder, settings.password_upload_dir, form.vars.f_filename)
        logger.info("Processing password file: %s" % (filename))
        resp_text = process_cracked_file(pw_file=filename, file_type=request.vars.f_type, message=request.vars.f_message)

    response.title = "%s :: Update Password Hashes by File" % (settings.title)
    if request.extension == "json":
        return dict()
    else:
        return dict(form=form, resp_text=resp_text)

@auth.requires_login()
def import_mass_password():
    """
    Process a mass run of medusa/hydra.. result file will have IP addresses, service and info
    """
    if request.extension == "load":
        buttons=[]
    else:
        buttons=['submit']

    from skaldship.general import check_datadir
    check_datadir(request.folder)

    form=SQLFORM.factory(
        Field('f_filename', 'upload', uploadfolder=os.path.join(request.folder, settings.password_upload_dir),
              label=T('Password file'), requires=IS_NOT_EMPTY(error_message=T('Filename required'))),
        Field('f_ftype', 'string', label=T('File Type'), default="Medusa",
              requires=IS_IN_SET(('Medusa', 'Hydra', 'Metasploit Creds CSV'))),
        Field('f_proto', 'string', label=T('Protocol'), default='tcp', requires=IS_IN_SET(('tcp', 'udp', 'info'))),
        Field('f_number', 'integer', label=T('Port Number'), requires=IS_INT_IN_RANGE(0, 65536)),
        Field('f_message', 'string', label=T('Message to add')),
        Field('f_add_hosts', 'boolean', label=T('Add Hosts'), comment=T('Add missing hosts to the database')),
        buttons=buttons, _action=URL('accounts', 'import_mass_password'), _id='import_mass_password',
    )

    if form.errors:
        response.flash = 'Error in form'
        return TABLE(*[TR(k, v) for k, v in form.errors.items()])
    elif form.accepts(request.vars, session):
        if request.vars.f_filename is not None:
            orig_filename = request.vars.f_filename.filename
        filename = os.path.join(request.folder, settings.password_upload_dir, form.vars.f_filename)
        logger.info("Processing password file: %s" % (filename))
        resp_text = process_mass_password(
            pw_file=filename,
            pw_type=request.vars.f_ftype,
            message=request.vars.f_message,
            proto=request.vars.f_proto,
            portnum=request.vars.f_number,
            add_hosts=request.vars.f_add_hosts,
            user_id=auth.user.id,
        )
        response.flash = resp_text

    response.title = "%s :: Import Mass Password File" % (settings.title)
    if request.extension == "json":
        return dict()
    else:
        return dict(form=form)

@auth.requires_login()
def paste():
    """
    Import and parse password pasted to a textbox into t_accounts
    """
    from skaldship.general import check_datadir
    check_datadir(request.folder)

    # Service_id is primary, host_id is secondary, if none then list
    # all the services
    svc_set = []
    url=URL('accounts', 'paste')
    if request.vars.has_key('service_id'):
        try:
            record = db.t_services[request.vars.service_id]
            svc_set.append((record.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[record.f_hosts_id]), record.f_proto, record.f_number)))
            url = URL('accounts', 'paste', vars={'service_id':request.vars.service_id})
        except:
            pass
    elif request.vars.has_key('host_id'):
        try:
            host_record = get_host_record(request.vars.host_id)
            svc_records = db(db.t_services.f_hosts_id == host_record.id).select(cache=(cache.ram, 30))
            url = URL('accounts', 'paste', vars={'host_id':request.vars.host_id})
            for record in svc_records:
                svc_set.append((record.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[record.f_hosts_id]), record.f_proto, record.f_number)))
        except:
            pass

    if len(svc_set) == 0:
        # all services
        svc_records = db(db.t_services).select(cache=(cache.ram,30))
        svc_set = []
        for record in svc_records:
            svc_set.append((record.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[record.f_hosts_id]), record.f_proto, record.f_number)))

    if request.extension == "load":
        buttons=[]
    else:
        buttons=['submit']

    form = SQLFORM.factory(
        Field('f_service', 'string', label=T('Host / Service'), requires=IS_IN_SET(svc_set), default=svc_set[0][0]),
        Field('f_pwtext', 'text', label=T('Password text')),
        Field('f_type', 'string', label=T('File type'), default='PWDUMP', requires=IS_IN_SET(settings.password_file_types)),
        Field('f_source', 'string', label=T('Source (if necessary)')),
        Field('f_add_to_evidence', 'boolean', label=T('Add file to Evidence')),
        buttons=buttons, _action=url, _id='accounts_paste_form'
        #_action=url, _id='accounts_paste_form', formstyle='bootstrap_modal'
    )

    resp_text = ""
    accounts_added = []
    accounts_updated = []
    if form.errors:
        response.flash = 'Error in form'
        return TABLE(*[TR(k, v) for k, v in form.errors.items()])
    elif form.accepts(request.vars, session):
        from utils import web2py_uuid
        host_id = db.t_services[form.vars.f_service].f_hosts_id
        pwd_file_dir = os.path.join(request.folder, 'data', 'passwords', 'other')
        if not os.path.exists(pwd_file_dir):
            from gluon.fileutils import mktree
            mktree(pwd_file_dir)
        filename = "%s-pwfile-%s" % (host_id, web2py_uuid())
        full_file_path = os.path.join(request.folder, 'data/passwords/other', filename)
        of = open(full_file_path, "w")
        of.write(form.vars.f_pwtext)
        of.close()

        logger.debug("Processing password file: %s" % (full_file_path))
        account_data = process_password_file(pw_file=full_file_path, file_type=request.vars.f_type, source=request.vars.f_source)
        response.headers['web2py-component-command'] = 'accounttable.fnReloadAjax();'
        resp_text = insert_or_update_acct(form.vars.f_service, account_data)

        if form.vars.f_add_to_evidence is True:
            # add the password file to evidence
            try:
                pwdata = open(full_file_path, "r").readlines()
            except Exception, e:
                logger.error("Error opening %s: %s" % (full_file_path, e))
                resp_text += "Error opening %s: %s\n" % (full_file_path, e)

            db.t_evidence.insert( f_hosts_id = host_id,
                                  f_type = 'Password File',
                                  f_text = form.vars.f_type,
                                  f_filename = filename,
                                  f_evidence = filename,
                                  f_data = pwdata)
            resp_text += "\n%s added to evidence\n" % (filename)
            db.commit()
        # cleanup/delete the temporary file
        #tmpfile.close()

    response.flash = resp_text
    if request.extension == "json":
        return dict()
    else:
        return dict(form=form)

########NEW FILE########
__FILENAME__ = api
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## API Library
##
## One of the quirks with JSON-RPC is that function variables must be submitted
## in order as they appear, no assigning them when passing. Thus if the
## function calls for three fields and you only want to pass the second
## field then submit (None, secondvariable, None)
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

__version__ = "1.0"
from skaldship.hosts import get_host_record, create_hostfilter_query
from skaldship.general import cvss_metrics, vuln_data

import logging
logger = logging.getLogger("web2py.app.kvasir")

@auth.requires_login()
def download():
    return response.download(request,db)

@auth.requires_login()
def call():
    #session.forget()
    return service()
### end requires

@service.jsonrpc
def version():
    """Returns the API version number"""
    return __version__

##-------------------------------------------------------------------------
## evidence
##-------------------------------------------------------------------------

@service.jsonrpc
def evidence_list(recid=None):
    """List evidence on a host

Accepts: Record ID, ipv4, ipv6 or hostname or None for all records

Returns: List of evidence records
"""
    record = get_host_record(recid)
    if not record:
        query = db.t_evidence.id > 0
    else:
        query = (db.t_evidence.f_hosts_id == record.id)
    data = db(query).select(cache=(cache.ram,120))

    return [(evidence.id, evidence.f_type, evidence.f_other_type,
             evidence.f_text, evidence.f_evidence, evidence.f_filename) for evidence in data]

##-------------------------------------------------------------------------

@service.jsonrpc
def evidence_download(filename):
    """Download an evidence file

Accepts: filename to download

Returns: f_data blob (base64 decoded, of course)
"""
    row=db(db.t_evidence.f_evidence==filename).select(db.t_evidence.f_data).first()
    if row is None:
        return None

    return row.f_data

##-------------------------------------------------------------------------

@service.jsonrpc
def evidence_del(records=[]):
    """Delete an evidence record id

Accepts: List of record IDs to delete

Returns: [(True/False, message)]
"""
    if not records: return (False, 'No records sent')
    msg = []
    for rec in records:
        try:
            del db.t_evidence[rec]
            msg.append((True, 'Record %s deleted' % (rec)))
        except Exception, e:
            msg.append((False, 'Error: %s' % (e)))

    return msg

##-------------------------------------------------------------------------

@service.jsonrpc
def evidence_add(recid, filename, f_data, f_type, f_other_type=None, f_text=''):
    """Receive evidence file and store in t_evidence.

Accepts: Host ID/IPv4/IPv6/Hostname, filename, data, type, other_type, text

Returns: True/False, Record ID/Message
"""

    if not recid: return (False, 'No record id or IP address')
    if not filename: return (False, 'No filename')
    if not f_data: return (False, 'No data')
    if not f_type: return (False, 'No type')

    record = get_host_record(recid)
    if not record: return (False, 'Invalid record id or IP address')

    try:
        recid = db.t_evidence.insert(f_hosts_id = record.id,
                                     f_filename = filename,
                                     f_evidence = filename,
                                     f_data = f_data,
                                     f_type = f_type,
                                     f_other_type = f_other_type,
                                     f_text = f_text)
        db.commit()
        return (True, recid)
    except Exception, e:
        db.commit()
        return (False, e)

##-------------------------------------------------------------------------
## hosts
##-------------------------------------------------------------------------

@service.jsonrpc
def host_list(hostfilter=(None, None)):
    """Returns a long list of hosts

Accepts: hostfilter=(type, value)"""
    query = (db.t_hosts.id > 0)
    query = create_hostfilter_query(hostfilter, query)

    data = db(query).select(cache=(cache.ram,120))
    return [(host.id, host.f_ipv4, host.f_ipv6, host.f_macaddr, host.f_hostname,
             host.f_netbios_name, db.auth_user[host.f_engineer].username, host.f_asset_group,
             host.f_confirmed) for host in data]

##-------------------------------------------------------------------------

@service.jsonrpc
def host_add(f_ipv4, f_ipv6, f_macaddr, f_hostname, f_netbios_name, f_engineer, f_asset_group, f_confirmed=False):
    """Adding a host

Accepts: ipv4, ipv6, macaddr, hostname, netbios name, engineer name (must exist), asset group, confirmed

Returns: (True/False, Record id or error message)
"""

    if f_ipv4 is None and f_ipv6 is None:
        return (False, "No IPv4 or IPv6 address provided.")

    if  db(db.auth_user.username == f_engineer).count() == 0:
        return (False, "Engineer not found in user database")

    try:
        record_id = db.insert(f_ipv4 = f_ipv4,
                              f_ipv6 = f_ipv6,
                              f_macaddr = f_macaddr,
                              f_hostname = f_hostname,
                              f_netbios_name = f_netbios_name,
                              f_confirmed = f_confirmed,
                              f_engineer = f_engineer,
                              f_asset_group = f_asset_group)
        db.commit()
    except Exception, e:
        db.commit()
        return (False, e)
    return (True, record_id)

##-------------------------------------------------------------------------

@service.jsonrpc
def host_del(hostrec = [], ipv4recs = [], ipv6recs = []):
    """Delete a host or group of hosts.

Accepts: dictionary of host record IDs, IPv4 or IPv6 addresses

Returns: (hostrecs deleted, ipv4 deleted, ipv6 deleted)
"""

    errors = []
    hostrec_deleted = []
    for host in hostrecs:
        try:
            del db.t_hosts[host]
            hostrec_deleted.append(host)
        except Exception, e:
            errors.append(e)
        db.commit()

    ipv4_deleted = []
    for ipv4 in ipv4recs:
        host_id = db(db.t_hosts.f_ipv4 == ipv4).select(cache=(cache.ram,120)).first()
        if host_id is not None:
            del db.t_hosts[host_id]
            ipv4_deleted.append(ipv4)
            db.commit()

    ipv6_deleted = []
    for ipv6 in ipv6recs:
        host_id = db(db.t_hosts.f_ipv6 == ipv6).select(cache=(cache.ram,120)).first()
        if host_id is not None:
            del db.t_hosts[host_id]
            ipv6_deleted.append(ipv6)
            db.commit()

    return (hostrec_deleted, ipv4_deleted, ipv6_deleted, errors)

##-------------------------------------------------------------------------

@service.jsonrpc
def host_info(hostrec):
    """Returns the detail of a host record

Accepts: Host ID/IPv4/IPv6/Hostname

Returns: Host record information
"""
    record = get_host_record(hostrec)

    if record is None:
        return (False, "Host record not found")

    return (True, record.id, record.f_ipv4, record.f_ipv6, record.f_macaddr,
            record.f_hostname, record.f_netbios_name, record.f_engineer,
            record.f_asset_group, record.f_confirmed)

##-------------------------------------------------------------------------

@service.jsonrpc
def host_details(hostrec):
    """Returns the details for htmlreport on a host"""
    record = get_host_record(hostrec)

    if record is None:
        return (False, "Not record not found")

    host = (record.id, record.f_ipv4, record.f_ipv6, record.f_macaddr,
            record.f_hostname, record.f_netbios_name, record.f_asset_group, record.f_confirmed)

    host_points = {}
    # build the host_points field which will cover:
    # the top t_host_os_ref cpe string
    os_list = db(db.t_host_os_refs.f_hosts_id == record.id).select(cache=(cache.ram,120))
    host_points['os'] = (0, 'Unknown')
    for os_rec in os_list:
        if os_rec.f_certainty > host_points['os'][0]:
            host_points['os'] = (os_rec.f_certainty, db.t_os[os_rec.f_os_id].f_title)
            host_points['os_info'] = os_rec.as_dict()

    # number of account(s) / passwords
    # the top 5 nexpid's with exploits
    # the top 5 vulnid's based on high cvss+severity
    host_points['account_cnt'] = 0
    host_points['password_cnt'] = 0
    host_points['vuln_cnt'] = 0
    host_points['vuln_exploited_cnt'] = 0
    host_points['vuln_potential_cnt'] = 0
    vulns = {}
    vuln_list = []
    services = db(db.t_services.f_hosts_id == record.id).select(cache=(cache.ram,120))
    for svc in services:
        for vuln in db(db.t_service_vulns.f_services_id == svc.id).select(cache=(cache.ram,120)):
            vulndata = db.t_vulndata[vuln.f_vulndata_id]
            vulns[vulndata.f_vulnid] = ( vulndata.f_severity, vulndata.f_cvss_score )
            vuln_list.append(vulndata)
        host_points['vuln_exploited_cnt'] += db((db.t_service_vulns.f_services_id==svc.id) & (db.t_service_vulns.f_status.like('%exploited%'))).count()
        host_points['vuln_potential_cnt'] += db((db.t_service_vulns.f_services_id==svc.id) & (db.t_service_vulns.f_status.like('%potential%'))).count()
        host_points['vuln_cnt'] += db(db.t_service_vulns.f_services_id==svc.id).count()
        host_points['account_cnt'] += db(db.t_accounts.f_services_id==svc.id).count()
        host_points['password_cnt'] += db((db.t_accounts.f_services_id==svc.id) & (db.t_accounts.f_password != None)).count()

    # breakdown of vuln severity
    sev_sum_dict = {}
    for a in range(1, 11):
        sev_sum_dict[a] = 0

    for k,v in vulns.iteritems():
        # take the severity and increment the sev_sum set item
        count = sev_sum_dict.setdefault(v[0], 0)
        count += 1
        sev_sum_dict[v[0]] = count

    sev_sum_spark = []
    sev_sum = []
    for k,v in sev_sum_dict.iteritems():
        sev_sum_spark.append(str(v))
        if v > 0:
            sev_sum.append("%s: %s" % (k, v))

    host_points['sev_sum_spark'] = ",".join(sev_sum_spark)
    host_points['sev_sum'] = " / ".join(sev_sum)

    # netbios record (or none if it's empty)
    netb_record = db(db.t_netbios.f_hosts_id == record.id).select(cache=(cache.ram,120)).first() or None
    if netb_record is not None:
        host_points['netb_domain'] = netb_record.f_domain
        host_points['netb_type'] = netb_record.f_type
    else:
        netbios = None

    # services, vulnerabilities and accounts
    services = db(db.t_services.f_hosts_id==record.id).select(db.t_services.id,
                                                              db.t_services.f_proto, db.t_services.f_number, db.t_services.f_status,
                                                              db.t_services.f_name, db.t_services.f_banner, cache=(cache.ram,60))

    service_list = []
    vuln_list = []
    acct_list = []
    for svc in services:
        # service info
        atxt = []
        q = db(db.t_service_info.f_services_id == svc.id).select(cache=(cache.ram,30))
        if len(q) > 0:
            addl = []
            for svcinfo in q:
                addl.append(TR(TD(svcinfo.f_name), TD(svcinfo.f_text)))
            atxt.append(TABLE(THEAD(TR(TH(T('Name')), TH(T('Text')))), TBODY(addl)).xml())
        else:
            atxt.append("")
        atxt.append("%s/%s" % (svc.f_proto, svc.f_number)),
        atxt.append(svc.f_status),
        atxt.append(svc.f_name),
        atxt.append(svc.f_banner),
        service_list.append(atxt)

        # vulnerabilities
        for vulninfo in db(db.t_service_vulns.f_services_id == svc.id).select(cache=(cache.ram,120)):
            atxt = []
            exploit_list = []
            vulndetails = db(db.t_vulndata.id == vulninfo.f_vulndata_id).select(cache=(cache.ram, 300)).first()
            exploits = db(db.t_exploit_references.f_vulndata_id == vulninfo.f_vulndata_id).select(cache=(cache.ram,120))
            if len(exploits) > 0:
                expl_count = "Yes (%d)" % (len(exploits))
                for expl in exploits:
                    for expl_data in db(db.t_exploits.id == expl.f_exploit_id).select(db.t_exploits.f_source, db.t_exploits.f_name, db.t_exploits.f_rank, db.t_exploits.f_level):
                        exploit_list.append("%s :: %s (%s/%s)" % (expl_data.f_source[0], expl_data.f_name, expl_data.f_rank, expl_data.f_level))
            else:
                expl_count = ""

            atxt.append("%s/%s" % (svc.f_proto, svc.f_number))
            atxt.append("%s-%s" % (svc.f_number, svc.f_proto.upper()))
            atxt.append(vulndetails.f_vulnid)
            atxt.append(vulndetails.f_severity)
            atxt.append(vulndetails.f_cvss_score)
            atxt.append(cvss_metrics(vulndetails))
            atxt.append(vulninfo.f_status)
            atxt.append(expl_count)
            atxt.append(MARKMIN(vulninfo.f_proof).xml())
            atxt.append(MARKMIN(vulndetails.f_description).xml())
            atxt.append(vulndetails.f_title)
            atxt.append("<br />\n".join(exploit_list))
            vuln_list.append(atxt)

        # accounts
        for r in db(db.t_accounts.f_services_id == svc.id).select(cache=(cache.ram,30)):
            atxt=[]
            atxt.append("%s/%s" % (svc.f_proto, svc.f_number)),
            atxt.append(r.f_username)
            atxt.append(r.f_fullname)
            atxt.append(r.f_password)
            atxt.append(r.f_hash1_type)
            atxt.append(r.f_hash1)
            atxt.append(r.f_hash2_type)
            atxt.append(r.f_hash2)
            atxt.append(r.f_uid)
            atxt.append(r.f_gid)
            atxt.append(r.f_lockout)
            atxt.append(r.f_duration)
            atxt.append(r.f_source)
            atxt.append(r.f_level)
            atxt.append(r.f_description)

            acct_list.append(atxt)

    # snmp strings
    snmp_list = []
    for snmp in db(db.t_snmp.f_hosts_id==record.id).select(cache=(cache.ram,120)):
        atxt = []
        atxt.append(snmp.f_community)
        atxt.append(snmp.f_version)
        atxt.append(snmp.f_access)

        snmp_list.append(atxt)

    return dict(host=host, host_points=host_points, service_list=service_list,
                acct_list=acct_list, vuln_list=vuln_list, snmp_list=snmp_list)

##-------------------------------------------------------------------------
## services
##-------------------------------------------------------------------------

@service.jsonrpc
def service_list(svc_rec=None, host_rec=None, hostfilter=(None, None)):
    """Returns a specific service or all services

Accepts: Service id, host record (ipv4, ipv6 or id) or hostfilter

Returns: [ service_id, host_id, ipv4, ipv6, hostname, proto, number, status, name, banner, [ vuln list ...] ]
"""
    if svc_rec is not None:
        if type(svc_rec) is type(int):
            query = (db.t_services.id == svc_rec)
        else:
            return []
    elif host_rec is not None:
        host_rec = get_host_record(host_rec)
        if host_rec:
            query = (db.t_services.f_hosts_id == host_rec.id)
        else:
            return []
    else:
        query = (db.t_services.id > 0)
    query = create_hostfilter_query(hostfilter, query, 't_services')

    data = db(query).select(cache=(cache.ram,120))

    return [(svc.t_services.id, svc.t_services.f_hosts_id, svc.t_hosts.f_ipv4,
             svc.t_hosts.f_ipv6, svc.t_hosts.f_hostname, svc.t_services.f_proto,
             svc.t_services.f_number, svc.t_services.f_status, svc.t_services.f_name,
             svc.t_services.f_banner) for svc in data]

#--------------------------------------------------------------------------

@service.jsonrpc
def service_list_only(host_rec=None, hostfilter=(None, None)):
    """Returns a list of ports

Accepts: Service id, host record (ipv4, ipv6 or id) or hostfilter

Returns: [ service_id, host_id, ipv4, ipv6, hostname, proto, number, status, name, banner, [ vuln list ...] ]
"""
    if host_rec is not None:
        host_rec = get_host_record(host_rec)
        if host_rec:
            query = (db.t_services.f_hosts_id == host_rec.id)
        else:
            return []
    else:
        query = (db.t_services.id > 0)
    query = create_hostfilter_query(hostfilter, query, 't_services')

    data = db(query).select(db.t_services.f_proto, db.t_services.f_number, distinct=True, cache=(cache.ram,120))

    return ["%s/%s" % (svc.f_number, svc.f_proto) for svc in data]

#--------------------------------------------------------------------------

@service.jsonrpc
def service_info(svc_rec=None, ipaddr=None, proto=None, port=None):
    """Returns the information about a service from either a svc_record id or
ip address/port combo. If a port doesn't exist it will add it if insert=True

Returns: [ service_id, host_id, ipv4, ipv6, hostname, proto, number, status, name, banner ]
"""
    if svc_rec:
        rows = db(db.t_services.id == svc_rec).select()
        if rows is not None:
            host_rec = get_host_record(rows[0].f_hosts_id)
        else:
            return []

    else:
        host_rec = get_host_record(ipaddr)
        if host_rec:
            query = (db.t_services.f_hosts_id == host_rec.id)
        else:
            return []
        if proto:
            query &= (db.t_services.f_proto == proto)
        if port:
            query &= (db.t_services.f_number == port)

        rows = db(query).select()

    retval = []
    for row in rows:
        retval.append([
            row.id, host_rec.f_ipv4, host_rec.f_ipv6, host_rec.f_hostname,
            row.f_proto, row.f_number, row.f_status,
            row.f_name, row.f_banner
        ])

    return retval

#--------------------------------------------------------------------------

@service.jsonrpc
def service_add(ipaddr=None, proto=None, port=None, fields={}):
    """Adds a service to the database"""
    host_rec = get_host_record(ipaddr)
    if not host_rec:
        return [False, 'No host record found']
    if not proto:
        return [False, 'No protocol provided']
    if not port:
        return [False, 'No port number provided']

    query = (db.t_services.f_hosts_id == host_rec.id) & (db.t_services.f_proto == proto) & (db.t_services.f_number == port)
    if db(query).count() > 0:
        return [False, 'Service already exists']

    fields.update( {
        'f_hosts_id': host_rec.id,
        'f_number': port,
        'f_proto': proto,
    } )

    try:
        svc_rec = db.t_services.insert(**fields)
    except Exception, e:
        return [False, 'Error inserting service record: %s' % e]

    return [True, svc_rec]

#--------------------------------------------------------------------------

@service.jsonrpc
def service_del(svc_rec=None, ipaddr=None, proto=None, port=None):
    """Deletes a service to the database"""
    host_rec = get_host_record(ipaddr)
    if svc_rec:
        query = (db.t_services.id == svc_rec)
    elif host_rec:
        query = (db.t_services.f_hosts_id == host_rec.id)
        if proto:
            query &= (db.t_services.f_proto == proto)
        if port:
            query &= (db.t_services.f_number == port)

    try:
        count = db(query).delete()
        db.commit()
        return [True, '%s record(s) deleted' % (count)]
    except Exception, e:
        return [False, 'Error: %s' % (e)]

#--------------------------------------------------------------------------

@service.jsonrpc
def service_rpt_index_stats(hostfilter=(None, None)):
    """Returns the services index statistics:

Port, Service Name, Number of Hosts, Unique Vulns, Vuln count
"""

    # builds a Row() of svc_ids and vuln counts, have to match these to
    #
    count = db.t_service_vulns.f_services_id.count()
    svc_id_vulncount = {}

    svc_vuln_q = (db.t_service_vulns.f_services_id == db.t_services.id)
    svc_vuln_q = create_hostfilter_query(hostfilter, svc_vuln_q, 't_services')

    all_svc_q = (db.t_services.id > 0)
    all_svc_q = create_hostfilter_query(hostfilter, all_svc_q, 't_services')

    for d in db(svc_vuln_q).select(db.t_service_vulns.f_services_id, count, groupby=db.t_service_vulns.f_services_id):
        svc_id_vulncount.setdefault(d.t_service_vulns.f_services_id, d._extra['COUNT(t_service_vulns.f_services_id)'])

    data = {}
    for port in db(all_svc_q).select(db.t_services.f_number, db.t_services.f_proto, db.t_services.f_name, distinct=True):
        svc_q = (db.t_services.f_proto == port.f_proto) & (db.t_services.f_number == port.f_number)
        svc_q = create_hostfilter_query(hostfilter, svc_q, 't_services')
        vuln_count = 0
        host_count = 0
        for port_list in db(svc_q).select(db.t_services.id, cache=(cache.ram,120)):
            host_count += 1

        q = svc_q & (db.t_services.id == db.t_service_vulns.f_services_id)
        vuln_count += db(q).count()
        unique_vuln_count = 0
        count = db.t_services.f_hosts_id.count()
        for row in db(q).select(db.t_services.f_hosts_id, count, groupby=db.t_services.f_hosts_id):
            unique_vuln_count = row._extra['COUNT(t_services.f_hosts_id)']

        service = "%s/%s" % (port.f_number, port.f_proto)
        #data.setdefault(service, [])
        data[service] = (
            port.f_name,
            host_count,
            unique_vuln_count,
            vuln_count,
        )

    return data

#--------------------------------------------------------------------------

@service.jsonrpc
def service_report_list(service_id=None, service_port=None, hostfilter=(None, None)):
    """Returns a list of ports with IPs and banners and vulns

    XXX: THIS IS REALLY REALLY REALLY REALLY SLOW!
"""

    query = (db.t_services.id > 0)

    if service_id is not None:
        query &= (db.t_services.id == service_id)

    if service_port is not None:
        number,proto = service_port.split('/')
        query &= (db.t_services.f_number == number) & (db.t_services.f_proto == proto)

    query = create_hostfilter_query(hostfilter, query, 't_services')

    port_dict = {}
    for port in db(query).select(distinct=True, cache=(cache.ram, 120)):
        port_info = "%s/%s" % (port.t_services.f_number, port.t_services.f_proto)
        #host_rec = get_host_record(port.f_hosts_id)
        vuln_list = []
        #for vuln in port.t_service_vulns.select(db.t_service_vulns.ALL, db.t_vulndata.ALL,
        #                                        left=db.t_vulndata.on(db.t_service_vulns.f_vulndata_id==db.t_vulndata.id)):
        for vuln_rec in port.t_services.t_service_vulns.select():
            vulndata = vuln_data(vuln_rec.f_vulndata_id, full=False)
            vuln_list.append((
                vulndata[1],   # f_vulnid
                vulndata[2],   # f_title
                vulndata[3],   # f_severity
                vulndata[4],   # f_cvss_score
            ))
            #vuln_list.append((vuln.t_vulndata.f_vulnid, vuln.t_vulndata.f_title, vuln.t_vulndata.f_severity, vuln.t_vulndata.f_cvss_score))
        port_list = port_dict.setdefault(port_info, [])
        port_dict[port_info].append((port.t_hosts.f_ipv4, port.t_services.f_banner, vuln_list))

    return port_dict

#--------------------------------------------------------------------------

@service.jsonrpc
def service_vulns_list(service_rec=None, service_port=None, hostfilter=(None, None)):
    """Returns a list of vulnerablities for a service

Accepts: Service Record ID

Returns: (True/False, Service info ...)

    XXX: THIS IS REALLY REALLY REALLY REALLY SLOW!
"""

    #from datetime import datetime, timedelta
    #start = datetime.now()
    query = (db.t_service_vulns.f_services_id == db.t_services.id)
    query &= (db.t_service_vulns.f_vulndata_id == db.t_vulndata.id)

    if service_rec is not None:
        query &= (db.t_services.id == service_rec)

    if service_port is not None:
        number,proto = service_port.split('/')
        query &= (db.t_services.f_number == number) & (db.t_services.f_proto == proto)

    query = create_hostfilter_query(hostfilter, query, 't_services')

    res = db(query).select(cache=(cache.ram,120))
    #logger.debug("api.service_vulns_list select took %s seconds" % (timedelta.total_seconds(datetime.now() - start)))
    #logger.debug("query = %s" % (db._lastsql))
    #start = datetime.now()
    res = res.as_list()
    #logger.debug("api.service_vulns_list as_list took %s seconds" % (timedelta.total_seconds(datetime.now() - start)))

    return res

#--------------------------------------------------------------------------

@service.jsonrpc
def service_vuln_iptable(hostfilter=(None, None)):
    """Returns a dict of services. Contains a list of IPs with (vuln, sev)

    '0/info': { 'host_id1': [ (ipv4, ipv6, hostname), ( (vuln1, 5), (vuln2, 10) ... ) ] },
              { 'host_id2': [ (ipv4, ipv6, hostname), ( (vuln1, 5) ) ] }

"""

    service_dict = {}
    # go through each t_service_vulns identifier that is unique
    query = (db.t_service_vulns.id > 0) & (db.t_service_vulns.f_services_id == db.t_services.id)
    query = create_hostfilter_query(hostfilter, query, 't_services')

    for service in db(query).select(db.t_service_vulns.f_services_id, groupby=db.t_service_vulns.f_services_id):
        # find all the records with the service_id
        q = (db.t_service_vulns.f_services_id == service.f_services_id)
        q &= (db.t_service_vulns.f_vulndata_id == db.t_vulndata.id)

        ip_dict = {}

        # go through each
        for row in db(q).select(cache=(cache.ram,120)):
            svc_rec = db.t_services(row.t_service_vulns.f_services_id)
            port_txt = "%s/%s" % (svc_rec.f_number, svc_rec.f_proto)
            host_rec = get_host_record(svc_rec.f_hosts_id)
            ip_info = ip_dict.setdefault(host_rec.f_ipv4, [])
            if row.t_vulndata.f_vulnid not in map(lambda x: x[0], ip_info):
                ip_info.append((row.t_vulndata.f_vulnid, row.t_vulndata.f_severity, row.t_vulndata.f_cvss_score))
            ip_dict[host_rec.f_ipv4] = ip_info

        for k,v in ip_dict.iteritems():
            service_dict.setdefault(port_txt, dict())
            service_dict[port_txt][k] = v

    return service_dict

##-------------------------------------------------------------------------
## accounts
##-------------------------------------------------------------------------

@service.jsonrpc
def accounts_list(svc_rec=None, hostfilter=(None, None), compromised=False):
    """Returns a list of accounts for a service or host

Accepts: Service id, hostfilter, compromised

Returns: [ service_id, ipv4, ipv6, hostname, account info... ]
"""

    query = (db.t_accounts.f_services_id==db.t_services.id)
    if svc_rec is not None:
        query = (db.t_accounts.f_service_id == svc_rec)
    else:
        query &= (db.t_accounts.id > 0)
    if compromised:
        query &= (db.t_accounts.f_compromised==True)
    query = create_hostfilter_query(hostfilter, query, 't_services')

    accounts = db(query).select(cache=(cache.ram,120))

    data = []
    for acct in accounts:
        data.append([acct.t_accounts.f_services_id, acct.t_hosts.f_ipv4,
                     acct.t_hosts.f_ipv6, acct.t_hosts.f_hostname,
                     acct.t_accounts.id, acct.t_accounts.f_username,
                     acct.t_accounts.f_fullname, acct.t_accounts.f_password,
                     acct.t_accounts.f_compromised, acct.t_accounts.f_hash1,
                     acct.t_accounts.f_hash1_type, acct.t_accounts.f_hash2,
                     acct.t_accounts.f_hash2_type, acct.t_accounts.f_source,
                     acct.t_accounts.f_uid, acct.t_accounts.f_gid,
                     acct.t_accounts.f_level, acct.t_accounts.f_domain,
                     acct.t_accounts.f_message, acct.t_accounts.f_lockout,
                     acct.t_accounts.f_duration, acct.t_accounts.f_active,
                     acct.t_accounts.f_description,
                     acct.t_services.f_proto, acct.t_services.f_number,
                   ])
        '''
        data.append([svc.id, hostrec.f_ipv4, hostrec.f_ipv6, hostrec.f_hostname,
                     acct.id, acct.f_username, acct.f_fullname, acct.f_password,
                     acct.f_compromised, acct.f_hash1, acct.f_hash1_type, acct.f_hash2,
                     acct.f_hash2_type, acct.f_source, acct.f_uid, acct.f_gid,
                     acct.f_level, acct.f_domain, acct.f_message, acct.f_lockout,
                     acct.f_duration, acct.f_active, acct.f_description,
                     svc.f_proto, svc.f_number,
                   ])
        '''
    return data

'''
##-------------------------------------------------------------------------

@service.jsonrpc
def accounts_compromised(service_rec=None, host_rec=None, account_rec=None):
    """Returns the compromised accounts for a service or host"""

    data = []
    query = (db.t_accounts.f_compromised == True)

    record = get_host_record(host_rec)

    if record is not None:
        query &= (db.t_services.f_hosts_id == record.id)
    elif service_rec is not None:
        query &= (db.t_accounts.f_services_id == service_rec)
    else:
        query &= (db.t_services.id == db.t_accounts.f_services_id)

    for rec in db(query).select(cache=(cache.ram,120)):
        hostrec = db(db.t_hosts.id == rec.t_services.f_hosts_id).select(cache=(cache.ram,120)).first()
        data.append([rec.t_services.id, hostrec.f_ipv4, hostrec.f_ipv6, hostrec.f_hostname,
                     rec.t_accounts.id, rec.t_accounts.f_username, rec.t_accounts.f_fullname,
                     rec.t_accounts.f_password, rec.t_accounts.f_compromised, rec.t_accounts.f_hash1,
                     rec.t_accounts.f_hash1_type, rec.t_accounts.f_hash2, rec.t_accounts.f_hash2_type,
                     rec.t_accounts.f_source, rec.t_accounts.f_uid, rec.t_accounts.f_gid,
                     rec.t_accounts.f_level, rec.t_accounts.f_domain, rec.t_accounts.f_message,
                     rec.t_accounts.f_lockout, rec.t_accounts.f_duration, rec.t_accounts.f_active,
                     rec.t_accounts.f_description, rec.t_services.f_proto, rec.t_services.f_number,
                     ])

    return data
'''

##-------------------------------------------------------------------------

@service.jsonrpc
def accounts_add(svc_rec, records = []):
    """Adds an account to a service

Accepts: Service record id (reqd), username (reqd), other fields can be empty

Returns: True/False, Error Msg/account_id
"""
    if db(db.t_services.id == svc_rec).count() != 1:
        return (False, 'Service record ID not found')

    if len(records) == 0:
        return (False, 'No records sent to insert')

    result = []
    for rec in records:
        rec['f_services_id'] = svc_rec
        if len(rec) == 0:
            result.append((False, 'No field values sent'))
            continue

        for key in rec.keys():
            if key not in db.t_accounts.fields:
                result.append((False, '%s not a valid field' % (key)))
                continue

        try:
            recid = db.t_accounts.insert(**rec)
            result.append((True, recid))
        except Exception, e:
            result.append((False, e))
        db.commit()

    return (True, result)

##-------------------------------------------------------------------------

@service.jsonrpc
def accounts_info(account_rec=None):
    """Returns the data from an account record"""
    acct = db.t_accounts[account_rec]
    if acct is None:
        return (False, "Account record not found")

    svc_rec = db.t_services[acct.f_services_id]
    hostrec = db.t_hosts[svc_rec.f_hosts_id]

    return (True, hostrec.f_ipv4, hostrec.f_ipv6, hostrec.f_hostname,
            svc_rec.f_proto, svc_rec.f_number,
            acct.id, acct.f_username, acct.f_fullname, acct.f_password,
            acct.f_compromised, acct.f_hash1, acct.f_hash1_type, acct.f_hash2,
            acct.f_hash2_type, acct.f_source, acct.f_uid, acct.f_gid,
            acct.f_level, acct.f_domain, acct.f_message, acct.f_lockout,
            acct.f_duration, acct.f_active, acct.f_description
            )

##-------------------------------------------------------------------------

@service.jsonrpc
def accounts_update(account_rec=None, values={}):
    """Updates an account record with specific values

Accepts: Service record id (reqd), dictionary of field values

Returns: (True/False, Message)
"""
    if account_rec is None:
        return (False, 'No account ID sent')

    if len(values) == 0:
        return (False, 'No field values sent')

    for key in values.keys():
        if key not in db.t_accounts.fields:
            return (False, '%s not a valid field' % (key))

    if db.t_accounts[account_rec] == None:
        return (False, 'Account ID record not found')

    try:
        db.t_accounts[account_rec] = values
        db.commit()
        return (True, 'Account record %s updated' % (account_rec))
    except Exception, e:
        return (False, 'Error: %s' % (e))

##-------------------------------------------------------------------------

@service.jsonrpc
def accounts_del(account_rec=None):
    """Delete an account"""
    if account_rec is None:
        return (False, ['No account record id sent'])

    result = []
    if type(account_rec) == type([]):
        for rec in account_rec:
            try:
                del db.t_accounts[rec]
                result.append("%s record id deleted" % (rec))
            except Exception, e:
                result.append("%s error: %s" % (rec, e))
            db.commit()
    else:
        try:
            del db.t_accounts[account_rec]
            result.append(True, "%s record id deleted" % (rec))
        except Exception, e:
            result.append(False, "%s error: %s" % (rec, e))
        db.commit()

    return result

##-------------------------------------------------------------------------

@service.jsonrpc
def accounts_import_file(svc_id=None, host_service=None, filename=None, pw_data=None, f_type=None, add_to_evidence=False):
    """Parses an imported file into account records and adds the file to evidence
if requested.

Accepts: Service record ID, host_service, filename, content of file, add to evidence boolean

Returns: (True/False, Message or dictionary of records/account names added)
"""
    # TODO: test this
    if host_service is not None:
        # we have a host_service combo of (iprecord, (proto, port))
        host_rec = get_host_record(host_service[0])
        if host_rec is None:
            return (False, 'Unable to find host record')

        svc_rec = get_service_rec(host_rec, host_service[1][0], host_service[1][1])
        if svc_rec is not None:
            svc_id = svc_rec.id

    if db(db.t_services.id == svc_id).count() != 1:
        return (False, 'Service record ID not found')

    if filename is None:
        return (False, 'No filename provided')

    if pw_data is None:
        return (False, 'No data provided')

    if f_type is None:
        return (False, 'No password filetype provided')

    from skaldship.passwords import process_password_file, insert_or_update_acct

    account_data = process_password_file(pw_data=pw_data, file_type=f_type)
    resp_text = insert_or_update_acct(svc_id, account_data)

    if add_to_evidence is True:
        # add the password file to evidence
        db.t_evidence.insert( f_hosts_id = db.t_services[svc_id].f_hosts_id,
                              f_type = 'Password File',
                              f_text = f_type,
                              f_filename = filename,
                              f_evidence = filename,
                              f_data = pw_data)
        resp_text += "\n%s added to evidence\n" % (filename)
        db.commit()

    return (True, resp_text)

##-------------------------------------------------------------------------

@service.jsonrpc
def list_pw_types():
    """
    Returns a list of supported password file types
    """
    return settings.password_file_types

##-------------------------------------------------------------------------

@service.jsonrpc
def accounts_index_data(hostfilter=(None, None)):
    """Returns a list of IP address and account statistics.
A compromised account is when the Password is not None"""
    query = (db.t_accounts.f_services_id==db.t_services.id)
    query = create_hostfilter_query(hostfilter, query, 't_services')
    data = {}
    for acct in db(query).select(cache=(cache.ram,120)):
        ipv4 = db.t_hosts[acct.t_services.f_hosts_id].f_ipv4
        data.setdefault(ipv4, {'discovered':0, 'compromised':0})
        data[ipv4]['discovered'] += 1
        if acct.t_accounts.f_password is not None:
            data[ipv4]['compromised'] += 1

    return data

##-------------------------------------------------------------------------
## SNMP
##-------------------------------------------------------------------------

@service.jsonrpc
def snmp_list_communities():
    """Returns a list of all known SNMP community strings

Accepts: None

Returns: [ string, string, string ...]
"""
    data = []
    for snmp in db(db.t_snmp).select(db.t_snmp.f_community, distinct=True):
        data.append(snmp.f_community)

    return data

##-------------------------------------------------------------------------

@service.jsonrpc
def snmp_list(snmpstring=None, hostfilter=(None, None)):
    """Returns a list of SNMP information for a host

Accepts: Record id, IPv4, IPv6 or Hostname - if not found return nothing

Returns: [ [ record_id, ipv4, ipv6, hostname, community, access, version ] ... ]
"""
    query = (db.t_snmp.id > 0)
    query = create_hostfilter_query(hostfilter, query, 't_snmp')

    if snmpstring is not None:
        query &= (db.t_snmp.f_community == snmpstring)

    data = []
    for snmp in db(query).select(cache=(cache.ram,120)):
        data.append([snmp.t_snmp.id, snmp.t_hosts.f_ipv4, snmp.t_hosts.f_ipv6, snmp.t_hosts.f_hostname,
                     snmp.t_snmp.f_community, snmp.t_snmp.f_access, snmp.t_snmp.f_version])

    return data

##-------------------------------------------------------------------------

@service.jsonrpc
def snmp_rpt_table(hostfilter=(None, None)):
    """Returns an array of tuples containing (communitystring, count_of_ips_with_string, perm)"""

    query = (db.t_snmp.id > 0)
    query = create_hostfilter_query(hostfilter, query, 't_snmp')

    results={}
    for snmp in db(query).select(cache=(cache.ram,120)):
        (junk, count, perm) = results.get(snmp.t_snmp.f_community, ("", 0, "READ"))
        if perm != 'WRITE' and snmp.t_snmp.f_access == 'WRITE':
            perm = 'WRITE'
        results[snmp.t_snmp.f_community] = (snmp.t_snmp.f_community, count+1, perm)
    return results.values()

##-------------------------------------------------------------------------

@service.jsonrpc
def snmp_add(hostrec=None, f_community=None, f_access=None, f_version=None):
    """Adds an SNMP record to a host

Accepts: Record id, IPv4, IPv6 or Hostname - if not found return nothing

Returns: ( True/False, record id )
"""
    record = get_host_record(hostrec)

    if record is None:
        return (False, 'Host record not found')

    if (not f_community) or (not f_access) or (not f_version):
        return (False, 'Community, access or version not specified')

    if f_access.lower() not in ['read', 'write']:
        return (False, 'Access can only be READ or WRITE')

    if f_version.lower() not in ['v1', 'v2c', 'v3']:
        return (False, 'Version can only be v1, v2c or v3')

    recid = db.t_snmp.insert(f_hosts_id=record.id, f_community=f_community,
                             f_access=f_access.upper(), f_version=f_version.lower())
    db.commit()

    if recid > 0:
        return (True, recid)
    else:
        return (False, 'Unable to insert record into database')

##-------------------------------------------------------------------------

@service.jsonrpc
def snmp_del(snmp_rec=None):
    """Delete an SNMP record

Accepts: Single or list of SNMP record ids

Returns: ( True/False, message )
"""
    if snmp_rec is None:
        return (False, ['No record id sent'])

    result = []
    if type(snmp_rec) == type(list()):
        for rec in snmp_rec:
            try:
                del db.t_snmp[rec]
                result.append((True, "%s record deleted" % (rec)))
            except Exception, e:
                result.append((False, "%s error: %s" % (rec, e)))
            db.commit()
    else:
        try:
            del db.t_snmp[snmp_rec]
            result = (True, "%s record deleted" % (snmp_rec))
        except Exception, e:
            result = (False, "%s error: %s" % (snmp_rec, e))
        db.commit()

    return result

##-------------------------------------------------------------------------
## Vulnerabilities
##-------------------------------------------------------------------------

@service.jsonrpc
def vuln_list(host_rec = None, svc_rec = None, hostfilter=(None, None)):
    """Returns a list of Vulnerabilities known to a host, a service or
all known Vulnerabilities

Accepts: host record (id, ipv4, ipv6, hostname), service_id or None for all

Returns: [ ( vulndata ... ) ... ]

If the entire list is desired then it returns:

    [ (vulndata ... vuln_cnt, [ list_of_vuln_ips ], [ services ]) ... ]

TODO: SPEEDUP!
"""
    #vuln_start = datetime.now()
    data = []
    if host_rec is not None:
        # build query of all services and hosts using hostfilter
        svc_query = (db.t_services.f_hosts_id == host_rec)

        # pull all the services and vulnerabilities for this host
        for svc in db(svc_query).select(cache=(cache.ram,120)):
            for vuln in svc.t_services.t_service_vulns.select(cache=(cache.ram,120)):
                data.append((vuln_data(vuln.f_vulndata_id, full=True),
                             svc.t_services.f_proto,
                             svc.t_services.f_number,
                             svc.t_services.f_name,
                             ))

        #logger.debug("api.vuln_list with host_reccompleted in %s seconds" % (timedelta.total_seconds(datetime.now() - vuln_start)))
        return data

    if svc_rec is not None:
        query = (db.t_service_vulns.f_services_id == svc_rec) & (db.t_service_vulns.f_vulndata_id==db.t_vulndata.id)
        for vuln in db(query).select(cache=(cache.ram,120)):
            data.append((vuln_data(vuln.t_vulndata, full=True)))
        #logger.debug("api.vuln_list with svc_rec completed in %s seconds" % (timedelta.total_seconds(datetime.now() - vuln_start)))
    else:
        count = db.t_service_vulns.f_vulndata_id.count()
        #services = []
        query = (db.t_service_vulns.id > 0) & (db.t_service_vulns.f_services_id == db.t_services.id)
        query = create_hostfilter_query(hostfilter, query, 't_services')
        for vuln in db(query).select(db.t_service_vulns.f_vulndata_id, count, groupby=(db.t_service_vulns.f_vulndata_id), distinct=True):
            vulndata = vuln_data(vuln.t_service_vulns.f_vulndata_id, full=True)
            vulndata += tuple([str(vuln[count])])
            vulndata += tuple([vuln_ip_info(vuln_id = vuln.t_service_vulns.f_vulndata_id, ip_list_only=False, hostfilter=hostfilter)])
            data.append(vulndata)

        #logger.debug("api.vuln_list with hostfilter completed in %s seconds" % (timedelta.total_seconds(datetime.now() - vuln_start)))
    return data

##-------------------------------------------------------------------------

@service.jsonrpc
def vuln_info(vuln_name = None, vuln_id = None):
    """Returns information about a vulnerability"""
    data = []

    query = (db.t_service_vulns.f_services_id == db.t_services.id) & (db.t_services.f_hosts_id == db.t_hosts.id)
    if vuln_name is not None:
        vuln_rec = db(db.t_vulndata.f_vulnid == vuln_name).select(cache=(cache.ram,120)).first()
        if vuln_rec is None:
            return ["Vulnerability %s not found" % (vuln_name)]

    elif vuln_id is not None:
        vuln_rec = db.t_vulndata[vuln_id]
        if vuln_rec == None:
            return ["Vulnerability ID %s not found" % (vuln_id)]

    return vuln_data(vuln_rec, full=True)

##-------------------------------------------------------------------------

@service.jsonrpc
def vuln_ip_info(vuln_name = None, vuln_id = None, ip_list_only=True, hostfilter=(None, None)):
    """Returns a list of all IP addresses with a vulnerability and their proof/status

If ip_list_only is false then adds proof and status
"""
    from gluon.contrib.markmin.markmin2html import markmin2html
    data = []

    query = (db.t_service_vulns.f_services_id == db.t_services.id)
    query = create_hostfilter_query(hostfilter, query, 't_services')
    if vuln_name is not None:
        vuln_rec = db(db.t_vulndata.f_vulnid == vuln_name).select(cache=(cache.ram, 60)).first()
        if vuln_rec is None:
            return ["Vulnerability %s not found" % (vuln_name)]
        query &= (db.t_service_vulns.f_vulndata_id == vuln_rec.id)

    elif vuln_id is not None:
        if db.t_vulndata[vuln_id] == None:
            return ["Vulnerability ID %s not found" % (vuln_id)]
        query &= (db.t_service_vulns.f_vulndata_id == vuln_id)

    for row in db(query).select(cache=(cache.ram, 60)):
        if row.t_hosts.f_ipv4 not in data and ip_list_only:
            data.append(row.t_hosts.f_ipv4, row.t_hosts.f_ipv6, row.t_hosts.f_hostname)
        else:
            data.append((row.t_hosts.f_ipv4, row.t_hosts.f_ipv6, row.t_hosts.f_hostname, markmin2html(row.t_service_vulns.f_proof), markmin2html(row.t_service_vulns.f_status)))

    return data

##-------------------------------------------------------------------------

@service.jsonrpc
def vuln_service_list(vuln_name = None, vuln_id = None, hostfilter=(None, None)):
    """Returns a list of services and IPs vulnerability has been found on:

'vuln-id': {'port1': [ (ipv4, ipv6, hostname ),
                       (ipv4, ipv6, hostname ) ]},
           {'port2': [ (ipv4, ipv6, hostname ) ]}

"""

    vulndata = db(db.t_vulndata).select(cache=(cache.ram,120)).as_dict()
    query = (db.t_service_vulns.f_services_id == db.t_services.id)
    query = create_hostfilter_query(hostfilter, query, 't_services')

    if vuln_name is not None:
        vuln_rec = db(db.t_vulndata.f_vulnid == vuln_name).select(cache=(cache.ram,120)).first()
        if vuln_rec is None:
            return ["Vulnerability %s not found" % (vuln_name)]
        query &= (db.t_service_vulns.f_vulndata_id == vuln_rec.id)

    elif vuln_id is not None:
        if db.t_vulndata[vuln_id] == None:
            return ["Vulnerability ID %s not found" % (vuln_id)]
        query &= (db.t_service_vulns.f_vulndata_id == vuln_id)

    data = {}
    for row in db(query).select(cache=(cache.ram,120)):
        port = "%s/%s" % (row.t_services.f_number, row.t_services.f_proto)
        host_rec = get_host_record(row.t_services.f_hosts_id)
        host_list = [(host_rec.f_ipv4, host_rec.f_ipv6, host_rec.f_hostname)]
        vulnid = db.t_vulndata[row.t_service_vulns.f_vulndata_id].f_vulnid

        port_dict = data.setdefault(vulnid, {})
        hlist = port_dict.setdefault(port, [])
        if host_list not in hlist:
            hlist.append(host_list)
        else:
            hlist = host_list
        port_dict[port] = hlist
        data[vulnid] = port_dict

    return data

##-------------------------------------------------------------------------

@service.jsonrpc
def vuln_count(svc_rec = None, host_rec = None):
    """Returns a count of vulnerabilities per service. If a service/host is provided then
adds unique count

XXX: This isn't complete
"""

    vulncount = {}

    if host_rec is not None:
        record = get_host_record(host_rec)
        if record is None:
            return []

        # pull all the services and vulnerabilities for this host
        host_vulncount = 0
        for svc in db(db.t_services.f_hosts_id == record.id).select(cache=(cache.ram,120)):
            query = (db.t_service_vulns.f_services_id == svc.id) & (db.t_service_vulns.f_vulndata_id==db.t_vulndata.id)
            host_vulncount += db(query).select(cache=(cache.ram,120)).count()

    if svc_rec is not None:
        query = (db.t_service_vulns.f_services_id == svc_rec) & (db.t_service_vulns.f_vulndata_id==db.t_vulndata.id)
        svc_vulncount = db(query).select(cache=(cache.ram,120)).count()
    else:
        vulncount = db(db.t_service_vulns).select(cache=(cache.ram,120)).count()

##-------------------------------------------------------------------------
## Operating Systems
##-------------------------------------------------------------------------

@service.jsonrpc
def os_list(hostfilter=(None, None)):
    """Returns the Operating Systems for a host or all OS records and hosts

Accepts: hostfilter

Returns: [ ( ipv4, ipv6, hostname, os records... ) ... ]
"""
    data = []
    query = (db.t_host_os_refs.f_hosts_id == db.t_hosts.id)
    query = create_hostfilter_query(hostfilter, query)

    for os_ref_rec in db(query).select(cache=(cache.ram,120)):
        os_rec = db.t_os[os_ref_rec.t_host_os_refs.f_os_id]
        data.append([os_ref_rec.t_hosts.f_ipv4,
                     os_ref_rec.t_hosts.f_ipv6,
                     os_ref_rec.t_hosts.f_hostname,
                     os_ref_rec.t_host_os_refs.f_certainty,
                     os_ref_rec.t_host_os_refs.f_class,
                     os_ref_rec.t_host_os_refs.f_family,
                     os_rec.f_cpename,
                     os_rec.f_title,
                     os_rec.f_vendor,
                     os_rec.f_product,
                     os_rec.f_version,
                     os_rec.f_update,
                     os_rec.f_edition,
                     os_rec.f_language
                     ])

    return data

##-------------------------------------------------------------------------

@service.jsonrpc
def os_report_list(hostfilter=(None, None)):
    """Returns a list of hosts and their top operating systems"""

    os_q = (db.t_host_os_refs.f_hosts_id == db.t_hosts.id)
    host_q = create_hostfilter_query(hostfilter)

    os_recs = db(os_q).select(cache=(cache.ram,120))
    data = []
    for host_rec in db(host_q).select(cache=(cache.ram,120)):
        highest = (0, None)
        for row in os_recs.find(lambda row: row.t_hosts.id == host_rec.id):
            if row.t_host_os_refs.f_certainty > highest[0]:
                highest = (row.t_host_os_refs.f_certainty, row)

        if highest[0] > 0:
            os_rec = db.t_os(highest[1].t_host_os_refs.f_os_id)
            data.append([row.t_hosts.id,
                         row.t_hosts.f_ipv4,
                         row.t_hosts.f_ipv6,
                         row.t_hosts.f_hostname,
                         highest[1].t_host_os_refs.f_certainty,
                         highest[1].t_host_os_refs.f_class,
                         highest[1].t_host_os_refs.f_family,
                         os_rec.f_cpename,
                         os_rec.f_title,
                         os_rec.f_vendor,
                         os_rec.f_product,
                         os_rec.f_version,
                         os_rec.f_update,
                         os_rec.f_edition,
                         os_rec.f_language
                         ])
    return data

##-------------------------------------------------------------------------
## NetBIOS
##-------------------------------------------------------------------------

@service.jsonrpc
def netbios_list(hostfilter=(None, None)):
    """Returns a list of NetBIOS workgroups/domains for an IP (or all IPs)"""
    data = []
    query = (db.t_netbios.id > 0)
    query = create_hostfilter_query(hostfilter, query, 't_netbios')

    for rec in db(query).select(cache=(cache.ram,120)):
        data.append((rec.t_hosts.f_ipv4, rec.t_hosts.f_ipv6, rec.t_hosts.f_hostname,
                     rec.t_netbios.f_type, rec.t_netbios.f_advertised_names,
                     rec.t_netbios.f_domain, rec.t_netbios.f_lockout_limit,
                     rec.t_netbios.f_lockout_duration, rec.t_netbios.f_shares))
    return data

##-------------------------------------------------------------------------

@service.jsonrpc
def netbios_rpt_table(hostfilter=(None, None)):
    """Returns a list of domains and host count"""
    data = []
    query = (db.t_netbios.id > 0)
    query = create_hostfilter_query(hostfilter, query, 't_netbios')

    count = db.t_netbios.f_domain.count()
    for rec in db(query).select(db.t_netbios.f_domain, count, groupby=db.t_netbios.f_domain, distinct=True):
        data.append((rec.t_netbios.f_domain, rec[count]))
    return data

##-------------------------------------------------------------------------

@service.jsonrpc
def netbios_domain_members(domain=None, hostfilter=(None, None)):
    """Returns a list of domain member IP addresses"""
    data = []
    q = (db.t_netbios.f_domain == domain) & (db.t_netbios.f_hosts_id == db.t_hosts.id)
    q = create_hostfilter_query(hostfilter, q, 't_netbios')
    for rec in db(q).select(cache=(cache.ram,120)):
        data.append(rec.t_hosts.f_ipv4)
    return data

##-------------------------------------------------------------------------

@service.jsonrpc
def netbios_domain_controllers(domain=None, hostfilter=(None, None)):
    """Returns a list of domain controller IPs"""
    data = []
    q = (db.t_netbios.f_domain == domain) & (db.t_netbios.f_hosts_id == db.t_hosts.id)
    q = create_hostfilter_query(hostfilter, q, 't_netbios')
    for rec in db(q).select(cache=(cache.ram,120)):
        if rec.t_netbios.f_type == "PDC" or rec.t_netbios.f_type == "BDC":
            data.append(rec.t_hosts.f_ipv4)
    return data

##-------------------------------------------------------------------------
## DB Table Counts
##-------------------------------------------------------------------------

@service.jsonrpc
def tbl_count(tables = [], hostfilter=(None, None)):
    """Returns the record count of a database or list of dbs"""

    if type(tables) == type(list()):
        data = {}
        for table in tables:
            if table in db:
                query = db[table].id > 0
                query = create_hostfilter_query(hostfilter, query, table)
                data[table] = db(query).count()
        return data
    else:
        if tables in db:
            query = db[tables].id > 0
            query = create_hostfilter_query(hostfilter, query, tables)
            cnt = db(query).count()
            return cnt
        else:
            return 0


##-------------------------------------------------------------------------
## Scanner Import
##-------------------------------------------------------------------------

@service.jsonrpc
def scanner_import(filename=None, filetype='', background=False, asset_group=None, engineer=1, msf_workspace=None, update_hosts=True):
    """Imports a Scanner output file"""

    if not filename:
        return dict(error="No filename provided")

    if filetype.lower() not in ['nexpose', 'nmap', 'qualys']:
        return dict(error="Invalid scanner type")

    if not asset_group:
        return dict(error="No asset group defined")

    if background:
        task_id = db.scheduler_task.insert(
            status='QUEUED',
            application_name=request.application,
            task_name='scanner_import_%s' % (os.path.basename(filename)),
            function_name='scanner_import',
            vars=gluon.contrib.simplejson.dumps({
                'scanner': filetype,
                'filename': filename,
                'asset_group': asset_group,
                'engineer': engineer,
                'msf_workspace': msf_workspace,
                'ip_ignore_list': None,
                'ip_include_list': None,
                'update_hosts': update_hosts,
            }),
            enabled=True,
            start_time = request.now,
            stop_time = request.now+datetime.timedelta(hours=1),
            repeats = 1, # never repeat
            period = 60, # only do it once
            timeout = 3600, # 1 hour in length
            group_name = settings.scheduler_group_name,
            sync_output = 5,
        )
        response = "Scan file import schedule initiated"
    else:
        if filetype.lower() == 'nexpose':
            from skaldship.nexpose import process_xml
            response = process_xml(
                filename=filename,
                asset_group=asset_group,
                engineer=engineer,
                msf_workspace=msf_workspace,
                ip_ignore_list=None,
                ip_include_list=None,
                update_hosts=update_hosts,
            )
        if filetype.lower() == "nmap":
            from skaldship.nmap import process_xml
            response = process_xml(
                filename=filename,
                addnoports=form.vars.f_addnoports,
                asset_group=form.vars.f_asset_group,
                engineer=form.vars.f_engineer,
                msf_workspace=form.vars.f_msf_workspace,
                ip_ignore_list=ip_exclude,
                ip_include_list=ip_include,
                update_hosts=form.vars.f_update_hosts,
            )

    return dict(response=response)

########NEW FILE########
__FILENAME__ = appadmin
# -*- coding: utf-8 -*-

# ##########################################################
# ## make sure administrator is on localhost
# ###########################################################

import os
import socket
import datetime
import copy
import gluon.contenttype
import gluon.fileutils

try:
    import pygraphviz as pgv
except ImportError:
    pgv = None

is_gae = request.env.web2py_runtime_gae or False

# ## critical --- make a copy of the environment

global_env = copy.copy(globals())
global_env['datetime'] = datetime

http_host = request.env.http_host.split(':')[0]
remote_addr = request.env.remote_addr
try:
    hosts = (http_host, socket.gethostname(),
             socket.gethostbyname(http_host),
             '::1', '127.0.0.1', '::ffff:127.0.0.1')
except:
    hosts = (http_host, )

if request.env.http_x_forwarded_for or request.is_https:
    session.secure()
elif (remote_addr not in hosts) and (remote_addr != "127.0.0.1") and \
    (request.function != 'manage'):
    raise HTTP(200, T('appadmin is disabled because insecure channel'))

if request.function == 'manage':
    if not 'auth' in globals() or not request.args:
        redirect(URL(request.controller, 'index'))
    manager_action = auth.settings.manager_actions.get(request.args(0), None)
    if manager_action is None and request.args(0) == 'auth':
        manager_action = dict(role=auth.settings.auth_manager_role,
                              heading=T('Manage Access Control'),
                              tables=[auth.table_user(),
                                      auth.table_group(),
                                      auth.table_permission()])
    manager_role = manager_action.get('role', None) if manager_action else None
    auth.requires_membership(manager_role)(lambda: None)()
    menu = False
elif (request.application == 'admin' and not session.authorized) or \
        (request.application != 'admin' and not gluon.fileutils.check_credentials(request)):
    redirect(URL('admin', 'default', 'index',
                 vars=dict(send=URL(args=request.args, vars=request.vars))))
else:
    response.subtitle = T('Database Administration (appadmin)')
    menu = True

ignore_rw = True
response.view = 'appadmin.html'
if menu:
    response.menu = [[T('design'), False, URL('admin', 'default', 'design',
                 args=[request.application])], [T('db'), False,
                 URL('index')], [T('state'), False,
                 URL('state')], [T('cache'), False,
                 URL('ccache')]]

# ##########################################################
# ## auxiliary functions
# ###########################################################

if False and request.tickets_db:
    from gluon.restricted import TicketStorage
    ts = TicketStorage()
    ts._get_table(request.tickets_db, ts.tablename, request.application)

def get_databases(request):
    dbs = {}
    for (key, value) in global_env.items():
        cond = False
        try:
            cond = isinstance(value, GQLDB)
        except:
            cond = isinstance(value, SQLDB)
        if cond:
            dbs[key] = value
    return dbs


databases = get_databases(None)


def eval_in_global_env(text):
    exec ('_ret=%s' % text, {}, global_env)
    return global_env['_ret']


def get_database(request):
    if request.args and request.args[0] in databases:
        return eval_in_global_env(request.args[0])
    else:
        session.flash = T('invalid request')
        redirect(URL('index'))


def get_table(request):
    db = get_database(request)
    if len(request.args) > 1 and request.args[1] in db.tables:
        return (db, request.args[1])
    else:
        session.flash = T('invalid request')
        redirect(URL('index'))


def get_query(request):
    try:
        return eval_in_global_env(request.vars.query)
    except Exception:
        return None


def query_by_table_type(tablename, db, request=request):
    keyed = hasattr(db[tablename], '_primarykey')
    if keyed:
        firstkey = db[tablename][db[tablename]._primarykey[0]]
        cond = '>0'
        if firstkey.type in ['string', 'text']:
            cond = '!=""'
        qry = '%s.%s.%s%s' % (
            request.args[0], request.args[1], firstkey.name, cond)
    else:
        qry = '%s.%s.id>0' % tuple(request.args[:2])
    return qry


# ##########################################################
# ## list all databases and tables
# ###########################################################
def index():
    return dict(databases=databases)


# ##########################################################
# ## insert a new record
# ###########################################################


def insert():
    (db, table) = get_table(request)
    form = SQLFORM(db[table], ignore_rw=ignore_rw)
    if form.accepts(request.vars, session):
        response.flash = T('new record inserted')
    return dict(form=form, table=db[table])


# ##########################################################
# ## list all records in table and insert new record
# ###########################################################


def download():
    import os
    db = get_database(request)
    return response.download(request, db)


def csv():
    import gluon.contenttype
    response.headers['Content-Type'] = \
        gluon.contenttype.contenttype('.csv')
    db = get_database(request)
    query = get_query(request)
    if not query:
        return None
    response.headers['Content-disposition'] = 'attachment; filename=%s_%s.csv'\
        % tuple(request.vars.query.split('.')[:2])
    return str(db(query, ignore_common_filters=True).select())


def import_csv(table, file):
    table.import_from_csv_file(file)


def select():
    import re
    db = get_database(request)
    dbname = request.args[0]
    try:
        is_imap = db._uri.startswith("imap://")
    except (KeyError, AttributeError, TypeError):
        is_imap = False
    regex = re.compile('(?P<table>\w+)\.(?P<field>\w+)=(?P<value>\d+)')
    if len(request.args) > 1 and hasattr(db[request.args[1]], '_primarykey'):
        regex = re.compile('(?P<table>\w+)\.(?P<field>\w+)=(?P<value>.+)')
    if request.vars.query:
        match = regex.match(request.vars.query)
        if match:
            request.vars.query = '%s.%s.%s==%s' % (request.args[0],
                                                   match.group('table'), match.group('field'),
                                                   match.group('value'))
    else:
        request.vars.query = session.last_query
    query = get_query(request)
    if request.vars.start:
        start = int(request.vars.start)
    else:
        start = 0
    nrows = 0

    step = 100
    fields = []

    if is_imap:
        step = 3
 
    stop = start + step

    table = None
    rows = []
    orderby = request.vars.orderby
    if orderby:
        orderby = dbname + '.' + orderby
        if orderby == session.last_orderby:
            if orderby[0] == '~':
                orderby = orderby[1:]
            else:
                orderby = '~' + orderby
    session.last_orderby = orderby
    session.last_query = request.vars.query
    form = FORM(TABLE(TR(T('Query:'), '', INPUT(_style='width:400px',
                _name='query', _value=request.vars.query or '',
                requires=IS_NOT_EMPTY(
                    error_message=T("Cannot be empty")))), TR(T('Update:'),
                INPUT(_name='update_check', _type='checkbox',
                value=False), INPUT(_style='width:400px',
                _name='update_fields', _value=request.vars.update_fields
                                    or '')), TR(T('Delete:'), INPUT(_name='delete_check',
                _class='delete', _type='checkbox', value=False), ''),
                TR('', '', INPUT(_type='submit', _value=T('submit')))),
                _action=URL(r=request, args=request.args))

    tb = None
    if form.accepts(request.vars, formname=None):
        regex = re.compile(request.args[0] + '\.(?P<table>\w+)\..+')
        match = regex.match(form.vars.query.strip())
        if match:
            table = match.group('table')
        try:
            nrows = db(query, ignore_common_filters=True).count()
            if form.vars.update_check and form.vars.update_fields:
                db(query, ignore_common_filters=True).update(
                    **eval_in_global_env('dict(%s)' % form.vars.update_fields))
                response.flash = T('%s %%{row} updated', nrows)
            elif form.vars.delete_check:
                db(query, ignore_common_filters=True).delete()
                response.flash = T('%s %%{row} deleted', nrows)
            nrows = db(query, ignore_common_filters=True).count()

            if is_imap:
                fields = [db[table][name] for name in
                    ("id", "uid", "created", "to",
                     "sender", "subject")]
            if orderby:
                rows = db(query, ignore_common_filters=True).select(
                              *fields, limitby=(start, stop),
                              orderby=eval_in_global_env(orderby))
            else:
                rows = db(query, ignore_common_filters=True).select(
                    *fields, limitby=(start, stop))
        except Exception, e:
            import traceback
            tb = traceback.format_exc()
            (rows, nrows) = ([], 0)
            response.flash = DIV(T('Invalid Query'), PRE(str(e)))
    # begin handle upload csv
    csv_table = table or request.vars.table
    if csv_table:
        formcsv = FORM(str(T('or import from csv file')) + " ",
                       INPUT(_type='file', _name='csvfile'),
                       INPUT(_type='hidden', _value=csv_table, _name='table'),
                       INPUT(_type='submit', _value=T('import')))
    else:
        formcsv = None
    if formcsv and formcsv.process().accepted:
        try:
            import_csv(db[request.vars.table],
                       request.vars.csvfile.file)
            response.flash = T('data uploaded')
        except Exception, e:
            response.flash = DIV(T('unable to parse csv file'), PRE(str(e)))
    # end handle upload csv

    return dict(
        form=form,
        table=table,
        start=start,
        stop=stop,
        step=step,
        nrows=nrows,
        rows=rows,
        query=request.vars.query,
        formcsv=formcsv,
        tb=tb
    )


# ##########################################################
# ## edit delete one record
# ###########################################################


def update():
    (db, table) = get_table(request)
    keyed = hasattr(db[table], '_primarykey')
    record = None
    db[table]._common_filter = None
    if keyed:
        key = [f for f in request.vars if f in db[table]._primarykey]
        if key:
            record = db(db[table][key[0]] == request.vars[key[
                        0]]).select().first()
    else:
        record = db(db[table].id == request.args(
            2)).select().first()

    if not record:
        qry = query_by_table_type(table, db)
        session.flash = T('record does not exist')
        redirect(URL('select', args=request.args[:1],
                     vars=dict(query=qry)))

    if keyed:
        for k in db[table]._primarykey:
            db[table][k].writable = False

    form = SQLFORM(
        db[table], record, deletable=True, delete_label=T('Check to delete'),
        ignore_rw=ignore_rw and not keyed,
        linkto=URL('select',
                   args=request.args[:1]), upload=URL(r=request,
                                                      f='download', args=request.args[:1]))

    if form.accepts(request.vars, session):
        session.flash = T('done!')
        qry = query_by_table_type(table, db)
        redirect(URL('select', args=request.args[:1],
                 vars=dict(query=qry)))
    return dict(form=form, table=db[table])


# ##########################################################
# ## get global variables
# ###########################################################


def state():
    return dict()


def ccache():
    if is_gae:
        form = FORM(
            P(TAG.BUTTON(T("Clear CACHE?"), _type="submit", _name="yes", _value="yes")))
    else:
        cache.ram.initialize()
        cache.disk.initialize()

        form = FORM(
            P(TAG.BUTTON(
                T("Clear CACHE?"), _type="submit", _name="yes", _value="yes")),
            P(TAG.BUTTON(
                T("Clear RAM"), _type="submit", _name="ram", _value="ram")),
            P(TAG.BUTTON(
                T("Clear DISK"), _type="submit", _name="disk", _value="disk")),
        )

    if form.accepts(request.vars, session):
        session.flash = ""
        if is_gae:
            if request.vars.yes:
                cache.ram.clear()
                session.flash += T("Cache Cleared")
        else:
            clear_ram = False
            clear_disk = False
            if request.vars.yes:
                clear_ram = clear_disk = True
            if request.vars.ram:
                clear_ram = True
            if request.vars.disk:
                clear_disk = True
            if clear_ram:
                cache.ram.clear()
                session.flash += T("Ram Cleared")
            if clear_disk:
                cache.disk.clear()
                session.flash += T("Disk Cleared")
        redirect(URL(r=request))

    try:
        from guppy import hpy
        hp = hpy()
    except ImportError:
        hp = False

    import shelve
    import os
    import copy
    import time
    import math
    from gluon import portalocker

    ram = {
        'entries': 0,
        'bytes': 0,
        'objects': 0,
        'hits': 0,
        'misses': 0,
        'ratio': 0,
        'oldest': time.time(),
        'keys': []
    }

    disk = copy.copy(ram)
    total = copy.copy(ram)
    disk['keys'] = []
    total['keys'] = []

    def GetInHMS(seconds):
        hours = math.floor(seconds / 3600)
        seconds -= hours * 3600
        minutes = math.floor(seconds / 60)
        seconds -= minutes * 60
        seconds = math.floor(seconds)

        return (hours, minutes, seconds)

    if is_gae:
        gae_stats = cache.ram.client.get_stats()
        try:
            gae_stats['ratio'] = ((gae_stats['hits'] * 100) /
                (gae_stats['hits'] + gae_stats['misses']))
        except ZeroDivisionError:
            gae_stats['ratio'] = T("?")
        gae_stats['oldest'] = GetInHMS(time.time() - gae_stats['oldest_item_age'])
        total.update(gae_stats)
    else:
        for key, value in cache.ram.storage.iteritems():
            if isinstance(value, dict):
                ram['hits'] = value['hit_total'] - value['misses']
                ram['misses'] = value['misses']
                try:
                    ram['ratio'] = ram['hits'] * 100 / value['hit_total']
                except (KeyError, ZeroDivisionError):
                    ram['ratio'] = 0
            else:
                if hp:
                    ram['bytes'] += hp.iso(value[1]).size
                    ram['objects'] += hp.iso(value[1]).count
                ram['entries'] += 1
                if value[0] < ram['oldest']:
                    ram['oldest'] = value[0]
                ram['keys'].append((key, GetInHMS(time.time() - value[0])))
        folder = os.path.join(request.folder,'cache')
        if not os.path.exists(folder):
            os.mkdir(folder)
        locker = open(os.path.join(folder, 'cache.lock'), 'a')
        portalocker.lock(locker, portalocker.LOCK_EX)
        disk_storage = shelve.open(
            os.path.join(folder, 'cache.shelve'))
        try:
            for key, value in disk_storage.items():
                if isinstance(value, dict):
                    disk['hits'] = value['hit_total'] - value['misses']
                    disk['misses'] = value['misses']
                    try:
                        disk['ratio'] = disk['hits'] * 100 / value['hit_total']
                    except (KeyError, ZeroDivisionError):
                        disk['ratio'] = 0
                else:
                    if hp:
                        disk['bytes'] += hp.iso(value[1]).size
                        disk['objects'] += hp.iso(value[1]).count
                    disk['entries'] += 1
                    if value[0] < disk['oldest']:
                        disk['oldest'] = value[0]
                    disk['keys'].append((key, GetInHMS(time.time() - value[0])))
        finally:
            portalocker.unlock(locker)
            locker.close()
            disk_storage.close()

        total['entries'] = ram['entries'] + disk['entries']
        total['bytes'] = ram['bytes'] + disk['bytes']
        total['objects'] = ram['objects'] + disk['objects']
        total['hits'] = ram['hits'] + disk['hits']
        total['misses'] = ram['misses'] + disk['misses']
        total['keys'] = ram['keys'] + disk['keys']
        try:
            total['ratio'] = total['hits'] * 100 / (total['hits'] +
                                                total['misses'])
        except (KeyError, ZeroDivisionError):
            total['ratio'] = 0

        if disk['oldest'] < ram['oldest']:
            total['oldest'] = disk['oldest']
        else:
            total['oldest'] = ram['oldest']

        ram['oldest'] = GetInHMS(time.time() - ram['oldest'])
        disk['oldest'] = GetInHMS(time.time() - disk['oldest'])
        total['oldest'] = GetInHMS(time.time() - total['oldest'])

    def key_table(keys):
        return TABLE(
            TR(TD(B(T('Key'))), TD(B(T('Time in Cache (h:m:s)')))),
            *[TR(TD(k[0]), TD('%02d:%02d:%02d' % k[1])) for k in keys],
            **dict(_class='cache-keys',
                   _style="border-collapse: separate; border-spacing: .5em;"))

    if not is_gae:
        ram['keys'] = key_table(ram['keys'])
        disk['keys'] = key_table(disk['keys'])
        total['keys'] = key_table(total['keys'])

    return dict(form=form, total=total,
                ram=ram, disk=disk, object_stats=hp != False)


def table_template(table):
    from gluon.html import TR, TD, TABLE, TAG

    def FONT(*args, **kwargs):
        return TAG.font(*args, **kwargs)

    def types(field):
        f_type = field.type
        if not isinstance(f_type,str):
            return ' '
        elif f_type == 'string':
            return field.length
        elif f_type == 'id':
            return B('pk')
        elif f_type.startswith('reference') or \
                f_type.startswith('list:reference'):
            return B('fk')
        else:
            return ' '

    # This is horribe HTML but the only one graphiz understands
    rows = []
    cellpadding = 4
    color = "#000000"
    bgcolor = "#FFFFFF"
    face = "Helvetica"
    face_bold = "Helvetica Bold"
    border = 0

    rows.append(TR(TD(FONT(table, _face=face_bold, _color=bgcolor),
                           _colspan=3, _cellpadding=cellpadding,
                           _align="center", _bgcolor=color)))
    for row in db[table]:
        rows.append(TR(TD(FONT(row.name, _color=color, _face=face_bold),
                              _align="left", _cellpadding=cellpadding,
                              _border=border),
                       TD(FONT(row.type, _color=color, _face=face),
                               _align="left", _cellpadding=cellpadding,
                               _border=border),
                       TD(FONT(types(row), _color=color, _face=face),
                               _align="center", _cellpadding=cellpadding,
                               _border=border)))
    return "< %s >" % TABLE(*rows, **dict(_bgcolor=bgcolor, _border=1,
                                          _cellborder=0, _cellspacing=0)
                             ).xml()


def bg_graph_model():
    graph = pgv.AGraph(layout='dot',  directed=True,  strict=False,  rankdir='LR')

    subgraphs = dict()
    for tablename in db.tables:
        if hasattr(db[tablename],'_meta_graphmodel'):
            meta_graphmodel = db[tablename]._meta_graphmodel
        else:
            meta_graphmodel = dict(group='Undefined', color='#ECECEC')

        group = meta_graphmodel['group'].replace(' ', '')
        if not subgraphs.has_key(group):
            subgraphs[group] = dict(meta=meta_graphmodel, tables=[])
            subgraphs[group]['tables'].append(tablename)
        else:
            subgraphs[group]['tables'].append(tablename)

        graph.add_node(tablename, name=tablename, shape='plaintext',
                       label=table_template(tablename))

    for n, key in enumerate(subgraphs.iterkeys()):
        graph.subgraph(nbunch=subgraphs[key]['tables'],
                    name='cluster%d' % n,
                    style='filled',
                    color=subgraphs[key]['meta']['color'],
                    label=subgraphs[key]['meta']['group'])

    for tablename in db.tables:
        for field in db[tablename]:
            f_type = field.type
            if isinstance(f_type,str) and (
                f_type.startswith('reference') or
                f_type.startswith('list:reference')):
                referenced_table = f_type.split()[1].split('.')[0]
                n1 = graph.get_node(tablename)
                n2 = graph.get_node(referenced_table)
                graph.add_edge(n1, n2, color="#4C4C4C", label='')

    graph.layout()
    if not request.args:
        response.headers['Content-Type'] = 'image/png'
        return graph.draw(format='png', prog='dot')
    else:
        response.headers['Content-Disposition']='attachment;filename=graph.%s'%request.args(0)
        if request.args(0) == 'dot':
            return graph.string()
        else:
            return graph.draw(format=request.args(0), prog='dot')

def graph_model():
    return dict(databases=databases, pgv=pgv)

def manage():
    tables = manager_action['tables']
    if isinstance(tables[0], str):
        db = manager_action.get('db', auth.db)
        db = globals()[db] if isinstance(db, str) else db
        tables = [db[table] for table in tables]
    if request.args(0) == 'auth':
        auth.table_user()._plural = T('Users')
        auth.table_group()._plural = T('Roles')
        auth.table_membership()._plural = T('Memberships')
        auth.table_permission()._plural = T('Permissions')
    if request.extension != 'load':
        return dict(heading=manager_action.get('heading',
                    T('Manage %(action)s') % dict(action=request.args(0).replace('_', ' ').title())),
                    tablenames=[table._tablename for table in tables],
                    labels=[table._plural.title() for table in tables])

    table = tables[request.args(1, cast=int)]
    formname = '%s_grid' % table._tablename
    linked_tables = orderby = None
    if request.args(0) == 'auth':
        auth.table_group()._id.readable = \
        auth.table_membership()._id.readable = \
        auth.table_permission()._id.readable = False
        auth.table_membership().user_id.label = T('User')
        auth.table_membership().group_id.label = T('Role')
        auth.table_permission().group_id.label = T('Role')
        auth.table_permission().name.label = T('Permission')
        if table == auth.table_user():
            linked_tables=[auth.settings.table_membership_name]
        elif table == auth.table_group():
            orderby = 'role' if not request.args(3) or '.group_id' not in request.args(3) else None
        elif table == auth.table_permission():
            orderby = 'group_id'
    kwargs = dict(user_signature=True, maxtextlength=1000,
                  orderby=orderby, linked_tables=linked_tables)
    smartgrid_args = manager_action.get('smartgrid_args', {})
    kwargs.update(**smartgrid_args.get('DEFAULT', {}))
    kwargs.update(**smartgrid_args.get(table._tablename, {}))
    grid = SQLFORM.smartgrid(table, args=request.args[:2], formname=formname, **kwargs)
    return grid

########NEW FILE########
__FILENAME__ = cpe
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## CPE controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir

@auth.requires_login()
def index():
    return dict()

##-------------------------------------------------------------------------
## os
##-------------------------------------------------------------------------

@auth.requires_signature()
@auth.requires_login()
def os_add_to_kvasir():
    """
    Adds a CPE OS record to the current Kvasir t_os db
    """
    count = 0
    if request.vars.has_key('ids'):
        for arg in request.vars.ids.split('|'):
            if arg is not '':
                cpe_record = db.t_cpe_os[arg]
                if cpe_record is None: continue
                osinfo = {}
                osinfo['f_cpename'] = cpe_record.f_cpename
                osinfo['f_edition'] = cpe_record.f_edition
                osinfo['f_language'] = cpe_record.f_language
                osinfo['f_product'] = cpe_record.f_product
                osinfo['f_title'] = cpe_record.f_title
                osinfo['f_vendor'] = cpe_record.f_vendor
                osinfo['f_version'] = cpe_record.f_version
                osinfo['f_isincpe'] = True
                try:
                    db.t_os.insert(**osinfo)
                    count += 1
                except:
                    pass
                db.commit()
    response.flash = "Added %s CPE OS record(s) to Kvasir" % (count)
    response.headers['web2py-component-command'] = "jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return

@auth.requires_login()
def os_add():
    response.title = "%s :: Add CPE OS" % (settings.title)
    form=crud.create(db.t_cpe_os,next='os_edit/[id]')
    return dict(form=form)

@auth.requires_login()
def os_edit():
    record = db.t_cpe_os(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('OS record not found')}))
    response.title = "%s :: Update CPE OS :: %s" % (settings.title, record.f_title)
    form=crud.update(db.t_cpe_os,record,next='os_edit/[id]',
                     ondelete=lambda form: redirect(URL('os_list')),
                     onaccept=crud.archive)
    return dict(form=form)

@auth.requires_login()
def os_list():
    response.title = "%s :: CPE Operating Systems" % (settings.title)
    if request.extension == 'json':
        if request.vars.has_key('iDisplayStart'):
            start = int(request.vars.iDisplayStart)
        else:
            start = 0
        if request.vars.has_key('iDisplayLength'):
            limit = start + int(request.vars.iDisplayLength)
            if limit == -1:
                limit = db(db.t_cpe_os).count()
        else:
            limit = int(auth.user.f_show_size)
        if request.vars.has_key('sSearch'):
            # sSearch global search box
            # only need to do cpename and title since the other fields
            # are just these broken out
            query = db.t_cpe_os.f_cpename.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_cpe_os.f_title.like("%%%s%%" % request.vars.sSearch)
        else:
            query = db.t_cpe_os

        if request.vars.iSortingCols == '1':
            # sorting by a column
            cols = ( db.t_cpe_os.id,
                     db.t_cpe_os.f_cpename,
                     db.t_cpe_os.f_title,
                     db.t_cpe_os.f_vendor,
                     db.t_cpe_os.f_product,
                     db.t_cpe_os.f_version,
                     db.t_cpe_os.f_update,
                     db.t_cpe_os.f_edition,
                     db.t_cpe_os.f_language,
                   )

            orderby = cols[int(request.vars.iSortCol_0)]
            if request.vars.sSortDir_0 == 'asc':
                rows=db(query)(db.t_cpe_os).select(orderby=orderby, limitby=(start, limit))
            else:
                rows=db(query)(db.t_cpe_os).select(orderby=~orderby, limitby=(start, limit))
        else:
            rows=db(query)(db.t_cpe_os).select(limitby=(start, limit))

        nolimit=db(query)(db.t_cpe_os).count()

        aaData = []
        # datatable formatting is specific
        for r in rows:
            atxt = []
            atxt.append(A(r.id, _target="os_update_%s" % (r.id), _href="os_edit/%s" % (r.id)).xml())
            atxt.append('<input class="add_to_kvasir" name="sel_id" id="sel_id" value="' + str(r.id) + '" type="checkbox">')
            atxt.append(r.f_cpename)
            atxt.append(r.f_title)
            atxt.append(r.f_vendor)
            atxt.append(r.f_product)
            atxt.append(r.f_version)
            atxt.append(r.f_update)
            atxt.append(r.f_edition)
            atxt.append(r.f_language)
            # add columns after this, don't do anything prior since it'll affect the hidden fields

            aaData.append(atxt)

        totalrecords = db(db.t_cpe_os).count()

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': totalrecords,
                   'iTotalDisplayRecords': nolimit,
                   'aaData': aaData,
                 }

        return result
    else:
        return dict()

##-------------------------------------------------------------------------
## apps
##-------------------------------------------------------------------------

'''
@auth.requires_login()
def apps_add():
    response.title = "%s :: Add CPE Application" % (settings.title)
    form=crud.create(db.t_cpe_apps,next='apps_edit/[id]')
    return dict(form=form)

@auth.requires_signature()
@auth.requires_login()
def apps_add_to_kvasir():
    """
    Adds a CPE App record to the current Kvasir t_os db
    """
    count = 0
    if request.vars.has_key('ids'):
        for arg in request.vars.ids.split('|'):
            if arg is not '':
                cpe_record = db.t_cpe_apps[arg]
                if cpe_record is None: continue
                info = {}
                info['f_cpename'] = cpe_record.f_cpename
                info['f_edition'] = cpe_record.f_edition
                info['f_language'] = cpe_record.f_language
                info['f_product'] = cpe_record.f_product
                info['f_title'] = cpe_record.f_title
                info['f_vendor'] = cpe_record.f_vendor
                info['f_version'] = cpe_record.f_version
                info['f_isincpe'] = True
                try:
                    db.t_apps.insert(**info)
                    count += 1
                except:
                    pass
                db.commit()
    response.flash = "Added %s CPE App record(s) to Kvasir" % (count)
    response.headers['web2py-component-command'] = "jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return

@auth.requires_login()
def apps_edit():
    record = db.t_cpe_apps(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Apps record not found')}))
    response.title = "%s :: Edit CPE Application :: %s" % (settings.title, record.f_title)
    form=crud.update(db.t_cpe_apps,record,next='apps_edit/[id]',
                     ondelete=lambda form: redirect(URL('apps_list')),
                     onaccept=crud.archive)
    return dict(form=form)

@auth.requires_login()
def apps_list():
    response.title = "%s :: CPE Applications" % (settings.title)
    if request.extension == 'json':
        if request.vars.has_key('iDisplayStart'):
            start = int(request.vars.iDisplayStart)
        else:
            start = 0
        if request.vars.has_key('iDisplayLength'):
            limit = start + int(request.vars.iDisplayLength)
            if limit == -1:
                limit = db(db.t_cpe_apps).count()
        else:
            limit = int(auth.user.f_show_size)
        if request.vars.has_key('sSearch'):
            # sSearch global search box
            # only need to do cpename and title since the other fields
            # are just these broken out
            query = db.t_cpe_apps.f_cpename.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_cpe_apps.f_title.like("%%%s%%" % request.vars.sSearch)
        else:
            query = db.t_cpe_apps

        if request.vars.iSortingCols == '1':
            # sorting by a column
            cols = ( db.t_cpe_apps.id,
                     db.t_cpe_apps.f_cpename,
                     db.t_cpe_apps.f_title,
                     db.t_cpe_apps.f_vendor,
                     db.t_cpe_apps.f_product,
                     db.t_cpe_apps.f_version,
                     db.t_cpe_apps.f_update,
                     db.t_cpe_apps.f_edition,
                     db.t_cpe_apps.f_language,
                   )

            orderby = cols[int(request.vars.iSortCol_0)]
            if request.vars.sSortDir_0 == 'asc':
                rows=db(query)(db.t_cpe_apps).select(orderby=orderby, limitby=(start, limit))
            else:
                rows=db(query)(db.t_cpe_apps).select(orderby=~orderby, limitby=(start, limit))
        else:
            rows=db(query)(db.t_cpe_apps).select(limitby=(start, limit))

        nolimit=db(query)(db.t_cpe_apps).count()

        aaData = []
        # datatable formatting is specific
        for r in rows:
            atxt = []
            atxt.append(A(r.id, _target="apps_update_%s" % (r.id), _href="apps_edit/%s" % (r.id)).xml())
            atxt.append('<input class="add_to_kvasir" name="sel_id" id="sel_id" value="' + str(r.id) + '" type="checkbox">')
            atxt.append(r.f_cpename)
            atxt.append(r.f_title)
            atxt.append(r.f_vendor)
            atxt.append(r.f_product)
            atxt.append(r.f_version)
            atxt.append(r.f_update)
            atxt.append(r.f_edition)
            atxt.append(r.f_language)
            # add columns after this, don't do anything prior since it'll affect the hidden fields

            aaData.append(atxt)

        totalrecords = db(db.t_cpe_apps).count()

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': totalrecords,
                   'iTotalDisplayRecords': nolimit,
                   'aaData': aaData,
                 }

        return result
    else:
        return dict()

##-------------------------------------------------------------------------
## hardware
##-------------------------------------------------------------------------

@auth.requires_login()
def hardware_add():
    response.title = "%s :: Add CPE Hardware" % (settings.title)
    form=crud.create(t_cpe_hardware_refs,next='hardware_edit/[id]')
    return dict(form=form)

@auth.requires_login()
def hardware_edit():
    record = db.t_app_fingerprints(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Hardware record not found')}))
    response.title = "%s :: Edit CPE Hardware :: %s" % (settings.title, record.f_title)
    form=crud.update(t_cpe_hardware,record,next='hardware_edit/[id]',
                     ondelete=lambda form: redirect(URL('hardware_list')),
                     onaccept=crud.archive)
    return dict(form=form)

@auth.requires_login()
def hardware_list():
    response.title = "%s :: CPE Hardware" % (settings.title)
    if request.extension == 'json':
        if request.vars.has_key('iDisplayStart'):
            start = int(request.vars.iDisplayStart)
        else:
            start = 0
        if request.vars.has_key('iDisplayLength'):
            limit = start + int(request.vars.iDisplayLength)
            if limit == -1:
                limit = db(db.t_pe_hardware).count()
        else:
            limit = int(auth.user.f_show_size)
        if request.vars.has_key('sSearch'):
            # sSearch global search box
            # only need to do cpename and title since the other fields
            # are just these broken out
            query = db.t_cpe_hardware.f_cpename.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_cpe_hardware.f_title.like("%%%s%%" % request.vars.sSearch)
        else:
            query = db.t_cpe_hardware

        if request.vars.iSortingCols == '1':
            # sorting by a column
            cols = ( db.t_cpe_hardware.id,
                     db.t_cpe_hardware.f_cpename,
                     db.t_cpe_hardware.f_title,
                     db.t_cpe_hardware.f_vendor,
                     db.t_cpe_hardware.f_product,
                     db.t_cpe_hardware.f_version,
                     db.t_cpe_hardware.f_update,
                     db.t_cpe_hardware.f_edition,
                     db.t_cpe_hardware.f_language,
                   )

            orderby = cols[int(request.vars.iSortCol_0)]
            if request.vars.sSortDir_0 == 'asc':
                rows=db(query)(db.t_cpe_hardware).select(orderby=orderby, limitby=(start, limit))
            else:
                rows=db(query)(db.t_cpe_hardware).select(orderby=~orderby, limitby=(start, limit))
        else:
            rows=db(query)(db.t_cpe_hardware).select(limitby=(start, limit))

        nolimit=db(query)(db.t_cpe_hardware).count()

        aaData = []
        # datatable formatting is specific
        for r in rows:
            atxt = []
            atxt.append(A(r.id, _target="hardware_update_%s" % (r.id), _href="hardware_edit/%s" % (r.id)).xml())
            atxt.append(r.f_cpename)
            atxt.append(r.f_title)
            atxt.append(r.f_vendor)
            atxt.append(r.f_product)
            atxt.append(r.f_version)
            atxt.append(r.f_update)
            atxt.append(r.f_edition)
            atxt.append(r.f_language)
            # add columns after this, don't do anything prior since it'll affect the hidden fields

            aaData.append(atxt)

        totalrecords = db(db.t_cpe_hardware).count()

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': totalrecords,
                   'iTotalDisplayRecords': nolimit,
                   'aaData': aaData,
                 }

        return result
    else:
        return dict()
'''

##-------------------------------------------------------------------------
## Purge CPE data from database
##-------------------------------------------------------------------------
@auth.requires_login()
def purge():
    response.title = "%s :: CPE Database Purge" % (settings.title)
    form = SQLFORM.factory(
        Field('cpe_os', type='boolean', label=T('CPE OS Database')),
        Field('cpe_apps', type='boolean', label=T('CPE Apps Database')),
        Field('cpe_hardware', type='boolean', label=T('CPE HW Database')),
        Field('are_you_sure', type='boolean', label=T('Are you sure?')),
        )

    if form.accepts(request.vars,session):
        if not form.vars.are_you_sure:
            form.errors.are_you_sure = 'ARE YOU SURE?'
        if form.vars.cpe_os:
            response.flash = 'Deleted CPE OS Data'

        if form.vars.cpe_apps:
            response.flash = 'Deleted CPE Apps Data'
        if form.vars.cpe_hardware:
            response.flash = 'Deleted CPE Hardware Data'
    elif form.errors:
        response.flash = 'Error in form'

    response.title = "%s :: CPE Purge" % (settings.title)
    return dict(form=form)

##-------------------------------------------------------------------------
## Import CPE XML file
##-------------------------------------------------------------------------
@auth.requires_login()
def import_cpe_xml():
    import os, sys
    response.title = "%s :: Import CPE XML Data" % (settings.title)

    form = SQLFORM.factory(
        Field('f_filename', 'upload', label=T('XML File'), uploadfolder=os.path.join(request.folder, 'data', 'misc')),
        Field('f_download_cpe', 'boolean', label=T('D/L from MITRE')),
        Field('f_wipe', 'boolean', label=T('Clear existing'), comment=T('Clears existing entries before importing.')),
        Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background')),
        table_name='cpe_xml'
    )

    if form.accepts(request.vars, session):
        if form.vars.f_filename:
            filename = os.path.join(request.folder,'data','misc',form.vars.f_filename)
        else:
            if not form.vars.f_download_cpe:
                form.errors.f_filename = "Must select file or download from MITRE"
                response.flash = 'Error in submission'
            filename = None

        if filename or form.vars.f_download_cpe:
            if form.vars.f_taskit:
                task = scheduler.queue_task(
                    cpe_import_xml,
                    pargs = [filename, form.vars.f_download_cpe, form.vars.f_wipe],
                    group_name = settings.scheduler_group_name,
                    sync_output = 5,
                    timeout = 1800   # 1/2 hour
                )
                if task.id:
                    redirect(URL('tasks', 'status', args=task.id))
                else:
                    resp_text = "Error submitting job: %s" % (task.errors)
            else:
                from skaldship.cpe import process_xml
                res = process_xml(filename, form.vars.f_download_cpe, form.vars.f_wipe)
                response.flash = res

    elif form.errors:
        response.flash = 'Error in submission'
    else:
        pass

    return dict(form=form)

##-------------------------------------------------------------------------
## backup/restore processes
##-------------------------------------------------------------------------

@auth.requires_login()
def backup():
    """
    Backup CPE database to CSV
    """
    s = StringIO.StringIO()
    db(db.t_cpe_os).select().export_to_csv_file(s)
    response.headers['Content-Type'] = 'text/csv'
    return s.getvalue()

########NEW FILE########
__FILENAME__ = default
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Default controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.general import get_oreally_404
from skaldship.hosts import host_title_maker
from skaldship.statistics import db_statistics, adv_db_statistics, graphs_index
crud.settings.formstyle = formstyle_bootstrap_kvasir

try:
    import cStringIO as StringIO
except ImportError:
    import StringIO

import logging
logger = logging.getLogger("web2py.app.kvasir")

### required - do no delete
def user():
    response.title = T('Kvasir Login')
    return dict(form=auth())

@auth.requires_login()
def download(): return response.download(request,db)

@auth.requires_login()
def call():
    session.forget()
    return service()
### end requires

@auth.requires_login()
def index():
    """
    Kvasir welcoming index page.
    """
    response.files.append(URL(request.application, 'static', 'js/jquery.tagcloud-2.js'))
    response.files.append(URL(request.application, 'static', 'js/highcharts.js'))

    statistics = db_statistics()
    adv_stats = adv_db_statistics()
    graphs = graphs_index()
    return dict(statistics=statistics, adv_stats=adv_stats, graphs=graphs)

def error():
    response.title = "%s :: Error!" % (settings.title)
    return dict(
        err404=get_oreally_404(request.folder),
        msg=request.vars.msg
    )

@auth.requires_login()
def purge_data():
    # Purges all the data except user tables
    response.title = "%s :: Database Purge" % (settings.title)

    users = db(db.auth_user).select()
    userlist = []
    for user in users:
        userlist.append( [ user.id, user.username ] )

    hosts = db(db.t_hosts).select()
    hostlist = []
    for host in hosts:
        hostlist.append( [ host.id, host_title_maker(host) ] )

    ag_rows = db(db.t_hosts).select(db.t_hosts.f_asset_group, distinct=True).as_list()
    asset_groups = []
    for ag in ag_rows:
        asset_groups.append(ag['f_asset_group'])

    form = SQLFORM.factory(
        Field('host', type='list:integer', label=T('Delete a host'), requires=IS_EMPTY_OR(IS_IN_SET(hostlist))),
        Field('engineer', type='list:integer', label=T('Hosts by user'), requires=IS_EMPTY_OR(IS_IN_SET(userlist))),
        Field('asset_group', type='string', label=T('Asset Group'), requires=IS_EMPTY_OR(IS_IN_SET(asset_groups))),
        Field('all_data', type='boolean', label=T('Truncate all tables')),
        Field('are_you_sure', type='boolean', label=T('Are you sure?'), requires=IS_NOT_EMPTY(error_message='ARE YOU SURE?!?!')),
        )

    if form.accepts(request.vars, session):
        if not form.vars.are_you_sure:
            form.errors.are_you_sure = 'ARE YOU SURE?'
        else:
            if form.vars.all_data:
                db.t_hosts.truncate(mode="CASCADE")
                db.t_services.truncate(mode="CASCADE")
                db.t_os.truncate(mode="CASCADE")
                db.t_host_os_refs.truncate(mode="CASCADE")
                db.t_apps.truncate(mode="CASCADE")
                db.t_services_apps_refs.truncate(mode="CASCADE")
                db.t_service_vulns.truncate(mode="CASCADE")
                db.t_service_info.truncate(mode="CASCADE")
                db.t_accounts.truncate(mode="CASCADE")
                db.t_host_notes.truncate(mode="CASCADE")
                db.t_evidence.truncate(mode="CASCADE")
                db.t_snmp.truncate(mode="CASCADE")
                db.commit()
                response.flash = 'All data purged'
            elif form.vars.host:
                host_id = form.vars.host
                del db.t_hosts[host_id]
                response.flash = "Host %s purged" % (form.vars.host)
            elif form.vars.engineer:
                # TODO: Test this
                delcnt = db(db.t_hosts.f_engineer == form.vars.engineer).delete()
                db.commit()
                response.flash = "Hosts owned by %s purged (%d records)" % (form.vars.engineer, delcnt)
            elif form.vars.asset_group:
                delcnt = db(db.t_hosts.f_asset_group == form.vars.asset_group).delete()
                db.commit()
                response.flash = "Asset group %s purged (%d records)" % (form.vars.asset_group, delcnt)
    elif form.errors:
        response.flash = 'Error in form'

    return dict(
        form=form,
        err404=get_oreally_404(request.folder),
    )

@auth.requires_login()
def update_dynamic_fields():
    """
    Executes the following functions that update dynamic field entries:

       skaldship.hosts.do_host_status
       skaldship.exploits.connect_exploits
    """
    response.title = "%s :: Update Dynamic Fields" % (settings.title)

    users = db(db.auth_user).select()
    userlist = []
    for user in users:
        userlist.append( [ user.id, user.username ] )

    ag = db(db.t_hosts).select(db.t_hosts.f_asset_group, distinct=True).as_list()
    asset_groups = map((lambda x: x['f_asset_group']), ag)

    form = SQLFORM.factory(
        Field('f_exploit_link', type='boolean', default=True, label=T('Exploit linking')),
        Field('f_host_status', type='boolean', default=True, label=T('Host Service/Vuln counts')),
        Field('f_asset_group', type='list:string', label=T('Asset Group'), requires=IS_EMPTY_OR(IS_IN_SET(asset_groups, multiple=False))),
        Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background task')),
    )

    from skaldship.hosts import do_host_status
    from skaldship.exploits import connect_exploits
    if form.accepts(request.vars, session):
        if form.vars.f_exploit_link:
            connect_exploits()
        if form.vars.f_host_status:
            if form.vars.f_taskit:
                task = scheduler.queue_task(
                    do_host_status,
                    pvars=dict(asset_group=form.vars.f_asset_group),
                    group_name=settings.scheduler_group_name,
                    sync_output=5,
                    timeout=300   # 5 minutes
                )
                if task.id:
                    redirect(URL('tasks', 'status', args=task.id))
                else:
                    resp_text = "Error submitting job: %s" % (task.errors)
            else:
                do_host_status(asset_group=form.vars.f_asset_group)
        response.flash = "Task completed!"

    elif form.errors:
        response.flash = 'Error in form'

    return dict(
        form=form,
        err404=get_oreally_404(request.folder),
    )

@auth.requires_login()
def database_backup():
    """
    Export/backup database in CSV format
    """
    response.title = "%s :: Database CSV Backup" % (settings.title)
    import os
    import gzip
    from datetime import datetime

    backup_dir = os.path.join(request.folder, 'data/backups')
    form = SQLFORM.factory(
        Field('download', type='boolean', default=True, label=T('Download')),
        Field('gzip', type='boolean', default=True, label=T('GZip Compress')),
    )

    if form.process().accepted:
        fname = "kvasir-%s-%s-%s" % (settings.customer, settings.assesment_type, datetime.now().strftime("%m%d%y-%H%M%S"))
        fname= "".join([x if x.isalnum() else "_" for x in fname])
        if form.vars.download:
            # generate csv and download
            s = StringIO.StringIO()
            db.export_to_csv_file(s)
            response.headers['Content-Type'] = 'text/csv'
            if form.vars.gzip:
                gz_s = StringIO.StringIO()
                response.headers['Content-Disposition'] = 'attachment; filename="%s.gz"' % fname
                gz = gzip.GzipFile(filename='temp.gz', mode='wb', fileobj=gz_s)
                gz.write(s.getvalue())
                gz.close()
                return gz_s.getvalue()
            else:
                response.headers['Content-Disposition'] = 'attachment; filename="%s"' % fname
                return s.getvalue()
        else:
            # generate csv and store locally
            from skaldship.general import check_datadir
            check_datadir(request.folder)
            tmpfile = os.path.join(request.folder, 'data/backups', fname)
            if form.vars.gzip:
                fobj = gzip.open(tmpfile + '.gz', 'wb')
            else:
                fobj = open(tmpfile, 'wb')
            db.export_to_csv_file(fobj)
            return redirect(URL('default', 'data_dir/backups'))
    elif form.errors:
        response.flash = "Error in form"

    return dict(form=form, backup_dir=backup_dir)

@auth.requires_login()
def database_restore():
    """
    Retore a database from CSV data
    """
    response.title = "%s :: Database CSV Restore" % (settings.title)
    import os
    import glob
    from skaldship.general import check_datadir

    check_datadir(request.folder)
    backup_dir = os.path.join(request.folder, 'data/backups')
    os.chdir(backup_dir)
    csv_files = []
    for ext in ["*.csv", "*.csv.gz"]:
        csv_files.extend(glob.glob(ext))

    form = SQLFORM.factory(
        Field('file', type='string', requires=IS_EMPTY_OR(IS_IN_SET(csv_files)), label=T('Local File')),
        Field('upload', type='upload', uploadfolder=backup_dir, label=T('Upload CSV File')),
        Field('wipe', type='boolean', default=False, label=T('Wipe all existing data'))
    )

    if form.process().accepted:
        if form.vars.wipe:
            db.t_hosts.truncate(mode="CASCADE")
            db.t_services.truncate(mode="CASCADE")
            db.t_os.truncate(mode="CASCADE")
            db.t_host_os_refs.truncate(mode="CASCADE")
            db.t_apps.truncate(mode="CASCADE")
            db.t_services_apps_refs.truncate(mode="CASCADE")
            db.t_service_vulns.truncate(mode="CASCADE")
            db.t_service_info.truncate(mode="CASCADE")
            db.t_accounts.truncate(mode="CASCADE")
            db.t_host_notes.truncate(mode="CASCADE")
            db.t_evidence.truncate(mode="CASCADE")
            db.t_snmp.truncate(mode="CASCADE")
            db.commit()

        if form.vars.file:
            fname = form.vars.file
        elif form.vars.upload:
            fname = form.vars.upload

        if fname.endswith('.gz'):
            import gzip
            fobj = gzip.open(fname, 'rb')
        else:
            fobj = open(fname, 'rb')

        db.import_from_csv_file(fobj)

    elif form.errors:
        response.flash = "Error in form"

    os.chdir(request.folder)
    return dict(form=form, backup_dir=backup_dir)

@auth.requires_login()
def wiki():
    # web2py wiki is a bit of a beast in a multi-user environment
    # The db.auth_user.id == 1 has implicit auth_editor permission. The databases for the wiki are
    # not created until the person who installed the database visits the wiki.
    # Any user id > 1 must manually be added to the auth_editor group unless manage_permission is False
    return auth.wiki(migrate=settings.migrate, manage_permissions=False)

@auth.requires_login()
def data_dir():
    from gluon.tools import Expose
    import os
    response.title = "%s :: Browsing Data Directory" % (settings.title)
    data_path = os.path.join(request.folder, 'data')
    return dict(files=Expose(data_path))

def ip_calc():
    response.files.append(URL(request.application, 'static', 'js/ipv4_calc.js'))
    response.files.append(URL(request.application, 'static', 'js/teredo_calc.js'))
    response.title = "%s :: IP Calculators" % (settings.title)
    return dict()

def redirect():
    redirect_url = request.vars.get('url', '')
    pwnwiki = request.vars.get('pwniki', False)
    response.title = "%s :: Redirecting to %s" % (settings.title, redirect_url)
    return dict(redirect_url=redirect_url, pwnwiki=pwnwiki)

########NEW FILE########
__FILENAME__ = evidence
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Evidence controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import get_host_record, host_title_maker, host_a_maker, create_hostfilter_query
from datetime import datetime
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir

@auth.requires_login()
def index():
    return dict()

##-------------------------------------------------------------------------
## evidence
##-------------------------------------------------------------------------

@auth.requires_login()
def add():
    if request.args(0) is not None:
        record = get_host_record(request.args(0))
        db.t_evidence.f_hosts_id.default = record.id
    else:
        record = None

    if request.extension == 'load':
        buttons=[]
    else:
        buttons=['submit']

    if record:
        form=SQLFORM(db.t_evidence, buttons=buttons, upload=URL('download'), fields=['f_type', 'f_other_type', 'f_text', 'f_evidence'],
                     _action=URL('add', args=[ record.id ]), _id="evidence_add_form")
    else:
        form=SQLFORM(db.t_evidence, buttons=buttons, upload=URL('download'), fields=['f_hosts_id', 'f_type', 'f_other_type', 'f_text', 'f_evidence'],
                     _action=URL('add'), _id="evidence_add_form")

    if request.vars.f_evidence is not None:
        form.vars.f_filename = request.vars.f_evidence.filename
    if form.accepts(request.vars, session):
        response.flash = "Evidence added"
        response.headers['web2py-component-command'] = 'evidencetable.fnReloadAjax();'
        return ""
    elif form.errors:
        response.flash = "Error in form submission"
        return TABLE(*[TR(k, v) for k, v in form.errors.items()])

    db.t_evidence.f_hosts_id.default = None
    response.title = "%s :: Add Evidence" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def edit():
    record = db.t_evidence(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Evidence record not found')}))
    response.title = "%s :: Evidence Update :: %s" % (settings.title, host_title_maker(db.t_hosts[record.f_hosts_id]))
    form=crud.update(db.t_evidence,record,next='edit/[id]',
                     ondelete=lambda form: redirect(URL('list')))
    return dict(form=form)

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    if request.vars.has_key('ids'):
        for r in request.vars.ids.split('|'):
            if r is not '':
                db(db.t_evidence.id == r).delete()
                count += 1
    db.commit()
    response.flash = "%s Evidence record(s) deleted" % (count)
    response.headers['web2py-component-command'] = 'evidencetable.fnReloadAjax();'
    return

@auth.requires_login()
def download():
    import gluon.contenttype as cc
    f_evidence =request.args[0]

    row=db(db.t_evidence.f_evidence==f_evidence).select(db.t_evidence.f_data, db.t_evidence.f_filename, db.t_evidence.f_evidence).first()

    response.headers['Content-Type']=cc.contenttype(f_evidence)
    # convert unknowns (x-XXXX) into text/plain
    if "/x-" in response.headers['Content-Type']:
        response.headers['Content-Type'].replace('x-log', 'plain')
    response.headers['Content-Disposition'] = "attachment; filename=%s" % (row.f_filename)
    response.headers['Content-Type']='text/plain'
    if row.f_data is not None:
        return row.f_data
    else:
        return ""

@auth.requires_login()
def list():
    """
    Returns a list of evidence based on a host (id, ipv4, ipv6) or all
    """
    import os, string
    if request.args(0) is not None:
        record = get_host_record(request.args(0))
        if record is None:
            redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
        response.title = "%s :: Evidence for host %s" % (settings.title, host_title_maker(record))
    else:
        response.title = "%s :: Evidence listing" % (settings.title)
        record = None

    aaData = []
    if request.extension == "json":
        if record is None:
            rows = db(db.t_evidence).select(db.t_evidence.id,
                                            db.t_evidence.f_hosts_id,
                                            db.t_evidence.f_type,
                                            db.t_evidence.f_other_type,
                                            db.t_evidence.f_text,
                                            db.t_evidence.f_filename,
                                            db.t_evidence.f_evidence,
                                            db.t_evidence.f_data.len()+1)
        else:
            rows = db(db.t_evidence.f_hosts_id == record.id).select(db.t_evidence.id,
                                                                    db.t_evidence.f_hosts_id,
                                                                    db.t_evidence.f_type,
                                                                    db.t_evidence.f_other_type,
                                                                    db.t_evidence.f_text,
                                                                    db.t_evidence.f_filename,
                                                                    db.t_evidence.f_evidence,
                                                                    db.t_evidence.f_data.len()+1)

        for r in rows:
            atxt = {}
            cnt = 0
            atxt[cnt] = A('edit', _target="evidence_edit_%s" % (r.t_evidence.id), _href=URL('edit', extension='html', args=r.t_evidence.id)).xml()
            cnt += 1
            if record is None:
                atxt[cnt] = host_a_maker(r.t_evidence.f_hosts_id).xml()
                cnt += 1
            if r.t_evidence.f_other_type:
                atxt[cnt] = "Other: %s" % (r.t_evidence.f_other_type)
            else:
                atxt[cnt] = r.t_evidence.f_type
            cnt += 1
            atxt[cnt] = r.t_evidence.f_text
            cnt += 1
            if r.t_evidence.f_filename is not None:
                if string.lower(os.path.splitext(r.t_evidence.f_filename)[1]) in ('.png', '.jpeg', '.jpg', '.gif'):
                    atxt[cnt] = A(IMG(_src=URL('download', args=[r.t_evidence.f_evidence]), _width="50%", _height="20%"),
                                  _href=URL('download', args=[r.t_evidence.f_evidence]),
                                  _target="evidence_image_%s" % (r.t_evidence.id), _id="evidence_image").xml()
                    cnt += 1
                    atxt[cnt] = "%sb" % (r._extra['(LENGTH(t_evidence.f_data) + 1)'])
                    cnt += 1
                else:
                    atxt[cnt] = A(r.t_evidence.f_filename, _target="evidence_other_%s" % (r.t_evidence.id), _id="evidence_other",
                                  _href=URL('download', args=[r.t_evidence.f_evidence])).xml()
                    cnt += 1
                    atxt[cnt] = "%sb" % (r._extra['(LENGTH(t_evidence.f_data) + 1)'])
                    cnt += 1
            else:
                atxt[cnt] = r.t_evidence.f_filename
                cnt += 1
            atxt['DT_RowId'] = r.t_evidence.id

            aaData.append(atxt)

        return { 'sEcho': request.vars.sEcho,
                 'iTotalRecords': len(aaData),
                 'aaData': aaData,
                 }

    if record:
        th_rows = (TH(T(''), _width="5%"),
                   TH(T('Type')),
                   TH(T('Text')),
                   TH(T('Evidence')),
                   TH(T('File Size')),
                   )
    else:
        th_rows = (TH(T(''), _width="5%"),
                   TH(T('Host')),
                   TH(T('Type')),
                   TH(T('Text')),
                   TH(T('Evidence'), _width="35%"),
                   TH(T('File Size')),
                   )

    evidence = TABLE(THEAD(TR(th_rows)),
                     _class="datatable",
                     _id="evidencetable",
                     _style="width:100%")

    return dict(evidence=evidence, host=record)

########NEW FILE########
__FILENAME__ = exploitdb
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2014 Cisco Systems, Inc.
##
## Exploit database controller
##
## Download from https://github.com/offensive-security/exploit-database and place in a directory
## In kvasir.yaml set the directory in exploitdb_path, e.g. exploitdb_path: "/usr/share/exploitdb"
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

try:
    import git
    if git.__version__ < '0.3.1':
        raise ImportError("Your version of git is %s. Upgrade to 0.3.1 or better." % git.__version__)
    have_git = True
except ImportError, e:
    have_git = False
    GIT_MISSING = 'Requires gitpython module, but not installed or incompatible version: %s' % e

from skaldship.general import exploitdb_update
import os
import mimetypes
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    response.title = "%s :: The Exploit Database" % (settings.title)
    return dict()


@auth.requires_login()
def list():
    """
    List exploitdb using server-side json
    """

    if request.vars.has_key('iDisplayStart'):
        start = int(request.vars.iDisplayStart)
    else:
        start = 0
    if request.vars.has_key('iDisplayLength'):
        if request.vars.iDisplayLength == '-1':
            limit = db(db.t_vulndata).count()
        else:
            limit = start + int(request.vars.iDisplayLength)
    else:
        limit = int(auth.user.f_show_size)

    if request.vars.has_key('sSearch'):
        # sSearch global search box
        srch = request.vars.sSearch
        query = (
            #(db.t_exploitdb.id == srch) | \
            (db.t_exploitdb.f_description.contains(srch)) | \
            (db.t_exploitdb.f_author.contains(srch)) | \
            (db.t_exploitdb.f_platform.contains(srch)) | \
            (db.t_exploitdb.f_type.contains(srch))
        )
    else:
        query = (db.t_exploitdb.id > 0)

    #orderby = db.t_expoitdb.f_date
    if request.vars.iSortingCols == '1':
        # sorting by a column - this is a little trippy because tuples start at 0
        # and datatables starts at 1 so we have to subtract 1 from iSortCol_0
        cols = (
            db.t_exploitdb.f_eid,
            db.t_exploitdb.f_description,
            db.t_exploitdb.f_date,
            db.t_exploitdb.f_author,
            db.t_exploitdb.f_platform,
            db.t_exploitdb.f_type,
            db.t_exploitdb.f_port,
        )

        orderby = cols[int(request.vars.iSortCol_0)]
        if request.vars.sSortDir_0 == 'asc':
            rows=db(query).select(orderby=orderby, limitby=(start, limit), cacheable=True)
        else:
            rows=db(query).select(orderby=~orderby, limitby=(start, limit), cacheable=True)
    else:
        rows=db(query).select(orderby=~orderby, limitby=(start,limit), cacheable=True)

    aaData = []
    for r in rows:
        aaData.append( {
            '0': A(r.f_eid, _id='id', _href=URL('detail', extension='html', args=[r.f_eid]), _target="_blank").xml(),
            '1': A(r.f_description, _id='description', _href=URL('detail', extension='html', args=[r.f_eid]), _target="_blank").xml(),
            '2': r.f_date,
            '3': r.f_author,
            '4': r.f_platform,
            '5': r.f_type,
            '6': r.f_port,
        } )

    total = db(db.t_exploitdb).count()
    result = {
        'sEcho': request.vars.sEcho,
        'iTotalRecords': total,
        'iTotalDisplayRecords': total,
        'aaData': aaData,
    }

    response.title = "%s :: The Exploit Database" % (settings.title)
    return result


@auth.requires_login()
def detail():
    """
    Serve up details of a specific exploitdb item
    """
    if request.args(0) is None:
        redirect(URL('index'))

    record = db(db.t_exploitdb.f_eid == request.args(0)).select(cacheable=True).first()
    if not record:
        redirect(URL('default', 'error', vars={'msg': T('Exploit DB record not found')}))

    contents = None
    shJsFile = None
    if record.f_file:
        mtype = mimetypes.guess_type(record.f_file)
        contents = "Binary or unknown file type. Download to view."
        if not mtype[0]:
            if record.f_file.endswith(".rb"):
                mtype = ['text/ruby']
        if mtype[0] and mtype[0].startswith("text") or mtype[0] == None:
            filename = os.path.join(settings.exploitdb_path, record.f_file)
            try:
                contents = "".join(open(filename, "r").readlines())
            except IOError, e:
                contents = "Binary or unknown file type. Download to view."
        extension = record.f_file[record.f_file.rfind('.')+1:]
        shBrushes = {
          'sh':   ['shBrushBash.js', 'bash'],
          'c':    ['shBrushCpp.js', 'c'],
          'cs':   ['shBrushCSharp.js', 'csharp'],
          'css':  ['shBrushCss.js', 'css'],
          'java': ['shBrushJava.js', 'java'],
          'js':   ['shBrushJScript.js', 'javascript'],
          'pl':   ['shBrushPerl.js', 'perl'],
          'php':  ['shBrushPhp.js', 'php'],
          'py':   ['shBrushPython.js', 'python'],
          'rb':   ['shBrushRuby.js', 'ruby'],
          'sql':  ['shBrushSql.js', 'sql'],
          'vb':   ['shBrushVb.js', 'vb'],
          'xml':  ['shBrushXml.js', 'xml'],
          'html': ['shBrushXml.js', 'html'],
          'htm':  ['shBrushXml.js', 'html'],
        }
        shJsFile = shBrushes.get(extension, ['shBrushPlain.js', 'plain'])

    response.title = "%s :: ExploitDB %s :: %s" % (settings.title, str(record.f_eid), record.f_description)
    return dict(record=record, contents=XML(contents), shJsFile=shJsFile)


@auth.requires_login()
def download():
    """
    Dowload exploitdb file
    """
    if request.args(0) is None:
        redirect(URL('index'))

    record = db(db.t_exploitdb.f_eid == request.args(0)).select(cacheable=True).first()
    if not record:
        redirect(URL('default', 'error', vars={'msg': T('Exploit DB record not found')}))

    filename = os.path.basename(record.f_file)
    fullfilename = os.path.join(settings.exploitdb_path, record.f_file)
    try:
        contents = open(fullfilename, "r").readlines()
    except IOError, e:
        contents = None

    mtype = mimetypes.guess_type(fullfilename)[0]
    if not mtype:
        mtype = 'text/txt'

    response.headers['Content-Type'] = mtype
    response.headers['Content-Disposition'] = 'attachment; filename="%s"' % filename
    return contents


@auth.requires_login()
def update_db():
    """
    Update the database with latest files.csv
    """
    if settings.exploitdb_path:
        if not os.path.exists(settings.exploitdb_path):
            msg = T('Exploit DB directory in kvasir.yaml not valid')
            if request.extension in ['load', 'json']:
                return dict(status='fail', message=msg)
            redirect(URL('default', 'error', vars={'msg': msg}))

    indexfile = os.path.join(settings.exploitdb_path, 'files.csv')

    if not os.path.exists(indexfile):
        session.flash = T('Exploit DB files.csv not found')
        redirect(URL('index'))

    dialog = FORM.confirm(T('Update'),
                          {T('Cancel'): URL('index')})

    if dialog.accepted:
        msg = exploitdb_update(indexfile)
        session.flash = msg
        redirect(URL('index'))

    return dict(dialog=dialog)

@auth.requires_login()
def git_pull():
    """
    Git Pull handler
    """
    if not have_git:
        session.flash = GIT_MISSING
        redirect(URL('index'))

    if settings.exploitdb_path:
        if not os.path.exists(settings.exploitdb_path):
            msg = T('Exploit DB directory in kvasir.yaml not valid')
            redirect(URL('default', 'error', vars={'msg': msg}))

    dialog = FORM.confirm(T('Pull'),
                          {T('Cancel'): URL('index')})

    if dialog.accepted:
        try:
            repo = git.Repo(settings.exploitdb_path)
            origin = repo.remotes.origin
            origin.fetch()
            origin.pull()
            msg = T("Exploit-database updated. ")
            indexfile = os.path.join(settings.exploitdb_path, 'files.csv')
            msg += exploitdb_update(indexfile)
            session.flash = msg

        except git.CheckoutError:
            session.flash = T(
                "Pull failed, certain files could not be checked out. Check logs for details.")
        except git.UnmergedEntriesError:
            session.flash = T(
                "Pull is not possible because you have unmerged files. Fix them up in the work tree, and then try again.")
        except git.GitCommandError:
            session.flash = T(
                "Pull failed, git exited abnormally. See logs for details.")
        except AssertionError:
            session.flash = T(
                "Pull is not possible because you have unmerged files. Fix them up in the work tree, and then try again.")
        except Exception, e:
            session.flash = T(
                "Error: %s" % (e)
            )

        redirect(URL('index'))

    return dict(dialog=dialog)


########NEW FILE########
__FILENAME__ = exploits
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Exploits controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import host_title_maker, get_host_record, create_hostfilter_query
from NexposeAPI import NexposeAPI
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir

@auth.requires_login()
def index():
    return dict()

##-------------------------------------------------------------------------
## exploits
##-------------------------------------------------------------------------

@auth.requires_login()
def add():
    if request.extension in ['load', 'json']:
        form=SQLFORM(db.t_exploits, buttons=[], _action=URL('add', extension=request.extension), _id="exploit_add_form")
    else:
        form=SQLFORM(db.t_exploits, _action=URL('add', extension=request.extension), _id="exploit_add_form")
    if form.accepts(request.vars, session):
        response.flash = 'Exploit added'
        response.headers['web2py-component-command'] = "exploitstable.fnReloadAjax();"
        return ""
    elif form.errors:
        response.flash = "Error in form submission"
        return TABLE(*[TR(k, v) for k, v in form.errors.items()])

    response.title = "%s :: Add Exploit" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def detail():
    record = db.t_exploits(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Exploit record not found')}))
    form=crud.read(db.t_exploits,record)
    response.title = "%s :: Exploit Detail" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def edit():
    record = db.t_exploits(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Exploit record not found')}))
    form=crud.update(db.t_exploits,record,next='detail/[id]',
                     ondelete=lambda form: redirect(URL('list')),
                     onaccept=crud.archive)
    response.title = "%s :: Update Exploit" % (settings.title)
    return dict(form=form)

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    for r in request.vars.ids.split('|'):
        if r is not None:
            db(db.t_exploits.id == r).delete()
            count += 1
    db.commit()
    response.flash = "%s Exploit(s) deleted" % (count)
    response.headers['web2py-component-command'] = "exploitstable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"

@auth.requires_login()
def list():
    aaData = []
    record = None
    if request.extension == "json":

        rows = db(db.t_exploits.id > 0).select()

        for r in rows:
            aTxt = {}
            aaData.append({
                '0': A('edit', _target="exploits_%s" % (r.id), _href=URL('edit.html', args=r.id)).xml(),
                '1': r.f_name,
                '2': r.f_title,
                '3': r.f_description,
                '4': r.f_source,
                '5': r.f_rank,
                '6': r.f_level,
                '7': r.f_vulnid,
                '8': r.f_cve,
                'DT_RowId': r.id
            })

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }

        return result

    response.title = "%s :: Exploits" % (settings.title)
    return dict()

@auth.requires_login()
def by_vulnid():
    """
    Returns a list of exploits for a vulnerability id
    """
    rows = db(db.t_exploits.f_vulnid.contains(request.args(0))).select()
    response.title = "%s :: Exploits by Vuln ID" % (settings.title)
    return rows

##-------------------------------------------------------------------------
## exploit list support (upload xml, match)
##-------------------------------------------------------------------------

@auth.requires_login()
def connect_exploits():
    """
    Call the connect_exploits() function which links known vulnerabilities to
    exploits based on f_vulnid or f_cve
    """
    form = SQLFORM.factory(
        Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background task')),
    )

    from skaldship.exploits import connect_exploits
    if form.accepts(request.vars, session):
        if form.vars.f_taskit:
            task = scheduler.queue_task(
                connect_exploits,
                group_name=settings.scheduler_group_name,
                sync_output=5,
                timeout=300   # 5 minutes
            )
            if task.id:
                redirect(URL('tasks', 'status', args=task.id))
            else:
                response.flash = "Error submitting job: %s" % (task.errors)
        else:
            connect_exploits()
            response.flash = "Exploits and vulnerabilities connected"
            redirect(URL('list'))

    response.title = "%s :: Connect Exploits" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def import_canvas_xml():
    """
    Process ImmunitySec's Exploit.xml which can be genrated from the URL
    http://exploitlist.immunityinc.com/ or by running ./canvasengine.py -e
    from your CANVAS directory

    http://exploitlist.immunityinc.com/home/serve/live
    """
    import os
    kvasir_path = os.path.join(request.folder, 'static/etc')
    form = SQLFORM.factory(
        Field('f_filename', 'upload', uploadfolder=os.path.join(request.folder, 'data/misc'), label=T('XML File')),
        Field('f_use_kvasir_local', 'boolean', label=T('Use Kvasir static path')),
        Field('f_use_local', 'boolean', label=T('Use local file path')),
        Field('f_pathname', 'string', default=kvasir_path, label=T('Local path')),
        Field('f_download', 'boolean', label=T('Download')),
        Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background task')),
        col3 = {
            'f_use_kvasir_local': 'static/etc/canvas_exploits.xml',
            'f_use_local': 'Directory where canvas_exploits.xml is located',
            'f_download': 'Download from ImmunitySec website',
        }
    )

    if form.errors:
        response.flash = 'Error in form'
    elif form.accepts(request.vars, session):
        if form.vars.f_use_local:
            filename = os.path.join(form.vars.f_pathname, 'canvas_exploits.xml')
        elif form.vars.f_use_kvasir_local:
            filename = os.path.join(request.folder,'static','etc','canvas_exploits.xml')
        elif form.vars.f_download:
            filename = None
        else:
            filename = os.path.join(request.folder,'data','misc',form.vars.f_filename)

        if form.vars.f_taskit:
            task = scheduler.queue_task(
                canvas_exploit_xml,
                pargs=[filename],
                group_name=settings.scheduler_group_name,
                sync_output=5,
                timeout=300   # 5 minutes
            )
            if task.id:
                redirect(URL('tasks', 'status', args=task.id))
            else:
                response.flash = "Error submitting job: %s" % (task.errors)
        else:
            from skaldship.canvas import process_exploits
            from skaldship.exploits import connect_exploits
            process_exploits(filename)
            connect_exploits()
            response.flash = "Canvas Exploit data uploaded"
            redirect(URL('list'))

    response.title = "%s :: Import ImmunitySec CANVAS Exploits XML" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def import_nexpose_xml():
    """
    Insert/Update exploit references from Nexpose exploits.xml file

    File is located in /opt/rapid7/nexpose/plugins/conf
    """
    import os
    response.title = "%s :: Import Nexpose Exploits XML" % (settings.title)
    form = SQLFORM.factory(
        Field('f_filename', 'upload', uploadfolder=os.path.join(request.folder, 'data', 'misc'), label=T('XML File')),
        Field('f_use_kvasir_local', 'boolean', label=T('Use Kvasir static path')),
        Field('f_use_local', 'boolean', label=T('Use local file path')),
        Field('f_pathname', 'string', default="/opt/rapid7/nexpose/plugins/conf", label=T('Local pathname')),
        Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background task')),
        col3 = {
            'f_use_kvasir_local': 'static/etc/nexpose_exploits.xml',
            'f_use_local': 'Directory where exploits.xml is located',
            'f_pathname': 'Requires Nexpose and possibly root access'
        }
    )

    if form.errors:
        response.flash = 'Error in form'
    elif form.accepts(request.vars, session):
        # process nexpose exploits.xml file

        if form.vars.f_use_local:
            filename = os.path.join(form.vars.f_pathname, 'exploits.xml')
        elif form.vars.f_use_kvasir_local:
            filename = os.path.join(request.folder,'static','etc','nexpose_exploits.xml')
        else:
            filename = os.path.join(request.folder,'data', 'misc', form.vars.f_filename)

        if form.vars.f_taskit:
            task = scheduler.queue_task(
                nexpose_exploit_xml,
                pargs=[filename],
                group_name=settings.scheduler_group_name,
                sync_output=5,
                timeout=300   # 5 minutes
            )
            if task.id:
                redirect(URL('tasks', 'status', args=task.id))
            else:
                response.flash = "Error submitting job: %s" % (task.errors)
        else:
            from skaldship.nexpose import process_exploits
            from skaldship.exploits import connect_exploits
            process_exploits(filename)
            connect_exploits()
            redirect(URL('list'))

    return dict(form=form)

########NEW FILE########
__FILENAME__ = hosts
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Hosts controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import create_hostfilter_query, get_host_record, pagination, host_title_maker
import gluon.contrib.simplejson
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return redirect(URL('hosts', 'list'))

##-------------------------------------------------------------------------
## hosts
##-------------------------------------------------------------------------

@auth.requires_signature()
@auth.requires_login()
def accessed():

    host = db.t_hosts(request.vars.get('host'))
    if not host:
        response.flash = "No host/ip address provided."
        return

    confval = request.vars.get('checked', None)
    if confval:
        if confval in ['1', 'true']:
            db(db.t_hosts.id == host.id).update(f_accessed = True)
            msg = "Host accessed"
        else:
            db(db.t_hosts.id == host.id).update(f_accessed = False)
            msg = "Host no longer accessed"
    else:
        if host.f_accessed:
            db(db.t_hosts.id == host.id).update(f_accessed = False)
            msg = "Host no longer accessed"
        else:
            db(db.t_hosts.id == host.id).update(f_accessed = True)
            msg = "Host accessed"

    db.commit()
    response.flash = msg
    return dict(msg=msg)

@auth.requires_signature()
@auth.requires_login()
def accessed_multi():
    """Access/unaccess tags multiple hosts via ajax"""

    count = 0
    if request.vars.get('method').lower() in ['accessed', 'unaccessed']:
        method = request.vars.get('method').lower()
        if method == 'accessed':
            conftype = True
        else:
            conftype = False

    if request.vars.has_key('ids'):
        for z in request.vars.ids.split('|'):
            if z is not '':

                db(db.t_hosts.id == z).update(f_accessed = conftype)
                count += 1

        db.commit()

    msg = "%s host(s) %sed" % (count, method)
    response.flash = msg
    response.headers['web2py-component-command'] = "hosttable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return dict(msg=msg)

@auth.requires_signature()
@auth.requires_login()
def followup():

    host = db.t_hosts(request.vars.get('host'))
    if not host:
        response.flash("No host/ip address provided.")
        return

    confval = request.vars.get('checked', None)
    if confval:
        if confval in ['1', 'true']:
            db(db.t_hosts.id == host.id).update(f_followup = True)
            msg = "Host marked for followup"
        else:
            db(db.t_hosts.id == host.id).update(f_followup = False)
            msg = "Host no longer marked for followup"
    else:
        if host.f_followup:
            db(db.t_hosts.id == host.id).update(f_followup = False)
            msg = "Host no longer marked for followup"
        else:
            db(db.t_hosts.id == host.id).update(f_followup = True)
            msg = "Host marked for followup"

    db.commit()
    response.flash = msg
    return dict(msg=msg)

@auth.requires_signature()
@auth.requires_login()
def followup_multi():
    """Followup tags multiple hosts via ajax"""

    count = 0
    if request.vars.get('method').lower() in ['followup', 'nofollowup']:
        method = request.vars.get('method').lower()
        if method == 'followup':
            conftype = True
        else:
            conftype = False

    if request.vars.has_key('ids'):
        for z in request.vars.ids.split('|'):
            if z is not '':

                db(db.t_hosts.id == z).update(f_followup = conftype)
                count += 1

        db.commit()

    msg = "%s host(s) marked %s" % (count, method)
    response.flash = msg
    response.js = "hosttable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return dict(msg=msg)

@auth.requires_signature()
@auth.requires_login()
def confirm():

    host = db.t_hosts(request.vars.get('host'))
    if not host:
        response.flash("No host/ip address provided.")
        return

    confval = request.vars.get('checked', None)
    if confval:
        if confval in ['1', 'true']:
            db(db.t_hosts.id == host.id).update(f_confirmed = True)
            msg = "Host confirmed"
        else:
            db(db.t_hosts.id == host.id).update(f_confirmed = False)
            msg = "Host no longer confirmed"
    else:
        if host.f_confirmed:
            db(db.t_hosts.id == host.id).update(f_confirmed = False)
            msg = "Host no longer confirmed"
        else:
            db(db.t_hosts.id == host.id).update(f_confirmed = True)
            msg = "Host confirmed"

    db.commit()
    response.flash = msg
    referrer = request.env.http_referer or ''
    if "hosts/list" in referrer:
        response.js = "hosttable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return dict(msg=msg)

@auth.requires_signature()
@auth.requires_login()
def confirmation_multi():
    """Confirms/unconfirms multiple hosts via ajax"""

    count = 0
    if request.vars.get('method').lower() in ['confirm', 'unconfirm']:
        method = request.vars.get('method').lower()
        if method == 'confirm':
            conftype = True
        else:
            conftype = False

    if request.vars.has_key('ids'):
        for z in request.vars.ids.split('|'):
            if z is not '':

                db(db.t_hosts.id == z).update(f_confirmed = conftype)
                count += 1

        db.commit()

    msg = "%s host(s) %sed" % (count, method)
    response.flash = msg
    response.headers['web2py-component-command'] = "hosttable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return dict(msg=msg)

@auth.requires_login()
def detail():

    if request.args(0) is None: redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))

    response.files.append(URL(request.application,'static','js/jquery.sparkline.js'))
    response.files.append(URL(request.application,'static','jstree/jstree.min.js'))

    #query = db.t_hosts.id == request.args(0)
    #query = create_hostfilter_query(session.hostfilter, query)

    record = get_host_record(request.args(0))

    if record is None:
        redirect(URL('hosts', 'list'))

    hostipv4=record.f_ipv4
    engineername = db.auth_user[record.f_engineer].username

    # to allow updating of the host record from this page
    host=crud.read(db.t_hosts,record)
    host.attributes['_id'] = "host_record"

    host_points = {}
    # build the host_points field which will cover:
    # the top t_host_os_ref cpe string
    os_list = db(db.t_host_os_refs.f_hosts_id == record.id).select()
    host_points['os'] = (0, 'Unknown')
    for os_rec in os_list:
        if os_rec.f_certainty > host_points['os'][0]:
            host_points['os'] = (os_rec.f_certainty, db.t_os[os_rec.f_os_id].f_title)

    host_points['account_cnt'] = 0
    host_points['password_cnt'] = 0
    host_points['cracked_pct'] = 0
    host_points['vuln_cnt'] = 0
    host_points['vuln_exploited_cnt'] = 0
    host_points['vuln_potential_cnt'] = 0
    vulns = {}
    vuln_list = []
    services = db(db.t_services.f_hosts_id == record.id).select()
    for svc in services:
        for vuln in db(db.t_service_vulns.f_services_id == svc.id).select():
            vulndata = db.t_vulndata[vuln.f_vulndata_id]
            vulns[vulndata.f_vulnid] = ( vulndata.f_severity, vulndata.f_cvss_score )
            vuln_list.append(vulndata)
        host_points['vuln_exploited_cnt'] += db((db.t_service_vulns.f_services_id==svc.id) & (db.t_service_vulns.f_status.like('%exploited%'))).count()
        host_points['vuln_potential_cnt'] += db((db.t_service_vulns.f_services_id==svc.id) & (db.t_service_vulns.f_status.like('%potential%'))).count()
        host_points['vuln_cnt'] += db(db.t_service_vulns.f_services_id==svc.id).count()
        host_points['account_cnt'] += db(db.t_accounts.f_services_id==svc.id).count()
        pwq = ((db.t_accounts.f_services_id==svc.id) & (db.t_accounts.f_compromised == True))
        #pwq &= (((db.t_accounts.f_password != None) | (db.t_accounts.f_password != '')) | (db.t_accounts.f_compromised == True))
        host_points['password_cnt'] += db(pwq).count()
        try:
            host_points['cracked_pct'] = 100 * (host_points['password_cnt'] / host_points['account_cnt'])
        except ZeroDivisionError:
            host_points['cracked_pct'] = 0

    # breakdown of vuln severity
    sev_sum_dict = {}
    for a in range(1, 11):
        sev_sum_dict[a] = 0

    for k,v in vulns.iteritems():
        # take the severity and increment the sev_sum set item
        if settings.use_cvss:
            severity = int(float(v[1]))
        else:
            severity = v[0]

        count = sev_sum_dict.setdefault(severity, 1)
        count += 1
        sev_sum_dict[severity] = count

    sev_sum_spark = []
    sev_sum = []
    for k,v in sev_sum_dict.iteritems():
        sev_sum_spark.append(str(v))
        if v > 0:
            sev_sum.append("%s: %s" % (k, v))

    host_points['sev_sum_spark'] = ",".join(sev_sum_spark)
    host_points['sev_sum'] = " / ".join(sev_sum)

    # netbios record (or none if it's empty)
    netb_record = db(db.t_netbios.f_hosts_id == record.id).select().first() or None
    if netb_record is not None:
        netbios=crud.update(db.t_netbios, netb_record,
                            ondelete=lambda netbios: redirect(URL('host_detail', args=[ record.id ])))
        host_points['netb_domain'] = netb_record.f_domain
        host_points['netb_type'] = netb_record.f_type
    else:
        db.t_netbios.f_hosts_id.default = record.id
        netbios = LOAD('netbios', 'add.load', args=[host.record.id], ajax=True, target='netbios_info')

    host_pagination = pagination(request, record)

    response.title = "%s :: Host info :: %s" % (settings.title, host_title_maker(record))
    return dict(host=host,
                netbios=netbios,
                host_points=host_points,
                host_pagination=host_pagination, hostipv4=hostipv4, engineername=engineername)

@auth.requires_login()
def popover():
    """
    Returns the detail of a host for popovers
    """
    host_rec = get_host_record(request.args(0))
    resp = {}
    if not host_rec:
        resp['title'] = "Host not found"
        resp['content'] = ""
    else:
        svcs = host_rec.t_services
        svc_cnt = 0
        vuln_cnt = 0
        acct_cnt = 0
        for svc in svcs.select():
            svc_cnt += 1
            vuln_cnt += svc.t_service_vulns.count()
            acct_cnt += svc.t_accounts.count()

        host_os = (0, 'Unknown')
        for os_rec in host_rec.t_host_os_refs.select():
            if os_rec.f_certainty > host_os[0]:
                host_os = (os_rec.f_certainty, db.t_os[os_rec.f_os_id].f_title)

        resp['title'] = host_title_maker(host_rec)
        resp['content'] = XML(TABLE(
            TR(TD(T('Asset Group')), TD(host_rec.f_asset_group)),
            TR(TD(T('Engineer')), TD(db.auth_user[host_rec.f_engineer].username)),
            TR(TD(T('OS')), TD("%s (%s)" % (host_os[1], host_os[0]))),
            TR(TD(T('Services')), TD(svc_cnt), _class="success"),
            TR(TD(T('Vulnerabilities')), TD(vuln_cnt), _class="error"),
            TR(TD(T('Accounts')), TD(acct_cnt), _class="warning"),
            _class="table table-condensed",
        ))

    return resp

@auth.requires_login()
def add():
    """
    Add a host record to the database
    """
    fields = [
        'f_ipv4',
        'f_ipv6',
        'f_hostname',
        'f_netbios_name',
        'f_macaddr',
        'f_engineer',
        'f_asset_group',
    ]
    db.t_hosts.f_engineer.default = auth.user.id
    if request.extension in ['load', 'json']:
        form=SQLFORM(db.t_hosts, fields=fields, buttons=[], formstyle='bootstrap', _id="hosts_add_form")
        #form=SQLFORM(db.t_hosts, fields=fields, formstyle='bootstrap')
        if form.process().accepted:
            response.flash = "Host Added"
            #response.headers['web2py-component-command'] = 'hosttable.fnReloadAjax();'
            response.js = 'hosttable.fnReloadAjax();'
            return
        elif form.errors:
            response.flash = "Error in form submission"
            #return TABLE(*[TR(k, v) for k, v in form.errors.items()])
    else:
        response.title = "%s :: Add Host" % (settings.title)
        form=crud.create(db.t_hosts,next='detail/[id]', fields=fields)
    return dict(form=form)

@auth.requires_login()
def edit():
    """Creates and process a form to edit a host record"""
    record = db.t_hosts(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    if request.extension in ['load', 'json']:
        form=SQLFORM(db.t_hosts, record, buttons=[], _action=URL('edit', args=[record.id]), _id="host_edit_form")
        if form.accepts(request.vars, session):
            response.flash = "Host information updated"
        elif form.errors:
            response.flash = "Error in form submission"
    else:
        response.title = "%s :: Edit Host" % (settings.title)
        form=crud.update(db.t_hosts, record, next='edit/[id]')

    return dict(form=form)

@auth.requires_login()
def list():
    response.title = "%s :: Host Listing" % (settings.title)
    # hostfilter is a session variable that can be
    # None -- no host filtering
    # (userid, <x>) - limit on user id
    # (assetgroup, <x>) - limit on asset group
    # (range, <x>) - limit on subnet (eg: 192.168)
    hostfilter = session.hostfilter
    if hostfilter is None:
        # if no filter is set then we blank it out
        if session.hostfilter is None:
            session.hostfilter = [(None, None), False]

    if request.extension == 'json':
        # from datetime import datetime, timedelta
        # host_start = datetime.now()
        tot_vuln = 0
        tot_hosts = 0

        """
        # load all the vulndata from service_vulns into a dictionary
        # so we only have to query the memory variables instead of
        # the database each time. We need to collect:
        # svc_vulndata[f_service_id] = (f_vulnid, f_severity, f_cvss_score)
        svc_vulndata = {}

        rows = s_service_vuln_data.select( db.t_vulndata.id, db.t_vulndata.f_vulnid, db.t_vulndata.f_severity, db.t_vulndata.f_cvss_score, cache=(cache.ram, 60))
        for r in rows:
            #exploitcount = db(db.t_exploit_references.f_vulndata_id == r.id).count()
            svc_vulndata[r.id] = ( r.f_vulnid,
                                   r.f_severity,
                                   r.f_cvss_score,
                                   r.t_exploit_references.count())
        """

        # build the query variable.. first all hosts then check
        # if a hostfilter is applied
        q = (db.t_hosts.id > 0)
        q = create_hostfilter_query(session.hostfilter, q)

        aaData = []
        rows = db(q).select(db.t_hosts.ALL, db.t_host_os_refs.f_certainty, db.t_os.f_title, db.auth_user.username,
                            left=(db.t_host_os_refs.on(db.t_hosts.id==db.t_host_os_refs.f_hosts_id),
                                  db.t_os.on(db.t_os.id==db.t_host_os_refs.f_os_id),
                                  db.auth_user.on(db.t_hosts.f_engineer==db.auth_user.id)),
                            orderby=db.t_hosts.id|~db.t_host_os_refs.f_certainty)
        # datatable formatting is specific, crud results are not
        seen = set()
        for r in rows:
            if r.t_hosts.id not in seen and not seen.add(r.t_hosts.id): # kludge way to select only rows per host with the best OS-guess
                spanflags = []
                if r.t_hosts.f_confirmed:
                    confirmed = 'hosts_select_confirmed'
                    spanflags.append('<span class="badge"><i class="icon-check"></i></span>')
                else:
                    confirmed = 'hosts_select_unconfirmed'

                if r.t_hosts.f_accessed:
                    spanflags.append('<span class="badge badge-success"><i class="icon-heart"></i></span>')
                if r.t_hosts.f_followup:
                    spanflags.append('<span class="badge badge-important"><i class="icon-flag"></i></span>')

                confirmed = '<div class="%s">%s</div>' % (confirmed, " ".join(spanflags))

                if r.t_hosts.f_ipv4:
                    ipv4 = A(r.t_hosts.f_ipv4, _id='ipv4', _href=URL('detail', extension='html', args=[r.t_hosts.id]), _target="host_detail_%s" % (r.t_hosts.id)).xml()
                else:
                    ipv4 = ""
                if r.t_hosts.f_ipv6:
                    ipv6 = A(r.t_hosts.f_ipv6, _id='ipv6', _href=URL('detail', extension='html', args=[r.t_hosts.id]), _target="host_detail_%s" % (r.t_hosts.id)).xml()
                else:
                    ipv6 = ""

                if r.t_os.f_title is None:
                    os = "Unknown"
                else:
                    os = r.t_os.f_title

                atxt = {
                     '0': confirmed,
                     '1': ipv4,
                     '2': ipv6,
                     '3': r.t_hosts.f_service_count,
                     '4': r.t_hosts.f_vuln_count,
                     '5': "<span class=\"severity_sparkline\" values=\"%s\"></span>" % (r.t_hosts.f_vuln_graph),
                     '6': r.t_hosts.f_exploit_count,
                     '7': r.t_hosts.f_hostname,
                     '8': r.t_hosts.f_netbios_name,
                     '9': os,
                     '10': r.auth_user.username,
                     '11': r.t_hosts.f_asset_group,
                     'DT_RowId': "%s" % (r.t_hosts.id),
                }

                aaData.append(atxt)
                #print("Total time in vuln processing: %s seconds" % (tot_vuln))
                #print("Host record processed in %s seconds" % (timedelta.total_seconds(datetime.now() - row_start)))
                tot_hosts += 1

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'iTotalDisplayRecords': len(aaData),
                   'aaData': aaData,
                   }

        #print("Host_select processed %s hosts in %s seconds" % (tot_hosts, timedelta.total_seconds(datetime.now() - host_start)))
        return result
    else:
        add_hosts = AddModal(
            db.t_hosts, 'Add Host', 'Add Host', 'Add Host',
            fields = [
                'f_ipv4',
                'f_ipv6',
                'f_hostname',
                'f_netbios_name',
                'f_macaddr',
                'f_engineer',
                'f_asset_group',
            ],
            cmd = 'hosttable.fnReloadAjax();'
        )
        db.t_hosts.id.comment = add_hosts.create()
        response.files.append(URL(request.application,'static','js/jquery.sparkline.js'))
        return dict(hostfilter=session.hostfilter, add_hosts=add_hosts)

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    for r in request.vars.ids.split('|'):
        if r is not None:
            db(db.t_hosts.id == r).delete()
            count += 1
    db.commit()
    response.flash = "%s Host(s) deleted" % (count)
    response.headers['web2py-component-command'] = "hosttable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    #response.js = "hosttable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"

    return dict()

@auth.requires_login()
def filter():
    """
    hostfilter is a session variable that can be
      None -- no host filtering
      (userid, <x>) - limit on user id
      (group, <x>) - limit on asset group
      (range, <x>) - limit on subnet (eg: 192.168)
    """
    if type(session.hostfilter) is type(None):
        session.hostfilter = [(None, None), False]

    hostfilter = session.hostfilter[0]
    unconfirmed = session.hostfilter[1]
    response.title = "%s :: Host Filter" % (settings.title)

    if request.extension == 'json':
        # process filter function requests
        function = request.vars.get('function', '')

        if function.lower() in ['assetgroup', 'range']:
            hostfilter = (function.lower(), request.vars.get('filter', ''))
        elif function.lower() == 'userid':
            userid = request.vars.get('userid', '')
            username = db.auth_user[userid].username or ''
            hostfilter = (function.lower(), username)
        elif function.lower() == 'clear':
            hostfilter = (None, None)

        if request.vars.has_key('_formname'):
            if request.vars.get('unconfirmed'):
                unconfirmed = True
            else:
                unconfirmed = False

        session.hostfilter = [hostfilter, unconfirmed]
        referrer = request.env.http_referer or ''
        if function:
            if "hosts/list" in referrer:
                response.headers['web2py-component-command'] = 'hosttable.fnReloadAjax();'
            elif "accounts/list" in referrer:
                response.headers['web2py-component-command'] = 'accounttable.fnReloadAjax();'
            elif "services/list" in referrer:
                response.headers['web2py-component-command'] = 'servicetable.fnReloadAjax();'
            elif "stats/vulnlist" in referrer:
                response.headers['web2py-component-command'] = 'vulntable.fnReloadAjax();'

        return dict(hostfilter=session.hostfilter)

    elif request.extension in ['load', 'html']:
        # send filter form
        if request.extension == 'load':
            buttons = []
        else:
            buttons = ['submit']
        form = SQLFORM.factory(
            Field('function', 'list', label=T('Filter by'), requires=IS_IN_SET(['UserID', 'AssetGroup', 'Range'], multiple=False)),
            Field('filter', 'string', label=T('Filter text')),
            Field('userid', db.auth_user, label=T('User'), requires=IS_EMPTY_OR(IS_IN_DB(db, db.auth_user.id, '%(username)s'))),
            Field('unconfirmed', 'boolean', default=False, label=T('Unconfirmed Only')),
            _id="host_filter_form",
            buttons=buttons,
        )
        if form.accepts(request.vars, session):
            function = request.vars.get('function', '')

            if function.lower() == 'userid':
                userid = request.vars.get('userid', '')
                username = db.auth_user[userid].username or ''
                hostfilter = (function.lower(), username)
            elif function.lower() in ['assetgroup', 'range']:
                hostfilter = (function.lower(), request.vars.get('filter', ''))
            elif function.lower() == 'clear':
                hostfilter = [(None, None), False]

            unconfirmed = form.vars.get('unconfirmed', False)
            if unconfirmed == 'on':
                unconfirmed = True
            else:
                unconfirmed = False
            session.hostfilter = [hostfilter, form.vars.unconfirmed]
        elif form.errors:
            pass
        return dict(form=form)

@auth.requires_login()
def csv_hostupdate():
    """Takes an uploaded csv file and processes it, updating host records"""
    import csv, os
    form=SQLFORM.factory(
        Field('csvfile', 'upload', uploadfolder=os.path.join(request.folder, 'data', 'misc'), label=T('CSV Filename')),
        Field('overwrite', 'boolean', default=False, label=T('Overwrite existing data')),
        Field('add_hosts', 'boolean', default=False, label=T('Add missing hosts')),
    )
    if form.errors:
        response.flash = 'Error in form'
    elif form.accepts(request.vars, session):
        filename = os.path.join(request.folder, 'data/misc', form.vars.csvfile)
        csv_rdr = csv.reader(open(filename, "r"))
        updated = 0
        skipped = 0
        for row in csv_rdr:
            record = None
            if row[0] != '':
                record=db(db.t_hosts.f_ipv4==row[0]).select().first()
            elif row[1] != '':
                record=db(db.t_hosts.f_ipv6==row[1]).select().first()
            if record is None:
                logging.warning("Host record not found for %s" % row)
                skipped += 1
                continue
            if record.f_hostname is None or record.f_hostname == '':
                record.update_record(f_hostname = row[2].strip('\n'))
                updated += 1
            elif form.vars.overwrite:
                record.update_record(f_hostname = row[2].strip('\n'))
                updated += 1
            else:
                skipped += 1
            db.commit()
        response.flash = "Updated %s records, skipped %s" % (updated, skipped)
        os.remove(filename)

    response.title = "%s :: CSV Hostname Update" % (settings.title)
    return dict(form=form)

##-------------------------------------------------------------------------
## mass host functions
##-------------------------------------------------------------------------

@auth.requires_signature()
@auth.requires_login()
def mass_os_refs():
    """Receives a list of host records and relevant OS information and assigns them!"""

    fields = ['f_os_id', 'f_certainty', 'f_class', 'f_family']
    host_ids = []
    if request.vars.has_key('host_ids'):
        for z in request.vars.host_ids.split('|'):
            if z is not '':
                host_ids.append(z)
    form=SQLFORM(db.t_host_os_refs, fields=fields, buttons=[], _id="mass_os_form")
    if form.validate():
        insert = []
        for idrec in host_ids:
            insert.append({
                'f_hosts_id': idrec,
                'f_os_id': form.vars.f_os_id,
                'f_class': form.vars.f_class,
                'f_family': form.vars.f_family,
                'f_certainty': form.vars.f_certainty,
            })
        db.t_host_os_refs.bulk_insert(insert)
        response.flash = "Operating System updated on %s host(s)" % (len(host_ids))
        response.headers['web2py-component-command'] = "hosttable.fnReloadAjax();  jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
        return
    elif form.errors:
        response.flash = "Error in form submission"

    return dict(form=form)

@auth.requires_signature()
@auth.requires_login()
def mass_asset_group():
    """Receives a list of host records and assetgroup and assigns them!"""

    host_ids = []
    if request.vars.has_key('host_ids'):
        for z in request.vars.host_ids.split('|'):
            if z is not '':
                host_ids.append(z)
    form=SQLFORM.factory(
        Field('asset_group', 'string', label=T('Asset Group')),
        buttons=[], _id="mass_asset_form")

    if form.validate():
        insert = []
        for idrec in host_ids:
            db(db.t_hosts.id == idrec).update(
                f_asset_group=form.vars.asset_group,
            )
            db.commit()
        response.flash = "%s host(s) assigned to %s" % (len(host_ids), form.vars.asset_group)
        response.headers['web2py-component-command'] = "hosttable.fnReloadAjax();  jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
        return
    elif form.errors:
        response.flash = "Error in form submission"

    return dict(form=form)

@auth.requires_signature()
@auth.requires_login()
def mass_vulndata():
    """
    Add a vulndata to a lot of hosts
    TODO: This!
    """

    host_ids = []
    if request.vars.has_key('host_ids'):
        for z in request.vars.host_ids.split('|'):
            if z is not '':
                host_ids.append(z)
    form=SQLFORM.factory(
        Field('vulndata', 'reference t_vulndata', label=T('Vulnerability')),
        buttons=[], _id="mass_asset_form")

    if form.validate():
        pass

#@auth.requires_signature()
@auth.requires_login()
def launch():
    """
    Launches a terminal session using the Scheduler
    """

    record = request.vars.get('record', None)
    if record is None:
        response.flash = "No record sent to launch"
        return dict()

    task = scheduler.queue_task(
        launch_terminal,
        pargs=[record, settings.launch_command],
        group_name = settings.scheduler_group_name,
        immediate=True,
    )

    if task.id:
        response.flash = "Terminal launch queued!"
    else:
        response.flash = "Error submitting job: %s" % (task.errors)

    return dict()

########NEW FILE########
__FILENAME__ = metasploit
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Metasploit controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import get_host_record, host_title_maker
from skaldship.metasploit import msf_get_config
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def api_settings():
    """Settings Metasploit API"""

    msf_settings = msf_get_config(session)
    response.title = "%s :: Metasploit API Settings" % (settings.title)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(error=str(error), alert=True, form=None)

    error=None
    alert=False
    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        workspaces = [w for w in msf.pro_workspaces().keys()]
        users = [u for u in msf.pro_users().get('users').keys()]
    except MSFAPIError, e:
        error = str(e)
        alert = True
        workspaces = []
        users = []

    form=SQLFORM.factory(
        Field('workspace', 'string', default=msf_settings['workspace'], label=T('Workspace Name'), requires=IS_IN_SET(workspaces)),
        Field('workspace_num', 'string', default=msf_settings['ws_num'], label=T('Workspace Number')),
        Field('user', 'string', default=msf_settings['user'], label=T('MSF User'), requires=IS_IN_SET(users)),
        Field('url', 'string', default=msf_settings['url'], label=T('MSF URL')),
        Field('key', 'string', default=msf_settings['key'], label=T('API Key')),
    )
    # NOTE: workspace_num must be manually entered since there's no way for us
    # to learn it from the API. We're just guessing otherwise - 1 is the default
    # workspace so it's more likely to exist
    if form.accepts(request, session):
        settings.msf_workspace = form.vars.workspace
        settings.msf_workspace_num = form.vars.workspace_num
        session.msf_key = form.vars.key
        session.msf_url = form.vars.url
        session.msf_user = form.vars.user
        response.flash = "MSF Settings updated"
    elif form.errors:
        response.flash = "Errors in your form!"
    return dict(form=form, error=str(error), alert=False)

##-------------------------------------------------------------------------
## mass bruteforce / exploit
##-------------------------------------------------------------------------

@auth.requires_login()
def bruteforce():
    """
    Launches a Metasploit Pro Bruteforce based upon a list of host records
    """
    response.title = "%s :: Metasploit Pro Bruteforce" % (settings.title)
    msf_settings = msf_get_config(session)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(alert=True, error=str(error), form=None)

    host_records = request.vars.host_records
    if host_records:
        def host_to_ip(host_rec):
            if isinstance(host_rec, (int, str)):
                host_rec = get_host_record(host_rec)
            if not host_rec:
                return None
            return host_rec.get('f_ipv4') or host_rec.get('f_ipv6')
        target_ips = '\n'.join([host_to_ip(x) for x in host_records.split('|')])
    else:
        target_ips = ''

    loot_list = []    # list of loot IDs and IPs
    alert = False
    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
    except MSFAPIError, error:
        return dict(alert=True, error=str(error), form=None)

    form = SQLFORM.factory(
        Field('targets', 'text', default=target_ips, label=T('Targets'), requires=IS_NOT_EMPTY(),
            comment=T('Targets to scan can be IP Addresses, ranged lists or subnets. One per line.'),
        ),
        Field('blacklist', 'text', label=T('Blacklisted hosts'),
            comment=T('Targets to blacklist can be IP Addresses, ranged lists or subnets. One per line.')
        ),
        Field('stop_on_success', 'boolean', default=True, label=T('Stop on success'),
            comment=T('Stop scanning a host after first user login success')
        ),
        Field('verbose', 'boolean', default=False, label=T('Verbose')),
        Field('include_known', 'boolean', default=True, label=T('Include known'),
            comment=T('Include known credentials from the Workspace')
        ),
        Field('dry_run', 'boolean', default=False, label=T('Dry run'),
            comment=T('Prepare for execution but do nothing')
        ),
        Field('scope', 'string', default='normal', label=T('Scope'),
            requires=IS_IN_SET(['quick', 'defaults', 'normal', 'deep', 'known', 'imported', '50k']),
        ),
        Field('speed', 'string', default=3, label=T('Speed'),
            requires=IS_IN_SET([
                [ 0, 'Glacial'], [1, 'Slow'], [2, 'Stealthy'], [3, 'Normal'], [4, 'Fast'], [5, 'Turbo']
            ])
        ),
        Field('services', 'list:string', label=T('Services'),
            requires=IS_EMPTY_OR(IS_IN_SET(
                ['Telnet', 'SSH', 'SMB', 'VNC', 'SNMP', 'Postgres', 'MySQL', 'MSSQL', 'Oracle',
                 'DB2', 'FTP', 'HTTP', 'HTTPS', 'EXEC', 'LOGIN', 'SHELL', ], multiple=(1,17))
            ),
            comment=T('List of services to bruteforce, multiples permitted')
        ),
        Field('addl_creds', 'text', label=T('Additional credentals'),
            comment=T('List additional credentials to test. One per line with space between username and password.')
        ),
        Field('getsession', 'boolean', default=True, label=T('Execute session'),
            comment=T('On successful access pop a shell or meterpreter session')
        ),
        Field('payload', 'string', default='auto', label=T('Payload method'),
            requires=IS_IN_SET(['auto', 'reverse', 'bind'])
        ),
        Field('payload_type', 'string', default='meterpreter', label=T('Paylod type'),
            requires=IS_IN_SET(['meterpreter', 'shell'])
        ),
        Field('payload_ports', 'string', default='4000-5000', label=T('Payload ports'),
            requires=IS_NOT_EMPTY(),
            comment=T('Port range for reverse/connect payloads'),
        ),
        Field('smb_domains', 'string', label=T('SMB Domains'),
            comment=T('List of SMB domains, separated by spaces, to use')
        ),
        Field('preverse_domains', 'boolean', default=True, label=T('Preserve Domains'),
            comment=T('Use previously identified SMB Domains with usernames')
        ),
        Field('mssql_windows_auth', 'boolean', default=False, label=T('MSSQL Windows Auth'),
            comment=T('MSSQL attempts should use NTLM instead of Standard mode')
        ),
        Field('skip_blank_pw', 'boolean', default=False, label=T('Blanks')),
        Field('skip_machine_names', 'boolean', default=False, label=T('Machine names')),
        Field('skip_builtin_windows', 'boolean', default=False, label=T('Built-in Windows')),
        Field('skip_builtin_unix', 'boolean', default=False, label=T('Built-in UNIX')),
        Field('recombine_creds', 'boolean', default=False, label=T('Recombine credentials')),
        Field('max_guess_per_svc', 'integer', default=0, label=T('Per service'), requires=IS_INT_IN_RANGE(0, 65535)),
        Field('max_guess_per_user', 'integer', default=0, label=T('Per user'), requires=IS_INT_IN_RANGE(0, 65535)),
        Field('max_guess_overall', 'integer', default=0, label=T('Overall'), requires=IS_INT_IN_RANGE(0, 65535)),
        Field('max_time_per_svc', 'integer', default=0, label=T('Per service'), requires=IS_INT_IN_RANGE(0, 1440),
            comment=T('Maximum time to bruteforce per service (in minutes')
        ),
        Field('max_time', 'integer', default=0, label=T('Overall'), requires=IS_INT_IN_RANGE(0, 65535),
            comment=T('Maximum time to run brute force (in minutes)')
        ),
        table_name='msfpro_bruteforce',
        _class="form-horizontal"
    )

    if form.process().accepted:
        args = {
            'workspace': msf_settings['msf_workspace'],
            'username': msf_settings['user'],
            'DS_WHITELIST_HOSTS': form.vars.targets,
            'DS_BLACKLIST_HOSTS': form.vars.blacklist,
            'DS_STOP_ON_SUCCESS': form.vars.stop_on_success,
            'DS_VERBOSE': form.vars.verbose,
            'DS_INCLUDE_KNOWN': form.vars.include_known,
            'DS_DRY_RUN': form.vars.dry_run,
            'DS_BRUTEFORCE_SCOPE': form.vars.scope,
            'DS_BRUTEFORCE_SPEED': form.vars.speed,
            'DS_BRUTEFORCE_SERVICES': " ".join(form.vars.services),
            'DS_BRUTEFORCE_GETSESSION': form.vars.getsession,
            'DS_QUICKMODE_CREDS': form.vars.addl_creds,
            'DS_PAYLOAD_METHOD': form.vars.payload,
            'DS_PAYLOAD_TYPE': form.vars.payload_type,
            'DS_PAYLOAD_PORTS': form.vars.payload_ports,
            'DS_SMB_DOMAINS': form.vars.smb_domains,
            'DS_PRESERVE_DOMAINS': form.vars.preverse_domains,
            'DS_MAXGUESSESPERSERVICE': form.vars.max_guess_per_svc,
            'DS_MAXGUESSESPERUSER': form.vars.max_guess_per_user,
            'DS_MAXGUESSESOVERALL': form.vars.max_guess_overall,
            'DS_MAXMINUTESPERSERVICE': form.vars.max_time_per_svc,
            'DS_MAXMINUTESOVERALL': form.vars.max_time,
            'DS_BRUTEFORCE_SKIP_BLANK_PASSWORDS': form.vars.skip_blank_pw,
            'DS_BRUTEFORCE_SKIP_MACHINE_NAMES': form.vars.skip_machine_names,
            'DS_BRUTEFORCE_SKIP_BUILTIN_WINDOWS_ACCOUNTS': form.vars.skip_builtin_windows,
            'DS_BRUTEFORCE_SKIP_BLANK_BUILTIN_UNIX_ACCOUNTS': form.vars.skip_builtin_unix,
            'DS_BRUTEFORCE_RECOMBINE_CREDS': form.vars.recombine_creds,
            'DS_MSSQL_WINDOWS_AUTH': form.vars.mssql_windows_auth
        }
        task = msf.start_bruteforce(args)
        msfurl = os.path.join(msf_settings['url'], 'workspaces', msf_settings['workspace_num'], 'tasks', task['task_id'])
        redirect(msfurl)
    elif form.errors:
        response.flash = "Error in form"

    return dict(form=form, alert=alert, error=False)

@auth.requires_login()
def exploit():
    """
    Launches Metasploit Pro Exploit based upon a list of host records
    """
    response.title = "%s :: Metasploit Pro Exploit" % (settings.title)
    msf_settings = msf_get_config(session)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(alert=True, error=str(error), form=None)

    host_records = request.vars.host_records
    if host_records:
        def host_to_ip(host_rec):
            if isinstance(host_rec, (int, str)):
                host_rec = get_host_record(host_rec)
            if not host_rec:
                return None
            return host_rec.get('f_ipv4') or host_rec.get('f_ipv6')
        target_ips = '\n'.join([host_to_ip(x) for x in host_records.split('|')])
    else:
        target_ips = ''

    module_list = []
    alert = False
    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        module_list = msf.module_list(modtype='exploits').get('modules')
    except MSFAPIError, error:
        return dict(alert=True, error=str(error), form=None)

    form = SQLFORM.factory(
        Field('targets', 'text', default=target_ips, label=T('Targets'), requires=IS_NOT_EMPTY(),
            comment=T('Targets to scan can be IP Addresses, ranged lists or subnets. One per line.')
        ),
        Field('blacklist_hosts', 'text', label=T('Blacklisted Targets'),
            comment=T('Targets to blacklist can be IP Addresses, ranged lists or subnets. One per line.')
        ),
        Field('ports', 'string', default='1-65535', label=T('Ports'), requires=IS_NOT_EMPTY(),
            comment=T('List of ports to match exploits to. Example: 21-23,80,443,8000-8999')
        ),
        Field('blacklist_ports', 'string', label=T('Blacklisted Ports'),
            comment=T('List of ports to not exploit. Example: 21-23,80,443,8000-8999')
        ),
        Field('min_rank', 'string', default='great', label=T('Minmum Exploit Rank'),
            requires=IS_IN_SET(['low', 'average', 'normal', 'good', 'great', 'excellent']),
            comment=T('Minimum reliability level of exploits to include')
        ),
        Field('exploit_speed', 'integer', default=5, label=T('Parallel Exploits'), requires=IS_INT_IN_RANGE(1, 11),
            comment=T('How many exploits to run in parallel (1-10)')
        ),
        Field('exploit_timeout', 'integer', default=5, label=T('Timeout (in minutes)'),  requires=IS_INT_IN_RANGE(0, 1440),
            comment=T('Maximum time (in minutes) an exploit is allowed to run')
        ),
        Field('limit_sessions', 'boolean', default=True, label=T('Limit sessions'),
            comment=T('Limit sessions to only one per exploited host')
        ),
        Field('ignore_fragile', 'boolean', default=True, label=T('Skip "fragile" devices'),
            comment=T('Avoid exploit attempts on fragile systems such as network devices and printers.')
        ),
        Field('filter_by_os', 'boolean', default=True, label=T('OS'),
            comment=T('Match exploits to Operating System, known vulnerabilities or ports')
        ),
        Field('filter_by_vuln', 'boolean', default=True, label=T('Vulnerabilities')),
        Field('filter_by_ports', 'boolean', default=True, label=T('Ports')),
        Field('dry_run', 'boolean', default=False, label=T('Dry run'),
            comment=T('Prepare for execution but do nothing')
        ),
        Field('payload', 'string', default='auto', label=T('Payload method'),
            requires=IS_IN_SET(['auto', 'reverse', 'bind'])
        ),
        Field('payload_type', 'string', default='meterpreter', label=T('Paylod type'),
            requires=IS_IN_SET(['meterpreter', 'shell'])
        ),
        Field('payload_ports', 'string', default='4000-5000', label=T('Payload ports'),
            requires=IS_NOT_EMPTY(),
            comment=T('Port range for reverse/connect payloads')),
        Field('evasion_tcp', 'integer', default=0, label=T('TCP Evasion Level'), requires=IS_INT_IN_RANGE(0, 4)),
        Field('evasion_app', 'integer', default=0, label=T('Application Evasion'), requires=IS_INT_IN_RANGE(0, 4)),
        Field('modules', 'list:string', label=T('Specifc Module(s)'),
            requires=IS_EMPTY_OR(IS_IN_SET(module_list, multiple=True)),
            comment=T('A whitelist of modules to execute, by default all that match are tried')
        ),
        table_name='msfpro_exploit',
        _class="form-horizontal"
    )

    if form.process().accepted:
        args = {
            'workspace': msf_settings['workspace'],
            'username': msf_settings['user'],
            'DS_WHITELIST_HOSTS': form.vars.targets,
            'DS_BLACKLIST_HOSTS': form.vars.blacklist_hosts,
            'DS_WHITELIST_PORTS': form.vars.ports,
            'DS_BLACKLIST_PORTS': form.vars.blacklist_ports,
            'DS_MinimumRank': form.vars.min_rank,
            'DS_EXPLOIT_SPEED': form.vars.exploit_speed,
            'DS_EXPLOIT_TIMEOUT': form.vars.exploit_timeout,
            'DS_LimitSessions': form.vars.limit_sessions,
            'DS_IgnoreFragileDevices': form.vars.ignore_fragile,
            'DS_FilterByOS': form.vars.filter_by_os,
            'DS_MATCH_VULNS': form.vars.filter_by_vuln,
            'DS_MATCH_PORTS': form.vars.filter_by_ports,
            'DS_OnlyMatch': form.vars.dry_run,
            'DS_PAYLOAD_METHOD': form.vars.payload,
            'DS_PAYLOAD_TYPE': form.vars.payload_type,
            'DS_PAYLOAD_PORTS': form.vars.payload_ports,
            'DS_EVASION_LEVEL_TCP': form.vars.evasion_tcp,
            'DS_EVASION_LEVEL_APP': form.vars.evasion_app,
            #'DS_ModuleFilter': form.vars.filter_by_os,
        }
        task = msf.start_exploit(args)
        msfurl = os.path.join(msf_settings['url'], 'workspaces', msf_settings['workspace_num'], 'tasks', task['task_id'])
        redirect(msfurl)
    elif form.errors:
        response.flash = "Error in form"

    return dict(form=form, alert=alert)

##-------------------------------------------------------------------------
## loots
##-------------------------------------------------------------------------

@auth.requires_login()
def import_pwdump():
    """Downloads a pwdump loot and processes it"""
    msf_settings = msf_get_config(session)
    alert = False
    error = None
    response.title = "%s :: Import Metasploit PWDUMP Loot" % (settings.title)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(alert=True, error=str(error), form=None)

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
        data = msf.loot_list(msf_settings['workspace'])
    except MSFAPIError, error:
        return dict(alert=True, error=str(error), form=None)

    if not alert:
        loot_list = []    # list of loot IDs and IPs
        loot_hosts = {}   # mapping of IP to loot IDs
        for k,v in data.iteritems():
            if v['ltype'] == 'host.windows.pwdump' or v['ltype'] == 'windows.hashes':
                loot_list.append([k, v['host']])
                loot_hosts.setdefault(v['host'], k)

        form=SQLFORM.factory(
            Field('hosts', 'list', requires=IS_IN_SET(loot_list, multiple=True), label=T('Host')),
            Field('host_text', 'text', label=T('Host list (1 per line)')),
            Field('addevidence', 'boolean', label=T('Add to Evidence')),
        )

        if form.accepts(request, session):
            from skaldship.metasploit import process_pwdump_loot
            data = []
            # based on which form data is entered, make a new loot_list
            if len(form.vars.hosts) > 0:
                loot_list = form.vars.hosts
            elif len(form.vars.host_text) > 0:
                for ip in form.vars.host_text.split('\n'):
                    try:
                        loot_list.append(loot_hosts[ip])
                    except:
                        logging.debug("%s not found in MSF loot list" % (ip))
                        continue

            retval = process_pwdump_loot(loot_list, msf)
            response.flash = "PWDUMP files imported\n%s" % (retval)
        elif form.errors:
            response.flash = "Errors in your form"
    else:
        form = None

    return dict(form=form, alert=alert, error=str(error))

@auth.requires_login()
def import_screenshots():
    """
    Import Screenshot files from Metasploit Pro into Kvasir
    """
    response.title = "%s :: Import Metasploit Screenshots" % (settings.title)
    msf_settings = msf_get_config(session)
    loot_apidata={}

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(form=None, error=str(error), alert=True)

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
        loot_apidata = msf.loot_list(msf_settings['workspace'])
    except MSFAPIError, error:
        return dict(form=None, error=str(error), alert=True)

    loot_list = []
    loot_dict = {}
    loot_hosts = {}
    for k,v in loot_apidata.iteritems():
        if v['ltype'] == 'host.windows.screenshot':
            loot_list.append([k, v['host']])
            loot_dict.setdefault(k, v['host'])
            loot_hosts.setdefault(v['host'], k)

    form=SQLFORM.factory(
        Field('host', 'list', requires=IS_IN_SET(loot_list, multiple=True), label=T('Host')),
        Field('host_text', 'text', label=T('Host list (1 per line)')),
    )

    if form.accepts(request, session):
        loots = []
        # based on which form data is entered, make a new loot_list
        if form.vars.hosts:
            loot_list = form.vars.hosts
        elif form.vars.host_text:
            for ip in form.vars.host_text.split('\n'):
                try:
                    loot_list.append(loot_hosts[ip])
                except:
                    logging.debug("%s not found in MSF loot list" % (ip))
                    continue

        loot_count = process_screenshot_loot(loot_list, msf)
        repsonse.flash = 'Screenshots added for %s host(s)' % (loot_count)

    elif form.errors:
        response.flash = "Errors in your form"

    return dict(form=form, alert=False, error=None)

@auth.requires_login()
def list_lootfiles():
    """
    Lists local loot files for import processing into Kvasir. This does not
    use the Metasploit API and depends upon a directory being local to the
    web2py server instance. The API is used to check if pro is installed
    and sets the loot_dir to Linux or Windows path
    """
    import os
    import re
    response.title = "%s :: Metasploit Loots" % (settings.title)
    msf_settings = msf_get_config(session)

    dbsvcs = db.t_services
    # TODO: from skaldship.db import get_services
    loot_dir = request.args(0)

    if not loot_dir:
        try:
            from MetasploitAPI import MetasploitAPI, MSFAPIError
            msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
            if msf.pro_about():
                if platform in ["linux", "linux2"]:
                    loot_dir = "/opt/metasploit_pro/apps/pro/loot"
                else:
                    loot_dir = "C:\\Metasploit\\apps\\pro\\loot"
        except ImportError, error:
            pass

    if not loot_dir:
        from sys import platform
        if platform in ["linux", "linux2", "darwin", "freebsd"]:
            loot_dir = os.path.join(os.environ.get('HOME'), '.msf4/loot')
        elif platform in ["win32", "cygwin"]:
            loot_dir = '$FINDYOUR/msf4/loot/path'

    try:
        os.chdir(loot_dir)
        loot_files = os.listdir(loot_dir)
    except OSError:
        loot_files = []

    loot_file_details = []
    for loot in loot_files:
        try:
            (timestamp, workspace, ipaddr, filetype, extension) = re.split('_', loot)
        except ValueError:
            logging.warn("Invalid loot file: %s" % (loot))
            continue

        # TODO: service_list = get_services(ipaddr)
        host_rec = get_host_record(ipaddr)
        services = []
        for service in db(dbsvcs.f_hosts_id==host_rec).select(dbsvcs.id, dbsvcs.f_proto, dbsvcs.f_number, cache=(cache.ram, 120)):
            services.append([service.id, "%s/%s" % (service.f_proto, service.f_number)])
        loot_file_details.append([
            workspace, ipaddr, services, filetype
        ])

    form_lootdir = SQLFORM.factory(
        Field('lootdir', 'string', default=loot_dir, requires=IS_NOT_EMPTY(), label=T('Metasploit Loot Directory')),
    )

    return dict(form_lootdir=form_lootdir, loot_file_details=loot_file_details)

##-------------------------------------------------------------------------
## report
##-------------------------------------------------------------------------

@auth.requires_login()
def import_report():
    """
    Import a MSF Pro XML Report.

    TODO: FINISH HIM!
    """

    msf_settings = msf_get_config(session)
    if msf_settings['workspace'] is None:
        redirect(URL('api_settings'))

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    if not msf.login():
        response.flash = "Error logging into Metasploit, check your settings"
        redirect(URL('api_settings'))

    form = SQLFORM.factory(
        Field('whitelist', 'text', label=T('Whitelist hosts/nets')),
        Field('blacklist', 'text', label=T('Blacklist hosts/nets')),
    )

    if form.accepts(request, sesssion):
        # build the configuration hash
        rpt_data = {}
        rpt_data['DS_REPORT_TYPE'] = 'XML'
        rpt_data['DS_WHITELIST_HOSTS'] = form.vars.whitelist
        rpt_data['DS_BLACKLIST_HOSTS'] = form.vars.blacklist
        rpt_data['Workdspace'] = msf_settings['workspace']

        # send the report request and get the task id
        rpt_taskid = msf.pro_start_report(rpt_data)

        # check the task status


        # download the report data

@auth.requires_login()
def import_report_xml():
    """
    Upload/import Metasploit XML export file
    """
    import time
    import os
    from skaldship.general import check_datadir

    msf_settings = msf_get_config(session)
    response.title = "%s :: Import Metasploit Pro Report XML" % (settings.title)
    filedir = os.path.join(request.folder,'data','scanfiles')
    fields = []
    alert = False
    error = None

    # buld the dropdown user list
    users = db(db.auth_user).select()
    userlist = []
    for user in users:
        userlist.append( [ user.id, user.username ] )

    fields.append(Field('f_filename', 'upload', uploadfolder=filedir, label=T('Metasploit XML File')))

    # check to see if we have a Metasploit Pro instance configured and talking
    # if so pull a list of the workspaces and present them
    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
        msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    except ImportError, error:
        msf = None

    if msf:
        try:
            msf_reports_res = msf.report_list(workspace=msf_settings['workspace'])
        except MSFAPIError, error:
            msf_reports_res = None

    if msf_reports_res:
        from datetime import datetime
        msf_reports = []
        for rpt in msf_reports_res.keys():
            report_name = "Generated: %s" % (datetime.strftime(datetime.fromtimestamp(msf_reports_res[rpt]['created_at']), "%m-%d-%y %H:%M:%S"))
            msf_reports.append([rpt, report_name])
        fields.append(Field('f_msf_report', type='string', label=T('MSF Pro Report'), requires=IS_EMPTY_OR(IS_IN_SET(msf_reports, zero=None))))

    fields.append(Field('f_engineer', type='integer', label=T('Engineer'), default=auth.user.id, requires=IS_IN_SET(userlist)))
    fields.append(Field('f_asset_group', type='string', label=T('Asset Group for new Hosts'), default="Metasploit Import", requires=IS_NOT_EMPTY()))
    fields.append(Field('f_include_list', type='text', label=T('Hosts to Only Include')))
    fields.append(Field('f_ignore_list', type='text', label=T('Hosts to Ignore')))
    fields.append(Field('f_update_hosts', type='boolean', default=True, label=T('Update Existing Hosts')))
    fields.append(Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background task')))
    form = SQLFORM.factory(*fields, table_name='metasploit_xml')

    if form.errors:
        response.flash = 'Error in form'
    elif form.accepts(request.vars, session):
        # build the hosts only/exclude list
        ip_exclude = []
        data = form.vars.get('f_ignore_list')
        if data:
            ip_exclude = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals
        ip_include = []
        data = form.vars.get('f_include_list')
        if data:
            ip_include = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals

        if form.vars.f_msf_report:
            try:
                msf_report = msf.report_download(rptid=form.vars.f_msf_report)
            except MSFAPIError, error:
                error = "Unable to download report from Metasploit Pro: %s" % (str(error))
                return dict(form=form, alert=True, error=error)
            check_datadir(request.folder)
            filename =  os.path.join(filedir, "msfpro-%s-%s.xml" % (msf_settings['workspace'], int(time.time())))
            fout = open(filename, "w")
            fout.write(msf_report['data'])
            fout.close()
            del(msf_report)
        else:
            filename = form.vars.f_filename
            filename = os.path.join(filedir, form.vars.f_filename)

        if form.vars.f_taskit:
            task = scheduler.queue_task(
                scanner_import,
                pvars=dict(
                    scanner='metasploit',
                    filename=filename,
                    asset_group=form.vars.f_asset_group,
                    engineer=form.vars.f_engineer,
                    ip_ignore_list=ip_exclude,
                    ip_include_list=ip_include,
                    update_hosts=form.vars.f_update_hosts,
                ),
                group_name=settings.scheduler_group_name,
                sync_output=5,
                timeout=settings.scheduler_timeout
            )
            if task.id:
                redirect(URL('tasks', 'status', args=task.id))
            else:
                response.flash = "Error submitting job: %s" % (task.errors)
        else:
            from skaldship.metasploit import process_report_xml
            logger.info("Starting Metasploit XML Import")
            result = process_report_xml(
                filename=filename,
                asset_group=form.vars.f_asset_group,
                engineer=form.vars.f_engineer,
                ip_ignore_list=ip_exclude,
                ip_include_list=ip_include,
                update_hosts=form.vars.f_update_hosts,
            )
            response.flash = result

    return dict(form=form, alert=alert, error=error)


##-------------------------------------------------------------------------
## sending data to metasploit
##-------------------------------------------------------------------------

@auth.requires_login()
def send_scanxml():
    """Sends scan XML output file to MSF Pro for importing"""
    import os

    response.title = "%s :: Send Scan XML Data to Metasploit" % (settings.title)
    msf_settings = msf_get_config(session)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(error=str(error), alert=True, form=None)

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
    except MSFAPIError, error:
        return dict(error=str(error), alert=True, form=None)

    filedir = os.path.join(request.folder,'data','scanfiles')
    try:
        scanfiles = os.listdir(filedir)
    except OSError:
        scanfiles = []
    file_select = []
    count = 0
    for fn in scanfiles:
        file_select.append([count, fn])
        count += 1

    form = SQLFORM.factory(
        Field('fname', 'string', requires=IS_IN_SET(file_select), label=T('Scan File')),
        Field('blacklist', 'text', label=T('Blacklisted hosts'),
            comment=T('Targets to blacklist can be IP Addresses, ranged lists or subnets. One per line.')
        ),
        Field('preserve_hosts', 'boolean', default=False, label=T('Preserve existing hosts')),
    )

    if form.accepts(request, session):
        fname = file_select[int(form.vars.fname)][1]
        fname = os.path.join(filedir, fname)

        try:
            scan_data = open(fname, "r+").readlines()
        except Exception, error:
            return dict(form=form, error=str(error), alert=True)

        task = msf.pro_import_data(
                msf_workspace,
                "".join(scan_data),
                {
                  'preserve_hosts': form.vars.preserve_hosts,
                  'blacklist_hosts': "\n".join(form.vars.blacklist)
                },
            )

        """
        # documented in API but not valid yet @9/6/13
        #validate = msf.pro_validate_import_file(fname)
        task = msf.pro_start_import({
                  'workspace': msf_workspace,
                  'username': msf_settings['user'],
                  'DS_PATH': fname,
                  'DS_PRESERVE_HOSTS': form.vars.preserve_hosts,
                  'DS_BLACKLIST_HOSTS': "\n".join(form.vars.blacklist),
                  'DS_REMOVE_FILE': False,
                  'DS_ImportTags': True,
                })
        """
        msfurl = os.path.join(msf_settings['url'], 'workspaces', msf_settings['workspace_num'], 'tasks', task['task_id'])
        redirect(msfurl)
    elif form.errors:
        response.flash = "Errors in your form"

    return dict(form=form, alert=False, error=None)

@auth.requires_login()
def send_accounts():
    """Builds a list of username:passwords and sends it to Metasploit"""
    msf_settings = msf_get_config(session)
    response.title = "%s :: Send Kvasir Passwords to Metasploit Pro" % (settings.title)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(error=str(error), alert=True, form=None)

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
    except MSFAPIError, error:
        return dict(error=str(error), alert=True, form=None)

    form = SQLFORM.factory(
        Field('userpass', 'boolean', default=True, label=T('User/Pass Combos')),
        Field('pwdump', 'boolean', default=True, label=T('PWDUMP Hashes')),
    )

    if form.accepts(request, session):
        pass
    elif form.errors:
        reseponse.flash = "Error in your form"
        return dict(form=form, alert=False, error=None)
    else:
        return dict(form=form, alert=False, error=None)

    """
    First build a list of username:passwords from the t_accounts database and
    make a temporary file, then start_import_creds the file

    Second build a pwdump list for LM/NT hashes, make a temporary file, then
    start_import_creds that file!

    Requires MSFPRO and Kvasir be on the same workstation.
    """
    tasks = {}
    import tempfile
    if form.vars.userpass:
        # build username:password file
        rows = db(db.t_accounts.f_compromised == True).select(db.t_accounts.f_username, db.t_accounts.f_password, cache=(cache.ram, 60))
        if rows is not None:
            tmpfile = tempfile.NamedTemporaryFile(delete=False)
            fname = tmpfile.name
            for row in rows:
                tmpfile.write("%s %s\n" % (row.f_username, row.f_password))
            tmpfile.close()
            opts = {
                'workspace': msf_workspace,
                'DS_FTYPE': 'userpass',
                'DS_IMPORT_PATH': fname,
                'DS_NAME': 'Kvasir import %s' % (fname),
                'DS_DESC': 'Kvasir import',
                'DS_REMOVE_FILE': True,
            }
            task = msf.pro_start_import_creds(opts)
            redirect(URL('task_log', args=task.get('id')))
        else:
            response.flash = "No user:pass combos to import"

    if form.vars.pwdump:
        # build pwdump file
        rows = db(db.t_accounts.f_hash1_type == "LM").select(
            db.t_accounts.f_username,
            db.t_accounts.f_uid,
            db.t_accounts.f_hash1,
            db.t_accounts.f_hash2,
            cache=(cache.ram, 60)
        )
        if rows is not None:
            tmpfile = tempfile.NamedTemporaryFile(delete=False)
            fname = tmpfile.name
            for row in rows:
                tmpfile.write("%s:%s:%s:%s:::\n" % (row.f_username, row.f_uid, row.f_hash1, row.f_hash2))

            tmpfile.close()
            opts = {
                'workspace': msf_workspace,
                'DS_FTYPE': 'pwdump',
                'DS_IMPORT_PATH': fname,
                'DS_NAME': 'Kvasir pwdump import %s' % (fname),
                'DS_DESC': 'Kvasir pwdump import',
                'DS_REMOVE_FILE': True,
            }
            task = msf.pro_start_import_creds(opts)
            redirect(URL('task_log', args=task.get('id')))
        else:
            response.flash = "No pwdump hashes to import"

    return dict(form=form, alert=False, error=None)

##-------------------------------------------------------------------------
## task monitoring / killing
##-------------------------------------------------------------------------

@auth.requires_login()
def task_list():
    """Obtains a list of tasks"""
    msf_settings = msf_get_config(session)
    response.title = "%s :: Metasploit Task List" % (settings.title)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(error=str(error), alert=True, tasks=None)

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
    except MSFAPIError, error:
        return dict(error=str(error), alert=True, tasks=None)

    tasks = msf.task_list()
    tasklist = []
    if request.vars.has_key('status'):
        # only return specific tasks as defined in status
        for taskid,task in tasks.iteritems():
            if task['status'] == request.vars.status.lower():
                tasklist.append({taskid: task})
    else:
        tasklist = tasks

    return dict(tasks=tasklist)

@auth.requires_login()
def task_status():
    """Show details of a specifc task (but not the log file)"""
    msf_settings = msf_get_config(session)
    response.title = "%s :: Metasploit Task Status" % (settings.title)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(error=str(error), alert=True, data=None)

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
    except MSFAPIError, error:
        return dict(error=str(error), alert=True, data=None)

    if not request.vars.has_key('taskid'):
        tasks = msf.task_list()
        task_list = []
        for taskid,task in tasks.iteritems():
            task_list.append(
                [taskid, "%s (%s) :: %s :: %s" % (
                    taskid,
                    tasks[taskid]['status'],
                    tasks[taskid]['description'],
                    tasks[taskid]['info'],
               )])
        form = SQLFORM.factory(
            Field('taskid', 'string', requires=IS_IN_SET(task_list), label=T('Task ID'))
        )
        return dict(form=form)

    data = msf.task_status(request.vars.taskid)
    return dict(data=data)

@auth.requires_login()
def task_log():
    """Show the details and log file of a specifc task"""
    msf_settings = msf_get_config(session)
    response.title = "%s :: Metasploit Task Log" % (settings.title)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(error=str(error), alert=True, data=None)

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
    except MSFAPIError, error:
        return dict(error=str(error), alert=True, data=None)

    if not request.vars.has_key('taskid'):
        tasks = msf.task_list()
        task_list = []
        for taskid,task in tasks.iteritems():
            task_list.append(
                [taskid, "%s (%s) :: %s :: %s" % (
                    taskid,
                    tasks[taskid]['status'],
                    tasks[taskid]['description'],
                    tasks[taskid]['info'],
               )])
        form = SQLFORM.factory(
            Field('taskid', 'string', requires=IS_IN_SET(task_list), label=T('Task ID'))
        )
        return dict(form=form)

    data = msf.task_log(request.vars.taskid)
    return dict(data=data)

@auth.requires_login()
def task_stop():
    """Stop a running task"""
    msf_settings = msf_get_config(session)

    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(error=str(error), alert=True, form=None)

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
    except MSFAPIError, error:
        return dict(error=str(error), alert=True, form=None)

    if not request.vars.has_key('taskid'):
        tasks = msf.task_list()
        task_list = []
        for taskid,task in tasks.iteritems():
            if tasks[taskid]['status'] == 'running':
                task_list.append(
                    [taskid, "%s (%s) :: %s :: %s" % (
                        taskid,
                        tasks[taskid]['status'],
                        tasks[taskid]['description'],
                        tasks[taskid]['info'],
                   )])
        form = SQLFORM.factory(
            Field('taskid', 'string', requires=IS_IN_SET(task_list), label=T('Task ID'))
        )
        return dict(form=form)

    response.title = "%s :: Stop Metasploit Task" % (settings.title)
    data = msf.task_stop(request.vars.taskid)
    return dict(data=data)

##-------------------------------------------------------------------------
## Targeted exploit
##-------------------------------------------------------------------------

@auth.requires_login()
def exploit_host():
    """
    Build an exploit for a specific target
    """

    msf_settings = msf_get_config(session)
    try:
        from MetasploitAPI import MetasploitAPI, MSFAPIError
    except ImportError, error:
        return dict(error=str(error), alert=True, form=None)

    msf = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
    try:
        msf.login()
    except MSFAPIError, error:
        return dict(error=str(error), alert=True, form=None)

    target = request.vars.f_target or None
    exploit = request.vars.f_exploit or None

    form = SQLFORM.factory()

########NEW FILE########
__FILENAME__ = nessus
# encoding: utf-8
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Nessus controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.metasploit import msf_get_config
from skaldship.nessus import nessus_get_config
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return dict()

@auth.requires_login()
def import_scan():
    """
    Upload and import Nexpose Scan file
    """
    msf_settings = msf_get_config(session)
    try:
        # check to see if we have a Metasploit RPC instance configured and talking
        from MetasploitAPI import MetasploitAPI
        msf_api = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
        working_msf_api = msf_api.login()
    except:
        working_msf_api = False

    from skaldship.general import check_datadir
    import time
    import os

    filedir = os.path.join(request.folder, 'data', 'scanfiles')
    response.title = "%s :: Import Nessus Scan Results" % (settings.title)
    fields = []

    # buld the dropdown user list
    users = db(db.auth_user).select()
    userlist = []
    for user in users:
        userlist.append([user.id, user.username])

    nessus_config = nessus_get_config(session)
    # {'ignored_vulnids': [19506, 11219, 34277],
    #  'servers': {'server_1': {'password': 'password',
    #                           'url': 'https://localhost:8834/',
    #                           'user': 'admin'}}}
    nessusreports = [[0, None]]
    import NessusAPI
    #if auth.user.f_nessus_host is not None:
    servers = nessus_config.get('servers', {})
    for k, v in servers.iteritems():
        try:
            # check to see if NessusAPI is working
            nessus = NessusAPI.NessusConnection(v.get('user'), v.get('password'), url=v.get('url'))
            reports = nessus.list_reports()
            for report in reports:
                ts = time.ctime(float(report.timestamp))
                nessusreports.append(["%s:%s" % (k, report.name), "%s: %s - %s (%s)" % (k, report.readablename, ts, report.status)])
        except Exception, e:
            logger.error("Error communicating with %s: %s" % (k, str(e)))

    if len(nessusreports) > 1:
        fields.append(Field('f_nessus_report', type='integer', label=T('Nessus Report'), requires=IS_IN_SET(nessusreports, zero=None)))

    fields.append(Field('f_filename', 'upload', uploadfolder=filedir, label=T('Nessus Scan File')))
    fields.append(Field('f_engineer', type='integer', label=T('Engineer'), default=auth.user.id, requires=IS_IN_SET(userlist)))
    fields.append(Field('f_asset_group', type='string', label=T('Asset Group'), requires=IS_NOT_EMPTY()))

    # check to see if we have a Metasploit Pro instance configured and talking
    # if so pull a list of the workspaces and present them
    if working_msf_api:
        msf_workspaces = ["None"]
        for w in msf_api.pro_workspaces().keys():
            msf_workspaces.append(w)
        fields.append(Field('f_msf_workspace', type='string', label=T('MSF Pro Workspace'), requires=IS_EMPTY_OR(IS_IN_SET(msf_workspaces, zero=None))))

    fields.append(Field('f_include_list', type='text', label=T('Hosts to Only Include')))
    fields.append(Field('f_ignore_list', type='text', label=T('Hosts to Ignore')))
    fields.append(Field('f_update_hosts', type='boolean', label=T('Update Host Information'), default=False))
    fields.append(Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background task')))
    form = SQLFORM.factory(*fields, table_name='nessus_scan')

    # form processing
    if form.errors:
        response.flash = 'Error in form'
    elif form.accepts(request.vars, session):
        # if no Nessus servers configured or valid, set report_name to 0
        if nessusreports == [[0, None]]:
            report_name = '0'
        else:
            if form.vars.f_nessus_report != "0":
                # If a nessus report is selected, try to parse it to set report_name
                try:
                    (server, report_name) = form.vars.f_nessus_report.split(':')
                except ValueError, e:
                    msg = "Invalid report name sent: %s" % (form.vars.f_nessus_report)
                    response.flash = msg
                    logging.error(msg)
                    return dict(form=form)
            else:
                # set report_name to 0 if no f_nessus_report sent
                report_name = '0'

        if report_name != '0':
            # download a report from a Nessus server
            filename = os.path.join(filedir, "nessus-%s-%s.xml" % (form.vars.f_asset_group, int(time.time())))
            check_datadir(request.folder)
            fout = open(filename, "w")
            try:
                # build a new nessus connection with the configured server details and download the report
                n_server = nessus_config.get('servers').get(server)
                nessus = NessusAPI.NessusConnection(n_server.get('user'), n_server.get('password'), url=n_server.get('url'))
                nessus.download_report(report_name, fout)
                fout.close()
            except Exception, e:
                msg = ("Error download Nessus report: %s" % (e))
                logger.error(msg)
                response.flash = msg
                return dict(form=form)
        else:
            filename = os.path.join(filedir, form.vars.f_filename)

        # build the hosts only/exclude list
        ip_exclude = []
        data = form.vars.get('f_ignore_list')
        if data:
            ip_exclude = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals
        ip_include = []
        data = form.vars.get('f_include_list')
        if data:
            ip_include = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals

        if form.vars.f_msf_workspace:
            msf_workspace = form.vars.f_msf_workspace
            if msf_workspace == "None":
                msf_workspace = None
        else:
            msf_workspace = None
        msf_settings = {'workspace': msf_workspace, 'url': msf_settings['url'], 'key': msf_settings['key']}

        if form.vars.f_taskit:
            task = scheduler.queue_task(
                scanner_import,
                pvars=dict(
                    scanner='nessus',
                    filename=filename,
                    asset_group=form.vars.f_asset_group,
                    engineer=form.vars.f_engineer,
                    msf_settings=msf_settings,
                    ip_ignore_list=ip_exclude,
                    ip_include_list=ip_include,
                    update_hosts=form.vars.f_update_hosts,
                ),
                group_name=settings.scheduler_group_name,
                sync_output=5,
                timeout=settings.scheduler_timeout
            )
            if task.id:
                redirect(URL('tasks', 'status', args=task.id))
            else:
                response.flash = "Error submitting job: %s" % (task.errors)
        else:
            from skaldship.nessus import process_scanfile
            logger.info("Starting Nessus Report Import")
            response.flash = process_scanfile(
                filename=filename,
                asset_group=form.vars.f_asset_group,
                engineer=form.vars.f_engineer,
                msf_settings=msf_settings,
                ip_ignore_list=ip_exclude,
                ip_include_list=ip_include,
                update_hosts=form.vars.f_update_hosts,
            )
            redirect(URL('default', 'index'))

    return dict(form=form)

########NEW FILE########
__FILENAME__ = netbios
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## NetBIOS controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import create_hostfilter_query, get_host_record, host_title_maker, host_a_maker
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


##-------------------------------------------------------------------------
## netbios
##-------------------------------------------------------------------------

@auth.requires_login()
def index():
    return redirect(URL('list'))

def list():
    form=SQLFORM.grid(
        db.t_netbios,
        details=False,
        maxtextlength=50,
    )
    response.title = "%s :: NetBIOS Data" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def add():
    if request.args(0):
        record = get_host_record(request.args(0))
        db.t_netbios.f_hosts_id.default = record.id
    response.title = "%s :: Add NetBIOS Data" % (settings.title)
    form=crud.create(db.t_netbios, next='edit/[id]', message="NetBIOS data added")
    return dict(form=form)

@auth.requires_login()
def edit():
    """Creates and process a form to edit a NetBIOS record"""
    referrer = request.env.http_referer or ''
    if "hosts/detail" in referrer:
        # check for hosts/detail form in referrer and grab the netbios record
        # given the t_hosts.id or provide an add form
        record = db(db.t_netbios.f_hosts_id==request.args(0)).select().first()
        if not record and request.extension == 'load':
            redirect(URL('add', args=request.args(0)))
        form=crud.update(db.t_netbios, record, next='edit/%s' % request.args(0),
                         message="NetBIOS data updated",
                         ondelete=lambda form: redirect(URL('add')))
    else:
        record = db.t_netbios(request.args(0)) or redirect(URL('add'))
        form=crud.update(db.t_netbios, record, next='edit/[id]',
                         ondelete=lambda form: redirect(URL('add')))

    response.title = "%s :: Edit NetBIOS" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def by_host():
    record = db(db.t_netbios.f_hosts_id==request.args(0)).select().first()
    if not record and request.extension == 'load':
        form=None
    else:
        form=crud.update(db.t_netbios, record, next='by_host/%s' % request.args(0),
                         message="NetBIOS data updated")

    addnetbios = AddModal(
        db.t_netbios, 'Add NetBIOS', 'Add', 'Add NetBIOS',
        #fields=[],
    )
    db.t_netbios.f_hosts_id.default = request.args(0)
    db.t_netbios.id.comment = addnetbios.create()

    return dict(form=form, addnetbios=addnetbios)

##-------------------------------------------------------------------------
## NetBIOS Domain Details
##-------------------------------------------------------------------------

@auth.requires_login()
def domain_detail():
    """Creates a page of all netbios related information for a workgroup/domain"""

    from gluon.serializers import json
    aaData = []
    acctData = []
    response.title = "%s :: NetBIOS Details" % (settings.title)
    if not request.vars.domain:
        response.title = "%s :: NetBIOS Details for ALL" % (settings.title)
        query = (db.t_netbios.id>0)
    else:
        response.title = "%s :: NetBIOS Details for %s" % (settings.title, request.vars.domain)
        query = (db.t_netbios.f_domain == request.vars.domain)

    query &= (db.t_netbios.f_hosts_id == db.t_hosts.id)
    # pull the list of all servers in a NetBIOS Domain
    servers = db(query)(db.t_netbios).select()

    for server in servers:
        #query = (db.t_accounts.f_services_id == db.t_services.id)
        # go through each service looking for any compr accounts,
        accts_list = []
        for svc in server.t_hosts.t_services.select():
            query = (db.t_accounts.f_services_id == db.t_services.id)
            query &= (db.t_services.id == svc.id)
            query &= ((db.t_accounts.f_password != None) | (db.t_accounts.f_hash1 != None))
            for accts in db(query).select():
                accts_list.append((accts.t_accounts.f_username,
                                   accts.t_accounts.f_password,
                                   accts.t_accounts.f_level,
                                   "%s:%s" % (accts.t_accounts.f_hash1, accts.t_accounts.f_hash2),
                                   accts.t_accounts.f_source,
                                   "%s/%s (%s)" % (accts.t_services.f_proto, accts.t_services.f_number, accts.t_services.f_name),
                                   ))

        atxt = []
        if len(accts_list) > 0:
            atxt.append(TD(IMG(_src=URL(request.application,'static','images/details_open.png'))))
        else:
            atxt.append(TD())
        atxt.append(TD(host_a_maker(server.t_hosts)))
        atxt.append(TD(json(accts_list)))
        atxt.append(TD(server.t_netbios.f_domain))
        atxt.append(TD(server.t_netbios.f_type))
        atxt.append(TD(server.t_netbios.f_lockout_duration))
        atxt.append(TD(server.t_netbios.f_shares))
        aaData.append(TR(atxt))

    table = TABLE(THEAD(TR(TH('', _width="5%"),
                           TH(T('Host')),
                           TH(T('Compr. Accts')),
                           TH(T('Domain')),
                           TH(T('Type')),
                           TH(T('Lockout Duration')),
                           TH(T('Shares'))
                           )  ),
                  TBODY(aaData),
                  _class="datatable",
                  _id="netbiostable",
                  _style="width:100%")

    domains= []
    for domain in db(db.t_netbios).select(db.t_netbios.f_domain, distinct=True):
        domains.append(domain.f_domain)

    return dict(table=table, domains=domains)
########NEW FILE########
__FILENAME__ = nexpose
# encoding: utf-8
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Nexpose controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from nxajax import NXAJAX, ScanTemplates
from skaldship.nexpose import nexpose_get_config
import logging
from skaldship.log import log
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return dict()

@auth.requires_login()
def vulnlist():
    """
    Produces a list of Nexpose vulnids for a select/search box
    """
    try:
        from lxml import etree
    except ImportError:
        try:
            from xml.etree import cElementTree as etree
        except ImportError:
            from xml.etree import ElementTree as etree

    from NexposeAPI import VulnData
    import os
    import time

    nexpose_config = nexpose_get_config()

    vuln_class = VulnData()
    vuln_class.host = nexpose_config['host']
    vuln_class.port = nexpose_config['port']

    nx_vuln_fname = os.path.join(request.folder, 'data', 'nexpose_vuln_summary.xml')
    if os.path.exists(nx_vuln_fname):
        # check to see if we should refresh the nexpose_vuln_summary.xml file
        ctime = os.stat(nx_vuln_fname).st_ctime
        if (time.time() - ctime >= 7500):
            update_summary = True
        else:
            update_summary = False
    else:
        update_summary = True

    if update_summary:
        if vuln_class.login(user_id=nexpose_config['user'], password=nexpose_config['password']):
            # pull the list out
            vuln_class.populate_summary()
            fout = open(nx_vuln_fname, "wb+")
            fout.writelines(vuln_class.vulnxml)
            fout.close()

    vulnxml = etree.parse(nx_vuln_fname)
    vdata = []
    counter = 0
    for vuln in vulnxml.iterfind('.//VulnerabilitySummary[@id]'):
        vdata.append([counter, vuln.get('id')])

    return dict(data=vdata)

@auth.requires_login()
def import_vulnid():
    """
    Downloads the detailed vulnerability data from Nexpose based on
    a vuln id passed to it
    """
    form = SQLFORM.factory(
        Field('nexid', 'string', label=T('Nexpose ID')),
        Field('nexid_list', 'text', label=T('Nexpose ID List'))
    )

    response.title = "%s :: Import Nexpose VulnID" % settings.title
    nexpose_config = nexpose_get_config()

    if form.process().accepted:
        from NexposeAPI import VulnData
        from skaldship.nexpose import vuln_parse

        nxvulns = VulnData()
        nxvulns.host = nexpose_config['host']
        nxvulns.port = nexpose_config['port']

        nexpose_ids = []
        if form.vars.nexid:
            nexpose_ids.extend([form.vars.nexid])
        if form.vars.nexid_list:
            nexpose_ids.extend(form.vars.nexid_list.split('\r\n'))

        res = nxvulns.login(user_id=nexpose_config['user'], password=nexpose_config['password'])
        if res:
            stats = {'added': 0, 'invalid':  0}
            for nexid in nexpose_ids:
                vulndetails = nxvulns.detail(nexid)
                if vulndetails is not None:
                    (vulnfields, references) = vuln_parse(vulndetails.find('Vulnerability'), fromapi=True)
                else:
                    stats['invalid'] += 1
                    continue

                # add the vulnerability to t_vulndata
                query = (db.t_vulndata.f_vulnid == nexid)
                vulnid = db.t_vulndata.update_or_insert(query, **vulnfields)
                if not vulnid:
                    row = db(query).select().first()
                    if row:
                        vulnid = row.id
                    else:
                        log(" [!] Could not find %s in database.." % nexid, logging.WARN)
                        stats['invalid'] += 1
                        continue

                db.commit()

                # add the references
                if vulnid is not None and references:
                    for reference in references:
                        # check to see if reference exists first
                        query = (db.t_vuln_refs.f_source == reference[0]) & (db.t_vuln_refs.f_text == reference[1])
                        ref_id = db.t_vuln_refs.update_or_insert(query, f_source=reference[0], f_text=reference[1])
                        if not ref_id:
                            ref_id = db(query).select().first().id

                        # make many-to-many relationship with t_vuln_data
                        db.t_vuln_references.update_or_insert(f_vuln_ref_id=ref_id, f_vulndata_id=vulnid)
                        db.commit()

                from skaldship.exploits import connect_exploits
                connect_exploits()
                log(" [-] Added Nexpose vulnerability: %s" % nexid)
                stats['added'] += 1
            response.flash = "%s added, %s skipped" % (stats['added'], stats['invalid'])
            return dict(form=form)
        else:
            response.flash = "Unable to login to Nexpose"
    elif form.errors:
        response.flash = "Error in form"

    return dict(form=form)

@auth.requires_login()
def vuln_update():
    # Update t_vulndata with vulndata from Nexpose
    # Requires username/password and hostname of a Nexpose
    # https instance. User can permit overwrite (updating)
    # the data if a Vulnerability ID exists in the db.

    response.title = "%s :: Nexpose Vulnerability Update" % settings.title
    form = SQLFORM.factory(
        Field('overwrite', 'boolean', default=False, label=T('Update existing')),
        Field('background', 'boolean', default=False, label=T('Run in background task'),
              requires=IS_NOT_EMPTY(error_message='Can only be run in background')
        ),
        Field('timeout', 'integer', default=144000, label=T('Timeout (in seconds)')),
        Field('do_import', 'boolean', default=False, label=T('Start the import'),
              requires=IS_NOT_EMPTY(error_message='Are you ready?')
        ),
    )

    nexpose_config = nexpose_get_config()
    if form.process().accepted:
        nexpose_server = {
            'host': nexpose_config['host'],
            'port': nexpose_config['port'],
            'user': nexpose_config['user'],
            'pw': nexpose_config['password'],
        }
        task = scheduler.queue_task(
            import_all_nexpose_vulndata,
            pvars=dict(
                overwrite=form.vars.overwrite,
                nexpose_server=nexpose_server,
            ),
            group_name=settings.scheduler_group_name,
            sync_output=5,
            repeats=1,
            timeout=form.vars.timeout,
        )
        if task.id:
            redirect(URL('tasks', 'status', args=task.id))
        else:
            response.flash = "Error submitting job: %s" % (task.errors)

    elif form.errors:
        response.flash = 'Error in form data'

    return dict(form=form)

@auth.requires_login()
def scan_template():
    from lxml import etree
    from StringIO import StringIO

    response.title = "%s :: Nexpose Scan Templates" % (settings.title)
    formupload = SQLFORM.factory(
        Field('f_filename', 'upload', uploadfolder=os.path.join(request.folder, 'data', 'misc'), label=T('Import Nexpose Scan Template')),
             _formname='uploader')

    if formupload.accepts(request.vars, formname='uploader'):
        najax = NXAJAX(session.najaxsession)
        template_class = ScanTemplates()
        filename = os.path.join(request.folder,'data','misc',formupload.vars.f_filename)
        template_xml = etree.parse(filename, etree.XMLParser())
        imported = template_class.importtemplate(etree.tostring(template_xml), najax)
        response.flash = imported
        templates = ScanTemplates.listscantemps(True, najax)
        parse_templates = DIV(TAG(templates).elements('templateid'))
        return dict(form="", form2=formupload, html=parse_templates)

    nexpose_config = nexpose_get_config()
    najax = NXAJAX(session.najaxsession)
    najax.host = nexpose_config['host']
    najax.port = nexpose_config['port']
    if najax.login(user_id=nexpose_config['user'], password=nexpose_config['password']):
        log("Logged in to Nexpose API. Session cached.")
        session.najaxsession = najax.getsession()
        template_class = ScanTemplates()
        templates = template_class.listscantemps(True, najax)
        response.flash = "Loaded %s scan templates" % (templates.count('<templateid>'))
        parse_templates = DIV(TAG(templates).elements('templateid'))
        return dict(form="", form2=formupload, html=parse_templates)
    else:
        response.flash = "Unable to login to Nexpose"

    return dict(form=formlogin, form2="", html="")

@auth.requires_login()
def list_scantemplates():
    response.title = "%s :: Nexpose Scan Templates" % (settings.title)
    html = DIV(TAG(session.templates).elements('templateid'))
    return dict(html=html)

@auth.requires_login()
def import_xml_scan():
    """
    Upload/import Nexpose XML Scan file via scheduler task
    """
    from NexposeAPI import NexposeAPI, Sites, Report
    from skaldship.general import check_datadir
    from skaldship.metasploit import msf_get_config
    import time
    import os
    msf_settings = msf_get_config(session)
    try:
        # check to see if we have a Metasploit RPC instance configured and talking
        from MetasploitAPI import MetasploitAPI
        msf_api = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
        working_msf_api = msf_api.login()
    except:
        working_msf_api = False

    filedir = os.path.join(request.folder, 'data', 'scanfiles')
    response.title = "%s :: Import Nexpose XML Scan Results" % (settings.title)
    fields = []

    # buld the dropdown user list
    users = db(db.auth_user).select()
    userlist = []
    for user in users:
        userlist.append( [ user.id, user.username ] )

    # check to see if nexpose is configured/active and get site listing
    nexpose_config = nexpose_get_config()
    nxsitelist = []
    if nexpose_config['host'] is not None and nexpose_config['user'] is not None:
        # see if the host is open/active first
        if nexpose_config['host'] is not None:
            sites = Sites()
            sites.host = nexpose_config['host']
            sites.port = nexpose_config['port']
            try:
                if sites.login(user_id=nexpose_config['user'], password=nexpose_config['password']):
                    sites = sites.listings()
                    nxsitelist.append( [ 0, None ] )
                    for k,v in sites.iteritems():
                        nxsitelist.append( [int(k), sites[k]['name']] )
            except Exception, e:
                pass

    if nxsitelist:
        fields.append(Field('f_nexpose_site', type='integer', label=T('Nexpose Site'), requires=IS_IN_SET(nxsitelist, zero=None)))

    fields.append(Field('f_filename', 'upload', uploadfolder=filedir, label=T('Nexpose XML File')))
    fields.append(Field('f_engineer', type='integer', label=T('Engineer'), default=auth.user.id, requires=IS_IN_SET(userlist)))
    fields.append(Field('f_asset_group', type='string', label=T('Asset Group'), requires=IS_NOT_EMPTY()))

    # If Metasploit available, pull a list of the workspaces and present them
    if working_msf_api:
        msf_workspaces = []
        msf_workspaces.append( "None" )
        for w in msf_api.pro_workspaces().keys():
            msf_workspaces.append(w)
        fields.append(Field('f_msf_workspace', type='string', label=T('MSF Pro Workspace'), requires=IS_EMPTY_OR(IS_IN_SET(msf_workspaces, zero=None))))

    fields.append(Field('f_include_list', type='text', label=T('Hosts to Only Include')))
    fields.append(Field('f_ignore_list', type='text', label=T('Hosts to Ignore')))
    fields.append(Field('f_update_hosts', type='boolean', label=T('Update Host Information'), default=False))
    fields.append(Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background task')))
    form = SQLFORM.factory(*fields, table_name='nexpose_xml')

    # form processing
    if form.errors:
        response.flash = 'Error in form'
    elif form.accepts(request.vars, session):
        # process a nexpose file
        if not nxsitelist:
            nexpose_site = '0'
        else:
            nexpose_site = form.vars.f_nexpose_site

        if nexpose_site != '0':
            report = Report()
            report.host = nexpose_config['host']
            report.port = nexpose_config['port']
            nx_loggedin = report.login(user_id=nexpose_config['user'], password=nexpose_config['password'])
            if nx_loggedin:
                # have nexpose generate the adhoc report
                check_datadir(request.folder)
                filename =  os.path.join(filedir, "%s-%s.xml" % (form.vars.f_asset_group, int(time.time())))
                fout = open(filename, "w")
                fout.write(report.adhoc_generate(filterid=nexpose_site))
                fout.close()
            else:
                response.flash = "Unable to login to Nexpose"
                return dict(form=form)
        else:
            filename = form.vars.f_filename
            filename = os.path.join(filedir, form.vars.f_filename)

        # build the hosts only/exclude list
        ip_exclude = []
        data = form.vars.get('f_ignore_list')
        if data:
            ip_exclude = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals
        ip_include = []
        data = form.vars.get('f_include_list')
        if data:
            ip_include = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals

        if form.vars.f_msf_workspace:
            msf_workspace = form.vars.f_msf_workspace
            if msf_workspace == "None":
                msf_workspace = None
        else:
            msf_workspace = None
        msf_settings = {'workspace': msf_workspace, 'url': msf_settings['url'], 'key': msf_settings['key']}

        if form.vars.f_taskit:
            task = scheduler.queue_task(
                scanner_import,
                pvars=dict(
                    scanner='nexpose',
                    filename=filename,
                    asset_group=form.vars.f_asset_group,
                    engineer=form.vars.f_engineer,
                    msf_settings=msf_settings,
                    ip_ignore_list=ip_exclude,
                    ip_include_list=ip_include,
                    update_hosts=form.vars.f_update_hosts,
                ),
                group_name=settings.scheduler_group_name,
                sync_output=5,
                timeout=settings.scheduler_timeout
            )
            if task.id:
                redirect(URL('tasks', 'status', args=task.id))
            else:
                response.flash = "Error submitting job: %s" % (task.errors)
        else:
            from skaldship.nexpose import process_xml
            log("Starting Nexpose XML Import")
            process_xml(
                filename=filename,
                asset_group=form.vars.f_asset_group,
                engineer=form.vars.f_engineer,
                msf_settings=msf_settings,
                ip_ignore_list=ip_exclude,
                ip_include_list=ip_include,
                update_hosts=form.vars.f_update_hosts,
            )
            response.flash = "Nexpose XML upload complete"
            redirect(URL('default', 'index'))

    return dict(form=form)

########NEW FILE########
__FILENAME__ = nmap
# encoding: utf-8
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Nmap controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return dict()

##-------------------------------------------------------------------------


@auth.requires_login()
def list_scripts():
    """
    Lists nmap scripts
    """
    from skaldship.nmap import script_metadata
    scr = script_metadata()
    return dict(scripts=scr)

##-------------------------------------------------------------------------


@auth.requires_login()
def import_xml_scan():
    """
    Upload/import Nmap XML Scan file via scheduler task
    """
    import time
    from skaldship.general import check_datadir
    from skaldship.metasploit import msf_get_config
    msf_settings = msf_get_config(session)

    try:
        # check to see if we have a Metasploit RPC instance configured and talking
        from MetasploitAPI import MetasploitAPI
        msf_api = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
        working_msf_api = msf_api.login()
    except:
        working_msf_api = False

    filedir = os.path.join(request.folder,'data','scanfiles')
    check_datadir(request.folder)
    response.title = "%s :: Import Nmap XML Scan Results" % (settings.title)

    fields = []

    # buld the dropdown user list
    users = db(db.auth_user).select()
    userlist = []
    for user in users:
        userlist.append( [ user.id, user.username ] )

    fields.append(Field('f_filename', 'upload', uploadfolder=filedir, label=T('Nmap XML File')))
    fields.append(Field('f_engineer', type='integer', label=T('Engineer'), default=auth.user.id, requires=IS_IN_SET(userlist)))
    fields.append(Field('f_asset_group', type='string', label=T('Asset Group'), requires=IS_NOT_EMPTY()))

    # If Metasploit available, pull a list of the workspaces and present them
    if working_msf_api:
        msf_workspaces = []
        msf_workspaces.append( "None" )
        for w in msf_api.pro_workspaces().keys():
            msf_workspaces.append(w)
        fields.append(Field('f_msf_workspace', type='string', label=T('MSF Pro Workspace'), requires=IS_EMPTY_OR(IS_IN_SET(msf_workspaces, zero=None))))

    fields.append(Field('f_addnoports', type='boolean', label=T('Add Hosts w/o Ports'), default=False))
    fields.append(Field('f_include_list', type='text', label=T('Hosts to Only Include')))
    fields.append(Field('f_ignore_list', type='text', label=T('Hosts to Ignore')))
    fields.append(Field('f_update_hosts', type='boolean', label=T('Update Host Information'), default=False))
    fields.append(Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background task')))
    form = SQLFORM.factory(*fields, table_name='nmap_xml')

    if form.errors:
        response.flash = 'Error in form'
    elif form.accepts(request.vars, session):
        # process a nmap file
        filename = form.vars.f_filename
        filename = os.path.join(filedir, form.vars.f_filename)

        # build the hosts only/exclude list
        ip_exclude = []
        data = form.vars.get('f_ignore_list')
        if data:
            ip_exclude = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals
        ip_include = []
        data = form.vars.get('f_include_list')
        if data:
            ip_include = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals

        if form.vars.f_msf_workspace:
            msf_workspace = form.vars.f_msf_workspace
            if msf_workspace == "None":
                msf_workspace = None
        else:
            msf_workspace = None
        msf_settings = {'workspace': msf_workspace, 'url': msf_settings['url'], 'key': msf_settings['key']}

        if form.vars.f_taskit:
            task = scheduler.queue_task(
                scanner_import,
                pvars=dict(
                    scanner='nmap',
                    filename=filename,
                    addnoports=form.vars.f_addnoports,
                    asset_group=form.vars.f_asset_group,
                    engineer=form.vars.f_engineer,
                    msf_settings=msf_settings,
                    ip_ignore_list=ip_exclude,
                    ip_include_list=ip_include,
                    update_hosts=form.vars.f_update_hosts,
                ),
                group_name=settings.scheduler_group_name,
                sync_output=5,
                timeout=settings.scheduler_timeout
            )
            if task.id:
                redirect(URL('tasks', 'status', args=task.id))
            else:
                response.flash = "Error submitting job: %s" % (task.errors)
        else:
            from skaldship.nmap import process_xml
            print("Starting Nmap XML Import")
            process_xml(
                filename=filename,
                addnoports=form.vars.f_addnoports,
                asset_group=form.vars.f_asset_group,
                engineer=form.vars.f_engineer,
                msf_settings=msf_settings,
                ip_ignore_list=ip_exclude,
                ip_include_list=ip_include,
                update_hosts=form.vars.f_update_hosts,
            )
            response.flash = "Nmap XML upload complete"
            redirect(URL('default', 'index'))

    return dict(form=form)

##-------------------------------------------------------------------------


@auth.requires_login()
def nmap_scan():
    """
    Run nmap scan and hand file over to parser
    """
    from skaldship.general import check_datadir
    import time
    import os

    response.title = "%s :: Run Nmap Scan" % (settings.title)

    scan_profiles = {
        'Ping Scan': ["-sn"],
        'Intense Scan': ["-T4", "-A", "-v"],
        'Intense Scan (All TCP Ports)': ["-p", "1-65535", "-T4", "-A", "-v"],
        'Intense Scan (No Ping)': ["-T4", "-A", "-v", "-Pn"],
        'Quick Scan': ["-T4", "-F"],
        'Quick Scan Plus': ["-sV", "-T4", "-O", "-F", "--version-light"],
        'Slow Comprehensive Scan': ["-sS", "-sU", "-T4", "-A", "-v", "-PE", "-PP", "-PS80,443", "-PA3389", "-PU40125", "-PY", "-g 53", "--script", "default"]
    }

    fields = []
    # buld the dropdown user list
    users = db(db.auth_user).select()
    userlist = []
    for user in users:
        userlist.append( [ user.id, user.username ] )

    fields.append(Field('f_engineer', type='integer', label=T('Engineer'), default=auth.user.id, requires=IS_IN_SET(userlist)))
    fields.append(Field('f_asset_group', type='string', label=T('Asset Group'), requires=IS_NOT_EMPTY()))
    fields.append(Field('f_scan_profile', label=T('Scan Profile'),
                        requires=IS_EMPTY_OR(IS_IN_SET(sorted(scan_profiles.keys()), zero=None))))
    fields.append(Field('f_scan_options', type='string', label=T('Scan Options')))
    fields.append(Field('f_target_list', type='text', label=T('Scan Targets')))
    fields.append(Field('f_blacklist', type='text', label=T('Blacklist')))
    fields.append(Field('f_addnoports', type='boolean', label=T('Add Hosts w/o Ports'), default=False))
    fields.append(Field('f_update_hosts', type='boolean', label=T('Update Host Information'), default=False))

    form = SQLFORM.factory(*fields, table_name='nmap_scan')

    if form.errors:
        response.flash = 'Error in form'
    elif form.accepts(request.vars, session):
        # process a nmap scan
        # build the hosts only/exclude list
        ip_blacklist = []
        data = form.vars.get('f_blacklist')
        if data:
            ip_blacklist = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals
        ip_targets = []
        data = form.vars.get('f_target_list')
        if data:
            ip_targets = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals

        if form.vars.f_scan_options:
            scan_options = form.vars.f_scan_options.split(' ')
        else:
            scan_options = scan_profiles[form.vars.f_scan_profile]

        check_datadir(request.folder)
        filename = "nmap-%s-%s.xml" % (form.vars.f_asset_group, int(time.time()))
        filedir = os.path.join(request.folder, 'data', 'scanfiles', filename)
        scan_options.extend(['--stats-every', '5s', '-oX', filedir])

        task = scheduler.queue_task(
            run_scanner,
            pvars=dict(
                scanner='nmap',
                asset_group=form.vars.f_asset_group,
                engineer=form.vars.f_engineer,
                target_list=ip_targets,
                blacklist=ip_blacklist,
                scan_options=scan_options,
                addnoports=form.vars.f_addnoports,
                update_hosts=form.vars.f_update_hosts,
            ),
            group_name=settings.scheduler_group_name,
            sync_output=5,
            timeout=settings.scheduler_timeout
        )

        if task.id:
            redirect(URL('tasks', 'status', args=task.id))
        else:
            response.flash = "Error submitting job: %s" % (task.errors)

    return dict(form=form)

########NEW FILE########
__FILENAME__ = notes
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Evidence controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import get_host_record, host_title_maker, host_a_maker
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return dict()

##-------------------------------------------------------------------------
## notes
##-------------------------------------------------------------------------

@auth.requires_login()
def add():
    if request.vars.has_key('id'):
        host_id = db.t_hosts[request.vars.id] or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    elif request.vars.has_key('ipv4'):
        host_id = db(db.t_hosts.f_ipv4 == request.vars.ipv4) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    elif request.vars.has_key('ipv6'):
        host_id = db(db.t_hosts.f_ipv6 == request.vars.ipv6) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    else:
        host_id = None

    if host_id:
        db.t_host_notes.f_hosts_id.default = host_id.id
        form=crud.create(db.t_host_notes, next=URL('edit', vars={'id': host_id.id}),
                         message="Note added")
        db.t_host_notes.f_hosts_id.default = None

    else:
        form=crud.create(db.t_host_notes,next='edit/[id]', message="Note added")

    response.title = "%s :: Create Host Note" % (settings.title)
    return dict(form=form)

#@auth.requires_signature()
@auth.requires_login()
def add_ajax():
    record = None
    if request.vars.has_key('f_hosts_id'):
        record = get_host_record(request.vars.f_hosts_id)
    if record:
        db.t_host_notes.f_hosts_id.default = record.id

    form=SQLFORM(db.t_host_notes, buttons=[], _action=URL('add_ajax', extension='json'), _id="notes_add_form")
    if form.accepts(request.vars, formname='t_host_notes_create'):
        response.flash = 'Note added'
        response.headers['web2py-component-command'] = 'notesumstable.fnReloadAjax(); notestable.fnReloadAjax();'
        return
    elif form.errors:
        response.flash = "Error in form submission"
        return TABLE(*[TR(k, v) for k, v in form.errors.items()])

    db.t_host_notes.f_hosts_id.default = None
    return dict(form=form)

@auth.requires_login()
def read():
    record = db.t_host_notes(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Note record not found')}))
    response.title = "%s :: Host Notes " % (settings.title)
    form=crud.read(db.t_host_notes,record)
    return dict(form=form)

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    if request.vars.has_key('note_ids'):
        for z in request.vars.note_ids.split('|'):
            if z is not '':
                db(db.t_host_notes.id == z).delete()
                db.commit()
                count += 1
    response.flash = "%s Note(s) deleted" % (count)
    # change the javascript reload command based upon the source page
    if 'notes/list' in request.env.http_web2py_component_location:
        response.headers['web2py-component-command'] = 'notestable.fnReloadAjax();'
    elif 'hosts/detail' in request.env.http_web2py_component_location:
        response.headers['web2py-component-command'] = 'notesumstable.fnReloadAjax();'
    return

@auth.requires_login()
def edit():
    record = db.t_host_notes(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Note record not found')}))
    response.title = "%s :: Update Host Note :: %s" % (settings.title, host_title_maker(db.t_hosts[record.f_hosts_id]))
    form=crud.update(db.t_host_notes,record,next='edit/[id]',
                     ondelete=lambda form: redirect(URL('list')))
    return dict(form=form)

@auth.requires_login()
def list():
    """
    Returns a list of notes records based upon an host identifier
    (id, ipv4, ipv6)
    """

    aaData = []
    response.title = "%s :: Notes" % (settings.title)
    if request.args(0) is not None:
        record = get_host_record(request.args(0))
        if record:
            response.title = "%s :: Notes for %s" % (settings.title, host_title_maker(record))
    else:
        record = None

    if request.extension == "json":
        if record:
            rows = db(db.t_host_notes.f_hosts_id == record.id).select()
        else:
            rows = db(db.t_host_notes.id > 0).select()

        for r in rows:
            aTxt = {}
            aaData.append({
                '0': A('edit', _target="host_notes_%s" % (r.id), _href=URL('edit', args=r.id)).xml(),
                '1': host_a_maker(r.f_hosts_id).xml(),
                '2': r.f_note,
                'DT_RowId': r.id
            })

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }

        return result

    if record:
        add_fields = ['f_note']
    else:
        add_fields = ['f_hosts_id', 'f_note']

    add_note = AddModal(
        db.t_host_notes, 'Add', 'Add Note', 'Add Note',
        #fields=add_fields,
        cmd='notestable.fnReloadAjax();',
        flash="Note added"
    )
    if record:
        db.t_host_notes.f_hosts_id.default = record.id
    db.t_host_notes.id.comment = add_note.create()

    notes = TABLE(THEAD(TR(TH(T('ID'), _width="5%"),
                           TH(T('Host'), _width="20%"),
                           TH(T('Note'), _width="95%"),
                           )  ),
                  _class="datatable",
                  _id="notestable",
                  _style="width:100%")

    return dict(notes=notes, host=record, add_note=add_note)

@auth.requires_login()
def summary_by_host():
    """
    Returns a list of notes records based upon an host identifier
    (id, ipv4, ipv6)
    """
    if request.args(0) is None: redirect(URL('default', 'error', vars={'msg': T('No host record provided')}))

    record = get_host_record(request.args(0))

    if record is None:
        redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))

    response.title = "%s :: Notes for host %s" % (settings.title, host_title_maker(record))
    rows = db(db.t_host_notes.f_hosts_id == record.id)(db.t_host_notes).select(db.t_host_notes.id, db.t_host_notes.f_note)

    aaData = []
    if request.extension == "json":
        for r in rows:
            # datatables json requires aaData to be specificly formatted
            atxt = []
            atxt.append('<a href="javascript:void()" onclick="delnotes_summ(' + str(r.id)  +')">X</a>')
            atxt.append(r.f_note)
            # add columns after this, don't do anything prior since it'll affect the hidden fields

            aaData.append(atxt)

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }
        return result

    notes = TABLE(THEAD(TR(TH(T('[X]'), _width="5%"),
                           TH(T('Note'), _width="90%"),
                           ), _style="display:none" ),
                  _class="table table-condensed", _id="notestable_summary", _style="width:100%")

    return dict(notes=notes)

########NEW FILE########
__FILENAME__ = os
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## OS controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import get_host_record, host_title_maker, host_a_maker
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return redirect(URL('list'))

##-------------------------------------------------------------------------
## t_os - this is the local engagement copy of CPE OS
##-------------------------------------------------------------------------

@auth.requires_login()
def add():
    response.title = "%s :: Add OS to Engagement" % (settings.title)
    form=crud.create(db.t_os,next='os_edit/[id]')
    return dict(form=form)

@auth.requires_login()
def read():
    record = db.t_os(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('OS record not found')}))
    response.title = "%s :: Engagement OS Entry Detail :: %s" % (settings.title, record.f_title)
    form=crud.read(db.t_os,record)
    return dict(form=form)

@auth.requires_login()
def edit():
    record = db.t_os(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('OS record not found')}))
    response.title = "%s :: Update Engagement OS :: %s" % (settings.title, record.f_title)
    form=crud.update(db.t_os,record,next='edit/[id]',
                     ondelete=lambda form: redirect(URL('list')))
    return dict(form=form)

@auth.requires_login()
def list():
    response.title = "%s :: Engagement OS Table" % (settings.title)
    if request.extension == 'json':
        query = db.t_os
        rows=db(query)(db.t_os).select()

        aaData = []
        # datatable formatting is specific
        for r in rows:
            atxt = {}
            atxt['0'] = A('edit', _target="os_update_%s" % (r.id), _href="edit/%s" % (r.id)).xml()
            atxt['1'] = r.f_cpename
            atxt['2'] = r.f_title
            atxt['3'] = r.f_vendor
            atxt['4'] = r.f_product
            atxt['5'] = r.f_version
            atxt['6'] = r.f_update
            atxt['7'] = r.f_edition
            atxt['8'] = r.f_language
            atxt['DT_RowId'] = r.id

            aaData.append(atxt)

        totalrecords = db(db.t_os).count()

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': totalrecords,
                   'aaData': aaData,
                   }

        return result
    else:
        add = AddModal(
            db.t_os, 'Add Manual', 'Add Manual', 'Add Manual OS',
            #fields=[],
            cmd='ostable.fnReloadAjax();'
        )
        db.t_os.id.comment = add.create()

        return dict(add=add)

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    for r in request.vars.ids.split('|'):
        if r is not None:
            db(db.t_os.id == r).delete()
            count += 1
    db.commit()
    response.flash = "%s OS record(s) deleted" % (count)
    response.headers['web2py-component-command'] = "ostable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    #response.js = "hosttable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"

    return dict()

##-------------------------------------------------------------------------
## references
##-------------------------------------------------------------------------

@auth.requires_login()
def refs_add():
    if request.vars.has_key('id'):
        host_id = db.t_hosts[request.vars.id] or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    elif request.vars.has_key('ipv4'):
        host_id = db(db.t_hosts.f_ipv4 == request.vars.ipv4) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    elif request.vars.has_key('ipv6'):
        host_id = db(db.t_hosts.f_ipv6 == request.vars.ipv6) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    else:
        host_id = None

    if host_id:
        db.t_host_os_refs.f_hosts_id.default = host_id.id
        db.t_host_os_refs.f_certainty.default = "1.0"
        form=crud.create(db.t_host_os_refs, next=URL('os_refs_create', vars={'id': host_id.id}),
                         message="OS added")
        db.t_host_os_refs.f_hosts_id.default = None
        db.t_host_os_refs.f_certainty.default = None
    else:
        form=crud.create(db.t_host_os_refs,next='os_refs_edit/[id]', message="OS added")

    response.title = "%s :: Connect OS to a Host" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def refs_edit():
    record = db.t_host_os_refs(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('OS Reference record not found')}))
    form=crud.update(db.t_host_os_refs,record,next='refs_edit/[id]',
                     ondelete=lambda form: redirect(URL('refs_list')))
    response.title = "%s :: OS for Host %s" % (settings.title, host_title_maker(db.t_hosts(record.f_hosts_id)))
    return dict(form=form)

@auth.requires_login()
def refs_list():
    #class VirtFields(object):
    #    def hostinfo(self):
    #        return host_title_maker(self.t_host_os_refs.f_hosts_id)
    #    def osinfo(self):
    #        return "%s :: %s" % (self.t_host_os_refs.f_os_id.f_cpename, self.t_host_os_refs.f_os_id.f_title)
    #db.t_host_os_refs.virtualfields.append(VirtFields())
    if request.extension == "json":
        rows=db(db.t_host_os_refs.f_os_id == db.t_os.id).select()
        aaData = []
        for row in rows:
            atxt = {}
            atxt['0'] =  A("edit", _target="os_refs_update_%s" % (row.t_host_os_refs.id), _href=URL('refs_edit',extension='html',args=row.t_host_os_refs.id)).xml()
            atxt['1'] = row.t_host_os_refs.f_certainty
            atxt['2'] = row.t_host_os_refs.f_class
            atxt['3'] = row.t_host_os_refs.f_family
            atxt['4'] = host_a_maker(row.t_host_os_refs.f_hosts_id).xml()
            atxt['5'] = "%s :: %s" % (row.t_os.f_cpename, row.t_os.f_title)
            atxt['DT_RowId'] = row.t_host_os_refs.id
            aaData.append(atxt)

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }

        return result

    response.title = "%s :: OS/Host References" % (settings.title)
    form = TABLE(THEAD(TR(TH(T(''), _width="5%"),
                          TH(T('Certainty')),
                          TH(T('Class')),
                          TH(T('Family')),
                          TH(T('Host')),
                          TH(T('OS')),
                          )  ),
                 TFOOT(TR(TH(), TH(), TH(), TH(), TH(), TH())),
                 _class="datatable",
                 _id="ostable",
                 _style="width:100%")

    add_ref = AddModal(
        db.t_host_os_refs, 'Link OS', 'Link OS', 'Link OS Reference',
        #fields=[],
        cmd='ostable.fnReloadAjax();'
    )
    db.t_host_os_refs.id.comment = add_ref.create()

    add_os = AddModal(
        db.t_os, 'Add Non-CPE to OS DB', 'Add Non-CPE to OS DB', 'Add Non-CPE to OS DB',
        #fields=[],
        )
    db.t_os.id.comment = add_os.create()

    return dict(form=form, add_ref=add_ref, add_os=add_os)

@auth.requires_signature()
@auth.requires_login()
def refs_delete():
    count = 0
    for r in request.vars.ids.split('|'):
        if db(db.t_host_os_refs.id == r).delete():
            count += 1
    db.commit()
    response.flash = "%s OS Record(s) deleted" % (count)
    response.headers['web2py-component-command'] = "ostable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return

@auth.requires_login()
def refs_by_host():
    """
    Returns a list of OS records based upon an host identifier
    (id, ipv4, ipv6)
    """
    if request.args(0) is None: redirect(URL('default', 'error', vars={'msg': T('No host record sent')}))

    record = get_host_record(request.args(0))

    if record is None:
        redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))

    response.title = "%s :: OS Records for %s" % (settings.title, host_title_maker(record))
    oslist = db(db.t_host_os_refs.f_hosts_id==record.id).select()

    aaData = []
    if request.extension == "json":
        for osdetail in oslist:
            osinfo = db.t_os(osdetail['f_os_id'])
            # datatables json requires aaData to be specificly formatted
            atxt = {}
            atxt['0'] = A('edit', _target="oswindow_%s" % (osdetail.id), _href=URL('refs_edit', args=[osdetail.id], extension='html')).xml()
            atxt['1'] = osdetail.f_family
            atxt['2'] = osdetail.f_class
            atxt['3'] = osdetail.f_certainty
            atxt['4'] = osinfo.f_cpename
            atxt['5'] = osinfo.f_title
            atxt['DT_RowId'] = osdetail.id

            aaData.append(atxt)

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }

        return result

    form = TABLE(THEAD(TR(TH(T(''), _width="5%"),
                          TH(T('Family')),
                          TH(T('Class')),
                          TH(T('Certainty')),
                          TH(T('CPE Name')),
                          TH(T('Title')),
                          )  ),
                 _class="datatable",
                 _id="ostable",
                 _style="width:100%")

    add_os_refs = AddModal(
        db.t_host_os_refs, 'Add', 'Add', 'Add OS',
        fields=['f_certainty', 'f_class', 'f_family', 'f_os_id'],
        cmd='ostable.fnReloadAjax();'
    )
    db.t_host_os_refs.f_hosts_id.default = record.id
    db.t_host_os_refs.id.comment = add_os_refs.create()

    add_non_cpe = AddModal(
        db.t_os, 'Add Non-CPE OS', 'Add Non-CPE OS', 'Add Non-CPE OS',
        #fields=[],
        #cmd='ostable.fnReloadAjax();'
    )
    db.t_os.id.comment = add_non_cpe.create()

    return dict(form=form, host=record, add_os_refs=add_os_refs, add_non_cpe=add_non_cpe)

@auth.requires_login()
def mass_assign():
    """
    Upload a CSV file that mass-assigns OS records to Hosts. If a CPE record is provided, look it up in the DB.
    If not lookup the vendor and product in the DB

    File format:

     ipaddress,cpe,family,vendor,product,certainty,osclass

    """
    response.title = "%s :: Mass OS Update" % (settings.title)
    form = SQLFORM.factory(
        Field('osfile', 'upload', uploadfolder=os.path.join(request.folder, 'data', 'misc'), label=T('OS CSV File')),
    )

    if form.accepts(request.vars,session):
        filename = os.path.join(request.folder,'data/misc',form.vars.osfile)
        import csv
        from skaldship.cpe import lookup_cpe
        #from skaldship.general import
        counter = 0
        with open(filename, "rb") as f:
            for row in csv.reader(f):
                host_id = get_host_record(row[0])
                if not host_id:
                    print "[%s] - Record not found" % (row[0])
                    continue

                cpe = row[1]
                family = row[2]
                vendor = row[3]
                product = row[4]
                certainty = row[5]
                osclass = row[6]
                os_id = None
                if cpe:
                    # we have a cpe entry from xml! hooray!
                    cpe_name = cpe.replace('cpe:/o:', '')
                    os_id = lookup_cpe(cpe_name)
                #else:
                    # no cpe attribute in xml, go through our messsy lookup
                #    os_id = guess_cpe_os(os_rec)

                if os_id:
                    db.t_host_os_refs.insert(f_certainty=certainty,
                                             f_family=family,
                                             f_class=osclass,
                                             f_hosts_id=host_id,
                                             f_os_id=os_id)
                    db.commit()
                    counter += 1
                else:
                    logger.error("OS not found: %s" % (row))
        response.flash = "%s Hosts updated with new OS records" % (counter)

    elif form.errors:
        response.flash = 'Error in form'

    return dict(form=form)

########NEW FILE########
__FILENAME__ = report
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Reporting controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.general import cvss_metrics
from skaldship.hosts import create_hostfilter_query
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return dict()

@auth.requires_login()
def spreadsheet():
    """
    Generate a Excel xlsx file of vulnerability and password statistics and charts
    """

    rows = db(db.t_hosts).select(db.t_hosts.f_asset_group, distinct=True)
    ags = [ag.f_asset_group for ag in rows]
    form=SQLFORM.factory(
        Field('ag_per_tab', 'boolean', default=False, label=T('Tab per Asset Group')),
        Field('asset_group', 'select', default=False, requires=IS_EMPTY_OR(IS_IN_SET(ags)), label=T('Specific Asset Group')),
    )
    response.title = "%s :: Excel Spreadsheet Generator" % (settings.title)
    if form.errors:
        response.flash = 'Error in form'
    elif form.process().accepted:
        if form.vars.asset_group:
            ags = [form.vars.asset_group]
        elif form.vars.ag_per_tab:
            rows = db(db.t_hosts).select(db.t_hosts.f_asset_group, distinct=True)
            ags = [ag.f_asset_group for ag in rows]
        else:
            ags = ['%']

        from skaldship.statistics import vulnlist, graphs_index
        from skaldship.general import vulntype_mapping
        import os
        from datetime import datetime
        from xlsxwriter.workbook import Workbook

        tmpfile = os.path.join(request.folder, 'data/stats/kvasir-stats-%s.xlsx' % datetime.now().strftime("%m%d%y-%H%M%S"))
        workbook = Workbook(tmpfile)
        bold = workbook.add_format({'bold': 1})

        # Create main statistics page / charts
        graphs = graphs_index()
        stat_worksheet = workbook.add_worksheet('Main Statistics')

        # Top Host Severity statistics / chart
        stat_worksheet.write('A1', 'Vuln Severity', bold)
        stat_worksheet.write('B1', 'Host Count', bold)
        row_num = 1
        col_num = 0
        for sev_cnt in graphs['top_host_sev_count_raw']:
            stat_worksheet.write_number(row_num, col_num, row_num)
            stat_worksheet.write_number(row_num, col_num+1, int(sev_cnt))
            row_num += 1

        stat_chart_host = workbook.add_chart({'type': 'column'})
        stat_chart_host.add_series({
            'categories': ["'Main Statistics'", 1, 0, row_num-1, 0],
            'values': ["'Main Statistics'", 1, 1, row_num-1, 1],
            'name': 'Host Count',
        })
        stat_chart_host.set_title({'name': 'Top Host Severities'})
        stat_chart_host.set_table({'show_keys': True})
        stat_chart_host.set_legend({'position': 'none'})
        stat_chart_host.set_x_axis({
            'min': 1,
            'max': 10,
            'name_font': {'bold': True},
        })
        stat_chart_host.set_size({'width': 768, 'height': 576})
        stat_worksheet.insert_chart('A13', stat_chart_host)

        # Vulnerability Severity statistics / chart
        stat_worksheet.write('D1', 'Vuln Severity', bold)
        stat_worksheet.write('E1', 'Vuln Count', bold)
        row_num = 1
        col_num = 3
        for sev_cnt in graphs['vuln_by_sev_count_raw']:
            stat_worksheet.write_number(row_num, col_num, row_num)
            stat_worksheet.write_number(row_num, col_num+1, int(sev_cnt))
            row_num += 1

        stat_chart_vulns = workbook.add_chart({'type': 'column'})
        stat_chart_vulns.add_series({
            'categories': ["'Main Statistics'", 1, 3, row_num-1, 3],
            'values': ["'Main Statistics'", 1, 4, row_num-1, 4],
            'name': 'Vulnerability Count',
        })
        stat_chart_vulns.set_title({'name': 'Top Vulnerability Severities'})
        stat_chart_vulns.set_table({'show_keys': True})
        stat_chart_vulns.set_legend({'position': 'none'})
        stat_chart_vulns.set_x_axis({
            'min': 1,
            'max': 10,
            'name_font': {'bold': True},
        })
        stat_chart_vulns.set_size({'width': 768, 'height': 576})
        stat_worksheet.insert_chart('G13', stat_chart_vulns)

        # Create tab(s) for vulnerability listings and charts
        for ag in ags:
            if ag == "%":
                ag = "Vulnlist"
                hostfilter = [(None, None), False]
            else:
                hostfilter = [('assetgroup', ag), False]

            vl_worksheet = workbook.add_worksheet(ag)
            vl_worksheet.write('A1', 'Vulnerability ID', bold)
            vl_worksheet.set_column(1, 0, 45)
            vl_worksheet.write('B1', 'Status', bold)
            vl_worksheet.set_column(1, 1, 20)
            vl_worksheet.write('C1', 'Count', bold)
            vl_worksheet.write('D1', 'Severity', bold)
            vl_worksheet.write('E1', 'CVSS Score', bold)

            # { 'vulnerability id': [ status, count, severity, cvss ] }
            vlist = vulnlist(hostfilter)
            vuln_count = 1
            vl_stats = {}
            for k, v in vlist.iteritems():
                col_num = 0
                for row in v:
                    (status, count, severity, cvss) = row
                    vl_worksheet.write_string(vuln_count, col_num, k)
                    vl_worksheet.write_string(vuln_count, col_num + 1, status)
                    vl_worksheet.write_number(vuln_count, col_num + 2, int(count))
                    vl_worksheet.write_number(vuln_count, col_num + 3, int(severity))
                    if cvss:
                        vl_worksheet.write_number(vuln_count, col_num + 4, float(cvss))
                    vuln_count += 1

                    # make vl_stats dictionary:
                    # { 'status': { 1: count, 2:count ... }}
                    vl_tmpstatus = vl_stats.setdefault(status, {
                        1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0
                    })
                    status_tmp = vl_tmpstatus.setdefault(severity, 0)
                    vl_tmpstatus[severity] = status_tmp + count
                    vl_stats[status] = vl_tmpstatus

            # create vulnerability severity distribution chart
            vl_chart_ws_name = "%s VulnChart" % (ag)
            vl_chart_worksheet = workbook.add_worksheet(vl_chart_ws_name)
            vl_chart_worksheet.write('A1', 'Severity', bold)
            vl_chart = workbook.add_chart({'type': 'column'})
            for k,v in vl_stats.iteritems():
                vl_chart_worksheet.write(0, col_num, k, bold)
                row_num = 1
                for k2,v2 in v.iteritems():
                    vl_chart_worksheet.write(row_num, 0, k2)
                    vl_chart_worksheet.write(row_num, 1, v2)
                    row_num += 1

                vl_chart.add_series({
                    'categories': ["'%s'" % (vl_chart_ws_name), 1, 0, row_num-1, 0],
                    'values': ["'%s'" % (vl_chart_ws_name), 1, col_num, row_num-1, col_num],
                    'name': k,
                    'color': vulntype_mapping(k)
                })
                col_num += 1

            # if multiple account groups, change title accordingly
            if ag == "Vulnlist":
                vl_chart.set_title({'name': 'Vulnerability Severity Distribution'})
            else:
                vl_chart.set_title({'name': 'Vulnerability Severity Distribution: %s' % (ag)})

            vl_chart.set_table({'show_keys': True})
            vl_chart.set_legend({'position': 'none'})
            vl_chart.set_x_axis({
                'min': 1,
                'max': 10,
                'name_font': {'bold': True},
            })
            vl_chart.set_size({'width': 960, 'height': 576})

            vl_chart_worksheet.insert_chart('A13', vl_chart)

        # Top 15 passwords and UNIX / Windows distribution and compromised pie charts
        password_worksheet = workbook.add_worksheet('Passwords')
        pw_cnt = db.t_accounts.f_password.count()
        top15 = db(db.t_accounts.f_password != None).select(db.t_accounts.f_password, pw_cnt, groupby=db.t_accounts.f_password, orderby=~pw_cnt, limitby=(0,15))
        password_worksheet.write('A1', 'Password', bold)
        password_worksheet.write('B1', 'Count', bold)
        row_num = 1
        for row in top15:
            password_worksheet.write(row_num, 0, row.t_accounts.f_password)
            password_worksheet.write(row_num, 1, row._extra['COUNT(t_accounts.f_password)'])
            row_num += 1

        # all done!
        workbook.close()
        redirect(URL('default', 'data_dir/stats'))

    return dict(form=form)

@auth.requires_login()
def customer_xml():
    """
    Generates an XML file suitable for Customer usage
    """

    from lxml import etree

    # grab the filter type and value if provided or from the session
    if session.hostfilter is None:
        f_type  = request.vars.f_type or None
        f_value = request.vars.f_value or None
    else:
        f_type  = session.hostfilter[0]
        f_value = session.hostfilter[1]

    location_attribute = '{%s}noNameSpaceSchemaLocation' % "http://www.w3.org/2001/XMLSchema-instance"
    kvasir_results_xml = etree.Element('KvasirResults', attrib={ location_attribute: 'kvasir.xsd', })

    summary_xml = etree.SubElement(kvasir_results_xml, 'summary')
    customer = etree.SubElement(summary_xml, 'customer')
    customer.text = settings.customer or 'CUSTOMER NAME'
    assessment = etree.SubElement(summary_xml, 'assessment')
    assessment.set('type', settings.assessment_type)
    start_date = etree.SubElement(assessment, 'start-date')
    start_date.text = settings.start_date or 'START DATE'
    end_date = etree.SubElement(assessment, 'end-date')
    end_date.text = settings.end_date or 'END DATE'

    hosts_xml = etree.SubElement(kvasir_results_xml, 'hosts')
    os_xml = etree.SubElement(kvasir_results_xml, 'os_records')
    vulns_xml = etree.SubElement(kvasir_results_xml, 'vulns')

    # this is a little hack to ensure a record is either blank or None
    # use it as "if variable not in notin:"
    notin = [ None, '' ]
    unknown_cpeid_counter = 0

    # go through each host, adding the os, services and vulns accordingly
    query = create_hostfilter_query([(f_type, f_value), False])
    for host_rec in db(query).select():
        host_xml = etree.SubElement(hosts_xml, 'host')
        host_xml.set('ipv4', host_rec.f_ipv4)
        host_xml.set('assetgroup', host_rec.f_asset_group)
        if host_rec.f_ipv6:
            host_xml.set('ipv6', host_rec.f_ipv6)
        if host_rec.f_macaddr:
            host_xml.set('macaddr', host_rec.f_macaddr)
        if host_rec.f_hostname:
            host_xml.set('hostname', host_rec.f_hostname.decode('utf-8'))
        if host_rec.f_netbios_name:
            host_xml.set('netbios', host_rec.f_netbios_name.decode('utf-8'))

        # build the os information using the highest certainty record
        highest = (0, None)
        for os_rec in db(db.t_host_os_refs.f_hosts_id == host_rec.id).select():
            if os_rec.f_certainty > highest[0]:
                highest = (os_rec.f_certainty, os_rec)

        if highest[0] > 0:
            # add os element to the host
            record = highest[1]
            os = etree.SubElement(host_xml, 'os')
            os.set('certainty', str(highest[0]))
            if record.f_class not in notin:
                os.set('class', record.f_class)
            if record.f_family not in notin:
                os.set('family', record.f_family)

            # since some os records may not have a cpe id we'll mask them with
            # using their title, replacing spaces with underscores
            t_os_rec = db.t_os[record.f_os_id]
            if t_os_rec.f_cpename in notin:
                cpeid = t_os_rec.f_title.replace(' ', '_')
            else:
                cpeid = t_os_rec.f_cpename

            os.set('id', cpeid)

            # if the id isn't in os_records, add it
            if len(os_xml.findall('.//os[@id="%s"]' % (os.get('id', None)))) < 1:
                os_info_xml = etree.SubElement(os_xml, 'os')
                os_rec = db.t_os[highest[1].f_os_id]
                os_info_xml.set('id', cpeid)
                os_info_xml.set('title', os_rec.f_title)

                if os_rec.f_vendor not in notin:
                    vendor = etree.SubElement(os_info_xml, 'vendor')
                    vendor.text = os_rec.f_vendor

                if os_rec.f_product not in notin:
                    product = etree.SubElement(os_info_xml, 'product')
                    product.text = os_rec.f_product

                if os_rec.f_version not in notin:
                    version = etree.SubElement(os_info_xml, 'version')
                    version.text = os_rec.f_version

                if os_rec.f_update not in notin:
                    update = etree.SubElement(os_info_xml, 'update')
                    update.text = os_rec.f_update

                if os_rec.f_edition not in notin:
                    edition = etree.SubElement(os_info_xml, 'edition')
                    edition.text = os_rec.f_edition

                if os_rec.f_language not in notin:
                    language = etree.SubElement(os_info_xml, 'language')
                    language.text = os_rec.f_language

        # snmp strings
        snmp_recs = db(db.t_snmp.f_hosts_id == host_rec.id).select()
        if len(snmp_recs) > 0:
            snmp_top_xml = etree.SubElement(hosts_xml, 'snmps')
            for record in snmp_recs:
                snmp_xml = etree.SubElement(snmp_top_xml, 'snmp')
                if record.f_community not in notin:
                    snmp_xml.set('community', record.f_community.decode('utf-8'))
                    snmp_xml.set('version', record.f_version)
                    snmp_xml.set('access', record.f_access)

        # netbios information
        netb_record = db(db.t_netbios.f_hosts_id == host_rec.id).select().first() or None
        if netb_record:
            netbios_xml = etree.SubElement(hosts_xml, 'netbios')
            if netb_record.f_type not in notin:
                netbios_xml.set('type', netb_record.f_type)
            if netb_record.f_domain not in notin:
                netbios_xml.set('domain', netb_record.f_domain.decode('utf-8'))
            if netb_record.f_lockout_limit not in notin:
                netbios_xml.set('lockout_limit', str(netb_record.f_lockout_limit))
            if netb_record.f_lockout_duration not in notin:
                netbios_xml.set('lockout_duration', str(netb_record.f_lockout_duration))

            if netb_record.f_advertised_names is not None:
                adv_names_xml = etree.SubElement(netbios_xml, 'advertised_names')
                for name in netb_record.f_advertised_names:
                    name_xml = etree.SubElement(adv_names_xml, 'name')
                    name.text = name.decode('utf-8')

        # build the services and vulnerabilities
        services_xml = etree.SubElement(host_xml, 'services')
        for svc_rec in db(db.t_services.f_hosts_id == host_rec.id).select():
            service_xml = etree.SubElement(services_xml, 'service')
            service_xml.set('proto', svc_rec.f_proto)
            service_xml.set('number', svc_rec.f_number)

            if svc_rec.f_name not in notin:
                name = etree.SubElement(service_xml, 'name')
                name.text = svc_rec.f_name.decode('utf-8')

            if svc_rec.f_banner not in notin:
                banner = etree.SubElement(service_xml, 'banner')
                banner.text = svc_rec.f_banner.decode('utf-8')

            # service configuration records
            svc_info_recs = db(db.t_service_info.f_services_id == svc_rec.id).select()
            if len(svc_info_recs) > 0:
                config_xml = etree.SubElement(service_xml, 'configuration')
                for info_rec in svc_info_recs:
                    rec_xml = etree.SubElement(config_xml, 'config')
                    if info_rec.f_name not in notin:
                        rec_xml.set('name', info_rec.f_name)
                        if info_rec.f_text not in notin:
                            rec_xml.text = info_rec.f_text.decode('utf-8')

            # vulnerabilities
            svc_vuln_recs = db(db.t_service_vulns.f_services_id == svc_rec.id).select()
            if len(svc_vuln_recs) > 0:
                svc_vulns_xml = etree.SubElement(service_xml, 'vulns')
                for vuln_rec in svc_vuln_recs:
                    vuln_xml = etree.SubElement(svc_vulns_xml, 'vuln')
                    vuln_xml.set('status', vuln_rec.f_status)
                    vuln_xml.set('id', db.t_vulndata[vuln_rec.f_vulndata_id].f_vulnid)
                    proof = etree.SubElement(vuln_xml, 'proof')
                    proof.text = etree.CDATA(unicode(MARKMIN(vuln_rec.f_proof).xml(), 'utf-8'))

                    # search for the nexpose id in vulns_xml
                    if len(vuln_xml.findall('.//vuln[@id="%s"]' % vuln_xml.get('id', None))) < 1:
                        new_vuln_xml = etree.SubElement(vulns_xml, 'vuln')
                        vulndata = db.t_vulndata[vuln_rec.f_vulndata_id]
                        new_vuln_xml.set('id', vulndata.f_vulnid)
                        new_vuln_xml.set('title', vulndata.f_title)
                        new_vuln_xml.set('severity', str(vulndata.f_severity))
                        new_vuln_xml.set('pci_sev', str(vulndata.f_pci_sev))
                        new_vuln_xml.set('cvss_score', str(vulndata.f_cvss_score))
                        new_vuln_xml.set('cvss_metric', cvss_metrics(vulndata))
                        description = etree.SubElement(new_vuln_xml, 'description')
                        description.text = etree.CDATA(unicode(MARKMIN(vulndata.f_description).xml(), 'utf-8'))
                        solution = etree.SubElement(new_vuln_xml, 'solution')
                        solution.text = etree.CDATA(unicode(MARKMIN(vulndata.f_solution).xml(), 'utf-8'))

                        # find vulnerability references and add them
                        vuln_refs = db(db.t_vuln_references.f_vulndata_id == vulndata.id).select()
                        if len(vuln_refs) > 0:
                            refs_xml = etree.SubElement(new_vuln_xml, 'references')
                            for ref_rec in vuln_refs:
                                record = db.t_vuln_refs[ref_rec.f_vuln_ref_id]
                                ref_xml = etree.SubElement(refs_xml, 'reference')
                                ref_xml.set('source', record.f_source)
                                ref_xml.text = record.f_text.decode('utf-8')

            # accounts
            accounts = db(db.t_accounts.f_services_id == svc_rec.id).select()
            if len(accounts) > 0:
                accounts_xml = etree.SubElement(service_xml, 'accounts')
                for acct_rec in accounts:
                    acct_xml = etree.SubElement(accounts_xml, 'account')

                    if acct_rec.f_username not in notin:
                        elem = etree.SubElement(acct_xml, 'username')
                        elem.text = acct_rec.f_username.decode('utf-8')

                    if acct_rec.f_fullname not in notin:
                        elem = etree.SubElement(acct_xml, 'fullname')
                        elem.text = acct_rec.f_fullname.decode('utf-8')

                    if acct_rec.f_password not in notin:
                        elem = etree.SubElement(acct_xml, 'password')
                        elem.text = acct_rec.f_password.decode('utf-8')

                    if acct_rec.f_hash1 not in notin:
                        elem = etree.SubElement(acct_xml, 'hash1')
                        elem.text = acct_rec.f_hash1

                    if acct_rec.f_hash1_type not in notin:
                        elem = etree.SubElement(acct_xml, 'hash1_type')
                        elem.text = acct_rec.f_hash1_type

                    if acct_rec.f_hash2 not in notin:
                        elem = etree.SubElement(acct_xml, 'hash2')
                        elem.text = acct_rec.f_hash2

                    if acct_rec.f_hash2_type not in notin:
                        elem = etree.SubElement(acct_xml, 'hash2_type')
                        elem.text = acct_rec.f_hash2_type

                    if acct_rec.f_uid not in notin:
                        elem = etree.SubElement(acct_xml, 'uid')
                        elem.text = acct_rec.f_uid

                    if acct_rec.f_gid not in notin:
                        elem = etree.SubElement(acct_xml, 'gid')
                        elem.text = acct_rec.f_gid

                    if acct_rec.f_level not in notin:
                        elem = etree.SubElement(acct_xml, 'level')
                        elem.text = acct_rec.f_level

                    if acct_rec.f_domain not in notin:
                        elem = etree.SubElement(acct_xml, 'domain')
                        elem.text = acct_rec.f_domain.decode('utf-8')

                    if acct_rec.f_description not in notin:
                        elem = etree.SubElement(acct_xml, 'description')
                        elem.text = acct_rec.f_description.decode('utf-8')

    result = etree.tostring(kvasir_results_xml, pretty_print=True, encoding=unicode)
    return result

########NEW FILE########
__FILENAME__ = services
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Services controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import get_host_record, host_title_maker, host_a_maker, create_hostfilter_query
import re
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return dict()

##-------------------------------------------------------------------------
## services
##-------------------------------------------------------------------------

@auth.requires_login()
def add():
    if request.vars.has_key('id'):
        host_id = db.t_hosts[request.vars.id] or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    elif request.vars.has_key('ipv4'):
        host_id = db(db.t_hosts.f_ipv4 == request.vars.ipv4) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    elif request.vars.has_key('ipv6'):
        host_id = db(db.t_hosts.f_ipv6 == request.vars.ipv6) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    else:
        host_id = None

    if host_id:
        db.t_services.f_hosts_id.default = host_id.id
        form=crud.create(db.t_services, next=URL('edit', vars={'id': host_id.id}),
                         message="Service added")
        db.t_services.f_hosts_id.default = None

    else:
        form=crud.create(db.t_services,next='read/[id]', message="Service added")

    response.title = "%s :: Add Service" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def read():
    record = db.t_services(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Service record not found')}))
    response.title = "%s :: Service Details :: %s/%s" % (settings.title, record.f_proto, record.f_number)
    #service=crud.read(db.t_services,record)
    service=crud.update(db.t_services,record,next='read/[id]',
                        ondelete=lambda form: redirect(URL('list')))

    # pagination
    svclist = []
    for svc_rec in db(db.t_services.f_hosts_id==record.f_hosts_id).select():
        if svc_rec.id == record.id:
            svclist.append(OPTION("%s/%s" % (svc_rec.f_proto, svc_rec.f_number), _value=svc_rec.id, _selected=True))
        else:
            svclist.append(OPTION("%s/%s" % (svc_rec.f_proto, svc_rec.f_number), _value=svc_rec.id))

    #pagination = pagination_services(db, request, session, record)
    pagination = None   # pagination_services is too slow and may not be necessary here

    vulntr = []
    accttr = []
    svcinfotr = []
    # vulnerabilities
    q = db(db.t_service_vulns.f_services_id == record.id).select()
    for k in q:
        vulninfo = db.t_vulndata(k.f_vulndata_id)
        if vulninfo:
            if settings.use_cvss:
                severity = vulninfo.f_cvss_score
            else:
                severity = vulninfo.f_severity
            vulntr.append(TR(TD("%s/%s" % (record.f_proto, record.f_number)),
                             TD(A(vulninfo.f_vulnid, _href=URL('vulns', 'vulninfo_by_vulnid', args=vulninfo.f_vulnid), _target="vulndata_%s" % (k.f_vulndata_id), extension='html').xml()),
                             TD(severity),
                             TD(k.f_status),
                             TD(XML(k.f_proof, sanitize=False).xml()),
                             ) )

    # accounts
    q = db(db.t_accounts.f_services_id == record.id).select()
    for k in q:
        accttr.append(TR(TD("%s/%s" % (record.f_proto, record.f_number)),
                         TD(k["f_username"]),
                         TD(k["f_password"]),
                         TD(k["f_source"]),
                         TD(k["f_level"]),
                         TD(k["f_description"]),
                         TD(k["f_services_id"]),
                         ) )

    # service info
    q = db(db.t_service_info.f_services_id == record.id).select()
    for k in q:
        svcinfotr.append(TR(TD("%s/%s" % (record.f_proto, record.f_number)),
                            TD(k["f_name"]),
                            TD(k["f_text"]),
                            ) )

    if len(svcinfotr) > 0:
        svcinfotable = TABLE(THEAD(TR(TH(T('Port')), TH(T('Name')), TH(T('Text')))),
                             TBODY(svcinfotr),
                             _style="width:100%",
                             _class="table table-condensed table-striped")
    else:
        svcinfotable = None

    if len(vulntr) > 0:
        vulns = TABLE(THEAD(TR(TH(T('Port')), TH(T('Vulnerability')), TH(T('Severity')), TH(T('Status')), TH(T('Proof')))),
                      TBODY(vulntr),
                      _style="width:100%",
                      _class="table table-condensed")
    else:
        vulns = None

    if len(accttr) > 0:
        accts = TABLE(THEAD(TR(TH(T('Port')), TH(T('Username')), TH(T('Password')), TH(T('Source')), TH(T('Level')), TH(T('Description')), TH(T('Service')))),
                      TBODY(accttr),
                      _style="width:100%",
                      _class="table table-condensed")
    else:
        accts = None

    # grab the notes
    #notes=db(db.t_host_notes.f_hosts_id == record.id)(db.t_host_notes).select(db.t_host_notes.id, db.t_host_notes.f_note)
    #notes = SQLTABLE( db(db.t_service_notes.f_services_id == record.id)(db.t_service_notes).select(db.t_service_notes.id, db.t_service_notes.f_note),
    #                  headers = 'labels',
    #                  _style="width:100%",
    #                  _class = "datatable",
    #                  )

    response.title = "%s :: Service info :: %s ::: %s/%s" % (settings.title, host_title_maker(db.t_hosts[record.f_hosts_id]), record.f_proto, record.f_number)
    return dict(service=service,
                record=record,
                svcinfotable=svcinfotable,
                vulns=vulns,
                #notes=notes,
                accts=accts,
                pagination=pagination)

@auth.requires_login()
def svc_infos():
    """
    Returns Service Info key/values in a grid based on f_services_id
    """
    svc_id = request.args(0) or redirect(URL('index'))
    db.t_service_info.f_services_id.default == svc_id
    infos = SQLFORM.grid(
        db.t_service_info.f_services_id == svc_id,
        args=[svc_id],
        fields = [ db.t_service_info.f_name, db.t_service_info.f_text ],
        maxtextlength=255,
        searchable=False,
        deletable=True,
        details=False,
        selectable=False,
        csv=False,
    )
    return infos

@auth.requires_login()
def edit():
    record = db.t_services(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Service record not found')}))
    response.title = "%s :: Update Service :: %s/%s" % (settings.title, record.f_proto, record.f_number)
    form=crud.update(db.t_services,record,next='read/[id]',
                     ondelete=lambda form: redirect(URL('list')))
    infos = LOAD(request.controller, 'svc_infos', args=[record.id], ajax=True)
    return dict(form=form, infos=infos)

@auth.requires_login()
def list():
    from skaldship.general import severity_mapping
    response.title = "%s :: Services" % (settings.title)

    # if no filter is set then we blank it out
    if session.hostfilter is None:
        session.hostfilter = [(None, None), False]

    if request.extension == 'json':

        q = (db.t_services.id > 0)
        proto = request.vars.f_proto
        pnum = request.vars.f_number
        if pnum:
            q &= (db.t_services.f_number == pnum)
        if proto:
            q &= (db.t_services.f_protocol == proto)

        q = create_hostfilter_query(session.hostfilter, q, 't_services')

        # Datatables Server-side: http://datatables.net/usage/server-side
        if request.vars.has_key('iDisplayStart'):
            start = int(request.vars.iDisplayStart)
        else:
            start = 0
        if request.vars.has_key('iDisplayLength'):
            if request.vars.iDisplayLength == '-1':
                limit = db(q).count()
            else:
                limit = start + int(request.vars.iDisplayLength)
        else:
            limit = int(auth.user.f_show_size)

        srch_data = request.vars.get('sSearch')
        if srch_data:
            # sSearch global search box

            # parse the search into fields (port:num proto:tcp etc)
            srch_vals = [
                ["port", db.t_services.f_number],
                ["proto", db.t_services.f_proto],
                ["status", db.t_services.f_status],
                ["name", db.t_services.f_name],
                ["banner", db.t_services.f_banner],
                ["ip", db.t_hosts.f_ipv4],
                ["ipv4", db.t_hosts.f_ipv4],
                ["ipv6", db.t_hosts.f_ipv6],
                ["hostname", db.t_hosts.f_hostname],
            ]

            parsed = False
            for val in srch_vals:
                srch_str = "%s:(?P<f>\w+)" % val[0]
                srch_res = re.findall(srch_str, srch_data)
                for res in srch_res:
                    parsed = True
                    if val[0] == 'banner':
                        q &= (val[1].contains(res))
                    else:
                        q &= (val[1].upper() == res.upper())

            if not parsed:
                q &= db.t_services.f_proto.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_services.f_number.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_services.f_name.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_services.f_banner.like("%%%s%%" % request.vars.sSearch) | \
                    db.t_services.f_status.like("%%%s%%" % request.vars.sSearch)

        if request.vars.iSortingCols == '1':
            cols = (
                None,
                None,
                None,
                db.t_services.f_hosts_id,
                db.t_services.f_proto,
                db.t_services.f_number,
                db.t_services.f_status,
                None,
                None,
                None,
                None,
                db.t_services.f_name,
                db.t_services.f_banner,
            )

            orderby = cols[int(request.vars.iSortCol_0)]
            if request.vars.sSortDir_0 == 'asc':
                rows=db(q).select(orderby=orderby, limitby=(start, limit))
            else:
                rows=db(q).select(orderby=~orderby, limitby=(start, limit))
        else:
            rows=db(q).select(limitby=(start, limit))

        nolimit = db(q).count()

        aaData = []

        # datatable formatting is specific
        # gather all the vulndata and exploits into a big row
        # later we'll do a find(lambda: row: row.<db>.<field> == <value>)
        # to slice it into the bits we need. Maybe it'll be faster?
        #vulndata = db().select(db.t_vulndata.f_vulnid, db.t_vulndata.id, db.t_exploit_references.f_exploit_id,
        #                       left=db.t_exploit_references.on(db.t_vulndata.id==db.t_exploit_references.f_vulndata_id))

        for r in rows:
            atxt = {}
            vulncount = 0
            vulns = db(db.t_service_vulns.f_services_id==r.t_services.id).select(db.t_service_vulns.f_vulndata_id, cache=(cache.ram, 60))

            vulnlist = []
            explist=[]
            for vuln in vulns:
                vuln_rec = db.t_vulndata[vuln.f_vulndata_id]
                if vuln_rec.f_vulnid not in vulnlist:
                    if settings.use_cvss:
                        vulnlist.append((vuln_rec.f_vulnid, vuln_rec.f_cvss_score))
                    else:
                        vulnlist.append((vuln_rec.f_vulnid, vuln_rec.f_severity))
                exploits = db(db.t_exploit_references.f_vulndata_id == vuln.f_vulndata_id).select(cache=(cache.ram, 60))
                if len(exploits) > 0:
                    for expinfo in exploits:
                        exp = db.t_exploits[expinfo.f_exploit_id]
                        exp_link = A(exp.f_name, _href=URL('exploits', 'edit', extension='html', args=exp.id), _target='blank')
                        explist.append(TR(TD(exp_link),
                                          TD(exp.f_title),
                                          TD(exp.f_source),
                                          TD(exp.f_rank)
                                          )  )

            q = r.t_services.t_service_info.select(cache=(cache.ram, 60))
            if (len(q) > 0) or (len(explist) > 0) or (len(vulnlist) > 0):
                atxt['0'] = IMG(_src=URL(request.application,'static','images/details_open.png')).xml()
            else:
                atxt['0'] = ""
            atxt['1'] = A("edit", _target="services_edit_%s" % (r.t_services.id), _href=URL('edit', args=[r.t_services.id], extension='html')).xml()
            if len(q) > 0:
                addl = []
                for svcinfo in q:
                    addl.append(TR(TD(svcinfo.f_name), TD(svcinfo.f_text)))
                atxt['2'] = TABLE(THEAD(TR(TH(T('Name')),
                                           TH(T('Text')))),
                                  TBODY(addl),
                                  _class="table table-condensed table-striped",
                                  _style="width:100%").xml()
            else:
                atxt['2'] = ''
            host_rec = db.t_hosts[r.t_services.f_hosts_id]
            atxt['3'] = host_a_maker(host_rec).xml(),
            atxt['4'] = r.t_services.f_proto

            # Append A tags around services with HTTP Ports
            if r.t_services.f_number in HTTP_PORTS and r.t_services.f_proto == "tcp" or r.t_services.f_name == "HTTP":
                atxt['5'] = A(r.t_services.f_number,
                              _href=URL('default', 'redirect', extension='html', vars={'url': "http://%s:%s/" % (host_rec.f_ipv4, r.t_services.f_number)}),
                              _target="%s-tcp-%s" % (host_rec.f_ipv4, r.t_services.f_number)).xml()
            elif r.t_services.f_number in HTTPS_PORTS and r.t_services.f_proto == "tcp" or r.t_services.f_name == "HTTPS":
                atxt['5'] = A(r.t_services.f_number,
                              _href=URL('default', 'redirect', extension='html', vars={'url': "https://%s:%s/" % (host_rec.f_ipv4, r.t_services.f_number)}),
                              _target="%s-tcp-%s" % (host_rec.f_ipv4, r.t_services.f_number)).xml()
            else:
                atxt['5'] = r.t_services.f_number

            atxt['6'] = r.t_services.f_status
            atxt['7'] = len(vulnlist)
            vulntxt = []
            for vuln in vulnlist:
                color = severity_mapping(vuln[1])[2]
                vulntxt.append(A(vuln[0], _id="vuln", _target="vulninfo_by_vulnid_%s" % (vuln[0]), _href=URL('vulns', 'vulninfo_by_vulnid', args=[vuln[0]], extension='html'),
                                 _style="color:"+color).xml())
            atxt['8'] = " :: ".join(vulntxt)
            if len(explist) > 0:
                atxt['9'] = "Yes (%d)" % (len(explist))
            else:
                atxt['9'] = ''
            if len(explist) > 0:
                atxt['10'] = TABLE(THEAD(TR(TH(T('Name')),
                                           TH(T('Title')),
                                           TH(T('Source')),
                                           TH(T('Rank')))),
                                  TBODY(explist),
                                  _class="table table-condensed",
                                  _style="width:100%").xml()
            else:
                atxt['10'] = ''
            atxt['11'] = r.t_services.f_name
            atxt['12'] = r.t_services.f_banner
            atxt['DT_RowId'] = r.t_services.id

            aaData.append(atxt)

        result = {
            'sEcho': request.vars.sEcho,
            'iTotalRecords': db(db.t_services).count(),
            'iTotalDisplayRecords': nolimit,
            'aaData': aaData,
        }

        return result
    else:
        add = AddModal(
            db.t_services, 'Add', 'Add', 'Add Service',
            #fields=[
            #    'f_proto', 'f_number', 'f_status', 'f_name', 'f_banner'
            #],
            cmd='servicetable.fnReloadAjax();'
        )
        db.t_services.id.comment = add.create()
        return dict(add=add)

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    if request.vars.has_key('ids'):
        for z in request.vars.ids.split('|'):
            if z is not '':
                db(db.t_services.id == z).delete()
                count += 1
        db.commit()
        response.flash = '%s Services(s) deleted' % (count)
        response.headers['web2py-component-command'] = "servicetable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return


@auth.requires_login()
def by_host():
    """
    Returns a list of services + serviceinfo based upon an host identifier
    (id, ipv4, ipv6)
    """
    record = get_host_record(request.args(0))
    if record is None:
        redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))

    response.title = "%s :: Services for %s" % (settings.title, host_title_maker(record))
    services = db(db.t_services.f_hosts_id==record.id).select(db.t_services.id,
                                                              db.t_services.f_proto, db.t_services.f_number, db.t_services.f_status,
                                                              db.t_services.f_name, db.t_services.f_banner)#, cache=(cache.ram,60))

    svcq = (db.t_services.f_hosts_id==record.id)
    infoq = (db.t_service_info.f_services_id == db.t_services.id)

    if request.extension == "json":
        #rows = db(svcq).select(db.t_services.ALL, db.t_service_info.ALL, left=db.t_service_info.on(infoq))
        aaData = []
        for svc in services:
            # service info
            atxt = {}
            q = db(db.t_service_info.f_services_id == svc.id).select()
            if len(q) > 0:
                addl = []
                for svcinfo in q:
                    addl.append(TR(TD(svcinfo.f_name), TD(svcinfo.f_text)))
                atxt['0'] = IMG(_src=URL(request.application,'static','images/details_open.png')).xml()
                atxt['1'] = TABLE(THEAD(TR(TH(T('Name')), TH(T('Text')))), TBODY(addl)).xml()
            else:
                atxt['0'] = ("")
                atxt['1'] = ("")
            atxt['2'] = A('edit', _target="services_edit_%s" % (svc.id),
                          _href=URL('edit', args=[svc['id']], extension='html')).xml()
            atxt['3'] = svc.f_proto
            if svc.f_number in HTTP_PORTS and svc.f_proto == "tcp" or svc.f_name == "HTTP":
                atxt['4'] = A(svc.f_number,
                              _href="http://%s:%s/" % (record.f_ipv4, svc.f_number),
                              _target="%s-tcp-%s" % (record.f_ipv4, svc.f_number)).xml()
            elif svc.f_number in HTTPS_PORTS and svc.f_proto == "tcp" or svc.f_name == "HTTPS":
                atxt['4'] = A(svc.f_number,
                              _href="https://%s:%s/" % (record.f_ipv4, svc.f_number),
                              _target="%s-tcp-%s" % (record.f_ipv4, svc.f_number)).xml()
            else:
                atxt['4'] = svc.f_number
            atxt['5'] = svc.f_status
            atxt['6'] = svc.f_name or ""
            atxt['7'] = svc.f_banner or ""
            atxt['DT_RowId'] = svc.id

            aaData.append(atxt)

        result = { 'sEcho': request.vars._,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }

        return result

    add = AddModal(
        db.t_services, 'Add', 'Add', 'Add Service',
        fields=[
            'f_proto', 'f_number', 'f_status', 'f_name', 'f_banner'
        ],
        cmd='servicetable.fnReloadAjax();'
    )
    db.t_services.f_hosts_id.default = record.id
    db.t_services.id.comment = add.create()

    form = TABLE(THEAD(TR(TH('', _width="5%"),
                          TH('Info'),
                          TH(T('')),
                          TH(T('Protocol')),
                          TH(T('Number')),
                          TH(T('Status')),
                          TH(T('Name')),
                          TH(T('Banner')),
                          )  ),
                 _class="datatable",
                 _id="servicetable",
                 _style="width:100%")

    return dict(form=form, host=record, add=add)

@auth.requires_login()
def hosts_with_port():
    """
    Creates a CSV file of ipv4,ipv6 for hosts with a user-specified
    tcp/udp port
    """

    # XXX: This is broken and needs some TLC

    # buld the dropdown user list
    #users = db(db.auth_user).select()
    #userlist = []
    #for user in users:
    #    userlist.append( [ user.id, user.username ] )

    form = SQLFORM.factory(
        Field('f_proto', 'string', label=T('Protocol'), default="tcp", requires=IS_IN_SET(("tcp", "udp", "info"))),
        Field('f_number', type='string', label=T('Port')),
        Field('f_name', type='string', label=T('Name (exact)')),
        Field('f_banner', type='string', label=T('Banner (contains)')),
        Field('ignore_filter', type='boolean', default=False, label=T('Ignore Hostfilter')),
        #Field('f_ipv4', type='boolean', default=True, label=T('Show IPv4')),
        #Field('f_ipv6', type='boolean', default=False, label=T('Show IPv6')),
        #Field('f_hostname', type='boolean', default=False, label=T('Show Hostname')),
        #Field('f_engineer', type='integer', label=T('Engineer'), default=auth.user.id, requires=IS_IN_SET(userlist)),
        #Field('f_asset_group', type='string', label=T('Asset Group'), requires=IS_NOT_EMPTY()),
        _action=URL(request.application,'services','hosts_with_port.csv'),
    )

    db_svcs = db.t_services
    db_hosts = db.t_hosts
    if form.accepts(request.vars, session):
        q = (db_svcs.id > 0)
        q1 = None
        if form.vars.f_number:
            q1 = (db_svcs.f_number == form.vars.f_number)
        if form.vars.f_proto:
            q2 = (db_svcs.f_proto == form.vars.f_proto)
            if q1:
                q1 = q1 & q2
        if form.vars.f_name:
            q2 = (db_svcs.f_name == form.vars.f_name)
            if q1:
                q1 = q1 & q2
        if form.vars.f_banner:
            q2 = (db_svcs.f_banner.contains(form.vars.f_banner))
            if q1:
                q1 = q1 & q2

        if q1:
            q = q & q1

        if not form.vars.ignore_filter:
            q = create_hostfilter_query(session.hostfilter, q, 't_services')
        else:
            q &= (db_svcs.f_hosts_id == db_hosts.id)

        ip_list = db(q).select(db_hosts.f_ipv4, db_hosts.f_ipv6, db_svcs.f_number, db_hosts.f_hostname, cache=(cache.ram, 60))

        return dict(
            ip_list=ip_list,
        )
    elif form.errors:
        response.flash = 'Error in form'
        redirect(URL('hosts_with_port', extension=''))

    return dict(form=form)

##-------------------------------------------------------------------------
## service_info
##-------------------------------------------------------------------------

@auth.requires_login()
def info_add():
    if request.vars.has_key('id'):
        host_id = db.t_hosts[request.vars.id] or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    elif request.vars.has_key('ipv4'):
        host_id = db(db.t_hosts.f_ipv4 == request.vars.ipv4) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    elif request.vars.has_key('ipv6'):
        host_id = db(db.t_hosts.f_ipv6 == request.vars.ipv6) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    else:
        host_id = None

    if host_id:
        db.t_service_info.f_hosts_id.default = host_id.id
        form=crud.create(db.t_service_info, next=URL('info_edit', vars={'id': host_id.id}),
                         message="Serivce Info added")
        db.t_service_info.f_hosts_id.default = None

    else:
        form=crud.create(db.t_service_info,next='info_edit/[id]', message="Service Info added")

    response.title = "%s :: Create Service Info" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def info_edit():
    record = db.t_service_info(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Service record not found')}))
    response.title = "%s :: Update Service Info :: %s" % (settings.title, record.f_services_id)
    form=crud.update(db.t_service_info,record,next='info_edit/[id]',
                     ondelete=lambda form: redirect(URL('info_list')))
    return dict(form=form)

@auth.requires_login()
def info_list():
    response.title = "%s :: Services Info" % (settings.title)
    if request.extension == 'json':
        rows=db(db.t_service_info).select()

        aaData = []
        # datatable formatting is specific
        for r in rows:
            svc = db.t_services[r.f_services_id]
            atxt = []
            atxt.append(A(r.id, _target="service_info_%s" % (r.id), _href="info_edit/%s" % (r.id)).xml(), )
            atxt.append(A(db.t_hosts[svc.f_hosts_id].f_ipv4, _target="host_detail_%s" % (svc.f_hosts_id), _href=URL('hosts', 'detail', args=svc.f_hosts_id, extension='html')).xml())
            atxt.append(svc.f_proto)
            atxt.append(svc.f_number)
            atxt.append(r.f_name)
            atxt.append(r.f_text)
            # add columns after this, don't do anything prior since it'll affect the hidden fields

            aaData.append(atxt)

        totalrecords = db(db.t_service_info).count()

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': totalrecords,
                   #'iTotalDisplayRecords': nolimit,
                   'aaData': aaData,
                   }

        return result
    else:
        return dict()

@auth.requires_login()
def info_by_svcid():
    svc = db.t_services(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Service record not found')}))
    response.title = "%s :: Service Info for %s" % (settings.title, svc.f_hosts_id.f_ipv4)
    rows = db(db.t_service_info.f_services_id == svc.id).select(db.t_service_info.id,
                                                                db.t_service_info.f_services_id,
                                                                db.t_service_info.f_name,
                                                                db.t_service_info.f_text)
    return dict(rows=rows)

##-------------------------------------------------------------------------
## services_apps
##-------------------------------------------------------------------------

@auth.requires_login()
def apps_add():
    form=crud.create(db.t_services_apps_refs,next='apps_edit/[id]')
    return dict(form=form)

@auth.requires_login()
def apps_edit():
    record = db.t_services_apps_refs(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Service record not found')}))
    form=crud.update(db.t_services_apps_refs,record,next='apps_edit/[id]',
                     ondelete=lambda form: redirect(URL('apps_list')))
    return dict(form=form)

@auth.requires_login()
def apps_list():
    f,v=request.args(0),request.args(1)
    query=f and db.t_services_apps_refs[f]==v or db.t_services_apps_refs
    rows=db(query)(db.t_services_apps_refs).select()
    return dict(rows=rows)

##-------------------------------------------------------------------------
## service tasks (web images, banner grabs, etc)
##-------------------------------------------------------------------------

@auth.requires_signature()
@auth.requires_login()
def valkyries_ajax():
    """
    Take a list of service_ids, build relevant data send them to the right valkyrie
    """

    valkyrie_type = request.vars.get('valkyrie')
    svc_count = 0
    good_count = 0
    if 'ids' in request.vars:
        svc_list = []
        for z in request.vars.ids.split('|'):
            if z is not '':
                svc_list.append(z)
        if len(svc_list) > 5 or request.vars.f_taskit:
            # we have to scheduler task 20 or more images because of timeouts
            # submit tasks in service groups of 50 at a time to be executed

            total_svcs = len(svc_list)
            task_ids = []
            for cnt in range(0, total_svcs, 50):
                task = scheduler.queue_task(
                    run_valkyrie,
                    pvars=dict(
                        valkyrie_type=valkyrie_type,
                        services=svc_list[cnt:cnt+49]
                    ),
                    group_name=settings.scheduler_group_name,
                    sync_output=5,
                    timeout=1800    # 30 minutes
                )
                if task.id:
                    task_ids.append(task.id)
                else:
                    logger.error("Error creating webshot task: %s" % task.error)
            msg = "%s web screenshot tasks for %s services started" % (len(task_ids), len(svc_list))

        else:
            if valkyrie_type == 'webshot':
                from skaldship.valkyries.webimaging import do_screenshot
            elif valkyrie_type == 'vncshot':
                from skaldship.valkyries.vncscreenshot import do_screenshot
            else:
                msg = "Unknown valkyrie type"
                reponse.flash = msg
                return dict(msg=msg)

            res = do_screenshot(svc_list)
            msg = "%s screenshot(s) taken from %s services(s), %s failed" % (res[0], len(svc_list), res[1])
            response.headers['web2py-component-command'] = "jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"

    else:
        msg = "No services sent!"

    response.flash = msg
    return dict(msg=msg)

########NEW FILE########
__FILENAME__ = shodanhq
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Metasploit controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

#from skaldship.general import get_oreally_404, host_title_maker


@auth.requires_login()
def download():
    return response.download(request, db)

@auth.requires_login()
def call():
    session.forget()
    return service()
### end requires

@auth.requires_login()
def index():
    return dict()

@auth.requires_login()
def import_report():
    """
    Upload/import ShodanHQ XML export file
    """
    import os

    #try:
    #    from shodan import WebAPI
    #    from shodan.api import WebAPIError
    #    webapi = WebAPI(settings.kvasir_config.get('shodanhq_api_key', '')
    #except ImportError:
    #    webapi = None

    filedir = os.path.join(request.folder, 'data', 'scanfiles')
    response.title = "%s :: Import ShodanHQ Data" % (settings.title)
    fields = []

    # buld the dropdown user list
    users = db(db.auth_user).select()
    userlist = []
    for user in users:
        userlist.append([user.id, user.username])

    #if webapi:
    #    fields.append(Field('f_query', 'string', label=A('ShodanHQ Query', _href="http://www.shodanhq.com/help/filters", _target="blank", _rel="noreferrer")))
    #    fields.append(Field('f_max_responses', 'integer', default=1000, label=T('Limit of searches'), requires=IS_INT_IN_RANGE(1, 1000001),
    #        comment="Maximum number of responses, in 100s up to 1,000,000"))
    #    fields.append(Field('f_hosts', 'text', label=T('IP Addresses')))

    fields.append(Field('f_filename', 'upload', uploadfolder=filedir, label=T('ShodanHQ XML File')))
    fields.append(Field('f_engineer', type='integer', label=T('Engineer'), default=auth.user.id, requires=IS_IN_SET(userlist)))
    fields.append(Field('f_asset_group', type='string', label=T('Asset Group for new Hosts'), default="ShodanHQ Import", requires=IS_NOT_EMPTY()))
    fields.append(Field('f_include_list', type='text', label=T('Hosts to Only Include')))
    fields.append(Field('f_ignore_list', type='text', label=T('Hosts to Ignore')))
    fields.append(Field('f_taskit', type='boolean', default=auth.user.f_scheduler_tasks, label=T('Run in background task')))
    form = SQLFORM.factory(*fields, table_name='shodanhq')

    if form.errors:
        response.flash = 'Error in form'
    elif form.accepts(request.vars, session):
        # build the hosts only/exclude list
        ip_exclude = []
        data = form.vars.get('f_ignore_list')
        if data:
            ip_exclude = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals
        ip_include = []
        data = form.vars.get('f_include_list')
        if data:
            ip_include = data.split('\r\n')
            # TODO: check for ip subnet/range and break it out to individuals

        filename = form.vars.f_filename
        if filename:
            filename = os.path.join(filedir, form.vars.f_filename)
        else:
            filename = None
        hosts = form.vars.get('f_hosts')
        max_responses = form.vars.get('f_max_responses')
        query = form.vars.get('f_query')

        if form.vars.f_taskit:
            task = scheduler.queue_task(
                scanner_import,
                pvars=dict(
                    scanner='shodanhq',
                    filename=filename,
                    host_list=hosts,
                    query=[query, max_responses],
                    asset_group=form.vars.f_asset_group,
                    engineer=form.vars.f_engineer,
                    ip_ignore_list=ip_exclude,
                    ip_include_list=ip_include,
                ),
                group_name=settings.scheduler_group_name,
                sync_output=5,
                timeout=settings.scheduler_timeout
            )
            if task.id:
                redirect(URL('tasks', 'status', args=task.id))
            else:
                response.flash = "Error submitting job: %s" % (task.errors)
        else:
            from skaldship.shodanhq import process_report
            result = process_report(
                filename=filename,
                host_list=hosts,
                query=[query, max_responses],
                asset_group=form.vars.f_asset_group,
                engineer=form.vars.f_engineer,
                ip_ignore_list=ip_exclude,
                ip_include_list=ip_include,
            )
            response.flash = result

    return dict(form=form)

########NEW FILE########
__FILENAME__ = snmp
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## SNMP controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import get_host_record, host_title_maker, host_a_maker
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return redirect(URL('list'))

##-------------------------------------------------------------------------
## snmp
##-------------------------------------------------------------------------

@auth.requires_login()
def add():
    if request.args(0) is not None:
        record = get_host_record(request.args(0))
        db.t_snmp.f_hosts_id.default = record.id

    response.title = "%s :: Create SNMP Entry" % (settings.title)
    form=crud.create(db.t_snmp,next='edit/[id]')
    db.t_snmp.f_hosts_id.default = None
    return dict(form=form)

@auth.requires_login()
def read():
    record = db.t_snmp(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('SNMP record not found')}))
    response.title = "%s :: SNMP :: %s" % (settings.title, host_title_maker(db.t_hosts[record.f_hosts_id]))
    form=crud.read(db.t_snmp,record)
    return dict(form=form)

@auth.requires_login()
def edit():
    record = db.t_snmp(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('SNMP record not found')}))
    response.title = "%s :: SNMP Update :: %s" % (settings.title, host_title_maker(db.t_hosts[record.f_hosts_id]))
    form=crud.update(db.t_snmp,record,next='read/[id]',
                     ondelete=lambda form: redirect(URL('list')))
    return dict(form=form)

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    if 'ids' in request.vars:
        for z in request.vars.ids.split('|'):
            if z is not '':
                db(db.t_host_snmp.id == z).delete()
                db.commit()
                count += 1
    msg = "%s SNMP record(s) deleted" % (count)
    response.flash = msg
    response.headers['web2py-component-command'] = 'snmptable.fnReloadAjax();'
    return dict(msg=msg)

@auth.requires_login()
def list():
    aaData = []
    response.title = "%s :: SNMP" % (settings.title)

    if request.extension == "json":
        rows = db(db.t_snmp.id > 0).select()

        for r in rows:
            aTxt = {}
            aaData.append({
                '0': A('edit', _target="snmp_%s" % (r.id), _href=URL('edit', args=r.id)).xml(),
                '1': host_a_maker(r.f_hosts_id).xml(),
                '2': r.f_community,
                '3': r.f_version,
                '4': r.f_access,
                'DT_RowId': r.id
            })

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }

        return result

    add = AddModal(
        db.t_snmp, 'Add', 'Add SNMP', 'Add SNMP',
        #fields=add_fields,
        cmd='snmptable.fnReloadAjax();',
        flash="SNMP entry added"
    )
    db.t_snmp.id.comment = add.create()

    table = TABLE(THEAD(TR(TH(T('ID'), _width="5%"),
                           TH(T('Host')),
                           TH(T('Community')),
                           TH(T('Version')),
                           TH(T('Access Level')),
                           )  ),
                  _class="datatable",
                  _id="snmptable",
                  _style="width:100%")

    return dict(table=table, add=add)

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    for r in request.vars.ids.split('|'):
        if r is not None:
            db(db.t_snmp.id == r).delete()
            count += 1
    db.commit()
    response.flash = "%s SNMP Record(s) deleted" % (count)
    response.headers['web2py-component-command'] = "snmptable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return

@auth.requires_login()
def by_host():
    """
    Returns a list of OS records based upon an host identifier
    (id, ipv4, ipv6)
    """
    if request.args(0) is None: redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))

    record = get_host_record(request.args(0))

    if record is None:
        redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))

    response.title = "%s :: SNMP Records for %s" % (settings.title, host_title_maker(record))
    snmplist = db(db.t_snmp.f_hosts_id==record.id).select()

    aaData = []
    if request.extension == "json":
        for snmp in snmplist:
            # datatables json requires aaData to be specificly formatted
            aaData.append({
                '0': A("edit", _target="snmp_update_%s" % (snmp.id), _href=URL('edit',extension='html',args=snmp.id)).xml(),
                '1': snmp.f_community,
                '2': snmp.f_version,
                '3': snmp.f_access,
                'DT_RowId': snmp.id,
            })

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }

        return result

    form = TABLE(THEAD(TR(TH(T('ID'), _width="5%"),
                          TH(T('Community')),
                          TH(T('Version')),
                          TH(T('Access')),
                          )  ),
                 _class="datatable",
                 _id="snmptable",
                 _style="width:100%")

    add = AddModal(
        db.t_snmp, 'Add', 'Add', 'Add SNMP String',
        fields=[ 'f_community', 'f_version', 'f_access'],
        cmd='snmptable.fnReloadAjax();'
    )
    db.t_snmp.f_hosts_id.default = record.id
    db.t_snmp.id.comment = add.create()

    return dict(form=form, host=record, add=add)

########NEW FILE########
__FILENAME__ = stats
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Statistics controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.general import severity_mapping
from skaldship.hosts import create_hostfilter_query
from skaldship.statistics import db_statistics, adv_db_statistics, graphs_index

import logging
logger = logging.getLogger("web2py.app.kvasir")


@auth.requires_login()
def index():
    """
    Index statistics page, shows all graphs (jqPlot) and vuln stats
    through LOAD pages.
    """
    response.title = "%s :: Statistics Home" % (settings.title)
    return dict()

@auth.requires_login()
def vulnlist():
    """
    Produces a list of vulnerabilities with severity, cvss and count.
    """
    from skaldship.statistics import vulnlist

    vulnid = request.args(0) or "%"
    vulnlist = vulnlist(query=vulnid)

    response.title = "%s :: Vulnerability Statistics" % (settings.title)
    return dict(vulnlist=vulnlist)#, statistics=statistics, adv_stats=adv_stats)

@auth.requires_login()
def passwords():
    """
    Password statistics
    """
    try:
        toplimit = int(request.args(0))
    except:
        toplimit = 15

    from collections import defaultdict

    #hash_types = db(db.t_accounts.id > 0).select(db.t_accounts.f_hash1_type, groupby=db.t_accounts.f_hash1_type).as_dict(key='f_hash1_type')
    #for htype in hash_types.keys():
    #    pass

    hash_cnt = db.t_accounts.f_hash1_type.count()
    hash_stats = db(db.t_accounts.id > 0).select(db.t_accounts.f_hash1_type, hash_cnt, groupby=db.t_accounts.f_hash1_type)

    pw_cnt = db.t_accounts.f_password.count()
    top = db(db.t_accounts.f_password != None).select(db.t_accounts.f_password, pw_cnt, groupby=db.t_accounts.f_password, orderby=~pw_cnt, limitby=(0,toplimit))

    top_sparx_table = []
    top_sparx_table.append("""
    <table>
    <title>Top 10 Passwords</title>
    <tgroup cols="2">
      <thead>
        <row>
          <entry>Password</entry>
          <entry>Count</entry>
        </row>
      </thead>

      <tbody>
""")
    for z in top:
        top_sparx_table.append("""
        <row>
            <entry>%s</entry>
            <entry>%s</entry>
        </row>
""" % (z.t_accounts.f_password, z._extra['COUNT(t_accounts.f_password)']))
    top_sparx_table.append("</tbody>\n</tgroup>\n</table>")

    from skaldship.passwords import password_class_stat
    pwlenstats = defaultdict(lambda: 0)
    pwstats = defaultdict(lambda: 0)
    passwords = db(db.t_accounts.f_compromised==True).select(db.t_accounts.f_password, cache=(cache.ram, 60))
    for (pw_lenstat, character_class, record) in password_class_stat(passwords):
        pwlenstats[pw_lenstat] += 1
        pwstats[character_class] += 1

    pwstats_sparx_table = []
    pwstats_sparx_table.append("""
    <table>
    <title>Password Type Statistics</title>
    <tgroup cols="2">
      <thead>
        <row>
          <entry>Password Type / Example</entry>
          <entry>Count</entry>
        </row>
      </thead>

      <tbody>
""")
    for z in pwstats:
        pwstats_sparx_table.append("""
        <row>
            <entry>%s</entry>
            <entry>%s</entry>
        </row>
""" % (z, pwstats[z]))
    pwstats_sparx_table.append("</tbody>\n</tgroup>\n</table>")

    response.title = "%s :: Password Statistics" % (settings.title)

    return dict(
        hash_stats=hash_stats,
        top=top,
        top_sparx_table=top_sparx_table,
        pwlenstats=pwlenstats,
        pwstats=pwstats,
        pwstats_sparx_table=pwstats_sparx_table
    )

@auth.requires_login()
def os():
    """
    Operating system statistics
    """

    hostfilter = session.hostfilter
    if hostfilter is None:
        # if no filter is set then we blank it out
        if session.hostfilter is None:
            session.hostfilter = [(None, None), False]

    q = create_hostfilter_query(session.hostfilter)

    rows = db(q).select(db.t_hosts.id, db.t_host_os_refs.f_certainty, db.t_os.f_title, db.t_os.f_vendor,
                        db.t_host_os_refs.f_family, db.t_host_os_refs.f_class,
                        left=(db.t_host_os_refs.on(db.t_hosts.id==db.t_host_os_refs.f_hosts_id),
                              db.t_os.on(db.t_os.id==db.t_host_os_refs.f_os_id)),
                        orderby=db.t_hosts.id|~db.t_host_os_refs.f_certainty, cache=(cache.ram, 60))

    seen = set()
    os_counts = {}
    vendor_counts = {}
    family_counts = {}
    class_counts = {}
    for r in rows:
        if r.t_hosts.id not in seen and not seen.add(r.t_hosts.id): # kludge way to select only rows per host with the best OS-guess
            os_title = r.t_os.f_title or 'Unknown'
            os_vendor = r.t_os.f_vendor or 'Unknown'
            os_family = r.t_host_os_refs.f_family or 'Unknown'
            os_class = r.t_host_os_refs.f_class or 'Unknown'

            # only capitalize if the first char of the string isn't already capitalized
            # this covers not destroying things like HP, IOS, etc
            #if os_vendor[0].islower(): os_vendor = os_vendor.capitalize()
            if os_family[0].islower(): os_family = os_family.capitalize()
            if os_class[0].islower(): os_class = os_class.capitalize()

            count = os_counts.setdefault(os_title, 0)
            count += 1
            os_counts[os_title] = count

            count = vendor_counts.setdefault(os_vendor, 0)
            count += 1
            vendor_counts[os_vendor] = count

            count = family_counts.setdefault(os_family, 0)
            count += 1
            family_counts[os_family] = count

            count = class_counts.setdefault(os_class, 0)
            count += 1
            class_counts[os_class] = count

    response.title = "%s :: Operating System Statistics" % (settings.title)

    return dict(
        vendor_counts=vendor_counts,
        os_counts=os_counts,
        family_counts=family_counts,
        class_counts=class_counts,
    )

@auth.requires_login()
def services():
    """
    Service statistics
    """

    t_hosts = db.t_hosts
    t_svcs = db.t_services

    hostfilter = session.hostfilter
    if hostfilter is None:
        # if no filter is set then we blank it out
        if session.hostfilter is None:
            session.hostfilter = [(None, None), False]

    q = (t_hosts.id>0)
    q = create_hostfilter_query(session.hostfilter, q, t_svcs)

    rows = db(q).select(t_svcs.f_proto, t_svcs.f_number, t_svcs.f_name, cache=(cache.ram, 60))
    port_counts = {}
    name_counts = {}
    for r in rows:
        port = "%s/%s" % (r.f_proto, r.f_number)
        sname = r.f_name

        count = port_counts.setdefault(port, 0)
        count += 1
        port_counts[port] = count

        count = name_counts.setdefault(sname, 0)
        count += 1
        name_counts[sname] = count

    response.title = "%s :: Service Statistics" % (settings.title)

    return dict(
        port_counts=port_counts,
        name_counts=name_counts,
    )

@auth.requires_login()
def graphs():
    """
    Products the main statistics and graphs
    """

    if request.vars.gtype.lower() == 'all':
        indexgraphs = graphs_index(db)


    return dict(indexgraphs = indexgraphs)

@auth.requires_login()
def basic():
    """
    Basic statistics from the default index page
    """
    statistics = db_statistics(db)
    adv_stats = adv_db_statistics(db)
    return dict(statistics=statistics, adv_stats=adv_stats)

@auth.requires_login()
def vulncloud():
    """
    Pablo's vulnerability tag cloud

    Vulnerability IDs are counted and colored via severity.
    1-3: grey
    4-5: blue
    6-7: magenta
    8-10: red

    IDs are then sized based on quantity in HTML.
    """

    if request.extension == "json":
        # build the json data
        vulncloud = {}
        vd = db.t_vulndata
        svc_vulns = db.t_service_vulns

        # grab the list of vulnerabilities

        q = (svc_vulns.f_vulndata_id == vd.id)
        if request.args(0) is not None:
            try:
                minsev = float(request.args(0))
            except:
                minsev = 8.0

            q &= (vd.f_cvss_score >= minsev)
            if settings.use_cvss:
                q &= (vd.f_cvss_score >= float(request.args(0)))
            else:
                q &= (vd.f_severity >= int(request.args(0)))

            vulns = db(q).select(
                vd.id, vd.f_vulnid, vd.f_severity, vd.f_cvss_score, cache=(cache.ram, 300)
            )
        else:
            vulns = db(vd.id > 0).select(vd.id, vd.f_vulnid, vd.f_severity, vd.f_cvss_score, cache=(cache.ram, 300))

        for row in vulns:
            count = db(db.t_service_vulns.f_vulndata_id == row.id).count()

            if count > 0:
                if settings.use_cvss:
                    severity = int(row.f_cvss_score)
                else:
                    severity = int(row.f_severity)

                vulncloud[row.f_vulnid] = vulncloud.setdefault(
                    row.f_vulnid, {'count': count, 'color': severity_mapping(severity)[2]}
                )

        cloud = []
        for k, v in vulncloud.iteritems():
            cloud.append({'tag': k, 'count': v['count'], 'color': v['color']})
        return dict(vulncloud=cloud)

    response.title = "%s :: Vulnerability Tag Cloud" % (settings.title)
    response.files.append(URL(request.application, 'static', 'js/jquery.tagcloud-2.js'))
    return dict()

    response.title = "%s :: Vulnerability Tag Cloud" % (settings.title)
    response.files.append(URL(request.application, 'static', 'js/jquery.tagcloud-2.js'))
    return dict()

@auth.requires_login()
def vulncircles():
    """
    Vulnerability Circles shows critical vulnerabilities based on a scientifically
    proven formula involving CVSS score, metrics, counts and accounts
    """
    response.title = "%s :: Vulnerability Circles" % (settings.title)
    response.files.append(URL(request.application, 'static', 'js/d3.min.js'))
    return dict()

@auth.requires_login()
def vulncircles_data():
    vulncircles = {}

    minsev = request.args(0) or 8
    if settings.use_cvss:
        rows = db(db.t_vulndata.f_cvss_score >= minsev).select(cache=(cache.ram, 300))
    else:
        rows = db(db.t_vulndata.f_severity >= minsev).select(cache=(cache.ram, 300))
    for row in rows:
        vulncount = db(db.t_service_vulns.f_vulndata_id == row.id).count()

        exploits = db(db.t_exploit_references.f_vulndata_id == row.id).select()
        exploit_modifier = 1
        for expl in exploits:
            rank = db.t_exploits[expl.f_exploit_id].f_rank
            if rank in ['Novice', 'Intermediate']:
                exploit_modifier = 5
            elif rank in ['Expert']:
                exploit_modifer = 2.5
            else:
                exploit_modifier = 1.5

            level = db.t_exploits[expl.f_exploit_id].f_level
            if level in ['normal', 'great', 'excellent']:
                exploit_modifer *= 5
            elif level in ['average', 'good']:
                exploit_modifer *= 2.5
            elif level in ['unknown', 'manual', 'low']:
                exploit_modifer *= 1.5

        expcount = len(exploits)

        # if an account is sourced from a vuln, modifier is applied
        query = (db.t_accounts.f_source == row.f_vulnid) & (db.t_accounts.f_compromised == True)
        accounts = db(query).count()
        if accounts:
            account_mod = 2
        else:
            account_mod = .1

        # cvss modifiers
        # access vector (Local, Adjacent, Network)
        if row.f_cvss_av.upper() == 'N':
            av_mod = 10
        elif row.f_cvss_av.upper() == 'A':
            av_mod = 5
        else:
            av_mod = 2

        # access complexity (High, Medium, Low)
        if row.f_cvss_ac.upper() == 'L':
            ac_mod = 10
        elif row.f_cvss_ac.upper() == 'M':
            ac_mod = 5
        else:
            ac_mod = 2

        # authentication (Multiple, Single, None)
        if row.f_cvss_au.upper() == 'N':
            au_mod = 10
        elif row.f_cvss_au.upper() == 'S':
            au_mod = 5
        else:
            au_mod = 2

        # confidentiality impact (None, Partial, Complete)
        if row.f_cvss_c.upper() == 'C':
            c_mod = 10
        elif row.f_cvss_c.upper() == 'P':
            c_mod = 5
        else:
            c_mod = 2

        # integrity impact (None, Partial, Complete)
        if row.f_cvss_i.upper() == 'C':
            i_mod = 10
        elif row.f_cvss_i.upper() == 'P':
            i_mod = 5
        else:
            i_mod = 2

        # severity setting check and calculations
        if settings.use_cvss:
            severity = float(row.f_cvss_score)
        else:
            severity = int(row.f_severity)

        diameter = int((vulncount + (expcount * 2) * exploit_modifier) *
                       (severity * (av_mod + ac_mod + au_mod + c_mod + i_mod)) * account_mod)

        if vulncount > 0:
            vulncircles[row.f_vulnid] = vulncircles.setdefault(row.f_vulnid, {
                'diameter': diameter, 'vulncount': vulncount, 'expcount': expcount,
                'severity': severity, 'title': row.f_title,
            })

    data = {}
    for (k,v) in vulncircles.iteritems():
        sev = v['severity']
        values = data.setdefault(sev, [])
        values.append({
            'name': k, 'size': v['diameter'], 'vulncount': v['vulncount'],
            'expcount': v['expcount'], 'severity': v['severity'], 'title': v['title']
        })
        data[sev] = values

    d3json = []
    for k in data.keys():
        parent = {'name': 'Sev ' + str(k), 'children': data[k]}
        d3json.append(parent)

    response.headers['Content-Type'] = 'text/json'
    import gluon.contrib.simplejson
    return gluon.contrib.simplejson.dumps(d3json)

########NEW FILE########
__FILENAME__ = tasks
# encoding: utf-8
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Tasks controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

import logging
logger = logging.getLogger("web2py.app.kvasir")


@auth.requires_login()
def index():
    """
    Displays all tasks in a big dataTable
    """
    response.title = "%s :: Tasks" % (settings.title)
    schd = db.scheduler_task
    #schd.id.represent = lambda r,id:A(id, _href=URL('tasks', 'runs', args=id))
    fields = [
        schd.id,
        schd.task_name,
        schd.status,
        schd.function_name,
        schd.args,
        schd.vars,
        schd.start_time,
        schd.next_run_time,
        schd.stop_time,
        schd.repeats,
        schd.retry_failed,
        schd.period,
        schd.timeout,
        schd.times_run,
        schd.times_failed,
        schd.last_run_time,
        schd.group_name,
        schd.assigned_worker_name,
    ]

    rows = db(schd.id > 0).select()
    return dict(rows=rows, fields=fields)

@auth.requires_login()
def status():
    """
    task_id is the scheduler_task identifier, not the record id.
    """
    task_id = request.args(0) or redirect(URL('index'))
    task = db(db.scheduler_task.id == task_id).select().first()
    if not task:
        redirect(URL('tasks','index'))
    response.title = "%s Task :: %s" % (settings.title, task.task_name)
    return dict(task=task)

@auth.requires_signature()
@auth.requires_login()
def stop():
    """
    Uses the scheduler.stop_task() to stop a task (wow!)
    """
    task_id = request.vars.get('id') or request.get(0)
    if not task_id:
        response.flash = "No task identifer sent"
        return dict()

    try:
        task_id = int(task_id)
    except:
        response.flash = "Invalid task identifier"
        return dict()

    res = scheduler.stop_task(task_id)
    if not res:
        response.flash = "Task %s not found (result = %s)" % (task_id, res)
    else:
        response.flash = "Stopping task %s (result = %s)" % (task_id, res)
        response.headers['web2py-component-command'] = '$("#stop_button").addClass("disabled");'
    return dict()

@auth.requires_login()
def output():
    """
    task_id is the scheduler_task identifier, not the record id.
    """
    task_id = request.args(0) or redirect(URL('index'))
    task = db(db.scheduler_run.task_id == task_id).select().last()
    response.title = "%s :: Task #%s :: Output" % (settings.title, task_id)
    if task:
        return dict(
            output=task.run_output,
            status=task.status,
            traceback=task.traceback,
            worker=task.worker_name,
            start=task.start_time,
            ended=task.stop_time,
        )
    else:
        newtask = db(db.scheduler_task.id == task_id).select().first()
        return dict(
            output="",
            status=newtask.status,
            traceback="",
            worker=newtask.assigned_worker_name,
            start="N/A",
            ended="",
        )

########NEW FILE########
__FILENAME__ = tooloutput
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2014 Cisco Systems, Inc.
##
## Tool output controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import get_host_record, host_title_maker, host_a_maker
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return redirect(URL('list'))


##-------------------------------------------------------------------------
## tool output
##-------------------------------------------------------------------------

@auth.requires_login()
def add():
    if request.args(0) is not None:
        record = get_host_record(request.args(0))
        db.t_tool_output.f_hosts_id.default = record.id
    else:
        record = None

    if request.extension == 'load':
        buttons=[]
    else:
        buttons=['submit']

    if record:
        form=SQLFORM(db.t_tool_output, buttons=buttons, upload=URL('download'), fields=['f_type', 'f_note', 'f_output'],
                     _action=URL('add', args=[ record.id ]), _id="tool_output_add_form")
    else:
        form=SQLFORM(db.t_tool_output, buttons=buttons, upload=URL('download'), fields=['f_hosts_id', 'f_type', 'f_note', 'f_output'],
                     _action=URL('add'), _id="tool_output_add_form")

    if request.vars.f_output is not None:
        form.vars.f_filename = request.vars.f_output.filename
    if form.accepts(request.vars, session):
        response.flash = "Tool output added"
        response.headers['web2py-component-command'] = 'tool_output_table.fnReloadAjax();'
        return ""
    elif form.errors:
        response.flash = "Error in form submission"
        return TABLE(*[TR(k, v) for k, v in form.errors.items()])

    db.t_tool_output.f_hosts_id.default = None
    response.title = "%s :: Add Evidence" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def edit():
    record = db.t_tool_output(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Record not found')}))
    response.title = "%s :: Tool Output Update :: %s" % (settings.title, host_title_maker(db.t_hosts[record.f_hosts_id]))
    form=crud.update(db.t_tool_output,record,next='edit/[id]',
                     ondelete=lambda form: redirect(URL('list')))
    return dict(form=form)

@auth.requires_signature()
@auth.requires_login()
def delete():
    count = 0
    if request.vars.has_key('ids'):
        for r in request.vars.ids.split('|'):
            if r is not '':
                db(db.t_tool_output.id == r).delete()
                count += 1
    db.commit()
    response.flash = "%s record(s) deleted" % (count)
    response.headers['web2py-component-command'] = 'tool_output_table.fnReloadAjax();'
    return

@auth.requires_login()
def download():
    import gluon.contenttype as cc
    f_output = request.args[0]

    row=db(db.t_tool_output.f_output==f_output).select(
        db.t_tool_output.f_data, db.t_tool_output.f_filename, db.t_tool_output.f_output).first()

    response.headers['Content-Type']=cc.contenttype(f_output)
    # convert unknowns (x-XXXX) into text/plain
    if "/x-" in response.headers['Content-Type']:
        response.headers['Content-Type'].replace('x-log', 'plain')
    response.headers['Content-Disposition'] = "attachment; filename=%s" % (row.f_filename)
    #response.headers['Content-Type']='text/plain'
    if row.f_data is not None:
        return row.f_data
    else:
        return ""

@auth.requires_login()
def view():
    import mimetypes
    row = db.t_tool_output[request.args[0]]

    if not row:
        return redirect(URL('default', 'error', vars={'msg': T('Record not found')}))

    mtype = mimetypes.guess_type(row.f_filename)
    contents = "Binary or unknown file type. Download to view."
    if mtype[0]:
        contents = row.f_data
    extension = row.f_filename[row.f_filename.rfind('.')+1:]
    shBrushes = {
      'sh':   ['shBrushBash.js', 'bash'],
      'c':    ['shBrushCpp.js', 'c'],
      'cs':   ['shBrushCSharp.js', 'csharp'],
      'css':  ['shBrushCss.js', 'css'],
      'java': ['shBrushJava.js', 'java'],
      'js':   ['shBrushJScript.js', 'javascript'],
      'pl':   ['shBrushPerl.js', 'perl'],
      'php':  ['shBrushPhp.js', 'php'],
      'py':   ['shBrushPython.js', 'python'],
      'rb':   ['shBrushRuby.js', 'ruby'],
      'sql':  ['shBrushSql.js', 'sql'],
      'vb':   ['shBrushVb.js', 'vb'],
      'xml':  ['shBrushXml.js', 'xml'],
      'html': ['shBrushXml.js', 'html'],
      'htm':  ['shBrushXml.js', 'html'],
    }
    shJsFile = shBrushes.get(extension, ['shBrushPlain.js', 'plain'])

    response.title = "%s :: %s :: %s" % (settings.title, row.f_filename, row.f_type)
    return dict(note=row.f_note, output=row.f_output, contents=contents, shJsFile=shJsFile)


@auth.requires_login()
def list():
    """
    Returns a list of tool_output based on a host (id, ipv4, ipv6) or all
    """
    import os, string
    if request.args(0) is not None:
        record = get_host_record(request.args(0))
        if record is None:
            redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
        response.title = "%s :: Tool Output for host %s" % (settings.title, host_title_maker(record))
    else:
        response.title = "%s :: Tool Output listing" % (settings.title)
        record = None

    aaData = []
    if request.extension == "json":
        if record is None:
            rows = db(db.t_tool_output).select(db.t_tool_output.id,
                                            db.t_tool_output.f_hosts_id,
                                            db.t_tool_output.f_type,
                                            db.t_tool_output.f_note,
                                            db.t_tool_output.f_filename,
                                            db.t_tool_output.f_output,
                                            db.t_tool_output.f_data.len()+1)
        else:
            rows = db(db.t_tool_output.f_hosts_id == record.id).select(db.t_tool_output.id,
                                                                    db.t_tool_output.f_hosts_id,
                                                                    db.t_tool_output.f_type,
                                                                    db.t_tool_output.f_note,
                                                                    db.t_tool_output.f_filename,
                                                                    db.t_tool_output.f_output,
                                                                    db.t_tool_output.f_data.len()+1)

        for r in rows:
            atxt = {}
            cnt = 0
            atxt[cnt] = A('edit', _target="tool_output_edit_%s" % (
                r.t_tool_output.id), _href=URL('edit', extension='html', args=r.t_tool_output.id)
            ).xml()
            cnt += 1
            if record is None:
                atxt[cnt] = host_a_maker(r.t_tool_output.f_hosts_id).xml()
                cnt += 1
            atxt[cnt] = r.t_tool_output.f_type
            cnt += 1
            atxt[cnt] = r.t_tool_output.f_note
            cnt += 1
            if r.t_tool_output.f_filename is not None:
                if string.lower(os.path.splitext(r.t_tool_output.f_filename)[1]) in ('.png', '.jpeg', '.jpg', '.gif'):
                    atxt[cnt] = A(IMG(_src=URL('download', args=[r.t_tool_output.f_output]), _width="50%", _height="20%"),
                                  _href=URL('download', args=[r.t_tool_output.f_output]),
                                  _target="tool_output_image_%s" % (r.t_tool_output.id), _id="tool_output_image").xml()
                    cnt += 1
                    atxt[cnt] = "%sb" % (r._extra['(LENGTH(t_tool_output.f_data) + 1)'])
                    cnt += 1
                else:
                    a = SPAN(
                        A(r.t_tool_output.f_filename, _target="tool_output_other_%s" % (r.t_tool_output.id),
                          _id="tool_output_other", _href=URL('view.html', args=[r.t_tool_output.id])),
                        " ",
                        A(I(" ", _class="icon-download"), _class="btn btn-mini",
                          _href=URL('download', args=r.t_tool_output.f_output))
                    )
                    atxt[cnt] = a.xml()
                    cnt += 1
                    atxt[cnt] = "%sb" % (r._extra['(LENGTH(t_tool_output.f_data) + 1)'])
                    cnt += 1
            else:
                atxt[cnt] = r.t_tool_output.f_filename
                cnt += 1
            atxt['DT_RowId'] = r.t_tool_output.id

            aaData.append(atxt)

        return {
            'sEcho': request.vars.sEcho,
            'iTotalRecords': len(aaData),
            'aaData': aaData,
        }

    if record:
        th_rows = (TH(T(''), _width="5%"),
                   TH(T('Type')),
                   TH(T('Note')),
                   TH(T('Output')),
                   TH(T('File Size')),
                   )
    else:
        th_rows = (TH(T(''), _width="5%"),
                   TH(T('Host')),
                   TH(T('Type')),
                   TH(T('Note')),
                   TH(T('Output'), _width="35%"),
                   TH(T('File Size')),
                   )

    tool_output = TABLE(THEAD(TR(th_rows)),
                     _class="datatable",
                     _id="tool_output_table",
                     _style="width:100%")

    return dict(tool_output=tool_output, host=record)

########NEW FILE########
__FILENAME__ = vulns
# -*- coding: utf-8 -*-
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Vulns controller
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import host_title_maker, host_a_maker, get_host_record, create_hostfilter_query
from skaldship.general import severity_mapping, cvss_metrics
import logging
logger = logging.getLogger("web2py.app.kvasir")
crud.settings.formstyle = formstyle_bootstrap_kvasir


@auth.requires_login()
def index():
    return redirect(URL('vulndata_list'))

##-------------------------------------------------------------------------
## vuln references and exploits SQLTABLE.grid() functions
##-------------------------------------------------------------------------

@auth.requires_login()
def vuln_refs():
    """
    Returns Vulnerability References in a grid based on f_vulndata_id
    """
    vuln_id = request.args(0) or redirect(URL('index'))
    db.t_vuln_references.f_vulndata_id.default == vuln_id
    refs = SQLFORM.grid(
        db.t_vuln_references.f_vulndata_id == vuln_id,
        args=[vuln_id],
        #fields = [ db.t_vuln_refs.f_source, db.t_vuln_refs.f_text ],
        fields = [ db.t_vuln_references.f_vuln_ref_id ],
        maxtextlength=255,
        searchable=False,
        deletable=True,
        details=False,
        selectable=False,
        csv=False,
        formstyle='bootstrap',
        formname='vuln_refs_grid',
        client_side_delete=True,
        paginate=None,
    )
    return refs

@auth.requires_login()
def vuln_exploits():
    """
    Returns vulnerability exploits in a grid based on f_vulndata_id
    """
    vuln_id = request.args(0) or redirect(URL('index'))
    db.t_exploit_references.f_vulndata_id == vuln_id
    exploits = SQLFORM.grid(
        db.t_exploit_references.f_vulndata_id == vuln_id,
        args=[vuln_id],
        fields = [ db.t_exploit_references.f_exploit_id ],
        maxtextlength=255,
        searchable=False,
        deletable=True,
        details=False,
        selectable=False,
        csv=False,
        formstyle='bootstrap',
        formname='vuln_exploits_grid',
        client_side_delete=True,
        paginate=None,
)
    return exploits

@auth.requires_login()
def vuln_hosts():
    """
    Returns a grid of hosts based on f_vulndata_id
    """
    vuln_id = request.args(0) or None
    query = (db.t_hosts.id > 0)
    query = create_hostfilter_query(session.hostfilter, query, 't_services')
    query &= (db.t_services.f_hosts_id == db.t_hosts.id)
    query &= (db.t_service_vulns.f_services_id == db.t_services.id)
    query &= (db.t_service_vulns.f_vulndata_id == vuln_id)
    hosts = SQLFORM.grid(
        query,
        args=[vuln_id],
        fields = [
            db.t_hosts.f_ipv4,
            db.t_hosts.f_ipv6,
            db.t_hosts.f_hostname,
            db.t_services.f_proto,
            db.t_services.f_number,
            db.t_service_vulns.f_proof,
        ],
        maxtextlength=255,
        searchable=True,
        deletable=True,
        details=False,
        selectable=True,
        create=False,
        csv=True,
        formstyle='bootstrap',
        formname='vuln_hosts_grid',
        client_side_delete=True,
        paginate=None,
    )
    return hosts

##-------------------------------------------------------------------------
## vulninfo_by_vulnid
##-------------------------------------------------------------------------

@auth.requires_login()
def vulninfo_by_vulnid():
    """
    Returns the vulnerablilty details
    """
    if request.args(0) is None:
        redirect(URL('default', 'error', vars={'msg': T('No Vulnerability ID sent')}))

    record = db(db.t_vulndata.f_vulnid==request.args(0)).select().first()
    if record is not None:
        # grab vuln references and format the table
        response.title = "%s :: Vulnerability Popup :: %s" % (settings.title, record.f_vulnid)
        #cvss_metrics = "AV:%s/AC:%s/Au:%s/C:%s/I:%s/A:%s" % (record.f_cvss_av,
        #                                                     record.f_cvss_ac,
        #                                                     record.f_cvss_au,
        #                                                     record.f_cvss_c,
        #                                                     record.f_cvss_i,
        #                                                     record.f_cvss_a)

        vulninfo = record
        cvssmetrics = cvss_metrics(record)

        refs = LOAD(request.controller, 'vuln_refs', args=[record.id], ajax=True)
        exploits = LOAD(request.controller, 'vuln_exploits', args=[record.id], ajax=True)

        # TODO: Add hosts with vulnerability -- include service info (proto/port) and
        # ability to delete vuln from service

        query = db.t_service_vulns.f_vulndata_id == record.id
        svc_vulns = db(query).select(db.t_service_vulns.f_services_id,
                                     db.t_service_vulns.f_proof,
                                     db.t_service_vulns.f_status,
                                     db.t_service_vulns.id,
                                     distinct=True)

        # if no filter is set then we blank it out
        if session.hostfilter is None:
            session.hostfilter = [(None, None), False]

        hosts_tr = []
        query = (db.t_hosts.id > 0)
        query = create_hostfilter_query(session.hostfilter, query, 't_services')
        hosts_dict = db(query).select(db.t_hosts.id, cache=(cache.ram, 30)).as_dict()
        hostlist = map(lambda x: x['id'], hosts_dict.itervalues())
        for svc_vuln in svc_vulns:
            svc = db.t_services[svc_vuln.f_services_id]
            if svc is None:
                logger.error("t_servics_vuln #%s does not link to a t_services.id!" % (svc_vuln.id))
                continue

            if svc.f_hosts_id not in hostlist:
                continue

            host_rec = db.t_hosts[svc.f_hosts_id]
            hosts_tr.append(TR(TD(SPAN(I(_class="icon-trash"), _name="host_del", _id=svc_vuln.id)),
                               TD(A(IMG(_src=URL(request.application, 'static/images', 'terminal.png'),
                                        _width="20",
                                        _height="20",
                                        _style="float:left"), "  ",
                                    _href="#", _onclick="launchterm('%s')" % (host_rec.id)),
                                   host_a_maker(host_rec)),
                               TD("%s/%s" % (svc.f_proto, svc.f_number)),
                               TD(MARKMIN(svc_vuln.f_proof)),
                               TD(svc_vuln.f_status),
                               _id=svc_vuln.id ) )

        if len(hosts_tr) > 0:
            hosts = TABLE(THEAD(TR(TH(T('Del'), _width="5%"),
                                   TH(T('Host Information')),
                                   TH(T('Port')),
                                   TH(T('Proof')),
                                   TH(T('Status')),
                                )  ),
                          TBODY(hosts_tr),
                          _id="vulntable", _class="datatable", _width="100%")
        else:
            hosts = None
    else:
        response.title = "%s :: Invalid Vulnerability ID"
        return dict(vulninfo={}, refs={}, exploits={}, hosts={})

    # vuln form data
    vuln=crud.read(db.t_vulndata, record) #returns read-only for for t_vulndata
    vuln.attributes['_id'] = "vuln_record"
    return dict(vuln=vuln, vulninfo=vulninfo, cvssmetrics=cvssmetrics, refs=refs, exploits=exploits, hosts=hosts)

##-------------------------------------------------------------------------
## vulndata
##-------------------------------------------------------------------------

@auth.requires_login()
def vulndata_add():
    response.title = "%s :: Add Vulnerability" % (settings.title)
    form=crud.create(db.t_vulndata,next='vulndata_edit/[id]', message="Vulnerability added")
    return dict(form=form)

@auth.requires_login()
def vulndata_edit():
    vuln_id = request.args(0) or redirect(URL('vulndata_list'))

    # first check for f_vulnid being passed as argument
    record = db(db.t_vulndata.f_vulnid == vuln_id).select().first()
    if not record:
        record = db.t_vulndata[vuln_id]
    if not record:
        redirect(URL('vulndata_list'))

    #form=crud.update(db.t_vulndata,record,next='vulndata_edit/[id]',
    #                 ondelete=lambda form: redirect(URL('vulndata_list')),
    #                 onaccept=crud.archive)

    form = SQLFORM(
        db.t_vulndata,
        record,
        submit_button='Update',
        deletable=True,
        formstyle='bootstrap',
        showid=False,
    )

    hosts = LOAD(request.controller , 'vuln_hosts', args=[record.id], ajax=True, target='vuln_hosts_grid')
    refs = LOAD(request.controller, 'vuln_refs', args=[record.id], ajax=True, target='vuln_refs_grid')
    exploits = LOAD(request.controller, 'vuln_exploits', args=[record.id], ajax=True, target='vuln_exploits_grid')

    if form.process().accepted:
       response.flash = 'Record updated'
    elif form.errors:
       response.flash = 'Error in form'

    response.title = "%s :: Edit Vulnerability :: %s" % (settings.title, record.f_vulnid)
    return dict(form=form, hosts=hosts, refs=refs, exploits=exploits)

# Edit Modal Form
@auth.requires_login()
def edit():
    """ Creates and processes vulndata modal form """
    record = db.t_vulndata(request.args(0)) or redirect(URL('error'))
    if request.extension in ['load', 'json']:
        form=SQLFORM(db.t_vulndata, record, buttons=[], _action=URL('edit', args=[record.id]), _id="vuln_edit_form")
    else:
        response.title = "%s :: Edit Vuln" % (settings.title)
        form=SQLFORM(db.t_vulndata, record, _id="vuln_edit_form")

    if form.accepts(request.vars, session):
        response.flash = "Vuln information updated"
    elif form.errors:
        response.flash = "Error in form submission"
    return dict(form=form)

@auth.requires_login()
def vulndata_list():
    response.title = "%s :: Vulnerabilites" % (settings.title)
    if request.extension == 'json':
        # Datatables Server-side: http://datatables.net/usage/server-side
        if request.vars.has_key('iDisplayStart'):
            start = int(request.vars.iDisplayStart)
        else:
            start = 0
        if request.vars.has_key('iDisplayLength'):
            if request.vars.iDisplayLength == '-1':
                limit = db(db.t_vulndata).count()
            else:
                limit = start + int(request.vars.iDisplayLength)
        else:
            limit = int(auth.user.f_show_size)

        if request.vars.has_key('sSearch'):
            # sSearch global search box
            query = db.t_vulndata.f_vulnid.like("%%%s%%" % request.vars.sSearch) | db.t_vulndata.f_title.like("%%%s%%" % request.vars.sSearch)
        else:
            query = db.t_vulndata.id > 0
        #query &= (db.t_service_vulns.f_vulndata_id == db.t_vulndata.id)

        #total_count = db.t_vulndata.id.count()
        if request.vars.iSortingCols == '1':
            # sorting by a column - this is a little trippy because tuples start at 0
            # and datatables starts at 1 so we have to subtract 1 from iSortCol_0
            cols = ( None,
                     db.t_vulndata.id,
                     db.t_vulndata.f_vulnid,
                     db.t_vulndata.f_title,
                     db.t_vulndata.f_severity,
                     None,
                     db.t_vulndata.f_cvss_score
                     )

            orderby = cols[int(request.vars.iSortCol_0) ]
            if request.vars.sSortDir_0 == 'asc':
                rows=db(query).select(orderby=orderby,
                                      limitby=(start, limit), cache=(cache.ram, 180))
            else:
                rows=db(query).select(orderby=~orderby,
                                      limitby=(start, limit), cache=(cache.ram, 180))
        else:
            rows=db(query).select(limitby=(start,limit), cache=(cache.ram, 180))

        nolimit=db(query).count()

        aaData = []
        # datatable formatting is specific
        for r in rows:
            atxt = []
            atxt.append(IMG(_src=URL(request.application,'static','images/details_open.png')).xml())
            #atxt.append(r.total_count)
            atxt.append(A(r.id, _target="vulndata_edit_%s" % (r.id), _href=URL('vulns', 'vulndata_edit', args=r.id, extension='html')).xml())
            atxt.append(A(r.f_vulnid, _target="vulninfo_%s" % (r.f_vulnid), _href=URL(request.application, 'vulns', 'vulninfo_by_vulnid', args=r.f_vulnid, extension='html')).xml())
            atxt.append(r.f_title)
            atxt.append(r.f_severity)
            atxt.append(r.f_pci_sev)
            atxt.append(r.f_cvss_score)
            atxt.append(MARKMIN(r.f_description).xml())
            atxt.append(MARKMIN(r.f_solution).xml())
            # add columns after this, don't do anything prior since it'll affect the hidden fields

            aaData.append(atxt)

        totalrecords = db(db.t_vulndata).count()

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': totalrecords,
                   'iTotalDisplayRecords': nolimit,
                   'aaData': aaData,
                   }

        return result
    else:
        return dict()

@auth.requires_login()
def vulndata_by_host():
    """
    Returns a list of vulnerabilties based upon an host identifier
    (id, ipv4, ipv6)
    """
    from skaldship.metasploit import msf_get_config
    msf_settings = msf_get_config(session)

    record = get_host_record(request.args(0))
    if record is None:
        redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))

    response.title = "%s :: Vulnerabilities for %s" % (settings.title, host_title_maker(record))
    services = db(db.t_services.f_hosts_id==record.id).select(db.t_services.id,
                                                              db.t_services.f_proto, db.t_services.f_number)

    if request.extension == "json":
        aaData = []
        for svc in services:
            # service info
            q = db(db.t_service_vulns.f_services_id == svc.id).select()
            for vulninfo in q:
                atxt = {}
                exploit_list = []
                vulndetails = db(db.t_vulndata.id == vulninfo.f_vulndata_id).select(cache=(cache.ram, 300)).first()
                exploits = db(db.t_exploit_references.f_vulndata_id == vulninfo.f_vulndata_id).select(orderby=~db.t_exploit_references.id)
                if len(exploits) > 0:
                    expl_count = "Yes (%d)" % (len(exploits))
                    for expl in exploits:
                        for expl_data in db(db.t_exploits.id == expl.f_exploit_id).select(cache=(cache.ram, 300)):
                            exp_link = expl_data.f_name
                            if expl_data.f_source == 'exploitdb':
                                if db.t_exploitdb[expl_data.f_title]:
                                    exploitdb_href = URL('exploitdb', 'detail.html', args=expl_data.f_title)
                                else:
                                    exploitdb_href = URL('default', 'redirect', extension='html', vars={'url': 'http://www.exploit-db.com/exploits/%s' % expl_data.f_title})
                                exp_link = A(IMG(_align="absmiddle", _width=16, _height=16, _src=URL('static','images/exploitdb.ico')), ' exploitdb - ' + expl_data.f_name,_href=exploitdb_href, _target="exploitdb_%s" % (expl_data.f_name))
                            elif expl_data.f_source == 'metasploit':
                                if session.msf_workspace:
                                    msf_uri = os.path.join(msf_settings['url'], 'workspaces', session.msf_workspace_num, 'modules', expl_data.f_title)
                                else:
                                    msf_uri = URL('default', 'redirect', extension='html', vars={'url': 'http://www.rapid7.com/db/modules/%s' % expl_data.f_title})
                                exp_link = A(IMG(_align="absmiddle", _width=16, _height=16, _src=URL('static','images/msf.gif')), ' metasploit - ' + expl_data.f_name, _href=msf_uri, _target="msf_%s" % (expl_data.f_name))
                            elif expl_data.f_source == 'canvas':
                                exp_link = SPAN(IMG(_align="absmiddle", _width=16, _height=16, _src=URL('static','images/canvas.png')), ' canvas - ' + expl_data.f_name)

                            exploit_list.append("%s : %s (%s/%s)" % (expl_data.f_title, exp_link, expl_data.f_rank, expl_data.f_level))
                else:
                    expl_count = ""

                atxt['0'] = IMG(_src=URL(request.application,'static','images/details_open.png')).xml()
                atxt['1'] = A('edit', _target="service_vuln_update_%s" % (vulninfo.id), _href=URL('vulns', 'service_vulns_edit', args=vulninfo.id, extension='html')).xml()
                if vulninfo.f_exploited:
                    atxt['2'] = '<input id="exploited" value="' + str(vulninfo.id) + '" type="checkbox", checked>'
                else:
                    atxt['2'] = '<input id="exploited" value="' + str(vulninfo.id) + '" type="checkbox">'
                atxt['3'] = "%s/%s" % (svc.f_proto, svc.f_number)
                atxt['4'] = A(vulndetails.f_vulnid, _target="vulndata_%s" % (vulndetails.id), _href=URL('vulns', 'vulninfo_by_vulnid', args=vulndetails.f_vulnid, extension='html')).xml()
                atxt['5'] = vulndetails.f_severity
                atxt['6'] = vulndetails.f_cvss_score
                atxt['7'] = SPAN(vulninfo.f_status,_id="vulninfo_status",_vulnstatus=vulninfo.f_status).xml()
                atxt['8'] = expl_count
                atxt['9'] = MARKMIN(vulninfo.f_proof).xml()
                atxt['10'] = MARKMIN(vulndetails.f_description).xml()
                atxt['11'] = vulndetails.f_title
                atxt['12'] = "<br />\n".join(exploit_list)
                atxt['DT_RowId'] = vulninfo.id
                aaData.append(atxt)

        result = { 'sEcho': request.vars.sEcho,
                   'iTotalRecords': len(aaData),
                   'aaData': aaData,
                   }

        return result

    add = AddModal(
        db.t_service_vulns, 'Add', 'Add', 'Add Vulnerability',
        #fields=[
        #],
        cmd='vulntable.fnReloadAjax();'
    )
    #db.t_service_vulns.f_services_id.default = svc.id
    svc_set = []
    for svc in services:
        svc_set.append([svc.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[record.id]), svc.f_proto, svc.f_number)])
    db.t_service_vulns.f_services_id.requires = IS_IN_SET(svc_set)
    db.t_service_vulns.id.comment = add.create()

    form = TABLE(THEAD(TR(TH('', _width="5%"),
                          TH(T(''), _width="5%"),
                          TH(T('Pwned'), width="5%"),
                          TH(T('Port')),
                          TH(T('Vuln ID')),
                          TH(T('Sev')),
                          TH(T('CVSS')),
                          TH(T('Status')),
                          TH(T('Exploits')),
                          TH(T('Proof')),
                          TH(T('Description')),
                          TH(T('Title')),
                          TH(T('Exploit List')),
                          )  ),
                 _class="datatable",
                 _id="vulntable",
                 _style="width:100%")

    return dict(form=form, host=record, add=add)

##-------------------------------------------------------------------------
## service_vulns
##-------------------------------------------------------------------------

@auth.requires_login()
def service_vulns_add():
    if request.vars.has_key('service'):
        svc = db.t_services[request.vars.service] or redirect(URL('default', 'error', vars={'msg': T('Service record not found')}))
        svc_id = [svc.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[svc.f_hosts_id]), svc.f_proto, svc.f_number)]
    else:
        svc_id = None

    if request.vars.has_key('host'):
        # grab services for a host
        host_id = db.t_hosts[request.vars.host] or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
        services = db(db.t_services.f_hosts_id == host_id.id).select()
        svc_set = []
        for svc in services:
            svc_set.append([svc.id, "%s :: %s/%s" % (host_title_maker(db.t_hosts[svc.f_hosts_id]), svc.f_proto, svc.f_number)])
    else:
        host_id = None

    if svc_id or host_id:
        if svc_id:
            db.t_service_vulns.f_services_id.default = svc_id
            response.title = "%s :: Add Service Vulnerablity :: %s" % (settings.title, svc)
        else:
            db.t_service_vulns.f_services_id.requires = IS_IN_SET(svc_set)
            response.title = "%s :: Add Service Vulnerablity :: %s" % (settings.title, host_title_maker(db.t_hosts[svc.f_hosts_id]))
        form=crud.create(db.t_service_vulns,message='Vulnerability added',next=URL('service_vulns_add', vars={'id': svc.id}))
        db.t_service_vulns.f_services_id.requires = None
        response.title = "%s :: Add Service Vulnerablity :: %s" % (settings.title, host_title_maker(db.t_hosts[svc.f_hosts_id]))
    else:
        form=crud.create(db.t_service_vulns,next='service_vulns_edit/[id]')
        response.title = "%s :: Add Service Vulnerablity" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def service_vulns_edit():
    record = db.t_service_vulns(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Service vulnerability record not found')}))
    form=crud.update(db.t_service_vulns,record,next='service_vulns_edit/[id]',
                     ondelete=lambda form: redirect(URL('service_vulns_list')),
                     onaccept=crud.archive)
    response.title = "%s :: Edit Service Vulnerablity" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def service_vulns_list():
    # XXX: this doesn't work yet. . .
    if session.hostfilter is None:
        session.hostfilter = [(None, None), False]

    query = (db.t_service_vulns.id > 0) & (db.t_service_vulns.f_services_id == db.t_services.id) & (db.t_service_vulns.f_vulndata_id == db.t_vulndata.id)
    query = create_hostfilter_query(session.hostfilter, query, 't_services')

    columns = [
        db.t_hosts.f_ipv4, db.t_hosts.f_ipv6, db.t_hosts.f_hostname, db.t_services.f_proto, db.t_services.f_number,
        db.t_vulndata.f_vulnid, db.t_service_vulns.f_status, db.t_service_vulns.f_proof,
        #db.t_service_vulns.id
    ]
    rows = SQLFORM.grid(query, columns, deletable=True, selectable=True, details=False, field_id=db.t_service_vulns.id)
    response.title = "Services with Vulnerabilities"
    return dict(rows=rows)

@auth.requires_login()
def new_service_vulns():
    # server-side processing of service vulns.. faster..better?
    # TODO: This...?
    response.title = "%s :: Services and Vulnerabilities" % (settings.title)
    if request.extension == 'json':
        if session.hostfilter is None:
            session.hostfilter = [(None, None), False]

        query = (db.t_service_vulns.id > 0) & (db.t_service_vulns.f_services_id == db.t_services.id) & (db.t_service_vulns.f_vulndata_id == db.t_vulndata.id)
        query = create_hostfilter_query(session.hostfilter, query, 't_services')

        if request.vars.has_key('iDisplayStart'):
            start = int(request.vars.iDisplayStart)
        else:
            start = 0
        if request.vars.has_key('iDisplayLength'):
            if request.vars.iDisplayLength == '-1':
                limit = db(query).count()
            else:
                limit = start + int(request.vars.iDisplayLength)
        else:
            limit = int(auth.user.f_show_size)

        if request.vars.has_key('sSearch'):
            # sSearch global search box
            query &= db.t_vulndata.f_vulnid.like("%%%s%%" % request.vars.sSearch) | db.t_vulndata.f_title.like("%%%s%%" % request.vars.sSearch)

        if request.vars.iSortingCols == '1':
            # sorting by a column - this is a little trippy because tuples start at 0
            # and datatables starts at 1 so we have to subtract 1 from iSortCol_0
            cols = ( None,
                     db.t_vulndata.id,
                     db.t_vulndata.f_vulnid,
                     db.t_vulndata.f_title,
                     db.t_vulndata.f_severity,
                     None,
                     db.t_vulndata.f_cvss_score
                     )

@auth.requires_signature()
@auth.requires_login()
def service_vulns_delete():
    count = 0
    for r in request.vars.ids.split('|'):
        if r is not '':
            db(db.t_service_vulns.id == r).delete()
            count += 1
    db.commit()
    response.flash = "%s Vuln(s) deleted" % (count)
    response.headers['web2py-component-command'] = 'vulntable.fnReloadAjax();'
    return

@auth.requires_signature()
@auth.requires_login()
def service_vuln_exploited():
    uncount = 0
    spacount = 0
    if request.vars.has_key('ids'):
        for r in request.vars.ids.split("|"):
            if r is not '':
                rec = db.t_service_vulns[int(r)]
                if rec.f_exploited:
                    db.t_service_vulns[int(r)] = dict(f_exploited = False, f_status = 'vulnerable-exploited')
                    uncount += 1
                else:
                    db.t_service_vulns[int(r)] = dict(f_exploited = True, f_status = 'exploited')
                    spacount += 1
        db.commit()
    response.flash = "%s Exploited / %s Un-exploited" % (spacount, uncount)
    referrer = request.env.http_referer or ''
    if 'vulninfo_by_vulnid' not in referrer:
        response.headers['web2py-component-command'] = "vulntable.fnReloadAjax(); jQuery('.datatable tr.DTTT_selected').removeClass('DTTT_selected');"
    return

##-------------------------------------------------------------------------
## vuln_references
##-------------------------------------------------------------------------

@auth.requires_login()
def vuln_references_add():
    if request.extension in ['load']:
        record = db.t_vuln_references(request.args(0)) or None
        form=SQLFORM(db.t_vuln_references, buttons=[], _action=URL('vuln_references_add'), _id="vulnrefs_link_form")
        if record:
            form.vars.f_vulndata_id = record.id

        if form.accepts(request.vars, session):
            response.flash = "Vulnerability Reference Added"
            #response.headers['web2py-component-command'] = 'hosttable.fnReloadAjax();'
            return
        elif form.errors:
            response.flash = "Error in form submission"
            return TABLE(*[TR(k, v) for k, v in form.errors.items()])
    else:
        form=crud.create(db.t_vuln_references,next='vuln_references_edit/[id]',message='Vulnerability added')
        response.title = "%s :: Add Vulnerability Reference" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def vuln_references_edit():
    record = db.t_vuln_references(request.args(0)) or redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))
    form=crud.update(db.t_vuln_references,record,next='vuln_references_edit/[id]',
                     ondelete=lambda form: redirect(URL('vuln_references_list')))
    response.title = "%s :: Edit Vulnerability Reference" % (settings.title)
    return dict(form=form)

@auth.requires_login()
def vuln_references_list():
    #f,v=request.args(0),request.args(1)
    #query=f and db.t_vuln_references[f]==v or db.t_vuln_references
    #rows=db(query).select()
    #return dict(rows=rows)
    rows = SQLFORM.smartgrid(db.t_vuln_references)
    response.title = "%s :: Vulnerability References" % (settings.title)
    return dict(rows=rows)

@auth.requires_login()
def vuln_references_by_vulnid():
    """
    Returns a list of vulnerability references by a vulnerability id #
    """
    rows = db(db.t_vuln_references.f_vulndata_id==request.args(0)).select()
    return rows

##-------------------------------------------------------------------------
## aa_by_host
##-------------------------------------------------------------------------

@auth.requires_login()
def aa_by_host():
    """
    Returns a list of vulnerabilties per port in a tree view format based upon an host identifier
    (id, ipv4, ipv6)
    """
    record = get_host_record(request.args(0))
    if record is None:
        redirect(URL('default', 'error', vars={'msg': T('Host record not found')}))

    treeul=UL(_id='aatree_ul')

    db_svcs = db.t_services
    db_svulns = db.t_service_vulns
    db_vulns = db.t_vulndata

    services = db(db_svcs.f_hosts_id==record.id).select(db_svcs.f_number, db_svcs.id,
                                                        db_svcs.f_proto, db_svcs.f_name,orderby=db_svcs.id)

    if settings.use_cvss:
        db_vsevs = db_vulns.f_cvss_score
    else:
        db_vsevs = db_vulns.f_severity

    tree = DIV(_id="aatree")
    for svc in services:

        nexlist = []
        nexlist_single = []
        expl_count = 0
        exploit_list = UL()
        exploitdb = 0
        metasploit = 0
        canvas = 0
        prev_f_status = ''
        vulnclass = ''
        for vulninfo in db(
                (db_svulns.f_services_id == svc.id) & (db_vulns.id == db_svulns.f_vulndata_id)
                ).select(orderby=~db_svulns.f_status|~db_vsevs, cache=(cache.ram, 120)):

            #init variables
            vulndetails = vulninfo.t_vulndata
            vulninfo = vulninfo.t_service_vulns

            cur_f_status = vulninfo.f_status

            #Generating the exploit lists

            exploits = db(db.t_exploit_references.f_vulndata_id == vulninfo.f_vulndata_id).select(orderby=~db.t_exploit_references.id)

            exploit_list_single = UL()
            if len(exploits) > 0:

                for expl in exploits:
                    for expl_data in db(db.t_exploits.id == expl.f_exploit_id).select(db.t_exploits.f_source, db.t_exploits.f_title, db.t_exploits.f_name, db.t_exploits.f_rank, db.t_exploits.f_level):
                        exp_link = expl_data.f_name
                        if expl_data.f_source == 'exploitdb':
                            exploitdb += 1
                            if db.t_exploitdb[expl_data.f_title]:
                                exploitdb_href = URL('exploitdb', 'detail.html', args=expl_data.f_title)
                            else:
                                exploitdb_href = URL('default', 'redirect', extension='html', vars={'url': 'http://www.exploit-db.com/exploits/%s' % expl_data.f_title})
                            exp_link = A(IMG(_align="absmiddle", _width=16, _height=16, _src=URL('static','images/exploitdb.ico')), ' exploitdb - ' + expl_data.f_name,_href=exploitdb_href, _target="exploitdb_%s" % (expl_data.f_name))
                        elif expl_data.f_source == 'metasploit':
                            metasploit += 1
                            if session.msf_workspace:
                                msf_uri = os.path.join(msf_settings['url'], session.msf_workspace, 'modules', expl_data.f_title)
                            else:
                                msf_uri = URL('default', 'redirect', extension='html', vars={'url': 'http://www.rapid7.com/db/modules/%s' % expl_data.f_title})
                            exp_link = A(IMG(_align="absmiddle", _width=16, _height=16, _src=URL('static','images/msf.gif')), ' metasploit - ' + expl_data.f_name, _href=msf_uri, _target="msf_%s" % (expl_data.f_name))
                        elif expl_data.f_source == 'canvas':
                            canvas += 1
                            exp_link = SPAN(IMG(_align="absmiddle", _width=16, _height=16, _src=URL('static','images/canvas.png')), ' canvas - ' + expl_data.f_name)
                            #expl_link = ' canvas - ' + expl_data.f_name
                        expl_count += 1
                        exploit_list_single.append(LI(expl_data.f_title , " : " , exp_link , " (" , expl_data.f_rank , "/" , expl_data.f_level, ")"))

            textdecoration=""
            if vulninfo.f_exploited == True and len(exploits) > 0:
                textdecoration="text-decoration:line-through underline; "
            elif vulninfo.f_exploited == True and len(exploits) == 0:
                textdecoration="text-decoration: line-through; "
            elif (vulninfo.f_exploited == False or vulninfo.f_exploited == None) and len(exploits) == 0:
                textdecoration="text-decoration: none;"

            #generation vuln link
            if settings.use_cvss:
                severity = int(float(vulndetails.f_cvss_score))
            else:
                severity = int(vulndetails.f_severity)
            style = textdecoration + "color:" + severity_mapping(severity - 1)[2]
            vuln_title_link = A(vulndetails.f_vulnid, _title = vulninfo.f_status+ ' Severity: ' + str(severity),
                                _style=style, _target="vulndata_%s" % (vulndetails.id),
                                _href=URL(request.application,'vulns', 'vulninfo_by_vulnid',
                                          args=vulndetails.f_vulnid, extension='html'))

            if cur_f_status != prev_f_status and prev_f_status != '':
                nexlist.append(SPAN(nexlist_single, _class=vulnclass)) #for a line in the bottom
                nexlist.append(' ')
                nexlist_single = []
            else:
                nexlist_single.append(' ')

            nexlist_single.append(vuln_title_link )
            prev_f_status = vulninfo.f_status
            vulnclass = ''

            #style for vuln links
            if vulninfo.f_status == 'vulnerable-version':
                vulnclass='host_detail_vulnerable-version'
            if vulninfo.f_status == 'vulnerable-exploited':
                vulnclass='host_detail_vulnerable-exploited'
            if vulninfo.f_status == 'potential':
                vulnclass='host_detail_potential'

            if len(exploit_list_single) > 0: exploit_list.append(LI(SPAN(vuln_title_link), exploit_list_single))

        #attach the last vuln list

        if len(nexlist_single)>0: nexlist.append(SPAN(nexlist_single, _class=vulnclass))
        service_disp=SPAN(svc.f_proto + '/' + svc.f_number + ' - ' + str(svc.f_name))
        expl_count = "Exploits - (%d)" % (expl_count)

        if len(nexlist)>0:
            if len(exploit_list) == 0:
                treeul.append(LI(service_disp,UL(LI(nexlist)))) #No exploits
            else:
                expl_count = SPAN(expl_count + " : metasploit (%d) exploitdb (%d) canvas (%d)" % (metasploit, exploitdb, canvas),_style="color:red")
                treeul.append(LI(service_disp,UL(LI(nexlist)), UL(LI(expl_count,exploit_list,_class="closed"))))
        else:
            treeul.append(LI(service_disp)) #No vulns

        tree = DIV(treeul, _id="aatree")
    return dict(tree=tree)


########NEW FILE########
__FILENAME__ = build-txt2mm
#!/usr/bin/env python

import re
import fileinput

#LEADER = "    "
LEADER = "\t"

def matchit(match):
	if match is None:
		return None
	return match.group('d')

table_re = re.compile("define_table\('(?P<d>\w+)'")
field_re = re.compile("Field\('(?P<d>\w+)'")
field_dbref = re.compile("db\.(?P<d>\w+),")

print "Kvasir Models"

for line in fileinput.input():
	table = matchit(table_re.search(line))
	if table:
		print LEADER, table

	field = matchit(field_re.search(line))
	#if field == "id":
	#	print "%s%s_%s" % (LEADER, field, table)
	if field:
		print LEADER, LEADER, field

	#field = matchit(field_re.search(line))
	#if field is not None:
	#	print "%sid_%s" % (LEADER, field)


########NEW FILE########
__FILENAME__ = es-es
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"actualice" es una expresin opcional como "campo1=\'nuevo_valor\'". No se puede actualizar o eliminar resultados de un JOIN',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': '%s filas eliminadas',
'%s rows updated': '%s filas actualizadas',
'(something like "it-it")': '(algo como "it-it")',
'A new version of web2py is available': 'Hay una nueva versin de web2py disponible',
'A new version of web2py is available: %s': 'Hay una nueva versin de web2py disponible: %s',
'ATTENTION: Login requires a secure (HTTPS) connection or running on localhost.': 'ATENCION: Inicio de sesin requiere una conexin segura (HTTPS) o localhost.',
'ATTENTION: TESTING IS NOT THREAD SAFE SO DO NOT PERFORM MULTIPLE TESTS CONCURRENTLY.': 'ATENCION: NO EJECUTE VARIAS PRUEBAS SIMULTANEAMENTE, NO SON THREAD SAFE.',
'ATTENTION: you cannot edit the running application!': 'ATENCION: no puede modificar la aplicacin que se ejecuta!',
'About': 'Acerca de',
'About application': 'Acerca de la aplicacin',
'Admin is disabled because insecure channel': 'Admin deshabilitado, el canal no es seguro',
'Admin is disabled because unsecure channel': 'Admin deshabilitado, el canal no es seguro',
'Administrator Password:': 'Contrasea del Administrador:',
'Are you sure you want to delete file "%s"?': 'Est seguro que desea eliminar el archivo "%s"?',
'Are you sure you want to uninstall application "%s"': 'Est seguro que desea desinstalar la aplicacin "%s"',
'Are you sure you want to uninstall application "%s"?': 'Est seguro que desea desinstalar la aplicacin "%s"?',
'Authentication': 'Autenticacin',
'Available databases and tables': 'Bases de datos y tablas disponibles',
'Cannot be empty': 'No puede estar vaco',
'Cannot compile: there are errors in your app.        Debug it, correct errors and try again.': 'No se puede compilar: hay errores en su aplicacin. Depure, corrija errores y vuelva a intentarlo.',
'Change Password': 'Cambie Contrasea',
'Check to delete': 'Marque para eliminar',
'Client IP': 'IP del Cliente',
'Controller': 'Controlador',
'Controllers': 'Controladores',
'Copyright': 'Derechos de autor',
'Create new application': 'Cree una nueva aplicacin',
'Current request': 'Solicitud en curso',
'Current response': 'Respuesta en curso',
'Current session': 'Sesin en curso',
'DB Model': 'Modelo "db"',
'DESIGN': 'DISEO',
'Database': 'Base de datos',
'Date and Time': 'Fecha y Hora',
'Delete': 'Elimine',
'Delete:': 'Elimine:',
'Deploy on Google App Engine': 'Instale en Google App Engine',
'Description': 'Descripcin',
'Design for': 'Diseo para',
'E-mail': 'Correo electrnico',
'EDIT': 'EDITAR',
'Edit': 'Editar',
'Edit Profile': 'Editar Perfil',
'Edit This App': 'Edite esta App',
'Edit application': 'Editar aplicacin',
'Edit current record': 'Edite el registro actual',
'Editing file': 'Editando archivo',
'Editing file "%s"': 'Editando archivo "%s"',
'Error logs for "%(app)s"': 'Bitcora de errores en "%(app)s"',
'First name': 'Nombre',
'Functions with no doctests will result in [passed] tests.': 'Funciones sin doctests equivalen a pruebas [aceptadas].',
'Group ID': 'ID de Grupo',
'Hello World': 'Hola Mundo',
'Import/Export': 'Importar/Exportar',
'Index': 'Indice',
'Installed applications': 'Aplicaciones instaladas',
'Internal State': 'Estado Interno',
'Invalid Query': 'Consulta invlida',
'Invalid action': 'Accin invlida',
'Invalid email': 'Correo invlido',
'Language files (static strings) updated': 'Archivos de lenguaje (cadenas estticas) actualizados',
'Languages': 'Lenguajes',
'Last name': 'Apellido',
'Last saved on:': 'Guardado en:',
'Layout': 'Diseo de pgina',
'License for': 'Licencia para',
'Login': 'Inicio de sesin',
'Login to the Administrative Interface': 'Inicio de sesin para la Interfaz Administrativa',
'Logout': 'Fin de sesin',
'Lost Password': 'Contrasea perdida',
'Main Menu': 'Men principal',
'Menu Model': 'Modelo "menu"',
'Models': 'Modelos',
'Modules': 'Mdulos',
'NO': 'NO',
'Name': 'Nombre',
'New Record': 'Registro nuevo',
'No databases in this application': 'No hay bases de datos en esta aplicacin',
'Origin': 'Origen',
'Original/Translation': 'Original/Traduccin',
'Password': 'Contrasea',
'Peeking at file': 'Visualizando archivo',
'Powered by': 'Este sitio usa',
'Query:': 'Consulta:',
'Record ID': 'ID de Registro',
'Register': 'Registrese',
'Registration key': 'Contrasea de Registro',
'Reset Password key': 'Reset Password key',
'Resolve Conflict file': 'archivo Resolucin de Conflicto',
'Role': 'Rol',
'Rows in table': 'Filas en la tabla',
'Rows selected': 'Filas seleccionadas',
'Saved file hash:': 'Hash del archivo guardado:',
'Static files': 'Archivos estticos',
'Stylesheet': 'Hoja de estilo',
'Sure you want to delete this object?': 'Est seguro que desea eliminar este objeto?',
'Table name': 'Nombre de la tabla',
'Testing application': 'Probando aplicacin',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'La "consulta" es una condicin como "db.tabla1.campo1==\'valor\'". Algo como "db.tabla1.campo1==db.tabla2.campo2" resulta en un JOIN SQL.',
'The output of the file is a dictionary that was rendered by the view': 'La salida del archivo es un diccionario escenificado por la vista',
'There are no controllers': 'No hay controladores',
'There are no models': 'No hay modelos',
'There are no modules': 'No hay mdulos',
'There are no static files': 'No hay archivos estticos',
'There are no translators, only default language is supported': 'No hay traductores, slo el lenguaje por defecto es soportado',
'There are no views': 'No hay vistas',
'This is a copy of the scaffolding application': 'Esta es una copia de la aplicacin de andamiaje',
'This is the %(filename)s template': 'Esta es la plantilla %(filename)s',
'Ticket': 'Tiquete',
'Timestamp': 'Timestamp',
'Unable to check for upgrades': 'No es posible verificar la existencia de actualizaciones',
'Unable to download': 'No es posible la descarga',
'Unable to download app': 'No es posible descarga la aplicacin',
'Update:': 'Actualice:',
'Upload existing application': 'Suba esta aplicacin',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Use (...)&(...) para AND, (...)|(...) para OR, y ~(...) para NOT, para crear consultas ms complejas.',
'User ID': 'ID de Usuario',
'View': 'Vista',
'Views': 'Vistas',
'Welcome': 'Welcome',
'Welcome %s': 'Bienvenido %s',
'Welcome to web2py': 'Bienvenido a web2py',
'Which called the function': 'La cual llam la funcin',
'YES': 'SI',
'You are successfully running web2py': 'Usted est ejecutando web2py exitosamente',
'You can modify this application and adapt it to your needs': 'Usted puede modificar esta aplicacin y adaptarla a sus necesidades',
'You visited the url': 'Usted visit la url',
'about': 'acerca de',
'additional code for your application': 'cdigo adicional para su aplicacin',
'admin disabled because no admin password': ' por falta de contrasea',
'admin disabled because not supported on google app engine': 'admin deshabilitado, no es soportado en GAE',
'admin disabled because unable to access password file': 'admin deshabilitado, imposible acceder al archivo con la contrasea',
'and rename it (required):': 'y renombrela (requerido):',
'and rename it:': ' y renombrelo:',
'appadmin': 'appadmin',
'appadmin is disabled because insecure channel': 'admin deshabilitado, el canal no es seguro',
'application "%s" uninstalled': 'aplicacin "%s" desinstalada',
'application compiled': 'aplicacin compilada',
'application is compiled and cannot be designed': 'la aplicacin est compilada y no puede ser modificada',
'cache': 'cache',
'cache, errors and sessions cleaned': 'cache, errores y sesiones eliminados',
'cannot create file': 'no es posible crear archivo',
'cannot upload file "%(filename)s"': 'no es posible subir archivo "%(filename)s"',
'change password': 'cambie contrasea',
'check all': 'marcar todos',
'clean': 'limpiar',
'click here for online examples': 'Haga clic aqu para ver ejemplos en lnea',
'click here for the administrative interface': 'Haga clic aqu para usar la interfaz administrativa',
'click to check for upgrades': 'haga clic para buscar actualizaciones',
'compile': 'compilar',
'compiled application removed': 'aplicacin compilada removida',
'controllers': 'controladores',
'create file with filename:': 'cree archivo con nombre:',
'create new application:': 'nombre de la nueva aplicacin:',
'crontab': 'crontab',
'currently saved or': 'actualmente guardado o',
'customize me!': 'Adaptame!',
'data uploaded': 'datos subidos',
'database': 'base de datos',
'database %s select': 'seleccin en base de datos %s',
'database administration': 'administracin base de datos',
'db': 'db',
'defines tables': 'define tablas',
'delete': 'eliminar',
'delete all checked': 'eliminar marcados',
'design': 'modificar',
'documentation': 'documentacin',
'done!': 'listo!',
'edit': 'editar',
'edit controller': 'editar controlador',
'edit profile': 'editar perfil',
'errors': 'errores',
'export as csv file': 'exportar como archivo CSV',
'exposes': 'expone',
'extends': 'extiende',
'failed to reload module': 'recarga del mdulo ha fallado',
'file "%(filename)s" created': 'archivo "%(filename)s" creado',
'file "%(filename)s" deleted': 'archivo "%(filename)s" eliminado',
'file "%(filename)s" uploaded': 'archivo "%(filename)s" subido',
'file "%(filename)s" was not deleted': 'archivo "%(filename)s" no fu eliminado',
'file "%s" of %s restored': 'archivo "%s" de %s restaurado',
'file changed on disk': 'archivo modificado en el disco',
'file does not exist': 'archivo no existe',
'file saved on %(time)s': 'archivo guardado %(time)s',
'file saved on %s': 'archivo guardado %s',
'help': 'ayuda',
'htmledit': 'htmledit',
'includes': 'incluye',
'insert new': 'inserte nuevo',
'insert new %s': 'inserte nuevo %s',
'internal error': 'error interno',
'invalid password': 'contrasea invlida',
'invalid request': 'solicitud invlida',
'invalid ticket': 'tiquete invlido',
'language file "%(filename)s" created/updated': 'archivo de lenguaje "%(filename)s" creado/actualizado',
'languages': 'lenguajes',
'languages updated': 'lenguajes actualizados',
'loading...': 'cargando...',
'located in the file': 'localizada en el archivo',
'login': 'inicio de sesin',
'logout': 'fin de sesin',
'lost password?': 'olvido la contrasea?',
'merge': 'combinar',
'models': 'modelos',
'modules': 'mdulos',
'new application "%s" created': 'nueva aplicacin "%s" creada',
'new record inserted': 'nuevo registro insertado',
'next 100 rows': '100 filas siguientes',
'or import from csv file': 'o importar desde archivo CSV',
'or provide application url:': 'o provea URL de la aplicacin:',
'pack all': 'empaquetar todo',
'pack compiled': 'empaquete compiladas',
'previous 100 rows': '100 filas anteriores',
'record': 'registro',
'record does not exist': 'el registro no existe',
'record id': 'id de registro',
'register': 'registrese',
'remove compiled': 'eliminar compiladas',
'restore': 'restaurar',
'revert': 'revertir',
'save': 'guardar',
'selected': 'seleccionado(s)',
'session expired': 'sesin expirada',
'shell': 'shell',
'site': 'sitio',
'some files could not be removed': 'algunos archivos no pudieron ser removidos',
'state': 'estado',
'static': 'estticos',
'table': 'tabla',
'test': 'probar',
'the application logic, each URL path is mapped in one exposed function in the controller': 'la lgica de la aplicacin, cada ruta URL se mapea en una funcin expuesta en el controlador',
'the data representation, define database tables and sets': 'la representacin de datos, define tablas y conjuntos de base de datos',
'the presentations layer, views are also known as templates': 'la capa de presentacin, las vistas tambin son llamadas plantillas',
'these files are served without processing, your images go here': 'estos archivos son servidos sin procesar, sus imgenes van aqu',
'to  previous version.': 'a la versin previa.',
'translation strings for the application': 'cadenas de caracteres de traduccin para la aplicacin',
'try': 'intente',
'try something like': 'intente algo como',
'unable to create application "%s"': 'no es posible crear la aplicacin "%s"',
'unable to delete file "%(filename)s"': 'no es posible eliminar el archivo "%(filename)s"',
'unable to parse csv file': 'no es posible analizar el archivo CSV',
'unable to uninstall "%s"': 'no es posible instalar "%s"',
'uncheck all': 'desmarcar todos',
'uninstall': 'desinstalar',
'update': 'actualizar',
'update all languages': 'actualizar todos los lenguajes',
'upload application:': 'subir aplicacin:',
'upload file:': 'suba archivo:',
'versioning': 'versiones',
'view': 'vista',
'views': 'vistas',
'web2py Recent Tweets': 'Tweets Recientes de web2py',
'web2py is up to date': 'web2py est actualizado',

'Notes': 'Notas',

}

########NEW FILE########
__FILENAME__ = fr-ca
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': '%s rang\xc3\xa9es effac\xc3\xa9es',
'%s rows updated': '%s rang\xc3\xa9es mises \xc3\xa0 jour',
'Authentication': 'Authentication',
'Available databases and tables': 'Bases de donn\xc3\xa9es et des tableaux disponibles',
'Cannot be empty': 'Ne peut pas \xc3\xaatre vide',
'Check to delete': 'Cliquez pour supprimer',
'Check to delete:': 'Cliquez pour supprimer:',
'Client IP': 'IP client',
'Controller': 'Contr\xc3\xb4leur',
'Copyright': 'Copyright',
'Current request': 'Demande actuelle',
'Current response': 'R\xc3\xa9ponse actuelle',
'Current session': 'Session en cours',
'DB Model': 'Mod\xc3\xa8le DB',
'Database': 'Base de donn\xc3\xa9es',
'Delete:': 'Supprimer:',
'Description': 'Description',
'E-mail': 'Courriel',
'Edit': '\xc3\x89diter',
'Edit This App': 'Modifier cette application',
'Edit current record': "Modifier l'enregistrement courant",
'First name': 'Pr\xc3\xa9nom',
'Function disabled': 'Function disabled',
'Group ID': 'Groupe ID',
'Hello World': 'Bonjour tout le monde',
'Import/Export': 'Importer/Exporter',
'Index': 'Index',
'Internal State': '\xc3\x89tat interne',
'Invalid Query': 'Requ\xc3\xaate Invalide',
'Invalid email': 'Courriel invalide',
'Last name': 'Nom',
'Layout': 'Mise en page',
'Login': 'Connectez-vous',
'Lost Password': 'Mot de passe perdu',
'Main Menu': 'Menu principal',
'Menu Model': 'Menu mod\xc3\xa8le',
'Name': 'Nom',
'New Record': 'Nouvel enregistrement',
'No databases in this application': "Cette application n'a pas de bases de donn\xc3\xa9es",
'Origin': 'Origine',
'Password': 'Mot de passe',
"Password fields don't match": 'Les mots de passe ne correspondent pas',
'Powered by': 'Powered par',
'Query:': 'Requ\xc3\xaate:',
'Record ID': 'Record ID',
'Register': "S'inscrire",
'Registration key': "Cl\xc3\xa9 d'enregistrement",
'Remember me (for 30 days)': 'Se souvenir de moi (pendant 30 jours)',
'Request reset password': 'Demande de r\xc3\xa9initialiser le mot cl\xc3\xa9',
'Reset Password key': 'R\xc3\xa9initialiser le mot cl\xc3\xa9',
'Role': 'R\xc3\xb4le',
'Rows in table': 'Lignes du tableau',
'Rows selected': 'Lignes s\xc3\xa9lectionn\xc3\xa9es',
'Stylesheet': 'Feuille de style',
'Submit': 'Soumettre',
'Sure you want to delete this object?': 'Souhaitez-vous vraiment effacer cet objet ?',
'Table name': 'Nom du tableau',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.',
'Timestamp': 'Timestamp',
'Update:': 'Mise \xc3\xa0 jour:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.',
'User %(id)s Logged-in': 'Utilisateur %(id)s connect\xc3\xa9',
'User %(id)s Registered': 'Utilisateur %(id)s enregistr\xc3\xa9',
'User ID': 'ID utilisateur',
'Verify Password': 'V\xc3\xa9rifiez le mot de passe',
'View': 'Pr\xc3\xa9sentation',
'Welcome %s': 'Bienvenue %s',
'Welcome to web2py': 'Bienvenue sur web2py',
'appadmin is disabled because insecure channel': 'appadmin is disabled because insecure channel',
'cache': 'cache',
'change password': 'changer le mot de passe',
'click here for online examples': 'cliquez ici pour voir des exemples enligne',
'click here for the administrative interface': "cliquez ici pour aller\xc3\xa0 l'interface d'administration",
'customize me!': 'me personnaliser!',
'data uploaded': 'donn\xc3\xa9es t\xc3\xa9l\xc3\xa9charg\xc3\xa9es',
'database': 'base de donn\xc3\xa9es',
'database %s select': 'base de donn\xc3\xa9es %s s\xc3\xa9lectionner',
'db': 'db',
'design': 'design',
'done!': 'fait!',
'edit profile': 'modifier le profil',
'export as csv file': 'exporter sous forme de fichier csv',
'insert new': 'ins\xc3\xa9rer un nouveau',
'insert new %s': 'ins\xc3\xa9rer un nouveau %s',
'invalid request': 'demande non valide',
'login': 'connectez-vous',
'logout': 'd\xc3\xa9connectez-vous',
'lost password': 'mot de passe perdu',
'new record inserted': 'nouvel enregistrement ins\xc3\xa9r\xc3\xa9',
'next 100 rows': '100 prochaines lignes',
'or import from csv file': "ou importer d'un fichier CSV",
'previous 100 rows': '100 lignes pr\xc3\xa9c\xc3\xa9dentes',
'record': 'enregistrement',
'record does not exist': "l'archive n'existe pas",
'record id': "id d'enregistrement",
'register': "s'inscrire",
'selected': 's\xc3\xa9lectionn\xc3\xa9',
'state': '\xc3\xa9tat',
'table': 'tableau',
'unable to parse csv file': "incapable d'analyser le fichier cvs",
}

########NEW FILE########
__FILENAME__ = fr-fr
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': '%s rang\xc3\xa9es effac\xc3\xa9es',
'%s rows updated': '%s rang\xc3\xa9es mises \xc3\xa0 jour',
'Authentication': 'Authentication',
'Available databases and tables': 'Available databases and tables',
'Cannot be empty': 'Cannot be empty',
'Check to delete': 'Check to delete',
'Client IP': 'Client IP',
'Controller': 'Controller',
'Copyright': 'Copyright',
'Current request': 'Current request',
'Current response': 'Current response',
'Current session': 'Current session',
'DB Model': 'DB Model',
'Database': 'Database',
'Delete:': 'Delete:',
'Description': 'Description',
'E-mail': 'E-mail',
'Edit': 'Edit',
'Edit This App': 'Edit This App',
'Edit current record': 'Edit current record',
'First name': 'First name',
'Group ID': 'Group ID',
'Hello World': 'Bonjour Monde',
'Import/Export': 'Import/Export',
'Index': 'Index',
'Internal State': 'Internal State',
'Invalid Query': 'Requ\xc3\xaate Invalide',
'Invalid email': 'Invalid email',
'Last name': 'Last name',
'Layout': 'Layout',
'Login': 'Login',
'Lost Password': 'Lost Password',
'Main Menu': 'Main Menu',
'Menu Model': 'Menu Model',
'Name': 'Name',
'New Record': 'New Record',
'No databases in this application': 'No databases in this application',
'Origin': 'Origin',
'Password': 'Password',
'Powered by': 'Powered by',
'Query:': 'Query:',
'Record ID': 'Record ID',
'Register': 'Register',
'Registration key': 'Registration key',
'Role': 'Role',
'Rows in table': 'Rows in table',
'Rows selected': 'Rows selected',
'Stylesheet': 'Stylesheet',
'Sure you want to delete this object?': 'Souhaitez vous vraiment effacercet objet?',
'Table name': 'Table name',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.',
'Timestamp': 'Timestamp',
'Update:': 'Update:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.',
'User ID': 'User ID',
'View': 'View',
'Welcome %s': 'Welcome %s',
'Welcome to web2py': 'Bienvenue sur web2py',
'appadmin is disabled because insecure channel': 'appadmin is disabled because insecure channel',
'cache': 'cache',
'change password': 'change password',
'click here for online examples': 'cliquez ici pour voir des exemples enligne',
'click here for the administrative interface': "cliquez ici pour aller\xc3\xa0 l'interface d'administration",
'customize me!': 'customize me!',
'data uploaded': 'donn\xc3\xa9es t\xc3\xa9l\xc3\xa9charg\xc3\xa9es',
'database': 'database',
'database %s select': 'database %s select',
'db': 'db',
'design': 'design',
'done!': 'fait!',
'edit profile': 'edit profile',
'export as csv file': 'export as csv file',
'insert new': 'insert new',
'insert new %s': 'insert new %s',
'invalid request': 'requ\xc3\xaate invalide',
'login': 'login',
'logout': 'logout',
'new record inserted': 'nouvelle archive ins\xc3\xa9r\xc3\xa9e',
'next 100 rows': 'next 100 rows',
'or import from csv file': 'or import from csv file',
'previous 100 rows': 'previous 100 rows',
'record': 'record',
'record does not exist': "l'archive n'existe pas",
'record id': 'record id',
'register': 'register',
'selected': 'selected',
'state': '\xc3\xa9tat',
'table': 'table',
'unable to parse csv file': "incapable d'analyser le fichier cvs",
}

########NEW FILE########
__FILENAME__ = hi-hi
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': '%s \xe0\xa4\xaa\xe0\xa4\x82\xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\x81 \xe0\xa4\xae\xe0\xa4\xbf\xe0\xa4\x9f\xe0\xa4\xbe\xe0\xa4\x8f\xe0\xa4\x81',
'%s rows updated': '%s \xe0\xa4\xaa\xe0\xa4\x82\xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\x81  \xe0\xa4\x85\xe0\xa4\xa6\xe0\xa5\x8d\xe0\xa4\xaf\xe0\xa4\xa4\xe0\xa4\xa8',
'Available databases and tables': '\xe0\xa4\x89\xe0\xa4\xaa\xe0\xa4\xb2\xe0\xa4\xac\xe0\xa5\x8d\xe0\xa4\xa7  \xe0\xa4\xa1\xe0\xa5\x87\xe0\xa4\x9f\xe0\xa4\xbe\xe0\xa4\xac\xe0\xa5\x87\xe0\xa4\xb8 \xe0\xa4\x94\xe0\xa4\xb0 \xe0\xa4\xa4\xe0\xa4\xbe\xe0\xa4\xb2\xe0\xa4\xbf\xe0\xa4\x95\xe0\xa4\xbe',
'Cannot be empty': '\xe0\xa4\x96\xe0\xa4\xbe\xe0\xa4\xb2\xe0\xa5\x80 \xe0\xa4\xa8\xe0\xa4\xb9\xe0\xa5\x80\xe0\xa4\x82 \xe0\xa4\xb9\xe0\xa5\x8b \xe0\xa4\xb8\xe0\xa4\x95\xe0\xa4\xa4\xe0\xa4\xbe',
'Change Password': '\xe0\xa4\xaa\xe0\xa4\xbe\xe0\xa4\xb8\xe0\xa4\xb5\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa1 \xe0\xa4\xac\xe0\xa4\xa6\xe0\xa4\xb2\xe0\xa5\x87\xe0\xa4\x82',
'Check to delete': '\xe0\xa4\xb9\xe0\xa4\x9f\xe0\xa4\xbe\xe0\xa4\xa8\xe0\xa5\x87 \xe0\xa4\x95\xe0\xa5\x87 \xe0\xa4\xb2\xe0\xa4\xbf\xe0\xa4\x8f \xe0\xa4\x9a\xe0\xa5\x81\xe0\xa4\xa8\xe0\xa5\x87\xe0\xa4\x82',
'Controller': 'Controller',
'Copyright': 'Copyright',
'Current request': '\xe0\xa4\xb5\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xae\xe0\xa4\xbe\xe0\xa4\xa8 \xe0\xa4\x85\xe0\xa4\xa8\xe0\xa5\x81\xe0\xa4\xb0\xe0\xa5\x8b\xe0\xa4\xa7',
'Current response': '\xe0\xa4\xb5\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xae\xe0\xa4\xbe\xe0\xa4\xa8 \xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xbf\xe0\xa4\xaf\xe0\xa4\xbe',
'Current session': '\xe0\xa4\xb5\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xae\xe0\xa4\xbe\xe0\xa4\xa8 \xe0\xa4\xb8\xe0\xa5\x87\xe0\xa4\xb6\xe0\xa4\xa8',
'DB Model': 'DB Model',
'Database': 'Database',
'Delete:': '\xe0\xa4\xae\xe0\xa4\xbf\xe0\xa4\x9f\xe0\xa4\xbe\xe0\xa4\xa8\xe0\xa4\xbe:',
'Edit': 'Edit',
'Edit Profile': '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa5\x8b\xe0\xa4\xab\xe0\xa4\xbc\xe0\xa4\xbe\xe0\xa4\x87\xe0\xa4\xb2 \xe0\xa4\xb8\xe0\xa4\x82\xe0\xa4\xaa\xe0\xa4\xbe\xe0\xa4\xa6\xe0\xa4\xbf\xe0\xa4\xa4 \xe0\xa4\x95\xe0\xa4\xb0\xe0\xa5\x87\xe0\xa4\x82',
'Edit This App': 'Edit This App',
'Edit current record': '\xe0\xa4\xb5\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xae\xe0\xa4\xbe\xe0\xa4\xa8 \xe0\xa4\xb0\xe0\xa5\x87\xe0\xa4\x95\xe0\xa5\x89\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa1 \xe0\xa4\xb8\xe0\xa4\x82\xe0\xa4\xaa\xe0\xa4\xbe\xe0\xa4\xa6\xe0\xa4\xbf\xe0\xa4\xa4 \xe0\xa4\x95\xe0\xa4\xb0\xe0\xa5\x87\xe0\xa4\x82 ',
'Hello World': 'Hello World',
'Hello from MyApp': 'Hello from MyApp',
'Import/Export': '\xe0\xa4\x86\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\xa4 / \xe0\xa4\xa8\xe0\xa4\xbf\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\xa4',
'Index': 'Index',
'Internal State': '\xe0\xa4\x86\xe0\xa4\x82\xe0\xa4\xa4\xe0\xa4\xb0\xe0\xa4\xbf\xe0\xa4\x95  \xe0\xa4\xb8\xe0\xa5\x8d\xe0\xa4\xa5\xe0\xa4\xbf\xe0\xa4\xa4\xe0\xa4\xbf',
'Invalid Query': '\xe0\xa4\x85\xe0\xa4\xae\xe0\xa4\xbe\xe0\xa4\xa8\xe0\xa5\x8d\xe0\xa4\xaf  \xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xb6\xe0\xa5\x8d\xe0\xa4\xa8',
'Layout': 'Layout',
'Login': '\xe0\xa4\xb2\xe0\xa5\x89\xe0\xa4\x97 \xe0\xa4\x87\xe0\xa4\xa8',
'Logout': '\xe0\xa4\xb2\xe0\xa5\x89\xe0\xa4\x97 \xe0\xa4\x86\xe0\xa4\x89\xe0\xa4\x9f',
'Lost Password': '\xe0\xa4\xaa\xe0\xa4\xbe\xe0\xa4\xb8\xe0\xa4\xb5\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa1 \xe0\xa4\x96\xe0\xa5\x8b \xe0\xa4\x97\xe0\xa4\xaf\xe0\xa4\xbe',
'Main Menu': 'Main Menu',
'Menu Model': 'Menu Model',
'New Record': '\xe0\xa4\xa8\xe0\xa4\xaf\xe0\xa4\xbe \xe0\xa4\xb0\xe0\xa5\x87\xe0\xa4\x95\xe0\xa5\x89\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa1',
'No databases in this application': '\xe0\xa4\x87\xe0\xa4\xb8  \xe0\xa4\x85\xe0\xa4\xa8\xe0\xa5\x81\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xaf\xe0\xa5\x8b\xe0\xa4\x97 \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82 \xe0\xa4\x95\xe0\xa5\x8b\xe0\xa4\x88 \xe0\xa4\xa1\xe0\xa5\x87\xe0\xa4\x9f\xe0\xa4\xbe\xe0\xa4\xac\xe0\xa5\x87\xe0\xa4\xb8 \xe0\xa4\xa8\xe0\xa4\xb9\xe0\xa5\x80\xe0\xa4\x82 \xe0\xa4\xb9\xe0\xa5\x88\xe0\xa4\x82',
'Powered by': 'Powered by',
'Query:': '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xb6\xe0\xa5\x8d\xe0\xa4\xa8:',
'Register': '\xe0\xa4\xaa\xe0\xa4\x82\xe0\xa4\x9c\xe0\xa5\x80\xe0\xa4\x95\xe0\xa5\x83\xe0\xa4\xa4 (\xe0\xa4\xb0\xe0\xa4\x9c\xe0\xa4\xbf\xe0\xa4\xb8\xe0\xa5\x8d\xe0\xa4\x9f\xe0\xa4\xb0) \xe0\xa4\x95\xe0\xa4\xb0\xe0\xa4\xa8\xe0\xa4\xbe ',
'Rows in table': '\xe0\xa4\xa4\xe0\xa4\xbe\xe0\xa4\xb2\xe0\xa4\xbf\xe0\xa4\x95\xe0\xa4\xbe \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82 \xe0\xa4\xaa\xe0\xa4\x82\xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\x81 ',
'Rows selected': '\xe0\xa4\x9a\xe0\xa4\xaf\xe0\xa4\xa8\xe0\xa4\xbf\xe0\xa4\xa4 (\xe0\xa4\x9a\xe0\xa5\x81\xe0\xa4\xa8\xe0\xa5\x87 \xe0\xa4\x97\xe0\xa4\xaf\xe0\xa5\x87) \xe0\xa4\xaa\xe0\xa4\x82\xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\x81 ',
'Stylesheet': 'Stylesheet',
'Sure you want to delete this object?': '\xe0\xa4\xb8\xe0\xa5\x81\xe0\xa4\xa8\xe0\xa4\xbf\xe0\xa4\xb6\xe0\xa5\x8d\xe0\xa4\x9a\xe0\xa4\xbf\xe0\xa4\xa4 \xe0\xa4\xb9\xe0\xa5\x88\xe0\xa4\x82 \xe0\xa4\x95\xe0\xa4\xbf \xe0\xa4\x86\xe0\xa4\xaa \xe0\xa4\x87\xe0\xa4\xb8 \xe0\xa4\xb5\xe0\xa4\xb8\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa5\x81 \xe0\xa4\x95\xe0\xa5\x8b \xe0\xa4\xb9\xe0\xa4\x9f\xe0\xa4\xbe\xe0\xa4\xa8\xe0\xa4\xbe \xe0\xa4\x9a\xe0\xa4\xbe\xe0\xa4\xb9\xe0\xa4\xa4\xe0\xa5\x87 \xe0\xa4\xb9\xe0\xa5\x88\xe0\xa4\x82?',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.',
'Update:': '\xe0\xa4\x85\xe0\xa4\xa6\xe0\xa5\x8d\xe0\xa4\xaf\xe0\xa4\xa4\xe0\xa4\xa8 \xe0\xa4\x95\xe0\xa4\xb0\xe0\xa4\xa8\xe0\xa4\xbe:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.',
'View': 'View',
'Welcome %s': 'Welcome %s',
'Welcome to web2py': '\xe0\xa4\xb5\xe0\xa5\x87\xe0\xa4\xac\xe0\xa5\xa8\xe0\xa4\xaa\xe0\xa4\xbe\xe0\xa4\x87 (web2py)  \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82 \xe0\xa4\x86\xe0\xa4\xaa\xe0\xa4\x95\xe0\xa4\xbe \xe0\xa4\xb8\xe0\xa5\x8d\xe0\xa4\xb5\xe0\xa4\xbe\xe0\xa4\x97\xe0\xa4\xa4 \xe0\xa4\xb9\xe0\xa5\x88',
'appadmin is disabled because insecure channel': '\xe0\xa4\x85\xe0\xa4\xaa \xe0\xa4\x86\xe0\xa4\xa1\xe0\xa4\xae\xe0\xa4\xbf\xe0\xa4\xa8 (appadmin) \xe0\xa4\x85\xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xb7\xe0\xa4\xae \xe0\xa4\xb9\xe0\xa5\x88 \xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xaf\xe0\xa5\x8b\xe0\xa4\x82\xe0\xa4\x95\xe0\xa4\xbf \xe0\xa4\x85\xe0\xa4\xb8\xe0\xa5\x81\xe0\xa4\xb0\xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xb7\xe0\xa4\xbf\xe0\xa4\xa4 \xe0\xa4\x9a\xe0\xa5\x88\xe0\xa4\xa8\xe0\xa4\xb2',
'cache': 'cache',
'change password': 'change password',
'click here for online examples': '\xe0\xa4\x91\xe0\xa4\xa8\xe0\xa4\xb2\xe0\xa4\xbe\xe0\xa4\x87\xe0\xa4\xa8 \xe0\xa4\x89\xe0\xa4\xa6\xe0\xa4\xbe\xe0\xa4\xb9\xe0\xa4\xb0\xe0\xa4\xa3 \xe0\xa4\x95\xe0\xa5\x87 \xe0\xa4\xb2\xe0\xa4\xbf\xe0\xa4\x8f \xe0\xa4\xaf\xe0\xa4\xb9\xe0\xa4\xbe\xe0\xa4\x81 \xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xb2\xe0\xa4\xbf\xe0\xa4\x95 \xe0\xa4\x95\xe0\xa4\xb0\xe0\xa5\x87\xe0\xa4\x82',
'click here for the administrative interface': '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xb6\xe0\xa4\xbe\xe0\xa4\xb8\xe0\xa4\xa8\xe0\xa4\xbf\xe0\xa4\x95 \xe0\xa4\x87\xe0\xa4\x82\xe0\xa4\x9f\xe0\xa4\xb0\xe0\xa4\xab\xe0\xa5\x87\xe0\xa4\xb8 \xe0\xa4\x95\xe0\xa5\x87 \xe0\xa4\xb2\xe0\xa4\xbf\xe0\xa4\x8f \xe0\xa4\xaf\xe0\xa4\xb9\xe0\xa4\xbe\xe0\xa4\x81 \xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xb2\xe0\xa4\xbf\xe0\xa4\x95 \xe0\xa4\x95\xe0\xa4\xb0\xe0\xa5\x87\xe0\xa4\x82',
'customize me!': '\xe0\xa4\xae\xe0\xa5\x81\xe0\xa4\x9d\xe0\xa5\x87 \xe0\xa4\x85\xe0\xa4\xa8\xe0\xa5\x81\xe0\xa4\x95\xe0\xa5\x82\xe0\xa4\xb2\xe0\xa4\xbf\xe0\xa4\xa4 (\xe0\xa4\x95\xe0\xa4\xb8\xe0\xa5\x8d\xe0\xa4\x9f\xe0\xa4\xae\xe0\xa4\xbe\xe0\xa4\x87\xe0\xa4\x9c\xe0\xa4\xbc) \xe0\xa4\x95\xe0\xa4\xb0\xe0\xa5\x87\xe0\xa4\x82!',
'data uploaded': '\xe0\xa4\xa1\xe0\xa4\xbe\xe0\xa4\x9f\xe0\xa4\xbe \xe0\xa4\x85\xe0\xa4\xaa\xe0\xa4\xb2\xe0\xa5\x8b\xe0\xa4\xa1 \xe0\xa4\xb8\xe0\xa4\xae\xe0\xa5\x8d\xe0\xa4\xaa\xe0\xa4\xa8\xe0\xa5\x8d\xe0\xa4\xa8 ',
'database': '\xe0\xa4\xa1\xe0\xa5\x87\xe0\xa4\x9f\xe0\xa4\xbe\xe0\xa4\xac\xe0\xa5\x87\xe0\xa4\xb8',
'database %s select': '\xe0\xa4\xa1\xe0\xa5\x87\xe0\xa4\x9f\xe0\xa4\xbe\xe0\xa4\xac\xe0\xa5\x87\xe0\xa4\xb8  %s \xe0\xa4\x9a\xe0\xa5\x81\xe0\xa4\xa8\xe0\xa5\x80 \xe0\xa4\xb9\xe0\xa5\x81\xe0\xa4\x88',
'db': 'db',
'design': '\xe0\xa4\xb0\xe0\xa4\x9a\xe0\xa4\xa8\xe0\xa4\xbe \xe0\xa4\x95\xe0\xa4\xb0\xe0\xa5\x87\xe0\xa4\x82',
'done!': '\xe0\xa4\xb9\xe0\xa5\x8b \xe0\xa4\x97\xe0\xa4\xaf\xe0\xa4\xbe!',
'edit profile': 'edit profile',
'export as csv file': 'csv \xe0\xa4\xab\xe0\xa4\xbc\xe0\xa4\xbe\xe0\xa4\x87\xe0\xa4\xb2 \xe0\xa4\x95\xe0\xa5\x87 \xe0\xa4\xb0\xe0\xa5\x82\xe0\xa4\xaa \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82 \xe0\xa4\xa8\xe0\xa4\xbf\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\xa4',
'insert new': '\xe0\xa4\xa8\xe0\xa4\xaf\xe0\xa4\xbe \xe0\xa4\xa1\xe0\xa4\xbe\xe0\xa4\xb2\xe0\xa5\x87\xe0\xa4\x82',
'insert new %s': '\xe0\xa4\xa8\xe0\xa4\xaf\xe0\xa4\xbe   %s  \xe0\xa4\xa1\xe0\xa4\xbe\xe0\xa4\xb2\xe0\xa5\x87\xe0\xa4\x82',
'invalid request': '\xe0\xa4\x85\xe0\xa4\xb5\xe0\xa5\x88\xe0\xa4\xa7 \xe0\xa4\x85\xe0\xa4\xa8\xe0\xa5\x81\xe0\xa4\xb0\xe0\xa5\x8b\xe0\xa4\xa7',
'login': 'login',
'logout': 'logout',
'new record inserted': '\xe0\xa4\xa8\xe0\xa4\xaf\xe0\xa4\xbe \xe0\xa4\xb0\xe0\xa5\x87\xe0\xa4\x95\xe0\xa5\x89\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa1 \xe0\xa4\xa1\xe0\xa4\xbe\xe0\xa4\xb2\xe0\xa4\xbe',
'next 100 rows': '\xe0\xa4\x85\xe0\xa4\x97\xe0\xa4\xb2\xe0\xa5\x87 100 \xe0\xa4\xaa\xe0\xa4\x82\xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\x81',
'or import from csv file': '\xe0\xa4\xaf\xe0\xa4\xbe  csv \xe0\xa4\xab\xe0\xa4\xbc\xe0\xa4\xbe\xe0\xa4\x87\xe0\xa4\xb2 \xe0\xa4\xb8\xe0\xa5\x87 \xe0\xa4\x86\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\xa4',
'previous 100 rows': '\xe0\xa4\xaa\xe0\xa4\xbf\xe0\xa4\x9b\xe0\xa4\xb2\xe0\xa5\x87 100 \xe0\xa4\xaa\xe0\xa4\x82\xe0\xa4\x95\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xaf\xe0\xa4\xbe\xe0\xa4\x81',
'record': 'record',
'record does not exist': '\xe0\xa4\xb0\xe0\xa4\xbf\xe0\xa4\x95\xe0\xa5\x89\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa1 \xe0\xa4\xae\xe0\xa5\x8c\xe0\xa4\x9c\xe0\xa5\x82\xe0\xa4\xa6 \xe0\xa4\xa8\xe0\xa4\xb9\xe0\xa5\x80\xe0\xa4\x82 \xe0\xa4\xb9\xe0\xa5\x88',
'record id': '\xe0\xa4\xb0\xe0\xa4\xbf\xe0\xa4\x95\xe0\xa5\x89\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa1 \xe0\xa4\xaa\xe0\xa4\xb9\xe0\xa4\x9a\xe0\xa4\xbe\xe0\xa4\xa8\xe0\xa4\x95\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa4\xe0\xa4\xbe (\xe0\xa4\x86\xe0\xa4\x88\xe0\xa4\xa1\xe0\xa5\x80)',
'register': 'register',
'selected': '\xe0\xa4\x9a\xe0\xa5\x81\xe0\xa4\xa8\xe0\xa4\xbe \xe0\xa4\xb9\xe0\xa5\x81\xe0\xa4\x86',
'state': '\xe0\xa4\xb8\xe0\xa5\x8d\xe0\xa4\xa5\xe0\xa4\xbf\xe0\xa4\xa4\xe0\xa4\xbf',
'table': '\xe0\xa4\xa4\xe0\xa4\xbe\xe0\xa4\xb2\xe0\xa4\xbf\xe0\xa4\x95\xe0\xa4\xbe',
'unable to parse csv file': 'csv \xe0\xa4\xab\xe0\xa4\xbc\xe0\xa4\xbe\xe0\xa4\x87\xe0\xa4\xb2 \xe0\xa4\xaa\xe0\xa4\xbe\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xb8 \xe0\xa4\x95\xe0\xa4\xb0\xe0\xa4\xa8\xe0\xa5\x87 \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82 \xe0\xa4\x85\xe0\xa4\xb8\xe0\xa4\xae\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa5',
}

########NEW FILE########
__FILENAME__ = hu-hu
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN',
'%Y-%m-%d': '%Y.%m.%d.',
'%Y-%m-%d %H:%M:%S': '%Y.%m.%d. %H:%M:%S',
'%s rows deleted': '%s sorok t\xc3\xb6rl\xc5\x91dtek',
'%s rows updated': '%s sorok friss\xc3\xadt\xc5\x91dtek',
'Available databases and tables': 'El\xc3\xa9rhet\xc5\x91 adatb\xc3\xa1zisok \xc3\xa9s t\xc3\xa1bl\xc3\xa1k',
'Cannot be empty': 'Nem lehet \xc3\xbcres',
'Check to delete': 'T\xc3\xb6rl\xc3\xa9shez v\xc3\xa1laszd ki',
'Client IP': 'Client IP',
'Controller': 'Controller',
'Copyright': 'Copyright',
'Current request': 'Jelenlegi lek\xc3\xa9rdez\xc3\xa9s',
'Current response': 'Jelenlegi v\xc3\xa1lasz',
'Current session': 'Jelenlegi folyamat',
'DB Model': 'DB Model',
'Database': 'Adatb\xc3\xa1zis',
'Delete:': 'T\xc3\xb6r\xc3\xb6l:',
'Description': 'Description',
'E-mail': 'E-mail',
'Edit': 'Szerkeszt',
'Edit This App': 'Alkalmaz\xc3\xa1st szerkeszt',
'Edit current record': 'Aktu\xc3\xa1lis bejegyz\xc3\xa9s szerkeszt\xc3\xa9se',
'First name': 'First name',
'Group ID': 'Group ID',
'Hello World': 'Hello Vil\xc3\xa1g',
'Import/Export': 'Import/Export',
'Index': 'Index',
'Internal State': 'Internal State',
'Invalid Query': 'Hib\xc3\xa1s lek\xc3\xa9rdez\xc3\xa9s',
'Invalid email': 'Invalid email',
'Last name': 'Last name',
'Layout': 'Szerkezet',
'Main Menu': 'F\xc5\x91men\xc3\xbc',
'Menu Model': 'Men\xc3\xbc model',
'Name': 'Name',
'New Record': '\xc3\x9aj bejegyz\xc3\xa9s',
'No databases in this application': 'Nincs adatb\xc3\xa1zis ebben az alkalmaz\xc3\xa1sban',
'Origin': 'Origin',
'Password': 'Password',
'Powered by': 'Powered by',
'Query:': 'Lek\xc3\xa9rdez\xc3\xa9s:',
'Record ID': 'Record ID',
'Registration key': 'Registration key',
'Reset Password key': 'Reset Password key',
'Role': 'Role',
'Rows in table': 'Sorok a t\xc3\xa1bl\xc3\xa1ban',
'Rows selected': 'Kiv\xc3\xa1lasztott sorok',
'Stylesheet': 'Stylesheet',
'Sure you want to delete this object?': 'Biztos t\xc3\xb6rli ezt az objektumot?',
'Table name': 'Table name',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.',
'Timestamp': 'Timestamp',
'Update:': 'Friss\xc3\xadt:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.',
'User ID': 'User ID',
'View': 'N\xc3\xa9zet',
'Welcome %s': 'Welcome %s',
'Welcome to web2py': 'Isten hozott a web2py-ban',
'appadmin is disabled because insecure channel': 'az appadmin a biztons\xc3\xa1gtalan csatorna miatt letiltva',
'cache': 'gyors\xc3\xadt\xc3\xb3t\xc3\xa1r',
'change password': 'jelsz\xc3\xb3 megv\xc3\xa1ltoztat\xc3\xa1sa',
'click here for online examples': 'online p\xc3\xa9ld\xc3\xa1k\xc3\xa9rt kattints ide',
'click here for the administrative interface': 'az adminisztr\xc3\xa1ci\xc3\xb3s fel\xc3\xbclet\xc3\xa9rt kattints ide',
'customize me!': 'v\xc3\xa1ltoztass meg!',
'data uploaded': 'adat felt\xc3\xb6ltve',
'database': 'adatb\xc3\xa1zis',
'database %s select': 'adatb\xc3\xa1zis %s kiv\xc3\xa1laszt\xc3\xa1s',
'db': 'db',
'design': 'design',
'done!': 'k\xc3\xa9sz!',
'edit profile': 'profil szerkeszt\xc3\xa9se',
'export as csv file': 'export\xc3\xa1l csv f\xc3\xa1jlba',
'insert new': '\xc3\xbaj beilleszt\xc3\xa9se',
'insert new %s': '\xc3\xbaj beilleszt\xc3\xa9se %s',
'invalid request': 'hib\xc3\xa1s k\xc3\xa9r\xc3\xa9s',
'login': 'bel\xc3\xa9p',
'logout': 'kil\xc3\xa9p',
'lost password': 'elveszett jelsz\xc3\xb3',
'new record inserted': '\xc3\xbaj bejegyz\xc3\xa9s felv\xc3\xa9ve',
'next 100 rows': 'k\xc3\xb6vetkez\xc5\x91 100 sor',
'or import from csv file': 'vagy bet\xc3\xb6lt\xc3\xa9s csv f\xc3\xa1jlb\xc3\xb3l',
'previous 100 rows': 'el\xc5\x91z\xc5\x91 100 sor',
'record': 'bejegyz\xc3\xa9s',
'record does not exist': 'bejegyz\xc3\xa9s nem l\xc3\xa9tezik',
'record id': 'bejegyz\xc3\xa9s id',
'register': 'regisztr\xc3\xa1ci\xc3\xb3',
'selected': 'kiv\xc3\xa1lasztott',
'state': '\xc3\xa1llapot',
'table': 't\xc3\xa1bla',
'unable to parse csv file': 'nem lehet a csv f\xc3\xa1jlt beolvasni',
}

########NEW FILE########
__FILENAME__ = hu
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN',
'%Y-%m-%d': '%Y.%m.%d.',
'%Y-%m-%d %H:%M:%S': '%Y.%m.%d. %H:%M:%S',
'%s rows deleted': '%s sorok t\xc3\xb6rl\xc5\x91dtek',
'%s rows updated': '%s sorok friss\xc3\xadt\xc5\x91dtek',
'Available databases and tables': 'El\xc3\xa9rhet\xc5\x91 adatb\xc3\xa1zisok \xc3\xa9s t\xc3\xa1bl\xc3\xa1k',
'Cannot be empty': 'Nem lehet \xc3\xbcres',
'Check to delete': 'T\xc3\xb6rl\xc3\xa9shez v\xc3\xa1laszd ki',
'Client IP': 'Client IP',
'Controller': 'Controller',
'Copyright': 'Copyright',
'Current request': 'Jelenlegi lek\xc3\xa9rdez\xc3\xa9s',
'Current response': 'Jelenlegi v\xc3\xa1lasz',
'Current session': 'Jelenlegi folyamat',
'DB Model': 'DB Model',
'Database': 'Adatb\xc3\xa1zis',
'Delete:': 'T\xc3\xb6r\xc3\xb6l:',
'Description': 'Description',
'E-mail': 'E-mail',
'Edit': 'Szerkeszt',
'Edit This App': 'Alkalmaz\xc3\xa1st szerkeszt',
'Edit current record': 'Aktu\xc3\xa1lis bejegyz\xc3\xa9s szerkeszt\xc3\xa9se',
'First name': 'First name',
'Group ID': 'Group ID',
'Hello World': 'Hello Vil\xc3\xa1g',
'Import/Export': 'Import/Export',
'Index': 'Index',
'Internal State': 'Internal State',
'Invalid Query': 'Hib\xc3\xa1s lek\xc3\xa9rdez\xc3\xa9s',
'Invalid email': 'Invalid email',
'Last name': 'Last name',
'Layout': 'Szerkezet',
'Main Menu': 'F\xc5\x91men\xc3\xbc',
'Menu Model': 'Men\xc3\xbc model',
'Name': 'Name',
'New Record': '\xc3\x9aj bejegyz\xc3\xa9s',
'No databases in this application': 'Nincs adatb\xc3\xa1zis ebben az alkalmaz\xc3\xa1sban',
'Origin': 'Origin',
'Password': 'Password',
'Powered by': 'Powered by',
'Query:': 'Lek\xc3\xa9rdez\xc3\xa9s:',
'Record ID': 'Record ID',
'Registration key': 'Registration key',
'Reset Password key': 'Reset Password key',
'Role': 'Role',
'Rows in table': 'Sorok a t\xc3\xa1bl\xc3\xa1ban',
'Rows selected': 'Kiv\xc3\xa1lasztott sorok',
'Stylesheet': 'Stylesheet',
'Sure you want to delete this object?': 'Biztos t\xc3\xb6rli ezt az objektumot?',
'Table name': 'Table name',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.',
'Timestamp': 'Timestamp',
'Update:': 'Friss\xc3\xadt:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.',
'User ID': 'User ID',
'View': 'N\xc3\xa9zet',
'Welcome %s': 'Welcome %s',
'Welcome to web2py': 'Isten hozott a web2py-ban',
'appadmin is disabled because insecure channel': 'az appadmin a biztons\xc3\xa1gtalan csatorna miatt letiltva',
'cache': 'gyors\xc3\xadt\xc3\xb3t\xc3\xa1r',
'change password': 'jelsz\xc3\xb3 megv\xc3\xa1ltoztat\xc3\xa1sa',
'click here for online examples': 'online p\xc3\xa9ld\xc3\xa1k\xc3\xa9rt kattints ide',
'click here for the administrative interface': 'az adminisztr\xc3\xa1ci\xc3\xb3s fel\xc3\xbclet\xc3\xa9rt kattints ide',
'customize me!': 'v\xc3\xa1ltoztass meg!',
'data uploaded': 'adat felt\xc3\xb6ltve',
'database': 'adatb\xc3\xa1zis',
'database %s select': 'adatb\xc3\xa1zis %s kiv\xc3\xa1laszt\xc3\xa1s',
'db': 'db',
'design': 'design',
'done!': 'k\xc3\xa9sz!',
'edit profile': 'profil szerkeszt\xc3\xa9se',
'export as csv file': 'export\xc3\xa1l csv f\xc3\xa1jlba',
'insert new': '\xc3\xbaj beilleszt\xc3\xa9se',
'insert new %s': '\xc3\xbaj beilleszt\xc3\xa9se %s',
'invalid request': 'hib\xc3\xa1s k\xc3\xa9r\xc3\xa9s',
'login': 'bel\xc3\xa9p',
'logout': 'kil\xc3\xa9p',
'lost password': 'elveszett jelsz\xc3\xb3',
'new record inserted': '\xc3\xbaj bejegyz\xc3\xa9s felv\xc3\xa9ve',
'next 100 rows': 'k\xc3\xb6vetkez\xc5\x91 100 sor',
'or import from csv file': 'vagy bet\xc3\xb6lt\xc3\xa9s csv f\xc3\xa1jlb\xc3\xb3l',
'previous 100 rows': 'el\xc5\x91z\xc5\x91 100 sor',
'record': 'bejegyz\xc3\xa9s',
'record does not exist': 'bejegyz\xc3\xa9s nem l\xc3\xa9tezik',
'record id': 'bejegyz\xc3\xa9s id',
'register': 'regisztr\xc3\xa1ci\xc3\xb3',
'selected': 'kiv\xc3\xa1lasztott',
'state': '\xc3\xa1llapot',
'table': 't\xc3\xa1bla',
'unable to parse csv file': 'nem lehet a csv f\xc3\xa1jlt beolvasni',
}

########NEW FILE########
__FILENAME__ = it-it
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update"  un\'espressione opzionale come "campo1=\'nuovo valore\'". Non si pu fare "update" o "delete" dei risultati di un JOIN ',
'%Y-%m-%d': '%d/%m/%Y',
'%Y-%m-%d %H:%M:%S': '%d/%m/%Y %H:%M:%S',
'%s rows deleted': '%s righe ("record") cancellate',
'%s rows updated': '%s righe ("record") modificate',
'Available databases and tables': 'Database e tabelle disponibili',
'Cannot be empty': 'Non pu essere vuoto',
'Check to delete': 'Seleziona per cancellare',
'Client IP': 'Client IP',
'Controller': 'Controller',
'Copyright': 'Copyright',
'Current request': 'Richiesta (request) corrente',
'Current response': 'Risposta (response) corrente',
'Current session': 'Sessione (session) corrente',
'DB Model': 'Modello di DB',
'Database': 'Database',
'Delete:': 'Cancella:',
'Description': 'Descrizione',
'E-mail': 'E-mail',
'Edit': 'Modifica',
'Edit This App': 'Modifica questa applicazione',
'Edit current record': 'Modifica record corrente',
'First name': 'Nome',
'Group ID': 'ID Gruppo',
'Hello World': 'Salve Mondo',
'Hello World in a flash!': 'Salve Mondo in un flash!',
'Import/Export': 'Importa/Esporta',
'Index': 'Indice',
'Internal State': 'Stato interno',
'Invalid Query': 'Richiesta (query) non valida',
'Invalid email': 'Email non valida',
'Last name': 'Cognome',
'Layout': 'Layout',
'Main Menu': 'Menu principale',
'Menu Model': 'Menu Modelli',
'Name': 'Nome',
'New Record': 'Nuovo elemento (record)',
'No databases in this application': 'Nessun database presente in questa applicazione',
'Origin': 'Origine',
'Password': 'Password',
'Powered by': 'Powered by',
'Query:': 'Richiesta (query):',
'Record ID': 'Record ID',
'Registration key': 'Chiave di Registazione',
'Reset Password key': 'Resetta chiave Password ',
'Role': 'Ruolo',
'Rows in table': 'Righe nella tabella',
'Rows selected': 'Righe selezionate',
'Stylesheet': 'Foglio di stile (stylesheet)',
'Sure you want to delete this object?': 'Vuoi veramente cancellare questo oggetto?',
'Table name': 'Nome tabella',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'La richiesta (query)  una condizione come ad esempio  "db.tabella1.campo1==\'valore\'". Una condizione come "db.tabella1.campo1==db.tabella2.campo2" produce un "JOIN" SQL.',
'The output of the file is a dictionary that was rendered by the view': 'L\'output del file  un "dictionary" che  stato visualizzato dalla vista',
'This is a copy of the scaffolding application': "Questa  una copia dell'applicazione di base (scaffold)",
'Timestamp': 'Ora (timestamp)',
'Update:': 'Aggiorna:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Per costruire richieste (query) pi complesse si usano (...)&(...) come "e" (AND), (...)|(...) come "o" (OR), e ~(...) come negazione (NOT).',
'User ID': 'ID Utente',
'View': 'Vista',
'Welcome %s': 'Benvenuto %s',
'Welcome to web2py': 'Benvenuto su web2py',
'Which called the function': 'che ha chiamato la funzione',
'You are successfully running web2py': 'Stai eseguendo web2py con successo',
'You can modify this application and adapt it to your needs': 'Puoi modificare questa applicazione adattandola alle tue necessit',
'You visited the url': "Hai visitato l'URL",
'appadmin is disabled because insecure channel': 'Amministrazione (appadmin) disabilitata: comunicazione non sicura',
'cache': 'cache',
'change password': 'Cambia password',
'click here for online examples': 'clicca per vedere gli esempi',
'click here for the administrative interface': "clicca per l'interfaccia amministrativa",
'customize me!': 'Personalizzami!',
'data uploaded': 'dati caricati',
'database': 'database',
'database %s select': 'database %s select',
'db': 'db',
'design': 'progetta',
'documentation': 'documentazione',
'done!': 'fatto!',
'edit profile': 'modifica profilo',
'export as csv file': 'esporta come file CSV',
'hello world': 'salve mondo',
'insert new': 'inserisci nuovo',
'insert new %s': 'inserisci nuovo %s',
'invalid request': 'richiesta non valida',
'located in the file': 'presente nel file',
'login': 'accesso',
'logout': 'uscita',
'lost password?': 'dimenticato la password?',
'new record inserted': 'nuovo record inserito',
'next 100 rows': 'prossime 100 righe',
'not authorized': 'non autorizzato',
'or import from csv file': 'oppure importa da file CSV',
'previous 100 rows': '100 righe precedenti',
'record': 'record',
'record does not exist': 'il record non esiste',
'record id': 'record id',
'register': 'registrazione',
'selected': 'selezionato',
'state': 'stato',
'table': 'tabella',
'unable to parse csv file': 'non riesco a decodificare questo file CSV',
}

########NEW FILE########
__FILENAME__ = it
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update"  un\'espressione opzionale come "campo1=\'nuovo valore\'". Non si pu fare "update" o "delete" dei risultati di un JOIN ',
'%Y-%m-%d': '%d/%m/%Y',
'%Y-%m-%d %H:%M:%S': '%d/%m/%Y %H:%M:%S',
'%s rows deleted': '%s righe ("record") cancellate',
'%s rows updated': '%s righe ("record") modificate',
'Available databases and tables': 'Database e tabelle disponibili',
'Cannot be empty': 'Non pu essere vuoto',
'Check to delete': 'Seleziona per cancellare',
'Client IP': 'Client IP',
'Controller': 'Controller',
'Copyright': 'Copyright',
'Current request': 'Richiesta (request) corrente',
'Current response': 'Risposta (response) corrente',
'Current session': 'Sessione (session) corrente',
'DB Model': 'Modello di DB',
'Database': 'Database',
'Delete:': 'Cancella:',
'Description': 'Descrizione',
'E-mail': 'E-mail',
'Edit': 'Modifica',
'Edit This App': 'Modifica questa applicazione',
'Edit current record': 'Modifica record corrente',
'First name': 'Nome',
'Group ID': 'ID Gruppo',
'Hello World': 'Salve Mondo',
'Hello World in a flash!': 'Salve Mondo in un flash!',
'Import/Export': 'Importa/Esporta',
'Index': 'Indice',
'Internal State': 'Stato interno',
'Invalid Query': 'Richiesta (query) non valida',
'Invalid email': 'Email non valida',
'Last name': 'Cognome',
'Layout': 'Layout',
'Main Menu': 'Menu principale',
'Menu Model': 'Menu Modelli',
'Name': 'Nome',
'New Record': 'Nuovo elemento (record)',
'No databases in this application': 'Nessun database presente in questa applicazione',
'Origin': 'Origine',
'Password': 'Password',
'Powered by': 'Powered by',
'Query:': 'Richiesta (query):',
'Record ID': 'Record ID',
'Registration key': 'Chiave di Registazione',
'Reset Password key': 'Resetta chiave Password ',
'Role': 'Ruolo',
'Rows in table': 'Righe nella tabella',
'Rows selected': 'Righe selezionate',
'Stylesheet': 'Foglio di stile (stylesheet)',
'Sure you want to delete this object?': 'Vuoi veramente cancellare questo oggetto?',
'Table name': 'Nome tabella',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'La richiesta (query)  una condizione come ad esempio  "db.tabella1.campo1==\'valore\'". Una condizione come "db.tabella1.campo1==db.tabella2.campo2" produce un "JOIN" SQL.',
'The output of the file is a dictionary that was rendered by the view': 'L\'output del file  un "dictionary" che  stato visualizzato dalla vista',
'This is a copy of the scaffolding application': "Questa  una copia dell'applicazione di base (scaffold)",
'Timestamp': 'Ora (timestamp)',
'Update:': 'Aggiorna:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Per costruire richieste (query) pi complesse si usano (...)&(...) come "e" (AND), (...)|(...) come "o" (OR), e ~(...) come negazione (NOT).',
'User ID': 'ID Utente',
'View': 'Vista',
'Welcome %s': 'Benvenuto %s',
'Welcome to web2py': 'Benvenuto su web2py',
'Which called the function': 'che ha chiamato la funzione',
'You are successfully running web2py': 'Stai eseguendo web2py con successo',
'You can modify this application and adapt it to your needs': 'Puoi modificare questa applicazione adattandola alle tue necessit',
'You visited the url': "Hai visitato l'URL",
'appadmin is disabled because insecure channel': 'Amministrazione (appadmin) disabilitata: comunicazione non sicura',
'cache': 'cache',
'change password': 'Cambia password',
'click here for online examples': 'clicca per vedere gli esempi',
'click here for the administrative interface': "clicca per l'interfaccia amministrativa",
'customize me!': 'Personalizzami!',
'data uploaded': 'dati caricati',
'database': 'database',
'database %s select': 'database %s select',
'db': 'db',
'design': 'progetta',
'documentation': 'documentazione',
'done!': 'fatto!',
'edit profile': 'modifica profilo',
'export as csv file': 'esporta come file CSV',
'hello world': 'salve mondo',
'insert new': 'inserisci nuovo',
'insert new %s': 'inserisci nuovo %s',
'invalid request': 'richiesta non valida',
'located in the file': 'presente nel file',
'login': 'accesso',
'logout': 'uscita',
'lost password?': 'dimenticato la password?',
'new record inserted': 'nuovo record inserito',
'next 100 rows': 'prossime 100 righe',
'not authorized': 'non autorizzato',
'or import from csv file': 'oppure importa da file CSV',
'previous 100 rows': '100 righe precedenti',
'record': 'record',
'record does not exist': 'il record non esiste',
'record id': 'record id',
'register': 'registrazione',
'selected': 'selezionato',
'state': 'stato',
'table': 'tabella',
'unable to parse csv file': 'non riesco a decodificare questo file CSV',
}

########NEW FILE########
__FILENAME__ = pl-pl
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"Uaktualnij" jest dodatkowym wyra\xc5\xbceniem postaci "pole1=\'nowawarto\xc5\x9b\xc4\x87\'". Nie mo\xc5\xbcesz uaktualni\xc4\x87 lub usun\xc4\x85\xc4\x87 wynik\xc3\xb3w z JOIN:',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': 'Wierszy usuni\xc4\x99tych: %s',
'%s rows updated': 'Wierszy uaktualnionych: %s',
'Available databases and tables': 'Dost\xc4\x99pne bazy danych i tabele',
'Cannot be empty': 'Nie mo\xc5\xbce by\xc4\x87 puste',
'Change Password': 'Change Password',
'Check to delete': 'Zaznacz aby usun\xc4\x85\xc4\x87',
'Controller': 'Controller',
'Copyright': 'Copyright',
'Current request': 'Aktualne \xc5\xbc\xc4\x85danie',
'Current response': 'Aktualna odpowied\xc5\xba',
'Current session': 'Aktualna sesja',
'DB Model': 'DB Model',
'Database': 'Database',
'Delete:': 'Usu\xc5\x84:',
'Edit': 'Edit',
'Edit Profile': 'Edit Profile',
'Edit This App': 'Edit This App',
'Edit current record': 'Edytuj aktualny rekord',
'Hello World': 'Witaj \xc5\x9awiecie',
'Import/Export': 'Importuj/eksportuj',
'Index': 'Index',
'Internal State': 'Stan wewn\xc4\x99trzny',
'Invalid Query': 'B\xc5\x82\xc4\x99dne zapytanie',
'Layout': 'Layout',
'Login': 'Zaloguj',
'Logout': 'Logout',
'Lost Password': 'Przypomnij has\xc5\x82o',
'Main Menu': 'Main Menu',
'Menu Model': 'Menu Model',
'New Record': 'Nowy rekord',
'No databases in this application': 'Brak baz danych w tej aplikacji',
'Powered by': 'Powered by',
'Query:': 'Zapytanie:',
'Register': 'Zarejestruj',
'Rows in table': 'Wiersze w tabeli',
'Rows selected': 'Wybrane wiersze',
'Stylesheet': 'Stylesheet',
'Sure you want to delete this object?': 'Czy na pewno chcesz usun\xc4\x85\xc4\x87 ten obiekt?',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': '"Zapytanie" jest warunkiem postaci "db.tabela1.pole1==\'warto\xc5\x9b\xc4\x87\'". Takie co\xc5\x9b jak "db.tabela1.pole1==db.tabela2.pole2" oznacza SQL JOIN.',
'Update:': 'Uaktualnij:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'U\xc5\xbcyj (...)&(...) jako AND, (...)|(...) jako OR oraz ~(...)  jako NOT do tworzenia bardziej skomplikowanych zapyta\xc5\x84.',
'View': 'View',
'Welcome %s': 'Welcome %s',
'Welcome to web2py': 'Witaj w web2py',
'appadmin is disabled because insecure channel': 'appadmin is disabled because insecure channel',
'cache': 'cache',
'change password': 'change password',
'click here for online examples': 'Kliknij aby przej\xc5\x9b\xc4\x87 do interaktywnych przyk\xc5\x82ad\xc3\xb3w',
'click here for the administrative interface': 'Kliknij aby przej\xc5\x9b\xc4\x87 do panelu administracyjnego',
'customize me!': 'dostosuj mnie!',
'data uploaded': 'dane wys\xc5\x82ane',
'database': 'baza danych',
'database %s select': 'wyb\xc3\xb3r z bazy danych %s',
'db': 'baza danych',
'design': 'projektuj',
'done!': 'zrobione!',
'edit profile': 'edit profile',
'export as csv file': 'eksportuj jako plik csv',
'insert new': 'wstaw nowy rekord tabeli',
'insert new %s': 'wstaw nowy rekord do tabeli %s',
'invalid request': 'B\xc5\x82\xc4\x99dne \xc5\xbc\xc4\x85danie',
'login': 'login',
'logout': 'logout',
'new record inserted': 'nowy rekord zosta\xc5\x82 wstawiony',
'next 100 rows': 'nast\xc4\x99pne 100 wierszy',
'or import from csv file': 'lub zaimportuj z pliku csv',
'previous 100 rows': 'poprzednie 100 wierszy',
'record': 'record',
'record does not exist': 'rekord nie istnieje',
'record id': 'id rekordu',
'register': 'register',
'selected': 'wybranych',
'state': 'stan',
'table': 'tabela',
'unable to parse csv file': 'nie mo\xc5\xbcna sparsowa\xc4\x87 pliku csv',
}

########NEW FILE########
__FILENAME__ = pl
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"Uaktualnij" jest dodatkowym wyra\xc5\xbceniem postaci "pole1=\'nowawarto\xc5\x9b\xc4\x87\'". Nie mo\xc5\xbcesz uaktualni\xc4\x87 lub usun\xc4\x85\xc4\x87 wynik\xc3\xb3w z JOIN:',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': 'Wierszy usuni\xc4\x99tych: %s',
'%s rows updated': 'Wierszy uaktualnionych: %s',
'Authentication': 'Uwierzytelnienie',
'Available databases and tables': 'Dost\xc4\x99pne bazy danych i tabele',
'Cannot be empty': 'Nie mo\xc5\xbce by\xc4\x87 puste',
'Change Password': 'Zmie\xc5\x84 has\xc5\x82o',
'Check to delete': 'Zaznacz aby usun\xc4\x85\xc4\x87',
'Check to delete:': 'Zaznacz aby usun\xc4\x85\xc4\x87:',
'Client IP': 'IP klienta',
'Controller': 'Kontroler',
'Copyright': 'Copyright',
'Current request': 'Aktualne \xc5\xbc\xc4\x85danie',
'Current response': 'Aktualna odpowied\xc5\xba',
'Current session': 'Aktualna sesja',
'DB Model': 'Model bazy danych',
'Database': 'Baza danych',
'Delete:': 'Usu\xc5\x84:',
'Description': 'Opis',
'E-mail': 'Adres e-mail',
'Edit': 'Edycja',
'Edit Profile': 'Edytuj profil',
'Edit This App': 'Edytuj t\xc4\x99 aplikacj\xc4\x99',
'Edit current record': 'Edytuj obecny rekord',
'First name': 'Imi\xc4\x99',
'Function disabled': 'Funkcja wy\xc5\x82\xc4\x85czona',
'Group ID': 'ID grupy',
'Hello World': 'Witaj \xc5\x9awiecie',
'Import/Export': 'Importuj/eksportuj',
'Index': 'Indeks',
'Internal State': 'Stan wewn\xc4\x99trzny',
'Invalid Query': 'B\xc5\x82\xc4\x99dne zapytanie',
'Invalid email': 'B\xc5\x82\xc4\x99dny adres email',
'Last name': 'Nazwisko',
'Layout': 'Uk\xc5\x82ad',
'Login': 'Zaloguj',
'Logout': 'Wyloguj',
'Lost Password': 'Przypomnij has\xc5\x82o',
'Main Menu': 'Menu g\xc5\x82\xc3\xb3wne',
'Menu Model': 'Model menu',
'Name': 'Nazwa',
'New Record': 'Nowy rekord',
'No databases in this application': 'Brak baz danych w tej aplikacji',
'Origin': '\xc5\xb9r\xc3\xb3d\xc5\x82o',
'Password': 'Has\xc5\x82o',
"Password fields don't match": 'Pola has\xc5\x82a nie s\xc4\x85 zgodne ze sob\xc4\x85',
'Powered by': 'Zasilane przez',
'Query:': 'Zapytanie:',
'Record ID': 'ID rekordu',
'Register': 'Zarejestruj',
'Registration key': 'Klucz rejestracji',
'Role': 'Rola',
'Rows in table': 'Wiersze w tabeli',
'Rows selected': 'Wybrane wiersze',
'Stylesheet': 'Arkusz styl\xc3\xb3w',
'Submit': 'Wy\xc5\x9blij',
'Sure you want to delete this object?': 'Czy na pewno chcesz usun\xc4\x85\xc4\x87 ten obiekt?',
'Table name': 'Nazwa tabeli',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': '"Zapytanie" jest warunkiem postaci "db.tabela1.pole1==\'warto\xc5\x9b\xc4\x87\'". Takie co\xc5\x9b jak "db.tabela1.pole1==db.tabela2.pole2" oznacza SQL JOIN.',
'Timestamp': 'Znacznik czasu',
'Update:': 'Uaktualnij:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'U\xc5\xbcyj (...)&(...) jako AND, (...)|(...) jako OR oraz ~(...)  jako NOT do tworzenia bardziej skomplikowanych zapyta\xc5\x84.',
'User %(id)s Registered': 'U\xc5\xbcytkownik %(id)s zosta\xc5\x82 zarejestrowany',
'User ID': 'ID u\xc5\xbcytkownika',
'Verify Password': 'Potwierd\xc5\xba has\xc5\x82o',
'View': 'Widok',
'Welcome %s': 'Welcome %s',
'Welcome to web2py': 'Witaj w web2py',
'appadmin is disabled because insecure channel': 'administracja aplikacji wy\xc5\x82\xc4\x85czona z powodu braku bezpiecznego po\xc5\x82\xc4\x85czenia',
'cache': 'cache',
'change password': 'change password',
'click here for online examples': 'Kliknij aby przej\xc5\x9b\xc4\x87 do interaktywnych przyk\xc5\x82ad\xc3\xb3w',
'click here for the administrative interface': 'Kliknij aby przej\xc5\x9b\xc4\x87 do panelu administracyjnego',
'customize me!': 'dostosuj mnie!',
'data uploaded': 'dane wys\xc5\x82ane',
'database': 'baza danych',
'database %s select': 'wyb\xc3\xb3r z bazy danych %s',
'db': 'baza danych',
'design': 'projektuj',
'done!': 'zrobione!',
'edit profile': 'edit profile',
'export as csv file': 'eksportuj jako plik csv',
'insert new': 'wstaw nowy rekord tabeli',
'insert new %s': 'wstaw nowy rekord do tabeli %s',
'invalid request': 'B\xc5\x82\xc4\x99dne \xc5\xbc\xc4\x85danie',
'login': 'login',
'logout': 'logout',
'new record inserted': 'nowy rekord zosta\xc5\x82 wstawiony',
'next 100 rows': 'nast\xc4\x99pne 100 wierszy',
'or import from csv file': 'lub zaimportuj z pliku csv',
'previous 100 rows': 'poprzednie 100 wierszy',
'record': 'rekord',
'record does not exist': 'rekord nie istnieje',
'record id': 'id rekordu',
'register': 'register',
'selected': 'wybranych',
'state': 'stan',
'table': 'tabela',
'unable to parse csv file': 'nie mo\xc5\xbcna sparsowa\xc4\x87 pliku csv',
}

########NEW FILE########
__FILENAME__ = plural-en
#!/usr/bin/env python
# -*- coding: utf-8 -*-
{
# "singular form (0)": ["first plural form (1)", "second plural form (2)", ...],
'hit': ['hits'],
'hour': ['hours'],
'minute': ['minutes'],
'second': ['seconds'],
}

########NEW FILE########
__FILENAME__ = pt-br
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update" \xc3\xa9 uma express\xc3\xa3o opcional como "campo1=\'novovalor\'". Voc\xc3\xaa n\xc3\xa3o pode atualizar ou apagar os resultados de um JOIN',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': '%s linhas apagadas',
'%s rows updated': '%s linhas atualizadas',
'Available databases and tables': 'Bancos de dados e tabelas dispon\xc3\xadveis',
'Cannot be empty': 'N\xc3\xa3o pode ser vazio',
'Check to delete': 'Marque para apagar',
'Controller': 'Controller',
'Copyright': 'Copyright',
'Current request': 'Requisi\xc3\xa7\xc3\xa3o atual',
'Current response': 'Resposta atual',
'Current session': 'Sess\xc3\xa3o atual',
'DB Model': 'DB Model',
'Database': 'Database',
'Delete:': 'Apagar:',
'Edit': 'Edit',
'Edit This App': 'Edit This App',
'Edit current record': 'Editar o registro atual',
'Hello World': 'Ol\xc3\xa1 Mundo',
'Import/Export': 'Importar/Exportar',
'Index': 'Index',
'Internal State': 'Estado Interno',
'Invalid Query': 'Consulta Inv\xc3\xa1lida',
'Layout': 'Layout',
'Login': 'Autentique-se',
'Lost Password': 'Esqueceu sua senha?',
'Main Menu': 'Main Menu',
'Menu Model': 'Menu Model',
'New Record': 'Novo Registro',
'No databases in this application': 'Sem bancos de dados nesta aplica\xc3\xa7\xc3\xa3o',
'Powered by': 'Powered by',
'Query:': 'Consulta:',
'Register': 'Registre-se',
'Rows in table': 'Linhas na tabela',
'Rows selected': 'Linhas selecionadas',
'Stylesheet': 'Stylesheet',
'Sure you want to delete this object?': 'Est\xc3\xa1 certo(a) que deseja apagar esse objeto ?',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'Uma "consulta" \xc3\xa9 uma condi\xc3\xa7\xc3\xa3o como "db.tabela1.campo1==\'valor\'". Express\xc3\xb5es como "db.tabela1.campo1==db.tabela2.campo2" resultam em um JOIN SQL.',
'Update:': 'Atualizar:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Use (...)&(...) para AND, (...)|(...) para OR, e ~(...)  para NOT para construir consultas mais complexas.',
'View': 'View',
'Welcome %s': 'Welcome %s',
'Welcome to web2py': 'Bem vindo ao web2py',
'appadmin is disabled because insecure channel': 'appadmin is disabled because insecure channel',
'cache': 'cache',
'change password': 'change password',
'click here for online examples': 'clique aqui para ver alguns exemplos',
'click here for the administrative interface': 'clique aqui para acessar a interface administrativa',
'customize me!': 'Personalize-me!',
'data uploaded': 'dados enviados',
'database': 'banco de dados',
'database %s select': 'Selecionar banco de dados %s',
'db': 'db',
'design': 'design',
'done!': 'conclu\xc3\xaddo!',
'edit profile': 'edit profile',
'export as csv file': 'exportar como um arquivo csv',
'insert new': 'inserir novo',
'insert new %s': 'inserir novo %s',
'invalid request': 'requisi\xc3\xa7\xc3\xa3o inv\xc3\xa1lida',
'login': 'login',
'logout': 'logout',
'new record inserted': 'novo registro inserido',
'next 100 rows': 'pr\xc3\xb3ximas 100 linhas',
'or import from csv file': 'ou importar de um arquivo csv',
'previous 100 rows': '100 linhas anteriores',
'record': 'record',
'record does not exist': 'registro n\xc3\xa3o existe',
'record id': 'id do registro',
'register': 'register',
'selected': 'selecionado',
'state': 'estado',
'table': 'tabela',
'unable to parse csv file': 'n\xc3\xa3o foi poss\xc3\xadvel analisar arquivo csv',
}

########NEW FILE########
__FILENAME__ = pt-pt
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update"  uma expresso opcional como "field1=\'newvalue\'". No pode actualizar ou eliminar os resultados de um JOIN',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': '%s linhas eliminadas',
'%s rows updated': '%s linhas actualizadas',
'About': 'About',
'Author Reference Auth User': 'Author Reference Auth User',
'Author Reference Auth User.username': 'Author Reference Auth User.username',
'Available databases and tables': 'bases de dados e tabelas disponveis',
'Cannot be empty': 'no pode ser vazio',
'Category Create': 'Category Create',
'Category Select': 'Category Select',
'Check to delete': 'seleccione para eliminar',
'Comment Create': 'Comment Create',
'Comment Select': 'Comment Select',
'Content': 'Content',
'Controller': 'Controlador',
'Copyright': 'Direitos de cpia',
'Created By': 'Created By',
'Created On': 'Created On',
'Current request': 'pedido currente',
'Current response': 'resposta currente',
'Current session': 'sesso currente',
'DB Model': 'Modelo de BD',
'Database': 'Base de dados',
'Delete:': 'Eliminar:',
'Edit': 'Editar',
'Edit This App': 'Edite esta aplicao',
'Edit current record': 'Edio de registo currente',
'Email': 'Email',
'First Name': 'First Name',
'For %s #%s': 'For %s #%s',
'Hello World': 'Ol Mundo',
'Import/Export': 'Importar/Exportar',
'Index': 'ndice',
'Internal State': 'Estado interno',
'Invalid Query': 'Consulta Invlida',
'Last Name': 'Last Name',
'Layout': 'Esboo',
'Main Menu': 'Menu Principal',
'Menu Model': 'Menu do Modelo',
'Modified By': 'Modified By',
'Modified On': 'Modified On',
'Name': 'Name',
'New Record': 'Novo Registo',
'No Data': 'No Data',
'No databases in this application': 'No h bases de dados nesta aplicao',
'Password': 'Password',
'Post Create': 'Post Create',
'Post Select': 'Post Select',
'Powered by': 'Suportado por',
'Query:': 'Interrogao:',
'Replyto Reference Post': 'Replyto Reference Post',
'Rows in table': 'Linhas numa tabela',
'Rows selected': 'Linhas seleccionadas',
'Stylesheet': 'Folha de estilo',
'Sure you want to delete this object?': 'Tem a certeza que deseja eliminar este objecto?',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'A "query"  uma condio do tipo "db.table1.field1==\'value\'". Algo como "db.table1.field1==db.table2.field2" resultaria num SQL JOIN.',
'Title': 'Title',
'Update:': 'Actualizao:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Utilize (...)&(...) para AND, (...)|(...) para OR, e ~(...)  para NOT para construir interrogaes mais complexas.',
'Username': 'Username',
'View': 'Vista',
'Welcome %s': 'Bem-vindo(a) %s',
'Welcome to Gluonization': 'Bem vindo ao Web2py',
'Welcome to web2py': 'Bem-vindo(a) ao web2py',
'When': 'When',
'appadmin is disabled because insecure channel': 'appadmin est desactivada pois o canal  inseguro',
'cache': 'cache',
'change password': 'alterar palavra-chave',
'click here for online examples': 'Clique aqui para exemplos online',
'click here for the administrative interface': 'Clique aqui para o painel administrativo',
'create new category': 'create new category',
'create new comment': 'create new comment',
'create new post': 'create new post',
'customize me!': 'Personaliza-me!',
'data uploaded': 'informao enviada',
'database': 'base de dados',
'database %s select': 'seleco de base de dados %s',
'db': 'bd',
'design': 'design',
'done!': 'concludo!',
'edit category': 'edit category',
'edit comment': 'edit comment',
'edit post': 'edit post',
'edit profile': 'Editar perfil',
'export as csv file': 'exportar como ficheiro csv',
'insert new': 'inserir novo',
'insert new %s': 'inserir novo %s',
'invalid request': 'Pedido Invlido',
'login': 'login',
'logout': 'logout',
'new record inserted': 'novo registo inserido',
'next 100 rows': 'prximas 100 linhas',
'or import from csv file': 'ou importe a partir de ficheiro csv',
'previous 100 rows': '100 linhas anteriores',
'record': 'registo',
'record does not exist': 'registo inexistente',
'record id': 'id de registo',
'register': 'register',
'search category': 'search category',
'search comment': 'search comment',
'search post': 'search post',
'select category': 'select category',
'select comment': 'select comment',
'select post': 'select post',
'selected': 'seleccionado(s)',
'show category': 'show category',
'show comment': 'show comment',
'show post': 'show post',
'state': 'estado',
'table': 'tabela',
'unable to parse csv file': 'no foi possvel carregar ficheiro csv',
}

########NEW FILE########
__FILENAME__ = pt
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update"  uma expresso opcional como "field1=\'newvalue\'". No pode actualizar ou eliminar os resultados de um JOIN',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': '%s linhas eliminadas',
'%s rows updated': '%s linhas actualizadas',
'About': 'About',
'Author Reference Auth User': 'Author Reference Auth User',
'Author Reference Auth User.username': 'Author Reference Auth User.username',
'Available databases and tables': 'bases de dados e tabelas disponveis',
'Cannot be empty': 'no pode ser vazio',
'Category Create': 'Category Create',
'Category Select': 'Category Select',
'Check to delete': 'seleccione para eliminar',
'Comment Create': 'Comment Create',
'Comment Select': 'Comment Select',
'Content': 'Content',
'Controller': 'Controlador',
'Copyright': 'Direitos de cpia',
'Created By': 'Created By',
'Created On': 'Created On',
'Current request': 'pedido currente',
'Current response': 'resposta currente',
'Current session': 'sesso currente',
'DB Model': 'Modelo de BD',
'Database': 'Base de dados',
'Delete:': 'Eliminar:',
'Edit': 'Editar',
'Edit This App': 'Edite esta aplicao',
'Edit current record': 'Edio de registo currente',
'Email': 'Email',
'First Name': 'First Name',
'For %s #%s': 'For %s #%s',
'Hello World': 'Ol Mundo',
'Import/Export': 'Importar/Exportar',
'Index': 'ndice',
'Internal State': 'Estado interno',
'Invalid Query': 'Consulta Invlida',
'Last Name': 'Last Name',
'Layout': 'Esboo',
'Main Menu': 'Menu Principal',
'Menu Model': 'Menu do Modelo',
'Modified By': 'Modified By',
'Modified On': 'Modified On',
'Name': 'Name',
'New Record': 'Novo Registo',
'No Data': 'No Data',
'No databases in this application': 'No h bases de dados nesta aplicao',
'Password': 'Password',
'Post Create': 'Post Create',
'Post Select': 'Post Select',
'Powered by': 'Suportado por',
'Query:': 'Interrogao:',
'Replyto Reference Post': 'Replyto Reference Post',
'Rows in table': 'Linhas numa tabela',
'Rows selected': 'Linhas seleccionadas',
'Stylesheet': 'Folha de estilo',
'Sure you want to delete this object?': 'Tem a certeza que deseja eliminar este objecto?',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': 'A "query"  uma condio do tipo "db.table1.field1==\'value\'". Algo como "db.table1.field1==db.table2.field2" resultaria num SQL JOIN.',
'Title': 'Title',
'Update:': 'Actualizao:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Utilize (...)&(...) para AND, (...)|(...) para OR, e ~(...)  para NOT para construir interrogaes mais complexas.',
'Username': 'Username',
'View': 'Vista',
'Welcome %s': 'Bem-vindo(a) %s',
'Welcome to Gluonization': 'Bem vindo ao Web2py',
'Welcome to web2py': 'Bem-vindo(a) ao web2py',
'When': 'When',
'appadmin is disabled because insecure channel': 'appadmin est desactivada pois o canal  inseguro',
'cache': 'cache',
'change password': 'alterar palavra-chave',
'click here for online examples': 'Clique aqui para exemplos online',
'click here for the administrative interface': 'Clique aqui para o painel administrativo',
'create new category': 'create new category',
'create new comment': 'create new comment',
'create new post': 'create new post',
'customize me!': 'Personaliza-me!',
'data uploaded': 'informao enviada',
'database': 'base de dados',
'database %s select': 'seleco de base de dados %s',
'db': 'bd',
'design': 'design',
'done!': 'concludo!',
'edit category': 'edit category',
'edit comment': 'edit comment',
'edit post': 'edit post',
'edit profile': 'Editar perfil',
'export as csv file': 'exportar como ficheiro csv',
'insert new': 'inserir novo',
'insert new %s': 'inserir novo %s',
'invalid request': 'Pedido Invlido',
'login': 'login',
'logout': 'logout',
'new record inserted': 'novo registo inserido',
'next 100 rows': 'prximas 100 linhas',
'or import from csv file': 'ou importe a partir de ficheiro csv',
'previous 100 rows': '100 linhas anteriores',
'record': 'registo',
'record does not exist': 'registo inexistente',
'record id': 'id de registo',
'register': 'register',
'search category': 'search category',
'search comment': 'search comment',
'search post': 'search post',
'select category': 'select category',
'select comment': 'select comment',
'select post': 'select post',
'selected': 'seleccionado(s)',
'show category': 'show category',
'show comment': 'show comment',
'show post': 'show post',
'state': 'estado',
'table': 'tabela',
'unable to parse csv file': 'no foi possvel carregar ficheiro csv',
}

########NEW FILE########
__FILENAME__ = ru-ru
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"" -    "field1=\' \'".   JOIN    .',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': '%s  ',
'%s rows updated': '%s  ',
'Available databases and tables': '   ',
'Cannot be empty': '  ',
'Change Password': ' ',
'Check to delete': '',
'Check to delete:': ':',
'Client IP': 'Client IP',
'Current request': ' ',
'Current response': ' ',
'Current session': ' ',
'Delete:': ':',
'Description': '',
'E-mail': 'E-mail',
'Edit Profile': ' ',
'Edit current record': '  ',
'First name': '',
'Group ID': 'Group ID',
'Hello World': '!',
'Import/Export': '/',
'Internal State': ' ',
'Invalid Query': ' ',
'Invalid email': ' email',
'Invalid login': ' ',
'Invalid password': ' ',
'Last name': '',
'Logged in': ' ',
'Logged out': ' ',
'Login': '',
'Logout': '',
'Lost Password': ' ?',
'Name': 'Name',
'New Record': ' ',
'New password': ' ',
'No databases in this application': '    ',
'Old password': ' ',
'Origin': '',
'Password': '',
"Password fields don't match": '  ',
'Query:': ':',
'Record ID': 'ID ',
'Register': '',
'Registration key': ' ',
'Remember me (for 30 days)': '  ( 30 )',
'Reset Password key': '  ',
'Role': '',
'Rows in table': '  ',
'Rows selected': ' ',
'Submit': '',
'Sure you want to delete this object?': '  ',
'Table name': ' ',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': '"" -    "db.table1.field1==\'\'".   "db.table1.field1==db.table2.field2"  SQL JOIN.',
'Timestamp': ' ',
'Update:': ':',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': '      "": (...)&(...), "": (...)|(...), "": ~(...).',
'User %(id)s Logged-in': ' %(id)s ',
'User %(id)s Logged-out': ' %(id)s ',
'User %(id)s Password changed': ' %(id)s  ',
'User %(id)s Profile updated': ' %(id)s  ',
'User %(id)s Registered': ' %(id)s ',
'User ID': 'ID ',
'Verify Password': ' ',
'Welcome to web2py': '   web2py',
'click here for online examples': ' -',
'click here for the administrative interface': ' ',
'customize me!': '  !',
'data uploaded': ' ',
'database': ' ',
'database %s select': '   %s',
'db': '',
'design': '',
'done!': '!',
'export as csv file': '   csv-',
'insert new': '',
'insert new %s': ' %s',
'invalid request': ' ',
'login': '',
'logout': '',
'new record inserted': '  ',
'next 100 rows': ' 100 ',
'or import from csv file': '   csv-',
'password': '',
'previous 100 rows': ' 100 ',
'profile': '',
'record does not exist': '  ',
'record id': 'id ',
'selected': '',
'state': '',
'table': '',
'unable to parse csv file': ' csv-',
}

########NEW FILE########
__FILENAME__ = sk-sk
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"update" je voliten vraz ako "field1=\'newvalue\'". Nemete upravova alebo zmaza vsledky JOINu',
'%Y-%m-%d': '%d.%m.%Y',
'%Y-%m-%d %H:%M:%S': '%d.%m.%Y %H:%M:%S',
'%s rows deleted': '%s zmazanch zznamov',
'%s rows updated': '%s upravench zznamov',
'Available databases and tables': 'Dostupn databzy a tabuky',
'Cannot be empty': 'Neme by przdne',
'Check to delete': 'Oznai na zmazanie',
'Controller': 'Controller',
'Copyright': 'Copyright',
'Current request': 'Aktulna poiadavka',
'Current response': 'Aktulna odpove',
'Current session': 'Aktulne sedenie',
'DB Model': 'DB Model',
'Database': 'Databza',
'Delete:': 'Zmaza:',
'Description': 'Popis',
'Edit': 'Upravi',
'Edit Profile': 'Upravi profil',
'Edit current record': 'Upravi aktulny zznam',
'First name': 'Krstn meno',
'Group ID': 'ID skupiny',
'Hello World': 'Ahoj svet',
'Import/Export': 'Import/Export',
'Index': 'Index',
'Internal State': 'Vntorn stav',
'Invalid email': 'Neplatn email',
'Invalid Query': 'Neplatn otzka',
'Invalid password': 'Nesprvne heslo',
'Last name': 'Priezvisko',
'Layout': 'Layout',
'Logged in': 'Prihlsen',
'Logged out': 'Odhlsen',
'Lost Password': 'Straten heslo?',
'Menu Model': 'Menu Model',
'Name': 'Meno',
'New Record': 'Nov zznam',
'New password': 'Nov heslo',
'No databases in this application': 'V tejto aplikcii nie s databzy',
'Old password': 'Star heslo',
'Origin': 'Pvod',
'Password': 'Heslo',
'Powered by': 'Powered by',
'Query:': 'Otzka:',
'Record ID': 'ID zznamu',
'Register': 'Zaregistrova sa',
'Registration key': 'Registran k',
'Remember me (for 30 days)': 'Zapamtaj si ma (na 30 dn)',
'Reset Password key': 'Nastavi registran k',
'Role': 'Rola',
'Rows in table': 'riadkov v tabuke',
'Rows selected': 'oznaench riadkov',
'Submit': 'Odosla',
'Stylesheet': 'Stylesheet',
'Sure you want to delete this object?': 'Ste si ist, e chcete zmaza tento objekt?',
'Table name': 'Nzov tabuky',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': '"query" je podmienka ako "db.table1.field1==\'value\'". Nieo ako "db.table1.field1==db.table2.field2" m za vsledok SQL JOIN.',
'The output of the file is a dictionary that was rendered by the view': 'Vstup zo sboru je slovnk, ktor bol zobrazen vo view',
'This is a copy of the scaffolding application': 'Toto je kpia skeletu aplikcie',
'Timestamp': 'asov peiatka',
'Update:': 'Upravi:',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': 'Pouite (...)&(...) pre AND, (...)|(...) pre OR a ~(...) pre NOT na poskladanie komplexnejch otzok.',
'User %(id)s Logged-in': 'Pouvate %(id)s prihlsen',
'User %(id)s Logged-out': 'Pouvate %(id)s odhlsen',
'User %(id)s Password changed': 'Pouvate %(id)s zmenil heslo',
'User %(id)s Profile updated': 'Pouvate %(id)s upravil profil',
'User %(id)s Registered': 'Pouvate %(id)s sa zaregistroval',
'User ID': 'ID pouvatea',
'Verify Password': 'Zopakujte heslo',
'View': 'Zobrazi',
'Welcome to web2py': 'Vitajte vo web2py',
'Which called the function': 'Ktor zavolal funkciu',
'You are successfully running web2py': 'spene ste spustili web2py',
'You can modify this application and adapt it to your needs': 'Mete upravi tto aplikciu a prispsobi ju svojim potrebm',
'You visited the url': 'Navtvili ste URL',
'appadmin is disabled because insecure channel': 'appadmin je zakzan bez zabezpeenho spojenia',
'cache': 'cache',
'click here for online examples': 'pre online prklady kliknite sem',
'click here for the administrative interface': 'pre administrtorsk rozhranie kliknite sem',
'customize me!': 'prispsob ma!',
'data uploaded': 'daje naplnen',
'database': 'databza',
'database %s select': 'databza %s vber',
'db': 'db',
'design': 'nvrh',
'documentation': 'dokumentcia',
'done!': 'hotovo!',
'export as csv file': 'exportova do csv sboru',
'insert new': 'vloi nov zznam ',
'insert new %s': 'vloi nov  zznam %s',
'invalid request': 'Neplatn poiadavka',
'located in the file': 'nachdzajci sa v sbore ',
'login': 'prihlsi',
'logout': 'odhlsi',
'lost password?': 'straten heslo?',
'new record inserted': 'nov zznam bol vloen',
'next 100 rows': 'alch 100 riadkov',
'or import from csv file': 'alebo naimportova z csv sboru',
'password': 'heslo',
'previous 100 rows': 'predchdzajcich 100 riadkov',
'record': 'zznam',
'record does not exist': 'zznam neexistuje',
'record id': 'id zznamu',
'register': 'registrova',
'selected': 'oznaench',
'state': 'stav',
'table': 'tabuka',
'unable to parse csv file': 'ned sa nata csv sbor',
}

########NEW FILE########
__FILENAME__ = zh-tw
# coding: utf8
{
'"update" is an optional expression like "field1=\'newvalue\'". You cannot update or delete the results of a JOIN': '"" ,  "1=\'\'".  JOIN  update  delete"',
'%Y-%m-%d': '%Y-%m-%d',
'%Y-%m-%d %H:%M:%S': '%Y-%m-%d %H:%M:%S',
'%s rows deleted': ' %s ',
'%s rows updated': ' %s ',
'(something like "it-it")': '( "zh-tw")',
'A new version of web2py is available': ' web2py ',
'A new version of web2py is available: %s': ' web2py : %s',
'ATTENTION: Login requires a secure (HTTPS) connection or running on localhost.': ': (HTTPS)(localhost).',
'ATTENTION: TESTING IS NOT THREAD SAFE SO DO NOT PERFORM MULTIPLE TESTS CONCURRENTLY.': ': ',
'ATTENTION: you cannot edit the running application!': ':!',
'About': '',
'About application': '',
'Admin is disabled because insecure channel': '(Admin)',
'Admin is disabled because unsecure channel': '(Admin)',
'Administrator Password:': ':',
'Are you sure you want to delete file "%s"?': '"%s"?',
'Are you sure you want to uninstall application "%s"': ' "%s"',
'Are you sure you want to uninstall application "%s"?': ' "%s"',
'Authentication': '',
'Available databases and tables': '',
'Cannot be empty': '',
'Cannot compile: there are errors in your app.        Debug it, correct errors and try again.': ':.',
'Change Password': '',
'Check to delete': '',
'Check to delete:': ':',
'Client IP': '(IP)',
'Controller': '',
'Controllers': '',
'Copyright': '',
'Create new application': '',
'Current request': '(request)',
'Current response': '(response)',
'Current session': '(session)',
'DB Model': '',
'DESIGN': '',
'Database': '',
'Date and Time': '',
'Delete': '',
'Delete:': ':',
'Deploy on Google App Engine': ' Google App Engine',
'Description': '',
'Design for': '',
'E-mail': '',
'EDIT': '',
'Edit': '',
'Edit Profile': '',
'Edit This App': '',
'Edit application': '',
'Edit current record': '',
'Editing file': '',
'Editing file "%s"': '"%s"',
'Error logs for "%(app)s"': '"%(app)s"',
'First name': '',
'Functions with no doctests will result in [passed] tests.': ' doctests  [passed].',
'Group ID': '',
'Hello World': '! ',
'Import/Export': '/',
'Index': '',
'Installed applications': '',
'Internal State': '',
'Invalid Query': '',
'Invalid action': '(action)',
'Invalid email': '',
'Language files (static strings) updated': '',
'Languages': '',
'Last name': '',
'Last saved on:': ':',
'Layout': '',
'License for': '',
'Login': '',
'Login to the Administrative Interface': '',
'Logout': '',
'Lost Password': '',
'Main Menu': '',
'Menu Model': '(menu)',
'Models': '',
'Modules': '',
'NO': '',
'Name': '',
'New Record': '',
'No databases in this application': '',
'Origin': '',
'Original/Translation': '/',
'Password': '',
"Password fields don't match": '',
'Peeking at file': '',
'Powered by': '',
'Query:': ':',
'Record ID': '',
'Register': '',
'Registration key': '',
'Remember me (for 30 days)': '(30 )',
'Reset Password key': '',
'Resolve Conflict file': '',
'Role': '',
'Rows in table': '',
'Rows selected': '',
'Saved file hash:': ':',
'Static files': '',
'Stylesheet': '',
'Submit': '',
'Sure you want to delete this object?': '?',
'Table name': '',
'Testing application': '',
'The "query" is a condition like "db.table1.field1==\'value\'". Something like "db.table1.field1==db.table2.field2" results in a SQL JOIN.': '"" "db.1.1==\'\'" . "db.1.1==db.2.2" JOIN SQL.',
'There are no controllers': '(controllers)',
'There are no models': '(models)',
'There are no modules': '(modules)',
'There are no static files': '',
'There are no translators, only default language is supported': ',',
'There are no views': '',
'This is the %(filename)s template': '%(filename)s(template)',
'Ticket': '',
'Timestamp': '',
'Unable to check for upgrades': '',
'Unable to download': '',
'Unable to download app': '',
'Update:': ':',
'Upload existing application': '',
'Use (...)&(...) for AND, (...)|(...) for OR, and ~(...)  for NOT to build more complex queries.': ', (...)&(...) , (...)|(...) , ~(...).',
'User %(id)s Logged-in': ' %(id)s ',
'User %(id)s Registered': ' %(id)s ',
'User ID': '',
'Verify Password': '',
'View': '',
'Views': '',
'Welcome %s': ' %s',
'Welcome to web2py': ' web2py',
'YES': '',
'about': '',
'appadmin is disabled because insecure channel': ',',
'cache': '',
'change password': '',
'click here for online examples': '',
'click here for the administrative interface': '',
'customize me!': '!',
'data uploaded': '',
'database': '',
'database %s select': ' %s ',
'db': 'db',
'design': '',
'done!': '!',
'edit profile': '',
'export as csv file': '(csv)',
'insert new': '',
'insert new %s': ' %s',
'invalid request': '(request)',
'login': '',
'logout': '',
'new record inserted': '',
'next 100 rows': ' 100 ',
'or import from csv file': '(CSV)',
'previous 100 rows': ' 100 ',
'record': '',
'record does not exist': '',
'record id': '',
'register': '',
'selected': '',
'state': '',
'table': '',
'unable to parse csv file': '(csv)',
}

########NEW FILE########
__FILENAME__ = 0
# -*- coding: utf-8 -*-

if 0:
    import db

from gluon import sanitizer
from gluon.custom_import import track_changes; track_changes(True)

###########
# GLOBALS!

HTTP_PORTS = [
    "80",
    "88",
    "8080",
    "8081",
    "9091",
    "8888",
    "5800",
    "5801",
    "5802",
    "2301",
]

HTTPS_PORTS = [
    "443",
    "8443",
    "2381",
]

###########
# Widgets!

def autocomplete_bootstrap(f,v):
    """
    Autocomplete widget using boostrap typeahead
    """
    import uuid
    d_id = "autocomplete-bs-" + str(uuid.uuid4())[:8]
    wrapper = DIV(_id=d_id)
    inp_id = "autocomplete-input-bs-" + str(uuid.uuid4())[:8]
    rows = f._db(f._table['id']>0).select(f,distinct=True)
    #itms = rows.as_list()
    itms = [XML(t.values()[0], sanitize=True).xml() for t in rows]
    #inp = SQLFORM.widgets.string.widget(f, v, _id=inp_id, **{'_data-provide': 'typeahead', '_data-source': itms, '_data-items': 8})
    inp = SQLFORM.widgets.string.widget(f, v, _id=inp_id, _autocomplete="off", **{"_data-provide":"typeahead"})
    itms_var = "autocomplete_bs_data_" + str(uuid.uuid4())[:8]
    scr = SCRIPT('$(document).ready(function() {var %s=%s; jQuery("#%s").typeahead({source: %s});});' % (itms_var, str(itms), inp_id, itms_var))
    wrapper.append(inp)
    wrapper.append(scr)
    return wrapper

########NEW FILE########
__FILENAME__ = 00_settings
# -*- coding: utf-8 -*-

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Global settings which are not dependent for initial setup
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

import socket
import platform
import os

try:
    import yaml
    from yaml.parser import ParserError
except ImportError:
    raise ImportError('PyYAML required. Please install it before continuing')

from gluon.storage import Storage
settings = Storage()

settings.kvasir_config = {}
kv_cfg_filename = os.path.join(os.environ.get('HOME'), '.kvasir', 'kvasir.yaml')
try:
    settings.kvasir_config = yaml.load(open(kv_cfg_filename, 'r'))
except IOError, e:
    kv_cfg_filename = os.environ.get('KVASIR_CONFIG', 'kvasir.yaml')
    try:
        settings.kvasir_config = yaml.load(open(kv_cfg_filename, 'r'))
    except IOError, e:
        kv_cfg_filename = os.path.join('applications', request.application, 'kvasir.yaml')
        try:
            settings.kvasir_config = yaml.load(open(kv_cfg_filename, 'r'))
        except IOError, e:
            raise IOError('Unable to load kvasir.yaml configuration. Please place it in $HOME/.kvasir or your application directory')
except yaml.parser.ParserError, e:
    raise yaml.parser.ParserError('Error parsing %s: %s' % (kv_cfg_filename, str(e)))

settings.kv_yaml_file = kv_cfg_filename

settings.title = 'Kvasir'
settings.subtitle = 'Beware of evil dwarves'

# Customer information for XML Report generator
settings.customer = settings.kvasir_config.get('customer', '')
settings.assessment_type = settings.kvasir_config.get('assessment_type', '')
settings.start_date = settings.kvasir_config.get('start_date', '')
settings.end_date = settings.kvasir_config.get('end_date', '')

# Global display / HTML
settings.author = 'Cisco Systems Security Posture Assessment Team'
settings.author_email = 'kvasirdevs@external.cisco.com'
settings.keywords = ''
settings.description = ''

# Authentication.
login = settings.kvasir_config.get('login', {})
settings.login_method = login.get('method', 'local')
settings.login_config = login.get('config', '')
del login

# CVSS or Severity
settings.use_cvss = settings.kvasir_config.get('use_cvss', False)

# Password upload default directory ($APPNAME/data/passwords/misc)
settings.password_upload_dir = settings.kvasir_config.get('password_upload_dir', 'data/passwords/misc')

# Scheduler
settings.scheduler_group_name = settings.kvasir_config.get('scheduler_group_name', socket.gethostname())
settings.scheduler_timeout = settings.kvasir_config.get('scheduler_timeout', 3600)   # 1 hour timeout default

# Launch command
settings.launch_command = settings.kvasir_config.get('launch_command', None)
if not settings.launch_command:
    # set default launch_command based on running OS
    platform_system = platform.system()
    if platform_system == 'Darwin':
        settings.launch_command = "osascript terminal.scpt _IP_ _DATADIR_ _LOGFILE_"
    elif platform_system == 'Linux':
        settings.launch_command = "gnome-terminal --window -t 'manual hacking: _IP_' -e 'script _LOGFILE_'"
    else:
        settings.launch_command = "xterm -sb -sl 1500 -vb -T 'manual hacking: _IP_' -n 'manual hacking: _IP_' -e script _LOGFILE_"

# Nmap
nmap_config = settings.kvasir_config.get('nmap', {})
settings.nmap_binary = nmap_config.get('binary', '/usr/local/bin/nmap')
settings.nmap_sharedir = nmap_config.get('sharedir', '/usr/local/share/nmap')
settings.nmap_scriptdir = nmap_config.get('scriptdir', '/usr/local/share/nmap/scripts')
settings.nmap_nselibdir = nmap_config.get('nselibdir', '/usr/local/share/nmap/nselib')
del nmap_config

# ShodanHQ
settings.shodanhq_apikey = settings.kvasir_config.get('shodanhq_api_key', '')

# pwnwiki.github.io
settings.pwnwiki_path = settings.kvasir_config.get('pwnwiki_path', 'http://pwnwiki.io/')

# exploitdb
settings.exploitdb_path = settings.kvasir_config.get('exploitdb_path', None)

# redirect_timer
settings.redirect_timer = settings.kvasir_config.get('redirect_timer', 10)

# Login image. File must be under static/ subdirectory
settings.login_image = settings.kvasir_config.get('login_image', 'images/Kvasir_portrait.png')

# Loading video - can be a movie file (mp4/m4v) or None
settings.loading_video = settings.kvasir_config.get('loading_video', None)

########NEW FILE########
__FILENAME__ = 01_form_modal
#-*- encoding:utf-8 -*-

##--------------------------------------#
## Kvasir
##
## (c) 2010-2014 Cisco Systems, Inc.
##
## Formstyles for Forms and Modals
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from gluon.html import A, DIV, H3, BUTTON, I, SCRIPT
from gluon.sqlhtml import SQLFORM
from gluon.compileapp import LOAD
from gluon.http import HTTP
from gluon import current


##-------------------------------------------------------------------------

def formstyle_bootstrap_modal(form, fields, **kwargs):
    """"
    Bootstrap format modal form layout
    """
    span = kwargs.get('span') or 'span8'
    select_attributes = kwargs.get('select_attributes', '')
    form.add_class('form-horizontal')
    parent = FIELDSET()
    for id, label, controls, help in fields:
        _controls = DIV(controls, _class='controls')
        # submit unflag by default
        _submit = False

        if isinstance(controls, INPUT):
            controls.add_class(span)
            if controls['_type'] == 'submit':
                # flag submit button
                _submit = True
                controls['_class'] = 'btn btn-primary'
            if controls['_type'] == 'file':
                controls['_class'] = 'input-file'

        # For password fields, which are wrapped in a CAT object.
        if isinstance(controls, CAT) and isinstance(controls[0], INPUT):
            controls[0].add_class(span)

        if isinstance(controls, SELECT):
            controls.add_class(span)

        if isinstance(controls, TEXTAREA):
            controls.add_class(span)

        if isinstance(label, LABEL):
            label['_class'] = 'control-label'
            if help:
                label.append(I(_class="icon-question-sign", _rel="tooltip", **{ '_data-content':help }))

        if _submit:
            # submit button has unwrapped label and controls, different class
            parent.append(DIV(label, BUTTON("Close", _class="btn", **{'_data-dismiss':'modal', '_aria-hidden':True}), controls, _class='modal-footer', _id=id))
            # unflag submit (possible side effect)
            _submit = False
        else:
            # unwrapped label
            _class = 'control-group'
            parent.append(DIV(label, _controls, _class=_class, _id=id))

    # append tooltip and chosen field attributes
    if 'id' not in form.attributes:
        form.attributes['_id'] = "%s-id" % (str(form.table))
    script_data = """$(document).ready(function() {{
    $("[rel=tooltip]").popover({{
        placement: 'right',
        trigger: 'hover',
    }});
    $('#{0:s} select').select2({{{1:s}}});
    {2:s}
}});""".format(form.attributes['_id'], select_attributes, kwargs.get('script', ''))
    parent.append(SCRIPT(script_data))
    return parent
SQLFORM.formstyles['bootstrap-modal'] = formstyle_bootstrap_modal

##-------------------------------------------------------------------------

def formstyle_bootstrap_kvasir(form, fields, **kwargs):
    """
    Bootstrap format form layout for Kvasir
    """
    span = kwargs.get('span') or 'span8'
    select_attributes = kwargs.get('select_attributes', '')
    form.add_class('form-horizontal')
    parent = FIELDSET()
    for id, label, controls, help in fields:
        # wrappers
        _controls = DIV(controls, _class='controls')
        # submit unflag by default
        _submit = False

        if isinstance(controls, INPUT):
            controls.add_class('span8')
            if controls['_type'] == 'submit':
                # flag submit button
                _submit = True
                controls['_class'] = 'btn btn-primary'
            if controls['_type'] == 'file':
                controls['_class'] = 'input-file'

        # For password fields, which are wrapped in a CAT object.
        if isinstance(controls, CAT) and isinstance(controls[0], INPUT):
            controls[0].add_class(span)

        if isinstance(controls, SELECT):
            controls.add_class(span)

        if isinstance(controls, TEXTAREA):
            controls.add_class(span)

        if isinstance(label, LABEL):
            label['_class'] = 'control-label'
            if help:
                label.append(I(_class="icon-question-sign", _rel="tooltip", **{ '_data-content':help }))

        if _submit:
            # submit button has unwrapped label and controls, different class
            parent.append(DIV(label, controls, _class='form-actions', _id=id))
            # unflag submit (possible side effect)
            _submit = False
        else:
            # unwrapped label
            parent.append(DIV(label, _controls, _class='control-group', _id=id))

    # append tooltip and select2 field attributes
    if '_id' not in form.attributes:
        form.attributes['_id'] = "%s-id" % (str(form.table))
    script_data = """$(document).ready(function() {{
    $("[rel=tooltip]").popover({{
        placement: 'right',
        trigger: 'hover',
    }});
    $('#{0:s} select').select2({{{1:s}}});
    {2:s}
}});""".format(form.attributes['_id'], select_attributes, kwargs.get('script', ''))
    parent.append(SCRIPT(script_data))
    return parent
SQLFORM.formstyles['bootstrap_kvasir'] = formstyle_bootstrap_kvasir
# overwrite the "default" table3cols with default bootstrap
SQLFORM.formstyles.table3cols = SQLFORM.formstyles.bootstrap_kvasir

##-------------------------------------------------------------------------

class AddModal(object):
    """
    AddModal provides a modular method for creating "Add ..." bootstrap modals
    and relevant FORM data.

    Usage in controller:
        add = AddModal(
            db.db_name, 'Add', 'Add', 'Add Thing',
            #fields=[],
            cmd='table.fnReloadAjax();'
        )
        #db.t_services.f_hosts_id.default = record.id
        db.db_name.id.comment = add.create()

    Usage in HTML:
        {{=XML(add.formModal())}}    // Generates the modal HTML
        {{=XML(add.btn_show())}}     // Generates the A(..) object
    """
    def __init__(self, table, value, title_btn, title_modal, fields=None, flash="Record created", cmd="", errormsg="Error in form", **kwargs):
        self.table = table
        self.value = value
        self.title_btn = title_btn
        self.title_modal = title_modal
        self.fields = fields
        self.flash = flash
        self.cmd = cmd
        self.errormsg = errormsg
        self.kwargs = kwargs
        self.key = str(self.table).replace('.', '_')
        self.modal_id = 'modal_%s' % self.key
        self._target = "c_" + self.key
        self.request = current.request
        self.response = current.response
        self.session = current.session
        self.script = kwargs.get('script')

    def btn_show(self, icon="icon-plus", btn_role="button", btn_class="btn btn-small"):
        """
        Generates an A() button object. By default a btn class and icon object are created
        but sending obj.btn_show(btn_role="", btn_class="", icon="") will clear all that.
        """
        btn_show_modal = A(I(_class=icon),
                           ' ', self.value,
                           **{"_role": btn_role,
                           "_class": btn_class,
                           "_data-toggle": "modal",
                           "_href": "#%s" % self.modal_id,
                           "_title": self.title_btn})
        return btn_show_modal

    def div_modal(self, content_modal):
        div_modal = DIV(
                        DIV(
                            # BUTTON("x", _type="button", _class="close", **{'data-dismiss':"modal", 'aria-hidden':"true"}),
                            BUTTON("x", **{"_type":"button", "_class": "close", "_data-dismiss": "modal", "_aria-hidden": "true"}),
                            H3(self.title_modal, _id="myModalLabel"),
                               _class="modal-header"),
                        DIV(content_modal, _class="modal-body", _id="host-modal"),
                        SCRIPT("$('#%s').on('show', function () { $(this).find('.modal-body').css({'overflow-y':'scroll', 'width':'auto', 'height':'auto'});});" % self.modal_id),
                        **{"_id": "%s" % self.modal_id,
                           "_class": "modal bigModal hide face",
                           "_tabindex": "-1",
                           "_role": "dialog",
                           "_data-keyboard": "false",
                           "_aria-hidden": "true",
                           "_aria-labelledby": "myModalLabel"}
                    )
        return div_modal

    def create(self):
        if self.request.get_vars._ajax_add == str(self.table):
            raise HTTP(200, self.checkForm(self.table))
        return self.btn_show()

    def formModal(self):
        return self.div_modal(LOAD(self.request.controller,
                                   self.request.function,
                                   args=self.request.args,
                                   vars=dict(_ajax_add=self.table),
                                   target=self._target,
                                   ajax=True)
                                  )

    def checkForm(self, table):
        formnamemodal = "formmodal_%s" % self.key
        form = SQLFORM(table, formname=formnamemodal, _class="form-horizontal", formstyle='bootstrap-modal', fields=self.fields, kwargs=self.kwargs)
        if self.script:
            form.append(SCRIPT(self.script))
        if form.process().accepted:
            command = "jQuery('#%s').modal('hide');" % (self.modal_id)
            command += self.cmd
            self.response.flash = self.flash
            self.response.js = command
        elif form.errors:
            self.response.flash = self.errormsg

        return form

########NEW FILE########
__FILENAME__ = buttons
# Template helpers

import os

def button(href, label):
  return A(SPAN(label),_class='button',_href=href)
  
def sp_button(href, label):
  return A(SPAN(label),_class='button special',_href=href)
  
def helpicon():
  return IMG(_src=URL(request.application, 'static', 'images/help.png'), _alt='help')
  
def searchbox(elementid):
  return TAG[''](LABEL(IMG(_src=URL(request.application, 'static', 'images/search.png'), _alt=T('filter')), _class='icon', _for=elementid), ' ', INPUT(_id=elementid, _type='text', _size=12))

########NEW FILE########
__FILENAME__ = db
# -*- coding: utf-8 -*-

from gluon.tools import Mail, Auth, Crud, Service, PluginManager
from gluon import current
from utils import web2py_uuid
import os

# Database settings
if 'db' in settings.kvasir_config:
    settings.migrate = settings.kvasir_config.get('db').get('migrate', True)
    settings.fake_migrate = settings.kvasir_config.get('db').get('fake_migrate', False)
    settings.database_uri = settings.kvasir_config.get('db').get('kvasir').get('uri')
    pool_size = settings.kvasir_config.get('db').get('kvasir').get('pool_size', 10)
    lazy_tables = settings.kvasir_config.get('db').get('lazy_tables', False)
    #settings.msfdb_uri = settings.kvasir_config.get('db').get('msf').get('uri')
else:
    settings.migrate = True
    settings.fake_migrate = False
    settings.database_uri = 'sqlite://kvasir.sqlite'
    pool_size = 5
    lazy_tables = False

if request.env.web2py_runtime_gae:            # if running on Google App Engine
    db = DAL('gae')                           # connect to Google BigTable
    session.connect(request, response, db = db) # and store sessions and tickets there
    ### or use the following lines to store sessions in Memcache
    # from gluon.contrib.memdb import MEMDB
    # from google.appengine.api.memcache import Client
    # session.connect(request, response, db = MEMDB(Client()))
else:                                         # else use a normal relational database
    db = DAL(settings.database_uri, check_reserved=['all'], lazy_tables=lazy_tables, pool_size=pool_size)

auth = Auth(db)                      # authentication/authorization
crud = Crud(db)                      # for CRUD helpers using auth
service = Service()                   # for json, xml, jsonrpc, xmlrpc, amfrpc
plugins = PluginManager()
#db._common_fields.append(auth.signature)

auth.settings.hmac_key = settings.kvasir_config.get('security_key') # before define_tables()
auth.settings.actions_disabled = [ 'register', 'request_reset_password', 'retrieve_username' ]
auth.settings.allow_basic_login=True
#crud.settings.auth = auth                      # =auth to enforce authorization on crud
current.auth = auth

response.generic_patterns = ['*.load', '*.json', '*.xml', '*.html', '*.csv']
response.combine_files = True
response.minify_files = True

##-------------------------------------------------------------------------
# logging

import logging, logging.handlers

def get_configured_logger(name):
    logger = logging.getLogger(name)
    if (len(logger.handlers) == 0):
        # This logger has no handlers, so we can assume it hasn't yet been configured
        formatter="%(asctime)s %(levelname)s %(process)s %(thread)s %(funcName)s():%(lineno)d %(message)s"
        logging.basicConfig(format=formatter, level=logging.DEBUG)

    return logger

# Assign application logger to a global var
logger = get_configured_logger(request.application)

########NEW FILE########
__FILENAME__ = db_01_static
#coding:utf-8

#--------------------------------------#
# Kvasir Static Table Definitions
#
# Vulnerabilities, Operating Systems
# Row entries are generally static in these tables,
# filled from master database and/or XML import scripts
#
# (c) 2010-2014 Cisco Systems, Inc.
#
# Author: Kurt Grutzmacher <kgrutzma@cisco.com>
#--------------------------------------#

########################################
auth.settings.extra_fields['auth_user']= [
    Field('f_host_detail', 'string', label=T('Host Detail Page'), default='detail'),
    Field('f_show_size', 'string', label=T('Table Show Start'), default='50', requires=IS_IN_SET(('10', '50', '100', '200', '500', 'All'), multiple=False)),
    Field('f_host_detail_tab', 'string', label=T('First Tab on Host Detail'), default='Services', requires=IS_IN_SET(('Services', 'Vulnerabilities', 'Notes', 'Evidence', 'OS', 'Accounts', 'SNMP'), multiple=False)),
    Field('f_tabletools', 'boolean', label=T('Enable TableTools'), default=True),
    Field('f_scheduler_tasks', 'boolean', label=T('Default Background Tasks'), default=True),
]
auth.define_tables(username=True, fake_migrate=settings.fake_migrate, migrate=settings.migrate)   # creates all needed tables

auth.settings.actions_disabled.append('register')
auth.settings.actions_disabled.append('retrieve_username')
auth.settings.actions_disabled.append('request_reset_password')
auth.settings.registration_requires_verification = False
auth.settings.registration_requires_approval = False

########################################
## Vulnerabilities
db.define_table('t_vulndata',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('vulns','vulndata_edit',args=id)))),
    Field('f_vulnid', type='string', length=255, unique=True, label=T('Vulnerability ID'),
          requires=[IS_NOT_EMPTY(), IS_SLUG()],
          represent=lambda id,row:SPAN(A(id,_href=URL('vulns','vulninfo_by_vulnid', args=id)))),
    Field('f_title', type='string', label=T('Title'), requires=IS_NOT_EMPTY()),
    Field('f_severity', type='integer', label=T('Severity'), requires=IS_IN_SET([x for x in range(0, 11)])),
    Field('f_pci_sev', type='integer', label=T('PCI Severity'), requires=IS_IN_SET([x for x in range(1, 6)])),
    Field('f_riskscore', type='string', label=T('Risk score')),
    Field('f_dt_published', type='datetime', label=T('Date Published')),
    Field('f_dt_added', type='datetime', label=T('Date Added')),
    Field('f_dt_modified', type='datetime', label=T('Date Modified')),
    Field('f_cvss_score', type='float', label=T('CVSS Score'), requires=IS_FLOAT_IN_RANGE(0, 10)),
    Field('f_cvss_i_score', type='float', label=T('CVSS Temporal Score')),
    Field('f_cvss_e_score', type='float', label=T('CVSS Enviromental Score')),
    Field('f_cvss_av', type='string', label=T('CVSS Access Vector'),
          requires=IS_EMPTY_OR(IS_IN_SET([('L', 'Local Access'), ('A', 'Adjacent Network'), ('N', 'Network')]))),
    Field('f_cvss_ac', type='string', label=T('CVSS Access Complexity'),
          requires=IS_EMPTY_OR(IS_IN_SET([('H', 'High'), ('M', 'Medium'), ('L', 'Low')]))),
    Field('f_cvss_au', type='string', label=T('CVSS Authentication'),
          requires=IS_EMPTY_OR(IS_IN_SET([('N', 'None required'), ('S', 'Single instance'), ('M', 'Multiple instances')]))),
    Field('f_cvss_c', type='string', label=T('CVSS Confidentiality Impact'),
          requires=IS_EMPTY_OR(IS_IN_SET([('N', 'None'), ('P', 'Partial'), ('C', 'Complete')]))),
    Field('f_cvss_i', type='string', label=T('CVSS Integrity Impact'),
          requires=IS_EMPTY_OR(IS_IN_SET([('N', 'None'), ('P', 'Partial'), ('C', 'Complete')]))),
    Field('f_cvss_a', type='string', label=T('CVSS Availablity Impact'),
          requires=IS_EMPTY_OR(IS_IN_SET([('N', 'None'), ('P', 'Partial'), ('C', 'Complete')]))),
    Field('f_description', type='text', length=65535, represent=lambda x, row: MARKMIN(x), label=T('Description')),
    Field('f_solution', type='text', length=65535, represent=lambda x, row: MARKMIN(x), label=T('Solution')),
    Field('f_source', type='string', length=255, widget=autocomplete_bootstrap, label=T('Source'),
          default="Kvasir"),
    auth.signature,
    format='%(f_vulnid)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

#db.t_vulndata.f_vulnid.represent=lambda i:SPAN(A(i,_id="vuln_details_%s" % (i),_href=URL('vulns', 'vulninfo_by_vulnid',args=i)))

def ref_id_represent(f_source, f_text):
    """
    Return a string representing f_text with a ulink based upon the source

    NOTE: This is hardly complete, a full map is available from CVE at
    http://cve.mitre.org/data/refs/index.html but for now these will do.
    """

    ulinks = { 'CVE': lambda x: A(x, _target="reference", _href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=%s" % (x)),
               'URL': lambda x: A(x, _target="reference", _href=x),
               'BID': lambda x: A(x, _target="reference", _href="http://www.securityfocus.com/bid/%s" %(x)),
               'XF':  lambda x: A(x, _target="reference", _href="http://xforce.iss.net/xforce/xfdb/%s" % (x)),
               'ISS':  lambda x: A(x, _target="reference", _href="http://xforce.iss.net/xforce/xfdb/%s" % (x)),
               'MSKB': lambda x: A(x, _target="reference", _href="http://support.microsoft.com/kb/%s" %(x)),
               'OSVDB': lambda x: A(x, _target="reference", _href="http://osvdb.org/show/osvdb/%s" % (x)),
               'SECUNIA': lambda x: A(x, _target="reference", _href="https://secunia.com/advisories/%s" % (x)),
               'MANDRAKE': lambda x: A(x, _target="reference", _href='http://lwn.net/Alerts/Mandrake/'),
               'MANDRIVA': lambda x: A(x, _target="reference", _href='http://www.mandriva.com/security/advisories'),
               'REDHAT': lambda x: A(x,_target="reference",  _href='http://www.redhat.com/support/errata/index.html'),
               'CERT': lambda x: A(x, _target="reference", _href='http://www.cert.org/advisories'),
               'CERT-VN': lambda x: A(x, _target="reference", _href="https://www.kb.cert.org/vuls/id/%s" % (x)),
               'SECTRACK': lambda x: A(x, _target="reference", _href='http://www.securitytracker.com/'),
               'OVAL': lambda x: A(x, _target="reference", _href='https://oval.mitre.org/repository/data/SearchDefinitionAdv?id=%s&advsearch=Search' % (x.replace('OVAL', ''))),
               'MS': lambda x: A(x, _target="reference", _href='http://www.microsoft.com/technet/security/current.aspx'),
               'GENTOO': lambda x: A(x, _target="reference", _href='http://www.gentoo.org/security/en/glsa/'),
               'SANS-06': lambda x: A(x, _target="reference", _href='http://isc.sans.org'),
               'SANS-07': lambda x: A(x, _target="reference", _href='http://isc.sans.org'),
               'SANS-08': lambda x: A(x, _target="reference", _href='http://isc.sans.org'),
               'SANS-09': lambda x: A(x, _target="reference", _href='http://isc.sans.org'),
               'SANS-10': lambda x: A(x, _target="reference", _href='http://isc.sans.org'),
               'SANS-11': lambda x: A(x, _target="reference", _href='http://isc.sans.org'),
               'AIXAPAR': lambda x: A(x, _target="reference", _href='http://www-01.ibm.com/support/search.wss?rs=0&apar=only'),
               'APPLE': lambda x: A(x, _target="reference", _href='http://lists.apple.com/archives/security-announce'),
               'CISCO': lambda x: A(x, _target="reference", _href='http://www.cisco.com/en/US/products/products_security_advisories_listing.html'),
               'AUSCERT': lambda x: A(x, _target="reference", _href='http://www.auscert.org.au/Information/advisories.html'),
               'BEA': lambda x: A(x, _target="reference", _href='http://dev2dev.bea.com/advisoriesnotifications/index.csp'),
               'CALDERA': lambda x: A(x, _target="reference", _href='http://www.calderasystems.com/support/security/'),
               'CHECKPOINT': lambda x: A(x, _target="reference", _href='http://www.checkpoint.com/defense/advisories/public/summary.html'),
               'CIAC': lambda x: A(x, _target="reference", _href='http://ciac.llnl.gov/cgi-bin/index/bulletins'),
               'COMPAQ': lambda x: A(x, _target="reference", _href='http://ftp.support.compaq.com/patches/.new/security.html'),
               'CONECTIVA': lambda x: A(x, _target="reference", _href='http://lwn.net/Alerts/Conectiva/'),
               'DEBIAN': lambda x: A(x, _target="reference", _href='http://www.debian.org/security/'),
               'EEYE': lambda x: A(x, _target="reference", _href='http://research.eeye.com/html/advisories/index.html'),
               'ENGARDE': lambda x: A(x, _target="reference", _href='http://lwn.net/Alerts/EnGarde/'),
               'EXPLOIT-DB': lambda x: A(x, _target="reference", _href='http://www.exploit-db.com/exploits/%s' % (x)),
               'FEDORA': lambda x: A(x, _target="reference", _href='http://www.redhat.com/archives/fedora-announce-list/'),
               'FREEBSD': lambda x: A(x, _target="reference", _href='http://www.freebsd.org/security/'),
               'FRSIRT': lambda x: A(x, _target="reference", _href='http://www.vupen.com/english/'),
               'FULLDISC': lambda x: A(x, _target="reference", _href='http://lists.grok.org.uk/pipermail/full-disclosure/'),
               'FarmerVenema': lambda x: A(x, _target="reference", _href='http://www.alw.nih.gov/Security/Docs/admin-guide-to-cracking.101.html'),
               'HP': lambda x: A(x, _target="reference", _href='http://www.itrc.hp.com/service/cki/secBullArchive.do'),
               'IDEFENSE': lambda x: A(x, _target="reference", _href='http://labs.idefense.com/intelligence/vulnerabilities/'),
               'NETBSD': lambda x: A(x, _target="reference", _href='http://www.netbsd.org/Security/advisory.html'),
               'OPENBSD': lambda x: A(x, _target="reference", _href='http://www.openbsd.org/security.html'),
               'SUN': lambda x: A(x, _target="reference", _href='http://search.sun.com/main/index.jsp?col=main-support-sunalerts&oneof=security&nh=100&rf=1&type=advanced&optstat=true'),
               'SUNALERT': lambda x: A(x, _target="reference", _href='http://search.sun.com/main/index.jsp?col=main-support-sunalerts&oneof=security&nh=100&rf=1&type=advanced&optstat=true'),
               'SUSE': lambda x: A(x, _target="reference", _href='http://www.novell.com/linux/security/advisories.html'),
               'UBUNTU': lambda x: A(x, _target="reference", _href='http://www.ubuntu.com/usn/'),
             }

    if ulinks.has_key(f_source):
        return(ulinks[f_source](f_text))
    else:
        return f_text

########################################
## Vulnerability references
db.define_table('t_vuln_refs',
    Field('id', 'id', represent=lambda id,row:SPAN(A(id,_href=URL('vuln_refs_edit',args=id)))),
    Field('f_source', type='text', label=T('Source'), requires=IS_NOT_EMPTY()),
    Field('f_text', type='text', label=T('Text'), requires=IS_NOT_EMPTY(), represent=lambda id,row: ref_id_represent(row.f_source, row.f_text)),
    auth.signature,
    format=lambda r:ref_id_represent(r.f_source, r.f_text),
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Many-to-Many table for vulns and references
## Creates a new set, s_vulnerabilities
db.define_table('t_vuln_references',
    Field('id', 'id', represent=lambda id,row:SPAN(A(id,_href=URL('vulns', 'vuln_references_edit',args=id)))),
    Field('f_vulndata_id', 'reference t_vulndata', label=T('Vulnerability'), represent=lambda id,row:A(db.t_vulndata[id].f_vulnid, _href=URL('vulndata_edit', args=id))),
    Field('f_vuln_ref_id', 'reference t_vuln_refs', label=T('Reference'), represent=lambda id,row: " :: ".join([db.t_vuln_refs[id].f_source, db.t_vuln_refs[id].f_text])),
    auth.signature,
    format=lambda r:XML(ref_id_represent(db.t_vuln_refs[r.f_vuln_ref_id].f_source, db.t_vuln_refs[r.f_vuln_ref_id].f_source)),
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)
s_vulnerabilities = db(db.t_vuln_refs.id == db.t_vuln_references.f_vuln_ref_id)

########################################
## Exploits
db.define_table('t_exploits',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('exploits', 'edit', args=id)))),
    Field('f_name', type='string', label=T('Name'), requires=IS_NOT_EMPTY()),
    Field('f_title', type='string', label=T('Title'), requires=IS_NOT_EMPTY()),
    Field('f_description', type='text', label=T('Description'), requires=IS_NOT_EMPTY()),
    Field('f_source', type='string', label=T('Source'), default='other',
        requires=IS_IN_SET(('exploitdb', 'metasploit', 'metasploit2', 'canvas', 'core', 'other'), multiple=False)),
    Field('f_rank', type='string', label=T('Exploit quality ranking'), default='Unknown',
        requires=IS_IN_SET(('Unknown', 'Novice', 'Intermediate', 'Expoert'))),
    Field('f_level', type='string', label=T('Exploit knowledge level'), default='unknown',
        requires=IS_IN_SET(('unknown', 'manual', 'low', 'average', 'good', 'normal', 'great', 'excellent'), multiple=False)),
    Field('f_vulnid', type='list:string', label=T('Vulnerability List')),
    Field('f_cve', type='list:string', label=T('CVE List')),
    auth.signature,
    format=lambda r: XML(A(" :: ".join([r.f_source, r.f_name, r.f_title, r.f_rank]),
                           _href=URL('exploits', 'edit', extension='html', args=r.id), _target='blank')),
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Many-to-Many table for exploits and references
## Creates a new set, s_exploit_info
db.define_table('t_exploit_references',
    Field('f_exploit_id', 'reference t_exploits', label=T('Exploit')),
    Field('f_vulndata_id', 'reference t_vulndata'),
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)
s_exploit_info = db((db.t_exploits.id==db.t_exploit_references.f_exploit_id) | (db.t_vuln_refs.id==db.t_vuln_references.f_vuln_ref_id))

########################################
## CPE pre-defined operating systems
db.define_table('t_cpe_os',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('cpe', 'os_edit',args=id)))),
    Field('f_cpename', type='string', label=T('CPE Name'), requires=IS_NOT_EMPTY()),
    Field('f_title', type='string', label=T('Title'), requires=IS_NOT_EMPTY()),
    Field('f_vendor', type='string', label=T('Vendor')),
    Field('f_product', type='string', label=T('Product')),
    Field('f_version', type='string', label=T('Version')),
    Field('f_update', type='string', label=T('Update')),
    Field('f_edition', type='string', label=T('Edition')),
    Field('f_language', type='string', label=T('Language')),
    auth.signature,
    format='%(f_cpename)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## CPE pre-defined applications
db.define_table('t_cpe_apps',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('cpe', 'apps_edit',args=id)))),
    Field('f_cpename', type='string', label=T('CPE Name'), requires=IS_NOT_EMPTY()),
    Field('f_title', type='string', label=T('Title'), requires=IS_NOT_EMPTY()),
    Field('f_vendor', type='string', label=T('Vendor')),
    Field('f_product', type='string', label=T('Product')),
    Field('f_version', type='string', label=T('Version')),
    Field('f_update', type='string', label=T('Update')),
    Field('f_edition', type='string', label=T('Edition')),
    Field('f_language', type='string', label=T('Language')),
    auth.signature,
    format='%(f_vendor)s %(f_product)s %(f_version)',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## CPE pre-defined hardware
db.define_table('t_cpe_hardware',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('cpe', 'hardware_edit',args=id)))),
    Field('f_cpename', type='string', label=T('CPE Name'), requires=IS_NOT_EMPTY()),
    Field('f_title', type='string', label=T('Title'), requires=IS_NOT_EMPTY()),
    Field('f_vendor', type='string', label=T('Vendor')),
    Field('f_product', type='string', label=T('Product')),
    Field('f_version', type='string', label=T('Version')),
    Field('f_update', type='string', label=T('Update')),
    Field('f_edition', type='string', label=T('Edition')),
    Field('f_language', type='string', label=T('Language')),
    auth.signature,
    format='%(f_vendor)s %(f_product)s %(f_version)',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## ExploitDB files.csv
## id,file,description,date,author,platform,type,port
db.define_table('t_exploitdb',
    Field('id', 'id', represent=lambda id,row:SPAN(A(id, _href=URL('exploitdb', 'detail', args=id)))),
    Field('f_eid', 'integer', label=T('ExploitDB ID')),
    Field('f_file', type='string', label=T('Filename')),
    Field('f_description', type='string', label=T('Description')),
    Field('f_date', type='string', label=T('Date')),
    Field('f_author', type='string', label=T('Author')),
    Field('f_platform', type='string', label=T('Platform')),
    Field('f_type', type='string', label=T('Type')),
    Field('f_port', type='string', label=T('Port')),
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########NEW FILE########
__FILENAME__ = db_02_kvasir
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir Dynamic Table Definitions
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Dynamic tables that create Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

from skaldship.hosts import host_title_maker


########################################
## Hosts table
########################################
db.define_table('t_hosts',
    Field('id','id', represent=lambda i, row:SPAN(A(i,_id="host_detail_%s" % (i),_href=URL('hosts', 'detail',args=i)))),
    Field('f_ipv4', type='string', length=15, unique=True, requires=IS_EMPTY_OR(IS_IPV4()), label=T('IPv4 Address')),
    Field('f_ipv6', type='string', label=T('IPv6 Address'), requires=IS_EMPTY_OR(IS_IPV6())),
    Field('f_macaddr', type='string', label=T('MAC Address')),
    Field('f_hostname', type='string', label=T('Hostname')),
    Field('f_netbios_name', type='string', label=T('NetBIOS Name')),
    Field('f_confirmed', type='boolean', default=False, label=T('Confirmed')),
    Field('f_accessed', type='boolean', default=False, label=T('Accessed'), comment=T('Host has been accessed by an Engineer')),
    Field('f_followup', type='boolean', label=T('Follow Up')),
    Field('f_engineer', type='reference auth_user', label=T('Engineer')),
    Field('f_asset_group', type='string', label=T('Asset Group'), widget=autocomplete_bootstrap, requires=IS_NOT_EMPTY()),
    Field('f_service_count', type='integer', default=0, label=T('Service Count')),
    Field('f_vuln_count', type='integer', default=0, label=T('Vuln Count')),
    Field('f_vuln_graph', type='string', default='0,0,0,0,0,0,0,0,0,0', label=T('Vuln Graph')),
    Field('f_exploit_count', type='integer', default=0, label=T('Exploit Count')),
    format=lambda r: host_title_maker(r),
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Customer operating system records
## These SHOULD be copied from CPE but
## if not they'll be flagged as such.
db.define_table('t_os',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('os', 'os_edit',args=id)))),
    Field('f_cpename', length=255, type='string', label=T('CPE Name'), unique=False),
    Field('f_title', type='string', label=T('Title'), requires=IS_NOT_EMPTY()),
    Field('f_vendor', type='string', label=T('Vendor'), widget=autocomplete_bootstrap, requires=IS_NOT_EMPTY()),
    Field('f_product', type='string', label=T('Product'), widget=autocomplete_bootstrap, requires=IS_NOT_EMPTY()),
    Field('f_version', type='string', label=T('Version')),
    Field('f_update', type='string', label=T('Update')),
    Field('f_edition', type='string', label=T('Edition')),
    Field('f_language', type='string', label=T('Language')),
    Field('f_isincpe', type='boolean', default=False, label=T('Sourced from CPE')),
    format='%(f_cpename)s :: %(f_title)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Many-to-Many table for hosts and os
## Creates a new set result, s_host_os
db.define_table('t_host_os_refs',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('os', 'os_ref_edit',args=id)))),
    Field('f_certainty', 'double', label=T('Certainty'), requires=IS_FLOAT_IN_RANGE(0, 1), comment=T('Must be a float in range of 0 to 1.0')),
    Field('f_class', 'string', label=T('Device Class'), widget=autocomplete_bootstrap, requires=IS_NOT_EMPTY()),
    Field('f_family', 'string', label=T('Family'), widget=autocomplete_bootstrap, requires=IS_NOT_EMPTY()),
    Field('f_hosts_id', type='reference t_hosts', label=T('Host'), represent=lambda id,row:XML(host_title_maker(db.t_hosts[id]))),
    Field('f_os_id', 'reference t_os', label=T('OS'), represent=lambda id,row:db.t_os[id].f_title),
    fake_migrate=settings.fake_migrate, migrate=settings.migrate,
    )
s_host_os = db((db.t_hosts.id==db.t_host_os_refs.f_hosts_id) | (db.t_os.id==db.t_host_os_refs.f_os_id))

########################################
## Services (ports, essentially)
db.define_table('t_services',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('services', 'edit',args=id)))),
    Field('f_proto', type='string', notnull=True, label=T('Protocol'), default="tcp", requires=IS_IN_SET(('tcp', 'udp', 'info', 'other'))),
    Field('f_number', type='string', notnull=True, label=T('Number'), requires=IS_INT_IN_RANGE(0, 65536)),
    Field('f_status', type='string', label=T('Status'), default="open", requires=IS_IN_SET(('open', 'closed', 'info'))),
    Field('f_name', type='string', label=T('Service Name'), widget=autocomplete_bootstrap),
    Field('f_banner', type='text', label=T('Banner')),
    Field('f_hosts_id', type='reference t_hosts', label=T('Host'), represent=lambda id,row:XML(host_title_maker(db.t_hosts[id]))),
    format=lambda r:XML("%s :: %s/%s" % (db.t_hosts[r.f_hosts_id.id].f_ipv4, r.f_proto, r.f_number)),
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Customer applications, like t_os
## these should be copies from t_cpe_apps
db.define_table('t_apps',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('apps', 'apps_edit',args=id)))),
    Field('f_cpename', type='string', label=T('CPE Name'), requires=IS_NOT_EMPTY()),
    Field('f_title', type='string', label=T('Title'), requires=IS_NOT_EMPTY()),
    Field('f_vendor', type='string', label=T('Vendor'), widget=autocomplete_bootstrap),
    Field('f_product', type='string', label=T('Product'), widget=autocomplete_bootstrap),
    Field('f_version', type='string', label=T('Version')),
    Field('f_update', type='string', label=T('Update')),
    Field('f_edition', type='string', label=T('Edition')),
    Field('f_language', type='string', label=T('Language')),
    Field('f_isincpe', type='boolean', default=False, label=T('Sourced from CPE')),
    format='%(f_vendor)s %(f_product)s %(f_version)',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Many-to-Many table for services and apps
## Creates a new set, s_app_fingerprints
db.define_table('t_services_apps_refs',
    Field('f_certainty', 'double', label=T('Certainty')),
    Field('f_services_id', 'reference t_services'),
    Field('f_apps_id', 'reference t_apps'),
    format='%(f_services_id.f_number)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate,
    )
s_app_fingerprints = db((db.t_services.id==db.t_services_apps_refs.f_services_id) | (db.t_apps.id==db.t_services_apps_refs.f_apps_id))

vuln_status_set = [
    'exploited',
    'vulnerable',
    'vulnerable-exploited',
    'vulnerable-version',
    'general',
    'potential',
    'exception-vulnerable-exploited',
    'exception-vulnerable-version',
    'exception-vulnerable-potential',
]

########################################
## Service vulnerabilities
## Vulns are associated to services, this links t_vulndata to t_services
## and adds status and proof fields
db.define_table('t_service_vulns',
    Field('id','id',represent=lambda id,row:SPAN(A(id,_href=URL('vulns' 'service_vulns_edit',args=id)))),
    Field('f_services_id', 'reference t_services', label=T('Service')),
    Field('f_vulndata_id', 'reference t_vulndata', label=T('Vulnerability')),
    Field('f_status', type='string', label=T('Status'),
        requires=IS_IN_SET(vuln_status_set)),
    Field('f_proof', type='text', length=65535, label=T('Proof'), represent=lambda x, row: MARKMIN(x)),
    Field('f_exploited', 'boolean', default=False, label=T('Exploited')),
    format='%(f_service.f_proto)s/%(f_service.f_number)s :: %(f_status)s :: %(f_vulnid)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

s_service_vuln_data = db(db.t_service_vulns.f_vulndata_id==db.t_vulndata.id)

########################################
## Service info
## Additional service info such as netbios names
db.define_table('t_service_info',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('services', 'info_edit',args=id)))),
    Field('f_services_id', 'reference t_services', label=T('Service'),
        represent=lambda id,row:XML("%s :: %s/%s" % (host_title_maker(db.t_hosts[db.t_services[id].f_hosts_id]), db.t_services[id].f_proto, db.t_services[id].f_number))),
    Field('f_name', type='text', label=T('Key'), requires=IS_NOT_EMPTY(), widget=autocomplete_bootstrap),
    Field('f_text', type='text', length=2048, label=T('Value'), requires=IS_NOT_EMPTY(), widget=autocomplete_bootstrap),
    format='%(f_service.f_proto)s/%(f_service.f_number)s :: %(f_name) :: %(f_text)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Accounts
## Linked to t_services

## list of password file types supported by the upload process
settings.password_file_types = (
    'PWDUMP',
    'MSCa$h Dump',
    'UNIX Passwd',
    'UNIX Shadow',
    'Medusa',
    'Hydra',
    'Metasploit Creds CSV',
    'Username:Password',
    'Usernames',
    'AccountDB',
)

db.define_table('t_accounts',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('edit',args=id)))),
    Field('f_services_id', 'reference t_services', label=T('Service'),
        represent=lambda id,row:XML("%s :: %s/%s" % (host_title_maker(db.t_hosts[db.t_services[id].f_hosts_id]), db.t_services[id].f_proto, db.t_services[id].f_number))),
    Field('f_username', type='string', label=T('Username'), widget=autocomplete_bootstrap, requires=IS_NOT_EMPTY()),
    Field('f_fullname', type='string', label=T('Fullname'), widget=autocomplete_bootstrap),
    Field('f_password', type='string', label=T('Password'), widget=autocomplete_bootstrap),
    Field('f_compromised', type='boolean', label=T('Compromised')),
    Field('f_hash1', type='string', label=T('Hash1')),
    Field('f_hash1_type', type='string', label=T('Hash1 Type'), widget=autocomplete_bootstrap),
    Field('f_hash2', type='string', label=T('Hash2')),
    Field('f_hash2_type', type='string', label=T('Hash2 Type'), widget=autocomplete_bootstrap),
    Field('f_source', type='string', label=T('Source'), widget=autocomplete_bootstrap),
    Field('f_uid', type='string', label=T('UID')),
    Field('f_gid', type='string', label=T('GID')),
    Field('f_level', type='string', label=T('Level'), requires=IS_IN_SET(('ADMIN', 'USER', 'SERVICE'), multiple=False), default="ADMIN"),
    Field('f_domain', type='string', label=T('Domain'), widget=autocomplete_bootstrap),
    Field('f_message', type='string', label=T('Message')),
    Field('f_lockout', type='boolean', label=T('Lockoutable'), default=False),
    Field('f_duration', type='integer', label=T('Lockout Duration')),
    Field('f_active', type='boolean', label=T('Active')),
    Field('f_description', type='string', label=T('Description')),
    format='%(f_username)s :: %(f_password)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

#s_service_accts = db( (db.t_accounts.f_services_id==db.t_services.id) & (db.t_hosts.id == db.t_services.f_hosts_id) )

########################################
## User Groups
#db.define_table('t_groups',
#    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('accounts', 'group_edit',args=id)))),
#    Field('f_services_id', db.t_services, label=T('Service')),
#    Field('f_name', type='string', label=T('Name')),
#    Field('f_password', type='string', label=T('Password')),
#    Field('f_groupid', type='string', label=T('GroupID')),
#    Field('f_sid', type='string', label=T('Windows SID')),
#    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Accounts -> Group References and Set()
## TODO: Nested Group references
#db.define_table('t_group_account_refs',
#    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('acct_groups_edit',args=id)))),
#    Field('f_accounts_id', db.t_accounts, label=T('Account')),
#    Field('f_groups_id', db.t_groups, label=T('Group')),
#    fake_migrate=settings.fake_migrate, migrate=settings.migrate)
#s_account_groups = db((db.t_accounts.id==db.t_group_account_refs.f_accounts_id) | (db.t_groups.id==db.t_group_account_refs.f_groups_id))

########################################
## Host-specific notes (not seen by customer)
db.define_table('t_host_notes',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('notes', 'edit',args=id)))),
    Field('f_hosts_id', type='reference t_hosts', label=T('Host'), represent=lambda id,row:XML(host_title_maker(db.t_hosts[id]))),
    Field('f_note', type='text', represent=lambda x, row: MARKMIN(x), label=T('Note'), requires=IS_NOT_EMPTY()),
    format='%(f_note)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Evidence files
db.define_table('t_evidence',
    Field('id', 'id', represent=lambda id, row: SPAN(A(id, _href=URL('evidence', 'edit', args=id)))),
    Field('f_hosts_id', type='reference t_hosts', label=T('Host'), represent=lambda id, row: XML(host_title_maker(db.t_hosts[id]))),
    Field('f_type', type='list:string', label=T('Type'), requires=IS_IN_SET(
        ('Log file', 'Screenshot', 'Password file', 'Router/Switch Config', 'Database Data', 'Session Log', 'Other'))),
    Field('f_other_type', type='string', label=T('Other Type')),
    Field('f_text', type='text', label=T('Text')),
    Field('f_filename', type='string', label=T('Filename')),
    # Files are stored in the database, to change this uncomment the next line and comment the one after it
    #Field('f_evidence', 'upload', uploadfolder=os.path.join(request.folder, 'data', 'evidence'), label=T('File'), autodelete=False),
    Field('f_evidence', 'upload', label=T('File'), uploadfield='f_data'),
    Field('f_data', 'blob'),
    format='%(f_type)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## Tool output
db.define_table('t_tool_output',
    Field('id', 'id', represent=lambda id, row: SPAN(A(id, _href=URL('tooloutput', 'edit', args=id)))),
    Field('f_hosts_id', type='reference t_hosts', label=T('Host'), represent=lambda id, row: XML(host_title_maker(db.t_hosts[id]))),
    Field('f_type', type='string', label=T('Type'), widget=autocomplete_bootstrap),
    Field('f_note', type='text', label=T('Note')),
    Field('f_filename', type='string', label=T('Filename')),
    # Files are stored in the database, to change this uncomment the next line and comment the one after it
    #Field('f_evidence', 'upload', uploadfolder=os.path.join(request.folder, 'data', 'tool_output'), label=T('File'), autodelete=False),
    Field('f_output', 'upload', label=T('File'), uploadfield='f_data'),
    Field('f_data', 'blob'),
    format='%(f_type)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## SNMP Info
db.define_table('t_snmp',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('snmp', 'edit', args=id)))),
    Field('f_hosts_id', type='reference t_hosts', label=T('Host'), represent=lambda id,row:XML(host_title_maker(db.t_hosts[id]))),
    Field('f_community', type='string', label=T('Community String'), widget=autocomplete_bootstrap, requires=IS_NOT_EMPTY()),
    Field('f_version', type='string', label=T('SNMP Version'), default="v1", requires=IS_IN_SET(('v1', 'v2c', 'v3'), multiple=False)),
    Field('f_access', type='string', label=T('Access Type'), default="READ", requires=IS_IN_SET(('READ', 'WRITE'), multiple=False)),
    format='%(f_note_type)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########################################
## NetBIOS Info
db.define_table('t_netbios',
    Field('id','id', represent=lambda id,row:SPAN(A(id,_href=URL('netbios', 'edit',args=id)))),
    Field('f_hosts_id', type='reference t_hosts', label=T('Host'), unique=True, represent=lambda id,row:XML(host_title_maker(db.t_hosts[id]))),
    Field('f_type', type='string', label=T('Server Type'), default="Workstation", requires=IS_IN_SET(('Server', 'Workstation', 'PDC', 'BDC', 'Other'))),
    Field('f_advertised_names', type='list:string', label=T('Advertised Names')),
    Field('f_domain', type='string', label=T('Domain Name'), widget=autocomplete_bootstrap),
    Field('f_lockout_limit', type='integer', label=T('Lockout Limit'), default=0),
    Field('f_lockout_duration', type='integer', label=T('Lockout Duration'), default=1440),
    Field('f_shares', type='list:string', label=T('Shares')),
    format='%(f_note_type)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########NEW FILE########
__FILENAME__ = db_03_support
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir Support Table Definitions
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

#########################################
## Event log
db.define_table('t_event_log',
    Field('id', 'id'),
    Field('f_text', 'string', label=T('Message')),
    Field('f_seen', 'boolean', default=False, label=T('Seen')),
    Field('f_ack', 'boolean', default=False, label=T('Acknowledged')),
    auth.signature,
    format='%(f_text)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

#########################################
## +5 bag of holding things
db.define_table('t_errata',
    Field('id', 'id'),
    Field('f_key', 'string', label=T('Key'), requires=IS_NOT_EMPTY()),
    Field('f_value', 'string', label=T('Value'), requires=IS_NOT_EMPTY()),
    auth.signature,
    format='%(f_value)s',
    fake_migrate=settings.fake_migrate, migrate=settings.migrate)

########NEW FILE########
__FILENAME__ = db_05_metasploit
# -*- coding: utf-8 -*-

##--------------------------------------#
## Metasploit Table Definitions
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

# Note that this makes direct queries to the Metasploit database. Kvasir
# will have full read/write privileges to all Workspaces.
#
# At this time only read-access is required as we do not do any writes
# to the db as our purpose here is to retrieve data from Metasploit
# that the API doesn't do. Any actions requested by Kvasir to Metasploit
# are performed through the API.

if settings.msfdb_uri:
    msfdb = DAL(settings.msfdb_uri, pool_size=10)

    migrate = False

    msfdb.define_table('cred_files',
        Field('id', type='id'),
        Field('workspace_id', type='integer', default=1),
        Field('path', type='string', length=1024),
        Field('ftype', type='string', length=16),
        Field('created_by', type='string', length=255),
        Field('name', type='string', length=512),
        Field('desc', type='string', length=1024),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        migrate=migrate)

    """
    XXX: creds must use a executesql query since it uses a python
         keyword as the field name ('pass')
    msfdb.define_table('creds',
        Field('id', type='id'),
        Field('service_id', type='integer'),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        Field('user', type='string', length=2048),
        Field('pass', type='string', length=4096),
        Field('active', type='boolean', default=True),
        Field('proof', type='string', length=4096),
        Field('ptype', type='string', length=256),
        Field('source_id', type='integer'),
        Field('source_type', type='string', length=255),
        migrate=migrate)
    """

    msfdb.define_table('exploit_attempts',
        Field('id', type='id'),
        Field('host_id', type='integer'),
        Field('service_id', type='integer'),
        Field('vuln_id', type='integer'),
        Field('attempted_at', type='datetime'),
        Field('exploited', type='boolean'),
        Field('fail_reason', type='string', length=255),
        Field('username', type='string', length=255),
        Field('module', type='text'),
        Field('session_id', type='integer'),
        Field('loot_id', type='integer'),
        Field('port', type='integer'),
        Field('proto', type='string', length=255),
        Field('fail_detail', type='text'),
        migrate=migrate)

    msfdb.define_table('exploited_hosts',
        Field('id', type='id'),
        Field('host_id', type='integer'),
        Field('service_id', type='integer'),
        Field('session_uuid', type='string', length=8),
        Field('name', type='string', length=2048),
        Field('payload', type='string', length=2048),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        migrate=migrate)

    msfdb.define_table('host_details',
        Field('id', type='id'),
        Field('host_id', type='integer'),
        Field('nx_console_id', type='integer'),
        Field('nx_device_id', type='integer'),
        Field('src', type='string', length=255),
        Field('nx_site_name', type='string', length=255),
        Field('nx_site_importance', type='string', length=255),
        Field('nx_scan_template', type='string', length=255),
        Field('nx_risk_score', type='double'),
        migrate=migrate)

    msfdb.define_table('hosts',
        Field('id', type='id'),
        Field('created_at', type='datetime'),
        Field('address', type='string'),
        Field('mac', type='string', length=255),
        Field('comm', type='string', length=255),
        Field('name', type='string', length=255),
        Field('state', type='string', length=255),
        Field('os_name', type='string', length=255),
        Field('os_flavor', type='string', length=255),
        Field('os_sp', type='string', length=255),
        Field('os_lang', type='string', length=255),
        Field('arch', type='string', length=255),
        Field('workspace_id', type='integer'),
        Field('updated_at', type='datetime'),
        Field('purpose', type='text'),
        Field('info', type='string', length=65536),
        Field('comments', type='text'),
        Field('scope', type='text'),
        Field('virtual_host', type='text'),
        Field('note_count', type='integer', default=0),
        Field('vuln_count', type='integer', default=0),
        Field('service_count', type='integer', default=0),
        Field('host_detail_count', type='integer', default=0),
        Field('exploit_attempt_count', type='integer', default=0),
        migrate=migrate)

    msfdb.define_table('hosts_tags',
        Field('host_id', type='integer'),
        Field('tag_id', type='integer'),
        migrate=migrate)

    """
    XXX: imported_creds must use a executesql query since it uses a python
         keyword as the field name ('pass')
    msfdb.define_table('imported_creds',
        Field('id', type='id'),
        Field('workspace_id', type='integer', default=1),
        Field('user', type='string', length=512),
        Field('pass', type='string', length=512),
        Field('ptype', type='string', length=16),
        migrate=migrate)
    """

    msfdb.define_table('loots',
        Field('id', type='id'),
        Field('workspace_id', type='integer', default=1),
        Field('host_id', type='integer'),
        Field('service_id', type='integer'),
        Field('ltype', type='string', length=512),
        Field('path', type='string', length=1024),
        Field('data', type='text'),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        Field('content_type', type='string', length=255),
        Field('name', type='text'),
        Field('info', type='text'),
        migrate=migrate)

    msfdb.define_table('notes',
        Field('id', type='id'),
        Field('created_at', type='datetime'),
        Field('ntype', type='string', length=512),
        Field('workspace_id', type='integer', default=1),
        Field('service_id', type='integer'),
        Field('host_id', type='integer'),
        Field('updated_at', type='datetime'),
        Field('critical', type='boolean'),
        Field('seen', type='boolean'),
        Field('data', type='text'),
        migrate=migrate)

    msfdb.define_table('refs',
        Field('id', type='id'),
        Field('ref_id', type='integer'),
        Field('created_at', type='datetime'),
        Field('name', type='string', length=512),
        Field('updated_at', type='datetime'),
        migrate=migrate)

    msfdb.define_table('services',
        Field('id', type='id'),
        Field('host_id', type='integer'),
        Field('created_at', type='datetime'),
        Field('port', type='integer'),
        Field('proto', type='string', length=16),
        Field('state', type='string', length=255),
        Field('name', type='string', length=255),
        Field('updated_at', type='datetime'),
        Field('info', type='text'),
        migrate=migrate)

    msfdb.define_table('session_events',
        Field('id', type='id'),
        Field('session_id', type='integer'),
        Field('etype', type='string', length=255),
        Field('command', type='blob'),
        Field('output', type='blob'),
        Field('remote_path', type='string', length=255),
        Field('local_path', type='string', length=255),
        Field('created_at', type='datetime'),
        migrate=migrate)

    msfdb.define_table('sessions',
        Field('id', type='id'),
        Field('host_id', type='integer'),
        Field('stype', type='string', length=255),
        Field('via_exploit', type='string', length=255),
        Field('via_payload', type='string', length=255),
        Field('desc', type='string', length=255),
        Field('port', type='integer'),
        Field('platform', type='string', length=255),
        Field('datastore', type='text'),
        Field('opened_at', type='datetime'),
        Field('closed_at', type='datetime'),
        Field('close_reason', type='string', length=255),
        Field('local_id', type='integer'),
        Field('last_seen', type='datetime'),
        migrate=migrate)

    msfdb.define_table('tags',
        Field('id', type='id'),
        Field('user_id', type='integer'),
        Field('name', type='string', length=1024),
        Field('desc', type='text'),
        Field('report_summary', type='boolean', default=False),
        Field('report_detail', type='boolean', default=False),
        Field('critical', type='boolean', default=False),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        migrate=migrate)

    msfdb.define_table('tasks',
        Field('id', type='id'),
        Field('workspace_id', type='integer', default=1),
        Field('created_by', type='string', length=255),
        Field('module', type='string', length=255),
        Field('completed_at', type='datetime'),
        Field('path', type='string', length=1024),
        Field('info', type='string', length=255),
        Field('description', type='string', length=255),
        Field('progress', type='integer'),
        Field('options', type='text'),
        Field('error', type='text'),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        Field('result', type='text'),
        Field('module_uuid', type='string', length=8),
        Field('settings', type='blob'),
        migrate=migrate)

    msfdb.define_table('vuln_attempts',
        Field('id', type='id'),
        Field('vuln_id', type='integer'),
        Field('attempted_at', type='datetime'),
        Field('exploited', type='boolean'),
        Field('fail_reason', type='string', length=255),
        Field('username', type='string', length=255),
        Field('module', type='text'),
        Field('session_id', type='integer'),
        Field('loot_id', type='integer'),
        Field('fail_detail', type='text'),
        migrate=migrate)

    msfdb.define_table('vuln_details',
        Field('id', type='id'),
        Field('vuln_id', type='integer'),
        Field('cvss_score', type='double'),
        Field('cvss_vector', type='string', length=255),
        Field('title', type='string', length=255),
        Field('description', type='text'),
        Field('solution', type='text'),
        Field('proof', type='blob'),
        Field('nx_console_id', type='integer'),
        Field('nx_device_id', type='integer'),
        Field('nx_vuln_id', type='string', length=255),
        Field('nx_severity', type='double'),
        Field('nx_pci_severity', type='double'),
        Field('nx_published', type='datetime'),
        Field('nx_added', type='datetime'),
        Field('nx_modified', type='datetime'),
        Field('nx_tags', type='text'),
        Field('nx_vuln_status', type='text'),
        Field('nx_proof_key', type='text'),
        Field('src', type='string', length=255),
        Field('nx_scan_id', type='integer'),
        Field('nx_vulnerable_since', type='datetime'),
        Field('nx_pci_compliance_status', type='string', length=255),
        migrate=migrate)

    msfdb.define_table('vulns',
        Field('id', type='id'),
        Field('host_id', type='integer'),
        Field('service_id', type='integer'),
        Field('created_at', type='datetime'),
        Field('name', type='string', length=255),
        Field('updated_at', type='datetime'),
        Field('info', type='string', length=65536),
        Field('exploited_at', type='datetime'),
        Field('vuln_detail_count', type='integer', default=0),
        Field('vuln_attempt_count', type='integer', default=0),
        migrate=migrate)

    msfdb.define_table('vulns_refs',
        Field('ref_id', type='integer'),
        Field('vuln_id', type='integer'),
        migrate=migrate)

    msfdb.define_table('web_forms',
        Field('id', type='id'),
        Field('web_site_id', type='integer'),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        Field('path', type='text'),
        Field('method', type='string', length=1024),
        Field('params', type='text'),
        Field('query', type='text'),
        migrate=migrate)

    msfdb.define_table('web_pages',
        Field('id', type='id'),
        Field('web_site_id', type='integer'),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        Field('path', type='text'),
        Field('query', type='text'),
        Field('code', type='integer'),
        Field('cookie', type='text'),
        Field('auth', type='text'),
        Field('ctype', type='text'),
        Field('mtime', type='datetime'),
        Field('location', type='text'),
        Field('headers', type='text'),
        Field('body', type='blob'),
        Field('request', type='blob'),
        migrate=migrate)

    msfdb.define_table('web_sites',
        Field('id', type='id'),
        Field('service_id', type='integer'),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        Field('vhost', type='string', length=2048),
        Field('comments', type='text'),
        Field('options', type='text'),
        migrate=migrate)

    msfdb.define_table('web_templates',
        Field('id', type='id'),
        Field('name', type='string', length=512),
        Field('title', type='string', length=512),
        Field('body', type='string', length=524288),
        Field('campaign_id', type='integer'),
        Field('prefs', type='text'),
        migrate=migrate)

    msfdb.define_table('web_vulns',
        Field('id', type='id'),
        Field('web_site_id', type='integer'),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        Field('path', type='text'),
        Field('method', type='string', length=1024),
        Field('params', type='text'),
        Field('pname', type='text'),
        Field('risk', type='integer'),
        Field('name', type='string', length=1024),
        Field('query', type='text'),
        Field('category', type='text'),
        Field('confidence', type='text'),
        Field('description', type='text'),
        Field('blame', type='text'),
        Field('request', type='blob'),
        Field('proof', type='blob'),
        Field('owner', type='string', length=255),
        Field('payload', type='text'),
        migrate=migrate)

    msfdb.define_table('wmap_requests',
        Field('id', type='id'),
        Field('host', type='string', length=255),
        Field('address', type='string'),
        Field('port', type='integer'),
        Field('ssl', type='integer'),
        Field('meth', type='string', length=32),
        Field('path', type='text'),
        Field('headers', type='text'),
        Field('query', type='text'),
        Field('body', type='text'),
        Field('respcode', type='string', length=16),
        Field('resphead', type='text'),
        Field('response', type='text'),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        migrate=migrate)

    msfdb.define_table('wmap_targets',
        Field('id', type='id'),
        Field('host', type='string', length=255),
        Field('address', type='string'),
        Field('port', type='integer'),
        Field('ssl', type='integer'),
        Field('selected', type='integer'),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        migrate=migrate)

    msfdb.define_table('workspaces',
        Field('id', type='id'),
        Field('name', type='string', length=255),
        Field('created_at', type='datetime'),
        Field('updated_at', type='datetime'),
        Field('boundary', type='string', length=4096),
        Field('description', type='string', length=4096),
        Field('owner_id', type='integer'),
        Field('limit_to_network', type='boolean', default=False),
        migrate=migrate)

########NEW FILE########
__FILENAME__ = menu
_a = request.application

response.logo = A(B('KVASIR'), _class="brand")
response.title = settings.title
response.subtitle = settings.subtitle
response.meta.author = '%s <%s>' % (settings.author, settings.author_email)
response.meta.keywords = settings.keywords
response.meta.description = settings.description

response.menu = [
    (T('Home'), False, URL(_a,'default','index'), []),
    # (A(I(_class='icon-home icon-white'), T('Home'), _href=URL('default', 'index')), False, []),
    (T('All Hosts'), False, URL(_a,'hosts','list'), []),
    # (A(I(_class='icon-th-list icon-white'), T('All Hosts'), _href=URL('hosts', 'list')), False, []),
    (T('Host Data'), False, '',
     [
         (T('Add Host'), False, URL(_a,'hosts','add'), []),
         (T('Services'), False, '',
          [
              (T('List All'), False, URL(_a,'services','list'), []),
              (T('All w/ Vulns'), False, URL(_a,'vulns','service_vulns_list'), []),
              (T('IPs w/ Port'), False, URL(_a,'services','hosts_with_port'), []),
              (T('Add'), False, URL(_a,'services','add'), []),
          ]),
         (T('Accounts'), False, '',
          [
              (T('List'), False, URL(_a,'accounts', 'list'), []),
              (T('Add'), False, URL(_a,'accounts', 'add'), []),
              (T('Import File'), False, URL(_a,'accounts', 'import_file'), []),
              (T('Mass Import'), False, URL(_a,'accounts', 'import_mass_password'), []),
              (T('Process crack file'), False, URL(_a,'accounts', 'update_hashes_by_file'), []),
              (T('Process john.pot'), False, URL(_a,'accounts', 'check_john_pot'), []),
          ]),
         (T('NetBIOS'), False, '',
          [
              (T('Domain Details'), False, URL(_a,'netbios','domain_detail'), []),
              (T('List'), False, URL(_a,'netbios','index'), []),
              (T('Add'), False, URL(_a,'netbios','add'), []),
          ]),
         (T('OS'), False, '',
          [
              (T('List'), False, URL(_a,'os','list'), []),
              (T('Add '), False, URL(_a,'os','add'), []),
              (T('List OS Refs'), False, URL(_a,'os','refs_list'), []),
              (T('Add OS Ref'), False, URL(_a,'os','refs_add'), []),
          ]),
         (T('Other'), False, '',
          [
              (T('List Evidence'), False, URL(_a,'evidence','list'), []),
              (T('List Notes'), False, URL(_a,'notes','list'), []),
              (T('List SNMP'), False, URL(_a,'snmp','list'), []),
              (T('List Tool Output'), False, URL(_a,'tooloutput','list'), []),
              (T('CSV Hostname Update'), False, URL(_a,'hosts','csv_hostupdate'), []),
          ]),
     ]),

    (T('Tasks'), False, URL(_a,'tasks','index'), []),

    (T('Metasploit'), False, '',
     [
         (T('Mass Jobs'), False, '',
         [
            (T('Bruteforce'), False, URL(_a, 'metasploit', 'bruteforce'), []),
            (T('Exploit'), False, URL(_a, 'metasploit', 'exploit'), []),
         ]),
         (T('Imports'), False, '',
         [
             (T('PWDUMP Files'), False, URL(_a, 'metasploit', 'import_pwdump'), []),
             (T('Screenshots'), False, URL(_a, 'metasploit', 'import_screenshots'), []),
             (T('Report XML'), False, URL(_a, 'metasploit', 'import_report_xml'), []),
         ]),
         (T('Send Accounts'), False, URL(_a, 'metasploit', 'send_accounts'), []),
         (T('Send Scan XML Files'), False, URL(_a, 'metasploit', 'send_scanxml'), []),
         (T('API Settings'), False, URL(_a, 'metasploit', 'api_settings'), []),
         #(T('Tasks'), False, URL(_a, 'metasploit', 'task_list'), []),
     ]),
    (T('Other'), False, '',
     [
         (T('Browse Data Directory'), False, URL(_a, 'default', 'data_dir'), []),
         (T('Customer XML'),URL(_a,'report','customer_xml.xml')==URL(),URL(_a,'report','customer_xml.xml'),[]),
         (T('Stats XLS'),URL(_a,'report','spreadsheet')==URL(),URL(_a,'report','spreadsheet'),[]),
         (T('Wiki'),URL(_a,'default','wiki')==URL(),URL(_a,'default','wiki'),[]),
         (T('Update DB Fields'),URL(_a,'default','update_dynamic_fields')==URL(),URL(_a,'default','update_dynamic_fields'),[]),
         (T('IP Calculator'), False, URL(_a, 'default', 'ip_calc'), []),
         (T('Exploit Database (local)'), False, URL(_a, 'exploitdb', 'index'), []),
         (T('PwnWiki'), False, URL(_a, 'default', 'redirect', vars={'url':settings.pwnwiki_path, 'pwnwiki': True}), []),
     ]),
    (T('Statistics'), False, '',
    [
        (T('Vulnlist'), False, URL(_a,'stats','vulnlist'), []),
        (T('Passwords'), False, URL(_a,'stats','passwords'), []),
        (T('OS'), False, URL(_a,'stats','os'), []),
        (T('Services'), False, URL(_a,'stats','services'), []),
        (T('VulnCircles'), False, URL(_a,'stats','vulncircles'), []),
    ]),
    (T('Import'), False ,'',
     [
         (T('Nexpose XML'), False, URL(_a,'nexpose','import_xml_scan'), []),
         (T('Nmap XML'), False, URL(_a,'nmap','import_xml_scan'), []),
         (T('Nmap Scan and Import'), False, URL(_a,'nmap','nmap_scan'), []),
         (T('Nessus Scanfile'), False, URL(_a,'nessus','import_scan'), []),
         (T('Metasploit XML'), False, URL(_a, 'metasploit', 'import_report_xml'), []),
         (T('ShodanHQ'), False, URL(_a, 'shodanhq', 'import_report'), []),
     ]),
    (T('Administration'), False, '',
     [
         (T('Nexpose'), False, '',
          [
              (T('Install/Update VulnData'),URL(_a,'nexpose','vuln_update')==URL(),URL(_a,'nexpose','vuln_update'),[]),
              #(T('Import Scan Template '),URL(_a,'nexpose','scan_template')==URL(),URL(_a,'nexpose','scan_template'),[]),
              (T('Import VulnID'), False, URL(_a, 'nexpose', 'import_vulnid'), []),
              (T('Import Exploit XML'),URL(_a,'exploits','import_nexpose_xml')==URL(),URL(_a,'exploits','import_nexpose_xml'),[]),
              (T('Purge Nexpose Data'),URL(_a,'nexpose','purge')==URL(),URL(_a,'nexpose','purge'),[]),
          ]),
         (T('VulnDB'), False, '',
          [
              (T('List Vulnerabilities'), False, URL(_a,'vulns','vulndata_list'),[]),
              (T('Add Vulnerability'), False, URL(_a,'vulns','vulndata_add'),[]),
              (T('List References'), False, URL(_a,'vulns','vuln_references_list'),[]),
              (T('List Exploits'), False, URL(_a,'exploits','list'),[]),
              (T('Connect Vulns/Exploits'), False, URL(_a,'exploits','connect_exploits'), []),
              (T('Import Nexpose Exploits'), False, URL(_a,'exploits','import_nexpose_xml'),[]),
              (T('Import CANVAS Exploits'), False, URL(_a,'exploits','import_canvas_xml'),[]),
          ]),
         (T('CPE Database'), False, '',
          [
              (T('Import CPE Data'), False, URL(_a,'cpe','import_cpe_xml'), []),
              (T('List OS DB'), False, URL(_a,'cpe','os_list'), []),
              (T('Add OS'), False, URL(_a,'cpe','os_add'), []),
              #(T('List Application DB'), False, URL(_a,'cpe','apps_list'), []),
              #(T('Add Application'), False, URL(_a,'cpe','apps_add'), []),
              #(T('List Hardware DB'), False, URL(_a,'cpe','hardware_list'), []),
              #(T('Add Hardware'), False, URL(_a,'cpe','hardware_add'), []),
              (T('Purge CPE DB'), False, URL(_a,'cpe','purge'), []),
          ]),
         (T('Last Resort'), False, '',
          [
              (T('CSV Backup'), False, URL(_a,'default','database_backup'),[]),
              (T('CSV Restore'), False, URL(_a,'default','database_restore'),[]),
              (T('Purge Data'), URL(_a,'default','purge_data')==URL(),URL(_a,'default','purge_data'),[]),
          ]),
     ]),
]

########NEW FILE########
__FILENAME__ = scheduler
# -*- coding: utf-8 -*-

##--------------------------------------#
## Kvasir Scheduler functions
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Scheduler functions for long running processes
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#

import os
import socket
from skaldship.hosts import get_host_record
from gluon.scheduler import Scheduler

import logging

logger = logging.getLogger("web2py.app.kvasir")

##----------------------------------------------------------------------------

def launch_terminal(record=None, launch_cmd=None):
    """
    Opens a terminal on the Web Server. This only works if the
    web2py server is running on the user's workstation.

    The command to execute is stored in the user's settings db
    under auth_user.f_launch_cmd. Variables translated:

       _IP_      -- The current IP Address (v4 by default, v6 if exists)
       _LOGFILE_ -- Session logfile name (we prepend the path)

    If an IPv6 address is used then ':' is changed to '_'

    Example:

    xterm -sb -sl 1500 -vb -T 'manual hacking: _IP_' -n 'manual hacking: _IP_' -e script _LOGFILE_
    """

    record = get_host_record(record)

    # only execute launch on requests from localhost!
    if request.env['remote_addr'] != '127.0.0.1':
        logger.error("Can only launch from localhost! remote_addr = %s" % (request.env['remote_addr']))
        return "Can only launch from localhost"

    if record is None:
        return "No record found"

    import string, os, subprocess
    import time

    # if no launch command use the default
    if not launch_cmd:
        launch_cmd = "xterm -sb -sl 1500 -vb -T 'manual hacking: _IP_' -n 'manual hacking: _IP_' -e 'script _LOGFILE_'"

    # check ip address
    if record.f_ipv6 is None or len(record.f_ipv6) == 0:
        ip = record.f_ipv4
        logip = record.f_ipv4
    else:
        ip = record.f_ipv6
        logip = record.f_ipv6.replace(":", "_")

    logdir = "session-logs"
    logfilename = "%s-%s.log" % (logip, time.strftime("%Y%m%d%H%M%S", time.localtime(time.time())))
    logfile = os.path.join(logdir, logfilename)
    launch_cmd = launch_cmd.replace("_IP_", ip)
    launch_cmd = launch_cmd.replace("_LOGFILE_", logfile)

    from skaldship.general import check_datadir
    # Check to see if data directories exist, create otherwise
    check_datadir(request.folder)
    datadir = os.path.join(os.getcwd(), request.folder, "data")

    # chdir to datadir!
    launch_cmd = launch_cmd.replace("_DATADIR_", datadir)
    os.chdir(datadir)

    # set environment variables
    os.environ['IP'] = ip
    os.environ['HOSTNAME'] = record.f_hostname or ""
    os.environ['DATADIR'] = datadir

    try:
        logger.info("Spawning: %s\n" % (launch_cmd))
        print("Spawning: %s" % (launch_cmd))
        subprocess.Popen(launch_cmd, shell=True)#, stdout=None, stdin=None, stderr=None)
    except Exception, e:
        logger.error("Error spawning launch cmd (%s): %s\n" % (launch_cmd, e))
        print("Error spawning launch cmd (%s): %s\n" % (launch_cmd, e))

    return False

##----------------------------------------------------------------------------

def run_scanner(
        scanner=None,
        asset_group=None,
        engineer=None,
        target_list=None,
        blacklist=None,
        scan_options=None,
        addnoports=False,
        update_hosts=False,
        **kwargs
):
    '''
    Schedule handler to process nmap scan
    '''
    from skaldship.log import log

    if not isinstance(scanner, str):
        return False
    scanner = scanner.upper()
    logger.info(" [*] Processing Nmap scan ")
    if scanner == 'NMAP':
        from skaldship.nmap import run_scan

        nmap_xml_file = run_scan(
            blacklist=blacklist,
            target_list=target_list,
            scan_options=scan_options,
        )

        if nmap_xml_file:
            from skaldship.nmap import process_xml
            log("Processing nmap xml file: %s" % (nmap_xml_file))
            process_xml(
                filename=nmap_xml_file,
                addnoports=addnoports,
                asset_group=asset_group,
                engineer=engineer,
                msf_settings={},
                ip_ignore_list=None,
                ip_include_list=None,
                update_hosts=update_hosts,
            )

##----------------------------------------------------------------------------

def canvas_exploit_xml(filename=None):
    """
    Process ImmunitySec CANVAS Exploits.xml file into the database
    """
    from skaldship.canvas import process_exploits
    from skaldship.exploits import connect_exploits

    process_exploits(filename)
    connect_exploits()
    return True

##----------------------------------------------------------------------------

def nexpose_exploit_xml(filename=None):
    """
    Process Nexpose exploits.xml file into the database
    """
    from skaldship.nexpose import process_exploits
    from skaldship.exploits import connect_exploits

    process_exploits(filename)
    connect_exploits()
    return True

##----------------------------------------------------------------------------

def scanner_import(
        scanner=None,
        filename=None,
        addnoports=False,
        asset_group=None,
        engineer=None,
        msf_settings={},
        ip_ignore_list=None,
        ip_include_list=None,
        update_hosts=False,
        **kwargs
):
    """
    Imports a Scanner XML file to Kvasir
    """
    if not isinstance(scanner, str):
        return False

    scanner = scanner.upper()
    if scanner == 'NMAP':
        from skaldship.nmap import process_xml

        logger.info("Processing nmap file: %s" % (filename))
        process_xml(
            filename=filename,
            addnoports=addnoports,
            asset_group=asset_group,
            engineer=engineer,
            msf_settings=msf_settings,
            ip_ignore_list=ip_ignore_list,
            ip_include_list=ip_include_list,
            update_hosts=update_hosts,
        )
    elif scanner == 'NEXPOSE':
        from skaldship.nexpose import process_xml

        logger.info("Processing Nexpose file: %s" % (filename))
        process_xml(
            filename=filename,
            asset_group=asset_group,
            engineer=engineer,
            msf_settings=msf_settings,
            ip_ignore_list=ip_ignore_list,
            ip_include_list=ip_include_list,
            update_hosts=update_hosts,
        )
    elif scanner == 'NESSUS':
        from skaldship.nessus import process_scanfile

        logger.info("Processing Nessus file: %s" % (filename))
        process_scanfile(
            filename=filename,
            asset_group=asset_group,
            engineer=engineer,
            msf_settings=msf_settings,
            ip_ignore_list=ip_ignore_list,
            ip_include_list=ip_include_list,
            update_hosts=update_hosts,
        )
    elif scanner == 'METASPLOIT':
        from skaldship.metasploit import process_report_xml

        logger.info("Processing Metasploit Pro file: %s" % filename)
        process_report_xml(
            filename=filename,
            asset_group=asset_group,
            engineer=engineer,
            ip_ignore_list=ip_ignore_list,
            ip_include_list=ip_include_list,
            update_hosts=update_hosts,
        )
    elif scanner == 'SHODANHQ':
        from skaldship.shodanhq import process_report

        logger.info("Processing ShodanHQ file: %s" % (filename))
        process_report(
            filename=filename,
            host_list=kwargs.get('hosts') or [],
            query=kwargs.get('query') or None,
            asset_group=asset_group,
            engineer=engineer,
            ip_ignore_list=ip_ignore_list,
            ip_include_list=ip_include_list,
            update_hosts=update_hosts,
        )
    return True

##----------------------------------------------------------------------------

def do_host_status(records=[], query=None, asset_group=None, hosts=[]):
    """
    Runs through the t_hosts table and updates the *_count entries.
    Can also run through a specific list of record IDs instead.
    """
    from skaldship.hosts import do_host_status

    do_host_status(records=records, query=query, asset_group=asset_group, hosts=hosts)
    return True

##------------------------------------------------------------------------

def accounts_import_file(filename=None, service=['info', '0'], f_type=None, f_source=None):
    """
    Processes an Imported password file to the accounts table
    """

    print("Processing password file: %s" % (filename))
    from skaldship.passwords import process_password_file, insert_or_update_acct

    account_data = process_password_file(pw_file=filename, file_type=f_type, source=f_source)
    resp_text = insert_or_update_acct(service, account_data)
    print(resp_text)
    return True

##------------------------------------------------------------------------

def cpe_import_xml(filename=None, download=False, wipe=False):
    """
    Process the CPE data through an uploaded file or have it download directly
    from the MITRE webserver
    """
    from skaldship.cpe import process_xml

    process_xml(filename, download, wipe)
    return True

##------------------------------------------------------------------------

def run_valkyrie(valkyrie_type=None, services=None):
    """
    Run a valkyrie
    """
    if valkyrie_type == 'webshot':
        from skaldship.valkyries.webimaging import do_screenshot
        do_screenshot(services)
    elif valkyrie_type == 'vncshot':
        from skaldship.valkyries.vncscreenshot import do_screenshot
        do_screenshot(services)

    return True

##-------------------------------------------------------

def import_all_nexpose_vulndata(overwrite=False, nexpose_server={}):
    """
    Import all vulnerability data from Nexpose
    """
    from skaldship.nexpose import import_all_vulndata

    import_all_vulndata(overwrite=overwrite, nexpose_server=nexpose_server)
    return True

##-------------------------------------------------------

scheduler = Scheduler(
    db=db,
    migrate=settings.migrate,
    group_names=[settings.scheduler_group_name],
)

########NEW FILE########
__FILENAME__ = MetasploitAPI
#!/usr/bin/env python
#coding:utf-8
# Author:  Kurt Grutzmacher -- <kgrutzma@cisco.com>
# Purpose: Metasploit integraton library
# Created: 03/25/10
# Modified: 02/20/12

import os, sys, logging, string, httplib
from pprint import pprint
try:
    import msgpack
except ImportError, e:
    raise Exception("Install 'msgpack' library: 'pip install msgpack-python'")
logger = logging.getLogger("web2py.app.kvasir")

"""
Metasploit integration library utilizing msgpack
"""

########################################################################
class MSFAPIError(Exception):
    pass

########################################################################
class MetasploitAPI:
    """
    Connects to the Metasploit Framework library via msgpack
    """

    def __init__(self, host="localhost:3790", ssl=True, apikey=None, username=None, password=None):
        self.log = logging.getLogger(self.__class__.__name__)
        self.username = username
        self.password = password
        self.apikey = apikey
        self.ssl = ssl
        if host.startswith('http'):
            from urlparse import urlsplit
            (self.host, self.port) = urlsplit(host)[1].split(':')
        else:
            (self.host, self.port) = host.split(':')
        self.port = int(self.port)
        self.debug = False
        self.libraryversion = "1.0"
        self.apiurl = "/api/%s" % (self.libraryversion)
        self.connected = False

    #-----------------------------------------------------------------#
    def build_command(self, command=[], *opts):
        """Builds an API command w/ msgpack, appends the API key and
        any options"""
        if type(command) is not type(list()):
            command = [command]
        if self.apikey:
            command.append(self.apikey)
        for k in opts:
            command.append(k)
        return msgpack.packb(command)

    #-----------------------------------------------------------------#
    def send(self, message=None):
        """Sends a message to Metasploit API and unpacks the response"""
        headers = {"Content-type" : "binary/message-pack"}

        if self.ssl:
            httpclient = httplib.HTTPSConnection(self.host, self.port)
        else:
            httpclient = httplib.HTTPConnection(self.host, self.port)
        try:
            httpclient.request("POST", self.apiurl, message, headers)
        except Exception, e:
            raise MSFAPIError("HTTP Client error:", e)

        response = httpclient.getresponse()
        if response.status == 200:
            try:
                res = msgpack.unpackb(response.read())
            except Exception, e:
                raise MSFAPIError("Unable to process response:", e)
            if res.get('error') is True:
                raise MSFAPIError("API Error:", res['error_string'])
        else:
            raise MSFAPIError("HTTP Error from MSF:", response.status)
        return res

    #-----------------------------------------------------------------#
    def login(self, host="localhost:3790", ssl=True, apikey=None, username=None, password=None):
        """Perform session setup, sending username/password if defined or
        validating API key. Sets self.connected to True when successfull"""

        self.connected = False

        if self.username:
            self.log.debug("Authenticating with username/password")
            self.apikey = None
            message = self.build_command(['auth.login', self.user, self.password])
            res = self.send(message)
            if res['result'] == 'success':
                self.apikey = res['token']
                self.connected = True
            else:
                self.log.error("Authentication failed!")
                self.connected = False

        elif self.apikey:
            # checking to be sure API key is valid
            self.log.debug("Obtaining Metasploit version and statistics")
            resp = self.version()
            if resp.has_key('version'):
                self.log.info("Server v%s, API %s, Ruby v%s" % (resp['version'], resp['api'], resp['ruby']))
                self.connected = True
            else:
                self.connected = False

        else:
            self.connected = False

        return self.connected

    #-----------------------------------------------------------------#
    def close(self):
        """Closes the connection"""
        self.connected = False

    ###################################################################
    # Metasploit core commands
    #-----------------------------------------------------------------#
    def version(self):
        """Obtain the Metasploit Framework version"""
        self.log.debug("Sending version request")
        message = self.build_command(['core.version'])
        return self.send(message)

    #-----------------------------------------------------------------#
    def add_module_path(self, mod_path=""):
        """Add a local file path to module search list"""
        self.log.debug("Adding %s to module path" % mod_path)
        message = self.build_command(['core.add_module_path'], mod_path)
        return self.send(message)

    #-----------------------------------------------------------------#
    def save(self):
        """Saves the global datastore configuration to disk"""
        self.log.debug("Saving the global datastore to disk")
        message = self.build_command(['core.save'])
        return self.send(message)

    ###################################################################
    # Metasploit Module commands
    #-----------------------------------------------------------------#
    def module_stats(self):
        """Obtain the module counts"""
        self.log.debug("Sending stats request")
        message = self.build_command(['core.module_stats'])
        return self.send(message)

    #-----------------------------------------------------------------#
    def reload_modules(self):
        """Reloads modules"""
        self.log.debug("Reloading modules")
        message = self.build_command(['core.reload_modules'])
        return self.send(message)

    #-----------------------------------------------------------------#
    def module_list(self, modtype="exploits"):
        """Obtain a list of Metasploit Framework modules

        Modtype can be: exploits, auxiliary, payloads, encoders, nops
        """
        self.log.debug("Sending module.%s request" % (modtype))
        message = self.build_command(["module.%s" % (modtype)])
        return self.send(message)

    #-----------------------------------------------------------------#
    def module_info(self, modtype="", module=""):
        """Obtain info on a specific module"""

        if modtype.lower() not in ["exploit", "auxiliary", "post", "payload", "encoder", "nop"]:
            self.log.debug("Invalid module type")
            return

        message = self.build_command(["module.info"], modtype, module)
        return self.send(message)

    #-----------------------------------------------------------------#
    def module_options(self, modtype="", module=""):
        """Obtain options on a specific module"""

        if modtype.lower() not in ["exploit", "auxiliary", "post", "payload", "encoder", "nop"]:
            self.log.debug("Invalid module type")
            return

        message = self.build_command(["module.options"], modtype, module)
        return self.send(message)

    #-----------------------------------------------------------------#
    def compatible_payloads(self, module=""):
        """Obtain compatible payloads for a specific module"""

        message = self.build_command(["module.compatible_payloads"], module)
        return self.send(message)

    #-----------------------------------------------------------------#
    def target_compatible_payloads(self, module="", target=1):
        """Obtain compatible payloads for a specific module and target"""

        message = self.build_command(["module.target.compatible_payloads"], module, target)
        return self.send(message)

    #-----------------------------------------------------------------#
    def compatible_sessions(self, module=""):
        """Obtain compatible sessions for a specific module"""

        message = self.build_command(["module.compatible_sessions"], module)
        return self.send(message)

    #-----------------------------------------------------------------#
    def module_execute(self, modtype="", module="", options={}):
        """Execute a specific module"""

        if module == "":
            self.log.debug("[module_execute] No module specified")
            return
        if modtype.lower() not in ["exploit", "auxiliary", "post"]:
            self.log.debug("[module_execute] Invalid module type specified")
            return

        message = self.build_command(["module.execute"], modtype, module, options)
        return self.send(message)

    ###################################################################
    # Metasploit Jobs, Sessions and Meterpreter
    #-----------------------------------------------------------------#
    def job_list(self):
        """Returns a list of running jobs and job names"""

        message = self.build_command(["job.list"])
        return self.send(message)

    #-----------------------------------------------------------------#
    def job_stop(self, jobid=""):
        """Kill a specific job by ID"""

        message = self.build_command(["job.kill"], jobid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def session_list(self):
        """Returns a list of running jobs and job names"""

        message = self.build_command(["session.list"])
        return self.send(message)

    #-----------------------------------------------------------------#
    def session_stop(self, jobid=""):
        """Kill a specific job by ID"""

        message = self.build_command(["session.kill"], jobid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def shell_read(self, sessionid="", readptr=0):
        """Read any pending output from a 'shell' session"""

        message = self.build_command(["session.shell_read"], sessionid, readptr)
        return self.send(message)

    #-----------------------------------------------------------------#
    def shell_write(self, sessionid="", data=None):
        """Write data to a 'shell' session"""

        message = self.build_command(["session.shell_write"], data)
        return self.send(message)

    #-----------------------------------------------------------------#
    def meterpreter_read(self, sessionid=""):
        """Read any pending output from a 'Meterpreter' session"""

        message = self.build_command(["session.meterpreter_read"], sessionid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def meterpreter_write(self, sessionid="", data=None):
        """Write data to a 'Meterpreter' session"""

        message = self.build_command(["session.meterpreter_write"], data)
        return self.send(message)

    #-----------------------------------------------------------------#
    def meterpreter_run_single(self, sessionid="", data=None):
        """Run a command in a 'Meterpreter' session, no matter who is interacting with it"""

        message = self.build_command(["session.meterpreter_run_single"], sessionid, data)
        return self.send(message)

    #-----------------------------------------------------------------#
    def meterpreter_script(self, sessionid="", data=None):
        """Run a script on a 'Meterpreter' session"""

        message = self.build_command(["session.meterpreter_script"], data)
        return self.send(message)

    #-----------------------------------------------------------------#
    def meterpreter_session_detach(self, sessionid=""):
        """Detatch a 'Meterpreter' session"""

        message = self.build_command(["session.meterpreter_session_detach"], sessionid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def meterpreter_session_kill(self, sessionid=""):
        """Terminates a 'Meterpreter' session"""

        message = self.build_command(["session.meterpreter_session_kill"], sessionid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def meterpreter_tabs(self, sessionid="", data=None):
        """Emulates pressing tab from a 'Meterpreter' session"""

        message = self.build_command(["session.meterpreter_tabs"], data)
        return self.send(message)

    #-----------------------------------------------------------------#
    def compatible_modules(self, sessionid=""):
        """Lists compatbile post modules for a session"""

        message = self.build_command(["session.compatible_modules"], sessionid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def shell_upgrade(self, sessionid="", host=None, port=4444):
        """Attempts to upgrade a shell to Meterpreter, expects multi/handler
        to be running on {host} and {port}"""

        message = self.build_command(["session.shell_upgrade"], sessionid, host, port)
        return self.send(message)

    #-----------------------------------------------------------------#
    def ring_clear(self, sessionid=""):
        """Wipes ring buffer from sessionid"""

        message = self.build_command(["session.ring_clear"], sessionid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def ring_last(self, sessionid=""):
        """Returns last issued read pointer for a session"""

        message = self.build_command(["session.ring_last"], sessionid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def ring_put(self, sessionid="", data=""):
        """Same as shell_write"""

        message = self.build_command(["session.ring_put"], sessionid, data)
        return self.send(message)

    #-----------------------------------------------------------------#
    def ring_read(self, sessionid="", readptr=0):
        """Same as shell_read"""

        message = self.build_command(["session.ring_read"], sessionid, readptr)
        return self.send(message)

    ###################################################################
    # Metasploit Pro Basic features
    #-----------------------------------------------------------------#
    def pro_about(self):
        """Whatdisabout?"""

        message = self.build_command(["pro.about"])
        return self.send(message)

    #-----------------------------------------------------------------#
    def pro_workspaces(self):
        """List active workspaces"""

        message = self.build_command(["pro.workspaces"])
        return self.send(message)

    #-----------------------------------------------------------------#
    def pro_workspace_add(self, options={}):
        """Adds a workspace"""

        message = self.build_command(["pro.workspaces_add"], options)
        return self.send(message)

    #-----------------------------------------------------------------#
    def pro_workspace_del(self, workspace=None):
        """Deletes a workspace"""

        message = self.build_command(["pro.workspaces_del"], workspace)
        return self.send(message)

    #-----------------------------------------------------------------#
    def pro_users(self):
        """List users"""

        message = self.build_command(["pro.users"])
        return self.send(message)

    #-----------------------------------------------------------------#
    # Metasploit Pro Import features
    #-----------------------------------------------------------------#
    def pro_import_data(self, workspace=None, data=None, options={}):
        """Import raw data into MSF"""

        # data should be a string, join it!
        if isinstance(data, list):
            data = ''.join(data)

        message = self.build_command(["pro.import_data"], workspace, data, options)
        return self.send(message)

    #-----------------------------------------------------------------#
    def pro_import_file(self, workspace=None, filename=None, options={}):
        """Import local file into MSF (local to MSF)"""

        message = self.build_command(["pro.import_file"], workspace, filename, options)
        return self.send(message)

    #-----------------------------------------------------------------#
    def pro_start_import(self, options={}):
        """Starts the Import action within Metasploit Pro"""
        message = self.build_command(["pro.start_import"], options)
        return self.send(message)

    #-----------------------------------------------------------------#
    def pro_validate_import_file(self, filename=None):
        """Validates a local file may be imported to MSF"""

        message = self.build_command(["pro.validate_import_file("], filename)
        return self.send(message)

    #-----------------------------------------------------------------#
    def pro_start_import_creds(self, options={}):
        """Import credentials such as users, passwords, hashes, and keys"""

        message = self.build_command(["pro.start_import_creds"], options)
        return self.send(message)

    ###################################################################
    # Metasploit Pro Report features
    #-----------------------------------------------------------------#
    def start_report(self, options={}):
        """Generates report/exportfunctons"""

        message = self.build_command(["pro.start_report"], options)
        return self.send(message)

    #-----------------------------------------------------------------#
    def report_list(self, workspace=None):
        """Lists reports"""

        message = self.build_command(["pro.report_list"], workspace)
        return self.send(message)

    #-----------------------------------------------------------------#
    def report_download(self, rptid=0):
        """Downloads a specific report"""

        message = self.build_command(["pro.report_download"], rptid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def report_download_by_task(self, taskid=0):
        """Downloads a report based upon a task ID"""

        message = self.build_command(["pro.report_download_by_task"], taskid)
        return self.send(message)

    ###################################################################
    # Metasploit Pro Task features
    #-----------------------------------------------------------------#
    def task_list(self):
        """Lists active tasks"""

        message = self.build_command(["pro.task_list"])
        return self.send(message)

    #-----------------------------------------------------------------#
    def task_status(self, task=None):
        """Current status of a task"""

        message = self.build_command(["pro.task_status"], task)
        return self.send(message)

    #-----------------------------------------------------------------#
    def task_stop(self, task=None):
        """Stops a task"""

        message = self.build_command(["pro.task_stop"], task)
        return self.send(message)

    #-----------------------------------------------------------------#
    def task_log(self, task=None):
        """Returns status and log data for a task"""

        message = self.build_command(["pro.task_log"], task)
        return self.send(message)

    #-----------------------------------------------------------------#
    def task_delete_log(self, task=None):
        """Deletes a log data for a task"""

        message = self.build_command(["pro.task_delete_log"], task)
        return self.send(message)

    ###################################################################
    # Metasploit Pro Exploit features
    #-----------------------------------------------------------------#
    def start_import_creds(self, options={}):
        """Imports credentials from a file"""

        message = self.build_command(["pro.start_import_creds"], options)
        return self.send(message)

    #-----------------------------------------------------------------#
    def start_bruteforce(self, options={}):
        """Starts a bruteforce task"""

        message = self.build_command(["pro.start_bruteforce"], options)
        return self.send(message)

    #-----------------------------------------------------------------#
    def start_exploit(self, options={}):
        """Starts an exploit task"""

        message = self.build_command(["pro.start_exploit"], options)
        return self.send(message)

    ###################################################################
    # Metasploit Pro Loot features
    #-----------------------------------------------------------------#
    def loot_list(self, workspace=None):
        """Lists the loot for a workspace"""

        message = self.build_command(["pro.loot_list"], workspace)
        return self.send(message)

    #-----------------------------------------------------------------#
    def loot_download(self, lootid=0):
        """Downloads a loot from an identifier"""

        message = self.build_command(["pro.loot_download"], lootid)
        return self.send(message)

    #-----------------------------------------------------------------#
    def debugmsg(self, message):
        """Simple debug message output"""
        from pprint import pprint
        if self.debug:
            pprint(message)

#----------------------------------------------------------------------
def listallmodules(msf):
    """Function to list all the modules"""

    from pprint import pprint

    payloads = msf.payloads()
    print "All Paylods:"
    for m in payloads:
        pprint("%s: %s" % (m, payloads[m]))

    exploits = msf.module_list("exploits")
    for f in exploits:
        print "Exploit name: %s" % (f)
        modinfo = msf.module_info("exploits", f)
        print "Module Info:"
        for m in modinfo:
            print("%s: %s" % (m, modinfo[m]))
        modopts = msf.module_options("exploits", f)
        print "Module Options:"
        for m in modopts:
            print("%s: %s" % (m, modopts[m]))

    auxiliary = msf.module_list("auxiliary")
    for f in auxiliary:
        print "Auxiliary name: %s" % (f)
        modinfo = msf.module_info("auxiliary", f)
        for m in modinfo:
            print("%s: %s" % (m, modinfo[m]))
        modopts = msf.module_options("exploits", f)
        print "Module Options:"
        for m in modopts:
            print("%s: %s" % (m, modinfo[m]))

#----------------------------------------------------------------------

def import_xml_report(filename=None):
    """
    Process a MSF Pro XML report
    """
    return

#----------------------------------------------------------------------
def login(options):
    """"
    Login!
    """

    msf = MetasploitAPI( options.username, options.password, options.server, options.ssl )

    print "-" * 75
    print "Metasploit Framework XMLRPC v%s" % (msf.libraryversion)

    if options.ssl:
        msf.ssl = True
    else:
        msf.ssl = False

    if not msf.connect():
        print "[main] Unable to continue, not connected!"
        sys.exit(1)

    print "Connected to Metasploit version %s" % (msf.version())
    print "-" * 75

    return msf

#----------------------------------------------------------------------
if __name__=='__main__':
    from optparse import OptionParser

    # set up commandline arguments
    Progname=os.path.basename(sys.argv[0])
    Usage="%prog usage: XXX:[command_line_args]\n" \
         "%prog usage: -h\n" \
         "%prog usage: -V"
    optparser = OptionParser(usage=Usage, version="%prog: $Id:$" )
    optparser.add_option("-d", "--debug", dest = "debug", action="store_true", help="Debugging messages")
    optparser.add_option("-v", "--verbose", dest = "verbose", action="store_true", help="Verbose messages")
    optparser.add_option("-u", "--username", dest = "username", action="store", default=None, help="Username")
    optparser.add_option("-p", "--password", dest = "password", action="store", default=None, help="Password")
    optparser.add_option("-k", "--apikey", dest = "apikey", action="store", default=None, help="API Key")
    optparser.add_option("-s", "--server", dest = "server", action="store", default="127.0.0.1:3790", help="Server address:port")
    optparser.add_option("-S", "--SSL", dest = "ssl", action="store_true", default=True, help="Use SSL encryption")
    optparser.add_option("-i", "--interactive", dest = "interactive", action="store_true", default=False, help="Go Interactive")
    optparser.add_option("-b", "--bpython", dest = "bpython", action="store_true", default=False, help="Use Bpython")

    (options, params) = optparser.parse_args()

    root_log = logging.getLogger()
    if options.debug:
        root_log.setLevel(logging.DEBUG)
    elif options.verbose:
        root_log.setLevel(logging.INFO)
    else:
        root_log.setLevel(logging.WARN)
    handler = logging.StreamHandler()
    logformat = "%(name)s: %(levelname)s: %(message)s"
    handler.setFormatter(logging.Formatter(logformat))
    root_log.addHandler(handler)
    log = logging.getLogger(Progname)

    msf = MetasploitAPI(host=options.server, ssl=options.ssl, apikey=options.apikey, username=options.username, password=options.password)
    msf.login()

    if options.interactive or options.bpython:
        log.info("Attempting to go interactive...")
        if options.bpython:
            try:
                import bpython
                bpython.embed(locals_={'msf':msf}, banner="\nWelcome to MetasploitAPI, use the variable 'msf'\n")
            except:
                log.warning('import bpython error; trying ipython...')
        else:
            try:
                import IPython
                if IPython.__version__ >= '0.11':
                    from IPython.frontend.terminal.embed import InteractiveShellEmbed
                    shell = InteractiveShellEmbed(user_ns={'msf':msf})
                    shell()
                else:
                    shell = IPython.Shell.IPShell(argv=[],user_ns={'msf':msf})
                    shell.mainloop()
            except:
                log.warning("Unable to go interactive")
        sys.exit(0)

    if msf:
        listallmodules(msf)

########NEW FILE########
__FILENAME__ = common
_FILEUPLOAD=False

from urllib2 import urlopen, Request, build_opener
from urllib import urlencode
from xml.dom.minidom import parse as parse_xml
from .utils import get_text_by_tag, PolicyParameters, NessusPolicy, \
        NessusReport, NessusScan
import os.path

try:
    from poster.encode import multipart_encode, MultipartParam
    from poster.streaminghttp import register_openers
    _FILEUPLOAD=True
except ImportError:
    pass


class NessusConnection(object):
    def __init__(self, username, password, url='https://localhost:8834'):
        self._username = username
        self._password = password
        self._url = url
        self._authenticated = False
        self._token = None

    def _get_reply(self, url, params={}):
        params['token'] = self._token

        f = urlopen(url, urlencode(params))
        dom = parse_xml(f)
        reply = dom.getElementsByTagName('reply')[0]
        status = get_text_by_tag(reply, 'status')
        if status != 'OK':
            raise Exception("Authentication failure")

        return reply

    def _authenticate(self):
        url = os.path.join(self._url, "login")
        params = dict(login=self._username, password=self._password)
        reply = self._get_reply(url, params)
        self._token = get_text_by_tag(reply, 'token')
        self._authenticated = True

    def upload_file(self, name, fd):
        assert _FILEUPLOAD, "poster needs to be installed, hg+https://bitbucket.org/chrisatlee/poster"
        
        if not self._authenticated:
            self._authenticate()

        url = os.path.join(self._url, "file/upload")


        params = (MultipartParam(name='Filedata',
                                 fileobj=fd,
                                 filename=name,
                                 filetype='application/octet-stream'),)

        datagen, headers = multipart_encode(params)
        request = Request(url, datagen, headers)

        opener = register_openers()
        opener.addheaders.append(('Cookie', 'token=%s' % self._token))
        
        reply = opener.open(request)
        
        dom = parse_xml(reply)

        reply_dom = dom.getElementsByTagName('reply')[0]

        status = get_text_by_tag(reply_dom, 'status')
        if status != 'OK':
            raise Exception("Upload Failed")

        return reply

    def import_policy(self, name, f):
         """
         Given a file, first upload it to the nessus scanner with a
         given filename and then instruct the nessus scanner to import
         the policy
         """
         self.upload_file(name, f)

         url = os.path.join(self._url, "file/policy/import")

         # generate a sequence number we can rely on
         seq  = abs(hash(name)) % 10000000
         params = dict(seq=seq,file=name)

         reply = self._get_reply(url, params)

         status = get_text_by_tag(reply, 'status')
         if status != 'OK':
             raise Exception("Policy Import Failed")
         
         return reply

    def list_policies(self):
        """Lists all policies"""
        if not self._authenticated:
            self._authenticate()

        policies = []

        url = os.path.join(self._url, "policy/list")
        reply = self._get_reply(url)
        node_policies = reply.getElementsByTagName("policy")
        for node_policy in node_policies:
            policies.append(NessusPolicy.from_node(node_policy))

        return policies

    def create_policy(self, policy_name, policy_parameters=PolicyParameters()):
        """Creates a nessus policy with a given name"""
        if not self._authenticated:
            self._authenticate()

        url = os.path.join(self._url, "policy/add")
        params = dict(policy_id=0, policy_name=policy_name,
                      **policy_parameters)
        reply = self._get_reply(url, params)

        node_policy = reply.getElementsByTagName("policy")[0]
        return NessusPolicy.from_node(node_policy)

    def delete_policy(self, policy_id, policy_name):
        """Deletes a particular policy"""
        if not self._authenticated:
            self._authenticate()

        url = os.path.join(self._url, "policy/delete")
        params = dict(policy_id=policy_id, policy_name=policy_name)
        self._get_reply(url, params)

    def create_scan(self, policy_id, scan_name, targets):
        if not self._authenticated:
            self._authenticate()

        url = os.path.join(self._url, "scan/new")
        params = dict(policy_id=policy_id, scan_name=scan_name,
                      target=",".join(targets))
        reply = self._get_reply(url, params)

        node_scan = reply.getElementsByTagName("scan")[0]
        return NessusScan.from_node(node_scan)

    def list_reports(self):
        if not self._authenticated:
            self._authenticate()

        reports = []

        url = os.path.join(self._url, "report/list")
        reply = self._get_reply(url)

        node_reports = reply.getElementsByTagName("report")
        for node_report in node_reports:
            reports.append(NessusReport.from_node(node_report))

        return reports

    def download_report(self, report_name, fp):
        if not self._authenticated:
            self._authenticate()

        url = os.path.join(self._url, "file/report/download")
        params = dict(token=self._token, report=report_name)
        f = urlopen(url, urlencode(params))

        # Now write it to an output file.
        while True:
            data = f.read(4096)
            if not data:
                break

            fp.write(data)

########NEW FILE########
__FILENAME__ = utils
def get_text_by_tag(start, tagname, default=None):
    """Returns a text node from a tag"""
    node_back = start.getElementsByTagName(tagname)[0]
    for node in node_back.childNodes:
        if node.nodeType == node.TEXT_NODE:
            return node.data

    return default


class PolicyParameters(dict):
    def __init__(self):
        super(PolicyParameters, self).__init__()
        self.max_hosts(80)

    def max_hosts(self, value):
        self["max_hosts"] = value

    def smb_credentials(self, username, password):
        self["Login configurations[entry]:SMB account :"] = username
        self["Login configurations[password]:SMB password :"] = password

    def ssh_credentials(self, username, password):
        self["SSH settings[entry]:SSH user name :"] = username
        self["SSH settings[password]:SSH password (unsafe!) :"] = password


class NessusPolicy(object):
    def __init__(self, id, name):
        self._id = id
        self._name = name

    @property
    def id(self):
        return self._id

    @property
    def name(self):
        return self._name

    @staticmethod
    def from_node(node):
        policy_id = int(get_text_by_tag(node, 'policyID'))
        policy_name = get_text_by_tag(node, 'policyName')
        return NessusPolicy(policy_id, policy_name)


class NessusScan(object):
    def __init__(self, uuid, owner, start_time, name):
        self._uuid = uuid
        self._owner = owner
        self._start_time = start_time
        self._name = name

    @property
    def uuid(self):
        return self._uuid

    @property
    def owner(self):
        return self._owner

    @property
    def start_time(self):
        return self._start_time

    @property
    def name(self):
        return self._name

    @staticmethod
    def from_node(node):
        uuid = get_text_by_tag(node, 'uuid')
        owner = get_text_by_tag(node, 'owner')
        start_time = get_text_by_tag(node, 'start_time')
        scan_name = get_text_by_tag(node, 'scan_name')

        return NessusScan(uuid, owner, start_time, scan_name)


class NessusReport(object):
    STATUS_COMPLETE = 'completed'
    STATUS_RUNNING = 'running'

    def __init__(self, timestamp, status, name, readablename):
        self._timestamp = timestamp
        self._status = status
        self._name = name
        self._readablename = readablename

    @property
    def timestamp(self):
        return self._timestamp

    @property
    def status(self):
        return self._status

    @property
    def name(self):
        return self._name

    @property
    def readablename(self):
        return self._readablename

    @staticmethod
    def from_node(node):
        timestamp = get_text_by_tag(node, 'timestamp')
        status = get_text_by_tag(node, 'status')
        name = get_text_by_tag(node, 'name')
        readablename = get_text_by_tag(node, 'readableName')

        return NessusReport(timestamp, status, name, readablename)

########NEW FILE########
__FILENAME__ = NexposeAPI
#!/usr/bin/env python
#coding:utf-8
# Author:  Kurt Grutzmacher -- <kgrutzma@cisco.com>
# Purpose: Nexpose API Interface
# Created: 01/28/10

__version__ = "1.1"
__author__ = "Kurt Grutzmacher <kgrutzma@cisco.com>"

import os, sys, random
import unittest
import urllib, urllib2
import logging
logger = logging.getLogger("web2py.app.kvasir")

try:
    from lxml import etree as etree
except ImportError:
    try:
        import xml.etree.cElementTree as etree
    except ImportError:
        try:
            import xml.etree.ElementTree as etree
        except ImportError:
            raise Exception, "Unable to load any XML libraries for etree!"\
                  "Please install an xml library or Python 2.5 at least"

########################################################################
class NexposeAPIError(RuntimeError):
    pass

########################################################################
class NexposeAPI():
    """
    Nexpose API class for Python

    XXX: This does not perform server certificate verification!
    """

    #----------------------------------------------------------------------
    def __init__(self, host='127.0.0.1', port='3780'):
        self.log = logging.getLogger(self.__class__.__name__)
        self.opener = urllib2.build_opener()
        self.host = host
        self.port = port
        self.apiversion = "1.1"
        self.opener = urllib2.build_opener()
        self.sessionid = None
        self.syncid = random.randint(1, 65535)

    #----------------------------------------------------------------------
    def isLoggedIn(self):
        """
        Check to see if a session is logged in.
        """

        if self.sessionid:
            return True
        else:
            return False

    #----------------------------------------------------------------------
    def send_command(self, post_data=""):
        """
        Send XML request to Nexpose server and parse response.
        Performs a login if self.sessionid is None
        """

        url = "https://%s:%s/api/%s/xml" %(self.host, self.port, self.apiversion)
        if len(post_data) == 0:
            self.log.error("No XML text sent, can't do anthing.")
            return

        #post_data = urllib.urlencode(bodytext)
        req = urllib2.Request(url, data=post_data, headers={"Content-Type": "text/xml"})
        try:
            return self.opener.open(req)
        except:
            (exc_type, exc_value, exc_tb) = sys.exc_info()
            try:
                result = etree.parse(exc_value)
                self.log.warn("Caught a traceback: %r. etree result = %r", exc_value, result)
            except:
                mesg = exc_value
            raise NexposeAPIError(mesg)

    #----------------------------------------------------------------------
    def make_xml(self, name="", attributes={}, isroot=False, *args, **kwargs):
        """
        Generate an XML document and return the ascii text

        Requires an element name (name) and a dictionary of attributes that
        will be added to the element.
        """
        root = etree.Element(name)

        if isroot:
            root.set("sync-id", str(self.syncid))
            if self.sessionid:
                root.set('session-id', self.sessionid)

        for item in attributes.items():
            if item[1] == None:
                item[1] == ""
            root.set(item[0], item[1])

        if len(args) > 0:
            # We have multiple subelements, lets add them now
            for record in args:
                for element in record.keys():
                    child = etree.SubElement(root, element)
                    for item in record[element].items():
                        child.set(item[0], item[1])

        # don't need to add the standard xml header so we can create multiple xml elements
        # with this routine. It will always return a string.
        #return etree.tostring(root, xml_declaration=True, encoding='iso-8859-1')
        try:
            result = etree.tostring(root)
        except Exception, e:
            self.log.error("Error creating XML: %s" % (e))
            result = ""

        return result

    #----------------------------------------------------------------------
    def login(self, user_id="nxadmin", password="password"):
        """
        Process a login request and report success/failure
        """

        if self.sessionid:
            # remove any existing session ID
            self.sessionid = None

        attributes = {
            'user-id': user_id,
            'password': password,
        }

        loginxml = self.make_xml('LoginRequest', attributes, isroot=True)
        self.log.debug("Sending Login request:\n%s" % (loginxml))
        try:
            result = self.send_command(loginxml)
        except NexposeAPIError, e:
            self.log.error("Error connecting to Nexpose: %s" % (e))
            return False

        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            self.sessionid = data.attrib['session-id']
            self.log.debug("Session-id: %s" % (self.sessionid))
            return True
        else:
            self.log.warn("Failed to login!")

        return False

    #----------------------------------------------------------------------
    def logout(self):
        """
        Process a logout request and report success/failure
        """

        attributes = {
        }

        xml = self.make_xml('LogoutRequest', attributes, isroot=True)
        self.log.debug("Sending logout request:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            self.sessionid = None
            return True
        else:
            self.log.warn("Logout failed, clearing session anyways")
            self.sessionid = None

        return False

########################################################################
class Sites(NexposeAPI):
    """
    Nexpose Site configuration class
    """

    #----------------------------------------------------------------------
    def __init__(self, sessionid=None):
        NexposeAPI.__init__(self)
        if sessionid is not None:
            self.sessionid = sessionid
        self.log = logging.getLogger(self.__class__.__name__)
        self.site_id = "0"
        self.name = ""
        self.description = ""
        self.riskfactor = ""
        self.hosts = []
        self.credentials = {}
        self.alerts = {}

    #----------------------------------------------------------------------
    def save(self, siteid="-1", hosts=[], name="", description="", template="pentest-audit"):
        """
        Save changes to a new or existing site.

        XXX: Fix this.. it doesn't respond correctly!
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        if not name:
            self.log.warn("No site name set, generating random name")
            import random, string
            name = "".join([random.choice(string.digits + string.letters) for i in xrange(15)])

        hostxml=[]
        for host in hosts:
            hostxml.append("<Host>%s</Host>" % (host))

        # manually build the request
        xml = """
<SiteSaveRequest session-id="%s">
    <Site id="%s" name="%s" description="%s">
        <Hosts>%s</Hosts>
        <Credentials></Credentials>
        <Alerting></Alerting>
        <ScanConfig configID="%s" name="%s" templateID="%s"></ScanConfig>"
    </Site>
</SiteSaveRequest>""" % (self.sessionid, siteid, name, description, "".join(hostxml), siteid, name, template)

        #xml = self.make_xml('SiteSaveRequest', attributes, isroot=True)
        self.log.debug("Sending SiteSaveRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            return True
        else:
            errormsg = data.xpath('//message')[0].text
            self.log.warn("Failed SiteSaveRequest: %s " % (errormsg))

        return False

    #----------------------------------------------------------------------
    def scan(self, siteid):
        """
        Starts a scan of a site. Requires the Site ID number
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        if siteid < 1:
            self.log.warn("No SiteID provided")
            return ""

        attributes = {
            'site-id': str(siteid),
        }

        xml = self.make_xml('SiteScanRequest', attributes, isroot=True)
        self.log.debug("Sending SiteScanRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        result = {}
        if data.attrib['success'] == '1':
            return "Scan Started"
        else:
            self.log.warn("SiteScanRequest failed")

        return ""

    #----------------------------------------------------------------------
    def listings(self):
        """
        Provide a list of all sites the user is authorized to view or manage.

        Returns a dict of sites keyed with site id.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
        }

        xml = self.make_xml('SiteListingRequest', attributes, isroot=True)
        self.log.debug("Sending SiteListing request:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        result = {}
        if data.attrib['success'] == '1':
            sites = data.findall('SiteSummary')
            for site in sites:
                result[site.attrib['id']] = {
                    'name': site.attrib['name'],
                    'description': site.attrib['description'],
                    'riskfactor': site.attrib['riskfactor'],
                    'riskscore': site.attrib['riskscore'],
                }
            return result
        else:
            self.log.warn("SiteListing failed")

        return {}

    #----------------------------------------------------------------------
    def config(self, siteid=None):
        """
        Provide the configuration of a site including its associated assets.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        if siteid < 1:
            self.log.warn("No SiteID provided")
            return ""

        attributes = {
            'site-id': str(siteid),
        }

        xml = self.make_xml('SiteConfigRequest', attributes, isroot=True)
        self.log.debug("Sending SiteConfigRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        result = {}
        if data.attrib['success'] == '1':

            # find all the ranges and put the resulting dict into an incremental
            # dict of ranges
            ranges = data.findall('Site/Hosts/range')
            result['ranges'] = {}
            if len(ranges) > 0:
                count = 0
                for r in ranges:
                    result['ranges'][count] = r.attrib
                    count += 1

            # place the scanconfig data into a dict
            scanconfig = data.find('Site/ScanConfig')
            if len(scanconfig) > 0:
                result['scanconfig'] = scanconfig.attrib
            else:
                result['scanconfig'] = {}

            return result
        else:
            self.log.warn("SiteListing failed")

        return ""

########################################################################
class AssetGroup(NexposeAPI):
    """
    TODO: AssetGroup!
    """

    #----------------------------------------------------------------------
    def __init__(self, sessionid=None):
        """Constructor"""
        NexposeAPI.__init__(self)
        if sessionid is not None:
            self.sessionid = sessionid
        self.log = logging.getLogger(self.__class__.__name__)


    #----------------------------------------------------------------------
    def listing(self):
        """
        Provide a list of all asset groups the user is authorized to view or manage.

        Returns a dict of asstegroups keyed with group id.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
        }

        xml = self.make_xml('AssetGroupListingRequest', attributes, isroot=True)
        self.log.debug("Sending AssetGroupListingRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        result = {}
        if data.attrib['success'] == '1':
            #sites = data.findall('SiteSummary')
            #for site in sites:
            #    result[site.attrib['id']] = {
            #        'name': site.attrib['name'],
            #        'description': site.attrib['description'],
            #        'riskfactor': site.attrib['riskfactor'],
            #        'riskscore': site.attrib['riskscore'],
            #    }
            return result
        else:
            self.log.warn("AssetGroupListing failed")

        return {}

########################################################################
class Scan(NexposeAPI):
    """"""

    #----------------------------------------------------------------------
    def __init__(self, sessionid=None):
        """Constructor"""
        NexposeAPI.__init__(self)
        if sessionid is not None:
            self.sessionid = sessionid
        self.log = logging.getLogger(self.__class__.__name__)

    #----------------------------------------------------------------------
    def scanactivity(self):
        """
        Provide a list of current scan activities across all scan engines managed by the security console.

        Returns a dict of scanactivity keyed with group id.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
        }

        xml = self.make_xml('ScanActivityRequest', attributes, isroot=True)
        self.log.debug("Sending ScanActivityRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        scans = tree.getroot()
        result = {}
        if scans.attrib['success'] == '1':
            #sites = data.findall('SiteSummary')
            #for site in sites:
            #    result[site.attrib['id']] = {
            #        'name': site.attrib['name'],
            #        'description': site.attrib['description'],
            #        'riskfactor': site.attrib['riskfactor'],
            #        'riskscore': site.attrib['riskscore'],
            #    }
            return result
        else:
            self.log.warn("ScanActivity failed")

        return {}

    #----------------------------------------------------------------------
    def scanaction(self, action="Pause", scanid="1"):
        """
        A multiple function that performs actions against Scans where ...

        action = Pause/Resume/Stop :: a running scan.
        action = Status :: return status of a scan
        action = Statistics :: return statistics of a scan

        Case is important with action.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
            'scan-id': scanid,
        }

        reqtype = "Scan%sRequest" % (action)
        xml = self.make_xml(reqtype, attributes, isroot=True)
        self.log.debug("Sending %s:\n%s" % (reqtype, xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if action in ['Pause', 'Resume', 'Stop']:
            if data.attrib['success'] == '1':
                self.log.warn("%s succeeded" % (reqtype))
                return True
            else:
                self.log.warn("%s failed: %s" % (reqtype, data.find('Failure/message').text))
                return False

        result = {}

        if action == "Status":
            result['engine-id'] = data.attrib['engine-id']
            result['scan-id'] = data.attrib['scan-id']
            result['status'] = data.attrib['status']
            return result

        if action == "Statistics":
            # TODO: This...
            return result

        return False

########################################################################
class VulnData(NexposeAPI):
    """
    The Nexpose vulnerability class. Supports vulnerabilities summary
    output and detailed vulnerability requests
    """

    def __init__(self, sessionid=None):
        """"""
        NexposeAPI.__init__(self)
        if sessionid is not None:
            self.sessionid = sessionid
        self.vulnerabilities = {}
        self.vulnxml = ""
        self.log = logging.getLogger(self.__class__.__name__)

    #----------------------------------------------------------------------
    def populate_summary(self):
        """
        Populates a summary list of vulnerabilities checked by Nexpose to a dict.

        Must provide a NexposeAPI instance to use.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
        }

        xml = self.make_xml('VulnerabilityListingRequest', attributes, isroot=True)
        self.log.debug("Sending VulnerabilityListingRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            vulns = data.findall('VulnerabilitySummary')
            self.vulnxml = etree.tostring(data)
            for vuln in vulns:
                self.vulnerabilities[vuln.attrib['id']] = vuln.attrib
                #del(self.vulnerabilities[vuln.attrib['id']]['id'])
        else:
            self.log.warn("VulnerabilityListing failed")
            return False

        self.log.debug("Loaded %s Vulnerabilities..." % (len(self.vulnerabilities)))
        return True

    #----------------------------------------------------------------------
    def csvout(self):
        """
        Retrn a string of vulnerability data in csv format
        """
        if (self.vulnerabilities) < 1:
            self.log.error("No vulnerabilities populated, returning empty")
            return ""

        import csv, StringIO

        csvout = StringIO.StringIO()
        writer = csv.DictWriter(csvout, fieldnames=('id', 'title', 'severity', 'pciSeverity', 'cvssScore', 'cvssVector', 'published', 'added', 'modified'))
        for vuln in self.vulnerabilities.keys():
            writer.writerow(self.vulnerabilities[vuln])

        return csvout.getvalue()

    #----------------------------------------------------------------------
    def detail(self, vulnid="1"):
        """
        Provide the full details of a vulnerability, including its description, cross-references, and solution.

        Returns a python dictionary result
        """

        attributes = {
            'vuln-id': vulnid,
        }

        xml = self.make_xml('VulnerabilityDetailsRequest', attributes, isroot=True)
        self.log.debug("Sending VulnerabilityDetailsRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            #result = {}
            #vuln = data.find('Vulnerability')
            #vulnid = vuln.attrib['id']
            #result[vulnid] = vuln.attrib
            #for child in vuln.getchildren():
            #    result[vulnid][child] = child.text
            return data
        else:
            self.log.warn("VulnerabilityDetailsRequest failed: %s" % (data.find('Failure/message').text))

        return None


########################################################################
class Report(NexposeAPI):
    """
    Nexpose Reporting Tempaltes and Configuration
    """

    #----------------------------------------------------------------------
    def __init__(self, sessionid=None):
        NexposeAPI.__init__(self)
        if sessionid is not None:
            self.sessionid = sessionid
        self.log = logging.getLogger(self.__class__.__name__)

    #----------------------------------------------------------------------
    def templates(self):
        """
        Provide a list of all report templates the user can access on the security console.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
        }

        xml = self.make_xml('ReportTemplateListingRequest', attributes, isroot=True)
        self.log.debug("Sending ReportTemplateListingRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            return data
        else:
            self.log.warn("ReportTemplateListingRequest failed: %s" % (data.find('Failure/message').text))

        return {}

    #----------------------------------------------------------------------
    def listing(self):
        """
        Provide a listing of all report definitions the user can access on the security console.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
        }

        xml = self.make_xml('ReportListingRequest', attributes, isroot=True)
        self.log.debug("Sending ReportListingRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            return data
        else:
            self.log.warn("ReportListingRequest failed: %s" % (data.find('Failure/message').text))

        return {}

    #----------------------------------------------------------------------
    def templateconfig(self, templateid=1):
        """
        Retrieve the configuration for a report template
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
            'template-id': str(templateid)
        }

        xml = self.make_xml('ReportTemplateConfigRequest', attributes, isroot=True)
        self.log.debug("Sending ReportTemplateConfigRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            return data
        else:
            self.log.warn("ReportTemplateConfigRequest failed: %s" % (data.find('Failure/message').text))

        return {}

    #----------------------------------------------------------------------
    def history(self, reportcfg=1):
        """
        Provide a history of all reports generated with the specified report definition.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
            'reportcfg-id': str(reportcfg)
        }

        xml = self.make_xml('ReportHistoryRequest', attributes, isroot=True)
        self.log.debug("Sending ReportHistoryRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            return data
        else:
            self.log.warn("ReportHistoryRequest failed: %s" % (data.find('Failure/message').text))

        return {}

    #----------------------------------------------------------------------
    def getconfig(self, reportcfg=1):
        """
        Retrieve the configuration for a report definition.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
            'reportcfg-id': str(reportcfg)
        }

        xml = self.make_xml('ReportConfigRequest', attributes, isroot=True)
        self.log.debug("Sending ReportConfigRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            return data
        else:
            self.log.warn("ReportConfigRequest failed: %s" % (data.find('Failure/message').text))

        return {}

    #----------------------------------------------------------------------
    def generate(self, reportid=1):
        """
        Generate a new report using the specified report definition.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
            'report-id': str(reportid)
        }

        xml = self.make_xml('ReportGenerateRequest', attributes, isroot=True)
        self.log.debug("Sending ReportGenerateRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            return data
        else:
            try:
                error = data.find('Failure/message').text
            except:
                error = data.find('Exception/message').text
            self.log.warn("ReportGenerateRequest failed: %s" % (error))

        return {}

    #----------------------------------------------------------------------
    def delete(self, reportcfgid=1, reportid=1):
        """
        Delete a previously generated report or report definition.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
            'reportcfg-id': str(reportcfgid),
            'report-id': str(reportid)
        }

        xml = self.make_xml('ReportDeleteRequest', attributes, isroot=True)
        self.log.debug("Sending ReportDeleteRequest:\n%s" % (xml))
        result = self.send_command(xml)
        tree = etree.parse(result)
        self.log.debug("Result: %s" % (etree.tostring(tree)))
        data = tree.getroot()
        if data.attrib['success'] == '1':
            return data
        else:
            try:
                error = data.find('Failure/message').text
            except:
                error = data.find('Exception/message').text
            self.log.warn("ReportDeleteRequest failed: %s" % (error))

        return {}

    #----------------------------------------------------------------------
    def adhoc_generate(self, templateid='full-audit', rptformat='scap-xml', filtertype='site', filterid='1', compareto=None):
        """
        Generate a report once using a simple configuration, and send it back in a multipart mime response.
        """

        if not self.isLoggedIn():
            self.log.warn("No Nexpose API instance provided...")
            return False

        attributes = {
        }

        # ghetto the xml generation here... oh well!

        xml = """
<ReportAdhocGenerateRequest session-id="%s" sync-id="%s">
  <AdhocReportConfig template-id="%s" format="%s"/>
  <Filters>
    <filter type="%s" id="%s"/>
  </Filters>
</ReportAdhocGenerateRequest>""" % (self.sessionid, self.syncid, templateid, rptformat, filtertype, filterid)

        if compareto:
            xml += """<Baseline compareTo="%(compareto)s"/>\n"""

        self.log.debug("Sending ReportAdhocGenerateRequest:\n%s" % (xml))
        result = self.send_command(xml)

        response = ''.join(result.readlines())
        if response.find('--AxB9sl3299asdjvbA') == -1:
            tree = etree.fromstring(response)
            try:
                error = tree.find('Failure/message').text
            except:
                error = tree.find('Exception/message').text
            self.log.warn("ReportAdhocGenerateRequest failed: %s" % (error))

            return ""

        resp_text = response.split('--AxB9sl3299asdjvbA')[1].split('response_xml')[1]
        tree = etree.fromstring(resp_text)
        self.log.debug("Response XML: %s" % (etree.tostring(tree)))

        if tree.get('success') == '1':
            # result is successful, return the report data
            import base64
            return base64.b64decode(response.split('--AxB9sl3299asdjvbA')[2].split('base64')[1])
        else:
            try:
                error = tree.find('Failure/message').text
            except:
                error = tree.find('Exception/message').text
            self.log.warn("ReportAdhocGenerateRequest failed: %s" % (error))

        return ""


########################################################################
class APIUnitTests(unittest.TestCase):
    """
    Nexpose API Unit Tests
    """

    def testLoginRequest(self):
        napi = NexposeAPI()
        napi.user_id = "test"
        napi.password = "password"
        validxml = "<?xml version='1.0' encoding='iso-8859-1'?>\n<LoginRequest><user-id>test</user-id><password>password</password></LoginRequest>"
        loginxml = napi.loginrequest()
        self.assertEqual(loginxml, validxml, "login:\n\texpecting: %s\n\treceived: %s" % (validxml, loginxml))

########################################################################

if __name__=='__main__':
    from optparse import OptionParser

    # set up commandline arguments
    Progname=os.path.basename(sys.argv[0])
    Usage="%prog usage: XXX:[command_line_args]\n" \
         "%prog usage: -h\n" \
         "%prog usage: -V"
    optparser = OptionParser(usage=Usage, version="%prog: $Id:$" )
    optparser.add_option("-d", "--debug", dest = "debug", action="store_true", help="log debugging messages")
    optparser.add_option("-t", "--tests", dest = "unittest", action="store_true", help="Perform unit tests")
    optparser.add_option("-u", "--userid", dest = "userid", action="store", default="nxadmin", help="Username")
    optparser.add_option("-p", "--passwd", dest = "password", action="store", default="password", help="Password")
    optparser.add_option("-v", "--verbose", dest = "verbose", action="store_true", help="be verbose")
    optparser.add_option("-s", "--server", dest = "server", action="store", default="localhost", help="Nexpose Server")
    optparser.add_option("-r", "--port", dest = "port", action="store", default="3780", help="Nexpose Port")
    optparser.add_option("-l", "--listvulns", dest = "listvulns", action="store", default=None, help="List vuln summary (xml|csv)")
    optparser.add_option("-i", "--interactive", dest = "interactive", action="store_true", help="Drop into an interactive prompt")
    optparser.add_option("-b", "--basictests", dest = "basictests", action="store_true", help="Run some basic tests")


    #optparser.add_option("-N", "--name", dest="var_n",
    #  action= "store" | "append" | "store_true" | "store_false"
    #  type = "int"
    #  default="foo", metavar="SOME_STRING", help="store a string")
    (options, params) = optparser.parse_args()

    root_log = logging.getLogger()
    if options.debug:
        root_log.setLevel(logging.DEBUG)
    elif options.verbose:
        root_log.setLevel(logging.INFO)
    else:
        root_log.setLevel(logging.WARN)
    handler = logging.StreamHandler()
    logformat = "%(name)s: %(levelname)s: %(message)s"
    handler.setFormatter(logging.Formatter(logformat))
    root_log.addHandler(handler)
    log = logging.getLogger(Progname)

    if options.unittest:
        suite = unittest.TestLoader().loadTestsFromTestCase(APIUnitTests)
        unittest.TextTestRunner(verbosity=2).run(suite)
        sys.exit(0)

    napi = NexposeAPI()
    napi.host = options.server
    napi.port = options.port
    napi.login(options.userid, options.password)

    if options.interactive:
        try:
            import IPython
        except ImportError:
            sys.exit("IPython not installed, won't continue...")

        argv = ['-pi1','In <\\#>:','-pi2','   .\\D.:','-po','Out<\\#>:']
        banner = '*** Starting Interactive Shell - Ctrl-D to exit...\n\nnapi is your NexposeAPI variable to play with\n'

        if IPython.__version__ >= "0.11":
            from IPython.config.loader import Config
            cfg = Config()
            cfg.InteractiveShellEmbed.prompt_in1="myprompt [\\#]> "
            cfg.InteractiveShellEmbed.prompt_out="myprompt [\\#]: "
            #cfg.InteractiveShellEmbed.profile=ipythonprofile
            # directly open the shell
            IPython.embed(config=cfg, banner2=banner)
        else:
            try:
                from IPython.Shell import IPShellEmbed
                argv = ['-pi1','In <\\#>:','-pi2','   .\\D.:','-po','Out<\\#>:']
                ipshell = IPShellEmbed(argv,banner='*** Starting Interactive Shell - Ctrl-D to exit...\n\nnapi is your NexposeAPI variable to play with\n')
                ipshell.set_exit_msg('Buh-bye!')
                ipshell()
            except ImportError, e:
                sys.exit("IPython not installed, won't continue...")

    if options.listvulns:
        vuln_class = VulnData(napi.sessionid)
        vuln_class.populate_summary()
        if (vuln_class.vulnerabilities) > 0:
            if options.listvulns.upper() == "CSV":
                print vuln_class.csvout()
            else:
                print vuln_class.vulnxml
        else:
            print "Error: No Vulnerabilities loaded, check your Nexpose server address or user/pass"

        sys.exit(0)

    if options.basictests:
        # do some basic testing here...
        sites = Sites(napi.sessionid)
        sites = sites.sitelisting()
        for site in sites:
            print "Site #%s: %s" % (site, sites[site]['name'])

        siteconfig = sites.siteconfig(napi, '2')

        print "\n\nSite configuration for SiteID 2\n"
        for count in siteconfig['ranges']:
            for a in siteconfig['ranges'][count]:
                print "%s => %s" % (a, siteconfig['ranges'][count][a])

        print "\nScanconfig for SiteID 2\n"
        for a in siteconfig['scanconfig']:
            print "%s => %s" % (a, siteconfig['scanconfig'][a])

        #napi.sitesave("2")

        assetgroup = AssetGroup(napi.sessionid)
        assetgroup.listing()

        scan_class = Scan(napi.sessionid)
        scan_class.scanactivity()
        scan_class.scanaction("Status", "1")
        scan_class.scanaction("Pause", "1")
        scan_class.scanaction("Resume", "1")
        scan_class.scanaction("Stop", "1")

    napi.logout()

########NEW FILE########
__FILENAME__ = nxajax
#!/usr/bin/python
# Oh.. theres an easier way to grab the nexposeCCSessionID: from the HTML response! argh!

import urllib2, urllib, cookielib
import os, sys, logging
from lxml import etree
from StringIO import StringIO

class NexposeAJAXError(RuntimeError):
    pass

class SmartRedirHandler(urllib2.HTTPRedirectHandler):
    """
    Inserts the nexposeCCSessionID cookie back into headers from an HTTP '303 See Other' response.
    Might be a bug in urllib2.HTTPRedirectHandler
    """
    def http_error_303(self, req, fp, code, msg, hdrs):
        nsession = hdrs.headers
        result = urllib2.HTTPRedirectHandler.http_error_303(self, req, fp, code, msg, hdrs)
        result.headers = nsession
        return result

class NXAJAX():
    """
    Nexpose's JavaScript-free magical 'AJAX' class for Python
    at least until the Nexpose API is extended

    Use getsession to store the logged in session. Pass an externally stored getsession to NXAJAX(getsession) to resume it.
    """
    def __init__(self, session=None):
        self.log = logging.getLogger(self.__class__.__name__)
        if session == None:
            self.cj = cookielib.LWPCookieJar()
        else:
            self.cj = cookielib.LWPCookieJar()
            for index, value in session.iteritems():
                self.cj.set_cookie(value)
        self.opener = urllib2.build_opener(SmartRedirHandler, urllib2.HTTPCookieProcessor(self.cj), urllib2.HTTPSHandler(debuglevel=1))
        # because we do some cookie-jacking, a numeric host cookie domain will not match the jacked cookie's domain apparently.
        self.host = "localhost"
        self.port = "3780"
        self.user_id = "nxadmin"
        self.password = "password"
        self.session = session

    def login(self):
        """
        Logs into Nexpose's web interface with supplied credentials.

        Returns a urllib2 response object. Session cookies will be available in najax.cj._cookies
        """
        attributes = {
                'loginRedir':'/home.html',
                'nexposeccusername':self.user_id,
                'nexposeccpassword':self.password,
                'login':'Login'
        }
        login_data = urllib.urlencode(attributes)
        request = urllib2.Request("https://%s:%s/login.html" %(self.host, self.port), login_data)
        return self.opener.open(request)

    def logout(self):
        request = urllib2.Request("https://%s:%s/logout.html" %(self.host, self.port))
        return self.opener.open(request)

    def send_ajax(self, path="", xmldata=""):
        """
        Performs either a GET or POST action and returns a urllib2 response object.

        Nexpose requires an additional header for authentication: nexposeCCSessionID
        This session id is pulled from a cookie value that is set during login
        """
        if len(xmldata) == 0:
            self.log.debug("Empty POST data. Performing GET for " + path)
            request = urllib2.Request("https://%s:%s/%s" %(self.host, self.port, path))
            return self.opener.open(request)
        else:
            self.log.debug("Sending POST to " + path)
            request = urllib2.Request("https://%s:%s/%s" %(self.host, self.port, path), xmldata, headers={"Content-Type": "text/xml","nexposeCCSessionID":self.cj._cookies['localhost.local']['/']['nexposeCCSessionID'].value})
            return self.opener.open(request)

    def getsession(self):
        """
        Need to have a way to access the session login info so it can be passed in later
        """
        getsession = {}
        for index, value in enumerate(self.cj):
            getsession[index] = value
        return getsession

class ScanTemplates():

    def __init__(self):
        self.log = logging.getLogger(self.__class__.__name__)

    def listscantemps(self, xml=False, najax=None):
        """
        Will list all Scan Templates. By default the output is a /r delimited list.
        Invoke with xml=True to have output in pretty XML
        """
        # TODO: consolidate XML output handling into its own function
        # TODO: XML philosophy: attributes or elements?
        if not najax:
            self.log.warn("No instance provided")
            return False
        else:
            response = najax.send_ajax("ajax/scantemplate_synopsis.txml?printDocType=0&tableID=ScanTemplateSynopsis")
            parser = etree.HTMLParser()
            tree = etree.parse(StringIO(response.read()), parser)
            result_xml = etree.Element("ScanTemplates")
            if xml:
                for child in tree.iterfind("//tr"):
                    #sub = etree.SubElement(result_xml, "ScanTemplate", id = child[0].text)
                    sub = etree.SubElement(result_xml, "templateid")
                    sub.text = child[0].text
                return etree.tostring(result_xml, pretty_print=True)
            else:
                for child in tree.iterfind("//tr"):
                    print child[0].text

    def exporttemplate(self, template, najax=None):
        """
        Exports the specified ScanTemplate XML string.
        'template' is the 'ScanTemplate id=' attribute from Nexpose-generated XML.
        """
        if not najax:
            self.log.warn("No instance provided")
            return False
        else:
            response = najax.send_ajax("ajax/scantemplate_config.txml?templateid=" + template)
            tree = etree.parse(StringIO(response.read()))
            return etree.tostring(tree.getroot())

    def importtemplate(self, template, najax=None):
        """
        Imports the specified template, where 'template' is a valid XML document.
        *No error handling other than a traceback if its invalid.
        *No checks are made to ensure the ScanTemplate is a valid Nexpose format.
        """
        # TODO error handling of an invalid Nexpose scan template
        if not najax:
            self.log.warn("No instance provided")
            return False
        else:
            #parsed = etree.parse(StringIO(template), etree.XMLParser())
            #Nexpose rejects urlencoded POSTDATA
            post_data = template
            response = najax.send_ajax("ajax/save_scantemplate_config.txml", post_data)
            tree = etree.parse(StringIO(response.read()))
            return etree.tostring(tree.getroot())

    def deletetemplate(self, template, najax=None):
        """
        Deletes the specified ScanTemplate
        """
        # TODO error handling
        if not najax:
            self.log.warn("No instance provided")
            return False
        else:
            post_data = urllib.urlencode({'templateid':template})
            response = najax.send_ajax("admin/scan-template-delete.html", post_data)
            return response.read()


if __name__=='__main__':
    from optparse import OptionParser

    # set up commandline arguments
    Progname=os.path.basename(sys.argv[0])
    Usage="%prog usage: XXX:[command_line_args]\n" \
         "%prog usage: -h\n" \
         "%prog usage: -V"
    optparser = OptionParser(usage=Usage, version="%prog: $Id:$" )
    optparser.add_option("-u", "--userid", dest = "userid", action="store", default="nxadmin", help="Username")
    optparser.add_option("-p", "--passwd", dest = "password", action="store", default="password", help="Password")
    optparser.add_option("-v", "--verbose", dest = "verbose", action="store_true", help="be verbose")
    optparser.add_option("-s", "--server", dest = "server", action="store", default="localhost", help="Nexpose Server")
    optparser.add_option("-r", "--port", dest = "port", action="store", default="3780", help="Nexpose Port")
    optparser.add_option("-d", "--debug", dest = "debug", action="store_true", help="log debugging messages")
    optparser.add_option("-l", "--list", dest = "listscantemps", action="store_true", help="List scan templates")
    optparser.add_option("-e", "--export", dest = "Export", action="store", help="Export scan template id list to stdout")
    optparser.add_option("-i", "--import", dest = "Import", action="store", help="Import scan template id list from xml file")
    optparser.add_option("-k", "--delete", dest = "Delete", action="store", help="Delete scan template id")
    optparser.add_option("-x", "--xmlout", dest = "xmlout", action="store_true", help="Output in XML")

    (options, params) = optparser.parse_args()

    root_log = logging.getLogger()
    if options.debug:
        root_log.setLevel(logging.DEBUG)
    elif options.verbose:
        root_log.setLevel(logging.INFO)
    else:
        root_log.setLevel(logging.WARN)
    handler = logging.StreamHandler()
    logformat = "%(name)s: %(levelname)s: %(message)s"
    handler.setFormatter(logging.Formatter(logformat))
    root_log.addHandler(handler)
    log = logging.getLogger(Progname)

    najax = NXAJAX()
    najax.user_id = options.userid
    najax.password = options.password
    najax.host = options.server
    najax.port = options.port
    najax.login()

    if options.listscantemps:
        scantemps = ScanTemplates()
        if options.xmlout:
            print scantemps.listscantemps(options.xmlout, najax)
        else:
            scantemps.listscantemps(options.xmlout, najax)
        sys.exit(0)

    if options.Export:
        scantemps = ScanTemplates()
        print scantemps.exporttemplate(options.Export, najax)
        sys.exit(0)

    if options.Import:
        scantemps = ScanTemplates()
        template = open(options.Import, 'r')
        template = template.read()
        print scantemps.importtemplate(template, najax)
        sys.exit(0)

    if options.Delete:
        scantemps = ScanTemplates()
        print scantemps.deletetemplate(options.Delete, najax)
        sys.exit(0)

########NEW FILE########
__FILENAME__ = canvas
# -*- coding: utf-8 -*-

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Immunity CANVAS Utilities for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from gluon import current
import gluon.contrib.simplejson
import sys, os, time, re, HTMLParser
from StringIO import StringIO
from skaldship.general import html_to_markmin
from skaldship.hosts import get_host_record, do_host_status
import logging
logger = logging.getLogger("web2py.app.kvasir")

try:
    from lxml import etree as etree
except ImportError:
    try:
        from xml.etree import cElementTree as etree
    except ImportError:
        try:
            import cElementTree as etree
        except ImportError:
            try:
                from xml.etree import ElementTree as etree
            except ImportError:
                try:
                    from elementtree import ElementTree as etree
                except ImportError:
                    raise("Unable to load any XML libraries for ElementTree!"\
                          "Please install an xml library or Python 2.5 at least")

##----------------------------------------------------------------------------

def process_exploits(filename=None):
    """
    Process Canvas Exploits.xml file into the database
    """

    localdb = current.globalenv['db']

    if filename is None:
        expurl = 'http://exploitlist.immunityinc.com/home/serve/live'
        from gluon.tools import fetch
        import sys
        try:
            print("Downloading CANVAS Exploits XML file... Please wait...")
            xmldata = fetch(expurl)
            print("Download complete. %s bytes received" % (sys.getsizeof(xmldata)))
        except Exception, e:
            raise Exception("Error downloading CPE XML file: %s" % (e))

    logging.info("Processing %s ..." % (filename))

    try:
        if filename is None:
            from StringIO import StringIO
            exploits = etree.parse(StringIO(xmldata))
        else:
            exploits = etree.parse(filename)
    except etree.ParseError, e:
        print("Error processing file: ", e)
        logging.error("Error processing file: %s" % (e))
        return e
    except IOError, e:
        print("Error opening file: ", e)
        logging.error("Error opening file: %s" % (e))
        return e

    r = exploits.getroot()

    # CANVAS uses CVE identifiers to link their exploits
    from exploits import add_exploit, connect_exploits
    counter = 0
    exploits_added = []
    for exploit in r.xpath('//Exploit'):
        #<Exploit cve="CVE-2008-4250" desc="Windows Server Service Underflow (MS08-067)" name="ms08_067"/>
        cve = exploit.get('cve')
        # sometimes they forget to put CVE- in front of the CVE ID
        if not cve.startswith('CVE-'):
            cve = "CVE-%s" % (cve)
        f_name = exploit.get('desc')
        f_title = exploit.get('name')  # seems backwards but not
        f_description = f_title
        f_source = 'canvas'
        f_rank = 'average'              # rank is not defined in xml, default to average
        f_level = 'Intermediate'        # level is not defined in xml, default to Intermediate

        res = add_exploit(
            cve=cve,
            f_name=f_name,
            f_title=f_title,
            f_description=f_description,
            f_source=f_source,
            f_rank=f_rank,
            f_level=f_level,
        )
        if res > 0:
            counter += 1
        else:
            logger.error("Error importing exploit: %s" % (f_name))

    connect_exploits()
    logging.info("%d exploits added/updated" % (counter))
    return True

##----------------------------------------------------------------------------

########NEW FILE########
__FILENAME__ = cpe
# -*- coding: utf-8 -*-

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## CPE Utilities for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from gluon import current
import gluon.contrib.simplejson
import sys, os, time, re, HTMLParser, string
from StringIO import StringIO
import logging
logger = logging.getLogger("web2py.app.kvasir")

##------------------------------------------------------------------------

def split_cpe(cpe=None):
    """
    Splits a CPE record entry into a dictionary using an ugly try/except
    block.

    >>> split_cpe('cpe:/o:freebsd:freebsd:5.2')
    {'product': 'freebsd', 'vendor': 'freebsd', 'version': '5.2', 'language': None, 'update': None, 'edition': None, 'part': 'o'}
    >>> split_cpe('cpe:/a:openbsd:openssh:3.6')
    {'product': 'openssh', 'vendor': 'openbsd', 'version': '3.6', 'language': None, 'update': None, 'edition': None, 'part': 'a'}
    """
    # title, part, vendor,     product,             version, update, edition,    language
    # cpe:/  o:    microsoft:  windows_server_2008: -:       gold:   datacenter: english
    part = None
    vendor = ""
    product = ""
    version = ""
    update = None
    edition = None
    language = None

    if cpe.startswith('cpe:/'):
        cpe = cpe.split('/')[1]

    try:
        part, vendor, product, version, update, edition, language = cpe.split(':')
    except ValueError:
        try:
            part, vendor, product, version, update, edition = cpe.split(':')
        except ValueError:
            try:
                part, vendor, product, version, update = cpe.split(':')
            except ValueError:
                try:
                    part, vendor, product, version = cpe.split(':')
                except ValueError:
                    try:
                        part, vendor, product, version = cpe.split(':')
                    except ValueError:
                        # if it gets this far then the file is bad
                        try:
                            part, vendor, product = cpe.split(':')
                        except ValueError, e:
                            logger.error("Uh, I have no idea what CPE data this is. Error: %s\n%s" % (e, cpe))

    return {
        'part': part,
        'vendor': vendor,
        'product': product,
        'version': version,
        'update': update,
        'edition': edition,
        'language': language
    }

##------------------------------------------------------------------------

def normalize_cpe(cpe_string):
    """
    Normalize CPE data given known formats or capitalize.

    >>> normalize_cpe('freebsd freebsd 5.2')
    'FreeBSD FreeBSD 5.2'
    >>> normalize_cpe('Microsoft Windows xp')
    'Microsoft Windows XP'
    >>> normalize_cpe('microsoft windows 2008 datacenter')
    'Microsoft Windows 2008 Datacenter'
    >>> normalize_cpe('apple iphone')
    'Apple iPhone'
    """

    exchange_table = {
        'hp': 'HP',
        'freebsd': 'FreeBSD',
        'openbsd': 'OpenBSD',
        'netbsd': 'NetBSD',
        'openssh': 'OpenSSH',
        'hpux': 'HP/UX',
        'cisco ios': 'Cisco IOS',
        'apple ios': 'Apple iOS',
        'ios': 'IOS',
        'iphone': 'iPhone',
        'windows_xp': 'Windows XP',
        'windows_nt': 'Windows NT',
        'windows_2000_server': 'Windows 2000 Server',
        'windows_2003_server': 'Windows 2003 Server',
        'windows_2008_server': 'Windows 2008 Server',
        'xp': 'XP',
        'nt': 'NT',
    }

    new_cpe = []
    for cpe in cpe_string.split(' '):
        new_cpe.append(exchange_table.get(cpe, cpe.title()))

    return " ".join(new_cpe)

##------------------------------------------------------------------------

def make_cpe_title(cpe=None):
    """
    Create a CPE title based on cpe string value using string.capwords()

    Ex: cpe:/o:microsoft:windows:xp == Microsoft Windows Xp

    Only care about vendor, product and version

    >>> make_cpe_title('cpe:/o:microsoft:windows:xp')
    'Microsoft Windows XP'
    >>> make_cpe_title('cpe:/o:freebsd:freebsd:5.2')
    'FreeBSD FreeBSD 5.2'
    >>> make_cpe_title('cpe:/o:cisco:ios:12.4')
    'Cisco IOS 12.4'
    """
    if cpe is None:
        return ""

    if not isinstance(cpe, dict):
        cpe_dict = split_cpe(cpe)

    parts = ['vendor', 'product', 'version']

    return normalize_cpe(" ".join([cpe_dict[part] for part in parts]))

##------------------------------------------------------------------------

def lookup_cpe(cpe_name=None):
    """
    Look up a CPE OS record:
      1. Look up the cpe name in t_os database
      2. If not found lookup values in t_cpe_os database
      3. If not found add to t_os database by splitting up the cpe name
    Returns os_id record
    """
    if not cpe_name:
        return None

    # cpe:/o: is stripped in the database
    cpe_name = cpe_name.replace('cpe:/o:', '')

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    os_id = None

    # lookup the CPE entry in t_os first
    cpe_res = db(db.t_os.f_cpename == cpe_name).select().first()
    if cpe_res is not None:
        # found CPE entry in local t_os database, assign cpe_res.id to os_id
        logger.info(" [-] Found CPE OS ID from t_os table: %s" % (cpe_res.f_title))
        os_id = cpe_res.id
    else:
        # CPE not found in t_os, search master CPE db and copy if found
        cpe_res = db(db.t_cpe_os.f_cpename == cpe_name).select().first()
        if cpe_res:
            # we have found a valid master CPE entry, copy it to t_os
            logger.info(" [-] Found CPE OS ID from t_cpe_os master table: %s" % (cpe_res.f_title))
            # first search to see if the f_title is already in the t_os table.
            title_recs = db(db.t_os.f_title == cpe_res.f_title).select().first()
            if not title_recs:
                logger.info(" [-] Adding CPE OS to t_os")
                os_id = db.t_os.insert(f_cpename = cpe_res.f_cpename,
                                       f_title = cpe_res.f_title,
                                       f_vendor = cpe_res.f_vendor,
                                       f_product = cpe_res.f_product,
                                       f_version = cpe_res.f_version,
                                       f_update = cpe_res.f_update,
                                       f_edition = cpe_res.f_edition,
                                       f_language = cpe_res.f_language,
                                       f_isincpe = True)
                db.commit()
            else:
                logger.info(" [-] Found CPE title already in t_os")
                os_id = title_recs.id
                #db.t_os(title_recs.id).update(f_cpename = cpe_res.f_cpename,
                #                              f_title = cpe_res.f_title,
                #                              f_vendor = cpe_res.f_vendor,
                #                              f_product = cpe_res.f_product,
                #                              f_version = cpe_res.f_version,
                #                              f_update = cpe_res.f_update,
                #                              f_edition = cpe_res.f_edition,
                #                              f_language = cpe_res.f_language,
                #                              f_isincpe = True)

    if not os_id:
        # no CPE or OS record found, insert a new t_os record
        cpe_dict = split_cpe("o:" + cpe_name)
        title = make_cpe_title("o:" + cpe_name)
        logger.info(" [!] No os_id found, inserting new record: %s" % (cpe_name))

        try:
            os_id = db.t_os.insert(
                f_cpename = cpe_name,
                f_title = title,
                f_vendor = cpe_dict['vendor'],
                f_product = cpe_dict['product'],
                f_version = cpe_dict['version'],
                f_update = cpe_dict['update'],
                f_edition = cpe_dict['edition'],
                f_language = cpe_dict['language']
            )
        except Exception, e:
            logger.error("Error inserting OS: %s" % (e))
        db.commit()

    return os_id

##------------------------------------------------------------------------

def process_xml(filename=None, download=False, wipe=False):
    """
    Process the CPE data through an uploaded file or have it download directly
    from the MITRE webserver
    """
    try:
        from lxml import etree
    except ImportError:
        try:
            import xml.etree.cElementTree as etree
        except ImportError:
            try:
                import xml.etree.ElementTree as etree
            except:
                raise Exception("Unable to find valid ElementTree module.")

    if download:
        # grab cpe data from http://static.nvd.nist.gov/feeds/xml/cpe/dictionary/official-cpe-dictionary_v2.2.xml
        from gluon.tools import fetch
        import sys
        try:
            logger.info("Downloading http://static.nvd.nist.gov/feeds/xml/cpe/dictionary/official-cpe-dictionary_v2.3.xml.gz... Please wait...")
            gz_cpedata = fetch('http://static.nvd.nist.gov/feeds/xml/cpe/dictionary/official-cpe-dictionary_v2.3.xml.gz')
            logger.info("Download complete. %s bytes received" % (sys.getsizeof(gz_cpedata)))
        except Exception, e:
            raise Exception("Error downloading CPE XML file: %s" % (e))

    logger.info("Processing CPE XML file...")

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    try:
        if download:
            import gzip
            from cStringIO import StringIO
            gz_cpedata = StringIO(gz_cpedata)
            infile = gzip.GzipFile(fileobj=gz_cpedata).read()
            cpe_xml = etree.parse(StringIO(infile))
        else:
            cpe_xml = etree.parse(filename)
    except etree.ParseError, e:
        raise Exception("Error loading CPE XML file: %s " % (e))

    root = cpe_xml.getroot()

    # from the t_errata table, a few key/value pairs:
    #
    #   cpe_last_upload = date/time of last upload
    #   cpe_timestamp   = timestamp from official dictionary
    #   cpe_schema_ver  = CPE schema version
    #   cpe_product_ver = CPE product version

    cpe_info = root.find('generator')
    curr_errata = {}
    curr_errata['timestamp'] = db(db.t_errata.f_key == 'cpe_timestamp').select().first()
    curr_errata['pversion'] = db(db.t_errata.f_key == 'cpe_product_ver').select().first()
    curr_errata['schemaver'] = db(db.t_errata.f_key == 'cpe_schema_ver').select().first()

    if cpe_info:
        pver = cpe_info.get('product_version')
        schemaver = cpe_info.get('schema_version')
        timestamp = cpe_info.get('timestamp')

        if pver:
            curr_errata['pversion'].update_record(
                f_key = 'cpe_product_ver',
                f_value = pver,
            )

        if timestamp:
            curr_errata['timestamp'].update_record(
                f_key = 'cpe_timestamp',
                f_value = timestamp,
            )

        if schemaver:
            curr_erata['schemaver'].update_record(
                f_key = 'cpe_schema_ver',
                f_value = cpe_schemaver,
            )
        db.commit()

    os_added = 0
    apps_added = 0
    hardware_added = 0
    if wipe:
        db.t_cpe_os.truncate(mode="CASCADE")
        db.t_cpe_hardware.truncate(mode="CASCADE")
        db.t_cpe_apps.truncate(mode="CASCADE")
        db.commit()

    for cpeitem in root.findall('{http://cpe.mitre.org/dictionary/2.0}cpe-item'):
        name = cpeitem.get('name')
        title = cpeitem.findtext('{http://cpe.mitre.org/dictionary/2.0}title')
        cpe_dict = split_cpe(name)
        name = name[7:]

        try:
            if cpe_dict['part'] == "o":
                resid = db.t_cpe_os.update_or_insert(
                    f_cpename = name,
                    f_title = title,
                    f_vendor = cpe_dict['vendor'],
                    f_product = cpe_dict['product'],
                    f_version = cpe_dict['version'],
                    f_update = cpe_dict['update'],
                    f_edition = cpe_dict['edition'],
                    f_language = cpe_dict['language']
                )
                os_added += 1
            """
            elif cpe_dict['part'] == "a":
                resid = db.t_cpe_apps.update_or_insert(
                    f_cpename = name,
                    f_title = title,
                    f_vendor = cpe_dict['vendor'],
                    f_product = cpe_dict['product'],
                    f_version = cpe_dict['version'],
                    f_update = cpe_dict['update'],
                    f_edition = cpe_dict['edition'],
                    f_language = cpe_dict['language']
                )
                apps_added += 1
            elif cpe_dict['part'] == "h":
                resid = db.t_cpe_hardware.update_or_insert(
                    f_cpename = name,
                    f_title = title,
                    f_vendor = cpe_dict['vendor'],
                    f_product = cpe_dict['product'],
                    f_version = cpe_dict['version'],
                    f_update = cpe_dict['update'],
                    f_edition = cpe_dict['edition'],
                    f_language = cpe_dict['language']
                )
                hardware_added += 1
            """
        except Exception, e:
            logger.warn("Exception adding CPE data: %s" % (e))
            pass

        db.commit()

    msg = 'CPE items added/updated (%d/O, %d/A, %d/H)' % (os_added, apps_added, hardware_added)
    logger.info(msg)
    return msg

##------------------------------------------------------------------------

def _doctest():
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    _doctest()

########NEW FILE########
__FILENAME__ = cvrf
# -*- coding: utf-8 -*-

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Common Vulnerability Reference File module for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from gluon import current
import gluon.contrib.simplejson
import sys, os, time, re, HTMLParser
from StringIO import StringIO
import logging
logger = logging.getLogger("web2py.app.kvasir")

##------------------------------------------------------------------------

process_xml(filename=None):
    """
    Processes a single CVRF XML file into t_vulndata structure.
    CVRF info can be found at http://www.icasi.org/cvrf
    """

    try:
        from lxml import etree
    except ImportError:
        try:
            import xml.etree.cElementTree as etree
        except ImportError:
            try:
                import xml.etree.ElementTree as etree
            except:
                raise Exception("Unable to find valid ElementTree module.")


    logger.info("Processing CVRF XML file [%s]" % (filename))

    db = current.globalenv['db']
    cache = current.globalenv['cache']


########NEW FILE########
__FILENAME__ = exploits
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Exploits utility module
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from gluon import current

import logging
logger = logging.getLogger("web2py.app.kvasir")

##-------------------------------------------------------------------------

def connect_exploits():
    """Connects Vulnerability IDs and Exploits together in t_exploit_references"""

    db = current.globalenv['db']
    cache = current.globalenv['cache']
    exploits_db = db.t_exploits
    exprefs_db = db.t_exploit_references
    vd = db.t_vulndata
    vr = db.t_vuln_refs
    vrefs = db.t_vuln_references
    s_vulndata = db(vr.id == vrefs.f_vuln_ref_id)

    db.t_exploit_references.truncate()
    db.commit()
    existing_vulnids = {}
    for r in db(vd).select(vd.id, vd.f_vulnid, cache=(cache.ram, 60)):
        existing_vulnids[r.f_vulnid] = r.id

    # Run through exploits with vulnid strings set
    exploits = db(exploits_db.f_vulnid != None).select(cache=(cache.ram, 60))
    refs = 0
    for exploit in exploits:
        for vulnid in exploit['f_vulnid']:
            if existing_vulnids.has_key(vulnid):
                refdata = { 'f_vulndata_id': existing_vulnids[vulnid], 'f_exploit_id': exploit.id }
                try:
                    exprefs_db.insert(**refdata)
                    refs += 1
                except:
                    pass
                db.commit()

    existing_cveids = {}
    for r in s_vulndata(vr.f_text != None).select(vr.f_text, vrefs.f_vulndata_id, cache=(cache.ram, 60)):
        existing_cveids[r.t_vuln_refs.f_text] = r.t_vuln_references.f_vulndata_id

    # run through exploits with cve strings set
    for exploit in db(exploits_db.f_cve != None).select(cache=(cache.ram, 60)):
        for cve in exploit['f_cve']:
            if existing_cveids.has_key(cve):
                refdata = { 'f_vulndata_id': existing_cveids[cve], 'f_exploit_id': exploit.id }
                try:
                    exprefs_db.insert(**refdata)
                    refs += 1
                except:
                    pass
                db.commit()

    return refs

##-------------------------------------------------------------------------

def add_exploit(
        cve=None,
        vuln_ids=None,
        f_name=None,
        f_title=None,
        f_description=None,
        f_source=None,
        f_rank='unknown',
        f_level='Unknown'
    ):
    """
    Adds an exploit to the database
    """
    db = current.globalenv['db']

    logging.info(" [*] Adding: %s -- %s" % (f_source, f_title))

    q = (db.t_exploits.f_title == f_title) & (db.t_exploits.f_source == f_source)
    exp_id = db.t_exploits.update_or_insert(
        q,
        f_name=f_name,
        f_title=f_title,
        f_description=f_description,
        f_source=f_source,
        f_rank=f_rank,
        f_level=f_level,
        f_vulnid=vuln_ids,
        f_cve=cve,
    )
    db.commit()

    return exp_id

########NEW FILE########
__FILENAME__ = general
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## General utility module
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""


from gluon import current
import logging
logger = logging.getLogger("web2py.app.kvasir")

#db = current.globalenv['db']
#cache = current.globalenv['cache']

##-------------------------------------------------------------------------

def utf_8_decoder(unicode_data):
    for line in unicode_data:
        yield line.decode('utf-8')

##-------------------------------------------------------------------------

def severity_mapping(sevnum='1', totype='color'):
    """
    Convert a severity number (1-10) to a name (Info, Low, Medium, High)
    or color
    """
    severitymap = [ (0, 'Informational', 'grey'),
                    (1, 'Informational', 'grey'),
                    (2, 'Informational', 'grey'),
                    (3, 'Low', 'green'),
                    (4, 'Low', 'green'),
                    (5, 'Medium', 'orange'),
                    (6, 'Medium', 'orange'),
                    (7, 'Medium', 'orange'),
                    (8, 'High', 'red'),
                    (9, 'High', 'red'),
                    (10, 'High', 'red'),
                  ]
    return severitymap[int(sevnum)]

##-------------------------------------------------------------------------

def vulntype_mapping(vulntype='exploited'):
    """
    Converts a vulnerability type to a color.
    """
    vulnmap = {
        'potential': 'grey',
        'vulnerable-version': 'green',
        'vulnerable-exploited': 'orange',
        'exploited': 'red',
    }

##-------------------------------------------------------------------------

def cvss_metrics(record):
    if record is None:
        return "NO RECORD SUBMITTED"

    return "AV:%s/AC:%s/Au:%s/C:%s/I:%s/A:%s" % (record.f_cvss_av,
                                                 record.f_cvss_ac,
                                                 record.f_cvss_au,
                                                 record.f_cvss_c,
                                                 record.f_cvss_i,
                                                 record.f_cvss_a)

##-------------------------------------------------------------------------

def vuln_data(vuln, html=True, full=True):
    """Returns a dict of all useful vulnerability data from a record,
    including printable cvss, references and exploits"""

    from gluon.contrib.markmin.markmin2html import markmin2html

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    if type(vuln) is type(int):
        vuln = db.t_vulndata[vuln]

    if vuln is None:
        return "NO RECORD SUBMITTED"

    if current.globalenv['settings'].use_cvss:
        severity = vuln.f_cvss_score
    else:
        severity = vuln.f_severity

    if full:
        # full == True means all information including references and exploits
        refdata = []
        for ref in db(db.t_vuln_references.f_vulndata_id == vuln.id).select(cache=(cache.ram, 300)):
            refdata.append([ db.t_vuln_refs[ref.f_vuln_ref_id].f_source,
                             db.t_vuln_refs[ref.f_vuln_ref_id].f_text ])

        expdata = []
        for exp_ref in db(db.t_exploit_references.f_vulndata_id == vuln.id).select(cache=(cache.ram, 300)):
            exp = db.t_exploits[exp_ref.id]
            if exp is not None:
                expdata.append([exp.f_name,
                                exp.f_title,
                                markmin2html(exp.f_description),
                                exp.f_source,
                                exp.f_rank,
                                exp.f_level
                              ])

        return (vuln.id,
                vuln.f_vulnid,
                vuln.f_title,
                severity_mapping(severity),
                vuln.f_cvss_score,
                cvss_metrics(vuln),
                markmin2html(vuln.f_description),
                markmin2html(vuln.f_solution),
                vuln.f_pci_sev,
                refdata,
                expdata,
               )

    else:
        # full = False means just the header info (vulnid, title, sevs, cvss)
        return (vuln.id,
                vuln.f_vulnid,
                vuln.f_title,
                severity_mapping(severity),
                vuln.f_cvss_score,
                cvss_metrics(vuln),
                vuln.f_pci_sev,
               )

##-------------------------------------------------------------------------

def make_good_url(url, addition="/"):
    """Appends addition to url, ensuring the right number of slashes
    exist and the path doesn't get clobbered"""

    if url is None:
        return None

    if addition[0] == "/":
        addition = addition.lstrip('/')
    urlpath = urlsplit(url)[2]
    if urlpath[len(urlpath)-1] == '/':
        url = urljoin(url, addition)
    else:
        url = urljoin(url, '/'+addition)
    return url

##-------------------------------------------------------------------------

def encode_url_for_xml(url):
    """
    Replaces special characters that XML doesn't like to see in URLs
    """
    if type(url) is not type(str()):
        return

    url = url.replace('&', '&amp;')
    url = url.replace('<', '&lt;')
    url = url.replace('>', '&gt;')
    url = url.replace('"', '%22')
    return url

##-------------------------------------------------------------------------

def get_url(options={}):
    """Connect to options['url'] and retrieve data"""
    if options.has_key('url') is None:
        return ""
    if options.has_key('username'):
        # add basic auth header
        key = base64.b64encode(options['username']+':'+options.get('password',''))
        headers = {'Authorization': 'Basic ' + key}
    else:
        headers = None

    values = { 'desc': options.get('type', ''),
               'description': options.get('name', '') }
    data = urllib.urlencode(values)

    try:
        req = urllib2.Request(options['url'], data, headers)
        response = urllib2.urlopen(req)
    except urllib2.URLError, e:
        raise Exception(e)

    return response.read()

##-------------------------------------------------------------------------

def get_oreally_404(rfolder):
    """
    Picks a random oreally image and returns the filename
    """
    import os
    from random import choice
    imgdir = os.path.join(rfolder, 'static/images/oreally')
    if os.path.isdir(imgdir):
        files = os.listdir(imgdir)
        return choice(files)

##-------------------------------------------------------------------------

def html_to_markmin(html):
    """
    Replace HTML with Markmin, converting unicode to references first

    >>> html_to_markmin('<p class="foo"><b>Bold</b><i>Italics</i><ol><li>Item 1</li><li><a href="http://kvasir.io">Kvasir</a></li></ol><br>')
    "**Bold**''Italics''\n- Item 1\n- [[Kvasir http://kvasir.io]]\n\n\n\n"
    >>> html_to_markmin(u'<p>asdfsadf</p>')
    'asdfsadf\n\n'
    >>> html_to_markmin(u'<p>\ufffdq\ufffd</p>')
    '\xef\xbf\xbdq\xef\xbf\xbd\n\n'
    >>> html_to_markmin('[[ a link http://url.com]]')
    '[[a link http://url.com]]'
    """
    if html is None:
        return ''
    from gluon.html import markmin_serializer, TAG
    html = html.encode('ascii', 'xmlcharrefreplace')    # cleanup unicode
    html = TAG(html).flatten(markmin_serializer)        # turn to markmin
    html = html.replace('[[ ', '[[')                      # fix bad url
    html = html.replace(' ]]', ']]')                      # fix bad url
    return html

##-------------------------------------------------------------------------

def check_datadir(folder=None):
    """
    Checks to see if data/ folder and sub-folders exist. Creates them if not.
    """
    if not folder:
        return False

    import os
    datadir = os.path.join(folder, 'data')
    if not os.path.exists(datadir):
        logger.info("Creating data directories in %s..." % datadir)
        os.mkdir(datadir, 0775)

    for dirname in [
        'passwords', 'passwords/unix', 'passwords/win', 'passwords/other', 'passwords/misc',
        'db', 'db/oracle', 'db/mysql', 'db/mssql', 'db/psql', 'db/other', 'stats',
        'screenshots', 'scanfiles', 'configs', 'misc', 'rpcclient', 'session-logs', 'backups'
    ]:
        d = os.path.join(datadir, dirname)
        if not os.path.exists(d):
            os.mkdir(d, 0755)

    return True

##-------------------------------------------------------------------------

def exploitdb_update(indexfile):
    """
    Update the t_exploitdb table
    """
    if not indexfile:
        return "No file sent to process"

    import csv
    db = current.globalenv['db']

    db.t_exploitdb.truncate()
    db.commit()

    count = 0
    reader = csv.DictReader(open(indexfile, 'rb'), delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    for line in reader:
        db.t_exploitdb.insert(
            f_eid=line['id'],
            f_file=line['file'],
            f_description=line['description'],
            f_date=line['date'],
            f_author=line['author'],
            f_platform=line['platform'],
            f_type=line['type'],
            f_port=line['port'],
        )
        count += 1

    db.commit()
    if db(db.t_exploitdb).count() == 0:
        message = 'Unable to load data'
    else:
        message = 'Load complete: %s records created' % (count)

    return message


##-------------------------------------------------------------------------

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = hosts
# -*- coding: utf-8 -*-

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Abstraction layer for Hosts and DB functions
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from skaldship.log import log
import logging
from gluon import current


##-------------------------------------------------------------------------

def add_or_update(hostfields, update=False):
    """
    Add a host and return the record. If update is True and host already exists
    then the record is updated and returned
    """
    if not isinstance(hostfields, dict()):
        log(" [!] Hostfields is not a dictionary", logging.ERROR)
        return None

    host_rec = db(db.t_hosts.f_ipv4 == hostfields.get('f_ipv4'))
    if not host_rec:
        host_rec = db(db.t_hosts.f_ipv4 == hostfields.get('f_ipv6'))

    if not host_rec:
        try:
            host_id = db.t_hosts.insert(**hostfields)
            db.commit()
        except Exception, e:
            log("Error adding host: %s" % strerror(e))
            return None

        host_rec = db.t_hosts[host_id]
        log(" [*] Added host: %s" % host_title_maker(host_rec))
    else:
        if update:
            host_rec.update(**hostfields)
            log(" [*] Updated host: %s" % host_title_maker(host_rec))

    return host_rec


##-------------------------------------------------------------------------

def get_host_record(argument):
    """
    Returns a t_host record based on the argument. If argument is a ipv4/ipv6
    address look it up and return it
    """

    if argument is None:
        return None

    from gluon.validators import IS_IPADDRESS
    db = current.globalenv['db']
    cache = current.globalenv['cache']

    record = db.t_hosts(argument) or None
    if record:
        return record
    else:
        if IS_IPADDRESS(is_ipv4=True)(argument)[1] == None:
            host_rec = db(db.t_hosts.f_ipv4 == argument).select().first()
            if host_rec:
                record = db.t_hosts(host_rec['id'])
            else:
                record = None
        elif IS_IPADDRESS(is_ipv6=True)(argument)[1] == None:
            host_rec = db(db.t_hosts.f_ipv6 == request.args(0)).select().first()
            if host_rec:
                record = db.t_hosts(host_rec['id'])
            else:
                record = None
        else:
            record = None

    return record


##-------------------------------------------------------------------------

def create_hostfilter_query(fdata=[(None, None), False], q=None, dbname=None):
    """
    Creates or appends a hostfilter to a query variable

    hostfilter is a set of filter_type and filter_value
    db is the database scoped from the application
    q is the base query we're appending to
    dbname is used to ensure the first query adds the dbname.f_hosts_id fields
    """
    from gluon.dal import Query

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    if isinstance(fdata, list):
        hostfilter = fdata[0] or (None, None)
        unconfirmed = fdata[1] or False
    else:
        hostfilter = fdata
        unconfirmed = False

    if isinstance(hostfilter, (list,tuple)):
        f_type, f_value = hostfilter
    else:
        f_type, f_value = (None, None)

    if db is None or cache is None:
        return None

    if q is None:
        q = (db.t_hosts.id > 0)

    if unconfirmed:
        q &= (db.t_hosts.f_confirmed==False)

    if dbname is not None:
        # dbname specified, must add this to the first query
        # so we only query against the right hosts and don't
        # create this large response with 4x the results we expect

        # the following tables are one table away from hosts so need
        # to connect them to their parent first
        db_parent_map = {
            't_accounts': 't_services',
            }
        if dbname in db_parent_map.keys():
            q & (db.t_hosts.id == db[db_parent_map[dbname]].f_hosts_id)
        else:
            q &= (db.t_hosts.id == db[dbname].f_hosts_id)

    # go through the f_types and if matched add to the query
    if f_type == "userid":
        if f_value is not None:
            try:
                f_value = int(f_value)
                user_id = db(db.auth_user.id == f_value).select(cache=(cache.ram,120)).first()
            except:
                f_value = f_value.lower()
                user_id = db(db.auth_user.username.lower() == f_value).select(cache=(cache.ram,120)).first()
            q = q & (db.t_hosts.f_engineer == user_id)
    elif f_type == "assetgroup":
        #logger.debug("assetgroup filter: %s" % (f_value))
        if "%" in f_value:
            q &= (db.t_hosts.f_asset_group.contains(f_value))
        else:
            q &= (db.t_hosts.f_asset_group == f_value)
    elif f_type == "range":
        q = q & (db.t_hosts.f_ipv4.contains(f_value))
    elif f_type == "ipv4_list":
        if len(f_value) > 0:
            ip_q = (db.t_hosts.f_ipv4 == f_value[0])
        for host in f_value[1:]:
            ip_q |= (db.t_hosts.f_ipv4 == host)
        q = q &  ip_q
    elif f_type == "ipv6_list":
        if len(f_value) > 0:
            ip_q = (db.t_hosts.f_ipv6 == f_value[0])
        for host in f_value[1:]:
            ip_q |= (db.t_hosts.f_ipv6 == host)
        q = q & ip_q

    #logger.debug("hostfilter query = %s" % (str(q)))
    return q


##-------------------------------------------------------------------------
def host_title_maker(record):
    """
    Given a t_host record, return a string value to place
    in the HTML TITLE (via response.title) or any other text
    place. Like a form field, json output, etc.
    """

    if record is None:
        return "Unknown"

    hostinfo = []
    if record.f_ipv4:
        hostinfo.append(record.f_ipv4)
    if record.f_ipv6:
        hostinfo.append(record.f_ipv6)
    if record.f_hostname:
        hostinfo.append(record.f_hostname)

    return " :: ".join(hostinfo)


##-------------------------------------------------------------------------

def host_a_maker(record=None):
    """
    Give a host record, return a A object that will open a new window
    to that host record
    """

    from gluon.html import A, I, URL, SPAN

    if record is None:
        return A()

    if isinstance(record, type([str, int])):
        record = get_host_record(record)

    host_a = A(host_title_maker(record), _target="host_detail_%s" % (record.id),
               _href=URL('hosts', 'detail', extension='html', args=record.id))

    info_a = SPAN(A(I(_class='icon-info-sign'), _href='#', _class='withajaxpopover',
               **{'_data-load':URL('hosts', 'popover.json', args=record.id),
                  '_data-trigger':'hover', '_data-delay':"{show: 500, hide: 100}",
                  '_data-placement':'right', '_data-html':'true', '_data-container':'#popoverwrap'}
              ), _id="popoverwrap")

    return SPAN(host_a, info_a)


##-------------------------------------------------------------------------

def do_host_status(records=[], query=None, asset_group=None, hosts=[]):
    """
    Runs through the t_hosts table and updates the *_count entries.
    Can also run through a specific list of record IDs instead.
    """

    db = current.globalenv['db']
    cache = current.globalenv['cache']
    settings = current.globalenv['settings']

    # load all the vulndata from service_vulns into a dictionary
    # so we only have to query the memory variables instead of
    # the database each time. We need to collect:
    # svc_vulndata[f_service_id] = (f_vulnid, f_severity, f_cvss_score)
    svc_vulndata = {}

    rows = db(db.t_service_vulns.f_vulndata_id==db.t_vulndata.id).select(
        db.t_vulndata.id,
        db.t_vulndata.f_vulnid,
        db.t_vulndata.f_severity,
        db.t_vulndata.f_cvss_score,
        cache=(cache.ram, 60)
    )

    for r in rows:
        #exploitcount = db(db.t_exploit_references.f_vulndata_id == r.id).count()
        svc_vulndata[r.id] = (
            r.f_vulnid,
            r.f_severity,
            r.f_cvss_score,
            db(db.t_exploit_references.f_vulndata_id == r.id).count()
        )

    if asset_group:
        query = (db.t_hosts.f_asset_group==asset_group)
    if query is None:
        query = (db.t_hosts.id > 0)
    for rec in hosts:
        query &= (db.t_hosts.id == rec)
    rows = db(query).select()
    for row in rows:
        # get number of vulns and services
        # optimizing the performance by inner joins
        ser_vulns = db((db.t_services.f_hosts_id==row.id) &
                       (db.t_services.id==db.t_service_vulns.f_services_id)) \
                  .select(db.t_service_vulns.f_vulndata_id, cache=(cache.ram, 30))

        vulncount = 0
        vuln_sev = {}
        exploitcount = 0
        #servicecount = services = db(db.t_services.f_hosts_id==r.id).count()
        servicecount = db(db.t_services.f_hosts_id==row.id).count()

        # XXX: this is kind of slow and could probably be improved upon. The cache helps but maybe
        # pre-loading vulndata into memory instead of querying the database for each one? That would
        # take more memory resources but be a huge speed boost.
        #vuln_start = datetime.now()
        for svcvuln in ser_vulns:
            vulncount += 1
            vdata = svc_vulndata[svcvuln.f_vulndata_id]
            vuln_sev[vdata[0]] = ( vdata[1], vdata[2] )
            # grab the exploit count
            exploitcount += vdata[3]
        #vuln_time = timedelta.total_seconds(datetime.now() - vuln_start)
        #tot_vuln += vuln_time
        #print("Vuln processed in %s seconds" % (vuln_time))

        # breakdown of vuln severity
        # prepopulate the dictionary with 0
        sev_sum_dict = {}
        for a in range(1, 11):
            sev_sum_dict[a] = 0

        # parse through the vuln_sev dictionary and count the severity types
        # then add them to their respective sev_sum_dict entry
        for k,v in vuln_sev.iteritems():
            # take the severity and increment the sev_sum set item
            if settings.use_cvss:
                severity = int(float(v[1]))
            else:
                severity = v[0]
            count = sev_sum_dict.setdefault(severity, 0)
            count += 1
            sev_sum_dict[severity] = count

        # make the sparkline data string
        spark_list = []
        for k,v in sev_sum_dict.iteritems():
            spark_list.append(str(v))
        vuln_sum_spark = ",".join(spark_list)

        row.update_record(
            f_service_count = servicecount,
            f_vuln_count = vulncount,
            f_vuln_graph = vuln_sum_spark,
            f_exploit_count = exploitcount,
        )
        db.commit()

    return

##-------------------------------------------------------------------------

def pagination(request, curr_host):
    # Pagination! Send it the db, request and current host record, get back
    # a dictionary to put into the view.
    # TODO: Remove db, request and session for current.globalenv

    db = current.globalenv['db']
    cache = current.globalenv['cache']
    session = current.globalenv['session']

    from gluon.html import OPTION, SELECT, FORM, A, INPUT, SCRIPT

    hostlist = []
    hostprev="#"
    hostnext="#"
    hostselected=0
    hostnextstyle=hostprevstyle=""
    hostprevtitle=hostnexttitle=""
    hostindex=1
    # Create more filters here
    if request.vars.filterconfirmed is not None:
        session.hostfilterconfirmed=request.vars.filterconfirmed

    if session.hostfilterconfirmed == 'Unconfirmed [H]osts':
        query = (db.t_hosts)
    else:
        query = (db.t_hosts.f_confirmed==False)

    if session.hostfilter:
        hostfilter = session.hostfilter[0]
        if hostfilter is not None:
            if hostfilter[0] == "userid":
                query &= (db.t_hosts.f_engineer == hostfilter[1])
            elif hostfilter[0] == "assetgroup":
                query &= (db.t_hosts.f_asset_group.contains(hostfilter[1]))
            elif hostfilter[0] == "range":
                query &= (db.t_hosts.f_ipv4.contains(hostfilter[1]))

    for h_rec in db(query).select():
        hostlist.append(OPTION(host_title_maker(h_rec), _value=h_rec.id))
        if hostselected != 0 and hostnext == "#":
            hostnext = h_rec.id
            hostnexttitle="Go to " + host_title_maker(h_rec)
        if h_rec.id == curr_host.id:
            hostselected = hostindex
        if hostselected == 0:
            hostprev = h_rec.id
            hostprevtitle="Go to " + host_title_maker(h_rec)
        hostindex=hostindex+1

    if hostprev == "#":
        hostprevstyle="display:none"
    if hostnext == "#":
        hostnextstyle="display:none"

    pagination = {}
    pagination['previous'] = A("(p)",_id="prevhostlink" ,_class="button", _href=hostprev, _style=hostprevstyle, _title=hostprevtitle)
    pagination['next'] = A("(n)", _id="nexthostlink", _class="button", _href=hostnext, _style=hostnextstyle, _title=hostnexttitle)
    pagination['form'] = FORM(
                              SELECT(
                                hostlist, value=request.args(0), _class="chosen-select", _id="host_select",
                                _name="host_select", _onchange="window.location.href=$('#host_select').val()",
                                **{'_data-placeholder':'Choose a host'}
                              ),
                              SCRIPT('$("#host_select").select2({width: "80%"});'),
                         )
    pagination['host_number'] = "( %d/%d )" % (hostselected, len(hostlist))

    return pagination

########NEW FILE########
__FILENAME__ = jtr
#!/usr/bin/env python

"""
JohnTheRipper classes/functions

JohnPot

ntpwchk - Finds the correct case of a password given a word and NT hash
using case permutations.
"""

__author__ =   "Kurt Grutzmacher <kgrutzma@cisco.com>"
__date__ =     "05/17/2012"
__revision__ = "1.11"

import sys, os
import fileinput, re
import logging
logger = logging.getLogger("web2py.app.kvasir")

###

class JohnPot:
    def __init__(self):
        self.potdata = {}
        self.win_hash_regex = re.compile("^(\$NT\$|\$LM\$|M\$\w+#)")
    def upper_windows(self, pwhash):
        # upper case windows hashes
        if pwhash.startswith("M$"):
            # upper case DCC hashes
            try:
                h1, h2 = pwhash.split("#")
            except Exception:
                raise Exception("Bad M$ line: ", pwhash)
            pwhash = "%s#%s" % (h1, h2.upper())
        else:
            # upper case LM/NT hashes
            pwhash = pwhash.upper()
        return pwhash
    def load(self, potfile):
        for p in fileinput.input(potfile):
            # find the first location of a colon separator
            loc = p.find(':')
            if loc > 0:
                # the password hash goes up to the first colon
                pwhash = p[0:loc]
                # the password is everything after the colon
                pw = p[loc+1:].strip('\n')
                if self.win_hash_regex.match(pwhash):
                    pwhash = self.upper_windows(pwhash)
                if pwhash.startswith('$rakp$'):
                    # ipmi hashes
                    pwhash = pwhash.replace('$rakp$', '')
                    pwhash = pwhash.replace('$', ':')
                self.potdata[pwhash] = pw
            else:
                logging.error("Invalid line: ", p)
                continue
        logging.info("Loaded %s hashes" % (len(self.potdata)))
    def get(self, k):
        return self.potdata.get(k)
    def search(self, k):
        # auto upper LM, NT and DCC hashes:
        k = k.strip('\n')
        if self.win_hash_regex.match(k):
            k = self.upper_windows(k)
        if self.potdata.has_key(k):
            # key as-is
            return self.potdata[k]
        if self.potdata.has_key(k.upper()):
            # upper case
            return self.potdata[k.upper()]
        if self.potdata.has_key(k.lower()):
            # lower case
            return self.potdata[k.lower()]
        else:
            # key not found!
            return None

########
def ntpwchk(password, lmhash, nthash):
    """Performs mutation on a cleartext to find it in NT"""
    try:
        import smbpasswd
    except ImportError:
        raise Exception("Requires smbpasswd module. Please install it")

    def generate_perm(word, val):
        for i in range(0,len(word)):
            if (val & 1 << i):
                word = word[:i] + word[i].upper() + word[i+1:]
        return word

    #----------------------------------------------------------------------
    def permutations(word):
        val = 0
        perms = []
        word = word.lower()
        while (val < (1 << len(word))):
            perms.append(generate_perm(word,val))
            val += 1
        return perms

    permutations = permutations(password)

    for mutation in permutations:
        if nthash.upper() == smbpasswd.nthash(mutation):
            return (True, mutation.strip('\n'))

    return (False, None)

########NEW FILE########
__FILENAME__ = log
# -*- coding: utf-8 -*-

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Logging / print functions for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

import inspect
import logging
logger = logging.getLogger("web2py.app.kvasir")


def log(message, level=logging.INFO, *args, **kwargs):
    """
    If we're in a scheduler task then print the message out to stdout
    so it will be picked up. Otherwise use the current logger module
    and send with the specified level
    """
    try:
        from gluon import current
        if 'W2P_TASK' in current.globalenv:
            print(message)
    except ImportError:
        pass

    # find the calling function
    try:
        callfunc = inspect.stack()[1][3]
    except:
        callfunc = 'unknown'

    try:
        callmod = inspect.getmodule(inspect.stack()[1]).__name__
    except:
        callmod = 'unknown'

    msg = logging.makeLogRecord({
        'name': logger.name,
        'msg': message,
        'levelno': level,
        'levelname': logging.getLevelName(level),
        'funcName': callfunc,
        'module': callmod,
    })

    logger.handle(msg)

########NEW FILE########
__FILENAME__ = metasploit
# -*- coding: utf-8 -*-

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Metasploit Utilities for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from gluon import current
import logging
from skaldship.hosts import get_host_record, do_host_status

logger = logging.getLogger("web2py.app.kvasir")


##-------------------------------------------------------------------------

def msf_get_config(session={}):
    """
    Returns a dict of metasploit configuration settings based on yaml or session
    """

    msf_config = current.globalenv['settings']['kvasir_config'].get('metasploit') or {}
    config = {}
    config['key'] = session.get('msf_key', msf_config.get('api_key'))
    config['url'] = session.get('msf_url', msf_config.get('url', 'https://localhost:3790'))

    config['ws_num'] = session.get('msf_workspace_num', 1)
    config['workspace'] = session.get('msf_workspace', 'default')
    config['user'] = session.get('msf_user', None)

    return config


##-------------------------------------------------------------------------

def process_pwdump_loot(loot_list=[], msf=None):
    """
    Takes an array of loot records in loot_list, downloads the pwdump file and
    adds the users.
    """
    from skaldship.passwords import process_password_file, insert_or_update_acct

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    logging.debug('loot_list = %s' % (loot_list))
    data = []
    for loot_id in loot_list:
        loot = msf.loot_download(loot_id)
        if loot['ltype'] not in ['host.windows.pwdump', 'windows.hashes']:
            logging.error("Loot is not a pwdump, it is a %s" % loot['ltype'])
            continue
        else:
            # process the pwdump file
            pw_data = loot['data'].split('\n')
            accounts = process_password_file(
                pw_data=pw_data,
                file_type='PWDUMP',
                source='Metasploit',
            )

            # find the info/0 service id for the host
            host_id = get_host_record(loot['host'])
            query = (db.t_services.f_number == '0') & (db.t_services.f_proto == 'info') & (db.t_services.f_hosts_id == host_id)
            svc_id = db(query).select().first()
            if svc_id is None:
                # info/0 not found.. add it!
                svc_id = db.t_services.insert(f_proto="info", f_number="0", f_status="info", f_hosts_id=host_id)
                db.commit()

            # insert or update the account records
            resp_text = insert_or_update_acct(svc_id.id, accounts)
            logging.info("Added pwdump records for host: %s" % (loot['host']))
            data.append({ loot['host']: resp_text })

    return data

##-------------------------------------------------------------------------

def process_screenshot_loot(loot_list=[], msf=None):
    """
    Takes an array of loot records in loot_list, downloads the screenshot and
    adds it to the database
    """

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    loot_count = 0
    for loot_id in loot_list:
        loot = msf.loot_download(loot_id)
        ip = loot_dict[loot_id]
        if loot['ltype'] != 'host.windows.screenshot':
            logging.error(" [!] %s/%s is not a screenshot, it is a %s" % (ip, loot['name'], loot['ltype']))
        else:
            record = get_host_record(ip)
            if not record:
                logging.error(" [!] Cannot find record for %s" % (ip))
                continue

            db.t_evidence.update_or_insert(
                f_hosts_id = record.id,
                f_filename = "%s-msfpro-%s.png" % (ip, loot['name']),
                f_evidence = "%s-msfpro-%s.png" % (ip, loot['name']),
                f_data = loot['data'],
                f_type = 'Screenshot',
                f_text = 'From MetasploitPRO'
            )
            db.commit()
            loot_count += 1

    return loot_count

##-------------------------------------------------------------------------

def process_loot_files(loot_list=[]):
    """
    Processes locally stored (to web2py) MSF password loot files into the
    account database.

    Args:
        loot_list: an array of [filename, settings.password_file_types, port, host_id]

    Returns:
        An array of [filename, result text]
    """
    from skaldship.passwords import process_password_file, insert_or_update_acct
    import os
    db = current.globalenv['db']

    data = []
    for loot in loot_list:
        if isinstance(loot, []):
            (filename, file_type, port) = loot
        else:
            logger.error("Invalid loot sent: %s" % (loot))
            continue

        try:
            (proto, number) = port.split('/')
        except AttributeError, e:
            logger.error("Invalid port sent: %s", port)

        try:
            pw_data = open(filename, "rb").readlines().split('\n')
        except IOError, e:
            logger.error("Error opening %s: %s" % (filename, e))

        accounts = process_password_file(
            pw_data=pw_data,
            file_type=file_type,
            source='Metasploit',
        )

        # find the info/0 service id for the host
        host_id = get_host_record(loot['host'])
        query = (db.t_services.f_number == number) & (db.t_services.f_proto == proto) & (db.t_services.f_hosts_id == host_id)
        svc_id = db(query).select().first()
        if svc_id is None:
            # info/0 not found.. add it!
            svc_id = db.t_services.insert(f_proto=proto, f_number=number, f_hosts_id=host_id)
            db.commit()

        # insert or update the account records
        resp_text = insert_or_update_acct(svc_id.id, accounts)
        logging.info("Added loot accounts for host: %s" % ())
        data.append({ loot['host']: resp_text })

##-------------------------------------------------------------------------

def process_report_xml(
    filename=None,
    ip_ignore_list=None,
    ip_include_list=None,
    engineer=1,
    asset_group="Metasploit Import",
    update_hosts=True,
    ):
    """
    Processes a Metasploit XML Export for the following data and adds to the db:

    - Hosts and services
    - Credentials

    Generate the XML report by using db_export -t xml filename.xml or through WebUI

    TODO: Auto-exploits successful exploit attemps if matching CVE/VulnDB entry found
    """

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    try:
        from lxml import etree
    except ImportError:
        try:
            import xml.etree.cElementTree as etree
        except ImportError:
            try:
                import xml.etree.ElementTree as etree
            except:
                raise Exception("Unable to find valid ElementTree module.")

    # build the hosts only/exclude list
    ip_exclude = []
    if ip_ignore_list:
        ip_exclude = ip_ignore_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals
    ip_only = []
    if ip_include_list:
        ip_only = ip_include_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals

    print(" [*] Processing Metasploit Pro report file: %s" % (filename))

    try:
        xml = etree.parse(filename)
    except etree.ParseError, e:
        raise Exception(" [!] Invalid XML file (%s): %s " % (filename, e))
        return

    root = xml.getroot()

    # parse the hosts now
    hosts = root.findall("hosts/host")
    print(" [-] Parsing %d hosts" % (len(hosts)))
    stats = {}
    stats['hosts_added'] = 0
    stats['hosts_skipped'] = 0
    stats['hosts_updated'] = 0
    stats['services_added'] = 0
    stats['services_updated'] = 0
    stats['accounts_added'] = 0
    stats['accounts_updated'] = 0

    from gluon.validators import IS_IPADDRESS
    from skaldship.passwords import lookup_hash

    for host in hosts:
        didwhat = "Unknown"
        if host.findtext('state') != "alive":
            stats['hosts_skipped'] += 1
            continue

        hostfields = {}
        ipaddr = host.findtext('address')

        if len(ip_only) > 0 and ipaddr not in ip_only:
            print(" [-] Node is not in the only list... skipping")
            #sys.stderr.write(msg)
            stats['hosts_skipped'] += 1
            continue

        if IS_IPADDRESS(is_ipv4=True)(ipaddr)[1] == None:
            # address is IPv4:
            hostfields['f_ipv4'] = ipaddr
        elif IS_IPADDRESS(is_ipv6=True)(ipaddr)[1] == None:
            hostfields['f_ipv6'] = ipaddr
        else:
            logger.error("Invalid IP Address in report: %s" % (ipaddr))
            print(" [!] Invalid IP Address in report: %s" % (ipaddr))
            continue

        macaddr = host.findtext('mac')
        if macaddr:
            hostfields['f_macaddr'] = macaddr

        hostname = host.findtext('name')
        if hostname:
            hostfields['f_hostname'] = hostname

        # check to see if IP exists in DB already
        if hostfields.has_key('f_ipv4'):
            host_rec = db(db.t_hosts.f_ipv4 == hostfields['f_ipv4']).select().first()
        else:
            host_rec = db(db.t_hosts.f_ipv6 == hostfields['f_ipv6']).select().first()
        if host_rec is None:
            hostfields['f_asset_group'] = asset_group
            hostfields['f_engineer'] = engineer
            host_id = db.t_hosts.insert(**hostfields)
            db.commit()
            stats['hosts_added'] += 1
            print(" [-] Adding IP: %s" % (ipaddr))
            #sys.stderr.write(msg)
        elif host_rec is not None and update_hosts:
            db.commit()
            if hostfields.has_key('f_ipv4'):
                host_id = db(db.t_hosts.f_ipv4 == hostfields['f_ipv4']).update(**hostfields)
                db.commit()
                host_id = get_host_record(hostfields['f_ipv4'])
                host_id = host_id.id
                stats['hosts_updated'] += 1
                print(" [-] Updating IP: %s" % (hostfields['f_ipv4']))
            else:
                host_id = db(db.t_hosts.f_ipv6 == hostfields['f_ipv6']).update(**hostfields)
                db.commit()
                host_id = get_host_record(hostfields['f_ipv6'])
                host_id = host_id.id
                stats['hosts_updated'] += 1
                print(" [-] Updating IP: %s" % (hostfields['f_ipv6']))
        else:
            stats['hosts_skipped'] += 1
            db.commit()
            print(" [-] Skipped IP: %s" % (ipaddr))
            continue

        # add the <info> and <comments> as a note to the host
        info_note = host.findtext('info') or ''
        if info_note.startswith('Domain controller for '):
            db.t_netbios.update_or_insert(
                f_hosts_id=host_id,
                f_type="PDC",
                f_domain=info_note[22:].upper()
            )
        else:
            db.t_host_notes.update_or_insert(
                f_hosts_id=host_id,
                f_note=info_note,
            )
        db.commit()
        for comment in host.findall('comments/comment'):
            db.t_host_notes.update_or_insert(
                f_hosts_id=host_id,
                f_note=comment.text,
            )

        # process the services, adding any new
        for svc in host.findall('services/service'):
            f_number = svc.findtext('port')
            f_proto = svc.findtext('proto')
            f_status = svc.findtext('state')
            f_name = svc.findtext('name') or ''
            f_banner = svc.findtext('info') or ''

            if f_name in ['http', 'https']:
                f_name = f_name.upper()

            query = (db.t_services.f_proto==f_proto) & (db.t_services.f_number==f_number) & (db.t_services.f_hosts_id==host_id)
            svc_row = db(query).select().first()
            if svc_row:
                # we found a service record! Check for similar status, names and banners
                do_update = False
                if svc_row.f_status != f_status:
                    svc_row.f_status=f_status
                    do_update = True
                if svc_row.f_name != f_name:
                    svc_row.f_name=f_name
                    do_update = True
                if svc_row.f_banner != f_banner:
                    svc_row.f_banner=f_banner
                    do_update = True

                if do_update:
                    svc_row.update_record()
                    db.commit()
                    didwhat = "Updated"
                    stats['services_updated'] += 1
            else:
                # we have a new service!
                svc_id = db.t_services.insert(
                    f_proto=f_proto,
                    f_number=f_number,
                    f_status=f_status,
                    f_name=f_name,
                    f_banner=f_banner,
                    f_hosts_id=host_id
                )
                db.commit()
                didwhat = "Added"
                stats['services_added'] += 1

            print(" [-] %s service: (%s) %s/%s" % (didwhat, ipaddr, f_proto, f_number))

        for cred in host.findall('creds/cred'):
            # handle credential data
            f_password = None
            f_compromised = False

            cred_type = cred.findtext('ptype')
            if cred_type == "smb_hash":
                # add smb hashes to info/0 service
                query = (db.t_services.f_proto=='info') & (db.t_services.f_number=='0') & (db.t_services.f_hosts_id==host_id)
                svc_row = db(query).select().first()
                if not svc_row:
                    svc_id = db.t_services.insert(f_proto='info', f_number='0', f_hosts_id=host_id)
                else:
                    svc_id = svc_row.id

                pwhash = cred.findtext('pass')
                f_password = lookup_hash(pwhash)
                (lm, nt) = pwhash.split(':')
                user = cred.findtext('user')
                query = (db.t_accounts.f_services_id==svc_id) & (db.t_accounts.f_username.upper()==user.upper())
                acct_row = db(query).select().first()
                if acct_row:
                    # we have an account already, lets see if the hashes are in there
                    h1 = acct_row.f_hash1
                    if isinstance(h1, str):
                        if acct_row.f_hash1.upper() != lm.upper():
                            acct_row.f_hash1=lm.upper()
                            acct_row.f_hash1_type = "LM"
                            acct_row.f_hash2=nt.upper()
                            acct_row.f_hash2_type = "NT"
                            if f_password:
                                acct_row.f_compromised = True
                                acct_row.f_password = f_password
                            if not acct_row.f_source:
                                acct_row.f_source = "Metasploit Import"
                            acct_row.update_record()
                            db.commit()
                            stats['accounts_updated'] += 1
                            didwhat = "Updated"
                else:
                    # add a new account record
                    if f_password:
                        f_compromised = True
                    else:
                        f_compromised = False
                    acct_data = dict(
                        f_services_id=svc_id,
                        f_username=user,
                        f_password=f_password,
                        f_compromised=f_compromised,
                        f_hash1=lm.upper(),
                        f_hash1_type='LM',
                        f_hash2=nt.upper(),
                        f_hash2_type='NT',
                        f_source="Metasploit Import"
                    )
                    acct_id = db.t_accounts.insert(**acct_data)
                    db.commit()
                    stats['accounts_added'] += 1
                    didwhat = "Added"

            elif cred_type == 'smb_challenge':
                # add smb challenge hashes to info/0 service
                query = (db.t_services.f_proto=='info') & (db.t_services.f_number=='0') & (db.t_services.f_hosts_id==host_id)
                svc_row = db(query).select().first()
                if not svc_row:
                    svc_id = db.t_services.insert(f_proto='info', f_number='0', f_hosts_id=host_id)
                else:
                    svc_id = svc_row.id

                user = cred.findtext('user')
                query = (db.t_accounts.f_services_id==svc_id) & (db.t_accounts.f_username.upper()==user.upper())
                acct_row = db(query).select().first()
                if acct_row:
                    # we have an account already, lets see if the hashes are in there
                    h1 = acct_row.f_hash1
                    if isinstance(h1, str):
                        if acct_row.f_hash1.upper() != lm.upper():
                            acct_row.f_password = f_password
                            acct_row.f_hash1 = pwhash.upper()
                            acct_row.f_hash1_type = 'NTCHALLENGE'
                            acct_row.f_domain = cred.findtext('proof')
                            if not acct_row.f_source:
                                acct_row.f_source = "Metasploit Capture"
                            acct_row.update_record()
                            db.commit()
                            stats['accounts_updated'] += 1
                            didwhat = "Updated"
                else:
                    # new account record
                    f_password = lookup_hash(pwhash)
                    if f_password:
                        f_compromised = True
                    else:
                        f_compromised = False
                    acct_data = dict(
                        f_services_id=svc_id,
                        f_username=user,
                        f_password=f_password,
                        f_compromised=f_compromised,
                        f_hash1=pwhash.upper(),
                        f_hash1_type='NTCHALLENGE',
                        f_source="Metasploit Capture"
                    )
                    acct_id = db.t_accounts.insert(**acct_data)
                    db.commit()
                    stats['accounts_added'] += 1
                    didwhat = "Added"

            else:
                # for cred_type == 'password' or 'exploit':
                # add regular password
                f_proto = 'tcp'
                f_number = cred.findtext('port')
                if f_number == '445':
                    f_proto='info'
                    f_number='0'

                query = (db.t_services.f_proto==f_proto) & (db.t_services.f_number==f_number) & (db.t_services.f_hosts_id==host_id)
                svc_row = db(query).select().first()
                if not svc_row:
                    svc_id = db.t_services.insert(f_proto=f_proto, f_number=f_number, f_hosts_id=host_id)
                else:
                    svc_id = svc_row.id

                f_password = cred.findtext('pass')
                if f_password == "*BLANK PASSWORD*":
                    f_password = ''

                user = cred.findtext('user')
                svcname = cred.findtext('sname')

                # do some case mangling for known variations we want in all upper case
                if svcname == "vnc":
                    user = "vnc"

                query = (db.t_accounts.f_services_id==svc_id) & (db.t_accounts.f_username.upper()==user.upper())
                acct_row = db(query).select().first()
                f_source = cred.findtext('type')
                if f_source == 'captured':
                    f_source = "Metasploit Capture"
                else:
                    f_source = "Metasploit Import"
                if acct_row:
                    # we have an account already, lets see if the hashes are in there
                    if acct_row.f_password != f_password:
                        acct_row.f_password = f_password
                        acct_row.f_compromised = True
                        if not acct_row.f_source:
                            acct_row.f_source = f_source
                        acct_row.update_record()
                        db.commit()
                        stats['accounts_updated'] += 1
                        didwhat = "Updated"
                else:
                    # new account record
                    acct_data = dict(
                        f_services_id=svc_id,
                        f_username=user,
                        f_password=f_password,
                        f_source=f_source,
                        f_compromised=True
                    )
                    acct_id = db.t_accounts.insert(**acct_data)
                    db.commit()
                    stats['accounts_added'] += 1
                    didwhat = "Added"

            print(" [-] Account %s: (%s) %s" % (didwhat, ipaddr, user))

    do_host_status()

    msg = " [*] Import complete: hosts: (%s/A, %s/U, %s/S) - services: (%s/A, %s/U), creds: (%s/A, %s/U)"\
        % (
            stats['hosts_added'],
            stats['hosts_updated'],
            stats['hosts_skipped'],
            stats['services_added'],
            stats['services_updated'],
            stats['accounts_added'],
            stats['accounts_updated']
        )

    print(msg)
    return msg

########NEW FILE########
__FILENAME__ = nessus
# -*- coding: utf-8 -*-

__version__ = "1.1"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2014 Cisco Systems, Inc.
##
## Nessus Utilities for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from gluon import current
import sys, os, time, re
from datetime import datetime
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO
from skaldship.hosts import get_host_record, do_host_status
from skaldship.exploits import connect_exploits
from skaldship.services import Services
from gluon.validators import IS_SLUG
from skaldship.log import log
import logging
logger = logging.getLogger("web2py.app.kvasir")

try:
    from lxml import etree
except ImportError:
    import sys
    if not sys.hexversion >= 0x02070000:
        raise Exception('python-lxml or Python 2.7 or higher required for Nessus parsing')
    try:
        from xml.etree import cElementTree as etree
    except ImportError:
        try:
            from xml.etree import ElementTree as etree
        except:
            raise Exception('No valid ElementTree parser found')

##-------------------------------------------------------------------------

def nessus_get_config(session={}):
    """
    Returns a dict of Nessus configuration settings based on yaml or session
    """

    nessus_config = current.globalenv['settings']['kvasir_config'].get('nessus') or {}
    config = {}
    config['ignored_plugins'] = nessus_config.get('ignored_plugins', [19506, 11219, 34277])
    config['servers'] = {}
    for server in nessus_config.get('servers'):
        for k,v in server.iteritems():
            config['servers'][k] = {
                'url': v.get('url', 'http://localhost:8834/'),
                'user': v.get('user', 'admin'),
                'password': v.get('password', 'password')
            }

    return config


##-------------------------------------------------------------------------

def vuln_time_convert(vtime=''):
    """
    Convert Nessus date (YYYY/MM/DD) into python datetime
    """
    if not vtime:
        tval = datetime(1970, 1, 1)
    else:
        if isinstance(vtime, str):
            (year, mon, day) = vtime.split('/')
            tval = time.strptime(vtime, "%Y/%m/%d")
    return datetime.fromtimestamp(time.mktime(tval))


##-------------------------------------------------------------------------
class NessusHosts:
    def __init__(self, engineer, asset_group, ip_include, ip_exclude, update_hosts):
        self.db = current.globalenv['db']
        self.engineer = engineer
        self.asset_group = asset_group
        self.ip_include = ip_include
        self.ip_exclude = ip_exclude
        self.update_hosts = update_hosts
        self.stats = {
            'added': 0,
            'skipped': 0,
            'updated': 0,
        }

    def parse(self, host_properties):
        """
        Parse out the <HostProperties> xml content or CSV line.

        There can be a number of <tag> entries that are either useful to us in
        t_hosts or other areas. These are processed and returned as dictionary
        entries in 'hostdata'

        Args:
            host_properties: A <HostProperties> section from .nessus or a CSV line

        Returns:
            t_hosts.id, { hostdata }
        """
        from gluon.validators import IS_IPADDRESS
        hostdata = {}
        if etree.iselement(host_properties):
            for tag in host_properties.findall('tag'):
                hostdata[tag.get('name')] = tag.text
            ipaddr = hostdata.get('host-ip')
        else:
            # with CSV each line has all the hostdata fields so we set them here for use later
            ipaddr = host_properties.get('IP Address')
            if not ipaddr:
                # Scanner CSV, use Host
                ipaddr = host_properties.get('Host')
            hostdata['mac-address'] = host_properties.get('MAC Address', '')
            hostdata['host-fqdn'] = host_properties.get('DNS Name', '')
            hostdata['netbios-name'] = host_properties.get('NetBIOS Name', '')

        if (ipaddr not in self.ip_include and self.ip_include) or (ipaddr in self.ip_exclude):
            log("Host in exclude or not in include list, skipping")
            self.stats['skipped'] += 1
            return None, {}

        host_id = get_host_record(ipaddr)
        if host_id and not self.update_hosts:
            return host_id, hostdata

        # new host found, pull what we need for t_hosts
        hostfields = {}
        hostfields['f_engineer'] = self.engineer
        hostfields['f_asset_group'] = self.asset_group
        hostfields['f_confirmed'] = False

        # check ipv4/ipv6 and set hostfields accordingly
        if IS_IPADDRESS(is_ipv4=True)(ipaddr)[1] is None:
            hostfields['f_ipv4'] = ipaddr
        elif IS_IPADDRESS(is_ipv6=True)(ipaddr)[1] is None:
            hostfields['f_ipv6'] = ipaddr
        else:
            log("Invalid IP Address in HostProperties: %s" % ipaddr, logging.ERROR)
            return None, {}

        # pull out relevant hostfields
        for (k,v) in hostdata.iteritems():
            if k == 'mac-address':
                # multiple mac addrs may appear wildly, just pull the first
                hostfields['f_macaddr'] = v[:v.find('\n')]
            elif k == 'host-fqdn':
                hostfields['f_hostname'] = v
            elif k == 'netbios-name':
                hostfields['f_netbios_name'] = v

        if not self.update_hosts and not host_id:
            result = self.db.t_hosts.validate_and_insert(**hostfields)
            if not result.id:
                log("Error adding host to DB: %s" % result.errors, logging.ERROR)
                return None, {}
            self.stats['added'] += 1
            host_id = result.id
            log(" [-] Adding host: %s" % ipaddr)
        elif self.update_hosts:
            if hostfields['f_ipv4']:
                host_id = self.db(self.db.t_hosts.f_ipv4 == hostfields['f_ipv4']).update(**hostfields)
                self.db.commit()
                host_id = get_host_record(hostfields['f_ipv4'])
                if host_id:
                    host_id = host_id.id
                log(" [-] Updating IP: %s" % (hostfields['f_ipv4']))
            else:
                host_id = self.db(self.db.t_hosts.f_ipv6 == hostfields['f_ipv6']).update(**hostfields)
                self.db.commit()
                host_id = get_host_record(hostfields['f_ipv6'])
                host_id = host_id.id
                log(" [-] Updating IP: %s" % (hostfields['f_ipv6']))
            self.stats['updated'] += 1

        return host_id, hostfields


##-------------------------------------------------------------------------
class NessusVulns:
    """
    Since Nessus puts all vulnerability data into the ReportHost section
    we need to hold a mapping of db.t_vulndata.id to pluginID and also keep
    a link of fname to pluginID.
    """
    def __init__(self):
        self.vulns = {}         # { 'pluginID': [db.t_vulndata.id, vulndata] }
        self.db = current.globalenv['db']
        self.cache = current.globalenv['cache']
        self.stats = {
            'added': 0,
            'processed': 0
        }
        # list of references to add. these are fields in the xml vulndata
        self.ref_types = ['cve', 'osvdb', 'bid', 'urls', 'cpe', 'cert']
        # list of references that are single fields in the xml vulndata
        self.single_refs = ['msft']

    def db_vuln_refs(self, vuln_id=None, vulndata={}, extradata={}):
        """
        Add or update vulnerability references such as CPE, MSF Bulletins, OSVDB, Bugtraq, etc.

        Args:
            vuln_id: The db.t_vulndata reference id
            vulndadta: A dictionary of vulnerability data from t_vulndata
            extradata: A dictionary of extra vulndata

        Returns:
            None
        """
        if not vulndata:
            log(" [!] No vulndata sent!", logging.ERROR)
            return

        if not extradata:
            log(" [!] No extradata sent!", logging.ERROR)
            return

        if not vuln_id:
            log(" [!] No vulnerability record id sent!", logging.ERROR)
            return

        ref_types = self.ref_types
        ref_types.extend(self.single_refs)
        # ugh this needs to be more pythonic. it's 1:30am and I'm tired
        for refname in ref_types:
            if refname in extradata:
                for reftext in extradata[refname]:
                    if reftext:
                        # add the vuln_ref
                        ref_id = self.db.t_vuln_refs.update_or_insert(
                            f_text=reftext,
                            f_source=refname.upper(),
                        )
                        if not ref_id:
                            ref_id = self.db(self.db.t_vuln_refs.f_text == reftext).select(
                                cache=(self.cache.ram, 180)
                            ).first().id

                        # link vuln_ref to vulndata
                        self.db.t_vuln_references.update_or_insert(
                            f_vulndata_id=vuln_id,
                            f_vuln_ref_id=ref_id
                        )

        return

    def parse(self, rpt_item):
        """
        PluginID data is built as the report is processed however we want to
        also be certain to not duplicate existing t_vulndata so a lookup is
        performed with both the pluginID and fname. If none found the record is
        entered into the database and populates the local dict

        Args:
            rpt_item: A ReportItem field (etree._Element or CSV line)

        Returns:
            t_vulndata.id: integer field of db.t_vulndata[id]
            vulndata: A dictionary of fields for t_vulndata
            extradata: A dictionary of extra data fields such as references
        """
        # TODO: Check validity of XML or CSV
        # if not etree.iselement(rpt_item):
        #    log("Invalid plugin data received: %s" % type(rpt_item), logging.ERROR)
        #    return (None, {}, {})

        # extract specific parts of ReportItem
        extradata = {}

        SF_RE = re.compile('Source File: (\w+).nasl')
        if etree.iselement(rpt_item):
            # XML element, parse it as such
            is_xml = True
            extradata['proto'] = rpt_item.get('protocol', 'info')
            extradata['port'] = rpt_item.get('port', 0)
            extradata['svcname'] = rpt_item.findtext('svc_name', '')
            extradata['plugin_output'] = rpt_item.findtext('plugin_output', '')
            extradata['exploit_available'] = rpt_item.findtext('exploit_available', 'false')
            fname = rpt_item.findtext('fname', '')
            pluginID = rpt_item.get('pluginID')
            f_title = rpt_item.findtext('plugin_name', '')
            f_riskscore = rpt_item.get('risk_factor', '')
            f_cvss_score = float(rpt_item.findtext('cvss_base_score', 0.0))
            f_cvss_i_score = float(rpt_item.findtext('cvss_temporal_score', 0.0))
            f_description = rpt_item.findtext('description')
            f_solution = rpt_item.findtext('solution')
            f_dt_published = rpt_item.findtext('plugin_publication_date')
            f_dt_added = rpt_item.findtext('plugin_publication_date')
            f_dt_modified = rpt_item.findtext('plugin_modification_date')
            severity = int(rpt_item.get('severity', 0))
            cvss_vectors = rpt_item.findtext('cvss_vector') # CVSS2#AV:N/AC:M/Au:N/C:P/I:P/A:P
        else:
            # CSV data, parse it as such
            is_xml = False
            extradata['proto'] = rpt_item.get('Protocol', 'info')
            extradata['port'] = rpt_item.get('Port', 0)
            extradata['svcname'] = ''  # TODO: Look this up in etc/services
            extradata['plugin_output'] = rpt_item.get('Plugin Text', rpt_item.get('Plugin Output', ''))
            extradata['exploit_available'] = rpt_item.get('Exploit?', 'false')
            pluginID = rpt_item.get('Plugin', rpt_item.get('Plugin ID'))
            f_title = rpt_item.get('Plugin Name', rpt_item.get('Name', ''))
            f_riskscore = rpt_item.get('Risk Factor', '')
            f_cvss_score = rpt_item.get('CVSS Base Score', rpt_item.get('CVSS', 0.0))
            f_cvss_i_score = rpt_item.get('CVSS Temporal Score', 0.0)
            f_description = rpt_item.get('Description')
            f_solution = rpt_item.get('Solution')
            f_dt_published = rpt_item.get('Plugin Publication Date')
            f_dt_added = rpt_item.get('Plugin Publication Date')
            f_dt_modified = rpt_item.get('Plugin Modification Date')
            severity = rpt_item.get('Severity', 0)
            cvss_vectors = rpt_item.get('CVSS Vector') # AV:N/AC:L/Au:N/C:P/I:P/A:N
            sf_re = SF_RE.search(extradata['plugin_output'])
            if sf_re:
                fname = sf_re.groups()[0]
            else:
                fname = None

            # CSV DictReader sets fields to '' so force float/int if nothing set
            if not f_cvss_score:
                f_cvss_score = 0.0
            if not f_cvss_i_score:
                f_cvss_i_score = 0.0

            # Severity may be not set, set it to zero then
            if not severity:
                severity = 0
            # Severity may also be a word, lets map them to numbers
            severity_map = {
                'Critical': 4,
                'High': 3,
                'Medium': 2,
                'Low': 1,
                'Info': 0,
            }
            if isinstance(severity, str):
                severity = severity_map[severity]

            if not extradata['port']:
                extradata['port'] = 0

            # CSV puts N/A for date fields but we need them to be None or real datetimes...
            if f_dt_published == "N/A":
                f_dt_published = None
            if f_dt_added == "N/A":
                f_dt_added = None
            if f_dt_modified == "N/A":
                f_dt_modified = None

        # set t_vulndata.f_vulnid based on pluginID if no filename is found
        extradata['pluginID'] = pluginID
        if fname:
            fname = fname.rstrip('.nasl')
            f_vulnid = IS_SLUG()("%s-%s" % (fname, pluginID))[0]     # slugify it
        else:
            f_vulnid = pluginID

        # references with multiple values
        for refdata in self.ref_types:
            extradata[refdata] = []
            if is_xml:
                for i in rpt_item.findall(refdata):
                    extradata[refdata].append(i.text)
            else:
                if rpt_item.get(refdata):
                    extradata[refdata].append(rpt_item.get(refdata))

        # single value references
        for refdata in self.single_refs:
            if is_xml:
                extradata[refdata] = [rpt_item.findtext(refdata)]
            else:
                if rpt_item.get(refdata):
                    extradata[refdata] = rpt_item.get(refdata)

        # check local dict, else check t_vulndata
        if pluginID in self.vulns:
            return self.vulns[pluginID][0], self.vulns[pluginID][1], extradata
        else:
            vuln_row = self.db(self.db.t_vulndata.f_vulnid == f_vulnid).select(cache=(self.cache.ram, 180)).first()
            if vuln_row:
                # exists in t_vulndata, return it
                vuln_id = vuln_row.id
                vulndata = vuln_row.as_dict()
                return vuln_id, vulndata, extradata

        # vulnerability-specific data
        vulndata = {
            'f_vulnid': f_vulnid,
            'f_title': f_title,
            'f_riskscore': f_riskscore,
            'f_cvss_score': f_cvss_score,
            'f_cvss_i_score': f_cvss_i_score,
            'f_description': f_description,
            'f_solution': f_solution,
            'f_dt_published': f_dt_published,
            'f_dt_added': f_dt_added,
            'f_dt_modified': f_dt_modified,
            'f_source': 'Nessus',
        }

        # Nessus only has 5 severity levels: 0, 1, 2, 3 and 4 .. We go to 11. Assign 0:0, 1:3, 2:5, 3:8, 4:10
        sevmap = {'0': 0, '1': 3 , '2': 5, '3': 8, '4': 10}
        vulndata['f_severity'] = sevmap[str(severity)]

        if cvss_vectors:
            if cvss_vectors.startswith("CVSS2"):
                cvss_vectors = cvss_vectors[6:]
            vulndata['f_cvss_av'] = cvss_vectors[3]
            vulndata['f_cvss_ac'] = cvss_vectors[8]
            vulndata['f_cvss_au'] = cvss_vectors[13]
            vulndata['f_cvss_c'] = cvss_vectors[17]
            vulndata['f_cvss_i'] = cvss_vectors[21]
            vulndata['f_cvss_a'] = cvss_vectors[25]
        else:
            vulndata['f_cvss_av'] = ''
            vulndata['f_cvss_ac'] = ''
            vulndata['f_cvss_au'] = ''
            vulndata['f_cvss_c'] = ''
            vulndata['f_cvss_i'] = ''
            vulndata['f_cvss_a'] = ''

        vuln_id = self.db.t_vulndata.update_or_insert(**vulndata)
        if not vuln_id:
            vuln_id = self.db(self.db.t_vulndata.f_vulnid == f_vulnid).select(cache=(self.cache.ram, 180)).first().id

        if vuln_id:
            self.stats['processed'] += 1
            self.vulns[pluginID] = [vuln_id, vulndata]
            self.db.commit()
            log(" [-] Adding vulnerability to vuln database: %s" % f_vulnid)
            # add/update vulnerability references
            self.db_vuln_refs(vuln_id, vulndata, extradata)
        else:
            log(" [!] Error inserting/finding vulnerability in database: %s" % f_vulnid, logging.ERROR)

        return vuln_id, vulndata, extradata


##-------------------------------------------------------------------------
def process_scanfile(
    filename=None,
    asset_group=None,
    engineer=None,
    msf_settings={},
    ip_ignore_list=None,
    ip_include_list=None,
    update_hosts=False,
    ):
    """
    Process a Nessus XML or CSV Report file. There are two types of CSV output, the first
    is very basic and is generated by a single Nessus instance. The second comes from the
    centralized manager. I forget what it's called but it packs more data. If you have a
    standalone scanner, always export/save as .nessus.

    Args:
        filename: A local filename to process
        asset_group: Asset group to assign hosts to
        engineer: Engineer record number to assign hosts to
        msf_workspace: If set a Metasploit workspace to send the scanfile to via the API
        ip_ignore_list: List of IP addresses to ignore
        ip_include_list: List of IP addresses to ONLY import (skip all others)
        update_hosts: Boolean to update/append to hosts, otherwise hosts are skipped

    Returns:
        msg: A string status message
    """
    from skaldship.cpe import lookup_cpe
    nessus_config = nessus_get_config()

    db = current.globalenv['db']
    cache = current.globalenv['cache']
    settings = current.globalenv['settings']

    # build the hosts only/exclude list
    ip_exclude = []
    if ip_ignore_list:
        ip_exclude = ip_ignore_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals
    ip_only = []
    if ip_include_list:
        ip_only = ip_include_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals

    log(" [*] Processing Nessus scan file %s" % filename)

    fIN = open(filename, "rb")
    # check to see if file is a CSV file, if so set nessus_csv to True
    line = fIN.readline()
    fIN.seek(0)
    if line.startswith('Plugin'):
        import csv
        csv.field_size_limit(sys.maxsize)           # field size must be increased
        nessus_iterator = csv.DictReader(fIN)
        nessus_csv_type = 'Standalone'
        log(" [*] CSV file is from Standalone scanner")
    elif line.startswith('"Plugin"'):
        import csv
        csv.field_size_limit(sys.maxsize)           # field size must be increased
        nessus_iterator = csv.DictReader(fIN)
        nessus_csv_type = 'SecurityCenter'
        log(" [*] CSV file is from SecurityCenter")
    else:
        nessus_csv_type = False
        try:
            nessus_xml = etree.parse(filename)
            log(" [*] XML file identified")
        except etree.ParseError, e:
            msg = " [!] Invalid Nessus scan file (%s): %s " % (filename, e)
            log(msg, logging.ERROR)
            return msg

        root = nessus_xml.getroot()
        nessus_iterator = root.findall("Report/ReportHost")

    nessus_hosts = NessusHosts(engineer, asset_group, ip_include_list, ip_ignore_list, update_hosts)
    nessus_vulns = NessusVulns()
    services = Services()
    svcs = db.t_services

    for host in nessus_iterator:
        if not nessus_csv_type:
            (host_id, hostdata) = nessus_hosts.parse(host.find('HostProperties'))
        else:
            (host_id, hostdata) = nessus_hosts.parse(host)

        if not host_id:
            # no host_id returned, it was either skipped or errored out
            continue

        # Time to parse the plugin data. This is where CSV and XML diverge.
        def _plugin_parse(host_id, vuln_id, vulndata, extradata):
            port = extradata['port']
            proto = extradata['proto']
            svcname = extradata['svcname']
            plugin_output = extradata['plugin_output']
            pluginID = extradata['pluginID']

            svc_id = services.get_id(
                proto=proto, port=port, svcname=svcname, host_id=host_id,
                create_or_update=True
            )

            # create t_service_vulns entry for this pluginID
            svc_vuln = {}
            svc_vuln['f_services_id'] = svc_id
            svc_vuln['f_vulndata_id'] = vuln_id
            svc_vuln['f_proof'] = plugin_output

            # you may be a vulnerability if...
            if extradata['exploit_available'] == 'true':
                # if you have exploits available you may be an extra special vulnerability
                svc_vuln['f_status'] = 'vulnerable-exploited'
            elif svcname == 'general':
                # if general service then you may not be a vulnerability
                svc_vuln['f_status'] = 'general'
            elif vulndata['f_severity'] == 0:
                # if there is no severity then you may not be a vulnerability
                svc_vuln['f_status'] = 'general'
            else:
                # you're a vulnerability
                svc_vuln['f_status'] = 'vulnerable'
            db.t_service_vulns.update_or_insert(**svc_vuln)

            ######################################################################################################
            ## Let the parsing of Nessus Plugin Output commence!
            ##
            ## Many Plugins provide useful data in plugin_output. We'll go through the list here and extract
            ## out the good bits and add them to Kvasir's database. Some Plugins will not be added as vulnerabilities
            ## because they're truly informational. This list will change if somebody keeps it up to date.
            ##
            ## TODO: This should be moved into a separate function so we can also process csv data
            ## TODO: Add t_service_info key/value records (standardize on Nexpose-like keys?)
            ##
            ######################################################################################################
            d = {}

            if pluginID in nessus_config.get('ignored_plugins'):
                return

            nessus_vulns.stats['added'] += 1
            #### SNMP
            if pluginID == '10264':
                # snmp community strings
                for snmp in re.findall(' - (.*)', plugin_output):
                    res = db.t_snmp.update_or_insert(f_hosts_id=host_id, f_community=snmp)
                    db.commit()

            #elif pluginID == '':
            #    continue

            #### SMB/NetBIOS
            if pluginID == '10860':
                # SMB Use Host SID to Enumerate Local Users
                for user in re.findall(' - (.*)', plugin_output):
                    username = user[0:user.find('(')-1]
                    try:
                        gid = re.findall('\(id (\d+)', user)[0]
                    except:
                        gid = '0'

                    # Windows users, local groups, and global groups
                    d['f_username'] = username
                    d['f_gid'] = gid
                    d['f_services_id'] = svc_id
                    d['f_source'] = '10860'
                    db.t_accounts.update_or_insert(**d)
                    db.commit()

            if pluginID == '17651':
                # Microsoft Windows SMB : Obtains the Password Policy
                d['f_hosts_id'] = host_id
                try:
                    d['f_lockout_duration'] = re.findall('Locked account time \(s\): (\d+)', plugin_output)[0]
                    d['f_lockout_limit'] = re.findall(
                        'Number of invalid logon before locked out \(s\): (\d+)', plugin_output
                    )[0]
                except IndexError:
                    d['f_lockout_duration'] = 1800
                    d['f_lockout_limit'] = 0
                db.t_netbios.update_or_insert(**d)
                db.commit()

            if pluginID == '10395':
                # Microsoft Windows SMB Shares Enumeration
                d['f_hosts_id'] = host_id
                d['f_shares'] = re.findall(' - (.*)', plugin_output)
                db.t_netbios.update_or_insert(**d)

            if pluginID == '10150':
                # Windows NetBIOS / SMB Remote Host Information Disclosure
                try:
                    d['f_hosts_id'] = host_id
                    d['f_domain'] = re.findall('(\w+).*= Workgroup / Domain name', plugin_output)[0]
                    db.t_netbios.update_or_insert(**d)
                except IndexError:
                    pass

            #### Banners
            if pluginID == '10092':
                # FTP Server Detection
                RE_10092 = re.compile('The remote FTP banner is :\n\n(.*)', re.DOTALL)
                try:
                    d['f_banner'] = RE_10092.findall(plugin_output)[0]
                    svcs[svc_id].update(**d)
                    db.commit()
                except Exception, e:
                    log("Error parsing FTP banner: %s" % str(e), logging.ERROR)

            if pluginID == '10267':
                # SSH Server Type and Version Information
                try:
                    d['f_banner'] = re.findall('SSH version : (.*)', plugin_output)[0]
                    svcs[svc_id].update(**d)
                    db.commit()
                except Exception, e:
                    log("Error parsing SSH banner: %s" % str(e), logging.ERROR)

            ### Operating Systems and CPE
            if pluginID == '45590':
                # Common Platform Enumeration (CPE)
                for cpe_os in re.findall('(cpe:/o:.*? )', plugin_output):
                    os_id = lookup_cpe(cpe_os.replace('cpe:/o:', '').rstrip(' '))
                    if os_id:
                        db.t_host_os_refs.update_or_insert(
                            f_certainty='0.90',     # just a stab
                            f_family='Unknown',     # not given in Nessus
                            f_class=hostdata.get('system-type'),
                            f_hosts_id=host_id,
                            f_os_id=os_id
                        )
                        db.commit()

        if not nessus_csv_type:
            rpt_items = []

            # Parse the XML <ReportItem> sections where plugins, ports and output are all in
            for rpt_item in host.iterfind('ReportItem'):
                (vuln_id, vulndata, extradata) = nessus_vulns.parse(rpt_item)
                if not vuln_id:
                    # no vulnerability id
                    continue
                _plugin_parse(host_id, vuln_id, vulndata, extradata)
        else:
            (vuln_id, vulndata, extradata) = nessus_vulns.parse(host)
            _plugin_parse(host_id, vuln_id, vulndata, extradata)

    if msf_settings.get('workspace'):
        try:
            # check to see if we have a Metasploit RPC instance configured and talking
            from MetasploitAPI import MetasploitAPI
            msf_api = MetasploitAPI(host=msf_settings.get('url'), apikey=msf_settings.get('key'))
            working_msf_api = msf_api.login()
        except Exception, error:
            log(" [!] Unable to authenticate to MSF API: %s" % str(error), logging.ERROR)
            working_msf_api = False

        try:
            scan_data = open(filename, "r+").readlines()
        except Exception, error:
            log(" [!] Error loading scan data to send to Metasploit: %s" % str(error), logging.ERROR)
            scan_data = None

        if scan_data and working_msf_api:
            task = msf_api.pro_import_data(
                msf_settings.get('workspace'),
                "".join(scan_data),
                {
                    #'preserve_hosts': form.vars.preserve_hosts,
                    'blacklist_hosts': "\n".join(ip_ignore_list)
                },
            )

            msf_workspace_num = session.msf_workspace_num or 'unknown'
            msfurl = os.path.join(msf_settings.get('url'), 'workspaces', msf_workspace_num, 'tasks', task['task_id'])
            log(" [*] Added file to MSF Pro: %s" % msfurl)

    # any new Nessus vulns need to be checked against exploits table and connected
    log(" [*] Connecting exploits to vulns and performing do_host_status")
    connect_exploits()
    do_host_status(asset_group=asset_group)

    msg = (' [*] Import complete: hosts: %s added, %s updated, %s skipped '
           '- %s vulns processed, %s added' % (
            nessus_hosts.stats['added'],
            nessus_hosts.stats['updated'],
            nessus_hosts.stats['skipped'],
            nessus_vulns.stats['processed'],
            nessus_vulns.stats['added']
            ))
    log(msg)
    return msg


##-------------------------------------------------------------------------
def main():
    pass

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = nexpose
# -*- coding: utf-8 -*-

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Nexpose Utilities for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from gluon import current
import time
import re
import HTMLParser
from datetime import datetime
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO
from skaldship.general import html_to_markmin
from skaldship.hosts import do_host_status
from skaldship.exploits import connect_exploits
import logging
from skaldship.log import log

# lxml is now a required library for processing nexpose xml files. This is
# due to the stdlib not supporting XSLT. Oh well! We'll let web2py handle
# the error.
from lxml import etree


##-------------------------------------------------------------------------

def nexpose_get_config():
    """
    Returns a dict of Nexpose configuration settings based on yaml or session
    """

    nexpose_config = current.globalenv['settings']['kvasir_config'].get('nexpose') or {}
    config = {}
    config['ignored_plugins'] = nexpose_config.get('ignored_plugins', [])
    config['host'] = nexpose_config.get('host', '127.0.0.1')
    config['port'] = nexpose_config.get('port', '3780')
    config['user'] = nexpose_config.get('user', 'nxadmin')
    config['password'] = nexpose_config.get('password', 'password')

    return config

##-------------------------------------------------------------------------

def nx_xml_to_html(vulnxml):
    """Transforms a Nexpose <ContainerBlockElement> to HTML using XSLT"""
    import os
    d = os.path.join(current.globalenv['request'].folder, "modules/skaldship/stylesheets/nexpose.xsl")
    vuln_xslt = etree.XML("".join(open(d, "r").readlines()))
    transform = etree.XSLT(vuln_xslt)

    if isinstance(vulnxml, (str, unicode)):
        # convert to stringio for etree parsing
        from StringIO import StringIO
        vulnxml = StringIO(vulnxml)

    vulnxml = etree.parse(vulnxml)
    result = transform(vulnxml)
    vulnhtml = etree.tostring(result, xml_declaration=False, encoding=unicode)

    return clean_html(vulnhtml)


##-------------------------------------------------------------------------

def clean_html(htmldata):
    """Cleans up the HTML using lxml.html clean_html for now."""
    import re
    try:
        from lxml.html.clean import clean_html
    except ImportError:
        log("You don't have lxml installed", logging.ERROR)
        return htmldata

    if htmldata is None:
        return htmldata
    newdata = clean_html(htmldata)
    newdata = newdata.replace('\n', ' ')
    newdata = newdata.replace('<div>', '')
    newdata = newdata.replace('</div>', '')
    newdata = newdata.replace('\t', '')         # tabs? not needed, no never
    newdata = re.sub(' +', ' ', newdata)

    return newdata


##-------------------------------------------------------------------------

def os_to_cpe(os_rec):
    """Takes a Nexpose XML OS field and finds CPE db id. If no CPE db id exists
then a new one is added."""

    if os_rec is None:
        return None

    result = {}
    result['f_vendor'] = None
    result['f_product'] = None
    result['f_version'] = None
    result['f_update'] = None
    result['f_edition'] = None
    result['f_language'] = None
    result['f_title'] = None

    if os_rec.attrib.has_key('title'):
        result['f_title'] = os_rec.attrib['title']
    else:
        # must build a title since one doesn't exist
        """
        title_data = []
        if os_rec.attrib.has_key('f_vendor'):
            title_data.append(os_key.attrib['f_vendor'])
        if os_rec.attrib.has_key('f_product'):
            title_data.append(os_key.attrib['f_product'])
        if os_rec.attrib.has_key('f_version'):
            title_data.append(os_key.attrib['f_version'])
        if os_rec.attrib.has_key('f_update'):
            title_data.append(os_key.attrib['f_update'])
        if os_rec.attrib.has_key('f_edition'):
            title_data.append(os_key.attrib['f_edition'])
        if os_rec.attrib.has_key('f_language'):
            title_data.append(os_key.attrib['f_language'])
        """
        os_keys = os_rec.keys()
        try:
            os_keys.remove('certainty')
        except:
            pass

        try:
            os_keys.remove('device-class')
        except:
            pass

        title_data = []
        for a in os_keys:
            title_data.append(os_rec.attrib[a])

        title_data = " ".join(title_data)
        out = []
        for word in title_data.split():
            if not word in out:
                out.append(word)
        result['f_title'] = " ".join(out)

    if os_rec.attrib.has_key('vendor'): result['f_vendor'] =os_rec.attrib['vendor'].lower()
    if os_rec.attrib.has_key('product'):
        result['f_product'] = os_rec.attrib['product'].lower()

        # this is annoying logic to handle cpe's variations of the product name
        # when nexpose starts putting cpe strings in their XML we can get rid of
        # a lot of this work and lookup the entries directly.
        if result['f_product'] == 'windows nt' or \
           result['f_product'] == 'windows ce' or \
           result['f_product'] == 'ms dos' or \
           result['f_product'] == 'windows 9x':
            result['f_product'] = result['f_product'].replace(" ", "-")
        else:
            result['f_product'] = result['f_product'].replace(" ", "_")
    if os_rec.attrib.has_key('arch'): result['f_edition'] = os_rec.attrib['arch'].lower()
    if os_rec.attrib.has_key('version'):
        result['f_update'] =os_rec.attrib['version'].lower()
        if result['f_vendor'] == "microsoft":
            try:
                result['f_version'], result['f_update']  = result['f_update'].split(" ")[0:1]
            except:
                pass
        if result['f_vendor'] == "cisco" or \
           result['f_vendor'] == "sun":
            result['f_update']  = result['f_version']
            result['f_version'] = ""

    return result


##-------------------------------------------------------------------------

def guess_cpe_os(os_rec):
    """
    A somewhat messy routine that tries to guess the operating system by
    looking through the official CPE dictionary. It's far from perfect
    but does an ok job... I think!
    """

    if os_rec is None:
        return None

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    osinfo = os_to_cpe(os_rec)

    # first look in the t_os table
    query = db.t_os.f_title == osinfo['f_title']
    os_row = db(query).select(cache=(cache.ram, 180)).first()

    if not os_row:
        query = db.t_os.f_vendor == osinfo['f_vendor']
        if osinfo['f_product'] is not '':
            query &= db.t_os.f_product == osinfo['f_product']
        if osinfo['f_version'] is not '':
            query &= db.t_os.f_version == osinfo['f_version']
        if osinfo['f_update'] is not '':
            query &= db.t_os.f_update  == osinfo['f_update']
        if osinfo['f_edition'] is not '':
            query &= db.t_os.f_edition == osinfo['f_edition']
        if osinfo['f_language'] is not '':
            query &= db.t_os.f_language == osinfo['f_language']
        os_row = db(query).select().first()

    if os_row:
        #msg = "Found (%d) OS IDs from customer tables. First one: %s" % (len(os_rows), os_rows[0].f_title)
        os_id = os_row.id

    else:
        # lookup in CPE OS database
        query = (db.t_cpe_os.f_vendor == osinfo['f_vendor'])
        if osinfo['f_product'] is not '':
            query &= (db.t_cpe_os.f_product == osinfo['f_product'])
        if osinfo['f_version'] is not '':
            query &= db.t_cpe_os.f_version == osinfo['f_version']
        if osinfo['f_update'] is not '':
            query &= db.t_cpe_os.f_update == osinfo['f_update']
        if osinfo['f_edition'] is not '':
            query &= db.t_cpe_os.f_edition == osinfo['f_edition']
        if osinfo['f_language'] is not '':
            query &= db.t_cpe_os.f_language == osinfo['f_language']

        os_row = db(query).select().first()

        if os_row:
            osinfo['f_isincpe'] = True
            osinfo['f_cpename'] = os_row.f_cpename
            os_id = db.t_os.insert(**osinfo)
            db.commit()
        else:
            osinfo['f_isincpe'] = False
            os_id = db.t_os.insert(**osinfo)
            db.commit()
    return os_id


##-------------------------------------------------------------------------

def vuln_time_convert(vtime=''):
    """Converts Nexpose timetsamp (YYYYMMDDTHHMMSSUUU) into python datetime"""
    if not vtime:
        tval = datetime(1970, 1, 1)
    else:
        if isinstance(vtime, str):
            if vtime[8] == "T":
                tstr = "%%Y%%m%%dT%%H%%M%%S%s" % vtime[15:]
                tval = time.strptime(vtime, tstr)
            else:
                log("Unknown datetime value: %s" % vtime, logging.ERROR)
        else:
            log("Invalid datetime value provided: %s" % vtime, logging.ERROR)
            tval = datetime(1970, 1, 1)
    return datetime.fromtimestamp(time.mktime(tval))


##-------------------------------------------------------------------------

def vuln_parse(vuln, fromapi=False):
    """Parses Nexpose vulnerability XML"""

    if vuln is None: return False, False

    vulnfields = {
        'f_vulnid': vuln.attrib['id'].lower(),
        'f_title': vuln.attrib['title'],
        'f_severity': vuln.attrib['severity'],
        'f_pci_sev': vuln.attrib['pciSeverity']
    }

    if 'published' in vuln.keys():
        vulnfields['f_dt_published'] = vuln_time_convert(vuln.attrib['published'])

    vulnfields['f_dt_added'] = vuln_time_convert(vuln.attrib['added'])
    vulnfields['f_dt_modified'] = vuln_time_convert(vuln.attrib['modified'])

    if 'cvssScore' in vuln.keys():
        vulnfields['f_cvss_score'] = vuln.attrib['cvssScore']

        cvss_vectors = vuln.attrib['cvssVector'] # cvssVector="(AV:N/AC:M/Au:N/C:P/I:P/A:P)"
        vulnfields['f_cvss_av'] = cvss_vectors[4]
        vulnfields['f_cvss_ac'] = cvss_vectors[9]
        vulnfields['f_cvss_au'] = cvss_vectors[14]
        vulnfields['f_cvss_c'] = cvss_vectors[18]
        vulnfields['f_cvss_i'] = cvss_vectors[22]
        vulnfields['f_cvss_a'] = cvss_vectors[26]

    # parse the first description field, since there can only be one
    d = vuln.find("description")
    if d is not None:
        if fromapi:
            result = etree.tostring(d)
            result = result.replace("<description>", "")
            result = result.replace("</description>", "")
            vulnfields['f_description'] = html_to_markmin(result)
        else:
            d = StringIO(etree.tostring(d))
            vulnfields['f_description'] = html_to_markmin(nx_xml_to_html(d))

    references = []
    for d in vuln.findall("references/reference"):
        references.append([d.attrib['source'], d.text])

    # right now we don't do anything with tags
    #tags = []
    #for d in vuln.findall("tags/tag"):
    #    tags.append(d.text)

    # parse the first solution field, since there can only be one
    d = vuln.find("solution")
    if d is not None:
        if fromapi:
            result = etree.tostring(d)
            result = result.replace("<solution>", "")
            result = result.replace("</solution>", "")
            vulnfields['f_solution'] = html_to_markmin(result)
        else:
            d = StringIO(etree.tostring(d))
            vulnfields['f_solution'] = html_to_markmin(nx_xml_to_html(d))

    vulnfields['f_source'] = 'Nexpose'
    return vulnfields, references


##-------------------------------------------------------------------------

def import_all_vulndata(overwrite=False, nexpose_server={}):
    """
    Uses the NexposeAPI and imports each and every vulnerability to Kvasir. Can take a looooong time.

    Args:
        overwrite: Whether or not to overwrite an existing t_vulndata record

    Returns:
        msg: A string message of status.
    """
    from NexposeAPI import VulnData
    db = current.globalenv['db']

    vuln_class = VulnData()
    vuln_class.host = nexpose_server.get('host', 'localhost')
    vuln_class.port = nexpose_server.get('port', '3780')
    if vuln_class.login(user_id=nexpose_server.get('user'), password=nexpose_server.get('pw')):
        log(" [*] Populating list of Nexpose vulnerability ID summaries")
        try:
            vuln_class.populate_summary()
        except Exception, e:
            log(" [!] Error populating summaries: %s" % str(e), logging.ERROR)
            return False

        try:
            vulnxml = etree.parse(StringIO(vuln_class.vulnxml))
        except Exception, e:
            log(" [!] Error parsing summary XML: %s" % str(e), logging.ERROR)
            return False

        vulns = vulnxml.findall('VulnerabilitySummary')
        log(" [*] %s vulnerabilities to parse" % len(vulns))

        if vuln_class.vulnerabilities > 0:
            existing_vulnids = []
            [existing_vulnids.extend([x['f_vulnid']]) for x in
             db(db.t_vulndata.f_source == "Nexpose").select(db.t_vulndata.f_vulnid).as_list()]

            log(" [*] Found %d vulnerabilities in the database already." % (len(existing_vulnids)))

            stats = {'added': 0, 'updated': 0, 'skipped': 0, 'errors': 0}
            for vuln in vulns:

                if vuln.attrib['id'] in existing_vulnids and not overwrite:
                    # skip over existing entries if we're not overwriting
                    stats['skipped'] += 1
                    continue

                try:
                    vulndetails = vuln_class.detail(vuln.attrib['id'])
                except Exception, e:
                    log(" [!] Error retrieving details for %s: %s" % (vuln.attrib['id'], str(e)), logging.ERROR)
                    stats['errors'] += 1
                    if stats['errors'] == 50:
                        log(" [!] Too many errors, aborting!", logging.ERROR)
                        return False
                    else:
                        continue

                if vulndetails is not None:
                    (vulnfields, references) = vuln_parse(vulndetails.find('Vulnerability'), fromapi=True)
                else:
                    log(" [!] Unable to find %s in Nexpose" % vuln.attrib['id'], logging.WARN)
                    continue

                # add the vulnerability to t_vulndata
                vulnid = db.t_vulndata.update_or_insert(**vulnfields)
                if not vulnid:
                    vulnid = db(db.t_vulndata.f_vulnid == vulnfields['f_vulnid']).select().first().id
                    stats['updated'] += 1
                    log(" [-] Updated %s" % vulnfields['f_vulnid'])
                else:
                    stats['added'] += 1
                    log(" [-] Added %s" % vulnfields['f_vulnid'])
                db.commit()

                # add the references
                if vulnid is not None and references:
                    for reference in references:
                        # check to see if reference exists first
                        query = (db.t_vuln_refs.f_source == reference[0]) & (db.t_vuln_refs.f_text == reference[1])
                        ref_id = db.t_vuln_refs.update_or_insert(query, f_source=reference[0], f_text=reference[1])
                        if not ref_id:
                            ref_id = db(query).select().first().id

                        # make many-to-many relationship with t_vuln_data
                        db.t_vuln_references.update_or_insert(f_vuln_ref_id=ref_id, f_vulndata_id=vulnid)
                        db.commit()

            from skaldship.exploits import connect_exploits
            connect_exploits()
            msg = "%s added, %s updated, %s skipped" % (stats['added'], stats['updated'], stats['skipped'])
            log(" [*] %s" % msg)
        else:
            msg = "No vulndata populated from Nexpose"
            log(" [!] Error: %s" % msg, logging.ERROR)

    else:
        msg = "Unable to communicate with Nexpose"
        log(" [!] Error: %s" % msg, logging.ERROR)

    return msg


##-------------------------------------------------------------------------

def process_exploits(filename=None):
    """
    Process Nexpose exploits.xml file into the database
    """

    log("Processing Nexpose exploits file: %s ..." % filename)

    try:
        exploits = etree.parse(filename)
    except etree.ParseError, e:
        raise Exception("Error processing file: %s" % e)
    except IOError, e:
        raise Exception("Error opening file: %s" % e)

    r = exploits.getroot()
    counter = 0
    from exploits import add_exploit, connect_exploits
    for exploit in r.findall('exploit'):
        #"adobe-unspec-bof-cve-2010-1297","13787","0day Exploit for Adobe Flash and Reader PoC (from the wild)","Description","1","Expert"
        f_name = exploit.findtext('name')
        f_title = exploit.findtext('id')
        f_description = unicode(exploit.findtext('description')).encode('iso-8859-1').decode('cp1252')
        f_description = f_description.replace("\\'", "'").replace('\\x', "0x")
        f_source = exploit.findtext('source')
        f_level = exploit.findtext('rank') or 'Unknown'         # exploiter experience level estimate
        f_rank = exploit.findtext('exploitrank') or 'Unknown'   # rank of the exploit

        # exploit records can have multiple Nexpose vulnerabilitiy identifiers
        f_vulnid = []
        for nex_id in exploit.findall("vulnerabilities/vulnerability"):
            f_vulnid.append(nex_id.get('id').lower())

        res = add_exploit(
            cve=None,
            vuln_ids=f_vulnid,
            f_name=f_name,
            f_title=f_title,
            f_description=f_description,
            f_source=f_source,
            f_level=f_level,
            f_rank=f_rank,
        )
        if res > 0:
            counter += 1
        else:
            log("Error importing exploit: %s" % f_name, logging.ERROR)

    connect_exploits()
    log("%d exploits added/updated" % counter)
    return True


##----------------------------------------------------------------------------

def process_xml(
    filename=None,
    asset_group=None,
    engineer=None,
    msf_settings={},
    ip_ignore_list=None,
    ip_include_list=None,
    update_hosts=False,
    ):
    # Upload and process Nexpose XML Scan file

    from skaldship.cpe import lookup_cpe
    from skaldship.hosts import get_host_record
    from gluon.validators import IS_IPADDRESS
    import os

    db = current.globalenv['db']
    session = current.globalenv['session']

    parser = HTMLParser.HTMLParser()
    user_id = db.auth_user(engineer)

    # build the hosts only/exclude list
    ip_exclude = []
    if ip_ignore_list:
        ip_exclude = ip_ignore_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals
    ip_only = []
    if ip_include_list:
        ip_only = ip_include_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals

    log(" [*] Processing Nexpose scan file %s" % filename)

    try:
        nexpose_xml = etree.parse(filename)
    except etree.ParseError, e:
        msg = " [!] Invalid Nexpose XML file (%s): %s " % (filename, e)
        log(msg, logging.ERROR)
        return msg

    root = nexpose_xml.getroot()

    existing_vulnids = db(db.t_vulndata()).select(db.t_vulndata.id, db.t_vulndata.f_vulnid).as_dict(key='f_vulnid')
    log(" [*] Found %d vulnerabilities in the database already." % len(existing_vulnids))

    # start with the vulnerability details
    vulns_added = 0
    vulns_skipped = 0
    vulns = root.findall("VulnerabilityDefinitions/vulnerability")
    log(" [*] Parsing %d vulnerabilities" % len(vulns))
    for vuln in vulns:

        # nexpose identifiers are always lower case in kvasir. UPPER CASE IS FOR SHOUTING!!!
        vulnid = vuln.attrib['id'].lower()
        if existing_vulnids.has_key(vulnid):
            #log(" [-] Skipping %s - It's in the db already" % vulnid)
            vulns_skipped += 1
        else:
            # add the vulnerability to t_vulndata - any duplicates are errored out
            (vulnfields, references) = vuln_parse(vuln, fromapi=False)
            try:
                vulnid = db.t_vulndata.update_or_insert(**vulnfields)
                if not vulnid:
                    vulnid = db(db.t_vulndata.f_vulnid == vulnfields['f_vulnid']).select().first().id
                vulns_added += 1
                db.commit()
            except Exception, e:
                log(" [!] Error inserting %s to vulndata: %s" % (vulnfields['f_vulnid'], e), logging.ERROR)
                vulnid = None
                db.commit()
                continue

            # add the references
            if vulnid is not None:
                for reference in references:
                    # check to see if reference exists first
                    ref_id = db(db.t_vuln_refs.f_text == reference[1])
                    if ref_id.count() == 0:
                        # add because it doesn't
                        ref_id = db.t_vuln_refs.insert(f_source=reference[0], f_text=reference[1])
                        db.commit()
                    else:
                        # pick the first reference as the ID
                        ref_id = ref_id.select()[0].id

                    # make many-to-many relationship with t_vuln_data
                    res = db.t_vuln_references.insert(f_vuln_ref_id=ref_id, f_vulndata_id=vulnid)
                    db.commit()

    log(" [*] %d Vulnerabilities added, %d skipped" % (vulns_added, vulns_skipped))

    # re-make the existing_vulnids dict() since we've updated the system
    existing_vulnids = db(db.t_vulndata()).select(db.t_vulndata.id, db.t_vulndata.f_vulnid).as_dict(key='f_vulnid')

    # parse the nodes now
    nodes = root.findall("nodes/node")
    log(" [-] Parsing %d nodes" % len(nodes))
    hoststats = {'added': 0, 'skipped': 0, 'updated': 0, 'errored': 0}
    hosts = []   # array of host_id fields
    for node in nodes:
        log(" [-] Node %s status is: %s" % (node.attrib['address'], node.attrib['status']))
        #sys.stderr.write(msg)
        if node.attrib['status'] != "alive":
            hoststats['skipped'] += 1
            continue

        if node.attrib['address'] in ip_exclude:
            log(" [-] Node is in exclude list... skipping")
            hoststats['skipped'] += 1
            continue

        nodefields = {}

        if len(ip_only) > 0 and node.attrib['address'] not in ip_only:
            log(" [-] Node is not in the only list... skipping")
            hoststats['skipped'] += 1
            continue

        # we'll just take the last hostname in the names list since it'll usually be the full dns name
        names = node.findall("names/name")
        for name in names:
            nodefields['f_hostname'] = name.text

        ip = node.attrib['address']

        if IS_IPADDRESS(is_ipv4=True)(ip):
            nodefields['f_ipv4'] = ip
        elif IS_IPADDRESS(is_ipv6=True)(ip):
            nodefields['f_ipv6'] = ip
        else:
            log(" [!] Invalid IP Address: %s" % ip, logging.ERROR)

        nodefields['f_engineer'] = user_id
        nodefields['f_asset_group'] = asset_group
        nodefields['f_confirmed'] = False

        if 'hardware-address' in node.attrib:
            nodefields['f_macaddr'] = node.attrib['hardware-address']
        if node.find('names/name') is not None:
            # XXX: for now just take the first hostname
            nodefields['f_hostname'] = node.find('names/name').text

        # check to see if IP exists in DB already
        query = (db.t_hosts.f_ipv4 == ip) | (db.t_hosts.f_ipv6 == ip)
        host_rec = db(query).select().first()
        if host_rec is None:
            host_id = db.t_hosts.insert(**nodefields)
            db.commit()
            hoststats['added'] += 1
            log(" [-] Adding IP: %s" % ip)
        elif update_hosts:
            db.commit()
            if 'f_ipv4' in nodefields:
                db(db.t_hosts.f_ipv4 == nodefields['f_ipv4']).update(**nodefields)
                db.commit()
                host_id = get_host_record(nodefields['f_ipv4'])
                host_id = host_id.id
                hoststats['updated'] += 1
                log(" [-] Updating IP: %s" % ip)
            else:
                db(db.t_hosts.f_ipv6 == nodefields['f_ipv6']).update(**nodefields)
                db.commit()
                host_id = get_host_record(nodefields['f_ipv6'])
                host_id = host_id.id
                hoststats['updated'] += 1
                log(" [-] Updating IP: %s" % ip)
        else:
            hoststats['skipped'] += 1
            db.commit()
            log(" [-] Skipped IP: %s" % ip)
            continue
        hosts.append(host_id)

        # tests that aren't specific to any port we wrap up into a meta service
        # called "INFO"
        tests = node.findall("tests/test")
        if len(tests) > 0:
            svc_id = db.t_services.update_or_insert(f_proto="info", f_number="0", f_status="info", f_hosts_id=host_id)
            db.commit()

        for test in tests:
            d = {}
            vulnid = test.get('id').lower()

            # we may have valid username.
            if "cifs-acct-" in vulnid:
                username = test.get('key')
                if username is not None:
                    d['f_services_id'] = svc_id
                    d['f_username'] = username
                    d['f_active'] = True
                    d['f_source'] = vulnid
                    query = (db.t_accounts.f_services_id == d['f_services_id']) &\
                            (db.t_accounts.f_username == d['f_username'])
                    db.t_accounts.update_or_insert(query, **d)
                    db.commit()

            if test.attrib['status'] == 'vulnerable-exploited' or \
               test.attrib['status'] == 'potential' or \
               test.attrib['status'] == 'exception-vulnerable-exploited' or \
               test.attrib['status'] == 'exception-vulnerable-version' or \
               test.attrib['status'] == 'exception-vulnerable-potential' or \
               test.attrib['status'] == 'vulnerable-version':
                if vulnid in existing_vulnids:
                    vuln_id = existing_vulnids[vulnid]['id']
                else:
                    continue

                if vulnid == 'cifs-nt-0001':
                    # Windows users, local groups, and global groups
                    infotext = nx_xml_to_html(StringIO(etree.tostring(test, xml_declaration=False)))
                    try:
                        unames = re.search("Found user\(s\): (?P<unames>.+?) </li>", infotext).group('unames')
                    except AttributeError, e:
                        # regex not found
                        continue
                    for uname in unames.split():
                        # add account
                        d['f_username'] = uname
                        d['f_services_id'] = svc_id
                        d['f_source'] = 'cifs-nt-0001'
                        db.t_accounts.update_or_insert(**d)
                        db.commit()

                test_str = etree.tostring(test, xml_declaration=False, encoding=unicode)
                test_str = test_str.encode('ascii', 'xmlcharrefreplace')
                proof = nx_xml_to_html(StringIO(test_str))
                proof = html_to_markmin(proof)

                if vulnid == 'cifs-insecure-acct-lockout-limit':
                    d['f_hosts_id'] = host_id
                    try:
                        d['f_lockout_limit'] = re.search("contains: (?P<l>\d+)", proof).group('l')
                    except AttributeError:
                        d['f_lockout_limit'] = 0
                    query = (db.t_netbios.f_hosts_id == host_id)
                    db.t_netbios.update_or_insert(query, **d)
                    db.commit()

                # Check for CIFS uid/pw
                if "cifs-" in vulnid:
                    try:
                        uid = re.search("uid\[(?P<u>.*?)\]", proof).group('u')
                        pw = re.search("pw\[(?P<p>.*?)\]", proof).group('p')
                        realm = re.search("realm\[(?P<r>.*?)\]", proof).group('r')
                        d = {
                            'f_services_id': svc_id,
                            'f_username': uid,
                            'f_password': pw,
                            'f_description': realm,
                            'f_active': True,
                            'f_compromised': True,
                            'f_source': vulnid
                        }
                        query = (db.t_accounts.f_services_id == svc_id) & (db.t_accounts.f_username == uid)
                        db.t_accounts.update_or_insert(query, **d)
                        db.commit()
                    except AttributeError:
                        db.commit()
                    except Exception, e:
                        log("Error inserting account (%s): %s" % (uid, e), logging.ERROR)
                    db.commit()

                # solaris-kcms-readfile shadow file
                if vulnid.lower() == "rpc-solaris-kcms-readfile":
                    # funky chicken stuff, if they mess with this output then we've got to
                    # change this around as well. thems the breaks, maynard!
                    shadow = parser.unescape(proof)
                    for line in shadow.split("<br />")[1:-1]:
                        user, pw, uid = line.split(':')[0:3]
                        d['f_services_id'] = svc_id
                        d['f_username'] = user
                        d['f_hash1'] = pw
                        d['f_hash1_type'] = "crypt"
                        d['f_uid'] = uid
                        d['f_source'] = "shadow"
                        d['f_active'] = True
                        d['f_source'] = "rpc-solaris-kcms-readfile"
                        query = (db.t_accounts.f_services_id == svc_id) & (db.t_accounts.f_username == user)
                        db.t_accounts.update_or_insert(query, **d)
                        db.commit()

                db.t_service_vulns.update_or_insert(
                    f_services_id=svc_id,
                    f_status=test.attrib['status'],
                    f_proof=proof,
                    f_vulndata_id=vuln_id
                )

                if "cisco-default-http-account" in vulnid.lower():
                    d['f_services_id'] = svc_id
                    d['f_username'] = vulnid.split('-')[4]
                    d['f_password'] = vulnid.split('-')[6]
                    d['f_source'] = "cisco-default-http-account"
                    query = (db.t_accounts.f_services_id == svc_id) & (db.t_accounts.f_username == d['f_username'])
                    db.t_accounts.update_or_insert(query, **d)
                    db.commit()

        # add services (ports) and resulting vulndata
        for endpoint in node.findall("endpoints/endpoint"):
            f_proto = endpoint.attrib['protocol']
            f_number = endpoint.attrib['port']
            f_status = endpoint.attrib['status']

            query = (db.t_services.f_hosts_id == host_id) \
                    & (db.t_services.f_proto == f_proto) \
                    & (db.t_services.f_number == f_number)
            svc_id = db.t_services.update_or_insert(
                query,
                f_proto=f_proto,
                f_number=f_number,
                f_status=f_status,
                f_hosts_id=host_id
            )
            if not svc_id:
                svc_id = db(query).select().first().id

            for service in endpoint.findall("services/service"):
                d = {}
                if 'name' in service.attrib:
                    db.t_services[svc_id] = dict(f_name=service.attrib['name'])

                for test in service.findall("tests/test"):
                    vulnid = test.get('id').lower()

                    if test.attrib['status'] == 'vulnerable-exploited' or \
                       test.attrib['status'] == 'potential' or \
                       test.attrib['status'] == 'exception-vulnerable-exploited' or \
                       test.attrib['status'] == 'exception-vulnerable-version' or \
                       test.attrib['status'] == 'exception-vulnerable-potential' or \
                       test.attrib['status'] == 'vulnerable-version':
                        if vulnid in existing_vulnids:
                            vuln_id = existing_vulnids[vulnid]['id']
                        else:
                            log(" [!] Unknown vulnid, Skipping! (id: %s)" % vulnid, logging.ERROR)
                            continue

                        test_str = etree.tostring(test, xml_declaration=False, encoding=unicode)
                        test_str = test_str.encode('ascii', 'xmlcharrefreplace')
                        proof = nx_xml_to_html(StringIO(test_str))
                        proof = html_to_markmin(proof)

                        # Check for SNMP strings
                        if "snmp-read-" in vulnid:
                            snmpstring = re.search("pw\[(?P<pw>.*?)\]", proof).group('pw')
                            db.t_snmp.update_or_insert(
                                f_hosts_id=host_id,
                                f_community=snmpstring,
                                f_access="READ",
                                f_version="v1"
                            )
                            db.commit()

                        if "snmp-write" in vulnid:
                            snmpstring = re.search("pw\[(?P<pw>.*?)\]", proof).group('pw')
                            db.t_snmp.update_or_insert(
                                f_hosts_id=host_id,
                                f_community=snmpstring,
                                f_access="WRITE",
                                f_version="v1"
                            )
                            db.commit()

                        # TODO: account names

                        # Dell DRAC root/calvin
                        if vulnid == "http-drac-default-login":
                            d['f_services_id'] = svc_id
                            d['f_username'] = 'root'
                            d['f_password'] = 'calvin'
                            d['f_active'] = True
                            d['f_compromised'] = True
                            d['f_source'] = vulnid
                            query = (db.t_accounts.f_services_id == svc_id) & (db.t_accounts.f_username == 'root')
                            db.t_accounts.update_or_insert(query, **d)
                            db.commit()

                        # Check for uid/pw
                        if "ftp-iis-" in vulnid or \
                           "telnet-" in vulnid or \
                           "cifs-" in vulnid or \
                           "tds-" in vulnid or \
                           "oracle-" in vulnid or \
                           "-default-" in vulnid or \
                           "ftp-generic-" in vulnid:
                            try:
                                uid = re.search("uid\[(?P<u>.*?)\]", proof).group('u')
                                pw = re.search("pw\[(?P<p>.*?)\]", proof).group('p')
                                realm = re.search("realm\[(?P<r>.*?)\]", proof).group('r')
                                d['f_services_id'] = svc_id
                                d['f_username'] = uid
                                d['f_password'] = pw
                                d['f_description'] = realm
                                d['f_active'] = True
                                d['f_compromised'] = True
                                d['f_source'] = vulnid
                                query = (db.t_accounts.f_services_id == svc_id) & (db.t_accounts.f_username == uid)
                                db.t_accounts.update_or_insert(query, **d)
                                db.commit()
                            except AttributeError:
                                db.commit()
                            except Exception, e:
                                log("Error inserting account (%s): %s" % (uid, e), logging.ERROR)
                            db.commit()

                        # cisco default http login accounts
                        if "cisco-default-http-account" in vulnid.lower():
                            d['f_services_id'] = svc_id
                            d['f_username'] = vulnid.split('-')[4]
                            d['f_password'] = vulnid.split('-')[6]
                            d['f_source'] = "cisco-default-http-account"
                            query = (db.t_accounts.f_services_id == svc_id) \
                                    & (db.t_accounts.f_username == d['f_username'])
                            db.t_accounts.update_or_insert(query, **d)
                            db.commit()

                        db.t_service_vulns.update_or_insert(
                            f_services_id=svc_id,
                            f_status=test.attrib['status'],
                            f_proof=proof,
                            f_vulndata_id=vuln_id
                        )
                        db.commit()

                for config in service.findall("configuration/config"):
                    db.t_service_info.update_or_insert(
                        f_services_id=svc_id,
                        f_name=config.attrib['name'],
                        f_text=config.text
                    )
                    db.commit()
                    if re.match('\w+.banner$', config.attrib['name']):
                        db.t_services[svc_id] = dict(f_banner=config.text)
                        db.commit()
                    if config.attrib['name'] == 'mac-address':
                        # update the mac address of the host
                        db.t_hosts[host_id] = dict(f_macaddr=config.text)
                        db.commit()
                    if "advertised-name" in config.attrib['name']:
                        # netbios computer name
                        d = config.text.split(" ")[0]
                        if "Computer Name" in config.text:
                            data = {'f_netbios_name': d}
                            # if hostname isn't defined then lowercase netbios name and put it in
                            if db.t_hosts[host_id].f_hostname is None:
                                data['f_hostname'] = d.lower()
                            db(db.t_hosts.id == host_id).update(**data)
                            db.commit()
                        elif "Domain Name" in config.text:
                            query = (db.t_netbios.f_hosts_id == host_id)
                            db.t_netbios.update_or_insert(query, f_hosts_id=host_id, f_domain=d)
                        db.commit()

        for os_rec in node.findall('fingerprints/os'):
            """
            <os  certainty="1.00" device-class="Workstation" vendor="Microsoft" family="Windows" product="Windows 2000 Professional" version="SP4" arch="x86"/>

            if using SCAP output the os line looks like:

            <os  certainty="0.66" device-class="General" vendor="Microsoft" family="Windows" product="Windows XP" arch="x86" cpe="cpe:/o:microsoft:windows_xp::sp3"/>
            """

            if os_rec.attrib.has_key('cpe'):
                # we have a cpe entry from xml! hooray!
                cpe_name = os_rec.attrib['cpe'].replace('cpe:/o:', '')
                os_id = lookup_cpe(cpe_name)
            else:
                # no cpe attribute in xml, go through our messy lookup
                os_id = guess_cpe_os(os_rec)

            if os_id is not None:
                db.t_host_os_refs.update_or_insert(
                    f_certainty=os_rec.attrib['certainty'],
                    f_family=os_rec.get('family', 'Unknown'),
                    f_class=os_rec.get('device-class', 'Other'),
                    f_hosts_id=host_id,
                    f_os_id=os_id
                )
                db.commit()
            else:
                log(" [!] os_rec could not be parsed: %s" % etree.tostring(os_rec), logging.ERROR)

        db.commit()

    if msf_settings.get('workspace'):
        try:
            # check to see if we have a Metasploit RPC instance configured and talking
            from MetasploitAPI import MetasploitAPI
            msf_api = MetasploitAPI(host=msf_settings.get('url'), apikey=msf_settings.get('key'))
            working_msf_api = msf_api.login()
        except Exception, error:
            log(" [!] Unable to authenticate to MSF API: %s" % str(error), logging.ERROR)
            working_msf_api = False

        try:
            scan_data = open(filename, "r+").readlines()
        except Exception, error:
            log(" [!] Error loading scan data to send to Metasploit: %s" % str(error), logging.ERROR)
            scan_data = None

        if scan_data and working_msf_api:
            task = msf_api.pro_import_data(
                msf_settings.get('workspace'),
                "".join(scan_data),
                {
                    #'preserve_hosts': form.vars.preserve_hosts,
                    'blacklist_hosts': "\n".join(ip_ignore_list)
                },
            )

            msf_workspace_num = session.msf_workspace_num or 'unknown'
            msfurl = os.path.join(msf_settings.get('url'), 'workspaces', msf_workspace_num, 'tasks', task['task_id'])
            log(" [*] Added file to MSF Pro: %s" % msfurl)

    # any new nexpose vulns need to be checked against exploits table and connected
    log(" [*] Connecting exploits to vulns and performing do_host_status")
    connect_exploits()
    do_host_status(asset_group=asset_group)

    msg = " [*] Import complete: hosts: %s added, %s skipped, %s errors - vulns: %s added, %s skipped" % (
        hoststats['added'],
        hoststats['skipped'],
        hoststats['errored'],
        vulns_added, vulns_skipped
    )
    log(msg)
    return msg

########NEW FILE########
__FILENAME__ = nmap
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Nmap Utilities for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""
from gluon import current
from skaldship.log import log
import logging

db = current.globalenv['db']
cache = current.globalenv['cache']
settings = current.globalenv['settings']
session = current.globalenv['session']


##-------------------------------------------------------------------------

def script_metadata():
    """
    Load nmap script metadata into a dictionary
    """
    try:
        from zenmapCore_Kvasir.ScriptMetadata import get_script_entries
    except ImportError, e:
        return dict(error="Cannot load zenmap python library: %s" % (e))

    scr_mdata = get_script_entries(settings.nmap_scriptdir, settings.nmap_nselibdir)
    scripts = {}
    for scr in scr_mdata:
        scripts[scr.filename] = {
            'usage': scr.usage,
            'description': scr.description,
            'arguments': scr.arguments,
            'categories': scr.categories,
            'author': scr.author,
            'output': scr.output,
            'url': scr.url,
        }
    return scripts


##-------------------------------------------------------------------------
def process_xml(
    filename=None,
    addnoports=False,
    asset_group=None,
    engineer=None,
    msf_settings={},
    ip_ignore_list=None,
    ip_include_list=None,
    update_hosts=False,
    ):
    # Upload and process Nmap XML Scan file
    import re
    import os
    from skaldship.hosts import get_host_record, do_host_status
    from skaldship.cpe import lookup_cpe
    from zenmapCore_Kvasir.NmapParser import NmapParser

    # output regexes
    RE_NETBIOS_NAME = re.compile('NetBIOS computer name: (?P<d>.*),')
    RE_NETBIOS_WORKGROUP = re.compile('Workgroup: (?P<d>.*),')
    RE_NETBIOS_MAC = re.compile('NetBIOS MAC: (?P<d>([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2}))')

    # build the hosts only/exclude list
    ip_exclude = []
    if ip_ignore_list:
        ip_exclude = ip_ignore_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals
    ip_only = []
    if ip_include_list:
        ip_only = ip_include_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals

    log(" [*] Processing Nmap scan file %s" % (filename))

    nmap_parsed = NmapParser()
    nmap_parsed.parse_file(filename)

    #existing_vulnids = db(db.t_vulndata()).select(db.t_vulndata.id, db.t_vulndata.f_vulnid).as_dict(key='f_vulnid')

    # parse the hosts, where all the goodies are
    log(" [-] Parsing %d hosts" % (len(nmap_parsed.hosts)))
    hoststats = {}
    hoststats['added'] = 0
    hoststats['skipped'] = 0
    hoststats['updated'] = 0
    hoststats['errored'] = 0
    hosts = []   # array of host_id fields

    svc_db = db.t_services
    for node in nmap_parsed.hosts:
        nodefields = {}

        if node.ipv6:
            ipaddr = node.ipv6
            nodefields['f_ipv4'] = ipaddr
        elif node.ip.get('type') == 'ipv4':
            ipaddr = node.ip.get('addr')
            nodefields['f_ipv4'] = ipaddr
        else:
            log(" [!] No IPv4/IPv6 address, skipping")
            continue

        try:
            nodefields['f_macaddr'] = node.mac['addr']
        except TypeError:
            nodefields['f_macaddr'] = None

        status = node.state

        log(" [-] Host %s status is: %s" % (ipaddr, status))
        if status != "up":
            hoststats['skipped'] += 1
            continue

        if ipaddr in ip_exclude:
            log(" [-] Host is in exclude list... skipping")
            hoststats['skipped'] += 1
            continue

        if len(ip_only) > 0 and ipaddr not in ip_only:
            log(" [-] Host is not in the only list... skipping")
            hoststats['skipped'] += 1
            continue

        if not node.ports and not addnoports:
            log(" [-] No ports open and not asked to add those kind... skipping")
            hoststats['skipped'] += 1
            continue

        # we'lll just take the last hostname in the names list since it'll usually be the full dns name
        for name in node.hostnames:
            nodefields['f_hostname'] = name['hostname']

        nodefields['f_engineer'] = engineer
        nodefields['f_asset_group'] = asset_group
        nodefields['f_confirmed'] = False

        # see if host exists, if so update. if not, insert!
        query = (db.t_hosts.f_ipv4 == ipaddr) | (db.t_hosts.f_ipv6 == ipaddr)
        host_rec = db(query).select().first()

        if host_rec is None:
            host_id = db.t_hosts.insert(**nodefields)
            db.commit()
            hoststats['added'] += 1
            log(" [-] Adding %s" % (ipaddr))
        elif host_rec is not None and update_hosts:
            db.commit()
            if 'f_ipv4' in nodefields:
                host_id = db(db.t_hosts.f_ipv4 == nodefields['f_ipv4']).update(**nodefields)
            else:
                host_id = db(db.t_hosts.f_ipv6 == nodefields['f_ipv6']).update(**nodefields)
            db.commit()
            host_id = get_host_record(ipaddr)
            host_id = host_id.id
            hoststats['updated'] += 1
            log(" [-] Updating %s" % (ipaddr))
        else:
            hoststats['skipped'] += 1
            db.commit()
            log(" [-] Skipped %s" % (ipaddr))
            continue
        hosts.append(host_id)

        # process OS related info
        for os in node.osmatches:
            os_id = None
            host_id = None
            f_title = os['name'] #title
            for k in os['osclasses']:
                if k.get('cpe') != None:
                    f_cpename= k['cpe'].replace('cpe:/o:', '')
                f_vendor = k['vendor']
                f_product = k['osfamily']
                f_version = k['osgen']
                f_class = k['type']
                f_family = k['osfamily']
                f_certainty= k['accuracy']

                cpe_res = db((db.t_os.f_cpename == f_cpename)&(db.t_os.f_title == f_title)).select().first()

                if cpe_res is not None:
                    os_id = cpe_res.id

                else:
                    try:
                        os_id = db.t_os.insert(
                        f_cpename = f_cpename,
                        f_title = f_title,
                        f_vendor = f_vendor,
                        f_product = f_product,
                        f_version = f_version,
                        )
                    except Exception, e:
                        logger.error("Error inserting OS: %s" % (e))

                    db.commit()

                if os_id and (f_class or f_family or f_certainty):
                    ipaddr = node.ip.get('addr')
                    host_id = get_host_record(ipaddr)
                    host_id = host_id.id
                    try:
                        db.t_host_os_refs.insert(f_certainty = f_certainty,
                                                 f_family = f_family,
                                                 f_class = f_class,
                                                 f_hosts_id = host_id,
                                                 f_os_id = os_id)
                    except Exception, e:
                        logger.error("Error inserting OS: %s" % (e))
                    db.commit()

        # process non-port <hostscript> entries. Add to info/0:
        for hostscripts in node.hostscripts:
            query = (svc_db.f_proto == 'info') & (svc_db.f_number == 0) & (svc_db.f_hosts_id == host_id)
            svc_id = db.t_services.update_or_insert(query, f_proto='info', f_number=0, f_status='open', f_hosts_id=host_id)
            if not svc_id:
                svc_rec = db(query).select(cache=(cache.ram, 180)).first()
                if svc_rec:
                    svc_id = svc_rec.id
                else:
                    log(" [!] Service record wasn't created", logging.ERROR)
                    continue

            db.commit()
            for script in hostscripts:
                script_id = script.id
                output = script.output
                db.t_service_info.update_or_insert(f_services_id=svc_id, f_name=script_id, f_text=output)
                db.commit()

                if script_id == 'nbstat':
                    # pull out NetBIOS info from nbstat output
                    result = RE_NETBIOS_MAC.search(output)
                    if 'd' in result.groupdict():
                        host_rec.update(f_macaddr=result.group('d'))
                        db.commit()
                    result = RE_NETBIOS_NAME.search(output)
                    if 'd' in result.groupdict():
                        host_rec.update(f_netbios_name=result.group('d'))
                        db.commit()
                    result = RE_NETBIOS_WORKGROUP.search(output)
                    if 'd' in result.groupdict():
                        db(db.t_netbios.update_or_insert(f_hosts_id=host_id, f_domain=result.group('d')))
                        db.commit()

        # add ports and resulting vulndata
        for port in node.ports:
            f_proto = port.get('protocol')
            f_number = port.get('portid')
            f_status = port.get('port_state')
            f_name = port.get('service_name')
            f_product = port.get('service_product')

            log(" [-] Adding port: %s/%s (%s)" % (f_proto, f_number, f_name))
            svc_id = db.t_services.update_or_insert(f_proto=f_proto, f_number=f_number, f_status=f_status, f_hosts_id=host_id, f_name=f_name)

            # if record exists, update returns None so look up the record:
            if svc_id is None:
                svc_id = db((db.t_services.f_hosts_id == host_id) &
                           (db.t_services.f_proto == f_proto) &
                           (db.t_services.f_number == f_number)).select('id').first()
                svc_id = svc_id.id

            if f_product:
                version = port.get('service_version')
                if version:
                    f_product += " (%s)" % (version)
                db.t_service_info.update_or_insert(f_services_id=svc_id, f_name=f_name, f_text=f_product)
                db.commit()

            # Process <script> service entries
            for script in port.get('scripts'):
                try:
                    db.t_service_info.update_or_insert(f_services_id=svc_id, f_name=script.get('id'), f_text=script.get('output'))
                except Exception, e:
                    logger.error("Error inserting Script: %s" % (e))
                db.commit()
                # check for banner id and update t_services banner field with the output
                if script.get('id') == "banner":
                    try:
                        db.t_services.update_or_insert((db.t_services.id == svc_id), f_banner = script.get('output'))
                    except Exception, e:
                        logger.error("Error inserting Banner: %s" % (e))
                    db.commit()


            # Process <cpe> service entries
            port_cpe = port.get('service_cpe')
            if port_cpe:
                cpe_id = port_cpe.replace('cpe:/', '')

                if cpe_id.startswith('a'):
                    # process CPE Applications
                    #log(" [-] Found Application CPE data: %s" % (cpe_id))
                    db.t_service_info.update_or_insert(f_services_id=svc_id, f_name='cpe.app', f_text="cpe:/%s" % (cpe_id))
                    db.commit()

                elif cpe_id.startswith('o'):
                    # process CPE Operating System

                    os_id = lookup_cpe(cpe_id[2:])

                    if os_id is not None:
                        db.t_host_os_refs.insert(f_certainty='0.9',
                                                 f_family='Unknown',
                                                 f_class='Other',
                                                 f_hosts_id=host_id,
                                                 f_os_id=os_id)
                        db.commit()
                    else:
                        # So no CPE or existing OS data, lets split up the CPE data and make our own
                        log(" [!] No os_id found, this is odd !!!")

    if msf_settings.get('workspace'):
        try:
            # check to see if we have a Metasploit RPC instance configured and talking
            from MetasploitAPI import MetasploitAPI
            msf_api = MetasploitAPI(host=msf_settings.get('url'), apikey=msf_settings.get('key'))
            working_msf_api = msf_api.login()
        except Exception, error:
            log(" [!] Unable to authenticate to MSF API: %s" % str(error), logging.ERROR)
            working_msf_api = False

        try:
            scan_data = open(filename, "r+").readlines()
        except Exception, error:
            log(" [!] Error loading scan data to send to Metasploit: %s" % str(error), logging.ERROR)
            scan_data = None

        if scan_data and working_msf_api:
            task = msf_api.pro_import_data(
                msf_settings.get('workspace'),
                "".join(scan_data),
                {
                    #'preserve_hosts': form.vars.preserve_hosts,
                    'blacklist_hosts': "\n".join(ip_ignore_list)
                },
            )

            msf_workspace_num = session.msf_workspace_num or 'unknown'
            msfurl = os.path.join(msf_settings.get('url'), 'workspaces', msf_workspace_num, 'tasks', task['task_id'])
            log(" [*] Added file to MSF Pro: %s" % msfurl)

    # any new nexpose vulns need to be checked against exploits table and connected
    log(" [*] Connecting exploits to vulns and performing do_host_status")
    do_host_status(asset_group=asset_group)

    log(" [*] Import complete: hosts: %s added, %s skipped" % (hoststats['added'],
                                                               hoststats['skipped'],
                                                              ))

##-------------------------------------------------------------------------

def run_scan(
    blacklist=None,
    target_list=None,
    scan_options=None,
    ):
    '''
    Executes nmap scan
    '''
    from zenmapCore_Kvasir.NmapCommand import NmapCommand
    from zenmapCore_Kvasir.NmapOptions import NmapOptions
    from time import sleep

    if scan_options[0] is not 'nmap':
        if 'nmap' in settings:
            scan_options.insert(0, settings.nmap)
        else:
            scan_options.insert(0, 'nmap')

    if target_list:
        data = []
        for ip in target_list:
            data.append(ip.strip(' \t\n\r'))
        target_list = data

    if blacklist:
        data = []
        for ip in blacklist:
            data.append(ip.strip(' \t\n\r'))
        blacklist = [','.join(map(str, data))]
        blacklist.insert(0, "--exclude")

    ops = NmapOptions()
    try:
        ops.parse(scan_options + target_list + blacklist)
    except Exception as e:
        log("[!] %s" % e)

    cmd = NmapCommand(ops.render_string())

    log(" [*] Starting Nmap Scan: %s" % (cmd.command))
    cmd.run_scan()

    try:
        cmd.scan_state()
    except Exception as e:
        log("[!] %s" % e)

    full_output = ""
    while cmd.scan_state():
        sleep(5)
        result = cmd.get_output()
        start = len(full_output) - len(result)
        output = result[start:]
        full_output = "%s%s" % (full_output, output)
        log(output)


    log(" [*] Nmap Scan Complete")

    filename = cmd.get_xml_output_filename()
    return filename



########NEW FILE########
__FILENAME__ = passwords
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## SPA password file processing module
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from collections import defaultdict
import re, string
from gluon import current
import logging
logger = logging.getLogger("web2py.app.kvasir")

# Module definitions
lowercase = set(string.lowercase)
uppercase = set(string.uppercase)
digits = set(string.digits)
specialchars = set("!@#$%^&*()_+-=`~[]\\{}|;':\",./<>? ")

has_lower = lambda s: bool( lowercase & set(s))
has_upper = lambda s: bool( uppercase & set(s))
has_digit = lambda s: bool( digits & set(s))
has_special = lambda s: bool( specialchars & set(s))

CHARSET_CHECKS = [
    ('lower', "Lowercase Alpha Characters (ex: password)",
        lambda pw: re.search(r"^[a-z]+$", pw)),
    ('digits', "Digits only (ex: 123456)",
        lambda pw: re.search(r"^[0-9]+$", pw)),
    ('upper', "Upppercase Alpha Characters (ex: PASSWORD)",
        lambda pw: re.search(r"^[A-Z]+$", pw)),
    ('nonalphanum', "Non-Alphanumeric Characters (ex: #$%^@!)",
        lambda pw: re.search(r"^[^a-zA-Z0-9]+$", pw)),
    ('lowerupperalpha', "Upper and Lowercase Alpha Characters (ex: PASSword)",
        lambda pw: (has_lower(pw) and has_upper(pw) and
                    not (has_digit(pw) or has_special(pw))) ),
    ('upperalphanum', "Uppercase Alphanumeric Characters (ex: PASS123)",
        lambda pw: (has_digit(pw) and has_upper(pw) and
                    not (has_lower(pw) or has_special(pw))) ),
    ('loweralphanum', "Lowercase Alphanumeric Characters (ex: pass123)",
        lambda pw: (has_digit(pw) and has_lower(pw) and
                    not (has_upper(pw) or has_special(pw))) ),
    ('lowerspecial', "Lowercase Alphanumeric with Special Chars (ex: pass!23)",
        lambda pw: (has_digit(pw) and not has_upper(pw) and has_lower(pw) and
                    has_special(pw)) ),
    ('upperspecial', "Uppercase Alphanumeric with Special Chars (ex: PASS!23)",
        lambda pw: (has_digit(pw) and has_upper(pw) and not has_lower(pw) and
                    has_special(pw)) ),
    ('lowerspecialonly', "Lowercase with Special Chars Only (ex: pass!@#)",
        lambda pw: (not has_digit(pw) and not has_upper(pw) and has_lower(pw) and
                    has_special(pw)) ),
    ('upperspecialonly', "Uppercase with Special Chars Only (ex: PASS!@#)",
        lambda pw: (not has_digit(pw) and has_upper(pw) and not has_lower(pw) and
                    has_special(pw)) ),
    ('uprlwralphanospecial', "Alphanumeric no Special Chars (ex: Pass123)",
        lambda pw: (has_lower(pw) and has_digit(pw) and has_upper(pw) and
                    not has_special(pw)) ),
    ('complexlwralphanodigit', "LowerAlpha + SpecialChars no digits (ex: Pass!23)",
        lambda pw: (has_lower(pw) and has_special(pw) and
                    not has_digit(pw) and not has_upper(pw)) ),
    ('complexuprlwralphanodigit', "UpperLowerAlpha + SpecialChars no digits (ex: PASS!@#)",
        lambda pw: (has_upper(pw) and has_lower(pw) and has_special(pw) and
                    not has_digit(pw)) ),
    ('complex', "Alphanumeric with Special Chars (ex: Pass!@#)",
        lambda pw: (has_digit(pw) and has_upper(pw) and has_lower(pw) and
                    has_special(pw)) ),
    ('other', "Something not matching", lambda pw: True),
]


##-------------------------------------------------------------------------

def password_class_stat(passwords):
    """Scans through the password, determining which character class
    it belongs to
    """
    for pw_rec in passwords:
        password = pw_rec.f_password
        if not password or password.lower() == "no password":
            pwlenstat = "blank"
        else:
            pwlenstat = len(password)
        for slug, text, fn in CHARSET_CHECKS:
            if password and fn(password):
                character_class = text
                break
        yield (pwlenstat, character_class, password)

##-------------------------------------------------------------------------

def lookup_hash(hash_data=None):
    """
    Looks up a hash in the database
    """
    if hash_data is None:
        return None

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    if len(hash_data) == 65 and hash_data[32] == ":":
        if hash_data.upper() == "AAD3B435B51404EEAAD3B435B51404EE:31D6CFE0D16AE931B73C59D7E0C089C0":
            # it's a blank password
            return ''

        # lm:nt combo.. split them out and lookup NTLM first
        # then if not found search the LM
        (lm, nt) = hash_data.split(':')

        query = ((db.t_accounts.f_hash2 == nt) | (db.t_accounts.f_hash2 == nt.upper())) & (db.t_accounts.f_password != None)
        row = db(query).select(db.t_accounts.f_password).first()
        if row is not None:
            # found an NTLM, return that otherwise build a LM lookup query
            return row.f_password
        if lm.upper() == "AAD3B435B51404EEAAD3B435B51404EE":
            # no ntlm found and lm is blank so it's unknown, return None
            return None
        query = ((db.t_accounts.f_hash1 == lm) | (db.t_accounts.f_hash1 == lm.upper())) & (db.t_accounts.f_password != None)
    else:
        query = ((db.t_accounts.f_hash1 == hash_data) | (db.t_accounts.f_hash1 == hash_data.upper())) & (db.t_accounts.f_password != None)

    row = db(query).select(db.t_accounts.f_password).first()
    if row is not None:
        return row.f_password

    return None

##-------------------------------------------------------------------------

def process_medusa(line):
    """
    Process a medusa line and return a dictionary:

    { 'ip'  : ip address
      'port': port info - can be port # or module name
      'user': username,
      'pass': password,
      'hash': ntlm hash if smbnt hash used
      'msg' : status message
    }
    """
    retval = {}

    try:
        data = line.split()
    except Exception, e:
        logger.error("Error processing medusa line: %s -- %s" % (e, line))
        return retval

    if " ".join(data[:2]) == "ACCOUNT FOUND:":
        retval['port'] = data[2]
        retval['ip'] = data[4]
        retval['user'] = data[6]

        if retval['user'] == "Password:":
            # no username provided, adjust the field modulator cap'n
            retval['user'] = None
            retval['pass'] = data[7]
            retavl['msg'] = " ".join(data[9:])
        elif data[7] == "Password:" and data[8][0] == '[':
            # so this will break if the password starts with "[" however there's no
            # better way I can think of to detect a blank password w/o regex. Other
            # ideas are welcome! data[8] will be the response message which could be
            # [SUCCESS] or [ERROR ...] or something else..
            retval['pass'] = ""
            retval['msg'] = " ".join(data[8:])
        else:
            # Standard password and message location
            retval['pass'] = data[8]
            retval['msg'] = " ".join(data[9:])

        if len(retval['pass']) == 68 and retval['pass'][65:68] == ":::":
            # we have an ntlm hash cap'n
            retval['hash'] = ":".join(retval['pass'].split(':')[:2])
            retval['pass'] = lookup_hash(retval['hash'])
            retval['type'] = 'smb'
        else:
            retval['type'] = 'cleartext'

        if retval['msg'].startswith("[SUCCESS"):
            retval['error'] = False
        else:
            retval['error'] = True

    return retval

##-------------------------------------------------------------------------

def process_hydra(line):
    """
    Process a hydra line and return a dictionary:

    { 'ip'  : ip address
      'port': port info - can be port # or module name
      'user': username,
      'pass': password,
      'hash': ntlm hash if smbnt hash used
      'msg' : status message
    }
    """
    # line: [22][ssh] host: 1.1.1.1   login: username   password: pw1234
    retval = {}
    try:
        data = line.split()
    except Exception, e:
        logger.error("Error processing hydra line: %s -- %s" % (e, line))
        return retval

    if data[1] == "host:":
        # these fields are always there.. sometimes password is not
        retval['port'] = data[0][1:data[0].find("]")]
        retval['ip'] = data[2]
        retval['user'] = data[4]

        if "password:" not in data:
            # no password provided, adjust the field modulator cap'n
            retval['pass'] = None
            if len(data) == 6:
                retval['msg'] = data[5]
        else:
            retval['pass'] = data[6]
            if len(data) == 8:
                retval['msg'] = data[7]

        # handle specific SMB errors:
        #if "[smb]" in data and "Error:" in data:

        if len(retval['pass']) == 68 and retval['pass'][65:68] == ":::":
            # we have an ntlm hash cap'n
            retval['hash'] = ":".join(retval['pass'].split(':')[:2])
            retval['pass'] = lookup_hash(retval['hash'])
            retval['type'] = 'smb'
        else:
            retval['type'] = 'cleartext'
            retval['hash'] = None

    retval['error'] = False
    return retval

##-------------------------------------------------------------------------

def process_msfcsv(line):
    """
    Process a metasploit creds output csv file and return a dictionary:
    { 'ip'  : ip address
      'port': port info - can be port # or module name
      'user': username,
      'pass': password,
      'hash': ntlm hash if smb_hash found
      'msg' : status message
    }
    """
    # host,port,user,pass,type,active?
    retval = {}
    hash_types = ['smb', 'rakp_hmac_sha1_hash', 'smb_challenge']
    import csv
    for data in csv.reader([line]):
        retval['ip'] = data[0]
        retval['port'] = data[1]
        retval['user'] = data[2]
        retval['pass'] = data[3]
        retval['type'] = data[4]
        retval['msg'] = 'from metasploit'
        # isactive = data[5] # unused

    #if len(retval['pass']) == 65 and retval['pass'].find(':') == 32:
    if retval['type'] in hash_types:
        # we have a hash cap'n
        retval['hash'] = retval['pass']
        retval['pass'] = lookup_hash(retval['hash'])
    else:
        retval['hash'] = None

    retval['error'] = False
    return retval

##-------------------------------------------------------------------------

def get_hashtype(pwhash):
    """
    Tries to figure out the type of hash
    """
    # man 3 crypt
    if pwhash[0:3] == "$1$":
        return "MD5"
    if pwhash[0:3] == "$2$" or pwhash[0:4] == "$2a$":
        return "Blowfish"
    if pwhash[0:3] == "$5$":
        return "SHA-256"
    if pwhash[0:3] == "$6$":
        return "SHA-512"
    else:
        return "DES"

##-------------------------------------------------------------------------

def crack_nt_hashes(hashes=[]):
    """
    Take a list of lm/nt hashes and run them through /opt/SPA/tools/jtr
    for simple wordlist, variation and the like. Should take less than
    2 minutes to process.
    TODO: crack_nt_hashes
    """

    return

##-------------------------------------------------------------------------

def insert_or_update_acct(svc_id=None, accounts={}):
    """
    Insert or updates account table in the database
    """

    if svc_id is None:
        return 'No Service ID record sent'

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    resp_text = "No accounts sent to update"
    accounts_added = []
    accounts_updated = []
    if db(db.t_services.id == svc_id).count() > 0:
        # we have a valid service, lets add/modify accounts!
        for username,acct_values in accounts.iteritems():
            # run through each account, if it already exists then check
            # to see if f_hash1 has value then don't overwrite it,
            # otherwise update the other fields
            acct_values['f_services_id'] = svc_id
            acct_values['f_username'] = username.strip('\r\n')
            if acct_values.get('f_password') is None:
                # no password has been set, lets search for the hash and copy the
                # cleartext over if it exists
                if acct_values.get('f_hash1_type') == "LM":
                    acct_values['f_password'] = lookup_hash("%s:%s" % (acct_values.get('f_hash1'), acct_values.get('f_hash2')))
                else:
                    acct_values['f_password'] = lookup_hash(acct_values.get('f_hash1'))

            q = (db.t_accounts.f_username == username) & (db.t_accounts.f_services_id == svc_id)
            acct_rec = db(q).select()
            if len(acct_rec) == 0:
                # add the record
                db.t_accounts.insert(**acct_values)
                accounts_added.append(username)
            else:
                # for existing records
                for rec in acct_rec:
                    if rec.f_hash1 is None or rec.f_hash1 == '':
                        # f_hash1 doesn't exist so update the record
                        db.t_accounts[rec.id] = acct_values
                        resp_text += "Updated %s<br/>" % (username)
                    elif rec.f_hash1 == acct_values['f_hash1']:
                        # f_hash1 exists and is the same so update everything else
                        acct_values.pop('f_hash1')
                        acct_values.pop('f_hash1_type')
                        if acct_values.has_key('f_hash2'):
                            # check for f_hash2
                            if rec.f_hash2 == acct_values['f_hash2']:
                                acct_values.pop('f_hash2')
                                acct_values.pop('f_hash2_type')
                        db.t_accounts[rec.id] = acct_values
                        accounts_updated.append(username)
                    else:
                        logger.error("%s has a different hash1 value. Nothing done. (orig: %s) (pwfile: %s)" % (username, rec.f_hash1, acct_values['f_hash1']))
                db.commit()
            resp_text = "Accounts added: %s\nAccounts Updated: %s\n" % (" ".join(accounts_added), " ".join(accounts_updated))
    else:
        resp_text = 'Invalid Service ID sent'

    logging.debug(resp_text)
    return resp_text

##-------------------------------------------------------------------------

def process_cracked_file(pw_file=None, file_type=None, message=""):
    """
    Process a file of cracked passwords and update the cleartext with
    the new results.
    """
    import fileinput

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    if pw_file is not None:
        try:
            fIN = fileinput.input(files=pw_file)
        except IOError, e:
            logger.error("Error opening %s: %s" % (pw_file, e))
            return "Error opening %s: %s" % (pw_file, e)

    accounts = {}
    if file_type == "JTR PWDUMP":
        logger.info("Processing JTR PWDUMP Result file...")
        for line in fIN:
            if line == "\n": continue
            if line.count(":") != 6: continue
            try:
                (username, password, lm, nt) = line.split(':')[0:4]
                accounts[nt] = password
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "JTR Shadow":
        pass

    elif file_type == "Hash:Password":
        for line in fIN:
            if line == "\n": continue
            if line.count(":") != 1: continue
            try:
                (enchash, cleartext) = line.split(':')[0:1]
                accounts[enchash] = password
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "Password:Hash":
        for line in fIN:
            if line == "\n": continue
            if line.count(":") != 1: continue
            try:
                (cleartext, enchash) = line.split(':')[0:1]
                accounts[enchash] = password
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    else:
        return "Unknown file type sent"

    for k,v in accounts.iteritems():
        query = (db.t_accounts.f_hash1 == k)|(db.t_accounts.f_hash2 == k)
        for row in db(query).select():
            row.update_record(f_password=v, f_compromised=True, f_message=message)
            db.commit()
    return "Password hash/cleartext updated"

##-------------------------------------------------------------------------

def process_password_file(pw_file=None, pw_data=None, file_type=None, source=None):
    """
    Process a password file and return a dictionary fit for t_accounts

    file_type values:

      ('PWDUMP', 'MSCa$h Dump', 'UNIX Passwd', 'UNIX Shadow', 'Medusa', 'Hydra', 'Username:Password', 'AccountDB')
    """
    import fileinput

    accounts = {}
    if pw_file is not None:
        try:
            pw_data = []
            for line in fileinput.input(files=pw_file):
                pw_data.append(line)
        except IOError, e:
            logger.error("Error opening %s: %s" % (pw_file, e))
            return accounts

    if file_type == 'PWDUMP':
        logger.debug("Processing PWDUMP file")
        if source is None:
            source = "PWDUMP"
        for line in pw_data:
            if line == "\n": continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            try:
                (username, uid, lm, nt) = line.split(':')[0:4]
                if uid == "500": level = "ADMIN"
                else: level = "USER"

                accounts[username] = { 'f_uid': uid, 'f_level': level, 'f_source': source,
                                       'f_hash1': lm.strip, 'f_hash1_type': 'LM',
                                       'f_hash2': nt.strip, 'f_hash2_type': 'NTLM' }
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "MSCa$h Dump":
        logger.debug("Processing MSCa$h file")
        if source is None:
            source = "MSCASH"
        for line in pw_data:
            if line == "\n": continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            try:
                (username, pwhash, domain) = line.split(':')
                accounts[username] = { 'f_hash1': pwhash, 'f_hash1_type': 'MSCASH', 'f_domain': domain, 'f_source': source}
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "UNIX Passwd":
        logger.debug("Processing UNIX Passwd file")
        if source is None:
            source == "UNIX Passwd"
        for line in pw_data:
            if line == "\n": continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            try:
                (username, pwhash, uid, gid, fullname) = line.split(':')[0:5]
                if uid == "0": level = "ADMIN"
                else: level = "USER"

                if len(pwhash) > 4:
                    hashtype = get_hashtype(pwhash)
                    accounts[username] = { 'f_uid': uid, 'f_gid': gid, 'f_level': level,
                                           'f_hash1': pwhash, 'f_hash1_type': hashtype,
                                           'f_fullname': fullname, 'f_source': source }
                else:
                    accounts[username] = { 'f_uid': uid, 'f_gid': gid, 'f_level': level,
                                           'f_fullname': fullname, 'f_source': source }

                logging.debug("Account -> %s" % (accounts[username]))
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "UNIX Shadow":
        logger.debug("Processing UNIX Shadow file")
        if source is None:
            source = "UNIX Shadow"
        for line in pw_data:
            if line == "\n": continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            try:
                (username, pwhash, last_changed, min_age, max_age, warning, inactivity, exp_date, reserved) = line.split(':')

                if len(pwhash) > 4:
                    hashtype = get_hashtype(pwhash)
                    accounts[username] = { 'f_hash1': pwhash, 'f_hash1_type': hashtype, 'f_source': source }
                else:
                    accounts[username] = { 'f_source': source }

            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "Username:Password":
        logger.debug("Processing Username:Password file")
        for line in pw_data:
            if line == "\n": continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            try:
                (username, password) = line.split(':')[0:2]
                if source is None:
                    source = "Username:Password"
                accounts[username] = { 'f_password': password.strip("\n"), 'f_source': source, 'f_compromised': True  }
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "Usernames":
        logger.debug("Processing Usernames only file")
        for line in pw_data:
            if line == "\n": continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            accounts[line.strip("\n")] = { 'f_compromised': False }

    elif file_type == "Medusa":
        logger.debug("Processing Medusa output file")
        if source is None:
            source = "Medusa"
        for line in pw_data:
            if line[0] == "#": continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            try:
                pw_data = process_medusa(line)
                # return { 'ip': ip, 'port': port, 'user': user, 'pass': pw, 'hash': ntlm, 'msg': msg }
                if pw_data.get('hash', None):
                    # we have an ntlm hash, split that instead of updating the password
                    (lm, nt) = pw_data['hash'].split(':')[0:2]
                    accounts[pw_data['user']] = { 'f_hash1': lm, 'f_hash1_type': 'LM',
                                                   'f_hash2': nt, 'f_hash2_type': 'NT', 'f_password': pw_data['pass'],
                                                   'f_description': pw_data['msg'], 'f_source': source, 'f_compromised': True }
                else:
                    accounts[pw_data['user']] = { 'f_password': pw_data['pass'],
                                                   'f_message': pw_data['msg'], 'f_source': source, 'f_compromised': True }
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "Hydra":
        logger.debug("Processing Hydra output file")
        if source is None:
            source = "Hydra"
        for line in pw_data:
            if line[0] == "#": continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            try:
                pw_data = process_hydra(line)
                if pw_data.has_key('hash'):
                    # we have an ntlm hash, split that instead of updating the password
                    (lm, nt) = pw_data['hash'].split(':')[0:2]
                    accounts[pw_data['user']] = { 'f_hash1': lm, 'f_hash1_type': 'LM',
                                                   'f_hash2': nt, 'f_hash2_type': 'NT',
                                                   'f_description': pw_data['msg'], 'f_source': source, 'f_compromised': True }
                else:
                    accounts[pw_data['user']] = { 'f_password': pw_data['pass'],
                                                   'f_message': pw_data['msg'], 'f_source': source, 'f_compromised': True }
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "Metasploit Creds CSV":
        logger.debug("Processing Metasploit Creds CSV output file")
        if source is None:
            source = "Metasploit"
        for line in pw_data:
            if line[0] != '"': continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            try:
                pw_data = process_msfcsv(line)
                if pw_data['type'] == 'smb':
                    # we have an ntlm hash, split that instead of updating the password
                    (lm, nt) = pw_data['hash'].split(':')[0:2]
                    accounts[pw_data['user']] = {
                        'f_hash1': lm, 'f_hash1_type': 'LM', 'f_hash2': nt, 'f_hash2_type': 'NT',
                        'f_message': pw_data['msg'], 'f_source': source, 'f_compromised': True
                    }
                else:
                    if pw_data['pass']:
                        compromised = True
                    else:
                        compromised = False
                    accounts[pw_data['user']] = {
                        'f_password': pw_data['pass'], 'f_hash1': pw_data['hash'], 'f_hash1_type': pw_data['type'],
                        'f_message': pw_data['msg'], 'f_source': source, 'f_compromised': compromised,
                    }
            except Exception, e:
                logger.error("Error with line (%s): %s" % (line, e))

    elif file_type == "Username Only":
        logger.debug("Processing Username only output file")
        if source is None:
            source = "Username list"
        for line in pw_data:
            if line[0] == "#": continue
            if line == "\n": continue
            line = line.replace('\n', '')   # remove any and all carriage returns!
            try:
                username = line.split(" ")[0]
                accounts[username] = { 'f_source': source }
            except:
                continue

    elif file_type == "AccountDB":
        logger.debug("Processing AccountDB output file")
        if source is None:
            source = "AccountDB"
        from StringIO import StringIO
        import csv
        for line in csv.reader(StringIO(''.join(pw_data))):
            line = line.replace('\n', '')   # remove any and all carriage returns!
            if len(line) == 10:
                IP, Port, User, Password, uid, gid, level, status, fullname, Comment = line
            else:
                logger.error("Line length != 10, skipping")
                continue
            if status == "DISABLED":
                status=False
            else:
                status=True
            if Password is not "":
                compromised=True
            else:
                compromised=False
            accounts[User] = {
                'f_password': Password,
                'f_uid': uid,
                'f_gid': gid,
                'f_level': level,
                'f_compromised': compromised,
                'f_active': status,
                'f_fullname': fullname,
                'f_description': Comment,
            }

    else:
        logger.error("Unknown file type provided")

    logging.debug(accounts)
    return accounts

##-------------------------------------------------------------------------

def process_mass_password(pw_file=None, pw_type=None, message=None, proto=None, portnum=None, add_hosts=False, user_id=1):
    """
    Process a medusa/hydra mass password run
    """
    import fileinput

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    added = 0
    updated = 0
    new_hosts = 0
    ip_dict = {}
    if pw_file is not None:
        try:
            fIN = fileinput.input(files=pw_file)
        except IOError, e:
            logger.error("Error opening %s: %s" % (pw_file, e))
            return "Error opening %s: %s" % (pw_file, e)
    else:
        return "No filename provided.. that's odd"

    if pw_type is None:
        return "No file type provided.. that's odder"

    if message is None:
        message = pw_type
    for line in fIN:
        if line[0] == "#": continue
        line = line.replace('\n', '')
        try:
            if pw_type == "Medusa":
                mass_pw_data = process_medusa(line)
            elif pw_type == "Hydra":
                mass_pw_data = process_hydra(line)
            elif pw_type == "Metasploit Creds CSV":
                mass_pw_data = process_msfcsv(line)
            else:
                mass_pw_data = {}
                mass_pw_data['error'] = 'Invalid password file type provided'
        except Exception, e:
            logger.error("Error with line (%s): %s" % (line, e))
            continue

        if not mass_pw_data.get('error'):
            # return { 'ip': ip, 'port': port, 'user': user, 'pass': pw, 'hash': ntlm, 'msg': msg }
            ip = mass_pw_data.get('ip')
            ip_accts = ip_dict.setdefault(ip, list())
            if mass_pw_data.get('type') == 'smb':
                # we have an ntlm hash, split that instead of updating the password
                (lm, nt) = mass_pw_data['hash'].split(':')[0:2]
                ip_accts.append({
                    'f_username': mass_pw_data.get('user'),
                    'f_hash1': lm, 'f_hash1_type': 'LM',
                    'f_hash2': nt, 'f_hash2_type': 'NT',
                    'f_number': portnum, 'f_proto': proto,
                    'f_password': mass_pw_data.get('pass'),
                    'f_message': mass_pw_data.get('msg'),
                    'f_source': message, 'f_compromised': True
                })
                ip_dict[ip] = ip_accts
            elif mass_pw_data.get('hash'):
                # we have a hash, not a password
                if mass_pw_data['pass']:
                    compromised = True
                else:
                    compromised = False
                ip_accts.append({
                    'f_number': portnum, 'f_proto': proto,
                    'f_username': mass_pw_data.get('user'),
                    'f_password': mass_pw_data.get('pass'),
                    'f_hash1': mass_pw_data.get('hash'),
                    'f_hash1_type': mass_pw_data.get('type'),
                    'f_message': mass_pw_data.get('msg'),
                    'f_source': message, 'f_compromised': compromised,
                })
                ip_dict[ip] = ip_accts
            else:
                # otherwise append the relevant information
                ip_accts.append({
                    'f_number': portnum, 'f_proto': proto,
                    'f_username': mass_pw_data.get('user'),
                    'f_password': mass_pw_data.get('pass'),
                    'f_message': mass_pw_data.get('msg'),
                    'f_source': message, 'f_compromised': True
                })
                ip_dict[ip] = ip_accts

    # run through the ip_accts now to add/update them to the database
    from skaldship.hosts import get_host_record
    for k,v in ip_dict.iteritems():
        for ip_acct in v:
            # build a query to find the service for this host/port combo
            query = (db.t_hosts.f_ipv4 == k) & (db.t_services.f_hosts_id == db.t_hosts.id)
            query &= (db.t_services.f_proto==ip_acct['f_proto']) & (db.t_services.f_number==ip_acct['f_number'])
            svc = db(query).select(db.t_services.id, cache=(cache.ram, 60)).first()
            if svc is None:
                # no service found, get the host record based on the IP
                host_rec = get_host_record(k)
                if host_rec is None and add_hosts:
                    # add host to the database, unfortunately all we know is the IP address so it's pretty bare.
                    # assign it to the current user and asset group of "new_hosts_medusa"
                    fields = {
                        'f_ipv4': k,
                        'f_engineer': user_id,
                        'f_asset_group': 'new_hosts_medusa',
                    }
                    host_rec = db.t_hosts.insert(**fields)
                    db.commit()
                    logger.info("Added new host from Medusa output: %s" % (k))
                    new_hosts += 1
                elif host_rec is None:
                    # no host and not asking to add hosts so print message and continue
                    logger.error("Unable to find host_rec for %s" % (k))
                    continue

                # add the new service to the host_rec
                fields = {
                    'f_hosts_id': host_rec.id,
                    'f_proto': ip_acct['f_proto'],
                    'f_number': ip_acct['f_number'],
                }
                svc_id = db.t_services.insert(**fields)
                db.commit()
                logger.info("Added new service (%s/%s) to host %s" % (ip_acct['f_proto'], ip_acct['f_number'], k))
            else:
                svc_id = svc.id

            # lookup the password from the lm/nt hash fields (if they exist)
            if ip_acct.has_key('f_hash1'):
                ip_acct['f_password'] = lookup_hash("%s:%s" % (ip_acct.get('f_hash1'), ip_acct.get('f_hash2')))

            # remove f_proto and f_number since they're not in t_accounts, add service id
            ip_acct.pop('f_proto')
            ip_acct.pop('f_number')
            ip_acct.update({'f_services_id': svc_id})
            query = (db.t_accounts.f_services_id == svc_id)&(db.t_accounts.f_username == ip_acct['f_username'])
            row = db(query).select().first()
            if row:
                row.update_record(**ip_acct)
                updated += 1
            else:
                db.t_accounts.insert(**ip_acct)
                added += 1
            db.commit()
    return "Completed: %s/Added, %s/Updated, %s/New Hosts" % (added, updated, new_hosts)


def _doctest():
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    _doctest()

########NEW FILE########
__FILENAME__ = qualys
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Qualys Utilities for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""
from gluon import current
import gluon.contrib.simplejson
import logging
logger = logging.getLogger("web2py.app.kvasir")

db = current.globalenv['db']
auth = current.globalenv['auth']

def process_xml(
    filename=None,
    addnoports=False,
    asset_group=None,
    engineer=None,
    msf_workspace=False,
    ip_ignore_list=None,
    ip_include_list=None,
    update_hosts=False,
    ):
    # Upload and process Qualys XML Scan file
    import os, time, re, HTMLParser
    from StringIO import StringIO
    from MetasploitAPI import MetasploitAPI
    from skaldship.hosts import html_to_markmin, get_host_record, do_host_status
    from skaldship.cpe import lookup_cpe

    parser = HTMLParser.HTMLParser()

    # output regexes
    RE_NETBIOS_NAME = re.compile('NetBIOS name: (?P<d>.*),')
    RE_NETBIOS_MAC = re.compile('NetBIOS MAC: (?P<d>([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2}))')
    RE_IPV4 = re.compile('^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$')

    if msf_workspace:
        msf = MetasploitAPI(host=user_id.f_msf_pro_url, apikey=user_id.f_msf_pro_key)
        if msf.login():
            logger.info(" [-] Authenticated to Metasploit PRO")
        else:
            logger.error(" [!] Unable to login to Metasploit PRO, check your API key")
            msf = None
    else:
        logger.warn(" [-] No Metasploit workspace provided!")
        msf = None

    try:
        from lxml import etree
    except ImportError:
        try:
            import xml.etree.cElementTree as etree
        except ImportError:
            try:
                import xml.etree.ElementTree as etree
            except:
                raise Exception("Unable to find valid ElementTree module.")

    # build the hosts only/exclude list
    ip_exclude = []
    if ip_ignore_list:
        ip_exclude = ip_ignore_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals
    ip_only = []
    if ip_include_list:
        ip_only = ip_include_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals

    print(" [*] Processing Qualys scan file %s" % (filename))

    try:
        nmap_xml = etree.parse(filename)
    except etree.ParseError, e:
        print(" [!] Invalid XML file (%s): %s " % (filename, e))
        return

    root = nmap_xml.getroot()

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    existing_vulnids = db(db.t_vulndata()).select(db.t_vulndata.id, db.t_vulndata.f_vulnid).as_dict(key='f_vulnid')
    #print(" [*] Found %d vulnerabilities in the database already." % (len(existing_vulnids)))

    # check for any CPE OS data
    if db(db.t_cpe_os).count() > 0:
        have_cpe = True
    else:
        have_cpe = False

    user_id = db.auth_user(engineer) or auth.user.id

    # parse the hosts, where all the goodies are
    nodes = root.findall('IP')
    print(" [-] Parsing %d hosts" % (len(nodes)))
    hoststats = {}
    hoststats['added'] = 0
    hoststats['skipped'] = 0
    hoststats['updated'] = 0
    hoststats['errored'] = 0
    hosts = []   # array of host_id fields
    vulns_added =0
    vulns_skipped = 0

    for node in nodes:
        nodefields = {}
        ip = node.get('value')
        if RE_IPV4.match(ip):
            nodefields['f_ipv4'] = ip
        else:
            nodefields['f_ipv6'] = ip

        nodefields['f_hostname'] = node.get('hostname')
        nodefields['f_netbios_name'] = node.findtext('NETBIOS_HOSTNAME')
        # nodefields['f_macaddr'] = address.get('addr')

        """
        status = node.find('status').get('state')

        print(" [-] Host %s status is: %s" % (ipaddr, status))
        if status != "up":
            hoststats['skipped'] += 1
            continue
        """

        if ipaddr in ip_exclude:
            print(" [-] Host is in exclude list... skipping")
            hoststats['skipped'] += 1
            continue

        if len(ip_only) > 0 and ipaddr not in ip_only:
            print(" [-] Host is not in the only list... skipping")
            hoststats['skipped'] += 1
            continue

        ports = node.findall('INFOS')
        if len(ports) < 1 and not addnoports:
            print(" [-] No ports open and not asked to add those kind... skipping")
            hoststats['skipped'] += 1
            continue

        nodefields['f_engineer'] = user_id
        nodefields['f_asset_group'] = asset_group
        nodefields['f_confirmed'] = False

        # check to see if IPv4/IPv6 exists in DB already
        if nodefields.has_key('f_ipv4'):
            host_rec = db(db.t_hosts.f_ipv4 == nodefields['f_ipv4']).select().first()
        elif nodefields.has_key('f_ipv6'):
            host_rec = db(db.t_hosts.f_ipv6 == nodefields['f_ipv6']).select().first()
        else:
            logging.warn("No IP Address found in record. Skipping")
            continue

        if host_rec is None:
            host_id = db.t_hosts.insert(**nodefields)
            db.commit()
            hoststats['added'] += 1
            print(" [-] Adding %s" % (ipaddr))
        elif host_rec is not None and update_hosts:
            db.commit()
            if iptype == 'ipv4':
                host_id = db(db.t_hosts.f_ipv4 == nodefields['f_ipv4']).update(**nodefields)
            elif iptype == 'ipv6':
                host_id = db(db.t_hosts.f_ipv6 == nodefields['f_ipv6']).update(**nodefields)
            db.commit()
            host_id = get_host_record(ipaddr)
            host_id = host_id.id
            hoststats['updated'] += 1
            print(" [-] Updating %s" % (ipaddr))
        else:
            hoststats['skipped'] += 1
            db.commit()
            print(" [-] Skipped %s" % (ipaddr))
            continue
        hosts.append(host_id)

        # :
        for hostscripts in node.findall('hostscript/script'):
            svc_id = db.t_services.update_or_insert(f_proto='info', f_number=0, f_status='open', f_hosts_id=host_id)
            db.commit()
            for script in hostscripts:
                script_id = script.get('id')
                output = script.get('output')
                svc_info = db.t_service_info.update_or_insert(f_services_id=svc_id, f_name=script_id, f_text=output)
                db.commit()

        # add ports and resulting vulndata
        for port in node.findall("ports/port"):
            f_proto = port.get('protocol')
            f_number = port.get('portid')
            f_status = port.find('state').get('state')

            port_svc = port.find('service')
            if port_svc:
                f_name = port_svc.get('name')
                f_product = port_svc.get('product')
                svc_fp = port_svc.get('servicefp')
            else:
                f_name = None
                f_product = None
                svc_fp = None

            print(" [-] Adding port: %s/%s (%s)" % (f_proto, f_number, f_name))
            svc_id = db.t_services.update_or_insert(f_proto=f_proto, f_number=f_number, f_status=f_status, f_hosts_id=host_id, f_name=f_name)

            if f_product:
                version = port.find('service').get('version', None)
                if version:
                    f_product += " (%s)" % (version)
                svc_info = db.t_service_info.update_or_insert(f_services_id=svc_id, f_name=f_name, f_text=f_product)
                db.commit()

            if svc_fp:
                svc_info = db.t_service_info.update_or_insert(f_services_id=svc_id, f_name=svc_fp, f_text=svc_fp)
                db.commit()

            # Process <script> service entries
            for script in port.findall('service/script'):
                svc_info = db.t_service_info.update_or_insert(f_services_id=svc_id, f_name=script.get('id'), f_text=script.get('output'))
                db.commit()

            # Process <cpe> service entries
            for port_cpe in port.findall('service/cpe'):
                cpe_id = port_cpe.text.replace('cpe:/', '')

                if cpe_id[0] == "a":
                    # process CPE Applications

                    print(" [-] Found Application CPE data: %s" % (cpe_id))
                    svc_info = db.t_service_info.update_or_insert(f_services_id=svc_id, f_name='CPE ID', f_text="cpe:/%s" % (cpe_id))
                    db.commit()

                elif cpe_id[0] == "o":
                    # process CPE Operating System

                    os_id = lookup_cpe(cpe_id[2:])

                    if os_id is not None:
                        db.t_host_os_refs.insert(f_certainty='0.9',
                                                 f_family='Unknown',
                                                 f_class='Other',
                                                 f_hosts_id=host_id,
                                                 f_os_id=os_id)
                        db.commit()
                    else:
                        # So no CPE or existing OS data, lets split up the CPE data and make our own
                        print(" [!] No os_id found, this is odd !!!")

                for config in port.findall("configuration/config"):
                    cfg_id = db.t_service_info.update_or_insert(f_services_id=svc_id, f_name=config.attrib['name'], f_text=config.text)
                    db.commit()
                    if re.match('\w+.banner$', config.attrib['name']):
                        db.t_services[svc_id] = dict(f_banner=config.text)
                        db.commit()
                    if config.attrib['name'] == 'mac-address':
                        # update the mac address of the host
                        db.t_hosts[host_id] = dict(f_macaddr = config.text)
                        db.commit()
                    if "advertised-name" in config.attrib['name']:
                        # netbios computer name
                        d = config.text.split(" ")[0]
                        if "Computer Name" in config.text:
                            data = {}
                            data['f_netbios_name'] = d
                            # if hostname isn't defined then lowercase netbios name and put it in
                            if db.t_hosts[host_id].f_hostname is None:
                                data['f_hostname'] = d.lower()
                            db(db.t_hosts.id == host_id).update(**data)
                        elif "Domain Name" in config.text:
                            db(db.t_netbios.f_hosts_id == host_id).update(f_domain=d) or db.t_netbios.insert(f_hosts_id=host_id, f_domain=d)
                        db.commit()

                for script in port.findall("script"):
                    # process <script> results. This data contains both info
                    # and vulnerability data. For now we'll take a list of
                    # known nmap vuln checks from private/nmap_vulns.csv and
                    # use that to separate between service_info and vulndata.
                    pass

    if msf is not None:
        # send the downloaded nexpose file to MSF for importing
        try:
            res = msf.pro_import_file(
                msf_workspace,
                filename,
                {
                    'DS_REMOVE_FILE': False,
                    'tag': asset_group,
                    },
            )
            print(" [*] Added file to MSF Pro: %s" % (res))
        except MSFAPIError, e:
            logging.error("MSFAPI Error: %s" % (e))
            pass

    # any new nexpose vulns need to be checked against exploits table and connected
    print(" [*] Connecting exploits to vulns and performing do_host_status")
    do_host_status(asset_group=asset_group)

    print(" [*] Import complete: hosts: %s added, %s skipped, %s errors - vulns: %s added, %s skipped" % (hoststats['added'],
                                                                                                          hoststats['skipped'],
                                                                                                          hoststats['errored'],
                                                                                                          vulns_added, vulns_skipped))

########NEW FILE########
__FILENAME__ = services
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2014 Cisco Systems, Inc.
##
## Services utility module
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##
##--------------------------------------#
"""

from gluon import current
from skaldship.log import log


##-------------------------------------------------------------------------
class Services:
    """
    Kvasir Services class. Provides basic functions to add, update, return service data
    """
    def __init__(self):
        self.db = current.globalenv['db']
        self.svc_db = self.db.t_services
        self.services = {}      # cached dict of services


    ##---------------------------------------------------------------------
    def _get_record(self, proto, port, svcname, host_id):
        """
        Returns a unique single record based on specific data
        """
        if not proto or not port or not host_id:
            return None

        query = (self.svc_db.f_proto==proto) & (self.svc_db.f_number==port) & (self.svc_db.f_hosts_id == host_id)
        record = self.db(query).select().first()
        if record.id not in self.services:
            self.services[record.id] = record

        return record


    ##---------------------------------------------------------------------
    def _update_or_insert(self, proto, port, svcname, host_id):
        """
        Our own update_or_insert routine
        """
        if not proto or not port or not host_id:
            return None

        svc_id = self.svc_db.update_or_insert(
            f_proto=proto, f_number=port, f_status=svcname, f_hosts_id=host_id
        )
        self.db.commit()
        if not svc_id:
            record = self._get_record(proto, port, svcname, host_id)
            if record:
                return record.id
            else:
                return None

        return svc_id


    ##---------------------------------------------------------------------
    def get_id(self, proto, port, svcname, host_id, create_or_update=False):
        """
        Returns the record identifier of a service based upon strict criteria.
        """

        if create_or_update:
            return self._update_or_insert(proto, port, svcname, host_id)

        record = self._get_record(proto, port, svcname, host_id)
        if record:
            return record.id

        return None


##-------------------------------------------------------------------------
def pagination_services(request, curr_service):
    # Pagination! Send it the db, request and current host record, get back
    # a dictionary to put into the view.

    db = current.globalenv['db']
    #cache = current.globalenv['cache']

    from gluon.html import OPTION, SELECT, FORM, A
    from skaldship.general import host_title_maker

    servicelist = []
    serviceprev = "#"
    servicenext = "#"
    serviceselected = 0
    servicenextstyle = serviceprevstyle = ""
    serviceprevtitle = servicenexttitle = ""
    serviceindex = 1
    servicecount = 0
    query = db.t_services.f_hosts_id == db.t_hosts.id

    """
    # Create more filters here
    if request.vars.filterconfirmed is not None:
        session.servicefilterconfirmed=request.vars.filterconfirmed

    if session.servicefilterconfirmed == 'Unconfirmed [H]osts':
        query=db.t_services
    else:
        query=db.t_services.f_confirmed==False

    """
    for h_rec in db(query).select(orderby=db.t_hosts.id):
        hostrecord = h_rec.t_hosts
        servicelist.append(OPTION(host_title_maker(hostrecord) + " - " + service_title_maker(h_rec.t_services), _value=h_rec.t_services.id))
        if serviceselected != 0 and servicenext == "#":
            servicenext = h_rec.t_services.id
            servicenexttitle = "Go to " + host_title_maker(hostrecord) + " - " + service_title_maker(h_rec.t_services)
        if h_rec.t_services.id == curr_service.id:
            serviceselected = serviceindex
        if serviceselected == 0:
            serviceprev = h_rec.t_services.id
            serviceprevtitle = "Go to " + host_title_maker(hostrecord) + " - " + service_title_maker(h_rec.t_services)
        if h_rec.t_services.f_hosts_id == curr_service.f_hosts_id:
            serviceindex += 1
            servicecount += 1

    if serviceprev == "#":
        serviceprevstyle = "display:none"
    if servicenext == "#":
        servicenextstyle = "display:none"

    pagination = {}
    pagination['form'] = FORM(A("<<(p)",_id="prevservicelink" ,_class="button", _href=serviceprev, _style=serviceprevstyle, _title=serviceprevtitle), "    ",
                              SELECT(servicelist, value=request.args(0), _class="autowidth", _id="service_select", _name="service_select", _onchange="window.location.href=$('#service_select').val()"), "  ", A("(n)>>", _id="nextservicelink", _class="button", _href=servicenext, _style=servicenextstyle, _title=servicenexttitle),_method='get')

    pagination['service_number'] = "( %d/%d )" % (serviceselected, servicecount)

    return pagination

##-------------------------------------------------------------------------


def get_service_record(host_rec=None, proto=None, pnum=None):
    """
    Returns a service record ID based on a host_record and proto/number

    XXX: This is not used yet
    """
    if host_rec is None:
        return None

    db = current.globalenv['db']

    query = (db.t_services.f_proto == proto)
    query &= (db.t_services.f_number == pnum)
    return host_rec.t_services(query).select().first()

##-------------------------------------------------------------------------


def service_title_maker(record):
    """
    Given a t_service record, return a string value to place
    in the HTML TITLE (via response.title) or any other text
    place. Like a form field, json output, etc.

    XXX: This is not used yet
    """

    if record is None or not hasattr(record, 'f_proto'):
        return "Unknown"

    serviceinfo = "%s/%s" % (record['f_proto'], record['f_number'])

    return serviceinfo

########NEW FILE########
__FILENAME__ = shodanhq
# -*- coding: utf-8 -*-

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## ShodanHQ Utilities for Kvasir
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from gluon import current
from skaldship.log import log
from skaldship.hosts import get_host_record, do_host_status

##-------------------------------------------------------------------------

class ShodanData():

    def __init__(self):
        self.stats = {}
        self.stats['hosts_added'] = 0
        self.stats['hosts_skipped'] = 0
        self.stats['hosts_updated'] = 0
        self.stats['services_added'] = 0
        self.stats['services_updated'] = 0
        self.ip_only = []
        self.ip_exclude = []
        self.engineer = None
        self.asset_group = 'ShodanHQ Import'
        self.hosts = []
        import re
        self.SMTP_FTP_220 = re.compile('^220.*')

    def parse_host(self, host):
        """
        Parse an XML host data from ShodanHQ results
        """
        from gluon.validators import IS_IPADDRESS
        db = current.globalenv['db']

        hostfields = {}
        ipaddr = host.get('ip')

        if self.ip_only and ipaddr not in self.ip_only:
            log(" [-] %s is not in the only list... skipping" % (ipaddr))
            #sys.stderr.write(msg)
            self.stats['hosts_skipped'] += 1
            return

        if ipaddr in self.ip_exclude:
            log(" [-] %s is in exclude list... skipping" % (ipaddr))

        if IS_IPADDRESS(is_ipv4=True)(ipaddr)[1] is None:
            # address is IPv4:
            hostfields['f_ipv4'] = ipaddr
        elif IS_IPADDRESS(is_ipv6=True)(ipaddr)[1] is None:
            hostfields['f_ipv6'] = ipaddr
        else:
            log(" [!] Invalid IP Address in report: %s" % (ipaddr))
            return

        hostname = host.findtext('hostnames')
        if hostname:
            hostfields['f_hostname'] = hostname

        # check to see if IP exists in DB already
        if 'f_ipv4' in hostfields:
            host_rec = db(db.t_hosts.f_ipv4 == hostfields['f_ipv4']).select().first()
        else:
            host_rec = db(db.t_hosts.f_ipv6 == hostfields['f_ipv6']).select().first()

        if host_rec is None:
            hostfields['f_asset_group'] = self.asset_group
            hostfields['f_engineer'] = self.engineer
            host_id = db.t_hosts.insert(**hostfields)
            db.commit()
            self.stats['hosts_added'] += 1
            log(" [-] Adding IP: %s" % (ipaddr))

        elif host_rec is not None:
            db.commit()
            if 'f_ipv4' in hostfields:
                host_id = db(db.t_hosts.f_ipv4 == hostfields['f_ipv4']).update(**hostfields)
                db.commit()
                host_id = get_host_record(hostfields['f_ipv4'])
                host_id = host_id.id
                self.stats['hosts_updated'] += 1
                log(" [-] Updating IP: %s" % (hostfields['f_ipv4']))
            else:
                host_id = db(db.t_hosts.f_ipv6 == hostfields['f_ipv6']).update(**hostfields)
                db.commit()
                host_id = get_host_record(hostfields['f_ipv6'])
                host_id = host_id.id
                self.stats['hosts_updated'] += 1
                log(" [-] Updating IP: %s" % (hostfields['f_ipv6']))

        else:
            self.stats['hosts_skipped'] += 1
            db.commit()
            log(" [-] Skipped IP: %s" % (ipaddr))
            return

        # process the service / data
        f_number = host.get('port')
        if f_number == '161':
            # only udp provided by shodanhq is snmp
            f_proto = 'udp'
        else:
            f_proto = 'tcp'

        f_status = 'open'
        f_name = ''
        addl_fields = {}

        # extract the data field for processing
        port_data = host.findtext('data')

        # for ssh, telnet and smtp throw data into the banner
        if f_number == '21':
            f_banner = "\n".join(self.SMTP_FTP_220.findall(port_data))
            f_name = 'FTP'
            addl_fields = {
                'ftp.banner': port_data,
            }
        elif f_number == '22':
            f_banner = port_data
            f_name = 'SSH'
            addl_fields = {
                'ssh.banner': port_data,
            }
        elif f_number == '23':
            f_banner = port_data
            f_name = 'Telnet'
        elif f_number == '25':
            f_banner = "\n".join(self.SMTP_FTP_220.findall(port_data))
            f_name = 'SMTP'
            addl_fields = {
                'smtp.banner': port_data,
            }
        elif f_number in HTTP_PORTS:
            # TODO: parse HTTP headers.. ugly
            f_banner = port_data
            f_name = 'HTTP'
            addl_fields = {
                'http.banner': port_data,
            }
        elif f_number == '1900':
            f_banner = port_data
            f_name = 'UPNP'
            addl_fields = {
                'upnp.banner': port_data,
            }
        else:
            f_banner = port_data

        query = (db.t_services.f_proto == f_proto) & (db.t_services.f_number == f_number) & (db.t_services.f_hosts_id == host_id)
        svc_row = db(query).select().first()
        if svc_row:
            # we found a service record! Check for similar status, names and banners
            do_update = False
            if svc_row.f_status != f_status:
                svc_row.f_status = f_status
                do_update = True
            if svc_row.f_name != f_name:
                svc_row.f_name = f_name
                do_update = True
            if svc_row.f_banner != f_banner:
                svc_row.f_banner = f_banner
                do_update = True

            svc_id = svc_row.id
            if do_update:
                svc_row.update_record()
                db.commit()
                didwhat = "Updated"
                self.stats['services_updated'] += 1
            else:
                didwhat = "Unaltered"
        else:
            # we have a new service!
            svc_id = db.t_services.insert(
                f_proto=f_proto,
                f_number=f_number,
                f_status=f_status,
                f_name=f_name,
                f_banner=f_banner,
                f_hosts_id=host_id
            )
            db.commit()
            didwhat = "Added"
            self.stats['services_added'] += 1

        log(" [-] %s service: (%s) %s/%s" % (didwhat, ipaddr, f_proto, f_number))

        for k, v in addl_fields.iteritems():
            # add additional field entries as service_info records
            db.t_service_info.update_or_insert(
                f_services_id=svc_id,
                f_name=k,
                f_text=v,
            )
            db.commit()

##---------------------------------------------------------

def process_report(
    filename=None,
    host_list=[],
    query=None,
    ip_ignore_list=None,
    ip_include_list=None,
    engineer=1,
    asset_group="ShodanHQ Import",
):
    """
    Processes a ShodanHQ XML Report adding records to the db
    """

    settings = current.globalenv['settings']

    #try:
    #    from shodan import WebAPI
    #    from shodan.api import WebAPIError
    #    webapi = WebAPI(settings.shodanhq_apikey)
    #except ImportError:
    #    webapi = None

    sd = ShodanData()
    sd.engineer = engineer
    sd.asset_group = asset_group

    # build the hosts only/exclude list
    if ip_ignore_list:
        sd.ip_exclude = ip_ignore_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals
    if ip_include_list:
        sd.ip_only = ip_include_list.split('\r\n')
        # TODO: check for ip subnet/range and break it out to individuals

    hosts = []
    if filename:
        log(" [*] Processing ShodanHQ report file: %s" % (filename))
        try:
            from lxml import etree
        except ImportError:
            try:
                import xml.etree.cElementTree as etree
            except ImportError:
                try:
                    import xml.etree.ElementTree as etree
                except:
                    raise Exception("Unable to find valid ElementTree module.")

        try:
            xml = etree.parse(filename)
        except etree.ParseError, e:
            raise Exception(" [!] Invalid XML file (%s): %s " % (filename, e))
            return

        root = xml.getroot()
        hosts = root.findall("host")

    """
    elif host_list and webapi:
        if not isinstance(host_list, list):
            host_list = [host_list]

        log(" [!] Searching for %s hosts from ShodanHQ" % (len(host_list)), level=logging.DEBUG)
        for h in host_list:
            try:
                host_result = webapi.host(h)
                if host_result.get('ip'):
                    hosts.append(host_result)
            except WebAPIError, e:
                log(" [!] (%s) ShodanHQ error response: %s" % (h, str(e)), level=logging.ERROR)
            except Exception, e:
                log(" [!] (%s) No response from ShodanHQ: %s" % (h, str(e)), level=logging.ERROR)

    elif query and webapi:
        log(" [!] Sending ShodanHQ WebAPI query: %s" % (str(query)), level=logging.DEBUG)
        try:
            query_result = webapi.search(query[0], limit=query[1])
            if query_result.get('total') > 0:
                hosts.append(query_result.get('matches'))
        except WebAPIError, e:
            log(" [!] (%s) ShodanHQ error response: %s" % (query, str(e)), level=logging.ERROR)
        except Exception, e:
            log(" [!] (%s) No response from ShodanHQ: %s" % (query, str(e)), level=logging.ERROR)
    """

    log(" [-] Parsing %d hosts" % (len(hosts)))
    for host in hosts:
        sd.parse_host(host)

    do_host_status()

    msg = " [*] Import complete: hosts: (%s/A, %s/U, %s/S) - services: (%s/A, %s/U)"\
        % (
            sd.stats['hosts_added'],
            sd.stats['hosts_updated'],
            sd.stats['hosts_skipped'],
            sd.stats['services_added'],
            sd.stats['services_updated'],
        )
    log(msg)
    return msg

########NEW FILE########
__FILENAME__ = statistics
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Statistical functions
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

from StringIO import StringIO

from gluon import current
from skaldship.hosts import create_hostfilter_query
from skaldship.general import severity_mapping

import logging
logger = logging.getLogger("web2py.app.kvasir")

def vulnlist(query="%", hostfilter=None):
    """
    Returns a vulnerability dictionary with counts:

    { 'vulnerability id': [ status, count, severity, cvss ] }
    """

    db = current.globalenv['db']
    session = current.globalenv['session']

    status = db.t_service_vulns.f_status
    svcv_id = db.t_service_vulns.id
    vulnid = db.t_vulndata.id
    f_vulnid = db.t_vulndata.f_vulnid
    sev = db.t_vulndata.f_severity
    cvss = db.t_vulndata.f_cvss_score

    hostfilter = hostfilter or session.hostfilter

    if hostfilter is None:
        # if no filter is set then we blank it out
        if session.hostfilter is None:
            session.hostfilter = [(None, None), False]

    query = (db.t_vulndata.f_vulnid.contains(query))
    query &= (db.t_service_vulns.f_services_id == db.t_services.id) & (db.t_service_vulns.f_vulndata_id == db.t_vulndata.id)
    query = create_hostfilter_query(hostfilter, query, 't_services')
    #query = (db.t_service_vulns.f_vulndata_id == db.t_vulndata.id)

    vulnlist = {}
    for rec in db(query).select(status, vulnid, f_vulnid, svcv_id, sev, cvss):
        q2 = (query & ((db.t_service_vulns.f_vulndata_id == rec.t_vulndata.id) &
             (db.t_service_vulns.f_status == rec.t_service_vulns.f_status)))
        count = db(q2).count()
        vstats = vulnlist.setdefault(rec.t_vulndata.f_vulnid, list())
        if rec.t_service_vulns.f_status not in map(lambda a:a[0], vstats):
            vstats.append([rec.t_service_vulns.f_status,
                            count,
                            rec.t_vulndata.f_severity,
                            rec.t_vulndata.f_cvss_score
                         ] )
            vulnlist[rec.t_vulndata.f_vulnid] = vstats

    return vulnlist

##-------------------------------------------------------------------------

def db_statistics():
    """
    Returns a dictionary of database statistics
    """
    db = current.globalenv['db']
    cache = current.globalenv['cache']

    svulns = db.t_service_vulns
    svcs = db.t_services
    vd = db.t_vulndata
    osdb = db.t_os
    accts = db.t_accounts
    hosts = db.t_hosts

    statistics = {}
    statistics['os_count'] = db(osdb).count()
    statistics['vulndata_count'] = db(vd).count()
    statistics['accounts'] = db(accts).count()
    statistics['compromised_accounts'] = db(accts.f_compromised == True).count()
    #pwq = (((accts.f_password != None) | (accts.f_password != '')) | (accts.f_compromised == True))
    statistics['passwords'] = db(accts.f_compromised==True).count()
    statistics['joe'] = db(accts.f_username == db.t_accounts.f_password).count()
    statistics['hosts'] = db(hosts).count()
    statistics['hosts_confirmed'] = db(hosts.f_confirmed == True).count()
    statistics['hosts_unconfirmed'] = db(hosts.f_confirmed == False).count()
    statistics['hosts_accessed'] = db(hosts.f_accessed == True).count()

    # XXX: Can this be turned into a db query to speed things up? It's awfully slow
    # generate count of hosts with a vulnerability. Have to tie db.t_service_vulns to
    # a db.t_services record
    service_vulns = db(svulns).select(
        svulns.f_services_id, svulns.f_vulndata_id, svcs.id, svcs.f_hosts_id,
        left=svcs.on(svulns.f_services_id == svcs.id),
        distinct=True,
    )
    vulnhosts = []
    for service in service_vulns:
        hostid = [service.t_services.f_hosts_id]
        if hostid not in vulnhosts:
            vulnhosts.append(hostid)

    # get host count of those with a sev >= 8
    statistics['high_vuln_host_count'] = 0
    if current.globalenv['settings'].use_cvss:
        maxhostsev = vd.f_cvss_score.max()
    else:
        maxhostsev = vd.f_severity.max()
    q = (svulns.f_services_id == db.t_services.id) & (vd.id == svulns.f_vulndata_id)
    for rec in db(q).select(maxhostsev, svcs.f_hosts_id, orderby=svcs.f_hosts_id, groupby=svcs.f_hosts_id):
        if rec[maxhostsev] >= 8:
            statistics['high_vuln_host_count'] += 1

    statistics['vuln_host_count'] = len(vulnhosts)
    if statistics['hosts'] > 0:
        statistics['vuln_host_pct'] = "%.2f%%" % (float(float(len(vulnhosts)) / float(statistics['hosts'])) * 100)
    else:
        statistics['vuln_host_pct'] = "0%"

    if statistics['hosts'] > 0:
        statistics['high_vuln_host_pct'] = "%.2f%%" % (float(float(statistics['high_vuln_host_count']) / float(statistics['hosts'])) * 100)
    else:
        statistics['high_vuln_host_pct'] = "0%"

    statistics['services'] = db(svcs).count()
    statistics['service_vulns'] = db(svulns).count()
    statistics['services_with_vulns'] = len(db(svulns).select(svulns.f_services_id,groupby=svulns.f_services_id))
    if current.globalenv['settings'].use_cvss:
        statistics['services_with_high_vulns'] = db((svulns.f_vulndata_id == vd.id) & (vd.f_cvss_score >= 8)).count()
    else:
        statistics['services_with_high_vulns'] = db((svulns.f_vulndata_id == vd.id) & (vd.f_severity >= 8)).count()
    #statistics['services_with_high_vulns'] = len(db(vd.f_severity >= 8).select(
    #    svulns.f_services_id, svulns.f_vulndata_id, vd.id,
    #    left=svulns.on(vd.id == svulns.f_vulndata_id), groupby=svulns.f_services_id|svulns.f_vulndata_id|vd.id
    #))

    return statistics

##-------------------------------------------------------------------------

def adv_db_statistics():
    """
    Returns a dictionary of Advance database statistics
    """

    db = current.globalenv['db']
    cache = current.globalenv['cache']

    statistics = {}

    #Netbios stats
    domains = []
    for rec in db(db.t_netbios).select(db.t_netbios.f_domain,distinct=True):
        if rec.f_domain:
            domains.append(rec.f_domain)
    statistics['domains'] = domains

    #Account stats
    rootquery = db((db.t_accounts.f_username=='root') & (db.t_accounts.f_compromised==True)).count()
    statistics['ROOT'] = rootquery
    adminquery=((db.t_accounts.f_username=='Administrator')|(db.t_accounts.f_uid==500)) & (db.t_accounts.f_compromised==True)
    statistics['Administrator'] = db(adminquery).count()
    userquery=(db.t_accounts.f_username!='Administrator')&(db.t_accounts.f_username!='root')&(db.t_accounts.f_uid!=500)&(db.t_accounts.f_uid!=0)
    statistics['USER'] = db(userquery  & (db.t_accounts.f_compromised==True)).count()

    #SNMP Stats
    statistics['SNMP_read'] = db(db.t_snmp.f_access=='READ').count()
    statistics['SNMP_write'] = db(db.t_snmp.f_access=='WRITE').count()
    #statistics['services_with_vulns'] = len(db(db.t_service_vulns.f_services_id).select(db.t_service_vulns.f_services_id,groupby=db.t_service_vulns.f_services_id))

    #Top services'
    #for rec in db(db.t_netbios).select(db.t_services.f_proto,distinct=True):
    #    statistics['domains'] = statistics['domains'] + ' ' + rec.f_domain

    return statistics

##-------------------------------------------------------------------------

def graphs_index():
    db = current.globalenv['db']
    cache = current.globalenv['cache']

    graph = {}

    host_by_sev = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    if current.globalenv['settings'].use_cvss:
        maxhostsev = db.t_vulndata.f_cvss_score.max()
    else:
        maxhostsev = db.t_vulndata.f_severity.max()

    q = (db.t_service_vulns.f_services_id == db.t_services.id) & (db.t_vulndata.id == db.t_service_vulns.f_vulndata_id)
    for rec in db(q).select(maxhostsev, db.t_services.f_hosts_id, orderby=db.t_services.f_hosts_id, groupby=db.t_services.f_hosts_id):
        host_by_sev[int(rec[maxhostsev])] += 1

    graph['top_host_sev_count'] = ''
    cnt = 0
    for h_rec in host_by_sev:
        graph['top_host_sev_count'] = graph['top_host_sev_count'] + "{ name: 'Sev %s', color: '%s', y: %d},\n" % (cnt, severity_mapping(cnt)[2], h_rec)
        cnt += 1
    graph['top_host_sev_count_raw'] = host_by_sev

    vuln_by_sev = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    count = db.t_vulndata.id.count()
    if current.globalenv['settings'].use_cvss:
        rows = db(db.t_vulndata.id == db.t_service_vulns.f_vulndata_id).select(
            db.t_vulndata.f_cvss_score, count, orderby=db.t_vulndata.f_cvss_score, groupby=db.t_vulndata.f_cvss_score)
    else:
        rows = db(db.t_vulndata.id == db.t_service_vulns.f_vulndata_id).select(
            db.t_vulndata.f_severity, count, orderby=db.t_vulndata.f_severity, groupby=db.t_vulndata.f_severity)
    for rec in rows:
        if current.globalenv['settings'].use_cvss:
            vuln_by_sev[int(rec.t_vulndata.f_cvss_score)] += rec[count]
        else:
            vuln_by_sev[rec.t_vulndata.f_severity] = rec[count]

    graph['vuln_by_sev_count'] = ''
    graph['vuln_by_sev_count_raw'] = vuln_by_sev
    cnt = 0
    for h_rec in vuln_by_sev:
        graph['vuln_by_sev_count'] += "{ name: 'Sev %s', color: '%s', y: %d},\n" % (
        cnt, severity_mapping(cnt)[2], h_rec)
        cnt += 1

    return graph


########NEW FILE########
__FILENAME__ = vncscreenshot
# encoding: utf-8

__version__ = "1.0"

"""
##-----------------------------------------------#
## Kvasir Skaldship VNC Screenshot Valkyrie
##
## Grab screenshots of VNC desktops using vncsnapshot
## http://sourceforge.net/projects/vncsnapshot/
##
## patches are required to get vncsnapshot to work in 64bit OS
## https://launchpadlibrarian.net/85079370/vncsnapshot_1.2a-5ubuntu1.diff.gz
## for debian/ubuntu do "apt-get install vncsnapshot"
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##-----------------------------------------------#
"""

#from skaldship.general import get_host_record
from subprocess import call
from skaldship.log import log
import logging

##----------------------------------------------------------------------------


def grab_screenshot(host=None, port=5900, outfile=None, vncsnapshot='vncsnapshot'):
    """
    Capture a JPEG image of a VNC Desktop using vncsnapshot

    @args:
        host: Host address
        port: Port address (5900, 5901, 5902, etc)
        outfile: Output filename, will overwrite but not remove failures

    @output:
        [True/False, jpeg image data]
    """
    import os
    from gluon import current
    from sys import platform

    if not outfile:
        raise Exception("No output filename provided")

    try:
        os.stat(vncsnapshot)
    except OSError:
        vncsnapshot = "/usr/local/bin/vncsnapshot"
        try:
            os.stat(vncsnapshot)
        except OSError:
            logging.error("Unable to locate vncsnapshot binary")
            return [False, None]

    folder = current.globalenv['request'].folder
    if platform in ["linux", "linux2"]:
        cmd = ["/usr/bin/timeout", "-k", "2", "10"]
    elif platform in ["darwin", "freebsd"]:
        cmd = [os.path.join(folder, 'private/timeout3'), "-t", "10"]
    else:
        cmd = []

    port = int(port)
    port -= 5900
    cmd.extend([vncsnapshot, '-compresslevel', '9', "%s:%s" % (host, port), outfile])
    #log("calling: %s" % str(cmd), logging.DEBUG)
    call(cmd)

    try:
        f = file(outfile)
        imgdata = f.read()
        f.close()
        result = True
    except:
        result = False
        imgdata = None

    return [result, imgdata]

##----------------------------------------------------------------------------


def do_screenshot(services=None):
    """
    Grab a screenshot and import it to the evidence db.
    """

    from gluon.dal import Row
    from gluon import current
    import os
    from skaldship.general import check_datadir
    from multiprocessing import Process

    db = current.globalenv['db']
    settings = current.globalenv['settings']

    if isinstance(services, int):
        services = [services]

    service_rows = []
    if isinstance(services, list):
        for svc in services:
            service_rows.append(db.t_services[svc])

    if isinstance(services, Row):
        service_rows = [services]

    good_count = 0
    invalid_count = 0

    for svc_rec in service_rows:
        if not isinstance(svc_rec, Row):
            invalid_count += 1
            continue

        # go with ipv6 if defined, else pick the ipv4 address
        ipaddr = svc_rec.f_hosts_id.f_ipv6 or svc_rec.f_hosts_id.f_ipv4
        port = svc_rec.f_number
        check_datadir(current.globalenv['request'].folder)
        folder = os.path.join(current.globalenv['request'].folder, "data/screenshots")
        filename = "%s-%st-vnc_screenshot.png" % (ipaddr.replace(':', '_'), port)

        res = grab_screenshot(ipaddr, port, os.path.join(folder, filename))
        if res[0]:
            query = (db.t_evidence.f_hosts_id == svc_rec.f_hosts_id) & (db.t_evidence.f_filename == filename)
            db.t_evidence.update_or_insert(
                query, f_filename=filename, f_hosts_id=svc_rec.f_hosts_id, f_data=res[1],
                f_evidence=filename, f_type="Screenshot", f_text="VNC Screenshot - %s:%s" % (ipaddr, port))
            db.commit()
            print(" [-] VNC screenshot obtained: %s:%s" % (ipaddr, port))
            good_count += 1
        else:
            print(" [!] VNC screenshot failed: %s:%s" % (ipaddr, port))
            invalid_count += 1

    return [good_count, invalid_count]

##----------------------------------------------------------------------------

if __name__ == "__main__":
    import sys
    from hashlib import md5

    if len(sys.argv) <= 2:
        sys.exit("Usage: %s <host> <port> <filename>" % (sys.argv[0]))

    host = sys.argv[1]
    port = sys.argv[2]
    outfile = sys.argv[3]

    result = grab_screenshot(host, port, outfile)
    imgresult = result[1]

    print "Result = %s" % result[0]

    try:
        f = file(outfile)
        imgdata = f.read()
        f.close()
    except Exception, e:
        sys.exit("Error processing outfile: %s" % (e))

    m1 = md5()
    m1.update(imgresult)
    print "imgresult md5 = %s" % m1.hexdigest()

    m2 = md5()
    m2.update(imgdata)
    print "  imgdata md5 = %s" % m2.hexdigest()

    if m1.digest() == m2.digest():
        print "Images matched. Everything worked!"
    else:
        print "Images don't match. Something borked."

########NEW FILE########
__FILENAME__ = webimaging
# encoding: utf-8

__version__ = "1.0"

"""
##-----------------------------------------------#
## Kvasir Skaldship WebImaging Valkyrie
##
## Grab screenshots of websites using phantomjs
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##-----------------------------------------------#
"""

from gluon import current
from subprocess import call
import os
import urllib
from skaldship.log import log
import logging

##----------------------------------------------------------------------------


def grab_screenshot(url=None, outfile=None, phantomjs="/usr/bin/phantomjs"):
    """
    Capture a PNG image of a URL using phantomjs

    @args:
        url: Website URL to retrieve
        outfile: Output filename, will overwrite but not remove failures
        phantomjs: Full path to phantomjs binary

    @output:
        [True/False, png image data]
    """
    import os
    db = current.globalenv['db']

    if not outfile:
        raise Exception("No output filename provided")

    try:
        os.stat(phantomjs)
    except OSError:
        phantomjs = "/usr/local/bin/phantomjs"
        try:
            os.stat(phantomjs)
        except OSError:
            logging.error("Unable to locate phantomjs binary")
            return [False, None]

    # encode the url to make sure it passes cleanly to phantomjs
    url = urllib.quote(url, safe='/:')
    folder = current.globalenv['request'].folder
    from sys import platform
    if platform in ["linux", "linux2"]:
        timeout = ["/usr/bin/timeout", "-k", "2", "5"]
    elif platform in ["darwin", "freebsd"]:
        timeout = [os.path.join(folder, 'private/timeout3'), "-t" "5"]
    else:
        timeout = []
    phantom = timeout + [phantomjs, "--ignore-ssl-errors=true", "%s/modules/skaldship/valkyries/webimaging.js" % (folder), url, outfile]
    log("calling: %s" % str(phantom), logging.DEBUG)
    call(phantom)
    try:
        f = file(outfile)
        imgdata = f.read()
        f.close()
        result = True
    except:
        result = False
        imgdata = None

    return [result, imgdata]

##----------------------------------------------------------------------------


def do_screenshot(services=None):
    """
    Grab a screenshot of a URL and import it to the evidence db.
    """

    from gluon.dal import Row
    from skaldship.general import check_datadir

    db = current.globalenv['db']
    settings = current.globalenv['settings']

    if isinstance(services, int):
        services = [services]

    service_rows = []
    if isinstance(services, list):
        for svc in services:
            service_rows.append(db.t_services[svc])

    if isinstance(services, Row):
        service_rows = [services]

    phantomjs = settings.get('phantomjs', 'phantomjs')
    good_count = 0
    invalid_count = 0
    for svc_rec in service_rows:
        if not isinstance(svc_rec, Row):
            invalid_count += 1
            continue

        # go with ipv6 if defined, else pick the ipv4 address
        ipaddr = svc_rec.f_hosts_id.f_ipv6 or svc_rec.f_hosts_id.f_ipv4
        port = "%s%s" % (svc_rec.f_number, svc_rec.f_proto[0])
        check_datadir(current.globalenv['request'].folder)
        folder = os.path.join(current.globalenv['request'].folder, "data/screenshots")
        filename = "%s-%s-webshot.png" % (ipaddr.replace(':', '_'), port)

        if svc_rec.f_name in ['http', 'https', 'HTTP', 'HTTPS']:
            scheme = svc_rec.f_name.lower()
        else:
            scheme = 'http'
        url = "%s://%s:%s/" % (scheme, ipaddr, svc_rec.f_number)

        res = grab_screenshot(url, os.path.join(folder, filename), phantomjs)
        if res[0]:
            query = (db.t_evidence.f_hosts_id == svc_rec.f_hosts_id) & (db.t_evidence.f_filename == filename)
            db.t_evidence.update_or_insert(
                query, f_filename=filename, f_hosts_id=svc_rec.f_hosts_id, f_data=res[1],
                f_evidence=filename, f_type="Screenshot", f_text="Web Screenshot - %s" % (url))
            db.commit()
            print(" [-] Web screenshot obtained: %s" % (url))
            good_count += 1
        else:
            print(" [!] Web screenshot failed: %s" % (url))
            invalid_count += 1

    return [good_count, invalid_count]

##----------------------------------------------------------------------------

if __name__ == "__main__":
    import sys
    from hashlib import md5

    if len(sys.argv) <= 2:
        sys.exit("Usage: %s <url> <filename>" % (sys.argv[0]))

    url = sys.argv[1]
    outfile = sys.argv[2]

    result = grab_screenshot(url, outfile)
    imgresult = result[1]

    print "Result = %s" % result[0]

    try:
        f = file(outfile)
        imgdata = f.read()
        f.close()
    except Exception, e:
        sys.exit("Error processing outfile: %s" % (e))

    m1 = md5()
    m1.update(imgresult)
    print "imgresult md5 = %s" % m1.hexdigest()

    m2 = md5()
    m2.update(imgdata)
    print "  imgdata md5 = %s" % m2.hexdigest()

    if m1.digest() == m2.digest():
        print "Images matched. Everything worked!"
    else:
        print "Images don't match. Something borked."

########NEW FILE########
__FILENAME__ = app
###############################################################################
#
# App - A class for writing the Excel XLSX App file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Package imports.
from . import xmlwriter
from .utility import encode_utf8


class App(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX App file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(App, self).__init__()

        self.part_names = []
        self.heading_pairs = []
        self.properties = {}

    def _add_part_name(self, part_name):
        # Add the name of a workbook Part such as 'Sheet1' or 'Print_Titles'.
        self.part_names.append(part_name)

    def _add_heading_pair(self, heading_pair):
        # Add the name of a workbook Heading Pair such as 'Worksheets',
        # 'Charts' or 'Named Ranges'.

        # Ignore empty pairs such as chartsheets.
        if not heading_pair[1]:
            return

        self.heading_pairs.append(('lpstr', heading_pair[0]))
        self.heading_pairs.append(('i4', heading_pair[1]))

    def _set_properties(self, properties):
        # Set the document properties.
        self.properties = properties

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        self._write_properties()
        self._write_application()
        self._write_doc_security()
        self._write_scale_crop()
        self._write_heading_pairs()
        self._write_titles_of_parts()
        self._write_manager()
        self._write_company()
        self._write_links_up_to_date()
        self._write_shared_doc()
        self._write_hyperlinks_changed()
        self._write_app_version()

        self._xml_end_tag('Properties')

        # Close the file.
        self._xml_close()

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_properties(self):
        # Write the <Properties> element.
        schema = 'http://schemas.openxmlformats.org/officeDocument/2006/'
        xmlns = schema + 'extended-properties'
        xmlns_vt = schema + 'docPropsVTypes'

        attributes = [
            ('xmlns', xmlns),
            ('xmlns:vt', xmlns_vt),
        ]

        self._xml_start_tag('Properties', attributes)

    def _write_application(self):
        # Write the <Application> element.
        self._xml_data_element('Application', 'Microsoft Excel')

    def _write_doc_security(self):
        # Write the <DocSecurity> element.
        self._xml_data_element('DocSecurity', '0')

    def _write_scale_crop(self):
        # Write the <ScaleCrop> element.
        self._xml_data_element('ScaleCrop', 'false')

    def _write_heading_pairs(self):
        # Write the <HeadingPairs> element.
        self._xml_start_tag('HeadingPairs')
        self._write_vt_vector('variant', self.heading_pairs)
        self._xml_end_tag('HeadingPairs')

    def _write_titles_of_parts(self):
        # Write the <TitlesOfParts> element.
        parts_data = []

        self._xml_start_tag('TitlesOfParts')

        for part_name in self.part_names:
            parts_data.append(('lpstr', part_name))

        self._write_vt_vector('lpstr', parts_data)

        self._xml_end_tag('TitlesOfParts')

    def _write_vt_vector(self, base_type, vector_data):
        # Write the <vt:vector> element.
        attributes = [
            ('size', len(vector_data)),
            ('baseType', base_type),
        ]

        self._xml_start_tag('vt:vector', attributes)

        for vt_data in vector_data:
            if base_type == 'variant':
                self._xml_start_tag('vt:variant')

            self._write_vt_data(vt_data)

            if base_type == 'variant':
                self._xml_end_tag('vt:variant')

        self._xml_end_tag('vt:vector')

    def _write_vt_data(self, vt_data):
        # Write the <vt:*> elements such as <vt:lpstr> and <vt:if>.
        self._xml_data_element("vt:%s" % vt_data[0], vt_data[1])

    def _write_company(self):
        company = self.properties.get('company', '')

        self._xml_data_element('Company', encode_utf8(company))

    def _write_manager(self):
        # Write the <Manager> element.
        if not 'manager' in self.properties:
            return

        manager = encode_utf8(self.properties['manager'])

        self._xml_data_element('Manager', manager)

    def _write_links_up_to_date(self):
        # Write the <LinksUpToDate> element.
        self._xml_data_element('LinksUpToDate', 'false')

    def _write_shared_doc(self):
        # Write the <SharedDoc> element.
        self._xml_data_element('SharedDoc', 'false')

    def _write_hyperlinks_changed(self):
        # Write the <HyperlinksChanged> element.
        self._xml_data_element('HyperlinksChanged', 'false')

    def _write_app_version(self):
        # Write the <AppVersion> element.
        self._xml_data_element('AppVersion', '12.0000')

########NEW FILE########
__FILENAME__ = chart
###############################################################################
#
# Chart - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#
import re
from warnings import warn

from . import xmlwriter
from .utility import xl_color
from .utility import xl_rowcol_to_cell
from .utility import encode_utf8


class Chart(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Chart file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, options=None):
        """
        Constructor.

        """

        super(Chart, self).__init__()

        self.subtype = None
        self.sheet_type = 0x0200
        self.orientation = 0x0
        self.series = []
        self.embedded = 0
        self.id = ''
        self.series_index = 0
        self.style_id = 2
        self.axis_ids = []
        self.axis2_ids = []
        self.cat_has_num_fmt = 0
        self.requires_category = 0
        self.legend_position = 'right'
        self.cat_axis_position = 'b'
        self.val_axis_position = 'l'
        self.formula_ids = {}
        self.formula_data = []
        self.horiz_cat_axis = 0
        self.horiz_val_axis = 1
        self.protection = 0
        self.chartarea = {}
        self.plotarea = {}
        self.x_axis = {}
        self.y_axis = {}
        self.y2_axis = {}
        self.x2_axis = {}
        self.chart_name = ''
        self.show_blanks = 'gap'
        self.show_hidden = 0
        self.show_crosses = 1
        self.width = 480
        self.height = 288
        self.x_scale = 1
        self.y_scale = 1
        self.x_offset = 0
        self.y_offset = 0
        self.table = None
        self.title_formula = None
        self.title_name = None
        self.cross_between = None
        self.default_marker = None
        self.series_gap = None
        self.series_overlap = None
        self.drop_lines = None
        self.hi_low_lines = None
        self.up_down_bars = None
        self.legend_delete_series = None
        self.smooth_allowed = False

        self._set_default_properties()

    def add_series(self, options):
        """
        Add a data series to a chart.

        Args:
            options:  A dictionary of chart series options.

        Returns:
            Nothing.

        """
        # Add a series and it's properties to a chart.

        # Check that the required input has been specified.
        if not 'values' in options:
            warn("Must specify 'values' in add_series()")
            return

        if self.requires_category and not 'categories' in options:
            warn("Must specify 'categories' in add_series() "
                 "for this chart type")

        # Convert list into a formula string.
        values = self._list_to_formula(options.get('values'))
        categories = self._list_to_formula(options.get('categories'))

        # Switch name and name_formula parameters if required.
        name, name_formula = self._process_names(options.get('name'),
                                                 options.get('name_formula'))

        # Get an id for the data equivalent to the range formula.
        cat_id = self._get_data_id(categories, options.get('categories_data'))
        val_id = self._get_data_id(values, options.get('values_data'))
        name_id = self._get_data_id(name_formula, options.get('name_data'))

        # Set the line properties for the series.
        line = self._get_line_properties(options.get('line'))

        # Allow 'border' as a synonym for 'line' in bar/column style charts.
        if options.get('border'):
            line = self._get_line_properties(options['border'])

        # Set the fill properties for the series.
        fill = self._get_fill_properties(options.get('fill'))

        # Set the marker properties for the series.
        marker = self._get_marker_properties(options.get('marker'))

        # Set the trendline properties for the series.
        trendline = self._get_trendline_properties(options.get('trendline'))

        # Set the line smooth property for the series.
        smooth = options.get('smooth')

        # Set the error bars properties for the series.
        y_error_bars = self._get_error_bars_props(options.get('y_error_bars'))
        x_error_bars = self._get_error_bars_props(options.get('x_error_bars'))

        error_bars = {'x_error_bars': x_error_bars,
                      'y_error_bars': y_error_bars}

        # Set the point properties for the series.
        points = self._get_points_properties(options.get('points'))

        # Set the labels properties for the series.
        labels = self._get_labels_properties(options.get('data_labels'))

        # Set the "invert if negative" fill property.
        invert_if_neg = options.get('invert_if_negative', False)

        # Set the gap for Bar/Column charts.
        if options.get('gap') is not None:
            self.series_gap = options['gap']

        # Set the overlap for Bar/Column charts.
        if options.get('overlap'):
            self.series_overlap = options['overlap']

        # Set the secondary axis properties.
        x2_axis = options.get('x2_axis')
        y2_axis = options.get('y2_axis')

        # Add the user supplied data to the internal structures.
        series = {
            'values': values,
            'categories': categories,
            'name': name,
            'name_formula': name_formula,
            'name_id': name_id,
            'val_data_id': val_id,
            'cat_data_id': cat_id,
            'line': line,
            'fill': fill,
            'marker': marker,
            'trendline': trendline,
            'labels': labels,
            'invert_if_neg': invert_if_neg,
            'x2_axis': x2_axis,
            'y2_axis': y2_axis,
            'points': points,
            'error_bars': error_bars,
            'smooth': smooth
        }

        self.series.append(series)

    def set_x_axis(self, options):
        """
        Set the chart X axis options.

        Args:
            options:  A dictionary of axis options.

        Returns:
            Nothing.

        """
        axis = self._convert_axis_args(self.x_axis, options)

        self.x_axis = axis

    def set_y_axis(self, options):
        """
        Set the chart Y axis options.

        Args:
            options: A dictionary of axis options.

        Returns:
            Nothing.

        """
        axis = self._convert_axis_args(self.y_axis, options)

        self.y_axis = axis

    def set_x2_axis(self, options):
        """
        Set the chart secondary X axis options.

        Args:
            options: A dictionary of axis options.

        Returns:
            Nothing.

        """
        axis = self._convert_axis_args(self.x2_axis, options)

        self.x2_axis = axis

    def set_y2_axis(self, options):
        """
        Set the chart secondary Y axis options.

        Args:
            options: A dictionary of axis options.

        Returns:
            Nothing.

        """
        axis = self._convert_axis_args(self.y2_axis, options)

        self.y2_axis = axis

    def set_title(self, options):
        """
        Set the chart title options.

        Args:
            options: A dictionary of chart title options.

        Returns:
            Nothing.

        """
        name, name_formula = self._process_names(options.get('name'),
                                                 options.get('name_formula'))

        data_id = self._get_data_id(name_formula, options.get('data'))

        self.title_name = name
        self.title_formula = name_formula
        self.title_data_id = data_id

        # Set the font properties if present.
        self.title_font = self._convert_font_args(options.get('name_font'))

    def set_legend(self, options):
        """
        Set the chart legend options.

        Args:
            options: A dictionary of chart legend options.

        Returns:
            Nothing.
        """
        self.legend_position = options.get('position', 'right')
        self.legend_delete_series = options.get('delete_series')

    def set_plotarea(self, options):
        """
        Set the chart plot area options.

        Args:
            options: A dictionary of chart plot area options.

        Returns:
            Nothing.
        """
        # Convert the user defined properties to internal properties.
        self.plotarea = self._get_area_properties(options)

    def set_chartarea(self, options):
        """
        Set the chart area options.

        Args:
            options: A dictionary of chart area options.

        Returns:
            Nothing.
        """
        # Convert the user defined properties to internal properties.
        self.chartarea = self._get_area_properties(options)

    def set_style(self, style_id):
        """
        Set the chart style type.

        Args:
            style_id: An int representing the chart style.

        Returns:
            Nothing.
        """
        # Set one of the 42 built-in Excel chart styles. The default is 2.
        if style_id is None:
            style_id = 2

        if style_id < 0 or style_id > 42:
            style_id = 2

        self.style_id = style_id

    def show_blanks_as(self, option):
        """
        Set the option for displaying blank data in a chart.

        Args:
            option: A string representing the display option.

        Returns:
            Nothing.
        """
        if not option:
            return

        valid_options = {
            'gap': 1,
            'zero': 1,
            'span': 1,
        }

        if not option in valid_options:
            warn("Unknown show_blanks_as() option '%s'" % option)
            return

        self.show_blanks = option

    def show_hidden_data(self):
        """
        Display data on charts from hidden rows or columns.

        Args:
            option: A string representing the display option.

        Returns:
            Nothing.
        """
        self.show_hidden = 1

    def set_size(self, options):
        """
        Set size or scale of the chart.

        Args:
            options: A dictionary of chart size options.

        Returns:
            Nothing.
        """

        # Set dimensions or scale for the chart.
        self.width = options.get('width', self.width)
        self.height = options.get('height', self.height)
        self.x_scale = options.get('x_scale', 1)
        self.y_scale = options.get('y_scale', 1)
        self.x_offset = options.get('x_offset', 0)
        self.x_offset = options.get('y_offset', 0)

    def set_table(self, options=None):
        """
        Set properties for an axis data table.

        Args:
            options: A dictionary of axis table options.

        Returns:
            Nothing.

        """
        if options is None:
            options = {}

        table = {}

        table['horizontal'] = options.get('horizontal', 1)
        table['vertical'] = options.get('vertical', 1)
        table['outline'] = options.get('outline', 1)
        table['show_keys'] = options.get('show_keys', 0)

        self.table = table

    def set_up_down_bars(self, options=None):
        """
        Set properties for the chart up-down bars.

        Args:
            options: A dictionary of options.

        Returns:
            Nothing.

        """
        if options is None:
            options = {}

        # Defaults.
        up_line = None
        up_fill = None
        down_line = None
        down_fill = None

        # Set properties for 'up' bar.
        if options.get('up'):
            if 'border' in options['up']:
                # Map border to line.
                up_line = self._get_line_properties(options['up']['border'])

            if 'line' in options['up']:
                up_line = self._get_line_properties(options['up']['line'])

            if 'fill' in options['up']:
                up_fill = self._get_line_properties(options['up']['fill'])

        # Set properties for 'down' bar.
        if options.get('down'):
            if 'border' in options['down']:
                # Map border to line.
                down_line = \
                    self._get_line_properties(options['down']['border'])

            if 'line' in options['down']:
                down_line = self._get_line_properties(options['down']['line'])

            if 'fill' in options['down']:
                down_fill = self._get_line_properties(options['down']['fill'])

        self.up_down_bars = {'up': {'line': up_line,
                                    'fill': up_fill,
                                    },
                             'down': {'line': down_line,
                                      'fill': down_fill,
                                      },
                             }

    def set_drop_lines(self, options=None):
        """
        Set properties for the chart drop lines.

        Args:
            options: A dictionary of options.

        Returns:
            Nothing.

        """
        if options is None:
            options = {}

        line = self._get_line_properties(options.get('line'))
        fill = self._get_fill_properties(options.get('fill'))

        self.drop_lines = {'line': line, 'fill': fill}

    def set_high_low_lines(self, options=None):
        """
        Set properties for the chart high-low lines.

        Args:
            options: A dictionary of options.

        Returns:
            Nothing.

        """
        if options is None:
            options = {}

        line = self._get_line_properties(options.get('line'))
        fill = self._get_fill_properties(options.get('fill'))

        self.hi_low_lines = {'line': line, 'fill': fill}

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        # Write the c:chartSpace element.
        self._write_chart_space()

        # Write the c:lang element.
        self._write_lang()

        # Write the c:style element.
        self._write_style()

        # Write the c:protection element.
        self._write_protection()

        # Write the c:chart element.
        self._write_chart()

        # Write the c:spPr element for the chartarea formatting.
        self._write_sp_pr(self.chartarea)

        # Write the c:printSettings element.
        if self.embedded:
            self._write_print_settings()

        # Close the worksheet tag.
        self._xml_end_tag('c:chartSpace')
        # Close the file.
        self._xml_close()

    def _convert_axis_args(self, axis, user_options):
        # Convert user defined axis values into private hash values.
        options = axis['defaults'].copy()
        options.update(user_options)

        name, name_formula = self._process_names(options.get('name'),
                                                 options.get('name_formula'))

        data_id = self._get_data_id(name_formula, options.get('data'))

        axis = {
            'defaults': axis['defaults'],
            'name': name,
            'formula': name_formula,
            'data_id': data_id,
            'reverse': options.get('reverse'),
            'min': options.get('min'),
            'max': options.get('max'),
            'minor_unit': options.get('minor_unit'),
            'major_unit': options.get('major_unit'),
            'minor_unit_type': options.get('minor_unit_type'),
            'major_unit_type': options.get('major_unit_type'),
            'log_base': options.get('log_base'),
            'crossing': options.get('crossing'),
            'position': options.get('position'),
            'label_position': options.get('label_position'),
            'num_format': options.get('num_format'),
            'num_format_linked': options.get('num_format_linked'),
        }

        # Encode any string options passed by the user.
        axis['num_format'] = encode_utf8(axis['num_format'])

        if 'visible' in options:
            axis['visible'] = options.get('visible')
        else:
            axis['visible'] = 1

        # Map major_gridlines properties.
        if (options.get('major_gridlines')
                and options['major_gridlines']['visible']):
            axis['major_gridlines'] = \
                self._get_gridline_properties(options['major_gridlines'])

        # Map minor_gridlines properties.
        if (options.get('minor_gridlines')
                and options['minor_gridlines']['visible']):
            axis['minor_gridlines'] = \
                self._get_gridline_properties(options['minor_gridlines'])

        # Only use the first letter of bottom, top, left or right.
        if axis.get('position'):
            axis['position'] = axis['position'].lower()[0]

        # Set the font properties if present.
        axis['num_font'] = self._convert_font_args(options.get('num_font'))
        axis['name_font'] = self._convert_font_args(options.get('name_font'))

        return axis

    def _convert_font_args(self, options):
        # Convert user defined font values into private dict values.
        if not options:
            return

        font = {
            'name': options.get('name'),
            'color': options.get('color'),
            'size': options.get('size'),
            'bold': options.get('bold'),
            'italic': options.get('italic'),
            'underline': options.get('underline'),
            'pitch_family': options.get('pitch_family'),
            'charset': options.get('charset'),
            'baseline': options.get('baseline', 0),
            'rotation': options.get('rotation'),
        }

        # Convert font size units.
        if font['size']:
            font['size'] *= 100

        # Convert rotation into 60,000ths of a degree.
        if (font['rotation']):
            font['rotation'] = 60000 * int(font['rotation'])

        return font

    def _list_to_formula(self, data):
        # Convert and list of row col values to a range formula.

        # If it isn't an array ref it is probably a formula already.
        if type(data) is not list:
            return data

        sheet = data[0]
        range1 = xl_rowcol_to_cell(data[1], data[2], True, True)
        range2 = xl_rowcol_to_cell(data[3], data[4], True, True)

        return sheet + '!' + range1 + ':' + range2

    def _process_names(self, name, name_formula):
        # Switch name and name_formula parameters if required.

        # Name looks like a formula, use it to set name_formula.
        if name is not None and re.match(r'^=?[^!]+!', name):
            name_formula = name
            name = ''

        name = encode_utf8(name)
        name_formula = encode_utf8(name_formula)

        return name, name_formula

    def _get_data_type(self, data):
        # Find the overall type of the data associated with a series.

        # Check for no data in the series.
        if data is None or len(data) == 0:
            return 'none'

        # Determine if data is numeric or strings.
        for token in (data):
            if token is None:
                continue

            try:
                float(token)
            except ValueError:
                # Not a number. Assume entire data series is string data.
                return 'str'

        # The series data was all numeric.
        return 'num'

    def _get_data_id(self, formula, data):
        # Assign an id to a each unique series formula or title/axis formula.
        # Repeated formulas such as for categories get the same id. If the
        # series or title has user specified data associated with it then
        # that is also stored. This data is used to populate cached Excel
        # data when creating a chart. If there is no user defined data then
        # it will be populated by the parent Workbook._add_chart_data().

        # Ignore series without a range formula.
        if not formula:
            return

        # Strip the leading '=' from the formula.
        if formula.startswith('='):
            formula = formula.lstrip('=')

        # Store the data id in a hash keyed by the formula and store the data
        # in a separate array with the same id.
        if not formula in self.formula_ids:
            # Haven't seen this formula before.
            formula_id = len(self.formula_data)

            self.formula_data.append(data)
            self.formula_ids[formula] = formula_id
        else:
            # Formula already seen. Return existing id.
            formula_id = self.formula_ids[formula]

            # Store user defined data if it isn't already there.
            if self.formula_data[formula_id] is None:
                self.formula_data[formula_id] = data

        return formula_id

    def _get_color(self, color):
        # Convert the user specified colour to an RGB colour.
        rgb_color = xl_color(color)

        # Remove leading FF from RGB colour for charts.
        rgb_color = re.sub(r'^FF', '', rgb_color)

        return rgb_color

    def _get_line_properties(self, line):
        # Convert user line properties to the structure required internally.

        if not line:
            return {'defined': False}

        dash_types = {
            'solid': 'solid',
            'round_dot': 'sysDot',
            'square_dot': 'sysDash',
            'dash': 'dash',
            'dash_dot': 'dashDot',
            'long_dash': 'lgDash',
            'long_dash_dot': 'lgDashDot',
            'long_dash_dot_dot': 'lgDashDotDot',
            'dot': 'dot',
            'system_dash_dot': 'sysDashDot',
            'system_dash_dot_dot': 'sysDashDotDot',
        }

        # Check the dash type.
        dash_type = line.get('dash_type')

        if dash_type is not None:
            if dash_type in dash_types:
                line['dash_type'] = dash_types[dash_type]
            else:
                warn("Unknown dash type '%s'" % dash_type)
                return

        line['defined'] = True

        return line

    def _get_fill_properties(self, fill):
        # Convert user fill properties to the structure required internally.

        if not fill:
            return {'defined': False}

        fill['defined'] = True

        return fill

    def _get_marker_properties(self, marker):
        # Convert user marker properties to the structure required internally.

        if not marker:
            return

        types = {
            'automatic': 'automatic',
            'none': 'none',
            'square': 'square',
            'diamond': 'diamond',
            'triangle': 'triangle',
            'x': 'x',
            'star': 'start',
            'dot': 'dot',
            'short_dash': 'dot',
            'dash': 'dash',
            'long_dash': 'dash',
            'circle': 'circle',
            'plus': 'plus',
            'picture': 'picture',
        }

        # Check for valid types.
        marker_type = marker.get('type')

        if marker_type is not None:
            if marker_type == 'automatic':
                marker['automatic'] = 1

            if marker_type in types:
                marker['type'] = types[marker_type]
            else:
                warn("Unknown marker type '%s" % marker_type)
                return

        # Set the line properties for the marker..
        line = self._get_line_properties(marker.get('line'))

        # Allow 'border' as a synonym for 'line'.
        if 'border' in marker:
            line = self._get_line_properties(marker['border'])

        # Set the fill properties for the marker.
        fill = self._get_fill_properties(marker.get('fill'))

        marker['line'] = line
        marker['fill'] = fill

        return marker

    def _get_trendline_properties(self, trendline):
        # Convert user trendline properties to structure required internally.

        if not trendline:
            return

        types = {
            'exponential': 'exp',
            'linear': 'linear',
            'log': 'log',
            'moving_average': 'movingAvg',
            'polynomial': 'poly',
            'power': 'power',
        }

        # Check the trendline type.
        trend_type = trendline.get('type')

        if trend_type in types:
            trendline['type'] = types[trend_type]
        else:
            warn("Unknown trendline type '%s'" % trend_type)
            return

        # Set the line properties for the trendline..
        line = self._get_line_properties(trendline.get('line'))

        # Allow 'border' as a synonym for 'line'.
        if 'border' in trendline:
            line = self._get_line_properties(trendline['border'])

        # Set the fill properties for the trendline.
        fill = self._get_fill_properties(trendline.get('fill'))

        trendline['line'] = line
        trendline['fill'] = fill

        return trendline

    def _get_error_bars_props(self, options):
        # Convert user error bars properties to structure required internally.
        if not options:
            return

        # Default values.
        error_bars = {
            'type': 'fixedVal',
            'value': 1,
            'endcap': 1,
            'direction': 'both'
        }

        types = {
            'fixed': 'fixedVal',
            'percentage': 'percentage',
            'standard_deviation': 'stdDev',
            'standard_error': 'stdErr',
        }

        # Check the error bars type.
        error_type = options['type']

        if error_type in types:
            error_bars['type'] = types[error_type]
        else:
            warn("Unknown error bars type '%s" % error_type)
            return

        # Set the value for error types that require it.
        if 'value' in options:
            error_bars['value'] = options['value']

        # Set the end-cap style.
        if 'end_style' in options:
            error_bars['endcap'] = options['end_style']

        # Set the error bar direction.
        if 'direction' in options:
            if options['direction'] == 'minus':
                error_bars['direction'] = 'minus'
            elif options['direction'] == 'plus':
                error_bars['direction'] = 'plus'
            else:
                # Default to 'both'.
                pass

        # Set the line properties for the error bars.
        error_bars['line'] = self._get_line_properties(options.get('line'))
        error_bars['fill'] = self._get_line_properties(options.get('fill'))

        return error_bars

    def _get_gridline_properties(self, options):
        # Convert user gridline properties to structure required internally.
        gridline = {}

        # Set the visible property for the gridline.
        gridline['visible'] = options.get('visible')

        # Set the line properties for the gridline.
        gridline['line'] = self._get_line_properties(options.get('line'))
        gridline['fill'] = self._get_line_properties(options.get('fill'))

        return gridline

    def _get_labels_properties(self, labels):
        # Convert user labels properties to the structure required internally.

        if not labels:
            return None

        # Map user defined label positions to Excel positions.
        position = labels.get('position')

        if position:
            positions = {
                'center': 'ctr',
                'right': 'r',
                'left': 'l',
                'top': 't',
                'above': 't',
                'bottom': 'b',
                'below': 'b',
                'inside_end': 'inEnd',
                'outside_end': 'outEnd',
                'best_fit': 'bestFit',
            }

            if position in positions:
                labels['position'] = positions[position]
            else:
                warn("Unknown label position '%s'" % position)
                return

        return labels

    def _get_area_properties(self, options):
        # Convert user area properties to the structure required internally.
        area = {}

        # Set the line properties for the chartarea.
        line = self._get_line_properties(options.get('line'))

        # Allow 'border' as a synonym for 'line'.
        if options.get('border'):
            line = self._get_line_properties(options['border'])

        # Set the fill properties for the chartarea.
        fill = self._get_fill_properties(options.get('fill'))

        area['line'] = line
        area['fill'] = fill

        return area

    def _get_points_properties(self, user_points):
        # Convert user points properties to structure required internally.
        points = []

        if not user_points:
            return

        for user_point in (user_points):
            point = {}

            if user_point is not None:

                # Set the line properties for the point.
                line = self._get_line_properties(user_point.get('line'))

                # Allow 'border' as a synonym for 'line'.
                if 'border' in user_point:
                    line = self._get_line_properties(user_point['border'])

                # Set the fill properties for the chartarea.
                fill = self._get_fill_properties(user_point.get('fill'))

                point['line'] = line
                point['fill'] = fill

            points.append(point)

        return points

    def _get_primary_axes_series(self):
        # Returns series which use the primary axes.
        primary_axes_series = []

        for series in (self.series):
            if not series['y2_axis']:
                primary_axes_series.append(series)

        return primary_axes_series

    def _get_secondary_axes_series(self):
        # Returns series which use the secondary axes.
        secondary_axes_series = []

        for series in (self.series):
            if series['y2_axis']:
                secondary_axes_series.append(series)

        return secondary_axes_series

    def _add_axis_ids(self, args):
        # Add unique ids for primary or secondary axes
        chart_id = 1 + int(self.id)
        axis_count = 1 + len(self.axis2_ids) + len(self.axis_ids)

        id1 = '5%03d%04d' % (chart_id, axis_count)
        id2 = '5%03d%04d' % (chart_id, axis_count + 1)

        if args['primary_axes']:
            self.axis_ids.append(id1)
            self.axis_ids.append(id2)

        if not args['primary_axes']:
            self.axis2_ids.append(id1)
            self.axis2_ids.append(id2)

    def _get_font_style_attributes(self, font):
        # _get_font_style_attributes.
        attributes = []

        if not font:
            return attributes

        if font['size']:
            attributes.append(('sz', font['size']))

        if font['bold'] is not None:
            attributes.append(('b', 0 + font['bold']))

        if font['italic'] is not None:
            attributes.append(('i', 0 + font['italic']))

        if font['underline'] is not None:
            attributes.append(('u', 'sng'))

        attributes.append(('baseline', font['baseline']))

        return attributes

    def _get_font_latin_attributes(self, font):
        # _get_font_latin_attributes.
        attributes = []

        if not font:
            return attributes

        if font['name'] is not None:
            attributes.append(('typeface', font['name']))

        if font['pitch_family'] is not None:
            attributes.append(('pitchFamily', font['pitch_family']))

        if font['charset'] is not None:
            attributes.append(('charset', font['charset']))

        return attributes

    def _set_default_properties(self):
        # Setup the default properties for a chart.

        self.x_axis['defaults'] = {
            'num_format': 'General',
            'major_gridlines': {'visible': 0}
        }

        self.y_axis['defaults'] = {
            'num_format': 'General',
            'major_gridlines': {'visible': 1}
        }

        self.x2_axis['defaults'] = {
            'num_format': 'General',
            'label_position': 'none',
            'crossing': 'max',
            'visible': 0
        }

        self.y2_axis['defaults'] = {
            'num_format': 'General',
            'major_gridlines': {'visible': 0},
            'position': 'right',
            'visible': 1
        }

        self.set_x_axis({})
        self.set_y_axis({})

        self.set_x2_axis({})
        self.set_y2_axis({})

    def _set_embedded_config_data(self):
        # Setup the default configuration data for an embedded chart.
        self.embedded = 1

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_chart_space(self):
        # Write the <c:chartSpace> element.
        schema = 'http://schemas.openxmlformats.org/'
        xmlns_c = schema + 'drawingml/2006/chart'
        xmlns_a = schema + 'drawingml/2006/main'
        xmlns_r = schema + 'officeDocument/2006/relationships'

        attributes = [
            ('xmlns:c', xmlns_c),
            ('xmlns:a', xmlns_a),
            ('xmlns:r', xmlns_r),
        ]

        self._xml_start_tag('c:chartSpace', attributes)

    def _write_lang(self):
        # Write the <c:lang> element.
        val = 'en-US'

        attributes = [('val', val)]

        self._xml_empty_tag('c:lang', attributes)

    def _write_style(self):
        # Write the <c:style> element.
        style_id = self.style_id

        # Don't write an element for the default style, 2.
        if style_id == 2:
            return

        attributes = [('val', style_id)]

        self._xml_empty_tag('c:style', attributes)

    def _write_chart(self):
        # Write the <c:chart> element.
        self._xml_start_tag('c:chart')

        # Write the chart title elements.
        if self.title_formula is not None:
            self._write_title_formula(self.title_formula, self.title_data_id,
                                      None, self.title_font)
        elif self.title_name is not None:
            self._write_title_rich(self.title_name, None, self.title_font)

        # Write the c:plotArea element.
        self._write_plot_area()

        # Write the c:legend element.
        self._write_legend()

        # Write the c:plotVisOnly element.
        self._write_plot_vis_only()

        # Write the c:dispBlanksAs element.
        self._write_disp_blanks_as()

        self._xml_end_tag('c:chart')

    def _write_disp_blanks_as(self):
        # Write the <c:dispBlanksAs> element.
        val = self.show_blanks

        # Ignore the default value.
        if val == 'gap':
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:dispBlanksAs', attributes)

    def _write_plot_area(self):
        # Write the <c:plotArea> element.
        self._xml_start_tag('c:plotArea')

        # Write the c:layout element.
        self._write_layout()

        # Write  subclass chart type elements for primary and secondary axes.
        self._write_chart_type({'primary_axes': True})
        self._write_chart_type({'primary_axes': False})

        # Write c:catAx and c:valAx elements for series using primary axes.
        self._write_cat_axis({'x_axis': self.x_axis,
                              'y_axis': self.y_axis,
                              'axis_ids': self.axis_ids
                              })

        self._write_val_axis({'x_axis': self.x_axis,
                              'y_axis': self.y_axis,
                              'axis_ids': self.axis_ids
                              })

        # Write c:valAx and c:catAx elements for series using secondary axes.
        self._write_val_axis({'x_axis': self.x2_axis,
                              'y_axis': self.y2_axis,
                              'axis_ids': self.axis2_ids
                              })

        self._write_cat_axis({'x_axis': self.x2_axis,
                              'y_axis': self.y2_axis,
                              'axis_ids': self.axis2_ids
                              })

        # Write the c:dTable element.
        self._write_d_table()

        # Write the c:spPr element for the plotarea formatting.
        self._write_sp_pr(self.plotarea)

        self._xml_end_tag('c:plotArea')

    def _write_layout(self):
        # Write the <c:layout> element.
        self._xml_empty_tag('c:layout')

    def _write_chart_type(self, options):
        # Write the chart type element. This method should be overridden
        # by the subclasses.
        return

    def _write_grouping(self, val):
        # Write the <c:grouping> element.
        attributes = [('val', val)]

        self._xml_empty_tag('c:grouping', attributes)

    def _write_series(self, series):
        # Write the series elements.
        self._write_ser(series)

    def _write_ser(self, series):
        # Write the <c:ser> element.
        index = self.series_index
        self.series_index += 1

        self._xml_start_tag('c:ser')

        # Write the c:idx element.
        self._write_idx(index)

        # Write the c:order element.
        self._write_order(index)

        # Write the series name.
        self._write_series_name(series)

        # Write the c:spPr element.
        self._write_sp_pr(series)

        # Write the c:marker element.
        self._write_marker(series['marker'])

        # Write the c:invertIfNegative element.
        self._write_c_invert_if_negative(series['invert_if_neg'])

        # Write the c:dPt element.
        self._write_d_pt(series['points'])

        # Write the c:dLbls element.
        self._write_d_lbls(series['labels'])

        # Write the c:trendline element.
        self._write_trendline(series['trendline'])

        # Write the c:errBars element.
        self._write_error_bars(series['error_bars'])

        # Write the c:cat element.
        self._write_cat(series)

        # Write the c:val element.
        self._write_val(series)

        # Write the c:smooth element.
        if self.smooth_allowed:
            self._write_c_smooth(series['smooth'])

        self._xml_end_tag('c:ser')

    def _write_idx(self, val):
        # Write the <c:idx> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:idx', attributes)

    def _write_order(self, val):
        # Write the <c:order> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:order', attributes)

    def _write_series_name(self, series):
        # Write the series name.

        if series['name_formula'] is not None:
            self._write_tx_formula(series['name_formula'], series['name_id'])
        elif series['name'] is not None:
            self._write_tx_value(series['name'])

    def _write_c_smooth(self, smooth):
        # Write the <c:smooth> element.

        if smooth:
            self._xml_empty_tag('c:smooth', [('val', '1')])

    def _write_cat(self, series):
        # Write the <c:cat> element.
        formula = series['categories']
        data_id = series['cat_data_id']
        data = None

        if data_id is not None:
            data = self.formula_data[data_id]

        # Ignore <c:cat> elements for charts without category values.
        if not formula:
            return

        self._xml_start_tag('c:cat')

        # Check the type of cached data.
        cat_type = self._get_data_type(data)

        if cat_type == 'str':
            self.cat_has_num_fmt = 0
            # Write the c:numRef element.
            self._write_str_ref(formula, data, cat_type)
        else:
            self.cat_has_num_fmt = 1
            # Write the c:numRef element.
            self._write_num_ref(formula, data, cat_type)

        self._xml_end_tag('c:cat')

    def _write_val(self, series):
        # Write the <c:val> element.
        formula = series['values']
        data_id = series['val_data_id']
        data = self.formula_data[data_id]

        self._xml_start_tag('c:val')

        # Unlike Cat axes data should only be numeric.
        # Write the c:numRef element.
        self._write_num_ref(formula, data, 'num')

        self._xml_end_tag('c:val')

    def _write_num_ref(self, formula, data, ref_type):
        # Write the <c:numRef> element.
        self._xml_start_tag('c:numRef')

        # Write the c:f element.
        self._write_series_formula(formula)

        if ref_type == 'num':
            # Write the c:numCache element.
            self._write_num_cache(data)
        elif ref_type == 'str':
            # Write the c:strCache element.
            self._write_str_cache(data)

        self._xml_end_tag('c:numRef')

    def _write_str_ref(self, formula, data, ref_type):
        # Write the <c:strRef> element.

        self._xml_start_tag('c:strRef')

        # Write the c:f element.
        self._write_series_formula(formula)

        if ref_type == 'num':
            # Write the c:numCache element.
            self._write_num_cache(data)
        elif ref_type == 'str':
            # Write the c:strCache element.
            self._write_str_cache(data)

        self._xml_end_tag('c:strRef')

    def _write_series_formula(self, formula):
        # Write the <c:f> element.

        # Strip the leading '=' from the formula.
        if formula.startswith('='):
            formula = formula.lstrip('=')

        self._xml_data_element('c:f', formula)

    def _write_axis_ids(self, args):
        # Write the <c:axId> elements for the primary or secondary axes.

        # Generate the axis ids.
        self._add_axis_ids(args)

        if args['primary_axes']:
            # Write the axis ids for the primary axes.
            self._write_axis_id(self.axis_ids[0])
            self._write_axis_id(self.axis_ids[1])
        else:
            # Write the axis ids for the secondary axes.
            self._write_axis_id(self.axis2_ids[0])
            self._write_axis_id(self.axis2_ids[1])

    def _write_axis_id(self, val):
        # Write the <c:axId> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:axId', attributes)

    def _write_cat_axis(self, args):
        # Write the <c:catAx> element. Usually the X axis.
        x_axis = args['x_axis']
        y_axis = args['y_axis']
        axis_ids = args['axis_ids']

        # If there are no axis_ids then we don't need to write this element.
        if axis_ids is None or not len(axis_ids):
            return

        position = self.cat_axis_position
        horiz = self.horiz_cat_axis

        # Overwrite the default axis position with a user supplied value.
        if x_axis.get('position'):
            position = x_axis['position']

        self._xml_start_tag('c:catAx')

        self._write_axis_id(axis_ids[0])

        # Write the c:scaling element.
        self._write_scaling(x_axis.get('reverse'),
                            None,
                            None,
                            None)

        if not x_axis.get('visible'):
            self._write_delete(1)

        # Write the c:axPos element.
        self._write_axis_pos(position, y_axis.get('reverse'))

        # Write the c:majorGridlines element.
        self._write_major_gridlines(x_axis.get('major_gridlines'))

        # Write the c:minorGridlines element.
        self._write_minor_gridlines(x_axis.get('minor_gridlines'))

        # Write the axis title elements.
        if x_axis['formula'] is not None:
            self._write_title_formula(x_axis['formula'],
                                      x_axis['data_id'],
                                      horiz,
                                      x_axis['name_font'])
        elif x_axis['name'] is not None:
            self._write_title_rich(x_axis['name'],
                                   horiz,
                                   x_axis['name_font'])

        # Write the c:numFmt element.
        self._write_cat_number_format(x_axis)

        # Write the c:majorTickMark element.
        self._write_major_tick_mark(x_axis.get('major_tick_mark'))

        # Write the c:tickLblPos element.
        self._write_tick_label_pos(x_axis.get('label_position'))

        # Write the axis font elements.
        self._write_axis_font(x_axis.get('num_font'))

        # Write the c:crossAx element.
        self._write_cross_axis(axis_ids[1])

        if self.show_crosses or x_axis.get('visible'):

            # Note, the category crossing comes from the value axis.
            if (y_axis.get('crossing') is None
                    or y_axis.get('crossing') == 'max'):

                # Write the c:crosses element.
                self._write_crosses(y_axis.get('crossing'))
            else:

                # Write the c:crossesAt element.
                self._write_c_crosses_at(y_axis.get('crossing'))

        # Write the c:auto element.
        self._write_auto(1)

        # Write the c:labelAlign element.
        self._write_label_align('ctr')

        # Write the c:labelOffset element.
        self._write_label_offset(100)

        self._xml_end_tag('c:catAx')

    def _write_val_axis(self, args):
        # Write the <c:valAx> element. Usually the Y axis.
        x_axis = args['x_axis']
        y_axis = args['y_axis']
        axis_ids = args['axis_ids']
        position = args.get('position', self.val_axis_position)
        horiz = self.horiz_val_axis

        # If there are no axis_ids then we don't need to write this element.
        if axis_ids is None or not len(axis_ids):
            return

        # Overwrite the default axis position with a user supplied value.
        position = y_axis.get('position') or position

        self._xml_start_tag('c:valAx')

        self._write_axis_id(axis_ids[1])

        # Write the c:scaling element.
        self._write_scaling(y_axis.get('reverse'),
                            y_axis.get('min'),
                            y_axis.get('max'),
                            y_axis.get('log_base'))

        if not y_axis.get('visible'):
            self._write_delete(1)

        # Write the c:axPos element.
        self._write_axis_pos(position, x_axis.get('reverse'))

        # Write the c:majorGridlines element.
        self._write_major_gridlines(y_axis.get('major_gridlines'))

        # Write the c:minorGridlines element.
        self._write_minor_gridlines(y_axis.get('minor_gridlines'))

        # Write the axis title elements.
        if y_axis['formula'] is not None:
            self._write_title_formula(y_axis['formula'],
                                      y_axis['data_id'],
                                      horiz,
                                      y_axis['name_font'])
        elif y_axis['name'] is not None:
            self._write_title_rich(y_axis['name'],
                                   horiz,
                                   y_axis.get('name_font'))

        # Write the c:numberFormat element.
        self._write_number_format(y_axis)

        # Write the c:majorTickMark element.
        self._write_major_tick_mark(y_axis.get('major_tick_mark'))

        # Write the c:tickLblPos element.
        self._write_tick_label_pos(y_axis.get('label_position'))

        # Write the axis font elements.
        self._write_axis_font(y_axis.get('num_font'))

        # Write the c:crossAx element.
        self._write_cross_axis(axis_ids[0])

        # Note, the category crossing comes from the value axis.
        if x_axis.get('crossing') is None or x_axis['crossing'] == 'max':

            # Write the c:crosses element.
            self._write_crosses(x_axis.get('crossing'))
        else:

            # Write the c:crossesAt element.
            self._write_c_crosses_at(x_axis.get('crossing'))

        # Write the c:crossBetween element.
        self._write_cross_between()

        # Write the c:majorUnit element.
        self._write_c_major_unit(y_axis.get('major_unit'))

        # Write the c:minorUnit element.
        self._write_c_minor_unit(y_axis.get('minor_unit'))

        self._xml_end_tag('c:valAx')

    def _write_cat_val_axis(self, args):
        # Write the <c:valAx> element. This is for the second valAx
        # in scatter plots. Usually the X axis.
        x_axis = args['x_axis']
        y_axis = args['y_axis']
        axis_ids = args['axis_ids']
        position = args['position'] or self.val_axis_position
        horiz = self.horiz_val_axis

        # If there are no axis_ids then we don't need to write this element.
        if axis_ids is None or not len(axis_ids):
            return

        # Overwrite the default axis position with a user supplied value.
        position = x_axis.get('position') or position

        self._xml_start_tag('c:valAx')

        self._write_axis_id(axis_ids[0])

        # Write the c:scaling element.
        self._write_scaling(x_axis.get('reverse'),
                            x_axis.get('min'),
                            x_axis.get('max'),
                            x_axis.get('log_base'))

        if not x_axis.get('visible'):
            self._write_delete(1)

        # Write the c:axPos element.
        self._write_axis_pos(position, y_axis.get('reverse'))

        # Write the c:majorGridlines element.
        self._write_major_gridlines(x_axis.get('major_gridlines'))

        # Write the c:minorGridlines element.
        self._write_minor_gridlines(x_axis.get('minor_gridlines'))

        # Write the axis title elements.
        if x_axis['formula'] is not None:
            self._write_title_formula(x_axis['formula'],
                                      y_axis['data_id'],
                                      horiz,
                                      x_axis['name_font'])
        elif x_axis['name'] is not None:
            self._write_title_rich(x_axis['name'],
                                   horiz,
                                   x_axis['name_font'])

        # Write the c:numberFormat element.
        self._write_number_format(x_axis)

        # Write the c:majorTickMark element.
        self._write_major_tick_mark(x_axis.get('major_tick_mark'))

        # Write the c:tickLblPos element.
        self._write_tick_label_pos(x_axis.get('label_position'))

        # Write the axis font elements.
        self._write_axis_font(x_axis.get('num_font'))

        # Write the c:crossAx element.
        self._write_cross_axis(axis_ids[1])

        # Note, the category crossing comes from the value axis.
        if y_axis.get('crossing') is None or y_axis['crossing'] == 'max':

            # Write the c:crosses element.
            self._write_crosses(y_axis.get('crossing'))
        else:

            # Write the c:crossesAt element.
            self._write_c_crosses_at(y_axis.get('crossing'))

        # Write the c:crossBetween element.
        self._write_cross_between()

        # Write the c:majorUnit element.
        self._write_c_major_unit(x_axis.get('major_unit'))

        # Write the c:minorUnit element.
        self._write_c_minor_unit(x_axis.get('minor_unit'))

        self._xml_end_tag('c:valAx')

    def _write_date_axis(self, args):
        # Write the <c:dateAx> element. Usually the X axis.
        x_axis = args['x_axis']
        y_axis = args['y_axis']
        axis_ids = args['axis_ids']

        # If there are no axis_ids then we don't need to write this element.
        if axis_ids is None or not len(axis_ids):
            return

        position = self.cat_axis_position

        # Overwrite the default axis position with a user supplied value.
        position = x_axis.get('position') or position

        self._xml_start_tag('c:dateAx')

        self._write_axis_id(axis_ids[0])

        # Write the c:scaling element.
        self._write_scaling(x_axis.get('reverse'),
                            x_axis.get('min'),
                            x_axis.get('max'),
                            x_axis.get('log_base'))

        if not x_axis.get('visible'):
            self._write_delete(1)

        # Write the c:axPos element.
        self._write_axis_pos(position, y_axis.get('reverse'))

        # Write the c:majorGridlines element.
        self._write_major_gridlines(x_axis.get('major_gridlines'))

        # Write the c:minorGridlines element.
        self._write_minor_gridlines(x_axis.get('minor_gridlines'))

        # Write the axis title elements.
        if x_axis['formula'] is not None:
            self._write_title_formula(x_axis['formula'],
                                      x_axis['data_id'],
                                      None,
                                      x_axis['name_font'])
        elif x_axis['name'] is not None:
            self._write_title_rich(x_axis['name'],
                                   None,
                                   x_axis['name_font'])

        # Write the c:numFmt element.
        self._write_number_format(x_axis)

        # Write the c:majorTickMark element.
        self._write_major_tick_mark(x_axis.get('major_tick_mark'))

        # Write the c:tickLblPos element.
        self._write_tick_label_pos(x_axis.get('label_position'))

        # Write the axis font elements.
        self._write_axis_font(x_axis.get('num_font'))

        # Write the c:crossAx element.
        self._write_cross_axis(axis_ids[1])

        if self.show_crosses or x_axis.get('visible'):

            # Note, the category crossing comes from the value axis.
            if (y_axis.get('crossing') is None
                    or y_axis.get('crossing') == 'max'):

                # Write the c:crosses element.
                self._write_crosses(y_axis.get('crossing'))
            else:

                # Write the c:crossesAt element.
                self._write_c_crosses_at(y_axis.get('crossing'))

        # Write the c:auto element.
        self._write_auto(1)

        # Write the c:labelOffset element.
        self._write_label_offset(100)

        # Write the c:majorUnit element.
        self._write_c_major_unit(x_axis.get('major_unit'))

        # Write the c:majorTimeUnit element.
        if x_axis.get('major_unit'):
            self._write_c_major_time_unit(x_axis['major_unit_type'])

        # Write the c:minorUnit element.
        self._write_c_minor_unit(x_axis.get('minor_unit'))

        # Write the c:minorTimeUnit element.
        if x_axis.get('minor_unit'):
            self._write_c_minor_time_unit(x_axis['minor_unit_type'])

        self._xml_end_tag('c:dateAx')

    def _write_scaling(self, reverse, min_val, max_val, log_base):
        # Write the <c:scaling> element.

        self._xml_start_tag('c:scaling')

        # Write the c:logBase element.
        self._write_c_log_base(log_base)

        # Write the c:orientation element.
        self._write_orientation(reverse)

        # Write the c:max element.
        self._write_c_max(max_val)

        # Write the c:min element.
        self._write_c_min(min_val)

        self._xml_end_tag('c:scaling')

    def _write_c_log_base(self, val):
        # Write the <c:logBase> element.

        if not val:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:logBase', attributes)

    def _write_orientation(self, reverse):
        # Write the <c:orientation> element.
        val = 'minMax'

        if reverse:
            val = 'maxMin'

        attributes = [('val', val)]

        self._xml_empty_tag('c:orientation', attributes)

    def _write_c_max(self, max_val):
        # Write the <c:max_val> element.

        if max_val is None:
            return

        attributes = [('val', max_val)]

        self._xml_empty_tag('c:max', attributes)

    def _write_c_min(self, min_val):
        # Write the <c:min_val> element.

        if min_val is None:
            return

        attributes = [('val', min_val)]

        self._xml_empty_tag('c:min', attributes)

    def _write_axis_pos(self, val, reverse):
        # Write the <c:axPos> element.

        if reverse:
            if val == 'l':
                val = 'r'
            if val == 'b':
                val = 't'

        attributes = [('val', val)]

        self._xml_empty_tag('c:axPos', attributes)

    def _write_number_format(self, axis):
        # Write the <c:numberFormat> element. Note: It is assumed that if
        # a user defined number format is supplied (i.e., non-default) then
        # the sourceLinked attribute is 0.
        # The user can override this if required.
        format_code = axis.get('num_format')
        source_linked = 1

        # Check if a user defined number format has been set.
        if (format_code is not None
                and format_code != axis['defaults']['num_format']):
            source_linked = 0

        # User override of sourceLinked.
        if axis.get('num_format_linked'):
            source_linked = 1

        attributes = [
            ('formatCode', format_code),
            ('sourceLinked', source_linked),
        ]

        self._xml_empty_tag('c:numFmt', attributes)

    def _write_cat_number_format(self, axis):
        # Write the <c:numFmt> element. Special case handler for category
        # axes which don't always have a number format.
        format_code = axis.get('num_format')
        source_linked = 1
        default_format = 1

        # Check if a user defined number format has been set.
        if (format_code is not None
                and format_code != axis['defaults']['num_format']):
            source_linked = 0
            default_format = 0

        # User override of linkedSource.
        if axis.get('num_format_linked'):
            source_linked = 1

        # Skip if cat doesn't have a num format (unless it is non-default).
        if not self.cat_has_num_fmt and default_format:
            return

        attributes = [
            ('formatCode', format_code),
            ('sourceLinked', source_linked),
        ]

        self._xml_empty_tag('c:numFmt', attributes)

    def _write_major_tick_mark(self, val):
        # Write the <c:majorTickMark> element.

        if not val:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:majorTickMark', attributes)

    def _write_tick_label_pos(self, val=None):
        # Write the <c:tickLblPos> element.
        if val is None or val == 'next_to':
            val = 'nextTo'

        attributes = [('val', val)]

        self._xml_empty_tag('c:tickLblPos', attributes)

    def _write_cross_axis(self, val):
        # Write the <c:crossAx> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:crossAx', attributes)

    def _write_crosses(self, val=None):
        # Write the <c:crosses> element.
        if val is None:
            val = 'autoZero'

        attributes = [('val', val)]

        self._xml_empty_tag('c:crosses', attributes)

    def _write_c_crosses_at(self, val):
        # Write the <c:crossesAt> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:crossesAt', attributes)

    def _write_auto(self, val):
        # Write the <c:auto> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:auto', attributes)

    def _write_label_align(self, val):
        # Write the <c:labelAlign> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:lblAlgn', attributes)

    def _write_label_offset(self, val):
        # Write the <c:labelOffset> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:lblOffset', attributes)

    def _write_major_gridlines(self, gridlines):
        # Write the <c:majorGridlines> element.

        if not gridlines:
            return

        if not gridlines['visible']:
            return

        if gridlines['line']['defined']:
            self._xml_start_tag('c:majorGridlines')

            # Write the c:spPr element.
            self._write_sp_pr(gridlines)

            self._xml_end_tag('c:majorGridlines')
        else:
            self._xml_empty_tag('c:majorGridlines')

    def _write_minor_gridlines(self, gridlines):
        # Write the <c:minorGridlines> element.

        if not gridlines:
            return

        if not gridlines['visible']:
            return

        if gridlines['line']['defined']:
            self._xml_start_tag('c:minorGridlines')

            # Write the c:spPr element.
            self._write_sp_pr(gridlines)

            self._xml_end_tag('c:minorGridlines')
        else:
            self._xml_empty_tag('c:minorGridlines')

    def _write_cross_between(self):
        # Write the <c:crossBetween> element.
        val = self.cross_between

        if val is None:
            val = 'between'

        attributes = [('val', val)]

        self._xml_empty_tag('c:crossBetween', attributes)

    def _write_c_major_unit(self, val):
        # Write the <c:majorUnit> element.

        if not val:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:majorUnit', attributes)

    def _write_c_minor_unit(self, val):
        # Write the <c:minorUnit> element.

        if not val:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:minorUnit', attributes)

    def _write_c_major_time_unit(self, val=None):
        # Write the <c:majorTimeUnit> element.
        if val is None:
            val = 'days'

        attributes = [('val', val)]

        self._xml_empty_tag('c:majorTimeUnit', attributes)

    def _write_c_minor_time_unit(self, val=None):
        # Write the <c:minorTimeUnit> element.
        if val is None:
            val = 'days'

        attributes = [('val', val)]

        self._xml_empty_tag('c:minorTimeUnit', attributes)

    def _write_legend(self):
        # Write the <c:legend> element.
        position = self.legend_position
        delete_series = []
        overlay = 0

        if (self.legend_delete_series is not None
                and type(self.legend_delete_series) is list):
            delete_series = self.legend_delete_series

        if 'overlay' in position:
            overlay = 1

        allowed = {
            'right': 'r',
            'left': 'l',
            'top': 't',
            'bottom': 'b',
        }

        if position == 'none':
            return

        if not position in allowed:
            return

        position = allowed[position]

        self._xml_start_tag('c:legend')

        # Write the c:legendPos element.
        self._write_legend_pos(position)

        # Remove series labels from the legend.
        for index in (delete_series):

            # Write the c:legendEntry element.
            self._write_legend_entry(index)

        # Write the c:layout element.
        self._write_layout()

        # Write the c:overlay element.
        if overlay:
            self._write_overlay()

        self._xml_end_tag('c:legend')

    def _write_legend_pos(self, val):
        # Write the <c:legendPos> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:legendPos', attributes)

    def _write_legend_entry(self, index):
        # Write the <c:legendEntry> element.

        self._xml_start_tag('c:legendEntry')

        # Write the c:idx element.
        self._write_idx(index)

        # Write the c:delete element.
        self._write_delete(1)

        self._xml_end_tag('c:legendEntry')

    def _write_overlay(self):
        # Write the <c:overlay> element.
        val = 1

        attributes = [('val', val)]

        self._xml_empty_tag('c:overlay', attributes)

    def _write_plot_vis_only(self):
        # Write the <c:plotVisOnly> element.
        val = 1

        # Ignore this element if we are plotting hidden data.
        if self.show_hidden:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:plotVisOnly', attributes)

    def _write_print_settings(self):
        # Write the <c:printSettings> element.
        self._xml_start_tag('c:printSettings')

        # Write the c:headerFooter element.
        self._write_header_footer()

        # Write the c:pageMargins element.
        self._write_page_margins()

        # Write the c:pageSetup element.
        self._write_page_setup()

        self._xml_end_tag('c:printSettings')

    def _write_header_footer(self):
        # Write the <c:headerFooter> element.
        self._xml_empty_tag('c:headerFooter')

    def _write_page_margins(self):
        # Write the <c:pageMargins> element.
        b = 0.75
        l = 0.7
        r = 0.7
        t = 0.75
        header = 0.3
        footer = 0.3

        attributes = [
            ('b', b),
            ('l', l),
            ('r', r),
            ('t', t),
            ('header', header),
            ('footer', footer),
        ]

        self._xml_empty_tag('c:pageMargins', attributes)

    def _write_page_setup(self):
        # Write the <c:pageSetup> element.
        self._xml_empty_tag('c:pageSetup')

    def _write_title_rich(self, title, horiz, font):
        # Write the <c:title> element for a rich string.

        self._xml_start_tag('c:title')

        # Write the c:tx element.
        self._write_tx_rich(title, horiz, font)

        # Write the c:layout element.
        self._write_layout()

        self._xml_end_tag('c:title')

    def _write_title_formula(self, title, data_id, horiz, font):
        # Write the <c:title> element for a rich string.

        self._xml_start_tag('c:title')

        # Write the c:tx element.
        self._write_tx_formula(title, data_id)

        # Write the c:layout element.
        self._write_layout()

        # Write the c:txPr element.
        self._write_tx_pr(horiz, font)

        self._xml_end_tag('c:title')

    def _write_tx_rich(self, title, horiz, font):
        # Write the <c:tx> element.

        self._xml_start_tag('c:tx')

        # Write the c:rich element.
        self._write_rich(title, horiz, font)

        self._xml_end_tag('c:tx')

    def _write_tx_value(self, title):
        # Write the <c:tx> element with a value such as for series names.

        self._xml_start_tag('c:tx')

        # Write the c:v element.
        self._write_v(title)

        self._xml_end_tag('c:tx')

    def _write_tx_formula(self, title, data_id):
        # Write the <c:tx> element.
        data = None

        if data_id is not None:
            data = self.formula_data[data_id]

        self._xml_start_tag('c:tx')

        # Write the c:strRef element.
        self._write_str_ref(title, data, 'str')

        self._xml_end_tag('c:tx')

    def _write_rich(self, title, horiz, font):
        # Write the <c:rich> element.

        self._xml_start_tag('c:rich')

        # Write the a:bodyPr element.
        self._write_a_body_pr(horiz)

        # Write the a:lstStyle element.
        self._write_a_lst_style()

        # Write the a:p element.
        self._write_a_p_rich(title, font)

        self._xml_end_tag('c:rich')

    def _write_a_body_pr(self, horiz):
        # Write the <a:bodyPr> element.
        rot = -5400000
        vert = 'horz'

        attributes = [
            ('rot', rot),
            ('vert', vert),
        ]

        if not horiz:
            attributes = []

        self._xml_empty_tag('a:bodyPr', attributes)

    def _write_axis_body_pr(self, rotation):
        # Write the <a:bodyPr> element for axis fonts.
        attributes = []

        if rotation is not None:
            attributes.append(('rot', rotation))

        self._xml_empty_tag('a:bodyPr', attributes)

    def _write_a_lst_style(self):
        # Write the <a:lstStyle> element.
        self._xml_empty_tag('a:lstStyle')

    def _write_a_p_rich(self, title, font):
        # Write the <a:p> element for rich string titles.

        self._xml_start_tag('a:p')

        # Write the a:pPr element.
        self._write_a_p_pr_rich(font)

        # Write the a:r element.
        self._write_a_r(title, font)

        self._xml_end_tag('a:p')

    def _write_a_p_formula(self, font):
        # Write the <a:p> element for formula titles.

        self._xml_start_tag('a:p')

        # Write the a:pPr element.
        self._write_a_p_pr_formula(font)

        # Write the a:endParaRPr element.
        self._write_a_end_para_rpr()

        self._xml_end_tag('a:p')

    def _write_a_p_pr_rich(self, font):
        # Write the <a:pPr> element for rich string titles.

        self._xml_start_tag('a:pPr')

        # Write the a:defRPr element.
        self._write_a_def_rpr(font)

        self._xml_end_tag('a:pPr')

    def _write_a_p_pr_formula(self, font):
        # Write the <a:pPr> element for formula titles.

        self._xml_start_tag('a:pPr')

        # Write the a:defRPr element.
        self._write_a_def_rpr(font)

        self._xml_end_tag('a:pPr')

    def _write_a_def_rpr(self, font):
        # Write the <a:defRPr> element.
        has_color = 0

        style_attributes = self._get_font_style_attributes(font)
        latin_attributes = self._get_font_latin_attributes(font)

        if font and font['color'] is not None:
            has_color = 1

        if latin_attributes or has_color:
            self._xml_start_tag('a:defRPr', style_attributes)

            if has_color:
                self._write_a_solid_fill({'color': font['color']})

            if latin_attributes:
                self._write_a_latin(latin_attributes)

            self._xml_end_tag('a:defRPr')
        else:
            self._xml_empty_tag('a:defRPr', style_attributes)

    def _write_a_end_para_rpr(self):
        # Write the <a:endParaRPr> element.
        lang = 'en-US'

        attributes = [('lang', lang)]

        self._xml_empty_tag('a:endParaRPr', attributes)

    def _write_a_r(self, title, font):
        # Write the <a:r> element.

        self._xml_start_tag('a:r')

        # Write the a:rPr element.
        self._write_a_r_pr(font)

        # Write the a:t element.
        self._write_a_t(title)

        self._xml_end_tag('a:r')

    def _write_a_r_pr(self, font):
        # Write the <a:rPr> element.
        has_color = 0
        lang = 'en-US'

        style_attributes = self._get_font_style_attributes(font)
        latin_attributes = self._get_font_latin_attributes(font)

        if font and font['color'] is not None:
            has_color = 1

        # Add the lang type to the attributes.
        style_attributes.insert(0, ('lang', lang))

        if latin_attributes or has_color:
            self._xml_start_tag('a:rPr', style_attributes)

            if has_color:
                self._write_a_solid_fill({'color': font['color']})

            if latin_attributes:
                self._write_a_latin(latin_attributes)

            self._xml_end_tag('a:rPr')
        else:
            self._xml_empty_tag('a:rPr', style_attributes)

    def _write_a_t(self, title):
        # Write the <a:t> element.

        self._xml_data_element('a:t', title)

    def _write_tx_pr(self, horiz, font):
        # Write the <c:txPr> element.

        self._xml_start_tag('c:txPr')

        # Write the a:bodyPr element.
        self._write_a_body_pr(horiz)

        # Write the a:lstStyle element.
        self._write_a_lst_style()

        # Write the a:p element.
        self._write_a_p_formula(font)

        self._xml_end_tag('c:txPr')

    def _write_marker(self, marker):
        # Write the <c:marker> element.
        if marker is None:
            marker = self.default_marker

        if not marker:
            return
        if 'automatic' in marker:
            return

        self._xml_start_tag('c:marker')

        # Write the c:symbol element.
        self._write_symbol(marker['type'])

        # Write the c:size element.
        if marker.get('size'):
            self._write_marker_size(marker['size'])

        # Write the c:spPr element.
        self._write_sp_pr(marker)

        self._xml_end_tag('c:marker')

    def _write_marker_value(self):
        # Write the <c:marker> element without a sub-element.
        style = self.default_marker

        if not style:
            return

        attributes = [('val', 1)]

        self._xml_empty_tag('c:marker', attributes)

    def _write_marker_size(self, val):
        # Write the <c:size> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:size', attributes)

    def _write_symbol(self, val):
        # Write the <c:symbol> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:symbol', attributes)

    def _write_sp_pr(self, series):
        # Write the <c:spPr> element.
        has_fill = False
        has_line = False

        if 'fill' in series and series['fill']['defined']:
            has_fill = True

        if 'line' in series and series['line']['defined']:
            has_line = True

        if not has_fill and not has_line:
            return

        self._xml_start_tag('c:spPr')

        # Write the fill elements for solid charts such as pie and bar.
        if series['fill'] is not None and series['fill']['defined']:
            if 'none' in series['fill']:
                # Write the a:noFill element.
                self._write_a_no_fill()
            else:
                # Write the a:solidFill element.
                self._write_a_solid_fill(series['fill'])

        # Write the a:ln element.
        if 'line' in series and series['line']['defined']:
            self._write_a_ln(series['line'])

        self._xml_end_tag('c:spPr')

    def _write_a_ln(self, line):
        # Write the <a:ln> element.
        attributes = []

        # Add the line width as an attribute.
        width = line.get('width')

        if width:
            # Round width to nearest 0.25, like Excel.
            width = int((width + 0.125) * 4) / 4.0

            # Convert to internal units.
            width = int(0.5 + (12700 * width))

            attributes = [('w', width)]

        self._xml_start_tag('a:ln', attributes)

        # Write the line fill.
        if 'none' in line:
            # Write the a:noFill element.
            self._write_a_no_fill()
        elif 'color' in line:
            # Write the a:solidFill element.
            self._write_a_solid_fill(line)

        # Write the line/dash type.
        line_type = line.get('dash_type')
        if line_type:
            # Write the a:prstDash element.
            self._write_a_prst_dash(line_type)

        self._xml_end_tag('a:ln')

    def _write_a_no_fill(self):
        # Write the <a:noFill> element.
        self._xml_empty_tag('a:noFill')

    def _write_a_solid_fill(self, line):
        # Write the <a:solidFill> element.

        self._xml_start_tag('a:solidFill')

        if 'color' in line:
            color = self._get_color(line['color'])

            # Write the a:srgbClr element.
            self._write_a_srgb_clr(color)

        self._xml_end_tag('a:solidFill')

    def _write_a_srgb_clr(self, val):
        # Write the <a:srgbClr> element.

        attributes = [('val', val)]

        self._xml_empty_tag('a:srgbClr', attributes)

    def _write_a_prst_dash(self, val):
        # Write the <a:prstDash> element.

        attributes = [('val', val)]

        self._xml_empty_tag('a:prstDash', attributes)

    def _write_trendline(self, trendline):
        # Write the <c:trendline> element.

        if not trendline:
            return

        self._xml_start_tag('c:trendline')

        # Write the c:name element.
        self._write_name(trendline.get('name'))

        # Write the c:spPr element.
        self._write_sp_pr(trendline)

        # Write the c:trendlineType element.
        self._write_trendline_type(trendline['type'])

        # Write the c:order element for polynomial trendlines.
        if trendline['type'] == 'poly':
            self._write_trendline_order(trendline.get('order'))

        # Write the c:period element for moving average trendlines.
        if trendline['type'] == 'movingAvg':
            self._write_period(trendline.get('period'))

        # Write the c:forward element.
        self._write_forward(trendline.get('forward'))

        # Write the c:backward element.
        self._write_backward(trendline.get('backward'))

        self._xml_end_tag('c:trendline')

    def _write_trendline_type(self, val):
        # Write the <c:trendlineType> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:trendlineType', attributes)

    def _write_name(self, data):
        # Write the <c:name> element.

        if data is None:
            return

        self._xml_data_element('c:name', data)

    def _write_trendline_order(self, val):
        # Write the <c:order> element.
        # val = _[0] is not None ? _[0]: 2

        attributes = [('val', val)]

        self._xml_empty_tag('c:order', attributes)

    def _write_period(self, val):
        # Write the <c:period> element.
        # val = _[0] is not None ? _[0]: 2

        attributes = [('val', val)]

        self._xml_empty_tag('c:period', attributes)

    def _write_forward(self, val):
        # Write the <c:forward> element.

        if not val:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:forward', attributes)

    def _write_backward(self, val):
        # Write the <c:backward> element.

        if not val:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:backward', attributes)

    def _write_hi_low_lines(self):
        # Write the <c:hiLowLines> element.
        hi_low_lines = self.hi_low_lines

        if hi_low_lines is None:
            return

        if 'line' in hi_low_lines and hi_low_lines['line']['defined']:

            self._xml_start_tag('c:hiLowLines')

            # Write the c:spPr element.
            self._write_sp_pr(hi_low_lines)

            self._xml_end_tag('c:hiLowLines')
        else:
            self._xml_empty_tag('c:hiLowLines')

    def _write_drop_lines(self):
        # Write the <c:dropLines> element.
        drop_lines = self.drop_lines

        if drop_lines is None:
            return

        if drop_lines['line']['defined']:

            self._xml_start_tag('c:dropLines')

            # Write the c:spPr element.
            self._write_sp_pr(drop_lines)

            self._xml_end_tag('c:dropLines')
        else:
            self._xml_empty_tag('c:dropLines')

    def _write_overlap(self, val):
        # Write the <c:overlap> element.

        if val is None:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:overlap', attributes)

    def _write_num_cache(self, data):
        # Write the <c:numCache> element.
        if data:
            count = len(data)
        else:
            count = 0

        self._xml_start_tag('c:numCache')

        # Write the c:formatCode element.
        self._write_format_code('General')

        # Write the c:ptCount element.
        self._write_pt_count(count)

        for i in range(count):
            token = data[i]

            if token is None:
                continue

            try:
                float(token)
            except ValueError:
                # Write non-numeric data as 0.
                token = 0

            # Write the c:pt element.
            self._write_pt(i, token)

        self._xml_end_tag('c:numCache')

    def _write_str_cache(self, data):
        # Write the <c:strCache> element.
        count = len(data)

        self._xml_start_tag('c:strCache')

        # Write the c:ptCount element.
        self._write_pt_count(count)

        for i in range(count):
            # Write the c:pt element.
            self._write_pt(i, data[i])

        self._xml_end_tag('c:strCache')

    def _write_format_code(self, data):
        # Write the <c:formatCode> element.

        self._xml_data_element('c:formatCode', data)

    def _write_pt_count(self, val):
        # Write the <c:ptCount> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:ptCount', attributes)

    def _write_pt(self, idx, value):
        # Write the <c:pt> element.

        if value is None:
            return

        attributes = [('idx', idx)]

        self._xml_start_tag('c:pt', attributes)

        # Write the c:v element.
        self._write_v(value)

        self._xml_end_tag('c:pt')

    def _write_v(self, data):
        # Write the <c:v> element.

        self._xml_data_element('c:v', data)

    def _write_protection(self):
        # Write the <c:protection> element.
        if not self.protection:
            return

        self._xml_empty_tag('c:protection')

    def _write_d_pt(self, points):
        # Write the <c:dPt> elements.
        index = -1

        if not points:
            return

        for point in (points):
            index += 1
            if not point:
                continue

            self._write_d_pt_point(index, point)

    def _write_d_pt_point(self, index, point):
        # Write an individual <c:dPt> element.

            self._xml_start_tag('c:dPt')

            # Write the c:idx element.
            self._write_idx(index)

            # Write the c:spPr element.
            self._write_sp_pr(point)

            self._xml_end_tag('c:dPt')

    def _write_d_lbls(self, labels):
        # Write the <c:dLbls> element.

        if not labels:
            return

        self._xml_start_tag('c:dLbls')

        # Write the c:dLblPos element.
        if labels.get('position'):
            self._write_d_lbl_pos(labels['position'])

        # Write the c:showVal element.
        if labels.get('value'):
            self._write_show_val()

        # Write the c:showCatName element.
        if labels.get('category'):
            self._write_show_cat_name()

        # Write the c:showSerName element.
        if labels.get('series_name'):
            self._write_show_ser_name()

        # Write the c:showPercent element.
        if labels.get('percentage'):
            self._write_show_percent()

        # Write the c:showLeaderLines element.
        if labels.get('leader_lines'):
            self._write_show_leader_lines()

        self._xml_end_tag('c:dLbls')

    def _write_show_val(self):
        # Write the <c:showVal> element.
        val = 1

        attributes = [('val', val)]

        self._xml_empty_tag('c:showVal', attributes)

    def _write_show_cat_name(self):
        # Write the <c:showCatName> element.
        val = 1

        attributes = [('val', val)]

        self._xml_empty_tag('c:showCatName', attributes)

    def _write_show_ser_name(self):
        # Write the <c:showSerName> element.
        val = 1

        attributes = [('val', val)]

        self._xml_empty_tag('c:showSerName', attributes)

    def _write_show_percent(self):
        # Write the <c:showPercent> element.
        val = 1

        attributes = [('val', val)]

        self._xml_empty_tag('c:showPercent', attributes)

    def _write_show_leader_lines(self):
        # Write the <c:showLeaderLines> element.
        val = 1

        attributes = [('val', val)]

        self._xml_empty_tag('c:showLeaderLines', attributes)

    def _write_d_lbl_pos(self, val):
        # Write the <c:dLblPos> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:dLblPos', attributes)

    def _write_delete(self, val):
        # Write the <c:delete> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:delete', attributes)

    def _write_c_invert_if_negative(self, invert):
        # Write the <c:invertIfNegative> element.
        val = 1

        if not invert:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:invertIfNegative', attributes)

    def _write_axis_font(self, font):
        # Write the axis font elements.

        if not font:
            return

        self._xml_start_tag('c:txPr')
        self._write_axis_body_pr(font.get('rotation'))
        self._write_a_lst_style()
        self._xml_start_tag('a:p')

        self._write_a_p_pr_rich(font)

        self._write_a_end_para_rpr()
        self._xml_end_tag('a:p')
        self._xml_end_tag('c:txPr')

    def _write_a_latin(self, attributes):
        # Write the <a:latin> element.
        self._xml_empty_tag('a:latin', attributes)

    def _write_d_table(self):
        # Write the <c:dTable> element.
        table = self.table

        if not table:
            return

        self._xml_start_tag('c:dTable')

        if table['horizontal']:
            # Write the c:showHorzBorder element.
            self._write_show_horz_border()

        if table['vertical']:
            # Write the c:showVertBorder element.
            self._write_show_vert_border()

        if table['outline']:
            # Write the c:showOutline element.
            self._write_show_outline()

        if table['show_keys']:
            # Write the c:showKeys element.
            self._write_show_keys()

        self._xml_end_tag('c:dTable')

    def _write_show_horz_border(self):
        # Write the <c:showHorzBorder> element.
        attributes = [('val', 1)]

        self._xml_empty_tag('c:showHorzBorder', attributes)

    def _write_show_vert_border(self):
        # Write the <c:showVertBorder> element.
        attributes = [('val', 1)]

        self._xml_empty_tag('c:showVertBorder', attributes)

    def _write_show_outline(self):
        # Write the <c:showOutline> element.
        attributes = [('val', 1)]

        self._xml_empty_tag('c:showOutline', attributes)

    def _write_show_keys(self):
        # Write the <c:showKeys> element.
        attributes = [('val', 1)]

        self._xml_empty_tag('c:showKeys', attributes)

    def _write_error_bars(self, error_bars):
        # Write the X and Y error bars.

        if not error_bars:
            return

        if error_bars['x_error_bars']:
            self._write_err_bars('x', error_bars['x_error_bars'])

        if error_bars['y_error_bars']:
            self._write_err_bars('y', error_bars['y_error_bars'])

    def _write_err_bars(self, direction, error_bars):
        # Write the <c:errBars> element.

        if not error_bars:
            return

        self._xml_start_tag('c:errBars')

        # Write the c:errDir element.
        self._write_err_dir(direction)

        # Write the c:errBarType element.
        self._write_err_bar_type(error_bars['direction'])

        # Write the c:errValType element.
        self._write_err_val_type(error_bars['type'])

        if not error_bars['endcap']:

            # Write the c:noEndCap element.
            self._write_no_end_cap()

        if error_bars['type'] != 'stdErr':

            # Write the c:val element.
            self._write_error_val(error_bars['value'])

        # Write the c:spPr element.
        self._write_sp_pr(error_bars)

        self._xml_end_tag('c:errBars')

    def _write_err_dir(self, val):
        # Write the <c:errDir> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:errDir', attributes)

    def _write_err_bar_type(self, val):
        # Write the <c:errBarType> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:errBarType', attributes)

    def _write_err_val_type(self, val):
        # Write the <c:errValType> element.

        attributes = [('val', val)]

        self._xml_empty_tag('c:errValType', attributes)

    def _write_no_end_cap(self):
        # Write the <c:noEndCap> element.
        attributes = [('val', 1)]

        self._xml_empty_tag('c:noEndCap', attributes)

    def _write_error_val(self, val):
        # Write the <c:val> element for error bars.

        attributes = [('val', val)]

        self._xml_empty_tag('c:val', attributes)

    def _write_up_down_bars(self):
        # Write the <c:upDownBars> element.
        up_down_bars = self.up_down_bars

        if up_down_bars is None:
            return

        self._xml_start_tag('c:upDownBars')

        # Write the c:gapWidth element.
        self._write_gap_width(150)

        # Write the c:upBars element.
        self._write_up_bars(up_down_bars.get('up'))

        # Write the c:downBars element.
        self._write_down_bars(up_down_bars.get('down'))

        self._xml_end_tag('c:upDownBars')

    def _write_gap_width(self, val):
        # Write the <c:gapWidth> element.

        if val is None:
            return

        attributes = [('val', val)]

        self._xml_empty_tag('c:gapWidth', attributes)

    def _write_up_bars(self, bar_format):
        # Write the <c:upBars> element.

        if bar_format['line'] and bar_format['line']['defined']:
            self._xml_start_tag('c:upBars')

            # Write the c:spPr element.
            self._write_sp_pr(bar_format)

            self._xml_end_tag('c:upBars')
        else:
            self._xml_empty_tag('c:upBars')

    def _write_down_bars(self, bar_format):
        # Write the <c:downBars> element.

        if bar_format['line'] and bar_format['line']['defined']:
            self._xml_start_tag('c:downBars')

            # Write the c:spPr element.
            self._write_sp_pr(bar_format)

            self._xml_end_tag('c:downBars')
        else:
            self._xml_empty_tag('c:downBars')

########NEW FILE########
__FILENAME__ = chart_area
###############################################################################
#
# ChartArea - A class for writing the Excel XLSX Area charts.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import chart


class ChartArea(chart.Chart):
    """
    A class for writing the Excel XLSX Area charts.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, options=None):
        """
        Constructor.

        """
        super(ChartArea, self).__init__()

        if options is None:
            options = {}

        self.subtype = options.get('subtype')

        if not self.subtype:
            self.subtype = 'standard'

        self.cross_between = 'midCat'
        self.show_crosses = 0

        # Override and reset the default axis values.
        if self.subtype == 'percent_stacked':
            self.y_axis['defaults']['num_format'] = '0%'

        self.set_y_axis({})

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _write_chart_type(self, args):
        # Override the virtual superclass method with a chart specific method.
        # Write the c:areaChart element.
        self._write_area_chart(args)

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################
    #
    def _write_area_chart(self, args):
        # Write the <c:areaChart> element.

        if args['primary_axes']:
            series = self._get_primary_axes_series()
        else:
            series = self._get_secondary_axes_series()

        if not len(series):
            return

        subtype = self.subtype

        if subtype == 'percent_stacked':
            subtype = 'percentStacked'

        self._xml_start_tag('c:areaChart')

        # Write the c:grouping element.
        self._write_grouping(subtype)

        # Write the series elements.
        for data in series:
            self._write_ser(data)

        # Write the c:dropLines element.
        self._write_drop_lines()

        # Write the c:marker element.
        self._write_marker_value()

        # Write the c:axId elements
        self._write_axis_ids(args)

        self._xml_end_tag('c:areaChart')

########NEW FILE########
__FILENAME__ = chart_bar
###############################################################################
#
# ChartBar - A class for writing the Excel XLSX Bar charts.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import chart


class ChartBar(chart.Chart):
    """
    A class for writing the Excel XLSX Bar charts.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, options=None):
        """
        Constructor.

        """
        super(ChartBar, self).__init__()

        if options is None:
            options = {}

        self.subtype = options.get('subtype')

        if not self.subtype:
            self.subtype = 'clustered'

        self.cat_axis_position = 'l'
        self.val_axis_position = 'b'
        self.horiz_val_axis = 0
        self.horiz_cat_axis = 1
        self.show_crosses = 0

        # Override and reset the default axis values.
        self.x_axis['defaults']['major_gridlines'] = {'visible': 1}
        self.y_axis['defaults']['major_gridlines'] = {'visible': 0}

        if self.subtype == 'percent_stacked':
            self.x_axis['defaults']['num_format'] = '0%'

        self.set_x_axis({})
        self.set_y_axis({})

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _write_chart_type(self, args):
        # Override the virtual superclass method with a chart specific method.
        if args['primary_axes']:
            # Reverse X and Y axes for Bar charts.
            tmp = self.y_axis
            self.y_axis = self.x_axis
            self.x_axis = tmp

            if self.y2_axis['position'] == 'r':
                self.y2_axis['position'] = 't'

        # Write the c:barChart element.
        self._write_bar_chart(args)

    def _write_bar_chart(self, args):
        # Write the <c:barChart> element.

        if args['primary_axes']:
            series = self._get_primary_axes_series()
        else:
            series = self._get_secondary_axes_series()

        if not len(series):
            return

        subtype = self.subtype
        if subtype == 'percent_stacked':
            subtype = 'percentStacked'

        # Set a default overlap for stacked charts.
        if 'stacked' in self.subtype:
            if self.series_overlap is None:
                self.series_overlap = 100

        self._xml_start_tag('c:barChart')

        # Write the c:barDir element.
        self._write_bar_dir()

        # Write the c:grouping element.
        self._write_grouping(subtype)

        # Write the c:ser elements.
        for data in series:
            self._write_ser(data)

        # Write the c:marker element.
        self._write_marker_value()

        # Write the c:gapWidth element.
        self._write_gap_width(self.series_gap)

        # Write the c:overlap element.
        self._write_overlap(self.series_overlap)

        # Write the c:axId elements
        self._write_axis_ids(args)

        self._xml_end_tag('c:barChart')

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_bar_dir(self):
        # Write the <c:barDir> element.
        val = 'bar'

        attributes = [('val', val)]

        self._xml_empty_tag('c:barDir', attributes)

    def _write_err_dir(self, val):
        # Overridden from Chart class since it is not used in Bar charts.
        pass

########NEW FILE########
__FILENAME__ = chart_column
###############################################################################
#
# ChartColumn - A class for writing the Excel XLSX Column charts.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import chart


class ChartColumn(chart.Chart):
    """
    A class for writing the Excel XLSX Column charts.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, options=None):
        """
        Constructor.

        """
        super(ChartColumn, self).__init__()

        if options is None:
            options = {}

        self.subtype = options.get('subtype')

        if not self.subtype:
            self.subtype = 'clustered'

        self.horiz_val_axis = 0

        if self.subtype == 'percent_stacked':
            self.y_axis['defaults']['num_format'] = '0%'

        self.set_y_axis({})

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _write_chart_type(self, args):
        # Override the virtual superclass method with a chart specific method.

        # Write the c:barChart element.
        self._write_bar_chart(args)

    def _write_bar_chart(self, args):
        # Write the <c:barChart> element.

        if args['primary_axes']:
            series = self._get_primary_axes_series()
        else:
            series = self._get_secondary_axes_series()

        if not len(series):
            return

        subtype = self.subtype
        if subtype == 'percent_stacked':
            subtype = 'percentStacked'

        # Set a default overlap for stacked charts.
        if 'stacked' in self.subtype:
            if self.series_overlap is None:
                self.series_overlap = 100

        self._xml_start_tag('c:barChart')

        # Write the c:barDir element.
        self._write_bar_dir()

        # Write the c:grouping element.
        self._write_grouping(subtype)

        # Write the c:ser elements.
        for data in series:
            self._write_ser(data)

        # Write the c:marker element.
        self._write_marker_value()

        # Write the c:gapWidth element.
        self._write_gap_width(self.series_gap)

        # Write the c:overlap element.
        self._write_overlap(self.series_overlap)

        # Write the c:axId elements
        self._write_axis_ids(args)

        self._xml_end_tag('c:barChart')

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_bar_dir(self):
        # Write the <c:barDir> element.
        val = 'col'

        attributes = [('val', val)]

        self._xml_empty_tag('c:barDir', attributes)

    def _write_err_dir(self, val):
        # Overridden from Chart class since it is not used in Column charts.
        pass

########NEW FILE########
__FILENAME__ = chart_line
###############################################################################
#
# ChartLine - A class for writing the Excel XLSX Line charts.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import chart


class ChartLine(chart.Chart):
    """
    A class for writing the Excel XLSX Line charts.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, options=None):
        """
        Constructor.

        """
        super(ChartLine, self).__init__()

        if options is None:
            options = {}

        self.default_marker = {'type': 'none'}
        self.smooth_allowed = True

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _write_chart_type(self, args):
        # Override the virtual superclass method with a chart specific method.
        # Write the c:lineChart element.
        self._write_line_chart(args)

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_line_chart(self, args):
        # Write the <c:lineChart> element.

        if args['primary_axes']:
            series = self._get_primary_axes_series()
        else:
            series = self._get_secondary_axes_series()

        if not len(series):
            return

        self._xml_start_tag('c:lineChart')

        # Write the c:grouping element.
        self._write_grouping('standard')

        # Write the series elements.
        for data in series:
            self._write_ser(data)

        # Write the c:dropLines element.
        self._write_drop_lines()

        # Write the c:hiLowLines element.
        self._write_hi_low_lines()

        # Write the c:upDownBars element.
        self._write_up_down_bars()

        # Write the c:marker element.
        self._write_marker_value()

        # Write the c:axId elements
        self._write_axis_ids(args)

        self._xml_end_tag('c:lineChart')

    def _write_d_pt_point(self, index, point):
        # Write an individual <c:dPt> element. Override the parent method to
        # add markers.

        self._xml_start_tag('c:dPt')

        # Write the c:idx element.
        self._write_idx(index)

        self._xml_start_tag('c:marker')

        # Write the c:spPr element.
        self._write_sp_pr(point)

        self._xml_end_tag('c:marker')

        self._xml_end_tag('c:dPt')

########NEW FILE########
__FILENAME__ = chart_pie
###############################################################################
#
# ChartPie - A class for writing the Excel XLSX Pie charts.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import chart


class ChartPie(chart.Chart):
    """
    A class for writing the Excel XLSX Pie charts.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, options=None):
        """
        Constructor.

        """
        super(ChartPie, self).__init__()

        if options is None:
            options = {}

        self.vary_data_color = 1

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _write_chart_type(self, args):
        # Override the virtual superclass method with a chart specific method.
        # Write the c:pieChart element.
        self._write_pie_chart(args)

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_pie_chart(self, args):
        # Write the <c:pieChart> element.  Over-ridden method to remove
        # axis_id code since Pie charts don't require val and cat axes.
        self._xml_start_tag('c:pieChart')

        # Write the c:varyColors element.
        self._write_vary_colors()

        # Write the series elements.
        for data in self.series:
            self._write_ser(data)

        # Write the c:firstSliceAng element.
        self._write_first_slice_ang()

        self._xml_end_tag('c:pieChart')

    def _write_plot_area(self):
        # Over-ridden method to remove the cat_axis() and val_axis() code
        # since Pie charts don't require those axes.
        #
        # Write the <c:plotArea> element.

        self._xml_start_tag('c:plotArea')

        # Write the c:layout element.
        self._write_layout()

        # Write the subclass chart type element.
        self._write_chart_type(None)

        self._xml_end_tag('c:plotArea')

    def _write_legend(self):
        # Over-ridden method to add <c:txPr> to legend.
        # Write the <c:legend> element.

        position = self.legend_position
        overlay = 0

        if 'overlay' in position:
            overlay = 1

        allowed = {
            'right': 'r',
            'left': 'l',
            'top': 't',
            'bottom': 'b',
        }

        if position == 'none':
            return

        if not position in allowed:
            return

        position = allowed[position]

        self._xml_start_tag('c:legend')

        # Write the c:legendPos element.
        self._write_legend_pos(position)

        # Write the c:layout element.
        self._write_layout()

        # Write the c:overlay element.
        if overlay:
            self._write_overlay()

        # Write the c:txPr element. Over-ridden.
        self._write_tx_pr_legend()

        self._xml_end_tag('c:legend')

    def _write_tx_pr_legend(self):
        # Write the <c:txPr> element for legends.
        horiz = 0

        self._xml_start_tag('c:txPr')

        # Write the a:bodyPr element.
        self._write_a_body_pr(horiz)

        # Write the a:lstStyle element.
        self._write_a_lst_style()

        # Write the a:p element.
        self._write_a_p_legend()

        self._xml_end_tag('c:txPr')

    def _write_a_p_legend(self):
        # Write the <a:p> element for legends.

        self._xml_start_tag('a:p')

        # Write the a:pPr element.
        self._write_a_p_pr_legend()

        # Write the a:endParaRPr element.
        self._write_a_end_para_rpr()

        self._xml_end_tag('a:p')

    def _write_a_p_pr_legend(self):
        # Write the <a:pPr> element for legends.
        attributes = [('rtl', 0)]

        self._xml_start_tag('a:pPr', attributes)

        # Write the a:defRPr element.
        self._write_a_def_rpr(None)

        self._xml_end_tag('a:pPr')

    def _write_vary_colors(self):
        # Write the <c:varyColors> element.
        attributes = [('val', 1)]

        self._xml_empty_tag('c:varyColors', attributes)

    def _write_first_slice_ang(self):
        # Write the <c:firstSliceAng> element.
        attributes = [('val', 0)]

        self._xml_empty_tag('c:firstSliceAng', attributes)

########NEW FILE########
__FILENAME__ = chart_radar
###############################################################################
#
# ChartRadar - A class for writing the Excel XLSX Radar charts.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import chart


class ChartRadar(chart.Chart):
    """
    A class for writing the Excel XLSX Radar charts.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, options=None):
        """
        Constructor.

        """
        super(ChartRadar, self).__init__()

        if options is None:
            options = {}

        self.subtype = options.get('subtype')

        if not self.subtype:
            self.subtype = 'marker'
            self.default_marker = {'type': 'none'}

        # Override and reset the default axis values.
        self.x_axis['defaults']['major_gridlines'] = {'visible': 1}
        self.set_x_axis({})

        # Hardcode major_tick_mark for now until there is an accessor.
        self.y_axis['major_tick_mark'] = 'cross'

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _write_chart_type(self, args):
        # Write the c:radarChart element.
        self._write_radar_chart(args)

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_radar_chart(self, args):
        # Write the <c:radarChart> element.

        if args['primary_axes']:
            series = self._get_primary_axes_series()
        else:
            series = self._get_secondary_axes_series()

        if not len(series):
            return

        self._xml_start_tag('c:radarChart')

        # Write the c:radarStyle element.
        self._write_radar_style()

        # Write the series elements.
        for data in series:
            self._write_ser(data)

        # Write the c:axId elements
        self._write_axis_ids(args)

        self._xml_end_tag('c:radarChart')

    def _write_radar_style(self):
        # Write the <c:radarStyle> element.
        val = 'marker'

        if self.subtype == 'filled':
            val = 'filled'

        attributes = [('val', val)]

        self._xml_empty_tag('c:radarStyle', attributes)

########NEW FILE########
__FILENAME__ = chart_scatter
###############################################################################
#
# ChartScatter - A class for writing the Excel XLSX Scatter charts.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import chart


class ChartScatter(chart.Chart):
    """
    A class for writing the Excel XLSX Scatter charts.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, options=None):
        """
        Constructor.

        """
        super(ChartScatter, self).__init__()

        if options is None:
            options = {}

        self.subtype = options.get('subtype')

        if not self.subtype:
            self.subtype = 'marker_only'

        self.cross_between = 'midCat'
        self.horiz_val_axis = 0
        self.val_axis_postion = 'b'
        self.smooth_allowed = True

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _write_chart_type(self, args):
        # Override the virtual superclass method with a chart specific method.
        # Write the c:scatterChart element.
        self._write_scatter_chart(args)

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_scatter_chart(self, args):
        # Write the <c:scatterChart> element.

        if args['primary_axes']:
            series = self._get_primary_axes_series()
        else:
            series = self._get_secondary_axes_series()

        if not len(series):
            return

        style = 'lineMarker'
        subtype = self.subtype

        # Set the user defined chart subtype.
        if subtype == 'marker_only':
            style = 'lineMarker'

        if subtype == 'straight_with_markers':
            style = 'lineMarker'

        if subtype == 'straight':
            style = 'lineMarker'

        if subtype == 'smooth_with_markers':
            style = 'smoothMarker'

        if subtype == 'smooth':
            style = 'smoothMarker'

        # Add default formatting to the series data.
        self._modify_series_formatting()

        self._xml_start_tag('c:scatterChart')

        # Write the c:scatterStyle element.
        self._write_scatter_style(style)

        # Write the series elements.
        for data in series:
            self._write_ser(data)

        # Write the c:marker element.
        self._write_marker_value()

        # Write the c:axId elements
        self._write_axis_ids(args)

        self._xml_end_tag('c:scatterChart')

    def _write_ser(self, series):
        # Over-ridden to write c:xVal/c:yVal instead of c:cat/c:val elements.
        # Write the <c:ser> element.

        index = self.series_index
        self.series_index += 1

        self._xml_start_tag('c:ser')

        # Write the c:idx element.
        self._write_idx(index)

        # Write the c:order element.
        self._write_order(index)

        # Write the series name.
        self._write_series_name(series)

        # Write the c:spPr element.
        self._write_sp_pr(series)

        # Write the c:marker element.
        self._write_marker(series.get('marker'))

        # Write the c:dPt element.
        self._write_d_pt(series.get('points'))

        # Write the c:dLbls element.
        self._write_d_lbls(series.get('labels'))

        # Write the c:trendline element.
        self._write_trendline(series.get('trendline'))

        # Write the c:errBars element.
        self._write_error_bars(series.get('error_bars'))

        # Write the c:xVal element.
        self._write_x_val(series)

        # Write the c:yVal element.
        self._write_y_val(series)

        # Write the c:smooth element.
        if 'smooth' in self.subtype and series['smooth'] is None:
            # Default is on for smooth scatter charts.
            self._write_c_smooth(True)
        else:
            self._write_c_smooth(series['smooth'])

        self._xml_end_tag('c:ser')

    def _write_plot_area(self):
        # Over-ridden to have 2 valAx elements for scatter charts instead
        # of catAx/valAx.
        #
        # Write the <c:plotArea> element.
        self._xml_start_tag('c:plotArea')

        # Write the c:layout element.
        self._write_layout()

        # Write the subclass chart elements for primary and secondary axes.
        self._write_chart_type({'primary_axes': 1})
        self._write_chart_type({'primary_axes': 0})

        # Write c:catAx and c:valAx elements for series using primary axes.
        self._write_cat_val_axis({'x_axis': self.x_axis,
                                  'y_axis': self.y_axis,
                                  'axis_ids': self.axis_ids,
                                  'position': 'b',
                                  })

        tmp = self.horiz_val_axis
        self.horiz_val_axis = 1

        self._write_val_axis({'x_axis': self.x_axis,
                              'y_axis': self.y_axis,
                              'axis_ids': self.axis_ids,
                              'position': 'l',
                              })

        self.horiz_val_axis = tmp

        # Write c:valAx and c:catAx elements for series using secondary axes
        self._write_cat_val_axis({'x_axis': self.x2_axis,
                                  'y_axis': self.y2_axis,
                                  'axis_ids': self.axis2_ids,
                                  'position': 'b',
                                  })
        self.horiz_val_axis = 1
        self._write_val_axis({'x_axis': self.x2_axis,
                              'y_axis': self.y2_axis,
                              'axis_ids': self.axis2_ids,
                              'position': 'l',
                              })

        # Write the c:spPr element for the plotarea formatting.
        self._write_sp_pr(self.plotarea)

        self._xml_end_tag('c:plotArea')

    def _write_x_val(self, series):
        # Write the <c:xVal> element.
        formula = series.get('categories')
        data_id = series.get('cat_data_id')
        data = self.formula_data[data_id]

        self._xml_start_tag('c:xVal')

        # Check the type of cached data.
        data_type = self._get_data_type(data)

        # TODO. Can a scatter plot have non-numeric data.
        if data_type == 'str':
            # Write the c:numRef element.
            self._write_str_ref(formula, data, data_type)
        else:
            # Write the c:numRef element.
            self._write_num_ref(formula, data, data_type)

        self._xml_end_tag('c:xVal')

    def _write_y_val(self, series):
        # Write the <c:yVal> element.
        formula = series.get('values')
        data_id = series.get('val_data_id')
        data = self.formula_data[data_id]

        self._xml_start_tag('c:yVal')

        # Unlike Cat axes data should only be numeric.
        # Write the c:numRef element.
        self._write_num_ref(formula, data, 'num')

        self._xml_end_tag('c:yVal')

    def _write_scatter_style(self, val):
        # Write the <c:scatterStyle> element.
        attributes = [('val', val)]

        self._xml_empty_tag('c:scatterStyle', attributes)

    def _modify_series_formatting(self):
        # Add default formatting to the series data unless it has already been
        # specified by the user.
        subtype = self.subtype

        # The default scatter style "markers only" requires a line type.
        if subtype == 'marker_only':

            # Go through each series and define default values.
            for series in self.series:

                # Set a line type unless there is already a user defined type.
                if not series['line']['defined']:
                    series['line'] = {'width': 2.25,
                                      'none': 1,
                                      'defined': 1,
                                      }

        # Turn markers off for subtypes that don't have them.
        if not 'marker' in subtype:

            # Go through each series and define default values.
            for series in self.series:
                # Set a marker type unless there is a user defined type.
                if series['marker'] is None or not series['marker']['defined']:
                    series['marker'] = {'type': 'none',
                                        'defined': 1,
                                        }

    def _write_d_pt_point(self, index, point):
        # Write an individual <c:dPt> element. Override the parent method to
        # add markers.

        self._xml_start_tag('c:dPt')

        # Write the c:idx element.
        self._write_idx(index)

        self._xml_start_tag('c:marker')

        # Write the c:spPr element.
        self._write_sp_pr(point)

        self._xml_end_tag('c:marker')

        self._xml_end_tag('c:dPt')

########NEW FILE########
__FILENAME__ = chart_stock
###############################################################################
#
# ChartStock - A class for writing the Excel XLSX Stock charts.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import chart


class ChartStock(chart.Chart):
    """
    A class for writing the Excel XLSX Stock charts.

    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, options=None):
        """
        Constructor.

        """
        super(ChartStock, self).__init__()

        if options is None:
            options = {}

        self.show_crosses = 0
        self.hi_low_lines = {}

        # Override and reset the default axis values.
        self.x_axis['defaults']['num_format'] = 'dd/mm/yyyy'
        self.x2_axis['defaults']['num_format'] = 'dd/mm/yyyy'

        self.set_x_axis({})
        self.set_x2_axis({})

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _write_chart_type(self, args):
        # Override the virtual superclass method with a chart specific method.
        # Write the c:stockChart element.
        self._write_stock_chart(args)

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_stock_chart(self, args):
    # Write the <c:stockChart> element.
    # Overridden to add hi_low_lines().

        if args['primary_axes']:
            series = self._get_primary_axes_series()
        else:
            series = self._get_secondary_axes_series()

        if not len(series):
            return

        # Add default formatting to the series data.
        self._modify_series_formatting()

        self._xml_start_tag('c:stockChart')

        # Write the series elements.
        for data in series:
            self._write_ser(data)

        # Write the c:dropLines element.
        self._write_drop_lines()

        # Write the c:hiLowLines element.
        if args.get('primary_axes'):
            self._write_hi_low_lines()

        # Write the c:upDownBars element.
        self._write_up_down_bars()

        # Write the c:marker element.
        self._write_marker_value()

        # Write the c:axId elements
        self._write_axis_ids(args)

        self._xml_end_tag('c:stockChart')

    def _write_plot_area(self):
        # Overridden to use _write_date_axis() instead of _write_cat_axis().
        self._xml_start_tag('c:plotArea')

        # Write the c:layout element.
        self._write_layout()

        # Write the subclass chart elements for primary and secondary axes.
        self._write_chart_type({'primary_axes': 1})
        self._write_chart_type({'primary_axes': 0})

        # Write c:catAx and c:valAx elements for series using primary axes.
        self._write_date_axis({'x_axis': self.x_axis,
                               'y_axis': self.y_axis,
                               'axis_ids': self.axis_ids
                               })

        self._write_val_axis({'x_axis': self.x_axis,
                              'y_axis': self.y_axis,
                              'axis_ids': self.axis_ids
                              })

        # Write c:valAx and c:catAx elements for series using secondary axes.
        self._write_val_axis({'x_axis': self.x2_axis,
                              'y_axis': self.y2_axis,
                              'axis_ids': self.axis2_ids
                              })

        self._write_date_axis({'x_axis': self.x2_axis,
                               'y_axis': self.y2_axis,
                               'axis_ids': self.axis2_ids
                               })

        # Write the c:dTable element.
        self._write_d_table()

        # Write the c:spPr element for the plotarea formatting.
        self._write_sp_pr(self.plotarea)

        self._xml_end_tag('c:plotArea')

    def _modify_series_formatting(self):
        # Add default formatting to the series data.

        index = 0

        for series in self.series:
            if index % 4 != 3:
                if not series['line']['defined']:
                    series['line'] = {'width': 2.25,
                                      'none': 1,
                                      'defined': 1}

                if series['marker'] is None:
                    if index % 4 == 2:
                        series['marker'] = {'type': 'dot', 'size': 3}
                    else:
                        series['marker'] = {'type': 'none'}

            index += 1

########NEW FILE########
__FILENAME__ = comments
###############################################################################
#
# Comments - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

import re

from . import xmlwriter
from .utility import xl_rowcol_to_cell


class Comments(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Comments file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(Comments, self).__init__()
        self.author_ids = {}

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self, comments_data=[]):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        # Write the comments element.
        self._write_comments()

        # Write the authors element.
        self._write_authors(comments_data)

        # Write the commentList element.
        self._write_comment_list(comments_data)

        self._xml_end_tag('comments')

        # Close the file.
        self._xml_close()

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_comments(self):
        # Write the <comments> element.
        xmlns = 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'

        attributes = [('xmlns', xmlns)]

        self._xml_start_tag('comments', attributes)

    def _write_authors(self, comment_data):
        # Write the <authors> element.
        author_count = 0

        self._xml_start_tag('authors')

        for comment in (comment_data):
            author = comment[3]

            if author is not None and not author in self.author_ids:
                # Store the author id.
                self.author_ids[author] = author_count
                author_count += 1

                # Write the author element.
                self._write_author(author)

        self._xml_end_tag('authors')

    def _write_author(self, data):
        # Write the <author> element.
        self._xml_data_element('author', data)

    def _write_comment_list(self, comment_data):
        # Write the <commentList> element.
        self._xml_start_tag('commentList')

        for comment in (comment_data):
            row = comment[0]
            col = comment[1]
            text = comment[2]
            author = comment[3]

            # Look up the author id.
            author_id = None
            if author is not None:
                author_id = self.author_ids[author]

            # Write the comment element.
            self._write_comment(row, col, text, author_id)

        self._xml_end_tag('commentList')

    def _write_comment(self, row, col, text, author_id):
        # Write the <comment> element.
        ref = xl_rowcol_to_cell(row, col)

        attributes = [('ref', ref)]

        if author_id is not None:
            attributes.append(('authorId', author_id))

        self._xml_start_tag('comment', attributes)

        # Write the text element.
        self._write_text(text)

        self._xml_end_tag('comment')

    def _write_text(self, text):
        # Write the <text> element.
        self._xml_start_tag('text')

        # Write the text r element.
        self._write_text_r(text)

        self._xml_end_tag('text')

    def _write_text_r(self, text):
        # Write the <r> element.
        self._xml_start_tag('r')

        # Write the rPr element.
        self._write_r_pr()

        # Write the text r element.
        self._write_text_t(text)

        self._xml_end_tag('r')

    def _write_text_t(self, text):
        # Write the text <t> element.
        attributes = []

        if re.search('^\s', text) or re.search('\s$', text):
            attributes.append(('xml:space', 'preserve'))

        self._xml_data_element('t', text, attributes)

    def _write_r_pr(self):
        # Write the <rPr> element.
        self._xml_start_tag('rPr')

        # Write the sz element.
        self._write_sz()

        # Write the color element.
        self._write_color()

        # Write the rFont element.
        self._write_r_font()

        # Write the family element.
        self._write_family()

        self._xml_end_tag('rPr')

    def _write_sz(self):
        # Write the <sz> element.
        attributes = [('val', 8)]

        self._xml_empty_tag('sz', attributes)

    def _write_color(self):
        # Write the <color> element.
        attributes = [('indexed', 81)]

        self._xml_empty_tag('color', attributes)

    def _write_r_font(self):
        # Write the <rFont> element.
        attributes = [('val', 'Tahoma')]

        self._xml_empty_tag('rFont', attributes)

    def _write_family(self):
        # Write the <family> element.
        attributes = [('val', 2)]

        self._xml_empty_tag('family', attributes)

########NEW FILE########
__FILENAME__ = contenttypes
###############################################################################
#
# ContentTypes - A class for writing the Excel XLSX ContentTypes file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import xmlwriter

# Long namespace strings used in the class.
app_package = 'application/vnd.openxmlformats-package.'
app_document = 'application/vnd.openxmlformats-officedocument.'

defaults = [
    ('rels', app_package + 'relationships+xml'),
    ('xml', 'application/xml'),
]

overrides = [
    ('/docProps/app.xml', app_document + 'extended-properties+xml'),
    ('/docProps/core.xml', app_package + 'core-properties+xml'),
    ('/xl/styles.xml', app_document + 'spreadsheetml.styles+xml'),
    ('/xl/theme/theme1.xml', app_document + 'theme+xml'),
    ('/xl/workbook.xml', app_document + 'spreadsheetml.sheet.main+xml'),
]


class ContentTypes(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX ContentTypes file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(ContentTypes, self).__init__()

        self.defaults = defaults[:]
        self.overrides = overrides[:]

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        self._xml_declaration
        self._write_types()
        self._write_defaults()
        self._write_overrides()

        self._xml_end_tag('Types')

        # Close the file.
        self._xml_close()

    def _add_default(self, default):
        # Add elements to the ContentTypes defaults.
        self.defaults.append(default)

    def _add_override(self, override):
        # Add elements to the ContentTypes overrides.
        self.overrides.append(override)

    def _add_worksheet_name(self, worksheet_name):
        # Add the name of a worksheet to the ContentTypes overrides.
        worksheet_name = "/xl/worksheets/" + worksheet_name + ".xml"

        self._add_override((worksheet_name,
                           app_document + 'spreadsheetml.worksheet+xml'))

    def _add_chartsheet_name(self, chartsheet_name):
        # Add the name of a chartsheet to the ContentTypes overrides.
        chartsheet_name = "/xl/chartsheets/" + chartsheet_name + ".xml"

        self._add_override((chartsheet_name,
                           app_document + 'spreadsheetml.chartsheet+xml'))

    def _add_chart_name(self, chart_name):
        # Add the name of a chart to the ContentTypes overrides.
        chart_name = "/xl/charts/" + chart_name + ".xml"

        self._add_override((chart_name, app_document + 'drawingml.chart+xml'))

    def _add_drawing_name(self, drawing_name):
        # Add the name of a drawing to the ContentTypes overrides.
        drawing_name = "/xl/drawings/" + drawing_name + ".xml"

        self._add_override((drawing_name, app_document + 'drawing+xml'))

    def _add_vml_name(self):
        # Add the name of a VML drawing to the ContentTypes defaults.
        self._add_default(('vml', app_document + 'vmlDrawing'))

    def _add_comment_name(self, comment_name):
        # Add the name of a comment to the ContentTypes overrides.
        comment_name = "/xl/" + comment_name + ".xml"

        self._add_override((comment_name,
                           app_document + 'spreadsheetml.comments+xml'))

    def _add_shared_strings(self):
        # Add the sharedStrings link to the ContentTypes overrides.
        self._add_override(('/xl/sharedStrings.xml',
                           app_document + 'spreadsheetml.sharedStrings+xml'))

    def _add_calc_chain(self):
        # Add the calcChain link to the ContentTypes overrides.
        self._add_override(('/xl/calcChain.xml',
                           app_document + 'spreadsheetml.calcChain+xml'))

    def _add_image_types(self, image_types):
        # Add the image default types.
        for image_type in image_types:
            self._add_default((image_type, 'image/' + image_type))

    def _add_table_name(self, table_name):
        # Add the name of a table to the ContentTypes overrides.
        table_name = "/xl/tables/" + table_name + ".xml"

        self._add_override((table_name,
                           app_document + 'spreadsheetml.table+xml'))

    def _add_vba_project(self):
        # Add a vbaProject to the ContentTypes defaults.

        # TODO: Fix when test is ported.
        # Change the workbook.xml content-type from xlsx to xlsx.
        # for aref in self.overrides:
        #    if aref[0] == '/xl/workbook.xml':
        #        aref[1]='application/vnd.ms-excel.sheet.macroEnabled.main+xml'

        self._add_default(('bin', 'application/vnd.ms-office.vbaProject'))

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_defaults(self):
        # Write out all of the <Default> types.

        for extension, content_type in self.defaults:
            self._xml_empty_tag('Default',
                                [('Extension', extension),
                                 ('ContentType', content_type)])

    def _write_overrides(self):
        # Write out all of the <Override> types.
        for part_name, content_type in self.overrides:
            self._xml_empty_tag('Override',
                                [('PartName', part_name),
                                 ('ContentType', content_type)])

    def _write_types(self):
        # Write the <Types> element.
        xmlns = 'http://schemas.openxmlformats.org/package/2006/content-types'

        attributes = [('xmlns', xmlns,)]
        self._xml_start_tag('Types', attributes)

    def _write_default(self, extension, content_type):
        # Write the <Default> element.
        attributes = [
            ('Extension', extension),
            ('ContentType', content_type),
        ]

        self._xml_empty_tag('Default', attributes)

    def _write_override(self, part_name, content_type):
        # Write the <Override> element.
        attributes = [
            ('PartName', part_name),
            ('ContentType', content_type),
        ]

        self._xml_empty_tag('Override', attributes)

########NEW FILE########
__FILENAME__ = core
###############################################################################
#
# Core - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Standard packages.
from datetime import datetime

# Package imports.
from . import xmlwriter
from .utility import encode_utf8


class Core(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Core file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(Core, self).__init__()

        self.properties = {}

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        self._write_cp_core_properties()
        self._write_dc_title()
        self._write_dc_subject()
        self._write_dc_creator()
        self._write_cp_keywords()
        self._write_dc_description()
        self._write_cp_last_modified_by()
        self._write_dcterms_created()
        self._write_dcterms_modified()
        self._write_cp_category()
        self._write_cp_content_status()

        self._xml_end_tag('cp:coreProperties')

        # Close the file.
        self._xml_close()

    def _set_properties(self, properties):
        # Set the document properties.
        self.properties = properties

    def _localtime_to_iso8601_date(self, date):
        # Convert to a ISO 8601 style "2010-01-01T00:00:00Z" date.
        if not date:
            date = datetime.now()

        return date.strftime("%Y-%m-%dT%H:%M:%SZ")

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_cp_core_properties(self):
        # Write the <cp:coreProperties> element.

        xmlns_cp = ('http://schemas.openxmlformats.org/package/2006/' +
                    'metadata/core-properties')
        xmlns_dc = 'http://purl.org/dc/elements/1.1/'
        xmlns_dcterms = 'http://purl.org/dc/terms/'
        xmlns_dcmitype = 'http://purl.org/dc/dcmitype/'
        xmlns_xsi = 'http://www.w3.org/2001/XMLSchema-instance'

        attributes = [
            ('xmlns:cp', xmlns_cp),
            ('xmlns:dc', xmlns_dc),
            ('xmlns:dcterms', xmlns_dcterms),
            ('xmlns:dcmitype', xmlns_dcmitype),
            ('xmlns:xsi', xmlns_xsi),
        ]

        self._xml_start_tag('cp:coreProperties', attributes)

    def _write_dc_creator(self):
        # Write the <dc:creator> element.
        data = self.properties.get('author', '')

        self._xml_data_element('dc:creator', encode_utf8(data))

    def _write_cp_last_modified_by(self):
        # Write the <cp:lastModifiedBy> element.
        data = self.properties.get('author', '')

        self._xml_data_element('cp:lastModifiedBy', encode_utf8(data))

    def _write_dcterms_created(self):
        # Write the <dcterms:created> element.
        date = self.properties.get('created', datetime.now())

        xsi_type = 'dcterms:W3CDTF'

        date = self._localtime_to_iso8601_date(date)

        attributes = [('xsi:type', xsi_type,)]

        self._xml_data_element('dcterms:created', date, attributes)

    def _write_dcterms_modified(self):
        # Write the <dcterms:modified> element.
        date = self.properties.get('created', datetime.now())

        xsi_type = 'dcterms:W3CDTF'

        date = self._localtime_to_iso8601_date(date)

        attributes = [('xsi:type', xsi_type,)]

        self._xml_data_element('dcterms:modified', date, attributes)

    def _write_dc_title(self):
        # Write the <dc:title> element.
        if 'title' in self.properties:
            data = self.properties['title']
        else:
            return

        self._xml_data_element('dc:title', encode_utf8(data))

    def _write_dc_subject(self):
        # Write the <dc:subject> element.
        if 'subject' in self.properties:
            data = self.properties['subject']
        else:
            return

        self._xml_data_element('dc:subject', encode_utf8(data))

    def _write_cp_keywords(self):
        # Write the <cp:keywords> element.
        if 'keywords' in self.properties:
            data = self.properties['keywords']
        else:
            return

        self._xml_data_element('cp:keywords', encode_utf8(data))

    def _write_dc_description(self):
        # Write the <dc:description> element.
        if 'comments' in self.properties:
            data = self.properties['comments']
        else:
            return

        self._xml_data_element('dc:description', encode_utf8(data))

    def _write_cp_category(self):
        # Write the <cp:category> element.
        if 'category' in self.properties:
            data = self.properties['category']
        else:
            return

        self._xml_data_element('cp:category', encode_utf8(data))

    def _write_cp_content_status(self):
        # Write the <cp:contentStatus> element.
        if 'status' in self.properties:
            data = self.properties['status']
        else:
            return

        self._xml_data_element('cp:contentStatus', encode_utf8(data))

########NEW FILE########
__FILENAME__ = drawing
###############################################################################
#
# Drawing - A class for writing the Excel XLSX Drawing file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import xmlwriter


class Drawing(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Drawing file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(Drawing, self).__init__()

        self.drawings = []
        self.embedded = 0
        self.orientation = 0

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        # Write the xdr:wsDr element.
        self._write_drawing_workspace()

        if self.embedded:
            index = 1
            for dimensions in self.drawings:
                # Write the xdr:twoCellAnchor element.
                self._write_two_cell_anchor(index, dimensions)
                index += 1
        else:
            # Write the xdr:absoluteAnchor element.
            self._write_absolute_anchor(1)

        self._xml_end_tag('xdr:wsDr')

        # Close the file.
        self._xml_close()

    def _add_drawing_object(self, drawing_object):
        # Add a chart, image or shape sub object to the drawing.
        self.drawings.append(drawing_object)

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_drawing_workspace(self):
        # Write the <xdr:wsDr> element.
        schema = 'http://schemas.openxmlformats.org/drawingml/'
        xmlns_xdr = schema + '2006/spreadsheetDrawing'
        xmlns_a = schema + '2006/main'

        attributes = [
            ('xmlns:xdr', xmlns_xdr),
            ('xmlns:a', xmlns_a),
        ]

        self._xml_start_tag('xdr:wsDr', attributes)

    def _write_two_cell_anchor(self, index, dimensions):
        # Write the <xdr:twoCellAnchor> element.
        anchor_type = dimensions[0]
        col_from = dimensions[1]
        row_from = dimensions[2]
        col_from_offset = dimensions[3]
        row_from_offset = dimensions[4]
        col_to = dimensions[5]
        row_to = dimensions[6]
        col_to_offset = dimensions[7]
        row_to_offset = dimensions[8]
        col_absolute = dimensions[9]
        row_absolute = dimensions[10]
        width = dimensions[11]
        height = dimensions[12]
        description = dimensions[13]
        shape = dimensions[14]

        attributes = []

        # Add attribute for images.
        if anchor_type == 2:
            attributes.append(('editAs', 'oneCell'))

        # Add editAs attribute for shapes.
        if shape and shape.editAs:
            attributes.append(('editAs', shape.editAs))

        self._xml_start_tag('xdr:twoCellAnchor', attributes)

        # Write the xdr:from element.
        self._write_from(
            col_from,
            row_from,
            col_from_offset,
            row_from_offset)

        # Write the xdr:from element.
        self._write_to(
            col_to,
            row_to,
            col_to_offset,
            row_to_offset)

        if anchor_type == 1:
            # Graphic frame.
            # Write the xdr:graphicFrame element for charts.
            self._write_graphic_frame(index, description)
        elif anchor_type == 2:
            # Write the xdr:pic element.
            self._write_pic(index, col_absolute, row_absolute, width,
                            height, description)
        else:
            # Write the xdr:sp element for shapes.
            self._write_sp(index, col_absolute, row_absolute, width, height,
                           shape)

        # Write the xdr:clientData element.
        self._write_client_data()

        self._xml_end_tag('xdr:twoCellAnchor')

    def _write_absolute_anchor(self, index):
        self._xml_start_tag('xdr:absoluteAnchor')
        # Write the <xdr:absoluteAnchor> element.

        # Different co-ordinates for horizontal (= 0) and vertical (= 1).
        if self.orientation == 0:
            # Write the xdr:pos element.
            self._write_pos(0, 0)

            # Write the xdr:ext element.
            self._write_ext(9308969, 6078325)

        else:
            # Write the xdr:pos element.
            self._write_pos(0, -47625)

            # Write the xdr:ext element.
            self._write_ext(6162675, 6124575)

        # Write the xdr:graphicFrame element.
        self._write_graphic_frame(index)

        # Write the xdr:clientData element.
        self._write_client_data()

        self._xml_end_tag('xdr:absoluteAnchor')

    def _write_from(self, col, row, col_offset, row_offset):
        # Write the <xdr:from> element.
        self._xml_start_tag('xdr:from')

        # Write the xdr:col element.
        self._write_col(col)

        # Write the xdr:colOff element.
        self._write_col_off(col_offset)

        # Write the xdr:row element.
        self._write_row(row)

        # Write the xdr:rowOff element.
        self._write_row_off(row_offset)

        self._xml_end_tag('xdr:from')

    def _write_to(self, col, row, col_offset, row_offset):
        # Write the <xdr:to> element.
        self._xml_start_tag('xdr:to')

        # Write the xdr:col element.
        self._write_col(col)

        # Write the xdr:colOff element.
        self._write_col_off(col_offset)

        # Write the xdr:row element.
        self._write_row(row)

        # Write the xdr:rowOff element.
        self._write_row_off(row_offset)

        self._xml_end_tag('xdr:to')

    def _write_col(self, data):
        # Write the <xdr:col> element.
        self._xml_data_element('xdr:col', data)

    def _write_col_off(self, data):
        # Write the <xdr:colOff> element.
        self._xml_data_element('xdr:colOff', data)

    def _write_row(self, data):
        # Write the <xdr:row> element.
        self._xml_data_element('xdr:row', data)

    def _write_row_off(self, data):
        # Write the <xdr:rowOff> element.
        self._xml_data_element('xdr:rowOff', data)

    def _write_pos(self, x, y):
        # Write the <xdr:pos> element.

        attributes = [('x', x), ('y', y)]

        self._xml_empty_tag('xdr:pos', attributes)

    def _write_ext(self, cx, cy):
        # Write the <xdr:ext> element.

        attributes = [('cx', cx), ('cy', cy)]

        self._xml_empty_tag('xdr:ext', attributes)

    def _write_graphic_frame(self, index, name):
        # Write the <xdr:graphicFrame> element.
        attributes = [('macro', '')]

        self._xml_start_tag('xdr:graphicFrame', attributes)

        # Write the xdr:nvGraphicFramePr element.
        self._write_nv_graphic_frame_pr(index, name)

        # Write the xdr:xfrm element.
        self._write_xfrm()

        # Write the a:graphic element.
        self._write_atag_graphic(index)

        self._xml_end_tag('xdr:graphicFrame')

    def _write_nv_graphic_frame_pr(self, index, name):
        # Write the <xdr:nvGraphicFramePr> element.

        if not name:
            name = 'Chart ' + str(index)

        self._xml_start_tag('xdr:nvGraphicFramePr')

        # Write the xdr:cNvPr element.
        self._write_c_nv_pr(index + 1, name)

        # Write the xdr:cNvGraphicFramePr element.
        self._write_c_nv_graphic_frame_pr()

        self._xml_end_tag('xdr:nvGraphicFramePr')

    def _write_c_nv_pr(self, index, name, descr=None):
        # Write the <xdr:cNvPr> element.

        attributes = [('id', index), ('name', name)]

        # Add description attribute for images.
        if descr is not None:
            attributes.append(('descr', descr))

        self._xml_empty_tag('xdr:cNvPr', attributes)

    def _write_c_nv_graphic_frame_pr(self):
        # Write the <xdr:cNvGraphicFramePr> element.
        if self.embedded:
            self._xml_empty_tag('xdr:cNvGraphicFramePr')
        else:
            self._xml_start_tag('xdr:cNvGraphicFramePr')

            # Write the a:graphicFrameLocks element.
            self._write_a_graphic_frame_locks()

            self._xml_end_tag('xdr:cNvGraphicFramePr')

    def _write_a_graphic_frame_locks(self):
        # Write the <a:graphicFrameLocks> element.
        attributes = [('noGrp', 1)]

        self._xml_empty_tag('a:graphicFrameLocks', attributes)

    def _write_xfrm(self):
        # Write the <xdr:xfrm> element.
        self._xml_start_tag('xdr:xfrm')

        # Write the xfrmOffset element.
        self._write_xfrm_offset()

        # Write the xfrmOffset element.
        self._write_xfrm_extension()

        self._xml_end_tag('xdr:xfrm')

    def _write_xfrm_offset(self):
        # Write the <a:off> xfrm sub-element.

        attributes = [
            ('x', 0),
            ('y', 0),
        ]

        self._xml_empty_tag('a:off', attributes)

    def _write_xfrm_extension(self):
        # Write the <a:ext> xfrm sub-element.

        attributes = [
            ('cx', 0),
            ('cy', 0),
        ]

        self._xml_empty_tag('a:ext', attributes)

    def _write_atag_graphic(self, index):
        # Write the <a:graphic> element.
        self._xml_start_tag('a:graphic')

        # Write the a:graphicData element.
        self._write_atag_graphic_data(index)

        self._xml_end_tag('a:graphic')

    def _write_atag_graphic_data(self, index):
        # Write the <a:graphicData> element.
        uri = 'http://schemas.openxmlformats.org/drawingml/2006/chart'

        attributes = [('uri', uri,)]

        self._xml_start_tag('a:graphicData', attributes)

        # Write the c:chart element.
        self._write_c_chart('rId' + str(index))

        self._xml_end_tag('a:graphicData')

    def _write_c_chart(self, r_id):
        # Write the <c:chart> element.

        schema = 'http://schemas.openxmlformats.org/'
        xmlns_c = schema + 'drawingml/2006/chart'
        xmlns_r = schema + 'officeDocument/2006/relationships'

        attributes = [
            ('xmlns:c', xmlns_c),
            ('xmlns:r', xmlns_r),
            ('r:id', r_id),
        ]

        self._xml_empty_tag('c:chart', attributes)

    def _write_client_data(self):
        # Write the <xdr:clientData> element.
        self._xml_empty_tag('xdr:clientData')

    def _write_sp(self, index, col_absolute, row_absolute,
                  width, height, shape):
        # Write the <xdr:sp> element.

        if shape and shape.connect:
            attributes = [('macro', '')]
            self._xml_start_tag('xdr:cxnSp', attributes)

            # Write the xdr:nvCxnSpPr element.
            self._write_nv_cxn_sp_pr(index, shape)

            # Write the xdr:spPr element.
            self._write_xdr_sp_pr(index, col_absolute, row_absolute, width,
                                  height, shape)

            self._xml_end_tag('xdr:cxnSp')
        else:
            # Add attribute for shapes.
            attributes = [('macro', ''), ('textlink', '')]
            self._xml_start_tag('xdr:sp', attributes)

            # Write the xdr:nvSpPr element.
            self._write_nv_sp_pr(index, shape)

            # Write the xdr:spPr element.
            self._write_xdr_sp_pr(index, col_absolute, row_absolute, width,
                                  height, shape)

            # Write the xdr:txBody element.
            if shape.text:
                self._write_txBody(col_absolute, row_absolute, width, height,
                                   shape)

            self._xml_end_tag('xdr:sp')

    def _write_nv_cxn_sp_pr(self, index, shape):
        # Write the <xdr:nvCxnSpPr> element.
        self._xml_start_tag('xdr:nvCxnSpPr')

        shape.name = shape.type + ' ' + index
        if shape.name is not None:
            self._write_c_nv_pr(shape.id, shape.name)

        self._xml_start_tag('xdr:cNvCxnSpPr')

        attributes = [('noChangeShapeType', '1')]
        self._xml_empty_tag('a:cxnSpLocks', attributes)

        if shape.start:
            attributes = [('id', shape.start), ('idx', shape.start_index)]
            self._xml_empty_tag('a:stCxn', attributes)

        if shape.end:
            attributes = [('id', shape.end), ('idx', shape.end_index)]
            self._xml_empty_tag('a:endCxn', attributes)
        self._xml_end_tag('xdr:cNvCxnSpPr')
        self._xml_end_tag('xdr:nvCxnSpPr')

    def _write_nv_sp_pr(self, index, shape):
        # Write the <xdr:NvSpPr> element.
        attributes = []

        self._xml_start_tag('xdr:nvSpPr')

        shape_name = shape.type + ' ' + index

        self._write_c_nv_pr(shape.id, shape_name)

        if shape.txBox:
            attributes = [('txBox', 1)]

        self._xml_start_tag('xdr:cNvSpPr', attributes)

        attributes = [('noChangeArrowheads', '1')]

        self._xml_empty_tag('a:spLocks', attributes)

        self._xml_end_tag('xdr:cNvSpPr')
        self._xml_end_tag('xdr:nvSpPr')

    def _write_pic(self, index, col_absolute, row_absolute,
                   width, height, description):
        # Write the <xdr:pic> element.
        self._xml_start_tag('xdr:pic')

        # Write the xdr:nvPicPr element.
        self._write_nv_pic_pr(index, description)

        # Write the xdr:blipFill element.
        self._write_blip_fill(index)

        # Pictures are rectangle shapes by default.
        shape = {'type': 'rect'}

        # Write the xdr:spPr element.
        self._write_sp_pr(col_absolute, row_absolute, width, height,
                          shape)

        self._xml_end_tag('xdr:pic')

    def _write_nv_pic_pr(self, index, description):
        # Write the <xdr:nvPicPr> element.
        self._xml_start_tag('xdr:nvPicPr')

        # Write the xdr:cNvPr element.
        self._write_c_nv_pr(index + 1, 'Picture ' + str(index), description)

        # Write the xdr:cNvPicPr element.
        self._write_c_nv_pic_pr()

        self._xml_end_tag('xdr:nvPicPr')

    def _write_c_nv_pic_pr(self):
        # Write the <xdr:cNvPicPr> element.
        self._xml_start_tag('xdr:cNvPicPr')

        # Write the a:picLocks element.
        self._write_a_pic_locks()

        self._xml_end_tag('xdr:cNvPicPr')

    def _write_a_pic_locks(self):
        # Write the <a:picLocks> element.
        attributes = [('noChangeAspect', 1)]

        self._xml_empty_tag('a:picLocks', attributes)

    def _write_blip_fill(self, index):
        # Write the <xdr:blipFill> element.
        self._xml_start_tag('xdr:blipFill')

        # Write the a:blip element.
        self._write_a_blip(index)

        # Write the a:stretch element.
        self._write_a_stretch()

        self._xml_end_tag('xdr:blipFill')

    def _write_a_blip(self, index):
        # Write the <a:blip> element.
        schema = 'http://schemas.openxmlformats.org/officeDocument/'
        xmlns_r = schema + '2006/relationships'
        r_embed = 'rId' + str(index)

        attributes = [
            ('xmlns:r', xmlns_r),
            ('r:embed', r_embed)]

        self._xml_empty_tag('a:blip', attributes)

    def _write_a_stretch(self):
        # Write the <a:stretch> element.
        self._xml_start_tag('a:stretch')

        # Write the a:fillRect element.
        self._write_a_fill_rect()

        self._xml_end_tag('a:stretch')

    def _write_a_fill_rect(self):
        # Write the <a:fillRect> element.
        self._xml_empty_tag('a:fillRect')

    def _write_sp_pr(self, col_absolute, row_absolute, width, height,
                     shape={}):
        # Write the <xdr:spPr> element, for charts.

        self._xml_start_tag('xdr:spPr')

        # Write the a:xfrm element.
        self._write_a_xfrm(col_absolute, row_absolute, width, height)

        # Write the a:prstGeom element.
        self._write_a_prst_geom(shape)

        self._xml_end_tag('xdr:spPr')

    def _write_xdr_sp_pr(self, index, col_absolute, row_absolute, width,
                         height, shape={}):
        # Write the <xdr:spPr> element for shapes.

        attributes = [('bwMode', 'auto')]

        self._xml_start_tag('xdr:spPr', attributes)

        # Write the a:xfrm element.
        self._write_a_xfrm(col_absolute, row_absolute, width, height, shape)

        # Write the a:prstGeom element.
        self._write_a_prst_geom(shape)

        fill = shape.fill

        if len(fill) > 1:

            # Write the a:solidFill element.
            self._write_a_solid_fill(fill)
        else:
            self._xml_empty_tag('a:noFill')

        # Write the a:ln element.
        self._write_a_ln(shape)

        self._xml_end_tag('xdr:spPr')

    def _write_a_xfrm(self, col_absolute, row_absolute, width, height,
                      shape={}):
        # Write the <a:xfrm> element.
        attributes = []

        if "rotation" in shape:
            rotation = shape.rotation
            rotation *= 60000
            attributes.append(('rot', rotation))

        if 'flip_h' in shape:
            attributes.append(('flipH', 1))
        if 'flip_v' in shape:
            attributes.append(('flipV', 1))

        self._xml_start_tag('a:xfrm', attributes)

        # Write the a:off element.
        self._write_a_off(col_absolute, row_absolute)

        # Write the a:ext element.
        self._write_a_ext(width, height)

        self._xml_end_tag('a:xfrm')

    def _write_a_off(self, x, y):
        # Write the <a:off> element.
        attributes = [
            ('x', x),
            ('y', y),
        ]

        self._xml_empty_tag('a:off', attributes)

    def _write_a_ext(self, cx, cy):
        # Write the <a:ext> element.
        attributes = [
            ('cx', cx),
            ('cy', cy),
        ]

        self._xml_empty_tag('a:ext', attributes)

    def _write_a_prst_geom(self, shape={}):
        # Write the <a:prstGeom> element.
        attributes = []

        if 'type' in shape:
            attributes = [('prst', shape['type'])]

        self._xml_start_tag('a:prstGeom', attributes)

        # Write the a:avLst element.
        self._write_a_av_lst(shape)

        self._xml_end_tag('a:prstGeom')

    def _write_a_av_lst(self, shape={}):
        # Write the <a:avLst> element.
        adjustments = []

        if 'adjustments' in shape:
            adjustments = shape['adjustments']

        if adjustments:
            self._xml_start_tag('a:avLst')

            i = 0
            for adj in adjustments:
                i += 1
                # Only connectors have multiple adjustments.
                if 'connect' in shape:
                    suffix = i
                else:
                    suffix = ''

                # Scale Adjustments: 100,000 = 100%.
                adj_int = int(adj * 1000)

                attributes = [('name', 'adj' + suffix),
                              ('fmla', 'val' + adj_int)]

                self._xml_empty_tag('a:gd', attributes)

            self._xml_end_tag('a:avLst')
        else:
            self._xml_empty_tag('a:avLst')

    def _write_a_solid_fill(self, rgb):
        # Write the <a:solidFill> element.
        if not rgb is not None:
            rgb = '000000'

        attributes = [('val', rgb)]

        self._xml_start_tag('a:solidFill')

        self._xml_empty_tag('a:srgbClr', attributes)

        self._xml_end_tag('a:solidFill')

    def _write_a_ln(self, shape={}):
        # Write the <a:ln> element.
        weight = shape.line_weight

        attributes = [('w', weight * 9525)]

        self._xml_start_tag('a:ln', attributes)

        line = shape.line

        if len(line) > 1:

            # Write the a:solidFill element.
            self._write_a_solid_fill(line)
        else:
            self._xml_empty_tag('a:noFill')

        if shape.line_type:

            attributes = [('val', shape.line_type)]
            self._xml_empty_tag('a:prstDash', attributes)

        if shape.connect:
            self._xml_empty_tag('a:round')
        else:
            attributes = [('lim', 800000)]
            self._xml_empty_tag('a:miter', attributes)

        self._xml_empty_tag('a:headEnd')
        self._xml_empty_tag('a:tailEnd')

        self._xml_end_tag('a:ln')

    def _write_txBody(self, col_absolute, row_absolute, width, height, shape):
        # Write the <xdr:txBody> element.
        attributes = [
            ('vertOverflow', "clip"),
            ('wrap', "square"),
            ('lIns', "27432"),
            ('tIns', "22860"),
            ('rIns', "27432"),
            ('bIns', "22860"),
            ('anchor', shape.valign),
            ('upright', "1"),
        ]

        self._xml_start_tag('xdr:txBody')
        self._xml_empty_tag('a:bodyPr', attributes)
        self._xml_empty_tag('a:lstStyle')

        self._xml_start_tag('a:p')

        rotation = shape.format.rotation
        if not rotation is not None:
            rotation = 0
        rotation *= 60000

        attributes = [('algn', shape.align), ('rtl', rotation)]
        self._xml_start_tag('a:pPr', attributes)

        attributes = [('sz', "1000")]
        self._xml_empty_tag('a:defRPr', attributes)

        self._xml_end_tag('a:pPr')
        self._xml_start_tag('a:r')

        size = shape.format.size
        if not size is not None:
            size = 8
        size *= 100

        bold = shape.format.bold
        if not bold is not None:
            bold = 0

        italic = shape.format.italic
        if not italic is not None:
            italic = 0

        underline = shape['format']['underline']
        if underline:
            underline = 'sng'
        else:
            underline = 'none'

        strike = shape['format']['font_strikeout']
        if strike:
            strike = 'Strike'
        else:
            strike = 'noStrike'

        attributes = [
            ('lang', 'en-US'),
            ('sz', size),
            ('b', bold),
            ('i', italic),
            ('u', underline),
            ('strike', strike),
            ('baseline', 0),
        ]

        self._xml_start_tag('a:rPr', attributes)

        color = shape.format.color
        if color is not None:
            color = shape._get_palette_color(color)
            # color =~ s/^FF//; # Remove leading FF from rgb for shape color.
        else:
            color = '000000'

        self._write_a_solid_fill(color)

        font = shape.format.font
        if font is not None:
            font = 'Calibri'

        attributes = [('typeface', font)]
        self._xml_empty_tag('a:latin', attributes)

        self._xml_empty_tag('a:cs', attributes)

        self._xml_end_tag('a:rPr')

        self._xml_data_element('a:t', shape.text)

        self._xml_end_tag('a:r')
        self._xml_end_tag('a:p')
        self._xml_end_tag('xdr:txBody')

########NEW FILE########
__FILENAME__ = format
###############################################################################
#
# Format - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Package imports.
from . import xmlwriter
from .utility import encode_utf8


class Format(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Format file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, properties={}, xf_indicies=None, dxf_indicies=None):
        """
        Constructor.

        """

        super(Format, self).__init__()

        self.xf_format_indices = xf_indicies
        self.dxf_format_indices = dxf_indicies
        self.xf_index = None
        self.dxf_index = None

        self.num_format = 0
        self.num_format_index = 0
        self.font_index = 0
        self.has_font = 0
        self.has_dxf_font = 0

        self.bold = 0
        self.underline = 0
        self.italic = 0
        self.font_name = 'Calibri'
        self.font_size = 11
        self.font_color = 0x0
        self.font_strikeout = 0
        self.font_outline = 0
        self.font_shadow = 0
        self.font_script = 0
        self.font_family = 2
        self.font_charset = 0
        self.font_scheme = 'minor'
        self.font_condense = 0
        self.font_extend = 0
        self.theme = 0
        self.hyperlink = 0

        self.hidden = 0
        self.locked = 1

        self.text_h_align = 0
        self.text_wrap = 0
        self.text_v_align = 0
        self.text_justlast = 0
        self.rotation = 0

        self.fg_color = 0
        self.bg_color = 0
        self.pattern = 0
        self.has_fill = 0
        self.has_dxf_fill = 0
        self.fill_index = 0
        self.fill_count = 0

        self.border_index = 0
        self.has_border = 0
        self.has_dxf_border = 0
        self.border_count = 0

        self.bottom = 0
        self.bottom_color = 0
        self.diag_border = 0
        self.diag_color = 0
        self.diag_type = 0
        self.left = 0
        self.left_color = 0
        self.right = 0
        self.right_color = 0
        self.top = 0
        self.top_color = 0

        self.indent = 0
        self.shrink = 0
        self.merge_range = 0
        self.reading_order = 0
        self.just_distrib = 0
        self.color_indexed = 0
        self.font_only = 0

        # Convert properties in the constructor to method calls.
        for key, value in properties.items():
            getattr(self, 'set_' + key)(value)

    ###########################################################################
    #
    # Format properties.
    #
    ###########################################################################

    def set_font_name(self, font_name):
        """
        Set the Format font_name property such as 'Time New Roman'. The
        default Excel font is 'Calibri'.

        Args:
            font_name: String with the font name. No default.

        Returns:
            Nothing.

        """
        self.font_name = encode_utf8(font_name)

    def set_font_size(self, font_size=11):
        """
        Set the Format font_size property. The default Excel font size is 11.

        Args:
            font_size: Int with font size. No default.

        Returns:
            Nothing.

        """
        self.font_size = font_size

    def set_font_color(self, font_color):
        """
        Set the Format font_color property. The Excel default is black.

        Args:
            font_color: String with the font color. No default.

        Returns:
            Nothing.

        """
        self.font_color = self._get_color(font_color)

    def set_bold(self, bold=1):
        """
        Set the Format bold property.

        Args:
            bold: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.bold = bold

    def set_italic(self, italic=1):
        """
        Set the Format italic property.

        Args:
            italic: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.italic = italic

    def set_underline(self, underline=1):
        """
        Set the Format underline property.

        Args:
            underline: Default is 1, single underline.

        Returns:
            Nothing.

        """
        self.underline = underline

    def set_font_strikeout(self, font_strikeout=1):
        """
        Set the Format font_strikeout property.

        Args:
            font_strikeout: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.font_strikeout = font_strikeout

    def set_font_script(self, font_script=1):
        """
        Set the Format font_script property.

        Args:
            font_script: Default is 1, superscript.

        Returns:
            Nothing.

        """
        self.font_script = font_script

    def set_font_outline(self, font_outline=1):
        """
        Set the Format font_outline property.

        Args:
            font_outline: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.font_outline = font_outline

    def set_font_shadow(self, font_shadow=1):
        """
        Set the Format font_shadow property.

        Args:
            font_shadow: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.font_shadow = font_shadow

    def set_num_format(self, num_format):
        """
        Set the Format num_format property such as '#,##0'.

        Args:
            num_format: String representing the number format. No default.

        Returns:
            Nothing.

        """
        try:
            float(num_format)
        except ValueError:
            num_format = encode_utf8(num_format)

        self.num_format = num_format

    def set_locked(self, locked=1):
        """
        Set the Format locked property.

        Args:
            locked: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.locked = locked

    def set_hidden(self, hidden=1):
        """
        Set the Format hidden property.

        Args:
            hidden: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.hidden = hidden

    def set_align(self, alignment):
        """
        Set the Format cell alignment.

        Args:
            alignment: String representing alignment. No default.

        Returns:
            Nothing.
        """
        alignment = alignment.lower()

        # Set horizontal alignment properties.
        if alignment == 'left':
            self.set_text_h_align(1)
        if alignment == 'centre':
            self.set_text_h_align(2)
        if alignment == 'center':
            self.set_text_h_align(2)
        if alignment == 'right':
            self.set_text_h_align(3)
        if alignment == 'fill':
            self.set_text_h_align(4)
        if alignment == 'justify':
            self.set_text_h_align(5)
        if alignment == 'center_across':
            self.set_text_h_align(6)
        if alignment == 'centre_across':
            self.set_text_h_align(6)
        if alignment == 'distributed':
            self.set_text_h_align(7)
        if alignment == 'justify_distributed':
            self.set_text_h_align(7)

        if alignment == 'justify_distributed':
            self.just_distrib = 1

        # Set vertical alignment properties.
        if alignment == 'top':
            self.set_text_v_align(1)
        if alignment == 'vcentre':
            self.set_text_v_align(2)
        if alignment == 'vcenter':
            self.set_text_v_align(2)
        if alignment == 'bottom':
            self.set_text_v_align(3)
        if alignment == 'vjustify':
            self.set_text_v_align(4)
        if alignment == 'vdistributed':
            self.set_text_v_align(5)

    def set_center_across(self, center_across=1):
        """
        Set the Format center_across property.

        Args:
            center_across: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.center_across = center_across

    def set_text_wrap(self, text_wrap=1):
        """
        Set the Format text_wrap property.

        Args:
            text_wrap: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.text_wrap = text_wrap

    def set_rotation(self, rotation):
        """
        Set the Format rotation property.

        Args:
            rotation: Rotation angle. No default.

        Returns:
            Nothing.

        """
        rotation = int(rotation)

        # Map user angle to Excel angle.
        if rotation == 270:
            rotation = 255
        elif rotation >= -90 or rotation <= 90:
            if rotation < 0:
                rotation = -rotation + 90
        else:
            raise Exception(
                "Rotation rotation outside range: -90 <= angle <= 90")

        self.rotation = rotation

    def set_indent(self, indent=1):
        """
        Set the Format indent property.

        Args:
            indent: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.indent = indent

    def set_shrink(self, shrink=1):
        """
        Set the Format shrink property.

        Args:
            shrink: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.shrink = shrink

    def set_text_justlast(self, text_justlast=1):
        """
        Set the Format text_justlast property.

        Args:
            text_justlast: Default is 1, turns property on.

        Returns:
            Nothing.

        """
        self.text_justlast = text_justlast

    def set_pattern(self, pattern=1):
        """
        Set the Format pattern property.

        Args:
            pattern: Default is 1, solid fill.

        Returns:
            Nothing.

        """
        self.pattern = pattern

    def set_bg_color(self, bg_color):
        """
        Set the Format bg_color property.

        Args:
            bg_color: Background colour. No default.

        Returns:
            Nothing.

        """
        self.bg_color = self._get_color(bg_color)

    def set_fg_color(self, fg_color):
        """
        Set the Format fg_color property.

        Args:
            fg_color: Foreground colour. No default.

        Returns:
            Nothing.

        """
        self.fg_color = self._get_color(fg_color)

    # set_border(style) Set cells borders to the same style
    def set_border(self, style=1):
        """
        Set the Format bottom property.

        Args:
            bottom: Default is 1, border type 1.

        Returns:
            Nothing.

        """
        self.set_bottom(style)
        self.set_top(style)
        self.set_left(style)
        self.set_right(style)

    # set_border_color(color) Set cells border to the same color
    def set_border_color(self, color):
        """
        Set the Format bottom property.

        Args:
            color: Color string. No default.

        Returns:
            Nothing.

        """
        self.set_bottom_color(color)
        self.set_top_color(color)
        self.set_left_color(color)
        self.set_right_color(color)

    def set_bottom(self, bottom=1):
        """
        Set the Format bottom property.

        Args:
            bottom: Default is 1, border type 1.

        Returns:
            Nothing.

        """
        self.bottom = bottom

    def set_bottom_color(self, bottom_color):
        """
        Set the Format bottom_color property.

        Args:
            bottom_color: Color string. No default.

        Returns:
            Nothing.

        """
        self.bottom_color = self._get_color(bottom_color)

    def set_diag_type(self, diag_type=1):
        """
        Set the Format diag_type property.

        Args:
            diag_type: Default is 1, border type 1.

        Returns:
            Nothing.

        """
        self.diag_type = diag_type

    def set_left(self, left=1):
        """
        Set the Format left property.

        Args:
            left: Default is 1, border type 1.

        Returns:
            Nothing.

        """
        self.left = left

    def set_left_color(self, left_color):
        """
        Set the Format left_color property.

        Args:
            left_color: Color string. No default.

        Returns:
            Nothing.

        """
        self.left_color = self._get_color(left_color)

    def set_right(self, right=1):
        """
        Set the Format right property.

        Args:
            right: Default is 1, border type 1.

        Returns:
            Nothing.

        """
        self.right = right

    def set_right_color(self, right_color):
        """
        Set the Format right_color property.

        Args:
            right_color: Color string. No default.

        Returns:
            Nothing.

        """
        self.right_color = self._get_color(right_color)

    def set_top(self, top=1):
        """
        Set the Format top property.

        Args:
            top: Default is 1, border type 1.

        Returns:
            Nothing.

        """
        self.top = top

    def set_top_color(self, top_color):
        """
        Set the Format top_color property.

        Args:
            top_color: Color string. No default.

        Returns:
            Nothing.

        """
        self.top_color = self._get_color(top_color)

    def set_diag_color(self, diag_color):
        """
        Set the Format diag_color property.

        Args:
            diag_color: Color string. No default.

        Returns:
            Nothing.

        """
        self.diag_color = self._get_color(diag_color)

    def set_diag_border(self, diag_border=1):
        """
        Set the Format diag_border property.

        Args:
            diag_border: Default is 1, border type 1.

        Returns:
            Nothing.

        """
        self.diag_border = diag_border

    ###########################################################################
    #
    # Internal Format properties. These aren't documented since they are
    # either only used internally or else are unlikely to be set by the user.
    #
    ###########################################################################

    def set_has_font(self, has_font=1):
        # Set the has_font property.
        self.has_font = has_font

    def set_has_fill(self, has_fill=1):
        # Set the has_fill property.
        self.has_fill = has_fill

    def set_font_index(self, font_index):
        # Set the font_index property.
        self.font_index = font_index

    def set_xf_index(self, xf_index):
        # Set the xf_index property.
        self.xf_index = xf_index

    def set_dxf_index(self, dxf_index):
        # Set the xf_index property.
        self.dxf_index = dxf_index

    def set_num_format_index(self, num_format_index):
        # Set the num_format_index property.
        self.num_format_index = num_format_index

    def set_text_h_align(self, text_h_align):
        # Set the text_h_align property.
        self.text_h_align = text_h_align

    def set_text_v_align(self, text_v_align):
        # Set the text_v_align property.
        self.text_v_align = text_v_align

    def set_reading_order(self, reading_order=1):
        # Set the reading_order property.
        self.reading_order = reading_order

    def set_valign(self, align):
        # Set vertical cell alignment. This is required by the constructor
        # properties dict to differentiate between the vertical and horizontal
        # properties.
        self.set_align(align)

    def set_font_family(self, font_family):
        # Set the Format font_family property.

        self.font_family = font_family

    def set_font_charset(self, font_charset):
        # Set the Format font_charset property.
        self.font_charset = font_charset

    def set_font_scheme(self, font_scheme):
        # Set the Format font_scheme property.
        self.font_scheme = font_scheme

    def set_font_condense(self, font_condense):
        # Set the Format font_condense property.
        self.font_condense = font_condense

    def set_font_extend(self, font_extend):
        # Set the Format font_extend property.
        self.font_extend = font_extend

    def set_theme(self, theme):
        # Set the Format theme property.
        self.theme = theme

    def set_hyperlink(self, hyperlink=1):
        # Set the properties for the hyperlink style. This doesn't
        # currently work. To be fixed when styles are supported.

        self.set_underline(1)
        self.set_theme(10)
        self.set_align('top')
        self.hyperlink = hyperlink

    def set_color_indexed(self, color_index):
        # Used in the cell comment format.
        self.color_indexed = color_index

    def set_font_only(self, font_only=True):
        # Used in the cell comment format.
        self.font_only = font_only

    # Compatibility methods.
    def set_font(self, font_name):
        #  For compatibility with Excel::Writer::XLSX.
        self.font_name = font_name

    def set_size(self, font_size):
        #  For compatibility with Excel::Writer::XLSX.
        self.font_size = font_size

    def set_color(self, font_color):
        #  For compatibility with Excel::Writer::XLSX.
        self.font_color = self._get_color(font_color)

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        # Close the file.
        self._xml_close()

    def _get_align_properties(self):
        # Return properties for an Style xf <alignment> sub-element.
        changed = 0
        align = []

        # Check if any alignment options in the format have been changed.
        if (self.text_h_align or self.text_v_align or self.indent
                or self.rotation or self.text_wrap or self.shrink
                or self.reading_order):
            changed = 1
        else:
            return changed, align

        # Indent is only allowed for horizontal left, right and distributed.
        # If it is defined for any other alignment or no alignment has
        # been set then default to left alignment.
        if (self.indent
                and self.text_h_align != 1
                and self.text_h_align != 3
                and self.text_h_align != 7):
            self.text_h_align = 1

        # Check for properties that are mutually exclusive.
        if self.text_wrap:
            self.shrink = 0
        if self.text_h_align == 4:
            self.shrink = 0
        if self.text_h_align == 5:
            self.shrink = 0
        if self.text_h_align == 7:
            self.shrink = 0
        if self.text_h_align != 7:
            self.just_distrib = 0
        if self.indent:
            self.just_distrib = 0

        continuous = 'centerContinuous'

        if self.text_h_align == 1:
            align.append(('horizontal', 'left'))
        if self.text_h_align == 2:
            align.append(('horizontal', 'center'))
        if self.text_h_align == 3:
            align.append(('horizontal', 'right'))
        if self.text_h_align == 4:
            align.append(('horizontal', 'fill'))
        if self.text_h_align == 5:
            align.append(('horizontal', 'justify'))
        if self.text_h_align == 6:
            align.append(('horizontal', continuous))
        if self.text_h_align == 7:
            align.append(('horizontal', 'distributed'))

        if self.just_distrib:
            align.append(('justifyLastLine', 1))

        # Property 'vertical' => 'bottom' is a default. It sets applyAlignment
        # without an alignment sub-element.
        if self.text_v_align == 1:
            align.append(('vertical', 'top'))
        if self.text_v_align == 2:
            align.append(('vertical', 'center'))
        if self.text_v_align == 4:
            align.append(('vertical', 'justify'))
        if self.text_v_align == 5:
            align.append(('vertical', 'distributed'))

        if self.indent:
            align.append(('indent', self.indent))
        if self.rotation:
            align.append(('textRotation', self.rotation))

        if self.text_wrap:
            align.append(('wrapText', 1))
        if self.shrink:
            align.append(('shrinkToFit', 1))

        if self.reading_order == 1:
            align.append(('readingOrder', 1))
        if self.reading_order == 2:
            align.append(('readingOrder', 2))

        return changed, align

    def _get_protection_properties(self):
        # Return properties for an Excel XML <Protection> element.
        attribs = []

        if not self.locked:
            attribs.append(('locked', 0))
        if self.hidden:
            attribs.append(('hidden', 1))

        return attribs

    def _get_format_key(self):
        # Returns a unique hash key for a font. Used by Workbook.
        key = ':'.join(str(x) for x in (
            self._get_font_key(),
            self._get_border_key(),
            self._get_fill_key(),
            self._get_alignment_key(),
            self.num_format,
            self.locked,
            self.hidden))

        return key

    def _get_font_key(self):
        # Returns a unique hash key for a font. Used by Workbook.
        key = ':'.join(str(x) for x in (
            self.bold,
            self.font_color,
            self.font_charset,
            self.font_family,
            self.font_outline,
            self.font_script,
            self.font_shadow,
            self.font_strikeout,
            self.font_name,
            self.italic,
            self.font_size,
            self.underline))

        return key

    def _get_border_key(self):
        # Returns a unique hash key for a border style. Used by Workbook.
        key = ':'.join(str(x) for x in (
            self.bottom,
            self.bottom_color,
            self.diag_border,
            self.diag_color,
            self.diag_type,
            self.left,
            self.left_color,
            self.right,
            self.right_color,
            self.top,
            self.top_color))

        return key

    def _get_fill_key(self):
        # Returns a unique hash key for a fill style. Used by Workbook.
        key = ':'.join(str(x) for x in (
            self.pattern,
            self.bg_color,
            self.fg_color))

        return key

    def _get_alignment_key(self):
        # Returns a unique hash key for alignment formats.

        key = ':'.join(str(x) for x in (
            self.text_h_align,
            self.text_v_align,
            self.indent,
            self.rotation,
            self.text_wrap,
            self.shrink,
            self.reading_order))

        return key

    def _get_xf_index(self):
        # Returns the index index number used by Excel to identify a format.
        if self.xf_index is not None:
            # Format already has an index number so return it.
            return self.xf_index
        else:
            # Format doesn't have an index number so assign one.
            key = self._get_format_key()

            if key in self.xf_format_indices:
                # Format matches existing format with an index.
                return self.xf_format_indices[key]
            else:
                # New format requiring an index. Note. +1 since Excel
                # has an implicit "General" format at index 0.
                index = 1 + len(self.xf_format_indices)
                self.xf_format_indices[key] = index
                self.xf_index = index
                return index

    def _get_dxf_index(self):
        # Returns the index index number used by Excel to identify a format.
        if self.dxf_index is not None:
            # Format already has an index number so return it.
            return self.dxf_index
        else:
            # Format doesn't have an index number so assign one.
            key = self._get_format_key()

            if key in self.dxf_format_indices:
                # Format matches existing format with an index.
                return self.dxf_format_indices[key]
            else:
                # New format requiring an index.
                index = len(self.dxf_format_indices)
                self.dxf_format_indices[key] = index
                self.dxf_index = index
                return index

    def _get_color(self, color):
        # Used in conjunction with the set_xxx_color methods to convert a
        # colour name into an RGB formatted string. These colours are for
        # backward compatibility with older versions of Excel.
        named_colors = {
            'black': '#000000',
            'blue': '#0000FF',
            'brown': '#800000',
            'cyan': '#00FFFF',
            'gray': '#808080',
            'green': '#008000',
            'lime': '#00FF00',
            'magenta': '#FF00FF',
            'navy': '#000080',
            'orange': '#FF6600',
            'pink': '#FF00FF',
            'purple': '#800080',
            'red': '#FF0000',
            'silver': '#C0C0C0',
            'white': '#FFFFFF',
            'yellow': '#FFFF00',
        }

        if color in named_colors:
            color = named_colors[color]

        return color

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

########NEW FILE########
__FILENAME__ = packager
###############################################################################
#
# Packager - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Standard packages.
import os
from shutil import copy

# Package imports.
from xlsxwriter.app import App
from xlsxwriter.contenttypes import ContentTypes
from xlsxwriter.core import Core
from xlsxwriter.relationships import Relationships
from xlsxwriter.sharedstrings import SharedStrings
from xlsxwriter.styles import Styles
from xlsxwriter.theme import Theme
from xlsxwriter.vml import Vml
from xlsxwriter.table import Table
from xlsxwriter.comments import Comments


class Packager(object):
    """
    A class for writing the Excel XLSX Packager file.

    This module is used in conjunction with XlsxWriter to create an
    Excel XLSX container file.

    From Wikipedia: The Open Packaging Conventions (OPC) is a
    container-file technology initially created by Microsoft to store
    a combination of XML and non-XML files that together form a single
    entity such as an Open XML Paper Specification (OpenXPS)
    document. http://en.wikipedia.org/wiki/Open_Packaging_Conventions.

    At its simplest an Excel XLSX file contains the following elements:

         ____ [Content_Types].xml
        |
        |____ docProps
        | |____ app.xml
        | |____ core.xml
        |
        |____ xl
        | |____ workbook.xml
        | |____ worksheets
        | | |____ sheet1.xml
        | |
        | |____ styles.xml
        | |
        | |____ theme
        | | |____ theme1.xml
        | |
        | |_____rels
        | |____ workbook.xml.rels
        |
        |_____rels
          |____ .rels

    The Packager class coordinates the classes that represent the
    elements of the package and writes them into the XLSX file.

    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(Packager, self).__init__()

        self.package_dir = ''
        self.workbook = None
        self.sheet_names = []
        self.worksheet_count = 0
        self.chartsheet_count = 0
        self.chart_count = 0
        self.drawing_count = 0
        self.table_count = 0
        self.named_ranges = []

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _set_package_dir(self, package_dir):
        # Set the XLSX OPC package directory.
        self.package_dir = package_dir

    def _add_workbook(self, workbook):
        # Add the Excel::Writer::XLSX::Workbook object to the package.
        self.workbook = workbook
        self.sheet_names = workbook.sheetnames
        self.chart_count = len(workbook.charts)
        self.drawing_count = len(workbook.drawings)
        self.num_vml_files = workbook.num_vml_files
        self.num_comment_files = workbook.num_comment_files
        self.named_ranges = workbook.named_ranges

        for worksheet in self.workbook.worksheets():
            if worksheet.is_chartsheet:
                self.chartsheet_count += 1
            else:
                self.worksheet_count += 1

    def _create_package(self):
        # Write the xml files that make up the XLSX OPC package.
        self._write_worksheet_files()
        self._write_chartsheet_files()
        self._write_workbook_file()
        self._write_chart_files()
        self._write_drawing_files()
        self._write_vml_files()
        self._write_comment_files()
        self._write_table_files()
        self._write_shared_strings_file()
        self._write_app_file()
        self._write_core_file()
        self._write_content_types_file()
        self._write_styles_file()
        self._write_theme_file()
        self._write_root_rels_file()
        self._write_workbook_rels_file()
        self._write_worksheet_rels_files()
        self._write_chartsheet_rels_files()
        self._write_drawing_rels_files()
        self._add_image_files()
        self._add_vba_project()

    def _write_workbook_file(self):
        # Write the workbook.xml file.
        xlsx_dir = self.package_dir
        workbook = self.workbook

        self._mkdir(xlsx_dir + '/xl')

        workbook._set_xml_writer(xlsx_dir + '/xl/workbook.xml')
        workbook._assemble_xml_file()

    def _write_worksheet_files(self):
        # Write the worksheet files.

        xlsx_dir = self.package_dir

        self._mkdir(xlsx_dir + '/xl')
        self._mkdir(xlsx_dir + '/xl/worksheets')

        index = 1
        for worksheet in self.workbook.worksheets():
            if worksheet.is_chartsheet:
                continue

            if worksheet.optimization == 1:
                worksheet._write_single_row()

            worksheet._set_xml_writer(
                xlsx_dir + '/xl/worksheets/sheet' + str(index) + '.xml')
            worksheet._assemble_xml_file()
            index += 1

    def _write_chartsheet_files(self):
        # Write the chartsheet files.

        xlsx_dir = self.package_dir

        index = 1
        for worksheet in self.workbook.worksheets():
            if not worksheet.is_chartsheet:
                continue

            self._mkdir(xlsx_dir + '/xl')
            self._mkdir(xlsx_dir + '/xl/chartsheets')

            worksheet._set_xml_writer(
                xlsx_dir + '/xl/chartsheets/sheet' + str(index) + '.xml')
            worksheet._assemble_xml_file()
            index += 1

    def _write_chart_files(self):
        # Write the chart files.

        xlsx_dir = self.package_dir

        if not self.workbook.charts:
            return

        self._mkdir(xlsx_dir + '/xl')
        self._mkdir(xlsx_dir + '/xl/charts')

        index = 1
        for chart in self.workbook.charts:
            chart._set_xml_writer(
                xlsx_dir + '/xl/charts/chart' + str(index) + '.xml')
            chart._assemble_xml_file()
            index += 1

    def _write_drawing_files(self):
        # Write the drawing files.

        xlsx_dir = self.package_dir

        if not self.drawing_count:
            return

        self._mkdir(xlsx_dir + '/xl')
        self._mkdir(xlsx_dir + '/xl/drawings')

        index = 1
        for drawing in self.workbook.drawings:
            drawing._set_xml_writer(
                xlsx_dir + '/xl/drawings/drawing' + str(index) + '.xml')
            drawing._assemble_xml_file()
            index += 1

    def _write_vml_files(self):
        # Write the comment VML files.
        xlsx_dir = self.package_dir

        index = 1
        for worksheet in self.workbook.worksheets():
            if not worksheet.has_vml:
                continue

            vml = Vml()

            self._mkdir(xlsx_dir + '/xl')
            self._mkdir(xlsx_dir + '/xl/drawings')

            vml._set_xml_writer(xlsx_dir
                                + '/xl/drawings/vmlDrawing'
                                + str(index) + '.vml')
            vml._assemble_xml_file(worksheet.vml_data_id,
                                   worksheet.vml_shape_id,
                                   worksheet.comments_array,
                                   worksheet.buttons_array)
            index += 1

    def _write_comment_files(self):
        # Write the comment files.
        xlsx_dir = self.package_dir

        index = 1
        for worksheet in self.workbook.worksheets():
            if not worksheet.has_comments:
                continue

            comment = Comments()

            self._mkdir(xlsx_dir + '/xl')
            self._mkdir(xlsx_dir + '/xl/drawings')

            comment._set_xml_writer(xlsx_dir + '/xl/comments'
                                    + str(index) + '.xml')
            comment._assemble_xml_file(worksheet.comments_array)
            index += 1

    def _write_shared_strings_file(self):
        # Write the sharedStrings.xml file.
        xlsx_dir = self.package_dir
        sst = SharedStrings()

        sst.string_table = self.workbook.str_table

        if not self.workbook.str_table.count:
            return

        self._mkdir(xlsx_dir + '/xl')

        sst._set_xml_writer(xlsx_dir + '/xl/sharedStrings.xml')
        sst._assemble_xml_file()

    def _write_app_file(self):
        # Write the app.xml file.
        xlsx_dir = self.package_dir
        properties = self.workbook.doc_properties
        app = App()

        self._mkdir(xlsx_dir + '/docProps')

        # Add the Worksheet heading pairs.
        app._add_heading_pair(['Worksheets', self.worksheet_count])

        # Add the Chartsheet heading pairs.
        app._add_heading_pair(['Charts', self.chartsheet_count])

        # Add the Worksheet parts.
        for worksheet in self.workbook.worksheets():
            if worksheet.is_chartsheet:
                continue
            app._add_part_name(worksheet.name)

        # Add the Chartsheet parts.
        for worksheet in self.workbook.worksheets():
            if not worksheet.is_chartsheet:
                continue
            app._add_part_name(worksheet.name)

        # Add the Named Range heading pairs.
        if self.named_ranges:
            app._add_heading_pair(['Named Ranges', len(self.named_ranges)])

        # Add the Named Ranges parts.
        for named_range in self.named_ranges:
            app._add_part_name(named_range)

        app._set_properties(properties)

        app._set_xml_writer(xlsx_dir + '/docProps/app.xml')
        app._assemble_xml_file()

    def _write_core_file(self):
        # Write the core.xml file.
        xlsx_dir = self.package_dir
        properties = self.workbook.doc_properties
        core = Core()

        self._mkdir(xlsx_dir + '/docProps')

        core._set_properties(properties)
        core._set_xml_writer(xlsx_dir + '/docProps/core.xml')
        core._assemble_xml_file()

    def _write_content_types_file(self):
        # Write the ContentTypes.xml file.
        xlsx_dir = self.package_dir
        content = ContentTypes()

        content._add_image_types(self.workbook.image_types)

        worksheet_index = 1
        chartsheet_index = 1
        for worksheet in self.workbook.worksheets():
            if worksheet.is_chartsheet:
                content._add_chartsheet_name('sheet' + str(chartsheet_index))
                chartsheet_index += 1
            else:
                content._add_worksheet_name('sheet' + str(worksheet_index))
                worksheet_index += 1

        for i in range(1, self.chart_count + 1):
            content._add_chart_name('chart' + str(i))

        for i in range(1, self.drawing_count + 1):
            content._add_drawing_name('drawing' + str(i))

        if self.num_vml_files:
            content._add_vml_name()

        for i in range(1, self.table_count + 1):
            content._add_table_name('table' + str(i))

        for i in range(1, self.num_comment_files + 1):
            content._add_comment_name('comments' + str(i))

        # Add the sharedString rel if there is string data in the workbook.
        if self.workbook.str_table.count:
            content._add_shared_strings()

        # Add vbaProject if present.
        if self.workbook.vba_project:
            content._add_vba_project()

        content._set_xml_writer(xlsx_dir + '/[Content_Types].xml')
        content._assemble_xml_file()

    def _write_styles_file(self):
        # Write the style xml file.
        xlsx_dir = self.package_dir
        xf_formats = self.workbook.xf_formats
        palette = self.workbook.palette
        font_count = self.workbook.font_count
        num_format_count = self.workbook.num_format_count
        border_count = self.workbook.border_count
        fill_count = self.workbook.fill_count
        custom_colors = self.workbook.custom_colors
        dxf_formats = self.workbook.dxf_formats

        styles = Styles()

        self._mkdir(xlsx_dir + '/xl')

        styles._set_style_properties([
            xf_formats,
            palette,
            font_count,
            num_format_count,
            border_count,
            fill_count,
            custom_colors,
            dxf_formats])

        styles._set_xml_writer(xlsx_dir + '/xl/styles.xml')
        styles._assemble_xml_file()

    def _write_theme_file(self):
        # Write the theme xml file.
        xlsx_dir = self.package_dir
        theme = Theme()

        self._mkdir(xlsx_dir + '/xl')
        self._mkdir(xlsx_dir + '/xl/theme')

        theme._set_xml_writer(xlsx_dir + '/xl/theme/theme1.xml')
        theme._assemble_xml_file()

    def _write_table_files(self):
        # Write the table files.
        xlsx_dir = self.package_dir

        index = 1
        for worksheet in self.workbook.worksheets():
            table_props = worksheet.tables

            if not table_props:
                continue

            self._mkdir(xlsx_dir + '/xl')
            self._mkdir(xlsx_dir + '/xl/tables')

            for table_props in table_props:
                table = Table()
                table._set_xml_writer(xlsx_dir + '/xl/tables/table'
                                      + str(index) + '.xml')
                table._set_properties(table_props)
                table._assemble_xml_file()
                self.table_count += 1
                index += 1

    def _write_root_rels_file(self):
        # Write the _rels/.rels xml file.

        xlsx_dir = self.package_dir
        rels = Relationships()

        self._mkdir(xlsx_dir + '/_rels')

        rels._add_document_relationship('/officeDocument', 'xl/workbook.xml')
        rels._add_package_relationship('/metadata/core-properties',
                                       'docProps/core.xml')
        rels._add_document_relationship('/extended-properties',
                                        'docProps/app.xml')

        rels._set_xml_writer(xlsx_dir + '/_rels/.rels')
        rels._assemble_xml_file()

    def _write_workbook_rels_file(self):
        # Write the _rels/.rels xml file.

        xlsx_dir = self.package_dir
        rels = Relationships()

        self._mkdir(xlsx_dir + '/xl')
        self._mkdir(xlsx_dir + '/xl/_rels')

        worksheet_index = 1
        chartsheet_index = 1

        for worksheet in self.workbook.worksheets():
            if worksheet.is_chartsheet:
                rels._add_document_relationship('/chartsheet',
                                                'chartsheets/sheet'
                                                + str(chartsheet_index)
                                                + '.xml')
                chartsheet_index += 1
            else:
                rels._add_document_relationship('/worksheet',
                                                'worksheets/sheet'
                                                + str(worksheet_index)
                                                + '.xml')
                worksheet_index += 1

        rels._add_document_relationship('/theme', 'theme/theme1.xml')
        rels._add_document_relationship('/styles', 'styles.xml')

        # Add the sharedString rel if there is string data in the workbook.
        if self.workbook.str_table.count:
            rels._add_document_relationship('/sharedStrings',
                                            'sharedStrings.xml')

        # Add vbaProject if present.
        if self.workbook.vba_project:
            rels._add_ms_package_relationship('/vbaProject', 'vbaProject.bin')

        rels._set_xml_writer(xlsx_dir + '/xl/_rels/workbook.xml.rels')
        rels._assemble_xml_file()

    def _write_worksheet_rels_files(self):
        # data such as hyperlinks or drawings.
        xlsx_dir = self.package_dir

        index = 0
        for worksheet in self.workbook.worksheets():

            if worksheet.is_chartsheet:
                continue

            index += 1

            external_links = (worksheet.external_hyper_links +
                              worksheet.external_drawing_links +
                              worksheet.external_vml_links +
                              worksheet.external_table_links +
                              worksheet.external_comment_links)

            if not external_links:
                continue

            # Create the worksheet .rels dirs.
            self._mkdir(xlsx_dir + '/xl')
            self._mkdir(xlsx_dir + '/xl/worksheets')
            self._mkdir(xlsx_dir + '/xl/worksheets/_rels')

            rels = Relationships()

            for link_data in external_links:
                rels._add_worksheet_relationship(*link_data)

            # Create .rels file such as /xl/worksheets/_rels/sheet1.xml.rels.
            rels._set_xml_writer(xlsx_dir + '/xl/worksheets/_rels/sheet'
                                 + str(index) + '.xml.rels')
            rels._assemble_xml_file()

    def _write_chartsheet_rels_files(self):
        # Write the chartsheet .rels files for links to drawing files.

        xlsx_dir = self.package_dir

        index = 0
        for worksheet in self.workbook.worksheets():

            if not worksheet.is_chartsheet:
                continue

            index += 1

            external_links = worksheet.external_drawing_links

            if not external_links:
                continue

            # Create the chartsheet .rels xlsx_dir.
            self._mkdir(xlsx_dir + '/xl')
            self._mkdir(xlsx_dir + '/xl/chartsheets')
            self._mkdir(xlsx_dir + '/xl/chartsheets/_rels')

            rels = Relationships()

            for link_data in external_links:
                rels._add_worksheet_relationship(link_data)

            # Create .rels file such as /xl/chartsheets/_rels/sheet1.xml.rels.
            rels._set_xml_writer(xlsx_dir + '/xl/chartsheets/_rels/sheet'
                                 + str(index) + '.xml.rels')
            rels._assemble_xml_file()

    def _write_drawing_rels_files(self):
        # Write the drawing .rels files for worksheets with charts or drawings.

        xlsx_dir = self.package_dir

        index = 0
        for worksheet in self.workbook.worksheets():
            if not worksheet.drawing_links:
                continue
            index += 1

            # Create the drawing .rels xlsx_dir.
            self._mkdir(xlsx_dir + '/xl')
            self._mkdir(xlsx_dir + '/xl/drawings')
            self._mkdir(xlsx_dir + '/xl/drawings/_rels')

            rels = Relationships()

            for drawing_data in worksheet.drawing_links:
                rels._add_document_relationship(*drawing_data)

            # Create .rels file such as /xl/drawings/_rels/sheet1.xml.rels.
            rels._set_xml_writer(xlsx_dir + '/xl/drawings/_rels/drawing'
                                 + str(index) + '.xml.rels')
            rels._assemble_xml_file()

    def _add_image_files(self):
        # Write the /xl/media/image?.xml files.
        xlsx_dir = self.package_dir
        workbook = self.workbook
        index = 1

        for image in workbook.images:
            filename = image[0]
            extension = '.' + image[1]

            self._mkdir(xlsx_dir + '/xl')
            self._mkdir(xlsx_dir + '/xl/media')

            copy(filename,
                 xlsx_dir + '/xl/media/image' + str(index) + extension)
            index += 1

    def _add_vba_project(self):
        # Write the vbaProject.bin file.
        # xlsx_dir = self.package_dir
        vba_project = self.workbook.vba_project

        if not vba_project:
            return

        # self._mkdir(xlsx_dir + '/xl')
        # copy(vba_project, xlsx_dir + '/xl/vbaProject.bin')

    def _mkdir(self, xlsx_dir):
        # TODO Wrap in try/catch.
        if not os.path.exists(xlsx_dir):
            os.makedirs(xlsx_dir)

########NEW FILE########
__FILENAME__ = relationships
###############################################################################
#
# Relationships - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Package imports.
from . import xmlwriter

# Long namespace strings used in the class.
schema_root = 'http://schemas.openxmlformats.org'
package_schema = schema_root + '/package/2006/relationships'
document_schema = schema_root + '/officeDocument/2006/relationships'


class Relationships(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Relationships file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(Relationships, self).__init__()

        self.relationships = []
        self.id = 1

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        self._write_relationships()

        # Close the file.
        self._xml_close()

    def _add_document_relationship(self, rel_type, target):
        # Add container relationship to XLSX .rels xml files.
        rel_type = document_schema + rel_type

        self.relationships.append((rel_type, target, None))

    def _add_package_relationship(self, rel_type, target):
        # Add container relationship to XLSX .rels xml files.
        rel_type = package_schema + rel_type

        self.relationships.append((rel_type, target, None))

    def _add_ms_package_relationship(self, rel_type, target):
        # Add container relationship to XLSX .rels xml files. Uses MS schema.
        schema = 'http://schemas.microsoft.com/office/2006/relationships'
        rel_type = schema + rel_type

        self.relationships.append((rel_type, target, None))

    def _add_worksheet_relationship(self, rel_type, target, target_mode=None):
        # Add worksheet relationship to sheet.rels xml files.
        rel_type = document_schema + rel_type

        self.relationships.append((rel_type, target, target_mode))

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_relationships(self):
        # Write the <Relationships> element.
        attributes = [('xmlns', package_schema,)]

        self._xml_start_tag('Relationships', attributes)

        for relationship in self.relationships:
            self._write_relationship(relationship)

        self._xml_end_tag('Relationships')

    def _write_relationship(self, relationship):
        # Write the <Relationship> element.
        rel_type, target, target_mode = relationship

        attributes = [
            ('Id', 'rId' + str(self.id)),
            ('Type', rel_type),
            ('Target', target),
        ]

        self.id += 1

        if target_mode:
            attributes.append(('TargetMode', target_mode))

        self._xml_empty_tag('Relationship', attributes)

########NEW FILE########
__FILENAME__ = sharedstrings
###############################################################################
#
# SharedStrings - A class for writing the Excel XLSX sharedStrings file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Standard packages.
import re

# Package imports.
from . import xmlwriter


class SharedStrings(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX sharedStrings file.

    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(SharedStrings, self).__init__()

        self.string_table = None

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        # Write the sst element.
        self._write_sst()

        # Write the sst strings.
        self._write_sst_strings()

        # Close the sst tag.
        self._xml_end_tag('sst')

        # Close the file.
        self._xml_close()

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_sst(self):
        # Write the <sst> element.
        xmlns = 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'

        attributes = [
            ('xmlns', xmlns),
            ('count', self.string_table.count),
            ('uniqueCount', self.string_table.unique_count),
        ]

        self._xml_start_tag('sst', attributes)

    def _write_sst_strings(self):
        # Write the sst string elements.

        for string in (self.string_table._get_strings()):
            self._write_si(string)

    def _write_si(self, string):
        # Write the <si> element.
        attributes = []

        # TODO: Fix control char encoding when unit test is ported.
        # Excel escapes control characters with _xHHHH_ and also escapes any
        # literal strings of that type by encoding the leading underscore.
        # So "\0" -> _x0000_ and "_x0000_" -> _x005F_x0000_.
        # The following substitutions deal with those cases.

        # Escape the escape.
        # string =~ s/(_x[0-9a-fA-F]{4}_)/_x005F1/g

        # Convert control character to the _xHHHH_ escape.
        # string =~ s/([\x00-\x08\x0B-\x1F])/sprintf "_x04X_", ord(1)/eg

        # Add attribute to preserve leading or trailing whitespace.
        if re.search('^\s', string) or re.search('\s$', string):
            attributes.append(('xml:space', 'preserve'))

        # Write any rich strings without further tags.
        if re.search('^<r>', string) and re.search('</r>$', string):
            self._xml_rich_si_element(string)
        else:
            self._xml_si_element(string, attributes)


# A metadata class to store Excel strings between worksheets.
class SharedStringTable(object):
    """
    A class to track Excel shared strings between worksheets.

    """

    def __init__(self):
        self.count = 0
        self.unique_count = 0
        self.string_table = {}
        self.string_array = []

    def _get_shared_string_index(self, string):
        """" Get the index of the string in the Shared String table. """
        if string not in self.string_table:
            # String isn't already stored in the table so add it.
            index = self.unique_count
            self.string_table[string] = index
            self.count += 1
            self.unique_count += 1
            return index
        else:
            # String exists in the table.
            index = self.string_table[string]
            self.count += 1
            return index

    def _get_shared_string(self, index):
        """" Get a shared string from the index. """
        return self.string_array[index]

    def _sort_string_data(self):
        """" Sort the shared string data and convert from dict to list. """
        self.string_array = sorted(self.string_table,
                                   key=self.string_table.__getitem__)
        self.string_table = {}

    def _get_strings(self):
        """" Return the sorted string list. """
        return self.string_array

########NEW FILE########
__FILENAME__ = styles
###############################################################################
#
# Styles - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Package imports.
from . import xmlwriter


class Styles(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Styles file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(Styles, self).__init__()

        self.xf_formats = []
        self.palette = []
        self.font_count = 0
        self.num_format_count = 0
        self.border_count = 0
        self.fill_count = 0
        self.custom_colors = []
        self.dxf_formats = []

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        # Add the style sheet.
        self._write_style_sheet()

        # Write the number formats.
        self._write_num_fmts()

        # Write the fonts.
        self._write_fonts()

        # Write the fills.
        self._write_fills()

        # Write the borders element.
        self._write_borders()

        # Write the cellStyleXfs element.
        self._write_cell_style_xfs()

        # Write the cellXfs element.
        self._write_cell_xfs()

        # Write the cellStyles element.
        self._write_cell_styles()

        # Write the dxfs element.
        self._write_dxfs()

        # Write the tableStyles element.
        self._write_table_styles()

        # Write the colors element.
        self._write_colors()

        # Close the style sheet tag.
        self._xml_end_tag('styleSheet')

        # Close the file.
        self._xml_close()

    def _set_style_properties(self, properties):
        # Pass in the Format objects and other properties used in the styles.

        self.xf_formats = properties[0]
        self.palette = properties[1]
        self.font_count = properties[2]
        self.num_format_count = properties[3]
        self.border_count = properties[4]
        self.fill_count = properties[5]
        self.custom_colors = properties[6]
        self.dxf_formats = properties[7]

    def _get_palette_color(self, color):
        # Convert the RGB color.
        if color[0] == '#':
            color = color[1:]

        return "FF" + color.upper()

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_style_sheet(self):
        # Write the <styleSheet> element.
        xmlns = 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'

        attributes = [('xmlns', xmlns)]
        self._xml_start_tag('styleSheet', attributes)

    def _write_num_fmts(self):
        # Write the <numFmts> element.
        if not self.num_format_count:
            return

        attributes = [('count', self.num_format_count)]
        self._xml_start_tag('numFmts', attributes)

        # Write the numFmts elements.
        for xf_format in self.xf_formats:
            # Ignore built-in number formats, i.e., < 164.
            if xf_format.num_format_index >= 164:
                self._write_num_fmt(xf_format.num_format_index,
                                    xf_format.num_format)

        self._xml_end_tag('numFmts')

    def _write_num_fmt(self, num_fmt_id, format_code):
        # Write the <numFmt> element.
        format_codes = {
            0: 'General',
            1: '0',
            2: '0.00',
            3: '#,##0',
            4: '#,##0.00',
            5: '($#,##0_);($#,##0)',
            6: '($#,##0_);[Red]($#,##0)',
            7: '($#,##0.00_);($#,##0.00)',
            8: '($#,##0.00_);[Red]($#,##0.00)',
            9: '0%',
            10: '0.00%',
            11: '0.00E+00',
            12: '# ?/?',
            13: '# ??/??',
            14: 'm/d/yy',
            15: 'd-mmm-yy',
            16: 'd-mmm',
            17: 'mmm-yy',
            18: 'h:mm AM/PM',
            19: 'h:mm:ss AM/PM',
            20: 'h:mm',
            21: 'h:mm:ss',
            22: 'm/d/yy h:mm',
            37: '(#,##0_);(#,##0)',
            38: '(#,##0_);[Red](#,##0)',
            39: '(#,##0.00_);(#,##0.00)',
            40: '(#,##0.00_);[Red](#,##0.00)',
            41: '_(* #,##0_);_(* (#,##0);_(* "-"_);_(_)',
            42: '_($* #,##0_);_($* (#,##0);_($* "-"_);_(_)',
            43: '_(* #,##0.00_);_(* (#,##0.00);_(* "-"??_);_(_)',
            44: '_($* #,##0.00_);_($* (#,##0.00);_($* "-"??_);_(_)',
            45: 'mm:ss',
            46: '[h]:mm:ss',
            47: 'mm:ss.0',
            48: '##0.0E+0',
            49: '@'}

        # Set the format code for built-in number formats.
        if num_fmt_id < 164:
            if num_fmt_id in format_codes:
                format_code = format_codes[num_fmt_id]
            else:
                format_code = 'General'

        attributes = [
            ('numFmtId', num_fmt_id),
            ('formatCode', format_code),
        ]

        self._xml_empty_tag('numFmt', attributes)

    def _write_fonts(self):
        # Write the <fonts> element.
        attributes = [('count', self.font_count)]
        self._xml_start_tag('fonts', attributes)

        # Write the font elements for xf_format objects that have them.
        for xf_format in self.xf_formats:
            if xf_format.has_font:
                self._write_font(xf_format)

        self._xml_end_tag('fonts')

    def _write_font(self, xf_format, is_dxf_format=False):
        # Write the <font> element.
        self._xml_start_tag('font')

        # The condense and extend elements are mainly used in dxf formats.
        if xf_format.font_condense:
            self._write_condense()

        if xf_format.font_extend:
            self._write_extend()

        if xf_format.bold:
            self._xml_empty_tag('b')

        if xf_format.italic:
            self._xml_empty_tag('i')

        if xf_format.font_strikeout:
            self._xml_empty_tag('strike')

        if xf_format.font_outline:
            self._xml_empty_tag('outline')

        if xf_format.font_shadow:
            self._xml_empty_tag('shadow')

        # Handle the underline variants.
        if xf_format.underline:
            self._write_underline(xf_format.underline)

        if xf_format.font_script == 1:
            self._write_vert_align('superscript')

        if xf_format.font_script == 2:
            self._write_vert_align('subscript')

        if not is_dxf_format:
            self._xml_empty_tag('sz', [('val', xf_format.font_size)])

        if xf_format.theme:
            self._write_color('theme', xf_format.theme)
        elif xf_format.color_indexed:
            self._write_color('indexed', xf_format.color_indexed)
        elif xf_format.font_color:
            color = self._get_palette_color(xf_format.font_color)
            self._write_color('rgb', color)
        elif not is_dxf_format:
            self._write_color('theme', 1)

        if not is_dxf_format:
            self._xml_empty_tag('name', [('val', xf_format.font_name)])
            self._xml_empty_tag('family', [('val', xf_format.font_family)])

            if xf_format.font_name == 'Calibri' and not xf_format.hyperlink:
                self._xml_empty_tag(
                    'scheme',
                    [('val', xf_format.font_scheme)])

        self._xml_end_tag('font')

    def _write_underline(self, underline):
        # Write the underline font element.

        if underline == 2:
            attributes = [('val', 'double')]
        elif underline == 33:
            attributes = [('val', 'singleAccounting')]
        elif underline == 34:
            attributes = [('val', 'doubleAccounting')]
        else:
            # Default to single underline.
            attributes = []

        self._xml_empty_tag('u', attributes)

    def _write_vert_align(self, val):
        # Write the <vertAlign> font sub-element.
        attributes = [('val', val)]

        self._xml_empty_tag('vertAlign', attributes)

    def _write_color(self, name, value):
        # Write the <color> element.
        attributes = [(name, value)]

        self._xml_empty_tag('color', attributes)

    def _write_fills(self):
        # Write the <fills> element.
        attributes = [('count', self.fill_count)]

        self._xml_start_tag('fills', attributes)

        # Write the default fill element.
        self._write_default_fill('none')
        self._write_default_fill('gray125')

        # Write the fill elements for xf_format objects that have them.
        for xf_format in self.xf_formats:
            if xf_format.has_fill:
                self._write_fill(xf_format)

        self._xml_end_tag('fills')

    def _write_default_fill(self, pattern_type):
        # Write the <fill> element for the default fills.
        self._xml_start_tag('fill')
        self._xml_empty_tag('patternFill', [('patternType', pattern_type)])
        self._xml_end_tag('fill')

    def _write_fill(self, xf_format, is_dxf_format=False):
        # Write the <fill> element.
        pattern = xf_format.pattern
        bg_color = xf_format.bg_color
        fg_color = xf_format.fg_color

        # Colors for dxf formats are handled differently from normal formats
        # since the normal xf_format reverses the meaning of BG and FG for
        # solid fills.
        if is_dxf_format:
            bg_color = xf_format.dxf_bg_color
            fg_color = xf_format.dxf_fg_color

        patterns = (
            'none',
            'solid',
            'mediumGray',
            'darkGray',
            'lightGray',
            'darkHorizontal',
            'darkVertical',
            'darkDown',
            'darkUp',
            'darkGrid',
            'darkTrellis',
            'lightHorizontal',
            'lightVertical',
            'lightDown',
            'lightUp',
            'lightGrid',
            'lightTrellis',
            'gray125',
            'gray0625',
        )

        self._xml_start_tag('fill')

        # The "none" pattern is handled differently for dxf formats.
        if is_dxf_format and pattern <= 1:
            self._xml_start_tag('patternFill')
        else:
            self._xml_start_tag(
                'patternFill',
                [('patternType', patterns[pattern])])

        if fg_color:
            fg_color = self._get_palette_color(fg_color)
            self._xml_empty_tag('fgColor', [('rgb', fg_color)])

        if bg_color:
            bg_color = self._get_palette_color(bg_color)
            self._xml_empty_tag('bgColor', [('rgb', bg_color)])
        else:
            if not is_dxf_format:
                self._xml_empty_tag('bgColor', [('indexed', 64)])

        self._xml_end_tag('patternFill')
        self._xml_end_tag('fill')

    def _write_borders(self):
        # Write the <borders> element.
        attributes = [('count', self.border_count)]

        self._xml_start_tag('borders', attributes)

        # Write the border elements for xf_format objects that have them.
        for xf_format in self.xf_formats:
            if xf_format.has_border:
                self._write_border(xf_format)

        self._xml_end_tag('borders')

    def _write_border(self, xf_format, is_dxf_format=False):
        # Write the <border> element.
        attributes = []

        # Diagonal borders add attributes to the <border> element.
        if xf_format.diag_type == 1:
            attributes.append(('diagonalUp', 1))
        elif xf_format.diag_type == 2:
            attributes.append(('diagonalDown', 1))
        elif xf_format.diag_type == 3:
            attributes.append(('diagonalUp', 1))
            attributes.append(('diagonalDown', 1))

        # Ensure that a default diag border is set if the diag type is set.
        if xf_format.diag_type and not xf_format.diag_border:
            xf_format.diag_border = 1

        # Write the start border tag.
        self._xml_start_tag('border', attributes)

        # Write the <border> sub elements.
        self._write_sub_border(
            'left',
            xf_format.left,
            xf_format.left_color)

        self._write_sub_border(
            'right',
            xf_format.right,
            xf_format.right_color)

        self._write_sub_border(
            'top',
            xf_format.top,
            xf_format.top_color)

        self._write_sub_border(
            'bottom',
            xf_format.bottom,
            xf_format.bottom_color)

        # Condition DXF formats don't allow diagonal borders.
        if not is_dxf_format:
            self._write_sub_border(
                'diagonal',
                xf_format.diag_border,
                xf_format.diag_color)

        if is_dxf_format:
            self._write_sub_border('vertical', None, None)
            self._write_sub_border('horizontal', None, None)

        self._xml_end_tag('border')

    def _write_sub_border(self, border_type, style, color):
        # Write the <border> sub elements such as <right>, <top>, etc.
        attributes = []

        if not style:
            self._xml_empty_tag(border_type)
            return

        border_styles = (
            'none',
            'thin',
            'medium',
            'dashed',
            'dotted',
            'thick',
            'double',
            'hair',
            'mediumDashed',
            'dashDot',
            'mediumDashDot',
            'dashDotDot',
            'mediumDashDotDot',
            'slantDashDot',
        )

        attributes.append(('style', border_styles[style]))

        self._xml_start_tag(border_type, attributes)

        if color:
            color = self._get_palette_color(color)
            self._xml_empty_tag('color', [('rgb', color)])
        else:
            self._xml_empty_tag('color', [('auto', 1)])

        self._xml_end_tag(border_type)

    def _write_cell_style_xfs(self):
        # Write the <cellStyleXfs> element.
        attributes = [('count', 1)]

        self._xml_start_tag('cellStyleXfs', attributes)
        self._write_style_xf()
        self._xml_end_tag('cellStyleXfs')

    def _write_cell_xfs(self):
        # Write the <cellXfs> element.
        formats = self.xf_formats

        # Workaround for when the last xf_format is used for the comment font
        # and shouldn't be used for cellXfs.
        last_format = formats[-1]
        if last_format.font_only:
            formats.pop()

        attributes = [('count', len(formats))]
        self._xml_start_tag('cellXfs', attributes)

        # Write the xf elements.
        for xf_format in formats:
            self._write_xf(xf_format)

        self._xml_end_tag('cellXfs')

    def _write_style_xf(self):
        # Write the style <xf> element.
        num_fmt_id = 0
        font_id = 0
        fill_id = 0
        border_id = 0

        attributes = [
            ('numFmtId', num_fmt_id),
            ('fontId', font_id),
            ('fillId', fill_id),
            ('borderId', border_id),
        ]

        self._xml_empty_tag('xf', attributes)

    def _write_xf(self, xf_format):
        # Write the <xf> element.
        num_fmt_id = xf_format.num_format_index
        font_id = xf_format.font_index
        fill_id = xf_format.fill_index
        border_id = xf_format.border_index
        xf_id = 0
        has_align = 0
        has_protect = 0

        attributes = [
            ('numFmtId', num_fmt_id),
            ('fontId', font_id),
            ('fillId', fill_id),
            ('borderId', border_id),
            ('xfId', xf_id),
        ]

        if xf_format.num_format_index > 0:
            attributes.append(('applyNumberFormat', 1))

        # Add applyFont attribute if XF format uses a font element.
        if xf_format.font_index > 0:
            attributes.append(('applyFont', 1))

        # Add applyFill attribute if XF format uses a fill element.
        if xf_format.fill_index > 0:
            attributes.append(('applyFill', 1))

        # Add applyBorder attribute if XF format uses a border element.
        if xf_format.border_index > 0:
            attributes.append(('applyBorder', 1))

        # Check if XF format has alignment properties set.
        (apply_align, align) = xf_format._get_align_properties()

        # Check if an alignment sub-element should be written.
        if apply_align and align:
            has_align = 1

        # We can also have applyAlignment without a sub-element.
        if apply_align:
            attributes.append(('applyAlignment', 1))

        # Check for cell protection properties.
        protection = xf_format._get_protection_properties()

        if protection:
            attributes.append(('applyProtection', 1))
            has_protect = 1

        # Write XF with sub-elements if required.
        if has_align or has_protect:
            self._xml_start_tag('xf', attributes)
            if has_align:
                self._xml_empty_tag('alignment', align)
            if has_protect:
                self._xml_empty_tag('protection', protection)
            self._xml_end_tag('xf')
        else:
            self._xml_empty_tag('xf', attributes)

    def _write_cell_styles(self):
        # Write the <cellStyles> element.
        attributes = [('count', 1)]

        self._xml_start_tag('cellStyles', attributes)
        self._write_cell_style()
        self._xml_end_tag('cellStyles')

    def _write_cell_style(self):
        # Write the <cellStyle> element.
        name = 'Normal'
        xf_id = 0
        builtin_id = 0

        attributes = [
            ('name', name),
            ('xfId', xf_id),
            ('builtinId', builtin_id),
        ]

        self._xml_empty_tag('cellStyle', attributes)

    def _write_dxfs(self):
        # Write the <dxfs> element.
        formats = self.dxf_formats
        count = len(formats)

        attributes = [('count', len(formats))]

        if count:
            self._xml_start_tag('dxfs', attributes)

            # Write the font elements for xf_format objects that have them.
            for xf_format in self.dxf_formats:
                self._xml_start_tag('dxf')
                if xf_format.has_dxf_font:
                    self._write_font(xf_format, 1)

                if xf_format.num_format_index:
                    self._write_num_fmt(xf_format.num_format_index,
                                        xf_format.num_format)

                if xf_format.has_dxf_fill:
                    self._write_fill(xf_format, True)
                if xf_format.has_dxf_border:
                    self._write_border(xf_format, True)
                self._xml_end_tag('dxf')

            self._xml_end_tag('dxfs')
        else:
            self._xml_empty_tag('dxfs', attributes)

    def _write_table_styles(self):
        # Write the <tableStyles> element.
        count = 0
        default_table_style = 'TableStyleMedium9'
        default_pivot_style = 'PivotStyleLight16'

        attributes = [
            ('count', count),
            ('defaultTableStyle', default_table_style),
            ('defaultPivotStyle', default_pivot_style),
        ]

        self._xml_empty_tag('tableStyles', attributes)

    def _write_colors(self):
        # Write the <colors> element.
        custom_colors = self.custom_colors

        if not custom_colors:
            return

        self._xml_start_tag('colors')
        self._write_mru_colors(custom_colors)
        self._xml_end_tag('colors')

    def _write_mru_colors(self, custom_colors):
        # Write the <mruColors> element for the most recently used colours.

        # Write the custom custom_colors in reverse order.
        custom_colors.reverse()

        # Limit the mruColors to the last 10.
        if len(custom_colors) > 10:
            custom_colors = custom_colors[0:10]

        self._xml_start_tag('mruColors')

        # Write the custom custom_colors in reverse order.
        for color in custom_colors:
            self._write_color('rgb', color)

        self._xml_end_tag('mruColors')

    def _write_condense(self):
        # Write the <condense> element.
        attributes = [('val', 0)]

        self._xml_empty_tag('condense', attributes)

    def _write_extend(self):
        # Write the <extend> element.
        attributes = [('val', 0)]

        self._xml_empty_tag('extend', attributes)

########NEW FILE########
__FILENAME__ = table
###############################################################################
#
# Table - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

from . import xmlwriter


class Table(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Table file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(Table, self).__init__()

        self.properties = {}

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        # Write the table element.
        self._write_table()

        # Write the autoFilter element.
        self._write_auto_filter()

        # Write the tableColumns element.
        self._write_table_columns()

        # Write the tableStyleInfo element.
        self._write_table_style_info()

        # Close the table tag.
        self._xml_end_tag('table')

        # Close the file.
        self._xml_close()

    def _set_properties(self, properties):
        # Set the document properties.
        self.properties = properties

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_table(self):
        # Write the <table> element.
        schema = 'http://schemas.openxmlformats.org/'
        xmlns = schema + 'spreadsheetml/2006/main'
        table_id = self.properties['id']
        name = self.properties['name']
        display_name = self.properties['name']
        ref = self.properties['range']
        totals_row_shown = self.properties['totals_row_shown']
        header_row_count = self.properties['header_row_count']

        attributes = [
            ('xmlns', xmlns),
            ('id', table_id),
            ('name', name),
            ('displayName', display_name),
            ('ref', ref),
        ]

        if not header_row_count:
            attributes.append(('headerRowCount', 0))

        if totals_row_shown:
            attributes.append(('totalsRowCount', 1))
        else:
            attributes.append(('totalsRowShown', 0))

        self._xml_start_tag('table', attributes)

    def _write_auto_filter(self):
        # Write the <autoFilter> element.
        autofilter = self.properties.get('autofilter', 0)

        if not autofilter:
            return

        attributes = [('ref', autofilter,)]

        self._xml_empty_tag('autoFilter', attributes)

    def _write_table_columns(self):
        # Write the <tableColumns> element.
        columns = self.properties['columns']

        count = len(columns)

        attributes = [('count', count)]

        self._xml_start_tag('tableColumns', attributes)

        for col_data in (columns):
            # Write the tableColumn element.
            self._write_table_column(col_data)

        self._xml_end_tag('tableColumns')

    def _write_table_column(self, col_data):
        # Write the <tableColumn> element.
        attributes = [
            ('id', col_data['id']),
            ('name', col_data['name']),
        ]

        if col_data.get('total_string'):
            attributes.append(('totalsRowLabel', col_data['total_string']))
        elif col_data.get('total_function'):
            attributes.append(('totalsRowFunction',
                               col_data['total_function']))

        if 'format' in col_data and col_data['format'] is not None:
            attributes.append(('dataDxfId', col_data['format']))

        if col_data.get('formula'):
            self._xml_start_tag('tableColumn', attributes)

            # Write the calculatedColumnFormula element.
            self._write_calculated_column_formula(col_data['formula'])

            self._xml_end_tag('tableColumn')
        else:
            self._xml_empty_tag('tableColumn', attributes)

    def _write_table_style_info(self):
        # Write the <tableStyleInfo> element.
        props = self.properties

        name = props['style']
        show_first_column = 0 + props['show_first_col']
        show_last_column = 0 + props['show_last_col']
        show_row_stripes = 0 + props['show_row_stripes']
        show_column_stripes = 0 + props['show_col_stripes']

        attributes = [
            ('name', name),
            ('showFirstColumn', show_first_column),
            ('showLastColumn', show_last_column),
            ('showRowStripes', show_row_stripes),
            ('showColumnStripes', show_column_stripes),
        ]

        self._xml_empty_tag('tableStyleInfo', attributes)

    def _write_calculated_column_formula(self, formula):
        # Write the <calculatedColumnFormula> element.
        self._xml_data_element('calculatedColumnFormula', formula)

########NEW FILE########
__FILENAME__ = theme
###############################################################################
# _*_ coding: utf-8
#
# Theme - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Standard packages.
from __future__ import unicode_literals
import codecs


class Theme(object):
    """
    A class for writing the Excel XLSX Theme file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """
        super(Theme, self).__init__()
        self.fh = None

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.
        self._write_theme_file()
        self.fh.close()

    def _set_xml_writer(self, filename):
        # Set the XML writer filehandle for the object.
        self.fh = codecs.open(filename, 'w', 'utf-8')

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_theme_file(self):
        # Write a default theme.xml file.
        self.fh.write("""<?xml version="1.0" encoding="UTF-8" standalone="yes"?>\n<a:theme xmlns:a="http://schemas.openxmlformats.org/drawingml/2006/main" name="Office Theme"><a:themeElements><a:clrScheme name="Office"><a:dk1><a:sysClr val="windowText" lastClr="000000"/></a:dk1><a:lt1><a:sysClr val="window" lastClr="FFFFFF"/></a:lt1><a:dk2><a:srgbClr val="1F497D"/></a:dk2><a:lt2><a:srgbClr val="EEECE1"/></a:lt2><a:accent1><a:srgbClr val="4F81BD"/></a:accent1><a:accent2><a:srgbClr val="C0504D"/></a:accent2><a:accent3><a:srgbClr val="9BBB59"/></a:accent3><a:accent4><a:srgbClr val="8064A2"/></a:accent4><a:accent5><a:srgbClr val="4BACC6"/></a:accent5><a:accent6><a:srgbClr val="F79646"/></a:accent6><a:hlink><a:srgbClr val="0000FF"/></a:hlink><a:folHlink><a:srgbClr val="800080"/></a:folHlink></a:clrScheme><a:fontScheme name="Office"><a:majorFont><a:latin typeface="Cambria"/><a:ea typeface=""/><a:cs typeface=""/><a:font script="Jpan" typeface=" "/><a:font script="Hang" typeface=" "/><a:font script="Hans" typeface=""/><a:font script="Hant" typeface=""/><a:font script="Arab" typeface="Times New Roman"/><a:font script="Hebr" typeface="Times New Roman"/><a:font script="Thai" typeface="Tahoma"/><a:font script="Ethi" typeface="Nyala"/><a:font script="Beng" typeface="Vrinda"/><a:font script="Gujr" typeface="Shruti"/><a:font script="Khmr" typeface="MoolBoran"/><a:font script="Knda" typeface="Tunga"/><a:font script="Guru" typeface="Raavi"/><a:font script="Cans" typeface="Euphemia"/><a:font script="Cher" typeface="Plantagenet Cherokee"/><a:font script="Yiii" typeface="Microsoft Yi Baiti"/><a:font script="Tibt" typeface="Microsoft Himalaya"/><a:font script="Thaa" typeface="MV Boli"/><a:font script="Deva" typeface="Mangal"/><a:font script="Telu" typeface="Gautami"/><a:font script="Taml" typeface="Latha"/><a:font script="Syrc" typeface="Estrangelo Edessa"/><a:font script="Orya" typeface="Kalinga"/><a:font script="Mlym" typeface="Kartika"/><a:font script="Laoo" typeface="DokChampa"/><a:font script="Sinh" typeface="Iskoola Pota"/><a:font script="Mong" typeface="Mongolian Baiti"/><a:font script="Viet" typeface="Times New Roman"/><a:font script="Uigh" typeface="Microsoft Uighur"/></a:majorFont><a:minorFont><a:latin typeface="Calibri"/><a:ea typeface=""/><a:cs typeface=""/><a:font script="Jpan" typeface=" "/><a:font script="Hang" typeface=" "/><a:font script="Hans" typeface=""/><a:font script="Hant" typeface=""/><a:font script="Arab" typeface="Arial"/><a:font script="Hebr" typeface="Arial"/><a:font script="Thai" typeface="Tahoma"/><a:font script="Ethi" typeface="Nyala"/><a:font script="Beng" typeface="Vrinda"/><a:font script="Gujr" typeface="Shruti"/><a:font script="Khmr" typeface="DaunPenh"/><a:font script="Knda" typeface="Tunga"/><a:font script="Guru" typeface="Raavi"/><a:font script="Cans" typeface="Euphemia"/><a:font script="Cher" typeface="Plantagenet Cherokee"/><a:font script="Yiii" typeface="Microsoft Yi Baiti"/><a:font script="Tibt" typeface="Microsoft Himalaya"/><a:font script="Thaa" typeface="MV Boli"/><a:font script="Deva" typeface="Mangal"/><a:font script="Telu" typeface="Gautami"/><a:font script="Taml" typeface="Latha"/><a:font script="Syrc" typeface="Estrangelo Edessa"/><a:font script="Orya" typeface="Kalinga"/><a:font script="Mlym" typeface="Kartika"/><a:font script="Laoo" typeface="DokChampa"/><a:font script="Sinh" typeface="Iskoola Pota"/><a:font script="Mong" typeface="Mongolian Baiti"/><a:font script="Viet" typeface="Arial"/><a:font script="Uigh" typeface="Microsoft Uighur"/></a:minorFont></a:fontScheme><a:fmtScheme name="Office"><a:fillStyleLst><a:solidFill><a:schemeClr val="phClr"/></a:solidFill><a:gradFill rotWithShape="1"><a:gsLst><a:gs pos="0"><a:schemeClr val="phClr"><a:tint val="50000"/><a:satMod val="300000"/></a:schemeClr></a:gs><a:gs pos="35000"><a:schemeClr val="phClr"><a:tint val="37000"/><a:satMod val="300000"/></a:schemeClr></a:gs><a:gs pos="100000"><a:schemeClr val="phClr"><a:tint val="15000"/><a:satMod val="350000"/></a:schemeClr></a:gs></a:gsLst><a:lin ang="16200000" scaled="1"/></a:gradFill><a:gradFill rotWithShape="1"><a:gsLst><a:gs pos="0"><a:schemeClr val="phClr"><a:shade val="51000"/><a:satMod val="130000"/></a:schemeClr></a:gs><a:gs pos="80000"><a:schemeClr val="phClr"><a:shade val="93000"/><a:satMod val="130000"/></a:schemeClr></a:gs><a:gs pos="100000"><a:schemeClr val="phClr"><a:shade val="94000"/><a:satMod val="135000"/></a:schemeClr></a:gs></a:gsLst><a:lin ang="16200000" scaled="0"/></a:gradFill></a:fillStyleLst><a:lnStyleLst><a:ln w="9525" cap="flat" cmpd="sng" algn="ctr"><a:solidFill><a:schemeClr val="phClr"><a:shade val="95000"/><a:satMod val="105000"/></a:schemeClr></a:solidFill><a:prstDash val="solid"/></a:ln><a:ln w="25400" cap="flat" cmpd="sng" algn="ctr"><a:solidFill><a:schemeClr val="phClr"/></a:solidFill><a:prstDash val="solid"/></a:ln><a:ln w="38100" cap="flat" cmpd="sng" algn="ctr"><a:solidFill><a:schemeClr val="phClr"/></a:solidFill><a:prstDash val="solid"/></a:ln></a:lnStyleLst><a:effectStyleLst><a:effectStyle><a:effectLst><a:outerShdw blurRad="40000" dist="20000" dir="5400000" rotWithShape="0"><a:srgbClr val="000000"><a:alpha val="38000"/></a:srgbClr></a:outerShdw></a:effectLst></a:effectStyle><a:effectStyle><a:effectLst><a:outerShdw blurRad="40000" dist="23000" dir="5400000" rotWithShape="0"><a:srgbClr val="000000"><a:alpha val="35000"/></a:srgbClr></a:outerShdw></a:effectLst></a:effectStyle><a:effectStyle><a:effectLst><a:outerShdw blurRad="40000" dist="23000" dir="5400000" rotWithShape="0"><a:srgbClr val="000000"><a:alpha val="35000"/></a:srgbClr></a:outerShdw></a:effectLst><a:scene3d><a:camera prst="orthographicFront"><a:rot lat="0" lon="0" rev="0"/></a:camera><a:lightRig rig="threePt" dir="t"><a:rot lat="0" lon="0" rev="1200000"/></a:lightRig></a:scene3d><a:sp3d><a:bevelT w="63500" h="25400"/></a:sp3d></a:effectStyle></a:effectStyleLst><a:bgFillStyleLst><a:solidFill><a:schemeClr val="phClr"/></a:solidFill><a:gradFill rotWithShape="1"><a:gsLst><a:gs pos="0"><a:schemeClr val="phClr"><a:tint val="40000"/><a:satMod val="350000"/></a:schemeClr></a:gs><a:gs pos="40000"><a:schemeClr val="phClr"><a:tint val="45000"/><a:shade val="99000"/><a:satMod val="350000"/></a:schemeClr></a:gs><a:gs pos="100000"><a:schemeClr val="phClr"><a:shade val="20000"/><a:satMod val="255000"/></a:schemeClr></a:gs></a:gsLst><a:path path="circle"><a:fillToRect l="50000" t="-80000" r="50000" b="180000"/></a:path></a:gradFill><a:gradFill rotWithShape="1"><a:gsLst><a:gs pos="0"><a:schemeClr val="phClr"><a:tint val="80000"/><a:satMod val="300000"/></a:schemeClr></a:gs><a:gs pos="100000"><a:schemeClr val="phClr"><a:shade val="30000"/><a:satMod val="200000"/></a:schemeClr></a:gs></a:gsLst><a:path path="circle"><a:fillToRect l="50000" t="50000" r="50000" b="50000"/></a:path></a:gradFill></a:bgFillStyleLst></a:fmtScheme></a:themeElements><a:objectDefaults/><a:extraClrSchemeLst/></a:theme>""")

########NEW FILE########
__FILENAME__ = utility
###############################################################################
#
# Worksheet - A class for writing Excel Worksheets.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#
import re
import sys
from warnings import warn

COL_NAMES = {}
range_parts = re.compile(r'(\$?)([A-Z]{1,3})(\$?)(\d+)')


def xl_rowcol_to_cell(row, col, row_abs=0, col_abs=0):
    """
    TODO. Add Utility.py docs.

    """
    row += 1  # Change to 1-index.
    row_abs = '$' if row_abs else ''
    col_abs = '$' if col_abs else ''

    col_str = xl_col_to_name(col, col_abs)

    return col_str + row_abs + str(row)


def xl_rowcol_to_cell_fast(row, col):
    """
    Optimised version of the xl_rowcol_to_cell function.

    """
    if col in COL_NAMES:
        col_str = COL_NAMES[col]
    else:
        col_str = xl_col_to_name(col)
        COL_NAMES[col] = col_str

    return col_str + str(row + 1)


def xl_col_to_name(col_num, col_abs=0):
    """
    TODO. Add Utility.py docs.

    """
    col_num += 1  # Change to 1-index.
    col_str = ''
    col_abs = '$' if col_abs else ''

    while col_num:

        # Set remainder from 1 .. 26
        remainder = col_num % 26

        if remainder == 0:
            remainder = 26

        # Convert the remainder to a character.
        col_letter = chr(ord('A') + remainder - 1)

        # Accumulate the column letters, right to left.
        col_str = col_letter + col_str

        # Get the next order of magnitude.
        col_num = int((col_num - 1) / 26)

    return col_abs + col_str


def xl_cell_to_rowcol(cell_str):
    """
    TODO. Add Utility.py docs.

    """
    if not cell_str:
        return (0, 0)

    match = range_parts.match(cell_str)
    col_str = match.group(2)
    row_str = match.group(4)

    # Convert base26 column string to number.
    expn = 0
    col = 0
    for char in reversed(col_str):
        col += (ord(char) - ord('A') + 1) * (26 ** expn)
        expn += 1

    # Convert 1-index to zero-index
    row = int(row_str) - 1
    col -= 1

    return row, col


def xl_cell_to_rowcol_abs(cell_str):
    """
    TODO. Add Utility.py docs.

    """
    if not cell_str:
        return (0, 0, 0, 0)

    match = range_parts.match(cell_str)

    col_abs = match.group(1)
    col_str = match.group(2)
    row_abs = match.group(3)
    row_str = match.group(4)

    if col_abs:
        col_abs = 1
    else:
        col_abs = 0

    if row_abs:
        row_abs = 1
    else:
        row_abs = 0

    # Convert base26 column string to number.
    expn = 0
    col = 0
    for char in reversed(col_str):
        col += (ord(char) - ord('A') + 1) * (26 ** expn)
        expn += 1

    # Convert 1-index to zero-index
    row = int(row_str) - 1
    col -= 1

    return row, col, row_abs, col_abs


def xl_range(first_row, first_col, last_row, last_col):
    """
    TODO. Add Utility.py docs.

    """
    range1 = xl_rowcol_to_cell(first_row, first_col)
    range2 = xl_rowcol_to_cell(last_row, last_col)

    return range1 + ':' + range2


def xl_color(color):
    # Used in conjunction with the XlsxWriter *color() methods to convert
    # a colour name into an RGB formatted string. These colours are for
    # backward compatibility with older versions of Excel.
    named_colors = {
        'black': '#000000',
        'blue': '#0000FF',
        'brown': '#800000',
        'cyan': '#00FFFF',
        'gray': '#808080',
        'green': '#008000',
        'lime': '#00FF00',
        'magenta': '#FF00FF',
        'navy': '#000080',
        'orange': '#FF6600',
        'pink': '#FF00FF',
        'purple': '#800080',
        'red': '#FF0000',
        'silver': '#C0C0C0',
        'white': '#FFFFFF',
        'yellow': '#FFFF00',
    }

    if color in named_colors:
        color = named_colors[color]

    if not re.match('#[0-9a-fA-F]{6}', color):
        warn("Color '%s' isn't a valid Excel color" % color)

    # Convert the RGB color to the Excel ARGB format.
    return "FF" + color.lstrip('#').upper()


def get_sparkline_style(style_id):
    styles = [
        {'series':   {'theme': "4", 'tint': "-0.499984740745262"},
         'negative': {'theme': "5"},
         'markers':  {'theme': "4", 'tint': "-0.499984740745262"},
         'first':    {'theme': "4", 'tint': "0.39997558519241921"},
         'last':     {'theme': "4", 'tint': "0.39997558519241921"},
         'high':     {'theme': "4"},
         'low':      {'theme': "4"},
         },  # 0
        {'series':   {'theme': "4", 'tint': "-0.499984740745262"},
         'negative': {'theme': "5"},
         'markers':  {'theme': "4", 'tint': "-0.499984740745262"},
         'first':    {'theme': "4", 'tint': "0.39997558519241921"},
         'last':     {'theme': "4", 'tint': "0.39997558519241921"},
         'high':     {'theme': "4"},
         'low':      {'theme': "4"},
         },  # 1
        {'series':   {'theme': "5", 'tint': "-0.499984740745262"},
         'negative': {'theme': "6"},
         'markers':  {'theme': "5", 'tint': "-0.499984740745262"},
         'first':    {'theme': "5", 'tint': "0.39997558519241921"},
         'last':     {'theme': "5", 'tint': "0.39997558519241921"},
         'high':     {'theme': "5"},
         'low':      {'theme': "5"},
         },  # 2
        {'series':   {'theme': "6", 'tint': "-0.499984740745262"},
         'negative': {'theme': "7"},
         'markers':  {'theme': "6", 'tint': "-0.499984740745262"},
         'first':    {'theme': "6", 'tint': "0.39997558519241921"},
         'last':     {'theme': "6", 'tint': "0.39997558519241921"},
         'high':     {'theme': "6"},
         'low':      {'theme': "6"},
         },  # 3
        {'series':   {'theme': "7", 'tint': "-0.499984740745262"},
         'negative': {'theme': "8"},
         'markers':  {'theme': "7", 'tint': "-0.499984740745262"},
         'first':    {'theme': "7", 'tint': "0.39997558519241921"},
         'last':     {'theme': "7", 'tint': "0.39997558519241921"},
         'high':     {'theme': "7"},
         'low':      {'theme': "7"},
         },  # 4
        {'series':   {'theme': "8", 'tint': "-0.499984740745262"},
         'negative': {'theme': "9"},
         'markers':  {'theme': "8", 'tint': "-0.499984740745262"},
         'first':    {'theme': "8", 'tint': "0.39997558519241921"},
         'last':     {'theme': "8", 'tint': "0.39997558519241921"},
         'high':     {'theme': "8"},
         'low':      {'theme': "8"},
         },  # 5
        {'series':   {'theme': "9", 'tint': "-0.499984740745262"},
         'negative': {'theme': "4"},
         'markers':  {'theme': "9", 'tint': "-0.499984740745262"},
         'first':    {'theme': "9", 'tint': "0.39997558519241921"},
         'last':     {'theme': "9", 'tint': "0.39997558519241921"},
         'high':     {'theme': "9"},
         'low':      {'theme': "9"},
         },  # 6
        {'series':   {'theme': "4", 'tint': "-0.249977111117893"},
         'negative': {'theme': "5"},
         'markers':  {'theme': "5", 'tint': "-0.249977111117893"},
         'first':    {'theme': "5", 'tint': "-0.249977111117893"},
         'last':     {'theme': "5", 'tint': "-0.249977111117893"},
         'high':     {'theme': "5", 'tint': "-0.249977111117893"},
         'low':      {'theme': "5", 'tint': "-0.249977111117893"},
         },  # 7
        {'series':   {'theme': "5", 'tint': "-0.249977111117893"},
         'negative': {'theme': "6"},
         'markers':  {'theme': "6", 'tint': "-0.249977111117893"},
         'first':    {'theme': "6", 'tint': "-0.249977111117893"},
         'last':     {'theme': "6", 'tint': "-0.249977111117893"},
         'high':     {'theme': "6", 'tint': "-0.249977111117893"},
         'low':      {'theme': "6", 'tint': "-0.249977111117893"},
         },  # 8
        {'series':   {'theme': "6", 'tint': "-0.249977111117893"},
         'negative': {'theme': "7"},
         'markers':  {'theme': "7", 'tint': "-0.249977111117893"},
         'first':    {'theme': "7", 'tint': "-0.249977111117893"},
         'last':     {'theme': "7", 'tint': "-0.249977111117893"},
         'high':     {'theme': "7", 'tint': "-0.249977111117893"},
         'low':      {'theme': "7", 'tint': "-0.249977111117893"},
         },  # 9
        {'series':   {'theme': "7", 'tint': "-0.249977111117893"},
         'negative': {'theme': "8"},
         'markers':  {'theme': "8", 'tint': "-0.249977111117893"},
         'first':    {'theme': "8", 'tint': "-0.249977111117893"},
         'last':     {'theme': "8", 'tint': "-0.249977111117893"},
         'high':     {'theme': "8", 'tint': "-0.249977111117893"},
         'low':      {'theme': "8", 'tint': "-0.249977111117893"},
         },  # 10
        {'series':   {'theme': "8", 'tint': "-0.249977111117893"},
         'negative': {'theme': "9"},
         'markers':  {'theme': "9", 'tint': "-0.249977111117893"},
         'first':    {'theme': "9", 'tint': "-0.249977111117893"},
         'last':     {'theme': "9", 'tint': "-0.249977111117893"},
         'high':     {'theme': "9", 'tint': "-0.249977111117893"},
         'low':      {'theme': "9", 'tint': "-0.249977111117893"},
         },  # 11
        {'series':   {'theme': "9", 'tint': "-0.249977111117893"},
         'negative': {'theme': "4"},
         'markers':  {'theme': "4", 'tint': "-0.249977111117893"},
         'first':    {'theme': "4", 'tint': "-0.249977111117893"},
         'last':     {'theme': "4", 'tint': "-0.249977111117893"},
         'high':     {'theme': "4", 'tint': "-0.249977111117893"},
         'low':      {'theme': "4", 'tint': "-0.249977111117893"},
         },  # 12
        {'series':   {'theme': "4"},
         'negative': {'theme': "5"},
         'markers':  {'theme': "4", 'tint': "-0.249977111117893"},
         'first':    {'theme': "4", 'tint': "-0.249977111117893"},
         'last':     {'theme': "4", 'tint': "-0.249977111117893"},
         'high':     {'theme': "4", 'tint': "-0.249977111117893"},
         'low':      {'theme': "4", 'tint': "-0.249977111117893"},
         },  # 13
        {'series':   {'theme': "5"},
         'negative': {'theme': "6"},
         'markers':  {'theme': "5", 'tint': "-0.249977111117893"},
         'first':    {'theme': "5", 'tint': "-0.249977111117893"},
         'last':     {'theme': "5", 'tint': "-0.249977111117893"},
         'high':     {'theme': "5", 'tint': "-0.249977111117893"},
         'low':      {'theme': "5", 'tint': "-0.249977111117893"},
         },  # 14
        {'series':   {'theme': "6"},
         'negative': {'theme': "7"},
         'markers':  {'theme': "6", 'tint': "-0.249977111117893"},
         'first':    {'theme': "6", 'tint': "-0.249977111117893"},
         'last':     {'theme': "6", 'tint': "-0.249977111117893"},
         'high':     {'theme': "6", 'tint': "-0.249977111117893"},
         'low':      {'theme': "6", 'tint': "-0.249977111117893"},
         },  # 15
        {'series':   {'theme': "7"},
         'negative': {'theme': "8"},
         'markers':  {'theme': "7", 'tint': "-0.249977111117893"},
         'first':    {'theme': "7", 'tint': "-0.249977111117893"},
         'last':     {'theme': "7", 'tint': "-0.249977111117893"},
         'high':     {'theme': "7", 'tint': "-0.249977111117893"},
         'low':      {'theme': "7", 'tint': "-0.249977111117893"},
         },  # 16
        {'series':   {'theme': "8"},
         'negative': {'theme': "9"},
         'markers':  {'theme': "8", 'tint': "-0.249977111117893"},
         'first':    {'theme': "8", 'tint': "-0.249977111117893"},
         'last':     {'theme': "8", 'tint': "-0.249977111117893"},
         'high':     {'theme': "8", 'tint': "-0.249977111117893"},
         'low':      {'theme': "8", 'tint': "-0.249977111117893"},
         },  # 17
        {'series':   {'theme': "9"},
         'negative': {'theme': "4"},
         'markers':  {'theme': "9", 'tint': "-0.249977111117893"},
         'first':    {'theme': "9", 'tint': "-0.249977111117893"},
         'last':     {'theme': "9", 'tint': "-0.249977111117893"},
         'high':     {'theme': "9", 'tint': "-0.249977111117893"},
         'low':      {'theme': "9", 'tint': "-0.249977111117893"},
         },  # 18
        {'series':   {'theme': "4", 'tint': "0.39997558519241921"},
         'negative': {'theme': "0", 'tint': "-0.499984740745262"},
         'markers':  {'theme': "4", 'tint': "0.79998168889431442"},
         'first':    {'theme': "4", 'tint': "-0.249977111117893"},
         'last':     {'theme': "4", 'tint': "-0.249977111117893"},
         'high':     {'theme': "4", 'tint': "-0.499984740745262"},
         'low':      {'theme': "4", 'tint': "-0.499984740745262"},
         },  # 19
        {'series':   {'theme': "5", 'tint': "0.39997558519241921"},
         'negative': {'theme': "0", 'tint': "-0.499984740745262"},
         'markers':  {'theme': "5", 'tint': "0.79998168889431442"},
         'first':    {'theme': "5", 'tint': "-0.249977111117893"},
         'last':     {'theme': "5", 'tint': "-0.249977111117893"},
         'high':     {'theme': "5", 'tint': "-0.499984740745262"},
         'low':      {'theme': "5", 'tint': "-0.499984740745262"},
         },  # 20
        {'series':   {'theme': "6", 'tint': "0.39997558519241921"},
         'negative': {'theme': "0", 'tint': "-0.499984740745262"},
         'markers':  {'theme': "6", 'tint': "0.79998168889431442"},
         'first':    {'theme': "6", 'tint': "-0.249977111117893"},
         'last':     {'theme': "6", 'tint': "-0.249977111117893"},
         'high':     {'theme': "6", 'tint': "-0.499984740745262"},
         'low':      {'theme': "6", 'tint': "-0.499984740745262"},
         },  # 21
        {'series':   {'theme': "7", 'tint': "0.39997558519241921"},
         'negative': {'theme': "0", 'tint': "-0.499984740745262"},
         'markers':  {'theme': "7", 'tint': "0.79998168889431442"},
         'first':    {'theme': "7", 'tint': "-0.249977111117893"},
         'last':     {'theme': "7", 'tint': "-0.249977111117893"},
         'high':     {'theme': "7", 'tint': "-0.499984740745262"},
         'low':      {'theme': "7", 'tint': "-0.499984740745262"},
         },  # 22
        {'series':   {'theme': "8", 'tint': "0.39997558519241921"},
         'negative': {'theme': "0", 'tint': "-0.499984740745262"},
         'markers':  {'theme': "8", 'tint': "0.79998168889431442"},
         'first':    {'theme': "8", 'tint': "-0.249977111117893"},
         'last':     {'theme': "8", 'tint': "-0.249977111117893"},
         'high':     {'theme': "8", 'tint': "-0.499984740745262"},
         'low':      {'theme': "8", 'tint': "-0.499984740745262"},
         },  # 23
        {'series':   {'theme': "9", 'tint': "0.39997558519241921"},
         'negative': {'theme': "0", 'tint': "-0.499984740745262"},
         'markers':  {'theme': "9", 'tint': "0.79998168889431442"},
         'first':    {'theme': "9", 'tint': "-0.249977111117893"},
         'last':     {'theme': "9", 'tint': "-0.249977111117893"},
         'high':     {'theme': "9", 'tint': "-0.499984740745262"},
         'low':      {'theme': "9", 'tint': "-0.499984740745262"},
         },  # 24
        {'series':   {'theme': "1", 'tint': "0.499984740745262"},
         'negative': {'theme': "1", 'tint': "0.249977111117893"},
         'markers':  {'theme': "1", 'tint': "0.249977111117893"},
         'first':    {'theme': "1", 'tint': "0.249977111117893"},
         'last':     {'theme': "1", 'tint': "0.249977111117893"},
         'high':     {'theme': "1", 'tint': "0.249977111117893"},
         'low':      {'theme': "1", 'tint': "0.249977111117893"},
         },  # 25
        {'series':   {'theme': "1", 'tint': "0.34998626667073579"},
         'negative': {'theme': "0", 'tint': "-0.249977111117893"},
         'markers':  {'theme': "0", 'tint': "-0.249977111117893"},
         'first':    {'theme': "0", 'tint': "-0.249977111117893"},
         'last':     {'theme': "0", 'tint': "-0.249977111117893"},
         'high':     {'theme': "0", 'tint': "-0.249977111117893"},
         'low':      {'theme': "0", 'tint': "-0.249977111117893"},
         },  # 26
        {'series':   {'rgb': "FF323232"},
         'negative': {'rgb': "FFD00000"},
         'markers':  {'rgb': "FFD00000"},
         'first':    {'rgb': "FFD00000"},
         'last':     {'rgb': "FFD00000"},
         'high':     {'rgb': "FFD00000"},
         'low':      {'rgb': "FFD00000"},
         },  # 27
        {'series':   {'rgb': "FF000000"},
         'negative': {'rgb': "FF0070C0"},
         'markers':  {'rgb': "FF0070C0"},
         'first':    {'rgb': "FF0070C0"},
         'last':     {'rgb': "FF0070C0"},
         'high':     {'rgb': "FF0070C0"},
         'low':      {'rgb': "FF0070C0"},
         },  # 28
        {'series':   {'rgb': "FF376092"},
         'negative': {'rgb': "FFD00000"},
         'markers':  {'rgb': "FFD00000"},
         'first':    {'rgb': "FFD00000"},
         'last':     {'rgb': "FFD00000"},
         'high':     {'rgb': "FFD00000"},
         'low':      {'rgb': "FFD00000"},
         },  # 29
        {'series':   {'rgb': "FF0070C0"},
         'negative': {'rgb': "FF000000"},
         'markers':  {'rgb': "FF000000"},
         'first':    {'rgb': "FF000000"},
         'last':     {'rgb': "FF000000"},
         'high':     {'rgb': "FF000000"},
         'low':      {'rgb': "FF000000"},
         },  # 30
        {'series':   {'rgb': "FF5F5F5F"},
         'negative': {'rgb': "FFFFB620"},
         'markers':  {'rgb': "FFD70077"},
         'first':    {'rgb': "FF5687C2"},
         'last':     {'rgb': "FF359CEB"},
         'high':     {'rgb': "FF56BE79"},
         'low':      {'rgb': "FFFF5055"},
         },  # 31
        {'series':   {'rgb': "FF5687C2"},
         'negative': {'rgb': "FFFFB620"},
         'markers':  {'rgb': "FFD70077"},
         'first':    {'rgb': "FF777777"},
         'last':     {'rgb': "FF359CEB"},
         'high':     {'rgb': "FF56BE79"},
         'low':      {'rgb': "FFFF5055"},
         },  # 32
        {'series':   {'rgb': "FFC6EFCE"},
         'negative': {'rgb': "FFFFC7CE"},
         'markers':  {'rgb': "FF8CADD6"},
         'first':    {'rgb': "FFFFDC47"},
         'last':     {'rgb': "FFFFEB9C"},
         'high':     {'rgb': "FF60D276"},
         'low':      {'rgb': "FFFF5367"},
         },  # 33
        {'series':   {'rgb': "FF00B050"},
         'negative': {'rgb': "FFFF0000"},
         'markers':  {'rgb': "FF0070C0"},
         'first':    {'rgb': "FFFFC000"},
         'last':     {'rgb': "FFFFC000"},
         'high':     {'rgb': "FF00B050"},
         'low':      {'rgb': "FFFF0000"},
         },  # 34
        {'series':   {'theme': "3"},
         'negative': {'theme': "9"},
         'markers':  {'theme': "8"},
         'first':    {'theme': "4"},
         'last':     {'theme': "5"},
         'high':     {'theme': "6"},
         'low':      {'theme': "7"},
         },  # 35
        {'series':   {'theme': "1"},
         'negative': {'theme': "9"},
         'markers':  {'theme': "8"},
         'first':    {'theme': "4"},
         'last':     {'theme': "5"},
         'high':     {'theme': "6"},
         'low':      {'theme': "7"},
         },  # 36
    ]

    return styles[style_id]


def encode_utf8(string):
    """
    Encode any strings passed by the user to UTF8 in Python 2.

    """
    if string is None or sys.version_info >= (3, 0):
        return string
    else:
        return string.encode('utf-8')

########NEW FILE########
__FILENAME__ = vml
###############################################################################
#
# Vml - A class for writing the Excel XLSX Vml file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Package imports.
from . import xmlwriter


class Vml(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Vml file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(Vml, self).__init__()

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################
    def _assemble_xml_file(self, data_id, vml_shape_id, comments_data,
                           buttons_data):
        # Assemble and write the XML file.
        z_index = 1

        self._write_xml_namespace()

        # Write the o:shapelayout element.
        self._write_shapelayout(data_id)

        if buttons_data:
            # Write the v:shapetype element.
            self._write_button_shapetype()

            for button in buttons_data:
                # Write the v:shape element.
                vml_shape_id += 1
                self._write_button_shape(vml_shape_id, z_index, button)
                z_index += 1

        if comments_data:
            # Write the v:shapetype element.
            self._write_comment_shapetype()

            for comment in comments_data:
                # Write the v:shape element.
                vml_shape_id += 1
                self._write_comment_shape(vml_shape_id, z_index, comment)
                z_index += 1

        self._xml_end_tag('xml')

        # Close the XML writer filehandle.
        self._xml_close()

    def _pixels_to_points(self, vertices):
        # Convert comment vertices from pixels to points.

        left, top, width, height = vertices[8:12]

        # Scale to pixels.
        left *= 0.75
        top *= 0.75
        width *= 0.75
        height *= 0.75

        return (left, top, width, height)

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################
    def _write_xml_namespace(self):
        # Write the <xml> element. This is the root element of VML.
        schema = 'urn:schemas-microsoft-com:'
        xmlns = schema + 'vml'
        xmlns_o = schema + 'office:office'
        xmlns_x = schema + 'office:excel'

        attributes = [
            ('xmlns:v', xmlns),
            ('xmlns:o', xmlns_o),
            ('xmlns:x', xmlns_x),
        ]

        self._xml_start_tag('xml', attributes)

    def _write_shapelayout(self, data_id):
        # Write the <o:shapelayout> element.
        attributes = [('v:ext', 'edit')]

        self._xml_start_tag('o:shapelayout', attributes)

        # Write the o:idmap element.
        self._write_idmap(data_id)

        self._xml_end_tag('o:shapelayout')

    def _write_idmap(self, data_id):
        # Write the <o:idmap> element.
        attributes = [
            ('v:ext', 'edit'),
            ('data', data_id),
        ]

        self._xml_empty_tag('o:idmap', attributes)

    def _write_comment_shapetype(self):
        # Write the <v:shapetype> element.
        shape_id = '_x0000_t202'
        coordsize = '21600,21600'
        spt = 202
        path = 'm,l,21600r21600,l21600,xe'

        attributes = [
            ('id', shape_id),
            ('coordsize', coordsize),
            ('o:spt', spt),
            ('path', path),
        ]

        self._xml_start_tag('v:shapetype', attributes)

        # Write the v:stroke element.
        self._write_stroke()

        # Write the v:path element.
        self._write_comment_path('t', 'rect')

        self._xml_end_tag('v:shapetype')

    def _write_button_shapetype(self):
        # Write the <v:shapetype> element.
        shape_id = '_x0000_t201'
        coordsize = '21600,21600'
        spt = 201
        path = 'm,l,21600r21600,l21600,xe'

        attributes = [
            ('id', shape_id),
            ('coordsize', coordsize),
            ('o:spt', spt),
            ('path', path),
        ]

        self._xml_start_tag('v:shapetype', attributes)

        # Write the v:stroke element.
        self._write_stroke()

        # Write the v:path element.
        self._write_button_path()

        # Write the o:lock element.
        self._write_shapetype_lock()

        self._xml_end_tag('v:shapetype')

    def _write_stroke(self):
        # Write the <v:stroke> element.
        joinstyle = 'miter'

        attributes = [('joinstyle', joinstyle)]

        self._xml_empty_tag('v:stroke', attributes)

    def _write_comment_path(self, gradientshapeok, connecttype):
        # Write the <v:path> element.
        attributes = []

        if gradientshapeok:
            attributes.append(('gradientshapeok', 't'))

        attributes.append(('o:connecttype', connecttype))

        self._xml_empty_tag('v:path', attributes)

    def _write_button_path(self):
        # Write the <v:path> element.
        shadowok = 'f'
        extrusionok = 'f'
        strokeok = 'f'
        fillok = 'f'
        connecttype = 'rect'

        attributes = [
            ('shadowok', shadowok),
            ('o:extrusionok', extrusionok),
            ('strokeok', strokeok),
            ('fillok', fillok),
            ('o:connecttype', connecttype),
        ]

        self._xml_empty_tag('v:path', attributes)

    def _write_shapetype_lock(self):
        # Write the <o:lock> element.
        ext = 'edit'
        shapetype = 't'

        attributes = [
            ('v:ext', ext),
            ('shapetype', shapetype),
        ]

        self._xml_empty_tag('o:lock', attributes)

    def _write_rotation_lock(self):
        # Write the <o:lock> element.
        ext = 'edit'
        rotation = 't'

        attributes = [
            ('v:ext', ext),
            ('rotation', rotation),
        ]

        self._xml_empty_tag('o:lock', attributes)

    def _write_comment_shape(self, shape_id, z_index, comment):
        # Write the <v:shape> element.
        shape_type = '#_x0000_t202'
        insetmode = 'auto'
        visibility = 'hidden'

        # Set the shape index.
        shape_id = '_x0000_s' + str(shape_id)

        # Get the comment parameters
        row = comment[0]
        col = comment[1]
        visible = comment[4]
        fillcolor = comment[5]
        vertices = comment[6]

        (left, top, width, height) = self._pixels_to_points(vertices)

        # Set the visibility.
        if visible:
            visibility = 'visible'

        style = (
            'position:absolute;'
            'margin-left:%.15gpt;'
            'margin-top:%.15gpt;'
            'width:%.15gpt;'
            'height:%.15gpt;'
            'z-index:%d;'
            'visibility:%s' % (left, top, width, height, z_index, visibility))

        attributes = [
            ('id', shape_id),
            ('type', shape_type),
            ('style', style),
            ('fillcolor', fillcolor),
            ('o:insetmode', insetmode),
        ]

        self._xml_start_tag('v:shape', attributes)

        # Write the v:fill element.
        self._write_comment_fill()

        # Write the v:shadow element.
        self._write_shadow()

        # Write the v:path element.
        self._write_comment_path(None, 'none')

        # Write the v:textbox element.
        self._write_comment_textbox()

        # Write the x:ClientData element.
        self._write_comment_client_data(row, col, visible, vertices)

        self._xml_end_tag('v:shape')

    def _write_button_shape(self, shape_id, z_index, button):
        # Write the <v:shape> element.
        shape_type = '#_x0000_t201'

        # Set the shape index.
        shape_id = '_x0000_s' + str(shape_id)

        # Get the button parameters.
        # row = button["_row"]
        # col = button["_col"]
        vertices = button["vertices"]

        (left, top, width, height) = self._pixels_to_points(vertices)

        style = (
            'position:absolute;'
            'margin-left:%.15gpt;'
            'margin-top:%.15gpt;'
            'width:%.15gpt;'
            'height:%.15gpt;'
            'z-index:%d;'
            'mso-wrap-style:tight' % (left, top, width, height, z_index))

        attributes = [
            ('id', shape_id),
            ('type', shape_type),
            ('style', style),
            ('o:button', 't'),
            ('fillcolor', 'buttonFace [67]'),
            ('strokecolor', 'windowText [64]'),
            ('o:insetmode', 'auto'),
        ]

        self._xml_start_tag('v:shape', attributes)

        # Write the v:fill element.
        self._write_button_fill()

        # Write the o:lock element.
        self._write_rotation_lock()

        # Write the v:textbox element.
        self._write_button_textbox(button["font"])

        # Write the x:ClientData element.
        self._write_button_client_data(button)

        self._xml_end_tag('v:shape')

    def _write_comment_fill(self):
        # Write the <v:fill> element.
        color_2 = '#ffffe1'

        attributes = [('color2', color_2)]

        self._xml_empty_tag('v:fill', attributes)

    def _write_button_fill(self):
        # Write the <v:fill> element.
        color_2 = 'buttonFace [67]'
        detectmouseclick = 't'

        attributes = [
            ('color2', color_2),
            ('o:detectmouseclick', detectmouseclick),
        ]

        self._xml_empty_tag('v:fill', attributes)

    def _write_shadow(self):
        # Write the <v:shadow> element.
        on = 't'
        color = 'black'
        obscured = 't'

        attributes = [
            ('on', on),
            ('color', color),
            ('obscured', obscured),
        ]

        self._xml_empty_tag('v:shadow', attributes)

    def _write_comment_textbox(self):
        # Write the <v:textbox> element.
        style = 'mso-direction-alt:auto'

        attributes = [('style', style)]

        self._xml_start_tag('v:textbox', attributes)

        # Write the div element.
        self._write_div('left')

        self._xml_end_tag('v:textbox')

    def _write_button_textbox(self, font):
        # Write the <v:textbox> element.
        style = 'mso-direction-alt:auto'

        attributes = [('style', style), ('o:singleclick', 'f')]

        self._xml_start_tag('v:textbox', attributes)

        # Write the div element.
        self._write_div('center', font)

        self._xml_end_tag('v:textbox')

    def _write_div(self, align, font=None):
        # Write the <div> element.

        style = 'text-align:' + align

        attributes = [('style', style)]

        self._xml_start_tag('div', attributes)

        if font:
            # Write the font element.
            self._write_font(font)

        self._xml_end_tag('div')

    def _write_font(self, font):
        # Write the <font> element.
        caption = font["caption"]
        face = 'Calibri'
        size = 220
        color = '#000000'

        attributes = [
            ('face', face),
            ('size', size),
            ('color', color),
        ]

        self._xml_data_element('font', caption, attributes)

    def _write_comment_client_data(self, row, col, visible, vertices):
        # Write the <x:ClientData> element.
        object_type = 'Note'

        attributes = [('ObjectType', object_type)]

        self._xml_start_tag('x:ClientData', attributes)

        # Write the x:MoveWithCells element.
        self._write_move_with_cells()

        # Write the x:SizeWithCells element.
        self._write_size_with_cells()

        # Write the x:Anchor element.
        self._write_anchor(vertices)

        # Write the x:AutoFill element.
        self._write_auto_fill()

        # Write the x:Row element.
        self._write_row(row)

        # Write the x:Column element.
        self._write_column(col)

        # Write the x:Visible element.
        if visible:
            self._write_visible()

        self._xml_end_tag('x:ClientData')

    def _write_button_client_data(self, button):
        # Write the <x:ClientData> element.
        macro = button["macro"]
        vertices = button["vertices"]

        object_type = 'Button'

        attributes = [('ObjectType', object_type)]

        self._xml_start_tag('x:ClientData', attributes)

        # Write the x:Anchor element.
        self._write_anchor(vertices)

        # Write the x:PrintObject element.
        self._write_print_object()

        # Write the x:AutoFill element.
        self._write_auto_fill()

        # Write the x:FmlaMacro element.
        self._write_fmla_macro(macro)

        # Write the x:TextHAlign element.
        self._write_text_halign()

        # Write the x:TextVAlign element.
        self._write_text_valign()

        self._xml_end_tag('x:ClientData')

    def _write_move_with_cells(self):
        # Write the <x:MoveWithCells> element.
        self._xml_empty_tag('x:MoveWithCells')

    def _write_size_with_cells(self):
        # Write the <x:SizeWithCells> element.
        self._xml_empty_tag('x:SizeWithCells')

    def _write_visible(self):
        # Write the <x:Visible> element.
        self._xml_empty_tag('x:Visible')

    def _write_anchor(self, vertices):
        # Write the <x:Anchor> element.
        (col_start, row_start, x1, y1, col_end, row_end, x2, y2) = vertices[:8]

        strings = [col_start, x1, row_start, y1, col_end, x2, row_end, y2]
        strings = [str(i) for i in strings]

        data = ", ".join(strings)

        self._xml_data_element('x:Anchor', data)

    def _write_auto_fill(self):
        # Write the <x:AutoFill> element.
        data = 'False'

        self._xml_data_element('x:AutoFill', data)

    def _write_row(self, data):
        # Write the <x:Row> element.
        self._xml_data_element('x:Row', data)

    def _write_column(self, data):
        # Write the <x:Column> element.
        self._xml_data_element('x:Column', data)

    def _write_print_object(self):
        # Write the <x:PrintObject> element.
        self._xml_data_element('x:PrintObject', 'False')

    def _write_text_halign(self):
        # Write the <x:TextHAlign> element.
        self._xml_data_element('x:TextHAlign', 'Center')

    def _write_text_valign(self):
        # Write the <x:TextVAlign> element.
        self._xml_data_element('x:TextVAlign', 'Center')

    def _write_fmla_macro(self, data):
        # Write the <x:FmlaMacro> element.
        self._xml_data_element('x:FmlaMacro', data)

########NEW FILE########
__FILENAME__ = workbook
###############################################################################
#
# Workbook - A class for writing the Excel XLSX Workbook file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Standard packages.
import re
import os
import tempfile
import operator
import shutil
from warnings import warn
from datetime import datetime
from zipfile import ZipFile, ZIP_DEFLATED
from struct import unpack

# Package imports.
from . import xmlwriter
from xlsxwriter.worksheet import Worksheet
from xlsxwriter.sharedstrings import SharedStringTable
from xlsxwriter.format import Format
from xlsxwriter.packager import Packager
from .utility import xl_cell_to_rowcol
from .utility import encode_utf8
from xlsxwriter.chart_area import ChartArea
from xlsxwriter.chart_bar import ChartBar
from xlsxwriter.chart_column import ChartColumn
from xlsxwriter.chart_line import ChartLine
from xlsxwriter.chart_pie import ChartPie
from xlsxwriter.chart_radar import ChartRadar
from xlsxwriter.chart_scatter import ChartScatter
from xlsxwriter.chart_stock import ChartStock


class Workbook(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Workbook file.


    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self, filename=None, options={}):
        """
        Constructor.

        """

        super(Workbook, self).__init__()

        self.filename = filename
        self.tmpdir = options.get('tmpdir', None)
        self.date_1904 = options.get('date_1904', False)
        self.worksheet_meta = WorksheetMeta()
        self.selected = 0
        self.fileclosed = 0
        self.filehandle = None
        self.internal_fh = 0
        self.sheet_name = 'Sheet'
        self.chart_name = 'Chart'
        self.sheetname_count = 0
        self.chartname_count = 0
        self.worksheets_objs = []
        self.charts = []
        self.drawings = []
        self.sheetnames = []
        self.formats = []
        self.xf_formats = []
        self.xf_format_indices = {}
        self.dxf_formats = []
        self.dxf_format_indices = {}
        self.palette = []
        self.font_count = 0
        self.num_format_count = 0
        self.defined_names = []
        self.named_ranges = []
        self.custom_colors = []
        self.doc_properties = {}
        self.localtime = datetime.now()
        self.num_vml_files = 0
        self.num_comment_files = 0
        self.optimization = options.get('constant_memory', 0)
        self.x_window = 240
        self.y_window = 15
        self.window_width = 16095
        self.window_height = 9660
        self.tab_ratio = 500
        self.str_table = SharedStringTable()
        self.vba_project = None
        self.vba_codename = None
        self.image_types = {}
        self.images = []

        # Add the default cell format.
        self.add_format({'xf_index': 0})

    def __del__(self):
        """Close file in destructor if it hasn't been closed explicitly."""
        if not self.fileclosed:
            self.close()

    def add_worksheet(self, name=None):
        """
        Add a new worksheet to the Excel workbook.

        Args:
            name: The worksheet name. Defaults to 'Sheet1', etc.

        Returns:
            Reference to a worksheet object.

        """
        sheet_index = len(self.worksheets_objs)
        name = self._check_sheetname(name)

        # Encode any string options passed by the user.
        name = encode_utf8(name)

        # Initialisation data to pass to the worksheet.
        init_data = {
            'name': name,
            'index': sheet_index,
            'str_table': self.str_table,
            'worksheet_meta': self.worksheet_meta,
            'optimization': self.optimization,
            'tmpdir': self.tmpdir,
            'date_1904': self.date_1904,
        }

        worksheet = Worksheet()
        worksheet._initialize(init_data)

        self.worksheets_objs.append(worksheet)
        self.sheetnames.append(name)

        return worksheet

    def add_format(self, properties={}):
        """
        Add a new Format to the Excel Workbook.

        Args:
            properties: The format properties.

        Returns:
            Reference to a Format object.

        """
        xf_format = Format(properties,
                           self.xf_format_indices,
                           self.dxf_format_indices)

        # Store the format reference.
        self.formats.append(xf_format)

        return xf_format

    def add_chart(self, options):
        """
        Create a chart object.

        Args:
            options: The chart type and subtype options.

        Returns:
            Reference to a Chart object.

        """

        # Type must be specified so we can create the required chart instance.
        chart_type = options.get('type', 'None')
        if chart_type is None:
            warn("Chart type must be defined in add_chart()")
            return

        if chart_type == 'area':
            chart = ChartArea(options)
        elif chart_type == 'bar':
            chart = ChartBar(options)
        elif chart_type == 'column':
            chart = ChartColumn(options)
        elif chart_type == 'line':
            chart = ChartLine(options)
        elif chart_type == 'pie':
            chart = ChartPie(options)
        elif chart_type == 'radar':
            chart = ChartRadar(options)
        elif chart_type == 'scatter':
            chart = ChartScatter(options)
        elif chart_type == 'stock':
            chart = ChartStock(options)
        else:
            warn("Unknown chart type '%s' in add_chart()" % chart_type)
            return

        # Set the embedded chart name if present.
        if 'name' in options:
            chart.chart_name = options['name']

        chart._set_embedded_config_data()
        self.charts.append(chart)

        return chart

    def close(self):
        """
        Call finalisation code and close file.

        Args:
            None.

        Returns:
            Nothing.

        """
        if not self.fileclosed:
            self.fileclosed = 1
            self._store_workbook()

    def set_properties(self, properties):
        """
        Set the document properties such as Title, Author etc.

        Args:
            properties: Dictionary of document properties.

        Returns:
            Nothing.

        """
        self.doc_properties = properties

    def define_name(self, name, formula):
        # Create a defined name in Excel. We handle global/workbook level
        # names and local/worksheet names.
        """
        Create a defined name in the workbook.

        Args:
            name:    The defined name.
            formula: The cell or range that the defined name refers to.

        Returns:
            Nothing.

        """
        sheet_index = None
        sheetname = ''

        # Remove the = sign from the formula if it exists.
        if formula.startswith('='):
            formula = formula.lstrip('=')

        # Local defined names are formatted like "Sheet1!name".
        sheet_parts = re.compile(r'^(.*)!(.*)$')
        match = sheet_parts.match(name)

        if match:
            sheetname = match.group(1)
            name = match.group(2)
            sheet_index = self._get_sheet_index(sheetname)

            # Warn if the sheet index wasn't found.
            if sheet_index is None:
                warn("Unknown sheet name '%s' in defined_name()" % sheetname)
                return -1
        else:
            # Use -1 to indicate global names.
            sheet_index = -1

        # Warn if the sheet name contains invalid chars as defined by Excel.
        if not re.match(r'^[a-zA-Z_\\][a-zA-Z_.]+', name):
            warn("Invalid Excel characters in defined_name(): '%s'" % name)
            return -1

        # Warn if the sheet name looks like a cell name.
        if re.match(r'^[a-zA-Z][a-zA-Z]?[a-dA-D]?[0-9]+$', name):
            warn("Name looks like a cell name in defined_name(): '%s'" % name)
            return -1

        # Encode any string options passed by the user.
        name = encode_utf8(name)
        formula = encode_utf8(formula)

        self.defined_names.append([name, sheet_index, formula, False])

    def worksheets(self):
        """
        Return a list of the worksheet objects in the workbook.

        Args:
            None.

        Returns:
            A list of worksheet objects.

        """
        return self.worksheets_objs

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Prepare format object for passing to Style.pm.
        self._prepare_format_properties()

        # Write the XML declaration.
        self._xml_declaration()

        # Write the workbook element.
        self._write_workbook()

        # Write the fileVersion element.
        self._write_file_version()

        # Write the workbookPr element.
        self._write_workbook_pr()

        # Write the bookViews element.
        self._write_book_views()

        # Write the sheets element.
        self._write_sheets()

        # Write the workbook defined names.
        self._write_defined_names()

        # Write the calcPr element.
        self._write_calc_pr()

        # Close the workbook tag.
        self._xml_end_tag('workbook')

        # Close the file.
        self._xml_close()

    def _store_workbook(self):
        # Assemble worksheets into a workbook.
        temp_dir = tempfile.mkdtemp(dir=self.tmpdir)
        packager = Packager()

        # Add a default worksheet if non have been added.
        if not self.worksheets():
            self.add_worksheet()

        # Ensure that at least one worksheet has been selected.
        if self.worksheet_meta.activesheet == 0:
            self.worksheets_objs[0].selected = 1
            self.worksheets_objs[0].hidden = 0

        # Set the active sheet.
        for sheet in self.worksheets():
            if sheet.index == self.worksheet_meta.activesheet:
                sheet.active = 1

        # Convert the SST strings data structure.
        self._prepare_sst_string_data()

        # Prepare the worksheet VML elements such as comments and buttons.
        self._prepare_vml()

        # Set the defined names for the worksheets such as Print Titles.
        self._prepare_defined_names()

        # Prepare the drawings, charts and images.
        self._prepare_drawings()

        # Add cached data to charts.
        self._add_chart_data()

        # Package the workbook.
        packager._add_workbook(self)
        packager._set_package_dir(temp_dir)
        packager._create_package()

        # Free up the Packager object.
        packager = None

        xlsx_file = ZipFile(self.filename, "w", compression=ZIP_DEFLATED)

        # Add separator to temp dir so we have a root to strip from paths.
        dir_root = os.path.join(temp_dir, '')

        # Iterate through files in the temp dir and add them to the xlsx file.
        for dirpath, _, filenames in os.walk(temp_dir):
            for name in filenames:
                abs_filename = os.path.join(dirpath, name)
                rel_filename = abs_filename.replace(dir_root, '')
                xlsx_file.write(abs_filename, rel_filename)

        shutil.rmtree(temp_dir)
        xlsx_file.close()

    def _check_sheetname(self, sheetname, is_chart=False):
        # Check for valid worksheet names. We check the length, if it contains
        # any invalid chars and if the sheetname is unique in the workbook.
        invalid_char = re.compile(r'[\[\]:*?/\\]')

        # Increment the Sheet/Chart number used for default sheet names below.
        if is_chart:
            self.chartname_count += 1
        else:
            self.sheetname_count += 1

        # Supply default Sheet/Chart sheetname if none has been defined.
        if sheetname is None:
            if is_chart:
                sheetname = self.chart_name + str(self.chartname_count)
            else:
                sheetname = self.sheet_name + str(self.sheetname_count)

        # Check that sheet sheetname is <= 31. Excel limit.
        if len(sheetname) > 31:
            raise Exception("Excel worksheet name '%s' must be <= 31 chars." %
                            sheetname)

        # Check that sheetname doesn't contain any invalid characters
        if invalid_char.search(sheetname):
            raise Exception(
                "Invalid Excel character '[]:*?/\\' in sheetname '%s'" %
                sheetname)

        # Check that the worksheet name doesn't already exist since this is a
        # fatal Excel error. The check must be case insensitive like Excel.
        for worksheet in self.worksheets():
            if sheetname.lower() == worksheet.name.lower():
                raise Exception(
                    "Sheetname '%s', with case ignored, is already in use." %
                    sheetname)

        return sheetname

    def _prepare_format_properties(self):
        # Prepare all Format properties prior to passing them to styles.py.

        # Separate format objects into XF and DXF formats.
        self._prepare_formats()

        # Set the font index for the format objects.
        self._prepare_fonts()

        # Set the number format index for the format objects.
        self._prepare_num_formats()

        # Set the border index for the format objects.
        self._prepare_borders()

        # Set the fill index for the format objects.
        self._prepare_fills()

    def _prepare_formats(self):
        # Iterate through the XF Format objects and separate them into
        # XF and DXF formats. The XF and DF formats then need to be sorted
        # back into index order rather than creation order.
        xf_formats = []
        dxf_formats = []

        # Sort into XF and DXF formats.
        for xf_format in self.formats:
            if xf_format.xf_index is not None:
                xf_formats.append(xf_format)

            if xf_format.dxf_index is not None:
                dxf_formats.append(xf_format)

        # Pre-extend the format lists.
        self.xf_formats = [None] * len(xf_formats)
        self.dxf_formats = [None] * len(dxf_formats)

        # Rearrange formats into index order.
        for xf_format in xf_formats:
            index = xf_format.xf_index
            self.xf_formats[index] = xf_format

        for dxf_format in dxf_formats:
            index = dxf_format.dxf_index
            self.dxf_formats[index] = dxf_format

    def _set_default_xf_indices(self):
        # Set the default index for each format. Mainly used for testing.
        for xf_format in self.formats:
            xf_format._get_xf_index()

    def _prepare_fonts(self):
        # Iterate through the XF Format objects and give them an index to
        # non-default font elements.
        fonts = {}
        index = 0

        for xf_format in self.xf_formats:
            key = xf_format._get_font_key()
            if key in fonts:
                # Font has already been used.
                xf_format.font_index = fonts[key]
                xf_format.has_font = 0
            else:
                # This is a new font.
                fonts[key] = index
                xf_format.font_index = index
                xf_format.has_font = 1
                index += 1

        self.font_count = index

        # For DXF formats we only need to check if the properties have changed.
        for xf_format in self.dxf_formats:
            # The only font properties that can change for a DXF format are:
            # color, bold, italic, underline and strikethrough.
            if (xf_format.font_color or xf_format.bold or xf_format.italic
                    or xf_format.underline or xf_format.font_strikeout):
                xf_format.has_dxf_font = 1

    def _prepare_num_formats(self):
        # User records is not None start from index 0xA4.
        num_formats = {}
        index = 164
        num_format_count = 0

        is_number = re.compile(r'^\d+$')
        is_zeroes = re.compile(r'^0+\d')

        for xf_format in (self.xf_formats + self.dxf_formats):
            num_format = xf_format.num_format
            # Check if num_format is an index to a built-in number format.
            # Also check for a string of zeros, which is a valid number
            # format string but would evaluate to zero.
            if (is_number.match(str(num_format))
                    and not is_zeroes.match(str(num_format))):
                # Index to a built-in number xf_format.
                xf_format.num_format_index = num_format
                continue

            if num_format in num_formats:
                # Number xf_format has already been used.
                xf_format.num_format_index = num_formats[num_format]
            else:
                # Add a new number xf_format.
                num_formats[num_format] = index
                xf_format.num_format_index = index
                index += 1

                # Only increase font count for XF formats (not DXF formats).
                if xf_format.xf_index:
                    num_format_count += 1

        self.num_format_count = num_format_count

    def _prepare_borders(self):
        # Iterate through the XF Format objects and give them an index to
        # non-default border elements.
        borders = {}
        index = 0

        for xf_format in self.xf_formats:
            key = xf_format._get_border_key()

            if key in borders:
                # Border has already been used.
                xf_format.border_index = borders[key]
                xf_format.has_border = 0
            else:
                # This is a new border.
                borders[key] = index
                xf_format.border_index = index
                xf_format.has_border = 1
                index += 1

        self.border_count = index

        # For DXF formats we only need to check if the properties have changed.
        has_border = re.compile(r'[^0:]')

        for xf_format in self.dxf_formats:
            key = xf_format._get_border_key()

            if has_border.search(key):
                xf_format.has_dxf_border = 1

    def _prepare_fills(self):
        # Iterate through the XF Format objects and give them an index to
        # non-default fill elements.
        # The user defined fill properties start from 2 since there are 2
        # default fills: patternType="none" and patternType="gray125".
        fills = {}
        index = 2  # Start from 2. See above.

        # Add the default fills.
        fills['0:0:0'] = 0
        fills['17:0:0'] = 1

        # Store the DXF colours separately since them may be reversed below.
        for xf_format in self.dxf_formats:
            if (xf_format.pattern or xf_format.bg_color or xf_format.fg_color):
                xf_format.has_dxf_fill = 1
                xf_format.dxf_bg_color = xf_format.bg_color
                xf_format.dxf_fg_color = xf_format.fg_color

        for xf_format in self.xf_formats:
            # The following logical statements jointly take care of special
            # cases in relation to cell colours and patterns:
            # 1. For a solid fill (_pattern == 1) Excel reverses the role of
            # foreground and background colours, and
            # 2. If the user specifies a foreground or background colour
            # without a pattern they probably wanted a solid fill, so we fill
            # in the defaults.
            if (xf_format.pattern == 1 and xf_format.bg_color != 0
                    and xf_format.fg_color != 0):
                tmp = xf_format.fg_color
                xf_format.fg_color = xf_format.bg_color
                xf_format.bg_color = tmp

            if (xf_format.pattern <= 1 and xf_format.bg_color != 0
                    and xf_format.fg_color == 0):
                xf_format.fg_color = xf_format.bg_color
                xf_format.bg_color = 0
                xf_format.pattern = 1

            if (xf_format.pattern <= 1 and xf_format.bg_color == 0
                    and xf_format.fg_color != 0):
                xf_format.bg_color = 0
                xf_format.pattern = 1

            key = xf_format._get_fill_key()

            if key in fills:
                # Fill has already been used.
                xf_format.fill_index = fills[key]
                xf_format.has_fill = 0
            else:
                # This is a new fill.
                fills[key] = index
                xf_format.fill_index = index
                xf_format.has_fill = 1
                index += 1

        self.fill_count = index

    def _prepare_defined_names(self):
        # Iterate through the worksheets and store any defined names in
        # addition to any user defined names. Stores the defined names
        # for the Workbook.xml and the named ranges for App.xml.
        defined_names = self.defined_names

        for sheet in self.worksheets():
            # Check for Print Area settings.
            if sheet.autofilter_area:
                hidden = 1
                sheet_range = sheet.autofilter_area
                # Store the defined names.
                defined_names.append(['_xlnm._FilterDatabase',
                                      sheet.index, sheet_range, hidden])

            # Check for Print Area settings.
            if sheet.print_area_range:
                hidden = 0
                sheet_range = sheet.print_area_range
                # Store the defined names.
                defined_names.append(['_xlnm.Print_Area',
                                      sheet.index, sheet_range, hidden])

            # Check for repeat rows/cols referred to as Print Titles.
            if sheet.repeat_col_range or sheet.repeat_row_range:
                hidden = 0
                sheet_range = ''
                if sheet.repeat_col_range and sheet.repeat_row_range:
                    sheet_range = (sheet.repeat_col_range + ',' +
                                   sheet.repeat_row_range)
                else:
                    sheet_range = (sheet.repeat_col_range +
                                   sheet.repeat_row_range)
                # Store the defined names.
                defined_names.append(['_xlnm.Print_Titles',
                                      sheet.index, sheet_range, hidden])

        defined_names = self._sort_defined_names(defined_names)
        self.defined_names = defined_names
        self.named_ranges = self._extract_named_ranges(defined_names)

    def _sort_defined_names(self, names):
        # Sort the list of list of internal and user defined names in
        # the same order as used by Excel.

        # Add a normalise name string to each list for sorting.
        for name_list in names:
            (defined_name, _, sheet_name, _) = name_list

            # Normalise the defined name by removing any leading '_xmln.'
            # from internal names and lowercasing the string.
            defined_name = defined_name.replace('_xlnm.', '').lower()

            # Normalise the sheetname by removing the leading quote and
            # lowercasing the string.
            sheet_name = sheet_name.lstrip("'").lower()

            name_list.append(defined_name + "::" + sheet_name)

        # Remove the extra key for sorting.
        names.sort(key=operator.itemgetter(4))

        for name_list in names:
            name_list.pop()

        return names

    def _prepare_drawings(self):
        # Iterate through the worksheets and set up chart and image drawings.
        chart_ref_id = 0
        image_ref_id = 0
        drawing_id = 0

        for sheet in self.worksheets():
            chart_count = len(sheet.charts)
            image_count = len(sheet.images)
            shape_count = len(sheet.shapes)

            if not (chart_count + image_count + shape_count):
                continue

            drawing_id += 1

            for index in range(chart_count):
                chart_ref_id += 1
                sheet._prepare_chart(index, chart_ref_id, drawing_id)

            for index in range(image_count):
                filename = sheet.images[index][2]
                (image_type, width, height, name) = \
                    self._get_image_properties(filename)
                image_ref_id += 1

                sheet._prepare_image(index, image_ref_id, drawing_id, width,
                                     height, name, image_type)

            # for index in range(shape_count):
            #    sheet._prepare_shape(index, drawing_id)

            drawing = sheet.drawing
            self.drawings.append(drawing)

        # Sort the workbook charts references into the order that the were
        # written from the worksheets above.
        self.charts = sorted(self.charts, key=lambda chart: chart.id)

        self.drawing_count = drawing_id

    def _get_image_properties(self, filename):
        # Extract dimension information from the image file.
        height = 0
        width = 0

        # Open the image file and read in the data.
        fh = open(filename, "rb")
        data = fh.read()

        # Get the image filename without the path.
        image_name = os.path.basename(filename)

        # Look for some common image file markers.
        marker1 = (unpack('3s', data[1:4]))[0]
        marker2 = (unpack('>H', data[:2]))[0]
        marker3 = (unpack('4s', data[6:10]))[0]
        marker4 = (unpack('2s', data[:2]))[0]

        if marker1 == b'PNG':
            self.image_types['png'] = 1
            (image_type, width, height) = self._process_png(data)

        elif (marker2 == 0xFFD8 and
              (marker3 == b'JFIF' or marker3 == b'EXIF')):
            self.image_types['jpeg'] = 1
            (image_type, width, height) = self._process_jpg(data)

        elif (marker4 == b'BM'):
            self.image_types['bmp'] = 1
            (image_type, width, height) = self._process_bmp(data)

        else:
            raise Exception("%s: Unknown or unsupported file type." % filename)

        # Check that we found the required data.
        if not height or not width:
            raise Exception("%s: no size data found in image file." % filename)

        # Store image data to copy it into file container.
        self.images.append([filename, image_type])

        fh.close()
        return (image_type, width, height, image_name)

    def _process_png(self, data):
        # Extract width and height information from a PNG file.
        width = (unpack('>I', data[16:20]))[0]
        height = (unpack('>I', data[20:24]))[0]

        return ('png', width, height)

    def _process_jpg(self, data):
        # Extract width and height information from a JPEG file.
        offset = 2
        data_length = len(data)

        # Search through the image data to find the 0xFFC0 marker.
        # The height and width are contained in the data for that
        # sub-element.
        found = 0
        while not found and offset < data_length:

            marker = (unpack('>H', data[offset + 0:offset + 2]))[0]
            length = (unpack('>H', data[offset + 2:offset + 4]))[0]

            if marker == 0xFFC0 or marker == 0xFFC2:
                height = (unpack('>H', data[offset + 5:offset + 7]))[0]
                width = (unpack('>H', data[offset + 7:offset + 9]))[0]
                found = 1
                continue

            offset = offset + length + 2

            if marker == 0xFFDA:
                found = 1
                continue

        return ('jpeg', width, height)

    def _process_bmp(self, data):
        # Extract width and height information from a BMP file.
        width = (unpack('<L', data[18:22]))[0]
        height = (unpack('<L', data[22:26]))[0]
        return ('bmp', width, height)

    def _extract_named_ranges(self, defined_names):
        # Extract the named ranges from the sorted list of defined names.
        # These are used in the App.xml file.
        named_ranges = []

        for defined_name in (defined_names):

            name = defined_name[0]
            index = defined_name[1]
            sheet_range = defined_name[2]

            # Skip autoFilter ranges.
            if name == '_xlnm._FilterDatabase':
                continue

            # We are only interested in defined names with ranges.
            if '!' in sheet_range:
                sheet_name, _ = sheet_range.split('!', 1)

                # Match Print_Area and Print_Titles xlnm types.
                if name.startswith('_xlnm.'):
                    xlnm_type = name.replace('_xlnm.', '')
                    name = sheet_name + '!' + xlnm_type
                elif index != -1:
                    name = sheet_name + '!' + name

                named_ranges.append(name)

        return named_ranges

    def _get_sheet_index(self, sheetname):
        # Convert a sheet name to its index. Return None otherwise.
        sheetname = sheetname.strip("'")

        if sheetname in self.sheetnames:
            return self.sheetnames.index(sheetname)
        else:
            return None

    #
    # Iterate through the worksheets and set up the VML objects.
    #
    def _prepare_vml(self):
        comment_id = 0
        vml_data_id = 1
        vml_shape_id = 1024
        vml_files = 0
        comment_files = 0

        for sheet in self.worksheets():
            if not sheet.has_vml:
                continue

            vml_files += 1

            if sheet.has_comments:
                comment_files += 1

            comment_id += 1
            count = sheet._prepare_vml_objects(vml_data_id,
                                               vml_shape_id,
                                               comment_id)

            # Each VML file should start with a shape id incremented by 1024.
            vml_data_id += 1 * int((1024 + count) / 1024)
            vml_shape_id += 1024 * int((1024 + count) / 1024)

        self.num_vml_files = vml_files
        self.num_comment_files = comment_files

        # Add a font format for cell comments.
        if comment_files > 0:
            xf = self.add_format({'font_name': 'Tahoma', 'font_size': 8,
                                  'color_indexed': 81, 'font_only': True})
            xf._get_xf_index()

    def _add_chart_data(self):
        # Add "cached" data to charts to provide the numCache and strCacher
        # data for series and title/axis ranges.
        worksheets = {}
        seen_ranges = {}

        # Map worksheet names to worksheet objects.
        for worksheet in self.worksheets():
            worksheets[worksheet.name] = worksheet

        for chart in self.charts:

            for c_range in chart.formula_ids.keys():
                r_id = chart.formula_ids[c_range]

                # Skip if the series has user defined data.
                if chart.formula_data[r_id] is not None:
                    if (not c_range in seen_ranges
                            or seen_ranges[c_range] is None):
                        data = chart.formula_data[r_id]
                        seen_ranges[c_range] = data
                    continue

                # Check to see if the data is already cached locally.
                if c_range in seen_ranges:
                    chart.formula_data[r_id] = seen_ranges[c_range]
                    continue

                # Convert the range formula to a sheet name and cell range.
                (sheetname, cells) = self._get_chart_range(c_range)

                # Skip if we couldn't parse the formula.
                if sheetname is None:
                    continue

                # Die if the name is unknown since it indicates a user error in
                # a chart series formula.
                if not sheetname in worksheets:
                    warn("Unknown worksheet reference '%s' in range "
                         "'%s' passed to add_series()" % (sheetname, c_range))

                # Find the worksheet object based on the sheet name.
                worksheet = worksheets[sheetname]

                # Get the data from the worksheet table.
                data = worksheet._get_range_data(*cells)

                # TODO
                #   # Ignore rich strings for now. Deparse later if necessary.
                #        if token =~ m{^<r>} and token =~ m{</r>$}:
                #            token = ''

                # Add the data to the chart.
                chart.formula_data[r_id] = data

                # Store range data locally to avoid lookup if seen again.
                seen_ranges[c_range] = data

    def _get_chart_range(self, c_range):
        # Convert a range formula such as Sheet1!$B$1:$B$5 into a sheet name
        # and cell range such as ( 'Sheet1', 0, 1, 4, 1 ).

        # Split the range formula into sheetname and cells at the last '!'.
        # TODO. Fix this to match from right.
        pos = c_range.find('!')
        if pos > 0:
            sheetname, cells = c_range.split('!')
        else:
            return None

        # Split the cell range into 2 cells or else use single cell for both.
        if cells.find(':') > 0:
            (cell_1, cell_2) = cells.split(':')
        else:
            (cell_1, cell_2) = (cells, cells)

        # Remove leading/trailing quotes and convert escaped quotes to single.
        sheetname = sheetname.strip("'")
        sheetname = sheetname.replace("''", "'")

        (row_start, col_start) = xl_cell_to_rowcol(cell_1)
        (row_end, col_end) = xl_cell_to_rowcol(cell_2)

        # Check that we have a 1D range only.
        if row_start != row_end and col_start != col_end:
            return None

        return (sheetname, [row_start, col_start, row_end, col_end])

    def _prepare_sst_string_data(self):
        # Convert the SST string data from a dict to a list.
        self.str_table._sort_string_data()

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_workbook(self):
        # Write <workbook> element.

        schema = 'http://schemas.openxmlformats.org'
        xmlns = schema + '/spreadsheetml/2006/main'
        xmlns_r = schema + '/officeDocument/2006/relationships'

        attributes = [
            ('xmlns', xmlns),
            ('xmlns:r', xmlns_r),
        ]

        self._xml_start_tag('workbook', attributes)

    def _write_file_version(self):
        # Write the <fileVersion> element.

        app_name = 'xl'
        last_edited = 4
        lowest_edited = 4
        rup_build = 4505

        attributes = [
            ('appName', app_name),
            ('lastEdited', last_edited),
            ('lowestEdited', lowest_edited),
            ('rupBuild', rup_build),
        ]

        if self.vba_project:
            attributes.append(
                ('codeName', '{37E998C4-C9E5-D4B9-71C8-EB1FF731991C}'))

        self._xml_empty_tag('fileVersion', attributes)

    def _write_workbook_pr(self):
        # Write <workbookPr> element.
        default_theme_version = 124226
        attributes = []

        if self.vba_codename:
            attributes.append(('codeName', self.vba_codename))
        if self.date_1904:
            attributes.append(('date1904', 1))

        attributes.append(('defaultThemeVersion', default_theme_version))

        self._xml_empty_tag('workbookPr', attributes)

    def _write_book_views(self):
        # Write <bookViews> element.
        self._xml_start_tag('bookViews')
        self._write_workbook_view()
        self._xml_end_tag('bookViews')

    def _write_workbook_view(self):
        # Write <workbookView> element.
        attributes = [
            ('xWindow', self.x_window),
            ('yWindow', self.y_window),
            ('windowWidth', self.window_width),
            ('windowHeight', self.window_height),
        ]

        # Store the tabRatio attribute when it isn't the default.
        if self.tab_ratio != 500:
            attributes.append(('tabRatio', self.tab_ratio))

        # Store the firstSheet attribute when it isn't the default.
        if self.worksheet_meta.firstsheet > 0:
            firstsheet = self.worksheet_meta.firstsheet + 1
            attributes.append(('firstSheet', firstsheet))

        # Store the activeTab attribute when it isn't the first sheet.
        if self.worksheet_meta.activesheet > 0:
            attributes.append(('activeTab', self.worksheet_meta.activesheet))

        self._xml_empty_tag('workbookView', attributes)

    def _write_sheets(self):
        # Write <sheets> element.
        self._xml_start_tag('sheets')

        id_num = 1
        for worksheet in self.worksheets():
            self._write_sheet(worksheet.name, id_num, worksheet.hidden)
            id_num += 1

        self._xml_end_tag('sheets')

    def _write_sheet(self, name, sheet_id, hidden):
        # Write <sheet> element.
        attributes = [
            ('name', name),
            ('sheetId', sheet_id),
        ]

        if hidden:
            attributes.append(('state', 'hidden'))

        attributes.append(('r:id', 'rId' + str(sheet_id)))

        self._xml_empty_tag('sheet', attributes)

    def _write_calc_pr(self):
        # Write the <calcPr> element.

        calc_id = '124519'

        attributes = [('calcId', calc_id)]

        self._xml_empty_tag('calcPr', attributes)

    def _write_defined_names(self):
        # Write the <definedNames> element.
        if not self.defined_names:
            return

        self._xml_start_tag('definedNames')

        for defined_name in (self.defined_names):
            self._write_defined_name(defined_name)

        self._xml_end_tag('definedNames')

    def _write_defined_name(self, defined_name):
        # Write the <definedName> element.
        name = defined_name[0]
        sheet_id = defined_name[1]
        sheet_range = defined_name[2]
        hidden = defined_name[3]

        attributes = [('name', name)]

        if sheet_id != -1:
            attributes.append(('localSheetId', sheet_id))
        if hidden:
            attributes.append(('hidden', 1))

        self._xml_data_element('definedName', sheet_range, attributes)


# A metadata class to share data between worksheets.
class WorksheetMeta(object):
    """
    A class to track worksheets data such as the active sheet and the
    first sheet..

    """

    def __init__(self):
        self.activesheet = 0
        self.firstsheet = 0
        self.table_count = 0

########NEW FILE########
__FILENAME__ = worksheet
###############################################################################
#
# Worksheet - A class for writing the Excel XLSX Worksheet file.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Standard packages.
import re
import datetime
import tempfile
from warnings import warn
from collections import defaultdict
from collections import namedtuple

# For compatibility between Python 2 and 3.
try:
    from StringIO import StringIO
except ImportError:
    from io import StringIO

# Package imports.
from . import xmlwriter
from .format import Format
from .drawing import Drawing
from .xmlwriter import XMLwriter
from .utility import xl_rowcol_to_cell
from .utility import xl_rowcol_to_cell_fast
from .utility import xl_cell_to_rowcol
from .utility import xl_col_to_name
from .utility import xl_range
from .utility import xl_color
from .utility import get_sparkline_style
from .utility import encode_utf8


###############################################################################
#
# Decorator functions.
#
###############################################################################
def convert_cell_args(method):
    """
    Decorator function to convert A1 notation in cell method calls
    to the default row/col notation.

    """
    def cell_wrapper(self, *args, **kwargs):

        try:
            # First arg is an int, default to row/col notation.
            if len(args):
                int(args[0])
            return method(self, *args, **kwargs)
        except ValueError:
            # First arg isn't an int, convert to A1 notation.
            new_args = list(xl_cell_to_rowcol(args[0]))
            new_args.extend(args[1:])
            return method(self, *new_args, **kwargs)

    return cell_wrapper


def convert_range_args(method):
    """
    Decorator function to convert A1 notation in range method calls
    to the default row/col notation.

    """
    def cell_wrapper(self, *args, **kwargs):

        try:
            # First arg is an int, default to row/col notation.
            if len(args):
                int(args[0])
            return method(self, *args, **kwargs)
        except ValueError:
            # First arg isn't an int, convert to A1 notation.
            if ':' in args[0]:
                cell_1, cell_2 = args[0].split(':')
                row_1, col_1 = xl_cell_to_rowcol(cell_1)
                row_2, col_2 = xl_cell_to_rowcol(cell_2)
            else:
                row_1, col_1 = xl_cell_to_rowcol(args[0])
                row_2, col_2 = row_1, col_1

            new_args = [row_1, col_1, row_2, col_2]
            new_args.extend(args[1:])
            return method(self, *new_args, **kwargs)

    return cell_wrapper


def convert_column_args(method):
    """
    Decorator function to convert A1 notation in columns method calls
    to the default row/col notation.

    """
    def column_wrapper(self, *args, **kwargs):

        try:
            # First arg is an int, default to row/col notation.
            if len(args):
                int(args[0])
            return method(self, *args, **kwargs)
        except ValueError:
            # First arg isn't an int, convert to A1 notation.
            cell_1, cell_2 = [col + '1' for col in args[0].split(':')]
            _, col_1 = xl_cell_to_rowcol(cell_1)
            _, col_2 = xl_cell_to_rowcol(cell_2)
            new_args = [col_1, col_2]
            new_args.extend(args[1:])
            return method(self, *new_args, **kwargs)

    return column_wrapper


###############################################################################
#
# Named tuples used for cell types.
#
###############################################################################
cell_string_tuple = namedtuple('String', 'string, format')
cell_number_tuple = namedtuple('Number', 'number, format')
cell_blank_tuple = namedtuple('Blank', 'format')
cell_formula_tuple = namedtuple('Formula', 'formula, format, value')
cell_arformula_tuple = namedtuple('ArrayFormula',
                                  'formula, format, value, range')


###############################################################################
#
# Worksheet Class definition.
#
###############################################################################
class Worksheet(xmlwriter.XMLwriter):
    """
    A class for writing the Excel XLSX Worksheet file.

    """

    ###########################################################################
    #
    # Public API.
    #
    ###########################################################################

    def __init__(self):
        """
        Constructor.

        """

        super(Worksheet, self).__init__()

        self.name = None
        self.index = None
        self.str_table = None
        self.palette = None
        self.optimization = 0
        self.tmpdir = None

        self.ext_sheets = []
        self.fileclosed = 0
        self.excel_version = 2007

        self.xls_rowmax = 1048576
        self.xls_colmax = 16384
        self.xls_strmax = 32767
        self.dim_rowmin = None
        self.dim_rowmax = None
        self.dim_colmin = None
        self.dim_colmax = None

        self.colinfo = []
        self.selections = []
        self.hidden = 0
        self.active = 0
        self.tab_color = 0

        self.panes = []
        self.active_pane = 3
        self.selected = 0

        self.page_setup_changed = 0
        self.paper_size = 0
        self.orientation = 1

        self.print_options_changed = 0
        self.hcenter = 0
        self.vcenter = 0
        self.print_gridlines = 0
        self.screen_gridlines = 1
        self.print_headers = 0

        self.header_footer_changed = 0
        self.header = ''
        self.footer = ''

        self.margin_left = 0.7
        self.margin_right = 0.7
        self.margin_top = 0.75
        self.margin_bottom = 0.75
        self.margin_header = 0.3
        self.margin_footer = 0.3

        self.repeat_row_range = ''
        self.repeat_col_range = ''
        self.print_area_range = ''

        self.page_order = 0
        self.black_white = 0
        self.draft_quality = 0
        self.print_comments = 0
        self.page_start = 0

        self.fit_page = 0
        self.fit_width = 0
        self.fit_height = 0

        self.hbreaks = []
        self.vbreaks = []

        self.protect_options = {}
        self.set_cols = {}
        self.set_rows = defaultdict(dict)

        self.zoom = 100
        self.zoom_scale_normal = 1
        self.print_scale = 100
        self.is_right_to_left = 0
        self.show_zeros = 1
        self.leading_zeros = 0

        self.outline_row_level = 0
        self.outline_col_level = 0
        self.outline_style = 0
        self.outline_below = 1
        self.outline_right = 1
        self.outline_on = 1
        self.outline_changed = 0

        self.default_row_height = 15
        self.default_row_zeroed = 0

        self.names = {}
        self.write_match = []
        self.table = defaultdict(dict)
        self.merge = []
        self.row_spans = {}

        self.has_vml = 0
        self.has_comments = 0
        self.comments = defaultdict(dict)
        self.comments_array = []
        self.comments_author = ''
        self.comments_visible = 0
        self.vml_shape_id = 1024
        self.buttons_array = []

        self.autofilter_area = ''
        self.autofilter_ref = None
        self.filter_range = []
        self.filter_on = 0
        self.filter_range = []
        self.filter_cols = {}
        self.filter_type = {}

        self.col_sizes = {}
        self.row_sizes = {}
        self.col_formats = {}
        self.col_size_changed = 0
        self.row_size_changed = 0

        self.last_shape_id = 1
        self.rel_count = 0
        self.hlink_count = 0
        self.hlink_refs = []
        self.external_hyper_links = []
        self.external_drawing_links = []
        self.external_comment_links = []
        self.external_vml_links = []
        self.external_table_links = []
        self.drawing_links = []
        self.charts = []
        self.images = []
        self.tables = []
        self.sparklines = []
        self.shapes = []
        self.shape_hash = {}
        self.drawing = 0

        self.rstring = ''
        self.previous_row = 0

        self.validations = []
        self.cond_formats = {}
        self.dxf_priority = 1
        self.is_chartsheet = 0
        self.page_view = 0

        self.vba_codename = None

        self.date_1904 = False
        self.epoch = datetime.datetime(1899, 12, 31)
        self.hyperlinks = defaultdict(dict)

    @convert_cell_args
    def write(self, row, col, *args):
        """
        Write data to a worksheet cell by calling the appropriate write_*()
        method based on the type of data being passed.

        Args:
            row:     The cell row (zero indexed).
            col:     The cell column (zero indexed).
            token:   Cell data.
            format:  An optional cell Format object.
            options: Any options to pass to sub function.

        Returns:
             0:    Success.
            -1:    Row or column is out of worksheet bounds.
            other: Return value of called method.

        """
        # Check the number of args passed.
        if not len(args):
            raise TypeError("write() takes at least 4 arguments (3 given)")

        # The first arg should be the token for all write calls.
        token = args[0]

        # Convert None to an empty string and thus a blank cell.
        if token is None:
            token = ''

        # Check for a datetime object.
        if self._is_supported_datetime(token):
            return self.write_datetime(row, col, *args)

        # Then check if the token to write is a number.
        try:
            float(token)
            return self.write_number(row, col, *args)
        except ValueError:
            # Not a number. Continue to the checks below.
            pass

        # Map the data to the appropriate write_*() method.
        if token == '':
            return self.write_blank(row, col, *args)
        elif token.startswith('='):
            return self.write_formula(row, col, *args)
        elif token.startswith('{') and token.endswith('}'):
            return self.write_formula(row, col, *args)
        elif re.match('[fh]tt?ps?://', token):
            return self.write_url(row, col, *args)
        elif re.match('mailto:', token):
            return self.write_url(row, col, *args)
        elif re.match('(in|ex)ternal:', token):
            return self.write_url(row, col, *args)
        else:
            return self.write_string(row, col, *args)

    @convert_cell_args
    def write_string(self, row, col, string, cell_format=None):
        """
        Write a string to a worksheet cell.

        Args:
            row:    The cell row (zero indexed).
            col:    The cell column (zero indexed).
            string: Cell data. Str.
            format: An optional cell Format object.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.
            -2: String truncated to 32k characters.

        """
        str_error = 0

        # Check that row and col are valid and store max and min values.
        if self._check_dimensions(row, col):
            return -1

        # Check that the string is < 32767 chars.
        if len(string) > self.xls_strmax:
            string = string[:self.xls_strmax]
            str_error = -2

        # Encode any string options passed by the user.
        string = encode_utf8(string)

        # Write a shared string or an in-line string in optimisation mode.
        if self.optimization == 0:
            string_index = self.str_table._get_shared_string_index(string)
        else:
            string_index = string

        # Write previous row if in in-line string optimization mode.
        if self.optimization and row > self.previous_row:
            self._write_single_row(row)

        # Store the cell data in the worksheet data table.
        self.table[row][col] = cell_string_tuple(string_index, cell_format)

        return str_error

    @convert_cell_args
    def write_number(self, row, col, number, cell_format=None):
        """
        Write a number to a worksheet cell.

        Args:
            row:         The cell row (zero indexed).
            col:         The cell column (zero indexed).
            number:      Cell data. Int or float.
            cell_format: An optional cell Format object.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.

        """
        number = float(number)

        # Check that row and col are valid and store max and min values.
        if self._check_dimensions(row, col):
            return -1

        # Write previous row if in in-line string optimization mode.
        if self.optimization and row > self.previous_row:
            self._write_single_row(row)

        # Store the cell data in the worksheet data table.
        self.table[row][col] = cell_number_tuple(number, cell_format)

        return 0

    @convert_cell_args
    def write_blank(self, row, col, blank, cell_format=None):
        """
        Write a blank cell with formatting to a worksheet cell. The blank
        token is ignored and the format only is written to the cell.

        Args:
            row:         The cell row (zero indexed).
            col:         The cell column (zero indexed).
            blank:       Any value. It is ignored.
            cell_format: An optional cell Format object.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.

        """
        # Don't write a blank cell unless it has a format.
        if cell_format is None:
            return 0

        # Check that row and col are valid and store max and min values.
        if self._check_dimensions(row, col):
            return -1

        # Write previous row if in in-line string optimization mode.
        if self.optimization and row > self.previous_row:
            self._write_single_row(row)

        # Store the cell data in the worksheet data table.
        self.table[row][col] = cell_blank_tuple(cell_format)

        return 0

    @convert_cell_args
    def write_formula(self, row, col, formula, cell_format=None, value=0):
        """
        Write a formula to a worksheet cell.

        Args:
            row:         The cell row (zero indexed).
            col:         The cell column (zero indexed).
            formula:     Cell formula.
            cell_format: An optional cell Format object.
            value:       An optional value for the formula. Default is 0.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.

        """
        # Check that row and col are valid and store max and min values.
        if self._check_dimensions(row, col):
            return -1

        # Hand off array formulas.
        if formula.startswith('{') and formula.endswith('}'):
            return self.write_array_formula(row, col, row, col, formula,
                                            cell_format, value)

        # Remove the formula '=' sign if it exists.
        if formula.startswith('='):
            formula = formula.lstrip('=')

        # Encode any string options passed by the user.
        formula = encode_utf8(formula)

        # Write previous row if in in-line string optimization mode.
        if self.optimization and row > self.previous_row:
            self._write_single_row(row)

        # Store the cell data in the worksheet data table.
        self.table[row][col] = cell_formula_tuple(formula, cell_format, value)

        return 0

    @convert_range_args
    def write_array_formula(self, first_row, first_col, last_row, last_col,
                            formula, cell_format=None, value=0):
        """
        Write a formula to a worksheet cell.

        Args:
            first_row:    The first row of the cell range. (zero indexed).
            first_col:    The first column of the cell range.
            last_row:     The last row of the cell range. (zero indexed).
            last_col:     The last column of the cell range.
            formula:      Cell formula.
            cell_format:  An optional cell Format object.
            value:        An optional value for the formula. Default is 0.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.

        """

        # Swap last row/col with first row/col as necessary.
        if first_row > last_row:
            first_row, last_row = last_row, first_row
        if first_col > last_col:
            first_col, last_col = last_col, first_col

        # Check that row and col are valid and store max and min values
        if self._check_dimensions(last_row, last_col):
            return -1

        # Define array range
        if first_row == last_row and first_col == last_col:
            cell_range = xl_rowcol_to_cell(first_row, first_col)
        else:
            cell_range = (xl_rowcol_to_cell(first_row, first_col) + ':'
                          + xl_rowcol_to_cell(last_row, last_col))

        # Remove array formula braces and the leading =.
        if formula[0] == '{':
            formula = formula[1:]
        if formula[0] == '=':
            formula = formula[1:]
        if formula[-1] == '}':
            formula = formula[:-1]

        # Encode any string options passed by the user.
        formula = encode_utf8(formula)

        # Write previous row if in in-line string optimization mode.
        if self.optimization and first_row > self.previous_row:
            self._write_single_row(first_row)

        # Store the cell data in the worksheet data table.
        self.table[first_row][first_col] = cell_arformula_tuple(formula,
                                                                cell_format,
                                                                value,
                                                                cell_range)

        # Pad out the rest of the area with formatted zeroes.
        if not self.optimization:
            for row in range(first_row, last_row + 1):
                for col in range(first_col, last_col + 1):
                    if row != first_row or col != first_col:
                        self.write_number(row, col, 0, cell_format)

        return 0

    @convert_cell_args
    def write_datetime(self, row, col, date, cell_format):
        """
        Write a date or time to a worksheet cell.

        Args:
            row:         The cell row (zero indexed).
            col:         The cell column (zero indexed).
            date:        Date and/or time as a datetime object.
            cell_format: A cell Format object.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.

        """
        # Check that row and col are valid and store max and min values.
        if self._check_dimensions(row, col):
            return -1

        # Write previous row if in in-line string optimization mode.
        if self.optimization and row > self.previous_row:
            self._write_single_row(row)

        # Convert datetime to an Excel date.
        number = self._convert_date_time(date)

        # Store the cell data in the worksheet data table.
        self.table[row][col] = cell_number_tuple(number, cell_format)

        return 0

    # Write a hyperlink. This is comprised of two elements: the displayed
    # string and the non-displayed link. The displayed string is the same as
    # the link unless an alternative string is specified. The display string
    # is written using the write_string() method. Therefore the max characters
    # string limit applies.
    #
    # The hyperlink can be to a http, ftp, mail, internal sheet, or external
    # directory urls.
    @convert_cell_args
    def write_url(self, row, col, url, cell_format=None,
                  string=None, tip=None):
        """
        Write a hyperlink to a worksheet cell.

        Args:
            row:    The cell row (zero indexed).
            col:    The cell column (zero indexed).
            url:    Hyperlink url.
            format: An optional cell Format object.
            string: An optional display string for the hyperlink.
            tip:    An optional tooltip.
        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.
            -2: String longer than 32767 characters.
            -3: URL longer than Excel limit of 255 characters
            -4: Exceeds Excel limit of 65,530 urls per worksheet
        """
        # Default link type such as http://.
        link_type = 1

        # Remove the URI scheme from internal links.
        if re.match("internal:", url):
            url = url.replace('internal:', '')
            link_type = 2

        # Remove the URI scheme from external links.
        if re.match("external:", url):
            url = url.replace('external:', '')
            link_type = 3

        # Set the displayed string to the URL unless defined by the user.
        if string is None:
            string = url

        # For external links change the directory separator from Unix to Dos.
        if link_type == 3:
            url = url.replace('/', '\\')
            string = string.replace('/', '\\')

        # Strip the mailto header.
        string = string.replace('mailto:', '')

        # Check that row and col are valid and store max and min values
        if self._check_dimensions(row, col):
            return -1

        # Check that the string is < 32767 chars
        str_error = 0
        if len(string) > self.xls_strmax:
            warn("Ignoring URL since it exceeds Excel's string limit of "
                 "32767 characters")
            return -2

        # Copy string for use in hyperlink elements.
        url_str = string

        # External links to URLs and to other Excel workbooks have slightly
        # different characteristics that we have to account for.
        if link_type == 1:
            # Escape URL unless it looks already escaped.
            if not re.search('%[0-9a-fA-F]{2}', url):
                # Can't use url.quote() here because it doesn't match Excel.
                url = url.replace('%', '%25')
                url = url.replace('"', '%22')
                url = url.replace(' ', '%20')
                url = url.replace('<', '%3c')
                url = url.replace('>', '%3e')
                url = url.replace('[', '%5b')
                url = url.replace(']', '%5d')
                url = url.replace('^', '%5e')
                url = url.replace('`', '%60')
                url = url.replace('{', '%7b')
                url = url.replace('}', '%7d')

            # Ordinary URL style external links don't have a "location" string.
            url_str = None

        elif link_type == 3:

            # External Workbook links need to be modified into correct format.
            # The URL will look something like 'c:\temp\file.xlsx#Sheet!A1'.
            # We need the part to the left of the # as the URL and the part to
            # the right as the "location" string (if it exists).
            if re.search('#', url):
                url, url_str = url.split('#')
            else:
                url_str = None

            # Add the file:/// URI to the url if non-local.
            # Windows style "C:/" link. # Network share.
            if (re.match('\w:', url) or re.match(r'\\', url)):
                url = 'file:///' + url

            # Convert a .\dir\file.xlsx link to dir\file.xlsx.
            url = re.sub(r'^\.\\', '', url)

            # Treat as a default external link now the data has been modified.
            link_type = 1

        # Excel limits escaped URL to 255 characters.
        if len(url) > 255:
            warn("Ignoring URL '%s' > 255 characters since it exceeds "
                 "Excel's limit for URLS" % url)
            return -3

        # Check the limit of URLS per worksheet.
        self.hlink_count += 1

        if self.hlink_count > 65530:
            warn("Ignoring URL '%s' since it exceeds Excel's limit of "
                 "65,530 URLS per worksheet." % url)
            return -5

        # Write previous row if in in-line string optimization mode.
        if self.optimization == 1 and row > self.previous_row:
            self._write_single_row(row)

        # Write the hyperlink string.
        self.write_string(row, col, string, cell_format)

        # Encode any string options passed by the user.
        url = encode_utf8(url)
        url_str = encode_utf8(url_str)
        tip = encode_utf8(tip)

        # Store the hyperlink data in a separate structure.
        self.hyperlinks[row][col] = {
            'link_type': link_type,
            'url': url,
            'str': url_str,
            'tip': tip}

        return str_error

    @convert_cell_args
    def write_rich_string(self, row, col, *args):
        """
        Write a "rich" string with multiple formats to a worksheet cell.

        Args:
            row:          The cell row (zero indexed).
            col:          The cell column (zero indexed).
            string_parts: String and format pairs.
            cell_format:  Optional Format object.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.
            -2: String truncated to 32k characters.
            -3: 2 consecutive formats used.

        """
        tokens = list(args)
        cell_format = None
        str_length = 0
        string_index = 0

        # Check that row and col are valid and store max and min values
        if self._check_dimensions(row, col):
            return -1

        # If the last arg is a format we use it as the cell format.
        if isinstance(tokens[-1], Format):
            cell_format = tokens.pop()

        # Create a temp XMLWriter object and use it to write the rich string
        # XML to a string.
        fh = StringIO()
        self.rstring = XMLwriter()
        self.rstring._set_filehandle(fh)

        # Create a temp format with the default font for unformatted fragments.
        default = Format()

        # Convert list of format, string tokens to pairs of (format, string)
        # except for the first string fragment which doesn't require a default
        # formatting run. Use the default for strings without a leading format.
        fragments = []
        previous = 'format'
        pos = 0

        for token in (tokens):
            if not isinstance(token, Format):
                # Token is a string.
                if previous != 'format':
                    # If previous token wasn't a format add one before string.
                    fragments.append(default)
                    fragments.append(encode_utf8(token))
                else:
                    # If previous token was a format just add the string.
                    fragments.append(encode_utf8(token))

                # Keep track of actual string str_length.
                str_length += len(token)
                previous = 'string'
            else:
                # Can't allow 2 formats in a row.
                if previous == 'format' and pos > 0:
                    return -3

                # Token is a format object. Add it to the fragment list.
                fragments.append(token)
                previous = 'format'

            pos += 1

        # If the first token is a string start the <r> element.
        if not isinstance(fragments[0], Format):
            self.rstring._xml_start_tag('r')

        # Write the XML elements for the $format $string fragments.
        for token in (fragments):
            if isinstance(token, Format):
                # Write the font run.
                self.rstring._xml_start_tag('r')
                self._write_font(token)
            else:
                # Write the string fragment part, with whitespace handling.
                attributes = []

                if re.search('^\s', token) or re.search('\s$', token):
                    attributes.append(('xml:space', 'preserve'))

                self.rstring._xml_data_element('t', token, attributes)
                self.rstring._xml_end_tag('r')

        # Read the in-memory string.
        string = self.rstring.fh.getvalue()

        # Check that the string is < 32767 chars.
        if str_length > self.xls_strmax:
            return -2

        # Write a shared string or an in-line string in optimisation mode.
        if self.optimization == 0:
            string_index = self.str_table._get_shared_string_index(string)
        else:
            string_index = string

        # Write previous row if in in-line string optimization mode.
        if self.optimization and row > self.previous_row:
            self._write_single_row(row)

        # Store the cell data in the worksheet data table.
        self.table[row][col] = cell_string_tuple(string_index, cell_format)

        return 0

    @convert_cell_args
    def write_row(self, row, col, data, cell_format=None):
        """
        Write a row of data starting from (row, col).

        Args:
            row:    The cell row (zero indexed).
            col:    The cell column (zero indexed).
            data:   A list of tokens to be written with write().
            format: An optional cell Format object.
        Returns:
            0:  Success.
            other: Return value of write() method.

        """
        for token in (data):
            error = self.write(row, col, token, cell_format)
            if error:
                return error
            col += 1

        return 0

    @convert_cell_args
    def write_column(self, row, col, data, cell_format=None):
        """
        Write a column of data starting from (row, col).

        Args:
            row:    The cell row (zero indexed).
            col:    The cell column (zero indexed).
            data:   A list of tokens to be written with write().
            format: An optional cell Format object.
        Returns:
            0:  Success.
            other: Return value of write() method.

        """
        for token in (data):
            error = self.write(row, col, token, cell_format)
            if error:
                return error
            row += 1

        return 0

    @convert_cell_args
    def insert_image(self, row, col, image, options={}):
        """
        Insert an image with its top-left corner in a worksheet cell.
        Args:
            row:     The cell row (zero indexed).
            col:     The cell column (zero indexed).
            image:   Path and filename for image in PNG, JPG or BMP format.
            options: Position and scale of the image.

        Returns:
            0:  Success.
        """
        x_offset = options.get('x_offset', 0)
        y_offset = options.get('y_offset', 0)
        x_scale = options.get('x_scale', 1)
        y_scale = options.get('y_scale', 1)

        # if not -e image:
        #    croak "Couldn't locate image: $!"

        self.images.append([row, col, image, x_offset, y_offset,
                            x_scale, y_scale])

    @convert_cell_args
    def insert_chart(self, row, col, chart, options={}):
        """
        Insert an chart with its top-left corner in a worksheet cell.
        Args:
            row:     The cell row (zero indexed).
            col:     The cell column (zero indexed).
            chart:   Chart object.
            options: Position and scale of the chart.

        Returns:
            0:  Success.
        """
        x_offset = options.get('x_offset', 0)
        y_offset = options.get('y_offset', 0)
        x_scale = options.get('x_scale', 1)
        y_scale = options.get('y_scale', 1)

        # Allow Chart to override the scale and offset.
        if chart.x_scale != 1:
            x_scale = chart.x_scale

        if chart.y_scale != 1:
            y_scale = chart.y_scale

        if chart.x_offset:
            x_offset = chart.x_offset

        if chart.y_offset:
            x_offset = chart.y_offset

        self.charts.append([row, col, chart,
                            x_offset, y_offset,
                            x_scale, y_scale])

    @convert_cell_args
    def write_comment(self, row, col, comment, options={}):
        """
        Write a comment to a worksheet cell.

        Args:
            row:     The cell row (zero indexed).
            col:     The cell column (zero indexed).
            comment: Cell comment. Str.
            options: Comment formatting options.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.
            -2: String longer than 32k characters.

        """
        # Check that row and col are valid and store max and min values
        if self._check_dimensions(row, col):
            return -1

        # Check that the comment string is < 32767 chars.
        if len(comment) > self.xls_strmax:
            return -2

        self.has_vml = 1
        self.has_comments = 1

        # Encode any string options passed by the user.
        comment = encode_utf8(comment)

        # Process the properties of the cell comment.
        self.comments[row][col] = \
            self._comment_params(row, col, comment, options)

    def show_comments(self):
        """
        Make any comments in the worksheet visible.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.comments_visible = 1

    def set_comments_author(self, author):
        """
        Set the default author of the cell comments.

        Args:
            author: Comment author name. String.

        Returns:
            Nothing.

        """
        self.comments_author = encode_utf8(author)

    def get_name(self):
        """
        Retrieve the worksheet name.

        Args:
            None.

        Returns:
            Nothing.

        """
        # There is no set_name() method. Name must be set in add_worksheet().
        return self.name

    def activate(self):
        """
        Set this worksheet as the active worksheet, i.e. the worksheet that is
        displayed when the workbook is opened. Also set it as selected.

        Note: An active worksheet cannot be hidden.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.hidden = 0
        self.selected = 1
        self.worksheet_meta.activesheet = self.index

    def select(self):
        """
        Set current worksheet as a selected worksheet, i.e. the worksheet
        has its tab highlighted.

        Note: A selected worksheet cannot be hidden.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.selected = 1
        self.hidden = 0

    def hide(self):
        """
        Hide the current worksheet.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.hidden = 1

        # A hidden worksheet shouldn't be active or selected.
        self.selected = 0
        self.activesheet = 0
        self.firstsheet = 0

    def set_first_sheet(self):
        """
        Set current worksheet as the first visible sheet. This is necessary
        when there are a large number of worksheets and the activated
        worksheet is not visible on the screen.

        Note: A selected worksheet cannot be hidden.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.hidden = 0  # Active worksheet can't be hidden.
        self.worksheet_meta.firstsheet = self.index

    @convert_column_args
    def set_column(self, firstcol, lastcol, width=None, cell_format=None,
                   options={}):
        """
        Set the width, and other properties of a single column or a
        range of columns.

        Args:
            firstcol:    First column (zero-indexed).
            lastcol:     Last column (zero-indexed). Can be same as firstcol.
            width:       Column width. (optional).
            cell_format: Column cell_format. (optional).
            options:     Dict of options such as hidden and level.

        Returns:
            0:  Success.
            -1: Column number is out of worksheet bounds.

        """
        # Ensure 2nd col is larger than first.
        if firstcol > lastcol:
            (firstcol, lastcol) = (lastcol, firstcol)

        # Don't modify the row dimensions when checking the columns.
        ignore_row = 1

        # Set optional column values.
        hidden = options.get('hidden', False)
        collapsed = options.get('collapsed', False)
        level = options.get('level', 0)
        # Store the column dimension only in some conditions.
        if cell_format or (width and hidden):
            ignore_col = 0
        else:
            ignore_col = 1

        # Check that each column is valid and store the max and min values.
        if self._check_dimensions(0, lastcol, ignore_row, ignore_col):
            return -1
        if self._check_dimensions(0, firstcol, ignore_row, ignore_col):
            return -1

        # Set the limits for the outline levels (0 <= x <= 7).
        if level < 0:
            level = 0
        if level > 7:
            level = 7

        if level > self.outline_col_level:
            self.outline_col_level = level

        # Store the column data.
        self.colinfo.append([firstcol, lastcol, width, cell_format, hidden,
                             level, collapsed])

        # Store the column change to allow optimisations.
        self.col_size_changed = 1

        # Store the col sizes for use when calculating image vertices taking
        # hidden columns into account. Also store the column formats.

        # Set width to zero if col is hidden
        if hidden:
            width = 0

        for col in range(firstcol, lastcol + 1):
            self.col_sizes[col] = width
            if cell_format:
                self.col_formats[col] = cell_format

        return 0

    def set_row(self, row, height=None, cell_format=None, options={}):
        """
        Set the width, and other properties of a row.
        range of columns.

        Args:
            row:         Row number (zero-indexed).
            height:      Row width. (optional).
            cell_format: Row cell_format. (optional).
            options:     Dict of options such as hidden, level and collapsed.

        Returns:
            0:  Success.
            -1: Row number is out of worksheet bounds.

        """
        # Use minimum col in _check_dimensions().
        if self.dim_colmin is not None:
            min_col = self.dim_colmin
        else:
            min_col = 0

        # Check that row is valid.
        if self._check_dimensions(row, min_col):
            return -1

        if height is None:
            height = self.default_row_height

        # Set optional row values.
        hidden = options.get('hidden', False)
        collapsed = options.get('collapsed', False)
        level = options.get('level', 0)

        # If the height is 0 the row is hidden and the height is the default.
        if height == 0:
            hidden = 1
            height = self.default_row_height

        # Set the limits for the outline levels (0 <= x <= 7).
        if level < 0:
            level = 0
        if level > 7:
            level = 7

        if level > self.outline_row_level:
            self.outline_row_level = level

        # Store the row properties.
        self.set_rows[row] = [height, cell_format, hidden, level, collapsed]

        # Store the row change to allow optimisations.
        self.row_size_changed = 1

        # Store the row sizes for use when calculating image vertices.
        self.row_sizes[row] = height

    def set_default_row(self, height=15, hide_unused_rows=False):
        """
        Set the default row properties.

        Args:
            height:           Default height. Optional, defaults to 15.
            hide_unused_rows: Hide unused rows. Optional, defaults to False.

        Returns:
            Nothing.

        """
        if height != 15:
            # Store the row change to allow optimisations.
            self.row_size_changed = 1
            self.default_row_height = height

        if hide_unused_rows:
            self.default_row_zeroed = 1

    @convert_range_args
    def merge_range(self, first_row, first_col, last_row, last_col,
                    data, cell_format=None):
        """
        Merge a range of cells.

        Args:
            first_row:    The first row of the cell range. (zero indexed).
            first_col:    The first column of the cell range.
            last_row:     The last row of the cell range. (zero indexed).
            last_col:     The last column of the cell range.
            data:         Cell data.
            cell_format:  Cell Format object.

        Returns:
             0:    Success.
            -1:    Row or column is out of worksheet bounds.
            other: Return value of write().

        """
        # Merge a range of cells. The first cell should contain the data and
        # the others should be blank. All cells should have the same format.

        # Excel doesn't allow a single cell to be merged
        if first_row == last_row and first_col == last_col:
            warn("Can't merge single cell")
            return

        # Swap last row/col with first row/col as necessary
        if first_row > last_row:
            (first_row, last_row) = (last_row, first_row)
        if first_col > last_col:
            (first_col, last_col) = (last_col, first_col)

        # Check that column number is valid and store the max value
        if self._check_dimensions(last_row, first_col):
            return

        # Store the merge range.
        self.merge.append([first_row, first_col, last_row, last_col])

        # Write the first cell
        self.write(first_row, first_col, data, cell_format)

        # Pad out the rest of the area with formatted blank cells.
        for row in range(first_row, last_row + 1):
            for col in range(first_col, last_col + 1):
                if row == first_row and col == first_col:
                    continue
                self.write_blank(row, col, '', cell_format)

    @convert_range_args
    def autofilter(self, first_row, first_col, last_row, last_col):
        """
        Set the autofilter area in the worksheet.

        Args:
            first_row:    The first row of the cell range. (zero indexed).
            first_col:    The first column of the cell range.
            last_row:     The last row of the cell range. (zero indexed).
            last_col:     The last column of the cell range.

        Returns:
             Nothing.

        """
        # Reverse max and min values if necessary.
        if last_row < first_row:
            (first_row, last_row) = (last_row, first_row)
        if last_col < first_col:
            (first_col, last_col) = (last_col, first_col)

        # Build up the print area range "Sheet1!$A$1:$C$13".
        area = self._convert_name_area(first_row, first_col,
                                       last_row, last_col)
        ref = xl_range(first_row, first_col, last_row, last_col)

        self.autofilter_area = area
        self.autofilter_ref = ref
        self.filter_range = [first_col, last_col]

    def filter_column(self, col, criteria):
        """
        Set the column filter criteria.

        Args:
            col:       Filter column (zero-indexed).
            criteria:  Filter criteria.

        Returns:
             Nothing.

        """
        if not self.autofilter_area:
            warn("Must call autofilter() before filter_column()")
            return

        # Check for a column reference in A1 notation and substitute.
        try:
            int(col)
        except ValueError:
            # Convert col ref to a cell ref and then to a col number.
            col_letter = col
            (_, col) = xl_cell_to_rowcol(col + '1')

            if col >= self.xls_colmax:
                warn("Invalid column '%s'" % col_letter)
                return

        (col_first, col_last) = self.filter_range

        # Reject column if it is outside filter range.
        if col < col_first or col > col_last:
            warn("Column '%d' outside autofilter() column range (%d, %d)"
                 % (col, col_first, col_last))
            return

        tokens = self._extract_filter_tokens(criteria)

        if not (len(tokens) == 3 or len(tokens) == 7):
            warn("Incorrect number of tokens in criteria '%s'" % criteria)

        tokens = self._parse_filter_expression(criteria, tokens)

        # Excel handles single or double custom filters as default filters.
        #  We need to check for them and handle them accordingly.
        if len(tokens) == 2 and tokens[0] == 2:
            # Single equality.
            self.filter_column_list(col, [tokens[1]])
        elif (len(tokens) == 5 and tokens[0] == 2 and tokens[2] == 1
              and tokens[3] == 2):
            # Double equality with "or" operator.
            self.filter_column_list(col, [tokens[1], tokens[4]])
        else:
            # Non default custom filter.
            self.filter_cols[col] = tokens
            self.filter_type[col] = 0

        self.filter_on = 1

    def filter_column_list(self, col, filters):
        """
        Set the column filter criteria in Excel 2007 list style.

        Args:
            col:      Filter column (zero-indexed).
            filters:  List of filter criteria to match.

        Returns:
             Nothing.

        """
        if not self.autofilter_area:
            warn("Must call autofilter() before filter_column()")
            return

        # Check for a column reference in A1 notation and substitute.
        try:
            int(col)
        except ValueError:
            # Convert col ref to a cell ref and then to a col number.
            col_letter = col
            (_, col) = xl_cell_to_rowcol(col + '1')

            if col >= self.xls_colmax:
                warn("Invalid column '%s'" % col_letter)
                return

        (col_first, col_last) = self.filter_range

        # Reject column if it is outside filter range.
        if col < col_first or col > col_last:
            warn("Column '%d' outside autofilter() column range "
                 "(%d,%d)" % (col, col_first, col_last))
            return

        self.filter_cols[col] = filters
        self.filter_type[col] = 1
        self.filter_on = 1

    @convert_range_args
    def data_validation(self, first_row, first_col, last_row, last_col,
                        options):
        """
        Add a data validation to a worksheet.

        Args:
            first_row:    The first row of the cell range. (zero indexed).
            first_col:    The first column of the cell range.
            last_row:     The last row of the cell range. (zero indexed).
            last_col:     The last column of the cell range.
            options:      Data validation options.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.
            -2: Incorrect parameter or option.
        """
        # Check that row and col are valid without storing the values.
        if self._check_dimensions(first_row, first_col, 1, 1):
            return -1
        if self._check_dimensions(last_row, last_col, 1, 1):
            return -1

        # List of valid input parameters.
        valid_parameters = {
            'validate': 1,
            'criteria': 1,
            'value': 1,
            'source': 1,
            'minimum': 1,
            'maximum': 1,
            'ignore_blank': 1,
            'dropdown': 1,
            'show_input': 1,
            'input_title': 1,
            'input_message': 1,
            'show_error': 1,
            'error_title': 1,
            'error_message': 1,
            'error_type': 1,
            'other_cells': 1,
        }

        # Check for valid input parameters.
        for param_key in options.keys():
            if not param_key in valid_parameters:
                warn("Unknown parameter 'param_key' in data_validation()")
                return -2

        # Map alternative parameter names 'source' or 'minimum' to 'value'.
        if 'source' in options:
            options['value'] = options['source']
        if 'minimum' in options:
            options['value'] = options['minimum']

        # 'validate' is a required parameter.
        if not 'validate' in options:
            warn("Parameter 'validate' is required in data_validation()")
            return -2

        # List of  valid validation types.
        valid_types = {
            'any': 'none',
            'any value': 'none',
            'whole number': 'whole',
            'whole': 'whole',
            'integer': 'whole',
            'decimal': 'decimal',
            'list': 'list',
            'date': 'date',
            'time': 'time',
            'text length': 'textLength',
            'length': 'textLength',
            'custom': 'custom',
        }

        # Check for valid validation types.
        if not options['validate'] in valid_types:
            warn("Unknown validation type '%s' for parameter "
                 "'validate' in data_validation()" % options['validate'])
            return -2
        else:
            options['validate'] = valid_types[options['validate']]

        # No action is required for validation type 'any'.
        if options['validate'] == 'none':
            return -2

        # The list and custom validations don't have a criteria so we use
        # a default of 'between'.
        if options['validate'] == 'list' or options['validate'] == 'custom':
            options['criteria'] = 'between'
            options['maximum'] = None

        # 'criteria' is a required parameter.
        if not 'criteria' in options:
            warn("Parameter 'criteria' is required in data_validation()")
            return -2

        # List of valid criteria types.
        criteria_types = {
            'between': 'between',
            'not between': 'notBetween',
            'equal to': 'equal',
            '=': 'equal',
            '==': 'equal',
            'not equal to': 'notEqual',
            '!=': 'notEqual',
            '<>': 'notEqual',
            'greater than': 'greaterThan',
            '>': 'greaterThan',
            'less than': 'lessThan',
            '<': 'lessThan',
            'greater than or equal to': 'greaterThanOrEqual',
            '>=': 'greaterThanOrEqual',
            'less than or equal to': 'lessThanOrEqual',
            '<=': 'lessThanOrEqual',
        }

        # Check for valid criteria types.
        if not options['criteria'] in criteria_types:
            warn("Unknown criteria type '%s' for parameter "
                 "'criteria' in data_validation()" % options['criteria'])
            return -2
        else:
            options['criteria'] = criteria_types[options['criteria']]

        # 'Between' and 'Not between' criteria require 2 values.
        if (options['criteria'] == 'between' or
                options['criteria'] == 'notBetween'):
            if not 'maximum' in options:
                warn("Parameter 'maximum' is required in data_validation() "
                     "when using 'between' or 'not between' criteria")
                return -2
        else:
            options['maximum'] = None

        # List of valid error dialog types.
        error_types = {
            'stop': 0,
            'warning': 1,
            'information': 2,
        }

        # Check for valid error dialog types.
        if not 'error_type' in options:
            options['error_type'] = 0
        elif not options['error_type'] in error_types:
            warn("Unknown criteria type '%s' for parameter 'error_type' "
                 "in data_validation()" % options['error_type'])
            return -2
        else:
            options['error_type'] = error_types[options['error_type']]

        # Convert date/times value if required.
        if options['validate'] == 'date' or options['validate'] == 'time':

            if options['value']:
                if not self._is_supported_datetime(options['value']):
                    warn("Data validation 'value/minimum' must be a "
                         "datetime object.")
                    return -2
                else:
                    date_time = self._convert_date_time(options['value'])
                    # Format date number to the same precision as Excel.
                    options['value'] = "%.15g" % date_time

            if options['maximum']:
                if not self._is_supported_datetime(options['maximum']):
                    warn("Conditional format 'maximum' must be a "
                         "datetime object.")
                    return -2
                else:
                    date_time = self._convert_date_time(options['maximum'])
                    options['maximum'] = "%.15g" % date_time

        # Set some defaults if they haven't been defined by the user.
        if not 'ignore_blank' in options:
            options['ignore_blank'] = 1
        if not 'dropdown' in options:
            options['dropdown'] = 1
        if not 'show_input' in options:
            options['show_input'] = 1
        if not 'show_error' in options:
            options['show_error'] = 1

        # These are the cells to which the validation is applied.
        options['cells'] = [[first_row, first_col, last_row, last_col]]

        # A (for now) undocumented parameter to pass additional cell ranges.
        if 'other_cells' in options:
            options['cells'].extend(options['other_cells'])

        # Store the validation information until we close the worksheet.
        self.validations.append(options)

    @convert_range_args
    def conditional_format(self, first_row, first_col, last_row, last_col,
                           options=None):
        """
        Add a conditional format to a worksheet.

        Args:
            first_row:    The first row of the cell range. (zero indexed).
            first_col:    The first column of the cell range.
            last_row:     The last row of the cell range. (zero indexed).
            last_col:     The last column of the cell range.
            options:      Conditional format options.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.
            -2: Incorrect parameter or option.
        """
        # Check that row and col are valid without storing the values.
        if self._check_dimensions(first_row, first_col, 1, 1):
            return -1
        if self._check_dimensions(last_row, last_col, 1, 1):
            return -1

        if options is None:
            options = {}

        # List of valid input parameters.
        valid_parameter = {
            'type': 1,
            'format': 1,
            'criteria': 1,
            'value': 1,
            'minimum': 1,
            'maximum': 1,
            'min_type': 1,
            'mid_type': 1,
            'max_type': 1,
            'min_value': 1,
            'mid_value': 1,
            'max_value': 1,
            'min_color': 1,
            'mid_color': 1,
            'max_color': 1,
            'multi_range': 1,
            'bar_color': 1}

        # Check for valid input parameters.
        for param_key in options.keys():
            if param_key not in valid_parameter:
                warn("Unknown parameter '%s' in conditional_formatting()" %
                     param_key)
                return -2

        # 'type' is a required parameter.
        if 'type' not in options:
            warn("Parameter 'type' is required in conditional_formatting()")
            return -2

        # List of  valid validation types.
        valid_type = {
            'cell': 'cellIs',
            'date': 'date',
            'time': 'time',
            'average': 'aboveAverage',
            'duplicate': 'duplicateValues',
            'unique': 'uniqueValues',
            'top': 'top10',
            'bottom': 'top10',
            'text': 'text',
            'time_period': 'timePeriod',
            'blanks': 'containsBlanks',
            'no_blanks': 'notContainsBlanks',
            'errors': 'containsErrors',
            'no_errors': 'notContainsErrors',
            '2_color_scale': '2_color_scale',
            '3_color_scale': '3_color_scale',
            'data_bar': 'dataBar',
            'formula': 'expression'}

        # Check for valid validation types.
        if options['type'] not in valid_type:
            warn("Unknown validation type '%s' for parameter 'type' "
                 "in conditional_formatting()" % options['type'])
            return -2
        else:
            if options['type'] == 'bottom':
                options['direction'] = 'bottom'
            options['type'] = valid_type[options['type']]

        # List of valid criteria types.
        criteria_type = {
            'between': 'between',
            'not between': 'notBetween',
            'equal to': 'equal',
            '=': 'equal',
            '==': 'equal',
            'not equal to': 'notEqual',
            '!=': 'notEqual',
            '<>': 'notEqual',
            'greater than': 'greaterThan',
            '>': 'greaterThan',
            'less than': 'lessThan',
            '<': 'lessThan',
            'greater than or equal to': 'greaterThanOrEqual',
            '>=': 'greaterThanOrEqual',
            'less than or equal to': 'lessThanOrEqual',
            '<=': 'lessThanOrEqual',
            'containing': 'containsText',
            'not containing': 'notContains',
            'begins with': 'beginsWith',
            'ends with': 'endsWith',
            'yesterday': 'yesterday',
            'today': 'today',
            'last 7 days': 'last7Days',
            'last week': 'lastWeek',
            'this week': 'thisWeek',
            'continue week': 'continueWeek',
            'last month': 'lastMonth',
            'this month': 'thisMonth',
            'continue month': 'continueMonth'}

        # Check for valid criteria types.
        if ('criteria' in options and options['criteria'] in criteria_type):
            options['criteria'] = criteria_type[options['criteria']]

        # Convert date/times value if required.
        if options['type'] == 'date' or options['type'] == 'time':
            options['type'] = 'cellIs'

            if 'value' in options:
                if not self._is_supported_datetime(options['value']):
                    warn("Conditional format 'value' must be a "
                         "datetime object.")
                    return -2
                else:
                    date_time = self._convert_date_time(options['value'])
                    # Format date number to the same precision as Excel.
                    options['value'] = "%.15g" % date_time

            if 'minimum' in options:
                if not self._is_supported_datetime(options['minimum']):
                    warn("Conditional format 'minimum' must be a "
                         "datetime object.")
                    return -2
                else:
                    date_time = self._convert_date_time(options['minimum'])
                    options['minimum'] = "%.15g" % date_time

            if 'maximum' in options:
                if not self._is_supported_datetime(options['maximum']):
                    warn("Conditional format 'maximum' must be a "
                         "datetime object.")
                    return -2
                else:
                    date_time = self._convert_date_time(options['maximum'])
                    options['maximum'] = "%.15g" % date_time

        # Swap last row/col for first row/col as necessary
        if first_row > last_row:
            first_row, last_row = last_row, first_row

        if first_col > last_col:
            first_col, last_col = last_col, first_col

        # Set the formatting range.
        # If the first and last cell are the same write a single cell.
        if first_row == last_row and first_col == last_col:
            cell_range = xl_rowcol_to_cell(first_row, first_col)
            start_cell = cell_range
        else:
            cell_range = xl_range(first_row, first_col, last_row, last_col)
            start_cell = xl_rowcol_to_cell(first_row, first_col)

        # Override with user defined multiple range if provided.
        if 'multi_range' in options:
            cell_range = options['multi_range']
            cell_range = cell_range.replace('$', '')

        # Get the dxf format index.
        if 'format' in options and options['format']:
            options['format'] = options['format']._get_dxf_index()

        # Set the priority based on the order of adding.
        options['priority'] = self.dxf_priority
        self.dxf_priority += 1

        # Special handling of text criteria.
        if options['type'] == 'text':

            if options['criteria'] == 'containsText':
                options['type'] = 'containsText'
                options['formula'] = ('NOT(ISERROR(SEARCH("%s",%s)))'
                                      % (options['value'], start_cell))
            elif options['criteria'] == 'notContains':
                options['type'] = 'notContainsText'
                options['formula'] = ('ISERROR(SEARCH("%s",%s))'
                                      % (options['value'], start_cell))
            elif options['criteria'] == 'beginsWith':
                options['type'] = 'beginsWith'
                options['formula'] = ('LEFT(%s,%d)="%s"'
                                      % (start_cell,
                                         len(options['value']),
                                         options['value']))
            elif options['criteria'] == 'endsWith':
                options['type'] = 'endsWith'
                options['formula'] = ('RIGHT(%s,%d)="%s"'
                                      % (start_cell,
                                         len(options['value']),
                                         options['value']))
            else:
                warn("Invalid text criteria 'options['criteria']' "
                     "in conditional_formatting()")

        # Special handling of time time_period criteria.
        if options['type'] == 'timePeriod':

            if options['criteria'] == 'yesterday':
                options['formula'] = 'FLOOR(%s,1)=TODAY()-1' % start_cell

            elif options['criteria'] == 'today':
                options['formula'] = 'FLOOR(%s,1)=TODAY()' % start_cell

            elif options['criteria'] == 'tomorrow':
                options['formula'] = 'FLOOR(%s,1)=TODAY()+1' % start_cell

            elif options['criteria'] == 'last7Days':
                options['formula'] = \
                    ('AND(TODAY()-FLOOR(%s,1)<=6,FLOOR(%s,1)<=TODAY())' %
                    (start_cell, start_cell))

            elif options['criteria'] == 'lastWeek':
                options['formula'] = \
                    ('AND(TODAY()-ROUNDDOWN(%s,0)>=(WEEKDAY(TODAY())),'
                     'TODAY()-ROUNDDOWN(%s,0)<(WEEKDAY(TODAY())+7))' %
                     (start_cell, start_cell))

            elif options['criteria'] == 'thisWeek':
                options['formula'] = \
                    ('AND(TODAY()-ROUNDDOWN(%s,0)<=WEEKDAY(TODAY())-1,'
                     'ROUNDDOWN(%s,0)-TODAY()<=7-WEEKDAY(TODAY()))' %
                     (start_cell, start_cell))

            elif options['criteria'] == 'continueWeek':
                options['formula'] = \
                    ('AND(ROUNDDOWN(%s,0)-TODAY()>(7-WEEKDAY(TODAY())),'
                     'ROUNDDOWN(%s,0)-TODAY()<(15-WEEKDAY(TODAY())))' %
                     (start_cell, start_cell))

            elif options['criteria'] == 'lastMonth':
                options['formula'] = \
                    ('AND(MONTH(%s)=MONTH(TODAY())-1,OR(YEAR(%s)=YEAR('
                     'TODAY()),AND(MONTH(%s)=1,YEAR(A1)=YEAR(TODAY())-1)))' %
                     (start_cell, start_cell, start_cell))

            elif options['criteria'] == 'thisMonth':
                options['formula'] = \
                    ('AND(MONTH(%s)=MONTH(TODAY()),YEAR(%s)=YEAR(TODAY()))' %
                    (start_cell, start_cell))

            elif options['criteria'] == 'continueMonth':
                options['formula'] = \
                    ('AND(MONTH(%s)=MONTH(TODAY())+1,OR(YEAR(%s)=YEAR('
                     'TODAY()),AND(MONTH(%s)=12,YEAR(%s)=YEAR(TODAY())+1)))' %
                     (start_cell, start_cell, start_cell, start_cell))

            else:
                warn("Invalid time_period criteria 'options['criteria']' "
                     "in conditional_formatting()")

        # Special handling of blanks/error types.
        if options['type'] == 'containsBlanks':
            options['formula'] = 'LEN(TRIM(%s))=0' % start_cell

        if options['type'] == 'notContainsBlanks':
            options['formula'] = 'LEN(TRIM(%s))>0' % start_cell

        if options['type'] == 'containsErrors':
            options['formula'] = 'ISERROR(%s)' % start_cell

        if options['type'] == 'notContainsErrors':
            options['formula'] = 'NOT(ISERROR(%s))' % start_cell

        # Special handling for 2 color scale.
        if options['type'] == '2_color_scale':
            options['type'] = 'colorScale'

            # Color scales don't use any additional formatting.
            options['format'] = None

            # Turn off 3 color parameters.
            options['mid_type'] = None
            options['mid_color'] = None

            options.setdefault('min_type', 'min')
            options.setdefault('max_type', 'max')
            options.setdefault('min_value', 0)
            options.setdefault('max_value', 0)
            options.setdefault('min_color', '#FF7128')
            options.setdefault('max_color', '#FFEF9C')

            options['min_color'] = xl_color(options['min_color'])
            options['max_color'] = xl_color(options['max_color'])

        # Special handling for 3 color scale.
        if options['type'] == '3_color_scale':
            options['type'] = 'colorScale'

            # Color scales don't use any additional formatting.
            options['format'] = None

            options.setdefault('min_type', 'min')
            options.setdefault('mid_type', 'percentile')
            options.setdefault('max_type', 'max')
            options.setdefault('min_value', 0)
            options.setdefault('max_value', 0)
            options.setdefault('min_color', '#F8696B')
            options.setdefault('mid_color', '#FFEB84')
            options.setdefault('max_color', '#63BE7B')

            options['min_color'] = xl_color(options['min_color'])
            options['mid_color'] = xl_color(options['mid_color'])
            options['max_color'] = xl_color(options['max_color'])

            # Set a default mid value.
            if not 'mid_value' in options:
                options['mid_value'] = 50

        # Special handling for data bar.
        if options['type'] == 'dataBar':

            # Color scales don't use any additional formatting.
            options['format'] = None

            options.setdefault('min_type', 'min')
            options.setdefault('max_type', 'max')
            options.setdefault('min_value', 0)
            options.setdefault('max_value', 0)
            options.setdefault('bar_color', '#638EC6')

            options['bar_color'] = xl_color(options['bar_color'])

        # Store the validation information until we close the worksheet.
        if cell_range in self.cond_formats:
            self.cond_formats[cell_range].append(options)
        else:
            self.cond_formats[cell_range] = [options]

    @convert_range_args
    def add_table(self, first_row, first_col, last_row, last_col,
                  options=None):
        """
        Add an Excel table to a worksheet.

        Args:
            first_row:    The first row of the cell range. (zero indexed).
            first_col:    The first column of the cell range.
            last_row:     The last row of the cell range. (zero indexed).
            last_col:     The last column of the cell range.
            options:      Table format options. (Optional)

        Returns:
            0:  Success.
            -1: Not supported in optimisation mode.
            -2: Row or column is out of worksheet bounds.
            -3: Incorrect parameter or option.
        """
        table = {}
        col_formats = {}

        if options is None:
            options = {}

        if self.optimization == 1:
            warn("add_table() isn't supported when set_optimization() is on")
            return -1

        # Check that row and col are valid without storing the values.
        if self._check_dimensions(first_row, first_col, 1, 1):
            return -2
        if self._check_dimensions(last_row, last_col, 1, 1):
            return -2

        # List of valid input parameters.
        valid_parameter = {
            'autofilter': 1,
            'banded_columns': 1,
            'banded_rows': 1,
            'columns': 1,
            'data': 1,
            'first_column': 1,
            'header_row': 1,
            'last_column': 1,
            'name': 1,
            'style': 1,
            'total_row': 1,
        }

        # Check for valid input parameters.
        for param_key in options.keys():
            if not param_key in valid_parameter:
                warn("Unknown parameter '%s' in add_table()" % param_key)
                return -3

        # Table count is a member of Workbook, global to all Worksheet.
        self.worksheet_meta.table_count += 1
        table['id'] = self.worksheet_meta.table_count

        # Turn on Excel's defaults.
        options['banded_rows'] = options.get('banded_rows', True)
        options['header_row'] = options.get('header_row', True)
        options['autofilter'] = options.get('autofilter', True)

        # Set the table options.
        table['show_first_col'] = options.get('first_column', False)
        table['show_last_col'] = options.get('last_column', False)
        table['show_row_stripes'] = options.get('banded_rows', False)
        table['show_col_stripes'] = options.get('banded_columns', False)
        table['header_row_count'] = options.get('header_row', 0)
        table['totals_row_shown'] = options.get('total_row', False)

        # Set the table name.
        if 'name' in options:
            table['name'] = options['name']
        else:
            # Set a default name.
            table['name'] = 'Table' + str(table['id'])

        # Set the table style.
        if 'style' in options:
            table['style'] = options['style']
            # Remove whitespace from style name.
            table['style'] = table['style'].replace(' ', '')
        else:
            table['style'] = "TableStyleMedium9"

        # Swap last row/col for first row/col as necessary.
        if first_row > last_row:
            (first_row, last_row) = (last_row, first_row)
        if first_col > last_col:
            (first_col, last_col) = (last_col, first_col)

        # Set the data range rows (without the header and footer).
        first_data_row = first_row
        last_data_row = last_row

        if 'header_row' in options:
            first_data_row += 1

        if 'total_row' in options:
            last_data_row -= 1

        # Set the table and autofilter ranges.
        table['range'] = xl_range(first_row, first_col,
                                  last_row, last_col)

        table['a_range'] = xl_range(first_row, first_col,
                                    last_data_row, last_col)

        # If the header row if off the default is to turn autofilter off.
        if not options['header_row']:
            options['autofilter'] = 0

        # Set the autofilter range.
        if options['autofilter']:
            table['autofilter'] = table['a_range']

        # Add the table columns.
        col_id = 1
        table['columns'] = []

        for col_num in range(first_col, last_col + 1):
            # Set up the default column data.
            col_data = {
                'id': col_id,
                'name': 'Column' + str(col_id),
                'total_string': '',
                'total_function': '',
                'formula': '',
                'format': None,
            }

            # Overwrite the defaults with any use defined values.
            if 'columns' in options:
                # Check if there are user defined values for this column.
                user_data = options['columns'][col_id - 1]

                if user_data:
                    # Get the column format.
                    xformat = user_data.get('format', None)

                    # Map user defined values to internal values.
                    if user_data.get('header'):
                        col_data['name'] = user_data['header']

                    # Handle the column formula.
                    if 'formula' in user_data and user_data['formula']:
                        formula = user_data['formula']

                        # Remove the formula '=' sign if it exists.
                        if formula.startswith('='):
                            formula = formula.lstrip('=')

                        # Covert Excel 2010 "@" ref to 2007 "#This Row".
                        formula = formula.replace('@', '[#This Row],')

                        col_data['formula'] = formula

                        for row in range(first_data_row, last_data_row + 1):
                            self.write_formula(row, col_num, formula, xformat)

                    # Handle the function for the total row.
                    if user_data.get('total_function'):
                        function = user_data['total_function']

                        # Massage the function name.
                        function = function.lower()
                        function = function.replace('_', '')
                        function = function.replace(' ', '')

                        if function == 'countnums':
                            function = 'countNums'
                        if function == 'stddev':
                            function = 'stdDev'

                        col_data['total_function'] = function

                        formula = \
                            self._table_function_to_formula(function,
                                                            col_data['name'])

                        self.write_formula(last_row, col_num, formula, xformat)

                    elif user_data.get('total_string'):
                        # Total label only (not a function).
                        total_string = user_data['total_string']
                        col_data['total_string'] = total_string

                        self.write_string(last_row, col_num, total_string,
                                          user_data.get('format'))

                    # Get the dxf format index.
                    if xformat is not None:
                        col_data['format'] = xformat._get_dxf_index()

                    # Store the column format for writing the cell data.
                    # It doesn't matter if it is undefined.
                    col_formats[col_id - 1] = xformat

            # Store the column data.
            table['columns'].append(col_data)

            # Write the column headers to the worksheet.
            if options['header_row']:
                self.write_string(first_row, col_num, col_data['name'])

            col_id += 1

        # Write the cell data if supplied.
        if 'data' in options:
            data = options['data']

            i = 0  # For indexing the row data.
            for row in range(first_data_row, last_data_row + 1):
                j = 0  # For indexing the col data.
                for col in range(first_col, last_col + 1):
                    if i < len(data) and j < len(data[i]):
                        token = data[i][j]
                        if j in col_formats:
                            self.write(row, col, token, col_formats[j])
                        else:
                            self.write(row, col, token, None)
                    j += 1
                i += 1

        # Store the table data.
        self.tables.append(table)

        # Store the link used for the rels file.
        self.external_table_links.append(['/table',
                                          '../tables/table'
                                          + str(table['id'])
                                          + '.xml'])

        return table

    @convert_cell_args
    def add_sparkline(self, row, col, options):
        """
        Add sparklines to the worksheet.

        Args:
            row:     The cell row (zero indexed).
            col:     The cell column (zero indexed).
            options: Sparkline formatting options.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.
            -2: Incorrect parameter or option.

        """

        # Check that row and col are valid without storing the values.
        if self._check_dimensions(row, col, 1, 1):
            return -1

        sparkline = {}
        sparkline['locations'] = [xl_rowcol_to_cell(row, col)]

        # List of valid input parameters.
        valid_parameters = {
            'location': True,
            'range': True,
            'type': True,
            'high_point': True,
            'low_point': True,
            'negative_points': True,
            'first_point': True,
            'last_point': True,
            'markers': True,
            'style': True,
            'series_color': True,
            'negative_color': True,
            'markers_color': True,
            'first_color': True,
            'last_color': True,
            'high_color': True,
            'low_color': True,
            'max': True,
            'min': True,
            'axis': True,
            'reverse': True,
            'empty_cells': True,
            'show_hidden': True,
            'plot_hidden': True,
            'date_axis': True,
            'weight': True,
        }

        # Check for valid input parameters.
        for param_key in options.keys():
            if not param_key in valid_parameters:
                warn("Unknown parameter '%s' in add_sparkline()" % param_key)
                return -1

        # 'range' is a required parameter.
        if not 'range' in options:
            warn("Parameter 'range' is required in add_sparkline()")
            return -2

        # Handle the sparkline type.
        spark_type = options.get('type', 'line')

        if spark_type not in ('line', 'column', 'win_loss'):
            warn("Parameter 'type' must be 'line', 'column' "
                 "or 'win_loss' in add_sparkline()")
            return -2

        if spark_type == 'win_loss':
            spark_type = 'stacked'
        sparkline['type'] = spark_type

        # We handle single location/range values or list of values.
        if 'location' in options:
            if type(options['location']) is list:
                sparkline['locations'] = options['location']
            else:
                sparkline['locations'] = [options['location']]

        if type(options['range']) is list:
            sparkline['ranges'] = options['range']
        else:
            sparkline['ranges'] = [options['range']]

        range_count = len(sparkline['ranges'])
        location_count = len(sparkline['locations'])

        # The ranges and locations must match.
        if range_count != location_count:
            warn("Must have the same number of location and range "
                 "parameters in add_sparkline()")
            return -2

        # Store the count.
        sparkline['count'] = len(sparkline['locations'])

        # Get the worksheet name for the range conversion below.
        sheetname = self._quote_sheetname(self.name)

        # Cleanup the input ranges.
        new_ranges = []
        for spark_range in sparkline['ranges']:

            # Remove the absolute reference $ symbols.
            spark_range = spark_range.replace('$', '')

            # Remove the = from formula.
            spark_range = spark_range.lstrip('=')

            # Convert a simple range into a full Sheet1!A1:D1 range.
            if '!' not in spark_range:
                spark_range = sheetname + "!" + spark_range

            new_ranges.append(spark_range)

        sparkline['ranges'] = new_ranges

        # Cleanup the input locations.
        new_locations = []
        for location in sparkline['locations']:
            location = location.replace('$', '')
            new_locations.append(location)

        sparkline['locations'] = new_locations

        # Map options.
        sparkline['high'] = options.get('high_point')
        sparkline['low'] = options.get('low_point')
        sparkline['negative'] = options.get('negative_points')
        sparkline['first'] = options.get('first_point')
        sparkline['last'] = options.get('last_point')
        sparkline['markers'] = options.get('markers')
        sparkline['min'] = options.get('min')
        sparkline['max'] = options.get('max')
        sparkline['axis'] = options.get('axis')
        sparkline['reverse'] = options.get('reverse')
        sparkline['hidden'] = options.get('show_hidden')
        sparkline['weight'] = options.get('weight')

        # Map empty cells options.
        empty = options.get('empty_cells', '')

        if empty == 'zero':
            sparkline['empty'] = 0
        elif empty == 'connect':
            sparkline['empty'] = 'span'
        else:
            sparkline['empty'] = 'gap'

        # Map the date axis range.
        date_range = options.get('date_axis')

        if date_range and '!' not in date_range:
            date_range = sheetname + "!" + date_range

        sparkline['date_axis'] = date_range

        # Set the sparkline styles.
        style_id = options.get('style', 0)
        style = get_sparkline_style(style_id)

        sparkline['series_color'] = style['series']
        sparkline['negative_color'] = style['negative']
        sparkline['markers_color'] = style['markers']
        sparkline['first_color'] = style['first']
        sparkline['last_color'] = style['last']
        sparkline['high_color'] = style['high']
        sparkline['low_color'] = style['low']

        # Override the style colours with user defined colours.
        self._set_spark_color(sparkline, options, 'series_color')
        self._set_spark_color(sparkline, options, 'negative_color')
        self._set_spark_color(sparkline, options, 'markers_color')
        self._set_spark_color(sparkline, options, 'first_color')
        self._set_spark_color(sparkline, options, 'last_color')
        self._set_spark_color(sparkline, options, 'high_color')
        self._set_spark_color(sparkline, options, 'low_color')

        self.sparklines.append(sparkline)

    @convert_range_args
    def set_selection(self, first_row, first_col, last_row, last_col):
        """
        Set the selected cell or cells in a worksheet

        Args:
            first_row:    The first row of the cell range. (zero indexed).
            first_col:    The first column of the cell range.
            last_row:     The last row of the cell range. (zero indexed).
            last_col:     The last column of the cell range.

        Returns:
            0:  Nothing.
        """
        pane = None

        # Range selection. Do this before swapping max/min to allow the
        # selection direction to be reversed.
        active_cell = xl_rowcol_to_cell(first_row, first_col)

        # Swap last row/col for first row/col if necessary
        if first_row > last_row:
            (first_row, last_row) = (last_row, first_row)

        if first_col > last_col:
            (first_col, last_col) = (last_col, first_col)

        # If the first and last cell are the same write a single cell.
        if (first_row == last_row) and (first_col == last_col):
            sqref = active_cell
        else:
            sqref = xl_range(first_row, first_col, last_row, last_col)

        # Selection isn't set for cell A1.
        if sqref == 'A1':
            return

        self.selections = [[pane, active_cell, sqref]]

    def outline_settings(self, outline_on=1, outline_below=1, outline_right=1,
                         outline_style=0):
        """
        Control outline settings.

        Args:
            outline_on:    Outlines are visible. Optional, defaults to True.
            outline_below: Show row outline symbols below the outline bar.
                           Optional, defaults to True.
            outline_right: Show column outline symbols to the right of the
                           outline bar. Optional, defaults to True.
            outline_style: Use Automatic style. Optional, defaults to False.

        Returns:
            0:  Nothing.
        """
        self.outline_on = outline_on
        self.outline_below = outline_below
        self.outline_right = outline_right
        self.outline_style = outline_style

        self.outline_changed = 1

    @convert_cell_args
    def freeze_panes(self, row, col, top_row=None, left_col=None, pane_type=0):
        """
        Create worksheet panes and mark them as frozen.

        Args:
            row:      The cell row (zero indexed).
            col:      The cell column (zero indexed).
            top_row:  Topmost visible row in scrolling region of pane.
            left_col: Leftmost visible row in scrolling region of pane.

        Returns:
            0:  Nothing.

        """
        if top_row is None:
            top_row = row

        if left_col is None:
            left_col = col

        self.panes = [row, col, top_row, left_col, pane_type]

    @convert_cell_args
    def split_panes(self, x, y, top_row=None, left_col=None):
        """
        Create worksheet panes and mark them as split.

        Args:
            x:        The position for the vertical split.
            y:        The position for the horizontal split.
            top_row:  Topmost visible row in scrolling region of pane.
            left_col: Leftmost visible row in scrolling region of pane.

        Returns:
            0:  Nothing.

        """
        # Same as freeze panes with a different pane type.
        self.freeze_panes(x, y, top_row, left_col, 2)

    def set_zoom(self, zoom=100):
        """
        Set the worksheet zoom factor.

        Args:
            zoom: Scale factor: 10 <= zoom <= 400.

        Returns:
            Nothing.

        """
        # Ensure the zoom scale is in Excel's range.
        if zoom < 10 or zoom > 400:
            warn("Zoom factor %d outside range: 10 <= zoom <= 400" % zoom)
            zoom = 100

        self.zoom = int(zoom)

    def right_to_left(self):
        """
        Display the worksheet right to left for some versions of Excel.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.is_right_to_left = 1

    def hide_zero(self):
        """
        Hide zero values in worksheet cells.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.show_zeros = 0

    def set_tab_color(self, color):
        """
        Set the colour of the worksheet tab.

        Args:
            color: A #RGB color index.

        Returns:
            Nothing.

        """
        self.tab_color = xl_color(color)

    def protect(self, password='', options=None):
        """
        Set the colour of the worksheet tab.

        Args:
            password: An optional password string.
            options:  A dictionary of worksheet objects to protect.

        Returns:
            Nothing.

        """
        if password != '':
            password = self._encode_password(password)

        if not options:
            options = {}

        # Default values for objects that can be protected.
        defaults = {
            'sheet': 1,
            'content': 0,
            'objects': 0,
            'scenarios': 0,
            'format_cells': 0,
            'format_columns': 0,
            'format_rows': 0,
            'insert_columns': 0,
            'insert_rows': 0,
            'insert_hyperlinks': 0,
            'delete_columns': 0,
            'delete_rows': 0,
            'select_locked_cells': 1,
            'sort': 0,
            'autofilter': 0,
            'pivot_tables': 0,
            'select_unlocked_cells': 1}

        # Overwrite the defaults with user specified values.
        for key in (options.keys()):

            if key in defaults:
                defaults[key] = options[key]
            else:
                warn("Unknown protection object: '%s'" % key)

        # Set the password after the user defined values.
        defaults['password'] = password

        self.protect_options = defaults

    ###########################################################################
    #
    # Public API. Page Setup methods.
    #
    ###########################################################################
    def set_landscape(self):
        """
        Set the page orientation as landscape.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.orientation = 0
        self.page_setup_changed = 1

    def set_portrait(self):
        """
        Set the page orientation as portrait.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.orientation = 1
        self.page_setup_changed = 1

    def set_page_view(self):
        """
        Set the page view mode.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.page_view = 1

    def set_paper(self, paper_size):
        """
        Set the paper type. US Letter = 1, A4 = 9.

        Args:
            paper_size: Paper index.

        Returns:
            Nothing.

        """
        if paper_size:
            self.paper_size = paper_size
            self.page_setup_changed = 1

    def center_horizontally(self):
        """
        Center the page horizontally.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.print_options_changed = 1
        self.hcenter = 1

    def center_vertically(self):
        """
        Center the page vertically.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.print_options_changed = 1
        self.vcenter = 1

    def set_margins(self, left=0.7, right=0.7, top=0.75, bottom=0.75):
        """
        Set all the page margins in inches.

        Args:
            left:   Left margin.
            right:  Right margin.
            top:    Top margin.
            bottom: Bottom margin.

        Returns:
            Nothing.

        """
        self.margin_left = left
        self.margin_right = right
        self.margin_top = top
        self.margin_bottom = bottom

    def set_header(self, header='', margin=0.3):
        """
        Set the page header caption and optional margin.

        Args:
            header: Header string.
            margin: Header margin.

        Returns:
            Nothing.

        """
        if len(header) >= 255:
            warn('Header string must be less than 255 characters')
            return

        self.header = encode_utf8(header)
        self.margin_header = margin
        self.header_footer_changed = 1

    def set_footer(self, footer='', margin=0.3):
        """
        Set the page footer caption and optional margin.

        Args:
            footer: Footer string.
            margin: Footer margin.

        Returns:
            Nothing.

        """
        if len(footer) >= 255:
            warn('Footer string must be less than 255 characters')
            return

        self.footer = encode_utf8(footer)
        self.margin_footer = margin
        self.header_footer_changed = 1

    def repeat_rows(self, first_row, last_row=None):
        """
        Set the rows to repeat at the top of each printed page.

        Args:
            first_row: Start row for range.
            last_row: End row for range.

        Returns:
            Nothing.

        """
        if last_row is None:
            last_row = first_row

        # Convert rows to 1 based.
        first_row += 1
        last_row += 1

        # Create the row range area like: $1:$2.
        area = '$%d:$%d' % (first_row, last_row)

        # Build up the print titles area "Sheet1!$1:$2"
        sheetname = self._quote_sheetname(self.name)
        self.repeat_row_range = sheetname + '!' + area

    @convert_column_args
    def repeat_columns(self, first_col, last_col=None):
        """
        Set the columns to repeat at the left hand side of each printed page.

        Args:
            first_col: Start column for range.
            last_col: End column for range.

        Returns:
            Nothing.

        """
        if last_col is None:
            last_col = first_col

        # Convert to A notation.
        first_col = xl_col_to_name(first_col, 1)
        last_col = xl_col_to_name(last_col, 1)

        # Create a column range like $C:$D.
        area = first_col + ':' + last_col

        # Build up the print area range "=Sheet2!$C:$D"
        sheetname = self._quote_sheetname(self.name)
        self.repeat_col_range = sheetname + "!" + area

    def hide_gridlines(self, option=1):
        """
        Set the option to hide gridlines on the screen and the printed page.

        Args:
            option:    0 : Don't hide gridlines
                       1 : Hide printed gridlines only
                       2 : Hide screen and printed gridlines

        Returns:
            Nothing.

        """
        if option == 0:
            self.print_gridlines = 1
            self.screen_gridlines = 1
            self.print_options_changed = 1
        elif option == 1:
            self.print_gridlines = 0
            self.screen_gridlines = 1
        else:
            self.print_gridlines = 0
            self.screen_gridlines = 0

    def print_row_col_headers(self):
        """
        Set the option to print the row and column headers on the printed page.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.print_headers = 1
        self.print_options_changed = 1

    @convert_range_args
    def print_area(self, first_row, first_col, last_row, last_col):
        """
        Set the print area in the current worksheet.

        Args:
            first_row:    The first row of the cell range. (zero indexed).
            first_col:    The first column of the cell range.
            last_row:     The last row of the cell range. (zero indexed).
            last_col:     The last column of the cell range.

        Returns:
            0:  Success.
            -1: Row or column is out of worksheet bounds.

        """
        # Set the print area in the current worksheet.

        # Ignore max print area since it is the same as no  area for Excel.
        if (first_row == 0 and first_col == 0
                and last_row == self.xls_rowmax - 1
                and last_col == self.xls_colmax - 1):
            return

        # Build up the print area range "Sheet1!$A$1:$C$13".
        area = self._convert_name_area(first_row, first_col,
                                       last_row, last_col)
        self.print_area_range = area

    def print_across(self):
        """
        Set the order in which pages are printed.

        Args:
            None.

        Returns:
            Nothing.

        """
        self.page_order = 1
        self.page_setup_changed = 1

    def fit_to_pages(self, width, height):
        """
        Fit the printed area to a specific number of pages both vertically and
        horizontally.

        Args:
            width:  Number of pages horizontally.
            height: Number of pages vertically.

        Returns:
            Nothing.

        """
        self.fit_page = 1
        self.fit_width = width
        self.fit_height = height
        self.page_setup_changed = 1

    def set_start_page(self, start_page):
        """
        Set the start page number when printing.

        Args:
            start_page: Start page number.

        Returns:
            Nothing.

        """
        self.page_start = start_page
        self.custom_start = 1

    def set_print_scale(self, scale):
        """
        Set the scale factor for the printed page.

        Args:
            scale: Print scale. 10 <= scale <= 400.

        Returns:
            Nothing.

        """
        # Confine the scale to Excel's range.
        if scale < 10 or scale > 400:
            warn("Print scale '%d' outside range: 10 <= scale <= 400" % scale)
            return

        # Turn off "fit to page" option when print scale is on.
        self.fit_page = 0

        self.print_scale = int(scale)
        self.page_setup_changed = 1

    def set_h_pagebreaks(self, breaks):
        """
        Set the horizontal page breaks on a worksheet.

        Args:
            breaks: List of rows where the page breaks should be added.

        Returns:
            Nothing.

        """
        self.hbreaks = breaks

    #
    # set_v_pagebreaks(@breaks)
    #
    # Store the vertical page breaks on a worksheet.
    #
    def set_v_pagebreaks(self, breaks):
        """
        Set the horizontal page breaks on a worksheet.

        Args:
            breaks: List of columns where the page breaks should be added.

        Returns:
            Nothing.

        """
        self.vbreaks = breaks

    ###########################################################################
    #
    # Private API.
    #
    ###########################################################################
    def _initialize(self, init_data):
        self.name = init_data['name']
        self.index = init_data['index']
        self.str_table = init_data['str_table']
        self.worksheet_meta = init_data['worksheet_meta']
        self.optimization = init_data['optimization']
        self.tmpdir = init_data['tmpdir']
        self.date_1904 = init_data['date_1904']

        if self.date_1904:
            self.epoch = datetime.datetime(1904, 1, 1)

        if self.optimization == 1:
            # Open a temp filehandle to store row data in optimization mode.
            self.row_data_fh = tempfile.TemporaryFile(mode='w+',
                                                      dir=self.tmpdir)

            # Also use this as the worksheet filehandle until the file is
            # due to be assembled.
            self.fh = self.row_data_fh

    def _assemble_xml_file(self):
        # Assemble and write the XML file.

        # Write the XML declaration.
        self._xml_declaration()

        # Write the root worksheet element.
        self._write_worksheet()

        # Write the worksheet properties.
        self._write_sheet_pr()

        # Write the worksheet dimensions.
        self._write_dimension()

        # Write the sheet view properties.
        self._write_sheet_views()

        # Write the sheet format properties.
        self._write_sheet_format_pr()

        # Write the sheet column info.
        self._write_cols()

        # Write the worksheet data such as rows columns and cells.
        if self.optimization == 0:
            self._write_sheet_data()
        else:
            self._write_optimized_sheet_data()

        # Write the sheetProtection element.
        self._write_sheet_protection()

        # Write the worksheet calculation properties.
        # self._write_sheet_calc_pr()

        # Write the worksheet phonetic properties.
        # self._write_phonetic_pr()

        # Write the autoFilter element.
        self._write_auto_filter()

        # Write the mergeCells element.
        self._write_merge_cells()

        # Write the conditional formats.
        self._write_conditional_formats()

        # Write the dataValidations element.
        self._write_data_validations()

        # Write the hyperlink element.
        self._write_hyperlinks()

        # Write the printOptions element.
        self._write_print_options()

        # Write the worksheet page_margins.
        self._write_page_margins()

        # Write the worksheet page setup.
        self._write_page_setup()

        # Write the headerFooter element.
        self._write_header_footer()

        # Write the rowBreaks element.
        self._write_row_breaks()

        # Write the colBreaks element.
        self._write_col_breaks()

        # Write the drawing element.
        self._write_drawings()

        # Write the legacyDrawing element.
        self._write_legacy_drawing()

        # Write the tableParts element.
        self._write_table_parts()

        # Write the extLst and sparklines.
        self._write_ext_sparklines()

        # Close the worksheet tag.
        self._xml_end_tag('worksheet')

        # Close the file.
        self._xml_close()

    def _check_dimensions(self, row, col, ignore_row=False, ignore_col=False):
        # Check that row and col are valid and store the max and min
        # values for use in other methods/elements. The ignore_row /
        # ignore_col flags is used to indicate that we wish to perform
        # the dimension check without storing the value. The ignore
        # flags are use by set_row() and data_validate.

        # Check that the row/col are within the worksheet bounds.
        if row >= self.xls_rowmax or col >= self.xls_colmax:
            return -1

        # In optimization mode we don't change dimensions for rows
        # that are already written.
        if not ignore_row and not ignore_col and self.optimization == 1:
            if row < self.previous_row:
                return -1

        if not ignore_row:
            if self.dim_rowmin is None or row < self.dim_rowmin:
                self.dim_rowmin = row
            if self.dim_rowmax is None or row > self.dim_rowmax:
                self.dim_rowmax = row

        if not ignore_col:
            if self.dim_colmin is None or col < self.dim_colmin:
                self.dim_colmin = col
            if self.dim_colmax is None or col > self.dim_colmax:
                self.dim_colmax = col

        return 0

    def _convert_date_time(self, dt_obj):
        # We handle datetime .datetime, .date and .time objects but convert
        # them to datetime.datetime objects and process them in the same way.
        if isinstance(dt_obj, datetime.datetime):
            pass
        elif isinstance(dt_obj, datetime.date):
            dt_obj = datetime.datetime.fromordinal(dt_obj.toordinal())
        elif isinstance(dt_obj, datetime.time):
            dt_obj = datetime.datetime.combine(self.epoch, dt_obj)
        else:
            raise TypeError("Unknown or unsupported datetime type")

        # Convert a Python datetime.datetime value to an Excel date number.
        delta = dt_obj - self.epoch
        excel_time = (delta.days
                      + (float(delta.seconds)
                      + float(delta.microseconds) / 1E6)
                      / (60 * 60 * 24))

        # Special case for datetime where time only has been specified and
        # the default date of 1900-01-01 is used.
        if dt_obj.isocalendar() == (1900, 1, 1):
            excel_time -= 1

        # Account for Excel erroneously treating 1900 as a leap year.
        if not self.date_1904 and excel_time > 59:
            excel_time += 1

        return excel_time

    def _options_changed(self):
        # Check to see if any of the worksheet options have changed.
        options_changed = 0
        print_changed = 0
        setup_changed = 0

        if (self.orientation == 0
                or self.hcenter == 1
                or self.vcenter == 1
                or self.header != ''
                or self.footer != ''
                or self.margin_header != 0.50
                or self.margin_footer != 0.50
                or self.margin_left != 0.75
                or self.margin_right != 0.75
                or self.margin_top != 1.00
                or self.margin_bottom != 1.00):
            setup_changed = 1

        # Special case for 1x1 page fit.
        if self.fit_width == 1 and self.fit_height == 1:
            options_changed = 1
            self.fit_width = 0
            self.fit_height = 0

        if (self.fit_width > 1
                or self.fit_height > 1
                or self.page_order == 1
                or self.black_white == 1
                or self.draft_quality == 1
                or self.print_comments == 1
                or self.paper_size != 0
                or self.print_scale != 100
                or self.print_gridlines == 1
                or self.print_headers == 1
                or self.hbreaks > 0
                or self.vbreaks > 0):
            print_changed = 1

        if (print_changed or setup_changed):
            options_changed = 1

        if self.screen_gridlines == 0:
            options_changed = 1
        if self.filter_on:
            options_changed = 1

        return (options_changed, print_changed, setup_changed)

    def _quote_sheetname(self, sheetname):
        # Sheetnames used in references should be quoted if they
        # contain any spaces, special characters or if the look like
        # something that isn't a sheet name.
        # TODO. Probably need to handle more special cases.
        if re.match(r'Sheet\d+', sheetname):
            return sheetname
        else:
            return "'%s'" % sheetname

    def _convert_name_area(self, row_num_1, col_num_1, row_num_2, col_num_2):
        # Convert zero indexed rows and columns to the format required by
        # worksheet named ranges, eg, "Sheet1!$A$1:$C$13".

        range1 = ''
        range2 = ''
        area = ''
        row_col_only = 0

        # Convert to A1 notation.
        col_char_1 = xl_col_to_name(col_num_1, 1)
        col_char_2 = xl_col_to_name(col_num_2, 1)
        row_char_1 = '$' + str(row_num_1 + 1)
        row_char_2 = '$' + str(row_num_2 + 1)

        # We need to handle special cases that refer to rows or columns only.
        if row_num_1 == 0 and row_num_2 == self.xls_rowmax - 1:
            range1 = col_char_1
            range2 = col_char_2
            row_col_only = 1
        elif col_num_1 == 0 and col_num_2 == self.xls_colmax - 1:
            range1 = row_char_1
            range2 = row_char_2
            row_col_only = 1
        else:
            range1 = col_char_1 + row_char_1
            range2 = col_char_2 + row_char_2

        # A repeated range is only written once (if it isn't a special case).
        if range1 == range2 and not row_col_only:
            area = range1
        else:
            area = range1 + ':' + range2

        # Build up the print area range "Sheet1!$A$1:$C$13".
        sheetname = self._quote_sheetname(self.name)
        area = sheetname + "!" + area

        return area

    def _sort_pagebreaks(self, breaks):
        # This is an internal method used to filter elements of a list of
        # pagebreaks used in the _store_hbreak() and _store_vbreak() methods.
        # It:
        #   1. Removes duplicate entries from the list.
        #   2. Sorts the list.
        #   3. Removes 0 from the list if present.
        if not breaks:
            return

        breaks_set = set(breaks)

        if 0 in breaks_set:
            breaks_set.remove(0)

        breaks_list = list(breaks_set)
        breaks_list.sort()

        # The Excel 2007 specification says that the maximum number of page
        # breaks is 1026. However, in practice it is actually 1023.
        max_num_breaks = 1023
        if len(breaks_list) > max_num_breaks:
            breaks_list = breaks_list[:max_num_breaks]

        return breaks_list

    def _extract_filter_tokens(self, expression):
        # Extract the tokens from the filter expression. The tokens are mainly
        # non-whitespace groups. The only tricky part is to extract string
        # tokens that contain whitespace and/or quoted double quotes (Excel's
        # escaped quotes).
        #
        # Examples: 'x <  2000'
        #           'x >  2000 and x <  5000'
        #           'x = "foo"'
        #           'x = "foo bar"'
        #           'x = "foo "" bar"'
        #
        if not expression:
            return []

        token_re = re.compile(r'"(?:[^"]|"")*"|\S+')
        tokens = token_re.findall(expression)

        new_tokens = []
        # Remove single leading and trailing quotes and un-escape other quotes.
        for token in tokens:
            if token.startswith('"'):
                token = token[1:]

            if token.endswith('"'):
                token = token[:-1]

            token = token.replace('""', '"')

            new_tokens.append(token)

        return new_tokens

    def _parse_filter_expression(self, expression, tokens):
        # Converts the tokens of a possibly conditional expression into 1 or 2
        # sub expressions for further parsing.
        #
        # Examples:
        #          ('x', '==', 2000) -> exp1
        #          ('x', '>',  2000, 'and', 'x', '<', 5000) -> exp1 and exp2

        if len(tokens) == 7:
            # The number of tokens will be either 3 (for 1 expression)
            # or 7 (for 2  expressions).
            conditional = tokens[3]

            if re.match('(and|&&)', conditional):
                conditional = 0
            elif re.match('(or|\|\|)', conditional):
                conditional = 1
            else:
                warn("Token '%s' is not a valid conditional "
                     "in filter expression '%s'" % (conditional, expression))

            expression_1 = self._parse_filter_tokens(expression, tokens[0:3])
            expression_2 = self._parse_filter_tokens(expression, tokens[4:7])

            return expression_1 + [conditional] + expression_2
        else:
            return self._parse_filter_tokens(expression, tokens)

    def _parse_filter_tokens(self, expression, tokens):
        # Parse the 3 tokens of a filter expression and return the operator
        # and token. The use of numbers instead of operators is a legacy of
        # Spreadsheet::WriteExcel.
        operators = {
            '==': 2,
            '=': 2,
            '=~': 2,
            'eq': 2,

            '!=': 5,
            '!~': 5,
            'ne': 5,
            '<>': 5,

            '<': 1,
            '<=': 3,
            '>': 4,
            '>=': 6,
        }

        operator = operators.get(tokens[1], None)
        token = tokens[2]

        # Special handling of "Top" filter expressions.
        if re.match('top|bottom', tokens[0].lower()):
            value = int(tokens[1])

            if (value < 1 or value > 500):
                warn("The value '%d' in expression '%s' "
                     "must be in the range 1 to 500" % (value, expression))

            token = token.lower()

            if token != 'items' and token != '%':
                warn("The type '%s' in expression '%s' "
                     "must be either 'items' or '%'" % (token, expression))

            if tokens[0].lower() == 'top':
                operator = 30
            else:
                operator = 32

            if tokens[2] == '%':
                operator += 1

            token = str(value)

        if not operator and tokens[0]:
            warn("Token '%s' is not a valid operator "
                 "in filter expression '%s'" % (token[0], expression))

        # Special handling for Blanks/NonBlanks.
        if re.match('blanks|nonblanks', token.lower()):
            # Only allow Equals or NotEqual in this context.
            if operator != 2 and operator != 5:
                warn("The operator '%s' in expression '%s' "
                     "is not valid in relation to Blanks/NonBlanks'"
                     % (tokens[1], expression))

            token = token.lower()

            # The operator should always be 2 (=) to flag a "simple" equality
            # in the binary record. Therefore we convert <> to =.
            if token == 'blanks':
                if operator == 5:
                    token = ' '
            else:
                if operator == 5:
                    operator = 2
                    token = 'blanks'
                else:
                    operator = 5
                    token = ' '

        # if the string token contains an Excel match character then change the
        # operator type to indicate a non "simple" equality.
        if operator == 2 and re.search('[*?]', token):
            operator = 22

        return [operator, token]

    def _encode_password(self, plaintext):
        # Encode the worksheet protection "password" as a simple hash.
        # Based on the algorithm by Daniel Rentz of OpenOffice.
        i = 0
        count = len(plaintext)
        digits = []

        for char in (plaintext):
            i += 1
            char = ord(char) << i
            low_15 = char & 0x7fff
            high_15 = char & 0x7fff << 15
            high_15 = high_15 >> 15
            char = low_15 | high_15
            digits.append(char)

        password_hash = 0x0000

        for digit in digits:
            password_hash ^= digit

        password_hash ^= count
        password_hash ^= 0xCE4B

        return "%X" % password_hash

    def _prepare_image(self, index, image_id, drawing_id, width, height,
                       name, image_type):
        # Set up images/drawings.
        drawing_type = 2
        (row, col, _, x_offset, y_offset, x_scale, y_scale) = \
            self.images[index]

        width *= x_scale
        height *= y_scale

        dimensions = self._position_object_emus(col, row, x_offset, y_offset,
                                                width, height)

        # Convert from pixels to emus.
        width = int(0.5 + (width * 9525))
        height = int(0.5 + (height * 9525))

        # Create a Drawing obj to use with worksheet unless one already exists.
        if not self.drawing:
            drawing = Drawing()
            drawing.embedded = 1
            self.drawing = drawing

            self.external_drawing_links.append(['/drawing',
                                                '../drawings/drawing'
                                                + str(drawing_id)
                                                + '.xml', None])
        else:
            drawing = self.drawing

        drawing_object = [drawing_type]
        drawing_object.extend(dimensions)
        drawing_object.extend([width, height, name, None])

        drawing._add_drawing_object(drawing_object)

        self.drawing_links.append(['/image',
                                   '../media/image'
                                   + str(image_id) + '.'
                                   + image_type])

    def _prepare_chart(self, index, chart_id, drawing_id):
        # Set up chart/drawings.
        drawing_type = 1

        (row, col, chart, x_offset, y_offset, x_scale, y_scale) = \
            self.charts[index]

        chart.id = chart_id - 1

        # Use user specified dimensions, if any.
        if chart.width:
            width = chart.width

        if chart.height:
            height = chart.height

        width = int(0.5 + (width * x_scale))
        height = int(0.5 + (height * y_scale))

        dimensions = self._position_object_emus(col, row, x_offset, y_offset,
                                                width, height)

        # Set the chart name for the embedded object if it has been specified.
        name = chart.chart_name

        # Create a Drawing obj to use with worksheet unless one already exists.
        if not self.drawing:
            drawing = Drawing()
            drawing.embedded = 1

            self.drawing = drawing

            self.external_drawing_links.append(['/drawing',
                                                '../drawings/drawing'
                                                + str(drawing_id)
                                                + '.xml'])
        else:
            drawing = self.drawing

        drawing_object = [drawing_type]
        drawing_object.extend(dimensions)
        drawing_object.extend([width, height, name, None])

        drawing._add_drawing_object(drawing_object)

        self.drawing_links.append(['/chart',
                                   '../charts/chart'
                                   + str(chart_id)
                                   + '.xml'])

    def _position_object_emus(self, col_start, row_start, x1, y1,
                              width, height):
        # Calculate the vertices that define the position of a graphical
        # object within the worksheet in EMUs.
        #
        # The vertices are expressed as English Metric Units (EMUs). There are
        # 12,700 EMUs per point. Therefore, 12,700 * 3 /4 = 9,525 EMUs per
        # pixel
        (col_start, row_start, x1, y1,
         col_end, row_end, x2, y2, x_abs, y_abs) = \
            self._position_object_pixels(col_start, row_start, x1, y1,
                                         width, height)

        # Convert the pixel values to EMUs. See above.
        x1 = int(0.5 + 9525 * x1)
        y1 = int(0.5 + 9525 * y1)
        x2 = int(0.5 + 9525 * x2)
        y2 = int(0.5 + 9525 * y2)
        x_abs = int(0.5 + 9525 * x_abs)
        y_abs = int(0.5 + 9525 * y_abs)

        return (col_start, row_start, x1, y1, col_end, row_end, x2, y2,
                x_abs, y_abs)

    # Calculate the vertices that define the position of a graphical object
    # within the worksheet in pixels.
    #
    #         +------------+------------+
    #         |     A      |      B     |
    #   +-----+------------+------------+
    #   |     |(x1,y1)     |            |
    #   |  1  |(A1)._______|______      |
    #   |     |    |              |     |
    #   |     |    |              |     |
    #   +-----+----|    OBJECT    |-----+
    #   |     |    |              |     |
    #   |  2  |    |______________.     |
    #   |     |            |        (B2)|
    #   |     |            |     (x2,y2)|
    #   +---- +------------+------------+
    #
    # Example of an object that covers some of the area from cell A1 to  B2.
    #
    # Based on the width and height of the object we need to calculate 8 vars:
    #
    #     col_start, row_start, col_end, row_end, x1, y1, x2, y2.
    #
    # We also calculate the absolute x and y position of the top left vertex of
    # the object. This is required for images.
    #
    # The width and height of the cells that the object occupies can be
    # variable and have to be taken into account.
    #
    # The values of col_start and row_start are passed in from the calling
    # function. The values of col_end and row_end are calculated by
    # subtracting the width and height of the object from the width and
    # height of the underlying cells.
    #
    def _position_object_pixels(self, col_start, row_start, x1, y1,
                                width, height):
        # col_start       # Col containing upper left corner of object.
        # x1              # Distance to left side of object.
        #
        # row_start       # Row containing top left corner of object.
        # y1              # Distance to top of object.
        #
        # col_end         # Col containing lower right corner of object.
        # x2              # Distance to right side of object.
        #
        # row_end         # Row containing bottom right corner of object.
        # y2              # Distance to bottom of object.
        #
        # width           # Width of object frame.
        # height          # Height of object frame.
        #
        # x_abs           # Absolute distance to left side of object.
        # y_abs           # Absolute distance to top side of object.
        x_abs = 0
        y_abs = 0

        # Calculate the absolute x offset of the top-left vertex.
        if self.col_size_changed:
            for col_id in range(1, col_start + 1):
                x_abs += self._size_col(col_id)
        else:
            # Optimisation for when the column widths haven't changed.
            x_abs += 64 * col_start

        x_abs += x1

        # Calculate the absolute y offset of the top-left vertex.
        # Store the column change to allow optimisations.
        if self.row_size_changed:
            for row_id in range(1, row_start + 1):
                y_abs += self._size_row(row_id)
        else:
            # Optimisation for when the row heights haven't changed.
            y_abs += 20 * row_start

        y_abs += y1

        # Adjust start column for offsets that are greater than the col width.
        while x1 >= self._size_col(col_start):
            x1 -= self._size_col(col_start)
            col_start += 1

        # Adjust start row for offsets that are greater than the row height.
        while y1 >= self._size_row(row_start):
            y1 -= self._size_row(row_start)
            row_start += 1

        # Initialise end cell to the same as the start cell.
        col_end = col_start
        row_end = row_start

        width = width + x1
        height = height + y1

        # Subtract the underlying cell widths to find end cell of the object.
        while width >= self._size_col(col_end):
            width -= self._size_col(col_end)
            col_end += 1

        # Subtract the underlying cell heights to find end cell of the object.

        while height >= self._size_row(row_end):
            height -= self._size_row(row_end)
            row_end += 1

        # TODO, is this required? Write testcase for image that fits cell.
        #
        # The following is only required for positioning drawing/chart objects
        # and not comments. It is probably the result of a bug.
        # if is_drawing:
        #    if width == 0:
        #        col_end -= 1
        #    if height == 0:
        #        row_end -= 1

        # The end vertices are whatever is left from the width and height.
        x2 = width
        y2 = height

        return ([col_start, row_start, x1, y1, col_end, row_end, x2, y2,
                x_abs, y_abs])

    def _size_col(self, col):
        # Convert the width of a cell from user's units to pixels. Excel rounds
        # the column width to the nearest pixel. If the width hasn't been set
        # by the user we use the default value. If the column is hidden it
        # has a value of zero.
        max_digit_width = 7  # For Calabri 11.
        padding = 5
        pixels = 0

        # Look up the cell value to see if it has been changed.
        if col in self.col_sizes:
            width = self.col_sizes[col]

            # Convert to pixels.
            if width == 0:
                pixels = 0
            elif width < 1:
                pixels = int(width * 12 + 0.5)
            else:
                pixels = int(width * max_digit_width + 0.5) + padding
        else:
            pixels = 64

        return pixels

    def _size_row(self, row):
        # Convert the height of a cell from user's units to pixels. If the
        # height hasn't been set by the user we use the default value. If
        #  the row is hidden it has a value of zero.
        pixels = 0

        # Look up the cell value to see if it has been changed
        if row in self.row_sizes:
            height = self.row_sizes[row]

            if height == 0:
                pixels = 0
            else:
                pixels = int(4.0 / 3.0 * height)
        else:
            pixels = int(4.0 / 3.0 * self.default_row_height)

        return pixels

    def _comment_params(self, row, col, string, options):
        # This method handles the additional optional parameters to
        # write_comment() as well as calculating the comment object
        # position and vertices.
        default_width = 128
        default_height = 74

        params = {
            'author': None,
            'color': '#ffffe1',
            'start_cell': None,
            'start_col': None,
            'start_row': None,
            'visible': None,
            'width': default_width,
            'height': default_height,
            'x_offset': None,
            'x_scale': 1,
            'y_offset': None,
            'y_scale': 1,
        }

        # Overwrite the defaults with any user supplied values. Incorrect or
        # misspelled parameters are silently ignored.
        for key in options.keys():
            params[key] = options[key]

        # Encode any string options passed by the user.
        params['author'] = encode_utf8(params['author'])

        # Ensure that a width and height have been set.
        if not params['width']:
            params['width'] = default_width
        if not params['height']:
            params['height'] = default_height

        # Set the comment background colour.
        params['color'] = xl_color(params['color']).lower()

        # Convert from Excel XML style colour to XML html style colour.
        params['color'] = params['color'].replace('ff', '#', 1)

        # Convert a cell reference to a row and column.
        if params['start_cell'] is not None:
            (start_row, start_col) = xl_cell_to_rowcol(params['start_cell'])
            params['start_row'] = start_row
            params['start_col'] = start_col

        # Set the default start cell and offsets for the comment. These are
        # generally fixed in relation to the parent cell. However there are
        # some edge cases for cells at the, er, edges.
        row_max = self.xls_rowmax
        col_max = self.xls_colmax

        if params['start_row'] is None:
            if row == 0:
                params['start_row'] = 0
            elif row == row_max - 3:
                params['start_row'] = row_max - 7
            elif row == row_max - 2:
                params['start_row'] = row_max - 6
            elif row == row_max - 1:
                params['start_row'] = row_max - 5
            else:
                params['start_row'] = row - 1

        if params['y_offset'] is None:
            if row == 0:
                params['y_offset'] = 2
            elif row == row_max - 3:
                params['y_offset'] = 16
            elif row == row_max - 2:
                params['y_offset'] = 16
            elif row == row_max - 1:
                params['y_offset'] = 14
            else:
                params['y_offset'] = 10

        if params['start_col'] is None:
            if col == col_max - 3:
                params['start_col'] = col_max - 6
            elif col == col_max - 2:
                params['start_col'] = col_max - 5
            elif col == col_max - 1:
                params['start_col'] = col_max - 4
            else:
                params['start_col'] = col + 1

        if params['x_offset'] is None:
            if col == col_max - 3:
                params['x_offset'] = 49
            elif col == col_max - 2:
                params['x_offset'] = 49
            elif col == col_max - 1:
                params['x_offset'] = 49
            else:
                params['x_offset'] = 15

        # Scale the size of the comment box if required.
        if params['x_scale']:
            params['width'] = params['width'] * params['x_scale']

        if params['y_scale']:
            params['height'] = params['height'] * params['y_scale']

        # Round the dimensions to the nearest pixel.
        params['width'] = int(0.5 + params['width'])
        params['height'] = int(0.5 + params['height'])

        # Calculate the positions of comment object.
        vertices = self._position_object_pixels(
            params['start_col'], params['start_row'], params['x_offset'],
            params['y_offset'], params['width'], params['height'])

        # Add the width and height for VML.
        vertices.append(params['width'])
        vertices.append(params['height'])

        return ([row, col, string, params['author'],
                 params['visible'], params['color']] + [vertices])

    def _prepare_vml_objects(self, vml_data_id, vml_shape_id, comment_id):
        comments = []
        # Sort the comments into row/column order for easier comparison
        # testing and set the external links for comments and buttons.
        row_nums = sorted(self.comments.keys())

        for row in row_nums:
            col_nums = sorted(self.comments[row].keys())

            for col in col_nums:
                # Set comment visibility if required and not user defined.
                if self.comments_visible:
                    if self.comments[row][col][4] is None:
                        self.comments[row][col][4] = 1

                # Set comment author if not already user defined.
                if self.comments[row][col][3] is None:
                    self.comments[row][col][3] = self.comments_author

                comments.append(self.comments[row][col])

        self.external_vml_links.append(['/vmlDrawing',
                                        '../drawings/vmlDrawing'
                                        + str(comment_id)
                                        + '.vml'])

        if self.has_comments:
            self.comments_array = comments

            self.external_comment_links.append(['/comments',
                                                '../comments'
                                                + str(comment_id)
                                                + '.xml'])

        count = len(comments)
        start_data_id = vml_data_id

        # The VML o:idmap data id contains a comma separated range when there
        # is more than one 1024 block of comments, like this: data="1,2".
        for i in range(int(count / 1024)):
            vml_data_id = '%s,%d' % (vml_data_id, start_data_id + i + 1)

        self.vml_data_id = vml_data_id
        self.vml_shape_id = vml_shape_id

        return count

    def _is_supported_datetime(self, dt):
        # Determine is an argument is a supported datetime object.
        return(isinstance(dt, datetime.datetime) or
               isinstance(dt, datetime.date) or
               isinstance(dt, datetime.time))

    def _table_function_to_formula(self, function, col_name):
        # Convert a table total function to a worksheet formula.
        formula = ''

        subtotals = {
            'average': 101,
            'countNums': 102,
            'count': 103,
            'max': 104,
            'min': 105,
            'stdDev': 107,
            'sum': 109,
            'var': 110,
        }

        if function in subtotals:
            func_num = subtotals[function]
            formula = "SUBTOTAL(%s,[%s])" % (func_num, col_name)
        else:
            warn("Unsupported function '%s' in add_table()" % function)

        return formula

    def _set_spark_color(self, sparkline, options, user_color):
        # Set the sparkline colour.
        if not user_color in options:
            return

        sparkline[user_color] = {'rgb': xl_color(options[user_color])}

    def _get_range_data(self, row_start, col_start, row_end, col_end):
        # Returns a range of data from the worksheet _table to be used in
        # chart cached data. Strings are returned as SST ids and decoded
        # in the workbook. Return None for data that doesn't exist since
        # Excel can chart series with data missing.

        if self.optimization:
            return ()

        data = []

        # Iterate through the table data.
        for row_num in range(row_start, row_end + 1):
            # Store None if row doesn't exist.
            if not row_num in self.table:
                data.append(None)
                continue

            for col_num in range(col_start, col_end + 1):

                if col_num in self.table[row_num]:
                    cell = self.table[row_num][col_num]

                    if type(cell).__name__ == 'Number':
                        # Return a number with Excel's precision.
                        data.append("%.15g" % cell.number)

                    elif type(cell).__name__ == 'String':
                        # Return a string from it's shared string index.
                        index = cell.string
                        string = self.str_table._get_shared_string(index)

                        data.append(string)

                    elif (type(cell).__name__ == 'Formula'
                            or type(cell).__name__ == 'ArrayFormula'):
                        # Return the formula value.
                        value = cell.value

                        if value is None:
                            value = 0

                        data.append(value)

                    elif type(cell).__name__ == 'Blank':
                        # Return a empty cell.
                        data.append('')
                else:

                    # Store None if column doesn't exist.
                    data.append(None)

        return data

    ###########################################################################
    #
    # The following font methods are, more or less, duplicated from the
    # Styles class. Not the cleanest version of reuse but works for now.
    #
    ###########################################################################
    def _write_font(self, xf_format):
        # Write the <font> element.
        xmlwriter = self.rstring

        xmlwriter._xml_start_tag('rPr')

        # Handle the main font properties.
        if xf_format.bold:
            xmlwriter._xml_empty_tag('b')
        if xf_format.italic:
            xmlwriter._xml_empty_tag('i')
        if xf_format.font_strikeout:
            xmlwriter._xml_empty_tag('strike')
        if xf_format.font_outline:
            xmlwriter._xml_empty_tag('outline')
        if xf_format.font_shadow:
            xmlwriter._xml_empty_tag('shadow')

        # Handle the underline variants.
        if xf_format.underline:
            self._write_underline(xf_format.underline)

        # Handle super/subscript.
        if xf_format.font_script == 1:
            self._write_vert_align('superscript')
        if xf_format.font_script == 2:
            self._write_vert_align('subscript')

        # Write the font size
        xmlwriter._xml_empty_tag('sz', [('val', xf_format.font_size)])

        # Handle colors.
        if xf_format.theme:
            self._write_color('theme', xf_format.theme)
        elif xf_format.color_indexed:
            self._write_color('indexed', xf_format.color_indexed)
        elif xf_format.font_color:
            color = self._get_palette_color(xf_format.font_color)
            self._write_rstring_color('rgb', color)
        else:
            self._write_rstring_color('theme', 1)

        # Write some other font properties related to font families.
        xmlwriter._xml_empty_tag('rFont', [('val', xf_format.font_name)])
        xmlwriter._xml_empty_tag('family', [('val', xf_format.font_family)])

        if xf_format.font_name == 'Calibri' and not xf_format.hyperlink:
            xmlwriter._xml_empty_tag('scheme',
                                     [('val', xf_format.font_scheme)])

        xmlwriter._xml_end_tag('rPr')

    def _write_underline(self, underline):
        # Write the underline font element.
        attributes = []

        # Handle the underline variants.
        if underline == 2:
            attributes = [('val', 'double')]
        elif underline == 33:
            attributes = [('val', 'singleAccounting')]
        elif underline == 34:
            attributes = [('val', 'doubleAccounting')]

        self.rstring._xml_empty_tag('u', attributes)

    def _write_vert_align(self, val):
        # Write the <vertAlign> font sub-element.
        attributes = [('val', val)]

        self.rstring._xml_empty_tag('vertAlign', attributes)

    def _write_rstring_color(self, name, value):
        # Write the <color> element.
        attributes = [(name, value)]

        self.rstring._xml_empty_tag('color', attributes)

    def _get_palette_color(self, color):
        # Convert the RGB color.
        if color[0] == '#':
            color = color[1:]

        return "FF" + color.upper()

    ###########################################################################
    #
    # XML methods.
    #
    ###########################################################################

    def _write_worksheet(self):
        # Write the <worksheet> element. This is the root element.

        schema = 'http://schemas.openxmlformats.org/'
        xmlns = schema + 'spreadsheetml/2006/main'
        xmlns_r = schema + 'officeDocument/2006/relationships'
        xmlns_mc = schema + 'markup-compatibility/2006'
        ms_schema = 'http://schemas.microsoft.com/'
        xmlns_x14ac = ms_schema + 'office/spreadsheetml/2009/9/ac'

        attributes = [
            ('xmlns', xmlns),
            ('xmlns:r', xmlns_r)]

        # Add some extra attributes for Excel 2010. Mainly for sparklines.
        if self.excel_version == 2010:
            attributes.append(('xmlns:mc', xmlns_mc))
            attributes.append(('xmlns:x14ac', xmlns_x14ac))
            attributes.append(('mc:Ignorable', 'x14ac'))

        self._xml_start_tag('worksheet', attributes)

    def _write_dimension(self):
        # Write the <dimension> element. This specifies the range of
        # cells in the worksheet. As a special case, empty
        # spreadsheets use 'A1' as a range.

        if self.dim_rowmin is None and self.dim_colmin is None:
            # If the min dimensions are not defined then no dimensions
            # have been set and we use the default 'A1'.
            ref = 'A1'

        elif self.dim_rowmin is None and self.dim_colmin is not None:
            # If the row dimensions aren't set but the column
            # dimensions are set then they have been changed via
            # set_column().

            if self.dim_colmin == self.dim_colmax:
                # The dimensions are a single cell and not a range.
                ref = xl_rowcol_to_cell(0, self.dim_colmin)
            else:
                # The dimensions are a cell range.
                cell_1 = xl_rowcol_to_cell(0, self.dim_colmin)
                cell_2 = xl_rowcol_to_cell(0, self.dim_colmax)
                ref = cell_1 + ':' + cell_2

        elif (self.dim_rowmin == self.dim_rowmax and
              self.dim_colmin == self.dim_colmax):
            # The dimensions are a single cell and not a range.
            ref = xl_rowcol_to_cell(self.dim_rowmin, self.dim_colmin)
        else:
            # The dimensions are a cell range.
            cell_1 = xl_rowcol_to_cell(self.dim_rowmin, self.dim_colmin)
            cell_2 = xl_rowcol_to_cell(self.dim_rowmax, self.dim_colmax)
            ref = cell_1 + ':' + cell_2

        self._xml_empty_tag('dimension', [('ref', ref)])

    def _write_sheet_views(self):
        # Write the <sheetViews> element.
        self._xml_start_tag('sheetViews')

        # Write the sheetView element.
        self._write_sheet_view()

        self._xml_end_tag('sheetViews')

    def _write_sheet_view(self):
        # Write the <sheetViews> element.
        attributes = []

        # Hide screen gridlines if required
        if not self.screen_gridlines:
            attributes.append(('showGridLines', 0))

        # Hide zeroes in cells.
        if not self.show_zeros:
            attributes.append(('showZeros', 0))

        # Display worksheet right to left for Hebrew, Arabic and others.
        if self.is_right_to_left:
            attributes.append(('rightToLeft', 1))

        # Show that the sheet tab is selected.
        if self.selected:
            attributes.append(('tabSelected', 1))

        # Turn outlines off. Also required in the outlinePr element.
        if not self.outline_on:
            attributes.append(("showOutlineSymbols", 0))

        # Set the page view/layout mode if required.
        if self.page_view:
            attributes.append(('view', 'pageLayout'))

        # Set the zoom level.
        if self.zoom != 100:
            if not self.page_view:
                attributes.append(('zoomScale', self.zoom))
                if self.zoom_scale_normal:
                    attributes.append(('zoomScaleNormal', self.zoom))

        attributes.append(('workbookViewId', 0))

        if self.panes or len(self.selections):
            self._xml_start_tag('sheetView', attributes)
            self._write_panes()
            self._write_selections()
            self._xml_end_tag('sheetView')
        else:
            self._xml_empty_tag('sheetView', attributes)

    def _write_sheet_format_pr(self):
        # Write the <sheetFormatPr> element.
        default_row_height = self.default_row_height
        row_level = self.outline_row_level
        col_level = self.outline_col_level

        attributes = [('defaultRowHeight', default_row_height)]

        if self.default_row_height != 15:
            attributes.append(('customHeight', 1))

        if self.default_row_zeroed:
            attributes.append(('zeroHeight', 1))

        if row_level:
            attributes.append(('outlineLevelRow', row_level))
        if col_level:
            attributes.append(('outlineLevelCol', col_level))

        if self.excel_version == 2010:
            attributes.append(('x14ac:dyDescent', '0.25'))

        self._xml_empty_tag('sheetFormatPr', attributes)

    def _write_cols(self):
        # Write the <cols> element and <col> sub elements.

        # Exit unless some column have been formatted.
        if not self.colinfo:
            return

        self._xml_start_tag('cols')

        for col_info in self.colinfo:
            self._write_col_info(col_info)

        self._xml_end_tag('cols')

    def _write_col_info(self, col_info):
        # Write the <col> element.

        (col_min, col_max, width, cell_format,
         hidden, level, collapsed) = col_info

        custom_width = 1
        xf_index = 0

        # Get the cell_format index.
        if cell_format:
            xf_index = cell_format._get_xf_index()

        # Set the Excel default column width.
        if width is None:
            if not hidden:
                width = 8.43
                custom_width = 0
            else:
                width = 0
        elif width == 8.43:
            # Width is defined but same as default.
            custom_width = 0

        # Convert column width from user units to character width.
        if width > 0:
            # For Calabri 11.
            max_digit_width = 7
            padding = 5
            width = int((float(width) * max_digit_width + padding)
                        / max_digit_width * 256.0) / 256.0

        attributes = [
            ('min', col_min + 1),
            ('max', col_max + 1),
            ('width', "%.15g" % width)]

        if xf_index:
            attributes.append(('style', xf_index))
        if hidden:
            attributes.append(('hidden', '1'))
        if custom_width:
            attributes.append(('customWidth', '1'))
        if level:
            attributes.append(('outlineLevel', level))
        if collapsed:
            attributes.append(('collapsed', '1'))

        self._xml_empty_tag('col', attributes)

    def _write_sheet_data(self):
        # Write the <sheetData> element.

        if self.dim_rowmin is None:
            # If the dimensions aren't defined there is no data to write.
            self._xml_empty_tag('sheetData')
        else:
            self._xml_start_tag('sheetData')
            self._write_rows()
            self._xml_end_tag('sheetData')

    def _write_optimized_sheet_data(self):
        # Write the <sheetData> element when the memory optimisation is on.
        # In this case we read the data stored in the temp file and rewrite
        # it to the XML sheet file.
        if self.dim_rowmin is None:
            # If the dimensions aren't defined then there is no data to write.
            self._xml_empty_tag('sheetData')
        else:
            self._xml_start_tag('sheetData')

            # Rewind the filehandle that was used for temp row data.
            buff_size = 65536
            self.row_data_fh.seek(0)
            data = self.row_data_fh.read(buff_size)

            while(data):
                self.fh.write(data)
                data = self.row_data_fh.read(buff_size)

            self.row_data_fh.close()

            self._xml_end_tag('sheetData')

    def _write_page_margins(self):
        # Write the <pageMargins> element.
        attributes = [
            ('left', self.margin_left),
            ('right', self.margin_right),
            ('top', self.margin_top),
            ('bottom', self.margin_bottom),
            ('header', self.margin_header),
            ('footer', self.margin_footer)]

        self._xml_empty_tag('pageMargins', attributes)

    def _write_page_setup(self):
        # Write the <pageSetup> element.
        #
        # The following is an example taken from Excel.
        #
        # <pageSetup
        #     paperSize="9"
        #     scale="110"
        #     fitToWidth="2"
        #     fitToHeight="2"
        #     pageOrder="overThenDown"
        #     orientation="portrait"
        #     blackAndWhite="1"
        #     draft="1"
        #     horizontalDpi="200"
        #     verticalDpi="200"
        #     r:id="rId1"
        # />
        #
        attributes = []

        # Skip this element if no page setup has changed.
        if not self.page_setup_changed:
            return

        # Set paper size.
        if self.paper_size:
            attributes.append(('paperSize', self.paper_size))

        # Set the print_scale.
        if self.print_scale != 100:
            attributes.append(('scale', self.print_scale))

        # Set the "Fit to page" properties.
        if self.fit_page and self.fit_width != 1:
            attributes.append(('fitToWidth', self.fit_width))

        if self.fit_page and self.fit_height != 1:
            attributes.append(('fitToHeight', self.fit_height))

        # Set the page print direction.
        if self.page_order:
            attributes.append(('pageOrder', "overThenDown"))

        # Set page orientation.
        if self.orientation:
            attributes.append(('orientation', 'portrait'))
        else:
            attributes.append(('orientation', 'landscape'))

        # Set start page for printing.
        if self.page_start != 0:
            attributes.append(('useFirstPageNumber', self.page_start))

        self._xml_empty_tag('pageSetup', attributes)

    def _write_print_options(self):
        # Write the <printOptions> element.
        attributes = []

        if not self.print_options_changed:
            return

        # Set horizontal centering.
        if self.hcenter:
            attributes.append(('horizontalCentered', 1))

        # Set vertical centering.
        if self.vcenter:
            attributes.append(('verticalCentered', 1))

        # Enable row and column headers.
        if self.print_headers:
            attributes.append(('headings', 1))

        # Set printed gridlines.
        if self.print_gridlines:
            attributes.append(('gridLines', 1))

        self._xml_empty_tag('printOptions', attributes)

    def _write_header_footer(self):
        # Write the <headerFooter> element.

        if not self.header_footer_changed:
            return

        self._xml_start_tag('headerFooter')

        if self.header:
            self._write_odd_header()
        if self.footer:
            self._write_odd_footer()

        self._xml_end_tag('headerFooter')

    def _write_odd_header(self):
        # Write the <headerFooter> element.
        self._xml_data_element('oddHeader', self.header)

    def _write_odd_footer(self):
        # Write the <headerFooter> element.
        self._xml_data_element('oddFooter', self.footer)

    def _write_rows(self):
        # Write out the worksheet data as a series of rows and cells.
        self._calculate_spans()

        for row_num in range(self.dim_rowmin, self.dim_rowmax + 1):

            if (row_num in self.set_rows or row_num in self.comments
                    or self.table[row_num]):
                # Only process rows with formatting, cell data and/or comments.

                span_index = int(row_num / 16)

                if span_index in self.row_spans:
                    span = self.row_spans[span_index]
                else:
                    span = None

                if self.table[row_num]:
                    # Write the cells if the row contains data.
                    if row_num not in self.set_rows:
                        self._write_row(row_num, span)
                    else:
                        self._write_row(row_num, span, self.set_rows[row_num])

                    for col_num in range(self.dim_colmin, self.dim_colmax + 1):
                        if col_num in self.table[row_num]:
                            col_ref = self.table[row_num][col_num]
                            self._write_cell(row_num, col_num, col_ref)

                    self._xml_end_tag('row')

                elif row_num in self.comments:
                    # Row with comments in cells.
                    self._write_empty_row(row_num, span,
                                          self.set_rows[row_num])
                else:
                    # Blank row with attributes only.
                    self._write_empty_row(row_num, span,
                                          self.set_rows[row_num])

    def _write_single_row(self, current_row_num=0):
        # Write out the worksheet data as a single row with cells.
        # This method is used when memory optimisation is on. A single
        # row is written and the data table is reset. That way only
        # one row of data is kept in memory at any one time. We don't
        # write span data in the optimised case since it is optional.

        # Set the new previous row as the current row.
        row_num = self.previous_row
        self.previous_row = current_row_num

        if (row_num in self.set_rows or row_num in self.comments
                or self.table[row_num]):
            # Only process rows with formatting, cell data and/or comments.

            # No span data in optimised mode.
            span = None

            if self.table[row_num]:
                # Write the cells if the row contains data.
                if row_num not in self.set_rows:
                    self._write_row(row_num, span)
                else:
                    self._write_row(row_num, span, self.set_rows[row_num])

                for col_num in range(self.dim_colmin, self.dim_colmax + 1):
                    if col_num in self.table[row_num]:
                        col_ref = self.table[row_num][col_num]
                        self._write_cell(row_num, col_num, col_ref)

                self._xml_end_tag('row')
            else:
                # Row attributes or comments only.
                self._write_empty_row(row_num, span, self.set_rows[row_num])

        # Reset table.
        self.table.clear()

    def _calculate_spans(self):
        # Calculate the "spans" attribute of the <row> tag. This is an
        # XLSX optimisation and isn't strictly required. However, it
        # makes comparing files easier. The span is the same for each
        # block of 16 rows.
        spans = {}
        span_min = None
        span_max = None

        for row_num in range(self.dim_rowmin, self.dim_rowmax + 1):

            if row_num in self.table:
                # Calculate spans for cell data.
                for col_num in range(self.dim_colmin, self.dim_colmax + 1):
                    if col_num in self.table[row_num]:
                        if span_min is None:
                            span_min = col_num
                            span_max = col_num
                        else:
                            if col_num < span_min:
                                span_min = col_num
                            if col_num > span_max:
                                span_max = col_num

            if row_num in self.comments:
                # Calculate spans for comments.
                for col_num in range(self.dim_colmin, self.dim_colmax + 1):
                    if (row_num in self.comments
                            and col_num in self.comments[row_num]):
                        if span_min is None:
                            span_min = col_num
                            span_max = col_num
                        else:
                            if col_num < span_min:
                                span_min = col_num
                            if col_num > span_max:
                                span_max = col_num

            if ((row_num + 1) % 16 == 0) or row_num == self.dim_rowmax:
                span_index = int(row_num / 16)

                if span_min is not None:
                    span_min += 1
                    span_max += 1
                    spans[span_index] = "%s:%s" % (span_min, span_max)
                    span_min = None

        self.row_spans = spans

    def _write_row(self, row, spans, properties=None, empty_row=False):
        # Write the <row> element.
        xf_index = 0

        if properties:
            height, cell_format, hidden, level, collapsed = properties
        else:
            height, cell_format, hidden, level, collapsed = None, None, 0, 0, 0

        if height is None:
            height = self.default_row_height

        attributes = [('r', row + 1)]

        # Get the cell_format index.
        if cell_format:
            xf_index = cell_format._get_xf_index()

        # Add row attributes where applicable.
        if spans:
            attributes.append(('spans', spans))
        if xf_index:
            attributes.append(('s', xf_index))
        if cell_format:
            attributes.append(('customFormat', 1))
        if height != 15:
            attributes.append(('ht', height))
        if hidden:
            attributes.append(('hidden', 1))
        if height != 15:
            attributes.append(('customHeight', 1))
        if level:
            attributes.append(('outlineLevel', level))
        if collapsed:
            attributes.append(('collapsed', 1))
        if self.excel_version == 2010:
            attributes.append(('x14ac:dyDescent', '0.25'))

        if empty_row:
            self._xml_empty_tag_unencoded('row', attributes)
        else:
            self._xml_start_tag_unencoded('row', attributes)

    def _write_empty_row(self, *args):
        # Write and empty <row> element.
        self._write_row(*args, empty_row=True)

    def _write_cell(self, row, col, cell):
        # Write the <cell> element.
        #
        # Note. This is the innermost loop so efficiency is important.
        cell_range = xl_rowcol_to_cell_fast(row, col)

        attributes = [('r', cell_range)]

        if cell.format:
            # Add the cell format index.
            xf_index = cell.format._get_xf_index()
            attributes.append(('s', xf_index))
        elif row in self.set_rows and self.set_rows[row][1]:
            # Add the row format.
            row_xf = self.set_rows[row][1]
            attributes.append(('s', row_xf._get_xf_index()))
        elif col in self.col_formats:
            # Add the column format.
            col_xf = self.col_formats[col]
            attributes.append(('s', col_xf._get_xf_index()))

        # Write the various cell types.
        if type(cell).__name__ == 'Number':
            # Write a number.
            self._xml_number_element(cell.number, attributes)

        elif type(cell).__name__ == 'String':
            # Write a string.
            string = cell.string

            if not self.optimization:
                # Write a shared string.
                self._xml_string_element(string, attributes)
            else:
                # Write an optimised in-line string.

                # Escape control characters. See SharedString.pm for details.
                string = re.sub('(_x[0-9a-fA-F]{4}_)', r'_x005F\1', string)
                string = re.sub(r'([\x00-\x08\x0B-\x1F])',
                                lambda match: "_x%04X_" %
                                ord(match.group(1)), string)

                # Write any rich strings without further tags.
                if re.search('^<r>', string) and re.search('</r>$', string):
                    self._xml_rich_inline_string(string, attributes)
                else:
                    # Add attribute to preserve leading or trailing whitespace.
                    preserve = 0
                    if re.search('^\s', string) or re.search('\s$', string):
                        preserve = 1

                    self._xml_inline_string(string, preserve, attributes)

        elif type(cell).__name__ == 'Formula':
            # Write a formula. First check if the formula value is a string.
            value = cell.value
            try:
                float(value)
            except ValueError:
                attributes.append(('t', 'str'))
                value = encode_utf8(value)

            self._xml_formula_element(cell.formula, value, attributes)

        elif type(cell).__name__ == 'ArrayFormula':
            # Write a array formula.

            # First check if the formula value is a string.
            try:
                float(cell.value)
            except ValueError:
                attributes.append(('t', 'str'))

            # Write an array formula.
            self._xml_start_tag('c', attributes)
            self._write_cell_array_formula(cell.formula, cell.range)
            self._write_cell_value(cell.value)
            self._xml_end_tag('c')

        elif type(cell).__name__ == 'Blank':
            # Write a empty cell.
            self._xml_empty_tag('c', attributes)

    def _write_cell_value(self, value):
        # Write the cell value <v> element.
        if value is None:
            value = ''

        try:
            float(value)
        except ValueError:
            value = encode_utf8(value)

        self._xml_data_element('v', value)

    def _write_cell_array_formula(self, formula, cell_range):
        # Write the cell array formula <f> element.
        attributes = [
            ('t', 'array'),
            ('ref', cell_range)
        ]

        self._xml_data_element('f', formula, attributes)

    def _write_sheet_pr(self):
        # Write the <sheetPr> element for Sheet level properties.
        attributes = []

        if (not self.fit_page
                and not self.filter_on
                and not self.tab_color
                and not self.outline_changed
                and not self.vba_codename):
            return

        if self.vba_codename:
            attributes.append(('codeName', self.vba_codename))

        if self.filter_on:
            attributes.append(('filterMode', 1))

        if (self.fit_page
                or self.tab_color
                or self.outline_changed):
            self._xml_start_tag('sheetPr', attributes)
            self._write_tab_color()
            self._write_outline_pr()
            self._write_page_set_up_pr()
            self._xml_end_tag('sheetPr')
        else:
            self._xml_empty_tag('sheetPr', attributes)

    def _write_page_set_up_pr(self):
        # Write the <pageSetUpPr> element.
        if not self.fit_page:
            return

        attributes = [('fitToPage', 1)]
        self._xml_empty_tag('pageSetUpPr', attributes)

    def _write_tab_color(self):
        # Write the <tabColor> element.
        color = self.tab_color

        if not color:
            return

        attributes = [('rgb', color)]

        self._xml_empty_tag('tabColor', attributes)

    def _write_outline_pr(self):
        # Write the <outlinePr> element.
        attributes = []

        if not self.outline_changed:
            return

        if self.outline_style:
            attributes.append(("applyStyles", 1))
        if not self.outline_below:
            attributes.append(("summaryBelow", 0))
        if not self.outline_right:
            attributes.append(("summaryRight", 0))
        if not self.outline_on:
            attributes.append(("showOutlineSymbols", 0))

        self._xml_empty_tag('outlinePr', attributes)

    def _write_row_breaks(self):
        # Write the <rowBreaks> element.
        page_breaks = self._sort_pagebreaks(self.hbreaks)

        if not page_breaks:
            return

        count = len(page_breaks)

        attributes = [
            ('count', count),
            ('manualBreakCount', count),
        ]

        self._xml_start_tag('rowBreaks', attributes)

        for row_num in (page_breaks):
            self._write_brk(row_num, 16383)

        self._xml_end_tag('rowBreaks')

    def _write_col_breaks(self):
        # Write the <colBreaks> element.
        page_breaks = self._sort_pagebreaks(self.vbreaks)

        if not page_breaks:
            return

        count = len(page_breaks)

        attributes = [
            ('count', count),
            ('manualBreakCount', count),
        ]

        self._xml_start_tag('colBreaks', attributes)

        for col_num in (page_breaks):
            self._write_brk(col_num, 1048575)

        self._xml_end_tag('colBreaks')

    def _write_brk(self, brk_id, brk_max):
        # Write the <brk> element.
        attributes = [
            ('id', brk_id),
            ('max', brk_max),
            ('man', 1)]

        self._xml_empty_tag('brk', attributes)

    def _write_merge_cells(self):
        # Write the <mergeCells> element.
        merged_cells = self.merge
        count = len(merged_cells)

        if not count:
            return

        attributes = [('count', count)]

        self._xml_start_tag('mergeCells', attributes)

        for merged_range in (merged_cells):

            # Write the mergeCell element.
            self._write_merge_cell(merged_range)

        self._xml_end_tag('mergeCells')

    def _write_merge_cell(self, merged_range):
        # Write the <mergeCell> element.
        (row_min, col_min, row_max, col_max) = merged_range

        # Convert the merge dimensions to a cell range.
        cell_1 = xl_rowcol_to_cell(row_min, col_min)
        cell_2 = xl_rowcol_to_cell(row_max, col_max)
        ref = cell_1 + ':' + cell_2

        attributes = [('ref', ref)]

        self._xml_empty_tag('mergeCell', attributes)

    def _write_hyperlinks(self):
        # Process any stored hyperlinks in row/col order and write the
        # <hyperlinks> element. The attributes are different for internal
        # and external links.
        hlink_refs = []
        display = None

        # Sort the hyperlinks into row order.
        row_nums = sorted(self.hyperlinks.keys())

        # Exit if there are no hyperlinks to process.
        if not row_nums:
            return

        # Iterate over the rows.
        for row_num in (row_nums):
            # Sort the hyperlinks into column order.
            col_nums = sorted(self.hyperlinks[row_num].keys())

            # Iterate over the columns.
            for col_num in (col_nums):
                # Get the link data for this cell.
                link = self.hyperlinks[row_num][col_num]
                link_type = link["link_type"]

                # If the cell isn't a string then we have to add the url as
                # the string to display.
                if (self.table
                        and self.table[row_num]
                        and self.table[row_num][col_num]):
                    cell = self.table[row_num][col_num]
                    if type(cell).__name__ != 'String':
                        display = link["url"]

                if link_type == 1:
                    # External link with rel file relationship.
                    self.rel_count += 1

                    hlink_refs.append([link_type,
                                       row_num,
                                       col_num,
                                       self.rel_count,
                                       link["str"],
                                       display,
                                       link["tip"]])

                    # Links for use by the packager.
                    self.external_hyper_links.append(['/hyperlink',
                                                      link["url"], 'External'])
                else:
                    # Internal link with rel file relationship.
                    hlink_refs.append([link_type,
                                       row_num,
                                       col_num,
                                       link["url"],
                                       link["str"],
                                       link["tip"]])

        # Write the hyperlink elements.
        self._xml_start_tag('hyperlinks')

        for args in (hlink_refs):
            link_type = args.pop(0)

            if link_type == 1:
                self._write_hyperlink_external(*args)
            elif link_type == 2:
                self._write_hyperlink_internal(*args)

        self._xml_end_tag('hyperlinks')

    def _write_hyperlink_external(self, row, col, id_num, location=None,
                                  display=None, tooltip=None):
        # Write the <hyperlink> element for external links.
        ref = xl_rowcol_to_cell(row, col)
        r_id = 'rId' + str(id_num)

        attributes = [
            ('ref', ref),
            ('r:id', r_id)]

        if location is not None:
            attributes.append(('location', location))
        if display is not None:
            attributes.append(('display', display))
        if tooltip is not None:
            attributes.append(('tooltip', tooltip))

        self._xml_empty_tag('hyperlink', attributes)

    def _write_hyperlink_internal(self, row, col, location=None, display=None,
                                  tooltip=None):
        # Write the <hyperlink> element for internal links.
        ref = xl_rowcol_to_cell(row, col)

        attributes = [
            ('ref', ref),
            ('location', location)]

        if tooltip is not None:
            attributes.append(('tooltip', tooltip))
        attributes.append(('display', display))

        self._xml_empty_tag('hyperlink', attributes)

    def _write_auto_filter(self):
        # Write the <autoFilter> element.
        if not self.autofilter_ref:
            return

        attributes = [('ref', self.autofilter_ref)]

        if self.filter_on:
            # Autofilter defined active filters.
            self._xml_start_tag('autoFilter', attributes)
            self._write_autofilters()
            self._xml_end_tag('autoFilter')

        else:
            # Autofilter defined without active filters.
            self._xml_empty_tag('autoFilter', attributes)

    def _write_autofilters(self):
        # Function to iterate through the columns that form part of an
        # autofilter range and write the appropriate filters.
        (col1, col2) = self.filter_range

        for col in range(col1, col2 + 1):
            # Skip if column doesn't have an active filter.
            if not col in self.filter_cols:
                continue

            # Retrieve the filter tokens and write the autofilter records.
            tokens = self.filter_cols[col]
            filter_type = self.filter_type[col]

            # Filters are relative to first column in the autofilter.
            self._write_filter_column(col - col1, filter_type, tokens)

    def _write_filter_column(self, col_id, filter_type, filters):
        # Write the <filterColumn> element.
        attributes = [('colId', col_id)]

        self._xml_start_tag('filterColumn', attributes)

        if filter_type == 1:
            # Type == 1 is the new XLSX style filter.
            self._write_filters(filters)
        else:
            # Type == 0 is the classic "custom" filter.
            self._write_custom_filters(filters)

        self._xml_end_tag('filterColumn')

    def _write_filters(self, filters):
        # Write the <filters> element.

        if len(filters) == 1 and filters[0] == 'blanks':
            # Special case for blank cells only.
            self._xml_empty_tag('filters', [('blank', 1)])
        else:
            # General case.
            self._xml_start_tag('filters')

            for autofilter in (filters):
                self._write_filter(autofilter)

            self._xml_end_tag('filters')

    def _write_filter(self, val):
        # Write the <filter> element.
        attributes = [('val', val)]

        self._xml_empty_tag('filter', attributes)

    def _write_custom_filters(self, tokens):
        # Write the <customFilters> element.
        if len(tokens) == 2:
            # One filter expression only.
            self._xml_start_tag('customFilters')
            self._write_custom_filter(*tokens)
            self._xml_end_tag('customFilters')
        else:
            # Two filter expressions.
            attributes = []

            # Check if the "join" operand is "and" or "or".
            if tokens[2] == 0:
                attributes = [('and', 1)]
            else:
                attributes = [('and', 0)]

            # Write the two custom filters.
            self._xml_start_tag('customFilters', attributes)
            self._write_custom_filter(tokens[0], tokens[1])
            self._write_custom_filter(tokens[3], tokens[4])
            self._xml_end_tag('customFilters')

    def _write_custom_filter(self, operator, val):
        # Write the <customFilter> element.
        attributes = []

        operators = {
            1: 'lessThan',
            2: 'equal',
            3: 'lessThanOrEqual',
            4: 'greaterThan',
            5: 'notEqual',
            6: 'greaterThanOrEqual',
            22: 'equal',
        }

        # Convert the operator from a number to a descriptive string.
        if operators[operator] is not None:
            operator = operators[operator]
        else:
            warn("Unknown operator = %s" % operator)

        # The 'equal' operator is the default attribute and isn't stored.
        if not operator == 'equal':
            attributes.append(('operator', operator))
        attributes.append(('val', val))

        self._xml_empty_tag('customFilter', attributes)

    def _write_sheet_protection(self):
        # Write the <sheetProtection> element.
        attributes = []

        if not self.protect_options:
            return

        options = self.protect_options

        if options['password']:
            attributes.append(('password', options['password']))
        if options['sheet']:
            attributes.append(('sheet', 1))
        if options['content']:
            attributes.append(('content', 1))
        if not options['objects']:
            attributes.append(('objects', 1))
        if not options['scenarios']:
            attributes.append(('scenarios', 1))
        if options['format_cells']:
            attributes.append(('formatCells', 0))
        if options['format_columns']:
            attributes.append(('formatColumns', 0))
        if options['format_rows']:
            attributes.append(('formatRows', 0))
        if options['insert_columns']:
            attributes.append(('insertColumns', 0))
        if options['insert_rows']:
            attributes.append(('insertRows', 0))
        if options['insert_hyperlinks']:
            attributes.append(('insertHyperlinks', 0))
        if options['delete_columns']:
            attributes.append(('deleteColumns', 0))
        if options['delete_rows']:
            attributes.append(('deleteRows', 0))
        if not options['select_locked_cells']:
            attributes.append(('selectLockedCells', 1))
        if options['sort']:
            attributes.append(('sort', 0))
        if options['autofilter']:
            attributes.append(('autoFilter', 0))
        if options['pivot_tables']:
            attributes.append(('pivotTables', 0))
        if not options['select_unlocked_cells']:
            attributes.append(('selectUnlockedCells', 1))

        self._xml_empty_tag('sheetProtection', attributes)

    def _write_drawings(self):
        # Write the <drawing> elements.
        if not self.drawing:
            return

        self.rel_count += 1
        self._write_drawing(self.rel_count)

    def _write_drawing(self, drawing_id):
        # Write the <drawing> element.
        r_id = 'rId' + str(drawing_id)

        attributes = [('r:id', r_id)]

        self._xml_empty_tag('drawing', attributes)

    def _write_legacy_drawing(self):
        # Write the <legacyDrawing> element.
        if not self.has_vml:
            return

        # Increment the relationship id for any drawings or comments.
        self.rel_count += 1
        r_id = 'rId' + str(self.rel_count)

        attributes = [('r:id', r_id)]

        self._xml_empty_tag('legacyDrawing', attributes)

    def _write_data_validations(self):
        # Write the <dataValidations> element.
        validations = self.validations
        count = len(validations)

        if not count:
            return

        attributes = [('count', count)]

        self._xml_start_tag('dataValidations', attributes)

        for validation in (validations):

            # Write the dataValidation element.
            self._write_data_validation(validation)

        self._xml_end_tag('dataValidations')

    def _write_data_validation(self, options):
        # Write the <dataValidation> element.
        sqref = ''
        attributes = []

        # Set the cell range(s) for the data validation.
        for cells in options['cells']:

            # Add a space between multiple cell ranges.
            if sqref != '':
                sqref += ' '

            (row_first, col_first, row_last, col_last) = cells

            # Swap last row/col for first row/col as necessary
            if row_first > row_last:
                (row_first, row_last) = (row_last, row_first)

            if col_first > col_last:
                (col_first, col_last) = (col_last, col_first)

            # If the first and last cell are the same write a single cell.
            if (row_first == row_last) and (col_first == col_last):
                sqref += xl_rowcol_to_cell(row_first, col_first)
            else:
                sqref += xl_range(row_first, col_first, row_last, col_last)

        attributes.append(('type', options['validate']))

        if options['criteria'] != 'between':
            attributes.append(('operator', options['criteria']))

        if 'error_type' in options:
            if options['error_type'] == 1:
                attributes.append(('errorStyle', 'warning'))
            if options['error_type'] == 2:
                attributes.append(('errorStyle', 'information'))

        if options['ignore_blank']:
            attributes.append(('allowBlank', 1))

        if not options['dropdown']:
            attributes.append(('showDropDown', 1))

        if options['show_input']:
            attributes.append(('showInputMessage', 1))

        if options['show_error']:
            attributes.append(('showErrorMessage', 1))

        if 'error_title' in options:
            attributes.append(('errorTitle', options['error_title']))

        if 'error_message' in options:
            attributes.append(('error', options['error_message']))

        if 'input_title' in options:
            attributes.append(('promptTitle', options['input_title']))

        if 'input_message' in options:
            attributes.append(('prompt', options['input_message']))

        attributes.append(('sqref', sqref))

        self._xml_start_tag('dataValidation', attributes)

        # Write the formula1 element.
        self._write_formula_1(options['value'])

        # Write the formula2 element.
        if options['maximum'] is not None:
            self._write_formula_2(options['maximum'])

        self._xml_end_tag('dataValidation')

    def _write_formula_1(self, formula):
        # Write the <formula1> element.

        if type(formula) is list:
            formula = ','.join([str(item) for item in formula])
            formula = '"%s"' % formula
        else:
            # Check if the formula is a number.
            try:
                float(formula)
            except ValueError:
                # Not a number. Remove the formula '=' sign if it exists.
                if formula.startswith('='):
                    formula = formula.lstrip('=')

        self._xml_data_element('formula1', formula)

    def _write_formula_2(self, formula):
        # Write the <formula2> element.

        # Check if the formula is a number.
        try:
            float(formula)
        except ValueError:
            # Not a number. Remove the formula '=' sign if it exists.
            if formula.startswith('='):
                formula = formula.lstrip('=')

        self._xml_data_element('formula2', formula)

    def _write_conditional_formats(self):
        # Write the Worksheet conditional formats.
        ranges = sorted(self.cond_formats.keys())

        if not ranges:
            return

        for cond_range in (ranges):
            self._write_conditional_formatting(cond_range,
                                               self.cond_formats[cond_range])

    def _write_conditional_formatting(self, cond_range, params):
        # Write the <conditionalFormatting> element.
        attributes = [('sqref', cond_range)]
        self._xml_start_tag('conditionalFormatting', attributes)
        for param in (params):
            # Write the cfRule element.
            self._write_cf_rule(param)
        self._xml_end_tag('conditionalFormatting')

    def _write_cf_rule(self, params):
        # Write the <cfRule> element.
        attributes = [('type', params['type'])]

        if 'format' in params and params['format'] is not None:
            attributes.append(('dxfId', params['format']))

        attributes.append(('priority', params['priority']))

        if params['type'] == 'cellIs':
            attributes.append(('operator', params['criteria']))

            self._xml_start_tag('cfRule', attributes)

            if 'minimum' in params and 'maximum' in params:
                self._write_formula(params['minimum'])
                self._write_formula(params['maximum'])
            else:
                self._write_formula(params['value'])

            self._xml_end_tag('cfRule')

        elif params['type'] == 'aboveAverage':
            if re.search('below', params['criteria']):
                attributes.append(('aboveAverage', 0))

            if re.search('equal', params['criteria']):
                attributes.append(('equalAverage', 1))

            if re.search('[123] std dev', params['criteria']):
                match = re.search('([123]) std dev', params['criteria'])
                attributes.append(('stdDev', match.group(1)))

            self._xml_empty_tag('cfRule', attributes)

        elif params['type'] == 'top10':
            if 'criteria' in params and params['criteria'] == '%':
                attributes.append(('percent', 1))

            if 'direction' in params:
                attributes.append(('bottom', 1))

            rank = params['value'] or 10
            attributes.append(('rank', rank))

            self._xml_empty_tag('cfRule', attributes)

        elif params['type'] == 'duplicateValues':
            self._xml_empty_tag('cfRule', attributes)

        elif params['type'] == 'uniqueValues':
            self._xml_empty_tag('cfRule', attributes)

        elif (params['type'] == 'containsText'
              or params['type'] == 'notContainsText'
              or params['type'] == 'beginsWith'
              or params['type'] == 'endsWith'):
            attributes.append(('operator', params['criteria']))
            attributes.append(('text', params['value']))
            self._xml_start_tag('cfRule', attributes)
            self._write_formula(params['formula'])
            self._xml_end_tag('cfRule')

        elif params['type'] == 'timePeriod':
            attributes.append(('timePeriod', params['criteria']))
            self._xml_start_tag('cfRule', attributes)
            self._write_formula(params['formula'])
            self._xml_end_tag('cfRule')

        elif (params['type'] == 'containsBlanks'
              or params['type'] == 'notContainsBlanks'
              or params['type'] == 'containsErrors'
              or params['type'] == 'notContainsErrors'):
            self._xml_start_tag('cfRule', attributes)
            self._write_formula(params['formula'])
            self._xml_end_tag('cfRule')

        elif params['type'] == 'colorScale':
            self._xml_start_tag('cfRule', attributes)
            self._write_color_scale(params)
            self._xml_end_tag('cfRule')

        elif params['type'] == 'dataBar':
            self._xml_start_tag('cfRule', attributes)
            self._write_data_bar(params)
            self._xml_end_tag('cfRule')

        elif params['type'] == 'expression':
            self._xml_start_tag('cfRule', attributes)
            self._write_formula(params['criteria'])
            self._xml_end_tag('cfRule')

    def _write_formula(self, formula):
        # Write the <formula> element.

        # Check if the formula is a number.
        try:
            float(formula)
        except ValueError:
            # Not a number. Remove the formula '=' sign if it exists.
            if formula.startswith('='):
                formula = formula.lstrip('=')

        self._xml_data_element('formula', formula)

    def _write_color_scale(self, param):
        # Write the <colorScale> element.

        self._xml_start_tag('colorScale')

        self._write_cfvo(param['min_type'], param['min_value'])

        if param['mid_type'] is not None:
            self._write_cfvo(param['mid_type'], param['mid_value'])

        self._write_cfvo(param['max_type'], param['max_value'])

        self._write_color('rgb', param['min_color'])

        if param['mid_color'] is not None:
            self._write_color('rgb', param['mid_color'])

        self._write_color('rgb', param['max_color'])

        self._xml_end_tag('colorScale')

    def _write_data_bar(self, param):
        # Write the <dataBar> element.
        self._xml_start_tag('dataBar')

        self._write_cfvo(param['min_type'], param['min_value'])
        self._write_cfvo(param['max_type'], param['max_value'])
        self._write_color('rgb', param['bar_color'])

        self._xml_end_tag('dataBar')

    def _write_cfvo(self, cf_type, val):
        # Write the <cfvo> element.
        attributes = [('type', cf_type), ('val', val)]

        self._xml_empty_tag('cfvo', attributes)

    def _write_color(self, name, value):
        # Write the <color> element.
        attributes = [(name, value)]

        self._xml_empty_tag('color', attributes)

    def _write_selections(self):
        # Write the <selection> elements.
        for selection in self.selections:
            self._write_selection(*selection)

    def _write_selection(self, pane, active_cell, sqref):
        # Write the <selection> element.
        attributes = []

        if pane:
            attributes.append(('pane', pane))

        if active_cell:
            attributes.append(('activeCell', active_cell))

        if sqref:
            attributes.append(('sqref', sqref))

        self._xml_empty_tag('selection', attributes)

    def _write_panes(self):
        # Write the frozen or split <pane> elements.
        panes = self.panes

        if not len(panes):
            return

        if panes[4] == 2:
            self._write_split_panes(*panes)
        else:
            self._write_freeze_panes(*panes)

    def _write_freeze_panes(self, row, col, top_row, left_col, pane_type):
        # Write the <pane> element for freeze panes.
        attributes = []

        y_split = row
        x_split = col
        top_left_cell = xl_rowcol_to_cell(top_row, left_col)
        active_pane = ''
        state = ''
        active_cell = ''
        sqref = ''

        # Move user cell selection to the panes.
        if self.selections:
            (_, active_cell, sqref) = self.selections[0]
            self.selections = []

        # Set the active pane.
        if row and col:
            active_pane = 'bottomRight'

            row_cell = xl_rowcol_to_cell(row, 0)
            col_cell = xl_rowcol_to_cell(0, col)

            self.selections.append(['topRight', col_cell, col_cell])
            self.selections.append(['bottomLeft', row_cell, row_cell])
            self.selections.append(['bottomRight', active_cell, sqref])

        elif col:
            active_pane = 'topRight'
            self.selections.append(['topRight', active_cell, sqref])

        else:
            active_pane = 'bottomLeft'
            self.selections.append(['bottomLeft', active_cell, sqref])

        # Set the pane type.
        if pane_type == 0:
            state = 'frozen'
        elif pane_type == 1:
            state = 'frozenSplit'
        else:
            state = 'split'

        if x_split:
            attributes.append(('xSplit', x_split))

        if y_split:
            attributes.append(('ySplit', y_split))

        attributes.append(('topLeftCell', top_left_cell))
        attributes.append(('activePane', active_pane))
        attributes.append(('state', state))

        self._xml_empty_tag('pane', attributes)

    def _write_split_panes(self, row, col, top_row, left_col, pane_type):
        # Write the <pane> element for split panes.
        attributes = []
        has_selection = 0
        active_pane = ''
        active_cell = ''
        sqref = ''

        y_split = row
        x_split = col

        # Move user cell selection to the panes.
        if self.selections:
            (_, active_cell, sqref) = self.selections[0]
            self.selections = []
            has_selection = 1

        # Convert the row and col to 1/20 twip units with padding.
        if y_split:
            y_split = int(20 * y_split + 300)

        if x_split:
            x_split = self._calculate_x_split_width(x_split)

        # For non-explicit topLeft definitions, estimate the cell offset based
        # on the pixels dimensions. This is only a workaround and doesn't take
        # adjusted cell dimensions into account.
        if top_row == row and left_col == col:
            top_row = int(0.5 + (y_split - 300) / 20 / 15)
            left_col = int(0.5 + (x_split - 390) / 20 / 3 * 4 / 64)

        top_left_cell = xl_rowcol_to_cell(top_row, left_col)

        # If there is no selection set the active cell to the top left cell.
        if not has_selection:
            active_cell = top_left_cell
            sqref = top_left_cell

        # Set the Cell selections.
        if row and col:
            active_pane = 'bottomRight'

            row_cell = xl_rowcol_to_cell(top_row, 0)
            col_cell = xl_rowcol_to_cell(0, left_col)

            self.selections.append(['topRight', col_cell, col_cell])
            self.selections.append(['bottomLeft', row_cell, row_cell])
            self.selections.append(['bottomRight', active_cell, sqref])

        elif col:
            active_pane = 'topRight'
            self.selections.append(['topRight', active_cell, sqref])

        else:
            active_pane = 'bottomLeft'
            self.selections.append(['bottomLeft', active_cell, sqref])

        # Format splits to the same precision as Excel.
        if x_split:
            attributes.append(('xSplit', "%.15g" % x_split))

        if y_split:
            attributes.append(('ySplit', "%.15g" % y_split))

        attributes.append(('topLeftCell', top_left_cell))

        if has_selection:
            attributes.append(('activePane', active_pane))

        self._xml_empty_tag('pane', attributes)

    def _calculate_x_split_width(self, width):
        # Convert column width from user units to pane split width.

        max_digit_width = 7  # For Calabri 11.
        padding = 5

        # Convert to pixels.
        if width < 1:
            pixels = int(width * 12 + 0.5)
        else:
            pixels = int(width * max_digit_width + 0.5) + padding

        # Convert to points.
        points = pixels * 3 / 4

        # Convert to twips (twentieths of a point).
        twips = points * 20

        # Add offset/padding.
        width = twips + 390

        return width

    def _write_table_parts(self):
        # Write the <tableParts> element.
        tables = self.tables
        count = len(tables)

        # Return if worksheet doesn't contain any tables.
        if not count:
            return

        attributes = [('count', count,)]

        self._xml_start_tag('tableParts', attributes)

        for _ in tables:

            # Write the tablePart element.
            self.rel_count += 1
            self._write_table_part(self.rel_count)

        self._xml_end_tag('tableParts')

    def _write_table_part(self, r_id):
        # Write the <tablePart> element.

        r_id = 'rId' + str(r_id)

        attributes = [('r:id', r_id,)]

        self._xml_empty_tag('tablePart', attributes)

    def _write_ext_sparklines(self):
        # Write the <extLst> element and sparkline sub-elements.
        sparklines = self.sparklines
        count = len(sparklines)

        # Return if worksheet doesn't contain any sparklines.
        if not count:
            return

        # Write the extLst element.
        self._xml_start_tag('extLst')

        # Write the ext element.
        self._write_ext()

        # Write the x14:sparklineGroups element.
        self._write_sparkline_groups()

        # Write the sparkline elements.
        for sparkline in reversed(sparklines):

            # Write the x14:sparklineGroup element.
            self._write_sparkline_group(sparkline)

            # Write the x14:colorSeries element.
            self._write_color_series(sparkline['series_color'])

            # Write the x14:colorNegative element.
            self._write_color_negative(sparkline['negative_color'])

            # Write the x14:colorAxis element.
            self._write_color_axis()

            # Write the x14:colorMarkers element.
            self._write_color_markers(sparkline['markers_color'])

            # Write the x14:colorFirst element.
            self._write_color_first(sparkline['first_color'])

            # Write the x14:colorLast element.
            self._write_color_last(sparkline['last_color'])

            # Write the x14:colorHigh element.
            self._write_color_high(sparkline['high_color'])

            # Write the x14:colorLow element.
            self._write_color_low(sparkline['low_color'])

            if sparkline['date_axis']:
                self._xml_data_element('xm:f', sparkline['date_axis'])

            self._write_sparklines(sparkline)

            self._xml_end_tag('x14:sparklineGroup')

        self._xml_end_tag('x14:sparklineGroups')
        self._xml_end_tag('ext')
        self._xml_end_tag('extLst')

    def _write_sparklines(self, sparkline):
        # Write the <x14:sparklines> element and <x14:sparkline> sub-elements.

        # Write the sparkline elements.
        self._xml_start_tag('x14:sparklines')

        for i in range(sparkline['count']):
            spark_range = sparkline['ranges'][i]
            location = sparkline['locations'][i]

            self._xml_start_tag('x14:sparkline')
            self._xml_data_element('xm:f', spark_range)
            self._xml_data_element('xm:sqref', location)
            self._xml_end_tag('x14:sparkline')

        self._xml_end_tag('x14:sparklines')

    def _write_ext(self):
        # Write the <ext> element.
        schema = 'http://schemas.microsoft.com/office/'
        xmlns_x_14 = schema + 'spreadsheetml/2009/9/main'
        uri = '{05C60535-1F16-4fd2-B633-F4F36F0B64E0}'

        attributes = [
            ('xmlns:x14', xmlns_x_14),
            ('uri', uri),
        ]

        self._xml_start_tag('ext', attributes)

    def _write_sparkline_groups(self):
        # Write the <x14:sparklineGroups> element.
        xmlns_xm = 'http://schemas.microsoft.com/office/excel/2006/main'

        attributes = [('xmlns:xm', xmlns_xm)]

        self._xml_start_tag('x14:sparklineGroups', attributes)

    def _write_sparkline_group(self, options):
        # Write the <x14:sparklineGroup> element.
        #
        # Example for order.
        #
        # <x14:sparklineGroup
        #     manualMax="0"
        #     manualMin="0"
        #     lineWeight="2.25"
        #     type="column"
        #     dateAxis="1"
        #     displayEmptyCellsAs="span"
        #     markers="1"
        #     high="1"
        #     low="1"
        #     first="1"
        #     last="1"
        #     negative="1"
        #     displayXAxis="1"
        #     displayHidden="1"
        #     minAxisType="custom"
        #     maxAxisType="custom"
        #     rightToLeft="1">
        #
        empty = options.get('empty')
        attributes = []

        if options.get('max'):
            if options['max'] == 'group':
                options['cust_max'] = 'group'
            else:
                attributes.append(('manualMax', options['max']))
                options['cust_max'] = 'custom'

        if options.get('min'):

            if options['min'] == 'group':
                options['cust_min'] = 'group'
            else:
                attributes.append(('manualMin', options['min']))
                options['cust_min'] = 'custom'

        # Ignore the default type attribute (line).
        if options['type'] != 'line':
            attributes.append(('type', options['type']))

        if options.get('weight'):
            attributes.append(('lineWeight', options['weight']))

        if options.get('date_axis'):
            attributes.append(('dateAxis', 1))

        if empty:
            attributes.append(('displayEmptyCellsAs', empty))

        if options.get('markers'):
            attributes.append(('markers', 1))

        if options.get('high'):
            attributes.append(('high', 1))

        if options.get('low'):
            attributes.append(('low', 1))

        if options.get('first'):
            attributes.append(('first', 1))

        if options.get('last'):
            attributes.append(('last', 1))

        if options.get('negative'):
            attributes.append(('negative', 1))

        if options.get('axis'):
            attributes.append(('displayXAxis', 1))

        if options.get('hidden'):
            attributes.append(('displayHidden', 1))

        if options.get('cust_min'):
            attributes.append(('minAxisType', options['cust_min']))

        if options.get('cust_max'):
            attributes.append(('maxAxisType', options['cust_max']))

        if options.get('reverse'):
            attributes.append(('rightToLeft', 1))

        self._xml_start_tag('x14:sparklineGroup', attributes)

    def _write_spark_color(self, element, color):
        # Helper function for the sparkline color functions below.
        attributes = []

        if color.get('rgb'):
            attributes.append(('rgb', color['rgb']))

        if color.get('theme'):
            attributes.append(('theme', color['theme']))

        if color.get('tint'):
            attributes.append(('tint', color['tint']))

        self._xml_empty_tag(element, attributes)

    def _write_color_series(self, color):
        # Write the <x14:colorSeries> element.
        self._write_spark_color('x14:colorSeries', color)

    def _write_color_negative(self, color):
        # Write the <x14:colorNegative> element.
        self._write_spark_color('x14:colorNegative', color)

    def _write_color_axis(self):
        # Write the <x14:colorAxis> element.
        self._write_spark_color('x14:colorAxis', {'rgb': 'FF000000'})

    def _write_color_markers(self, color):
        # Write the <x14:colorMarkers> element.
        self._write_spark_color('x14:colorMarkers', color)

    def _write_color_first(self, color):
        # Write the <x14:colorFirst> element.
        self._write_spark_color('x14:colorFirst', color)

    def _write_color_last(self, color):
        # Write the <x14:colorLast> element.
        self._write_spark_color('x14:colorLast', color)

    def _write_color_high(self, color):
        # Write the <x14:colorHigh> element.
        self._write_spark_color('x14:colorHigh', color)

    def _write_color_low(self, color):
        # Write the <x14:colorLow> element.
        self._write_spark_color('x14:colorLow', color)

########NEW FILE########
__FILENAME__ = xmlwriter
###############################################################################
#
# XMLwriter - A base class for XlsxWriter classes.
#
# Used in conjunction with XlsxWriter.
#
# Copyright 2013, John McNamara, jmcnamara@cpan.org
#

# Standard packages.
import re
import sys


class XMLwriter(object):
    """
    Simple XML writer class.

    """

    def __init__(self):
        self.fh = None
        self.escapes = re.compile('["&<>]')
        self.internal_fh = False

    def _set_filehandle(self, filehandle):
        # Set the writer filehandle directly. Mainly for testing.
        self.fh = filehandle
        self.internal_fh = False

    def _set_xml_writer(self, filename):
        # Set the XML writer filehandle for the object. This can either be
        # done using _set_filehandle(), usually for testing, or later via
        # this method, when assembling the xlsx file.
        self.internal_fh = True
        if sys.version_info >= (3, 0):
            self.fh = open(filename, 'w', encoding='utf-8')
        else:
            self.fh = open(filename, 'w')

    def _xml_close(self):
        # Close the XML filehandle if we created it.
        if self.internal_fh:
            self.fh.close()

    def _xml_declaration(self):
        # Write the XML declaration.
        self.fh.write(
            """<?xml version="1.0" encoding="UTF-8" standalone="yes"?>\n""")

    def _xml_start_tag(self, tag, attributes=[]):
        # Write an XML start tag with optional attributes.
        for key, value in attributes:
            value = self._escape_attributes(value)
            tag = tag + ' %s="%s"' % (key, value)

        self.fh.write("<%s>" % tag)

    def _xml_start_tag_unencoded(self, tag, attributes=[]):
        # Write an XML start tag with optional, unencoded, attributes.
        # This is a minor speed optimisation for elements that don't
        # need encoding.
        for key, value in attributes:
            tag = tag + ' %s="%s"' % (key, value)

        self.fh.write("<%s>" % tag)

    def _xml_end_tag(self, tag):
        # Write an XML end tag.
        self.fh.write("</%s>" % tag)

    def _xml_empty_tag(self, tag, attributes=[]):
        # Write an empty XML tag with optional attributes.
        for key, value in attributes:
            value = self._escape_attributes(value)
            tag = tag + ' %s="%s"' % (key, value)

        self.fh.write("<%s/>" % tag)

    def _xml_empty_tag_unencoded(self, tag, attributes=[]):
        # Write an XML start tag with optional, unencoded, attributes.
        # This is a minor speed optimisation for elements that don't
        # need encoding.
        for key, value in attributes:
            tag = tag + ' %s="%s"' % (key, value)

        self.fh.write("<%s/>" % tag)

    def _xml_data_element(self, tag, data, attributes=[]):
        # Write an XML element containing data with optional attributes.
        end_tag = tag

        for key, value in attributes:
            value = self._escape_attributes(value)
            tag = tag + ' %s="%s"' % (key, value)

        data = self._escape_data(data)
        self.fh.write("<%s>%s</%s>" % (tag, data, end_tag))

    def _xml_string_element(self, index, attributes=[]):
        # Optimised tag writer for <c> cell string elements in the inner loop.
        attr = ''

        for key, value in attributes:
            value = self._escape_attributes(value)
            attr = attr + ' %s="%s"' % (key, value)

        self.fh.write("""<c%s t="s"><v>%d</v></c>""" % (attr, index))

    def _xml_si_element(self, string, attributes=[]):
        # Optimised tag writer for shared strings <si> elements.
        attr = ''

        for key, value in attributes:
            value = self._escape_attributes(value)
            attr = attr + ' %s="%s"' % (key, value)

        string = self._escape_data(string)

        self.fh.write("""<si><t%s>%s</t></si>""" % (attr, string))

    def _xml_rich_si_element(self, string):
        # Optimised tag writer for shared strings <si> rich string elements.

        self.fh.write("""<si>%s</si>""" % string)

    def _xml_number_element(self, number, attributes=[]):
        # Optimised tag writer for <c> cell number elements in the inner loop.
        attr = ''

        for key, value in attributes:
            value = self._escape_attributes(value)
            attr = attr + ' %s="%s"' % (key, value)

        self.fh.write("""<c%s><v>%.15g</v></c>""" % (attr, number))

    def _xml_formula_element(self, formula, result, attributes=[]):
        # Optimised tag writer for <c> cell formula elements in the inner loop.
        attr = ''

        for key, value in attributes:
            value = self._escape_attributes(value)
            attr = attr + ' %s="%s"' % (key, value)

        self.fh.write("""<c%s><f>%s</f><v>%s</v></c>"""
                      % (attr, self._escape_data(formula),
                      self._escape_data(str(result))))

    def _xml_inline_string(self, string, preserve, attributes=[]):
        # Optimised tag writer for inlineStr cell elements in the inner loop.
        attr = ''
        t_attr = ''

        # Set the <t> attribute to preserve whitespace.
        if preserve:
            t_attr = ' xml:space="preserve"'

        for key, value in attributes:
            value = self._escape_attributes(value)
            attr = attr + ' %s="%s"' % (key, value)

        string = self._escape_data(string)

        self.fh.write("""<c%s t="inlineStr"><is><t%s>%s</t></is></c>""" %
                      (attr, t_attr, string))

    def _xml_rich_inline_string(self, string, attributes=[]):
        # Optimised tag writer for rich inlineStr in the inner loop.
        attr = ''

        for key, value in attributes:
            value = self._escape_attributes(value)
            attr = attr + ' %s="%s"' % (key, value)

        self.fh.write("""<c%s t="inlineStr"><is>%s</is></c>""" %
                      (attr, string))

    def _escape_attributes(self, attribute):
        # Escape XML characters in attributes.
        try:
            if not self.escapes.search(attribute):
                return attribute
        except TypeError:
            return attribute

        attribute = attribute.replace('&', '&amp;')
        attribute = attribute.replace('"', '&quot;')
        attribute = attribute.replace('<', '&lt;')
        attribute = attribute.replace('>', '&gt;')

        return attribute

    def _escape_data(self, data):
        # Escape XML characters in data sections of tags.  Note, this
        # is different from _escape_attributes() in that double quotes
        # are not escaped by Excel.
        try:
            if not self.escapes.search(data):
                return data
        except TypeError:
            return data

        data = data.replace('&', '&amp;')
        data = data.replace('<', '&lt;')
        data = data.replace('>', '&gt;')

        return data

########NEW FILE########
__FILENAME__ = BasePaths
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import os
import os.path
import sys

from zenmapCore_Kvasir.Name import APP_NAME

def fs_dec(s):
    """Decode s from the filesystem decoding, handling various possible
    errors."""
    enc = sys.getfilesystemencoding()
    if enc is None:
        enc = "UTF-8"
    return s.decode(enc)

def fs_enc(u):
    """Encode u to the filesystem decoding, handling various possible
    errors."""
    enc = sys.getfilesystemencoding()
    if enc is None:
        enc = "UTF-8"
    return u.encode(enc)

# We can't just use os.path.expanduser(u"~") to get a unicode version of the
# home directory, because os.path.expanduser doesn't properly decode the raw
# byte string from the file system encoding. You get a UnicodeDecodeError on
# systems like Windows where the file system encoding is different from the
# result of sys.getdefaultencoding(). So we call os.path.expanduser with a plain
# string and decode it from the filesystem encoding.
HOME = fs_dec(os.path.expanduser("~"))

# The base_paths dict in this file gives symbolic names to various files. For
# example, use base_paths.target_list instead of 'target_list.txt'.

base_paths = dict(user_config_file = APP_NAME + '.conf',
                  user_config_dir = os.path.join(HOME, '.' + APP_NAME),
                  scan_profile = 'scan_profile.usp',
                  profile_editor = 'profile_editor.xml',
                  recent_scans = 'recent_scans.txt',
                  target_list = 'target_list.txt',
                  options = 'options.xml',
                  user_home = HOME,
                  db = APP_NAME + ".db",
                  version = APP_NAME + "_version")

########NEW FILE########
__FILENAME__ = Diff
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import datetime
import os
import subprocess
import sys
import tempfile
import xml.sax

from zenmapCore.Name import APP_NAME
from zenmapCore.NmapParser import NmapParserSAX
from zenmapCore.UmitConf import PathsConfig
from zenmapCore.UmitLogging import log
import zenmapCore.Paths

# The [paths] configuration from zenmap.conf, used to get ndiff_command_path.
paths_config = PathsConfig()

class NdiffParseException(Exception):
    pass

def get_path():
    """Return a value for the PATH environment variable that is appropriate
    for the current platform. It will be the PATH from the environment plus
    possibly some platform-specific directories."""
    path_env = os.getenv("PATH")
    if path_env is None:
        search_paths = []
    else:
        search_paths = path_env.split(os.pathsep)
    for path in zenmapCore.Paths.get_extra_executable_search_paths():
        if path not in search_paths:
            search_paths.append(path)
    return os.pathsep.join(search_paths)

class NdiffCommand(subprocess.Popen):
    def __init__(self, filename_a, filename_b, temporary_filenames = []):
        self.temporary_filenames = temporary_filenames

        search_paths = get_path()
        env = dict(os.environ)
        env["PATH"] = search_paths
        if getattr(sys, "frozen", None) == "macosx_app":
            # These variables are set by py2app, but they can interfere with
            # Ndiff because Ndiff is also a Python application. Without removing
            # these, Ndiff will attempt to run using the py2app-bundled Python
            # library, and may run into version or architecture mismatches.
            if env.has_key("PYTHONPATH"):
                del env["PYTHONPATH"]
            if env.has_key("PYTHONHOME"):
                del env["PYTHONHOME"]

        command_list = [paths_config.ndiff_command_path, "--verbose", "--", filename_a, filename_b]
        self.stdout_file = tempfile.TemporaryFile(mode = "rb", prefix = APP_NAME + "-ndiff-", suffix = ".xml")

        log.debug("Running command: %s" % repr(command_list))
        # See zenmapCore.NmapCommand.py for an explanation of the shell argument.
        subprocess.Popen.__init__(self, command_list, stdout = self.stdout_file, stderr = self.stdout_file, env = env, shell = (sys.platform == "win32"))

    def get_scan_diff(self):
        self.wait()
        self.stdout_file.seek(0)

        return self.stdout_file.read()

    def close(self):
        """Clean up temporary files."""
        self.stdout_file.close()
        for filename in self.temporary_filenames:
            log.debug("Remove temporary diff file %s." % filename)
            os.remove(filename)
        self.temporary_filenames = []

    def kill(self):
        self.close()

def ndiff(scan_a, scan_b):
    """Run Ndiff on two scan results, which may be filenames or NmapParserSAX
    objects, and return a running NdiffCommand object."""
    temporary_filenames = []

    if isinstance(scan_a, NmapParserSAX):
        fd, filename_a = tempfile.mkstemp(prefix = APP_NAME + "-diff-", suffix = ".xml")
        temporary_filenames.append(filename_a)
        f = os.fdopen(fd, "wb")
        scan_a.write_xml(f)
        f.close()
    else:
        filename_a = scan_a

    if isinstance(scan_b, NmapParserSAX):
        fd, filename_b = tempfile.mkstemp(prefix = APP_NAME + "-diff-", suffix = ".xml")
        temporary_filenames.append(filename_b)
        f = os.fdopen(fd, "wb")
        scan_b.write_xml(f)
        f.close()
    else:
        filename_b = scan_b

    return NdiffCommand(filename_a, filename_b, temporary_filenames)

########NEW FILE########
__FILENAME__ = I18N
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import locale
import os
import sys

from zenmapCore_Kvasir.Name import APP_NAME

def get_locales():
    """Get a list of locales to use based on system configuration."""
    locales = []
    # locale.getdefaultlocales already looks at LANG et al. on Unix but not on
    # Windows. We look at the environment variables first to allow overriding
    # the system-wide setting on Windows.
    for envar in ("LANGUAGE", "LC_ALL", "LC_MESSAGES", "LANG"):
        val = os.environ.get(envar)
        if val:
            locales = val.split(":")
            break
    try:
        loc, enc = locale.getdefaultlocale()
        if loc is not None:
            locales.append(loc)
    except ValueError:
        # locale.getdefaultlocale can fail with ValueError on certain locale
        # names; it has been seen with at least en_NG.
        # http://bugs.python.org/issue6895
        pass
    return locales

def install_gettext(locale_dir):
    try:
        locale.setlocale(locale.LC_ALL, '')
    except locale.Error:
        # This can happen if the LANG environment variable is set to something
        # invalid, like LANG=nothing or LANG=en_US/utf8 or LANG=us-ascii.
        # Continue without internationalization.
        pass

    try:
        import gettext
    except ImportError:
        pass
    else:
        t = gettext.translation(APP_NAME, locale_dir, languages = get_locales(), fallback = True)
        t.install(unicode = True)

# Install a dummy _ function so modules can safely use it after importing this
# module, even if they don't install the gettext version.

import __builtin__
__builtin__.__dict__["_"] = lambda s: s

########NEW FILE########
__FILENAME__ = Name
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

# This file contains global definitions of program names. The plain names are
# the usually lower-case program or package names. The display names are
# properly capitalized for use in human-readable sentences.

APP_NAME = "zenmap"
APP_DISPLAY_NAME = "Zenmap"
APP_WEB_SITE = "http://nmap.org/zenmap"
APP_DOWNLOAD_SITE = "http://nmap.org/download.html"
APP_DOCUMENTATION_SITE = "http://nmap.org/docs.html"
APP_COPYRIGHT = "Copyright 2005-2009 Insecure.Com LLC"

NMAP_DISPLAY_NAME = u"Nmap"
NMAP_WEB_SITE = "http://nmap.org"

UMIT_DISPLAY_NAME = "Umit"
UMIT_WEB_SITE = "http://www.umitproject.org/"

########NEW FILE########
__FILENAME__ = NetworkInventory
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import os
import re
import unittest
import zenmapCore
import zenmapCore.NmapParser
from zenmapGUI.SearchGUI import SearchParser
from SearchResult import HostSearch

class NetworkInventory(object):
    """This class acts as a container for aggregated scans. It is also
    responsible for opening/saving the aggregation from/to persistent storage."""
    def __init__(self, filename=None):
        # A list of all scans that make up this inventory
        self.scans = []

        # A dictionary mapping parsed scans to filenames they were loaded from
        self.filenames = {}

        # A dictionary mapping IP addresses into HostInfo objects
        self.hosts = {}

        if filename != None:
            self.open_from_file(filename)

    def add_scan(self, scan, filename=None):
        """Adds a scan to the list of scans. The object passed as an argument
        should be a parsed nmap result."""
        from time import localtime

        for host in scan.get_hosts():
            addr = ""
            if host.ipv6 is not None:
                # This is an IPv6 host, so we add the IPv6 address to the map
                addr = host.ipv6["addr"]
            elif host.ip is not None:
                # IPv4
                addr = host.ip["addr"]

            if addr not in self.hosts:
                # Add this host to the hosts dictionary, mapped by IP address
                self.hosts[addr] = host.make_clone()
            else:
                # This host is already present in the host list, so we need to update its
                # info with the info held in the current host object
                old_host = self.hosts[addr]
                # We need to find old_host's scan date
                old_date = localtime(0)
                for old_scan in self.scans:
                    if old_host in old_scan.get_hosts():
                        old_date = old_scan.get_date()
                new_date = scan.get_date()
                self._update_host_info(old_host, host, old_date, new_date, scan)

        self.scans.append(scan)

        if filename != None:
            basename = os.path.basename(filename)

            if basename in self.filenames.values():
                # We need to generate a new filename, since this basename already exists
                base = basename
                ext = "xml"
                try:
                    base, ext = basename.rsplit(".", 1)
                except ValueError:
                    pass

                counter = 2
                while basename in self.filenames.values():
                    basename = "%s %s.%s" % (base, counter, ext)
                    counter += 1

            self.filenames[scan] = basename

    def remove_scan(self, scan):
        """Removes a scan and any host information it contained from the inventory."""
        # Note: If a scan is passed in that isn't in the inventory then this
        # method will throw a ValueError Exception and will not finish
        # Remove the scan from our scan list
        self.scans.remove(scan)

        # Clear the host dictionary
        self.hosts = {}

        # Remember the scan list
        scans = self.scans

        # Empty it
        self.scans = []

        # Delete the filename entry, if any
        if scan in self.filenames:
            del self.filenames[scan]

        # For each scan in the remembered list, append it to the scan list and update
        # the host list accordingly
        for scan in scans:
            self.add_scan(scan)

    def _update_host_info(self, old_host, new_host, old_date, new_date, new_scan):
        """This function is called when a host needs to be added to the hosts
        dictionary, but another HostInfo object for that host already exists
        in the dictionary (from a previous scan). In that case, we need to
        update the original HostInfo object so that it holds information from
        both scans."""

        # Ports
        old_list = []
        old_list.extend(old_host.ports)
        for new_port in new_host.ports:
            # Check if new_port is already present in old_host's ports
            for old_port in old_host.ports:
                if old_port["portid"] == new_port["portid"] and old_port["protocol"] == new_port["protocol"]:
                    old_list.remove(old_port)
                    # We update old_host's port information to reflect the latest known port state
                    if old_date < new_date:
                        index = old_host.ports.index(old_port)
                        old_host.ports[index] = new_port
                    # Finished processing this new_port, we jump to the next one
                    break
            else:
                # This new_port isn't present in old_host, so we simply append it to
                # old_host's port info
                old_host.ports.append(new_port)
        
        ports = new_scan.get_port_protocol_dict()

        #remove ports which are no longer up
        if old_date < new_date:
            for defunct_port in old_list:
                #Check if defunt_port is in ports and that the protocol matches
                port_number = int(defunct_port['portid'])
                if port_number in ports:
                    if defunct_port['protocol'] in ports[port_number]:
                        old_host.ports.remove(defunct_port)

        # extraports, ipidsequence, state, tcpsequence, tcptssequence, uptime
        if old_date < new_date:
            old_host.extraports = new_host.extraports
            old_host.ipidsequence = new_host.ipidsequence
            old_host.state = new_host.state
            old_host.tcpsequence = new_host.tcpsequence
            old_host.tcptssequence = new_host.tcptssequence
            old_host.uptime = new_host.uptime

        # Comment
        if old_host.comment == "":
            old_host.comment = new_host.comment
        elif new_host.comment != "":
            old_host.comment = "%s\n\n%s" % (old_host.comment, new_host.comment)

        # Hostnames
        # Replace old_host's hostname with new_host's if old_host has no
        # hostname or new_host's is newer.
        if len(new_host.hostnames) > 0 and \
           (len(old_host.hostnames) == 0 or old_date < new_date):
            old_host.hostnames = new_host.hostnames

        # MAC address
        # If there was no MAC address set in old_host, set it to whatever is in new_host.mac.
        # Do the same if both hosts have a MAC address set, but new_host's address is newer.
        if old_host.mac is None or \
           (old_host.mac is not None and new_host.mac is not None and old_date < new_date):
            old_host.mac = new_host.mac

        # OS detection fields
        # Replace old_host's OS detection fields with new_host's if old_host has no
        # OS detection info or new_host's info is newer.
        if len(new_host.osmatches) > 0 and (len(old_host.osmatches) == 0 or old_date < new_date):
            old_host.osmatches = new_host.osmatches
            old_host.ports_used = new_host.ports_used

        # Traceroute information
        if len(new_host.trace) > 0 and (len(old_host.trace) == 0 or old_date < new_date):
            old_host.trace = new_host.trace

    def get_scans(self):
        return self.scans

    def get_hosts(self):
        return self.hosts.values()

    def get_hosts_up(self):
        return filter(lambda h: h.get_state() == 'up', self.hosts.values())

    def get_hosts_down(self):
        return filter(lambda h: h.get_state() == 'down', self.hosts.values())

    def open_from_file(self, path):
        """Loads a scan from the given file."""
        from zenmapCore.NmapParser import NmapParser

        parsed = NmapParser(path)
        parsed.parse()
        self.add_scan(parsed, path)

    def open_from_dir(self, path):
        """Loads all scans from the given directory into the network inventory."""
        from zenmapCore.NmapParser import NmapParser

        for filename in os.listdir(path):
            fullpath = os.path.join(path, filename)
            if os.path.isdir(fullpath):
                continue
            parsed = NmapParser(fullpath)
            parsed.parse()
            self.add_scan(parsed, filename=fullpath)

    def save_to_file(self, path, index, format = "xml"):
        """Saves the scan with the given list index into a file with a given
        path. With format = "xml", saves Nmap XML; otherwise saves plain text
        output."""
        f = open(path, 'w')
        if format == "xml":
            self.get_scans()[index].write_xml(f)
            self.filenames[self.get_scans()[index]] = f
        else:
            self.get_scans()[index].write_text(f)
        f.close()

    def _generate_filenames(self, path):
        """Generates filenames for all scans that don't already have a filename."""
        # The directory must not contain filenames other than those in the self.filenames dictionary
        for filename in os.listdir(path):
            if os.path.basename(filename) not in self.filenames.values():
                raise Exception("The destination directory contains a file (%s) that's not a part "
                                "of the current inventory. The inventory will not be saved." %
                                os.path.basename(filename))

        for scan in self.scans:
            if scan in self.filenames:
                # This scan already has a filename
                continue

            date = "%04d%02d%02d%02d%02d" % (scan.date[0], scan.date[1], scan.date[2],
                                             scan.date[3], scan.date[4])
            filename = scan.get_scan_name()

            # Prepend the date
            filename = "%s %s" % (date, filename)

            # Sanitize the filename
            for char in ["\"", "'", "/", "\\", "?", "*", ":", ";"]:
                if char in filename:
                    filename = filename.replace(char, "_")

            # Filename length check
            # (http://en.wikipedia.org/wiki/Filename#Comparison_of_file_name_limitations)
            if len(filename) > 250:
                filename = filename[:250]

            # TODO: Filename security checks?

            # Try to open the file in append mode. If file.tell() returns a greater-than-zero
            # value, this means that the file already exists and has some data in it, so we
            # choose another filename until we successfully open a zero-length file.
            filename_full = filename + ".xml"
            counter = 2
            while filename_full in self.filenames.values():
                # There's already a scan with this filename, so we generate a new name by appending
                # the counter value before the file extension.
                filename_full = "%s %s.xml" % (filename, str(counter))
                counter += 1

            # Add the filename to the list of saved filenames
            self.filenames[scan] = filename_full

    def save_to_dir(self, path):
        """Saves all scans in the inventory into a given directory and returns
        a list of (full-path) filenames that were used to save the scans."""
        self._generate_filenames(path)

        for scan, filename in self.filenames.iteritems():
            f = open(os.path.join(path, filename), "w")
            scan.write_xml(f)
            f.close()

        return self.filenames.values()

    def open_from_db(self, id):
        pass

    def save_to_db(self):
        # For now, this saves each scan making up the inventory separately in
        # the database.
        from time import time
        from cStringIO import StringIO
        from zenmapCore.UmitDB import Scans

        for parsed in self.get_scans():
            f = StringIO()
            parsed.write_xml(f)

            scan = Scans(scan_name = parsed.scan_name,
                         nmap_xml_output = f.getvalue(),
                         date = time())

class FilteredNetworkInventory(NetworkInventory):
    def __init__(self, filename = None):
        NetworkInventory.__init__(self, filename)

        # A dictionary listing host filtering criteria
        self.search_dict = {}
        self.filtered_hosts = []
        search_keywords = dict()
        search_keywords["target"] = "target"
        search_keywords["t"] = "target"
        search_keywords["inroute"] = "in_route"
        search_keywords["ir"] = "in_route"
        search_keywords["hostname"] = "hostname"
        search_keywords["service"] = "service"
        search_keywords["s"] = "service"
        search_keywords["os"] = "os"
        search_keywords["open"] = "open"
        search_keywords["op"] = "open"
        search_keywords["closed"] = "closed"
        search_keywords["cp"] = "closed"
        search_keywords["filtered"] = "filtered"
        search_keywords["fp"] = "filtered"
        search_keywords["unfiltered"] = "unfiltered"
        search_keywords["ufp"] = "unfiltered"
        search_keywords["open|filtered"] = "open_filtered"
        search_keywords["ofp"] = "open_filtered"
        search_keywords["closed|filtered"] = "closed_filtered"
        search_keywords["cfp"] = "closed_filtered"
        self.search_parser = SearchParser(self, search_keywords)

    # FIXME: This method doesn't do anything.  We just need to support
    # the type of interface that SearchParser expects in order to use it.
    # Perhaps, we will eventually refactor the SearchParser a little bit
    # more?
    def init_search_dirs(self, junk):
        pass

    def get_hosts(self):
        if len(self.search_dict) > 0:
            return self.filtered_hosts
        else:
            return NetworkInventory.get_hosts(self)
    def get_hosts_up(self):
        if len(self.search_dict) > 0:
            return filter(lambda h: h.get_state() == 'up', self.filtered_hosts)
        else:
            return NetworkInventory.get_hosts_up(self)
    def get_hosts_down(self):
        if len(self.search_dict) > 0:
            return filter(lambda h: h.get_state() == 'down', self.filtered_hosts)
        else:
            return NetworkInventory.get_hosts_down(self)
    def get_total_host_count(self):
        return len(self.hosts)
    def _match_all_args(self, host, operator, args):
        """A helper function that calls the matching function for the given
        operator and each of its arguments."""
        for arg in args:
            positive = True
            if arg != "" and arg[0] == "!":
                arg = arg[1:]
                positive = False
            if positive != self.__getattribute__("match_%s" % operator)(host, arg):
                # No match for this operator
                return False
        else:
            # if the operator is not supported, pretend its true
            # All arguments for this operator produced a match
            return True
    def get_host_count(self):
        return len(self.network_inventory.hosts)
    def match_keyword(self, host, keyword):
        return self.match_os(host, keyword) or\
               self.match_target(host, keyword) or\
               self.match_service(host, keyword)
    def match_target(self, host, name):
        return HostSearch.match_target(host, name)
    def match_in_route(self, host, hop):
        hops = host.get_trace().get('hops', [])
        return hop in hops
    def match_hostname(self, host, hostname):
        return HostSearch.match_hostname(host, hostname)
    def match_service(self, host, service):
        return HostSearch.match_service(host, service)
    def match_os(self, host, os):
        return HostSearch.match_os(host, os)
    def match_open(self, host, portno):
        host_ports = host.get_ports()
        return HostSearch.match_port(host_ports, portno, "open")
    def match_closed(self, host, portno):
        host_ports = host.get_ports()
        return HostSearch.match_port(host_ports, portno, "closed")
    def match_filtered(self, host, portno):
        host_ports = host.get_ports()
        return HostSearch.match_port(host_ports, portno, "filtered")
    def match_unfiltered(self, host, portno):
        host_ports = host.get_ports()
        return HostSearch.match_port(host_ports, portno, "unfiltered")
    def match_open_filtered(self, host, portno):
        host_ports = host.get_ports()
        return HostSearch.match_port(host_ports, portno, "open|filtered")
    def match_closed_filtered(self, host, portno):
        host_ports = host.get_ports()
        return HostSearch.match_port(host_ports, portno, "closed|filtered")
    def apply_filter(self, filter_text):
        self.filter_text = filter_text.lower()
        self.search_parser.update(self.filter_text)
        self.filtered_hosts = []
        for hostname, host in self.hosts.iteritems():
            # For each host in this scan
            # Test each given operator against the current host
            for operator, args in self.search_dict.iteritems():
                if not self._match_all_args(host, operator, args):
                    # No match => we discard this scan_result
                    break
            else:
                # All operator-matching functions have returned True, so this
                # host satisfies all conditions
                self.filtered_hosts.append(host)

class NetworkInventoryTest(unittest.TestCase):
    def test_no_external_modification(self):
        """Test that HostInfo objects passed into the inventory are not modified
        during aggregation."""
        scan_1 = zenmapCore.NmapParser.ParserBasics()
        host_a = zenmapCore.NmapParser.HostInfo()
        host_a.hostnames = ["a"]
        host_a.set_state('up')
        scan_1.start = "1000000000"
        scan_1.nmap["hosts"] = [host_a]

        scan_2 = zenmapCore.NmapParser.ParserBasics()
        host_b = zenmapCore.NmapParser.HostInfo()
        host_b.hostnames = ["b"]
        host_b.set_state('up')
        scan_2.start = "1000000001"
        scan_2.nmap["hosts"] = [host_b]

        inv = NetworkInventory()
        inv.add_scan(scan_1)
        inv.add_scan(scan_2)

        self.assertEqual(host_a.hostnames, ["a"])
        self.assertEqual(host_b.hostnames, ["b"])
        self.assertEqual(scan_1.nmap["hosts"], [host_a])
        self.assertEqual(scan_2.nmap["hosts"], [host_b])
        self.assertEqual(inv.get_hosts_up()[0].hostnames, ["b"])
    def test_cancel_and_remove_scan(self):
        """Test that canceling and removing a scan does not blow away the inventory hosts"""
        added_ips = ['10.0.0.1','10.0.0.2']
        removed_ips = ['10.0.0.3']
        scan_1 = zenmapCore.NmapParser.ParserBasics()
        host_a = zenmapCore.NmapParser.HostInfo()
        host_a.hostnames = ["a"]
        host_a.set_ip({'addr':added_ips[0]})
        scan_1.start = "1000000000"
        scan_1.nmap["hosts"] = [host_a]

        scan_2 = zenmapCore.NmapParser.ParserBasics()
        host_b = zenmapCore.NmapParser.HostInfo()
        host_b.hostnames = ["b"]
        host_b.set_ip({'addr':added_ips[1]})
        scan_2.start = "1000000001"
        scan_2.nmap["hosts"] = [host_b]

        scan_3 = zenmapCore.NmapParser.ParserBasics()
        host_c = zenmapCore.NmapParser.HostInfo()
        host_c.hostnames = ["b"]
        host_c.set_ip({'addr':removed_ips[0]})
        scan_3.start = "1000000001"
        scan_3.nmap["hosts"] = [host_c]

        inv = NetworkInventory()
        inv.add_scan(scan_1)
        inv.add_scan(scan_2)
        try:
            inv.remove_scan(scan_3)
        except:
            pass
        self.assertEqual(added_ips, inv.hosts.keys())
        self.assertEqual(host_a.hostnames, ["a"])
        self.assertEqual(host_b.hostnames, ["b"])

class FilteredNetworkInventoryTest(unittest.TestCase):
    def test_filter(self):
        """Test that the filter still works after moving code to the """
        """HostSearch class"""
        from zenmapCore.NmapParser import NmapParser
        inv = FilteredNetworkInventory()
        scan = NmapParser()
        scan.parse_file("test/xml_test9.xml")
        filter_text = "open:22 os:linux service:openssh"
        inv.add_scan(scan)
        inv.apply_filter(filter_text)
        assert(len(inv.get_hosts()) == 2)

class PortChangeTest(unittest.TestCase):
    def test_port(self):
        """Verify that the port status (open/filtered/closed) is diplayed """ \
        """correctly when the port status changes in newer scans"""
        from zenmapCore.NmapParser import NmapParser
        inv = NetworkInventory()
        scan1 = NmapParser()
        scan1.parse_file("test/xml_test13.xml")
        inv.add_scan(scan1)
        scan2 = NmapParser()
        scan2.parse_file("test/xml_test14.xml")
        inv.add_scan(scan2)
        assert(len(inv.get_hosts()[0].ports) == 2)
        scan3 = NmapParser()
        scan3.parse_file("test/xml_test15.xml")
        inv.add_scan(scan3)
        assert(len(inv.get_hosts()[0].ports) == 0)

        # Additional test case for when the two scans have port scan ranges
        # which do not overlap. Example nmap -F -sU versus 
        # nmap -F scanme.nmap.org 
        inv = NetworkInventory()
        scan4 = NmapParser()
        scan4.parse_file("test/xml_test16.xml")
        inv.add_scan(scan4)
        assert(len(inv.get_hosts()[0].ports)==3)
        scan5 = NmapParser()
        scan5.parse_file("test/xml_test17.xml")
        inv.add_scan(scan5)
        assert(len(inv.get_hosts()[0].ports)==7)

if __name__ == "__main__":
    unittest.main()
    if False:

        scan1 = NmapParser("/home/ndwi/scanz/neobee_1.xml")
        scan1.parse()
        scan2 = NmapParser("/home/ndwi/scanz/scanme_nmap_org.usr")
        scan2.parse()

        inventory1 = NetworkInventory()
        inventory1.add_scan(scan1)
        inventory1.add_scan(scan2)

        for host in inventory1.get_hosts():
            print "%s" % host.ip["addr"],
            #if len(host.hostnames) > 0:
            #    print "[%s]:" % host.hostnames[0]["hostname"]
            #else:
            #    print ":"
            #for port in host.ports:
            #    print "  %s: %s" % (port["portid"], port["port_state"])
            #print "  OS matches: %s" % host.osmatches
            #print "  Ports used: %s" % host.ports_used
            #print "  Trace: %s" % host.trace
            #if "hops" in host.trace:
            #    print "         (%d)" % len(host.trace["hops"])

        inventory1.remove_scan(scan2)
        print
        for host in inventory1.get_hosts():
            print "%s" % host.ip["addr"],

        inventory1.add_scan(scan2)
        print
        for host in inventory1.get_hosts():
            print "%s" % host.ip["addr"],

        dir = "/home/ndwi/scanz/top01"
        inventory1.save_to_dir(dir)

        inventory2 = NetworkInventory()
        inventory2.open_from_dir(dir)

        print
        for host in inventory2.get_hosts():
            print "%s" % host.ip["addr"],

########NEW FILE########
__FILENAME__ = NmapCommand
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

# This file contains the definitions of the NmapCommand class, which represents
# and runs an Nmap command line.

import codecs
import errno
import locale
import sys
import os
import re
import tempfile
import unittest

import zenmapCore_Kvasir.I18N

from types import StringTypes
try:
    import subprocess
except ImportError, e:
    raise ImportError(str(e) + ".\n" + _("Python 2.4 or later is required."))

# Removed Path(s) dependency to support a Kvasir configurable item
#import zenmapCore_Kvasir.Paths
#from zenmapCore_Kvasir.Paths import Path
from zenmapCore_Kvasir.NmapOptions import NmapOptions, split_quoted, join_quoted
from zenmapCore_Kvasir.UmitLogging import log
#from zenmapCore.UmitConf import PathsConfig
from zenmapCore_Kvasir.Name import APP_NAME


# The [paths] configuration from zenmap.conf, used to get nmap_command_path.
# This is not needed for Kvasir when set via config_item
#paths_config = PathsConfig()

log.debug(">>> Platform: %s" % sys.platform)

def wrap_file_in_preferred_encoding(f):
    """Wrap an open file to automatically decode its contents when reading from
    the encoding given by locale.getpreferredencoding, or just return the file
    if that doesn't work.

    The nmap executable will write its output in whatever the system encoding
    is. Nmap's output is usually all ASCII, but time zone it prints can be in a
    different encoding. If it is not decoded correctly it will be displayed as
    garbage characters. This function assists in reading the Nmap output. We
    don't know for sure what the encoding used is, but we take a best guess and
    decode the output into a proper unicode object so that the screen display
    and XML writer interpret it correctly."""

    try:
        preferredencoding = locale.getpreferredencoding()
    except locale.Error:
        # This can happen if the LANG environment variable is set to something
        # weird.
        preferredencoding = None

    if preferredencoding is not None:
        try:
            reader = codecs.getreader(preferredencoding)
            return reader(f, "replace")
        except LookupError:
            # The lookup failed. This can happen if the preferred encoding is
            # unknown ("X-MAC-KOREAN" has been observed). Ignore it and return
            # the unwrapped file.
            log.debug("Unknown encoding \"%s\"." % preferredencoding)

    return f

def escape_nmap_filename(filename):
    """Escape '%' characters so they are not interpreted as strftime format
    specifiers, which are not supported by Zenmap."""
    return filename.replace("%", "%%")

class NmapCommand(object):
    """This class represents an Nmap command line. It is responsible for
    starting, stopping, and returning the results from a command-line scan. A
    command line is represented as a string but it is split into a list of
    arguments for execution.
    
    The normal output (stdout and stderr) are written to the file object
    self.stdout_file."""

    def __init__(self, command):
        """Initialize an Nmap command. This creates temporary files for
        redirecting the various types of output and sets the backing
        command-line string."""
        self.command = command
        self.command_process = None

        self.stdout_file = None

        self.ops = NmapOptions()
        self.ops.parse_string(command)
        # Replace the executable name with the value of nmap_command_path.
        #self.ops.executable = paths_config.nmap_command_path

        # Normally we generate a random temporary filename to save XML output
        # to. If we find -oX or -oA, the user has chosen his own output file.
        # Set self.xml_is_temp to False and don't delete the file when we're
        # done.
        self.xml_is_temp = True
        self.xml_output_filename = None
        if self.ops["-oX"]:
            self.xml_is_temp = False
            self.xml_output_filename = self.ops["-oX"]
        if self.ops["-oA"]:
            self.xml_is_temp = False
            self.xml_output_filename = self.ops["-oA"] + ".xml"

        # Escape '%' to avoid strftime expansion.
        for op in ("-oA", "-oX", "-oG", "-oN", "-oS"):
            if self.ops[op]:
                self.ops[op] = escape_nmap_filename(self.ops[op])

        if self.xml_is_temp:
            self.xml_output_filename = tempfile.mktemp(prefix = APP_NAME + "-", suffix = ".xml")
            self.ops["-oX"] = escape_nmap_filename(self.xml_output_filename)

        log.debug(">>> Temporary files:")
        log.debug(">>> XML OUTPUT: %s" % self.xml_output_filename)

    def close(self):
        """Close and remove temporary output files used by the command."""
        self.stdout_file.close()
        if self.xml_is_temp:
            try:
                os.remove(self.xml_output_filename)
            except OSError, e:
                if e.errno != errno.ENOENT:
                    raise

    def kill(self):
        """Kill the nmap subprocess."""
        log.debug(">>> Killing scan process %s" % self.command_process.pid)

        if sys.platform != "win32":
            try:
                from signal import SIGKILL
                os.kill(self.command_process.pid, SIGKILL)
                self.command_process.wait()
            except:
                pass
        else:
            try:
                import ctypes
                ctypes.windll.kernel32.TerminateProcess(int(self.command_process._handle), -1)
            except:
                pass

    def get_path(self):
        """Return a value for the PATH environment variable that is appropriate
        for the current platform. It will be the PATH from the environment plus
        possibly some platform-specific directories."""
        path_env = os.getenv("PATH")
        if path_env is None:
            search_paths = []
        else:
            search_paths = path_env.split(os.pathsep)
        #this is not necessary for Kvasir
        #for path in zenmapCore_Kvasir.Paths.get_extra_executable_search_paths():
        #    if path not in search_paths:
        #        search_paths.append(path)
        return os.pathsep.join(search_paths)

    def run_scan(self, stderr = None):
        """Run the command represented by this class."""

        # We don't need a file name for stdout output, just a handle. A
        # TemporaryFile is deleted as soon as it is closed, and in Unix is
        # unlinked immediately after creation so it's not even visible.
        f = tempfile.TemporaryFile(mode = "rb", prefix = APP_NAME + "-stdout-")
        self.stdout_file = wrap_file_in_preferred_encoding(f)
        if stderr is None:
            stderr = f

        search_paths = self.get_path()
        env = dict(os.environ)
        env["PATH"] = search_paths
        log.debug("PATH=%s" % env["PATH"])

        command_list = self.ops.render()
        log.debug("Running command: %s" % repr(command_list))

        startupinfo = None
        if sys.platform == "win32":
            # This keeps a terminal window from opening.
            startupinfo = subprocess.STARTUPINFO()
            try:
                startupinfo.dwFlags |= subprocess._subprocess.STARTF_USESHOWWINDOW
            except AttributeError:
                # This name is used before Python 2.6.5.
                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

        self.command_process = subprocess.Popen(command_list, bufsize=1,
                                     stdin=subprocess.PIPE,
                                     stdout=f,
                                     stderr=stderr,
                                     startupinfo = startupinfo,
                                     env=env)

    def scan_state(self):
        """Return the current state of a running scan. A return value of True
        means the scan is running and a return value of False means the scan
        subprocess completed successfully. If the subprocess terminated with an
        error an exception is raised. The scan must have been started with
        run_scan before calling this method."""
        if self.command_process == None:
            raise Exception("Scan is not running yet!")

        state = self.command_process.poll()

        if state == None:
            return True # True means that the process is still running
        elif state == 0:
            return False # False means that the process had a successful exit
        else:
            log.warning("An error occurred during the scan execution!")
            log.warning("Command that raised the exception: '%s'" % self.ops.render_string())
            log.warning("Scan output:\n%s" % self.get_output())

            raise Exception("An error occurred during the scan execution!\n\n'%s'" % self.get_output())

    def get_output(self):
        """Return the complete contents of the self.stdout_file. This modifies
        the file pointer."""
        self.stdout_file.seek(0)
        return self.stdout_file.read()

    def get_xml_output_filename(self):
        """Return the name of the XML (-oX) output file."""
        return self.xml_output_filename

if __name__ == '__main__':
    unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(SplitQuotedTest))

########NEW FILE########
__FILENAME__ = NmapOptions
#!/usr/bin/env python

# This is an Nmap command line parser. It has two main parts:
#
#   getopt_long_only_extras, which is like getopt_long_only with robust handling
#   of unknown options.
#
#   NmapOptions, a class representing a set of Nmap options.
#
# NmapOptions is the class for external use. NmapOptions.parse parses a list of
# a command followed by command-line arguments. NmapOptions.render returns a
# list of of a command followed by arguments. NmapOptions.parse_string and
# NmapOptions.render_string first split strings into lists, following certain
# quoting rules.
#
# >>> ops = NmapOptions()
# >>> ops.parse(["nmap", "-v", "--script", "safe", "localhost"])
# >>> ops.executable
# 'nmap'
# >>> ops.target_specs
# ['localhost']
# >>> ops["-v"]
# 1
# >>> ops["--script"]
# 'safe'
#
# The command line may be modified by accessing member variables:
#
# >>> ops.executable = "C:\Program Files\Nmap\nmap.exe"
# >>> ops["-v"] = 2
# >>> ops["-oX"] = "output.xml"
# >>> ops.render()
# ['C:\\Program Files\\Nmap\\nmap.exe', '-v', '-v', '-oX', 'output.xml', '--script', 'safe', 'localhost']
# >>> ops.render_string()
# '"C:\\Program Files\\Nmap\\nmap.exe" -v -v -oX output.xml --script safe localhost'
#
# A primary design consideration was robust handling of unknown options. That
# gives this code a degree of independence from Nmap's own list of options. If
# an option is added to Nmap but not added here, that option is treated as an
# "extra," an uninterpreted string that is inserted verbatim into the option
# list. Because the unknown option may or may not take an argument, pains are
# taken to avoid interpreting any option ambiguously.
#
# Consider the following case, where -x is an unknown option:
#   nmap -x -e eth0 scanme.nmap.org
# If -x, whatever it is, does not take an argument, it is equivalent to
#   nmap -e eth0 scanme.nmap.org -x
# that is, a scan of scanme.nmap.org over interface eth0. But if it does take an
# argument, its argument is "-e", and the command line is the same as
#   nmap eth0 scanme.nmap.org -x -e
# which is a scan of the two hosts eth0 and scanme.nmap.org, over the default
# interface. In either case scanme.nmap.org is a target but the other arguments
# are ambiguous. To resolve this, once an unknown option is found, all following
# arguments that can be interpreted ambiguously are removed with it and placed
# in the extras, with normal option processing resumed only when there is no
# more ambiguity. This ensures that such options maintain their relative order
# when rendered again to output. In this example "-x -e eth0" will always appear
# in that order, and the -e option will be uninterpreted.
#
# To add a new option, one should do the following:
# 1) Add a test case to the NmapOptionsTest::test_options() method for the new
#    option and make sure it initially fails.
# 2) Add the new option to NmapOptions.SHORT_OPTIONS and/or
#    NmapOptions.LONG_OPTIONS.
# 3) Add an appropriate case to NmapOptions::handle_result(). This should
#    include a line something like
#      self[opt] = True
#    or, if the option has an argument 'arg':
#      self[opt] = arg
# 4) Add an appropriate case to NmapOptions::render()
#    This should include a check to make sure the option was set in
#    handle_result:
#      if self[opt]:
#    or, if self[opt] contains arguments
#      if self[opt] is not None:
#    If the check passed, then opt should be added to opt_list.
# 5) Edit profile_editor.xml to display the new option in the GUI.
# 6) Depending on the option, one may need to edit
#    get_option_check_auxiliary_widget in OptionBuilder.py.
# 7) Make sure the test case works now.

class option:
    """A single option, part of a pool of potential options. It's just a name
    and a flag saying if the option takes no argument, if an agument is
    optional, or if an argument is required."""
    NO_ARGUMENT = 0
    REQUIRED_ARGUMENT = 1
    OPTIONAL_ARGUMENT = 2

    def __init__(self, name, has_arg):
        self.name = name
        self.has_arg = has_arg

def split_quoted(s):
    """Like str.split, except that no splits occur inside quoted strings, and
    quoted strings are unquoted."""
    r = []
    i = 0
    while i < len(s) and s[i].isspace():
        i += 1
    while i < len(s):
        part = []
        while i < len(s) and not s[i].isspace():
            c = s[i]
            if c == "\"" or c == "'":
                begin = c
                i += 1
                while i < len(s):
                    c = s[i]
                    if c == begin:
                        i += 1
                        break
                    elif c == "\\":
                        i += 1
                        if i < len(s):
                            c = s[i]
                        # Otherwise, ignore the error and leave the backslash at
                        # the end of the string.
                    part.append(c)
                    i += 1
            else:
                part.append(c)
                i += 1
        r.append("".join(part))
        while i < len(s) and s[i].isspace():
            i += 1

    return r

def maybe_quote(s):
    """Return s quoted if it needs to be, otherwise unchanged."""
    for c in s:
        if c == "\"" or c == "\\" or c == "'" or c.isspace():
            break
    else:
        return s

    r = []
    for c in s:
        if c == "\"":
            r.append("\\\"")
        elif c == "\\":
            r.append("\\\\")
        else:
            r.append(c)

    return "\"" + "".join(r) + "\""

def join_quoted(l):
    return " ".join([maybe_quote(x) for x in l])

def make_options(short_opts, long_opts):
    """Parse a short option specification string and long option tuples into a
    list of option objects."""
    options = []
    for name, has_arg in long_opts:
        options.append(option(name, has_arg))

    while len(short_opts) > 0:
        name = short_opts[0]
        short_opts = short_opts[1:]
        assert name != ":"
        num_colons = 0
        while len(short_opts) > 0 and short_opts[0] == ":":
            short_opts = short_opts[1:]
            num_colons += 1
        if num_colons == 0:
            has_arg = option.NO_ARGUMENT
        elif num_colons == 1:
            has_arg = option.REQUIRED_ARGUMENT
        else:
            has_arg = option.OPTIONAL_ARGUMENT
        options.append(option(name, has_arg))

    return options

lookup_option_cache = {}

def lookup_option(name, options):
    """Find an option with the given (possibly abbreviated) name. None is
    returned if no options match or if the name is ambiguous (more than one
    option matches with no exact match)."""

    # This function turns out to be a huge bottleneck. Therefore we memoize it.
    # We hash on the option name and the id of the options list, because lists
    # aren't hashable. This means that the options list can't change after the
    # first time you call this function, or you will get stale results. Turning
    # the list into a tuple and hashing that is too slow.
    cache_code = (name, id(options))
    try:
        return lookup_option_cache[cache_code]
    except KeyError:
        pass

    # Nmap treats '_' the same as '-' in long option names.
    def canonicalize_name(name):
        return name.replace("_", "-")

    name = canonicalize_name(name)
    matches = [o for o in options if canonicalize_name(o.name).startswith(name)]
    if len(matches) == 0:
        # No match.
        lookup_option_cache[cache_code] = None
    elif len(matches) == 1:
        # Only one match--not an ambiguous abbreviation.
        lookup_option_cache[cache_code] = matches[0]
    else:
        # More than one match--return only an exact match.
        for match in matches:
            if canonicalize_name(match.name) == name:
                lookup_option_cache[cache_code] = match
                break
        else:
            # No exact matches
            lookup_option_cache[cache_code] = None
    return lookup_option_cache[cache_code]

def split_option(cmd_arg, options):
    """Split an option into a name, argument (if any), and possible remainder.
    It is not an error if the option does not include an argument even though it
    is required; the caller must take the argument from the next command-line
    argument. The remainder is what's left over after stripping a single short
    option that doesn't take an argument. At most one of argument and remainder
    will be non-None.
    Examples:
    >>> split_option("-v", [option("v", option.NO_ARGUMENT)])
    ('v', None, None)
    >>> split_option("--min-rate", [option("min-rate", option.REQUIRED_ARGUMENT)])
    ('min-rate', None, None)
    >>> split_option("--min-rate=100", [option("min-rate", option.REQUIRED_ARGUMENT)])
    ('min-rate', '100', None)
    >>> split_option("-d9", [option("d", option.OPTIONAL_ARGUMENT)])
    ('d', '9', None)
    >>> split_option("-AFn", [option("A", option.NO_ARGUMENT)])
    ('A', None, '-Fn')
    >>> split_option("-Amin-rate", [option("A", option.NO_ARGUMENT)])
    ('A', None, '-min-rate')
    """
    if cmd_arg.startswith("--"):
        name = cmd_arg[2:]
        index = name.find('=')
        if index < 0:
            arg = None
        else:
            name, arg = name[:index], name[index + 1:]
        return name, arg, None
    elif cmd_arg.startswith("-"):
        name = cmd_arg[1:]
        # Check for a lone -.
        if name == "":
            return name, None, None
        # First see if it's really a long option (or a single short option).
        index = name.find('=')
        if index < 0:
            arg = None
        else:
            name, arg = name[:index], name[index + 1:]
        if lookup_option(name, options) is not None:
            return name, arg, None
        # No luck. Must be a short option.
        name = cmd_arg[1]
        option = lookup_option(name, options)
        if option is None:
            # An unknown short option. Return the whole thing.
            return cmd_arg[1:], None, None
        rest = cmd_arg[2:]
        if rest == "":
            return name, None, None
        if option.has_arg == option.NO_ARGUMENT:
            return name, None, "-" + rest
        else:
            return name, rest, None
    else:
        assert False, cmd_arg

def get_option(cmd_args, options):
    """Find and return the first option (plus a possible option argument) or
    positional argument from the command-line option list in cmd_args. The
    return value will have one of the following forms:
    a string, representing a positional argument;
    an (option, argument) pair (argument may be None);
    a (None, extra, ...) tuple, where extra, ... is a chain of an unknown option
        and its following arguments that cannot be interpreted unambiguously; or
    None, at the end of the option list."""
    if len(cmd_args) == 0:
        return None
    cmd_arg = cmd_args.pop(0)
    if cmd_arg == "--":
        if len(cmd_args) == 0:
            return None
        # Grab the positional argument and replace the --.
        name = cmd_args[0]
        cmd_args[0] = "--"
        return name
    # A normal positional argument.
    if not cmd_arg.startswith("-"):
        return cmd_arg
    name, arg, remainder = split_option(cmd_arg, options)
    if remainder is not None:
        cmd_args.insert(0, remainder)
    option = lookup_option(name, options)
    if option is None:
        # Unrecognized option.
        if arg is not None:
            return (None, cmd_arg)
        else:
            extras = [None, cmd_arg]
            # We found an unknown option but we have a problem--we don't know if
            # it takes an argument or not. So what we do is, we simulate what
            # would happen both if the option took and argument and if it
            # didn't. The sync function does that by calling this function in a
            # loop.
            rest = sync(cmd_args[1:], cmd_args[:], options)
            # rest is the part of the argument list that is the same whether or
            # not the unknown option takes an argument. Put everything up until
            # rest begins in the extras, then set cmd_args to rest.
            extras += cmd_args[0:len(cmd_args) - len(rest)]
            del cmd_args[0:len(cmd_args) - len(rest)]
            return tuple(extras)
    elif option.has_arg == option.NO_ARGUMENT and arg is not None:
        # It has an arg but it shouldn't (like --send-ip=5). Treat it as
        # an extra.
        return (None, cmd_arg)
    elif option.has_arg == option.REQUIRED_ARGUMENT and arg is None:
        # An argument is required but not yet read.
        if len(cmd_args) == 0:
            # No more args. Treat it as an extra.
            return (None, cmd_arg)
        else:
            arg = cmd_args.pop(0)
            return (option.name, arg)
    else:
        return (option.name, arg)

def sync(a, b, options):
    """Given two command-line argument lists, incrementally get an option from
    whichever is longer until both lists are equal. Return the resulting
    list."""
    while a != b:
        if len(a) > len(b):
            get_option(a, options)
        else:
            get_option(b, options)
    return a

def getopt_long_only_extras(cmd_args, short_opts, long_opts):
    """This is a generator version of getopt_long_only that additionally has
    robust handling of unknown options. Each of the items in the sequence it
    yields will be one of the following:
    a string, representing a positional argument;
    an (option, argument) pair (argument may be None);
    a (None, extra, ...) tuple, where extra, ... is a chain of an unknown option
        and its following arguments that cannot be interpreted unambiguously; or
    None, at the end of the option list."""
    options = make_options(short_opts, long_opts)
    # get_option modifies its list of arguments in place. Don't modify the
    # original list.
    cmd_args_copy = cmd_args[:]
    while True:
        result = get_option(cmd_args_copy, options)
        if result is None:
            break
        yield result

class NmapOptions(object):
    SHORT_OPTIONS = "6Ab:D:d::e:Ffg:hi:M:m:nO::o:P:p:RrS:s:T:v::V"
    LONG_OPTIONS = (
        ("allports", option.NO_ARGUMENT),
        ("append-output", option.NO_ARGUMENT),
        ("badsum", option.NO_ARGUMENT),
        ("data-length", option.REQUIRED_ARGUMENT),
        ("datadir", option.REQUIRED_ARGUMENT),
        ("debug", option.OPTIONAL_ARGUMENT),
        ("defeat-rst-ratelimit", option.NO_ARGUMENT),
        ("dns-servers", option.REQUIRED_ARGUMENT),
        ("exclude", option.REQUIRED_ARGUMENT),
        ("excludefile", option.REQUIRED_ARGUMENT),
        ("fuzzy", option.NO_ARGUMENT),
        ("help", option.NO_ARGUMENT),
        ("host-timeout", option.REQUIRED_ARGUMENT),
        ("iL", option.REQUIRED_ARGUMENT),
        ("iR", option.REQUIRED_ARGUMENT),
        ("iflist", option.NO_ARGUMENT),
        ("initial-rtt-timeout", option.REQUIRED_ARGUMENT),
        ("ip-options", option.REQUIRED_ARGUMENT),
        ("log-errors", option.NO_ARGUMENT),
        ("max-hostgroup", option.REQUIRED_ARGUMENT),
        ("max-os-tries", option.REQUIRED_ARGUMENT),
        ("max-parallelism", option.REQUIRED_ARGUMENT),
        ("max-rate", option.REQUIRED_ARGUMENT),
        ("max-retries", option.REQUIRED_ARGUMENT),
        ("max-rtt-timeout", option.REQUIRED_ARGUMENT),
        ("max-scan-delay", option.REQUIRED_ARGUMENT),
        ("min-hostgroup", option.REQUIRED_ARGUMENT),
        ("min-parallelism", option.REQUIRED_ARGUMENT),
        ("min-rate", option.REQUIRED_ARGUMENT),
        ("min-retries", option.REQUIRED_ARGUMENT),
        ("min-rtt-timeout", option.REQUIRED_ARGUMENT),
        ("mtu", option.REQUIRED_ARGUMENT),
        ("no-stylesheet", option.NO_ARGUMENT),
        ("oA", option.REQUIRED_ARGUMENT),
        ("oG", option.REQUIRED_ARGUMENT),
        ("oM", option.REQUIRED_ARGUMENT),
        ("oN", option.REQUIRED_ARGUMENT),
        ("oS", option.REQUIRED_ARGUMENT),
        ("oX", option.REQUIRED_ARGUMENT),
        ("open", option.NO_ARGUMENT),
        ("osscan-guess", option.NO_ARGUMENT),
        ("osscan-limit", option.NO_ARGUMENT),
        ("packet-trace", option.NO_ARGUMENT),
        ("port-ratio", option.REQUIRED_ARGUMENT),
        ("privileged", option.NO_ARGUMENT),
        ("randomize-hosts", option.NO_ARGUMENT),
        ("reason", option.NO_ARGUMENT),
        ("release-memory", option.NO_ARGUMENT),
        ("scan-delay", option.REQUIRED_ARGUMENT),
        ("scanflags", option.REQUIRED_ARGUMENT),
        ("sI", option.REQUIRED_ARGUMENT),
        ("script", option.REQUIRED_ARGUMENT),
        ("script-args", option.REQUIRED_ARGUMENT),
        ("script-trace", option.NO_ARGUMENT),
        ("script-updatedb", option.NO_ARGUMENT),
        ("script-help", option.REQUIRED_ARGUMENT),
        ("send-eth", option.NO_ARGUMENT),
        ("send-ip", option.NO_ARGUMENT),
        ("servicedb", option.REQUIRED_ARGUMENT),
        ("source-port", option.REQUIRED_ARGUMENT),
        ("spoof-mac", option.REQUIRED_ARGUMENT),
        ("stylesheet", option.REQUIRED_ARGUMENT),
        ("system-dns", option.NO_ARGUMENT),
        ("timing", option.REQUIRED_ARGUMENT),
        ("top-ports", option.REQUIRED_ARGUMENT),
        ("traceroute", option.NO_ARGUMENT),
        ("ttl", option.REQUIRED_ARGUMENT),
        ("unprivileged", option.NO_ARGUMENT),
        ("verbose", option.OPTIONAL_ARGUMENT),
        ("version", option.NO_ARGUMENT),
        ("version-all", option.NO_ARGUMENT),
        ("version-intensity", option.REQUIRED_ARGUMENT),
        ("version-light", option.NO_ARGUMENT),
        ("version-trace", option.NO_ARGUMENT),
        ("versiondb", option.REQUIRED_ARGUMENT),
        ("webxml", option.NO_ARGUMENT),
    )

    # Sets of options that should be treated as equivalent from the point of
    # view of the external interface. For example, ops["--timing"] means the
    # same thing as ops["-T"].
    EQUIVALENT_OPTIONS = (
        ("debug", "d"),
        ("help", "h"),
        ("iL", "i"),
        ("max-parallelism", "M"),
        ("osscan-guess", "fuzzy"),
        ("oG", "oM", "m"),
        ("oN", "o"),
        ("sP", "sn"),
        ("P", "PE", "PI"),
        ("PA", "PT"),
        ("P0", "PD", "PN", "Pn"),
        ("rH", "randomize-hosts"),
        ("source-port", "g"),
        ("timing", "T"),
        ("verbose", "v"),
        ("version", "V"),
    )
    EQUIVALENCE_MAP = {}
    for set in EQUIVALENT_OPTIONS:
        base = set[0]
        aliases = set[1:]
        for alias in aliases:
            EQUIVALENCE_MAP[alias] = base

    TIMING_PROFILE_NAMES = {
        "paranoid": 0, "sneaky": 1, "polite": 2,
        "normal": 3, "aggressive": 4, "insane": 5
    }

    def __init__(self):
        self.options = make_options(self.SHORT_OPTIONS, self.LONG_OPTIONS)

        self.clear()

    def clear(self):
        self._executable = None
        self.target_specs = []
        self.extras = []

        # This is the internal mapping of option names to values.
        self.d = {}

    def _set_executable(self, executable):
        self._executable = executable

    executable = property(lambda self: self._executable or "nmap", _set_executable)

    def canonicalize_name(self, name):
        opt, arg, remainder = split_option(name, self.options)
        assert remainder == None
        if arg is None:
            option = lookup_option(opt, self.options)
            if option:
                option = option.name
            else:
                option = opt
        else:
            option = name.lstrip("-")
        option = NmapOptions.EQUIVALENCE_MAP.get(option, option)
        return option

    def __getitem__(self, key):
        return self.d.get(self.canonicalize_name(key))

    def __setitem__(self, key, value):
        self.d[self.canonicalize_name(key)] = value

    def setdefault(self, key, default):
        return self.d.setdefault(self.canonicalize_name(key), default)

    def handle_result(self, result):
        if isinstance(result, basestring):
            # A positional argument.
            self.target_specs.append(result)
            return
        elif result[0] == None:
            # An unknown option.
            self.extras.extend(result[1:])
            return

        # A normal option.
        opt, arg = result
        if opt in ("6", "A", "F", "h", "n", "R", "r", "V"):
            self["-" + opt] = True
        elif opt in (\
            "allports",
            "append-output",
            "badsum",
            "defeat-rst-ratelimit",
            "fuzzy",
            "help",
            "iflist",
            "log-errors",
            "no-stylesheet",
            "open",
            "osscan-guess",
            "osscan-limit",
            "packet-trace",
            "privileged",
            "randomize-hosts",
            "reason",
            "release-memory",
            "script-trace",
            "script-updatedb",
            "send-eth",
            "send-ip",
            "system-dns",
            "traceroute",
            "unprivileged",
            "version",
            "version-all",
            "version-light",
            "version-trace",
            "webxml",
            ):
            self["--" + opt] = True
        elif opt in ("b", "D", "e", "g", "i", "iL", "m", "M", "o", "oA", "oG", "oM", "oN", "oS", "oX", "p", "S", "sI"):
            assert arg is not None
            if self["-" + opt] is None:
                self["-" + opt] = arg
            else:
                self.extras.extend(("-" + opt, arg))
        elif opt in (\
            "datadir",
            "data-length",
            "dns-servers",
            "exclude",
            "excludefile",
            "host-timeout",
            "initial-rtt-timeout",
            "ip-options",
            "max-hostgroup",
            "max-os-tries",
            "max-parallelism",
            "max-rate",
            "max-retries",
            "max-rtt-timeout",
            "max-scan-delay",
            "min-hostgroup",
            "min-parallelism",
            "min-rate",
            "min-retries",
            "min-rtt-timeout",
            "mtu",
            "port-ratio",
            "scan-delay",
            "scanflags",
            "script",
            "script-args",
            "script-help",
            "servicedb",
            "source-port",
            "spoof-mac",
            "stylesheet",
            "top-ports",
            "ttl",
            "versiondb",
            "version-intensity",
            ):
            assert arg is not None
            if self["--" + opt] is None:
                self["--" + opt] = arg
            else:
                self.extras.extend(("--" + opt, arg))
        elif opt == "d" or opt == "debug":
            if arg is None:
                arg = ""
            try:
                self["-d"] = int(arg)
            except ValueError:
                if reduce(lambda x, y: x and y, map(lambda z: z == "d", arg), True):
                    self.setdefault("-d", 0)
                    self["-d"] += len(arg) + 1
                else:
                    self.extras.append("-d%s" % arg)
        elif opt == "f":
            self.setdefault("-f", 0)
            self["-f"] += 1
        elif opt == "iR":
            if self["-iR"] is None:
                try:
                    self["-iR"] = int(arg)
                except ValueError:
                    self.extras.extend(("-iR", arg))
            else:
                self.extras.extend(("-iR", arg))
        elif opt == "O":
            if arg is None:
                if self["-O"] is None:
                    self["-O"] = True
                else:
                    self.extras.append("-O")
            else:
                if self["-O"] is None:
                    self["-O"] = arg
                else:
                    self.extras.append("-O%s" % arg)
        elif opt == "P":
            type, ports = arg[:1], arg[1:]
            if type == "0" or type == "D" or type == "N" or type == "n" and ports == "":
                self["-Pn"] = True
            elif (type == "" or type == "I" or type == "E") and ports == "":
                self["-PE"] = True
            elif type == "M" and ports == "":
                self["-PM"] = True
            elif type == "P" and ports == "":
                self["-PP"] = True
            elif type == "R" and ports == "":
                self["-PR"] = True
            elif type == "S":
                self["-PS"] = ports
            elif type == "T" or type == "A":
                self["-PA"] = ports
            elif type == "U":
                self["-PU"] = ports
            elif type == "O":
                self["-PO"] = ports
            elif type == "B":
                self["-PB"] = ports
            elif type == "Y":
                self["-PY"] = ports
            else:
                self.extras.append("-P%s" % arg)
        elif opt == "s":
            for type in arg:
                if type in "ACFLMNOPRSTUVWXYZn":
                    self["-s%s" % type] = True
                else:
                    self.extras.append("-s%s" % type)
        elif opt == "T" or opt == "timing":
            if self["-T"] is None:
                try:
                    self["-T"] = int(arg)
                except ValueError:
                    try:
                        self["-T"] = self.TIMING_PROFILE_NAMES[arg.lower()]
                    except KeyError:
                        self.extras.extend(("-T", arg))
            else:
                self.extras.extend(("-T", arg))
        elif opt == "v" or opt == "verbose":
            if arg is None:
                arg = ""
            try:
                self["-v"] = int(arg)
            except ValueError:
                if reduce(lambda x, y: x and y, map(lambda z: z == "v", arg), True):
                    self.setdefault("-v", 0)
                    self["-v"] += len(arg) + 1
                else:
                    self.extras.append("-v%s" % arg)
        else:
            assert False, (opt, arg)

    def parse(self, opt_list):
        self.clear()

        if len(opt_list) > 0:
            self.executable = opt_list[0]

        for result in getopt_long_only_extras(opt_list[1:], self.SHORT_OPTIONS, self.LONG_OPTIONS):
            self.handle_result(result)

    def parse_string(self, opt_string):
        self.parse(split_quoted(opt_string))

    def render(self):
        opt_list = []

        for opt in ("-sA", "-sC", "-sF", "-sL", "-sM", "-sN", "-sO", "-sn", "-sR", "-sS", "-sT", "-sU", "-sV", "-sW", "-sX", "-sY", "-sZ"):
            if self[opt]:
                opt_list.append(opt)

        if self["-sI"] is not None:
            opt_list.extend(("-sI", self["-sI"]))

        for opt in ("-6",):
            if self[opt]:
                opt_list.append(opt)

        if self["-p"] is not None:
            opt_list.extend(("-p", self["-p"]))

        if self["-T"] is not None:
            opt_list.append("-T%s" % str(self["-T"]))

        if self["-O"] is not None:
            if isinstance(self["-O"], basestring):
                opt_list.append("-O%s" % self["-O"])
            elif self["-O"]:
                opt_list.append("-O")

        if self["-A"]:
            opt_list.append("-A")

        if self["-d"]:
            if self["-d"] == 1:
                opt_list.append("-d")
            elif self["-d"] > 1:
                opt_list.append("-d%s" % self["-d"])

        if self["-f"]:
            opt_list.extend(["-f"] * self["-f"])
        if self["-v"]:
            opt_list.extend(["-v"] * self["-v"])

        if self["-F"]:
            opt_list.append("-F")
        if self["-n"]:
            opt_list.append("-n")

        if self["-iL"] is not None:
            opt_list.extend(("-iL", self["-iL"]))
        if self["-iR"] is not None:
            opt_list.extend(("-iR", str(self["-iR"])))

        for opt in ("-oA", "-oG", "-oN", "-oS", "-oX"):
            if self[opt] is not None:
                opt_list.extend((opt, self[opt]))

        for opt in ("--min-hostgroup", "--max-hostgroup",
            "--min-parallelism", "--max-parallelism",
            "--min-rtt-timeout", "--max-rtt-timeout", "--initial-rtt-timeout",
            "--scan-delay", "--max-scan-delay",
            "--min-rate", "--max-rate",
            "--max-retries", "--max-os-tries", "--host-timeout"):
            if self[opt] is not None:
                opt_list.extend((opt, self[opt]))

        for ping_option in ("-Pn", "-PE", "-PM", "-PP", "-PR"):
            if self[ping_option]:
                opt_list.append(ping_option)
        for ping_option in ("-PS", "-PA", "-PU", "-PO", "-PY"):
            if self[ping_option] is not None:
                opt_list.append(ping_option + self[ping_option])
        if self["-PB"] is not None:
            if isinstance(self["-PB"], basestring):
                opt_list.append("-PB" + self["-PB"])
            elif self["-PB"]:
                opt_list.append("-PB")

        for opt in (\
            "--allports",
            "--append-output",
            "--badsum",
            "--defeat-rst-ratelimit",
            "--fuzzy",
            "--help",
            "--iflist",
            "--log-errors",
            "--no-stylesheet",
            "--open",
            "--osscan-guess",
            "--osscan-limit",
            "--packet-trace",
            "--privileged",
            "-r",
            "-R",
            "--randomize-hosts",
            "--reason",
            "--release-memory",
            "--script-trace",
            "--script-updatedb",
            "--send-eth",
            "--send-ip",
            "--system-dns",
            "--traceroute",
            "--unprivileged",
            "--version",
            "--version-all",
            "--version-light",
            "--version-trace",
            "--webxml",
            ):
            if self[opt]:
                opt_list.append(opt)

        for opt in (\
            "-b",
            "-D",
            "--datadir",
            "--data-length",
            "--dns-servers",
            "-e",
            "--exclude",
            "--excludefile",
            "-g",
            "--ip-options",
            "--mtu",
            "--port-ratio",
            "-S",
            "--scanflags",
            "--script",
            "--script-args",
            "--script-help",
            "--servicedb",
            "--spoof-mac",
            "--stylesheet",
            "--top-ports",
            "--ttl",
            "--versiondb",
            "--version-intensity",
            ):
            if self[opt] is not None:
                opt_list.extend((opt, self[opt]))

        opt_list.extend(self.target_specs)

        opt_list.extend(self.extras)

        return [self.executable] + opt_list

    def render_string(self):
        return join_quoted(self.render())

import doctest
import unittest

class NmapOptionsTest(unittest.TestCase):
    def test_clear(self):
        """Test that a new object starts without defining any options, that the
        clear method removes all options, and that parsing the empty string or
        an empty list removes all options."""
        TEST = "nmap -T4 -A -v localhost --webxml"
        ops = NmapOptions()
        self.assertTrue(len(ops.render()) == 1)
        ops.parse_string(TEST)
        self.assertFalse(len(ops.render()) == 1)
        ops.clear()
        self.assertTrue(len(ops.render()) == 1)
        ops.parse_string(TEST)
        ops.parse_string("")
        self.assertEqual(ops.render_string(), "nmap")
        ops.parse_string(TEST)
        ops.parse([])
        self.assertEqual(ops.render_string(), "nmap")

    def test_default_executable(self):
        """Test that there is a default executable member set."""
        ops = NmapOptions()
        self.assertNotNull(ops.executable)

    def test_default_executable(self):
        """Test that you can set the executable."""
        ops = NmapOptions()
        ops.executable = "foo"
        self.assertEqual(ops.executable, "foo")
        self.assertEqual(ops.render(), ["foo"])

    def test_render(self):
        """Test that the render method returns a list."""
        TEST = "nmap -T4 -A -v localhost --webxml"
        ops = NmapOptions()
        ops.parse_string(TEST)
        self.assertTrue(type(ops.render()) == list, "type == %s" % type(ops.render))

    def test_quoted(self):
        """Test that strings can be quoted."""
        ops = NmapOptions()

        ops.parse_string('nmap --script ""')
        self.assertEqual(ops["--script"], "")
        ops.parse_string("nmap --script ''")
        self.assertEqual(ops["--script"], "")

        ops.parse_string('nmap --script test one two three')
        self.assertEqual(ops["--script"], "test")
        self.assertEqual(ops.target_specs, ["one", "two", "three"])
        ops.parse_string('nmap --script "test" one two three')
        self.assertEqual(ops["--script"], "test")
        self.assertEqual(ops.target_specs, ["one", "two", "three"])
        ops.parse_string('nmap --script "test one" two three')
        self.assertEqual(ops["--script"], "test one")
        self.assertEqual(ops.target_specs, ["two", "three"])
        ops.parse_string('nmap --script test" one" two three')
        self.assertEqual(ops["--script"], "test one")
        self.assertEqual(ops.target_specs, ["two", "three"])
        ops.parse_string('nmap --script test" one"""" two" three')
        self.assertEqual(ops["--script"], "test one two")
        self.assertEqual(ops.target_specs, ["three"])

        ops.parse_string("nmap --script test one two three")
        self.assertEqual(ops["--script"], "test")
        self.assertEqual(ops.target_specs, ["one", "two", "three"])
        ops.parse_string("nmap --script 'test' one two three")
        self.assertEqual(ops["--script"], "test")
        self.assertEqual(ops.target_specs, ["one", "two", "three"])
        ops.parse_string("nmap --script 'test one' two three")
        self.assertEqual(ops["--script"], "test one")
        self.assertEqual(ops.target_specs, ["two", "three"])
        ops.parse_string("nmap --script test' one' two three")
        self.assertEqual(ops["--script"], "test one")
        self.assertEqual(ops.target_specs, ["two", "three"])
        ops.parse_string("nmap --script test' one'''' two' three")
        self.assertEqual(ops["--script"], "test one two")
        self.assertEqual(ops.target_specs, ["three"])

        ops.parse_string('nmap --script "ab\\\"cd"')
        self.assertEqual(ops["--script"], "ab\"cd")
        ops.parse_string('nmap --script "ab\\\\cd"')
        self.assertEqual(ops["--script"], "ab\\cd")
        ops.parse_string('nmap --script "ab\\\'cd"')
        self.assertEqual(ops["--script"], "ab'cd")
        ops.parse_string("nmap --script 'ab\\\"cd'")
        self.assertEqual(ops["--script"], 'ab"cd')

        ops.parse_string('nmap "--script" test')
        self.assertEqual(ops["--script"], "test")
        ops.parse_string("nmap '--script' test")
        self.assertEqual(ops["--script"], "test")

        ops.parse_string('"nmap foo" --script test')
        self.assertEqual(ops.executable, "nmap foo")
        ops.parse_string("'nmap foo' --script test")
        self.assertEqual(ops.executable, "nmap foo")

    def test_render_quoted(self):
        """Test that strings that need to be quoted are quoted."""
        ops = NmapOptions()
        ops.parse_string('"/path/ /nmap" --script "test one two three"')
        self.assertEqual(ops.executable, "/path/ /nmap")
        self.assertEqual(ops["--script"], "test one two three")
        self.assertEqual(ops.target_specs, [])
        s = ops.render_string()
        ops.parse_string(s)
        self.assertEqual(ops.executable, "/path/ /nmap")
        self.assertEqual(ops["--script"], "test one two three")
        self.assertEqual(ops.target_specs, [])

    def test_end(self):
        """Test that -- ends argument processing."""
        ops = NmapOptions()
        ops.parse_string("nmap -v -- -v")
        self.assertTrue(ops["-v"] == 1)
        self.assertTrue(ops.target_specs == ["-v"])

    def test_roundtrip(self):
        """Test that parsing and re-rendering a previous rendering gives the
        same thing as the previous rendering."""
        TESTS = (
            "nmap",
            "nmap -v",
            "nmap -vv",
            "nmap -d -v",
            "nmap -d -d",
            "nmap -d -v -d",
            "nmap localhost",
            "nmap -oX - 192.168.0.1 -PS10",
        )
        ops = NmapOptions()
        for test in TESTS:
            ops.parse_string(test)
            opt_string_1 = ops.render_string()
            ops.parse_string(opt_string_1)
            opt_string_2 = ops.render_string()
            self.assertEqual(opt_string_1, opt_string_2)

    def test_underscores(self):
        """Test that underscores in option names are treated the same as
        dashes (and are canonicalized to dashes)."""
        ops = NmapOptions()
        ops.parse_string("nmap --osscan_guess")
        self.assertTrue("--osscan-guess" in ops.render_string())

    def test_args(self):
        """Test potentially tricky argument scenarios."""
        ops = NmapOptions()
        ops.parse_string("nmap -d9")
        self.assertTrue(len(ops.target_specs) == 0)
        self.assertTrue(ops["-d"] == 9, ops["-d"])
        ops.parse_string("nmap -d 9")
        self.assertTrue(ops.target_specs == ["9"])
        self.assertTrue(ops["-d"] == 1)

    def test_repetition(self):
        """Test options that can be repeated to increase their effect."""
        ops = NmapOptions()
        ops.parse_string("nmap -vv")
        self.assertTrue(ops["-v"] == 2)
        ops.parse_string("nmap -v -v")
        self.assertTrue(ops["-v"] == 2)
        ops.parse_string("nmap -ff")
        self.assertTrue(ops["-f"] == 2)
        ops.parse_string("nmap -f -f")
        self.assertTrue(ops["-f"] == 2)
        # Note: unlike -d, -v doesn't take an optional numeric argument.
        ops.parse_string("nmap -d2 -d")
        self.assertTrue(ops["-d"] == 3)

    def test_scan_types(self):
        """Test that multiple scan types given to the -s option are all
        interpreted correctly."""
        ops = NmapOptions()
        ops.parse_string("nmap -s")
        self.assertTrue(ops.extras == ["-s"])
        ops.parse_string("nmap -sS")
        self.assertTrue(ops.extras == [])
        self.assertTrue(ops["-sS"])
        self.assertTrue(not ops["-sU"])
        ops.parse_string("nmap -sSU")
        self.assertTrue(ops["-sS"])
        self.assertTrue(ops["-sU"])

    def test_extras(self):
        """Test that unknown arguments are correctly recorded. A few subtleties
        are tested:
        1. Unknown options are not simply discarded.
        2. When an unknown option is found, any following arguments that could
           have a different meaning depending on whether the unknown option
           takes an argument are moved with the argument to the extras.
        3. Any arguments moved to the extras are not otherwise interpreted.
        4. Extra options so copied are copied in blocks, keeping their original
           ordering with each block."""
        ops = NmapOptions()

        ops.parse_string("nmap --fee")
        self.assertTrue(ops.extras == ["--fee"])
        self.assertTrue(ops.render_string() == "nmap --fee")

        # Note: -x is not a real Nmap option.

        ops.parse_string("nmap -x")
        self.assertTrue(ops.extras == ["-x"])
        self.assertTrue(ops.render_string() == "nmap -x")

        ops.parse_string("nmap -v --fie scanme.nmap.org -d")
        self.assertTrue(ops.extras == ["--fie", "scanme.nmap.org"])
        self.assertTrue(ops["-v"] == 1)
        self.assertTrue(ops["-d"] == 1)
        self.assertTrue(len(ops.target_specs) == 0)

        ops.parse_string("nmap -v --foe=5 scanme.nmap.org -d")
        self.assertTrue(ops.extras == ["--foe=5"])
        self.assertTrue(ops.target_specs == ["scanme.nmap.org"])

        ops.parse_string("nmap --fum -oX out.xml -v")
        self.assertTrue(ops.extras == ["--fum", "-oX", "out.xml"])
        self.assertTrue(ops["-v"] == 1)

        ops.parse_string("nmap -x -A localhost")
        self.assertTrue(ops.extras == ["-x", "-A"])

        ops.parse_string("nmap -x --fee -A localhost")
        self.assertTrue(ops.extras == ["-x", "--fee", "-A"])

        ops.parse_string("nmap -x -x --timing 3 localhost")
        self.assertTrue(ops.extras == ["-x", "-x", "--timing", "3"])
        self.assertTrue(ops.target_specs == ["localhost"])

        ops.parse_string("nmap -x -x --timing=3 localhost")
        self.assertTrue(ops.extras == ["-x", "-x", "--timing=3"])
        self.assertTrue(ops.target_specs == ["localhost"])

        ops.parse_string("nmap -x -Ad9")
        self.assertTrue(ops.extras == ["-x", "-Ad9"])

        ops.parse_string("nmap -xrest")
        self.assertTrue(ops.extras == ["-xrest"])

        # Options that can't be given more than once should end up in extras.
        ops.parse_string("nmap -p 53 -p 80 -O --mtu 50 --mtu 100 -O2")
        self.assertTrue(ops["-p"] == "53")
        self.assertTrue(ops["--mtu"] == "50")
        self.assertTrue(ops["-O"])
        self.assertTrue(ops.extras == ["-p", "80", "--mtu", "100", "-O2"])

    def test_quirks(self):
        """Test the handling of constructions whose interpretation isn't
        specified in documentation, but should match that of GNU getopt."""
        ops = NmapOptions()
        # Long options can be written with one dash.
        ops.parse_string("nmap -min-rate 100")
        self.assertTrue(ops["--min-rate"] == "100")
        ops.parse_string("nmap -min-rate=100")
        self.assertTrue(ops["--min-rate"] == "100")

        # Short options not taking an argument can be followed by a long option.
        ops.parse_string("nmap -nFmin-rate 100")
        self.assertTrue(ops["-n"])
        self.assertTrue(ops["-F"])
        self.assertTrue(ops["--min-rate"] == "100")

        # Short options taking an argument consume the rest of the argument.
        ops.parse_string("nmap -nFp1-100")
        self.assertTrue(ops["-n"])
        self.assertTrue(ops["-F"])
        self.assertTrue(ops["-p"] == "1-100")

    def test_conversion(self):
        """Test that failed integer conversions cause the option to wind up in
        the extras."""
        ops = NmapOptions()
        ops.parse_string("nmap -d#")
        self.assertTrue(ops.extras == ["-d#"])
        ops.parse_string("nmap -T monkeys")
        self.assertTrue(ops["-T"] == None)
        self.assertTrue(ops.extras == ["-T", "monkeys"])
        ops.parse_string("nmap -iR monkeys")
        self.assertTrue(ops["-iR"] == None)
        self.assertTrue(ops.extras == ["-iR", "monkeys"])

    def test_read_unknown(self):
        """Test that getting the value of non-options returns None."""
        ops = NmapOptions()
        self.assertEqual(ops["-x"], None)
        self.assertEqual(ops["--nonoption"], None)

    def test_canonical_option_names(self):
        """Test that equivalent option names are properly canonicalized, so that
        ops["--timing"] and ops["-T"] mean the same thing, for example."""
        EQUIVS = (
            ("--debug", "-d"),
            ("--help", "-h"),
            ("-iL", "-i"),
            ("--max-parallelism", "-M"),
            ("--osscan-guess", "--fuzzy"),
            ("-oG", "-oM", "-m"),
            ("-oN", "-o"),
            ("-sP", "-sn"),
            ("-P", "-PE", "-PI"),
            ("-PA", "-PT"),
            ("-P0", "-PD", "-PN", "-Pn"),
            ("--source-port", "-g"),
            ("--timing", "-T"),
            ("--verbose", "-v"),
            ("--version", "-V"),
            ("--min-rate", "-min-rate", "--min_rate", "-min_rate")
        )
        ops = NmapOptions()
        for set in EQUIVS:
            for opt in set:
                ops.clear()
                ops[opt] = "test"
                for other in set:
                    self.assertTrue(ops[other] == "test",
                        "%s and %s not the same" % (opt, other))

    def test_options(self):
        """Test that all options that are supposed to be supported are really
        supported. They must be parsed and not as extras, and must produce
        output on rendering that can be parsed again."""
        TESTS = ["-" + opt for opt in "6AFfhnRrVv"]
        TESTS += ["-b host", "-D 192.168.0.1,ME,RND", "-d", "-d -d", "-d2",
            "-e eth0", "-f -f", "-g 53", "-i input.txt", "-M 100",
            "-m output.gnmap", "-O", "-O2", "-o output.nmap", "-p 1-100",
            "-S 192.168.0.1", "-T0", "-v -v"]
        TESTS += ["-s" + opt for opt in "ACFLMNnOPRSTUVWXYZ"]
        TESTS += ["-P" + opt for opt in "IEMP0NnDRBSTAUOY"]
        TESTS += ["-P" + opt + "100" for opt in "STAUOY"]
        TESTS += [
            "--version",
            "--verbose",
            "--datadir=dir",
            "--datadir dir",
            "--servicedb=db",
            "--servicedb db",
            "--versiondb=db",
            "--versiondb db",
            "--debug",
            "--debug=3",
            "--debug 3",
            "--help",
            "--iflist",
            "--release-memory",
            "--max-os-tries=10",
            "--max-os-tries 10",
            "--max-parallelism=10",
            "--min-parallelism 10",
            "--timing=0",
            "--timing 0",
            "--max-rtt-timeout=10",
            "--max-rtt-timeout 10",
            "--min-rtt-timeout=10",
            "--min-rtt-timeout 10",
            "--initial-rtt-timeout=10",
            "--initial-rtt-timeout 10",
            "--excludefile=file",
            "--excludefile file",
            "--exclude=192.168.0.0",
            "--exclude 192.168.0.0",
            "--max-hostgroup=10",
            "--max-hostgroup 10",
            "--min-hostgroup=10",
            "--min-hostgroup 10",
            "--open",
            "--scanflags=RST,ACK",
            "--scanflags RST,ACK",
            "--defeat-rst-ratelimit",
            "--host-timeout=10",
            "--host-timeout 10",
            "--scan-delay=10",
            "--scan-delay 10",
            "--max-scan-delay=10",
            "--max-scan-delay 10",
            "--max-retries=10",
            "--max-retries 10",
            "--source-port=53",
            "--source-port 53",
            "--randomize-hosts",
            "--osscan-limit",
            "--osscan-guess",
            "--fuzzy",
            "--packet-trace",
            "--version-trace",
            "--data-length=10",
            "--data-length 10",
            "--send-eth",
            "--send-ip",
            "--stylesheet=style.xml",
            "--stylesheet style.xml",
            "--no-stylesheet",
            "--webxml",
            "--privileged",
            "--unprivileged",
            "--mtu=1500",
            "--mtu 1500",
            "--append-output",
            "--spoof-mac=00:00:00:00:00:00",
            "--spoof-mac 00:00:00:00:00:00",
            "--badsum",
            "--ttl=64",
            "--ttl 64",
            "--traceroute",
            "--reason",
            "--allports",
            "--version-intensity=5",
            "--version-intensity 5",
            "--version-light",
            "--version-all",
            "--system-dns",
            "--log-errors",
            "--dns-servers=localhost",
            "--dns-servers localhost",
            "--port-ratio=0.5",
            "--port-ratio 0.5",
            "--top-ports=1000",
            "--top-ports 1000",
            "--script=script.nse",
            "--script script.nse",
            "--script-trace",
            "--script-updatedb",
            "--script-args=none",
            "--script-args none",
            "--script-help=script.nse",
            "--script-help script.nse",
            "--ip-options=S",
            "--ip-options S",
            "--min-rate=10",
            "--min-rate 10",
            "--max-rate=10",
            "--max-rate 10",
            "-iL=input.txt",
            "-iL input.txt",
            "-iR=1000",
            "-iR 1000",
            "-oA=out",
            "-oA out",
            "-oG=out.gnmap",
            "-oG out.gnmap",
            "-oM=out.gnmap",
            "-oM out.gnmap",
            "-oN=out.nmap",
            "-oN out.nmap",
            "-oS=out.skid",
            "-oS out.skid",
            "-oX=out.xml",
            "-oX out.xml",
            "-sI=zombie.example.com",
            "-sI zombie.example.com",
            ]

        # The following options are present in the Nmap source but are not
        # tested for because they are deprecated or not documented or whatever.
        # "-I",
        # "--noninteractive",
        # "--thc",
        # "--nogcc",
        # "-rH",
        # "-ff",
        # "-vv",
        # "-oH",

        ops = NmapOptions()
        for test in TESTS:
            ops.parse_string("nmap " + test)
            opt_list_1 = ops.render()
            self.assertTrue(len(opt_list_1) > 1, "%s missing on render" % test)
            self.assertTrue(len(ops.extras) == 0, "%s caused extras: %s" % (test, repr(ops.extras)))
            ops.parse(opt_list_1)
            opt_list_2 = ops.render()
            self.assertTrue(opt_list_1 == opt_list_2, "Result of parsing and rendering %s not parsable again" % test)
            self.assertTrue(len(ops.extras) == 0, "Result of parsing and rendering %s left extras: %s" % (test, ops.extras))

class SplitQuotedTest(unittest.TestCase):
    """A unittest class that tests the split_quoted function."""

    def test_split(self):
        self.assertEqual(split_quoted(''), [])
        self.assertEqual(split_quoted('a'), ['a'])
        self.assertEqual(split_quoted('a b c'), 'a b c'.split())

    def test_quotes(self):
        self.assertEqual(split_quoted('a "b" c'), ['a', 'b', 'c'])
        self.assertEqual(split_quoted('a "b c"'), ['a', 'b c'])
        self.assertEqual(split_quoted('a "b c""d e"'), ['a', 'b cd e'])
        self.assertEqual(split_quoted('a "b c"z"d e"'), ['a', 'b czd e'])

    def test_backslash(self):
        self.assertEqual(split_quoted('"\\""'), ['"'])
        self.assertEqual(split_quoted('\\"\\""'), ['\\"'])
        self.assertEqual(split_quoted('"\\"\\""'), ['""'])

if __name__ == "__main__":
    doctest.testmod()
    unittest.main()

########NEW FILE########
__FILENAME__ = NmapParser
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import re
import locale
import os
import os.path
import time
import socket
import StringIO
import copy

from types import StringTypes
from xml.sax import make_parser
from xml.sax import SAXException
from xml.sax.handler import ContentHandler
from xml.sax.saxutils import XMLGenerator
from xml.sax.xmlreader import AttributesImpl as Attributes

import zenmapCore_Kvasir.I18N
from zenmapCore_Kvasir.UmitLogging import log
from zenmapCore_Kvasir.NmapOptions import NmapOptions, split_quoted, join_quoted
from zenmapCore_Kvasir.StringPool import unique

# The version of the Nmap DTD this file understands and emits.
XML_OUTPUT_VERSION = "1.04"

class HostInfo(object):
    def __init__(self):
        self.comment = None;
        self._tcpsequence = {}
        self._osmatches = []
        self._ports = []
        self._ports_used = []
        self._extraports = []
        self._uptime = {}
        self._hostnames = []
        self._tcptssequence = {}
        self._ipidsequence = {}
        self._ip = None
        self._ipv6 = None
        self._mac = None
        self._state = ''
        self._comment = ''
        self._trace = {}
        self._hostscripts = []

    def make_clone(self):
        clone = HostInfo()
        clone.comment = self.comment
        clone._tcpsequence = copy.deepcopy(self._tcpsequence)
        clone._osmatches = copy.deepcopy(self._osmatches)
        clone._ports = copy.deepcopy(self._ports)
        clone._ports_used = self._ports_used
        clone._extraports = self._extraports
        clone._uptime = copy.deepcopy(self._uptime)
        clone._hostnames = copy.deepcopy(self._hostnames)
        clone._tcptssequence = copy.deepcopy(self._tcptssequence)
        clone._ipidsequence = copy.deepcopy(self._ipidsequence)
        clone._ip = copy.deepcopy(self._ip)
        clone._ipv6 = copy.deepcopy(self._ipv6)
        clone._mac = copy.deepcopy(self._mac)
        clone._state = self._state
        clone._comment = self._comment
        clone._trace = copy.deepcopy(self._trace)
        clone._hostscripts = copy.deepcopy(self._hostscripts)

        return clone

    # tcpsequence is a dict of the form
    # {'index': u'203',
    #  'values': u'3637785D,35B440D1,35E9FC3B,3640DB42,355F5931,3601AE14',
    #  'difficulty': u'Good luck!'}
    def set_tcpsequence(self, sequence):
        self._tcpsequence = sequence

    def get_tcpsequence(self):
        if self._tcpsequence:
            return self._tcpsequence
        return {}

    # tcptssequence is a dict of the form
    # {'values': u'71D0483C,71D048A3,71D0490C,71D04973,71D049DB,71D04A45',
    #  'class': u'1000HZ'}
    def set_tcptssequence(self, sequence):
        self._tcptssequence = sequence

    def get_tcptssequence(self):
        if self._tcptssequence:
            return self._tcptssequence
        return {}

    # ipidsequence is a dict of the form
    # {'values': u'0,0,0,0,0,0', 'class': u'All zeros'}
    def set_ipidsequence(self, sequence):
        self._ipidsequence = sequence

    def get_ipidsequence(self):
        if self._ipidsequence:
            return self._ipidsequence
        return {}

    # osmatches is a list of dicts of the form
    # {'name': u'Linux 2.6.24', 'accuracy': u'98', 'line': u'1000',
    #  'osclasses': ...}
    # where each 'osclasses' element is a dict of the form
    # {'vendor': u'Linux', 'osfamily': u'Linux', 'type': u'general purpose',
    #  'osgen': u'2.6.X', 'accuracy': u'98', 'cpe':'cpe:/o:linux:2.6'}
    def set_osmatches(self, matches):
        self._osmatches = matches

    def get_osmatches(self):
        return self._osmatches

    def get_best_osmatch(self):
        """Return the OS match with the highest accuracy."""
        if not self._osmatches:
            return None
        def osmatch_key(osmatch):
            try:
                return -float(osmatch["accuracy"])
            except ValueError:
                return 0
        return sorted(self._osmatches, key = osmatch_key)[0]


    # ports_used is a list like
    # [{'state': u'open', 'portid': u'22', 'proto': u'tcp'},
    #  {'state': u'closed', 'portid': u'25', 'proto': u'tcp'},
    #  {'state': u'closed', 'portid': u'44054', 'proto': u'udp'}]
    # but not all three elements are necessarily present.
    def set_ports_used(self, ports):
        self._ports_used = ports

    def get_ports_used(self):
        return self._ports_used

    # uptime is a dict of the form
    # {'seconds': u'1909493', 'lastboot': u'Wed Jul 2 06:48:31 2008'}
    def set_uptime(self, uptime):
        self._uptime = uptime

    def get_uptime(self):
        if self._uptime:
            return self._uptime

        # Avoid empty dict return
        return {"seconds":"", "lastboot":""}

    # ports is an array containing dicts of the form
    # {'port_state': u'open', 'portid': u'22', 'protocol': u'tcp',
    #  'service_conf': u'10', 'service_extrainfo': u'protocol 2.0',
    #  'service_method': u'probed', 'service_name': u'ssh',
    #  'service_product': u'OpenSSH', 'service_version': u'4.3'}
    def set_ports(self, ports):
        self._ports = ports

    def get_ports(self):
        return self._ports

    # hostscripts is an array containing dicts of the form
    # {'id': u'script-name', 'output': u'script-output'}
    def set_hostscripts(self, hostscripts):
        self._hostscripts = hostscripts

    def get_hostscripts(self):
        return self._hostscripts

    """
    def get_hostscripts(self):
        hostscripts = []
        for s in self._hostscripts:
            hostscripts.append({
                'id': s.get("id", ""),
                'output': s.get("output", ""),
            })
        return hostscripts

    def set_hostscripts(self, hostscripts):
        self._hostscripts = hostscripts
    """

    # extraports is an array of dicts of the form
    # {'count': u'1709', 'state': u'filtered'}
    def set_extraports(self, port_list):
        self._extraports = port_list

    def get_extraports(self):
        return self._extraports

    # hostnames is a list containing dicts of the form
    # [{'hostname': u'scanme.nmap.org', 'hostname_type': u'PTR'}]
    def set_hostnames(self, hostname_list):
        self._hostnames = hostname_list

    def get_hostnames(self):
        return self._hostnames

    # ip, ipv6, and mac are either None or dicts of the form
    # {'vendor': u'', 'type': u'ipv4', 'addr': u'64.13.134.52'}
    def set_ip(self, addr):
        self._ip = addr

    def get_ip(self):
        return self._ip

    def set_mac(self, addr):
        self._mac = addr

    def get_mac(self):
        return self._mac

    def set_ipv6(self, addr):
        self._ipv6 = addr

    def get_ipv6(self):
        return self._ipv6

    def get_addrs_for_sort(self):
        """Return a list of addresses as opaque values sorted such that
        1) IPv4 comes before IPv6 comes before MAC, and
        2) addresses are sorted according to their binary values, not their
           string representation.
        Use this function to the the comparison key when sorting a list of
        hosts by address."""
        l = []
        if self.ip:
            l.append((1, socket.inet_aton(self.ip["addr"])))
        if self.ipv6:
            try:
                l.append((1, socket.inet_pton(socket.AF_INET6, self.ipv6["addr"])))
            except AttributeError:
                # Windows doesn't have socket.inet_pton. Go alphabetical.
                # Encode to a byte string for possible comparison with binary
                # address strings (which can't be converted to unicode).
                l.append((1, self.ipv6["addr"].encode("utf-8")))
        if self.mac:
            l.append((3, "".join([chr(int(x, 16)) for x in self.mac["addr"].split(":")])))
        l.sort()
        return l

    # comment is a string.
    def get_comment(self):
        return self._comment

    def set_comment(self, comment):
        self._comment = comment

    # state is a string like u'up' or u'down'.
    def set_state(self, status):
        self._state = status

    def get_state(self):
        return self._state

    def get_hostname(self):
        hostname = None
        if len(self._hostnames) > 0:
            hostname = self._hostnames[0]["hostname"]

        address = self.ip or self.ipv6 or self.mac
        if address is not None:
            address = address["addr"]

        if hostname is not None:
            if address is not None:
                return "%s (%s)" % (hostname, address)
            else:
                return hostname
        else:
            if address is not None:
                return address
            else:
                return _("Unknown Host")

    def get_port_count_by_states(self, states):
        count = 0

        for p in self.ports:
            state = p.get('port_state')
            if state in states:
                count += 1

        for extra in self.get_extraports():
            if extra['state'] in states:
                count += int(extra['count'])

        return count

    def get_open_ports(self):
        return self.get_port_count_by_states(('open', 'open|filtered'))

    def get_filtered_ports(self):
        return self.get_port_count_by_states(('filtered', 'open|filtered', 'closed|filtered'))

    def get_closed_ports(self):
        return self.get_port_count_by_states(('closed', 'closed|filtered'))

    def get_scanned_ports(self):
        scanned = 0

        for p in self.ports:
            scanned+=1

        for extra in self.get_extraports():
            scanned += int(extra["count"])

        return scanned

    def get_services(self):
        services = []
        for p in self.ports:
            services.append({"service_name":p.get("service_name", _("unknown")),
                             "portid":p.get("portid", ""),
                             "service_version":p.get("service_version",
                                                     _("Unknown version")),
                             "service_product":p.get("service_product", ""),
                             "service_extrainfo":p.get("service_extrainfo", ""),
                             "port_state":p.get("port_state", _("Unknown")),
                             "protocol":p.get("protocol", "")})
        return services

    def get_trace(self):
        return self._trace

    def set_trace(self, trace):
        self._trace = trace

    def append_trace_hop(self, hop):
        if "hops" in self._trace:
            self._trace["hops"].append(hop)
        else:
            self._trace["hops"] = [hop]

    def set_trace_error(self, errorstr):
        self._trace["error"] = errorstr

    # Properties
    tcpsequence = property(get_tcpsequence, set_tcpsequence)
    osmatches = property(get_osmatches, set_osmatches)
    ports = property(get_ports, set_ports)
    ports_used = property(get_ports_used, set_ports_used)
    extraports = property(get_extraports, set_extraports)
    uptime = property(get_uptime, set_uptime)
    hostnames = property(get_hostnames, set_hostnames)
    tcptssequence = property(get_tcptssequence, set_tcptssequence)
    ipidsequence = property(get_ipidsequence, set_ipidsequence)
    ip = property(get_ip, set_ip)
    ipv6 = property(get_ipv6, set_ipv6)
    mac = property(get_mac, set_mac)
    state = property(get_state, set_state)
    comment = property(get_comment, set_comment)
    services = property(get_services)
    trace = property(get_trace, set_trace)
    hostscripts = property(get_hostscripts, set_hostscripts)

class ParserBasics(object):
    def __init__ (self):
        # This flag informs us whether the XML output file is temporary (True),
        # or user specified (False). If any of them is user-specified, it
        # doesn't get stripped out of the command string in set_nmap_command.
        self.xml_is_temp = True

        self.nmap = {'nmaprun':{},\
                     'scaninfo':[],\
                     'verbose':'',\
                     'debugging':'',\
                     'hosts':[],\
                     'runstats':{}}

        self.ops = NmapOptions()
        self._nmap_output = None

    def set_xml_is_temp(self, xml_is_temp):
        # This flag is False if a user has specified his own -oX option - in
        # which case we not should remove the -oX option from the command
        # string. A value of True means that we're using a temporary file which
        # should be removed from the command string (see set_nmap_command).
        self.xml_is_temp = xml_is_temp

    def get_profile_name(self):
        return self.nmap['nmaprun'].get('profile_name', '')

    def set_profile_name(self, name):
        self.nmap['nmaprun']['profile_name'] = name

    def get_targets(self):
        return self.ops.target_specs

    def set_targets(self, targets):
        self.ops.target_specs = targets

    def get_nmap_output(self):
        return self._nmap_output

    def set_nmap_output(self, nmap_output):
        self._nmap_output = nmap_output

    def get_debugging_level (self):
        return self.nmap.get('debugging', '')

    def set_debugging_level(self, level):
        self.nmap['debugging'] = level

    def get_verbose_level (self):
        return self.nmap.get('verbose', '')

    def set_verbose_level(self, level):
        self.nmap['verbose'] = level

    def get_scaninfo(self):
        return self.nmap.get('scaninfo', '')

    def set_scaninfo(self, info):
        self.nmap['scaninfo'] = info

    def get_services_scanned (self):
        if self._services_scanned == None:
            return self._services_scanned

        services = []
        for scan in self.nmap.get('scaninfo', []):
            services.append(scan['services'])

        self._services_scanned = ','.join(services)
        return self._services_scanned

    def set_services_scanned (self, services_scanned):
        self._services_scanned = services_scanned

    def get_nmap_command (self):
        return self.ops.render_string()

    def set_nmap_command(self, command):
        self.ops.parse_string(command)
        if self.xml_is_temp:
            self.ops["-oX"] = None
        self.nmap['nmaprun']['args'] = self.ops.render_string()

    def get_scan_type (self):
        types = []
        for t in self.nmap.get('scaninfo', []):
            types.append(t['type'])
        return types

    def get_protocol (self):
        protocols = []
        for proto in self.nmap.get('scaninfo', []):
            protocols.append(proto['protocol'])
        return protocols

    def get_num_services (self):
        if self._num_services == None:
            return self._num_services

        num = 0
        for n in self.nmap.get('scaninfo', []):
            num += int(n['numservices'])

        self._num_services = num
        return self._num_services

    def set_num_services (self, num_services):
        self._num_services = num_services

    def get_date (self):
        epoch = int(self.nmap['nmaprun'].get('start', '0'))
        return time.localtime (epoch)

    def get_start(self):
        return self.nmap['nmaprun'].get('start', '0')

    def set_start(self, start):
        self.nmap['nmaprun']['start'] = start

    def set_date(self, date):
        if type(date) == type(int):
            self.nmap['nmaprun']['start'] = date
        else:
            raise Exception("Wrong date format. Date should be saved \
in epoch format!")

    def get_open_ports(self):
        ports = 0

        for h in self.nmap.get('hosts', []):
            ports += h.get_open_ports()

        return ports

    def get_filtered_ports(self):
        ports = 0

        for h in self.nmap.get('hosts', []):
            ports += h.get_filtered_ports()

        return ports

    def get_closed_ports(self):
        ports = 0

        for h in self.nmap['hosts']:
            ports += h.get_closed_ports()

        return ports

    def get_formatted_date(self):
        return time.strftime("%B %d, %Y - %H:%M", self.get_date())

    def get_scanner (self):
        return self.nmap['nmaprun'].get('scanner', '')

    def set_scanner(self, scanner):
        self.nmap['nmaprun']['scanner'] = scanner

    def get_scanner_version (self):
        return self.nmap['nmaprun'].get('version', '')

    def set_scanner_version(self, version):
        self.nmap['nmaprun']['version'] = version

    # IPv4
    def get_ipv4(self):
        hosts = self.nmap.get('hosts')
        if hosts is None:
            return []
        return [host.ip for host in hosts if host.ip is not None]

    # MAC
    def get_mac(self):
        hosts = self.nmap.get('hosts')
        if hosts is None:
            return []
        return [host.mac for host in hosts if host.mac is not None]

    # IPv6
    def get_ipv6(self):
        hosts = self.nmap.get('hosts')
        if hosts is None:
            return []
        return [host.ipv6 for host in hosts if host.ipv6 is not None]

    def get_hostnames (self):
        hostnames = []
        for host in self.nmap.get('hosts', []):
            hostnames += host.get_hostnames()
        return hostnames

    def get_hosts(self):
        return self.nmap.get('hosts', None)

    def get_runstats(self):
        return self.nmap.get('runstats', None)

    def set_runstats(self, stats):
        self.nmap['runstats'] = stats

    def get_hosts_down(self):
        return int(self.nmap['runstats'].get('hosts_down', '0'))

    def set_hosts_down(self, down):
        self.nmap['runstats']['hosts_down'] = int(down)

    def get_hosts_up(self):
        return int(self.nmap['runstats'].get('hosts_up', '0'))

    def set_hosts_up(self, up):
        self.nmap['runstats']['hosts_up'] = int(up)

    def get_hosts_scanned(self):
        return int(self.nmap['runstats'].get('hosts_scanned', '0'))

    def set_hosts_scanned(self, scanned):
        self.nmap['runstats']['hosts_scanned'] = int(scanned)

    def get_finish_time (self):
        return time.localtime(int(self.nmap['runstats'].get('finished_time',
                                                            '0')))

    def set_finish_time(self, finish):
        self.nmap['runstats']['finished_time'] = int(finish)

    def get_finish_epoc_time(self):
        return int(self.nmap['runstats'].get('finished_time', '0'))

    def set_finish_epoc_time(self, time):
        self.nmap['runstats']['finished_time'] = time

    def get_scan_name(self):
        """Get a human-readable string representing this scan."""
        scan_name = self.nmap.get("scan_name")
        if scan_name:
            return scan_name
        if self.profile_name and self.get_targets():
            return _("%s on %s") % (self.profile_name, join_quoted(self.get_targets()))
        return self.get_nmap_command()

    def set_scan_name(self, scan_name):
        self.nmap["scan_name"] = scan_name

    def get_formatted_finish_date(self):
        return time.strftime("%B %d, %Y - %H:%M", self.get_finish_time())

    def get_port_protocol_dict(self):
        #Create a dict of port -> protocol for all ports scanned
        ports = {}
        for scaninfo in self.scaninfo:
            services_string = scaninfo['services'].strip()
            if services_string == "":
                services_array = []
            else:
                services_array = services_string.split(',')
            for item in services_array:
                if item.find('-') == -1:
                    if int(item) not in ports:
                        ports[int(item)] = []
                    ports[int(item)].append(scaninfo['protocol'])
                else:
                    begin,end = item.split('-')
                    for port in range(int(begin),int(end)+1):
                        if int(port) not in ports:
                            ports[int(port)] = []
                        ports[int(port)].append(scaninfo['protocol'])
        return ports

    profile_name = property(get_profile_name, set_profile_name)
    nmap_output = property(get_nmap_output, set_nmap_output)
    debugging_level = property(get_debugging_level, set_debugging_level)
    verbose_level = property(get_verbose_level, set_verbose_level)
    scaninfo = property(get_scaninfo, set_scaninfo)
    services_scanned = property(get_services_scanned, set_services_scanned)
    nmap_command = property(get_nmap_command, set_nmap_command)
    scan_type = property(get_scan_type)
    protocol = property(get_protocol)
    num_services = property(get_num_services, set_num_services)
    date = property(get_date, set_date)
    open_ports = property(get_open_ports)
    filtered_ports = property(get_filtered_ports)
    closed_ports = property(get_closed_ports)
    formatted_date = property(get_formatted_date)
    scanner = property(get_scanner, set_scanner)
    scanner_version = property(get_scanner_version, set_scanner_version)
    ipv4 = property(get_ipv4)
    mac = property(get_mac)
    ipv6 = property(get_ipv6)
    hostnames = property(get_hostnames)
    hosts = property(get_hosts)
    runstats = property(get_runstats, set_runstats)
    hosts_down = property(get_hosts_down, set_hosts_down)
    hosts_up = property(get_hosts_up, set_hosts_up)
    hosts_scanned = property(get_hosts_scanned, set_hosts_scanned)
    finish_time = property(get_finish_time, set_finish_time)
    finish_epoc_time = property(get_finish_epoc_time, set_finish_epoc_time)
    formatted_finish_date = property(get_formatted_finish_date)
    start = property(get_start, set_start)
    scan_name = property(get_scan_name, set_scan_name)

    _num_services = None
    _services_scanned = None

class NmapParserSAX(ParserBasics, ContentHandler):
    def __init__(self):
        ParserBasics.__init__(self)

        # The text inside an xml-stylesheet processing instruction, like
        # 'href="file:///usr/share/nmap/nmap.xsl" type="text/xsl"'.
        self.xml_stylesheet_data = None

        self.in_interactive_output = False
        self.in_run_stats = False
        self.in_host = False
        self.in_hostnames = False
        self.in_ports = False
        self.in_port = False
        self.in_port_service = False
        self.in_port_service_cpe = False
        self.port_service_cpe = ""
        self.in_portscript = False
        self.in_os = False
        self.in_host_os_cpe = False
        self.in_trace = False
        self.in_hostscript = False
        self.list_extraports = []

        self.filename = None

        self.unsaved = False

    def set_parser(self, parser):
        self.parser = parser

    def parse(self, f):
        """Parse an Nmap XML file from the file-like object f."""
        self.parser.parse(f)

    def parse_file(self, filename):
        """Parse an Nmap XML file from the named file."""
        f = open(filename, "r")
        try:
            self.parse(f)
            self.filename = filename
        finally:
            f.close()

    def _parse_nmaprun(self, attrs):
        run_tag = "nmaprun"

        if self._nmap_output is None and attrs.has_key("nmap_output"):
            self._nmap_output = attrs["nmap_output"]
        self.nmap[run_tag]["profile_name"] = attrs.get("profile_name", "")
        self.nmap[run_tag]["start"] = attrs.get("start", "")
        self.nmap[run_tag]["args"] = attrs.get("args", "")
        self.nmap[run_tag]["scanner"] = attrs.get("scanner", "")
        self.nmap[run_tag]["version"] = attrs.get("version", "")
        self.nmap[run_tag]["xmloutputversion"] = attrs.get("xmloutputversion", "")

        self.nmap_command = self.nmap[run_tag]["args"]

    def _parse_output(self, attrs):
        if attrs.get("type") != "interactive":
            return
        if self.in_interactive_output:
            raise SAXException("Unexpected nested \"output\" element.")
        self.in_interactive_output = True
        self.nmap_output = ""

    def _parse_scaninfo(self, attrs):
        dic = {}

        dic["type"] = unique(attrs.get("type", ""))
        dic["protocol"] = unique(attrs.get("protocol", ""))
        dic["numservices"] = attrs.get("numservices", "")
        dic["services"] = attrs.get("services", "")

        self.nmap["scaninfo"].append(dic)

    def _parse_verbose(self, attrs):
        self.nmap["verbose"] = attrs.get("level", "")

    def _parse_debugging(self, attrs):
        self.nmap["debugging"] = attrs.get("level", "")

    def _parse_runstats_finished(self, attrs):
        self.nmap["runstats"]["finished_time"] = attrs.get("time", "")

    def _parse_runstats_hosts(self, attrs):
        self.nmap["runstats"]["hosts_up"] = attrs.get("up", "")
        self.nmap["runstats"]["hosts_down"] = attrs.get("down", "")
        self.nmap["runstats"]["hosts_scanned"] = attrs.get("total", "")

    def _parse_host(self, attrs):
        self.host_info = HostInfo()
        self.host_info.comment = attrs.get("comment", "")

    def _parse_host_status(self, attrs):
        self.host_info.set_state(unique(attrs.get("state", "")))

    def _parse_host_address(self, attrs):
        address_attributes = {"type":unique(attrs.get("addrtype", "")),
                              "vendor":attrs.get("vendor", ""),
                              "addr":attrs.get("addr", "")}

        if address_attributes["type"] == "ipv4":
            self.host_info.set_ip(address_attributes)
        elif address_attributes["type"] == "ipv6":
            self.host_info.set_ipv6(address_attributes)
        elif address_attributes["type"] == "mac":
            self.host_info.set_mac(address_attributes)

    def _parse_host_hostname(self, attrs):
        self.list_hostnames.append({"hostname":attrs.get("name", ""),
                                    "hostname_type":attrs.get("type", "")})

    def _parse_host_extraports(self, attrs):
        self.list_extraports.append({"state":unique(attrs.get("state", "")),
                                     "count":attrs.get("count", "")})

    def _parse_host_port(self, attrs):
        self.dic_port = {"protocol":unique(attrs.get("protocol", "")),
                         "portid":unique(attrs.get("portid", ""))}

    def _parse_host_port_state(self, attrs):
        self.dic_port["port_state"] = unique(attrs.get("state", ""))
        self.dic_port["reason"] = unique(attrs.get("reason", ""))
        self.dic_port["reason_ttl"] = unique(attrs.get("reason_ttl", ""))

    def _parse_host_port_service(self, attrs):
        self.dic_port["service_name"] = attrs.get("name", "")
        self.dic_port["service_method"] = unique(attrs.get("method", ""))
        self.dic_port["service_conf"] = attrs.get("conf", "")
        self.dic_port["service_product"] = attrs.get("product", "")
        self.dic_port["service_version"] = attrs.get("version", "")
        self.dic_port["service_extrainfo"] = attrs.get("extrainfo", "")

    def _parse_host_port_script(self, attrs):
        self.port_scripts.append({
            "id": unique(attrs.get("id", "")),
            "output": unique(attrs.get("output", ""))
        })

    def _parse_host_osmatch(self, attrs):
        osmatch = self._parsing(attrs, [], ['name', 'accuracy', 'line'])
        osmatch['osclasses'] = []
        self.list_osmatch.append(osmatch)

    def _parse_host_portused(self, attrs):
        self.list_portused.append(self._parsing(attrs, ['state', 'proto', 'portid'], []))

    def _parse_host_osclass(self, attrs):
        self.list_osclass.append(self._parsing(attrs, ['type', 'vendor', 'osfamily', 'osgen'], ['accuracy']))

    def _parsing(self, attrs, unique_names, other_names):
        # Returns a dict with the attributes of a given tag with the
        # atributes names as keys and their respective values
        dic = {}
        for at in unique_names:
            dic[at] = unique(attrs.get(at, ""))
        for at in other_names:
            dic[at] = attrs.get(at, "")
        return dic

    def _parse_host_uptime(self, attrs):
        self.host_info.set_uptime(self._parsing(attrs, [], ["seconds", "lastboot"]))


    def _parse_host_tcpsequence(self, attrs):
        self.host_info.set_tcpsequence(self._parsing(attrs, ['difficulty'], ['index', 'values']))

    def _parse_host_tcptssequence(self, attrs):
        self.host_info.set_tcptssequence(self._parsing(attrs, ['class'], ['values']))

    def _parse_host_ipidsequence(self, attrs):
        self.host_info.set_ipidsequence(self._parsing(attrs, ['class'], ['values']))

    def _parse_host_trace(self, attrs):
        trace = {}
        for attr in ["proto", "port"]:
            trace[attr] = unique(attrs.get(attr, ""))
        self.host_info.set_trace(trace)

    def _parse_host_trace_hop(self, attrs):
        hop = self._parsing(attrs, [], ["ttl", "rtt", "ipaddr", "host"])
        self.host_info.append_trace_hop(hop)

    def _parse_host_trace_error(self, attrs):
        self.host_info.set_trace_error(unique(attrs.get("errorstr", "")))

    def _parse_hostscripts(self, attrs):
        self.dic_hostscript = {
            "id": unique(attrs.get("id", "")),
            "output": unique(attrs.get("output", ""))
        }

    def processingInstruction(self, target, data):
        if target == "xml-stylesheet":
            self.xml_stylesheet_data = data

    def startElement(self, name, attrs):
        if name == "nmaprun":
            self._parse_nmaprun(attrs)
        if name == "output":
            self._parse_output(attrs)
        elif name == "scaninfo":
            self._parse_scaninfo(attrs)
        elif name == "verbose":
            self._parse_verbose(attrs)
        elif name == "debugging":
            self._parse_debugging(attrs)
        elif name == "runstats":
            self.in_run_stats = True
        elif self.in_run_stats and name == "finished":
            self._parse_runstats_finished(attrs)
        elif self.in_run_stats and name == "hosts":
            self._parse_runstats_hosts(attrs)
        elif name == "host":
            self.in_host = True
            self._parse_host(attrs)
            self.list_ports = []
            self.list_extraports = []
        elif self.in_host and name == "status":
            self._parse_host_status(attrs)
        elif self.in_host and name == "address":
            self._parse_host_address(attrs)
        elif self.in_host and name == "hostnames":
            self.in_hostnames = True
            self.list_hostnames = []
        elif self.in_host and self.in_hostnames and name == "hostname":
            self._parse_host_hostname(attrs)
        elif self.in_host and name == "ports":
            self.in_ports = True
        elif self.in_host and self.in_ports and name == "extraports":
            self._parse_host_extraports(attrs)
        elif self.in_host and self.in_ports and name == "port":
            self.in_port = True
            self.port_scripts = []
            self._parse_host_port(attrs)
        elif self.in_host and self.in_ports and \
             self.in_port and name == "state":
            self._parse_host_port_state(attrs)
        elif self.in_host and self.in_ports and \
             self.in_port and name == "service":
            self.in_port_service = True
            self._parse_host_port_service(attrs)
        elif self.in_host and self.in_ports and self.in_port and \
             self.in_port_service and name == "cpe":
            self.port_service_cpe = ""
            self.in_port_service_cpe = True
        elif self.in_host and self.in_ports and \
             self.in_port and name == "script":
            self._parse_host_port_script(attrs)
        elif self.in_host and name == "os":
            self.in_os = True
            self.list_portused = []
            self.list_osmatch = []
        elif self.in_host and self.in_os and name == "osmatch":
            self._parse_host_osmatch(attrs)
        elif self.in_host and self.in_os and name == "portused":
            self._parse_host_portused(attrs)
        elif self.in_host and self.in_os and name == "osclass":
            self.list_osclass = []
            self._parse_host_osclass(attrs)
        elif self.in_host and self.in_os and name == "cpe":
            self.host_os_cpe = ''
            self.in_host_os_cpe = True
        elif self.in_host and name == "uptime":
            self._parse_host_uptime(attrs)
        elif self.in_host and name == "tcpsequence":
            self._parse_host_tcpsequence(attrs)
        elif self.in_host and name == "tcptssequence":
            self._parse_host_tcptssequence(attrs)
        elif self.in_host and name == "ipidsequence":
            self._parse_host_ipidsequence(attrs)
        elif self.in_host and name == "trace":
            self.in_trace = True
            self._parse_host_trace(attrs)
        elif self.in_host and self.in_trace and name == "hop":
            self._parse_host_trace_hop(attrs)
        elif self.in_host and self.in_trace and name == "error":
            self._parse_host_trace_error(attrs)
        elif self.in_host and name == "hostscript":
            self.in_hostscript = True
            self.list_hostscripts = []
        elif self.in_host and self.in_hostscript and name == "script":
            self._parse_hostscripts(attrs)

    def endElement(self, name):
        if name == "runstats":
            self.in_interactive_output = False
        elif name == "runstats":
            self.in_run_stats = False
        elif name == "host":
            self.in_host = False
            self.host_info.set_extraports(self.list_extraports)
            self.host_info.set_ports(self.list_ports)
            self.nmap["hosts"].append(self.host_info)
        elif self.in_host and name == "hostnames":
            self.in_hostnames = False
            self.host_info.set_hostnames(self.list_hostnames)
        elif self.in_host and name == "ports":
            self.in_ports = False
        elif self.in_host and self.in_ports and name == "port":
            self.in_port = False
            self.in_port_service = False
            self.in_port_service_cpe = False
            self.dic_port["service_cpe"] = self.port_service_cpe
            self.dic_port['scripts'] = self.port_scripts
            self.list_ports.append(self.dic_port)
            del(self.dic_port)
            del(self.port_scripts)
            self.port_service_cpe = ""
        elif self.in_host and self.in_os and name == "cpe":
            self.list_osclass[-1].update({'cpe': self.host_os_cpe})
            self.in_host_os_cpe = False
        elif self.in_host and self.in_os and name == "osmatch":
            self.list_osmatch[-1]['osclasses'].extend(self.list_osclass)
            self.list_osclass = []
        elif self.in_host and self.in_os and name == "os":
            self.in_os = False
            self.host_info.set_ports_used(self.list_portused)
            self.host_info.set_osmatches(self.list_osmatch)

            del(self.list_portused)
            del(self.list_osmatch)
        elif self.in_host and self.in_trace and name == "trace":
            self.in_trace = False
        elif self.in_host and self.in_hostscript and name == "script":
            self.list_hostscripts.append(self.dic_hostscript)
            del(self.dic_hostscript)
        elif self.in_host and self.in_hostscript and name == "hostscript":
            self.in_hostscript = False

    def characters(self, content):
        if self.in_host_os_cpe:
            self.host_os_cpe += content
        if self.in_port_service_cpe:
            self.port_service_cpe += content
        if self.in_interactive_output:
            self.nmap_output += content

    def write_text(self, f):
        """Write the Nmap text output of this object to the file-like object
        f."""
        if self._nmap_output is None:
            return
        f.write(self.nmap_output)

    def write_xml(self, f):
        """Write the XML representation of this object to the file-like object
        f."""
        writer = XMLGenerator(f)
        writer.startDocument()
        if self.xml_stylesheet_data is not None:
            writer.processingInstruction("xml-stylesheet", self.xml_stylesheet_data)
        self._write_nmaprun(writer)
        self._write_scaninfo(writer)
        self._write_verbose(writer)
        self._write_debugging(writer)
        self._write_output(writer)
        self._write_hosts(writer)
        self._write_runstats(writer)
        writer.endElement("nmaprun")
        writer.endDocument()

    def get_xml(self):
        """Return a string containing the XML representation of this scan."""
        buffer = StringIO.StringIO()
        self.write_xml(buffer)
        string = buffer.getvalue()
        buffer.close()
        return string

    def write_xml_to_file(self, filename):
        """Write the XML representation of this scan to the file whose name is
        given."""
        fd = open(filename, "wb")
        self.write_xml(fd)
        fd.close()

    def _write_output(self, writer):
        if self._nmap_output is None:
            return
        writer.startElement("output", Attributes({"type": "interactive"}))
        writer.characters(self.nmap_output)
        writer.endElement("output")

    def _write_runstats(self, writer):
        ##################
        # Runstats element
        writer.startElement("runstats", Attributes(dict()))

        ## Finished element
        writer.startElement("finished",
                        Attributes(dict(time = str(self.finish_epoc_time),
                                        timestr = time.ctime(time.mktime(self.get_finish_time())))))
        writer.endElement("finished")

        ## Hosts element
        writer.startElement("hosts",
                            Attributes(dict(up = str(self.hosts_up),
                                            down = str(self.hosts_down),
                                            total = str(self.hosts_scanned))))
        writer.endElement("hosts")


        writer.endElement("runstats")
        # End of Runstats element
        #########################

    def _write_hosts(self, writer):
        for host in self.hosts:
            # Start host element
            writer.startElement("host",
                                Attributes(dict(comment=host.comment)))

            # Status element
            writer.startElement("status",
                                Attributes(dict(state=host.state)))
            writer.endElement("status")


            ##################
            # Address elements
            ## IPv4
            if host.ip is not None:
                writer.startElement("address",
                            Attributes(dict(addr=host.ip.get("addr", ""),
                                        vendor=host.ip.get("vendor", ""),
                                        addrtype=host.ip.get("type", ""))))
                writer.endElement("address")

            ## IPv6
            if host.ipv6 is not None:
                writer.startElement("address",
                            Attributes(dict(addr=host.ipv6.get("addr", ""),
                                        vendor=host.ipv6.get("vendor", ""),
                                        addrtype=host.ipv6.get("type", ""))))
                writer.endElement("address")

            ## MAC
            if host.mac is not None:
                writer.startElement("address",
                            Attributes(dict(addr=host.mac.get("addr", ""),
                                        vendor=host.mac.get("vendor", ""),
                                        addrtype=host.mac.get("type", ""))))
                writer.endElement("address")
            # End of Address elements
            #########################


            ###################
            # Hostnames element
            writer.startElement("hostnames", Attributes({}))

            for hname in host.hostnames:
                writer.startElement("hostname",
                        Attributes(dict(name = hname.get("hostname", ""),
                                    type = hname.get("hostname_type", ""))))

                writer.endElement("hostname")

            writer.endElement("hostnames")
            # End of Hostnames element
            ##########################


            ###############
            # Ports element
            writer.startElement("ports", Attributes({}))

            ## Extraports elements
            for ext in host.get_extraports():
                writer.startElement("extraports",
                    Attributes(dict(count = ext.get("count", ""),
                                    state = ext.get("state", ""))))
                writer.endElement("extraports")

            ## Port elements
            for p in host.ports:
                writer.startElement("port",
                    Attributes(dict(portid = p.get("portid", ""),
                                    protocol = p.get("protocol", ""))))

                ### Port state
                writer.startElement("state",
                    Attributes(dict(state=p.get("port_state", ""),
                                    reason=p.get("reason", ""),
                                    reason_ttl=p.get("reason_ttl", ""))))
                writer.endElement("state")

                ### Port service info
                d = {}
                for xml_attr, member in (("conf", "service_conf"),
                        ("method", "service_method"),
                        ("name", "service_name"),
                        ("product", "service_product"),
                        ("version", "service_version"),
                        ("extrainfo", "service_extrainfo")):
                    if p.get(member):
                        d[xml_attr] = p.get(member)
                writer.startElement("service", Attributes(d))
                writer.endElement("service")

                writer.endElement("port")

            writer.endElement("ports")
            # End of Ports element
            ######################


            ############
            # OS element
            writer.startElement("os", Attributes({}))

            ## Ports used elements
            for pu in host.ports_used:
                writer.startElement("portused",
                            Attributes(dict(state = pu.get("state", ""),
                                            proto = pu.get("proto", ""),
                                            portid = pu.get("portid", ""))))
                writer.endElement("portused")

            ## Osmatch elements
            for om in host.osmatches:
                writer.startElement("osmatch",
                    Attributes(dict(name = om.get("name", ""),
                                    accuracy = om.get("accuracy", ""),
                                    line = om.get("line", ""))))
                ## Osclass elements
                for oc in om['osclasses']:
                    writer.startElement("osclass",
                        Attributes(dict(vendor = oc.get("vendor", ""),
                                        osfamily = oc.get("osfamily", ""),
                                        type = oc.get("type", ""),
                                        osgen = oc.get("osgen", ""),
                                        accuracy = oc.get("accuracy", ""))))
                    writer.endElement("osclass")
                writer.endElement("osmatch")

            writer.endElement("os")
            # End of OS element
            ###################

            # Uptime element
            writer.startElement("uptime",
                Attributes(dict(seconds = host.uptime.get("seconds", ""),
                            lastboot = host.uptime.get("lastboot", ""))))
            writer.endElement("uptime")

            #####################
            # Sequences elementes
            ## TCP Sequence element
            # Cannot use dict() here, because of the 'class' attribute.
            writer.startElement("tcpsequence",
                Attributes({"index":host.tcpsequence.get("index", ""),
                        "difficulty":host.tcpsequence.get("difficulty", ""),
                        "values":host.tcpsequence.get("values", "")}))
            writer.endElement("tcpsequence")

            ## IP ID Sequence element
            writer.startElement("ipidsequence",
                Attributes({"class":host.ipidsequence.get("class", ""),
                            "values":host.ipidsequence.get("values", "")}))
            writer.endElement("ipidsequence")

            ## TCP TS Sequence element
            writer.startElement("tcptssequence",
                Attributes({"class":host.tcptssequence.get("class", ""),
                        "values":host.tcptssequence.get("values", "")}))
            writer.endElement("tcptssequence")
            # End of sequences elements
            ###########################

            ## Trace element
            if len(host.trace) > 0:
                writer.startElement("trace",
                    Attributes({"proto":host.trace.get("proto", ""),
                                "port":host.trace.get("port", "")}))

                if "hops" in host.trace:
                    for hop in host.trace["hops"]:
                        writer.startElement("hop",
                            Attributes({"ttl":hop["ttl"],
                                        "rtt":hop["rtt"],
                                        "ipaddr":hop["ipaddr"],
                                        "host":hop["host"]}))
                        writer.endElement("hop")

                if "error" in host.trace:
                    writer.startElement("error",
                        Attributes({"errorstr":host.trace["error"]}))
                    writer.endElement("error")

                writer.endElement("trace")
            # End of trace element
            ###########################

            # End host element
            writer.endElement("host")

    def _write_debugging(self, writer):
        writer.startElement("debugging", Attributes(dict(
                                            level=str(self.debugging_level))))
        writer.endElement("debugging")

    def _write_verbose(self, writer):
        writer.startElement("verbose", Attributes(dict(
                                            level=str(self.verbose_level))))
        writer.endElement("verbose")

    def _write_scaninfo(self, writer):
        for scan in self.scaninfo:
            writer.startElement("scaninfo",
                Attributes(dict(type = scan.get("type", ""),
                                protocol = scan.get("protocol", ""),
                                numservices = scan.get("numservices", ""),
                                services = scan.get("services", ""))))
            writer.endElement("scaninfo")

    def _write_nmaprun(self, writer):
        writer.startElement("nmaprun",
                Attributes(dict(args = str(self.nmap_command),
                                profile_name = str(self.profile_name),
                                scanner = str(self.scanner),
                                start = str(self.start),
                                startstr = time.ctime(time.mktime(self.get_date())),
                                version = str(self.scanner_version),
                                xmloutputversion = str(XML_OUTPUT_VERSION))))

    def set_unsaved(self):
        self.unsaved = True

    def is_unsaved(self):
        return self.unsaved

def nmap_parser_sax():
    parser = make_parser()
    nmap_parser = NmapParserSAX()

    parser.setContentHandler(nmap_parser)
    nmap_parser.set_parser(parser)

    return nmap_parser

NmapParser = nmap_parser_sax

if __name__ == '__main__':
    import sys

    file_to_parse = sys.argv[1]

    np = NmapParser()
    np.parse_file(file_to_parse)

    for host in np.hosts:
        print "%s:" % host.ip["addr"]
        print "  Comment:", repr(host.comment)
        print "  TCP sequence:", repr(host.tcpsequence)
        print "  TCP TS sequence:", repr(host.tcptssequence)
        print "  IP ID sequence:", repr(host.ipidsequence)
        print "  Uptime:", repr(host.uptime)
        print "  OS Match:", repr(host.osmatches)
        print "  Ports:"
        for p in host.ports:
            print "\t%s" % repr(p)
        print "  Ports used:", repr(host.ports_used)
        print "  OS Matches:", repr(host.osmatches)
        print "  Hostnames:", repr(host.hostnames)
        print "  IP:", repr(host.ip)
        print "  IPv6:", repr(host.ipv6)
        print "  MAC:", repr(host.mac)
        print "  State:", repr(host.state)
        if "hops" in host.trace:
            print "  Trace:"
            for hop in host.trace["hops"]:
                print "    ", repr(hop)
            print
        if host.hostscripts:
            print "  Hostscripts:"
            for s in host.hostscripts:
                print "\t%s:" % repr(p.id)
                print "\t\t".join(repr(p.output))

########NEW FILE########
__FILENAME__ = NSEDocParser
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import re

class NSEDocEvent (object):
    def __init__(self, type, text = None):
        self.type = type
        self.text = text

def nsedoc_parse_sub(text, pos):
    """Parse paragraph-level NSEDoc markup, inside of paragraphs and lists.
    Returns the position after the end of parsing followed by a list of
    events."""
    events = []
    m = re.match(r'([^\n]*?)<code>(.*?)</code>', text[pos:], re.S)
    if m:
        if m.group(1):
            events.append(NSEDocEvent("text", m.group(1).replace("\n", " ")))
        events.append(NSEDocEvent("code", m.group(2)))
        return pos + m.end(), events
    m = re.match(r'[^\n]*(\n|$)', text[pos:])
    if m:
        if m.group():
            events.append(NSEDocEvent("text", m.group().replace("\n", " ")))
        return pos + m.end(), events
    return pos, events

def nsedoc_parse(text):
    """Parse text marked up for NSEDoc. This is a generator that returns a
    sequence of NSEDocEvents. The type of the event may be "paragraph_start",
    "paragraph_end", "list_start", "list_end", "list_item_start",
    "list_item_end", "text", or "code". The types "text" and "code" have a text
    member with the text that they contain."""
    i = 0
    j = 0
    in_list = False

    while i < len(text):
        while i < len(text) and text[i].isspace():
            i += 1
        if i >= len(text):
            break
        yield NSEDocEvent("paragraph_start")
        while i < len(text):
            if re.match(r'\s*(\n|$)', text[i:]):
                break
            if text.startswith("* ", i):
                if not in_list:
                    yield NSEDocEvent("list_start")
                    in_list = True
                i += 2
                yield NSEDocEvent("list_item_start")
                i, events = nsedoc_parse_sub(text, i)
                for event in events:
                    yield event
                yield NSEDocEvent("list_item_end")
            else:
                if in_list:
                    yield NSEDocEvent("list_end")
                    in_list = False
                i, events = nsedoc_parse_sub(text, i)
                for event in events:
                    yield event
        if in_list:
            yield NSEDocEvent("list_end")
            in_list = False
        yield NSEDocEvent("paragraph_end")

########NEW FILE########
__FILENAME__ = Paths
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

from os.path import join, dirname

import errno
import os
import os.path
import sys
import shutil

from zenmapCore_Kvasir.BasePaths import base_paths, fs_dec
from zenmapCore_Kvasir.Version import VERSION
from zenmapCore_Kvasir.Name import APP_NAME
from zenmapCore_Kvasir.UmitOptionParser import option_parser

# Find out the prefix under which data files (interface definition XML,
# pixmaps, etc.) are stored. This can vary depending on whether we are running
# in an executable package and what type of package it is, which we check using
# the sys.frozen attribute. See
# http://mail.python.org/pipermail/pythonmac-sig/2004-November/012121.html.
def get_prefix():
    frozen = getattr(sys, "frozen", None)
    if frozen == "macosx_app":
        # A py2app .app bundle.
        return os.path.join(dirname(fs_dec(sys.executable)), "..", "Resources")
    elif frozen is not None:
        # Assume a py2exe executable.
        return dirname(fs_dec(sys.executable))
    else:
        # Normal script execution. Look in the current directory to allow
        # running from the distribution.
        return os.path.abspath(os.path.dirname(fs_dec(sys.argv[0])))

prefix = get_prefix()

# These lines are overwritten by the installer to hard-code the installed
# locations.
CONFIG_DIR = join(prefix, "share", APP_NAME, "config")
LOCALE_DIR = join(prefix, "share", APP_NAME, "locale")
MISC_DIR = join(prefix, "share", APP_NAME, "misc")
PIXMAPS_DIR = join(prefix, "share", "zenmap", "pixmaps")
DOCS_DIR = join(prefix, "share", APP_NAME, "docs")
NMAPDATADIR = join(prefix, "..")
def get_extra_executable_search_paths():
    """Return a list of additional executable search paths as a convenience for
    platforms where the default PATH is inadequate."""
    if sys.platform == 'darwin':
        return ["/usr/local/bin"]
    elif sys.platform == 'win32':
        return [dirname(sys.executable)]
    return []

#######
# Paths
class Paths(object):
    """Paths
    """
    hardcoded = ["config_dir",
                 "locale_dir",
                 "pixmaps_dir",
                 "misc_dir",
                 "docs_dir"]

    config_files_list = ["config_file",
                         "scan_profile",
                         "version"]

    empty_config_files_list = ["target_list",
                               "recent_scans",
                               "db"]

    misc_files_list = ["options",
                       "profile_editor"]

    def __init__(self):
        self.user_config_dir = option_parser.get_confdir()
        self.user_config_file = os.path.join(self.user_config_dir, base_paths['user_config_file'])
        self.config_dir = CONFIG_DIR
        self.locale_dir = LOCALE_DIR
        self.pixmaps_dir = PIXMAPS_DIR
        self.misc_dir = MISC_DIR
        self.docs_dir = DOCS_DIR
        self.nmap_dir = NMAPDATADIR

    def __getattr__(self, name):
        if name in self.hardcoded:
            return self.__dict__[name]

        elif name in self.config_files_list:
            return return_if_exists(join(self.user_config_dir, base_paths[name]))

        elif name in self.empty_config_files_list:
            return return_if_exists(join(self.user_config_dir, base_paths[name]), True)

        elif name in self.misc_files_list:
            return return_if_exists(join(self.misc_dir, base_paths[name]))

        try:
            return self.__dict__[name]
        except:
            raise NameError(name)

    def __setattr__(self, name, value):
        self.__dict__[name] = value

def create_dir(path):
    """Create a directory with os.makedirs without raising an error if the
        directory already exists."""
    try:
        os.makedirs(path)
    except OSError, e:
        if e.errno != errno.EEXIST:
            raise

def create_user_config_dir(user_dir, template_dir):
    """Create a user configuration directory by creating the directory if
    necessary, then copying all the files from the given template directory,
    skipping any that already exist."""
    from zenmapCore_Kvasir.UmitLogging import log
    log.debug(">>> Create user dir at %s" % user_dir)
    create_dir(user_dir)

    for filename in os.listdir(template_dir):
        template_filename = os.path.join(template_dir, filename)
        user_filename = os.path.join(user_dir, filename)
        # Only copy regular files.
        if not os.path.isfile(template_filename):
            continue
        # Don't overwrite existing files.
        if os.path.exists(user_filename):
            log.debug(">>> %s already exists." % user_filename)
            continue
        shutil.copyfile(template_filename, user_filename)
        log.debug(">>> Copy %s to %s." % (template_filename, user_filename))

def return_if_exists(path, create=False):
    path = os.path.abspath(path)
    if os.path.exists(path):
        return path
    elif create:
        f = open(path, "w")
        f.close()
        return path
    raise Exception("File '%s' does not exist or could not be found!" % path)

############
# Singleton!
Path = Paths()

if __name__ == '__main__':
    print ">>> SAVED DIRECTORIES:"
    print ">>> LOCALE DIR:", Path.locale_dir
    print ">>> PIXMAPS DIR:", Path.pixmaps_dir
    print ">>> CONFIG DIR:", Path.config_dir
    print
    print ">>> FILES:"
    print ">>> USER CONFIG FILE:", Path.user_config_file
    print ">>> CONFIG FILE:", Path.user_config_file
    print ">>> TARGET_LIST:", Path.target_list
    print ">>> PROFILE_EDITOR:", Path.profile_editor
    print ">>> SCAN_PROFILE:", Path.scan_profile
    print ">>> RECENT_SCANS:", Path.recent_scans
    print ">>> OPTIONS:", Path.options
    print
    print ">>> DB:", Path.db
    print ">>> VERSION:", Path.version

########NEW FILE########
__FILENAME__ = RecentScans
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

from os import access, R_OK, W_OK
from os.path import dirname
from zenmapCore.Paths import Path

class RecentScans(object):
    def __init__(self):
        self.temp_list = []

        try:
            self.recents_scans_file = Path.recent_scans
        except:
            self.recents_scans_file = False

        if self.recents_scans_file and \
            (access(self.recents_scans_file, R_OK and W_OK) or \
             access(dirname(self.recents_scans_file), R_OK and W_OK)):
            self.using_file = True

            # Recovering saved targets
            recent_file = open(self.recents_scans_file, "r")
            self.temp_list = [t for t in recent_file.read().split(";") \
                                    if t != "" and t != "\n"]
            recent_file.close()
        else:
            self.using_file = False

    def save(self):
        if self.using_file:
            recent_file = open(self.recents_scans_file, "w")
            recent_file.write(";".join(self.temp_list))
            recent_file.close()

    def add_recent_scan(self, recent_scan):
        if recent_scan in self.temp_list:
            return

        self.temp_list.append(recent_scan)
        self.save()

    def clean_list(self):
        del self.temp_list
        self.temp_list = []
        self.save()

    def get_recent_scans_list(self):
        t = self.temp_list[:]
        t.reverse()
        return t

recent_scans = RecentScans()

if __name__ == "__main__":
    import sys
    from os.path import split
    r = RecentScans()
    print ">>> Getting empty list:", r.get_recent_scans_list()
    print ">>> Adding recent scan bla:", r.add_recent_scan("bla")
    print ">>> Getting recent scan list:", r.get_recent_scans_list()
    del r

########NEW FILE########
__FILENAME__ = ScriptArgsParser
#!/usr/bin/env python

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

# This module parses the --script-args and stores in the form of key/value
# pairs. The logic is same as in nse_main.lua, except that values are not
# returned as tables but as strings.

import re
# "^%s*([^'\"%s{},=][^{},=]-)%s*[},=]"
unquoted_re = re.compile(r'\s*([^\'"\s{},=][^{},=]*?)\s*([},=]|$)')
# "^%s*(['\"])(.-[^\\])%1%s*[},=]"
quoted_re = re.compile(r'\s*(([\'"])(.*?[^\\])\2)\s*([},=]|$)')
# "^%s*(['\"])%1%s*[},=]"
empty_re = re.compile(r'\s*(([\'"])\2)\s*([},=]|$)')

def parse_string(s, start):
    """Parses a single string that is quoted, unquoted or empty. It returns the
    found string along with the next starting position """
    for pattern in unquoted_re, quoted_re, empty_re:
        m = pattern.match(s, start) or quoted_re.match(s, start)
        if m:
            return m.group(1), m.end(1)
        raise ValueError("No string found at %s." % repr(s[start:]))

def next_char(s, start):
    """Returns the next character and position in the string."""
    while start < len(s) and s[start].isspace():
        start += 1
    if start < len(s):
        return s[start], start
    else:
        return None, start

def parse_value(s, start):
    """If the string starting from start is a name-value pair, returns a
    name-value tuple. Otherwise returns a plain string."""
    nc, j = next_char(s, start)
    if nc == "{":
        j = parse_table(s, j)
        return s[start:j], j
    else:
        tmp, j = parse_string(s, j)
        nc, j = next_char(s, j)
        if nc == "=":
            # Key/value?
            j += 1
            begin = j
            nc, j = next_char(s, j)
            if nc == "{":
                j = parse_table(s, j)
            else:
                dummy, j = parse_string(s, j)
            return (tmp, s[begin:j]), j
        else:
            return s[start:j], j

def parse_table(s, start):
    """This function is responsible for parsing a table; i.e, a string that
    starts with '{'. It returns the position where the balancing pair of braces
    gets closed."""
    nc, j = next_char(s, start)
    if not nc or nc != "{":
        raise ValueError("No '{' found at %s." % repr(s[start:]))
    j += 1
    while True:
        nc, j = next_char(s, j)
        if nc == "}":
            # End of table.
            return j + 1
        else:
            # Replace this with a call to parse_value.
            v, j = parse_value(s, j)
            nc, j = next_char(s, j)
            if nc == ",":
                j += 1

def parse_script_args(s):
    """Main function responsible for parsing the script args and storing the
    name-value pairs in a list. If an invalid argument is present it stores the
    value as None."""
    args = []
    nc, j = next_char(s, 0)
    try:
        while nc is not None:
            val, j = parse_value(s, j)
            if type(val) == str:
                raise ValueError("Only name-value pairs expected in parse_script_args.")
            else:
                args.append(val)
            nc, j = next_char(s, j)
            if nc == ",":
                j += 1
                nc, j = next_char(s, j)
    except ValueError:
        return None
    return args

def parse_script_args_dict(raw_argument):
    """Wrapper function that copies the name-value pairs from a list into a
    dictionary."""
    args_dict = {}
    args = parse_script_args(raw_argument)
    if args is None:
        return None
    for item in args:
        if(len(item) == 2): # only key/value pairs are stored
            args_dict[item[0]] = item[1]
    return args_dict

if __name__ == '__main__':
    TESTS = (
        ('', []),
        ('a=b,c=d', [('a', 'b'), ('c', 'd')]),
        ('a="b=c"', [('a', '"b=c"')]),
        ('a="def\\"ghi"', [('a', '"def\\"ghi"')]),
        ('a={one,{two,{three}}}', [('a', '{one,{two,{three}}}')]),
        ('a={"quoted}quoted"}', [('a', '{"quoted}quoted"}')]),
        ('a="unterminated', None),
        ('user=foo,pass=",{}=bar",whois={whodb=nofollow+ripe},userdb=C:\\Some\\Path\\To\\File', [('user', 'foo'), ('pass', '",{}=bar"'), ('whois', '{whodb=nofollow+ripe}'), ('userdb', 'C:\\Some\\Path\\To\\File')]),
     )

    for test, expected in TESTS:
        args_dict = parse_script_args_dict(test)
        print args_dict
        args = parse_script_args(test)
        if args == expected:
            print "PASS" , test
            continue
        print "FAIL", test
        if args is None:
            print "Parsing error"
        else:
            print "%d args" % len(args)
            for a, v in args:
                print a, "=", v
        if expected is None:
            print "Expected parsing error"
        else:
            print "Expected %d args" % len(expected)
            for a, v in expected:
                print a, "=", v

########NEW FILE########
__FILENAME__ = ScriptMetadata
#!/usr/bin/env python

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

# This module has two classes. ScriptDB is responsible for parsing the
# script.db file and fetching each script's name and categories.
# ScriptMetadata gets the description, categories, @usage, @output, and
# arguments from the script itself.

import re
import os
import sys

##-----------[ Kvasir changes ]---------------##
#from zenmapCore.Paths import Path
##-----------[ Kvasir changes ]---------------##
from zenmapCore_Kvasir.UmitLogging import log

class ScriptDB (object):
    """Class responsible for parsing the script.db file, fetching script
    names and categories."""
    LUA_STRING_ESCAPES = {
        "a": "\a", "b": "\b", "f": "\f", "n": "\n", "r": "\r",
        "t": "\t", "v": "\v", "\\": "\\", "\"": "\"", "'": "'", "0": "\0"
    }
    def __init__(self, script_db_path = None):
        self.unget_buf = ""

        self.f = open(script_db_path, "r")
        try:
            self.entries_list = self.parse()
        finally:
            self.f.close()

    def getchar(self):
        if self.unget_buf:
            c = self.unget_buf[-1]
            self.unget_buf = self.unget_buf[:-1]
            return c
        else:
            return self.f.read(1)

    def unget(self, data):
        if data:
            self.unget_buf += data

    def parse(self):
        """Parses a script.db entry and returns it as a dictionary. An entry
        looks like this:
        Entry { filename = "afp-brute.nse", categories = { "auth", "intrusive", } }
        """
        entries = []
        while True:
            entry = self.parse_entry()
            if not entry:
                break
            entries.append(entry)
        return entries

    def token(self):
        """Returns a tuple whose first element is a type ("string", "ident", or
        "delim") and whose second element is the token text."""
        c = self.getchar()
        while c.isspace():
            c = self.getchar()
        if not c:
            return None
        if c.isalpha() or c == "_":
            ident = []
            while c.isalpha() or c.isdigit() or c == "_":
                ident.append(c)
                c = self.getchar()
            self.unget(c)
            return ("ident", "".join(ident))
        elif c in "'\"":
            string = []
            begin_quote = c
            c = self.getchar()
            while c != begin_quote:
                if c == "\\":
                    repl = None
                    c = self.getchar()
                    if not c:
                        raise ScriptDBSyntaxError()
                    if c.isdigit():
                        d1 = c
                        d2 = self.getchar()
                        d3 = self.getchar()
                        if d1 and d2 and d3:
                            n = int(d1 + d2 + d3)
                            if n > 255:
                                raise ScriptDBSyntaxError()
                            repl = chr(n)
                        else:
                            self.unget(d3)
                            self.unget(d2)
                    if not repl:
                        repl = self.LUA_STRING_ESCAPES.get(c)
                    if not repl:
                        raise ScriptDBSyntaxError()
                    c = repl
                string.append(c)
                c = self.getchar()
            return ("string", "".join(string))
        elif c in "{},=":
            return ("delim", c)
        else:
            raise ScriptDBSyntaxError()

    def expect(self, tokens):
        for token in tokens:
            t = self.token()
            if t != token:
                raise ScriptDBSyntaxError()

    def parse_entry(self):
        entry = {}
        token = self.token()
        if not token:
            return None
        self.expect((("delim", "{"), ("ident", "filename"), ("delim", "=")))
        token = self.token()
        if not token or token[0] != "string":
            raise ScriptDBSyntaxError()
        entry["filename"] = token[1]
        self.expect((("delim", ","), ("ident", "categories"), ("delim", "="), ("delim", "{")))
        entry["categories"] = []
        token = self.token()
        if token and token[0] == "string":
            entry["categories"].append(token[1])
        token = self.token()
        while token == ("delim", ","):
            token = self.token()
            if token and token[0] == "string":
                entry["categories"].append(token[1])
            else:
                break
            token = self.token()
        if token != ("delim", "}"):
            raise ScriptDBSyntaxError()
        token = self.token()
        if token == ("delim", ","):
            token = self.token()
        if token != ("delim", "}"):
            raise ScriptDBSyntaxError()
        return entry

    def get_entries_list(self):
        return self.entries_list

def nsedoc_tags_iter(f):
    in_doc_comment = False
    tag_name = None
    tag_text = None
    for line in f:
        # New LuaDoc comment?
        if re.match(r'^\s*---', line):
            in_doc_comment = True
        if not in_doc_comment:
            continue
        # New LuaDoc tag?
        m = re.match(r'^\s*--+\s*@(\w+)\s*(.*)', line, re.S)
        if m:
            if tag_name:
                yield tag_name, tag_text
            tag_name = None
            tag_text = None
            tag_name = m.group(1)
            tag_text = m.group(2)
        else:
            # Still in comment?
            m = re.match(r'^\s*--+\s*(.*)', line)
            if m:
                # Add to text if we're in a tag.
                if tag_name:
                    tag_text += m.group(1) + "\n"
            else:
                in_doc_comment = False
                if tag_name:
                    yield tag_name, tag_text
                tag_name = None
                tag_text = None

class ScriptMetadata (object):
    """Class responsible for parsing all the script information."""

    class Entry (object):
        """An instance of this class is used to store all the information
        related to a particular script."""
        def __init__(self, filename):
            self.filename = filename
            self.categories = []
            self.arguments = [] # Arguments including library arguments.
            self.license = ""
            self.author = ""
            self.description = ""
            self.output = ""
            self.usage = ""

        url = property(lambda self: "http://nmap.org/nsedoc/scripts/" + os.path.splitext(self.filename)[0] + ".html")

    def __init__(self, scripts_dir, nselib_dir):
        self.scripts_dir = scripts_dir
        self.nselib_dir = nselib_dir
        self.library_arguments = {}
        self.library_requires = {}
        self.construct_library_arguments()

    def get_metadata(self, filename):
        entry = self.Entry(filename)
        entry.description = self.get_string_variable(filename, "description")
        entry.arguments = self.get_arguments(entry.filename)
        entry.license = self.get_string_variable(filename, "license")
        entry.author = self.get_string_variable(filename, "author")

        filepath = os.path.join(self.scripts_dir, filename)
        f = open(filepath, "r")
        try:
            for tag_name, tag_text in nsedoc_tags_iter(f):
                if tag_name == "output" and not entry.output:
                    entry.output = tag_text
                elif tag_name == "usage" and not entry.usage:
                    entry.usage = tag_text
        finally:
            f.close()

        return entry

    @staticmethod
    def get_file_contents(filename):
        f = open(filename, "r")
        try:
            contents = f.read()
        finally:
            f.close()
        return contents

    def get_string_variable(self, filename, varname):
        contents = ScriptMetadata.get_file_contents(os.path.join(self.scripts_dir, filename))
        # Short string?
        m = re.search(re.escape(varname) + r'\s*=\s*(["\'])(.*?[^\\])\1', contents)
        if m:
            return m.group(2)
        # Long string?
        m = re.search(re.escape(varname) + r'\s*=\s*\[(=*)\[(.*?)\]\1\]', contents, re.S)
        if m:
            return m.group(2)
        return None

    @staticmethod
    def get_requires(filename):
        f = open(filename, "r")
        try:
            requires = ScriptMetadata.get_requires_from_file(f)
        finally:
            f.close()
        return requires

    @staticmethod
    def get_requires_from_file(f):
        require_expr = re.compile(r'.*\brequire\s*\(?([\'\"])([\w._-]+)\1\)?')
        requires = []
        for line in f.readlines():
            m = require_expr.match(line)
            if m:
                requires.append(m.group(2))
        return requires

    @staticmethod
    def get_script_args(filename):
        f = open(filename, "r")
        try:
            args = ScriptMetadata.get_script_args_from_file(f)
        finally:
            f.close()
        return args

    @staticmethod
    def get_script_args_from_file(f):
        """Extracts a list of script arguments from the file given. Results are
        returned as a list of (argname, description) tuples."""
        args = []
        for tag_name, tag_text in nsedoc_tags_iter(f):
            m = re.match(r'([\w._-]+)', tag_text)
            if (tag_name == "arg" or tag_name == "args") and m:
                args.append((m.group(1), re.sub(r'^[\w._-]+','',tag_text)))
        return args

    def get_arguments(self, filename):
        """Returns list of arguments including library arguments on
        passing the file name."""
        filepath = os.path.join(self.scripts_dir, filename)
        script_args = self.get_script_args(filepath)

        # Recursively walk through the libraries required by the script (and
        # the libraries they require, etc.), adding all arguments.
        library_args = []
        seen = set()
        pool = set(self.get_requires(filepath))
        while pool:
            require = pool.pop()
            if require in seen:
                continue
            seen.add(require)
            sub_requires = self.library_requires.get(require)
            if sub_requires:
                pool.update(set(sub_requires))
            require_args = self.library_arguments.get(require)
            if require_args:
                library_args += require_args

        return script_args + library_args

    def construct_library_arguments(self):
        """Constructs a dictionary of library arguments using library
        names as keys and arguments as values. Each argument is really a
        (name, description) tuple."""
        for filename in os.listdir(self.nselib_dir):
            filepath = os.path.join(self.nselib_dir, filename)
            if not os.path.isfile(filepath):
                continue

            base, ext = os.path.splitext(filename)
            if ext == ".lua" or ext == ".luadoc":
                libname = base
            else:
                libname = filename

            self.library_arguments[libname] = self.get_script_args(filepath)
            self.library_requires[libname] = self.get_requires(filepath)

def get_script_entries(scripts_dir, nselib_dir):
    """Merge the information obtained so far into one single entry for
    each script and return it."""
    metadata = ScriptMetadata(scripts_dir, nselib_dir)
    try:
        scriptdb = ScriptDB(os.path.join(scripts_dir, "script.db"))
    except IOError:
        return []
    entries = []
    for dbentry in scriptdb.get_entries_list():
        entry = metadata.get_metadata(dbentry["filename"])
        # Categories is the only thing ScriptMetadata doesn't take care of.
        entry.categories = dbentry["categories"]
        entries.append(entry)
    return entries

if __name__ == '__main__':
    for entry in get_script_entries():
        print "*" * 75
        print "Filename:", entry.filename
        print "Categories:", entry.categories
        print "License:", entry.license
        print "Author:", entry.author
        print "URL:", entry.url
        print "Description:", entry.description
        print "Arguments:", [x[0] for x in entry.arguments]
        print "Output:"
        print entry.output
        print "Usage:"
        print entry.usage
        print "*" * 75

########NEW FILE########
__FILENAME__ = SearchResult
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import os
import os.path
import re
import StringIO
import unittest

from glob import glob
from types import StringTypes

from zenmapCore.Name import APP_NAME
from zenmapCore.NmapOptions import NmapOptions
from zenmapCore.NmapParser import NmapParser
from zenmapCore.UmitLogging import log

class HostSearch(object):
    @staticmethod
    def match_target(host, name):
        name = name.lower()
        mac = host.get_mac()
        ip = host.get_ip()
        ipv6 = host.get_ipv6()

        if mac and mac.has_key('addr'):
            if name in mac['addr'].lower(): return True
        if ip and ip.has_key('addr'):
            if name in ip['addr'].lower(): return True
        if ipv6 and ipv6.has_key('addr'):
            if name in ipv6['addr'].lower(): return True

        if HostSearch.match_hostname(host, name):
            return True
        return False
    @staticmethod
    def match_hostname(host, hostname):
        hostname = hostname.lower()
        hostnames = host.get_hostnames()
        for hn in hostnames:
            if hostname in hn['hostname'].lower():
                return True
        else:
            return False
    @staticmethod
    def match_service(host, service):
        for port in host.get_ports():
            # We concatenate all useful fields and add them to the list
            if port['port_state'] not in ['open','open|filtered']:
                continue
            version = port.get("service_name", "") + " " + \
                      port.get("service_product", "") + " " + \
                      port.get("service_version", "") + " " + \
                      port.get("service_extrainfo", "")
            if service in version.lower():
                return True
        else:
            return False
    @staticmethod
    def match_os(host, os):
        os = os.lower()

        osmatches = host.get_osmatches()

        for osmatch in osmatches:
            os_str = osmatch['name'].lower()
            for osclass in osmatch['osclasses']:
                os_str += " " + osclass['vendor'].lower() + " " +\
                          osclass['osfamily'].lower() + " " +\
                          osclass['type'].lower()
            if os in os_str:
                return True

        return False
    @staticmethod
    def match_port(host_ports, port, port_state):
        # Check if the port is parsable, if not return False silently
        if re.match("^\d+$", port) == None:
            return False

        for hp in host_ports:
            if hp['portid'] == port and hp['port_state'] == port_state:
                return True

        return False

class SearchResult(object):
    def __init__(self):
        """This constructor is always called by SearchResult subclasses."""
        pass

    def search(self, **kargs):
        """Performs a search on each parsed scan. Since the 'and' operator is
        implicit, the search fails as soon as one of the tests fails. The
        kargs argument is a map having operators as keys and argument lists as
        values."""

        for scan_result in self.get_scan_results():
            self.parsed_scan = scan_result

            # Test each given operator against the current parsed result
            for operator, args in kargs.iteritems():
                if not self._match_all_args(operator, args):
                    # No match => we discard this scan_result
                    break
            else:
                # All operator-matching functions have returned True, so this scan_result
                # satisfies all conditions
                yield self.parsed_scan

    def _match_all_args(self, operator, args):
        """A helper function that calls the matching function for the given
        operator and each of its arguments."""
        for arg in args:
            positive = True
            if arg != "" and arg[0] == "!":
                arg = arg[1:]
                positive = False
            if positive != self.__getattribute__("match_%s" % operator)(arg):
                # No match for this operator
                return False
        else:
            # All arguments for this operator produced a match
            return True

    def get_scan_results(self):
        # To be implemented by classes that are going to inherit this one
        pass

    def basic_match(self, keyword, property):
        if keyword == "*" or keyword == "":
            return True

        return keyword.lower() in str(self.parsed_scan.__getattribute__(property)).lower()

    def match_keyword(self, keyword):
        log.debug("Match keyword: %s" % keyword)

        return self.basic_match(keyword, "nmap_output") or \
               self.match_profile(keyword) or \
               self.match_target(keyword)

    def match_profile(self, profile):
        log.debug("Match profile: %s" % profile)
        log.debug("Comparing: %s == %s ??" % (str(self.parsed_scan.profile_name).lower(),
                                              "*%s*" % profile.lower()))
        if profile == "*" or profile == "" or \
           profile.lower() in str(self.parsed_scan.profile_name).lower():
            return True
        return False

    def match_option(self, option):
        log.debug("Match option: %s" % option)

        if option == "*" or option == "":
            return True

        # NOTE: Option matching treats "_" and "-" the same, just like the optcmp
        #       function in utils.cc . Also, option matching is case-sensitive.
        option = option.replace("_", "-")

        ops = NmapOptions()
        ops.parse_string(self.parsed_scan.get_nmap_command())

        if "(" in option and ")" in option:
            # The syntax allows matching option arguments as "opt:option_name(value)".
            # Since we've received only the "option_name(value)" part, we need to parse it.
            optname = option[:option.find("(")]
            optval = option[option.find("(")+1:option.find(")")]

            val = ops["--" + optname]
            if val is None:
                val = ops["-" + optname]
            if val is None:
                return False
            return str(val) == optval or str(val) == optval
        else:
            return ops["--" + option] is not None or ops["-" + option] is not None

    def match_date(self, date_arg, operator="date"):
        # The parsed scan's get_date() returns a time.struct_time, so we
        # need to convert it to a date object
        from datetime import date, datetime
        scd = self.parsed_scan.get_date()
        scan_date = date(scd.tm_year, scd.tm_mon, scd.tm_mday)

        # Check if we have any fuzzy operators ("~") in our string
        fuzz = 0
        if "~" in date_arg:
            # Count 'em, and strip 'em
            fuzz = date_arg.count("~")
            date_arg = date_arg.replace("~", "")

        if re.match("\d\d\d\d-\d\d-\d\d$", date_arg) != None:
            year, month, day = date_arg.split("-")
            parsed_date = date(int(year), int(month), int(day))
        elif re.match("[-|\+]\d+$", date_arg):
            # We need to convert from the "-n" format (n days ago) to a date object
            # (I found this in some old code, don't ask :) )
            parsed_date = date.fromordinal(date.today().toordinal() + int(date_arg))
        else:
            # Fail silently
            return False

        # Now that we have both the scan date and the user date converted to date objects,
        # we need to make a comparison based on the operator (date, after, before).
        if operator == "date":
            return abs((scan_date - parsed_date).days) <= fuzz
        # We ignore fuzziness for after: and before:
        elif operator == "after":
            return (scan_date - parsed_date).days >= 0
        elif operator == "before":
            return (parsed_date - scan_date).days >= 0

    def match_after(self, date_arg):
        return self.match_date(date_arg, operator="after")

    def match_before(self, date_arg):
        return self.match_date(date_arg, operator="before")

    def match_target(self, target):
        log.debug("Match target: %s" % target)

        for spec in self.parsed_scan.get_targets():
            if target in spec:
                return True
        else:
            # We search the (rDNS) hostnames list
            for host in self.parsed_scan.get_hosts():
                if HostSearch.match_target(host, target):
                    return True
        return False

    def match_os(self, os):
        # If you have lots of big scans in your DB (with a lot of hosts scanned),
        # you're probably better off using the keyword (freetext) search. Keyword
        # search just greps through the nmap output, while this function iterates
        # through all parsed OS-related values for every host in every scan!
        hosts = self.parsed_scan.get_hosts()
        for host in hosts:
            if HostSearch.match_os(host, os):
                return True
        return False

    def match_scanned(self, ports):
        if ports == "":
            return True

        # Transform a comma-delimited string containing ports into a list
        ports = filter(lambda not_empty: not_empty, ports.split(","))

        # Check if they're parsable, if not return False silently
        for port in ports:
            if re.match("^\d+$", port) == None:
                return False

        # Make a list of all scanned ports
        services = []
        for scaninfo in self.parsed_scan.get_scaninfo():
            services.append( scaninfo["services"].split(",") )

        # These two loops iterate over search ports and over scanned ports. As soon as
        # the search finds a given port among the scanned ports, it breaks from the services
        # loop and continues with the next port in the ports list. If a port isn't
        # found in the services list, the function immediately returns False.
        for port in ports:
            for service in services:
                if "-" in service and \
                   int(port) >= int(service.split("-")[0]) and \
                   int(port) <= int(service.split("-")[1]):
                    # Port range, and our port was inside
                    break
                elif port == service:
                    break
            else:
                return False
        else:
            # The ports loop finished for all ports, which means the search was successful.
            return True

    def match_port(self, ports, port_state):
        log.debug("Match port:%s" % ports)

        # Transform a comma-delimited string containing ports into a list
        ports = filter(lambda not_empty: not_empty, ports.split(","))

        for host in self.parsed_scan.get_hosts():
            for port in ports:
                if not HostSearch.match_port(host.get_ports(), port, port_state):
                    break
            else:
                return True
        else:
            return False

    def match_open(self, port):
        return self.match_port(port, "open")

    def match_filtered(self, port):
        return self.match_port(port, "filtered")

    def match_closed(self, port):
        return self.match_port(port, "closed")

    def match_unfiltered(self, port):
        return self.match_port(port, "unfiltered")

    def match_open_filtered(self, port):
        return self.match_port(port, "open|filtered")

    def match_closed_filtered(self, port):
        return self.match_port(port, "closed|filtered")

    def match_service(self, sversion):
        if sversion == "" or sversion == "*":
            return True

        for host in self.parsed_scan.get_hosts():
            if HostSearch.match_service(host, sversion):
                return True
        else:
            return False

    def match_in_route(self, host):
        if host == "" or host == "*":
            return True
        host = host.lower()

        # Since the parser doesn't parse traceroute output, we need to cheat and look
        # the host up in the Nmap output, in the Traceroute section of the scan.
        nmap_out = self.parsed_scan.get_nmap_output()
        tr_pos = 0
        traceroutes = []        # A scan holds one traceroute section per host
        while tr_pos != -1:
            # Find the beginning and the end of the traceroute section, and append
            # the substring to the traceroutes list
            tr_pos = nmap_out.find("TRACEROUTE", tr_pos+1)
            tr_end_pos = nmap_out.find("\n\n", tr_pos)
            if tr_pos != -1:
                traceroutes.append(nmap_out[tr_pos:tr_end_pos])

        for tr in traceroutes:
            if host in tr.lower():
                return True
        else:
            return False

    def match_dir(self, dir):
        # The dir: operator is handled by the SearchParser class, we ignore it here.
        return True

class SearchDummy(SearchResult):
    """A dummy search class that returns no results. It is used as a placeholder
    when SearchDB can't be used."""
    def get_scan_results(self):
        return []

class SearchDB(SearchResult, object):
    def __init__(self):
        SearchResult.__init__(self)
        log.debug(">>> Getting scan results stored in data base")
        self.scan_results = []
        from zenmapCore.UmitDB import UmitDB
        u = UmitDB()

        for scan in u.get_scans():
            log.debug(">>> Retrieving result of scans_id %s" % scan.scans_id)
            log.debug(">>> Nmap xml output: %s" % scan.nmap_xml_output)

            try:
                buffer = StringIO.StringIO(scan.nmap_xml_output)
                parsed = NmapParser()
                parsed.parse(buffer)
                buffer.close()
            except Exception, e:
                log.warning(">>> Error loading scan with ID %u from database: %s" % (scan.scans_id, str(e)))
            else:
                self.scan_results.append(parsed)

    def get_scan_results(self):
        return self.scan_results

class SearchDir(SearchResult, object):
    def __init__(self, search_directory, file_extensions=["usr"]):
        SearchResult.__init__(self)
        log.debug(">>> SearchDir initialized")
        self.search_directory = search_directory

        if type(file_extensions) in StringTypes:
            self.file_extensions = file_extensions.split(";")
        elif type(file_extensions) == type([]):
            self.file_extensions = file_extensions
        else:
            raise Exception("Wrong file extension format! '%s'" % file_extensions)

        log.debug(">>> Getting directory's scan results")
        self.scan_results = []
        files = []
        for ext in self.file_extensions:
            files += glob(os.path.join(self.search_directory, "*.%s" % ext))

        log.debug(">>> Scan results at selected directory: %s" % files)
        for scan_file in files:
            log.debug(">>> Retrieving scan result %s" % scan_file)
            if os.access(scan_file, os.R_OK) and os.path.isfile(scan_file):

                try:
                    parsed = NmapParser()
                    parsed.parse_file(scan_file)
                except:
                    pass
                else:
                    self.scan_results.append(parsed)

    def get_scan_results(self):
        return self.scan_results

class SearchResultTest(unittest.TestCase):
    class SearchClass(SearchResult):
        """This class is for use by the unit testing code"""
        def __init__(self, filenames):
            SearchResult.__init__(self)
            self.scan_results = []
            for filename in filenames:
                scan = NmapParser()
                scan.parse_file(filename)
                self.scan_results.append(scan)
        def get_scan_results(self):
            return self.scan_results

    def setUp(self):
        files = ["test/xml_test%d.xml" % no for no in range(1, 13)]
        self.search_result = self.SearchClass(files)

    def _test_skeleton(self, key, val):
        results = []
        search = {key:[val]}
        for scan in self.search_result.search(**search):
            results.append(scan)
        return len(results)
    def test_match_os(self):
        """Test that checks if the match_os predicate works"""
        assert(self._test_skeleton('os','linux') == 2)
    def test_match_target(self):
        """Test that checks if the match_target predicate works"""
        assert(self._test_skeleton('target','localhost') == 4)
    def test_match_port_open(self):
        """Test that checks if the match_open predicate works"""
        assert(self._test_skeleton('open', '22') == 7)
    def test_match_port_closed(self):
        """Test that checks if the match_closed predicate works"""
        assert(self._test_skeleton('open', '22') == 7)
        assert(self._test_skeleton('closed', '22') == 9)
    def test_match_service(self):
        """Test that checks if the match_service predicate works"""
        assert(self._test_skeleton('service', 'apache') == 9)
        assert(self._test_skeleton('service', 'openssh') == 7)
    def test_match_service_version(self):
        """Test that checks if the match_service predicate works when """
        """checking version"""
        assert(self._test_skeleton('service', '2.0.52') == 7)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = StringPool
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

UNIQUE_STRING_MAP = {}
def unique(s):
    """Return a single unique representation of s (unique as to id),
    letting s be garbage collected."""
    return UNIQUE_STRING_MAP.setdefault(s, s)

########NEW FILE########
__FILENAME__ = TargetList
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

from os import access, R_OK, W_OK
from os.path import dirname
from zenmapCore.Paths import Path

class TargetList(object):
    def __init__(self):
        self.temp_list = []

        try:
            self.target_list_file = Path.target_list
        except:
            self.target_list_file = False

        #import pdb; pdb.set_trace()
        if self.target_list_file and \
            (access(self.target_list_file, R_OK and W_OK) or \
             access(dirname(self.target_list_file), R_OK and W_OK)):
            self.using_file = True

            # Recovering saved targets
            target_file = open(self.target_list_file, "r")
            self.temp_list = [t for t in target_file.read().split(";") \
                                    if t != "" and t != "\n"]
            target_file.close()
        else:
            self.using_file = False

    def save(self):
        if self.using_file:
            target_file = open(self.target_list_file, "w")
            target_file.write(";".join(self.temp_list))
            target_file.close()

    def add_target(self, target):
        if target in self.temp_list:
            return

        self.temp_list.append(target)
        self.save()

    def clean_list(self):
        del self.temp_list
        self.temp_list = []
        self.save()

    def get_target_list(self):
        t = self.temp_list[:]
        t.reverse()
        return t

target_list = TargetList()

if __name__ == "__main__":
    import sys
    from os.path import split
    t = TargetList()
    print ">>> Getting empty list:", t.get_target_list()
    print ">>> Adding target 127.0.0.1:", t.add_target("127.0.0.3")
    print ">>> Getting target list:", t.get_target_list()
    del t

########NEW FILE########
__FILENAME__ = UmitConf
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import re

from types import StringTypes
from ConfigParser import DuplicateSectionError, NoSectionError, NoOptionError

from zenmapCore.Paths import Path
from zenmapCore.UmitLogging import log
from zenmapCore.UmitConfigParser import UmitConfigParser
import zenmapCore.I18N

# This is the global configuration parser object that represents the contents of
# zenmap.conf. It should be initialized once by the application. Most
# interaction with the global parser is done by other classes in this file, like
# SearchConfig, that wrap specific configuration sections.
config_parser = UmitConfigParser()

# Check if running on Maemo
MAEMO = False
try:
    import hildon
    MAEMO = True
except ImportError:
    pass

def is_maemo():
    return MAEMO

class SearchConfig(UmitConfigParser, object):
    section_name = "search"

    def __init__(self):
        if not config_parser.has_section(self.section_name):
            self.create_section()

    def save_changes(self):
        config_parser.save_changes()

    def create_section(self):
        config_parser.add_section(self.section_name)
        self.directory = ""
        self.file_extension = "xml"
        self.save_time = "60;days"
        self.store_results = True
        self.search_db = True

    def _get_it(self, p_name, default):
        return config_parser.get(self.section_name, p_name, default)

    def _set_it(self, p_name, value):
        config_parser.set(self.section_name, p_name, value)

    def boolean_sanity(self, attr):
        if attr == True or \
           attr == "True" or \
           attr == "true" or \
           attr == "1":

            return 1

        return 0

    def get_directory(self):
        return self._get_it("directory", "")

    def set_directory(self, directory):
        self._set_it("directory", directory)

    def get_file_extension(self):
        return self._get_it("file_extension", "xml").split(";")

    def set_file_extension(self, file_extension):
        if type(file_extension) == type([]):
            self._set_it("file_extension", ";".join(file_extension))
        elif type(file_extension) in StringTypes:
            self._set_it("file_extension", file_extension)

    def get_save_time(self):
        return self._get_it("save_time", "60;days").split(";")

    def set_save_time(self, save_time):
        if type(save_time) == type([]):
            self._set_it("save_time", ";".join(save_time))
        elif type(save_time) in StringTypes:
            self._set_it("save_time", save_time)

    def get_store_results(self):
        return self.boolean_sanity(self._get_it("store_results", True))

    def set_store_results(self, store_results):
        self._set_it("store_results", self.boolean_sanity(store_results))

    def get_search_db(self):
        return self.boolean_sanity(self._get_it("search_db", True))

    def set_search_db(self, search_db):
        self._set_it("search_db", self.boolean_sanity(search_db))

    def get_converted_save_time(self):
        try:
            return int(self.save_time[0]) * self.time_list[self.save_time[1]]
        except:
            # If something goes wrong, return a save time of 60 days
            return 60 * 60 * 24 * 60

    def get_time_list(self):
        # Time as key, seconds a value
        return {"hours": 60 * 60,
                "days": 60 * 60 * 24,
                "weeks": 60 * 60 * 24 * 7,
                "months": 60 * 60 * 24 * 7 * 30,
                "years": 60 * 60 * 24 * 7 * 30 * 12,
                "minutes": 60,
                "seconds": 1}

    directory = property(get_directory, set_directory)
    file_extension = property(get_file_extension, set_file_extension)
    save_time = property(get_save_time, set_save_time)
    store_results = property(get_store_results, set_store_results)
    search_db = property(get_search_db, set_search_db)
    converted_save_time = property(get_converted_save_time)
    time_list = property(get_time_list)


class Profile(UmitConfigParser, object):
    """This class represents not just one profile, but a whole collection of
    them found in a config file such as scan_profiles.usp. The methods therefore
    all take an argument that is the name of the profile to work on."""

    def __init__(self, user_profile = None, *args):
        UmitConfigParser.__init__(self, *args)

        if not user_profile:
            user_profile = Path.scan_profile

        fconf = open(user_profile, 'r')
        self.readfp(fconf, user_profile)

        fconf.close()
        del(fconf)

        self.attributes = {}

    def _get_it(self, profile, attribute):
        if self._verify_profile(profile):
            return self.get(profile, attribute)
        return ""

    def _set_it(self, profile, attribute, value=''):
        if self._verify_profile(profile):
            return self.set(profile, attribute, value)

    def add_profile(self, profile_name, **attributes):
        """Add a profile with the given name and attributes to the collection of
        profiles. If a profile with the same name exists, it is not overwritten,
        and the method returns immediately. The backing file for the profiles is
        automatically updated."""

        log.debug(">>> Add Profile '%s': %s" % (profile_name, attributes))

        try:
            self.add_section(profile_name)
        except DuplicateSectionError:
            return None

        # Set each of the attributes ("command", "description") in the
        # ConfigParser.
        for attr in attributes:
            self._set_it(profile_name, attr, attributes[attr])

        self.save_changes()

    def remove_profile(self, profile_name):
        try: self.remove_section(profile_name)
        except: pass
        self.save_changes()

    def _verify_profile(self, profile_name):
        if profile_name not in self.sections():
            return False
        return True

class CommandProfile (Profile, object):
    """This class is a wrapper around Profile that provides accessors for the
    attributes of a profile: command and description"""
    def __init__(self, user_profile = None):
        Profile.__init__(self, user_profile)

    def get_command(self, profile):
        command_string = self._get_it(profile, 'command')
        # Old versions of Zenmap used to append "%s" to commands and use that to
        # substitute the target. Ignore it if present.
        if command_string.endswith("%s"):
            command_string = command_string[:-len("%s")]
        return command_string

    def get_description(self, profile):
        return self._get_it(profile, 'description')

    def set_command(self, profile, command=''):
        self._set_it(profile, 'command', command)

    def set_description(self, profile, description=''):
        self._set_it(profile, 'description', description)

    def get_profile(self, profile_name):
        return {'profile':profile_name, \
                'command':self.get_command(profile_name), \
                'description':self.get_description(profile_name)}


class NmapOutputHighlight(object):
    setts = ["bold", "italic", "underline", "text", "highlight", "regex"]

    def save_changes(self):
        config_parser.save_changes()

    def __get_it(self, p_name):
        property_name = "%s_highlight" % p_name

        try:
            return self.sanity_settings([config_parser.get(property_name,
                                                         prop,
                                                         True) \
                                         for prop in self.setts])
        except:
            settings = []
            prop_settings = self.default_highlights[p_name]
            settings.append(prop_settings["bold"])
            settings.append(prop_settings["italic"])
            settings.append(prop_settings["underline"])
            settings.append(prop_settings["text"])
            settings.append(prop_settings["highlight"])
            settings.append(prop_settings["regex"])

            self.__set_it(p_name, settings)

            return settings

    def __set_it(self, property_name, settings):
        property_name = "%s_highlight" % property_name
        settings = self.sanity_settings(list(settings))

        [config_parser.set(property_name, self.setts[pos], settings[pos]) \
         for pos in xrange(len(settings))]

    def sanity_settings(self, settings):
        """This method tries to convert insane settings to sanity ones ;-)
        If user send a True, "True" or "true" value, for example, it tries to
        convert then to the integer 1.
        Same to False, "False", etc.

        Sequence: [bold, italic, underline, text, highlight, regex]
        """
        #log.debug(">>> Sanitize %s" % str(settings))

        settings[0] = self.boolean_sanity(settings[0])
        settings[1] = self.boolean_sanity(settings[1])
        settings[2] = self.boolean_sanity(settings[2])

        tuple_regex = "[\(\[]\s?(\d+)\s?,\s?(\d+)\s?,\s?(\d+)\s?[\)\]]"
        if isinstance(settings[3], basestring):
            settings[3] = [int(t) for t in re.findall(tuple_regex, settings[3])[0]]

        if isinstance(settings[4], basestring):
            settings[4]= [int(h) for h in re.findall(tuple_regex, settings[4])[0]]

        return settings

    def boolean_sanity(self, attr):
        if attr == True or attr == "True" or attr == "true" or attr == "1":
            return 1
        return 0

    def get_date(self):
        return self.__get_it("date")

    def set_date(self, settings):
        self.__set_it("date", settings)

    def get_hostname(self):
        return self.__get_it("hostname")

    def set_hostname(self, settings):
        self.__set_it("hostname", settings)

    def get_ip(self):
        return self.__get_it("ip")

    def set_ip(self, settings):
        self.__set_it("ip", settings)

    def get_port_list(self):
        return self.__get_it("port_list")

    def set_port_list(self, settings):
        self.__set_it("port_list", settings)

    def get_open_port(self):
        return self.__get_it("open_port")

    def set_open_port(self, settings):
        self.__set_it("open_port", settings)

    def get_closed_port(self):
        return self.__get_it("closed_port")

    def set_closed_port(self, settings):
        self.__set_it("closed_port", settings)

    def get_filtered_port(self):
        return self.__get_it("filtered_port")

    def set_filtered_port(self, settings):
        self.__set_it("filtered_port", settings)

    def get_details(self):
        return self.__get_it("details")

    def set_details(self, settings):
        self.__set_it("details", settings)

    def get_enable(self):
        enable = True
        try:
            enable = config_parser.get("output_highlight", "enable_highlight")
        except NoSectionError:
            config_parser.set("output_highlight", "enable_highlight", str(True))

        if enable == "False" or enable == "0" or enable == "":
            return False
        return True

    def set_enable(self, enable):
        if enable == False or enable == "0" or enable == None or enable == "":
            config_parser.set("output_highlight", "enable_highlight", str(False))
        else:
            config_parser.set("output_highlight", "enable_highlight", str(True))

    date = property(get_date, set_date)
    hostname = property(get_hostname, set_hostname)
    ip = property(get_ip, set_ip)
    port_list = property(get_port_list, set_port_list)
    open_port = property(get_open_port, set_open_port)
    closed_port = property(get_closed_port, set_closed_port)
    filtered_port = property(get_filtered_port, set_filtered_port)
    details = property(get_details, set_details)
    enable = property(get_enable, set_enable)

    # These settings are made when there is nothing set yet. They set the "factory" \
    # default to highlight colors
    default_highlights = {"date":{"bold":str(True),
                            "italic":str(False),
                            "underline":str(False),
                            "text":[0, 0, 0],
                            "highlight":[65535, 65535, 65535],
                            "regex":"\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}\s.{1,4}"},
                          "hostname":{"bold":str(True),
                            "italic":str(True),
                            "underline":str(True),
                            "text":[0, 111, 65535],
                            "highlight":[65535, 65535, 65535],
                            "regex":"(\w{2,}://)*[\w-]{2,}\.[\w-]{2,}(\.[\w-]{2,})*(/[[\w-]{2,}]*)*"},
                          "ip":{"bold":str(True),
                            "italic":str(False),
                            "underline":str(False),
                            "text":[0, 0, 0],
                            "highlight":[65535, 65535, 65535],
                            "regex":"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}"},
                          "port_list":{"bold":str(True),
                            "italic":str(False),
                            "underline":str(False),
                            "text":[0, 1272, 28362],
                            "highlight":[65535, 65535, 65535],
                            "regex":"PORT\s+STATE\s+SERVICE(\s+VERSION)?[^\n]*"},
                          "open_port":{"bold":str(True),
                            "italic":str(False),
                            "underline":str(False),
                            "text":[0, 41036, 2396],
                            "highlight":[65535, 65535, 65535],
                            "regex":"\d{1,5}/.{1,5}\s+open\s+.*"},
                          "closed_port":{"bold":str(False),
                            "italic":str(False),
                            "underline":str(False),
                            "text":[65535, 0, 0],
                            "highlight":[65535, 65535, 65535],
                            "regex":"\d{1,5}/.{1,5}\s+closed\s+.*"},
                          "filtered_port":{"bold":str(False),
                            "italic":str(False),
                            "underline":str(False),
                            "text":[38502, 39119, 0],
                            "highlight":[65535, 65535, 65535],
                            "regex":"\d{1,5}/.{1,5}\s+filtered\s+.*"},
                          "details":{"bold":str(True),
                            "italic":str(False),
                            "underline":str(True),
                            "text":[0, 0, 0],
                            "highlight":[65535, 65535, 65535],
                            "regex":"^(\w{2,}[\s]{,3}){,4}:"}}

# Retrieve details from zenmap.conf regarding paths subsection
# (e.g. nmap_command_path) - jurand
class PathsConfig(object):
    section_name = "paths"

    # This accounts for missing entries conf file.
    # Defaults to "nmap" if these errors occur.
    # NoOptionError, NoSectionError
    def __get_it(self, p_name, default):
        try:
            return config_parser.get(self.section_name, p_name)
        except (NoOptionError,NoSectionError):
            log.debug(">>> Using default \"%s\" for \"%s\"." % (default, p_name))
            return default

    def __set_it(self, property_name, settings):
        config_parser.set(self.section_name, property_name, settings)

    def get_nmap_command_path(self):
        return self.__get_it("nmap_command_path", "nmap")

    def set_nmap_command_path(self, settings):
        self.__set_it("nmap_command_path", settings)

    def get_ndiff_command_path(self):
        return self.__get_it("ndiff_command_path", "ndiff")

    def set_ndiff_command_path(self, settings):
        self.__set_it("ndiff_command_path", settings)

    nmap_command_path = property(get_nmap_command_path, set_nmap_command_path)
    ndiff_command_path = property(get_ndiff_command_path, set_ndiff_command_path)

# Exceptions
class ProfileNotFound:
    def __init__ (self, profile):
        self.profile = profile
    def __str__ (self):
        return "No profile named '"+self.profile+"' found!"

class ProfileCouldNotBeSaved:
    def __init__ (self, profile):
        self.profile = profile
    def __str__ (self):
        return "Profile named '"+self.profile+"' could not be saved!"

########NEW FILE########
__FILENAME__ = UmitConfigParser
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

from os.path import exists
from ConfigParser import ConfigParser, DEFAULTSECT, NoOptionError, NoSectionError
from zenmapCore.UmitLogging import log

class UmitConfigParser(ConfigParser):
    filenames = None
    fp = None

    def __init__(self, *args):
        ConfigParser.__init__(self, *args)

    def set(self, section, option, value):
        if not self.has_section(section):
            self.add_section(section)

        ConfigParser.set(self, section, option, value)
        self.save_changes()

    def read(self, filename):
        log.debug(">>> Trying to parse: %s" % filename)

        self.filename = ConfigParser.read(self, filename)
        return self.filename

    def readfp(self, fp, filename=None):
        ConfigParser.readfp(self, fp, filename)
        self.fp = fp
        self.filenames = filename

    def save_changes(self):
        if self.filenames:
            filename = None
            if isinstance(self.filenames, basestring):
                filename = self.filenames
            elif isinstance(self.filenames, list):
                if len(self.filenames) == 1:
                    filename = self.filenames[0]
                else:
                    raise ValueError("UmitConfigParser can't handle a list of filenames: %s" % self.filenames)
            else:
                raise ValueError("UmitConfigParser can't handle a filename of type %s: %s" % (type(self.filenames), self.filenames))
            self.write(open(filename, 'w'))
        elif self.fp:
            self.write(self.fp)

    def write(self, fp):
        '''Write alphabetically sorted config files'''
        if self._defaults:
            fp.write("[%s]\n" % DEFAULTSECT)

            items = self._defaults.items()
            items.sort()

            for (key, value) in items:
                fp.write("%s = %s\n" % (key, str(value).replace('\n', '\n\t')))
            fp.write("\n")

        sects = self._sections.keys()
        sects.sort()

        for section in sects:
            fp.write("[%s]\n" % section)
            for (key, value) in self._sections[section].items():
                if key != "__name__":
                    fp.write("%s = %s\n" %
                             (key, str(value).replace('\n', '\n\t')))
            fp.write("\n")

def test_umit_conf_content(filename):
    parser = ConfigParser()
    parser.read(filename)

    # Paths section
    section = "paths"
    assert get_or_false(parser, section, "nmap_command_path")


def get_or_false(parser, section, option):
    try:
        result = parser.get(section, option)
        return result
    except NoOptionError:
        return False
    except NoSectionError:
        return False

########NEW FILE########
__FILENAME__ = UmitDB
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import sys

try:
    import hashlib
    md5 = hashlib.md5
except ImportError:
    import md5
    md5 = md5.new

sqlite = None
try:
    from pysqlite2 import dbapi2 as sqlite
except ImportError:
    try:
        # In case this script is been running under python2.5 with sqlite3
        import sqlite3 as sqlite
    except ImportError:
        raise ImportError(_("No module named dbapi2.pysqlite2 or sqlite3"))

from time import time

from zenmapCore.Paths import Path
from zenmapCore.UmitLogging import log


umitdb = ""

try:
    umitdb = Path.db
except:
    import os.path
    from BasePaths import base_paths

    umitdb = os.path.join(Path.user_config_dir, base_paths["db"])
    Path.db = umitdb


from os.path import exists, dirname
from os import access, R_OK, W_OK

using_memory = False
if not exists(umitdb) or \
   not access(umitdb, R_OK and W_OK) or \
   not access(dirname(umitdb), R_OK and W_OK):
    # Tells sqlite to use memory instead of a physics file to avoid crash
    # and still serve user with most features
    umitdb = ":memory:"
    using_memory = True

if isinstance(umitdb, str):
    fs_enc = sys.getfilesystemencoding()
    if fs_enc is None:
        fs_enc = "UTF-8"
    umitdb = umitdb.decode(fs_enc)

# pyslite 2.4.0 doesn't handle a unicode database name, though earlier and later
# versions do. Encode to UTF-8 as pysqlite would do internally anyway.
umitdb = umitdb.encode("UTF-8")

connection = sqlite.connect(umitdb)

# By default pysqlite will raise an OperationalError when trying to return a
# TEXT data type that is not UTF-8 (it always tries to decode text in order to
# return a unicdoe object). We store XML in the database, which may have a
# different encoding, so instruct pysqlite to return a plain str for TEXT data
# types, and not to attempt any decoding.
try:
    connection.text_factory = str
except AttributeError:
    # However, text_factory is available only in pysqlite 2.1.0 and later.
    pass

class Table(object):
    def __init__(self, table_name):
        self.table_name = table_name
        self.table_id = "%s_id" % table_name

        self.cursor = connection.cursor()

    def get_item(self, item_name):
        if self.__getattribute__("_%s" % item_name):
            return self.__getattribute__("_%s" % item_name)

        sql = "SELECT %s FROM %s WHERE %s_id = %s" % (item_name, self.table_name,
                                                      self.table_name,
                                                      self.__getattribute__(self.table_id))

        self.cursor.execute(sql)

        self.__setattr__("_%s" % item_name, self.cursor.fetchall()[0][0])
        return self.__getattribute__("_%s" % item_name)

    def set_item(self, item_name, item_value):
        if item_value == self.__getattribute__("_%s" % item_name):
            return None

        sql = "UPDATE %s SET %s = ? WHERE %s_id = %s" % (self.table_name, item_name,
                                                         self.table_name,
                                                         self.__getattribute__(self.table_id))
        self.cursor.execute(sql, (item_value,))
        connection.commit()
        self.__setattr__("_%s" % item_name, item_value)

    def insert(self, **kargs):
        sql = "INSERT INTO %s ("
        for k in kargs.keys():
            sql += k
            sql += ", "
        else:
            sql = sql[:][:-2]
            sql += ") VALUES ("

        for v in xrange(len(kargs.values())):
            sql += "?, "
        else:
            sql = sql[:][:-2]
            sql += ")"

        sql %= self.table_name

        self.cursor.execute(sql, tuple(kargs.values()))
        connection.commit()

        sql = "SELECT MAX(%s_id) FROM %s;" % (self.table_name, self.table_name)
        self.cursor.execute(sql)
        return self.cursor.fetchall()[0][0]

class UmitDB(object):
    def __init__(self):
        self.cursor = connection.cursor()

    def create_db(self):
        drop_string = ("DROP TABLE scans;",)

        try:
            for d in drop_string:
                self.cursor.execute(d)
        except:
            connection.rollback()
        else:
            connection.commit()


        creation_string = ("""CREATE TABLE scans (scans_id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                  scan_name TEXT,
                                                  nmap_xml_output TEXT,
                                                  digest TEXT,
                                                  date INTEGER)""",)

        for c in creation_string:
            self.cursor.execute(c)
            connection.commit()

    def add_scan(self, **kargs):
        return Scans(**kargs)

    def get_scans_ids(self):
        sql = "SELECT scans_id FROM scans;"
        self.cursor.execute(sql)
        return [sid[0] for sid in self.cursor.fetchall()]

    def get_scans(self):
        scans_ids = self.get_scans_ids()
        for sid in scans_ids:
            yield Scans(scans_id=sid)

    def cleanup(self, save_time):
        log.debug(">>> Cleaning up data base.")
        log.debug(">>> Removing results olders than %s seconds" % save_time)
        self.cursor.execute("SELECT scans_id FROM scans WHERE date < ?", (time() - save_time,))

        for sid in [sid[0] for sid in self.cursor.fetchall()]:
            log.debug(">>> Removing results with scans_id %s" % sid)
            self.cursor.execute("DELETE FROM scans WHERE scans_id = ?", (sid, ))
        else:
            connection.commit()
            log.debug(">>> Data base successfully cleaned up!")


class Scans(Table, object):
    def __init__(self, **kargs):
        Table.__init__(self, "scans")
        if "scans_id" in kargs.keys():
            self.scans_id = kargs["scans_id"]
        else:
            log.debug(">>> Creating new scan result entry at data base")
            fields = ["scan_name", "nmap_xml_output", "date"]

            for k in kargs.keys():
                if k not in fields:
                    raise Exception("Wrong table field passed to creation method. '%s'" % k)

            if "nmap_xml_output" not in kargs.keys() or not kargs["nmap_xml_output"]:
                raise Exception("Can't save result without xml output")

            if not self.verify_digest(md5(kargs["nmap_xml_output"]).hexdigest()):
                raise Exception("XML output registered already!")

            self.scans_id = self.insert(**kargs)

    def verify_digest(self, digest):
        self.cursor.execute("SELECT scans_id FROM scans WHERE digest = ?", (digest, ))
        result = self.cursor.fetchall()
        if result:
            return False
        return True

    def add_host(self, **kargs):
        kargs.update({self.table_id:self.scans_id})
        return Hosts(**kargs)

    def get_hosts(self):
        sql = "SELECT hosts_id FROM hosts WHERE scans_id= %s" % self.scans_id

        self.cursor.execute(sql)
        result = self.cursor.fetchall()

        for h in result:
            yield Hosts(hosts_id=h[0])

    def get_scans_id(self):
        return self._scans_id

    def set_scans_id(self, scans_id):
        if scans_id != self._scans_id:
            self._scans_id = scans_id

    def get_scan_name(self):
        return self.get_item("scan_name")

    def set_scan_name(self, scan_name):
        self.set_item("scan_name", scan_name)

    def get_nmap_xml_output(self):
        return self.get_item("nmap_xml_output")

    def set_nmap_xml_output(self, nmap_xml_output):
        self.set_item("nmap_xml_output", nmap_xml_output)
        self.set_item("digest", md5(nmap_xml_output).hexdigest())

    def get_date(self):
        return self.get_item("date")

    def set_date(self, date):
        self.set_item("date", date)

    scans_id = property(get_scans_id, set_scans_id)
    scan_name = property(get_scan_name, set_scan_name)
    nmap_xml_output = property(get_nmap_xml_output, set_nmap_xml_output)
    date = property(get_date, set_date)

    _scans_id = None
    _scan_name = None
    _nmap_xml_output = None
    _date = None


######################################################################
# Verify if data base exists and if it does have the required tables.
# If something is wrong, re-create table
def verify_db():
    cursor = connection.cursor()
    try:
        cursor.execute("SELECT scans_id FROM scans WHERE date = 0")
    except sqlite.OperationalError:
        u = UmitDB()
        u.create_db()
verify_db()

######################################################################

if __name__ == "__main__":
    from pprint import pprint

    u = UmitDB()

    #print "Creating Data Base"
    #u.create_db()

    #print "Creating new scan"
    #s = u.add_scan(scan_name="Fake scan", nmap_xml_output="", date="007")

    #s = Scans(scans_id=2)
    #print s.scans_id
    #print s.scan_name
    #print s.nmap_xml_output
    #print s.date

    sql = "SELECT * FROM scans;"
    u.cursor.execute(sql)
    print "Scans:",
    pprint(u.cursor.fetchall())

########NEW FILE########
__FILENAME__ = UmitLogging
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/


from logging import Logger, StreamHandler, Formatter
from zenmapCore_Kvasir.Name import APP_DISPLAY_NAME
#from zenmapCore.UmitOptionParser import option_parser

LOGLEVEL = 10

class Log(Logger, object):
    def __init__(self, name, level=0):
        Logger.__init__(self, name, level)
        self.formatter = self.format

        handler = StreamHandler()
        handler.setFormatter(self.formatter)

        self.addHandler(handler)

    def get_formatter(self):
        return self.__formatter

    def set_formatter(self, fmt):
        self.__formatter = Formatter(fmt)


    format = "%(levelname)s - %(asctime)s - %(message)s"

    formatter = property(get_formatter, set_formatter, doc="")
    __formatter = Formatter(format)


# Import this!
log = Log(APP_DISPLAY_NAME, LOGLEVEL)

if __name__ == '__main__':
    log.debug("Debug Message")
    log.info("Info Message")
    log.warning("Warning Message")
    log.error("Error Message")
    log.critical("Critical Message")

########NEW FILE########
__FILENAME__ = UmitOptionParser
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# ***********************IMPORTANT NMAP LICENSE TERMS************************
# *                                                                         *
# * The Nmap Security Scanner is (C) 1996-2013 Insecure.Com LLC. Nmap is    *
# * also a registered trademark of Insecure.Com LLC.  This program is free  *
# * software; you may redistribute and/or modify it under the terms of the  *
# * GNU General Public License as published by the Free Software            *
# * Foundation; Version 2 ("GPL"), BUT ONLY WITH ALL OF THE CLARIFICATIONS  *
# * AND EXCEPTIONS DESCRIBED HEREIN.  This guarantees your right to use,    *
# * modify, and redistribute this software under certain conditions.  If    *
# * you wish to embed Nmap technology into proprietary software, we sell    *
# * alternative licenses (contact sales@nmap.com).  Dozens of software      *
# * vendors already license Nmap technology such as host discovery, port    *
# * scanning, OS detection, version detection, and the Nmap Scripting       *
# * Engine.                                                                 *
# *                                                                         *
# * Note that the GPL places important restrictions on "derivative works",  *
# * yet it does not provide a detailed definition of that term.  To avoid   *
# * misunderstandings, we interpret that term as broadly as copyright law   *
# * allows.  For example, we consider an application to constitute a        *
# * derivative work for the purpose of this license if it does any of the   *
# * following with any software or content covered by this license          *
# * ("Covered Software"):                                                   *
# *                                                                         *
# * o Integrates source code from Covered Software.                         *
# *                                                                         *
# * o Reads or includes copyrighted data files, such as Nmap's nmap-os-db   *
# * or nmap-service-probes.                                                 *
# *                                                                         *
# * o Is designed specifically to execute Covered Software and parse the    *
# * results (as opposed to typical shell or execution-menu apps, which will *
# * execute anything you tell them to).                                     *
# *                                                                         *
# * o Includes Covered Software in a proprietary executable installer.  The *
# * installers produced by InstallShield are an example of this.  Including *
# * Nmap with other software in compressed or archival form does not        *
# * trigger this provision, provided appropriate open source decompression  *
# * or de-archiving software is widely available for no charge.  For the    *
# * purposes of this license, an installer is considered to include Covered *
# * Software even if it actually retrieves a copy of Covered Software from  *
# * another source during runtime (such as by downloading it from the       *
# * Internet).                                                              *
# *                                                                         *
# * o Links (statically or dynamically) to a library which does any of the  *
# * above.                                                                  *
# *                                                                         *
# * o Executes a helper program, module, or script to do any of the above.  *
# *                                                                         *
# * This list is not exclusive, but is meant to clarify our interpretation  *
# * of derived works with some common examples.  Other people may interpret *
# * the plain GPL differently, so we consider this a special exception to   *
# * the GPL that we apply to Covered Software.  Works which meet any of     *
# * these conditions must conform to all of the terms of this license,      *
# * particularly including the GPL Section 3 requirements of providing      *
# * source code and allowing free redistribution of the work as a whole.    *
# *                                                                         *
# * As another special exception to the GPL terms, Insecure.Com LLC grants  *
# * permission to link the code of this program with any version of the     *
# * OpenSSL library which is distributed under a license identical to that  *
# * listed in the included docs/licenses/OpenSSL.txt file, and distribute   *
# * linked combinations including the two.                                  *
# *                                                                         *
# * Any redistribution of Covered Software, including any derived works,    *
# * must obey and carry forward all of the terms of this license, including *
# * obeying all GPL rules and restrictions.  For example, source code of    *
# * the whole work must be provided and free redistribution must be         *
# * allowed.  All GPL references to "this License", are to be treated as    *
# * including the terms and conditions of this license text as well.       *
# *                                                                         *
# * Because this license imposes special exceptions to the GPL, Covered     *
# * Work may not be combined (even as part of a larger work) with plain GPL *
# * software.  The terms, conditions, and exceptions of this license must   *
# * be included as well.  This license is incompatible with some other open *
# * source licenses as well.  In some cases we can relicense portions of    *
# * Nmap or grant special permissions to use it in other open source        *
# * software.  Please contact fyodor@nmap.org with any such requests.       *
# * Similarly, we don't incorporate incompatible open source software into  *
# * Covered Software without special permission from the copyright holders. *
# *                                                                         *
# * If you have any questions about the licensing restrictions on using     *
# * Nmap in other works, are happy to help.  As mentioned above, we also    *
# * offer alternative license to integrate Nmap into proprietary            *
# * applications and appliances.  These contracts have been sold to dozens  *
# * of software vendors, and generally include a perpetual license as well  *
# * as providing for priority support and updates.  They also fund the      *
# * continued development of Nmap.  Please email sales@nmap.com for further *
# * information.                                                            *
# *                                                                         *
# * If you have received a written license agreement or contract for        *
# * Covered Software stating terms other than these, you may choose to use  *
# * and redistribute Covered Software under those terms instead of these.   *
# *                                                                         *
# * Source is provided to this software because we believe users have a     *
# * right to know exactly what a program is going to do before they run it. *
# * This also allows you to audit the software for security holes (none     *
# * have been found so far).                                                *
# *                                                                         *
# * Source code also allows you to port Nmap to new platforms, fix bugs,    *
# * and add new features.  You are highly encouraged to send your changes   *
# * to the dev@nmap.org mailing list for possible incorporation into the    *
# * main distribution.  By sending these changes to Fyodor or one of the    *
# * Insecure.Org development mailing lists, or checking them into the Nmap  *
# * source code repository, it is understood (unless you specify otherwise) *
# * that you are offering the Nmap Project (Insecure.Com LLC) the           *
# * unlimited, non-exclusive right to reuse, modify, and relicense the      *
# * code.  Nmap will always be available Open Source, but this is important *
# * because the inability to relicense code has caused devastating problems *
# * for other Free Software projects (such as KDE and NASM).  We also       *
# * occasionally relicense the code to third parties as discussed above.    *
# * If you wish to specify special license conditions of your               *
# * contributions, just say so when you send them.                          *
# *                                                                         *
# * This program is distributed in the hope that it will be useful, but     *
# * WITHOUT ANY WARRANTY; without even the implied warranty of              *
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the Nmap      *
# * license file for more details (it's in a COPYING file included with     *
# * Nmap, and also available from https://svn.nmap.org/nmap/COPYING         *
# *                                                                         *
# ***************************************************************************/

import re


from optparse import OptionParser
from zenmapCore_Kvasir.Name import APP_NAME, NMAP_DISPLAY_NAME
from zenmapCore_Kvasir.Version import VERSION
import zenmapCore_Kvasir.I18N
from zenmapCore_Kvasir.BasePaths import base_paths

class UmitOptionParser(OptionParser):
    def __init__(self, args=False):
        OptionParser.__init__(self, version="%%prog %s" % VERSION)

        self.set_usage("%prog [options] [result files]")

        self.add_option("--confdir",
            default = base_paths["user_config_dir"],
            dest = "confdir",
            metavar = "DIR",
            help = _("\
Use DIR as the user configuration directory. Default: %default"))

        ## Open Scan Results (GUI)
        ### Run, opening the specified scan result file, which should be
        ### a nmap XML output file.
        ### This option should be verified if there is no options, and user
        ### specified some positional arguments, which should be considered as
        ### scan result files.
        self.add_option("-f", "--file",
                        default=[],
                        action="append",
                        type="string",
                        dest="result_files",
                        help=_("Specify a scan result file in Nmap XML output \
format. Can be used more than once to specify several \
scan result files."))

        ## Run nmap with args (GUI)
        ### Open and run nmap with specified args. The positional
        ### args should be used to feed the nmap command
        self.add_option("-n", "--nmap",
                        default=[],
                        action="callback",
                        callback=self.__nmap_callback,
                        help=_("Run %s with the specified args.") % NMAP_DISPLAY_NAME)

        ## Execute a profile against a target (GUI)
        ### Positional args should be taken as targets to feed this scan
        self.add_option("-p", "--profile",
                        default="",
                        action="store",
                        help=_("Begin with the specified profile \
selected. If combined with the -t (--target) option, \
automatically run the profile against the specified target."))

        ## Targets (GUI)
        ### Specify a target to be used along with other command line option
        ### or simply opens with the first tab target field filled with
        ### the target specified with this option
        self.add_option("-t", "--target",
                        default=False,
                        action="store",
                    help=_("Specify a target to be used along with other \
options. If specified alone, open with the target field filled with the \
specified target"))

        ## Verbosity
        self.add_option("-v", "--verbose",
                        default=0,
                        action="count",
                        help=_("Increase verbosity of the output. May be \
used more than once to get even more verbosity"))

        # Parsing options and arguments
        if args:
            self.options, self.args = self.parse_args(args)
        else:
            self.options, self.args = self.parse_args()

    def __nmap_callback(self, option, opt_str, value, parser):
        nmap_args = []
        # Iterate over next arguments that were passed at the command line
        # that wasn't parsed yet.
        while parser.rargs:
            # Store the next argument in a specific list
            nmap_args.append(parser.rargs[0])

            # Remove the added argument from rargs to avoid it's latter
            # parsing by optparse
            del parser.rargs[0]

        # Set the variable nmap at parser.values, so you may call option.nmap
        # and have the nmap_args as result
        setattr(parser.values, "nmap", nmap_args)

    def get_confdir(self):
        return self.options.confdir

    def get_nmap(self):
        """Return a list of nmap arguments or False if this option was not
        called by the user"""

        try:
            nmap = self.options.nmap
            if nmap:
                return nmap
        except AttributeError:
            return False

    def get_profile(self):
        """Return a string with the profile name, or False if no profile
        option was specified by the user"""
        if self.options.profile != "":
            return self.options.profile
        return False

    def get_target(self):
        """Returns a string with the target specified, or False if this option
        wass not called by the user"""
        return self.options.target

    def get_open_results(self):
        """Returns a list of strings with the name of the files specified with
        the -f (--file) option and every positional argument."""
        files = []
        # Add arguments given with -f.
        if self.options.result_files:
            files = self.options.result_files[:]
        # Add any other arguments.
        files += self.args
        return files

    def get_verbose(self):
        """Returns an integer representing the verbosity level of the
        application. Verbosity level starts in 40, which means that only
        messages above the ERROR level are going to be reported at the output.
        As this value gets lower, the verbosity increases.
        """
        return 40 - (self.options.verbose * 10)

option_parser = UmitOptionParser()

if __name__ == "__main__":
    opt = UmitOptionParser()
    options, args = opt.parse_args()

########NEW FILE########
__FILENAME__ = Version
VERSION = "6.41SVN"

########NEW FILE########
__FILENAME__ = db_csv
#!/usr/bin/env python
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Backup/Restore Kvasir database as CSV
##
## Run from a shell using web2py.
##
## Export:
##   ./web2py.py -R applications/$appname/private/db_csv.py -S $appname -M -A -e $appname.csv.gz
##
## Import:
##   ./web2py.py -R applications/$appname/private/db_csv.py -S $appname -M -A -i <filename>
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

import sys
import re
import gzip
from optparse import OptionParser
try:
    import cStringIO as StringIO
except ImportError:
    import StringIO


##--------------------------------------------------------------------

optparser = OptionParser(version=__version__)

optparser.add_option("-i", "--import", dest="infile",
    action="store", default=None, help="Import CSV file")
optparser.add_option("-W", "--wipe", dest="wipe",
    action="store_true", default=False, help="Wipe any existing DB data")
optparser.add_option("-e", "--export", dest="export",
    action="store", default=False, help="Export to CSV file")
optparser.add_option("-g", "--gzip", dest="gzip",
    action="store_true", default=False, help="GZip Compression")

(options, params) = optparser.parse_args()

if options.export:
    if options.gzip or options.export.endswith('.gz'):
        of = gzip.open(options.export, 'wb')
    else:
        of = open(options.export, 'wb')
    db.export_to_csv_file(of)
    of.close()

elif options.infile:
    fname = options.infile
    if options.wipe:
        db.t_hosts.truncate(mode="CASCADE")
        db.t_services.truncate(mode="CASCADE")
        db.t_os.truncate(mode="CASCADE")
        db.t_host_os_refs.truncate(mode="CASCADE")
        db.t_apps.truncate(mode="CASCADE")
        db.t_services_apps_refs.truncate(mode="CASCADE")
        db.t_service_vulns.truncate(mode="CASCADE")
        db.t_service_info.truncate(mode="CASCADE")
        db.t_accounts.truncate(mode="CASCADE")
        db.t_host_notes.truncate(mode="CASCADE")
        db.t_evidence.truncate(mode="CASCADE")
        db.t_snmp.truncate(mode="CASCADE")
        db.commit()

        if fname.endswith('.gz'):
            fobj = gzip.open(fname, 'rb')
        else:
            fobj = open(fname, 'rb')

        db.import_from_csv_file(fobj)

else:
    optparser.print_help()

########NEW FILE########
__FILENAME__ = dedupe
#!/usr/bin/env python
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## De-dupify some tables
##
## Run from a shell using web2py:
##
##   ./web2py.py -R applications/$appname/private/dedupe.py -S $appname -M -A -t <table> --dry-run
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

import sys
import argparse

##--------------------------------------------------------------------

parser = argparse.ArgumentParser(description='Kvasir De-Dupe Table Entries')

parser.add_argument('-f', '--fields', action='append', help="Fields to validate")
parser.add_argument('-d', '--dry-run', action='store_true', default=False, help="Dry run")
parser.add_argument('table', help="Table name to de-dupe")

args = parser.parse_args()

sys.stdout.write("\nKvasir De-Dupe Table Entries\n============================\n\n")

if not args.table:
    parser.print_help()
    sys.exit("[!] Requires a table name\n")
else:
    table = db[args.table]

if args.table not in db.tables:
    sys.stderr.write("[!] %s is not a valid table name.\n\nValid table names are:\n\n" % args.table)
    for tn in db.tables:
        sys.stderr.write("\t%s\n" % tn)
    sys.exit("\n")
else:
    sys.stdout.write(" [*] Database %s valid\n" % args.table)

invalid = False
for field in args.fields:
    sys.stdout.write(' [*] Validating field: %s -- ' % field)
    if field not in table.fields:
        invalid = True
        sys.stdout.write("NOT a valid fieldname\n")
    else:
        sys.stdout.write("OK\n")

if invalid:
    sys.stdout.write(" [-] Valid fields:\n\n")
    for field in table.fields:
        sys.stdout.write("\t%s\n" % field)
    sys.exit("\n")


sql_cmd = """
DELETE FROM %s
WHERE id IN (SELECT id
    FROM (SELECT id,
        row_number() over (partition BY %s ORDER BY id) AS rnum
            FROM %s) t
    WHERE t.rnum > 1);
""" % (args.table, ", ".join(args.fields), args.table)

pre_count = db(table).count()

sys.stdout.write(" [-] %s records currently in the database\n" % pre_count)

if args.dry_run:
    sys.stdout.write(" [*] Dry run selected, would have executed:\n")
    for ln in sql_cmd.split("\n"):
        sys.stdout.write("\t%s\n" % ln)
else:
    sys.stdout.write(" [*] Executing de-dupe query, go grab a tasty beverage!\n")
    res = db.executesql(sql_cmd)
    db.commit()
    post_count = db(table).count()
    sys.stdout.write(" [-] %s records after de-duplication\n" % post_count)

########NEW FILE########
__FILENAME__ = gen_pwfile
#!/usr/bin/env python
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Generates a password file format for input into
## tools like john the ripper
##
## Run from a shell using web2py:
##
##   ./web2py.py -R applications/kvasir_public/private/gen_pwfile.py -S $appname -M -A -o pwdump -H ntlm
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

import sys, os, csv
from optparse import OptionParser, OptionGroup
from skaldship.hosts import get_host_record

########################################################################
class HostQuery:
    """Represents the host_query for services"""

    #----------------------------------------------------------------------
    def __init__(self):
        self.host_query = None

    #----------------------------------------------------------------------
    def show(self):
        return self.host_query

    #----------------------------------------------------------------------
    def add_host(self, address=None, ports=None):
        """Looks up the host and adds the result to the query"""
        host_rec = get_host_record(address)
        if host_rec is None:
            sys.stderr.write("%s invalid address!\n" % (address))
        else:
            q = (db.t_services.f_hosts_id == host_rec.id)
            for port in ports:
                q &= (db.t_services.f_proto == port[0])
                q &= (db.t_services.f_number == port[1])
            if self.host_query is None:
                self.host_query = q
            else:
                self.host_query |= q

        return

##--------------------------------------------------------------------

pwtypes = [
    ("pwdump", "Windows PWDUMP"),
    ("unix", "UNIX Passwd/shadow"),
    ("userpass", "Usernamne:Password"),
    ("csv", "ipv4,ipv6,hostname,username,cleartext,hash_1,hash1_type,hash2,hash2_type"),
]

optparser = OptionParser(version=__version__)

optparser.add_option("-o", "--otype", dest="output_type",
    action="store", default=None, help="Output file type")
optparser.add_option("-L", "--list-types", dest="list_types",
    action="store_true", help="List all password file types")

select_group = OptionGroup(optparser, "Selection Criteria")
select_group.add_option("-p", "--port", dest="port",
    action="append", default=[], help="Port (tcp/445) or leave blank for all")
select_group.add_option("-H", "--hash", dest="hash_type",
    action="store", default=None, help="Hash type (LM, NT, MD5, crypt, etc)")
select_group.add_option("-e", "--empty", dest="empty",
    action="store_true", default=False, help="Include records without hashes")
optparser.add_option_group(select_group)

input_group = OptionGroup(optparser, "Source Input Options")
input_group.add_option("-4", "--ipv4", dest="ipv4",
    action="append", default=[], help="IPv4 Address or leave blank for all")
input_group.add_option("-6", "--ipv6", dest="ipv6",
    action="append", default=[], help="IPv6 Address or leave blank for all")
input_group.add_option("-l", "--list", dest="filelist",
    action="store", default=None, help="File of IP addresses, one per line")

optparser.add_option_group(input_group)
(options, params) = optparser.parse_args()

if options.list_types:
    sys.stdout.write("\nPassword file types for output\n")
    sys.stdout.write("------------------------------\n\n")
    for a in pwtypes:
        sys.stdout.write("\t%10s .................. %s\n" % (a[0], a[1]))
    sys.stdout.write("\n")
    sys.exit(0)

if options.output_type is None:
    sys.exit("\nMust provide a password file type\n")

ports = []
for port in options.port:
    try:
        (proto, number) = port.split("/")
        ports.append((proto.lower(), number))
    except:
        sys.stderr.write("Invalid port specified: %s" % options.port)

# the large, all accounts query
query = (db.t_accounts.id > 0)

# build a host_query if ipv4 and/or ipv6 addresses are supplied
host_query = HostQuery()
for ip in options.ipv4:
    host_query.add_host(ip, ports)
for ip in options.ipv6:
    host_query.add_host(ip, ports)
if options.filelist:
    for ip in open(options.filelist, "r").readlines():
        host_query.add_host(ip, ports)

if host_query.show() is not None:
    query = (db.t_accounts.active == True) & host_query.show()

# build the port query but only if we haven't specified any ipv4
# or ipv6 addresses.
port_q = (db.t_services.id > 0)
if options.port and host_query.show() is None:
    for p,n in ports:
        port_q &= (db.t_services.f_proto == p)
        port_q &= (db.t_services.f_number == n)
    query &= port_q

"""
if host_query.show() is not None:
    q = (db.t_services)
    q &= host_query.show()
    q &= port_q
    print q.__str__()
    host_rows = db(q).select()
    print host_rows.as_list()
"""

#print "Query =", query.__str__()
output_type = options.output_type.lower()
query &= (db.t_accounts.f_services_id == db.t_services.id)
if options.hash_type:
    query &= (db.t_accounts.f_hash1_type.like(options.hash_type)) | (db.t_accounts.f_hash2_type.like(options.hash_type))
query &= (db.t_services.f_hosts_id == db.t_hosts.id)
#print db(query).select().as_list()

if output_type == "csv":
    output = csv.writer(sys.stdout, quoting=csv.QUOTE_MINIMAL)
    output.writerow(["IPv4", "IPv6", "Hostname", "Account", "Full Name", "Password", "Hash 1 type", "Hash 1", "Hash Type", "Hash 2"])
else:
    output = ""

for row in db(query).select():
    acct = row.t_accounts
    host = row.t_hosts
    # Windows PWDUMP format:
    if not options.empty and (not acct.f_hash1 and not acct.f_hash2):
        continue
    if output_type == "pwdump":
        output += ":".join((acct.f_username, acct.f_uid or "1000", acct.f_hash1 or "", acct.f_hash2 or "", "", "", "", ""))
        output += "\n"
        #print acct
    # UNIX Shadow format:
    elif output_type == "unix":
        output += ":".join((acct.f_username, acct.f_hash1 or "", acct.f_uid or "0", acct.f_gid or "0", acct.f_fullname or "", "/", "/"))
        output += "\n"
    elif output_type == "userpass":
        output += ":".join((acct.f_username, acct.f_password or ""))
        output += "\n"
    elif output_type == "csv":
        output.writerow([host.f_ipv4, host.f_ipv6, host.f_hostname,
                         acct.f_username, acct.f_fullname, acct.f_password,
                         acct.f_hash1_type, acct.f_hash1, acct.f_hash2_type, acct.f_hash2])

if type(output) == str:
    print output



########NEW FILE########
__FILENAME__ = import_nmap
#!/usr/bin/env python
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Import Nmap scan from the command line for Kvasir
##
## Run from a shell using web2py:
##
## ./web2py.py -R applications/$appname/private/import_nmap.py -S $appname -M -A -f filename -g asset_group -e engineer 
##
## Author: Edward Zaborowski
##--------------------------------------#
"""

import sys,time
import getpass
from optparse import OptionParser, OptionGroup
from skaldship.metasploit import msf_get_config

##--------------------------------------------------------------------

optparser = OptionParser(version=__version__)

optparser.add_option("-f", "--filename", dest="filename",
	action="store", default=None, help="Nmap XML filename")
optparser.add_option("-g", "--group", dest="asset_group",
	action="store", default="default", help="Asset group to assign hosts (default: 'default')")
optparser.add_option("-e", "--engineer", dest="engineer",
	action="store", default=getpass.getuser(), help="User to import data.")
optparser.add_option("-n", "--noports", dest="noports",
 	action="store_true", default=False, help="Add hosts without ports.")
optparser.add_option("-u", "--update", dest="update_hosts",
	action="store_true", default=False, help="Update hosts.")
optparser.add_option("-m", "--msfidx", dest="msfidx",
	action="store", default=0, help="Metasploit workspace index")
  
(options, params) = optparser.parse_args()

rows = db(db.auth_user.username==options.engineer)

if rows.count() != 1:
	exit("An error was encountered when selecting a user. Please try with a valid user name.")

msf_settings = msf_get_config(session)

msf_workspaces = [ None ]

try:
	# check to see if we have a Metasploit RPC instance configured and talking
	from MetasploitAPI import MetasploitAPI
	msf_api = MetasploitAPI(host=msf_settings['url'], apikey=msf_settings['key'])
	working_msf_api = msf_api.login()
except:
	working_msf_api = False

if working_msf_api:
	for w in msf_api.pro_workspaces().keys():
		msf_workspaces.append(w)

try:
	msf_workspace = msf_workspaces[int(options.msfidx)]
except IndexError:
	exit("An invalid workspace index has been provided. Aborting.")

msf_settings = {'workspace': msf_workspace, 'url': msf_settings['url'], 'key': msf_settings['key']}

task_vars = dict(
    scanner='nmap',
    filename=options.filename,
    addnoports=options.noports,
    asset_group=options.asset_group,
    engineer="%s" % rows.select().first().id,
    msf_settings=msf_settings,
    ip_ignore_list=[],
    ip_include_list=[],
    update_hosts=options.update_hosts
  )

task = scheduler.queue_task(
    scanner_import,
    pvars=task_vars,
    group_name=settings.scheduler_group_name,
    sync_output=5,
    timeout=settings.scheduler_timeout,
    immediate=True
)

if task.id:
    db.commit()
    exit('Success (%s).' % task.id)
else:
    exit('Error submitting job: %s' % (task.errors))

########NEW FILE########
__FILENAME__ = update_evidence
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2011 Cisco Systems, Inc.
##
## Looks at the screenshots directory and uploads any that are not
## in the database. Tries to be smart about those that are still
## open by checking the MD5 sum.
##
## Execute by:
##
##    /opt/SPA/web2py/web2py.py -S <appname> -M -R applications/<appname>/private/update_evidence -d applications/<appname>/data/files/screenshots -t S
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

import sys, os, glob
import hashlib, re
from optparse import OptionParser, OptionGroup
from skaldship.hosts import get_host_record

IPV4_REGEX = re.compile("^(?P<ipv4>((25[0-5]|2[0-4]\d|1\d\d|[1-9]\d|\d)\.){3}(25[0-5]|2[0-4]\d|1\d\d|[1-9]\d|\d))")

##--------------------------------------------------------------------

def md5_file(filename):
    if not filename:
        return

    md5 = hashlib.md5()
    try:
        with open(filename,'rb') as f:
            for chunk in iter(lambda: f.read(128*md5.block_size), ''):
                md5.update(chunk)
            return md5.hexdigest()
    except Exception, e:
        print "Error obtaining md5sum of %s: %s" % (filename, e)

    return

##--------------------------------------------------------------------

def md5_content(content):
    if not content:
        return

    md5 = hashlib.md5()
    md5.update(content)
    return md5.hexdigest()

##--------------------------------------------------------------------

def update_db(f_type=None, record=None, data=None, filename=None, ipv4_addr=None):
    """Adds or updates an existing record id"""

    if record is None:
        # inserting a new record into the database
        if ipv4_addr is None:
            print "ERROR: No IPv4 address provided"
            return False

        host_id = get_host_record(ipv4_addr)
        if not host_id:
            print "ERROR: IPv4 address %s is not a host in the database" % (ipv4_addr)
            return False

        try:
            db.t_evidence.insert(
                f_hosts_id = host_id.id,
                f_filename = filename,
                f_data = data,
                f_type = f_type
            )
        except Exception, e:
            print "ERROR inserting record:", e
            db.commit()
            return False

    else:
        # updating an existing record's data
        try:
            db.t_evidence[record].update(f_data = data)
        except Exception, e:
            print "ERROR updating record:", e
            db.commit()
            return False

    db.commit()
    return True

##--------------------------------------------------------------------

def get_ipv4(filename):
    ipv4_addr = IPV4_REGEX.match(filename)
    if ipv4_addr:
        return ipv4_addr.group('ipv4')
    else:
        return

##--------------------------------------------------------------------


def Run(directory=None, filename=None, f_type=None, ipv4_addr=None):

    print "======================================================================="
    print "\nAdding %s files to the Evidence database\n" % (f_type)

    default_locations = {
        'Session Log': 'session-logs/',
        'Screenshot': 'screenshots/',
    }

    if directory is not None and filename is None:
        currpath = os.path.abspath(os.path.curdir)
        directory = os.path.join(currpath, request.folder, default_locations[f_type])

        if f_type == "Session Logs":
            dir_glob = os.path.join(directory, '*.log')
        else:
            dir_glob = os.path.join(directory, '*')

        # build a list of database entires based on filename, md5 and record identifier
        db_entries = {}
        for row in db(db.t_evidence.f_type == f_type).select(db.t_evidence.f_filename, db.t_evidence.id, db.t_evidence.f_data):
            md5_data = md5_content(row.f_data)
            filename = row.f_filename
            db_entries[filename] = { 'md5': md5_data, 'id': row.id }

        for glob_result in glob.glob(dir_glob):
            (file_path, filename) = os.path.split(glob_result)

            ipv4_addr = get_ipv4(filename)
            if ipv4_addr is None:
                print "No IPv4 address provided or found in the filename: %s" % (filename)
                print "For files the IP address must be the first part of the name."
                print "Example: 192.168.1.0-description.png"
                continue

            full_path_filename = os.path.join(file_path, filename)
            md5_filesum = md5_file(full_path_filename)
            try:
                data = ''.join(open(full_path_filename, 'r').readlines())
            except Exception, e:
                print "ERROR opening %s: %s" % (full_path_filename, e)
                continue

            if db_entries.has_key(filename):
                if db_entries[filename]['md5'] != md5_filesum:
                    print "%s: md5 sums do not match, updating database..." % (filename)
                    res = update_db(f_type, db_entries[filename]['id'], data)
                else:
                    continue
            else:
                print "%s is NOT in database, adding it..." % (filename)
                res = update_db(f_type, None, data, filename, ipv4_addr)

    elif filename is not None:
        (dir_path, filename) = os.path.split(filename)

        if ipv4_addr is None:
            ipv4_addr = get_ipv4(filename)
            if ipv4_addr is None:
                print "No IPv4 address provided or found in the filename: %s" % (filename)
                print "For files the IP address must be the first part of the name."
                print "Example: 192.168.1.0-description.png"
                return

        try:
            data = ''.join(open(os.path.join(dir_path, filename), 'r').readlines())
        except Exception, e:
            print "ERROR opening %s: %s" % (filename, e)
            return

        db_row = db(db.t_evidence.f_filename == filename).select(db.t_evidence.f_filename, db.t_evidence.id, db.t_evidence.f_data).first()
        if db_row is None:
            print "%s is NOT in database, adding it..." % (filename)
            res = update_db(f_type, None, data, filename, ipv4_addr)
        else:
            print "%s is in the database, updating database..." % (filename)
            res = update_db(f_type, db_row.id, data, filename, ipv4_addr)

    return

##--------------------------------------------------------------------

# set up commandline arguments
optparser = OptionParser(version=__version__)

optparser.add_option("-t", "--type", dest="f_type",
    action="store", default=None, help="Evidence type ([S]creenshot, Session[L]og)")

file_group = OptionGroup(optparser, "File-based Options")
file_group.add_option("-i", "--ipv4", dest="ipv4",
    action="store", default=None, help="IPv4 Address (if only a filename)")
file_group.add_option("-f", "--file", dest="filename",
    action="store", default=None, help="Filename to insert/update")
optparser.add_option_group(file_group)

directory_group = OptionGroup(optparser, "Directory-based Options")
directory_group.add_option("-d", "--dir", dest="directory",
    action="store", default=None, help="Directory location")
optparser.add_option_group(directory_group)

(options, params) = optparser.parse_args()

if options.f_type is None:
    sys.exit("\nMust provide an evidence type\n")

f_type = options.f_type.lower()

if f_type in ["SessionLog", "l", "L"]:
    f_type = "Session Log"
elif f_type in ["Screenshot", "s", "S"]:
    f_type = "Screenshot"
else:
    sys.exit("\nMust provide an evidence type")

Run(options.directory, options.filename, f_type, options.ipv4)

########NEW FILE########
__FILENAME__ = upload_pwfile
#!/usr/bin/env python
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Sends the password file(s) via Kvasir API
##
##--------------------------------------#
"""

from optparse import OptionParser, OptionGroup, OptionValueError
import sys, os, glob, re
sys.path.append('/opt/SPA/tools/lib/python')
from AutoSPAngAPI import Accounts, Services

# this is a mess... should use ipaddr library instead
IPV4_REGEX = re.compile("^(?P<ipv4>((25[0-5]|2[0-4]\d|1\d\d|[1-9]\d|\d)\.){3}(25[0-5]|2[0-4]\d|1\d\d|[1-9]\d|\d))")
IPV6_REGEX = re.compile("^(?P<ipv6>\s*((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(%.+)?)")

# grab the ASPANG_URI if defined
ASPANG_URI = os.environ.get('ASPANG_URI', "http://localhost:8000/kvasir/api/call/jsonrpc")
PW_TYPES = ('PWDUMP', 'MSCa$h Dump', 'UNIX Passwd', 'UNIX Shadow', 'Medusa', 'Hydra', 'Username:Password', 'Usernames')

##--------------------------------------------------------------------

def process_pwfiles(options):
    """Processes each options.pw_file_infos record"""
    for pw_file_info in options.pw_file_infos:

        print("----------------------------------------------------------------")
        print(" [*] Processing password file: %s" % (pw_file_info['full_path']))
        try:
            pw_data = open(pw_file_info['full_path'], "r").readlines()
        except Exception, e:
            print " [!] Unable to load file: %s" % (e)
            return False

        acct_api = Accounts(options.aspanguri)
        svc_rec = get_svc_rec(options, pw_file_info['ipaddr'])
        if type(svc_rec) == type(int()):
            res = acct_api.upload_file(
                filename=pw_file_info['filename'],
                pw_data=pw_data,
                service_rec=svc_rec,
                add_to_evidence=options.add_to_evidence,
                f_type=options.f_type,
            )
            for line in res[1].split("\n"):
                if len(line) > 0:
                    print(" [-] %s" % (line))
        else:
            print(" [!] No service record found and none added for %s %s/%s" % (
                pw_file_info['ipaddr'], options.proto, options.port
            ))

##--------------------------------------------------------------------

def get_address(options, filename):
    if options.ipaddr is None:
        address = get_ipv4(filename)
        if address is None:
            address = get_ipv6(filename)
            if address is None:
                print " [!] No IPv4/IPv6 address provided or found in the filename: %s" % (filename)
            else:
                address = ipv6_addr
    else:
        address = options.ipaddr

    return address

##--------------------------------------------------------------------

def get_svc_rec(options, address):
    """Calls the Services.info() API to find the service record id"""
    svc_api = Services(options.aspanguri)
    svc = svc_api.info_or_add(ipaddr=address, proto=options.proto, port=options.port)
    if svc:
        return svc[0][0]
    else:
        # No service record found, lets try to add it if desired
        if options.add_to_services:
            retval = svc_api.add(ipaddr=address, proto=options.proto, port=options.port)
            if not retval[0]:
                print(retval[1])
                return None
            else:
                return retval[1]

##--------------------------------------------------------------------

def get_ipv4(filename):
    ipv4_addr = IPV4_REGEX.match(filename)
    if ipv4_addr:
        return ipv4_addr.group('ipv4')
    else:
        return

##--------------------------------------------------------------------

def get_ipv6(filename):
    ipv6_addr = IPV6_REGEX.match(filename)
    if ipv6_addr:
        return ipv6_addr.group('ipv6')
    else:
        return

##--------------------------------------------------------------------

def Run(options):

    options.pw_file_infos = []
    if options.directory is not None and options.filename is None:

        print " [*] Processing directory: %s" % (options.directory)
        for file_path, dirs, filenames in os.walk(options.directory):
            for filename in filenames:
                address = get_address(options, filename)
                if address:
                    full_path_filename = os.path.join(file_path, filename)
                    options.pw_file_infos.append({
                        'ipaddr': address,
                        'full_path': full_path_filename,
                        'filename': filename
                    })

    elif options.filename is not None:
        (file_path, options.filename) = os.path.split(options.filename)
        if not file_path:
            file_path = os.getcwd()
        full_path_filename = os.path.join(file_path, options.filename)

        address = get_address(options, options.filename)
        options.pw_file_infos.append({
            'ipaddr': address,
            'full_path': full_path_filename,
            'filename': options.filename
        })

    process_pwfiles(options)

    return

##--------------------------------------------------------------------

def check_pwtypes(options, opt_str, value, parser):
    if value in PW_TYPES:
        parser.values.f_type = value
    else:
        raise OptionValueError(" [!] Invalid password file type: %s\nValid entries are:\n\n%s" % (value, ', '.join(PW_TYPES)))

##--------------------------------------------------------------------
if __name__=='__main__':
    # set up commandline arguments
    optparser = OptionParser(version=__version__)

    optparser.add_option("-p", "--port", dest="port",
        action="store", default=None, help="Protocol/Port (info/0, tcp/445) for all files")
    optparser.add_option("-e", "--add_evidence", dest="add_to_evidence",
        action="store_true", help="Add to evidence table")
    optparser.add_option("-a", "--add_service", dest="add_to_services",
        action="store_true", help="Add to services DB if not found")
    optparser.add_option('-u', '--uri', dest='aspanguri',
        action="store", default=ASPANG_URI, help="Kvasir API URI (eg: %s)" % (ASPANG_URI))

    file_group = OptionGroup(optparser, "File-based Options")
    file_group.add_option("-i", "--ip", dest="ipaddr",
        action="store", default=None, help="IPv4/IPv6 Address (if non-std filename)")
    file_group.add_option("-f", "--file", dest="filename",
        action="store", default=None, help="Filename to insert/update")
    file_group.add_option('-t', '--type', dest='f_type', type='string',
        action='callback', callback=check_pwtypes, help='Password file type: %s' % (', '.join(PW_TYPES)))
    optparser.add_option_group(file_group)

    directory_group = OptionGroup(optparser, "Directory-based Options")
    directory_group.add_option("-d", "--dir", dest="directory",
        action="store", default=None, help="Directory location, all files should be named <ipdadr>-value")
    optparser.add_option_group(directory_group)

    (options, params) = optparser.parse_args()

    print " ----------------------------------------------------------"
    print " -- Password file upload, Kvasir master pwner edition --"
    print " ----------------------------------------------------------\n"

    if options.f_type is None:
        raise OptionValueError("\n [!] Must provide a password file type\nValid entries are:\n\n%s" % (', '.join(PW_TYPES)))

    if options.filename is None and options.directory is None:
        raise OptionValueError("\n [!] Must provide a filename or directory of files to upload.\n")

    try:
        (options.proto, options.port) = options.port.split('/')
    except Exception:
        raise OptionValueError("\n [!] Invalid port provided. Must be something like info/0 or tcp/22\n")

    Run(options)

########NEW FILE########
__FILENAME__ = user
#!/usr/bin/env python
# encoding: utf-8

__version__ = "1.0"

"""
##--------------------------------------#
## Kvasir
##
## (c) 2010-2013 Cisco Systems, Inc.
##
## Create/Change password users for Kvasir
##
## Run from a shell using web2py:
##
##   ./web2py.py -R applications/$appname/private/user.py -S $appname -M -A -u username -p password
##
## Author: Kurt Grutzmacher <kgrutzma@cisco.com>
##--------------------------------------#
"""

import sys
import getpass
from optparse import OptionParser, OptionGroup

##--------------------------------------------------------------------

optparser = OptionParser(version=__version__)

optparser.add_option("-u", "--user", dest="user",
    action="store", default=None, help="Username")
optparser.add_option("-p", "--password", dest="password",
    action="store", default=None, help="Password")
optparser.add_option("-P", "--prompt", dest="prompt",
    action="store_true", default=False, help="Prompt for password")
optparser.add_option("-n", "--nochange", dest="nochange",
    action="store_true", default=False, help="Do not change the user information")
optparser.add_option("-f", "--force", dest="forcechange",
    action="store_true", default=False, help="Force the change of user information without prompt")

(options, params) = optparser.parse_args()

print "\n\nKvasir User Add/Modify Management\n"
if not options.user:
    user = raw_input("Username: ")
else:
    user = options.user

if not user:
    sys.exit("No username provided\n")

# see if the user exists first
user_row = db(db.auth_user.username == user).select().first()
if user_row:
    # user exists, update password
    if options.nochange:
        sys.exit("Not changing user...\n")
    if not options.forcechange:
        ask_update = raw_input("User exists, update password? [y/N]: ")
        if ask_update not in ['Y', 'y'] :
            sys.exit("Ok, leaving user as-is...\n")

if not options.password or options.prompt:
    password = getpass.getpass("Password: ")
else:
    password = options.password

if not password or password == '':
    sys.exit("Password cannot be blank\n")

if user_row:
    # user exists, update password
    print "Updating password for %s..." % (user)
    user_row.update(password=password)
    db.commit()

else:
    # new user
    print "Adding %s to Kvasir user database..." % (user)
    db.auth_user.validate_and_insert(
        username=user,
        password=password
    )
    db.commit()

########NEW FILE########

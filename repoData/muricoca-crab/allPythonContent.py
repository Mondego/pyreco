__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# crab documentation build configuration file, created by
# sphinx-quickstart on Wed May 11 02:16:09 2011.
#
# This file is execfile()d with the current directory set to its containing dir
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration ----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'crab'
copyright = u'2011,Marcel Caraciolo,Bruno Melo,Ricardo Caspirro,Rodrigo Alves'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.0.1'
# The full version, including alpha/beta/rc tags.
release = '0.0.1'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output --------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'crabdoc'


# -- Options for LaTeX output -------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual])
latex_documents = [
  ('index', 'crab.tex', u'crab Documentation',
   u'Marcel Caraciolo, Bruno Melo, Ricardo Caspirro, Rodrigo Alves', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output -------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'crab', u'crab Documentation',
     [u'Marcel Caraciolo, Bruno Melo, Ricardo Caspirro, Rodrigo Alves'], 1)
]

########NEW FILE########
__FILENAME__ = base
#-*- coding:utf-8 -*-

"""
Base Recommender Models.
"""

# Authors: Marcel Caraciolo <marcel@muricoca.com>
#          Bruno Melo <bruno@muricoca.com>
# License: BSD Style.

from scikits.learn.base import BaseEstimator


class BaseRecommender(BaseEstimator):
    """
    Base Class for Recommenders that suggest items for users.

    Should not be used directly, use derived classes instead

    Attributes
    ----------
     model:  DataModel
          Defines the data model where data is fetched.

     with_preference: bool
          Defines if the recommendations come along with the
          estimated preferences. (default= False)

    """

    def __init__(self, model, with_preference=False):
        self.model = model
        self.with_preference = with_preference

    def recommend(self, user_id, how_many, **params):
        '''
        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.
        how_many: int
                 Desired number of recommendations
        rescorer:  function, optional
                 Rescoring function to apply before final list of
                 recommendations.

        Returns
        ---------
        Return a list of recommended items, ordered from most strongly
        recommend to least.

        '''
        raise NotImplementedError("BaseRecommender is an abstract class.")

    def estimate_preference(self, user_id, item_id, **params):
        '''
        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.

        item_id: int or string
                Item for which recommendations are to be computed.

        Returns
        -------
        Return an estimated preference if the user has not expressed a
        preference for the item, or else the user's actual preference for the
        item. If a preference cannot be estimated, returns None.
        '''
        raise NotImplementedError("BaseRecommender is an abstract class.")

    def all_other_items(self, user_id, **params):
        '''
        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.

        Returns
        --------
        Return all items in the `model` for which the user has not expressed
        the preference and could possibly be recommended to the user.
        '''
        raise NotImplementedError("BaseRecommender is an abstract class.")

    def set_preference(self, user_id, item_id, value):
        '''
        Set a new preference of a user for a specific item with a certain
        magnitude.

        Parameters
        ----------
        user_id: int or string
                 User for which the preference will be updated.

        item_id: int or string
                 Item that will be updated.

        value:  The new magnitude for the preference of a item_id from a
                user_id.

        '''
        self.model.set_preference(user_id, item_id, value)

    def remove_preference(self, user_id, item_id):
        '''
        Remove a preference of a user for a specific item

        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.
        item_id: int or string
                 Item that will be removed the preference for the user_id.

        '''
        self.model.remove_preference(user_id, item_id)

########NEW FILE########
__FILENAME__ = base

"""
Base IO code for all datasets
"""

# Authors: Marcel Caraciolo <marcel@muricoca.com>
#          Bruno Melo <bruno@muricoca.com>
# License: BSD Style.

from os.path import dirname
from os.path import join
import numpy as np


class Bunch(dict):
    """
    Container object for datasets: dictionary-like object
    that exposes its keys and attributes. """

    def __init__(self, **kwargs):
        dict.__init__(self, kwargs)
        self.__dict__ = self


def load_movielens_r100k(load_timestamp=False):
    """ Load and return the MovieLens dataset with
        100,000 ratings (only the user ids, item ids, timestamps
        and ratings).

    Parameters
    ----------
    load_timestamp: bool, optional (default=False)
        Whether it loads the timestamp.

    Return
    ------
    data: Bunch
        Dictionary-like object, the interesting attributes are:
        'data', the full data in the shape:
            {user_id: { item_id: (rating, timestamp),
                       item_id2: (rating2, timestamp2) }, ...} and
        'user_ids': the user labels with respective ids in the shape:
            {user_id: label, user_id2: label2, ...} and
        'item_ids': the item labels with respective ids in the shape:
            {item_id: label, item_id2: label2, ...} and
        DESCR, the full description of the dataset.

    Examples
    --------
    To load the MovieLens data::

    >>> from scikits.crab.datasets import load_movielens_r100k
    >>> movies = load_movielens_r100k()
    >>> len(movies['data'])
    943
    >>> len(movies['item_ids'])
    1682

    """
    base_dir = join(dirname(__file__), 'data/')
    #Read data
    if load_timestamp:
        data_m = np.loadtxt(base_dir + 'movielens100k.data',
                delimiter='\t', dtype=int)
        data_movies = {}
        for user_id, item_id, rating, timestamp in data_m:
            data_movies.setdefault(user_id, {})
            data_movies[user_id][item_id] = (timestamp, int(rating))
    else:
        data_m = np.loadtxt(base_dir + 'movielens100k.data',
                delimiter='\t', usecols=(0, 1, 2), dtype=int)

        data_movies = {}
        for user_id, item_id, rating in data_m:
            data_movies.setdefault(user_id, {})
            data_movies[user_id][item_id] = int(rating)

    #Read the titles
    data_titles = np.loadtxt(base_dir + 'movielens100k.item',
             delimiter='|', usecols=(0, 1), dtype=str)

    data_t = []
    for item_id, label in data_titles:
        data_t.append((int(item_id), label))
    data_titles = dict(data_t)

    fdescr = open(dirname(__file__) + '/descr/movielens100k.rst')

    return Bunch(data=data_movies, item_ids=data_titles,
                 user_ids=None, DESCR=fdescr.read())


def load_sample_songs():
    """ Load and return the songs dataset with
         49 ratings (only the user ids, item ids and ratings).

    Return
    ------
    data: Bunch
        Dictionary-like object, the interesting attributes are:
        'data', the full data in the shape:
            {user_id: { item_id: (rating, timestamp),
                       item_id2: (rating2, timestamp2) }, ...} and
        'user_ids': the user labels with respective ids in the shape:
            {user_id: label, user_id2: label2, ...} and
        'item_ids': the item labels with respective ids in the shape:
            {item_id: label, item_id2: label2, ...} and
        DESCR, the full description of the dataset.

    Examples
    --------
    To load the sample songs data::

    >>> from scikits.crab.datasets import load_sample_songs
    >>> songs = load_sample_songs()
    >>> len(songs['data'])
    8
    >>> len(songs['item_ids'])
    8

    """
    base_dir = join(dirname(__file__), 'data/')

    #Read data
    data_m = np.loadtxt(base_dir + 'sample_songs.csv',
                delimiter=',', dtype=str)
    item_ids = []
    user_ids = []
    data_songs = {}
    for user_id, item_id, rating in data_m:
        if user_id not in user_ids:
            user_ids.append(user_id)
        if item_id not in item_ids:
            item_ids.append(item_id)
        u_ix = user_ids.index(user_id) + 1
        i_ix = item_ids.index(item_id) + 1
        data_songs.setdefault(u_ix, {})
        data_songs[u_ix][i_ix] = float(rating)

    data_t = []
    for no, item_id in enumerate(item_ids):
        data_t.append((no + 1, item_id))
    data_titles = dict(data_t)

    data_u = []
    for no, user_id in enumerate(user_ids):
        data_u.append((no + 1, user_id))
    data_users = dict(data_u)

    fdescr = open(dirname(__file__) + '/descr/sample_songs.rst')

    return Bunch(data=data_songs, item_ids=data_titles,
                 user_ids=data_users, DESCR=fdescr.read())


def load_sample_movies():
    """ Load and return the movies dataset with
         n ratings (only the user ids, item ids and ratings).

    Return
    ------
    data: Bunch
        Dictionary-like object, the interesting attributes are:
        'data', the full data in the shape:
            {user_id: { item_id: (rating, timestamp),
                       item_id2: (rating2, timestamp2) }, ...} and
        'user_ids': the user labels with respective ids in the shape:
            {user_id: label, user_id2: label2, ...} and
        'item_ids': the item labels with respective ids in the shape:
            {item_id: label, item_id2: label2, ...} and
        DESCR, the full description of the dataset.

    Examples
    --------
    To load the sample movies data::

    >>> from scikits.crab.datasets import load_sample_movies
    >>> movies = load_sample_movies()
    >>> len(movies['data'])
    7
    >>> len(movies['item_ids'])
    6

    """
    base_dir = join(dirname(__file__), 'data/')

    #Read data
    data_m = np.loadtxt(base_dir + 'sample_movies.csv',
                delimiter=';', dtype=str)
    item_ids = []
    user_ids = []
    data_songs = {}
    for user_id, item_id, rating in data_m:
        if user_id not in user_ids:
            user_ids.append(user_id)
        if item_id not in item_ids:
            item_ids.append(item_id)
        u_ix = user_ids.index(user_id) + 1
        i_ix = item_ids.index(item_id) + 1
        data_songs.setdefault(u_ix, {})
        data_songs[u_ix][i_ix] = float(rating)

    data_t = []
    for no, item_id in enumerate(item_ids):
        data_t.append((no + 1, item_id))
    data_titles = dict(data_t)

    data_u = []
    for no, user_id in enumerate(user_ids):
        data_u.append((no + 1, user_id))
    data_users = dict(data_u)

    fdescr = open(dirname(__file__) + '/descr/sample_movies.rst')

    return Bunch(data=data_songs, item_ids=data_titles,
                 user_ids=data_users, DESCR=fdescr.read())

########NEW FILE########
__FILENAME__ = book_crossing
"""Caching loader for the Book-Crossing Dataset

The description of the dataset is available on the official website at:

    http://www.informatik.uni-freiburg.de/~cziegler/BX/

Quoting the introduction:

    Collected by Cai-Nicolas Ziegler in a 4-week crawl
    (August / September 2004) from the Book-Crossing community
    with kind permission from Ron Hornbaker, CTO of Humankind
    Systems. Contains 278,858 users (anonymized but with
    demographic information) providing 1,149,780 ratings
    (explicit / implicit) about 271,379 books.


This dataset loader will download the dataset,
which its size is around 22 Mb compressed. Once
uncompressed the train set is around 130 MB.

The data is downloaded, extracted and cached in the '~/scikit_crab_data'
folder.

References
----------
Improving Recommendation Lists Through Topic Diversification,
Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, Georg Lausen;
Proceedings of the 14th International World Wide Web Conference (WWW '05),
May 10-14, 2005, Chiba, Japan.


"""
# Copyright (c) 2011 Marcel Caraciolo <marcel@muricoca.com>
# License: Simplified BSD

import os
import urllib
import logging
import zipfile
from os.path import dirname
from os.path import join
import numpy as np
from base import Bunch
import csv

logger = logging.getLogger(__name__)

URL = "http://www.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip"
ARCHIVE_NAME = "BX-CSV-Dump.zip"


def download_book_crossings(target_dir):
    """ Download the book-crossing data and unzip it """
    archive_path = os.path.join(target_dir, ARCHIVE_NAME)

    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    if not os.path.exists(archive_path):
        logger.warn("Downloading dataset from %s (77 MB)", URL)
        opener = urllib.urlopen(URL)
        open(archive_path, 'wb').write(opener.read())

    logger.info("Decompressing %s", archive_path)

    source_zip = zipfile.ZipFile(archive_path, 'r')
    archives = []
    for name in source_zip.namelist():
        if name.find('.csv') != -1:
            source_zip.extract(name, target_dir)
            archives.append(name)
    source_zip.close()
    os.remove(archive_path)

    return archives


def load_bookcrossings(data_home=None, download_if_missing=True,
                     implicit=False):
    """
    Load the filenames of the Book Crossings dataset

    data_home: optional, default: None
        Specify the storage folder for the datasets. If None,
        all files is stored in '~/data subfolders.

    download_if_missing: optional, True by default
        If False, raise an IOError if the data is not locally available
        instead of trying to download the data from the source site.

    implicit: optional, False by default
        If True, it will load the implicit ratings expressed by rating 0,
        otherwise it will load the explicit ratings expressed by rating 1-10.

    Examples
    --------
    >>> from os.path import join
    >>> from os.path import dirname
    >>> from scikits.crab.datasets.book_crossing import load_bookcrossings
    >>> data_home = join(dirname(__file__), 'scikits/crab/datasets/tests/data/')
    >>> books = load_bookcrossings(data_home)
    >>> len(books.data)
    26
    >>> len(books.item_ids)
    100

    """

    if data_home:
        if not os.path.exists(data_home):
            os.makedirs(data_home)
    else:
        data_home = join(dirname(__file__), 'data/')

    try:
        if not os.path.exists(os.path.join(data_home, 'BX-Book-Ratings.csv')) \
            and not open(os.path.join(data_home, 'BX-Books.csv')):
            raise IOError
    except Exception, e:
        print 80 * '_'
        print 'Loading files failed'
        print 80 * '_'
        print e

        if download_if_missing:
            print 'downloading the dataset...'
            try:
                download_book_crossings(data_home)
            except:
                raise Exception('FAIL: Problems during the download.')
            print 'dataset downloaded.'
        else:
            raise IOError('Book-Crossing dataset not found')

    #TO FIX: it is not working for np.loadtxt
    #ratings_m = np.loadtxt(os.path.join(data_home, 'BX-Book-Ratings.csv'),
    #            delimiter=';', skiprows=1)

    ratings_m = csv.reader(open(os.path.join(data_home,
                'BX-Book-Ratings.csv')), delimiter=';')
    ratings_m.next()
    data_books = {}
    if implicit:
        for user_id, item_id, rating in ratings_m:
            if rating == "0":
                data_books.setdefault(user_id, {})
                data_books[user_id][item_id] = True
    else:
        for user_id, item_id, rating in ratings_m:
            rating = int(rating)
            if rating != "0":
                data_books.setdefault(user_id, {})
                data_books[user_id][item_id] = int(rating)

    #Read the titles
    data_titles = np.loadtxt(os.path.join(data_home, 'BX-Books.csv'),
             delimiter=';', usecols=(0, 1), dtype=str)

    data_t = []
    for item_id, label in data_titles:
        data_t.append((item_id, label))
    data_titles = dict(data_t)

    fdescr = open(dirname(__file__) + '/descr/book-crossing.rst')

    return Bunch(data=data_books, item_ids=data_titles,
                 user_ids=None, DESCR=fdescr.read())

########NEW FILE########
__FILENAME__ = test_base
from os.path import join
from os.path import dirname
from nose.tools import assert_equal, assert_raises
from ..base import load_movielens_r100k, load_sample_songs, load_sample_movies
from ..book_crossing import load_bookcrossings
from ...utils.testing import assert_in


def test_movielens_r100k():
    #with timestamp = False
    movies = load_movielens_r100k()
    assert_in(movies, in_=['data', 'item_ids', 'user_ids', 'DESCR'])
    assert_equal(len(movies.data), 943)
    assert_equal(len(movies.item_ids), 1682)
    assert_equal(sum([len(items) for key, items in
             movies.data.iteritems()]), 100000)

    #with timestamp = True
    movies = load_movielens_r100k(True)
    assert_in(movies, in_=['data', 'item_ids', 'user_ids', 'DESCR'])
    assert_equal(len(movies.data), 943)
    assert_equal(len(movies.item_ids), 1682)
    assert_equal(movies.data[1][1], (874965758, 5))
    assert_equal(sum([len(items) for key, items in
             movies.data.iteritems()]), 100000)


def test_sample_songs():
    songs = load_sample_songs()
    assert_in(songs, in_=['data', 'item_ids', 'user_ids', 'DESCR'])
    assert_equal(len(songs.data), 8)
    assert_equal(len(songs.item_ids), 8)
    assert_equal(sum([len(items) for key, items in
             songs.data.iteritems()]), 49)


def test_sample_movies():
    movies = load_sample_movies()
    assert_in(movies, in_=['data', 'item_ids', 'user_ids', 'DESCR'])
    assert_equal(len(movies.data), 7)
    assert_equal(len(movies.item_ids), 6)
    assert_equal(sum([len(items) for key, items in
             movies.data.iteritems()]), 35)


def test_load_bookcrossings():
    data_home = join(dirname(__file__), 'data/')
    #explicit with sample data from tests/data
    books = load_bookcrossings(data_home)
    assert_equal(len(books.data), 26)
    assert_equal(len(books.item_ids), 100)
    assert_equal(sum([len(items) for key, items in
             books.data.iteritems()]), 99)

    #implicit with sample data from tests/data
    books = load_bookcrossings(data_home, implicit=True)
    assert_equal(len(books.data), 15)
    assert_equal(len(books.item_ids), 100)
    assert_equal(sum([len(items) for key, items in
             books.data.iteritems()]), 60)

    #explicit with download denied.
    data_home = dirname(__file__)
    assert_raises(IOError, load_bookcrossings, data_home, implicit=False,
                download_if_missing=False)

########NEW FILE########
__FILENAME__ = base
#-*- coding:utf-8 -*-

"""Utilities to evaluate the predictive performance of the recommenders
"""

# Authors: Marcel Caraciolo <marcel@muricoca.com>

# License: BSD Style.


class RecommenderEvaluator(object):
    """
    Basic Interface which is responsible to evaluate the quality of Recommender
    recommendations. The range of values that may be returned depends on the
    implementation. but lower values must mean better recommendations, with 0
    being the lowest / best possible evaluation, meaning a perfect match.

    """

    def evaluate(self, recommender, metrics=None, **kwargs):
        """
        Evaluates the predictor

        Parameters
        ----------

        recommender: The BaseRecommender instance
                The recommender instance to be evaluated.

        metrics: [None|'rmse'|'f1score'|'precision'|'recall'|'nmae'|'mae']
        If metrics is None, all metrics available will be evaluated.
        Otherwise it will return the specified metric evaluated.

        Returns
        -------
        Returns scores representing how well the recommender estimated the
        preferences match real values.
        """
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def evaluate_online(self, metrics=None, **kwargs):
        """
        Online evaluation for recommendation prediction

        Parameters
        ----------
        metrics: [None|'rmse'|'f1score'|'precision'|'recall'|'nmae'|'mae']
        If metrics is None, all metrics available will be evaluated.
        Otherwise it will return the specified metric evaluated.

        Returns
        -------

        Returns scores representing how well the recommender estimated the
        preferences match real values.
        """
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def evaluate_on_split(self, metrics=None, **kwargs):
        """
        Evaluate on the folds of a dataset split

        Parameters
        ----------
        metrics: [None|'rmse'|'f1score'|'precision'|'recall'|'nmae'|'mae']
        If metrics is None, all metrics available will be evaluated.
        Otherwise it will return the specified metric evaluated.

        Returns
        -------
        Returns scores representing how well the recommender estimated the
        preferences match real values.
        """
        raise NotImplementedError("cannot instantiate Abstract Base Class")

########NEW FILE########
__FILENAME__ = classes
 #-*- coding:utf-8 -*-

"""
This module contains main implementations that encapsulate
    retrieval-related statistics about the quality of the recommender's
    recommendations.

"""
# Authors: Marcel Caraciolo <marcel@muricoca.com>
# License: BSD Style.

import operator
import numpy as np
from base import RecommenderEvaluator
from metrics import root_mean_square_error
from metrics import mean_absolute_error
from metrics import normalized_mean_absolute_error
from metrics import evaluation_error
from cross_validation import KFold
from metrics import precision_score
from metrics import recall_score
from metrics import f1_score
from sampling import SplitSampling
from scikits.learn.base import clone
from ..models.utils import ItemNotFoundError, UserNotFoundError


#Collaborative Filtering Evaluator
#==================================

evaluation_metrics = {
        'rmse': root_mean_square_error,
        'mae': mean_absolute_error,
        'nmae': normalized_mean_absolute_error,
        'precision': precision_score,
        'recall': recall_score,
        'f1score': f1_score
}


def check_sampling(sampling, n):
    """Input checker utility for building a
       sampling in a user friendly way.

   Parameters
   ===========
    sampling: a float, a sampling generator instance, or None
        The input specifying which sampling generator to use.
        It can be an float, in which case it is the the proportion of
        the dataset to include in the training set in SplitSampling.
        None, in which case all the elements are used,
        or another object, that will then be used as a cv generator.

    n: an integer.
        The number of elements.

    """
    if sampling is None:
        sampling = 1.0
    if operator.isNumberType(sampling):
        sampling = SplitSampling(n, evaluation_fraction=sampling)

    return sampling


def check_cv(cv, n):
    """Input checker utility for building a
       cross validation in a user friendly way.

   Parameters
   ===========
    sampling: an integer, a cv generator instance, or None
        The input specifying which cv generator to use.
        It can be an integer, in which case it is the number
        of folds in a KFold, None, in which case 3 fold is used,
        or another object, that will then be used as a cv generator.

    n: an integer.
        The number of elements.

    """
    if cv is None:
        cv = 3
    if operator.isNumberType(cv):
        cv = KFold(n, cv, indices=True)

    return cv


class CfEvaluator(RecommenderEvaluator):

    """
    Examples
    --------
    >>> from scikits.crab.similarities import UserSimilarity
    >>> from scikits.crab.metrics import  euclidean_distances
    >>> from scikits.crab.models import  MatrixPreferenceDataModel
    >>> from scikits.crab.recommenders.knn import UserBasedRecommender
    >>> from scikits.crab.metrics.classes import CfEvaluator
    >>> from scikits.crab.recommenders.knn.neighborhood_strategies import NearestNeighborsStrategy
    >>> movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, \
    'Snakes on a Plane': 3.5, \
    'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \
    'The Night Listener': 3.0}, \
    'Paola Pow': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \
    'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \
    'You, Me and Dupree': 3.5}, \
    'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, \
    'Superman Returns': 3.5, 'The Night Listener': 4.0}, \
    'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, \
    'The Night Listener': 4.5, 'Superman Returns': 4.0, \
    'You, Me and Dupree': 2.5}, \
    'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
    'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, \
    'You, Me and Dupree': 2.0}, \
    'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
    'The Night Listener': 3.0, 'Superman Returns': 5.0, \
    'You, Me and Dupree': 3.5}, \
    'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0, \
    'Superman Returns':4.0}, \
    'Maria Gabriela': {}}
    >>> model = MatrixPreferenceDataModel(movies)
    >>> similarity = UserSimilarity(model, euclidean_distances)
    >>> neighborhood = NearestNeighborsStrategy()
    >>> recsys = UserBasedRecommender(model, similarity, neighborhood)
    >>> evaluator = CfEvaluator()
    >>> all_scores = evaluator.evaluate(recsys, permutation=False)
    >>> all_scores
    {'rmse': 0.23590725429603751, 'recall': 1.0, 'precision': 1.0, \
    'mae': 0.21812065003607684, 'f1score': 1.0, 'nmae': 0.054530162509019209}
    >>> rmse = evaluator.evaluate_on_split(recsys, metric='rmse', permutation=False)
    >>> rmse
    ({'error': [{'rmse': 0.35355339059327379}, \
     {'rmse': 0.97109049202292397},  \
     {'rmse': 0.39418387598407179}]},  \
     {'final_error': {'avg': {'rmse': 0.57294258620008975}, \
     'stdev': {'rmse': 0.28202130565981975}}})

    """

    def _build_recommender(self, dataset, recommender):
        """
        Build a clone recommender with the given dataset
        as the training set.

        Parameters
        ----------

        dataset: dict
            The dataset with the user's preferences.

        recommender: A scikits.crab.base.BaseRecommender object.
            The given recommender to be cloned.

        """
        recommender_training = clone(recommender)
        #if the recommender's model has the build_model implemented.

        if not recommender.model.has_preference_values():
            recommender_training.model.dataset = \
                    recommender_training.model._load_dataset(dataset.copy())
        else:
            recommender_training.model.dataset = dataset

        if hasattr(recommender_training.model, 'build_model'):
            recommender_training.model.build_model()

        return recommender_training

    def evaluate(self, recommender, metric=None, **kwargs):
        """
        Evaluates the predictor

        Parameters
        ----------
        recommender: The BaseRecommender instance
                The recommender instance to be evaluated.

        metric: [None|'rmse'|'f1score'|'precision'|'recall'|'nmae'|'mae']
            If metrics is None, all metrics available will be evaluated.
        Otherwise it will return the specified metric evaluated.

        sampling_users:  float or sampling, optional, default = None
            If an float is passed, it is the percentage of evaluated
        users. If sampling_users is None, all users are used in the
        evaluation. Specific sampling objects can be passed, see
        scikits.crab.metrics.sampling module for the list of possible
        objects.

        sampling_ratings:  float or sampling, optional, default = None
            If an float is passed, it is the percentage of evaluated
        ratings. If sampling_ratings is None, 70% will be used in the
        training set and 30% in the test set. Specific sampling objects
        can be passed, see scikits.crab.metrics.sampling module
        for the list of possible objects.

        at: integer, optional, default = None
            This number at is the 'at' value, as in 'precision at 5'.  For
        example this would mean precision or recall evaluated by removing
        the top 5 preferences for a user and then finding the percentage of
        those 5 items included in the top 5 recommendations for that user.
        If at is None, it will consider all the top 3 elements.

        Returns
        -------
        Returns a dictionary containing the evaluation results:
        (NMAE, MAE, RMSE, Precision, Recall, F1-Score)

        """
        sampling_users = kwargs.pop('sampling_users', None)
        sampling_ratings = kwargs.pop('sampling_ratings', 0.7)
        permutation = kwargs.pop('permutation', True)
        at = kwargs.pop('at', 3)

        if metric not in evaluation_metrics and metric is not None:
            raise ValueError('metric %s is not recognized. valid keywords \
              are %s' % (metric, evaluation_metrics.keys()))

        n_users = recommender.model.users_count()
        sampling_users = check_sampling(sampling_users, n_users)
        users_set, _ = sampling_users.split(permutation=permutation)

        training_set = {}
        testing_set = {}

        #Select the users to be evaluated.
        user_ids = recommender.model.user_ids()
        for user_id in user_ids[users_set]:
            #Select the ratings to be evaluated.
            preferences = recommender.model.preferences_from_user(user_id)

            sampling_eval = check_sampling(sampling_ratings, \
                                             len(preferences))
            train_set, test_set = sampling_eval.split(indices=True,
                                        permutation=permutation)

            preferences = list(preferences)
            if recommender.model.has_preference_values():
                training_set[user_id] = dict((preferences[idx]
                             for idx in train_set)) if preferences else {}
                testing_set[user_id] = [preferences[idx]
                             for idx in test_set] if preferences else []
            else:
                training_set[user_id] = dict(((preferences[idx], 1.0)
                             for idx in train_set)) if preferences else {}
                testing_set[user_id] = [(preferences[idx], 1.0)
                             for idx in test_set] if preferences else []

        #Evaluate the recommender.
        recommender_training = self._build_recommender(training_set, \
                                recommender)

        real_preferences = []
        estimated_preferences = []

        for user_id, preferences in testing_set.iteritems():
            for item_id, preference in preferences:
            #Estimate the preferences
                try:
                    estimated = recommender_training.estimate_preference(
                                user_id, item_id)
                    real_preferences.append(preference)
                except ItemNotFoundError:
                    # It is possible that an item exists in the test data but
                    # not training data in which case an exception will be
                    # throw. Just ignore it and move on
                    continue
                estimated_preferences.append(estimated)

        #Return the error results.
        if metric in ['rmse', 'mae', 'nmae']:
            eval_function = evaluation_metrics[metric]
            if metric == 'nmae':
                return {metric: eval_function(real_preferences,
                                          estimated_preferences,
                                recommender.model.maximum_preference_value(),
                                recommender.model.minimum_preference_value())}
            return {metric: eval_function(real_preferences,
                                          estimated_preferences)}

        #IR_Statistics
        relevant_arrays = []
        real_arrays = []

        #Select the users to be evaluated.
        user_ids = recommender.model.user_ids()
        for user_id in user_ids[users_set]:
            preferences = recommender.model.preferences_from_user(user_id)
            preferences = list(preferences)
            if len(preferences) < 2 * at:
                # Really not enough prefs to meaningfully evaluate the user
                continue

            # List some most-preferred items that would count as most
            if not recommender.model.has_preference_values():
                preferences = [(preference, 1.0) for preference in preferences]

            preferences = sorted(preferences, key=lambda x: x[1], reverse=True)
            relevant_item_ids = [item_id for item_id, preference
                                    in preferences[:at]]

            if len(relevant_item_ids) == 0:
                continue

            training_set = {}
            for other_user_id in recommender.model.user_ids():
                preferences_other_user = \
                    recommender.model.preferences_from_user(other_user_id)

                if not recommender.model.has_preference_values():
                    preferences_other_user = [(preference, 1.0)
                                     for preference in preferences_other_user]
                if other_user_id == user_id:
                    preferences_other_user = \
                        [pref for pref in preferences_other_user \
                            if pref[0] not in relevant_item_ids]

                    if preferences_other_user:
                        training_set[other_user_id] = \
                            dict(preferences_other_user)
                else:
                    training_set[other_user_id] = dict(preferences_other_user)

            #Evaluate the recommender
            recommender_training = self._build_recommender(training_set, \
                        recommender)

            try:
                preferences = \
                    recommender_training.model.preferences_from_user(user_id)
                preferences = list(preferences)
                if not preferences:
                    continue
            except:
                #Excluded all prefs for the user. move on.
                continue

            recommended_items = recommender_training.recommend(user_id, at)
            relevant_arrays.append(list(relevant_item_ids))
            real_arrays.append(list(recommended_items))

        relevant_arrays = np.array(relevant_arrays)
        real_arrays = np.array(real_arrays)

        #Return the IR results.
        if metric in ['precision', 'recall', 'f1score']:
            eval_function = evaluation_metrics[metric]
            return {metric: eval_function(real_arrays, relevant_arrays)}

        if metric is None:
            #Return all
            mae, nmae, rmse = evaluation_error(real_preferences,
                        estimated_preferences,
                        recommender.model.maximum_preference_value(),
                        recommender.model.minimum_preference_value())
            f = f1_score(real_arrays, relevant_arrays)
            r = recall_score(real_arrays, relevant_arrays)
            p = precision_score(real_arrays, relevant_arrays)

            return {'mae': mae, 'nmae': nmae, 'rmse': rmse,
                    'precision': p, 'recall': r, 'f1score': f}

    def evaluate_on_split(self, recommender, metric=None, cv=None, **kwargs):
        """
        Evaluate on the folds of a dataset split

        Parameters
        ----------
        recommender: The BaseRecommender instance
                The recommender instance to be evaluated.

        metric: [None|'rmse'|'f1score'|'precision'|'recall'|'nmae'|'mae']
            If metrics is None, all metrics available will be evaluated.
        Otherwise it will return the specified metric evaluated.

        sampling_users:  float or sampling, optional, default = None
            If an float is passed, it is the percentage of evaluated
        users. If sampling_users is None, all users are used in the
        evaluation. Specific sampling objects can be passed, see
        scikits.crab.metrics.sampling module for the list of possible
        objects.

        cv: integer or crossvalidation, optional, default = None
            If an integer is passed, it is the number of fold (default 3).
            Specific sampling objects can be passed, see
            scikits.crab.metrics.cross_validation module for the list of
            possible objects.

        at: integer, optional, default = None
            This number at is the 'at' value, as in 'precision at 5'.  For
        example this would mean precision or recall evaluated by removing
        the top 5 preferences for a user and then finding the percentage of
        those 5 items included in the top 5 recommendations for that user.
        If at is None, it will consider all the top 3 elements.

        Returns
        -------
        score: dict
            a dictionary containing the average results over
            the different permutations on the split.

        permutation_scores : array, shape = [n_permutations]
            The scores obtained for each permutations.

        """
        sampling_users = kwargs.pop('sampling_users', 0.7)
        permutation = kwargs.pop('permutation', True)
        at = kwargs.pop('at', 3)

        if metric not in evaluation_metrics and metric is not None:
            raise ValueError('metric %s is not recognized. valid keywords \
              are %s' % (metric, evaluation_metrics.keys()))

        permutation_scores_error = []
        permutation_scores_ir = []
        final_score_error = {'avg': {}, 'stdev': {}}
        final_score_ir = {'avg': {}, 'stdev': {}}

        n_users = recommender.model.users_count()
        sampling_users = check_sampling(sampling_users, n_users)
        users_set, _ = sampling_users.split(permutation=permutation)

        total_ratings = []
        #Select the users to be evaluated.
        user_ids = recommender.model.user_ids()
        for user_id in user_ids[users_set]:
            #Select the ratings to be evaluated.
            preferences = recommender.model.preferences_from_user(user_id)
            preferences = list(preferences)
            total_ratings.extend([(user_id, preference)
                                 for preference in preferences])

        n_ratings = len(total_ratings)
        cross_val = check_cv(cv, n_ratings)
        #Defining the splits and run on the splits.
        for train_set, test_set in cross_val:

            training_set = {}
            testing_set = {}

            for idx in train_set:
                user_id, pref = total_ratings[idx]
                if recommender.model.has_preference_values():
                    training_set.setdefault(user_id, {})
                    training_set[user_id][pref[0]] = pref[1]
                else:
                    training_set.setdefault(user_id, {})
                    training_set[user_id][pref] = 1.0

            for idx in test_set:
                user_id, pref = total_ratings[idx]
                if recommender.model.has_preference_values():
                    testing_set.setdefault(user_id, [])
                    testing_set[user_id].append(pref)
                else:
                    testing_set.setdefault(user_id, [])
                    testing_set[user_id].append((pref, 1.0))

            #Evaluate the recommender.
            recommender_training = self._build_recommender(training_set, \
                                    recommender)

            real_preferences = []
            estimated_preferences = []

            for user_id, preferences in testing_set.iteritems():
                for item_id, preference in preferences:
                    #Estimate the preferences
                    try:
                        estimated = recommender_training.estimate_preference(
                                    user_id, item_id)
                        real_preferences.append(preference)
                    except:
                        # It is possible that an item exists
                        #in the test data but
                        # not training data in which case
                        #an exception will be
                        # throw. Just ignore it and move on
                        continue
                    estimated_preferences.append(estimated)

            #Return the error results.
            if metric in ['rmse', 'mae', 'nmae']:
                eval_function = evaluation_metrics[metric]
                if metric == 'nmae':
                    permutation_scores_error.append({
                                metric: eval_function(real_preferences,
                                                 estimated_preferences,
                                recommender.model.maximum_preference_value(),
                                recommender.model.minimum_preference_value())})
                else:
                    permutation_scores_error.append(
                    {metric: eval_function(real_preferences,
                                       estimated_preferences)})
            elif metric is None:
                #Return all
                mae, nmae, rmse = evaluation_error(real_preferences,
                        estimated_preferences,
                        recommender.model.maximum_preference_value(),
                        recommender.model.minimum_preference_value())
                permutation_scores_error.append({'mae': mae, 'nmae': nmae,
                                                  'rmse': rmse})

        #IR_Statistics (Precision, Recall and F1-Score)
        n_users = recommender.model.users_count()
        cross_val = check_cv(cv, n_users)

        for train_idx, test_idx in cross_val:
            relevant_arrays = []
            real_arrays = []
            for user_id in user_ids[train_idx]:
                preferences = recommender.model.preferences_from_user(user_id)
                preferences = list(preferences)
                if len(preferences) < 2 * at:
                    # Really not enough prefs to meaningfully evaluate the user
                    continue

                # List some most-preferred items that would count as most
                if not recommender.model.has_preference_values():
                    preferences = [(preference, 1.0) for preference in preferences]

                preferences = sorted(preferences, key=lambda x: x[1], reverse=True)
                relevant_item_ids = [item_id for item_id, preference
                                        in preferences[:at]]

                if len(relevant_item_ids) == 0:
                    continue

                #Build the training set.
                training_set = {}
                for other_user_id in recommender.model.user_ids():
                    preferences_other_user = \
                        recommender.model.preferences_from_user(other_user_id)

                    if not recommender.model.has_preference_values():
                        preferences_other_user = [(preference, 1.0)
                                         for preference in preferences_other_user]
                    if other_user_id == user_id:
                        preferences_other_user = \
                            [pref for pref in preferences_other_user \
                                if pref[0] not in relevant_item_ids]

                        if preferences_other_user:
                            training_set[other_user_id] = \
                                dict(preferences_other_user)
                    else:
                        training_set[other_user_id] = dict(preferences_other_user)

                #Evaluate the recommender
                recommender_training = self._build_recommender(training_set, \
                            recommender)

                try:
                    preferences = \
                        recommender_training.model.preferences_from_user(user_id)
                    preferences = list(preferences)
                    if not preferences:
                        continue
                except:
                    #Excluded all prefs for the user. move on.
                    continue

                recommended_items = recommender_training.recommend(user_id, at)
                relevant_arrays.append(list(relevant_item_ids))
                real_arrays.append(list(recommended_items))

            relevant_arrays = np.array(relevant_arrays)
            real_arrays = np.array(real_arrays)

            #Return the IR results.
            if metric in ['precision', 'recall', 'f1score']:
                eval_function = evaluation_metrics[metric]
                permutation_scores_ir.append({metric: eval_function(real_arrays,
                                          relevant_arrays)})
            elif metric is None:
                f = f1_score(real_arrays, relevant_arrays)
                r = recall_score(real_arrays, relevant_arrays)
                p = precision_score(real_arrays, relevant_arrays)
                permutation_scores_ir.append({'precision': p, 'recall': r, 'f1score': f})

        #Compute the final score for Error Statistics
        for result in permutation_scores_error:
            for key in result:
                final_score_error['avg'].setdefault(key, [])
                final_score_error['avg'][key].append(result[key])
        for key in final_score_error['avg']:
            final_score_error['stdev'][key] = np.std(final_score_error['avg'][key])
            final_score_error['avg'][key] = np.average(final_score_error['avg'][key])

        #Compute the final score for IR statistics
        for result in permutation_scores_ir:
            for key in result:
                final_score_ir['avg'].setdefault(key, [])
                final_score_ir['avg'][key].append(result[key])
        for key in final_score_ir['avg']:
            final_score_ir['stdev'][key] = np.std(final_score_ir['avg'][key])
            final_score_ir['avg'][key] = np.average(final_score_ir['avg'][key])

        permutation_scores = {}
        scores = {}
        if permutation_scores_error:
            permutation_scores['error'] = permutation_scores_error
            scores['final_error'] = final_score_error
        if permutation_scores_ir:
            permutation_scores['ir'] = permutation_scores_ir
            scores.setdefault('final_error', {})
            scores['final_error'].setdefault('avg', {})
            scores['final_error'].setdefault('stdev', {})
            scores['final_error']['avg'].update(final_score_ir['avg'])
            scores['final_error']['stdev'].update(final_score_ir['stdev'])

        return permutation_scores, scores

########NEW FILE########
__FILENAME__ = cross_validation
"""Utilities for cross validation and performance evaluation"""

# Author: Marcel Caraciolo <marcel@muricoca.com>
# License: BSD Style.

import numpy as np
from ..utils.extmath import factorial, combinations
from ..utils import check_random_state
from math import ceil


class LeaveOneOut(object):
    """Leave-One-Out cross validation iterator.

    Provides train/test indices to split user preferences in train
    and test sets. Each sample is used once as a test set (singleton)
    while the remaining samples form the training set.

    Due to the high number of test sets (which is the same as the
    number of samples) this cross validation method can be very costly.
    For large datasets one should favor KFold or ShuffleSplit.

    Parameters
    ==========
    n: int
        Total number of user preferences

    indices: boolean, optional (default False)
        Return train/test split with integer indices or boolean mask.
        Integer indices are useful when dealing with sparse matrices
        that cannot be indexed by boolean masks.

    Examples
    ========
    >>> from scikits.crab.metrics import LeaveOneOut
    >>> X = np.array(['userA', 'userB', 'userC'])
    >>> loo = LeaveOneOut(3)
    >>> len(loo)
    3
    >>> print loo
    scikits.crab.metrics.cross_validation.LeaveOneOut(n=3)
    >>> for train_index, test_index in loo:
    ...    print "TRAIN:", train_index, "TEST:", test_index
    ...    X_train, X_test = X[train_index], X[test_index]
    ...    print X_train, X_test
    TRAIN: [False  True  True] TEST: [ True False False]
    ['userB' 'userC'] ['userA']
    TRAIN: [ True False  True] TEST: [False  True False]
    ['userA' 'userC'] ['userB']
    TRAIN: [ True  True False] TEST: [False False  True]
    ['userA' 'userB'] ['userC']

    """
    def __init__(self, n, indices=False):
        self.n = n
        self.indices = indices

    def __iter__(self):
        n = self.n
        for i in xrange(n):
            test_index = np.zeros(n, dtype=np.bool)
            test_index[i] = True
            train_index = np.logical_not(test_index)
            if self.indices:
                ind = np.arange(n)
                train_index = ind[train_index]
                test_index = ind[test_index]
            yield train_index, test_index

    def __repr__(self):
        return '%s.%s(n=%i)' % (
            self.__class__.__module__,
            self.__class__.__name__,
            self.n,
        )

    def __len__(self):
        return self.n


class LeavePOut(object):
    """Leave-P-Out cross validation iterator

    Provides train/test indices to split user preferences in train test sets.
    The test set is built using p samples while the remaining samples form
    the training set.

    Due to the high number of iterations which grows with the number of
    samples this cross validation method can be very costly. For large
    datasets one should favor KFold or ShuffleSplit.

    Parameters
    ===========
    n: int
        Total number of user_profiles

    p: int
        Size of the test sets

    indices: boolean, optional (default False)
        Return train/test split with integer indices or boolean mask.
        Integer indices are useful when dealing with sparse matrices
        that cannot be indexed by boolean masks.

    Examples
    ========
    >>> from scikits.crab.metrics import LeavePOut
    >>> X = np.array(['userA', 'userB', 'userC'])
    >>> lpo = LeavePOut(3, 2)
    >>> len(lpo)
    3
    >>> print lpo
    scikits.crab.metrics.cross_validation.LeavePOut(n=3, p=2)
    >>> for train_index, test_index in lpo:
    ...    print "TRAIN:", train_index, "TEST:", test_index
    ...    X_train, X_test = X[train_index], X[test_index]
    TRAIN: [False False  True] TEST: [ True  True False]
    TRAIN: [False  True False] TEST: [ True False  True]
    TRAIN: [ True False False] TEST: [False  True  True]

    """
    def __init__(self, n, p, indices=False):
        self.n = n
        self.p = p
        self.indices = indices

    def __iter__(self):
        n = self.n
        p = self.p
        comb = combinations(range(n), p)
        for idx in comb:
            test_index = np.zeros(n, dtype=np.bool)
            test_index[np.array(idx)] = True
            train_index = np.logical_not(test_index)
            if self.indices:
                ind = np.arange(n)
                train_index = ind[train_index]
                test_index = ind[test_index]
            yield train_index, test_index

    def __repr__(self):
        return '%s.%s(n=%i, p=%i)' % (
            self.__class__.__module__,
            self.__class__.__name__,
            self.n,
            self.p,
        )

    def __len__(self):
        return (factorial(self.n) / factorial(self.n - self.p)
                / factorial(self.p))


class KFold(object):
    """K-Folds cross validation iterator

    Provides train/test indices to split user preferences in train test sets.
    Split dataset into k consecutive folds (without shuffling).

    Each fold is then used a validation set once while the k - 1 remaining
    fold form the training set.

    Parameters
    ----------
    n: int
        Total number of user preferences

    k: int
        Number of folds

    indices: boolean, optional (default False)
        Return train/test split with integer indices or boolean mask.
        Integer indices are useful when dealing with sparse matrices
        that cannot be indexed by boolean masks.

    Examples
    --------
    >>> from scikits.crab.metrics import KFold
    >>> X = np.array(['userA', 'userB', 'userC', 'userD'])
    >>> kf = KFold(4, k=2)
    >>> len(kf)
    2
    >>> print kf
    scikits.crab.metrics.cross_validation.KFold(n=4, k=2)
    >>> for train_index, test_index in kf:
    ...    print "TRAIN:", train_index, "TEST:", test_index
    ...    X_train, X_test = X[train_index], X[test_index]
    TRAIN: [False False  True  True] TEST: [ True  True False False]
    TRAIN: [ True  True False False] TEST: [False False  True  True]

    Notes
    -----
    All the folds have size trunc(n_samples / n_folds), the last one has the
    complementary.

    """
    def __init__(self, n, k, indices=False):
        assert k > 0, ValueError('Cannot have number of folds k below 1.')
        assert k <= n, ValueError('Cannot have number of folds k=%d, '
                                  'greater than the number '
                                  'of samples: %d.' % (k, n))
        self.n = n
        self.k = k
        self.indices = indices

    def __iter__(self):
        n = self.n
        k = self.k
        j = ceil(n / k)

        for i in xrange(k):
            test_index = np.zeros(n, dtype=np.bool)
            if i < k - 1:
                test_index[i * j:(i + 1) * j] = True
            else:
                test_index[i * j:] = True
            train_index = np.logical_not(test_index)
            if self.indices:
                ind = np.arange(n)
                train_index = ind[train_index]
                test_index = ind[test_index]
            yield train_index, test_index

    def __repr__(self):
        return '%s.%s(n=%i, k=%i)' % (
            self.__class__.__module__,
            self.__class__.__name__,
            self.n,
            self.k,
        )

    def __len__(self):
        return self.k


class ShuffleSplit(object):
    """Random permutation cross-validation iterator.

    Yields indices to split user preferences into training and test sets.

    Note: contrary to other cross-validation strategies, random splits
    do not guarantee that all folds will be different, although this is
    still very likely for sizeable datasets.

    Parameters
    ----------
    n : int
        Total number of elements in the dataset.

    n_iterations : int (default 10)
        Number of re-shuffling & splitting iterations.

    test_fraction : float (default 0.1)
        Should be between 0.0 and 1.0 and represent the proportion of
        the dataset to include in the test split.

    indices : boolean, optional (default False)
        Return train/test split with integer indices or boolean mask.
        Integer indices are useful when dealing with sparse matrices
        that cannot be indexed by boolean masks.

    random_state : int or RandomState
        Pseudo-random number generator state used for random sampling.

    Examples
    ----------
    >>> from scikits.crab.metrics import ShuffleSplit
    >>> rs = ShuffleSplit(4, n_iterations=3, test_fraction=.25,
    ...                             random_state=0)
    >>> len(rs)
    3
    >>> print rs
    ... # doctest: +ELLIPSIS
    ShuffleSplit(4, n_iterations=3, test_fraction=0.25, indices=False, ...)
    >>> for train_index, test_index in rs:
    ...    print "TRAIN:", train_index, "TEST:", test_index
    ...
    TRAIN: [False  True  True  True] TEST: [ True False False False]
    TRAIN: [ True  True  True False] TEST: [False False False  True]
    TRAIN: [ True False  True  True] TEST: [False  True False False]
    """
    def __init__(self, n, n_iterations=10, test_fraction=0.1,
                indices=False, random_state=None):
        self.n = n
        self.n_iterations = n_iterations
        self.test_fraction = test_fraction
        self.random_state = random_state
        self.indices = indices

    def __iter__(self):
        rng = self.random_state = check_random_state(self.random_state)
        n_test = ceil(self.test_fraction * self.n)
        for i in range(self.n_iterations):
            #random partition
            permutation = rng.permutation(self.n)
            ind_train = permutation[:-n_test]
            ind_test = permutation[-n_test:]
            if self.indices:
                yield ind_train, ind_test
            else:
                train_mask = np.zeros(self.n, dtype=np.bool)
                train_mask[ind_train] = True
                test_mask = np.zeros(self.n, dtype=np.bool)
                test_mask[ind_test] = True
                yield train_mask, test_mask

    def __repr__(self):
        return ('%s(%d, n_iterations=%d, test_fraction=%s, indices=%s, '
                'random_state=%d)' % (
                    self.__class__.__name__,
                    self.n,
                    self.n_iterations,
                    str(self.test_fraction),
                    self.indices,
                    self.random_state,
                ))

    def __len__(self):
        return self.n_iterations

########NEW FILE########
__FILENAME__ = metrics
#-*- coding:utf-8 -*-

"""
This module contains basic implementations that encapsulate
    retrieval-related statistics about the quality of the recommender's
    recommendations.
"""

# Authors: Marcel Caraciolo <marcel@muricoca.com>

# License: BSD Style.

import numpy as np
from ..utils import check_arrays, unique_labels


def root_mean_square_error(y_real, y_pred):
    """
    It computes the root mean squared difference (RMSE)
    between predicted and actual ratings for users.

    Parameters
    ----------
    y_real : array-like

    y_pred : array-like

    Returns
    -------

    Positive floating point value: the best value is 0.0.

    return the mean square error

    """
    y_real, y_pred = check_arrays(y_real, y_pred)

    return np.sqrt((np.sum((y_pred - y_real) ** 2)) / y_real.shape[0])


def mean_absolute_error(y_real, y_pred):
    """
    It computes the average absolute difference (MAE)
    between predicted and actual ratings for users.

    Parameters
    ----------
    y_real : array-like

    y_pred : array-like

    Returns
    -------

    Positive floating point value: the best value is 0.0.

    return the mean absolute error


    """
    y_real, y_pred = check_arrays(y_real, y_pred)

    return np.sum(np.abs(y_pred - y_real)) / y_real.size


def normalized_mean_absolute_error(y_real, y_pred, max_rating, min_rating):
    """
    It computes the normalized average absolute difference (NMAE)
    between predicted and actual ratings for users.

    Parameters
    ----------
    y_real : array-like
        The real ratings.

    y_pred : array-like
        The predicted ratings.

    max_rating:
        The maximum rating of the model.

    min_rating:
        The minimum rating of the model.

    Returns
    -------

    Positive floating point value: the best value is 0.0.

    return the normalized mean absolute error


    """
    y_real, y_pred = check_arrays(y_real, y_pred)
    mae = mean_absolute_error(y_real, y_pred)
    return mae / (max_rating - min_rating)


def evaluation_error(y_real, y_pred, max_rating, min_rating):
    """
    It computes the NMAE, MAE and RMSE between predicted
    and actual ratings for users.

    Parameters
    ----------
    y_real : array-like
        The real ratings.

    y_pred : array-like
        The predicted ratings.

    max_rating:
        The maximum rating of the model.

    min_rating:
        The minimum rating of the model.

    Returns
    -------
    mae: Positive floating point value: the best value is 0.0.
    nmae: Positive floating point value: the best value is 0.0.
    rmse: Positive floating point value: the best value is 0.0.

    """
    mae = mean_absolute_error(y_real, y_pred)
    nmae = normalized_mean_absolute_error(y_real, y_pred,
             max_rating, min_rating)
    rmse = root_mean_square_error(y_real, y_pred)

    return mae, nmae, rmse


def precision_score(y_real, y_pred):
    """Compute the precision

    The precision is the ratio :math:`tp / (tp + fp)` where tp is the
    number of true positives and fp the number of false positives.
    In recommendation systems the precision is the proportion of
     recommendations that are good recommendations.

    The best value is 1 and the worst value is 0.

    Parameters
    ----------
    y_real : array, shape = [n_samples]
        true targets

    y_pred : array, shape = [n_samples]
        predicted targets

    Returns
    -------
    precision : float

    """
    p, _, _ = precision_recall_fscore(y_real, y_pred)
    return np.average(p)


def recall_score(y_real, y_pred):
    """Compute the recall

    The recall is the ratio :math:`tp / (tp + fn)` where tp is the number of
    true positives and fn the number of false negatives.
    In recommendation systems the recall  is the proportion of good
    recommendations that appear in top recommendations.

    The best value is 1 and the worst value is 0.

    Parameters
    ----------
    y_real : array, shape = [n_samples]
        true targets

    y_pred : array, shape = [n_samples]
        predicted targets

    Returns
    -------
    recall : float
        ...
    """
    _, r, _ = precision_recall_fscore(y_real, y_pred)
    return np.average(r)


def f1_score(y_real, y_pred):
    """Compute f1 score

    The F1 score can be interpreted as a weighted average of the precision
    and recall, where an F1 score reaches its best value at 1 and worst
    score at 0. The relative contribution of precision and recall to the f1
    score are equal.

        F_1 = 2 * (precision * recall) / (precision + recall)

    See: http://en.wikipedia.org/wiki/F1_score

    In the recommender systems the F1-Score is considered an single value
    obtained combining both the precision and recall measures and
    indicates an overall utility of the recommendation list.


    Parameters
    ----------
    y_real : array, shape = [n_samples]
        true targets

    y_pred : array, shape = [n_samples]
        predicted targets

    Returns
    -------
    f1_score : float
        f1_score of ...

    References
    ----------
    http://en.wikipedia.org/wiki/F1_score

    """
    return fbeta_score(y_real, y_pred, 1)


def fbeta_score(y_real, y_pred, beta):
    """Compute fbeta score

    The F_beta score is the weighted harmonic mean of precision and recall,
    reaching its optimal value at 1 and its worst value at 0.

    Parameters
    ----------
    y_real : array, shape = [n_samples]
        true targets

    y_pred : array, shape = [n_samples]
        predicted targets

    beta: float
        The beta parameter determines the weight of precision in the combined
        score. beta < 1 lends more weight to precision, while beta > 1 favors
        precision (beta == 0 considers only precision, beta == inf only
        recall).

    Returns
    -------
    fbeta_score : float
        fbeta_score of ...

    See also
    --------
    R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern Information Retrieval.
    Addison Wesley, pp. 327-328.

    http://en.wikipedia.org/wiki/F1_score

    """
    _, _, f = precision_recall_fscore(y_real, y_pred, beta=beta)

    return np.average(f)


def precision_recall_fscore(y_real, y_pred, beta=1.0):
    """Compute precisions, recalls, f-measures
       for recommender systems


    The precision is the ratio :math:`tp / (tp + fp)` where tp is the number of
    true positives and fp the number of false positives. In recommender systems
    ...

    The recall is the ratio :math:`tp / (tp + fn)` where tp is the number of
    true positives and fn the number of false negatives. In recommender
    systems...

    The F_beta score can be interpreted as a weighted harmonic mean of
    the precision and recall, where an F_beta score reaches its best
    value at 1 and worst score at 0.

    The F_beta score weights recall beta as much as precision. beta = 1.0 means
    recall and precision are as important.

    Parameters
    ----------
    y_real : array, shape = [n_samples]
        true recommended items

    y_pred : array, shape = [n_samples]
        predicted recommended items

    beta : float, 1.0 by default
        the strength of recall versus precision in the f-score

    Returns
    -------
    precision: array, shape = [n_unique_labels], dtype = np.double
    recall: array, shape = [n_unique_labels], dtype = np.double
    f1_score: array, shape = [n_unique_labels], dtype = np.double

    References
    ----------
    http://en.wikipedia.org/wiki/Precision_and_recall

    """
    y_real, y_pred = check_arrays(y_real, y_pred)
    assert(beta > 0)

    n_users = y_real.shape[0]
    precision = np.zeros(n_users, dtype=np.double)
    recall = np.zeros(n_users, dtype=np.double)
    fscore = np.zeros(n_users, dtype=np.double)

    try:
        # oddly, we may get an "invalid" rather than a "divide" error here
        old_err_settings = np.seterr(divide='ignore', invalid='ignore')

        for i, y_items_pred in enumerate(y_pred):
            intersection_size = np.intersect1d(y_items_pred, y_real[i]).size
            precision[i] = (intersection_size / float(len(y_real[i]))) \
                                    if len(y_real[i])  else 0.0
            recall[i] = (intersection_size / float(len(y_items_pred))) \
                                    if len(y_items_pred) else 0.0

        # handle division by 0.0 in precision and recall
        precision[np.isnan(precision)] = 0.0
        recall[np.isnan(precision)] = 0.0

        #fbeta Score
        beta2 = beta ** 2
        fscore = (1 + beta2) * (precision * recall) \
                    / (beta2 * precision + recall)

        #handle division by 0.0 in fscore
        fscore[(precision + recall) == 0.0] = 0.0

    finally:
        np.seterr(**old_err_settings)

    return precision, recall, fscore


def evaluation_report(y_real, y_pred, labels=None, target_names=None):
    """Build a text report showing the main recommender metrics

    Parameters
    ----------
    y_real : array, shape = [n_samples]
        true targets

    y_pred : array, shape = [n_samples]
        estimated targets

    labels : array, shape = [n_labels]
        optional list of label indices to include in the report

    target_names : list of strings
        optional display names matching the labels (same order)

    Returns
    -------
    report : string
        Text summary of the precision, recall, f1-score.

    """

    if labels is None:
        labels = unique_labels(y_real)
    else:
        labels = np.asarray(labels, dtype=np.int)

    last_line_heading = 'avg / total'

    if target_names is None:
        width = len(last_line_heading)
        target_names = ['%d' % l for l in labels]
    else:
        width = max(len(cn) for cn in target_names)
        width = max(width, len(last_line_heading))

    headers = ["precision", "recall", "f1-score"]
    fmt = '%% %ds' % width  # first column: class name
    fmt += '  '
    fmt += ' '.join(['% 9s' for _ in headers])
    fmt += '\n'

    headers = [""] + headers
    report = fmt % tuple(headers)
    report += '\n'
    p, r, f1 = precision_recall_fscore(y_real, y_pred)
    for i, label in enumerate(labels):
        values = [target_names[i]]
        for v in (p[i], r[i], f1[i]):
            values += ["%0.2f" % float(v)]
        report += fmt % tuple(values)

    report += '\n'

    # compute averages
    values = [last_line_heading]
    for v in (np.average(p),
              np.average(r),
              np.average(f1)):
        values += ["%0.2f" % float(v)]
    report += fmt % tuple(values)
    return report

########NEW FILE########
__FILENAME__ = pairwise
#-*- coding:utf-8 -*-

"""Utilities to evaluate pairwise distances or metrics between 2
sets of points.

"""

# Authors: Marcel Caraciolo <marcel@muricoca.com>
#          Bruno Melo <bruno@muricoca.com>
# License: BSD Style.

import numpy as np
import scipy.spatial.distance as ssd


def euclidean_distances(X, Y, squared=False, inverse=True):
    """
    Considering the rows of X (and Y=X) as vectors, compute the
    distance matrix between each pair of vectors.

    An implementation of a "similarity" based on the Euclidean "distance"
    between two vectors X and Y. Thinking of items as dimensions and
    preferences as points along those dimensions, a distance is computed using
    all items (dimensions) where both users have expressed a preference for
    that item. This is simply the square root of the sum of the squares of
    differences in position (preference) along each dimension.

    Parameters
    ----------
    X: array of shape (n_samples_1, n_features)

    Y: array of shape (n_samples_2, n_features)

    squared: boolean, optional
        This routine will return squared Euclidean distances instead.

    inverse: boolean, optional
        This routine will return the inverse Euclidean distances instead.

    Returns
    -------
    distances: array of shape (n_samples_1, n_samples_2)

    Examples
    --------
    >>> from scikits.crab.metrics.pairwise import euclidean_distances
    >>> X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0],[3.0, 3.5, 1.5, 5.0, 3.5,3.0]]
    >>> # distrance between rows of X
    >>> euclidean_distances(X, X)
    array([[ 1.        ,  0.29429806],
           [ 0.29429806,  1.        ]])
    >>> # get distance to origin
    >>> X = [[1.0, 0.0],[1.0,1.0]]
    >>> euclidean_distances(X, [[0.0, 0.0]])
    array([[ 0.5       ],
          [ 0.41421356]])

    """
    # should not need X_norm_squared because if you could precompute that as
    # well as Y, then you should just pre-compute the output and not even
    # call this function.
    if X is Y:
        X = Y = np.asanyarray(X)
    else:
        X = np.asanyarray(X)
        Y = np.asanyarray(Y)

    if X.shape[1] != Y.shape[1]:
        raise ValueError("Incompatible dimension for X and Y matrices")

    if squared:
        return ssd.cdist(X, Y, 'sqeuclidean')

    XY = ssd.cdist(X, Y)
    return  np.divide(1.0, (1.0 + XY)) if inverse else XY

euclidian_distances = euclidean_distances  # both spelling for backward compat


def pearson_correlation(X, Y):
    """
    Considering the rows of X (and Y=X) as vectors, compute the
    distance matrix between each pair of vectors.

    This correlation implementation is equivalent to the cosine similarity
    since the data it receives is assumed to be centered -- mean is 0. The
    correlation may be interpreted as the cosine of the angle between the two
    vectors defined by the users' preference values.

    Parameters
    ----------
    X: array of shape (n_samples_1, n_features)

    Y: array of shape (n_samples_2, n_features)

    Returns
    -------
    distances: array of shape (n_samples_1, n_samples_2)

    Examples
    --------
    >>> from scikits.crab.metrics.pairwise import pearson_correlation
    >>> X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0],[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    >>> # distance between rows of X
    >>> pearson_correlation(X, X)
    array([[ 1., 1.],
           [ 1., 1.]])
    >>> pearson_correlation(X, [[3.0, 3.5, 1.5, 5.0, 3.5,3.0]])
    array([[ 0.39605902],
               [ 0.39605902]])
    """
    # should not need X_norm_squared because if you could precompute that as
    # well as Y, then you should just pre-compute the output and not even
    # call this function.
    if X is Y:
        X = Y = np.asanyarray(X)
    else:
        X = np.asanyarray(X)
        Y = np.asanyarray(Y)

    if X.shape[1] != Y.shape[1]:
        raise ValueError("Incompatible dimension for X and Y matrices")

    XY = ssd.cdist(X, Y, 'correlation', 2)

    return 1 - XY


def jaccard_coefficient(X, Y):
    """
    Considering the rows of X (and Y=X) as vectors, compute the
    distance matrix between each pair of vectors.

    This correlation implementation is a statistic used for comparing the
    similarity and diversity of sample sets.
    The Jaccard coefficient measures similarity between sample sets,
    and is defined as the size of the intersection divided by the size of the
    union of the sample sets.

    Parameters
    ----------
    X: array of shape (n_samples_1, n_features)

    Y: array of shape (n_samples_2, n_features)

    Returns
    -------
    distances: array of shape (n_samples_1, n_samples_2)

    Examples
    --------
    >>> from scikits.crab.metrics.pairwise import jaccard_coefficient
    >>> X = [['a', 'b', 'c', 'd'],['e', 'f','g']]
    >>> # distance between rows of X
    >>> jaccard_coefficient(X, X)
    array([[ 1.,  0.],
           [ 0.,  1.]])

    >>> jaccard_coefficient(X, [['a', 'b', 'c', 'k']])
    array([[ 0.6],
           [ 0. ]])
    """
    # should not need X_norm_squared because if you could precompute that as
    # well as Y, then you should just pre-compute the output and not even
    # call this function.
    if X is Y:
        X = Y = np.asanyarray(X)
    else:
        X = np.asanyarray(X)
        Y = np.asanyarray(Y)

    #TODO: Check if it is possible to optimize this function
    result = []
    i = 0
    for arrayX in X:
        result.append([])
        for arrayY in Y:
            n_XY = np.intersect1d(arrayY, arrayX).size
            result[i].append(n_XY / (float(len(arrayX)) + len(arrayY) - n_XY))
        result[i] = np.array(result[i])
        i += 1

    #XY = np.array([ [np.intersect1d(y,x).size / (float(len(x)) + len(y) - np.intersect1d(y,x).size)]  for y in Y  for x in X]) 
    return np.array(result)


def manhattan_distances(X, Y):
    """
    Considering the rows of X (and Y=X) as vectors, compute the
    distance matrix between each pair of vectors.

    This distance implementation is the distance between two points in a grid
    based on a strictly horizontal and/or vertical path (that is, along the
    grid lines as opposed to the diagonal or "as the crow flies" distance.
    The Manhattan distance is the simple sum of the horizontal and vertical
    components, whereas the diagonal distance might be computed by applying the
    Pythagorean theorem.

    Parameters
    ----------
    X: array of shape (n_samples_1, n_features)

    Y: array of shape (n_samples_2, n_features)

    Returns
    -------
    distances: array of shape (n_samples_1, n_samples_2)

    Examples
    --------
    >>> from scikits.crab.metrics.pairwise  import manhattan_distances
    >>> X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0],[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    >>> # distance between rows of X
    >>> manhattan_distances(X, X)
    array([[ 1.,  1.],
           [ 1.,  1.]])
    >>> manhattan_distances(X, [[3.0, 3.5, 1.5, 5.0, 3.5,3.0]])
    array([[ 0.25],
          [ 0.25]])
    """
    # should not need X_norm_squared because if you could precompute that as
    # well as Y, then you should just pre-compute the output and not even
    # call this function.
    if X is Y:
        X = Y = np.asanyarray(X)
    else:
        X = np.asanyarray(X)
        Y = np.asanyarray(Y)

    if X.shape[1] != Y.shape[1]:
        raise ValueError("Incompatible dimension for X and Y matrices")

    XY = ssd.cdist(X, Y, 'cityblock')

    return 1.0 - (XY / float(X.shape[1]))


def sorensen_coefficient(X, Y):
    """
    Considering the rows of X (and Y=X) as vectors, compute the
    distance matrix between each pair of vectors.

    The Srensen index, also known as Srensens similarity coefficient,
    is a statistic used for comparing the similarity of two samples.
    It was developed by the botanist Thorvald Srensen and published in 1948.
    [1]
    See the link:http://en.wikipedia.org/wiki/S%C3%B8rensen_similarity_index

    This is intended for "binary" data sets where a user either expresses a
    generic "yes" preference for an item or has no preference. The actual
    preference values do not matter here, only their presence or absence.

    Parameters
    ----------
    X: array of shape (n_samples_1, n_features)

    Y: array of shape (n_samples_2, n_features)

    Returns
    -------
    distances: array of shape (n_samples_1, n_samples_2)

    Examples
    --------
    >>> from scikits.crab.metrics.pairwise import sorensen_coefficient
    >>> X = [['a', 'b', 'c', 'd'],['e', 'f','g']]
    >>> # distance between rows of X
    >>> sorensen_coefficient(X, X)
    array([[ 1.,  0.],
          [ 0.,  1.]])
    >>> sorensen_coefficient(X, [['a', 'b', 'c', 'k']])
    array([[ 0.75], [ 0.  ]])

    """
    # should not need X_norm_squared because if you could precompute that as
    # well as Y, then you should just pre-compute the output and not even
    # call this function.
    if X is Y:
        X = Y = np.asanyarray(X)
    else:
        X = np.asanyarray(X)
        Y = np.asanyarray(Y)

    #TODO: Check if it is possible to optimize this function
    #XY = np.array([np.intersect1d(x,y).size for y in Y  for x in X])

    XY = []
    i = 0
    for arrayX in X:
        XY.append([])
        for arrayY in Y:
            XY[i].append(2 * np.intersect1d(arrayX, arrayY).size / float(len(arrayX) + len(arrayY)))

        XY[i] = np.array(XY[i])
        i += 1

    XY = np.array(XY)

    return XY


def tanimoto_coefficient(X, Y):
    """
    Considering the rows of X (and Y=X) as vectors, compute the
    distance matrix between each pair of vectors.

    An implementation of a "similarity" based on the Tanimoto coefficient,
    or extended Jaccard coefficient.

    This is intended for "binary" data sets where a user either expresses a
    generic "yes" preference for an item or has no preference. The actual
    preference values do not matter here, only their presence or absence.

    Parameters
    ----------
    X: array of shape n_samples_1

    Y: array of shape n_samples_2

    Returns
    -------
    distances: array of shape (n_samples_1, n_samples_2)

    Examples
    --------
    >>> from scikits.crab.metrics.pairwise  import tanimoto_coefficient
    >>> X =  [['a', 'b', 'c', 'd'],['e', 'f','g']]
    >>> # distance between rows of X
    >>> tanimoto_coefficient(X, X)
    array([[ 1.,  0.],
           [ 0.,  1.]])
    >>> tanimoto_coefficient(X, [['a', 'b', 'c', 'k']])
    array([[ 0.6],
           [ 0. ]])

    """
    # should not need X_norm_squared because if you could precompute that as
    # well as Y, then you should just pre-compute the output and not even
    # call this function.
    if X is Y:
        X = Y = np.asanyarray(X)
    else:
        X = np.asanyarray(X)
        Y = np.asanyarray(Y)

    #TODO: Check if it is possible to optimize this function
    result = []
    i = 0
    for arrayX in X:
        result.append([])
        for arrayY in Y:
            n_XY = np.intersect1d(arrayY, arrayX).size
            result[i].append(n_XY / (float(len(arrayX)) + len(arrayY) - n_XY))
        result[i] = np.array(result[i])
        i += 1

    #XY = np.array([ [np.intersect1d(y,x).size / (float(len(x)) + len(y) - np.intersect1d(y,x).size)]  for y in Y  for x in X]) 

    return np.array(result)


def cosine_distances(X, Y):
    """
    Considering the rows of X (and Y=X) as vectors, compute the
    distance matrix between each pair of vectors.

     An implementation of the cosine similarity. The result is the cosine of
     the angle formed between the two preference vectors.
     Note that this similarity does not "center" its data, shifts the user's
     preference values so that each of their means is 0. For this behavior,
     use Pearson Coefficient, which actually is mathematically
     equivalent for centered data.

    Parameters
    ----------
    X: array of shape (n_samples_1, n_features)

    Y: array of shape (n_samples_2, n_features)

    Returns
    -------
    distances: array of shape (n_samples_1, n_samples_2)

    Examples
    --------
    >>> from scikits.crab.metrics.pairwise  import cosine_distances
    >>> X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0],[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    >>> # distance between rows of X
    >>> cosine_distances(X, X)
    array([[ 1.,  1.],
          [ 1.,  1.]])
    >>> cosine_distances(X, [[3.0, 3.5, 1.5, 5.0, 3.5,3.0]])
    array([[ 0.9606463],
           [ 0.9606463]])

    """
    # should not need X_norm_squared because if you could precompute that as
    # well as Y, then you should just pre-compute the output and not even
    # call this function.
    if X is Y:
        X = Y = np.asanyarray(X)
    else:
        X = np.asanyarray(X)
        Y = np.asanyarray(Y)

    if X.shape[1] != Y.shape[1]:
        raise ValueError("Incompatible dimension for X and Y matrices")

    return 1. - ssd.cdist(X, Y, 'cosine')


def spearman_coefficient(X, Y):
    """
    Considering the rows of X (and Y=X) as vectors, compute the
    distance matrix between each pair of vectors.

    Like  Pearson Coefficient , but compares relative ranking of preference
    values instead of preference values themselves. That is, each user's
    preferences are sorted and then assign a rank as their preference value,
    with 1 being assigned to the least preferred item.

    Parameters
    ----------
    X: array of shape (n_samples_1, n_features)

    Y: array of shape (n_samples_2, n_features)

    Returns
    -------
    distances: array of shape (n_samples_1, n_samples_2)

    Examples
    --------
    >>> from scikits.crab.metrics.pairwise  import spearman_coefficient
    >>> X = [[('a',2.5),('b', 3.5), ('c',3.0), ('d',3.5)],[ ('e', 2.5),('f', 3.0), ('g', 2.5), ('h', 4.0)] ]
    >>> # distance between rows of X
    >>> spearman_coefficient(X, X)
    array([[ 1.,  0.],
           [ 0.,  1.]])
    >>> spearman_coefficient(X, [[('a',2.5),('b', 3.5), ('c',3.0), ('k',3.5)]])
    array([[ 1.],
           [ 0.]])
    """
    # should not need X_norm_squared because if you could precompute that as
    # well as Y, then you should just pre-compute the output and not even
    # call this function.
    if X is Y:
        X = Y = np.asanyarray(X, dtype=[('x', 'S30'), ('y', float)])
    else:
        X = np.asanyarray(X,  dtype=[('x', 'S30'), ('y', float)])
        Y = np.asanyarray(Y,  dtype=[('x', 'S30'), ('y', float)])

    if X.shape[1] != Y.shape[1]:
        raise ValueError("Incompatible dimension for X and Y matrices")

    X.sort(order='y')
    Y.sort(order='y')

    result = []

    #TODO: Check if it is possible to optimize this function
    i = 0
    for arrayX in X:
        result.append([])
        for arrayY in Y:
            Y_keys = [key for key, value in arrayY]

            XY = [(key, value) for key, value in arrayX if key in Y_keys]

            sumDiffSq = 0.0
            for index, tup in enumerate(XY):
                sumDiffSq += pow((index + 1) - (Y_keys.index(tup[0]) + 1), 2.0)

            n = len(XY)
            if n == 0:
                result[i].append(0.0)
            else:
                result[i].append(1.0 - ((6.0 * sumDiffSq) / (n * (n * n - 1))))
        result[i] = np.asanyarray(result[i])
        i += 1

    return np.asanyarray(result)


def loglikehood_coefficient(n_items, X, Y):
    """
    Considering the rows of X (and Y=X) as vectors, compute the
    distance matrix between each pair of vectors.

    Parameters
    ----------
    n_items: int
        Number of items in the model.

    X: array of shape (n_samples_1, n_features)

    Y: array of shape (n_samples_2, n_features)

    Returns
    -------
    distances: array of shape (n_samples_1, n_samples_2)

    Examples
    --------
    >>> from scikits.crab.metrics.pairwise import loglikehood_coefficient
    >>> X = [['a', 'b', 'c', 'd'],  ['e', 'f','g', 'h']]
    >>> # distance between rows of X
    >>> n_items = 7
    >>> loglikehood_coefficient(n_items,X, X)
    array([[ 1.,  0.],
          [ 0.,  1.]])
    >>> n_items = 8
    >>> loglikehood_coefficient(n_items, X, [['a', 'b', 'c', 'k']])
    array([[ 0.67668852],
          [ 0.        ]])


    References
    ----------
    See http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.5962 and
    http://tdunning.blogspot.com/2008/03/surprise-and-coincidence.html.
    """
    # should not need X_norm_squared because if you could precompute that as
    # well as Y, then you should just pre-compute the output and not even
    # call this function.

    def safeLog(d):
        if d <= 0.0:
            return 0.0
        else:
            return np.log(d)

    def logL(p, k, n):
        return k * safeLog(p) + (n - k) * safeLog(1.0 - p)

    def twoLogLambda(k1, k2, n1, n2):
        p = (k1 + k2) / (n1 + n2)
        return 2.0 * (logL(k1 / n1, k1, n1) + logL(k2 / n2, k2, n2)
                      - logL(p, k1, n1) - logL(p, k2, n2))

    if X is Y:
        X = Y = np.asanyarray(X)
    else:
        X = np.asanyarray(X)
        Y = np.asanyarray(Y)

    result = []

    # TODO: Check if it is possible to optimize this function

    i = 0
    for arrayX in X:
        result.append([])
        for arrayY in Y:
            XY = np.intersect1d(arrayX, arrayY)

            if XY.size == 0:
                result[i].append(0.0)
            else:
                nX = arrayX.size
                nY = arrayY.size
                if (nX - XY.size == 0)  or (n_items - nY) == 0:
                    result[i].append(1.0)
                else:
                    logLikelihood = twoLogLambda(float(XY.size),
                                                 float(nX - XY.size),
                                                 float(nY),
                                                 float(n_items - nY))

                    result[i].append(1.0 - 1.0 / (1.0 + float(logLikelihood)))
        result[i] = np.asanyarray(result[i])
        i += 1

    return np.asanyarray(result)

########NEW FILE########
__FILENAME__ = sampling
"""Utilities for sampling techniques"""

# Author: Marcel Caraciolo <marcel@muricoca.com>
# License: BSD Style.

import numpy as np
from ..utils import check_random_state
from math import ceil


class SplitSampling(object):
    """ Random Split Sampling the dataset into two sets.

    Parameters
    ----------
    n : int
        Total number of elements in the dataset.

    evaluation_fraction : float (default 0.7)
        Should be between 0.0 and 1.0 and represent the proportion of
        the dataset to include in the training set.

    indices : boolean, optional (default False)
        Return  split with integer indices or boolean mask.
        Integer indices are useful when dealing with sparse matrices
        that cannot be indexed by boolean masks.

    random_state : int or RandomState
        Pseudo-random number generator state used for random sampling.

    """
    def __init__(self, n, evaluation_fraction=0.7, indices=False,
            random_state=None):
        self.n = n
        self.evaluation_fraction = evaluation_fraction
        self.random_state = random_state
        self.indices = indices

    def split(self, evaluation_fraction=None, indices=False,
             random_state=None, permutation=True):
        """
        Random Split Sampling the dataset into two sets.

        Parameters
        ----------
        evaluation_fraction : float (default None)
            Should be between 0.0 and 1.0 and represent the proportion of
            the dataset to include in the training set. If evaluation_fraction
            is None, it will be used the one passed in the constructor.

        indices : boolean, optional (default False)
            Return  split with integer indices or boolean mask.
            Integer indices are useful when dealing with sparse matrices
            that cannot be indexed by boolean masks.

        random_state : int or RandomState
            Pseudo-random number generator state used for random sampling.

        permutation: boolean, optional (default True)
            For testing purposes, to deactivate the permutation.

        """
        if evaluation_fraction is not None:
            self.evaluation_fraction = evaluation_fraction
        if random_state is not None:
            self.random_state = random_state

        self.indices = indices

        rng = self.random_state = check_random_state(self.random_state)
        n_train = ceil(self.evaluation_fraction * self.n)
        #random partition
        permutation = rng.permutation(self.n) if permutation \
                             else np.arange(self.n)
        ind_train = permutation[-n_train:]
        ind_ignore = permutation[:-n_train]
        if self.indices:
            return ind_train, ind_ignore
        else:
            train_mask = np.zeros(self.n, dtype=np.bool)
            train_mask[ind_train] = True
            test_mask = np.zeros(self.n, dtype=np.bool)
            test_mask[ind_ignore] = True
            return train_mask, test_mask

    def __repr__(self):
        return ('%s(%d, evaluation_fraction=%s, indices=%s, '
                'random_state=%d)' % (
                    self.__class__.__name__,
                    self.n,
                    str(self.evaluation_fraction),
                    self.indices,
                    self.random_state,
                ))

########NEW FILE########
__FILENAME__ = test_classes
from nose.tools import assert_equals, assert_almost_equals, assert_raises, assert_true
from ...similarities.basic_similarities import UserSimilarity
from ...metrics.pairwise import  euclidean_distances, jaccard_coefficient
from ...models.classes import  MatrixPreferenceDataModel, \
     MatrixBooleanPrefDataModel
from ...recommenders.knn import  UserBasedRecommender
from ..classes import CfEvaluator
from ...recommenders.knn.neighborhood_strategies import  NearestNeighborsStrategy


movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,
     'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,
     'The Night Listener': 3.0},
    'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,
     'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,
     'You, Me and Dupree': 3.5},
    'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,
     'Superman Returns': 3.5, 'The Night Listener': 4.0},
    'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,
     'The Night Listener': 4.5, 'Superman Returns': 4.0,
     'You, Me and Dupree': 2.5},
    'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
     'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,
     'You, Me and Dupree': 2.0},
    'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
     'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},
    'Penny Frewman': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0,
      'Superman Returns': 4.0},
    'Maria Gabriela': {}}

model = MatrixPreferenceDataModel(movies)
boolean_model = MatrixBooleanPrefDataModel(movies)
similarity = UserSimilarity(model, euclidean_distances)
boolean_similarity = UserSimilarity(boolean_model, jaccard_coefficient)
neighborhood = NearestNeighborsStrategy()
recsys = UserBasedRecommender(model, similarity, neighborhood)
boolean_recsys = UserBasedRecommender(boolean_model, boolean_similarity, neighborhood)


def test_root_CfEvaluator_evaluate():
    """Check evaluate method in CfEvaluator """
    evaluator = CfEvaluator()

    #Test with invalid metric
    assert_raises(ValueError, evaluator.evaluate, recsys, 'rank')

    #Test with specified metric
    rmse = evaluator.evaluate(recsys, 'rmse', permutation=False)
    assert_true(rmse['rmse'] >= 0.0 and rmse['rmse'] <= 1.0)

    mae = evaluator.evaluate(recsys, 'mae', permutation=False)
    assert_true(mae['mae'] >= 0.0 and mae['mae'] <= 1.0)

    nmae = evaluator.evaluate(recsys, 'nmae', permutation=False)
    assert_true(nmae['nmae'] >= 0.0 and nmae['nmae'] <= 1.0)

    precision = evaluator.evaluate(recsys, 'precision',
                                permutation=False)
    assert_true(precision['precision'] >= 0.0 and precision['precision'] <= 1.0)

    recall = evaluator.evaluate(recsys, 'recall', permutation=False)
    assert_true(recall['recall'] >= 0.0 and recall['recall'] <= 1.0)

    f1score = evaluator.evaluate(recsys, 'f1score', permutation=False)
    assert_true(f1score['f1score'] >= 0.0 and f1score['f1score'] <= 1.0)

    all_scores = evaluator.evaluate(recsys, permutation=False)
    assert_true(all_scores['f1score'] >= 0.0 and all_scores['f1score'] <= 1.0)
    assert_true(all_scores['recall'] >= 0.0 and all_scores['recall'] <= 1.0)
    assert_true(all_scores['precision'] >= 0.0 and all_scores['precision'] <= 1.0)
    assert_true(all_scores['nmae'] >= 0.0 and all_scores['nmae'] <= 1.0)
    assert_true(all_scores['mae'] >= 0.0 and all_scores['mae'] <= 1.0)
    assert_true(all_scores['rmse'] >= 0.0 and all_scores['rmse'] <= 1.0)

    #With values at sampling.
    nmae = evaluator.evaluate(recsys, 'nmae', permutation=False,
                    sampling_users=0.6, sampling_ratings=0.6)
    assert_true(nmae['nmae'] >= 0.0 and nmae['nmae'] <= 1.0)

    #Test with boolean recsys
    assert_raises(ValueError, evaluator.evaluate, boolean_recsys, 'rank')

    #Test with specified metric
    rmse = evaluator.evaluate(boolean_recsys, 'rmse', permutation=False)
    assert_true(rmse['rmse'] >= 0.0 and rmse['rmse'] <= 1.0)

    mae = evaluator.evaluate(boolean_recsys, 'mae', permutation=False)
    assert_true(mae['mae'] >= 0.0 and mae['mae'] <= 1.0)

    nmae = evaluator.evaluate(boolean_recsys, 'nmae', permutation=False)
    assert_true(nmae['nmae'] >= 0.0 and nmae['nmae'] <= 1.0)

    precision = evaluator.evaluate(boolean_recsys, 'precision',
                                permutation=False)
    assert_true(precision['precision'] >= 0.0 and precision['precision'] <= 1.0)

    recall = evaluator.evaluate(boolean_recsys, 'recall', permutation=False)
    assert_true(recall['recall'] >= 0.0 and recall['recall'] <= 1.0)

    f1score = evaluator.evaluate(boolean_recsys, 'f1score', permutation=False)
    assert_true(f1score['f1score'] >= 0.0 and f1score['f1score'] <= 1.0)

    all_scores = evaluator.evaluate(recsys, permutation=False)
    assert_true(all_scores['f1score'] >= 0.0 and all_scores['f1score'] <= 1.0)
    assert_true(all_scores['recall'] >= 0.0 and all_scores['recall'] <= 1.0)
    assert_true(all_scores['precision'] >= 0.0 and all_scores['precision'] <= 1.0)
    assert_true(all_scores['nmae'] >= 0.0 and all_scores['nmae'] <= 1.0)
    assert_true(all_scores['mae'] >= 0.0 and all_scores['mae'] <= 1.0)
    assert_true(all_scores['rmse'] >= 0.0 and all_scores['rmse'] <= 1.0)

    #With values at sampling.
    nmae = evaluator.evaluate(boolean_recsys, 'nmae', permutation=False,
                    sampling_users=0.6, sampling_ratings=0.6)
    assert_true(nmae['nmae'] >= 0.0 and nmae['nmae'] <= 1.0)


def test_root_CfEvaluator_evaluate_on_split():
    """Check evaluate_on_split method in CfEvaluator """
    evaluator = CfEvaluator()

    #Test with invalid metric
    assert_raises(ValueError, evaluator.evaluate_on_split, recsys, 'rank')

    #Test with specified metric
    rmse = evaluator.evaluate_on_split(recsys, 'rmse', permutation=False)
    for p in rmse[0]['error']:
        assert_true(p['rmse'] >= 0.0 and p['rmse'] <= 1.0)
    assert_true(rmse[1]['final_error']['avg']['rmse'] >= 0.0 and
                rmse[1]['final_error']['stdev']['rmse'] <= 1.0)

    mae = evaluator.evaluate_on_split(recsys, 'mae', permutation=False)
    for p in mae[0]['error']:
        assert_true(p['mae'] >= 0.0 and p['mae'] <= 1.0)
    assert_true(mae[1]['final_error']['avg']['mae'] >= 0.0 and
                mae[1]['final_error']['stdev']['mae'] <= 1.0)

    nmae = evaluator.evaluate_on_split(recsys, 'nmae', permutation=False)
    for p in nmae[0]['error']:
        assert_true(p['nmae'] >= 0.0 and p['nmae'] <= 1.0)
    assert_true(nmae[1]['final_error']['avg']['nmae'] >= 0.0 and
                nmae[1]['final_error']['stdev']['nmae'] <= 1.0)

    #Test with IR statistics
    precision = evaluator.evaluate_on_split(recsys, 'precision', permutation=False)
    for p in precision[0]['ir']:
        assert_true(p['precision'] >= 0.0 and p['precision'] <= 1.0)
    assert_true(precision[1]['final_error']['avg']['precision'] >= 0.0 and
                precision[1]['final_error']['stdev']['precision'] <= 1.0)

    recall = evaluator.evaluate_on_split(recsys, 'recall', permutation=False)
    for p in recall[0]['ir']:
        assert_true(p['recall'] >= 0.0 and p['recall'] <= 1.0)
    assert_true(recall[1]['final_error']['avg']['recall'] >= 0.0 and
                recall[1]['final_error']['stdev']['recall'] <= 1.0)

    f1score = evaluator.evaluate_on_split(recsys, 'f1score', permutation=False)
    for p in f1score[0]['ir']:
        assert_true(p['f1score'] >= 0.0 and p['f1score'] <= 1.0)
    assert_true(f1score[1]['final_error']['avg']['f1score'] >= 0.0 and
                f1score[1]['final_error']['stdev']['f1score'] <= 1.0)

    all_scores = evaluator.evaluate_on_split(recsys, permutation=False)
    for p in all_scores[0]['ir']:
        assert_true(p['f1score'] >= 0.0 and p['f1score'] <= 1.0)
        assert_true(p['recall'] >= 0.0 and p['recall'] <= 1.0)
        assert_true(p['precision'] >= 0.0 and p['precision'] <= 1.0)
    for p in all_scores[0]['error']:
        assert_true(p['mae'] >= 0.0 and p['mae'] <= 1.0)
        assert_true(p['rmse'] >= 0.0 and p['rmse'] <= 1.0)
        assert_true(p['nmae'] >= 0.0 and p['nmae'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['f1score'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['f1score'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['recall'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['recall'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['precision'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['precision'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['rmse'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['rmse'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['mae'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['mae'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['nmae'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['nmae'] <= 1.0)

    #Test with boolean model
    #Test with invalid metric
    assert_raises(ValueError, evaluator.evaluate_on_split, boolean_recsys, 'rank')

    #Test with specified metric
    rmse = evaluator.evaluate_on_split(boolean_recsys, 'rmse', permutation=False)
    for p in rmse[0]['error']:
        assert_true(p['rmse'] >= 0.0 and p['rmse'] <= 1.0)
    assert_true(rmse[1]['final_error']['avg']['rmse'] >= 0.0 and
                rmse[1]['final_error']['stdev']['rmse'] <= 1.0)

    mae = evaluator.evaluate_on_split(boolean_recsys, 'mae', permutation=False)
    for p in mae[0]['error']:
        assert_true(p['mae'] >= 0.0 and p['mae'] <= 1.0)
    assert_true(mae[1]['final_error']['avg']['mae'] >= 0.0 and
                mae[1]['final_error']['stdev']['mae'] <= 1.0)

    nmae = evaluator.evaluate_on_split(boolean_recsys, 'nmae', permutation=False)
    for p in nmae[0]['error']:
        assert_true(p['nmae'] >= 0.0 and p['nmae'] <= 1.0)
    assert_true(nmae[1]['final_error']['avg']['nmae'] >= 0.0 and
                nmae[1]['final_error']['stdev']['nmae'] <= 1.0)

    #Test with IR statistics
    precision = evaluator.evaluate_on_split(boolean_recsys, 'precision', permutation=False)
    for p in precision[0]['ir']:
        assert_true(p['precision'] >= 0.0 and p['precision'] <= 1.0)
    assert_true(precision[1]['final_error']['avg']['precision'] >= 0.0 and
                precision[1]['final_error']['stdev']['precision'] <= 1.0)

    recall = evaluator.evaluate_on_split(boolean_recsys, 'recall', permutation=False)
    for p in recall[0]['ir']:
        assert_true(p['recall'] >= 0.0 and p['recall'] <= 1.0)
    assert_true(recall[1]['final_error']['avg']['recall'] >= 0.0 and
                recall[1]['final_error']['stdev']['recall'] <= 1.0)

    f1score = evaluator.evaluate_on_split(boolean_recsys, 'f1score', permutation=False)
    for p in f1score[0]['ir']:
        assert_true(p['f1score'] >= 0.0 and p['f1score'] <= 1.0)
    assert_true(f1score[1]['final_error']['avg']['f1score'] >= 0.0 and
                f1score[1]['final_error']['stdev']['f1score'] <= 1.0)

    all_scores = evaluator.evaluate_on_split(boolean_recsys, permutation=False)
    for p in all_scores[0]['ir']:
        assert_true(p['f1score'] >= 0.0 and p['f1score'] <= 1.0)
        assert_true(p['recall'] >= 0.0 and p['recall'] <= 1.0)
        assert_true(p['precision'] >= 0.0 and p['precision'] <= 1.0)

    for p in all_scores[0]['error']:
        assert_true(p['mae'] >= 0.0 and p['mae'] <= 1.0)
        assert_true(p['rmse'] >= 0.0 and p['rmse'] <= 1.0)
        assert_true(p['nmae'] >= 0.0 and p['nmae'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['f1score'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['f1score'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['recall'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['recall'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['precision'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['precision'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['rmse'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['rmse'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['mae'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['mae'] <= 1.0)
    assert_true(all_scores[1]['final_error']['avg']['nmae'] >= 0.0 and
                all_scores[1]['final_error']['stdev']['nmae'] <= 1.0)

########NEW FILE########
__FILENAME__ = test_cross_validation
import numpy as np
from ..cross_validation import LeaveOneOut, LeavePOut, KFold, ShuffleSplit
from numpy.testing import assert_array_equal


def test_LeaveOneOut():
    X = np.array(['userA', 'userB', 'userC', 'userD'])
    loo = LeaveOneOut(4)
    results_train = [['userB', 'userC', 'userD'], ['userA', 'userC', 'userD'],
                ['userA', 'userB', 'userD'], ['userA', 'userB', 'userC']]
    results_test = [['userA'], ['userB'], ['userC'], ['userD']]
    for index, sample in enumerate(loo):
        assert_array_equal(X[sample[0]], results_train[index])
        assert_array_equal(X[sample[1]], results_test[index])

    loo = LeaveOneOut(4, True)
    for index, sample in enumerate(loo):
        assert_array_equal(X[sample[0]], results_train[index])
        assert_array_equal(X[sample[1]], results_test[index])


def test_LeavePOut():
    X = np.array(['userA', 'userB', 'userC'])
    loo = LeavePOut(3, 2)
    results_train = [['userC'], ['userB'], ['userA']]
    results_test = [['userA', 'userB'], ['userA', 'userC'],
                    ['userB', 'userC']]
    for index, sample in enumerate(loo):
        assert_array_equal(X[sample[0]], results_train[index])
        assert_array_equal(X[sample[1]], results_test[index])

    loo = LeavePOut(3, 2, True)
    for index, sample in enumerate(loo):
        assert_array_equal(X[sample[0]], results_train[index])
        assert_array_equal(X[sample[1]], results_test[index])


def test_KFold():
    X = np.array(['userA', 'userB', 'userC', 'userD'])
    kfold = KFold(4, 2)
    results_train = [['userC', 'userD'], ['userA', 'userB'],
                    ['userC', 'userD'], ['userA', 'userB']]
    results_test = [['userA', 'userB'], ['userC', 'userD'],
                    ['userA', 'userB'], ['userC', 'userD']]
    for index, sample in enumerate(kfold):
        assert_array_equal(X[sample[0]], results_train[index])
        assert_array_equal(X[sample[1]], results_test[index])

    kfold = KFold(4, 2, True)
    for index, sample in enumerate(kfold):
        assert_array_equal(X[sample[0]], results_train[index])
        assert_array_equal(X[sample[1]], results_test[index])

########NEW FILE########
__FILENAME__ = test_metrics
import numpy as np
from nose.tools import assert_equals, assert_almost_equals
from ..metrics import root_mean_square_error, mean_absolute_error,\
                        normalized_mean_absolute_error, precision_recall_fscore, \
                        precision_score, recall_score, f1_score, evaluation_report,\
                        evaluation_error
from numpy.testing import assert_array_almost_equal


def test_root_mean_square_error():
    """Check that the metric Root Mean Squared Error (RMSE) """
    y_real = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    y_pred = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    assert_equals(0.0, root_mean_square_error(y_real, y_pred))

    y_real = np.array([3.0, 1.0, 2.0, 1.0, 1.0])
    y_pred = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    assert_almost_equals(1.8973665961, root_mean_square_error(y_real, y_pred))


def test_root_mean_absolute_error():
    """Check that the metric Mean Absolute Error (MAE) """
    y_real = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    y_pred = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    assert_equals(0.0, mean_absolute_error(y_real, y_pred))

    y_real = np.array([3.0, 1.0, 2.0, 1.0, 1.0])
    y_pred = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    assert_almost_equals(1.6, mean_absolute_error(y_real, y_pred))


def test_root_normalized_mean_absolute_error():
    """Check that the metric Normalized Mean Absolute Error (NMAE) """
    max_rating = 5.0
    min_rating = 1.0
    y_real = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    y_pred = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    assert_equals(0.0, normalized_mean_absolute_error(y_real, y_pred, max_rating, min_rating))

    y_real = np.array([3.0, 1.0, 2.0, 1.0, 1.0])
    y_pred = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    assert_almost_equals(0.4, normalized_mean_absolute_error(y_real, y_pred,
                max_rating, min_rating))


def test_evaluation_error():
    """ Check the error evaluation """
    max_rating = 5.0
    min_rating = 1.0
    y_real = np.array([0.0, 1.0, 0.0, 2.0, 3.0])
    y_pred = np.array([0.0, 1.0, 0.0, 2.0, 3.0])

    mae, nmae, rmse = evaluation_error(y_real, y_pred, max_rating, min_rating)
    assert_equals(mae, 0.0)
    assert_equals(nmae, 0.0)
    assert_equals(rmse, 0.0)


def test_precision_recall_f1_score():
    """Test Precision Recall and F1 Score """
    y_real = np.array([['a', 'b', 'c'], ['a', 'b', 'e', 'f', 'g'], ['a', 'b']])
    y_pred = np.array([['a', 'b', 'c'], ['a', 'b', 'c', 'd'], ['e', 'f']])

    p, r, f = precision_recall_fscore(y_real, y_pred)
    assert_array_almost_equal(p, [1, 0.4, 0], 2)
    assert_array_almost_equal(r, [1., 0.5, 0], 2)
    assert_array_almost_equal(f, [1., 0.44, 0], 2)

    ps = precision_score(y_real, y_pred)
    assert_array_almost_equal(ps, 0.4666, 2)

    rs = recall_score(y_real, y_pred)
    assert_array_almost_equal(rs, 0.5, 2)

    fs = f1_score(y_real, y_pred)
    assert_array_almost_equal(fs, 0.48, 2)


def test_zero_precision_recall():
    """Check that pathological cases do not bring NaNs"""

    try:
        old_error_settings = np.seterr(all='raise')

        y_real = np.array([['a', 'b', 'c']])
        y_pred = np.array([[]])

        assert_array_almost_equal(precision_score(y_real, y_pred), 0.0, 2)
        assert_array_almost_equal(recall_score(y_real, y_pred), 0.0, 2)
        assert_array_almost_equal(f1_score(y_real, y_pred), 0.0, 2)

    finally:
        np.seterr(**old_error_settings)


def test_evaluation_report():
    """Test evaluation report"""
    y_real = np.array([['a', 'b', 'c'], ['a', 'b', 'e', 'f', 'g'], ['a', 'b']])
    y_pred = np.array([['a', 'b', 'c'], ['a', 'b', 'c', 'd'], ['e', 'f']])
    labels = np.array(['user_id1', 'user_id2', 'user_id3'])
    # print evaluation report with class names
    expected_report = """\
             precision    recall  f1-score

   user_id1       1.00      1.00      1.00
   user_id2       0.40      0.50      0.44
   user_id3       0.00      0.00      0.00

avg / total       0.47      0.50      0.48
"""
    report = evaluation_report(
        y_real, y_pred,
        target_names=labels)
    assert_equals(report, expected_report)
    # print classification report with label detection
    expected_report = """\
             precision    recall  f1-score

          0       1.00      1.00      1.00
          1       0.40      0.50      0.44
          2       0.00      0.00      0.00

avg / total       0.47      0.50      0.48
"""
    report = evaluation_report(y_real, y_pred)
    assert_equals(report, expected_report)

########NEW FILE########
__FILENAME__ = test_pairwise
import numpy as np

from numpy.testing import assert_array_almost_equal

from nose.tools import assert_raises

from ..pairwise import euclidean_distances, pearson_correlation, \
                       jaccard_coefficient, manhattan_distances,  \
                       sorensen_coefficient, tanimoto_coefficient, \
                       cosine_distances, \
                       spearman_coefficient, loglikehood_coefficient

np.random.seed(0)

#class testPairwise(TestCase):


def test_euclidean_distances():
    """Check that the pairwise euclidian distances computation"""
    #Idepontent Test
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    D = euclidean_distances(X, X)
    assert_array_almost_equal(D, [[1.]])

    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    D = euclidean_distances(X, X, inverse=False)
    assert_array_almost_equal(D, [[0.]])

    #Vector x Non Vector
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[]]
    assert_raises(ValueError, euclidean_distances, X, Y)

    #Vector A x Vector B
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = euclidean_distances(X, Y)
    assert_array_almost_equal(D, [[0.29429806]])

    #Vector N x 1
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = euclidean_distances(X, Y)
    assert_array_almost_equal(D, [[0.29429806], [0.29429806]])

    #N-Dimmensional Vectors
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    D = euclidean_distances(X, Y)
    assert_array_almost_equal(D, [[0.29429806, 1.], [0.29429806,  1.]])

    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = euclidean_distances(X, X)
    assert_array_almost_equal(D, [[1., 0.29429806], [0.29429806, 1.]])

    X = [[1.0, 0.0], [1.0, 1.0]]
    Y = [[0.0, 0.0]]
    D = euclidean_distances(X, Y)
    assert_array_almost_equal(D, [[0.5], [0.41421356]])


def test_pearson_correlation():
    """ Check that the pairwise Pearson distances computation"""
    #Idepontent Test
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    D = pearson_correlation(X, X)
    assert_array_almost_equal(D, [[1.]])

    #Vector x Non Vector
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[]]
    assert_raises(ValueError, pearson_correlation, X, Y)

    #Vector A x Vector B
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = pearson_correlation(X, Y)
    assert_array_almost_equal(D, [[0.3960590]])

    #Vector N x 1
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = pearson_correlation(X, Y)
    assert_array_almost_equal(D, [[0.3960590], [0.3960590]])

    #N-Dimmensional Vectors
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    D = pearson_correlation(X, Y)
    assert_array_almost_equal(D, [[0.3960590, 1.], [0.3960590, 1.]])

    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = pearson_correlation(X, X)
    assert_array_almost_equal(D, [[1., 0.39605902], [0.39605902, 1.]])

    X = [[1.0, 0.0], [1.0, 1.0]]
    Y = [[0.0, 0.0]]
    D = pearson_correlation(X, Y)
    assert_array_almost_equal(D, [[np.nan], [np.nan]])


def test_spearman_distances():
    """ Check that the pairwise Spearman distances computation"""
    #Idepontent Test
    X = [[('a', 2.5), ('b', 3.5), ('c', 3.0), ('d', 3.5),
          ('e', 2.5), ('f', 3.0)]]
    D = spearman_coefficient(X, X)
    assert_array_almost_equal(D, [[1.]])

    #Vector x Non Vector
    X = [[('a', 2.5), ('b', 3.5), ('c', 3.0), ('d', 3.5),
          ('e', 2.5), ('f', 3.0)]]
    Y = [[]]
    assert_raises(ValueError, spearman_coefficient, X, Y)

    #Vector A x Vector B
    X = [[('a', 2.5), ('b', 3.5), ('c', 3.0), ('d', 3.5),
          ('e', 2.5), ('f', 3.0)]]
    Y = [[('a', 3.0), ('b', 3.5), ('c', 1.5), ('d', 5.0),
          ('e', 3.5), ('f', 3.0)]]
    D = spearman_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.5428571428]])

    #Vector N x 1
    X = [[('a', 2.5), ('b', 3.5), ('c', 3.0), ('d', 3.5)],
         [('e', 2.5), ('f', 3.0), ('g', 2.5), ('h', 4.0)]]
    Y = [[('a', 2.5), ('b', 3.5), ('c', 3.0), ('k', 3.5)]]
    D = spearman_coefficient(X, Y)
    assert_array_almost_equal(D, [[1.], [0.]])

    #N-Dimmensional Vectors
    X = [[('a', 2.5), ('b', 3.5), ('c', 3.0), ('d', 3.5)],
         [('e', 2.5), ('f', 3.0), ('g', 2.5), ('h', 4.0)]]
    Y = [[('a', 2.5), ('b', 3.5), ('c', 3.0), ('d', 3.5)],
         [('e', 2.5), ('f', 3.0), ('g', 2.5), ('h', 4.0)]]
    D = spearman_coefficient(X, Y)
    assert_array_almost_equal(D, [[1., 0.], [0., 1.]])


def test_tanimoto_distances():
    """ Check that the pairwise Tanimoto distances computation"""
    #Idepontent Test
    X = [['a', 'b', 'c']]
    D = tanimoto_coefficient(X, X)
    assert_array_almost_equal(D, [[1.]])

    #Vector x Non Vector
    X = [['a', 'b', 'c']]
    Y = [[]]
    D = tanimoto_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.]])

    #Vector A x Vector B
    X = [[1, 2, 3, 4]]
    Y = [[2, 3]]
    D = tanimoto_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.5]])

    #BUG FIX: How to fix for multi-dimm arrays

    #Vector N x 1
    X = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]
    Y = [['a', 'b', 'c', 'k']]
    D = tanimoto_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.6], [0.]])

    #N-Dimmensional Vectors
    X = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]
    Y = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]
    D = tanimoto_coefficient(X, Y)
    assert_array_almost_equal(D, [[1., 0.], [0., 1.]])

    X = [[0, 1], [1, 1]]
    D = tanimoto_coefficient(X, X)
    assert_array_almost_equal(D, [[1., 0.33333333], [0.33333333, 0.33333333]])

    X = [[0, 1], [1, 1]]
    Y = [[0, 0]]
    D = tanimoto_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.3333333], [0.]])


def test_cosine_distances():
    """ Check that the pairwise Cosine distances computation"""
    #Idepontent Test
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    D = cosine_distances(X, X)
    assert_array_almost_equal(D, [[1.]])
    #Vector x Non Vector
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[]]
    assert_raises(ValueError, pearson_correlation, X, Y)
    #Vector A x Vector B
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = cosine_distances(X, Y)
    assert_array_almost_equal(D, [[0.960646301]])
    #Vector N x 1
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = cosine_distances(X, Y)
    assert_array_almost_equal(D, [[0.960646301], [0.960646301]])

    #N-Dimmensional Vectors
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    D = cosine_distances(X, Y)
    assert_array_almost_equal(D, [[0.960646301, 1.], [0.960646301, 1.]])

    X = [[0, 1], [1, 1]]
    D = cosine_distances(X, X)
    assert_array_almost_equal(D, [[1., 0.70710678], [0.70710678, 1.]])

    X = [[0, 1], [1, 1]]
    Y = [[0, 0]]
    D = cosine_distances(X, Y)
    assert_array_almost_equal(D, [[np.nan], [np.nan]])


def test_loglikehood_distances():
    """ Check that the pairwise LogLikehood distances computation"""
    #Idepontent Test
    X = [['a', 'b', 'c']]
    n_items = 3
    D = loglikehood_coefficient(n_items, X, X)
    assert_array_almost_equal(D, [[1.]])

    #Vector x Non Vector
    X = [['a', 'b', 'c']]
    Y = [[]]
    n_items = 3
    D = loglikehood_coefficient(n_items, X, Y)
    assert_array_almost_equal(D, [[0.]])

    #Vector A x Vector B
    X = [[1, 2, 3, 4]]
    Y = [[2, 3]]
    n_items = 4
    D = loglikehood_coefficient(n_items, X, Y)
    assert_array_almost_equal(D, [[0.]])

    #BUG FIX: How to fix for multi-dimm arrays

    #Vector N x 1
    X = [['a', 'b', 'c', 'd'],  ['e', 'f', 'g', 'h']]
    Y = [['a', 'b', 'c', 'k']]
    n_items = 8
    D = loglikehood_coefficient(n_items, X, Y)
    assert_array_almost_equal(D, [[0.67668852], [0.]])

    #N-Dimmensional Vectors
    X = [['a', 'b', 'c', 'd'], ['e', 'f', 'g', 'h']]
    Y = [['a', 'b', 'c', 'd'], ['e', 'f', 'g', 'h']]
    n_items = 7
    D = loglikehood_coefficient(n_items, X, Y)
    assert_array_almost_equal(D, [[1., 0.], [0., 1.]])


def test_sorensen_distances():
    """ Check that the pairwise Sorensen distances computation"""
    #Idepontent Test
    X = [['a', 'b', 'c']]
    D = sorensen_coefficient(X, X)
    assert_array_almost_equal(D, [[1.]])

    #Vector x Non Vector
    X = [['a', 'b', 'c']]
    Y = [[]]
    D = sorensen_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.]])

    #Vector A x Vector B
    X = [[1, 2, 3, 4]]
    Y = [[2, 3]]
    D = sorensen_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.666666]])

    #BUG FIX: How to fix for multi-dimm arrays

    #Vector N x 1
    X = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]
    Y = [['a', 'b', 'c', 'k']]
    D = sorensen_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.75], [0.]])

    #N-Dimmensional Vectors
    X = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]
    Y = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]
    D = sorensen_coefficient(X, Y)
    assert_array_almost_equal(D, [[1., 0.], [0., 1.]])

    X = [[0, 1], [1, 2]]
    D = sorensen_coefficient(X, X)
    assert_array_almost_equal(D, [[1., 0.5], [0.5, 1.]])

    X = [[0, 1], [1, 2]]
    Y = [[0, 0]]
    D = sorensen_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.5], [0.]])


def test_manthattan_distances():
    """ Check that the pairwise Manhattan distances computation"""
    #Idepontent Test
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    D = manhattan_distances(X, X)
    assert_array_almost_equal(D, [[1.]])

    #Vector x Non Vector
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[]]
    assert_raises(ValueError, manhattan_distances, X, Y)

    #Vector A x Vector B
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = manhattan_distances(X, Y)
    assert_array_almost_equal(D, [[0.25]])

    #BUG FIX: How to fix for multi-dimm arrays

    #Vector N x 1
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[3.0, 3.5, 1.5, 5.0, 3.5, 3.0]]
    D = manhattan_distances(X, Y)
    assert_array_almost_equal(D, [[0.25], [0.25]])

    #N-Dimmensional Vectors
    X = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    Y = [[2.5, 3.5, 3.0, 3.5, 2.5, 3.0], [2.5, 3.5, 3.0, 3.5, 2.5, 3.0]]
    D = manhattan_distances(X, Y)
    assert_array_almost_equal(D, [[1., 1.], [1., 1.]])

    X = [[0,1],[1,1]]
    D = manhattan_distances(X,X)
    assert_array_almost_equal(D,[[1., 0.5], [0.5, 1.]])    

    X = [[0, 1], [1, 1]]
    Y = [[0, 0]]
    D = manhattan_distances(X, Y)
    assert_array_almost_equal(D, [[0.5], [0.]])


def test_jaccard_distances():
    """ Check that the pairwise Jaccard distances computation"""
    #Idepontent Test
    X = [['a', 'b', 'c']]
    D = jaccard_coefficient(X, X)
    assert_array_almost_equal(D, [[1.]])

    #Vector x Non Vector
    X = [['a', 'b', 'c']]
    Y = [[]]
    D = jaccard_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.]])

    #Vector A x Vector B
    X = [[1, 2, 3, 4]]
    Y = [[2, 3]]
    D = jaccard_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.5]])

    #BUG FIX: How to fix for multi-dimm arrays

    #Vector N x 1
    X = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]
    Y = [['a', 'b', 'c', 'k']]
    D = jaccard_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.6], [0.]])

    #N-Dimmensional Vectors
    X = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]
    Y = [['a', 'b', 'c', 'd'], ['e', 'f', 'g']]
    D = jaccard_coefficient(X, Y)
    assert_array_almost_equal(D, [[1., 0.], [0., 1.]])

    X = [[0, 1], [1, 2]]
    D = jaccard_coefficient(X, X)
    assert_array_almost_equal(D, [[1., 0.33333333], [0.33333333, 1.]])

    X = [[0, 1], [1, 2]]
    Y = [[0, 3]]
    D = jaccard_coefficient(X, Y)
    assert_array_almost_equal(D, [[0.33333333], [0.]])


#if __name__ == '__main__':
#    run_module_suite()

########NEW FILE########
__FILENAME__ = base
#-*- coding:utf-8 -*-

"""
Base Data Models.
"""
# Authors: Bruno Melo <bruno@muricoca.com>
# License: BSD Style


class BaseDataModel(object):

    def user_ids(self):
        '''
        Returns
        --------
        Return all user ids in the model, in order
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def item_ids(self):
        '''
        Returns
        -------
        Return a iterator of all item ids in the model, in order
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def preferences_values_from_user(self, user_id, order_by_id=True):
        '''
        Parameters
        ----------
        user_id: user id in the model
                 int or string

        order_by_id: bool
                If True order by user_id otherwise by the preference values.
                default = True
        Returns
        ---------
        Return user's preferences only the values.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def preferences_from_user(self, user_id, order_by_id=True):
        '''
        Parameters
        ----------
        user_id: user id in the model
                 int or string

        order_by_id: bool
                If True order by user_id otherwise by the preference values.
                default = True
        Returns
        ---------
        Return user's preferences, ordered by user id (if order_by_id is True)
        or by the preference values (if order_by_id is False), as an array.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def items_from_user(self, user_id):
        '''
        Parameters
        ----------
        user_id: user id in the model
                int or string

        Returns
        -------
        Return ids of items user expresses a preference for
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def preferences_for_item(self, item_id, order_by_id=True):
        '''
        Parameters
        ----------
        item_id: id of the item in the model
                string or int

        order_by_id: bool
                If True order by user_id otherwise by the preference values.
                default = True
        Returns
        ----------
        Return all existing Preferences expressed for that item,
        ordered by user id (if order_by_id is True) or by the preference values
        (if order_by_id is False), as an array.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def preference_value(self, user_id, item_id):
        '''
        Parameters
        ----------
        user_id: user id in the model
                int or string

        item_id: id of the item in the model
                string or int

        Returns
        ---------
        Retrieves the preference value for a single user and item.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def preference_time(self, user_id, item_id):
        '''
        Parameters
        ----------
        user_id: user id in the model
                int or string

        item_id: id of the item in the model
                string or int

        Returns
        ---------
        Retrieves the time at which a preference value from a user and item
        was set, if known. Time is expressed in the usual way, as a number
        of milliseconds since the epoch.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def users_count(self):
        '''
        Returns
        -------
        Return total number of users known to the model.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def items_count(self):
        '''
        Returns
        --------
        Return total number of items known to the model.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def set_preference(self, user_id, item_id, value=None):
        '''
        Parameters
        ----------
        user_id: user id in the model
                int or string

        item_id: id of the item in the model
                string or int

        value:  the preference
                bool or float

        Sets a particular preference (item plus rating) for a user.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def remove_preference(self, user_id, item_id):
        '''
        Parameters
        ----------
        user_id: user id in the model
                int or string

        item_id: id of the item in the model
                string or int

        Removes a particular preference for a user.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def has_preference_values(self):
        '''
        Returns
        -------
        Return True if this implementation actually it is not a 'boolean'
        data model, otherwise returns False.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def maximum_preference_value(self):
        '''
        Returns
        --------
        Return the maximum preference value that is possible in the current
        problem domain being evaluated.
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def minimum_preference_value(self):
        '''
        Returns
        --------
        Returns the minimum preference value that is possible in the current
        problem domain being evaluated
        '''
        raise NotImplementedError("cannot instantiate Abstract Base Class")

########NEW FILE########
__FILENAME__ = classes
#-*- coding:utf-8 -*-

"""
Several Basic Data models.

"""
# Authors: Marcel Caraciolo <marcel@muricoca.com>
# License: BSD Style


import numpy as np
from .base import BaseDataModel
from .utils import UserNotFoundError, ItemNotFoundError
import logging

logger = logging.getLogger('crab')


###############################################################################
# MatrixDataModel
class MatrixPreferenceDataModel(BaseDataModel):
    '''
    Matrix with preferences based Data model
    A DataModel backed by a python dict structured data.
    This class expects a simple dictionary where each
    element contains a userID, followed by itemID,
    followed by preference value and optional timestamp.

    {userID:{itemID:preference, itemID2:preference2},
       userID2:{itemID:preference3,itemID4:preference5}}

    Preference value is the parameter that the user simply
     expresses the degree of preference for an item.

    Parameters
    ----------
    dataset dict, shape  = {userID:{itemID:preference, itemID2:preference2},
              userID2:{itemID:preference3,itemID4:preference5}}

    Examples
    ---------
    >>> from scikits.crab.models.classes import MatrixPreferenceDataModel
    >>> model = MatrixPreferenceDataModel({})
    >>> #empty dataset
    >>> model.user_ids()
    array([], dtype=float64)
    >>> model.item_ids()
    array([], dtype=float64)
    >>> movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, \
     'Snakes on a Plane': 3.5, \
     'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \
     'The Night Listener': 3.0}, \
     'Paola Pow': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \
     'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 3.5}, \
    'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, \
     'Superman Returns': 3.5, 'The Night Listener': 4.0}, \
    'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, \
     'The Night Listener': 4.5, 'Superman Returns': 4.0, \
     'You, Me and Dupree': 2.5}, \
    'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 2.0}, \
    'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'The Night Listener': 3.0, 'Superman Returns': 5.0, \
     'You, Me and Dupree': 3.5}, \
    'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0, \
    'Superman Returns':4.0}, \
    'Maria Gabriela': {}}
    >>> model = MatrixPreferenceDataModel(movies)
    >>> #non-empty dataset
    >>> model.user_ids()
    array(['Leopoldo Pires', 'Lorena Abreu', 'Marcel Caraciolo',
               'Maria Gabriela', 'Paola Pow', 'Penny Frewman', 'Sheldom',
               'Steve Gates'],
              dtype='|S16')
    >>> model.item_ids()
    array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
               'Superman Returns', 'The Night Listener', 'You, Me and Dupree'],
              dtype='|S18')
    >>> model.preferences_from_user('Sheldom')
    [('Lady in the Water', 3.0), ('Snakes on a Plane', 4.0), ('Superman Returns', 5.0),
        ('The Night Listener', 3.0), ('You, Me and Dupree', 3.5)]
    '''
    def __init__(self, dataset):
        BaseDataModel.__init__(self)
        self.dataset = dataset
        self.build_model()

    def __getitem__(self, user_id):
        return self.preferences_from_user(user_id)

    def __iter__(self):
        for index, user in enumerate(self.user_ids()):
            yield user, self[user]

    def __len__(self):
        return self.index.shape

    def build_model(self):
        '''
        Returns
        -------
        self:
             Build the data model
        '''
        #Is it important to store as numpy array ?
        self._user_ids = np.asanyarray(self.dataset.keys())
        self._user_ids.sort()

        #Is it important to store as numpy array ?
        self._item_ids = []
        for items in self.dataset.itervalues():
            self._item_ids.extend(items.keys())

        self._item_ids = np.unique(np.array(self._item_ids))
        self._item_ids.sort()

        self.max_pref = -np.inf
        self.min_pref = np.inf

        logger.info("creating matrix for %d users and %d items" % \
                    (self._user_ids.size, self._item_ids.size))

        self.index = np.empty(shape=(self._user_ids.size, self._item_ids.size))
        for userno, user_id in enumerate(self._user_ids):
            if userno % 2 == 0:
                logger.debug("PROGRESS: at user_id #%i/%i" %  \
                    (userno, self._user_ids.size))
            for itemno, item_id in enumerate(self._item_ids):
                r = self.dataset[user_id].get(item_id, np.NaN) #Is it to be np.NaN or 0 ?!!
                self.index[userno, itemno] = r

        if self.index.size:
            self.max_pref = np.nanmax(self.index)
            self.min_pref = np.nanmin(self.index)

    def user_ids(self):
        '''
        Returns
        -------
        self.user_ids:  numpy array of shape [n_user_ids]
                        Return all user ids in the model, in order
        '''
        return self._user_ids

    def item_ids(self):
        '''
        Returns
        -------
        self.item_ids:  numpy array of shape [n_item_ids]
                    Return all item ids in the model, in order
        '''
        return self._item_ids

    def preference_values_from_user(self, user_id):
        '''
        Returns
        --------
        Return user's preferences values as an array

        Notes
        --------
        This method is a particular method in MatrixDataModel
        '''
        user_id_loc = np.where(self._user_ids == user_id)
        if not user_id_loc[0].size:
            #user_id not found
            raise UserNotFoundError

        preferences = self.index[user_id_loc]

        return preferences

    def preferences_from_user(self, user_id, order_by_id=True):
        '''
        Returns
        -------
        self.user_preferences :  list [(item_id,preference)]
         Return user's preferences, ordered by user ID (if order_by_id is True)
         or by the preference values (if order_by_id is False), as an array.

        '''
        preferences = self.preference_values_from_user(user_id)

        #think in a way to return as numpy array and how to remove the nan values efficiently.
        data = zip(self._item_ids, preferences.flatten())

        if order_by_id:
            return [(item_id, preference)  for item_id, preference in data \
                         if not np.isnan(preference)]
        else:
            return sorted([(item_id, preference)  for item_id, preference in data \
                         if not np.isnan(preference)], key=lambda item: - item[1])

    def has_preference_values(self):
        '''
        Returns
        -------
        True/False:  bool
                     Return True if this implementation actually
                     it is not a 'boolean' data model, otherwise returns False.
        '''
        return True

    def maximum_preference_value(self):
        '''
        Returns
        ---------
        self.max_preference:  float
                Return the maximum preference value that is possible in the
                 current problem domain being evaluated.
        '''
        return self.max_pref

    def minimum_preference_value(self):
        '''
        Returns
        ---------
        self.min_preference:  float
                Returns the minimum preference value that is possible in the
                current problem domain being evaluated
        '''
        return self.min_pref

    def users_count(self):
        '''
        Returns
        --------
        n_users:  int
                  Return total number of users known to the model.
        '''
        return self._user_ids.size

    def items_count(self):
        '''
        Returns
        --------
        n_items:  int
                  Return total number of items known to the model.
        '''
        return self._item_ids.size

    def items_from_user(self, user_id):
        '''
        Returns
        -------
        items_from_user : numpy array of shape [item_id,..]
                 Return IDs of items user expresses a preference for
        '''
        preferences = self.preferences_from_user(user_id)
        return [key for key, value in preferences]

    def preferences_for_item(self, item_id, order_by_id=True):
        '''
        Returns
        -------
        preferences: numpy array of shape [(item_id,preference)]
                     Return all existing Preferences expressed for that item,
        '''
        item_id_loc = np.where(self._item_ids == item_id)
        if not item_id_loc[0].size:
            #item_id not found
            raise ItemNotFoundError('Item not found')
        preferences = self.index[:, item_id_loc]

        #think in a way to return as numpy array and how to remove the nan values efficiently.
        data = zip(self._user_ids, preferences.flatten())
        if order_by_id:
            return [(user_id, preference)  for user_id, preference in data \
                         if not np.isnan(preference)]
        else:
            return sorted([(user_id, preference)  for user_id, preference in data \
                         if not np.isnan(preference)], key=lambda user: - user[1])

    def preference_value(self, user_id, item_id):
        '''
        Returns
        -------
        preference:  float
                     Retrieves the preference value for a single user and item.
        '''
        item_id_loc = np.where(self._item_ids == item_id)
        user_id_loc = np.where(self._user_ids == user_id)

        if not user_id_loc[0].size:
            raise UserNotFoundError('user_id in the model not found')

        if not item_id_loc[0].size:
            raise ItemNotFoundError('item_id in the model not found')

        return self.index[user_id_loc, item_id_loc].flatten()[0]

    def set_preference(self, user_id, item_id, value):
        '''
        Returns
        --------
        self
            Sets a particular preference (item plus rating) for a user.
        '''
        user_id_loc = np.where(self._user_ids == user_id)
        if not user_id_loc[0].size:
            raise UserNotFoundError('user_id in the model not found')

        #ALLOW NEW ITEMS
        #if not item_id_loc[0].size:
        #    raise ItemNotFoundError('item_id in the model not found')

        #How not use the dataset in memory ?!
        self.dataset[user_id][item_id] = value
        self.build_model()

    def remove_preference(self, user_id, item_id):
        '''
        Returns
        --------
        self
            Removes a particular preference for a user.
        '''
        user_id_loc = np.where(self._user_ids == user_id)
        item_id_loc = np.where(self._item_ids == item_id)

        if not user_id_loc[0].size:
            raise UserNotFoundError('user_id in the model not found')

        if not item_id_loc[0].size:
            raise ItemNotFoundError('item_id in the model not found')

        del self.dataset[user_id][item_id]
        self.build_model()

    def __repr__(self):
        return "<MatrixPreferenceDataModel (%d by %d)>" % (self.index.shape[0],
                        self.index.shape[1])

    def _repr_matrix(self, matrix):
        s = ""
        cellWidth = 11
        shape = matrix.shape
        for i in range(shape[0]):
            for j in range(shape[1]):
                v = matrix[i, j]
                if np.isnan(v):
                    s += "---".center(cellWidth)
                else:
                    exp = np.log(abs(v))
                    if abs(exp) <= 4:
                        if exp < 0:
                            s += ("%9.6f" % v).ljust(cellWidth)
                        else:
                            s += ("%9.*f" % (6, v)).ljust(cellWidth)
                    else:
                        s += ("%9.2e" % v).ljust(cellWidth)
            s += "\n"
        return s[:-1]

    def __unicode__(self):
        """
        Write out a representative picture of this matrix.

        The upper left corner of the matrix will be shown, with up to 20x5
        entries, and the rows and columns will be labeled with up to 8
        characters.
        """
        matrix = self._repr_matrix(self.index[:20, :5])
        lines = matrix.split('\n')
        headers = [repr(self)[1:-1]]
        if self._item_ids.size:
            col_headers = [('%-8s' % unicode(item)[:8]) for item in self._item_ids[:5]]
            headers.append(' ' + ('   '.join(col_headers)))

        if self._user_ids.size:
            for (i, line) in enumerate(lines):
                lines[i] = ('%-8s' % unicode(self._user_ids[i])[:8]) + line
            for (i, line) in enumerate(headers):
                if i > 0:
                    headers[i] = ' ' * 8 + line
        lines = headers + lines
        if self.index.shape[1] > 5 and self.index.shape[0] > 0:
            lines[1] += ' ...'
        if self.index.shape[0] > 20:
            lines.append('...')

        return '\n'.join(line.rstrip() for line in lines)

    def __str__(self):
        return unicode(self).encode('utf-8')


###############################################################################
# MatrixBooleanDataModel
class MatrixBooleanPrefDataModel(BaseDataModel):
    '''
    Matrix with preferences based Boolean Data model
    This class expects a simple dictionary where each
    element contains a userID, followed by the itemIDs
    where the itemIDs represents the preference
    for that item and optional timestamp. It also can
    receive the dict with the preference values used
    at DictPreferenceDataModel.

    Preference value is the presence of the item in the list of
    preferences for that user.

    Parameters
    ----------
    dataset dict, shape  = {userID:{itemID:preference, itemID2:preference2},
              userID2:{itemID:preference3,itemID4:preference5}} or
                  {userID:[itemID,itemID2,itemID3], userID2:[itemID1, itemID2,...]...}

    Examples
    ---------
    >>> from scikits.crab.models.classes import MatrixBooleanPrefDataModel
    >>> model = MatrixBooleanPrefDataModel({})
    >>> #empty dataset
    >>> model.user_ids()
    array([], dtype=float64)
    >>> model.item_ids()
    array([], dtype=float64)
    >>> movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, \
     'Snakes on a Plane': 3.5, \
     'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \
     'The Night Listener': 3.0}, \
     'Paola Pow': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \
     'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 3.5}, \
    'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, \
     'Superman Returns': 3.5, 'The Night Listener': 4.0}, \
    'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, \
     'The Night Listener': 4.5, 'Superman Returns': 4.0, \
     'You, Me and Dupree': 2.5}, \
    'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 2.0}, \
    'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'The Night Listener': 3.0, 'Superman Returns': 5.0, \
     'You, Me and Dupree': 3.5}, \
    'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0, \
    'Superman Returns':4.0}, \
    'Maria Gabriela': {}}
    >>> model = MatrixBooleanPrefDataModel(movies)
    >>> #non-empty dataset
    >>> model.user_ids()
    array(['Leopoldo Pires', 'Lorena Abreu', 'Marcel Caraciolo',
               'Maria Gabriela', 'Paola Pow', 'Penny Frewman', 'Sheldom',
               'Steve Gates'],
              dtype='|S16')
    >>> model.item_ids()
    array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
               'Superman Returns', 'The Night Listener', 'You, Me and Dupree'],
              dtype='|S18')
    >>> model.preferences_from_user('Sheldom')
    array(['Lady in the Water', 'Snakes on a Plane', 'Superman Returns',
           'The Night Listener', 'You, Me and Dupree'],
          dtype='|S18')
    '''
    def __init__(self, dataset):
        BaseDataModel.__init__(self)
        self.dataset = self._load_dataset(dataset.copy())
        self.build_model()

    def _load_dataset(self, dataset):
        '''
        Returns
        -------
        dataset: dict of shape {user_id:[item_id,item_id2,...]}

        Load the dataset which the input can be the
        {user_id:{item_id:preference,...},...}
        or the {user_id:[item_id,item_id2,...],...}
        '''
        if dataset:
            key = dataset.keys()[0]
            if isinstance(dataset[key], dict):
                for key in dataset:
                    dataset[key] = dataset[key].keys()

        return dataset

    def __getitem__(self, user_id):
        return self.preferences_from_user(user_id)

    def __iter__(self):
        for index, user in enumerate(self.user_ids()):
            yield user, self[user]

    def __len__(self):
        return self.index.shape

    def build_model(self):
        '''
        Returns
        -------
        self:
             Build the data model
        '''

        self._user_ids = np.asanyarray(self.dataset.keys())
        self._user_ids.sort()

        self._item_ids = np.array([])
        for items in self.dataset.itervalues():
            self._item_ids = np.append(self._item_ids, items)

        self._item_ids = np.unique(self._item_ids)
        self._item_ids.sort()

        logger.info("creating matrix for %d users and %d items" % \
                    (self._user_ids.size, self._item_ids.size))

        self.index = np.empty(shape=(self._user_ids.size, self._item_ids.size), dtype=bool)
        for userno, user_id in enumerate(self._user_ids):
            if userno % 2 == 0:
                logger.debug("PROGRESS: at user_id #%i/%i" %  \
                    (userno, self._user_ids.size))
            for itemno, item_id in enumerate(self._item_ids):
                r = True if item_id in self.dataset[user_id] else False
                self.index[userno, itemno] = r

    def user_ids(self):
        '''
        Returns
        -------
        self.user_ids:  numpy array of shape [n_user_ids]
                        Return all user ids in the model, in order
        '''
        return self._user_ids

    def item_ids(self):
        '''
        Returns
        -------
        self.item_ids:  numpy array of shape [n_item_ids]
                    Return all item ids in the model, in order
        '''
        return self._item_ids

    def preference_values_from_user(self, user_id):
        '''
        Returns
        --------
        Return user's preferences values as an array

        Notes
        --------
        This method is a particular method in MatrixDataModel
        '''
        user_id_loc = np.where(self._user_ids == user_id)
        if not user_id_loc[0].size:
            #user_id not found
            raise UserNotFoundError

        preferences = self.index[user_id_loc]

        return preferences

    def preferences_from_user(self, user_id, order_by_id=True):
        '''
        Returns
        -------
        self.user_preferences :  list [(item_id,preference)]
         Return user's preferences, ordered by user ID (if order_by_id is True)
         or by the preference values (if order_by_id is False), as an array.

        '''
        preferences = self.preference_values_from_user(user_id)

        preferences = preferences.flatten()

        return self._item_ids[preferences]

    def has_preference_values(self):
        '''
        Returns
        -------
        True/False:  bool
                     Return True if this implementation actually
                     it is not a 'boolean' data model, otherwise returns False.
        '''
        return False

    def users_count(self):
        '''
        Returns
        --------
        n_users:  int
                  Return total number of users known to the model.
        '''
        return self._user_ids.size

    def items_count(self):
        '''
        Returns
        --------
        n_items:  int
                  Return total number of items known to the model.
        '''
        return self._item_ids.size

    def items_from_user(self, user_id):
        '''
        Returns
        -------
        items_from_user : numpy array of shape [item_id,..]
                 Return IDs of items user expresses a preference for
        '''
        preferences = self.preferences_from_user(user_id)
        return preferences

    def preferences_for_item(self, item_id, order_by_id=True):
        '''
        Returns
        -------
        preferences: numpy array of shape [(item_id,preference)]
                     Return all existing Preferences expressed for that item,
        '''
        item_id_loc = np.where(self._item_ids == item_id)
        if not item_id_loc[0].size:
            #item_id not found
            raise ItemNotFoundError('Item not found')
        preferences = self.index[:, item_id_loc]

        preferences = preferences.flatten()

        return self._user_ids[preferences]

    def preference_value(self, user_id, item_id):
        '''
        Returns
        -------
        preference:  float
                     Retrieves the preference value for a single user and item.
        '''
        item_id_loc = np.where(self._item_ids == item_id)
        user_id_loc = np.where(self._user_ids == user_id)

        if not user_id_loc[0].size:
            raise UserNotFoundError('user_id in the model not found')

        if not item_id_loc[0].size:
            raise ItemNotFoundError('item_id in the model not found')

        return 1.0 if self.index[user_id_loc, item_id_loc].flatten()[0] else np.NaN

    def set_preference(self, user_id, item_id, value=None):
        '''
        Returns
        --------
        self
            Sets a particular preference (item plus rating) for a user.
        '''
        user_id_loc = np.where(self._user_ids == user_id)
        if not user_id_loc[0].size:
            raise UserNotFoundError('user_id in the model not found')

        #ALLOW NEW ITEMS
        #if not item_id_loc[0].size:
        #    raise ItemNotFoundError('item_id in the model not found')

        #How not use the dataset in memory ?!
        self.dataset[user_id].append(item_id)
        self.build_model()

    def remove_preference(self, user_id, item_id):
        '''
        Returns
        --------
        self
            Removes a particular preference for a user.
        '''
        user_id_loc = np.where(self._user_ids == user_id)
        item_id_loc = np.where(self._item_ids == item_id)

        if not user_id_loc[0].size:
            raise UserNotFoundError('user_id in the model not found')

        if not item_id_loc[0].size:
            raise ItemNotFoundError('item_id in the model not found')

        self.dataset[user_id].remove(item_id)
        self.build_model()

    def maximum_preference_value(self):
        '''
        Returns
        ---------
        self.max_preference:  float
                Return the maximum preference value that is possible in the
                 current problem domain being evaluated.
        '''
        return 1.0

    def minimum_preference_value(self):
        '''
        Returns
        ---------
        self.min_preference:  float
                Returns the minimum preference value that is possible in the
                current problem domain being evaluated
        '''
        return 0.0

    def __repr__(self):
        return "<MatrixBooleanPrefDataModel (%d by %d)>" % (self.index.shape[0],
                        self.index.shape[1])

    def _repr_matrix(self, matrix):
        s = ""
        cellWidth = 11
        shape = matrix.shape
        for i in range(shape[0]):
            for j in range(shape[1]):
                v = matrix[i, j]
                if not v:
                    s += "---".center(cellWidth)
                else:
                    exp = np.log(abs(v))
                    if abs(exp) <= 4:
                        if exp < 0:
                            s += ("%9.6f" % v).ljust(cellWidth)
                        else:
                            s += ("%9.*f" % (6, v)).ljust(cellWidth)
                    else:
                        s += ("%9.2e" % v).ljust(cellWidth)
            s += "\n"
        return s[:-1]

    def __unicode__(self):
        """
        Write out a representative picture of this matrix.

        The upper left corner of the matrix will be shown, with up to 20x5
        entries, and the rows and columns will be labeled with up to 8
        characters.
        """
        matrix = self._repr_matrix(self.index[:20, :5])
        lines = matrix.split('\n')
        headers = [repr(self)[1:-1]]
        if self._item_ids.size:
            col_headers = [('%-8s' % unicode(item)[:8]) for item in self._item_ids[:5]]
            headers.append(' ' + ('   '.join(col_headers)))

        if self._user_ids.size:
            for (i, line) in enumerate(lines):
                lines[i] = ('%-8s' % unicode(self._user_ids[i])[:8]) + line
            for (i, line) in enumerate(headers):
                if i > 0:
                    headers[i] = ' ' * 8 + line
        lines = headers + lines
        if self.index.shape[1] > 5 and self.index.shape[0] > 0:
            lines[1] += ' ...'
        if self.index.shape[0] > 20:
            lines.append('...')

        return '\n'.join(line.rstrip() for line in lines)

    def __str__(self):
        return unicode(self).encode('utf-8')

########NEW FILE########
__FILENAME__ = test_models
import numpy as np
from numpy.testing import assert_array_equal
from nose.tools import assert_raises, assert_equals

from ..classes import  MatrixPreferenceDataModel,  \
             MatrixBooleanPrefDataModel
from ..utils import UserNotFoundError, ItemNotFoundError
from ...datasets import load_sample_songs


#Simple Movies DataSet
movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,
 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,
 'The Night Listener': 3.0},
'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,
 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 3.5},
'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,
 'Superman Returns': 3.5, 'The Night Listener': 4.0},
'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,
 'The Night Listener': 4.5, 'Superman Returns': 4.0,
 'You, Me and Dupree': 2.5},
'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 2.0},
'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},
'Penny Frewman': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0, 'Superman Returns': 4.0},
'Maria Gabriela': {}}


movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,
 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,
 'The Night Listener': 3.0},
'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,
 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 3.5},
'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,
 'Superman Returns': 3.5, 'The Night Listener': 4.0},
'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,
 'The Night Listener': 4.5, 'Superman Returns': 4.0,
 'You, Me and Dupree': 2.5},
'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 2.0},
'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},
'Penny Frewman': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0, 'Superman Returns': 4.0},
'Maria Gabriela': {}}


def test_basic_methods_MatrixPreferenceDataModel():
    #Empty Dataset
    model = MatrixPreferenceDataModel({})
    assert_equals(model.dataset, {})
    assert_array_equal(np.array([]), model.user_ids())
    assert_array_equal(np.array([]), model.item_ids())
    assert_equals(True, model.has_preference_values())
    assert_equals(0, model.users_count())
    assert_equals(0, model.items_count())
    assert_equals(-np.inf, model.maximum_preference_value())
    assert_equals(np.inf, model.minimum_preference_value())

    assert("MatrixPreferenceDataModel (0 by 0)" in model.__str__())

    #Simple DataSet
    model = MatrixPreferenceDataModel(movies)
    assert_equals(model.dataset, movies)
    assert_array_equal(np.array(['Leopoldo Pires', 'Lorena Abreu', 'Luciana Nunes',
      'Marcel Caraciolo', 'Maria Gabriela', 'Penny Frewman', 'Sheldom', 'Steve Gates']),
      model.user_ids())
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
               'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), model.item_ids())
    assert_equals(True, model.has_preference_values())
    assert_equals(8, model.users_count())
    assert_equals(6, model.items_count())
    assert_equals(5.0, model.maximum_preference_value())
    assert_equals(1.0, model.minimum_preference_value())
    assert_equals([('Just My Luck', 3.0), ('Lady in the Water', 2.5),
             ('Snakes on a Plane', 3.5), ('Superman Returns', 3.5),
             ('The Night Listener', 3.0), ('You, Me and Dupree', 2.5)], model['Marcel Caraciolo'])
    elements = [pref  for pref in model]
    assert_array_equal([('Lady in the Water', 2.5), ('Snakes on a Plane', 3.0), \
         ('Superman Returns', 3.5), ('The Night Listener', 4.0)], elements[0][1])

    assert("MatrixPreferenceDataModel (8 by 6)" in model.__str__())

    #SampleSongs DataSet
    songs = load_sample_songs()
    model = MatrixPreferenceDataModel(songs.data)
    assert_equals(model.dataset, songs.data)
    assert_array_equal(np.array([1, 2, 3, 4, 5, 6, 7, 8]),
      model.user_ids())
    assert_array_equal(np.array([1, 2, 3, 4, 5, 6, 7, 8]), model.item_ids())
    assert_equals(True, model.has_preference_values())
    assert_equals(8, model.users_count())
    assert_equals(8, model.items_count())
    assert_equals(5.0, model.maximum_preference_value())
    assert_equals(1.0, model.minimum_preference_value())
    assert_equals([(1, 2.5), (2, 3.5), (3, 5.0), (4, 2.0), (5, 4.5), (6, 1.5), (7, 2.0)], model[1])
    elements = [pref  for pref in model]
    assert_array_equal([(1, 2.5), (2, 3.5), (3, 5.0), (4, 2.0), (5, 4.5), (6, 1.5), (7, 2.0)],
            elements[0][1])
    assert("MatrixPreferenceDataModel (8 by 8)" in model.__str__())


def test_preferences_from_user_exists_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    #ordered by item_id
    assert_array_equal(np.array([('Just My Luck', 3.0), ('Snakes on a Plane', 3.5),
       ('Superman Returns', 4.0), ('The Night Listener', 4.5), ('You, Me and Dupree', 2.5)]),
        model.preferences_from_user('Lorena Abreu'))

    #ordered by rating (reverse)
    assert_array_equal(np.array([('The Night Listener', 4.5), ('Superman Returns', 4.0), \
       ('Snakes on a Plane', 3.5), ('Just My Luck', 3.0), ('You, Me and Dupree', 2.5)]), \
          model.preferences_from_user('Lorena Abreu', order_by_id=False))


def test_preferences_from_user_exists_no_preferences_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    assert_array_equal(np.array([]), model.preferences_from_user('Maria Gabriela'))


def test_preferences_from_user_non_existing_user_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    assert_raises(UserNotFoundError, model.preferences_from_user, 'Flavia')


def test_item_ids_from_user_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
           'Superman Returns', 'The Night Listener', 'You, Me and Dupree']),
      model.items_from_user('Marcel Caraciolo'))


def test_preferences_for_item_existing_item_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    #ordered by item_id
    assert_array_equal(np.array([('Leopoldo Pires', 3.5), ('Lorena Abreu', 4.0), \
           ('Luciana Nunes', 5.0), ('Marcel Caraciolo', 3.5), \
           ('Penny Frewman', 4.0), ('Sheldom', 5.0), ('Steve Gates', 3.0)]),
       model.preferences_for_item('Superman Returns'))
    #ordered by rating (reverse)
    assert_array_equal(np.array([('Luciana Nunes', 5.0), ('Sheldom', 5.0), ('Lorena Abreu', 4.0), \
           ('Penny Frewman', 4.0), ('Leopoldo Pires', 3.5), \
           ('Marcel Caraciolo', 3.5), ('Steve Gates', 3.0)]),
           model.preferences_for_item('Superman Returns', order_by_id=False))


def test_preferences_for_item_existing_item_no_preferences_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    assert_array_equal(np.array([]), model.preferences_for_item, 'The Night Listener')


def test_preferences_for_item_non_existing_item_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    assert_raises(ItemNotFoundError, model.preferences_for_item, 'Back to the future')


def test_preference_value_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    assert_equals(3.5, model.preference_value('Marcel Caraciolo', 'Superman Returns'))


def test_preference_value__invalid_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    assert_raises(UserNotFoundError, model.preference_value, 'Flavia', 'Superman Returns')
    assert_raises(ItemNotFoundError, model.preference_value, 'Marcel Caraciolo', 'Back to the future')
    assert_array_equal(np.nan, model.preference_value('Maria Gabriela', 'The Night Listener'))


def test_set_preference_value_MatrixPreferenceDataModel():
    #Add
    model = MatrixPreferenceDataModel(movies)
    model.set_preference('Maria Gabriela', 'Superman Returns', 2.0)
    assert_equals(2.0, model.preference_value('Maria Gabriela', 'Superman Returns'))
    #Edit
    model = MatrixPreferenceDataModel(movies)
    model.set_preference('Marcel Caraciolo', 'Superman Returns', 1.0)
    assert_equals(1.0, model.preference_value('Marcel Caraciolo', 'Superman Returns'))
    #invalid
    assert_raises(UserNotFoundError, model.set_preference, 'Carlos', 'Superman Returns', 2.0)
    #assert_raises(ItemNotFoundError,model.set_preference,'Marcel Caraciolo','Indiana Jones', 1.0)


def test_remove_preference_value_MatrixPreferenceDataModel():
    model = MatrixPreferenceDataModel(movies)
    model.remove_preference('Maria Gabriela', 'Superman Returns')
    assert_array_equal(np.nan, model.preference_value('Maria Gabriela', 'Superman Returns'))
    assert_raises(ItemNotFoundError, model.remove_preference, 'Marcel Caraciolo', 'Indiana Jones')

movies_boolean = {
'Marcel Caraciolo': ['Lady in the Water', 'Snakes on a Plane',
 'Just My Luck', 'Superman Returns', 'You, Me and Dupree',
 'The Night Listener'],
'Luciana Nunes': ['Lady in the Water', 'Snakes on a Plane',
 'Just My Luck', 'Superman Returns', 'The Night Listener',
 'You, Me and Dupree'],
'Leopoldo Pires': ['Lady in the Water', 'Snakes on a Plane',
 'Superman Returns', 'The Night Listener'],
'Lorena Abreu': ['Snakes on a Plane', 'Just My Luck',
 'The Night Listener', 'Superman Returns',
 'You, Me and Dupree'],
'Steve Gates': ['Lady in the Water', 'Snakes on a Plane',
 'Just My Luck', 'Superman Returns', 'The Night Listener',
 'You, Me and Dupree'],
'Sheldom': ['Lady in the Water', 'Snakes on a Plane',
 'The Night Listener', 'Superman Returns', 'You, Me and Dupree'],
'Penny Frewman': ['Snakes on a Plane', 'You, Me and Dupree', 'Superman Returns'],
'Maria Gabriela': []
}

songs_boolean = {1: [1, 2, 3, 4, 5, 6, 7], 2: [1, 2, 3, 5, 6],
                3: [1, 2, 3, 4, 5, 6], 4: [1, 3, 4, 5, 6, 7, 8],
                5: [1, 2, 3, 4, 6, 7, 8], 6: [2, 3, 4, 6, 7, 8],
                7: [2, 3, 4, 5, 6, 8], 8: [8, 1, 4, 5, 7]}


def test_basic_methods_MatrixBooleanPrefDataModel():
    #Empty Dataset
    model = MatrixBooleanPrefDataModel({})
    assert_equals(model.dataset, {})
    assert_array_equal(np.array([]), model.user_ids())
    assert_array_equal(np.array([]), model.item_ids())
    assert_equals(False, model.has_preference_values())
    assert_equals(0, model.users_count())
    assert_equals(0, model.items_count())
    assert("MatrixBooleanPrefDataModel (0 by 0)" in model.__str__())

    #Simple DataSet
    model = MatrixBooleanPrefDataModel(movies_boolean)
    assert_equals(model.dataset, movies_boolean)
    assert_array_equal(np.array(['Leopoldo Pires', 'Lorena Abreu', 'Luciana Nunes',
      'Marcel Caraciolo', 'Maria Gabriela', 'Penny Frewman', 'Sheldom', 'Steve Gates']),
      model.user_ids())
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
               'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), model.item_ids())
    assert_equals(False, model.has_preference_values())
    assert_equals(8, model.users_count())
    assert_equals(6, model.items_count())
    assert_array_equal(['Just My Luck', 'Lady in the Water',
             'Snakes on a Plane', 'Superman Returns',
             'The Night Listener', 'You, Me and Dupree'], model['Marcel Caraciolo'])
    elements = [pref  for pref in model]
    assert_array_equal(['Lady in the Water', 'Snakes on a Plane', \
         'Superman Returns', 'The Night Listener'], elements[0][1])
    assert("MatrixBooleanPrefDataModel (8 by 6)" in model.__str__())

    songs = load_sample_songs()
    model = MatrixBooleanPrefDataModel(songs.data)
    assert_equals(model.dataset, songs_boolean)
    assert_array_equal(np.array([1, 2, 3, 4, 5, 6, 7, 8]),
      model.user_ids())
    assert_array_equal(np.array([1, 2, 3, 4, 5, 6, 7, 8]), model.item_ids())
    assert_equals(False, model.has_preference_values())
    assert_equals(8, model.users_count())
    assert_equals(8, model.items_count())
    assert_equals(1.0, model.maximum_preference_value())
    assert_equals(0.0, model.minimum_preference_value())
    assert_array_equal([1, 2, 3, 4, 5, 6, 7], model[1])
    elements = [pref  for pref in model]
    assert_array_equal([1, 2, 3, 4, 5, 6, 7],
            elements[0][1])
    assert("MatrixBooleanPrefDataModel (8 by 8)" in model.__str__())


def test_preferences_from_user_exists_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    #ordered by item_id
    assert_array_equal(np.array(['Just My Luck', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']),
        model.preferences_from_user('Lorena Abreu'))

    #ordered by rating (reverse)
    assert_array_equal(np.array(['Just My Luck', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']),
        model.preferences_from_user('Lorena Abreu', order_by_id=False))


def test_preferences_from_user_exists_no_preferences_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    assert_array_equal(np.array([],
      dtype='|S18'), model.preferences_from_user('Maria Gabriela'))


def test_preferences_from_user_non_existing_user_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    assert_raises(UserNotFoundError, model.preferences_from_user, 'Flavia')


def test_item_ids_from_user_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
           'Superman Returns', 'The Night Listener', 'You, Me and Dupree']),
      model.items_from_user('Marcel Caraciolo'))


def test_preferences_for_item_existing_item_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    #ordered by item_id
    assert_array_equal(np.array(['Leopoldo Pires', 'Lorena Abreu', \
           'Luciana Nunes', 'Marcel Caraciolo', \
           'Penny Frewman', 'Sheldom', 'Steve Gates']),
       model.preferences_for_item('Superman Returns'))
    #ordered by rating (reverse)
    assert_array_equal(np.array(['Leopoldo Pires', 'Lorena Abreu', \
           'Luciana Nunes', 'Marcel Caraciolo', \
           'Penny Frewman', 'Sheldom', 'Steve Gates']),
                model.preferences_for_item('Superman Returns', order_by_id=False))


def test_preferences_for_item_existing_item_no_preferences_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    assert_array_equal(np.array([]), model.preferences_for_item, 'The Night Listener')


def test_preferences_for_item_non_existing_item_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    assert_raises(ItemNotFoundError, model.preferences_for_item, 'Back to the future')


def test_preference_value_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    assert_equals(1.0, model.preference_value('Marcel Caraciolo', 'Superman Returns'))


def test_preference_value__invalid_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    assert_raises(UserNotFoundError, model.preference_value, 'Flavia', 'Superman Returns')
    assert_raises(ItemNotFoundError, model.preference_value, 'Marcel Caraciolo', 'Back to the future')
    assert_array_equal(np.NaN, model.preference_value('Maria Gabriela', 'The Night Listener'))


def test_set_preference_value_MatrixBooleanPrefDataModel():
    #Add
    model = MatrixBooleanPrefDataModel(movies_boolean)
    model.set_preference('Maria Gabriela', 'Superman Returns')
    assert_equals(1.0, model.preference_value('Maria Gabriela', 'Superman Returns'))
    #Edit
    model = MatrixBooleanPrefDataModel(movies_boolean)
    model.set_preference('Marcel Caraciolo', 'Superman Returns')
    assert_equals(1.0, model.preference_value('Marcel Caraciolo', 'Superman Returns'))
    #invalid
    assert_raises(UserNotFoundError, model.set_preference, 'Carlos', 'Superman Returns')
    #assert_raises(ItemNotFoundError,model.set_preference,'Marcel Caraciolo','Indiana Jones', 1.0)


def test_remove_preference_value_MatrixBooleanPrefDataModel():
    model = MatrixBooleanPrefDataModel(movies_boolean)
    model.remove_preference('Maria Gabriela', 'Superman Returns')
    assert_array_equal(np.NaN, model.preference_value('Maria Gabriela', 'Superman Returns'))
    assert_raises(ItemNotFoundError, model.remove_preference, 'Marcel Caraciolo', 'Indiana Jones')

########NEW FILE########
__FILENAME__ = utils
#-*- coding:utf-8 -*-

"""
Utilities functions and classes for models.

"""
#Authors: Marcel Caraciolo <marcel@muricoca.com>
#License: BSD Style


class UserNotFoundError(Exception):
    pass


class ItemNotFoundError(Exception):
    pass

########NEW FILE########
__FILENAME__ = base
"""
Generalized Recommender models.

This module contains basic recommender interfaces used throughout
the whole scikit-crab package.

The interfaces are realized as abstract base classes (ie., some optional
functionality is provided in the interface itself, so that the interfaces
can be subclassed).

"""

# Author: Marcel Caraciolo <marcel@muricoca.com>
#
# License: BSD Style.

from ..base import BaseRecommender

#===========================
#Memory Based Recommender


class MemoryBasedRecommender(BaseRecommender):
    pass

########NEW FILE########
__FILENAME__ = base
"""
Generalized Recommender models amd utility classes.

This module contains basic memory recommender interfaces used throughout
the whole scikit-crab package as also utility classes.

The interfaces are realized as abstract base classes (ie., some optional
functionality is provided in the interface itself, so that the interfaces
can be subclassed).

"""

# Author: Marcel Caraciolo <marcel@muricoca.com>
#
# License: BSD Style.

from ..base import MemoryBasedRecommender

#===========================
#Item-based Recommender Interface


class ItemRecommender(MemoryBasedRecommender):

    def most_similar_items(self, item_id, how_many=None):
        '''
        Return the most similar items to the given item, ordered
        from most similar to least.

        Parameters
        -----------
        item_id:  int or string
            ID of item for which to find most similar other items

        how_many: int
            Desired number of most similar items to find
        '''
        raise NotImplementedError("ItemRecommender is an abstract class.")

    def recommended_because(self, user_id, item_id, how_many, **params):
        '''
        Returns the items that were most influential in recommending a given item
        to a given user. In most implementations, this method will return items
        that the user prefers and that are similar to the given item.

        Parameters
        -----------
        user_id : int or string
            ID of the user who was recommended the item

        item_id: int or string
            ID of item that was recommended

        how_many: int
            Maximum number of items to return.

        Returns
        ----------
        The list of items ordered from most influential in recommended the given item to least
        '''
        raise NotImplementedError("ItemRecommender is an abstract class.")


#===========================
#User-based Recommender Interface


class UserRecommender(MemoryBasedRecommender):

    def most_similar_users(self, user_id, how_many=None):
        '''
        Return the most similar users to the given user, ordered
        from most similar to least.

        Parameters
        -----------
        user_id:  int or string
            ID of user for which to find most similar other users

        how_many: int
            Desired number of most similar users to find
        '''
        raise NotImplementedError("UserRecommender is an abstract class.")

    def recommended_because(self, user_id, item_id, how_many, **params):
        '''
        Returns the users that were most influential in recommending a given item
        to a given user. In most implementations, this method will return users
        that prefers the recommended item and that are similar to the given user.

        Parameters
        -----------
        user_id : int or string
            ID of the user who was recommended the item

        item_id: int or string
            ID of item that was recommended

        how_many: int
            Maximum number of items to return.

        Returns
        ----------
        The list of users ordered from most influential in recommended the given item to least
        '''
        raise NotImplementedError("UserRecommender is an abstract class.")

#===========================
# Base Item Candidate Strategy


class BaseCandidateItemsStrategy(object):
    '''
    Base implementation for retrieving
    all items that could possibly be recommended to the user
    '''

    def candidate_items(self, user_id, data_model, **params):
        '''
        Return the candidate items that could possibly be recommended to the user

        Parameters
        -----------
        user_id:  int or string
            ID of user for which to find most similar other users

        data_model: The data model that will be the source for the possible
            candidates
        '''
        raise NotImplementedError("BaseCandidateItemsStrategy is an abstract class.")


#===========================
# Base User Candidates Strategies

class BaseUserNeighborhoodStrategy(object):
    '''
    Base implementation for retrieving
    all users that could possibly be select as part of the neighborhood.
    '''

    def user_neighborhood(self, user_id, data_model, n_similarity='user_similarity',
                distance=None, n_users=None, **params):
        '''
        Computes a neighborhood consisting of the  n users to a given user based on the
        strategy implemented in this method.
        Parameters
        -----------
        user_id:  int or string
            ID of user for which to find most similar other users

        data_model: DataModel instance
            The data model that will be the source for the possible
            candidates

        n_similarity: string
            The similarity to compute the neighborhood (default = user_similarity)

        distance: function
            Pairwise metric to compute the similarity between the users.

        nhood_size: int
            The neighborhood size (default = None all users)

        '''
        raise NotImplementedError("BaseCandidateItemsStrategy is an abstract class.")

########NEW FILE########
__FILENAME__ = classes
"""
Generalized Recommender models.

This module contains basic memory recommender interfaces used throughout
the whole scikit-crab package.

The interfaces are realized as abstract base classes (ie., some optional
functionality is provided in the interface itself, so that the interfaces
can be subclassed).

"""

# Author: Marcel Caraciolo <marcel@muricoca.com>
#
# License: BSD Style.

from base import ItemRecommender, UserRecommender
from item_strategies import ItemsNeighborhoodStrategy
from neighborhood_strategies import NearestNeighborsStrategy
import numpy as np


class ItemBasedRecommender(ItemRecommender):
    """
    Item Based Collaborative Filtering Recommender.


    Parameters
    -----------
    data_model: The data model instance that will be data source
         for the recommender.

    similarity: The Item Similarity instance that will be used to
        score the items that will be recommended.

    items_selection_strategy: The item candidates strategy that you
     can choose for selecting the possible items to recommend.
     default = ItemsNeighborhoodStrategy

    capper: bool (default=True)
        Cap the preferences with maximum and minimum preferences
        in the model.
    with_preference: bool (default=False)
        Return the recommendations with the estimated preferences if True.

    Attributes
    -----------
    `model`: The data model instance that will be data source
         for the recommender.

    `similarity`: The Item Similarity instance that will be used to
        score the items that will be recommended.

    `items_selection_strategy`: The item candidates strategy that you
         can choose for selecting the possible items to recommend.
         default = ItemsNeighborhoodStrategy

    `capper`: bool (default=True)
        Cap the preferences with maximum and minimum preferences
        in the model.
    `with_preference`: bool (default=False)
        Return the recommendations with the estimated preferences if True.

    Examples
    -----------
    >>> from scikits.crab.models.classes import MatrixPreferenceDataModel
    >>> from scikits.crab.recommenders.knn.classes import ItemBasedRecommender
    >>> from scikits.crab.similarities.basic_similarities import ItemSimilarity
    >>> from scikits.crab.recommenders.knn.item_strategies import ItemsNeighborhoodStrategy
    >>> from scikits.crab.metrics.pairwise import euclidean_distances
    >>> movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, \
     'Snakes on a Plane': 3.5, \
     'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \
     'The Night Listener': 3.0}, \
     'Paola Pow': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \
     'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 3.5}, \
    'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, \
     'Superman Returns': 3.5, 'The Night Listener': 4.0}, \
    'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, \
     'The Night Listener': 4.5, 'Superman Returns': 4.0, \
     'You, Me and Dupree': 2.5}, \
    'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 2.0}, \
    'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'The Night Listener': 3.0, 'Superman Returns': 5.0, \
     'You, Me and Dupree': 3.5}, \
    'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0, \
    'Superman Returns':4.0}, \
    'Maria Gabriela': {}}
    >>> model = MatrixPreferenceDataModel(movies)
    >>> items_strategy = ItemsNeighborhoodStrategy()
    >>> similarity = ItemSimilarity(model, euclidean_distances)
    >>> recsys = ItemBasedRecommender(model, similarity, items_strategy)
    >>> #Return the recommendations for the given user.
    >>> recsys.recommend('Leopoldo Pires')
    ['Just My Luck', 'You, Me and Dupree']
    >>> #Return the 2 explanations for the given recommendation.
    >>> recsys.recommended_because('Leopoldo Pires', 'Just My Luck',2)
    ['The Night Listener', 'Superman Returns']

    Notes
    -----------
    This ItemBasedRecommender does not yet provide
    suppot for rescorer functions.

    References
    -----------
    Item-based collaborative filtering recommendation algorithms by Sarwar
    http://portal.acm.org/citation.cfm?id=372071

    """

    def __init__(self, model, similarity, items_selection_strategy=None,
                capper=True, with_preference=False):
        ItemRecommender.__init__(self, model, with_preference)
        self.similarity = similarity
        self.capper = capper
        if items_selection_strategy is None:
            self.items_selection_strategy = ItemsNeighborhoodStrategy()
        else:
            self.items_selection_strategy = items_selection_strategy

    def recommend(self, user_id, how_many=None, **params):
        '''
        Return a list of recommended items, ordered from most strongly
        recommend to least.

        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.
        how_many: int
                 Desired number of recommendations (default=None ALL)

        '''
        self._set_params(**params)

        candidate_items = self.all_other_items(user_id)

        recommendable_items = self._top_matches(user_id, \
                 candidate_items, how_many)

        return recommendable_items

    def estimate_preference(self, user_id, item_id, **params):
        '''
        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.

        item_id:  int or string
            ID of item for which wants to find the estimated preference.

        Returns
        -------
        Return an estimated preference if the user has not expressed a
        preference for the item, or else the user's actual preference for the
        item. If a preference cannot be estimated, returns None.
        '''
        preference = self.model.preference_value(user_id, item_id)

        if not np.isnan(preference):
            return preference

        #TODO: It needs optimization
        prefs = self.model.preferences_from_user(user_id)

        if not self.model.has_preference_values():
            prefs = [(pref, 1.0) for pref in prefs]

        similarities = \
            np.array([self.similarity.get_similarity(item_id, to_item_id) \
            for to_item_id, pref in prefs if to_item_id != item_id]).flatten()

        prefs = np.array([pref for it, pref in prefs])
        prefs_sim = np.sum(prefs[~np.isnan(similarities)] *
                             similarities[~np.isnan(similarities)])
        total_similarity = np.sum(similarities)

        #Throw out the estimate if it was based on no data points,
        #of course, but also if based on
        #just one. This is a bit of a band-aid on the 'stock'
        #item-based algorithm for the moment.
        #The reason is that in this case the estimate is, simply,
        #the user's rating for one item
        #that happened to have a defined similarity.
        #The similarity score doesn't matter, and that
        #seems like a bad situation.
        if total_similarity == 0.0 or \
           not similarities[~np.isnan(similarities)].size:
            return np.nan

        estimated = prefs_sim / total_similarity

        if self.capper:
            max_p = self.model.maximum_preference_value()
            min_p = self.model.minimum_preference_value()
            estimated = max_p if estimated > max_p else min_p \
                     if estimated < min_p else estimated
        return estimated

    def all_other_items(self, user_id, **params):
        '''
        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.

        Returns
        ---------
        Return items in the `model` for which the user has not expressed
        the preference and could possibly be recommended to the user.

        '''
        return self.items_selection_strategy.candidate_items(user_id, \
                            self.model)

    def _top_matches(self, source_id, target_ids, how_many=None, **params):
        '''
        Parameters
        ----------
        target_ids: array of shape [n_target_ids]

        source_id: int or string
                item id to compare against.

        how_many: int
            Desired number of most top items to recommend (default=None ALL)

        Returns
        --------
        Return the top N matches
        It can be user_ids or item_ids.
        '''
        #Empty target_ids
        if target_ids.size == 0:
            return np.array([])

        estimate_preferences = np.vectorize(self.estimate_preference)

        preferences = estimate_preferences(source_id, target_ids)

        preference_values = preferences[~np.isnan(preferences)]
        target_ids = target_ids[~np.isnan(preferences)]

        sorted_preferences = np.lexsort((preference_values,))[::-1]

        sorted_preferences = sorted_preferences[0:how_many] \
             if how_many and sorted_preferences.size > how_many \
                else sorted_preferences

        if self.with_preference:
            top_n_recs = [(target_ids[ind], \
                     preferences[ind]) for ind in sorted_preferences]
        else:
            top_n_recs = [target_ids[ind]
                 for ind in sorted_preferences]

        return top_n_recs

    def most_similar_items(self, item_id, how_many=None):
        '''
        Return the most similar items to the given item, ordered
        from most similar to least.

        Parameters
        -----------
        item_id:  int or string
            ID of item for which to find most similar other items

        how_many: int
            Desired number of most similar items to find (default=None ALL)
        '''
        old_how_many = self.similarity.num_best
        #+1 since it returns the identity.
        self.similarity.num_best = how_many + 1 \
                    if how_many is not None else None
        similarities = self.similarity[item_id]
        self.similarity.num_best = old_how_many

        return np.array([item for item, pref in similarities \
            if item != item_id and not np.isnan(pref)])

    def recommended_because(self, user_id, item_id, how_many=None, **params):
        '''
        Returns the items that were most influential in recommending a
        given item to a given user. In most implementations, this
        method will return items that the user prefers and that
        are similar to the given item.

        Parameters
        -----------
        user_id : int or string
            ID of the user who was recommended the item

        item_id: int or string
            ID of item that was recommended

        how_many: int
            Maximum number of items to return (default=None ALL)

        Returns
        ----------
        The list of items ordered from most influential in
        recommended the given item to least
        '''
        preferences = self.model.preferences_from_user(user_id)

        if self.model.has_preference_values():
            similarities = \
                np.array([self.similarity.get_similarity(item_id, to_item_id) \
                    for to_item_id, pref in preferences
                        if to_item_id != item_id]).flatten()
            prefs = np.array([pref for it, pref in preferences])
            item_ids = np.array([it for it, pref in preferences])
        else:
            similarities = \
                np.array([self.similarity.get_similarity(item_id, to_item_id) \
                for to_item_id in preferences
                    if to_item_id != item_id]).flatten()
            prefs = np.array([1.0 for it in preferences])
            item_ids = np.array(preferences)

        scores = prefs[~np.isnan(similarities)] * \
             (1.0 + similarities[~np.isnan(similarities)])

        sorted_preferences = np.lexsort((scores,))[::-1]

        sorted_preferences = sorted_preferences[0:how_many] \
             if how_many and sorted_preferences.size > how_many \
                 else sorted_preferences

        if self.with_preference:
            top_n_recs = [(item_ids[ind], \
                     prefs[ind]) for ind in sorted_preferences]
        else:
            top_n_recs = [item_ids[ind]
                 for ind in sorted_preferences]

        return top_n_recs


#=====================
#User Based Recommender

class UserBasedRecommender(UserRecommender):
    """
    User Based Collaborative Filtering Recommender.


    Parameters
    -----------
    data_model: The data model instance that will be data source
         for the recommender.

    similarity: The User Similarity instance that will be used to
        score the users that are the most similar to the user.

    neighborhood_strategy: The user neighborhood strategy that you
         can choose for selecting the most similar users to find
         the items to recommend.
         default = NearestNeighborsStrategy

    capper: bool (default=True)
        Cap the preferences with maximum and minimum preferences
        in the model.
    with_preference: bool (default=False)
        Return the recommendations with the estimated preferences if True.

    Attributes
    -----------
    `model`: The data model instance that will be data source
         for the recommender.

    `similarity`: The User Similarity instance that will be used to
        score the users that are the most similar to the user.

    `neighborhood_strategy`: The user neighborhood strategy that you
         can choose for selecting the most similar users to find
         the items to recommend.
         default = NearestNeighborsStrategy

    `capper`: bool (default=True)
        Cap the preferences with maximum and minimum preferences
        in the model.
    `with_preference`: bool (default=False)
        Return the recommendations with the estimated preferences if True.

    Examples
    -----------
    >>> from scikits.crab.models.classes import MatrixPreferenceDataModel
    >>> from scikits.crab.recommenders.knn.classes import UserBasedRecommender
    >>> from scikits.crab.similarities.basic_similarities import UserSimilarity
    >>> from scikits.crab.recommenders.knn.neighborhood_strategies import NearestNeighborsStrategy
    >>> from scikits.crab.metrics.pairwise import euclidean_distances
    >>> movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, \
     'Snakes on a Plane': 3.5, \
     'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \
     'The Night Listener': 3.0}, \
     'Paola Pow': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \
     'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 3.5}, \
    'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, \
     'Superman Returns': 3.5, 'The Night Listener': 4.0}, \
    'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, \
     'The Night Listener': 4.5, 'Superman Returns': 4.0, \
     'You, Me and Dupree': 2.5}, \
    'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 2.0}, \
    'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'The Night Listener': 3.0, 'Superman Returns': 5.0, \
     'You, Me and Dupree': 3.5}, \
    'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0, \
    'Superman Returns':4.0}, \
    'Maria Gabriela': {}}
    >>> model = MatrixPreferenceDataModel(movies)
    >>> nhood_strategy = NearestNeighborsStrategy()
    >>> similarity = UserSimilarity(model, euclidean_distances)
    >>> recsys = UserBasedRecommender(model, similarity, nhood_strategy)
    >>> #Return the recommendations for the given user.
    >>> recsys.recommend('Leopoldo Pires')
    ['Just My Luck', 'You, Me and Dupree']
    >>> #Return the 2 explanations for the given recommendation.
    >>> recsys.recommended_because('Leopoldo Pires', 'Just My Luck',2)
    ['Lorena Abreu', 'Marcel Caraciolo']

    Notes
    -----------
    This UserBasedRecommender does not yet provide
    suppot for rescorer functions.

    References
    -----------
    User-based collaborative filtering recommendation algorithms by

    """

    def __init__(self, model, similarity, neighborhood_strategy=None,
                capper=True, with_preference=False):
        UserRecommender.__init__(self, model, with_preference)
        self.similarity = similarity
        self.capper = capper
        if neighborhood_strategy is None:
            self.neighborhood_strategy = NearestNeighborsStrategy()
        else:
            self.neighborhood_strategy = neighborhood_strategy

    def all_other_items(self, user_id, **params):
        '''
        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed. (default= 'user_similarity')

        Optional Parameters
        --------------------
        n_similarity: string
            The similarity used in the neighborhood strategy

        distance: the metrics.pairwise function to set.
                The pairwise function to compute the similarity (default = euclidean_distances)

        nhood_size:  int
            The neighborhood size (default=None  ALL)

        minimal_similarity: float
            minimal similarity required for neighbors (default = 0.0)

        sampling_rate: int
            percentage of users to consider when building neighborhood
                (default = 1)

        Returns
        ---------
        Return items in the `model` for which the user has not expressed
        the preference and could possibly be recommended to the user.

        '''
        n_similarity = params.pop('n_similarity', 'user_similarity')
        distance = params.pop('distance', self.similarity.distance)
        nhood_size = params.pop('nhood_size', None)

        nearest_neighbors = self.neighborhood_strategy.user_neighborhood(user_id,
                self.model, n_similarity, distance, nhood_size, **params)

        items_from_user_id = self.model.items_from_user(user_id)
        possible_items = []
        for to_user_id in nearest_neighbors:
            possible_items.extend(self.model.items_from_user(to_user_id))

        possible_items = np.unique(np.array(possible_items).flatten())

        return np.setdiff1d(possible_items, items_from_user_id)

    def estimate_preference(self, user_id, item_id, **params):
        '''
        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.

        item_id:  int or string
            ID of item for which wants to find the estimated preference.

        Returns
        -------
        Return an estimated preference if the user has not expressed a
        preference for the item, or else the user's actual preference for the
        item. If a preference cannot be estimated, returns None.
        '''

        preference = self.model.preference_value(user_id, item_id)
        if not np.isnan(preference):
            return preference

        n_similarity = params.pop('n_similarity', 'user_similarity')
        distance = params.pop('distance', self.similarity.distance)
        nhood_size = params.pop('nhood_size', None)

        nearest_neighbors = self.neighborhood_strategy.user_neighborhood(user_id,
                self.model, n_similarity, distance, nhood_size, **params)

        preference = 0.0
        total_similarity = 0.0

        similarities = np.array([self.similarity.get_similarity(user_id, to_user_id)
                for to_user_id in nearest_neighbors]).flatten()

        prefs = np.array([self.model.preference_value(to_user_id, item_id)
                 for to_user_id in nearest_neighbors])

        prefs = prefs[~np.isnan(prefs)]
        similarities = similarities[~np.isnan(prefs)]

        prefs_sim = np.sum(prefs[~np.isnan(similarities)] *
                             similarities[~np.isnan(similarities)])
        total_similarity = np.sum(similarities)

        #Throw out the estimate if it was based on no data points,
        #of course, but also if based on just one. This is a bit
        #of a band-aid on the 'stock' item-based algorithm for
        #the moment. The reason is that in this case the estimate
        #is, simply, the user's rating for one item that happened
        #to have a defined similarity. The similarity score doesn't
        #matter, and that seems like a bad situation.
        if total_similarity == 0.0 or \
           not similarities[~np.isnan(similarities)].size:
            return np.nan

        estimated = prefs_sim / total_similarity

        if self.capper:
            max_p = self.model.maximum_preference_value()
            min_p = self.model.minimum_preference_value()
            estimated = max_p if estimated > max_p else min_p \
                     if estimated < min_p else estimated

        return estimated

    def most_similar_users(self, user_id, how_many=None):
        '''
        Return the most similar users to the given user, ordered
        from most similar to least.

        Parameters
        -----------
        user_id:  int or string
            ID of user for which to find most similar other users

        how_many: int
            Desired number of most similar users to find (default=None ALL)
        '''
        old_how_many = self.similarity.num_best
        #+1 since it returns the identity.
        self.similarity.num_best = how_many + 1 \
                    if how_many is not None else None
        similarities = self.similarity[user_id]
        self.similarity.num_best = old_how_many
        return np.array([to_user_id for to_user_id, pref in similarities \
            if user_id != to_user_id and not np.isnan(pref)])

    def recommend(self, user_id, how_many=None, **params):
        '''
        Return a list of recommended items, ordered from most strongly
        recommend to least.

        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.
        how_many: int
                 Desired number of recommendations (default=None ALL)

        '''

        self._set_params(**params)

        candidate_items = self.all_other_items(user_id, **params)

        recommendable_items = self._top_matches(user_id, \
                 candidate_items, how_many)

        return recommendable_items

    def _top_matches(self, source_id, target_ids, how_many=None, **params):
        '''
        Parameters
        ----------
        target_ids: array of shape [n_target_ids]

        source_id: int or string
                item id to compare against.

        how_many: int
            Desired number of most top items to recommend (default=None ALL)

        Returns
        --------
        Return the top N matches
        It can be user_ids or item_ids.
        '''
        #Empty target_ids
        if target_ids.size == 0:
            return np.array([])

        estimate_preferences = np.vectorize(self.estimate_preference)

        preferences = estimate_preferences(source_id, target_ids)

        preference_values = preferences[~np.isnan(preferences)]
        target_ids = target_ids[~np.isnan(preferences)]

        sorted_preferences = np.lexsort((preference_values,))[::-1]

        sorted_preferences = sorted_preferences[0:how_many] \
             if how_many and sorted_preferences.size > how_many \
                else sorted_preferences

        if self.with_preference:
            top_n_recs = [(target_ids[ind], \
                     preferences[ind]) for ind in sorted_preferences]
        else:
            top_n_recs = [target_ids[ind]
                 for ind in sorted_preferences]

        return top_n_recs

    def recommended_because(self, user_id, item_id, how_many=None, **params):
        '''
        Returns the users that were most influential in recommending a
        given item to a given user. In most implementations, this
        method will return users that prefers the recommended item and that
        are similar to the given user.

        Parameters
        -----------
        user_id : int or string
            ID of the user who was recommended the item

        item_id: int or string
            ID of item that was recommended

        how_many: int
            Maximum number of items to return (default=None ALL)

        Returns
        ----------
        The list of items ordered from most influential in
        recommended the given item to least
        '''
        preferences = self.model.preferences_for_item(item_id)

        if self.model.has_preference_values():
            similarities = \
                np.array([self.similarity.get_similarity(user_id, to_user_id) \
                    for to_user_id, pref in preferences
                        if to_user_id != user_id]).flatten()
            prefs = np.array([pref for it, pref in preferences])
            user_ids = np.array([usr for usr, pref in preferences])
        else:
            similarities = \
                np.array([self.similarity.get_similarity(user_id, to_user_id) \
                for to_user_id in preferences
                    if to_user_id != user_id]).flatten()
            prefs = np.array([1.0 for it in preferences])
            user_ids = np.array(preferences)

        scores = prefs[~np.isnan(similarities)] * \
             (1.0 + similarities[~np.isnan(similarities)])

        sorted_preferences = np.lexsort((scores,))[::-1]

        sorted_preferences = sorted_preferences[0:how_many] \
             if how_many and sorted_preferences.size > how_many \
                 else sorted_preferences

        if self.with_preference:
            top_n_recs = [(user_ids[ind], \
                     prefs[ind]) for ind in sorted_preferences]
        else:
            top_n_recs = [user_ids[ind]
                 for ind in sorted_preferences]

        return top_n_recs

########NEW FILE########
__FILENAME__ = item_strategies
"""
Strategies for items selection to be a
possible candidate to be recommended.

Please check the base.BaseCandidateItemsStrategy before
implement your own strategy.

"""

# Author: Marcel Caraciolo <marcel@muricoca.com>
#
# License: BSD Style.

from base import BaseCandidateItemsStrategy
import numpy as np


class AllPossibleItemsStrategy(BaseCandidateItemsStrategy):
    '''
    Returns all items that have not been rated by the user.
    This strategy is not recommended for large datasets and
    it is the dummiest one.
    '''

    def candidate_items(self, user_id, data_model, **params):
        #Get all the item_ids preferred from the user
        preferences = data_model.items_from_user(user_id)
        #Get all posible items from the data_model
        possible_items = data_model.item_ids()
        return np.setdiff1d(possible_items, preferences, assume_unique=True)


class ItemsNeighborhoodStrategy(BaseCandidateItemsStrategy):
    '''
    Returns all items that have not been rated by the user and were
    preferred by another user that has preferred at least one item that the
    current has preferred too.
    '''

    def candidate_items(self, user_id, data_model, **params):
        #Get all the item_ids preferred from the user
        preferences = data_model.items_from_user(user_id)
        possible_items = np.array([])
        for item_id in preferences:
            item_preferences = data_model.preferences_for_item(item_id)
            if data_model.has_preference_values():
                for user_id, score in item_preferences:
                    possible_items = np.append(possible_items, \
                        data_model.items_from_user(user_id))
            else:
                for user_id in item_preferences:
                    possible_items = np.append(possible_items, \
                        data_model.items_from_user(user_id))
        possible_items = np.unique(possible_items)

        return np.setdiff1d(possible_items, preferences, assume_unique=True)

########NEW FILE########
__FILENAME__ = neighborhood_strategies
"""
Strategies for users selection to be a
possible candidate to be member of a user neighborhood.

Please check the base.BaseUserNeighborhoodStrategy before
implement your own strategy.

"""

# Author: Marcel Caraciolo <marcel@muricoca.com>
#
# License: BSD Style.

from base import BaseUserNeighborhoodStrategy
import numpy as np
from ...similarities.basic_similarities import UserSimilarity
from ...metrics.pairwise import euclidean_distances


class AllNeighborsStrategy(BaseUserNeighborhoodStrategy):
    '''
    Returns
    --------
    Returns all users in the model.
    This strategy is not recommended for large datasets and
    it is the dummiest one.
    '''
    def user_neighborhood(self, user_id, data_model, similarity='user_similarity',
        distance=None, nhood_size=None, **params):
        '''
        Computes a neighborhood consisting of the  n users to a given user
        based on the strategy implemented in this method.

        Parameters
        -----------
        user_id:  int or string
            ID of user for which to find most similar other users

        data_model: DataModel instance
            The data model that will be the source for the possible
            candidates

        similarity: string
            The similarity to compute the neighborhood  (default = 'user_similarity')
            |user_similarity'|

        distance: function
            Pairwise metric to compute the similarity between the users.

        nhood_size: int
            The neighborhood size (default = None all users)

        '''
        user_ids = data_model.user_ids()
        return user_ids[user_ids != user_id] if user_ids.size else user_ids


class NearestNeighborsStrategy(BaseUserNeighborhoodStrategy):
    '''
    Returns
    --------
    Returns the neighborhood consisting of the nearest n
    users to a given user. "Nearest" in this context is
    defined by the Similarity.

    Parameters
    -----------
    user_id:  int or string
        ID of user for which to find most similar other users

    data_model: DataModel instance
        The data model that will be the source for the possible
        candidates

    similarity: string
        The similarity to compute the neighborhood  (default = 'user_similarity')
        |user_similarity'|

    distance: function
        Pairwise metric to compute the similarity between the users.

    nhood_size: int
        The neighborhood size (default = None all users)

    '''
    def __init__(self):
        self.similarity = None

    def _sampling(self, data_model, sampling_rate):
        #TODO: Still to be implemented in a best way
        return data_model

    def _set_similarity(self, data_model, similarity, distance, nhood_size):
        if not isinstance(self.similarity, UserSimilarity) \
             or not distance == self.similarity.distance:
            nhood_size = nhood_size if not nhood_size else nhood_size + 1
            self.similarity = UserSimilarity(data_model, distance, nhood_size)

    def user_neighborhood(self, user_id, data_model, n_similarity='user_similarity',
             distance=None, nhood_size=None, **params):
        '''
        Computes a neighborhood consisting of the  n users to a given
        user based on the strategy implemented in this method.
        Parameters
        -----------
        user_id:  int or string
            ID of user for which to find most similar other users

        data_model: DataModel instance
            The data model that will be the source for the possible
            candidates

        n_similarity: string
            The similarity to compute the neighborhood (Default = 'user_similarity')

        nhood_size: int
            The neighborhood size (default = None all users)

        Optional Parameters
        --------------------
        minimal_similarity: float
            minimal similarity required for neighbors (default = 0.0)

        sampling_rate: int
            percentage of users to consider when building neighborhood
                (default = 1)

        '''
        minimal_similarity = params.get('minimal_similarity', 0.0)
        sampling_rate = params.get('sampling_rate', 1.0)

        data_model = self._sampling(data_model, sampling_rate)
        #set the nhood_size at Similarity , and use Similarity to get the top_users
        if distance is None:
            distance = euclidean_distances
        if n_similarity == 'user_similarity':
            self._set_similarity(data_model, n_similarity, distance, nhood_size)
        else:
            raise ValueError('similarity argument must be user_similarity')

        neighborhood = [to_user_id for to_user_id, score in self.similarity[user_id] \
                           if not np.isnan(score) and score >= minimal_similarity and user_id != to_user_id]

        return neighborhood

########NEW FILE########
__FILENAME__ = test_classes
import numpy as np
from numpy.testing import assert_array_equal
from nose.tools import assert_raises, assert_equals, assert_almost_equals
from ....models.classes import  MatrixPreferenceDataModel, \
     MatrixBooleanPrefDataModel
from ..item_strategies import ItemsNeighborhoodStrategy, AllPossibleItemsStrategy
from ..neighborhood_strategies import AllNeighborsStrategy, NearestNeighborsStrategy
from ....similarities.basic_similarities import ItemSimilarity, UserSimilarity
from ..classes import ItemBasedRecommender, UserBasedRecommender
from ....models.utils import ItemNotFoundError, UserNotFoundError
from ....metrics.pairwise import euclidean_distances, jaccard_coefficient, pearson_correlation


movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,
 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,
 'The Night Listener': 3.0},
'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,
 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 3.5},
'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,
 'Superman Returns': 3.5, 'The Night Listener': 4.0},
'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,
 'The Night Listener': 4.5, 'Superman Returns': 4.0,
 'You, Me and Dupree': 2.5},
'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 2.0},
'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},
'Penny Frewman': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0, 'Superman Returns': 4.0},
'Maria Gabriela': {}}

matrix_model = MatrixPreferenceDataModel(movies)
boolean_matrix_model = MatrixBooleanPrefDataModel(movies)


def test_create_ItemBasedRecommender():
    items_strategy = AllPossibleItemsStrategy()
    similarity = ItemSimilarity(matrix_model, euclidean_distances)
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_equals(recsys.similarity, similarity)
    assert_equals(recsys.items_selection_strategy, items_strategy)
    assert_equals(recsys.model, matrix_model)
    assert_equals(recsys.capper, True)


def test_create_UserBasedRecommender():
    nhood_strategy = AllNeighborsStrategy()
    similarity = UserSimilarity(matrix_model, euclidean_distances)
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_equals(recsys.similarity, similarity)
    assert_equals(recsys.neighborhood_strategy, nhood_strategy)
    assert_equals(recsys.model, matrix_model)
    assert_equals(recsys.capper, True)


def test_all_other_items_ItemBasedRecommender():
    items_strategy = AllPossibleItemsStrategy()
    similarity = ItemSimilarity(matrix_model, euclidean_distances)
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))

    similarity = ItemSimilarity(boolean_matrix_model, jaccard_coefficient)
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))


def test_all_other_items_UserBasedRecommender():
    nhood_strategy = AllNeighborsStrategy()
    similarity = UserSimilarity(boolean_matrix_model, jaccard_coefficient)
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))

    similarity = UserSimilarity(boolean_matrix_model, jaccard_coefficient)
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['Lady in the Water']),
        recsys.all_other_items(user_id='Lorena Abreu', distance=pearson_correlation, nhood_size=2, minimal_similarity=0.1))
    assert_array_equal(np.array([], dtype='|S'),
         recsys.all_other_items(user_id='Marcel Caraciolo', distance=pearson_correlation, nhood_size=2, minimal_similarity=0.1))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']),
        recsys.all_other_items(user_id='Maria Gabriela', distance=pearson_correlation, nhood_size=2, minimal_similarity=0.1))

    similarity = UserSimilarity(matrix_model, euclidean_distances)
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))

    nhood_strategy = NearestNeighborsStrategy()
    similarity = UserSimilarity(matrix_model, pearson_correlation)
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)

    assert_array_equal(np.array(['Lady in the Water']),
        recsys.all_other_items(user_id='Lorena Abreu', distance=pearson_correlation, nhood_size=2, minimal_similarity=0.1))
    assert_array_equal(np.array([], dtype='|S'),
         recsys.all_other_items(user_id='Marcel Caraciolo', distance=pearson_correlation, nhood_size=3))
    assert_array_equal(np.array([]),
        recsys.all_other_items(user_id='Maria Gabriela', distance=euclidean_distances, nhood_size=2))


def test_estimate_preference_ItemBasedRecommender():
    items_strategy = ItemsNeighborhoodStrategy()
    similarity = ItemSimilarity(matrix_model, euclidean_distances)
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_almost_equals(3.5, recsys.estimate_preference('Marcel Caraciolo', 'Superman Returns'))
    assert_almost_equals(3.14717875510, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'))
    #With capper = False
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy, False)
    assert_almost_equals(3.14717875510, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'))
    #Non-Preferences
    assert_array_equal(np.nan, recsys.estimate_preference('Maria Gabriela', 'You, Me and Dupree'))

    items_strategy = ItemsNeighborhoodStrategy()
    similarity = ItemSimilarity(boolean_matrix_model, jaccard_coefficient)
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    assert_almost_equals(1.0, recsys.estimate_preference('Marcel Caraciolo', 'Superman Returns'))
    assert_almost_equals(1.0, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'))
    #With capper = False
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy, False)
    assert_almost_equals(1.0, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'))
    #Non-Preferences
    assert_array_equal(np.NaN, recsys.estimate_preference('Maria Gabriela', 'You, Me and Dupree'))


def test_estimate_preference_UserBasedRecommender():
    nhood_strategy = NearestNeighborsStrategy()
    similarity = UserSimilarity(matrix_model, euclidean_distances)
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_almost_equals(3.5, recsys.estimate_preference('Marcel Caraciolo', 'Superman Returns'))
    assert_almost_equals(2.4533792305691886, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'))

    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_almost_equals(3.5, recsys.estimate_preference('Marcel Caraciolo', 'Superman Returns'))
    assert_almost_equals(2.8960083169728952,
         recsys.estimate_preference(user_id='Leopoldo Pires', item_id='You, Me and Dupree',
                distance=pearson_correlation, nhood_size=4, minimal_similarity=-1.0))

    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_almost_equals(2.0653946891716108,
         recsys.estimate_preference(user_id='Leopoldo Pires', item_id='You, Me and Dupree',
                 nhood_size=4))

    #With capper = False
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy, False)
    assert_almost_equals(2.0653946891716108, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'))
    assert_almost_equals(2.8960083169728952,
         recsys.estimate_preference(user_id='Leopoldo Pires', item_id='You, Me and Dupree',
                distance=pearson_correlation, nhood_size=4, minimal_similarity=-1.0))

    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy, False)
    assert_almost_equals(2.0653946891716108,
         recsys.estimate_preference(user_id='Leopoldo Pires', item_id='You, Me and Dupree',
                 nhood_size=4))

    #Non-Preferences
    assert_array_equal(np.nan, recsys.estimate_preference('Maria Gabriela', 'You, Me and Dupree'))

    nhood_strategy = NearestNeighborsStrategy()
    similarity = UserSimilarity(boolean_matrix_model, jaccard_coefficient)
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_almost_equals(1.0, recsys.estimate_preference('Marcel Caraciolo', 'Superman Returns'))
    assert_almost_equals(1.0, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'))
    assert_almost_equals(1.0,
         recsys.estimate_preference(user_id='Leopoldo Pires', item_id='You, Me and Dupree',
                distance=jaccard_coefficient, nhood_size=3))

    #With capper = False
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy, False)
    assert_almost_equals(1.0, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'))
    #Non-Preferences
    assert_array_equal(np.NaN, recsys.estimate_preference('Maria Gabriela', 'You, Me and Dupree'))


def test_most_similar_items_ItemBasedRecommender():
    items_strategy = ItemsNeighborhoodStrategy()
    similarity = ItemSimilarity(matrix_model, euclidean_distances)
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    #semi items
    assert_array_equal(np.array(['Snakes on a Plane', \
        'The Night Listener', 'Lady in the Water', 'Just My Luck']), \
            recsys.most_similar_items('Superman Returns', 4))
    #all items
    assert_array_equal(np.array(['Lady in the Water', 'You, Me and Dupree', \
     'The Night Listener', 'Snakes on a Plane', 'Superman Returns']), \
            recsys.most_similar_items('Just My Luck'))
    #Non-existing
    assert_raises(ItemNotFoundError, recsys.most_similar_items, 'Back to the Future')
    #Exceed the limit
    assert_array_equal(np.array(['Lady in the Water', 'You, Me and Dupree', 'The Night Listener', \
       'Snakes on a Plane', 'Superman Returns']), \
            recsys.most_similar_items('Just My Luck', 20))
    #Empty
    assert_array_equal(np.array([]), \
            recsys.most_similar_items('Just My Luck', 0))

    items_strategy = ItemsNeighborhoodStrategy()
    similarity = ItemSimilarity(boolean_matrix_model, jaccard_coefficient)
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    #semi items
    assert_array_equal(np.array(['Snakes on a Plane', 'The Night Listener', \
    'You, Me and Dupree', 'Lady in the Water']), \
            recsys.most_similar_items('Superman Returns', 4))
    #all items
    assert_array_equal(np.array(['The Night Listener', 'You, Me and Dupree', \
        'Snakes on a Plane', 'Superman Returns', 'Lady in the Water']), \
            recsys.most_similar_items('Just My Luck'))
    #Non-existing
    assert_raises(ItemNotFoundError, recsys.most_similar_items, 'Back to the Future')
    #Exceed the limit
    assert_array_equal(np.array(['The Night Listener', 'You, Me and Dupree', 'Snakes on a Plane',
       'Superman Returns', 'Lady in the Water']), \
            recsys.most_similar_items('Just My Luck', 20))
    #Empty
    assert_array_equal(np.array([]), \
            recsys.most_similar_items('Just My Luck', 0))


def test_most_similar_users_UserBasedRecommender():
    nhood_strategy = NearestNeighborsStrategy()
    similarity = UserSimilarity(matrix_model, euclidean_distances)
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    #semi items
    assert_array_equal(np.array(['Leopoldo Pires', 'Steve Gates', 'Lorena Abreu',
         'Penny Frewman']), \
            recsys.most_similar_users('Marcel Caraciolo', 4))
    #all items
    assert_array_equal(np.array(['Lorena Abreu', 'Marcel Caraciolo', 'Penny Frewman', \
    'Steve Gates', 'Luciana Nunes', 'Sheldom']), \
            recsys.most_similar_users('Leopoldo Pires'))
    #Non-existing
    assert_array_equal(np.array([]), \
            recsys.most_similar_users('Maria Gabriela'))
    #Exceed the limit
    assert_array_equal(np.array(['Lorena Abreu', 'Marcel Caraciolo', 'Penny Frewman', \
    'Steve Gates', 'Luciana Nunes', 'Sheldom']), \
            recsys.most_similar_users('Leopoldo Pires', 20))
    #Empty
    assert_array_equal(np.array([]), \
            recsys.most_similar_users('Sheldom', 0))

    nhood_strategy = NearestNeighborsStrategy()
    similarity = UserSimilarity(boolean_matrix_model, jaccard_coefficient)
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    #semi items
    assert_array_equal(np.array(['Luciana Nunes', 'Steve Gates', \
            'Lorena Abreu', 'Sheldom']), \
            recsys.most_similar_users('Marcel Caraciolo', 4))
    #all items
    assert_array_equal(np.array(['Sheldom', 'Luciana Nunes', 'Marcel Caraciolo',
     'Steve Gates', 'Lorena Abreu', 'Penny Frewman']), \
            recsys.most_similar_users('Leopoldo Pires'))
    #Non-existing
    assert_array_equal(np.array([]), \
            recsys.most_similar_users('Maria Gabriela'))
    #Exceed the limit
    assert_array_equal(np.array(['Sheldom', 'Luciana Nunes', 'Marcel Caraciolo',
     'Steve Gates', 'Lorena Abreu', 'Penny Frewman']), \
            recsys.most_similar_users('Leopoldo Pires', 20))
    #Empty
    assert_array_equal(np.array([]), \
            recsys.most_similar_users('Sheldom', 0))


def test_recommend_ItemBasedRecommender():
    items_strategy = ItemsNeighborhoodStrategy()
    similarity = ItemSimilarity(matrix_model, euclidean_distances)
    #Empty Recommendation
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_array_equal(np.array([]), recsys.recommend('Marcel Caraciolo'))

    #Semi Recommendation
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['Just My Luck', 'You, Me and Dupree']), \
        recsys.recommend('Leopoldo Pires'))

    #Semi Recommendation
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['Just My Luck']), \
        recsys.recommend('Leopoldo Pires', 1))

    #Empty Recommendation
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #Test with params update
    recsys.recommend(user_id='Maria Gabriela', similarity=similarity)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #with_preference
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy, True, True)
    assert_equals('Just My Luck', recsys.recommend('Leopoldo Pires')[0][0])
    assert_equals('You, Me and Dupree', recsys.recommend('Leopoldo Pires')[1][0])

    assert_almost_equals(3.20597, recsys.recommend('Leopoldo Pires')[0][1], 2)
    assert_almost_equals(3.147178755, recsys.recommend('Leopoldo Pires')[1][1], 2)

    similarity = ItemSimilarity(boolean_matrix_model, jaccard_coefficient)
    #Empty Recommendation
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    assert_array_equal(np.array([]), recsys.recommend('Marcel Caraciolo'))

    #Semi Recommendation
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['You, Me and Dupree', 'Just My Luck']), \
        recsys.recommend('Leopoldo Pires'))

    #Semi Recommendation
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['You, Me and Dupree']), \
        recsys.recommend('Leopoldo Pires', 1))

    #Empty Recommendation
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #Test with params update
    recsys.recommend(user_id='Maria Gabriela', similarity=similarity)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #with_preference
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy, True, True)
    assert_equals('You, Me and Dupree', recsys.recommend('Leopoldo Pires')[0][0])
    assert_equals('Just My Luck', recsys.recommend('Leopoldo Pires')[1][0])

    assert_almost_equals(1.0, recsys.recommend('Leopoldo Pires')[0][1], 2)
    assert_almost_equals(1.0, recsys.recommend('Leopoldo Pires')[1][1], 2)


def test_recommend_UserBasedRecommender():
    nhood_strategy = NearestNeighborsStrategy()
    similarity = UserSimilarity(matrix_model, euclidean_distances)
    #Empty Recommendation
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array([]), recsys.recommend('Marcel Caraciolo'))

    #Semi Recommendation
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['Just My Luck', 'You, Me and Dupree']), \
        recsys.recommend('Leopoldo Pires'))

    #Semi Recommendation
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['Just My Luck']), \
        recsys.recommend('Leopoldo Pires', 1))

    #Empty Recommendation
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #Test with params update
    recsys.recommend(user_id='Maria Gabriela', similarity=similarity)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #with_preference
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy, True, True)
    assert_equals('Just My Luck', recsys.recommend('Leopoldo Pires')[0][0])
    assert_equals('You, Me and Dupree', recsys.recommend('Leopoldo Pires')[1][0])

    assert_almost_equals(2.456743361464, recsys.recommend('Leopoldo Pires')[0][1], 2)
    assert_almost_equals(2.453379, recsys.recommend('Leopoldo Pires')[1][1], 2)

    similarity = UserSimilarity(boolean_matrix_model, jaccard_coefficient)
    #Empty Recommendation
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array([]), recsys.recommend('Marcel Caraciolo'))

    #Semi Recommendation
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['You, Me and Dupree', 'Just My Luck']), \
        recsys.recommend('Leopoldo Pires'))

    #Semi Recommendation
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['You, Me and Dupree']), \
        recsys.recommend('Leopoldo Pires', 1))

    #Empty Recommendation
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #Test with params update
    recsys.recommend(user_id='Maria Gabriela', similarity=similarity)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #with_preference
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy, True, True)
    assert_equals('You, Me and Dupree', recsys.recommend('Leopoldo Pires')[0][0])
    assert_equals('Just My Luck', recsys.recommend('Leopoldo Pires')[1][0])

    assert_almost_equals(1.0, recsys.recommend('Leopoldo Pires')[0][1], 2)
    assert_almost_equals(1.0, recsys.recommend('Leopoldo Pires')[1][1], 2)


def test_recommend_because_ItemBasedRecommender():
    items_strategy = ItemsNeighborhoodStrategy()
    similarity = ItemSimilarity(matrix_model, euclidean_distances)
    #Full Recommendation Because
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['The Night Listener', 'Superman Returns', \
    'Snakes on a Plane', 'Lady in the Water']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck'))
    #over-items
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['The Night Listener', 'Superman Returns', \
    'Snakes on a Plane', 'Lady in the Water']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 20))
    #Semi
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['The Night Listener', 'Superman Returns']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 2))

    #Non-Existing
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy)
    assert_array_equal(np.array([]), \
        recsys.recommended_because('Maria Gabriela', 'Just My Luck', 2))

    #with_preference
    recsys = ItemBasedRecommender(matrix_model, similarity, items_strategy, True, True)
    assert_array_equal(np.array([('The Night Listener', 4.0), \
                ('Superman Returns', 3.5)]), \
                recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 2))

    #boolean_matrix_model
    similarity = ItemSimilarity(boolean_matrix_model, jaccard_coefficient)
    #Full Recommendation Because
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['The Night Listener', 'Superman Returns', \
    'Snakes on a Plane', 'Lady in the Water']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck'))
    #over-items
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['The Night Listener', 'Superman Returns', \
    'Snakes on a Plane', 'Lady in the Water']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 20))
    #Semi
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    assert_array_equal(np.array(['The Night Listener', 'Superman Returns']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 2))

    #Non-Existing
    recsys = ItemBasedRecommender(boolean_matrix_model, similarity, items_strategy)
    assert_array_equal(np.array([]), \
        recsys.recommended_because('Maria Gabriela', 'Just My Luck', 2))



def test_recommend_because_UserBasedRecommender():
    nhood_strategy = NearestNeighborsStrategy()
    similarity = UserSimilarity(matrix_model, euclidean_distances)
    #Full Recommendation Because
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['Lorena Abreu', 'Marcel Caraciolo', \
        'Steve Gates', 'Luciana Nunes']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck'))
    #over-items
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['Lorena Abreu', 'Marcel Caraciolo', \
        'Steve Gates', 'Luciana Nunes']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 20))
    #Semi
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['Lorena Abreu', 'Marcel Caraciolo']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 2))

    #Non-Existing
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array([]), \
        recsys.recommended_because('Maria Gabriela', 'Just My Luck', 2))

    #with_preference
    recsys = UserBasedRecommender(matrix_model, similarity, nhood_strategy, True, True)
    assert_array_equal(np.array([('Lorena Abreu', 3.0), \
                ('Marcel Caraciolo', 3.0)]), \
                recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 2))

    #boolean_matrix_model
    similarity = UserSimilarity(boolean_matrix_model, jaccard_coefficient)
    #Full Recommendation Because
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['Steve Gates', 'Marcel Caraciolo', 'Luciana Nunes', \
    'Lorena Abreu']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck'))
    #over-items
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['Steve Gates', 'Marcel Caraciolo', 'Luciana Nunes', \
    'Lorena Abreu']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 20))
    #Semi
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array(['Steve Gates', 'Marcel Caraciolo']), \
        recsys.recommended_because('Leopoldo Pires', 'Just My Luck', 2))

    #Non-Existing
    recsys = UserBasedRecommender(boolean_matrix_model, similarity, nhood_strategy)
    assert_array_equal(np.array([]), \
        recsys.recommended_because('Maria Gabriela', 'Just My Luck', 2))

########NEW FILE########
__FILENAME__ = test_item_strategies
import numpy as np
from numpy.testing import assert_array_equal
from nose.tools import assert_raises
from ....models.classes import  MatrixPreferenceDataModel
from ..item_strategies import ItemsNeighborhoodStrategy, AllPossibleItemsStrategy
from ....models.utils import UserNotFoundError


movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,
 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,
 'The Night Listener': 3.0},
'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,
 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 3.5},
'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,
 'Superman Returns': 3.5, 'The Night Listener': 4.0},
'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,
 'The Night Listener': 4.5, 'Superman Returns': 4.0,
 'You, Me and Dupree': 2.5},
'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 2.0},
'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},
'Penny Frewman': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0, 'Superman Returns': 4.0},
'Maria Gabriela': {}}


def test_ItemsNeighborhoodStrategy():
    #Empty Dataset
    model = MatrixPreferenceDataModel({})
    strategy = ItemsNeighborhoodStrategy()
    assert_raises(UserNotFoundError, strategy.candidate_items, 'Lorena Abreu', model)

    #Possible candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = ItemsNeighborhoodStrategy()
    assert_array_equal(np.array(['Lady in the Water']), strategy.candidate_items('Lorena Abreu', model))

    #Empty candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = ItemsNeighborhoodStrategy()
    assert_array_equal(np.array([], dtype='|S'), strategy.candidate_items('Marcel Caraciolo', model))

    #Empty candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = ItemsNeighborhoodStrategy()
    assert_array_equal(np.array([], dtype=bool), strategy.candidate_items('Maria Gabriela', model))


def test_AllPossibleItemsStrategy():
    #Empty Dataset
    model = MatrixPreferenceDataModel({})
    strategy = AllPossibleItemsStrategy()
    assert_raises(UserNotFoundError, strategy.candidate_items, 'Lorena Abreu', model)

    #Possible candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = AllPossibleItemsStrategy()
    assert_array_equal(np.array(['Lady in the Water']), strategy.candidate_items('Lorena Abreu', model))

    #Empty candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = AllPossibleItemsStrategy()
    assert_array_equal(np.array([], dtype='|S'), strategy.candidate_items('Marcel Caraciolo', model))

    #Empty candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = AllPossibleItemsStrategy()
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), strategy.candidate_items('Maria Gabriela', model))

########NEW FILE########
__FILENAME__ = test_neighborhood_strategies
import numpy as np
from numpy.testing import assert_array_equal
from ....models.classes import MatrixPreferenceDataModel
from ..neighborhood_strategies import AllNeighborsStrategy, NearestNeighborsStrategy


movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,
 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,
 'The Night Listener': 3.0},
'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,
 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 3.5},
'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,
 'Superman Returns': 3.5, 'The Night Listener': 4.0},
'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,
 'The Night Listener': 4.5, 'Superman Returns': 4.0,
 'You, Me and Dupree': 2.5},
'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 2.0},
'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},
'Penny Frewman': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0, 'Superman Returns': 4.0},
'Maria Gabriela': {}}


def test_AllNeighborsStrategy():
    #Empty Dataset
    model = MatrixPreferenceDataModel({})
    strategy = AllNeighborsStrategy()
    assert_array_equal(np.array([]), strategy.user_neighborhood('Lorena Abreu', model))

    #Possible candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = AllNeighborsStrategy()
    assert_array_equal(np.array(['Leopoldo Pires', 'Luciana Nunes', 'Marcel Caraciolo',
       'Maria Gabriela', 'Penny Frewman', 'Sheldom', 'Steve Gates']), strategy.user_neighborhood('Lorena Abreu', model))


def test_NearestNeighborsStrategy():
    #Empty Dataset
    model = MatrixPreferenceDataModel({})
    strategy = NearestNeighborsStrategy()
    assert_array_equal(np.array([]), strategy.user_neighborhood('Lorena Abreu', model))

    #Possible candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = NearestNeighborsStrategy()
    assert_array_equal(np.array(['Leopoldo Pires', 'Marcel Caraciolo', 'Penny Frewman',
     'Sheldom', 'Steve Gates', 'Luciana Nunes'], dtype='|S16'),
       strategy.user_neighborhood('Lorena Abreu', model))

    #Test with neighborhood size limited.
    model = MatrixPreferenceDataModel(movies)
    strategy = NearestNeighborsStrategy()
    assert_array_equal(np.array(['Leopoldo Pires', 'Marcel Caraciolo'],
            dtype='|S16'), strategy.user_neighborhood(user_id='Lorena Abreu', data_model=model,
                nhood_size=2))

    #Test with minimal_similarity
    model = MatrixPreferenceDataModel(movies)
    strategy = NearestNeighborsStrategy()
    assert_array_equal(np.array(['Leopoldo Pires']),
        strategy.user_neighborhood(user_id='Lorena Abreu', data_model=model,
                minimal_similarity=0.4))

    #Empty candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = NearestNeighborsStrategy()
    assert_array_equal(np.array(['Leopoldo Pires', 'Steve Gates', 'Lorena Abreu', 'Penny Frewman',
    'Sheldom', 'Luciana Nunes'], dtype='|S14'),
        strategy.user_neighborhood('Marcel Caraciolo', model))

    #Empty candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = NearestNeighborsStrategy()
    assert_array_equal(np.array([], dtype=bool), strategy.user_neighborhood('Maria Gabriela', model))

    #Raise exception with an invalid similarity
    #Empty candidates
    model = MatrixPreferenceDataModel(movies)
    strategy = NearestNeighborsStrategy()
    assert_array_equal(np.array([], dtype=bool), strategy.user_neighborhood('Maria Gabriela', model))

########NEW FILE########
__FILENAME__ = base
"""
Generalized Recommender models amd utility classes.

This module contains basic memory recommender interfaces used throughout
the whole scikit-crab package as also utility classes.

The interfaces are realized as abstract base classes (ie., some optional
functionality is provided in the interface itself, so that the interfaces
can be subclassed).

"""

# Author: Marcel Caraciolo <marcel@muricoca.com>
#
# License: BSD Style.

from ..base import MemoryBasedRecommender

#===========================
#Matrix Factorization Recommender Interface


class SVDRecommender(MemoryBasedRecommender):

    def factorize(self):
        '''
        Factorize the ratings matrix with a factorization
         technique implemented in this method.

        Parameters
        -----------

        Returns
        -----------
        '''
        raise NotImplementedError("ItemRecommender is an abstract class.")

    def train(self):
        '''
        Train the recommender with the matrix factorization method chosen.

        Parameters
        -----------

        Returns
        ----------

        '''
        raise NotImplementedError("ItemRecommender is an abstract class.")

########NEW FILE########
__FILENAME__ = classes
"""
Generalized Recommender models.

This module contains matrix factorization recommender interfaces
used throughout the whole scikit-crab package.

The interfaces are realized as abstract base classes (ie., some optional
functionality is provided in the interface itself, so that the interfaces
can be subclassed).

"""

# Author: Marcel Caraciolo <marcel@muricoca.com>
#
# License: BSD Style.
import random

from base import SVDRecommender
from ..knn.item_strategies import ItemsNeighborhoodStrategy
import numpy as np
from math import sqrt
import logging

logger = logging.getLogger('crab')


class MatrixFactorBasedRecommender(SVDRecommender):
    """
    Matrix Factorization Based Recommender using
    Expectation Maximization algorithm.

    Parameters
    -----------
    data_model: The data model instance that will be data source
         for the recommender.

    items_selection_strategy: The item candidates strategy that you
     can choose for selecting the possible items to recommend.
     default = ItemsNeighborhoodStrategy

    n_features: int
            Number of latent factors. default = 10

    learning_rate: float
        Learning rate used. default =  0.01

    regularization: float
            Parameter used to prevent overfitting. default = 0.02

    init_mean: float
            Mean of the normal distribution used to initialize
            the factors. default = 0.1

    init_std: float
            Standard deviation of the normal distribution used to
            initialize the factors. default = 0.1

    n_interations: int
            Number of iterations over the training data. default = 30

    capper: bool (default=True)
        Cap the preferences with maximum and minimum preferences
        in the model.
    with_preference: bool (default=False)
        Return the recommendations with the estimated preferences if True.

    Attributes
    -----------
    `model`: The data model instance that will be data source
         for the recommender.

    `items_selection_strategy`: The item candidates strategy that you
         can choose for selecting the possible items to recommend.
         default = ItemsNeighborhoodStrategy

    `n_features`:  int
            Number of latent factors. default = 10

    `learning_rate`: float
            Learning rate used. default = 0.01

    `regularization`: float
            Parameter used to prevent overfitting. default = 0.02

    `random_noise`: float
            Parameter used to initialize the latent factors.

    `n_interations`: int
            Number of iterations over the training data

    `capper`: bool (default=True)
        Cap the preferences with maximum and minimum preferences
        in the model.

    `with_preference`: bool (default=False)
        Return the recommendations with the estimated preferences if True.

    `user_factors`: array of shape [n_users, n_features]
             Matrix containing the latent item factors

    `item_factors`: array of shape [n_items, n_features]
            Matrix containing the latent item factors

    Examples
    -----------
    >>> from scikits.crab.models.classes import MatrixPreferenceDataModel
    >>> from scikits.crab.recommenders.svd.classes import MatrixFactorBasedRecommender
    >>> from scikits.crab.recommenders.knn.item_strategies import ItemsNeighborhoodStrategy
    >>> movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, \
     'Snakes on a Plane': 3.5, \
     'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \
     'The Night Listener': 3.0}, \
     'Paola Pow': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \
     'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 3.5}, \
    'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, \
     'Superman Returns': 3.5, 'The Night Listener': 4.0}, \
    'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, \
     'The Night Listener': 4.5, 'Superman Returns': 4.0, \
     'You, Me and Dupree': 2.5}, \
    'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 2.0}, \
    'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'The Night Listener': 3.0, 'Superman Returns': 5.0, \
     'You, Me and Dupree': 3.5}, \
    'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0, \
    'Superman Returns':4.0}, \
    'Maria Gabriela': {}}
    >>> model = MatrixPreferenceDataModel(movies)
    >>> items_strategy = ItemsNeighborhoodStrategy()
    >>> recsys = MatrixFactorBasedRecommender( \
        model=model, \
        items_selection_strategy=items_strategy, \
        n_features=2)
    >>> #Return the recommendations for the given user.
    >>> recsys.recommend('Leopoldo Pires')
    ['Just My Luck', 'You, Me and Dupree']


    Notes
    -----------
    This MatrixFactorizationRecommender does not yet provide
    suppot for rescorer functions.

    This MatrixFactorizationRecommender does not yet provide
    suppot for DictDataModels.

    References
    -----------


    """

    def __init__(self, model, items_selection_strategy=None,
            n_features=10, learning_rate=0.01, regularization=0.02, init_mean=0.1,
            init_stdev=0.1, n_interations=30, capper=True, with_preference=False):
        SVDRecommender.__init__(self, model, with_preference)
        self.capper = capper
        self.n_features = n_features
        self.learning_rate = learning_rate
        self.regularization = regularization
        self.init_mean = init_mean
        self.init_stdev = init_stdev
        self.n_interations = n_interations
        self._global_bias = self._get_average_preference()
        self.user_factors = None
        self.item_factors = None

        if items_selection_strategy is None:
            self.items_selection_strategy = ItemsNeighborhoodStrategy()
        else:
            self.items_selection_strategy = items_selection_strategy

        self.factorize()

    def _init_models(self):
        num_users = self.model.users_count()
        num_items = self.model.items_count()

        self.user_factors = np.empty(shape=(num_users, self.n_features),
                    dtype=float)

        self.item_factors = np.empty(shape=(num_items, self.n_features),
                    dtype=float)

        '''
        pref_interval = self.model.max_preference() - self.model.min_preference()
        default_value = math.sqrt(global_bias - pref_interval * 0.1) / self.n_features
        interval = pref_interval * 0.1 / self.n_features

        for i in range(len(self.n_features)):
            for user_idx in self.model.num_users():
                self.user_factors[user_idx, i] = default_value + (random.random() - 0.5) * interval * 0.2

        for i in range(len(self.n_features)):
            for item_idx in self.model.num_items():
                self.item_factors[item_idx, i] = default_value + (random.random() - 0.5) * interval * 0.2
        '''
        #Initialize the matrix with normal distributed (Gaussian) Noise
        self.user_factors = self.init_mean * np.random.randn(num_users, self.n_features) + self.init_stdev ** 2
        self.item_factors = self.init_mean * np.random.randn(num_items, self.n_features) + self.init_stdev ** 2

    def _get_average_preference(self):
        if hasattr(self.model, 'index'):
            mdat = np.ma.masked_array(self.model.index, np.isnan(self.model.index))
        else:
            raise TypeError('This model is not yet supported for this recommender.')
        return np.mean(mdat)

    def _predict(self, user_index, item_index, trailing=True):
        #Compute the scalar product between two rows of two matrices
        result = self._global_bias + np.sum(self.user_factors[user_index] *
                                            self.item_factors[item_index])
        if trailing:
            max_preference = self.model.max_preference()
            min_preference = self.model.min_preference()
            if result > max_preference:
                result = max_preference
            elif result < min_preference:
                result = min_preference

        return result

    def _train(self, rating_indices, update_user, update_item):
        '''
        Iterate once over rating data and adjust corresponding factors (stochastic gradient descent)
        '''
        err_total = 0.0
        for user_idx, item_idx in rating_indices:
            p = self._predict(user_idx, item_idx, False)
            err = self.model.index[user_idx, item_idx] - p
            err_total += (err ** 2.0)

            #Adjust the factors
            u_f = self.user_factors[user_idx]
            i_f = self.item_factors[item_idx]

            #Compute factor updates
            delta_u = err * i_f - self.regularization * u_f
            delta_i = err * u_f - self.regularization * i_f
            #if necessary apply updates
            if update_user:
                self.user_factors[user_idx] += self.learning_rate * delta_u
            if update_item:
                self.item_factors[item_idx] += self.learning_rate * delta_i

        return err_total

    def _rating_indices(self):
        if hasattr(self.model, 'index'):
            rating_indices = [(idx, jdx) for idx in range(self.model.users_count())
                                for jdx in range(self.model.items_count())
                        if not np.isnan(self.model.index[idx, jdx])]
        else:
            raise TypeError('This model is not yet supported for this recommender.')

        return rating_indices

    def learn_factors(self, update_user=True, update_item=True):
        rating_indices = self._rating_indices()
        random.shuffle(rating_indices)

        for index in range(self.n_interations):
            err = self._train(rating_indices, update_user, update_item)
            rmse = sqrt(err / len(rating_indices))
            logger.debug("Finished the interation %i with RMSE %f" %  \
                    (index, rmse))

    def factorize(self):
        #init factor matrices
        self._init_models()
        #Learn the model parameters
        self.learn_factors()

    def recommend(self, user_id, how_many=None, **params):
        '''
        Return a list of recommended items, ordered from most strongly
        recommend to least.

        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.
        how_many: int
                 Desired number of recommendations (default=None ALL)

        '''
        self._set_params(**params)

        candidate_items = self.all_other_items(user_id)

        recommendable_items = self._top_matches(user_id, \
                 candidate_items, how_many)

        return recommendable_items

    def estimate_preference(self, user_id, item_id, **params):
        '''
        A preference is estimated by computing the dot-product
        of the user and item feature vectors.
        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.

        item_id:  int or string
            ID of item for which wants to find the estimated preference.

        Returns
        -------
        Return an estimated preference if the user has not expressed a
        preference for the item, or else the user's actual preference for the
        item. If a preference cannot be estimated, returns None.
        '''

        preference = self.model.preference_value(user_id, item_id)
        if not np.isnan(preference):
            return preference

        #How to catch the user_id and item_id from the matrix.

        user_features = self.user_factors[np.where(self.model.user_ids() == user_id)]
        item_features = self.item_factors[np.where(self.model.item_ids() == item_id)]

        estimated = self._global_bias + np.sum(user_features * item_features)

        if self.capper:
            max_p = self.model.maximum_preference_value()
            min_p = self.model.minimum_preference_value()
            estimated = max_p if estimated > max_p else min_p \
                     if estimated < min_p else estimated
        return estimated

    def all_other_items(self, user_id, **params):
        '''
        Parameters
        ----------
        user_id: int or string
                 User for which recommendations are to be computed.

        Returns
        ---------
        Return items in the `model` for which the user has not expressed
        the preference and could possibly be recommended to the user.

        '''
        return self.items_selection_strategy.candidate_items(user_id, \
                            self.model)

    def _top_matches(self, source_id, target_ids, how_many=None, **params):
        '''
        Parameters
        ----------
        target_ids: array of shape [n_target_ids]

        source_id: int or string
                item id to compare against.

        how_many: int
            Desired number of most top items to recommend (default=None ALL)

        Returns
        --------
        Return the top N matches
        It can be user_ids or item_ids.
        '''
        #Empty target_ids
        if target_ids.size == 0:
            return np.array([])

        estimate_preferences = np.vectorize(self.estimate_preference)

        preferences = estimate_preferences(source_id, target_ids)

        preferences = preferences[~np.isnan(preferences)]
        target_ids = target_ids[~np.isnan(preferences)]

        sorted_preferences = np.lexsort((preferences,))[::-1]

        sorted_preferences = sorted_preferences[0:how_many] \
             if how_many and sorted_preferences.size > how_many \
                else sorted_preferences

        if self.with_preference:
            top_n_recs = [(target_ids[ind], \
                     preferences[ind]) for ind in sorted_preferences]
        else:
            top_n_recs = [target_ids[ind]
                 for ind in sorted_preferences]

        return top_n_recs

########NEW FILE########
__FILENAME__ = test_classes
import numpy as np
from numpy.testing import assert_array_equal
from nose.tools import assert_equals, assert_almost_equals
from ...knn.item_strategies import AllPossibleItemsStrategy, ItemsNeighborhoodStrategy
from ....models.classes import  MatrixPreferenceDataModel, \
     MatrixBooleanPrefDataModel
from ..classes import MatrixFactorBasedRecommender


movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,
 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,
 'The Night Listener': 3.0},
'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,
 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 3.5},
'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,
 'Superman Returns': 3.5, 'The Night Listener': 4.0},
'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,
 'The Night Listener': 4.5, 'Superman Returns': 4.0,
 'You, Me and Dupree': 2.5},
'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 2.0},
'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},
'Penny Frewman': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0, 'Superman Returns': 4.0},
'Maria Gabriela': {}}

matrix_model = MatrixPreferenceDataModel(movies)
boolean_matrix_model = MatrixBooleanPrefDataModel(movies)


def test_create_MatrixFactorBasedRecommender():
    items_strategy = AllPossibleItemsStrategy()
    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_equals(recsys.items_selection_strategy, items_strategy)
    assert_equals(recsys.model, matrix_model)
    assert_equals(recsys.capper, True)
    assert_equals(recsys.learning_rate, 0.01)
    assert_equals(recsys.regularization, 0.02)
    assert_equals(recsys.init_mean, 0.1)
    assert_equals(recsys.n_interations, 30)
    assert_equals(recsys.init_stdev, 0.1)
    assert_equals(recsys.with_preference, False)
    assert_equals(recsys.user_factors.shape, (8, 2))
    assert_equals(recsys.item_factors.shape, (6, 2))
    assert_equals(recsys._global_bias, 3.2285714285714286)


def test_all_other_items_MatrixFactorBasedRecommender():
    items_strategy = AllPossibleItemsStrategy()
    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))

    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))

    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))

    recsys = MatrixFactorBasedRecommender(
        model=boolean_matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))

    recsys = MatrixFactorBasedRecommender(
        model=boolean_matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))

    recsys = MatrixFactorBasedRecommender(
        model=boolean_matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)

    assert_array_equal(np.array(['Lady in the Water']), recsys.all_other_items('Lorena Abreu'))
    assert_array_equal(np.array([], dtype='|S'), recsys.all_other_items('Marcel Caraciolo'))
    assert_array_equal(np.array(['Just My Luck', 'Lady in the Water', 'Snakes on a Plane',
       'Superman Returns', 'The Night Listener', 'You, Me and Dupree']), recsys.all_other_items('Maria Gabriela'))


def test_estimate_preference_MatrixFactorBasedRecommender():
    items_strategy = ItemsNeighborhoodStrategy()
    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_almost_equals(3.5, recsys.estimate_preference('Marcel Caraciolo', 'Superman Returns'))
    assert_almost_equals(3.206, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'), 1)

    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=3)
    assert_almost_equals(3.5, recsys.estimate_preference('Marcel Caraciolo', 'Superman Returns'))
    assert_almost_equals(3.21,
         recsys.estimate_preference(user_id='Leopoldo Pires', item_id='You, Me and Dupree'), 1)

    #With capper = False
    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2, capper=False)
    assert_almost_equals(3.23, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'), 1)

    #Boolean Matrix Model
    recsys = MatrixFactorBasedRecommender(
        model=boolean_matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_almost_equals(1.0, recsys.estimate_preference('Marcel Caraciolo', 'Superman Returns'))
    assert_almost_equals(0.72, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'), 1)
    assert_almost_equals(0.73,
         recsys.estimate_preference(user_id='Leopoldo Pires', item_id='You, Me and Dupree'), 1)

    #With capper = False
    recsys = MatrixFactorBasedRecommender(
        model=boolean_matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2, capper=False)
    assert_almost_equals(0.73, recsys.estimate_preference('Leopoldo Pires', 'You, Me and Dupree'), 1)
    #Non-Preferences
    assert_almost_equals(0.7258, recsys.estimate_preference('Maria Gabriela', 'You, Me and Dupree'), 1)


def test_recommend_MatrixFactorBasedRecommender():
    items_strategy = ItemsNeighborhoodStrategy()
    #Empty Recommendation
    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_array_equal(np.array([]), recsys.recommend('Marcel Caraciolo'))

    #Semi Recommendation
    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_array_equal(np.array(['You, Me and Dupree', 'Just My Luck']), \
        recsys.recommend('Leopoldo Pires'))

    #Semi Recommendation
    assert_array_equal(np.array(['You, Me and Dupree']), \
        recsys.recommend('Leopoldo Pires', 1))

    #Empty Recommendation
    recsys = MatrixFactorBasedRecommender(
        model=matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #Test with params update
    recsys.recommend(user_id='Maria Gabriela', n_features=2)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #with_preference
    #recsys = MatrixFactorBasedRecommender(
    #    model=matrix_model,
    #    items_selection_strategy=items_strategy,
    #    n_features=2, with_preference=True)
    #assert_array_equal(np.array([('Just My Luck', 3.20597319063), \
    #            ('You, Me and Dupree', 3.14717875510)]), \
    #            recsys.recommend('Leopoldo Pires'))

    #Empty Recommendation
    recsys = MatrixFactorBasedRecommender(
        model=boolean_matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_array_equal(np.array([]), recsys.recommend('Marcel Caraciolo'))

    #Semi Recommendation
    recsys = MatrixFactorBasedRecommender(
        model=boolean_matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_array_equal(np.array(['Just My Luck', 'You, Me and Dupree']), \
    recsys.recommend('Leopoldo Pires'))

    #Semi Recommendation
    recsys = MatrixFactorBasedRecommender(
        model=boolean_matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_array_equal(np.array(['You, Me and Dupree']), \
        recsys.recommend('Leopoldo Pires', 1))

    #Empty Recommendation
    recsys = MatrixFactorBasedRecommender(
        model=boolean_matrix_model,
        items_selection_strategy=items_strategy,
        n_features=2)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #Test with params update
    recsys.recommend(user_id='Maria Gabriela', n_features=2)
    assert_array_equal(np.array([]), recsys.recommend('Maria Gabriela'))

    #with_preference
    #recsys = MatrixFactorBasedRecommender(
    #    model=boolean_matrix_model,
    #    items_selection_strategy=items_strategy,
    #    n_features=2)
    #assert_array_equal(np.array([('Just My Luck', 3.20597), \
    #            ('You, Me and Dupree', 3.1471)]), \
    #            recsys.recommend('Leopoldo Pires'))
########NEW FILE########
__FILENAME__ = base
#-*- coding:utf-8 -*-

"""
Base Similarity Models.

"""
#Authors: Marcel Caraciolo <marcel@muricoca.com>
#License: BSD Style
import numpy as np


class BaseSimilarity(object):
    """
    Base Class for similarity that searches over a set of items/users.

    In all instances, there is a data model against which we want to perform
    the similarity search.

    For each similarity search, the input is a item/user and the output are its
    similarities to individual items/users.

    Similarity queries are realized by calling ``self[query_item]``.
    There is also a convenience wrapper, where iterating over `self` yields
    similarities of each object in the model against the whole data model (ie.,
    the query is each item/user in turn).

    Should not be used directly, use derived classes instead

    Attributes
    ----------

     `model`:  DataModel
          Defines the data model where data is fetched.
     `distance`: Function
          Pairwise Function between two vectors.
      `num_best': int
          If it is left unspecified, similarity queries return a full list (one
          float for every item in the model, including the query item).

          If `num_best` is set, queries return `num_best` most similar items,
          as a sorted list.

    """
    def __init__(self, model, distance, num_best=None):
        self.model = model
        self.distance = distance
        self._set_num_best(num_best)

    def _set_num_best(self, num_best):
        self.num_best = num_best

    def get_similarity(self, source_id, target_id):
        """
        Return similarity of the `source_id` to a specific `target_id` in the
        model.
        """
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def get_similarities(self, source_id):
        """

        Return similarity of the `source_id` to all sources in the model.

        """
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def __getitem__(self, source_id):
        """
        Get similarities of the `source_id` to all sources in the model.
        """
        all_sims = self.get_similarities(source_id)

        #return either all similarities as a list,
        #or only self.num_best most similar,
        #depending on settings from the constructor

        tops = sorted(all_sims, key=lambda x: -x[1])

        if all_sims:
            item_ids, preferences = zip(*all_sims)
            preferences = np.array(preferences).flatten()
            item_ids = np.array(item_ids).flatten()
            sorted_prefs = np.argsort(-preferences)
            tops = zip(item_ids[sorted_prefs], preferences[sorted_prefs])

        # return at most numBest top 2-tuples (label, sim)
        return tops[:self.num_best] if self.num_best is not None else tops

########NEW FILE########
__FILENAME__ = basic_similarities
#-*- coding:utf-8 -*-

"""
This module contains functions and classes for computing similarities across
a collection of vectors.
"""
#Authors: Marcel Caraciolo <marcel@muricoca.com>
#License: BSD Style


import numpy as np
from base import BaseSimilarity
from ..metrics.pairwise import loglikehood_coefficient


def find_common_elements(source_preferences, target_preferences):
    ''' Returns the preferences from both vectors '''
    src = dict(source_preferences)
    tgt = dict(target_preferences)

    inter = np.intersect1d(src.keys(), tgt.keys())

    common_preferences = zip(*[(src[item], tgt[item]) for item in inter \
            if not np.isnan(src[item]) and not np.isnan(tgt[item])])
    if common_preferences:
        return np.asarray([common_preferences[0]]), np.asarray([common_preferences[1]])
    else:
            return np.asarray([[]]), np.asarray([[]])

###############################################################################
# User Similarity


class UserSimilarity(BaseSimilarity):
    '''
    Returns the degree of similarity, of two users, based on the their preferences.
    Implementations of this class define a notion of similarity between two users.
    Implementations should  return values in the range 0.0 to 1.0, with 1.0 representing
    perfect similarity.

    Parameters
    ----------
    `model`:  DataModel
         Defines the data model where data is fetched.
    `distance`: Function
         Pairwise Function between two vectors.
     `num_best`: int
         If it is left unspecified, similarity queries return a full list (one
         float for every item in the model, including the query item).

         If `num_best` is set, queries return `num_best` most similar items, as a
         sorted list.

    Methods
    ---------
    get_similarity()
    Return similarity of the `source_id` to a specific `target_id` in the model.

    get_similarities()
    Return similarity of the `source_id` to all sources in the model.

    Examples
    ---------
    >>> from scikits.crab.models.classes import MatrixPreferenceDataModel
    >>> from scikits.crab.metrics.pairwise import cosine_distances
    >>> from scikits.crab.similarities.basic_similarities import UserSimilarity
    >>> movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, \
     'Snakes on a Plane': 3.5, \
     'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \
     'The Night Listener': 3.0}, \
     'Paola Pow': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \
     'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 3.5}, \
    'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, \
     'Superman Returns': 3.5, 'The Night Listener': 4.0}, \
    'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, \
     'The Night Listener': 4.5, 'Superman Returns': 4.0, \
     'You, Me and Dupree': 2.5}, \
    'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 2.0}, \
    'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'The Night Listener': 3.0, 'Superman Returns': 5.0, \
     'You, Me and Dupree': 3.5}, \
    'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0, \
    'Superman Returns':4.0}, \
    'Maria Gabriela': {}}
    >>> model = MatrixPreferenceDataModel(movies)
    >>> similarity = UserSimilarity(model, cosine_distances, 3)
    >>> similarity['Marcel Caraciolo']
    [('Marcel Caraciolo', 1.0), ('Sheldom', 0.99127582693458016),
      ('Lorena Abreu', 0.98658676452792504)]

   '''

    def __init__(self, model, distance, num_best=None):
        BaseSimilarity.__init__(self, model, distance, num_best)

    def get_similarity(self, source_id, target_id):
        source_preferences = self.model.preferences_from_user(source_id)
        target_preferences = self.model.preferences_from_user(target_id)

        if self.model.has_preference_values():
            source_preferences, target_preferences = \
                find_common_elements(source_preferences, target_preferences)

        if source_preferences.ndim == 1 and target_preferences.ndim == 1:
            source_preferences = np.asarray([source_preferences])
            target_preferences = np.asarray([target_preferences])

        if self.distance == loglikehood_coefficient:
            return self.distance(self.model.items_count(), \
                source_preferences, target_preferences) \
                if not source_preferences.shape[1] == 0 and \
                not target_preferences.shape[1] == 0 else np.array([[np.nan]])

        #evaluate the similarity between the two users vectors.
        return self.distance(source_preferences, target_preferences) \
            if not source_preferences.shape[1] == 0 \
                and not target_preferences.shape[1] == 0 else np.array([[np.nan]])

    def get_similarities(self, source_id):
        return[(other_id, self.get_similarity(source_id, other_id))  for other_id, v in self.model]

    def __iter__(self):
        """
        For each object in model, compute the similarity function against all other objects and yield the result.
        """
        for source_id, preferences in self.model:
            yield source_id, self[source_id]

###############################################################################
# Item Similarity


class ItemSimilarity(BaseSimilarity):
    '''
    Returns the degree of similarity, of two items, based on its preferences by the users.
    Implementations of this class define a notion of similarity between two items.
    Implementations should  return values in the range 0.0 to 1.0, with 1.0 representing
    perfect similarity.

    Parameters
    ----------

    `model`:  DataModel
         Defines the data model where data is fetched.
    `distance`: Function
         Pairwise Function between two vectors.
     `num_best`: int
         If it is left unspecified, similarity queries return a full list (one
         float for every item in the model, including the query item).

         If `num_best` is set, queries return `num_best` most similar items, as a
         sorted list.

    Methods
    ---------

    get_similarity()
    Return similarity of the `source_id` to a specific `target_id` in the model.

    get_similarities()
    Return similarity of the `source_id` to all sources in the model.

    Examples
    ---------
    >>> from scikits.crab.models.classes import MatrixPreferenceDataModel
    >>> from scikits.crab.metrics.pairwise import cosine_distances
    >>> from scikits.crab.similarities.basic_similarities import ItemSimilarity
    >>> movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, \
     'Snakes on a Plane': 3.5, \
     'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \
     'The Night Listener': 3.0}, \
     'Paola Pow': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \
     'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 3.5}, \
    'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, \
     'Superman Returns': 3.5, 'The Night Listener': 4.0}, \
    'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, \
     'The Night Listener': 4.5, 'Superman Returns': 4.0, \
     'You, Me and Dupree': 2.5}, \
    'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, \
     'You, Me and Dupree': 2.0}, \
    'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \
     'The Night Listener': 3.0, 'Superman Returns': 5.0, \
     'You, Me and Dupree': 3.5}, \
    'Penny Frewman': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0, \
    'Superman Returns':4.0}, \
    'Maria Gabriela': {}}
    >>> model = MatrixPreferenceDataModel(movies)
    >>> similarity = ItemSimilarity(model, cosine_distances, 3)
    >>> similarity['The Night Listener']
    [('The Night Listener', 1.0), ('Lady in the Water', 0.98188311415053031),
        ('Just My Luck', 0.97489347126452108)]

    '''

    def __init__(self, model, distance, num_best=None):
        BaseSimilarity.__init__(self, model, distance, num_best)

    def get_similarity(self, source_id, target_id):
        source_preferences = self.model.preferences_for_item(source_id)
        target_preferences = self.model.preferences_for_item(target_id)

        if self.model.has_preference_values():
            source_preferences, target_preferences = \
                find_common_elements(source_preferences, target_preferences)

        if source_preferences.ndim == 1 and target_preferences.ndim == 1:
            source_preferences = np.asarray([source_preferences])
            target_preferences = np.asarray([target_preferences])

        if self.distance == loglikehood_coefficient:
            return self.distance(self.model.items_count(), \
                source_preferences, target_preferences) \
                if not source_preferences.shape[1] == 0 and \
                    not target_preferences.shape[1] == 0 else np.array([[np.nan]])

        #Evaluate the similarity between the two users vectors.
        return self.distance(source_preferences, target_preferences) \
            if not source_preferences.shape[1] == 0 and \
                not target_preferences.shape[1] == 0 else np.array([[np.nan]])

    def get_similarities(self, source_id):
        return [(other_id, self.get_similarity(source_id, other_id)) for other_id in self.model.item_ids()]

    def __iter__(self):
        """
        For each object in model, compute the similarity function against all other objects and yield the result.
        """
        for item_id in self.model.item_ids():
            yield item_id, self[item_id]

########NEW FILE########
__FILENAME__ = test_base
#-*- coding:utf-8 -*-

"""
Base Similarity Models.
"""

# Authors: Marcel Caraciolo <marcel@muricoca.com>
#          Bruno Melo <bruno@muricoca.com>
# License: BSD Style.

import unittest

from ..base import BaseSimilarity

#test classes


class MySimilarity(BaseSimilarity):
    def __init__(self, model, distance):
        BaseSimilarity.__init__(self, model, distance)

###############################################################################
# The tests


class testBaseSimilarity(unittest.TestCase):
    pass


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_basic_similarities
import numpy as np
from numpy.testing import assert_array_almost_equal, assert_array_equal
from nose.tools import assert_raises, assert_equals
from ..basic_similarities import UserSimilarity, ItemSimilarity, find_common_elements
from ...metrics.pairwise import cosine_distances, \
    pearson_correlation, euclidean_distances, manhattan_distances, jaccard_coefficient, \
    sorensen_coefficient, loglikehood_coefficient
from ...models.classes import  MatrixPreferenceDataModel, \
     MatrixBooleanPrefDataModel

#Simple Movies DataSet

movies = {'Marcel Caraciolo': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,
 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,
 'The Night Listener': 3.0},
'Luciana Nunes': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,
 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 3.5},
'Leopoldo Pires': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,
 'Superman Returns': 3.5, 'The Night Listener': 4.0},
'Lorena Abreu': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,
 'The Night Listener': 4.5, 'Superman Returns': 4.0,
 'You, Me and Dupree': 2.5},
'Steve Gates': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,
 'You, Me and Dupree': 2.0},
'Sheldom': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,
 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},
'Penny Frewman': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0, 'Superman Returns': 4.0},
'Maria Gabriela': {}}


def test_find_common_elements():
    #MatrixModel
    model_matrix = MatrixPreferenceDataModel(movies)
    source_preferences = model_matrix.preferences_from_user('Marcel Caraciolo')
    target_preferences = model_matrix.preferences_from_user('Leopoldo Pires')
    assert_array_equal(np.array([[2.5, 3.5, 3.5, 3.0]]), find_common_elements(source_preferences, target_preferences)[0])
    assert_array_equal(np.array([[2.5, 3.0, 3.5, 4.0]]), find_common_elements(source_preferences, target_preferences)[1])

    #MatrixModel
    source_preferences = model_matrix.preferences_from_user('Marcel Caraciolo')
    target_preferences = model_matrix.preferences_from_user('Luciana Nunes')
    assert_array_equal(np.array([[3.,  2.5,  3.5,  3.5,  3.,  2.5]]), find_common_elements(source_preferences, target_preferences)[0])
    assert_array_equal(np.array([[1.5,  3.,  3.5,  5.,  3.,  3.5]]), find_common_elements(source_preferences, target_preferences)[1])

    #MatrixModel
    source_preferences = model_matrix.preferences_from_user('Marcel Caraciolo')
    target_preferences = model_matrix.preferences_from_user('Maria Gabriela')
    assert_array_equal(np.array([[]]), find_common_elements(source_preferences, target_preferences)[0])
    assert_array_equal(np.array([[]]), find_common_elements(source_preferences, target_preferences)[1])

    #MatrixModel
    source_preferences = model_matrix.preferences_for_item('Snakes on a Plane')
    target_preferences = model_matrix.preferences_for_item('Superman Returns')
    assert_array_equal(np.array([[3.,  3.5,  3.5,  3.5,  4.5,  4.,  4.]]), find_common_elements(source_preferences, target_preferences)[0])
    assert_array_equal(np.array([[3.5,  4.,  5.,  3.5,  4.,  5.,  3.]]), find_common_elements(source_preferences, target_preferences)[1])

    model_matrix.set_preference('Maria Gabriela', 'Back to the Future', 3.5)

    source_preferences = model_matrix.preferences_for_item('Back to the Future')
    target_preferences = model_matrix.preferences_for_item('Superman Returns')
    assert_array_equal(np.array([[]]), find_common_elements(source_preferences, target_preferences)[0])
    assert_array_equal(np.array([[]]), find_common_elements(source_preferences, target_preferences)[1])


def test_get__item___UserSimilarity():
    #Cosine #With limits
    #MatrixModel
    model = MatrixPreferenceDataModel(movies)
    similarity = UserSimilarity(model, cosine_distances, 3)

    assert_array_equal(np.array([[1.]]), similarity['Marcel Caraciolo'][0][1])
    assert_equals('Marcel Caraciolo', similarity['Marcel Caraciolo'][0][0])

    assert_array_almost_equal(np.array([[0.99127583]]), similarity['Marcel Caraciolo'][1][1])
    assert_equals('Sheldom', similarity['Marcel Caraciolo'][1][0])

    assert_array_almost_equal(np.array([[0.98658676]]), similarity['Marcel Caraciolo'][2][1])
    assert_equals('Lorena Abreu', similarity['Marcel Caraciolo'][2][0])

    #Pearson Without limits
    similarity = UserSimilarity(model, pearson_correlation)

    assert_array_almost_equal(np.array([[1.]]), similarity['Leopoldo Pires'][0][1])
    assert_equals('Leopoldo Pires', similarity['Leopoldo Pires'][0][0])

    assert_array_almost_equal(np.array([[1.]]), similarity['Leopoldo Pires'][1][1])
    assert_equals('Lorena Abreu', similarity['Leopoldo Pires'][1][0])

    assert_array_almost_equal(np.array([[0.40451992]]), similarity['Leopoldo Pires'][2][1])
    assert_equals('Marcel Caraciolo', similarity['Leopoldo Pires'][2][0])

    assert_array_almost_equal(np.array([[0.2045983]]), similarity['Leopoldo Pires'][3][1])
    assert_equals('Luciana Nunes', similarity['Leopoldo Pires'][3][0])

    assert_array_almost_equal(np.array([[0.13483997]]), similarity['Leopoldo Pires'][4][1])
    assert_equals('Sheldom', similarity['Leopoldo Pires'][4][0])

    assert_array_almost_equal(np.array([[-0.25819889]]), similarity['Leopoldo Pires'][5][1])
    assert_equals('Steve Gates', similarity['Leopoldo Pires'][5][0])

    assert_array_almost_equal(np.array([[-1.]]), similarity['Leopoldo Pires'][6][1])
    assert_equals('Penny Frewman', similarity['Leopoldo Pires'][6][0])

    assert_array_almost_equal(np.array([[np.nan]]), similarity['Leopoldo Pires'][7][1])
    assert_equals('Maria Gabriela', similarity['Leopoldo Pires'][7][0])

    #Euclidean Without limits
    similarity = UserSimilarity(model, euclidean_distances)

    assert_array_equal(np.array([[1.]]), similarity['Steve Gates'][0][1])
    assert_equals('Steve Gates', similarity['Steve Gates'][0][0])

    assert_array_almost_equal(np.array([[0.41421356]]), similarity['Steve Gates'][1][1])
    assert_equals('Marcel Caraciolo', similarity['Steve Gates'][1][0])

    assert_array_almost_equal(np.array([[0.4]]), similarity['Steve Gates'][2][1])
    assert_equals('Penny Frewman', similarity['Steve Gates'][2][0])

    assert_array_almost_equal(np.array([[0.38742589]]), similarity['Steve Gates'][3][1])
    assert_equals('Leopoldo Pires', similarity['Steve Gates'][3][0])

    assert_array_almost_equal(np.array([[0.31451986]]), similarity['Steve Gates'][4][1])
    assert_equals('Lorena Abreu', similarity['Steve Gates'][4][0])

    assert_array_almost_equal(np.array([[0.28571429]]), similarity['Steve Gates'][5][1])
    assert_equals('Sheldom', similarity['Steve Gates'][5][0])

    assert_array_almost_equal(np.array([[0.2779263]]), similarity['Steve Gates'][6][1])
    assert_equals('Luciana Nunes', similarity['Steve Gates'][6][0])

    assert_array_almost_equal(np.array([[np.nan]]), similarity['Steve Gates'][7][1])
    assert_equals('Maria Gabriela', similarity['Steve Gates'][7][0])

    #Manhattan Without limits
    similarity = UserSimilarity(model, manhattan_distances, 0)

    assert_equals([], similarity['Steve Gates'])

    similarity = UserSimilarity(model, manhattan_distances, 20)

    assert_array_equal(np.array([[1.]]), similarity['Steve Gates'][0][1])
    assert_equals('Steve Gates', similarity['Steve Gates'][0][0])

    assert_array_almost_equal(np.array([[0.5]]), similarity['Steve Gates'][1][1])
    assert_equals('Marcel Caraciolo', similarity['Steve Gates'][1][0])

    assert_array_almost_equal(np.array([[0.3]]), similarity['Steve Gates'][2][1])
    assert_equals('Sheldom', similarity['Steve Gates'][2][0])

    assert_array_almost_equal(np.array([[0.25]]), similarity['Steve Gates'][3][1])
    assert_equals('Leopoldo Pires', similarity['Steve Gates'][3][0])

    assert_array_almost_equal(np.array([[0.25]]), similarity['Steve Gates'][4][1])
    assert_equals('Luciana Nunes', similarity['Steve Gates'][4][0])

    assert_array_almost_equal(np.array([[0.16666667]]), similarity['Steve Gates'][5][1])
    assert_equals('Penny Frewman', similarity['Steve Gates'][5][0])

    assert_array_almost_equal(np.array([[0.1]]), similarity['Steve Gates'][6][1])
    assert_equals('Lorena Abreu', similarity['Steve Gates'][6][0])

    assert_array_almost_equal(np.array([[np.nan]]), similarity['Steve Gates'][7][1])
    assert_equals('Maria Gabriela', similarity['Steve Gates'][7][0])

    #MatrixBooleanModel
    model = MatrixBooleanPrefDataModel(movies)
    similarity = UserSimilarity(model, jaccard_coefficient, 3)
    assert_array_equal(np.array([[1.]]), similarity['Marcel Caraciolo'][0][1])
    assert_equals('Luciana Nunes', similarity['Marcel Caraciolo'][0][0])

    assert_array_almost_equal(np.array([[1.]]), similarity['Marcel Caraciolo'][1][1])
    assert_equals('Marcel Caraciolo', similarity['Marcel Caraciolo'][1][0])

    assert_array_almost_equal(np.array([[1.]]), similarity['Marcel Caraciolo'][2][1])
    assert_equals('Steve Gates', similarity['Marcel Caraciolo'][2][0])

    #sorensen Without limits
    similarity = UserSimilarity(model, sorensen_coefficient)

    assert_array_almost_equal(np.array([[1.]]), similarity['Leopoldo Pires'][0][1])
    assert_equals('Leopoldo Pires', similarity['Leopoldo Pires'][0][0])

    assert_array_almost_equal(np.array([[0.88888889]]), similarity['Leopoldo Pires'][1][1])
    assert_equals('Sheldom', similarity['Leopoldo Pires'][1][0])

    assert_array_almost_equal(np.array([[0.8]]), similarity['Leopoldo Pires'][2][1])
    assert_equals('Luciana Nunes', similarity['Leopoldo Pires'][2][0])

    assert_array_almost_equal(np.array([[0.8]]), similarity['Leopoldo Pires'][3][1])
    assert_equals('Marcel Caraciolo', similarity['Leopoldo Pires'][3][0])

    assert_array_almost_equal(np.array([[0.8]]), similarity['Leopoldo Pires'][4][1])
    assert_equals('Steve Gates', similarity['Leopoldo Pires'][4][0])

    assert_array_almost_equal(np.array([[0.66666667]]), similarity['Leopoldo Pires'][5][1])
    assert_equals('Lorena Abreu', similarity['Leopoldo Pires'][5][0])

    assert_array_almost_equal(np.array([[0.57142857]]), similarity['Leopoldo Pires'][6][1])
    assert_equals('Penny Frewman', similarity['Leopoldo Pires'][6][0])

    assert_array_almost_equal(np.array([[0.]]), similarity['Leopoldo Pires'][7][1])
    assert_equals('Maria Gabriela', similarity['Leopoldo Pires'][7][0])

    #loglikehood with limits

    similarity = UserSimilarity(model, loglikehood_coefficient, 0)
    assert_equals([], similarity['Steve Gates'])

    similarity = UserSimilarity(model, loglikehood_coefficient, 20)

    assert_array_equal(np.array([[1.]]), similarity['Steve Gates'][0][1])
    assert_equals('Luciana Nunes', similarity['Steve Gates'][0][0])

    assert_array_almost_equal(np.array([[1.]]), similarity['Steve Gates'][1][1])
    assert_equals('Marcel Caraciolo', similarity['Steve Gates'][1][0])

    assert_array_almost_equal(np.array([[1.]]), similarity['Steve Gates'][2][1])
    assert_equals('Steve Gates', similarity['Steve Gates'][2][0])

    assert_array_almost_equal(np.array([[0.74804989]]), similarity['Steve Gates'][3][1])
    assert_equals('Lorena Abreu', similarity['Steve Gates'][3][0])

    assert_array_almost_equal(np.array([[0.74804989]]), similarity['Steve Gates'][4][1])
    assert_equals('Sheldom', similarity['Steve Gates'][4][0])

    assert_array_almost_equal(np.array([[0.65783229]]), similarity['Steve Gates'][5][1])
    assert_equals('Leopoldo Pires', similarity['Steve Gates'][5][0])

    assert_array_almost_equal(np.array([[0.55415805]]), similarity['Steve Gates'][6][1])
    assert_equals('Penny Frewman', similarity['Steve Gates'][6][0])

    assert_array_almost_equal(np.array([[0.0]]), similarity['Steve Gates'][7][1])
    assert_equals('Maria Gabriela', similarity['Steve Gates'][7][0])


def test_get_similarities__UserSimilarity():
    #MatrixModel
    model = MatrixPreferenceDataModel(movies)

    similarity = UserSimilarity(model, cosine_distances, 3)

    sim = similarity.get_similarities('Marcel Caraciolo')

    assert_equals(len(sim), model.users_count())

    similarity = UserSimilarity(model, pearson_correlation)

    sim = similarity.get_similarities('Leopoldo Pires')

    assert_equals(len(sim), model.users_count())

    similarity = UserSimilarity(model, euclidean_distances)

    sim = similarity.get_similarities('Steve Gates')

    assert_equals(len(sim), model.users_count())

    similarity = UserSimilarity(model, manhattan_distances, 0)

    sim = similarity.get_similarities('Steve Gates')

    assert_equals(len(sim), model.users_count())

    similarity = UserSimilarity(model, manhattan_distances, 20)

    sim = similarity.get_similarities('Steve Gates')

    assert_equals(len(sim), model.users_count())

    #MatrixBooleanPrefDataModel
    model = MatrixBooleanPrefDataModel(movies)

    similarity = UserSimilarity(model, sorensen_coefficient, 3)

    sim = similarity.get_similarities('Marcel Caraciolo')

    assert_equals(len(sim), model.users_count())

    similarity = UserSimilarity(model, loglikehood_coefficient)

    sim = similarity.get_similarities('Leopoldo Pires')

    assert_equals(len(sim), model.users_count())

    similarity = UserSimilarity(model, jaccard_coefficient)

    sim = similarity.get_similarities('Steve Gates')

    assert_equals(len(sim), model.users_count())

    similarity = UserSimilarity(model, loglikehood_coefficient, 0)

    sim = similarity.get_similarities('Steve Gates')

    assert_equals(len(sim), model.users_count())

    similarity = UserSimilarity(model, sorensen_coefficient, 20)

    sim = similarity.get_similarities('Steve Gates')

    assert_equals(len(sim), model.users_count())


def test__iter__UserSimilarity():
    #MatrixModel
    model = MatrixPreferenceDataModel(movies)
    similarity = UserSimilarity(model, cosine_distances, 3)

    source_ids = []
    prefs = []
    for source_id, preferences in similarity:
        source_ids.append(source_id)
        prefs.append(preferences)
    assert_equals(len(source_ids), model.users_count())

    for pref in prefs:
        assert_equals(len(pref), 3)

    similarity = UserSimilarity(model, pearson_correlation)

    source_ids = []
    prefs = []
    for source_id, preferences in similarity:
        source_ids.append(source_id)
        prefs.append(preferences)
    assert_equals(len(source_ids), model.users_count())

    for pref in prefs:
        assert_equals(len(pref), model.users_count())

    similarity = UserSimilarity(model, manhattan_distances, 0)

    source_ids = []
    prefs = []
    for source_id, preferences in similarity:
        source_ids.append(source_id)
        prefs.append(preferences)
    assert_equals(len(source_ids), model.users_count())

    for pref in prefs:
        assert_equals(len(pref), 0)

    similarity = UserSimilarity(model, manhattan_distances, 20)

    source_ids = []
    prefs = []
    for source_id, preferences in similarity:
        source_ids.append(source_id)
        prefs.append(preferences)
    assert_equals(len(source_ids), model.users_count())

    for pref in prefs:
        assert_equals(len(pref), model.users_count())

    #MatrixBooleanPrefDataModel
    model = MatrixBooleanPrefDataModel(movies)
    similarity = UserSimilarity(model, jaccard_coefficient, 3)

    source_ids = []
    prefs = []
    for source_id, preferences in similarity:
        source_ids.append(source_id)
        prefs.append(preferences)
    assert_equals(len(source_ids), model.users_count())

    for pref in prefs:
        assert_equals(len(pref), 3)

    similarity = UserSimilarity(model, loglikehood_coefficient)

    source_ids = []
    prefs = []
    for source_id, preferences in similarity:
        source_ids.append(source_id)
        prefs.append(preferences)
    assert_equals(len(source_ids), model.users_count())

    for pref in prefs:
        assert_equals(len(pref), model.users_count())

    similarity = UserSimilarity(model, sorensen_coefficient, 0)

    source_ids = []
    prefs = []
    for source_id, preferences in similarity:
        source_ids.append(source_id)
        prefs.append(preferences)
    assert_equals(len(source_ids), model.users_count())

    for pref in prefs:
        assert_equals(len(pref), 0)

    similarity = UserSimilarity(model, loglikehood_coefficient, 20)

    source_ids = []
    prefs = []
    for source_id, preferences in similarity:
        source_ids.append(source_id)
        prefs.append(preferences)
    assert_equals(len(source_ids), model.users_count())

    for pref in prefs:
        assert_equals(len(pref), model.users_count())


def test_get__item___ItemSimilarity():
    #MATRIXMODEL
    #Cosine #With limits
    model = MatrixPreferenceDataModel(movies)
    similarity = ItemSimilarity(model, cosine_distances, 3)

    assert_array_equal(np.array([[1.]]), similarity['Snakes on a Plane'][0][1])
    assert_equals('Snakes on a Plane', similarity['Snakes on a Plane'][0][0])

    assert_array_almost_equal(np.array([[0.99773877]]), similarity['Snakes on a Plane'][1][1])
    assert_equals('Lady in the Water', similarity['Snakes on a Plane'][1][0])

    assert_array_almost_equal(np.array([[0.9798780]]), similarity['Snakes on a Plane'][2][1])
    assert_equals('Superman Returns', similarity['Snakes on a Plane'][2][0])

    #Pearson Without limits
    similarity = ItemSimilarity(model, pearson_correlation)

    assert_array_equal(np.array([[1.]]), similarity['The Night Listener'][0][1])
    assert_equals('The Night Listener', similarity['The Night Listener'][0][0])

    assert_array_almost_equal(np.array([[0.55555556]]), similarity['The Night Listener'][1][1])
    assert_equals('Just My Luck', similarity['The Night Listener'][1][0])

    assert_array_almost_equal(np.array([[-0.17984719]]), similarity['The Night Listener'][2][1])
    assert_equals('Superman Returns', similarity['The Night Listener'][2][0])

    assert_array_almost_equal(np.array([[-0.25]]), similarity['The Night Listener'][3][1])
    assert_equals('You, Me and Dupree', similarity['The Night Listener'][3][0])

    assert_array_almost_equal(np.array([[-0.56635211]]), similarity['The Night Listener'][4][1])
    assert_equals('Snakes on a Plane', similarity['The Night Listener'][4][0])

    assert_array_almost_equal(np.array([[-0.61237244]]), similarity['The Night Listener'][5][1])
    assert_equals('Lady in the Water', similarity['The Night Listener'][5][0])

    assert_array_almost_equal(np.array([[np.nan]]), similarity['The Night Listener'][6][1])
    assert_equals('Back to the Future', similarity['The Night Listener'][6][0])

    similarity = ItemSimilarity(model, euclidean_distances)

    assert_array_equal(np.array([[1.]]), similarity['The Night Listener'][0][1])
    assert_equals('The Night Listener', similarity['The Night Listener'][0][0])

    assert_array_almost_equal(np.array([[0.38742589]]), similarity['The Night Listener'][1][1])
    assert_equals('Lady in the Water', similarity['The Night Listener'][1][0])

    assert_array_almost_equal(np.array([[0.32037724]]), similarity['The Night Listener'][2][1])
    assert_equals('Snakes on a Plane', similarity['The Night Listener'][2][0])

    assert_array_almost_equal(np.array([[0.29893508]]), similarity['The Night Listener'][3][1])
    assert_equals('Just My Luck', similarity['The Night Listener'][3][0])

    assert_array_almost_equal(np.array([[0.29429806]]), similarity['The Night Listener'][4][1])
    assert_equals('You, Me and Dupree', similarity['The Night Listener'][4][0])

    assert_array_almost_equal(np.array([[0.25265031]]), similarity['The Night Listener'][5][1])
    assert_equals('Superman Returns', similarity['The Night Listener'][5][0])

    assert_array_almost_equal(np.array([[np.nan]]), similarity['The Night Listener'][6][1])
    assert_equals('Back to the Future', similarity['The Night Listener'][6][0])

    similarity = ItemSimilarity(model, manhattan_distances, 0)

    assert_equals([], similarity['Lady in the Water'])

    similarity = ItemSimilarity(model, manhattan_distances, 20)

    assert_array_almost_equal(np.array([[1.]]), similarity['Snakes on a Plane'][0][1])
    assert_equals('Snakes on a Plane', similarity['Snakes on a Plane'][0][0])

    assert_array_almost_equal(np.array([[0.28571429]]), similarity['Snakes on a Plane'][1][1])
    assert_equals('Superman Returns', similarity['Snakes on a Plane'][1][0])

    assert_array_almost_equal(np.array([[0.2]]), similarity['Snakes on a Plane'][2][1])
    assert_equals('Lady in the Water', similarity['Snakes on a Plane'][2][0])

    assert_array_almost_equal(np.array([[0.16666667]]), similarity['Snakes on a Plane'][3][1])
    assert_equals('The Night Listener', similarity['Snakes on a Plane'][3][0])

    assert_array_almost_equal(np.array([[-0.25]]), similarity['Snakes on a Plane'][4][1])
    assert_equals('Just My Luck', similarity['Snakes on a Plane'][4][0])

    assert_array_almost_equal(np.array([[-0.33333333]]), similarity['Snakes on a Plane'][5][1])
    assert_equals('You, Me and Dupree', similarity['Snakes on a Plane'][5][0])

    #MatrixBooleanPrefDataModel
    #Jaccard #With limits
    model = MatrixBooleanPrefDataModel(movies)
    similarity = ItemSimilarity(model, jaccard_coefficient, 3)

    assert_array_equal(np.array([[1.]]), similarity['Snakes on a Plane'][0][1])
    assert_equals('Snakes on a Plane', similarity['Snakes on a Plane'][0][0])

    assert_array_almost_equal(np.array([[1.]]), similarity['Snakes on a Plane'][1][1])
    assert_equals('Superman Returns', similarity['Snakes on a Plane'][1][0])

    assert_array_almost_equal(np.array([[0.85714286]]), similarity['Snakes on a Plane'][2][1])
    assert_equals('The Night Listener', similarity['Snakes on a Plane'][2][0])

    #Sorensen Without limits
    similarity = ItemSimilarity(model, sorensen_coefficient)

    assert_array_equal(np.array([[1.]]), similarity['The Night Listener'][0][1])
    assert_equals('The Night Listener', similarity['The Night Listener'][0][0])

    assert_array_almost_equal(np.array([[0.92307692]]), similarity['The Night Listener'][1][1])
    assert_equals('Snakes on a Plane', similarity['The Night Listener'][1][0])

    assert_array_almost_equal(np.array([[0.92307692]]), similarity['The Night Listener'][2][1])
    assert_equals('Superman Returns', similarity['The Night Listener'][2][0])

    assert_array_almost_equal(np.array([[0.90909091]]), similarity['The Night Listener'][3][1])
    assert_equals('Lady in the Water', similarity['The Night Listener'][3][0])

    assert_array_almost_equal(np.array([[0.83333333]]), similarity['The Night Listener'][4][1])
    assert_equals('You, Me and Dupree', similarity['The Night Listener'][4][0])

    assert_array_almost_equal(np.array([[0.8]]), similarity['The Night Listener'][5][1])
    assert_equals('Just My Luck', similarity['The Night Listener'][5][0])

    assert_array_almost_equal(np.array([[0.]]), similarity['The Night Listener'][6][1])
    assert_equals('Back to the Future', similarity['The Night Listener'][6][0])

    similarity = ItemSimilarity(model, loglikehood_coefficient)

    assert_array_equal(np.array([[1.]]), similarity['The Night Listener'][0][1])
    assert_equals('Snakes on a Plane', similarity['The Night Listener'][0][0])

    assert_array_almost_equal(np.array([[1.]]), similarity['The Night Listener'][1][1])
    assert_equals('Superman Returns', similarity['The Night Listener'][1][0])

    assert_array_almost_equal(np.array([[1.]]), similarity['The Night Listener'][2][1])
    assert_equals('The Night Listener', similarity['The Night Listener'][2][0])

    assert_array_almost_equal(np.array([[0.74804989]]), similarity['The Night Listener'][3][1])
    assert_equals('Lady in the Water', similarity['The Night Listener'][3][0])

    assert_array_almost_equal(np.array([[0.65783229]]), similarity['The Night Listener'][4][1])
    assert_equals('Just My Luck', similarity['The Night Listener'][4][0])

    assert_array_almost_equal(np.array([[0.25087682]]), similarity['The Night Listener'][5][1])
    assert_equals('You, Me and Dupree', similarity['The Night Listener'][5][0])

    assert_array_almost_equal(np.array([[0.]]), similarity['The Night Listener'][6][1])
    assert_equals('Back to the Future', similarity['The Night Listener'][6][0])

    similarity = ItemSimilarity(model, jaccard_coefficient, 0)

    assert_equals([], similarity['Lady in the Water'])

    similarity = ItemSimilarity(model, sorensen_coefficient, 20)

    assert_array_almost_equal(np.array([[1.]]), similarity['Snakes on a Plane'][0][1])
    assert_equals('Snakes on a Plane', similarity['Snakes on a Plane'][0][0])

    assert_array_almost_equal(np.array([[1.]]), similarity['Snakes on a Plane'][1][1])
    assert_equals('Superman Returns', similarity['Snakes on a Plane'][1][0])

    assert_array_almost_equal(np.array([[0.92307692]]), similarity['Snakes on a Plane'][2][1])
    assert_equals('The Night Listener', similarity['Snakes on a Plane'][2][0])

    assert_array_almost_equal(np.array([[0.92307692]]), similarity['Snakes on a Plane'][3][1])
    assert_equals('You, Me and Dupree', similarity['Snakes on a Plane'][3][0])

    assert_array_almost_equal(np.array([[0.8333333333]]), similarity['Snakes on a Plane'][4][1])
    assert_equals('Lady in the Water', similarity['Snakes on a Plane'][4][0])

    assert_array_almost_equal(np.array([[0.72727272]]), similarity['Snakes on a Plane'][5][1])
    assert_equals('Just My Luck', similarity['Snakes on a Plane'][5][0])

    assert_array_almost_equal(np.array([[0.]]), similarity['Snakes on a Plane'][6][1])
    assert_equals('Back to the Future', similarity['Snakes on a Plane'][6][0])


def test_get_similarities__ItemSimilarity():
    #MatrixModel
    model = MatrixPreferenceDataModel(movies)

    similarity = ItemSimilarity(model, cosine_distances, 3)

    sim = similarity.get_similarities('Snakes on a Plane')

    assert_equals(len(sim), model.items_count())

    #Pearson Without limits
    similarity = ItemSimilarity(model, pearson_correlation)

    sim = similarity.get_similarities('Lady in the Water')

    assert_equals(len(sim), model.items_count())

    similarity = ItemSimilarity(model, euclidean_distances)

    sim = similarity.get_similarities('Lady in the Water')

    assert_equals(len(sim), model.items_count())

    similarity = ItemSimilarity(model, manhattan_distances, 0)

    sim = similarity.get_similarities('Lady in the Water')

    assert_equals(len(sim), model.items_count())

    similarity = ItemSimilarity(model, manhattan_distances, 20)

    sim = similarity.get_similarities('Lady in the Water')

    assert_equals(len(sim), model.items_count())

    #MatrixBooleanPrefDataModel
    model = MatrixBooleanPrefDataModel(movies)

    similarity = ItemSimilarity(model, jaccard_coefficient, 3)

    sim = similarity.get_similarities('Snakes on a Plane')

    assert_equals(len(sim), model.items_count())

    #Sorensen Without limits
    similarity = ItemSimilarity(model, sorensen_coefficient)

    sim = similarity.get_similarities('Lady in the Water')

    assert_equals(len(sim), model.items_count())

    similarity = ItemSimilarity(model, loglikehood_coefficient)

    sim = similarity.get_similarities('Lady in the Water')

    assert_equals(len(sim), model.items_count())

    similarity = ItemSimilarity(model, loglikehood_coefficient, 0)

    sim = similarity.get_similarities('Lady in the Water')

    assert_equals(len(sim), model.items_count())

    similarity = ItemSimilarity(model, sorensen_coefficient, 20)

    sim = similarity.get_similarities('Lady in the Water')

    assert_equals(len(sim), model.items_count())


def test__iter__ItemSimilarity():
    #MATRIXMODEL
    model = MatrixPreferenceDataModel(movies)
    similarity = ItemSimilarity(model, cosine_distances, 3)

    item_ids = []
    prefs = []
    for item_id, preferences in similarity:
        item_ids.append(item_id)
        prefs.append(preferences)
    assert_equals(len(item_ids), model.items_count())

    for pref in prefs:
        assert_equals(len(pref), 3)

    similarity = ItemSimilarity(model, pearson_correlation)

    item_ids = []
    prefs = []
    for item_id, preferences in similarity:
        item_ids.append(item_id)
        prefs.append(preferences)
    assert_equals(len(item_ids), model.items_count())

    for pref in prefs:
        assert_equals(len(pref), model.items_count())

    similarity = ItemSimilarity(model, manhattan_distances, 0)

    item_ids = []
    prefs = []
    for item_id, preferences in similarity:
        item_ids.append(item_id)
        prefs.append(preferences)
    assert_equals(len(item_ids), model.items_count())

    for pref in prefs:
        assert_equals(len(pref), 0)

    similarity = ItemSimilarity(model, manhattan_distances, 20)

    item_ids = []
    prefs = []
    for item_id, preferences in similarity:
        item_ids.append(item_id)
        prefs.append(preferences)
    assert_equals(len(item_ids), model.items_count())

    for pref in prefs:
        assert_equals(len(pref), model.items_count())

    #MatrixBooleanPrefDataModel
    model = MatrixBooleanPrefDataModel(movies)
    similarity = ItemSimilarity(model, sorensen_coefficient, 3)

    item_ids = []
    prefs = []
    for item_id, preferences in similarity:
        item_ids.append(item_id)
        prefs.append(preferences)
    assert_equals(len(item_ids), model.items_count())

    for pref in prefs:
        assert_equals(len(pref), 3)

    similarity = ItemSimilarity(model, jaccard_coefficient)

    item_ids = []
    prefs = []
    for item_id, preferences in similarity:
        item_ids.append(item_id)
        prefs.append(preferences)
    assert_equals(len(item_ids), model.items_count())

    for pref in prefs:
        assert_equals(len(pref), model.items_count())

    similarity = ItemSimilarity(model, loglikehood_coefficient, 0)

    item_ids = []
    prefs = []
    for item_id, preferences in similarity:
        item_ids.append(item_id)
        prefs.append(preferences)
    assert_equals(len(item_ids), model.items_count())

    for pref in prefs:
        assert_equals(len(pref), 0)

    similarity = ItemSimilarity(model, sorensen_coefficient, 20)

    item_ids = []
    prefs = []
    for item_id, preferences in similarity:
        item_ids.append(item_id)
        prefs.append(preferences)
    assert_equals(len(item_ids), model.items_count())

    for pref in prefs:
        assert_equals(len(pref), model.items_count())

########NEW FILE########
__FILENAME__ = test_base
#-*- coding:utf-8 -*-

"""
Base Recommender Models.
"""

# Authors: Marcel Caraciolo <marcel@muricoca.com>
#          Bruno Melo <bruno@muricoca.com>
# License: BSD Style.

import unittest

from ..base import BaseRecommender

#test classes


class MyRecommender(BaseRecommender):
    def __init__(self, model):
        BaseRecommender.__init__(self, model)

###############################################################################
# The tests


class testBaseRecommender(unittest.TestCase):
    pass

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = extmath
"""
Extended math utilities.
"""
# Authors: Marcel Caraciolo <marcel@muricoca.com>
# License: BSD
import math

try:
    import itertools
    combinations = itertools.combinations
except AttributeError:
    def combinations(seq, r=None):
        """Generator returning combinations of items from sequence <seq>
        taken <r> at a time. Order is not significant. If <r> is not given,
        the entire sequence is returned.
        """
        if r == None:
            r = len(seq)
        if r <= 0:
            yield []
        else:
            for i in xrange(len(seq)):
                for cc in combinations(seq[i + 1:], r - 1):
                    yield [seq[i]] + cc

try:
    factorial = math.factorial
except AttributeError:
    # math.factorial is only available in Python >= 2.6
    def factorial(x):
        # don't use reduce operator or 2to3 will fail.
        # ripped from http://www.joelbdalley.com/page.pl?38
        # Ensure that n is a Natural number
        n = abs(int(x))
        if n < 1:
            n = 1

        # Store n! in variable x
        x = 1

        # Compute n!
        for i in range(1, n + 1):
            x = i * x

        # Return n!
        return x

########NEW FILE########
__FILENAME__ = testing
"""Testing utilities."""

# Copyright (c) 2011 Marcel Caraciolo <marcel@muricoca.com>
# License: Simplified BSD


def assert_in(obj, in_=None, out_=None):
    """Checks that all names in `in_` as in `obj`, but no name
    in `out_` is."""
    if in_ is not None:
        for name in in_:
            assert name in obj
    if out_ is not None:
        for name in out_:
            assert name not in obj

########NEW FILE########

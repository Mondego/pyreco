__FILENAME__ = rgb_composite
#!/usr/bin/env python
#-*- coding:utf-8 -*-
#pylint: disable=W0223
"""
RGB Composite Image Demo

Last Update: Feb 13, 2012
keith.hughitt@nasa.gov

This example application demonstrates how SunPy can be used to build GUI
applications using PyQt. The purpose of this simple application is to
combine three JPEG 2000 images from Helioviewer.org into a single composite
RGB image and provide a mechanism to control the contribution of each color
channel to the final image.

The GUI was built using Qt 4 Designer. To generate the Python code associated
with rgb_composite.ui, use the pyuic4 tool, e.g.:

  pyuic4 rgb_composite.ui > Ui_RGBComposite.py
  
TODO:
    * Fix bug causing composite image plot to become distored when channel
      weights are adjusted.
    * Have file -> save call savefig() on the rgb image
    * Show actual dates below each image.
    * Wavelength adjustment support.
    * Refactor/simplify
    * Make all images expand to fill available space.
"""
import sys
import datetime
import sunpy
import Ui_RGBComposite
import numpy as np
from sunpy.net.helioviewer import HelioviewerClient
from sunpy.map import Map
from sunpy.util import toggle_pylab
from PyQt4 import QtGui, QtCore
from matplotlib import pyplot as plt
from matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure

def main():
    app = QtGui.QApplication(sys.argv)
    win = RGBCompositeImageApp()
    win.show()
    sys.exit(app.exec_())

class RGBCompositeImageApp(QtGui.QMainWindow):
    def __init__(self):
        QtGui.QMainWindow.__init__(self)
        self.ui = Ui_RGBComposite.Ui_RGBComposite()
        self.ui.setupUi(self)
        
        # Helioviewer client
        self._hv = HelioviewerClient()
        self._datasources = None
        
        # Loaded images
        self.red = None
        self.green = None
        self.blue = None
        
        # Color channel weights
        self._weights=[1., 1., 1.]
        
        # Setup UI
        self._load_data_sources()
        
        # Load initial data
        self._load_defaults()
        
        # Initialize event handlers
        self._initEvents()
        
    def _load_data_sources(self):
        """Downloads and displays latest images for default wavelengths"""
        self._datasources = self._hv.get_data_sources()['SDO']['AIA']['AIA']
        sorted_datasources = sorted(self._datasources, key=int)
        
        for wl in sorted_datasources:
            self.ui.redWavelengthSelect.addItem(wl, self._datasources[wl])
            self.ui.greenWavelengthSelect.addItem(wl, self._datasources[wl])
            self.ui.blueWavelengthSelect.addItem(wl, self._datasources[wl])
            
        # Default wavelengths: 304, 193, 171
        self.ui.redWavelengthSelect.setCurrentIndex(5)
        self.ui.greenWavelengthSelect.setCurrentIndex(3)
        self.ui.blueWavelengthSelect.setCurrentIndex(2)
        
    def _load_defaults(self):
        """Load initial images"""
        now = datetime.datetime.utcnow()
        self.ui.dateTimeEdit.setDateTime(now)
        
        r = self._hv.download_jp2(now, sourceId=self._datasources['304']['sourceId'])
        g = self._hv.download_jp2(now, sourceId=self._datasources['193']['sourceId'])
        b = self._hv.download_jp2(now, sourceId=self._datasources['171']['sourceId'])
        
        self.red = sunpy.make_map(r)
        self.green = sunpy.make_map(g)
        self.blue = sunpy.make_map(b)
        
        self._updateRedPreview()
        self._updateGreenPreview()
        self._updateBluePreview()
        
        self._createCompositeImage()
        
    def _updateCompositeImage(self):
        """Updates the composite image"""
        # Remove old composite
        rgb = RGBCompositeMap(self.red, self.green, self.blue)
        self.ui.compositeContainer.removeWidget(self.ui.compositeImage)
        self.ui.compositeImage.close()
        
        # Plot new one
        self.ui.compositeImage = RGBCompositePlot(rgb, 512, 512)
        self.ui.compositeImage.set_rgb_weights(self._weights)
        self.ui.compositeContainer.addWidget(self.ui.compositeImage, 1)
        self.ui.compositeContainer.update()
        
    def _createCompositeImage(self):
        """Creates an initial composite image plot"""
        rgb = RGBCompositeMap(self.red, self.green, self.blue)
        self.ui.compositeContainer.removeWidget(self.ui.compositePlaceholder)
        self.ui.compositePlaceholder.close()
        self.ui.compositeImage = RGBCompositePlot(rgb, 512, 512)
        self.ui.compositeContainer.addWidget(self.ui.compositeImage, 1)
        self.ui.compositeContainer.update()

    def _updateRedPreview(self):
        """Updates the red preview image"""
        if hasattr(self.ui, "redPreviewImage"):
            self.ui.redPreview.removeWidget(self.ui.redPreviewImage)
            self.ui.redPreviewImage.close()
        else:
            self.ui.redPreview.removeWidget(self.ui.redPlaceholder)
            self.ui.redPlaceholder.close()
            
        self.ui.redPreviewImage = SunPyPlot(self.red, 256, 256) #cmap=cm.Reds_r
        self.ui.redPreview.addWidget(self.ui.redPreviewImage, 1)
        self.ui.redPreview.update()
        
    def _updateGreenPreview(self):
        """Updates the green preview image"""
        if hasattr(self.ui, "greenPreviewImage"):
            self.ui.greenPreview.removeWidget(self.ui.greenPreviewImage)
            self.ui.greenPreviewImage.close()
        else:
            self.ui.greenPreview.removeWidget(self.ui.greenPlaceholder)
            self.ui.greenPlaceholder.close()

        self.ui.greenPreviewImage = SunPyPlot(self.green, 256, 256) #cmap=cm.Greens_r
        self.ui.greenPreview.addWidget(self.ui.greenPreviewImage, 1)
        self.ui.greenPreview.update()
        
    def _updateBluePreview(self):
        """Updates the blue preview image"""
        if hasattr(self.ui, "bluePreviewImage"):
            self.ui.bluePreview.removeWidget(self.ui.bluePreviewImage)
            self.ui.bluePreviewImage.close() 
        else:
            self.ui.bluePreview.removeWidget(self.ui.bluePlaceholder)
            self.ui.bluePlaceholder.close()    
        
        self.ui.bluePreviewImage = SunPyPlot(self.blue, 256, 256) #cmap=cm.Blues_r
        self.ui.bluePreview.addWidget(self.ui.bluePreviewImage, 1)
        self.ui.bluePreview.update()
        
    def _initEvents(self):
        """Initialize event handlers"""
        self.connect(self.ui.redWeightSlider, QtCore.SIGNAL('valueChanged(int)'), self.onRedWeightChange)
        self.connect(self.ui.greenWeightSlider, QtCore.SIGNAL('valueChanged(int)'), self.onGreenWeightChange)
        self.connect(self.ui.blueWeightSlider, QtCore.SIGNAL('valueChanged(int)'), self.onBlueWeightChange)
        self.connect(self.ui.dateTimeEdit, QtCore.SIGNAL('dateTimeChanged(QDateTime)'), self.onDateChange)
        self.connect(self.ui.dateTimeEdit, QtCore.SIGNAL('dateTimeChanged(QDateTime)'), self.onDateChange)
        self.connect(self.ui.actionSave, QtCore.SIGNAL('activated()'), self.onSaveClick)
        
    def onRedWeightChange(self, value):
        """Red channel weight changed"""
        self._weights[0] = value / 100.
        self.ui.compositeImage.set_rgb_weights((self._weights))
        
    def onGreenWeightChange(self, value):
        """Green channel weight changed"""
        self._weights[1] = value / 100.
        self.ui.compositeImage.set_rgb_weights((self._weights))
        
    def onBlueWeightChange(self, value):
        """Blue channel weight changed"""
        self._weights[2] = value / 100.
        self.ui.compositeImage.set_rgb_weights((self._weights))
        
    def onDateChange(self, qdatetime):
        """Updates the images when the date is changed"""
        dt = qdatetime.toPyDateTime()

        r = self._hv.download_jp2(dt, sourceId=self._datasources['304']['sourceId'])
        g = self._hv.download_jp2(dt, sourceId=self._datasources['193']['sourceId'])
        b = self._hv.download_jp2(dt, sourceId=self._datasources['171']['sourceId'])
        
        self.red = sunpy.make_map(r)
        self.green = sunpy.make_map(g)
        self.blue = sunpy.make_map(b)
        
        self._updateRedPreview()
        self._updateGreenPreview()
        self._updateBluePreview()

        self._updateCompositeImage()
        
    def onSaveClick(self):
        """Save the composite image"""
        filename = QtGui.QFileDialog.getSaveFileName(self, "Save image", "composite.png")
        self.ui.compositeImage.figure.savefig(str(filename))

class SunPyPlot(FigureCanvas):
    """SunPy preview image"""
    def __init__(self, map_, width, height, parent=None, dpi=100, 
                 **matplot_args): #pylint: disable=W0613
        #self._widthHint = width
        #self._heightHint = height
        
        self._origMap = map_
        self._map = map_.resample((width, height))
        
        # Old way (segfaults in some environements)
        #self.figure = self._map.plot_simple(**matplot_args)
        #FigureCanvas.__init__(self, self.figure)
        
        self.figure = Figure()
        self._map.plot(figure=self.figure, basic_plot=True, **matplot_args)
        self.axes = self.figure.gca()
        FigureCanvas.__init__(self, self.figure)
        
        # How can we get the canvas to preserve its aspect ratio when expanding?
        #sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Preferred, QtGui.QSizePolicy.Preferred)
        #sizePolicy.setHeightForWidth(True)
        #self.setSizePolicy(sizePolicy)
        
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Fixed)
        self.setSizePolicy(sizePolicy)
        self.setMinimumSize(QtCore.QSize(width, height))
        self.setMaximumSize(QtCore.QSize(width, height))   
        #FigureCanvas.updateGeometry(self)        

class PreviewImagePlot(SunPyPlot):
    """Qt representation of a preview thumbnail"""
    def __init__(self, map_, width, height, parent=None, dpi=100, **matplot_args):
        SunPyPlot.__init__(self, map_, width, height, parent=None, dpi=100, **matplot_args)
        
class RGBCompositePlot(SunPyPlot):
    """Qt representation of an RGB composite image"""
    def __init__(self, map_, width, height, parent=None, dpi=100, **matplot_args):
        SunPyPlot.__init__(self, map_, width, height, parent=None, dpi=100, **matplot_args)
    
    def set_rgb_weights(self, weights):
        """Update RGB Composite image weights"""
        weightArray = np.ones((self._map.shape[0], self._map.shape[1], 3))
        
        # Rescale data using new weights
        for i in range(3):
            weightArray[:,:,i] *= weights[i]
        new_data = (self._map * weightArray).astype(np.uint8)
        
        # Update AxesImage data (faster than calling imshow again)
        ax = self.figure.get_axes()[0].images[0] 
        ax.set_data(new_data)
        self.draw()

#    def heightForWidth(self, width):
#        """Preserves 1:1 aspect ratio"""
#        return width
#    
#    def sizeHint(self):
#        """Preview image default size"""
#        return QtCore.QSize(self._widthHint, self._heightHint)
    
class RGBCompositeMap(sunpy.MapCube):
    """A composite map where each color channel is associated with a separate
       datasource."""
    def __new__(cls, red, green, blue, **kwargs):
        headers = []
        data = np.zeros((red.shape[0], red.shape[1], 3), dtype=np.uint8)
        
        # convert input to maps
        for i, item in enumerate([red, green, blue]):
            if isinstance(item, Map):
                map_ = item
            else:
                map_ = Map.read(item)
                
            data[:,:,i] = map_
            headers.append(map_.get_header(original=True))

        obj = np.asarray(data).view(cls)
        obj._headers = headers

        return obj

    def __init__(self, *args, **kwargs):
        sunpy.MapCube.__init__(self, args, kwargs)
        
    def resample(self, dimensions, method='linear'):
        """Returns a new Map that has been resampled up or down
        
        See `sunpy.map.Map.resample`
        """
        resampled = []
        
        for map_ in self.transpose(2, 0, 1): #pylint: disable=E1101
            resampled.append(map_.resample(dimensions, method))

        return self.__class__(*resampled)
        
    @toggle_pylab
    def plot(self, figure=None, basic_plot=None, **matplot_args):
        """Plots the map object using matplotlib
        
        Parameters
        ----------
        **matplot_args : dict
            Matplotlib Any additional imshow arguments that should be used
            when plotting the image.
        """
        if figure is None:
            figure = plt.figure(frameon=False)
        
        axes = plt.Axes(figure, [0., 0., 1., 1.])
        axes.set_axis_off()
        figure.add_axes(axes)

        axes.imshow(self, origin='lower', aspect='normal', **matplot_args)
        return figure

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = Ui_RGBComposite
# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'rgb_composite.ui'
#
# Created: Tue Feb 14 13:31:49 2012
#      by: PyQt4 UI code generator 4.8.5
#
# WARNING! All changes made in this file will be lost!

from PyQt4 import QtCore, QtGui

try:
    _fromUtf8 = QtCore.QString.fromUtf8
except AttributeError:
    _fromUtf8 = lambda s: s

class Ui_RGBComposite(object):
    def setupUi(self, RGBComposite):
        RGBComposite.setObjectName(_fromUtf8("RGBComposite"))
        RGBComposite.resize(1080, 972)
        RGBComposite.setWindowTitle(QtGui.QApplication.translate("RGBComposite", "SunPy - RGB Composite Image Demo", None, QtGui.QApplication.UnicodeUTF8))
        self.centralwidget = QtGui.QWidget(RGBComposite)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Expanding)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.centralwidget.sizePolicy().hasHeightForWidth())
        self.centralwidget.setSizePolicy(sizePolicy)
        self.centralwidget.setObjectName(_fromUtf8("centralwidget"))
        self.verticalLayoutWidget = QtGui.QWidget(self.centralwidget)
        self.verticalLayoutWidget.setGeometry(QtCore.QRect(-1, -1, 1031, 875))
        self.verticalLayoutWidget.setObjectName(_fromUtf8("verticalLayoutWidget"))
        self.outerContainer = QtGui.QVBoxLayout(self.verticalLayoutWidget)
        self.outerContainer.setSizeConstraint(QtGui.QLayout.SetMinimumSize)
        self.outerContainer.setMargin(0)
        self.outerContainer.setMargin(0)
        self.outerContainer.setObjectName(_fromUtf8("outerContainer"))
        self.compositeContainer = QtGui.QHBoxLayout()
        self.compositeContainer.setContentsMargins(-1, 10, -1, -1)
        self.compositeContainer.setObjectName(_fromUtf8("compositeContainer"))
        self.compositePlaceholder = QtGui.QWidget(self.verticalLayoutWidget)
        self.compositePlaceholder.setEnabled(True)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Fixed)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.compositePlaceholder.sizePolicy().hasHeightForWidth())
        self.compositePlaceholder.setSizePolicy(sizePolicy)
        self.compositePlaceholder.setMinimumSize(QtCore.QSize(512, 512))
        self.compositePlaceholder.setMaximumSize(QtCore.QSize(512, 512))
        self.compositePlaceholder.setObjectName(_fromUtf8("compositePlaceholder"))
        self.compositeContainer.addWidget(self.compositePlaceholder)
        self.outerContainer.addLayout(self.compositeContainer)
        self.dateForm = QtGui.QHBoxLayout()
        self.dateForm.setContentsMargins(10, -1, 10, -1)
        self.dateForm.setObjectName(_fromUtf8("dateForm"))
        self.label = QtGui.QLabel(self.verticalLayoutWidget)
        self.label.setText(QtGui.QApplication.translate("RGBComposite", "Date:", None, QtGui.QApplication.UnicodeUTF8))
        self.label.setObjectName(_fromUtf8("label"))
        self.dateForm.addWidget(self.label)
        self.dateTimeEdit = QtGui.QDateTimeEdit(self.verticalLayoutWidget)
        self.dateTimeEdit.setMinimumSize(QtCore.QSize(220, 0))
        self.dateTimeEdit.setTime(QtCore.QTime(15, 0, 0))
        self.dateTimeEdit.setDisplayFormat(QtGui.QApplication.translate("RGBComposite", "yyyy/MM/dd HH:mm:ss UT", None, QtGui.QApplication.UnicodeUTF8))
        self.dateTimeEdit.setCalendarPopup(True)
        self.dateTimeEdit.setTimeSpec(QtCore.Qt.UTC)
        self.dateTimeEdit.setObjectName(_fromUtf8("dateTimeEdit"))
        self.dateForm.addWidget(self.dateTimeEdit)
        spacerItem = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)
        self.dateForm.addItem(spacerItem)
        self.outerContainer.addLayout(self.dateForm)
        self.wavelengthForm = QtGui.QHBoxLayout()
        self.wavelengthForm.setMargin(10)
        self.wavelengthForm.setObjectName(_fromUtf8("wavelengthForm"))
        self.redContainer = QtGui.QVBoxLayout()
        self.redContainer.setObjectName(_fromUtf8("redContainer"))
        self.redLabel = QtGui.QLabel(self.verticalLayoutWidget)
        self.redLabel.setText(QtGui.QApplication.translate("RGBComposite", "Red", None, QtGui.QApplication.UnicodeUTF8))
        self.redLabel.setAlignment(QtCore.Qt.AlignCenter)
        self.redLabel.setObjectName(_fromUtf8("redLabel"))
        self.redContainer.addWidget(self.redLabel)
        self.redPreview = QtGui.QHBoxLayout()
        self.redPreview.setObjectName(_fromUtf8("redPreview"))
        self.redPlaceholder = QtGui.QWidget(self.verticalLayoutWidget)
        self.redPlaceholder.setEnabled(True)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Fixed)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.redPlaceholder.sizePolicy().hasHeightForWidth())
        self.redPlaceholder.setSizePolicy(sizePolicy)
        self.redPlaceholder.setMinimumSize(QtCore.QSize(128, 128))
        self.redPlaceholder.setMaximumSize(QtCore.QSize(128, 128))
        self.redPlaceholder.setObjectName(_fromUtf8("redPlaceholder"))
        self.redPreview.addWidget(self.redPlaceholder)
        self.redContainer.addLayout(self.redPreview)
        self.redWavelength = QtGui.QHBoxLayout()
        self.redWavelength.setObjectName(_fromUtf8("redWavelength"))
        self.redWavelengthLabel = QtGui.QLabel(self.verticalLayoutWidget)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Preferred)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.redWavelengthLabel.sizePolicy().hasHeightForWidth())
        self.redWavelengthLabel.setSizePolicy(sizePolicy)
        self.redWavelengthLabel.setMinimumSize(QtCore.QSize(90, 0))
        self.redWavelengthLabel.setMaximumSize(QtCore.QSize(16777215, 16777215))
        self.redWavelengthLabel.setText(QtGui.QApplication.translate("RGBComposite", "Wavelength: ", None, QtGui.QApplication.UnicodeUTF8))
        self.redWavelengthLabel.setObjectName(_fromUtf8("redWavelengthLabel"))
        self.redWavelength.addWidget(self.redWavelengthLabel)
        self.redWavelengthSelect = QtGui.QComboBox(self.verticalLayoutWidget)
        self.redWavelengthSelect.setMinimumSize(QtCore.QSize(130, 0))
        self.redWavelengthSelect.setObjectName(_fromUtf8("redWavelengthSelect"))
        self.redWavelength.addWidget(self.redWavelengthSelect)
        spacerItem1 = QtGui.QSpacerItem(30, 20, QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)
        self.redWavelength.addItem(spacerItem1)
        self.redContainer.addLayout(self.redWavelength)
        self.redWeight = QtGui.QHBoxLayout()
        self.redWeight.setObjectName(_fromUtf8("redWeight"))
        self.redWeightLabel = QtGui.QLabel(self.verticalLayoutWidget)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Preferred)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.redWeightLabel.sizePolicy().hasHeightForWidth())
        self.redWeightLabel.setSizePolicy(sizePolicy)
        self.redWeightLabel.setMinimumSize(QtCore.QSize(90, 0))
        self.redWeightLabel.setText(QtGui.QApplication.translate("RGBComposite", "Weight: ", None, QtGui.QApplication.UnicodeUTF8))
        self.redWeightLabel.setObjectName(_fromUtf8("redWeightLabel"))
        self.redWeight.addWidget(self.redWeightLabel)
        self.redWeightSlider = QtGui.QSlider(self.verticalLayoutWidget)
        self.redWeightSlider.setMaximum(100)
        self.redWeightSlider.setSingleStep(5)
        self.redWeightSlider.setProperty("value", 100)
        self.redWeightSlider.setOrientation(QtCore.Qt.Horizontal)
        self.redWeightSlider.setObjectName(_fromUtf8("redWeightSlider"))
        self.redWeight.addWidget(self.redWeightSlider)
        self.redContainer.addLayout(self.redWeight)
        self.wavelengthForm.addLayout(self.redContainer)
        self.greenContainer = QtGui.QVBoxLayout()
        self.greenContainer.setObjectName(_fromUtf8("greenContainer"))
        self.greenLabel = QtGui.QLabel(self.verticalLayoutWidget)
        self.greenLabel.setText(QtGui.QApplication.translate("RGBComposite", "Green", None, QtGui.QApplication.UnicodeUTF8))
        self.greenLabel.setAlignment(QtCore.Qt.AlignCenter)
        self.greenLabel.setObjectName(_fromUtf8("greenLabel"))
        self.greenContainer.addWidget(self.greenLabel)
        self.greenPreview = QtGui.QHBoxLayout()
        self.greenPreview.setObjectName(_fromUtf8("greenPreview"))
        self.greenPlaceholder = QtGui.QWidget(self.verticalLayoutWidget)
        self.greenPlaceholder.setEnabled(True)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Fixed)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.greenPlaceholder.sizePolicy().hasHeightForWidth())
        self.greenPlaceholder.setSizePolicy(sizePolicy)
        self.greenPlaceholder.setMinimumSize(QtCore.QSize(128, 128))
        self.greenPlaceholder.setMaximumSize(QtCore.QSize(128, 128))
        self.greenPlaceholder.setObjectName(_fromUtf8("greenPlaceholder"))
        self.greenPreview.addWidget(self.greenPlaceholder)
        self.greenContainer.addLayout(self.greenPreview)
        self.greenWavelength = QtGui.QHBoxLayout()
        self.greenWavelength.setObjectName(_fromUtf8("greenWavelength"))
        self.greenWavelengthLabel = QtGui.QLabel(self.verticalLayoutWidget)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Preferred)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.greenWavelengthLabel.sizePolicy().hasHeightForWidth())
        self.greenWavelengthLabel.setSizePolicy(sizePolicy)
        self.greenWavelengthLabel.setMinimumSize(QtCore.QSize(90, 0))
        self.greenWavelengthLabel.setMaximumSize(QtCore.QSize(16777215, 16777215))
        self.greenWavelengthLabel.setText(QtGui.QApplication.translate("RGBComposite", "Wavelength: ", None, QtGui.QApplication.UnicodeUTF8))
        self.greenWavelengthLabel.setObjectName(_fromUtf8("greenWavelengthLabel"))
        self.greenWavelength.addWidget(self.greenWavelengthLabel)
        self.greenWavelengthSelect = QtGui.QComboBox(self.verticalLayoutWidget)
        self.greenWavelengthSelect.setMinimumSize(QtCore.QSize(130, 0))
        self.greenWavelengthSelect.setObjectName(_fromUtf8("greenWavelengthSelect"))
        self.greenWavelength.addWidget(self.greenWavelengthSelect)
        spacerItem2 = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)
        self.greenWavelength.addItem(spacerItem2)
        self.greenContainer.addLayout(self.greenWavelength)
        self.greenWeight = QtGui.QHBoxLayout()
        self.greenWeight.setObjectName(_fromUtf8("greenWeight"))
        self.greenWeightLabel = QtGui.QLabel(self.verticalLayoutWidget)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Preferred)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.greenWeightLabel.sizePolicy().hasHeightForWidth())
        self.greenWeightLabel.setSizePolicy(sizePolicy)
        self.greenWeightLabel.setMinimumSize(QtCore.QSize(90, 0))
        self.greenWeightLabel.setText(QtGui.QApplication.translate("RGBComposite", "Weight: ", None, QtGui.QApplication.UnicodeUTF8))
        self.greenWeightLabel.setObjectName(_fromUtf8("greenWeightLabel"))
        self.greenWeight.addWidget(self.greenWeightLabel)
        self.greenWeightSlider = QtGui.QSlider(self.verticalLayoutWidget)
        self.greenWeightSlider.setMaximum(100)
        self.greenWeightSlider.setProperty("value", 100)
        self.greenWeightSlider.setOrientation(QtCore.Qt.Horizontal)
        self.greenWeightSlider.setObjectName(_fromUtf8("greenWeightSlider"))
        self.greenWeight.addWidget(self.greenWeightSlider)
        self.greenContainer.addLayout(self.greenWeight)
        self.wavelengthForm.addLayout(self.greenContainer)
        self.blueContainer = QtGui.QVBoxLayout()
        self.blueContainer.setObjectName(_fromUtf8("blueContainer"))
        self.blueLabel = QtGui.QLabel(self.verticalLayoutWidget)
        self.blueLabel.setText(QtGui.QApplication.translate("RGBComposite", "Blue", None, QtGui.QApplication.UnicodeUTF8))
        self.blueLabel.setAlignment(QtCore.Qt.AlignCenter)
        self.blueLabel.setObjectName(_fromUtf8("blueLabel"))
        self.blueContainer.addWidget(self.blueLabel)
        self.bluePreview = QtGui.QHBoxLayout()
        self.bluePreview.setObjectName(_fromUtf8("bluePreview"))
        self.bluePlaceholder = QtGui.QWidget(self.verticalLayoutWidget)
        self.bluePlaceholder.setEnabled(True)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Fixed)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.bluePlaceholder.sizePolicy().hasHeightForWidth())
        self.bluePlaceholder.setSizePolicy(sizePolicy)
        self.bluePlaceholder.setMinimumSize(QtCore.QSize(128, 128))
        self.bluePlaceholder.setMaximumSize(QtCore.QSize(128, 128))
        self.bluePlaceholder.setObjectName(_fromUtf8("bluePlaceholder"))
        self.bluePreview.addWidget(self.bluePlaceholder)
        self.blueContainer.addLayout(self.bluePreview)
        self.blueWavelength_2 = QtGui.QHBoxLayout()
        self.blueWavelength_2.setObjectName(_fromUtf8("blueWavelength_2"))
        self.blueWavelengthLabel_2 = QtGui.QLabel(self.verticalLayoutWidget)
        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Preferred)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.blueWavelengthLabel_2.sizePolicy().hasHeightForWidth())
        self.blueWavelengthLabel_2.setSizePolicy(sizePolicy)
        self.blueWavelengthLabel_2.setMinimumSize(QtCore.QSize(90, 0))
        self.blueWavelengthLabel_2.setMaximumSize(QtCore.QSize(16777215, 16777215))
        self.blueWavelengthLabel_2.setText(QtGui.QApplication.translate("RGBComposite", "Wavelength: ", None, QtGui.QApplication.UnicodeUTF8))
        self.blueWavelengthLabel_2.setObjectName(_fromUtf8("blueWavelengthLabel_2"))
        self.blueWavelength_2.addWidget(self.blueWavelengthLabel_2)
        self.blueWavelengthSelect = QtGui.QComboBox(self.verticalLayoutWidget)
        self.blueWavelengthSelect.setMinimumSize(QtCore.QSize(130, 0))
        self.blueWavelengthSelect.setObjectName(_fromUtf8("blueWavelengthSelect"))
        self.blueWavelength_2.addWidget(self.blueWavelengthSelect)
        spacerItem3 = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)
        self.blueWavelength_2.addItem(spacerItem3)
        self.blueContainer.addLayout(self.blueWavelength_2)
        self.blueWeight = QtGui.QHBoxLayout()
        self.blueWeight.setObjectName(_fromUtf8("blueWeight"))
        self.blueWeightLabel = QtGui.QLabel(self.verticalLayoutWidget)
        self.blueWeightLabel.setText(QtGui.QApplication.translate("RGBComposite", "Weight: ", None, QtGui.QApplication.UnicodeUTF8))
        self.blueWeightLabel.setObjectName(_fromUtf8("blueWeightLabel"))
        self.blueWeight.addWidget(self.blueWeightLabel)
        self.blueWeightSlider = QtGui.QSlider(self.verticalLayoutWidget)
        self.blueWeightSlider.setMaximum(100)
        self.blueWeightSlider.setProperty("value", 100)
        self.blueWeightSlider.setOrientation(QtCore.Qt.Horizontal)
        self.blueWeightSlider.setObjectName(_fromUtf8("blueWeightSlider"))
        self.blueWeight.addWidget(self.blueWeightSlider)
        self.blueContainer.addLayout(self.blueWeight)
        self.wavelengthForm.addLayout(self.blueContainer)
        self.outerContainer.addLayout(self.wavelengthForm)
        RGBComposite.setCentralWidget(self.centralwidget)
        self.statusbar = QtGui.QStatusBar(RGBComposite)
        self.statusbar.setObjectName(_fromUtf8("statusbar"))
        RGBComposite.setStatusBar(self.statusbar)
        self.menubar = QtGui.QMenuBar(RGBComposite)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 1080, 25))
        self.menubar.setObjectName(_fromUtf8("menubar"))
        self.menuFile = QtGui.QMenu(self.menubar)
        self.menuFile.setTitle(QtGui.QApplication.translate("RGBComposite", "&File", None, QtGui.QApplication.UnicodeUTF8))
        self.menuFile.setObjectName(_fromUtf8("menuFile"))
        RGBComposite.setMenuBar(self.menubar)
        self.actionSave = QtGui.QAction(RGBComposite)
        self.actionSave.setText(QtGui.QApplication.translate("RGBComposite", "&Save", None, QtGui.QApplication.UnicodeUTF8))
        self.actionSave.setShortcut(QtGui.QApplication.translate("RGBComposite", "Ctrl+S", None, QtGui.QApplication.UnicodeUTF8))
        self.actionSave.setObjectName(_fromUtf8("actionSave"))
        self.menuFile.addAction(self.actionSave)
        self.menubar.addAction(self.menuFile.menuAction())

        self.retranslateUi(RGBComposite)
        QtCore.QMetaObject.connectSlotsByName(RGBComposite)

    def retranslateUi(self, RGBComposite):
        pass


########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
# Licensed under a 3-clause BSD style license - see LICENSE.rst
#
# Astropy documentation build configuration file.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this file.
#
# All configuration values have a default. Some values are defined in
# the global Astropy configuration which is loaded here before anything else.
# See astropy.sphinx.conf for which values are set there.

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
# sys.path.insert(0, os.path.abspath('..'))
# IMPORTANT: the above commented section was generated by sphinx-quickstart, but
# is *NOT* appropriate for astropy or Astropy affiliated packages. It is left
# commented out with this explanation to make it clear why this should not be
# done. If the sys.path entry above is added, when the astropy.sphinx.conf
# import occurs, it will import the *source* version of astropy instead of the
# version installed (if invoked as "make html" or directly with sphinx), or the
# version in the build directory (if "python setup.py build_sphinx" is used).
# Thus, any C-extensions that are needed to build the documentation will *not*
# be accessible, and the documentation will not build correctly.

# -- Mock Modules -------------------------------------------------------------

import sys
from mock import Mock
mock = Mock()

modules = {}

try:
    import skimage 
except ImportError:
    modules.update({'skimage':mock, 'skimage.feature':mock.module})

sys.modules.update(modules)

# -- General configuration ----------------------------------------------------

# Load all of the global Astropy configuration
from astropy.sphinx.conf import *

# If your documentation needs a minimal Sphinx version, state it here.
needs_sphinx = '1.1'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns.append('_templates')

# This is added to the end of RST files - a good place to put substitutions to
# be used globally.
rst_epilog += """
"""

# -- Project information ------------------------------------------------------

# This does not *have* to match the package name, but typically does
project = u'SunPy'
author = u'The SunPy Community'
copyright = u'2013, ' + author

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.

import sunpy
# The short X.Y version.
version = sunpy.__version__.split('-', 1)[0]
# The full version, including alpha/beta/rc tags.
release = sunpy.__version__

intersphinx_mapping.pop('h5py',None)
intersphinx_mapping['astropy'] = ('http://docs.astropy.org/en/stable/', None)
intersphinx_mapping['sqlalchemy'] = ('http://docs.sqlalchemy.org/en/rel_0_8/', None)
intersphinx_mapping['pandas'] = ('http://pandas.pydata.org/pandas-docs/stable/', None)
intersphinx_mapping['skimage'] = ('http://scikit-image.org/docs/stable/', None)

# -- Options for HTML output ---------------------------------------------------

# A NOTE ON HTML THEMES
# The global astropy configuration uses a custom theme, 'bootstrap-astropy',
# which is installed along with astropy. A different theme can be used or
# the options for this theme can be modified by overriding some of the
# variables set in the global configuration. The variables set in the
# global configuration are listed below, commented out.

# Add any paths that contain custom themes here, relative to this directory.
# To use a different custom theme, add the directory containing the theme.
#html_theme_path = []

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes. To override the custom theme, set this to the
# name of a builtin theme or the name of a custom theme in html_theme_path.
html_theme = 'default'

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
html_favicon = "../logo/favicon.ico"
#html_logo = "../logo/sunpy_logo_compact_192x239.png"

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = ''

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
html_title = '{0} v{1}'.format(project, release)

# Output file base name for HTML help builder.
htmlhelp_basename = project + 'doc'


# -- Options for LaTeX output --------------------------------------------------

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [('index', project + '.tex', project + u' Documentation',
                    author, 'manual')]


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [('index', project.lower(), project + u' Documentation',
              [author], 1)]


## -- Options for the edit_on_github extension ----------------------------------------
#
extensions += ['astropy.sphinx.ext.edit_on_github', 'sphinx.ext.doctest']

## Don't import the module as "version" or it will override the
## "version" configuration parameter
# TODO: make this smart like astropy
edit_on_github_project = "sunpy/sunpy"
edit_on_github_branch = "master"

edit_on_github_source_root = ""
edit_on_github_doc_root = "docs"

########NEW FILE########
__FILENAME__ = cm
"""
This module provides a set of colormaps specific for solar data.
"""
from __future__ import absolute_import

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm

from sunpy.cm import color_tables as ct

__all__ = ['get_cmap', 'show_colormaps']

sdoaia94 = ct.aia_color_table(94)
sdoaia131 = ct.aia_color_table(131)
sdoaia171 = ct.aia_color_table(171)
sdoaia193 = ct.aia_color_table(193)
sdoaia211 = ct.aia_color_table(211)
sdoaia304 = ct.aia_color_table(304)
sdoaia335 = ct.aia_color_table(335)
sdoaia1600 = ct.aia_color_table(1600)
sdoaia1700 = ct.aia_color_table(1700)
sdoaia4500 = ct.aia_color_table(4500)

sohoeit171 = ct.eit_color_table(171)
sohoeit195 = ct.eit_color_table(195)
sohoeit284 = ct.eit_color_table(284)
sohoeit304 = ct.eit_color_table(304)

soholasco2 = ct.lasco_color_table(2)
soholasco3 = ct.lasco_color_table(3)

stereocor1 = ct.cor_color_table(1)
stereocor2 = ct.cor_color_table(2)

yohkohsxtal = ct.sxt_color_table('al')
yohkohsxtwh = ct.sxt_color_table('wh')

hinodexrt = ct.xrt_color_table()
hinodesotintensity = ct.sot_color_table('intensity')
#hinodesotstokesquv = ct.sot_color_table('stokesQUV')
#hinodesotmagneticf = ct.sot_color_table('magnetic field')
#hinodesotvelocity = ct.sot_color_table('velocity')
#hinodesotwidth =  ct.sot_color_table('width')

trace171 = ct.trace_color_table('171')
trace195 = ct.trace_color_table('195')
trace284 = ct.trace_color_table('284')
trace1216 = ct.trace_color_table('1216')
trace1550 = ct.trace_color_table('1550')
trace1600 = ct.trace_color_table('1600')
trace1700 = ct.trace_color_table('1700')
traceWL = ct.trace_color_table('WL')

hmimag = ct.hmi_mag_color_table()

cmlist = {
          'sdoaia94': sdoaia94,
          'sdoaia131': sdoaia131,
          'sdoaia171': sdoaia171,
          'sdoaia193': sdoaia193,
          'sdoaia211': sdoaia211,
          'sdoaia304': sdoaia304,
          'sdoaia335': sdoaia335,
          'sdoaia1600': sdoaia1600,
          'sdoaia1700': sdoaia1700,
          'sdoaia4500': sdoaia4500,
          'sohoeit171': sohoeit171,
          'sohoeit195': sohoeit195,
          'sohoeit284': sohoeit284,
          'sohoeit304': sohoeit304,
          'soholasco2': soholasco2,
          'soholasco3': soholasco3,
          'stereocor1': stereocor1,
          'stereocor2': stereocor2,
          'rhessi': cm.jet,  # pylint: disable=E1101
          'yohkohsxtal': yohkohsxtal,
          'yohkohsxtwh': yohkohsxtwh,
          'hinodexrt': hinodexrt,
          'hinodesotintensity': hinodesotintensity,
          #'hinodesotstokesquv': hinodesotstokesquv,
          #'hinodesotmagneticf': hinodesotmagneticf,
          #'hinodesotvelocity': hinodesotvelocity,
          #'hinodesotwidth': hinodesotwidth,
          'trace171': trace171,
          'trace195': trace195,
          'trace284': trace284,
          'trace1216': trace1216,
          'trace1550': trace1550,
          'trace1600': trace1600,
          'trace1700': trace1700,
          'traceWL': traceWL,
          'hmimag': hmimag
          }


def get_cmap(name='sdoaia94'):
    """Get a colormap.

    Parameters
    ----------
    name : string
        The name of a color map.

    Returns
    -------
    value : matplotlib colormap

    See Also
    --------

    Examples
    --------
    >>> import sunpy.cm as cm
    >>> colormap = cm.get_cmap(name = 'sdoaia94')

    References
    ----------
    | http://matplotlib.sourceforge.net/api/cm_api.html

    """
    if name in cmlist:
        return cmlist.get(name)
    else:
        raise ValueError("Colormap %s is not recognized" % name)


def show_colormaps():
    """Displays a plot of the custom color maps supported in SunPy.

    Parameters
    ----------
    None : none

    Returns
    -------
    None : none

    See Also
    --------

    Examples
    --------
    >>> import sunpy.cm as cm
    >>> cm.show_colormaps()

    References
    ----------

    """
    maps = sorted(cmlist)
    nmaps = len(maps) + 1

    a = np.linspace(0, 1, 256).reshape(1, -1)  # pylint: disable=E1103
    a = np.vstack((a, a))

    fig = plt.figure(figsize=(5, 10))
    fig.subplots_adjust(top=0.99, bottom=0.01, left=0.2, right=0.99)
    for i, name in enumerate(maps):
        ax = plt.subplot(nmaps, 1, i + 1)
        plt.axis("off")
        plt.imshow(a, aspect='auto', cmap=get_cmap(name), origin='lower')
        pos = list(ax.get_position().bounds)
        fig.text(pos[0] - 0.01, pos[1], name, fontsize=10,
                 horizontalalignment='right')

    plt.show()

#def test_equalize(data):
#    """Returns a color map which performs histogram equalization on the data.
#
#    Parameters
#    ----------
#    data : ndarray
#
#    Returns
#    -------
#    value : matplotlib colormap
#
#    See Also
#    --------
#
#    Examples
#    --------
#    >>> import sunpy.cm as cm
#    >>> cm.test_equalize()
#
#    Reference
#    ---------
#    | http://matplotlib.sourceforge.net/api/cm_api.html
#
#    .. warning:: this function is under development
#
#    .. todo:: finish coding this function!
#
#    """
#    dfile = cbook.get_sample_data('s1045.ima', asfileobj=False)
#
#    im = np.fromstring(file(dfile, 'rb').read(), np.uint16).astype(float)
#    im.shape = 256, 256
#
#    #imshow(im, ColormapJet(256))
#    #imshow(im, cmap=cm.jet)
#
#    imvals = np.sort(im.flatten())
#    lo = imvals[0]
#    hi = imvals[-1]
#    steps = (imvals[::len(imvals)/256] - lo) / (hi - lo)
#    num_steps = float(len(steps))
#    interps = [(s, idx/num_steps, idx/num_steps) for idx,
#        s in enumerate(steps)]
#    interps.append((1, 1, 1))
#    cdict = {'red': interps,
#             'green': interps,
#             'blue': interps}
#    histeq_cmap = colors.LinearSegmentedColormap('HistEq', cdict)
#    pylab.figure()
#    pylab.imshow(im, cmap=histeq_cmap)
#    pylab.title('histeq')
#    pylab.show()

########NEW FILE########
__FILENAME__ = color_tables
"""
Nothing here but dictionaries for generating LinearSegmentedColormaps,
and a dictionary of these dictionaries.
"""
from __future__ import absolute_import

import numpy as np
import matplotlib.colors as colors

__all__ = ['aia_color_table', 'lasco_color_table', 'eit_color_table',
           'sxt_color_table', 'xrt_color_table', 'trace_color_table',
           'sot_color_table', 'hmi_mag_color_table',]

# FIXME: Give me a proper name.
def _mkx(i, steps, n):
    """ Generate list according to pattern of g0 and b0. """
    x = []
    for step in steps:
        x.extend(range(i, step + n, n))
        i = step + (n - 1)
    return x

def padfr(lst, len_, pad=0):
    """ Pad lst to contain at least len_ items by adding pad to the front. """
    diff = len_ - len(lst)
    diff = 0 if diff < 0 else diff
    return [pad] * diff + lst

def paden(lst, len_, pad=0):
    """ Pad lst to contain at least len_ items by adding pad to the end. """
    diff = len_ - len(lst)
    diff = 0 if diff < 0 else diff
    return lst + [pad] * diff


# The following values describe color table 3 for IDL (Red Temperature)
r0 = np.array(paden([0,1,2,4,5,7,8,10,11,13,14,15,17,18,20,21,23,24,26,27,28,
                     30,31,33,34,36,37,39,40,42,43,44,46,47,49,50,52,53,55,56,
                     57,59,60,62,63,65,66,68,69,70,72,73,75,76,78,79,81,82,84,
                     85,86,88,89,91,92,94,95,97,98,99,101,102,104,105,107,108,
                     110,111,113,114,115,117,118,120,121,123,124,126,127,128,
                     130,131,133,134,136,137,139,140,141,143,144,146,147,149,
                     150,152,153,155,156,157,159,160,162,163,165,166,168,169,
                     170,172,173,175,176,178,179,181,182,184,185,186,188,189,
                     191,192,194,195,197,198,199,201,202,204,205,207,208,210,
                     211,212,214,215,217,218,220,221,223,224,226,227,228,230,
                     231,233,234,236,237,239,240,241,243,244,246,247,249,250,
                     252,253], 256, 255))

g0 = np.array(padfr(_mkx(1, xrange(17, 256, 17), 2), 256))
b0 = np.array(padfr(_mkx(3, xrange(51, 256, 51), 4), 256))

c0 = np.arange(256, dtype='f')
c1 = (np.sqrt(c0) * np.sqrt(255.0)).astype('f')
c2 = (np.arange(256)**2 / 255.0).astype('f')
c3 = ((c1 + c2/2.0) * 255.0 / (c1.max() + c2.max()/2.0)).astype('f')

def aia_color_table(wavelength):
    '''Returns one of the fundamental color tables for SDO AIA images.
       Based on aia_lct.pro part of SDO/AIA on SSWIDL written by Karel Schriver (2010/04/12).
    '''
    try:
        r, g, b = {
            1600: (c3, c3, c2), 1700: (c1, c0, c0), 4500: (c0, c0, b0/2.0),
            94: (c2, c3, c0), 131: (g0, r0, r0), 171: (r0, c0, b0),
            193: (c1, c0, c2), 211: (c1, c0, c3), 304: (r0, g0, b0),
            335: (c2, c0, c1)
        }[wavelength]
    except KeyError:
        raise ValueError(
            "Invalid AIA wavelength. Valid values are "
            "1600,1700,4500,94,131,171,193,211,304,335."
        )
   
    # Now create the color dictionary in the correct format
    cdict = create_cdict(r, g, b)
    
    return colors.LinearSegmentedColormap('mytable', cdict)

eit_yellow_r = np.array([0,1,2,3,5,6,7,8,10,11,12,14,15,16,17,19,20,21,22,24,25,26,28,29,30,31,33,34,35,36
,38,39,40,42,43,44,45,47,48,49,51,52,53,54,56,57,58,59,61,62,63,65,66,67,68,70,71,72,73,75
,76,77,79,80,81,82,84,85,86,87,89,90,91,93,94,95,96,98,99,100,102,103,104,105,107,108,109,110,112,113
,114,116,117,118,119,121,122,123,124,126,127,128,130,131,132,133,135,136,137,138,140,141,142,144,145,146,147,149,150,151
,153,154,155,156,158,159,160,161,163,164,165,167,168,169,170,172,173,174,175,177,178,179,181,182,183,184,186,187,188,189
,191,192,193,195,196,197,198,200,201,202,204,205,206,207,209,210,211,212,214,215,216,218,219,220,221,223,224,225,226,228
,229,230,232,233,234,235,237,238,239,240,242,243,244,246,247,248,249,251,252,253,255,255,255,255,255,255,255,255,255,255
,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255
,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255])

eit_yellow_g = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29
,30,31,32,33,34,35,36,37,38,39,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60
,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,82,83,84,85,86,87,88,89,90,91
,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121
,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152
,153,154,155,156,157,158,159,160,161,162,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183
,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,205,206,206,207,209,209,210,210,212,213
,213,215,216,216,218,219,219,221,221,222,224,224,225,227,227,228,230,230,231,231,232,234,234,235,237,237,238,240,240,241
,241,243,244,244,246,247,247,249,250,250,252,252,253,255,255,255])

eit_yellow_b = np.concatenate((np.zeros(201).astype('int'), np.array([7,7,15,22,22,30,30,37,45
,45,52,60,60,67,75,75,82,82,90,97,97,105,112,112,120,127,127,135,135,142,150,150,157,165,165,172,180,180,187
,187,195,202,202,210,217,217,225,232,232,240,240,247,255,255,255])))
 
eit_dark_blue_r = np.concatenate((np.zeros(206).astype('int'), np.array([9,13,21,25
,25,29,33,41,49,53,57,65,69,73,77,77,85,94,98,102,110,114,118,122,134,134,138,142,146,154,158,162,166,179
,183,183,187,191,199,203,207,215,223,227,231,231,235,243,247,255])))

eit_dark_blue_g = np.concatenate((np.zeros(128).astype('int'), np.array([2,2,4,5,7,12,13,15,17,20,21,21,23,25,29,31,33,34,37,39,41,41
,44,47,49,50,52,55,57,60,61,61,65,66,68,69,73,76,77,79,82,82,84,85,87,92,94,95,97,100,102,103
,103,105,110,111,113,114,118,119,121,122,122,127,129,130,132,135,137,138,142,145,145,146,148,150,153,154,158,159,162,164
,164,166,167,170,174,175,177,180,182,183,185,185,188,191,193,195,198,199,201,203,207,207,209,211,212,215,217,219,220,225
,227,227,228,230,233,235,236,239,243,244,246,246,247,251,252,255])))

eit_dark_blue_b = np.concatenate((np.zeros(52).astype('int'), np.array([1,4,5,6,8,8,10,12
,14,16,18,20,21,23,25,25,28,29,31,33,35,36,37,42,43,43,44,46,48,50,51,52,56,58,59,61,61,63
,65,66,69,71,73,74,75,78,78,80,81,84,86,88,89,90,93,94,94,97,99,101,103,104,105,108,111,112,112,113
,116,118,119,120,124,126,127,128,131,131,132,134,135,139,141,142,143,146,147,147,149,150,154,155,157,158,161,162,164,164
,166,169,170,172,173,176,177,180,181,181,184,185,187,188,191,193,195,196,199,199,200,202,203,207,208,210,211,214,215,217
,217,218,222,223,225,226,229,230,231,233,233,237,238,240,241,244,245,246,249,252,252,253,255,255,255,255,255,255,255,255
,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255
,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255])))

eit_dark_green_r = np.concatenate((np.zeros(130).astype('int'), np.array([1,3,4,9,11,12,14,17,19,19,20,22,27,29,30,32,35,37,38,38
,41,45,46,48,50,53,54,58,59,59,62,64,66,67,71,74,75,77,80,80,82,83,85,90,91,93,95,98,100,101
,101,103,108,109,111,112,116,117,119,121,121,125,127,129,130,133,135,137,140,143,143,145,146,148,151,153,156,158,161,163
,163,164,166,169,172,174,175,179,180,182,183,183,187,190,192,193,196,198,200,201,206,206,208,209,211,214,216,217,219,224
,225,225,227,229,232,234,235,238,242,243,245,245,246,250,251,255])))

eit_dark_green_g = np.concatenate((np.zeros(52).astype('int'), np.array([1,3,4,5,6,6,8,9
,11,12,14,15,16,17,19,19,21,22,23,25,26,27,28,31,32,32,33,34,36,37,38,39,42,43,44,45,45,47
,48,49,51,53,54,55,56,58,58,59,60,62,64,65,66,67,69,70,70,72,73,75,76,77,78,80,82,83,83,84
,86,87,88,89,92,93,94,95,97,97,98,99,100,103,104,105,106,108,109,109,110,111,114,115,116,117,119,120,121,121
,123,125,126,127,128,130,131,133,134,134,136,137,138,139,141,143,144,145,147,147,148,149,150,153,154,155,156,158,159,160
,160,161,164,165,166,167,169,170,171,172,172,175,176,177,178,180,181,182,184,186,186,187,188,189,191,192,194,195,197,198
,198,199,200,202,204,205,206,208,209,210,211,211,213,215,216,217,219,220,221,222,225,225,226,227,228,230,231,232,233,236
,237,237,238,239,241,242,243,245,247,248,249,249,250,252,253,255])))

eit_dark_green_b = np.concatenate((np.zeros(197).astype('int'), np.array([3,10,17,17,20,24,27,34,37,44,48,55,58
,58,62,65,72,79,82,86,93,96,99,103,103,110,117,120,124,130,134,137,141,151,151,155,158,161,168,172,175,179,189
,192,192,196,199,206,210,213,220,227,230,234,234,237,244,248,255])))

eit_dark_red_r = np.concatenate((np.zeros(52).astype('int'), np.array([1,4,5,7,8,8,11,13
,15,17,20,21,23,24,27,27,30,31,33,36,37,39,40,44,46,46,47,49,52,53,55,56,60,62,63,65,65,68
,69,70,73,76,78,79,81,84,84,85,86,89,92,94,95,97,99,101,101,104,105,108,110,111,113,115,118,120,120,121
,124,126,127,128,133,134,136,137,140,140,141,143,144,149,150,152,153,156,157,157,159,160,165,166,168,169,172,173,175,175
,178,181,182,184,185,188,189,192,194,194,197,198,199,201,204,207,208,210,212,212,214,215,217,221,223,224,226,228,230,231
,231,233,237,239,240,241,244,246,247,249,249,253,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255
,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255
,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255])))

eit_dark_red_g = np.concatenate((np.zeros(148).astype('int'), np.array([1,1
,5,9,11,13,15,18,20,24,26,26,30,32,34,35,39,43,45,47,51,51,52,54,56,62,64,66,68,71,73,75
,75,77,83,85,86,88,92,94,96,98,98,103,105,107,109,113,115,117,120,124,124,126,128,130,134,136,139,141,145,147
,147,149,151,154,158,160,162,166,168,170,171,171,175,179,181,183,187,188,190,192,198,198,200,202,204,207,209,211,213,219
,221,221,222,224,228,230,232,236,239,241,243,243,245,249,251,255])))

eit_dark_red_b = np.concatenate((np.zeros(204).astype('int'), np.array([3,7,15,19,27,31
,31,35,39,47,54,58,62,70,74,78,82,82,90,98,102,105,113,117,121,125,137,137,141,145,149,156,160,164,168,180
,184,184,188,192,200,204,207,215,223,227,231,231,235,243,247,255])))

def eit_color_table(wavelength):
    '''Returns one of the fundamental color tables for SOHO EIT images.'''
    # SOHO EIT Color tables
    # EIT 171 IDL Name EIT Dark Bot Blue
    # EIT 195 IDL Name EIT Dark Bot Green
    # EIT 284 IDL Name EIT Dark Bot Yellow
    # EIT 304 IDL Name EIT Dark Bot Red
    try:
        r, g, b = {
            171: (eit_dark_blue_r, eit_dark_blue_g, eit_dark_blue_b), 
            195: (eit_dark_green_r, eit_dark_green_g, eit_dark_green_b), 
            284: (eit_yellow_r, eit_yellow_g, eit_yellow_b),
            304: (eit_dark_red_r, eit_dark_red_g, eit_dark_red_b)
        }[wavelength]
    except KeyError:
        raise ValueError(
            "Invalid EIT wavelength. Valid values are "
            "171, 195, 284, 304."
        )


    # Now create the color dictionary in the correct format
    cdict = create_cdict(r, g, b)

    return colors.LinearSegmentedColormap('mytable', cdict)

lasco_c2_r = np.concatenate((np.array([0,1,2,5,8,11,14,17,20,23,26,28,31,34,37,42,44,47,50,55,57,60,65,68,70,75,78,82,85,88,92,95,99,102,107
,110,114,117,121,124,128,133,136,140,143,147,152,155,159,163,166,170,175,178,182,186,189,194,198,201,205,210,214,217,221,226,230,233,237,241
,246,250,253]), 255*np.ones(183)))

lasco_c2_g = np.concatenate((np.zeros(52).astype('int'), np.array([1,5,11,17,20,26,32,35,41,47,52,56,62,68,73,77,83,88
,94,100,103,109,115,120,126,130,136,141,147,153,158,164,168,173,179,185,190,196,202,207,213,217,222,228,234,239,245,251]), 255*np.ones(156)))

lasco_c2_b = np.concatenate((np.zeros(78).astype('int'), np.array([7,19,31,43,54,66,74,86,98,109,121,133,145,156,168,176,188,200,211,223,235,247]),255*np.ones(156)))

lasco_c3_r = np.concatenate((np.zeros(77).astype('int'), np.array([5,13,25,33,45,53,65,73,85,94,106,114,126,134,146,154,166,175,187,195,207,215,227,235,247]),255*np.ones(154)))

lasco_c3_g = np.concatenate((np.zeros(39).astype('int'), np.array([4,7,12,15,20,23,28,31,36,39,44,47,52,55,60,63,68,71,76,79,84,87,92,95,100,103,108,111,116,119,124
,127,132,135,140,143,148,151,156,159,164,167,172,175,180,183,188,191,196,199,204,207,212,215,220,223,228,231,236,239,244,247,252]),255*np.ones(154)))

lasco_c3_b = np.concatenate((np.array([0,4,6,10,13,17,20,24,27,31,33,37,40,44,47,51,54,58,61,65,67,71,74,78,81,85,88,92,94,99,101,105,108,112,115
,119,122,126,128,132,135,139,142,146,149,153,155,160,162,166,169,173,176,180,183,187,189,193,196,200,203,207,210,214,217,221,223,227,230,234
,237,241,244,248,250]),255*np.ones(181)))

def lasco_color_table(number):
    '''Returns one of the fundamental color tables for SOHO LASCO images.'''
    # SOHO LASCO Color tables
    # LASCO C2 white light IDL Name 
    # LASCO C3 white light IDL Name 
    try:
        r, g, b = {
            2: (lasco_c2_r, lasco_c2_g, lasco_c2_b), 
            3: (lasco_c3_r, lasco_c3_g, lasco_c3_b), 
        }[number]
    except KeyError:
        raise ValueError(
            "Invalid LASCO number. Valid values are "
            "2, 3."
        )

    # Now create the color dictionary in the correct format
    cdict = create_cdict(r, g, b)
    return colors.LinearSegmentedColormap('mytable', cdict)

# Translated from the JP2Gen IDL SXT code lct_yla_gold.pro.  Might be better
# to explicitly copy the numbers from the IDL calculation.  This is a little
# more compact.
sxt_gold_r = np.concatenate((255.0*np.array(range(0,185))/185.0,
                            255*np.ones(71)))
sxt_gold_g = 255*(np.array(range(0,256))**1.25)/(255.0**1.25)
sxt_gold_b = np.concatenate((np.zeros(185),255.0*np.array(range(0,71))/71.0))
                            
grayscale = np.array(range(0,256))

def sxt_color_table(sxt_filter):
    '''Returns one of the fundamental color tables for Yokhoh SXT images.'''
    try:
        r, g, b = {
            'al': (sxt_gold_r, sxt_gold_g, sxt_gold_b), 
            'wh': (grayscale, grayscale, grayscale)
        }[sxt_filter]
    except KeyError:
        raise ValueError(
            "Invalid SXT filter type number. Valid values are "
            "'al', 'wh'."
        )

    # Now create the color dictionary in the correct format
    cdict = create_cdict(r, g, b)
    return colors.LinearSegmentedColormap('mytable', cdict)

def xrt_color_table():
    '''Returns the color table used for all Hinode XRT images.'''
    # Now create the color dictionary in the correct format
    cdict = create_cdict(r0, g0, b0)
    return colors.LinearSegmentedColormap('mytable', cdict)

#Stereo Secchi COR colour tables
stereo_cor1_b = np.array(
      [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   3,   6,   6,  10,  13,  13,  17,
        20,  20,  24,  27,  27,  31,  31,  34,  37,  37,  41,  44,  44,
        48,  48,  51,  55,  55,  58,  58,  62,  62,  65,  68,  68,  72,
        72,  75,  75,  79,  82,  82,  86,  86,  89,  89,  93,  93,  96,
        96,  99, 103, 103, 106, 106, 110, 110, 113, 113, 117, 117, 120,
       120, 124, 124, 127, 127, 130, 130, 134, 134, 137, 137, 141, 141,
       144, 144, 148, 148, 151, 151, 155, 155, 158, 158, 161, 161, 165,
       165, 168, 168, 168, 172, 172, 175, 175, 179, 179, 182, 182, 186,
       186, 189, 189, 189, 192, 192, 196, 196, 199, 199, 203, 203, 203,
       206, 206, 210, 210, 213, 213, 213, 217, 217, 220, 220, 223, 223,
       223, 227, 227, 230, 230, 234, 234, 234, 237, 237, 241, 241, 241,
       244, 244, 248, 248, 248, 251, 251, 255, 255], dtype=np.uint8)

stereo_cor1_g = np.array(
      [  0,  27,  36,  43,  48,  53,  57,  60,  64,  67,  69,  72,  75,
        77,  80,  82,  84,  86,  88,  90,  92,  94,  95,  97,  99, 100,
       102, 104, 105, 107, 108, 110, 111, 112, 114, 115, 116, 118, 119,
       120, 121, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134,
       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,
       147, 148, 149, 150, 151, 152, 153, 154, 154, 155, 156, 157, 158,
       159, 159, 160, 161, 162, 163, 163, 164, 165, 166, 167, 167, 168,
       169, 170, 170, 171, 172, 172, 173, 174, 175, 175, 176, 177, 177,
       178, 179, 179, 180, 181, 181, 182, 183, 183, 184, 185, 185, 186,
       187, 187, 188, 189, 189, 190, 190, 191, 192, 192, 193, 194, 194,
       195, 195, 196, 197, 197, 198, 198, 199, 199, 200, 201, 201, 202,
       202, 203, 203, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209,
       209, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216,
       216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222,
       223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229,
       229, 230, 230, 230, 231, 231, 232, 232, 233, 233, 234, 234, 235,
       235, 236, 236, 236, 237, 237, 238, 238, 239, 239, 240, 240, 240,
       241, 241, 242, 242, 243, 243, 243, 244, 244, 245, 245, 246, 246,
       246, 247, 247, 248, 248, 249, 249, 249, 250, 250, 251, 251, 251,
       252, 252, 253, 253, 253, 254, 254, 255, 255], dtype=np.uint8)

stereo_cor1_r = np.array(
      [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,   4,
         8,  11,  12,  16,  17,  20,  22,  24,  27,  29,  30,  33,  35,
        37,  38,  41,  43,  45,  46,  48,  50,  51,  54,  56,  58,  59,
        61,  62,  64,  66,  67,  69,  71,  72,  74,  75,  77,  79,  80,
        80,  82,  83,  85,  87,  88,  90,  91,  91,  93,  95,  96,  98,
       100, 100, 101, 103, 104, 106, 106, 108, 109, 111, 112, 112, 114,
       116, 117, 117, 119, 121, 121, 122, 124, 125, 125, 127, 129, 129,
       130, 132, 132, 133, 135, 135, 137, 138, 138, 140, 142, 142, 143,
       145, 145, 146, 148, 148, 150, 150, 151, 153, 153, 154, 156, 156,
       158, 158, 159, 161, 161, 163, 163, 164, 164, 166, 167, 167, 169,
       169, 171, 171, 172, 174, 174, 175, 175, 177, 177, 179, 179, 180,
       180, 182, 183, 183, 185, 185, 187, 187, 188, 188, 190, 190, 192,
       192, 193, 193, 195, 195, 196, 196, 198, 198, 200, 200, 201, 201,
       203, 203, 204, 204, 206, 206, 208, 208, 209, 209, 211, 211, 213,
       213, 214, 214, 214, 216, 216, 217, 217, 219, 219, 221, 221, 222,
       222, 224, 224, 224, 225, 225, 227, 227, 229, 229, 230, 230, 230,
       232, 232, 234, 234, 235, 235, 235, 237, 237, 238, 238, 240, 240,
       240, 242, 242, 243, 243, 245, 245, 245, 246, 246, 248, 248, 248,
       250, 250, 251, 251, 251, 253, 253, 255, 255], dtype=np.uint8)

stereo_cor2_b = np.array(
      [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         7,  19,  31,  43,  54,  66,  74,  86,  98, 109, 121, 133, 145,
       156, 168, 176, 188, 200, 211, 223, 235, 247, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255], dtype=np.uint8)
       
stereo_cor2_g = np.array(
      [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         1,   5,  11,  17,  20,  26,  32,  35,  41,  47,  52,  56,  62,
        68,  73,  77,  83,  88,  94, 100, 103, 109, 115, 120, 126, 130,
       136, 141, 147, 153, 158, 164, 168, 173, 179, 185, 190, 196, 202,
       207, 213, 217, 222, 228, 234, 239, 245, 251, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255], dtype=np.uint8)
       
stereo_cor2_r = np.array(
      [  0,   1,   2,   5,   8,  11,  14,  17,  20,  23,  26,  28,  31,
        34,  37,  42,  44,  47,  50,  55,  57,  60,  65,  68,  70,  75,
        78,  82,  85,  88,  92,  95,  99, 102, 107, 110, 114, 117, 121,
       124, 128, 133, 136, 140, 143, 147, 152, 155, 159, 163, 166, 170,
       175, 178, 182, 186, 189, 194, 198, 201, 205, 210, 214, 217, 221,
       226, 230, 233, 237, 241, 246, 250, 253, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
       255, 255, 255, 255, 255, 255, 255, 255, 255], dtype=np.uint8)

def cor_color_table(number):
    '''Returns one of the fundamental color tables for STEREO coronagraph
    images.'''
    # STEREO COR Color tables
    try:
        r, g, b = {
            1: (stereo_cor1_r, stereo_cor1_g, stereo_cor1_b), 
            2: (stereo_cor2_r, stereo_cor2_g, stereo_cor2_b), 
        }[number]
    except KeyError:
        raise ValueError(
            "Invalid COR number. Valid values are "
            "1, 2."
        )

    # Now create the color dictionary in the correct format
    cdict = create_cdict(r, g, b)
    
    return colors.LinearSegmentedColormap('mytable', cdict)


# Standard TRACE color RGB triples, as defined by the SSWIDL program
# trace_colors.pro.  See http://hesperia.gsfc.nasa.gov/ssw/trace/idl/util/trace_colors.pro
trace_171_r = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 5, 7, 8, 10, 12, 14, 15, 17, 19, 21, 22, 24, 26, 28, 29, 31, 33, 35, 37, 38, 40, 42, 44, 45, 47, 49, 51, 52, 54, 56, 58, 59, 61, 63, 65, 67, 68, 70, 72, 74, 75, 77, 79, 81, 82, 84, 86, 88, 90, 91, 93, 95, 97, 98, 100, 102, 104, 105, 107, 109, 111, 112, 114, 116, 118, 119, 121, 123, 125, 127, 128, 130, 132, 134, 135, 137, 139, 141, 142, 144, 146, 148, 150, 151, 153, 155, 157, 158, 160, 162, 164, 165, 167, 169, 171, 172, 174, 176, 178, 180, 180, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229], dtype=np.uint8)
trace_171_g = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 63, 64, 66, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 83, 84, 86, 87, 89, 90, 92, 93, 95, 96, 98, 100, 101, 103, 104, 106, 107, 109, 110, 112, 113, 115, 116, 118, 119, 121, 123, 124, 126, 127, 129, 130, 132, 133, 135, 136, 138, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 158, 159, 161, 163, 164, 166, 167, 169, 170, 172, 173, 175, 176, 178, 180, 180, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229], dtype=np.uint8)
trace_171_b = np.array([0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 30, 31, 32, 33, 35, 36, 37, 39, 40, 41, 43, 44, 45, 47, 48, 49, 50, 52, 53, 54, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 69, 70, 71, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 94, 95, 96, 98, 99, 100, 101, 103, 104, 105, 107, 108, 109, 111, 112, 113, 115, 116, 117, 118, 120, 121, 122, 124, 125, 126, 128, 129, 130, 132, 133, 134, 135, 137, 138, 139, 141, 142, 143, 145, 146, 147, 149, 150, 151, 152, 154, 155, 156, 158, 159, 160, 162, 163, 164, 166, 167, 168, 169, 171, 172, 173, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 188, 189, 190, 192, 193, 194, 196, 197, 198, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229, 229, 230, 230, 231, 231, 232, 233, 233, 234, 234, 235, 235, 236, 236, 237, 237, 238, 238, 239, 239, 240, 240, 241, 241, 242, 242, 243, 244, 244, 245, 245, 246, 246, 247, 247, 248, 248, 249, 249, 250, 250, 251, 251, 252, 252, 253], dtype=np.uint8)
 
trace_195_r = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 5, 7, 8, 10, 12, 14, 15, 17, 19, 21, 22, 24, 26, 28, 29, 31, 33, 35, 37, 38, 40, 42, 44, 45, 47, 49, 51, 52, 54, 56, 58, 59, 61, 63, 65, 67, 68, 70, 72, 74, 75, 77, 79, 81, 82, 84, 86, 88, 90, 91, 93, 95, 97, 98, 100, 102, 104, 105, 107, 109, 111, 112, 114, 116, 118, 119, 121, 123, 125, 127, 128, 130, 132, 134, 135, 137, 139, 141, 142, 144, 146, 148, 150, 151, 153, 155, 157, 158, 160, 162, 164, 165, 167, 169, 171, 172, 174, 176, 178, 180, 180, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229], dtype=np.uint8)
trace_195_g = np.array([0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 30, 31, 32, 33, 35, 36, 37, 39, 40, 41, 43, 44, 45, 47, 48, 49, 50, 52, 53, 54, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 69, 70, 71, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 94, 95, 96, 98, 99, 100, 101, 103, 104, 105, 107, 108, 109, 111, 112, 113, 115, 116, 117, 118, 120, 121, 122, 124, 125, 126, 128, 129, 130, 132, 133, 134, 135, 137, 138, 139, 141, 142, 143, 145, 146, 147, 149, 150, 151, 152, 154, 155, 156, 158, 159, 160, 162, 163, 164, 166, 167, 168, 169, 171, 172, 173, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 188, 189, 190, 192, 193, 194, 196, 197, 198, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229, 229, 230, 230, 231, 231, 232, 233, 233, 234, 234, 235, 235, 236, 236, 237, 237, 238, 238, 239, 239, 240, 240, 241, 241, 242, 242, 243, 244, 244, 245, 245, 246, 246, 247, 247, 248, 248, 249, 249, 250, 250, 251, 251, 252, 252, 253], dtype=np.uint8)
trace_195_b = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 63, 64, 66, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 83, 84, 86, 87, 89, 90, 92, 93, 95, 96, 98, 100, 101, 103, 104, 106, 107, 109, 110, 112, 113, 115, 116, 118, 119, 121, 123, 124, 126, 127, 129, 130, 132, 133, 135, 136, 138, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 158, 159, 161, 163, 164, 166, 167, 169, 170, 172, 173, 175, 176, 178, 180, 180, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229], dtype=np.uint8)
 
trace_284_r = np.array([0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 30, 31, 32, 33, 35, 36, 37, 39, 40, 41, 43, 44, 45, 47, 48, 49, 50, 52, 53, 54, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 69, 70, 71, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 94, 95, 96, 98, 99, 100, 101, 103, 104, 105, 107, 108, 109, 111, 112, 113, 115, 116, 117, 118, 120, 121, 122, 124, 125, 126, 128, 129, 130, 132, 133, 134, 135, 137, 138, 139, 141, 142, 143, 145, 146, 147, 149, 150, 151, 152, 154, 155, 156, 158, 159, 160, 162, 163, 164, 166, 167, 168, 169, 171, 172, 173, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 188, 189, 190, 192, 193, 194, 196, 197, 198, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229, 229, 230, 230, 231, 231, 232, 233, 233, 234, 234, 235, 235, 236, 236, 237, 237, 238, 238, 239, 239, 240, 240, 241, 241, 242, 242, 243, 244, 244, 245, 245, 246, 246, 247, 247, 248, 248, 249, 249, 250, 250, 251, 251, 252, 252, 253], dtype=np.uint8)
trace_284_g = np.array([0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 30, 31, 32, 33, 35, 36, 37, 39, 40, 41, 43, 44, 45, 47, 48, 49, 50, 52, 53, 54, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 69, 70, 71, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 94, 95, 96, 98, 99, 100, 101, 103, 104, 105, 107, 108, 109, 111, 112, 113, 115, 116, 117, 118, 120, 121, 122, 124, 125, 126, 128, 129, 130, 132, 133, 134, 135, 137, 138, 139, 141, 142, 143, 145, 146, 147, 149, 150, 151, 152, 154, 155, 156, 158, 159, 160, 162, 163, 164, 166, 167, 168, 169, 171, 172, 173, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 188, 189, 190, 192, 193, 194, 196, 197, 198, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229, 229, 230, 230, 231, 231, 232, 233, 233, 234, 234, 235, 235, 236, 236, 237, 237, 238, 238, 239, 239, 240, 240, 241, 241, 242, 242, 243, 244, 244, 245, 245, 246, 246, 247, 247, 248, 248, 249, 249, 250, 250, 251, 251, 252, 252, 253], dtype=np.uint8)
trace_284_b = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 5, 7, 8, 10, 12, 14, 15, 17, 19, 21, 22, 24, 26, 28, 29, 31, 33, 35, 37, 38, 40, 42, 44, 45, 47, 49, 51, 52, 54, 56, 58, 59, 61, 63, 65, 67, 68, 70, 72, 74, 75, 77, 79, 81, 82, 84, 86, 88, 90, 91, 93, 95, 97, 98, 100, 102, 104, 105, 107, 109, 111, 112, 114, 116, 118, 119, 121, 123, 125, 127, 128, 130, 132, 134, 135, 137, 139, 141, 142, 144, 146, 148, 150, 151, 153, 155, 157, 158, 160, 162, 164, 165, 167, 169, 171, 172, 174, 176, 178, 180, 180, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229], dtype=np.uint8)
 
trace_1216_r = np.array([0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254], dtype=np.uint8)
trace_1216_g = np.array([0, 1, 2, 4, 5, 7, 8, 10, 11, 12, 14, 15, 17, 18, 20, 21, 23, 24, 25, 27, 28, 30, 31, 33, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47, 48, 50, 51, 53, 54, 56, 57, 58, 60, 61, 63, 64, 66, 67, 69, 70, 71, 73, 74, 76, 77, 79, 80, 81, 83, 84, 86, 87, 89, 90, 92, 93, 94, 96, 97, 99, 100, 102, 103, 104, 106, 107, 109, 110, 112, 113, 115, 116, 117, 119, 120, 122, 123, 125, 126, 127, 129, 130, 132, 133, 135, 136, 138, 139, 140, 142, 143, 145, 146, 148, 149, 150, 152, 153, 155, 156, 158, 159, 161, 162, 163, 165, 166, 168, 169, 171, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 186, 188, 189, 191, 192, 194, 195, 196, 198, 199, 201, 202, 204, 205, 207, 208, 209, 211, 212, 214, 215, 217, 218, 220, 220, 220, 220, 221, 221, 221, 222, 222, 222, 223, 223, 223, 224, 224, 224, 225, 225, 225, 226, 226, 226, 227, 227, 227, 228, 228, 228, 229, 229, 229, 230, 230, 230, 231, 231, 231, 232, 232, 232, 233, 233, 233, 234, 234, 234, 235, 235, 235, 236, 236, 236, 237, 237, 237, 238, 238, 238, 239, 239, 239, 240, 240, 240, 241, 241, 241, 242, 242, 242, 243, 243, 243, 244, 244, 244, 245, 245, 245, 246, 246, 246, 247, 247, 247, 248, 248, 248, 249, 249, 249, 250, 250, 250, 251, 251, 251, 252, 252, 252, 253, 253, 253], dtype=np.uint8)
trace_1216_b = np.array([0, 1, 2, 4, 5, 7, 8, 10, 11, 12, 14, 15, 17, 18, 20, 21, 23, 24, 25, 27, 28, 30, 31, 33, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47, 48, 50, 51, 53, 54, 56, 57, 58, 60, 61, 63, 64, 66, 67, 69, 70, 71, 73, 74, 76, 77, 79, 80, 81, 83, 84, 86, 87, 89, 90, 92, 93, 94, 96, 97, 99, 100, 102, 103, 104, 106, 107, 109, 110, 112, 113, 115, 116, 117, 119, 120, 122, 123, 125, 126, 127, 129, 130, 132, 133, 135, 136, 138, 139, 140, 142, 143, 145, 146, 148, 149, 150, 152, 153, 155, 156, 158, 159, 161, 162, 163, 165, 166, 168, 169, 171, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 186, 188, 189, 191, 192, 194, 195, 196, 198, 199, 201, 202, 204, 205, 207, 208, 209, 211, 212, 214, 215, 217, 218, 220, 220, 220, 220, 221, 221, 221, 222, 222, 222, 223, 223, 223, 224, 224, 224, 225, 225, 225, 226, 226, 226, 227, 227, 227, 228, 228, 228, 229, 229, 229, 230, 230, 230, 231, 231, 231, 232, 232, 232, 233, 233, 233, 234, 234, 234, 235, 235, 235, 236, 236, 236, 237, 237, 237, 238, 238, 238, 239, 239, 239, 240, 240, 240, 241, 241, 241, 242, 242, 242, 243, 243, 243, 244, 244, 244, 245, 245, 245, 246, 246, 246, 247, 247, 247, 248, 248, 248, 249, 249, 249, 250, 250, 250, 251, 251, 251, 252, 252, 252, 253, 253, 253], dtype=np.uint8)
 
trace_1550_r = np.array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 190, 191, 191, 192, 193, 193, 194, 194, 195, 196, 196, 197, 198, 198, 199, 199, 200, 201, 201, 202, 203, 203, 204, 204, 205, 206, 206, 207, 208, 208, 209, 209, 210, 211, 211, 212, 212, 213, 214, 214, 215, 216, 216, 217, 217, 218, 219, 219, 220, 221, 221, 222, 222, 223, 224, 224, 225, 226, 226, 227, 227, 228, 229, 229, 230, 231, 231, 232, 232, 233, 234, 234, 235, 235, 236, 237, 237, 238, 239, 239, 240, 240, 241, 242, 242, 243, 244, 244, 245, 245, 246, 247, 247, 248, 249, 249, 250, 250, 251, 252, 252, 253], dtype=np.uint8)
trace_1550_g = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 20, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 37, 38, 40, 41, 43, 44, 46, 47, 49, 50, 52, 53, 55, 57, 58, 60, 61, 63, 64, 66, 67, 69, 70, 72, 74, 75, 77, 78, 80, 81, 83, 84, 86, 87, 89, 91, 92, 94, 95, 97, 98, 100, 101, 103, 104, 106, 107, 109, 111, 112, 114, 115, 117, 118, 120, 121, 123, 124, 126, 128, 129, 131, 132, 134, 135, 137, 138, 140, 141, 143, 145, 146, 148, 149, 151, 152, 154, 155, 157, 158, 160, 161, 163, 165, 166, 168, 169, 171, 172, 174, 175, 177, 178, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 216, 216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229, 229, 230, 230, 231, 231, 232], dtype=np.uint8)
trace_1550_b = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127, 129, 131, 133, 135, 137, 139, 141, 143, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 216, 216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229, 229, 230, 230, 231, 231, 232], dtype=np.uint8)
 
trace_1600_r = np.array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 190, 191, 191, 192, 193, 193, 194, 194, 195, 196, 196, 197, 198, 198, 199, 199, 200, 201, 201, 202, 203, 203, 204, 204, 205, 206, 206, 207, 208, 208, 209, 209, 210, 211, 211, 212, 212, 213, 214, 214, 215, 216, 216, 217, 217, 218, 219, 219, 220, 221, 221, 222, 222, 223, 224, 224, 225, 226, 226, 227, 227, 228, 229, 229, 230, 231, 231, 232, 232, 233, 234, 234, 235, 235, 236, 237, 237, 238, 239, 239, 240, 240, 241, 242, 242, 243, 244, 244, 245, 245, 246, 247, 247, 248, 249, 249, 250, 250, 251, 252, 252, 253], dtype=np.uint8)
trace_1600_g = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 5, 7, 8, 10, 12, 14, 15, 17, 19, 21, 23, 24, 26, 28, 30, 31, 33, 35, 37, 38, 40, 42, 44, 46, 47, 49, 51, 53, 54, 56, 58, 60, 61, 63, 65, 67, 69, 70, 72, 74, 76, 77, 79, 81, 83, 84, 86, 88, 90, 92, 93, 95, 97, 99, 100, 102, 104, 106, 107, 109, 111, 113, 115, 116, 118, 120, 122, 123, 125, 127, 129, 130, 132, 134, 136, 138, 139, 141, 143, 145, 146, 148, 150, 152, 153, 155, 157, 159, 161, 162, 164, 166, 168, 169, 171, 173, 175, 176, 178, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 216, 216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229, 229, 230, 230, 231, 231, 232], dtype=np.uint8)
trace_1600_b = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 11, 17, 22, 28, 34, 39, 45, 51, 57, 62, 68, 74, 79, 85, 91, 96, 102, 108, 114, 119, 125, 131, 136, 142, 142, 143, 143, 144, 144, 144, 145, 145, 146, 146, 146, 147, 147, 148, 148, 148, 149, 149, 150, 150, 150, 151, 151, 152, 152, 152, 153, 153, 154, 154, 154, 155, 155, 156, 156, 156, 157, 157, 158, 158, 158, 159, 159, 160, 160, 160, 161, 161, 162, 162, 162, 163, 163, 164, 164, 164, 165, 165, 166, 166, 166, 167, 167, 168, 168, 168, 169, 169, 170, 170, 170, 171, 171, 172, 172, 172, 173, 173, 174, 174, 174, 175, 175, 176, 176, 176, 177, 177, 178, 178, 178, 179, 179, 180, 180, 180, 181, 181, 182, 182, 182, 183], dtype=np.uint8)
 
trace_1700_r = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 170, 171, 172, 173, 174, 174, 175, 176, 177, 178, 178, 179, 180, 181, 182, 183, 183, 184, 185, 186, 187, 187, 188, 189, 190, 191, 192, 192, 193, 194, 195, 196, 196, 197, 198, 199, 200, 200, 201, 202, 203, 204, 205, 205, 206, 207, 208, 209, 209, 210, 211, 212, 213, 214, 214, 215, 216, 217, 218, 218, 219, 220, 221, 222, 223, 223, 224, 225, 226, 227, 227, 228, 229, 230, 231, 231, 232, 233, 234, 235, 236, 236, 237, 238, 239, 240, 240, 241, 242, 243, 244, 245, 245, 246, 247, 248, 249, 249, 250, 251, 252, 253], dtype=np.uint8)
trace_1700_g = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 16, 18, 20, 21, 23, 25, 26, 28, 30, 31, 33, 35, 36, 38, 40, 41, 43, 45, 46, 48, 50, 51, 53, 55, 56, 58, 60, 61, 63, 65, 66, 68, 70, 71, 73, 75, 76, 78, 80, 81, 83, 85, 86, 88, 90, 91, 93, 95, 96, 98, 100, 101, 103, 105, 106, 108, 110, 111, 113, 115, 116, 118, 120, 121, 123, 125, 126, 128, 130, 131, 133, 135, 136, 138, 140, 141, 143, 145, 146, 148, 150, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 170, 171, 171, 172, 172, 173, 173, 174, 174, 175, 175, 176, 176, 177, 178, 178, 179, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224], dtype=np.uint8)
trace_1700_b = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 11, 14, 18, 22, 26, 29, 33, 37, 41, 44, 48, 52, 56, 59, 63, 67, 71, 74, 78, 82, 86, 89, 93, 93, 94, 94, 94, 94, 95, 95, 95, 96, 96, 96, 97, 97, 97, 97, 98, 98, 98, 99, 99, 99, 99, 100, 100, 100, 101, 101, 101, 102, 102, 102, 102, 103, 103, 103, 104, 104, 104, 104, 105, 105, 105, 106, 106, 106, 107, 107, 107, 107, 108, 108, 108, 109, 109, 109, 109, 110, 110, 110, 111, 111, 111, 112, 112, 112, 112, 113, 113, 113, 114, 114, 114, 114, 115, 115, 115, 116, 116, 116, 116, 117, 117, 117, 118, 118, 118, 119, 119, 119, 119, 120, 120, 120, 121, 121, 121, 121, 122, 122, 122, 123, 123], dtype=np.uint8)

def trace_color_table(measurement):
    '''Returns one of the standard color tables for TRACE JP2 files.'''
    # TRACE color tables
    try:
        r, g, b = {
            '171': (trace_171_r, trace_171_g, trace_171_b), 
            '195': (trace_195_r, trace_195_g, trace_195_b),
            '284': (trace_284_r, trace_284_g, trace_284_b),
            '1216': (trace_1216_r, trace_1216_g, trace_1216_b),
            '1550': (trace_1550_r, trace_1550_g, trace_1550_b),
            '1600': (trace_1600_r, trace_1600_g, trace_1600_b),
            '1700': (trace_1700_r, trace_1700_g, trace_1700_b),
            'WL': (grayscale, grayscale, grayscale)
        }[measurement]
    except KeyError:
        raise ValueError(
            "Invalid TRACE filter waveband passed.  Valid values are "
            "171, 195, 284, 1216, 1550, 1600, 1700, WL"
        )

    # Now create the color dictionary in the correct format
    cdict = create_cdict(r, g, b)

    # Return the color table
    return colors.LinearSegmentedColormap('mytable', cdict)

def sot_color_table(measurement):
    '''Returns one of the standard color tables for SOT files (following osdc convention).
    The relations between observation and color have been defined in hinode.py'''
    try:
        r, g, b = {
            'intensity': (r0, g0, b0),
            # TODO
            #'stokesQUV': (),
            #'magnetic field': (),
            #'velocity': (),
            #'width': (),
            }[measurement]
    except KeyError:
        raise ValueError(
            "Invalid (or not supported) SOT type. Valid values are: "
            "intensity" #TODO, stokesQUV, magnetic field, velocity, width."
            )
    
    cdict = create_cdict(r, g, b)
    return colors.LinearSegmentedColormap('mytable', cdict)

hmi_mag_r = np.array([0.537255, 0.549020, 0.556863, 0.564706, 0.584314, 0.592157, 0.603922, 0.619608, 0.631373,
             0.639216, 0.654902, 0.666667, 0.674510, 0.686275, 0.701961, 0.713726, 0.721569, 0.737255,
             0.749020, 0.756863, 0.764706, 0.784314, 0.792157, 0.803922, 0.819608, 0.831373, 0.839216,
             0.854902, 0.866667, 0.874510, 0.886275, 0.901961, 0.909804, 0.921569, 0.937255, 0.949020,
             0.956863, 0.972549, 0.984314, 1.000000, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157,
             0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157,
             0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157,
             0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157,
             0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.996078,
             0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078,
             0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078,
             0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 1.000000,
             1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000,
             1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000,
             0.780392, 0.709804, 0.603922, 0.372549, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,
             0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,
             0.000000, 0.000000, 0.000000, 0.000000, 0.784314, 0.772549, 0.760784, 0.749020, 0.741176,
             0.729412, 0.717647, 0.709804, 0.698039, 0.686275, 0.678431, 0.666667, 0.654902, 0.647059,
             0.635294, 0.623529, 0.615686, 0.603922, 0.592157, 0.580392, 0.572549, 0.560784, 0.549020,
             0.541176, 0.529412, 0.517647, 0.509804, 0.498039, 0.486275, 0.478431, 0.466667, 0.454902,
             0.447059, 0.435294, 0.423529, 0.411765, 0.403922, 0.392157, 0.380392, 0.372549, 0.360784,
             0.349020, 0.341176, 0.329412, 0.317647, 0.309804, 0.298039, 0.286275, 0.278431, 0.266667,
             0.254902, 0.247059, 0.235294, 0.219608, 0.203922, 0.192157, 0.176471, 0.164706, 0.149020,
             0.137255, 0.121569, 0.109804, 0.0941176, 0.0823529, 0.0666667, 0.0549020, 0.0392157,
             0.0274510, 0.0117647, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,
             0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,
             0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,
             0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,
             0.00000, 0.00000])
hmi_mag_g = np.array([0.00000, 0.00392157, 0.00784314, 0.0156863, 0.0156863, 0.0274510, 0.0313726, 0.0392157,
             0.0431373, 0.0549020, 0.0549020, 0.0666667, 0.0666667, 0.0784314, 0.0784314, 0.0901961,
             0.0901961, 0.101961, 0.109804, 0.109804, 0.121569, 0.121569, 0.129412, 0.137255, 0.137255,
             0.149020, 0.156863, 0.164706, 0.164706, 0.176471, 0.176471, 0.184314, 0.192157, 0.200000,
             0.203922, 0.211765, 0.219608, 0.219608, 0.231373, 0.239216, 0.247059, 0.254902, 0.262745,
             0.274510, 0.282353, 0.294118, 0.301961, 0.313726, 0.321569, 0.333333, 0.341176, 0.352941,
             0.360784, 0.372549, 0.380392, 0.388235, 0.396078, 0.407843, 0.415686, 0.423529, 0.435294,
             0.443137, 0.450980, 0.458824, 0.470588, 0.478431, 0.486275, 0.498039, 0.505882, 0.513726,
             0.525490, 0.533333, 0.541176, 0.549020, 0.560784, 0.568627, 0.576471, 0.588235, 0.596078,
             0.603922, 0.615686, 0.623529, 0.631373, 0.639216, 0.650980, 0.658824, 0.666667, 0.678431,
             0.686275, 0.694118, 0.701961, 0.713726, 0.721569, 0.729412, 0.741176, 0.749020, 0.756863,
             0.768627, 0.776471, 0.784314, 0.792157, 0.803922, 0.811765, 0.819608, 0.831373, 0.839216,
             0.847059, 0.858824, 0.403922, 0.435294, 0.470588, 0.505882, 0.541176, 0.576471, 0.611765,
             0.647059, 0.682353, 0.717647, 0.752941, 0.788235, 0.823529, 0.858824, 0.894118, 0.929412,
             0.964706, 1.00000, 0.784314, 0.709804, 0.603922, 0.505882, 0.470588, 0.498039, 0.529412,
             0.560784, 0.592157, 0.623529, 0.654902, 0.686275, 0.717647, 0.749020, 0.780392, 0.811765,
             0.843137, 0.874510, 0.905882, 0.937255, 0.968627, 1.00000, 0.784314, 0.776471, 0.768627,
             0.760784, 0.752941, 0.745098, 0.737255, 0.729412, 0.721569, 0.717647, 0.709804, 0.701961,
             0.694118, 0.686275, 0.678431, 0.670588, 0.662745, 0.658824, 0.650980, 0.643137, 0.635294,
             0.627451, 0.619608, 0.611765, 0.603922, 0.596078, 0.592157, 0.584314, 0.576471, 0.568627,
             0.560784, 0.552941, 0.545098, 0.537255, 0.533333, 0.525490, 0.517647, 0.509804, 0.501961,
             0.494118, 0.486275, 0.478431, 0.470588, 0.466667, 0.458824, 0.450980, 0.443137, 0.435294,
             0.427451, 0.419608, 0.411765, 0.407843, 0.396078, 0.384314, 0.376471, 0.364706, 0.352941,
             0.345098, 0.333333, 0.321569, 0.313726, 0.301961, 0.290196, 0.282353, 0.270588, 0.258824,
             0.250980, 0.239216, 0.227451, 0.219608, 0.211765, 0.203922, 0.200000, 0.192157, 0.184314,
             0.176471, 0.164706, 0.164706, 0.156863, 0.149020, 0.137255, 0.137255, 0.129412, 0.121569,
             0.109804, 0.109804, 0.101961, 0.0901961, 0.0901961, 0.0784314, 0.0784314, 0.0666667,
             0.0666667, 0.0549020, 0.0509804, 0.0431373, 0.0392157, 0.0313726, 0.0156863, 0.0156863,
             0.00784314, 0.00392157, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000])
hmi_mag_b = np.array([0.0784314, 0.0666667, 0.0666667, 0.0666667, 0.0666667, 0.0666667, 0.0627451, 0.0627451,
             0.0549020, 0.0549020, 0.0549020, 0.0549020, 0.0509804, 0.0509804, 0.0431373, 0.0431373,
             0.0431373, 0.0431373, 0.0392157, 0.0392157, 0.0313726, 0.0313726, 0.0313726, 0.0313726,
             0.0274510, 0.0274510, 0.0156863, 0.0156863, 0.0156863, 0.0156863, 0.0156863, 0.0156863,
             0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.00392157, 0.00392157, 0.00000, 0.00000,
             0.00784314, 0.0156863, 0.0274510, 0.0392157, 0.0509804, 0.0627451, 0.0745098, 0.0862745,
             0.0980392, 0.109804, 0.121569, 0.133333, 0.145098, 0.156863, 0.164706, 0.176471, 0.188235,
             0.196078, 0.207843, 0.219608, 0.231373, 0.239216, 0.250980, 0.262745, 0.274510, 0.282353,
             0.294118, 0.305882, 0.317647, 0.325490, 0.337255, 0.349020, 0.360784, 0.368627, 0.380392,
             0.392157, 0.403922, 0.411765, 0.423529, 0.435294, 0.447059, 0.454902, 0.466667, 0.478431,
             0.486275, 0.498039, 0.509804, 0.521569, 0.529412, 0.541176, 0.552941, 0.564706, 0.572549,
             0.584314, 0.596078, 0.607843, 0.615686, 0.627451, 0.639216, 0.650980, 0.658824, 0.670588,
             0.682353, 0.694118, 0.701961, 0.713726, 0.725490, 0.737255, 0.00000, 0.00000, 0.00000,
             0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,
             0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.509804, 0.650980, 0.654902, 0.407843,
             0.0549020, 0.0705882, 0.0901961, 0.105882, 0.125490, 0.141176, 0.160784, 0.180392, 0.196078,
             0.215686, 0.231373, 0.250980, 0.270588, 0.286275, 0.305882, 0.321569, 0.341176, 0.360784,
             1.000000, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078,
             0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078, 0.996078,
             0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157,
             0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.988235,
             0.988235, 0.988235, 0.988235, 0.988235, 0.988235, 0.988235, 0.988235, 0.988235, 0.988235,
             0.988235, 0.988235, 0.988235, 0.988235, 0.988235, 0.988235, 0.988235, 0.988235, 0.984314,
             0.984314, 0.984314, 0.984314, 0.984314, 0.984314, 0.984314, 0.984314, 0.984314, 0.984314,
             0.984314, 0.984314, 0.984314, 0.984314, 0.984314, 0.984314, 0.984314, 0.964706, 0.956863,
             0.937255, 0.929412, 0.909804, 0.894118, 0.886275, 0.866667, 0.858824, 0.839216, 0.827451,
             0.811765, 0.800000, 0.784314, 0.764706, 0.756863, 0.737255, 0.729412, 0.713726, 0.701961,
             0.686275, 0.674510, 0.658824, 0.647059, 0.631373, 0.619608, 0.603922, 0.584314, 0.576471,
             0.556863, 0.549020, 0.529412, 0.521569, 0.474510, 0.427451, 0.376471, 0.329412, 0.286275])

def hmi_mag_color_table():
    '''Returns an alternate HMI Magnetogram color table; from Stanford University / JSOC
           Reference: http://jsoc.stanford.edu/data/hmi/HMI_M.ColorTable.pdf

      Example usage for NRT data:
           import sunpy.map
           import sunpy.cm
           hmi = sunpy.map.Map('fblos.fits')
           hmi.cmap = sunpy.cm.get_cmap('hmimag')
           hmi.peek(vmin=-1500.0, vmax=1500.0)
           
           OR (for a basic plot with pixel values on the axes)
               import numpy as np
               import matplotlib.pyplot as plt
               import sunpy.map
               import sunpy.cm
               hmi = sunpy.map.Map('fblos.fits')
               plt.imshow(np.clip(hmi.data, -1500.0, 1500.0), cmap=sunpy.cm.get_cmap('hmimag'), origin='lower')
               plt.show()

      Example usage for science (Level 1.0) data:
            import numpy as np
            import sunpy.map
            import sunpy.cm
            hmi = sunpy.map.Map('hmi.m_45s.2014.05.11_12_00_45_TAI.magnetogram.fits')
            hmir = hmi.rotate(-np.radians(hmi.rotation_angle['y']),
                              rotation_center=(hmi.reference_pixel['y']-1.0,
                              hmi.reference_pixel['x']-1.0))
            hmir.cmap = sunpy.cm.get_cmap('hmimag')
            hmir.peek(vmin=-1500.0, vmax=1500.0)
    '''
    
    cdict = create_cdict(hmi_mag_r * 255, hmi_mag_g * 255, hmi_mag_b * 255)
    
    return colors.LinearSegmentedColormap('mytable', cdict)

def create_cdict(r, g, b):
    """ Create the color tuples in the correct format"""
    i = np.linspace(0, 1, r0.size)
    
    cdict = dict(
        (name, list(zip(i, el/255.0, el/255.0)))
        for el, name in [(r, 'red'),  (g, 'green'), (b, 'blue')]
    )
    return cdict
 
    

########NEW FILE########
__FILENAME__ = attrs
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from __future__ import absolute_import

from sqlalchemy import or_, and_, not_
from astropy.units import Unit, nm, equivalencies

from sunpy.time import parse_time
from sunpy.net.vso import attrs as vso_attrs
from sunpy.net.attr import AttrWalker, Attr, ValueAttr, AttrAnd, AttrOr
from sunpy.database.tables import DatabaseEntry, Tag as TableTag,\
    FitsHeaderEntry as TableFitsHeaderEntry

__all__ = [
    'Starred', 'Tag', 'Path', 'DownloadTime', 'FitsHeaderEntry', 'walker']

# This frozenset has been hardcoded to denote VSO attributes that are currently supported, on derdon's request.
SUPPORTED_SIMPLE_VSO_ATTRS = frozenset(['source', 'provider', 'physobs', 'instrument'])
SUPPORTED_NONVSO_ATTRS = frozenset(['starred'])

class _BooleanAttr(object):
    def __init__(self, value, make):
        self.value = bool(value)
        self.make = make

    def __and__(self, other):
        if not isinstance(other, self.make):
            return AttrAnd([self, other])
        attr = self.make()
        attr.value = self.value and other.value
        return attr

    def __or__(self, other):
        if not isinstance(other, self.make):
            return AttrOr([self, other])
        attr = self.make()
        attr.value = self.value or other.value
        return attr

    def __nonzero__(self):
        return self.value

    def __invert__(self):
        attr = self.make()
        attr.value = not self.value
        return attr

    def __eq__(self, other):
        return isinstance(other, self.make) and self.value == other.value

    def collides(self, other):  # pragma: no cover
        return False

    def __repr__(self):
        return '<{0}{1}()>'.format(
            '~' if not self.value else '', self.__class__.__name__)


class Starred(_BooleanAttr, Attr):
    def __init__(self):
        _BooleanAttr.__init__(self, True, self.__class__)


class Tag(Attr):
    def __init__(self, tagname):
        self.tagname = tagname
        self.inverted = False

    def __invert__(self):
        tag = self.__class__(self.tagname)
        tag.inverted = True
        return tag

    def collides(self, other):  # pragma: no cover
        return False

    def __repr__(self):
        return '<{0}Tag({1!r})>'.format(
            '~' if self.inverted else '', self.tagname)


class Path(Attr):
    def __init__(self, value):
        self.value = value
        self.inverted = False

    def __invert__(self):
        path = self.__class__(self.value)
        path.inverted = True
        return path

    def collides(self, other):  # pragma: no cover
        return isinstance(other, self.__class__)

    def __repr__(self):
        return '<{0}Path({1!r})>'.format(
            '~' if self.inverted else '', self.value)


# TODO: support excluding ranges as soon as
# vso_attrs._Range.__xor__ is fixed / renamed
class DownloadTime(Attr, vso_attrs._Range):
    def __init__(self, start, end):
        self.start = parse_time(start)
        self.end = parse_time(end)
        self.inverted = False
        vso_attrs._Range.__init__(self, start, end, self.__class__)

    def __invert__(self):
        download_time = self.__class__(self.start, self.end)
        download_time.inverted = True
        return download_time

    def collides(self, other):  # pragma: no cover
        return isinstance(other, self.__class__)

    def __repr__(self):
        return '<{0}DownloadTime({1!r}, {2!r})>'.format(
            '~' if self.inverted else '', self.start, self.end)


class FitsHeaderEntry(Attr):
    def __init__(self, key, value):
        self.key = key
        self.value = value
        self.inverted = False

    def __invert__(self):
        entry = self.__class__(self.key, self.value)
        entry.inverted = True
        return entry

    def collides(self, other):  # pragma: no cover
        return False

    def __repr__(self):
        return '<{0}FitsHeaderEntry({1!r}, {2!r})>'.format(
            '~' if self.inverted else '', self.key, self.value)


walker = AttrWalker()


@walker.add_creator(AttrOr)
def _create(wlk, root, session):
    entries = set()
    for attr in root.attrs:
        # make sure to add only new entries to the set to avoid duplicates
        entries.update(set(wlk.create(attr, session)) - entries)
    return list(entries)


@walker.add_creator(AttrAnd)
def _create(wlk, root, session):
    entries = [set(wlk.create(attr, session)) for attr in root.attrs]
    return list(set.intersection(*entries))


@walker.add_creator(ValueAttr)
def _create(wlk, root, session):
    query = session.query(DatabaseEntry)
    for key, value in root.attrs.iteritems():
        typ = key[0]
        if typ == 'tag':
            criterion = TableTag.name.in_([value])
            # `key[1]` is here the `inverted` attribute of the tag. That means
            # that if it is True, the given tag must not be included in the
            # resulting entries.
            if key[1]:
                query = query.filter(~DatabaseEntry.tags.any(criterion))
            else:
                query = query.filter(DatabaseEntry.tags.any(criterion))
        elif typ == 'fitsheaderentry':
            key, val, inverted = value
            key_criterion = TableFitsHeaderEntry.key == key
            value_criterion = TableFitsHeaderEntry.value == val
            if inverted:
                query = query.filter(not_(and_(
                    DatabaseEntry.fits_header_entries.any(key_criterion),
                    DatabaseEntry.fits_header_entries.any(value_criterion))))
            else:
                query = query.filter(and_(
                    DatabaseEntry.fits_header_entries.any(key_criterion),
                    DatabaseEntry.fits_header_entries.any(value_criterion)))
        elif typ == 'download time':
            start, end, inverted = value
            if inverted:
                query = query.filter(
                    ~DatabaseEntry.download_time.between(start, end))
            else:
                query = query.filter(
                    DatabaseEntry.download_time.between(start, end))
        elif typ == 'path':
            path, inverted = value
            if inverted:
                query = query.filter(or_(
                    DatabaseEntry.path != path, DatabaseEntry.path == None))
            else:
                query = query.filter(DatabaseEntry.path == path)
        elif typ == 'wave':
            min_, max_, unit = value
            waveunit = Unit(unit)
            # convert min_ and max_ to nm from the unit `waveunit`
            wavemin = waveunit.to(nm, min_, equivalencies.spectral())
            wavemax = waveunit.to(nm, max_, equivalencies.spectral())
            query = query.filter(and_(
                DatabaseEntry.wavemin >= wavemin,
                DatabaseEntry.wavemax <= wavemax))
        elif typ == 'time':
            start, end, near = value
            query = query.filter(and_(
                DatabaseEntry.observation_time_start < end,
                DatabaseEntry.observation_time_end > start))
        else:
            if typ.lower() not in SUPPORTED_SIMPLE_VSO_ATTRS.union(SUPPORTED_NONVSO_ATTRS):
                raise NotImplementedError("The attribute {0!r} is not yet supported to query a database.".format(typ))
            query = query.filter_by(**{typ: value})
    return query.all()


@walker.add_converter(Tag)
def _convert(attr):
    return ValueAttr({('tag', attr.inverted): attr.tagname})


@walker.add_converter(Starred)
def _convert(attr):
    return ValueAttr({('starred', ): attr.value})


@walker.add_converter(Path)
def _convert(attr):
    return ValueAttr({('path', ): (attr.value, attr.inverted)})


@walker.add_converter(DownloadTime)
def _convert(attr):
    return ValueAttr({
        ('download time', ): (attr.start, attr.end, attr.inverted)})


@walker.add_converter(FitsHeaderEntry)
def _convert(attr):
    return ValueAttr(
        {('fitsheaderentry', ): (attr.key, attr.value, attr.inverted)})


@walker.add_converter(vso_attrs._VSOSimpleAttr)
def _convert(attr):
    return ValueAttr({(attr.__class__.__name__.lower(), ): attr.value})


@walker.add_converter(vso_attrs.Wave)
def _convert(attr):
    return ValueAttr({('wave', ): (attr.min, attr.max, attr.unit)})


@walker.add_converter(vso_attrs.Time)
def _convert(attr):
    return ValueAttr({('time', ): (attr.start, attr.end, attr.near)})

########NEW FILE########
__FILENAME__ = caching
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from __future__ import absolute_import

from abc import ABCMeta, abstractmethod, abstractproperty
from collections import MutableMapping

from sunpy.util.odict import OrderedDict
from sunpy.util.counter import Counter

__all__ = ['BaseCache', 'LRUCache', 'LFUCache']


class BaseCache(object):
    """
    BaseCache is a class that saves and operates on an OrderedDict. It has a
    certain capacity, stored in the attribute `maxsize`. Whether this
    capacity is reached, can be checked by using the boolean property
    `is_full`. To implement a custom cache, inherit from this class and
    override the methods ``__getitem__`` and ``__setitem__``.
    Call the method `sunpy.database.caching.BaseCache.callback` as soon
    as an item from the cache is removed.
    """
    __metaclass__ = ABCMeta

    def __init__(self, maxsize=float('inf')):
        self.maxsize = maxsize
        self._dict = OrderedDict()

    def get(self, key, default=None):  # pragma: no cover
        """Return the corresponding value to `key` if `key` is in the cache,
        `default` otherwise. This method has no side-effects, multiple calls
        with the same cache and the same passed key must always return the same
        value.

        """
        try:
            return self._dict[key]
        except KeyError:
            return default

    @abstractmethod
    def __getitem__(self, key):
        """abstract method: this method must be overwritten by inheriting
        subclasses. It defines what happens if an item from the cache is
        attempted to be accessed.

        """
        return  # pragma: no cover

    @abstractmethod
    def __setitem__(self, key, value):
        """abstract method: this method must be overwritten by inheriting
        subclasses. It defines what happens if a new value should be assigned
        to the given key. If the given key does already exist in the cache or
        not must be checked by the person who implements this method.
        """

    @abstractproperty
    def to_be_removed(self):
        """The item that will be removed on the next
        :meth:`sunpy.database.caching.BaseCache.remove` call.

        """

    @abstractmethod
    def remove(self):
        """Call this method to manually remove one item from the cache. Which
        item is removed, depends on the implementation of the cache. After the
        item has been removed, the callback method is called.

        """

    def callback(self, key, value):
        """This method should be called (by convention) if an item is removed
        from the cache because it is full. The passed key and value are the
        ones that are removed. By default this method does nothing, but it
        can be customized in a custom cache that inherits from this base class.

        """

    @property
    def is_full(self):
        """True if the number of items in the cache equals :attr:`maxsize`,
        False otherwise.

        """
        return len(self._dict) == self.maxsize

    def __delitem__(self, key):
        self._dict.__delitem__(key)

    def __contains__(self, key):
        return key in self._dict.keys()

    def __len__(self):
        return len(self._dict)

    def __iter__(self):
        for key in self._dict.__iter__():
            yield key

    def __reversed__(self):  # pragma: no cover
        for key in self._dict.__reversed__():
            yield key

    def clear(self):  # pragma: no cover
        return self._dict.clear()

    def keys(self):  # pragma: no cover
        return self._dict.keys()

    def values(self):  # pragma: no cover
        return self._dict.values()

    def items(self):  # pragma: no cover
        return self._dict.items()

    def iterkeys(self):  # pragma: no cover
        return self._dict.iterkeys()

    def itervalues(self):  # pragma: no cover
        for value in self._dict.itervalues():
            yield value

    def iteritems(self):  # pragma: no cover
        for key, value in self._dict.iteritems():
            yield key, value

    def update(self, *args, **kwds):  # pragma: no cover
        self._dict.update(*args, **kwds)

    def pop(self, key, default=MutableMapping._MutableMapping__marker):  # pragma: no cover
        return self._dict.pop(key, default)

    def setdefault(self, key, default=None):  # pragma: no cover
        return self._dict.setdefault(key, default)

    def popitem(self, last=True):  # pragma: no cover
        return self._dict.popitem(last)

    def __reduce__(self):  # pragma: no cover
        return self._dict.__reduce__()

    def copy(self):  # pragma: no cover
        return self._dict.copy()

    def __eq__(self, other):  # pragma: no cover
        return self._dict.__eq__(other)

    def __ne__(self, other):  # pragma: no cover
        return self._dict.__ne__(other)

    def viewkeys(self):  # pragma: no cover
        return self._dict.viewkeys()

    def viewvalues(self):  # pragma: no cover
        return self._dict.viewvalues()

    def viewitems(self):  # pragma: no cover
        return self._dict.viewitems()

    @classmethod
    def fromkeys(cls, iterable, value=None):  # pragma: no cover
        return OrderedDict.fromkeys(iterable, value)

    def __repr__(self):  # pragma: no cover
        return '{0}({1!r})'.format(self.__class__.__name__, dict(self._dict))


class LRUCache(BaseCache):
    """
    LRUCache
    """
    @property
    def to_be_removed(self):
        """Return the least recently used key and its corresponding value as a
        tuple.

        """
        return self.iteritems().next()

    def remove(self):
        """Remove the least recently used item."""
        self.callback(*self.popitem(last=False))

    def __getitem__(self, key):
        """Returns the value which is associated to the given key and put it
        with its associated value to the end of this cache.

        Raises
        ------
        KeyError
            If the key cannot be found in the cache.

        """
        if key in self:
            value = self._dict.__getitem__(key)
            del self[key]
            self._dict.__setitem__(key, value)
            return value
        raise KeyError

    def __setitem__(self, key, value):
        """If the key does already exist in the cache, move it to the end of
        this cache. Otherwise, set a new value and put it to the end of this
        cache. If the cache is full, remove the least recently used item before
        inserting the new key-value pair.

        """
        if key in self:
            del self[key]
        if self.is_full:
            self.remove()
        self._dict.__setitem__(key, value)


class LFUCache(BaseCache):
    """
    LFUCache
    """
    def __init__(self, maxsize=float('inf')):
        self.usage_counter = Counter()
        BaseCache.__init__(self, maxsize)

    @property
    def to_be_removed(self):
        """Returns the key with the lowest times of access and its
        corresponding value as a tuple.

        """
        min_ = float('inf')
        lfu_key = None
        for k, v in self.usage_counter.iteritems():
            if v < min_:
                min_ = v
                lfu_key = k
        return lfu_key, self.get(lfu_key)

    def remove(self):
        """Remove the least frequently used item."""
        lfu_key, val = self.to_be_removed
        del self[lfu_key]
        del self.usage_counter[lfu_key]
        self.callback(lfu_key, val)

    def __getitem__(self, key):
        """Returns the value which is associated to the given key and
        increments the frequency counter of this key.

        Raises
        ------
        KeyError
            If the key cannot be found in the cache.

        """
        value = self._dict.__getitem__(key)
        self.usage_counter[key] += 1
        return value

    def __setitem__(self, key, value):
        """Increment the frequency counter of the given key if it is already
        present in the cache, otherwise set it to 1. If the cache is full,
        remove the least frequently used item before inserting the new
        key-value pair.

        """
        self.usage_counter[key] += 1
        if self.is_full:
            self.remove()
        self._dict.__setitem__(key, value)

########NEW FILE########
__FILENAME__ = commands
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from __future__ import absolute_import

from abc import ABCMeta, abstractmethod
import collections
import os

from sqlalchemy.orm import make_transient
from sqlalchemy.exc import InvalidRequestError


__all__ = [
    'EmptyCommandStackError', 'NoSuchEntryError', 'NonRemovableTagError',
    'DatabaseOperation', 'AddEntry', 'RemoveEntry', 'EditEntry',
    'CommandManager']


class EmptyCommandStackError(Exception):
    """This exception is raised if it is attempted to pop from a command stack
    even though it is empty.

    """


class NoSuchEntryError(Exception):
    """This exception is raised if it is attempted to remove an entry even
    though it does not exist in the database.

    """
    def __init__(self, database_entry):
        self.database_entry = database_entry

    def __str__(self):  # pragma: no cover
        return (
            'the database entry {0!r} cannot be removed because it '
            'is not stored in the database'.format(self.database_entry))


class NonRemovableTagError(Exception):
    """This exception is raised if it is attempted to remove a tag from a
    database entry even though it is not saved in this entry.

    """
    def __init__(self, database_entry, tag):
        self.database_entry = tag
        self.tag = tag

    def __str__(self):  # pragma: no cover
        errmsg = 'the tag {0} cannot be removed from the database entry {1!r}'
        return errmsg.format(self.database_entry, self.tag)


class DatabaseOperation(object):
    """This is the abstract main class for all database operations. To
    implement a new operation, inherit from this class and override the methods
    __call__ and undo. Both these methods get no parameters (except for self of
    course). The undo method is expected to do the exact opposite of the
    __call__ method, so that calling __call__ *and* undo multiple times in a
    row must not have any side-effects. This is not checked in any way, though.

    """
    __metaclass__ = ABCMeta

    @abstractmethod
    def __call__(self):
        return  # pragma: no cover

    @abstractmethod
    def undo(self):
        return  # pragma: no cover


class AddEntry(DatabaseOperation):
    """Add a new database entry to this session. It is not checked whether an
    equivalent entry is already saved in the session; this has to be checked by
    the caller. The ``undo`` method removes the entry from the session again.

    """
    def __init__(self, session, database_entry):
        self.session = session
        self.database_entry = database_entry

    def __call__(self):
        try:
            self.session.add(self.database_entry)
        except InvalidRequestError:
            # database entry cannot be added because it was removed from the
            # database -> use make_transient to send this object back to
            # the transient state
            make_transient(self.database_entry)
            self.session.add(self.database_entry)

    def undo(self):
        try:
            self.session.delete(self.database_entry)
        except InvalidRequestError:
            # database entry cannot be removed because the last call was not
            # followed by a commit -> use make_transient to revert putting the
            # entry into the pending state
            make_transient(self.database_entry)

    def __repr__(self):
        return '<{0}(session {1!r}, entry id {2})>'.format(
            self.__class__.__name__, self.session, self.database_entry.id)


class RemoveEntry(DatabaseOperation):
    """Remove the given database entry from the session. If it cannot be
    removed, because it is not stored in the session,
    :exc:`sunpy.database.NoSuchEntryError` is raised. The ``undo`` method puts
    the database entry back into the session object.

    """
    def __init__(self, session, entry):
        self.session = session
        self.entry = entry

    def __call__(self):
        try:
            self.session.delete(self.entry)
        except InvalidRequestError:
            # self.database_entry cannot be removed becaused it's not stored in
            # the database
            raise NoSuchEntryError(self.entry)

    def undo(self):
        make_transient(self.entry)
        self.session.add(self.entry)

    def __repr__(self):
        return '<{0}(session {1!r}, entry {2!r})>'.format(
            self.__class__.__name__, self.session, self.entry)


class EditEntry(DatabaseOperation):
    """Change the properties of the database entry. The given keyword arguments
    are used to set the attributes of the entry. The keys represent the
    attribute name and the values represent the new value of this attribute.
    Example: ``EditEntry(entry, foo='bar')`` will set the attribute ``foo`` of
    ``entry`` to the value ``'bar'``.

    """
    def __init__(self, database_entry, **kwargs):
        self.database_entry = database_entry
        if not kwargs:
            raise ValueError("at least one keyword argument must be given")
        self.kwargs = kwargs
        self.prev_values = {}

    def __call__(self):
        for k, v in self.kwargs.iteritems():
            # save those values in the dict prev_values that will be changed
            # so that they can be recovered
            self.prev_values[k] = getattr(self.database_entry, k)
            setattr(self.database_entry, k, v)

    def undo(self):
        for k, v in self.prev_values.iteritems():
            setattr(self.database_entry, k, v)

    def __repr__(self):
        return '<EditEntry(kwargs {0!r}, entry id {1})>'.format(
            self.kwargs, self.database_entry.id)


class AddTag(DatabaseOperation):
    def __init__(self, session, database_entry, tag):
        self.session = session
        self.database_entry = database_entry
        self.tag = tag

    def __call__(self):
        try:
            self.database_entry.tags.append(self.tag)
        except InvalidRequestError:
            # self.tag cannot be added because it was just removed
            # -> put it back to transient state
            make_transient(self.tag)
            self.database_entry.tags.append(self.tag)

    def undo(self):
        self.database_entry.tags.remove(self.tag)
        if not self.tag.data:
            # remove the tag from the database as well if it was the last tag
            # assigned to an entry
            try:
                RemoveEntry(self.session, self.tag)()
            except NoSuchEntryError:
                # entry cannot be removed because tag is only connected to
                # entries which are not saved in the database
                # -> can be safely ignored
                pass

    def __repr__(self):
        return "<AddTag(tag '{0}', session {1!r}, entry id {2})>".format(
            self.tag, self.session, self.database_entry.id)


class RemoveTag(DatabaseOperation):
    """Remove the tag from the given database entry. If the tag cannot be
    removed from the database entry because it is not assigned to the entry,
    :exc:`sunpy.database.NonRemovableTagError` is raised. The ``undo`` method
    puts the removed tag back into the tag list of the database entry.

    """
    def __init__(self, session, database_entry, tag):
        self.session = session
        self.database_entry = database_entry
        self.tag = tag

    def __call__(self):
        try:
            self.database_entry.tags.remove(self.tag)
        except ValueError:
            # tag not saved in entry
            raise NonRemovableTagError(self.database_entry, self.tag)
        else:
            if not self.tag.data:
                # remove the tag from the database as well if it was the last tag
                # assigned to an entry
                try:
                    RemoveEntry(self.session, self.tag)()
                except NoSuchEntryError:
                    # entry cannot be removed because tag is only connected to
                    # entries which are not saved in the database
                    # -> can be safely ignored
                    pass

    def undo(self):
        try:
            self.database_entry.tags.append(self.tag)
        except InvalidRequestError:
            # self.tag cannot be added because it was just removed
            # -> put it back to transient state
            make_transient(self.tag)
            self.database_entry.tags.append(self.tag)

    def __repr__(self):
        return "<RemoveTag(tag '{0}', session {1!r}, entry id {2})>".format(
            self.tag, self.session, self.database_entry.id)


class CommandManager(object):
    """The CommandManager saves all executed and reverted commands to act as an
    undo-redo-manager. All executed commands are saved in the list attribute
    ``undo_commands`` and all undone commands are saved in the list attribute
    ``redo_commands``. It is not recommended to alter these stacks directly;
    instead, use the methods ``push_undo_command``, ``pop_undo_command``,
    ``push_redo_command``, and ``pop_redo_command``, respectively.

    """
    def __init__(self):
        self.undo_commands = []
        self.redo_commands = []

    def clear_histories(self):
        """Clears all entries from the undo and redo history. If one or
        both of the histories are already empty, no exception is raised.

        """
        del self.undo_commands[:]
        del self.redo_commands[:]

    def push_undo_command(self, command):
        """Push the given command to the undo command stack."""
        self.undo_commands.append(command)

    def pop_undo_command(self):
        """Remove the last command from the undo command stack and return it.
        If the command stack is empty,
        :exc:`sunpy.database.commands.EmptyCommandStackError` is raised.

        """
        try:
            last_undo_command = self.undo_commands.pop()
        except IndexError:
            raise EmptyCommandStackError()
        return last_undo_command

    def push_redo_command(self, command):
        """Push the given command to the redo command stack."""
        self.redo_commands.append(command)

    def pop_redo_command(self):
        """Remove the last command from the redo command stack and return it.
        If the command stack is empty,
        :exc:`sunpy.database.commands.EmptyCommandStackError` is raised.

        """
        try:
            last_redo_command = self.redo_commands.pop()
        except IndexError:
            raise EmptyCommandStackError()
        return last_redo_command

    def do(self, command):
        """Execute the given command (a subclass of DatabaseOperation).
        Exceptions raised from the command are not caught. The passed argument
        may also be an iterable of commands. In this case, every command of the
        iterable is executed and only one entry is saved in the undo history.

        """
        if isinstance(command, collections.Iterable):
            for cmd in command:
                # FIXME: What follows is the worst hack of my life. Enjoy.
                # Without it, the test test_clear_database would fail.
                f = open(os.devnull, 'w'); f.write(repr(cmd)); f.flush()
                cmd()
        else:
            command()
        self.push_undo_command(command)
        # clear the redo stack when a new command was executed
        self.redo_commands[:] = []

    def undo(self, n=1):
        """Undo the last n commands. The default is to undo only the last
        command. If there is no command that can be undone because n is too big
        or because no command has been executed yet,
        :exc:`sunpy.database.commands.EmptyCommandStackError` is raised.

        """
        for _ in xrange(n):
            command = self.pop_undo_command()
            if isinstance(command, collections.Iterable):
                for cmd in reversed(command):
                    cmd.undo()
            else:
                command.undo()
            self.push_redo_command(command)

    def redo(self, n=1):
        """Redo the last n commands which have been undone using the undo
        method. The default is to redo only the last command which has been
        undone using the undo method. If there is no command that can be redone
        because n is too big or because no command has been undone yet,
        :exc:`sunpy.database.commands.EmptyCommandStackError` is raised.

        """
        for _ in xrange(n):
            command = self.pop_redo_command()
            if isinstance(command, collections.Iterable):
                for cmd in command:
                    cmd()
            else:
                command()
            self.push_undo_command(command)

########NEW FILE########
__FILENAME__ = database
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from __future__ import absolute_import

import itertools
import operator
from datetime import datetime
from contextlib import contextmanager

from sqlalchemy import create_engine, exists
from sqlalchemy.orm import sessionmaker

import sunpy
from sunpy.database import commands, tables, serialize
from sunpy.database.caching import LRUCache
from sunpy.database.attrs import walker
from sunpy.net.hek2vso import H2VClient
from sunpy.net.attr import and_
from sunpy.net.vso import VSOClient


class EntryNotFoundError(Exception):
    """This exception is raised if a database entry cannot be found by its
    unique ID.

    """
    def __init__(self, entry_id):
        self.entry_id = entry_id

    def __str__(self):  # pragma: no cover
        return 'an entry with the ID {0:d} does not exist'.format(
            self.entry_id)


class EntryAlreadyAddedError(Exception):
    """This exception is raised if a database entry is attempted to be added to
    the database although it was already saved in it.

    """
    def __init__(self, database_entry):
        self.database_entry = database_entry

    def __str__(self):  # pragma: no cover
        return (
            'the entry {0!r} was already added '
            'to the database'.format(self.database_entry))


class EntryAlreadyStarredError(Exception):
    """This exception is raised if a database entry is marked as starred
    using :meth:`Database.star` although it was already starred before this
    operation.

    """
    def __init__(self, database_entry):
        self.database_entry = database_entry

    def __str__(self):  # pragma: no cover
        return (
            'the entry {0!r} is already marked '
            'as starred'.format(self.database_entry))


class EntryAlreadyUnstarredError(Exception):
    """This exception is raised if the star mark from a database entry is
    attempted to be removed although the entry is not starred.

    """
    def __init__(self, database_entry):
        self.database_entry = database_entry

    def __str__(self):  # pragma: no cover
        return (
            'the entry {0!r} is already not marked '
            'as starred'.format(self.database_entry))


class NoSuchTagError(Exception):
    """This exception is raised if a tag cannot be found in a database by its
    name.

    """
    def __init__(self, tag_name):
        self.tag_name = tag_name

    def __str__(self):  # pragma: no cover
        return 'the tag {0!r} is not saved in the database'.format(
            self.tag_name)


class TagAlreadyAssignedError(Exception):
    """This exception is raised if it is attempted to assign a tag to a
    database entry but the database entry already has this tag assigned.

    """
    def __init__(self, database_entry, tag_name):
        self.database_entry = database_entry
        self.tag_name = tag_name

    def __str__(self):  # pragma: no cover
        errmsg = 'the database entry {0!r} has already assigned the tag {1!r}'
        return errmsg.format(self.database_entry, self.tag_name)


@contextmanager
def disable_undo(database):
    """A context manager to disable saving the used commands in the undo
    history. This may be useful when it's important to save memory because a
    big number of entries in the undo history may occupy a lot of memory space.

    Examples
    --------
    >>> with disable_undo(database) as db:
    ...     db.add(entry)
    >>> database.undo()
    >>> Traceback (most recent call last):
        ...
    EmptyCommandStackError
    """
    database._enable_history = False
    yield database
    database._enable_history = True


class Database(object):
    """
    Database(url[, CacheClass[, cache_size[, default_waveunit]]])

    Parameters
    ----------
    url : str
        A URL describing the database. This value is simply passed to
        :func:`sqlalchemy.create_engine`
        If not specified the value will be read from the sunpy config file.
    CacheClass : sunpy.database.caching.BaseCache
        A concrete cache implementation of the abstract class BaseCache.
        Builtin supported values for this parameters are
        :class:`sunpy.database.caching.LRUCache` and
        :class:`sunpy.database.caching.LFUCache`.
        The default value is :class:`sunpy.database.caching.LRUCache`.
    cache_size : int
        The maximum number of database entries, default is no limit.
    default_waveunit : str, optional
        The wavelength unit that will be used if an entry is added to the
        database but its wavelength unit cannot be found (either in the file or
        the VSO query result block, depending on the way the entry was added).
        If `None` (the default), attempting to add an entry without knowing the
        wavelength unit results in a
        :exc:`sunpy.database.WaveunitNotFoundError`.
    """
    """
    Attributes
    ----------
    session : sqlalchemy.orm.session.Session
        A SQLAlchemy session object. This may be used for advanced queries and
        advanced manipulations and should only be used by people who are
        experienced with SQLAlchemy.

    cache_size: int
        The maximum number of database entries. This attribute is read-only. To
        change this value, use the method
        :meth:`sunpy.database.Database.set_cache_size`.

    tags : list of sunpy.database.Tag objects
        A list of all saved tags in database. This attribute is read-only.

    default_waveunit : str
        See "Parameters" section.

    Methods
    -------
    set_cache_size(cache_size)
        Set a new value for the maxiumum number of database entries in the
        cache. Use the value ``float('inf')`` to disable caching.
    commit()
        Flush pending changes and commit the current transaction.
    get_entry_by_id(id)
        Get the database entry which has the given unique ID number assigned.
    get_tag(tagname)
        Get the tag which has the given unique tagname assigned. Returns None
        if no tag with the given name is saved in the database.
    tag(entry, *tags)
        Assign the given database entry the given tags. If no tags are given,
        TypeError is raised.
    star(entry, ignore_already_starred=False)
        Mark the given database entry as starred. If ``ignore_already_starred``
        is False and the given entry is already marked as starred,
        EntryAlreadyStarredError is raised.
    unstar(entry, ignore_already_unstarred=False)
        Remove the starred mark of the given entry. If
        ``ignore_already_unstarred`` is False and the entry is not marked as
        starred, EntryAlreadyUnstarredError is raised.
    add(entry, ignore_already_added=False)
        Add the given database entry to the database. If
        ``ignore_already_added`` is False and the given entry is already saved
        in the database, EntryAlreadyAddedError is raised.
    edit(entry, **kwargs)
        Change the given database entry so that it interprets the passed
        key-value pairs as new values where the keys represent the attributes
        of this entry. If no keywords arguments are given, :exc:`ValueError` is
        raised.
    remove(entry)
        Remove the given entry from the database.
    undo(n=1)
        Redo the last n operations.
    redo(n=1)
        Redo the last n undone operations.
    __contains__(entry)
        Return True if the given database entry is saved in the database,
        False otherwise.
    __iter__()
        Return an iterator over all database entries.
    __len__()
        Get the number of database entries.

    """
    def __init__(self, url=None, CacheClass=LRUCache, cache_size=float('inf'),
            default_waveunit=None):
        if url is None:
            url = sunpy.config.get('database', 'url')
        self._engine = create_engine(url)
        self._session_cls = sessionmaker(bind=self._engine)
        self.session = self._session_cls()
        self._command_manager = commands.CommandManager()
        self.default_waveunit = default_waveunit
        self._enable_history = True

        class Cache(CacheClass):
            def callback(this, entry_id, database_entry):
                self.remove(database_entry)

            def append(this, value):
                this[max(this or [0]) + 1] = value
        self._create_tables()
        self._cache = Cache(cache_size)
        for entry in self:
            self._cache[entry.id] = entry

    @property
    def url(self):
        """The sqlalchemy url of the database instance"""
        return str(self._engine.url)

    @property
    def cache_size(self):
        return len(self._cache)

    @property
    def cache_maxsize(self):
        return self._cache.maxsize

    def set_cache_size(self, cache_size):
        """Set a new value for the maxiumum number of database entries in the
        cache. Use the value ``float('inf')`` to disable caching. If the new
        cache is smaller than the previous one and cannot contain all the
        entries anymore, entries are removed from the cache until the number of
        entries equals the cache size. Which entries are removed depends on the
        implementation of the cache (e.g.
        :class:`sunpy.database.caching.LRUCache`,
        :class:`sunpy.database.caching.LFUCache`).

        """
        cmds = []
        # remove items from the cache if the given argument is lower than the
        # current cache size
        while cache_size < self.cache_size:
            # remove items from the cache until cache_size == maxsize of the
            # cache
            entry_id, entry = self._cache.to_be_removed
            cmd = commands.RemoveEntry(self.session, entry)
            if self._enable_history:
                cmds.append(cmd)
            else:
                cmd()
            del self._cache[entry_id]
        self._cache.maxsize = cache_size
        if cmds:
            self._command_manager.do(cmds)

    def _create_tables(self, checkfirst=True):
        """Initialise the database by creating all necessary tables. If
        ``checkfirst`` is True, already existing tables are not attempted to be
        created.

        """
        metadata = tables.Base.metadata
        metadata.create_all(self._engine, checkfirst=checkfirst)

    def commit(self):
        """Flush pending changes and commit the current transaction. This is a
        shortcut for :meth:`session.commit()`.

        """
        self.session.commit()

    def _download_and_collect_entries(self, query_result, client=None,
            path=None, progress=False):
        if client is None:
            client = VSOClient()
        for block in query_result:
            paths = client.get([block], path).wait(progress=progress)
            for path in paths:
                qr_entry = tables.DatabaseEntry._from_query_result_block(block)
                file_entries = list(
                    tables.entries_from_file(path, self.default_waveunit))
                for entry in file_entries:
                    entry.source = qr_entry.source
                    entry.provider = qr_entry.provider
                    entry.physobs = qr_entry.physobs
                    entry.fileid = qr_entry.fileid
                    entry.observation_time_start =\
                        qr_entry.observation_time_start
                    entry.observation_time_end = qr_entry.observation_time_end
                    entry.instrument = qr_entry.instrument
                    entry.size = qr_entry.size
                    entry.wavemin = qr_entry.wavemin
                    entry.wavemax = qr_entry.wavemax
                    entry.path = path
                    entry.download_time = datetime.utcnow()
                    yield entry

    def download(self, *query, **kwargs):
        """download(*query, client=sunpy.net.vso.VSOClient(), path=None, progress=False)
        Search for data using the VSO interface (see
        :meth:`sunpy.net.vso.VSOClient.query`). If querying the VSO results in
        no data, no operation is performed. Concrete, this means that no entry
        is added to the database and no file is downloaded. Otherwise, the
        retrieved search result is used to download all files that belong to
        this search result. After that, all the gathered information (the one
        from the VSO query result and the one from the downloaded FITS files)
        is added to the database in a way that each FITS header is represented
        by one database entry.

        """
        if not query:
            raise TypeError('at least one attribute required')
        client = kwargs.pop('client', None)
        path = kwargs.pop('path', None)
        progress = kwargs.pop('progress', False)
        if kwargs:
            k, v = kwargs.popitem()
            raise TypeError('unexpected keyword argument {0!r}'.format(k))
        if client is None:
            client = VSOClient()
        qr = client.query(*query)
        # don't do anything if querying the VSO results in no data
        if not qr:
            return
        entries = list(self._download_and_collect_entries(
            qr, client, path, progress))
        dump = serialize.dump_query(and_(*query))
        (dump_exists,), = self.session.query(
            exists().where(tables.JSONDump.dump == tables.JSONDump(dump).dump))
        if dump_exists:
            # dump already exists in table jsondumps -> edit instead of add
            # update all entries with the fileid `entry.fileid`
            for entry in entries:
                old_entry = self.session.query(
                    tables.DatabaseEntry).filter_by(fileid=entry.fileid).first()
                if old_entry is not None:
                    attrs = [
                        'source', 'provider', 'physobs',
                        'observation_time_start', 'observation_time_end',
                        'instrument', 'size', 'wavemin', 'wavemax',
                        'download_time']
                    kwargs = dict((k, getattr(entry, k)) for k in attrs)
                    cmd = commands.EditEntry(old_entry, **kwargs)
                    if self._enable_history:
                        self._command_manager.do(cmd)
                    else:
                        cmd()
        else:
            self.add_many(entries)
            # serialize the query and save the serialization in the database
            # for two reasons:
            #   1. to avoid unnecessary downloading in future calls of
            #      ``fetch``
            #   2. to know whether to add or to edit entries in future calls of
            #      ``download`` (this method)
            self.session.add(tables.JSONDump(dump))

    def fetch(self, *query, **kwargs):
        """fetch(*query[, path])
        Check if the query has already been used to collect new data using the
        :meth:`sunpy.database.Database.download` method. If yes, query the
        database using the method :meth:`sunpy.database.Database.query` and
        return the result. Otherwise, call
        :meth:`sunpy.database.Database.download` and return the result.

        """
        if not query:
            raise TypeError('at least one attribute required')
        path = kwargs.pop('path', None)
        if kwargs:
            k, v = kwargs.popitem()
            raise TypeError('unexpected keyword argument {0!r}'.format(k))
        dump = serialize.dump_query(and_(*query))
        (dump_exists,), = self.session.query(
            exists().where(tables.JSONDump.dump == tables.JSONDump(dump).dump))
        if dump_exists:
            return self.query(*query)
        return self.download(*query, path=path)

    def query(self, *query, **kwargs):
        """
        query(*query[, sortby])
        Send the given query to the database and return a list of
        database entries that satisfy all of the given attributes.

        Apart from the attributes supported by the VSO interface, the following
        attributes are supported:

            - :class:`sunpy.database.attrs.Starred`

            - :class:`sunpy.database.attrs.Tag`

            - :class:`sunpy.database.attrs.Path`

            - :class:`sunpy.database.attrs.DownloadTime`

            - :class:`sunpy.database.attrs.FitsHeaderEntry`

        An important difference to the VSO attributes is that these attributes
        may also be used in negated form using the tilde ~ operator.

        Parameters
        ----------
        query : list
            A variable number of attributes that are chained together via the
            boolean AND operator. The | operator may be used between attributes
            to express the boolean OR operator.
        sortby : str, optional
            The column by which to sort the returned entries. The default is to
            sort by the start of the observation. See the attributes of
            :class:`sunpy.database.tables.DatabaseEntry` for a list of all
            possible values.

        Raises
        ------
        TypeError
            if no attribute is given or if some keyword argument other than
            'sortby' is given.

        Examples
        --------
        The query in the following example searches for all non-starred entries
        with the tag 'foo' or 'bar' (or both).

        >>> database.query(~attrs.Starred(), attrs.Tag('foo') | attrs.Tag('bar'))

        """
        if not query:
            raise TypeError('at least one attribute required')
        sortby = kwargs.pop('sortby', 'observation_time_start')
        if kwargs:
            k, v = kwargs.popitem()
            raise TypeError('unexpected keyword argument {0!r}'.format(k))
        return sorted(
            walker.create(and_(*query), self.session),
            key=operator.attrgetter(sortby))

    def get_entry_by_id(self, entry_id):
        """Get a database entry by its unique ID number. If an entry with the
        given ID does not exist, :exc:`sunpy.database.EntryNotFoundError` is
        raised.

        """
        try:
            return self._cache[entry_id]
        except KeyError:
            raise EntryNotFoundError(entry_id)

    @property
    def tags(self):
        return self.session.query(tables.Tag).all()

    def get_tag(self, tag_name):
        """Get the tag which has the given name. If no such tag exists,
        :exc:`sunpy.database.NoSuchTagError` is raised.

        """
        for tag in self.tags:
            if tag_name == tag.name:
                return tag
        raise NoSuchTagError(tag_name)

    def tag(self, database_entry, *tags):
        """Assign the given database entry the given tags.

        Raises
        ------
        TypeError
            If no tags are given.

        sunpy.database.TagAlreadyAssignedError
            If at least one of the given tags is already assigned to the given
            database entry.

        """
        if not tags:
            raise TypeError('at least one tag must be given')
        # avoid duplicates
        tag_names = set(tags)
        cmds = []
        for tag_name in tag_names:
            try:
                tag = self.get_tag(tag_name)
                if tag in database_entry.tags:
                    raise TagAlreadyAssignedError(database_entry, tag_names)
            except NoSuchTagError:
                # tag does not exist yet -> create it
                tag = tables.Tag(tag_name)
            cmd = commands.AddTag(self.session, database_entry, tag)
            if self._enable_history:
                cmds.append(cmd)
            else:
                cmd()
        if cmds:
            self._command_manager.do(cmds)

    def remove_tag(self, database_entry, tag_name):
        """Remove the given tag from the database entry. If the tag is not
        connected to any entry after this operation, the tag itself is removed
        from the database as well.

        Raises
        ------
        sunpy.database.NoSuchTagError
            If the tag is not connected to the given entry.

        """
        tag = self.get_tag(tag_name)
        cmds = []
        remove_tag_cmd = commands.RemoveTag(self.session, database_entry, tag)
        remove_tag_cmd()
        if self._enable_history:
            cmds.append(remove_tag_cmd)
        if not tag.data:
            remove_entry_cmd = commands.RemoveEntry(self.session, tag)
            remove_entry_cmd()
            if self._enable_history:
                cmds.append(remove_entry_cmd)
        if self._enable_history:
            self._command_manager.push_undo_command(cmds)

    def star(self, database_entry, ignore_already_starred=False):
        """Mark the given database entry as starred. If this entry is already
        marked as starred, the behaviour depends on the optional argument
        ``ignore_already_starred``: if it is ``False`` (the default),
        :exc:`sunpy.database.EntryAlreadyStarredError` is raised. Otherwise,
        the entry is kept as starred and no exception is raised.

        """
        if database_entry.starred and not ignore_already_starred:
            raise EntryAlreadyStarredError(database_entry)
        self.edit(database_entry, starred=True)

    def unstar(self, database_entry, ignore_already_unstarred=False):
        """Remove the starred mark of the given entry. If this entry is not
        marked as starred, the behaviour depends on the optional argument
        ``ignore_already_unstarred``: if it is ``False`` (the default),
        :exc:`sunpy.database.EntryAlreadyUnstarredError` is raised. Otherwise,
        the entry is kept as unstarred and no exception is raised.

        """
        if not database_entry.starred and not ignore_already_unstarred:
            raise EntryAlreadyUnstarredError(database_entry)
        self.edit(database_entry, starred=False)

    def add_many(self, database_entries, ignore_already_added=False):
        """Add a row of database entries "at once". If this method is used,
        only one entry is saved in the undo history.

        Parameters
        ----------
        database_entries : iterable of sunpy.database.tables.DatabaseEntry
            The database entries that will be added to the database.

        ignore_already_added : bool, optional
            See Database.add

        """
        cmds = []
        for database_entry in database_entries:
            # use list(self) instead of simply self because __contains__ checks
            # for existence in the database and not only all attributes except
            # ID.
            if database_entry in list(self) and not ignore_already_added:
                raise EntryAlreadyAddedError(database_entry)
            cmd = commands.AddEntry(self.session, database_entry)
            if self._enable_history:
                cmds.append(cmd)
            else:
                cmd()
            if database_entry.id is None:
                self._cache.append(database_entry)
            else:
                self._cache[database_entry.id] = database_entry
        if cmds:
            self._command_manager.do(cmds)

    def add(self, database_entry, ignore_already_added=False):
        """Add the given database entry to the database table.

        Parameters
        ----------
        database_entry : sunpy.database.tables.DatabaseEntry
            The database entry that will be added to this database.

        ignore_already_added : bool, optional
            If True, attempts to add an already existing database entry will
            result in a :exc:`sunpy.database.EntryAlreadyAddedError`.
            Otherwise, a new entry will be added and there will be duplicates
            in the database.

        """
        if database_entry in self and not ignore_already_added:
            raise EntryAlreadyAddedError(database_entry)
        add_entry_cmd = commands.AddEntry(self.session, database_entry)
        if self._enable_history:
            self._command_manager.do(add_entry_cmd)
        else:
            add_entry_cmd()
        if database_entry.id is None:
            self._cache.append(database_entry)
        else:
            self._cache[database_entry.id] = database_entry

    def add_from_hek_query_result(self, query_result,
            ignore_already_added=False):
        """Add database entries from a HEK query result.

        Parameters
        ----------
        query_result : list
            The value returned by :meth:`sunpy.net.hek.HEKClient().query`

        ignore_already_added : bool
            See :meth:`sunpy.database.Database.add`.

        """
        vso_qr = itertools.chain.from_iterable(
            H2VClient().translate_and_query(query_result))
        self.add_from_vso_query_result(vso_qr, ignore_already_added)

    def download_from_vso_query_result(self, query_result, client=None,
            path=None, progress=False, ignore_already_added=False):
        """download(query_result, client=sunpy.net.vso.VSOClient(),
        path=None, progress=False, ignore_already_added=False)

        Add new database entries from a VSO query result and download the
        corresponding data files. See :meth:`sunpy.database.Database.download`
        for information about the parameters `client`, `path`, `progress`.

        Parameters
        ----------
        query_result : sunpy.net.vso.vso.QueryResponse
            A VSO query response that was returned by the ``query`` method of a
            :class:`sunpy.net.vso.VSOClient` object.

        ignore_already_added : bool
            See :meth:`sunpy.database.Database.add`.

        """
        if not query_result:
            return
        self.add_many(self._download_and_collect_entries(
            query_result, client, path, progress))

    def add_from_vso_query_result(self, query_result,
            ignore_already_added=False):
        """Generate database entries from a VSO query result and add all the
        generated entries to this database.

        Parameters
        ----------
        query_result : sunpy.net.vso.vso.QueryResponse
            A VSO query response that was returned by the ``query`` method of a
            :class:`sunpy.net.vso.VSOClient` object.

        ignore_already_added : bool
            See :meth:`sunpy.database.Database.add`.

        """
        self.add_many(
            tables.entries_from_query_result(
                query_result, self.default_waveunit),
            ignore_already_added)

    def add_from_dir(self, path, recursive=False, pattern='*',
            ignore_already_added=False):
        """Search the given directory for FITS files and use their FITS headers
        to add new entries to the database. Note that one entry in the database
        is assined to a list of FITS headers, so not the number of FITS headers
        but the number of FITS files which have been read determine the number
        of database entries that will be added. FITS files are detected by
        reading the content of each file, the `pattern` argument may be used to
        avoid reading entire directories if one knows that all FITS files have
        the same filename extension.

        Parameters
        ----------
        path : string
            The directory where to look for FITS files.

        recursive : bool, optional
            If True, the given directory will be searched recursively.
            Otherwise, only the given directory and no subdirectories are
            searched. The default is `False`, i.e. the given directory is not
            searched recursively.

        pattern : string, optional
            The pattern can be used to filter the list of filenames before the
            files are attempted to be read. The default is to collect all
            files. This value is passed to the function :func:`fnmatch.filter`,
            see its documentation for more information on the supported syntax.

        ignore_already_added : bool, optional
            See :meth:`sunpy.database.Database.add`.

        """
        cmds = []
        entries = tables.entries_from_dir(
            path, recursive, pattern, self.default_waveunit)
        for database_entry, filepath in entries:
            if database_entry in list(self) and not ignore_already_added:
                raise EntryAlreadyAddedError(database_entry)
            cmd = commands.AddEntry(self.session, database_entry)
            if self._enable_history:
                cmds.append(cmd)
            else:
                cmd()
            self._cache.append(database_entry)
        if cmds:
            self._command_manager.do(cmds)

    def add_from_file(self, file, ignore_already_added=False):
        """Generate as many database entries as there are FITS headers in the
        given file and add them to the database.

        Parameters
        ----------
        file : str or file-like object
            Either a path pointing to a FITS file or a an opened file-like
            object. If an opened file object, its mode must be one of the
            following rb, rb+, or ab+.

        ignore_already_added : bool, optional
            See :meth:`sunpy.database.Database.add`.

        """
        self.add_many(
            tables.entries_from_file(file, self.default_waveunit),
            ignore_already_added)

    def edit(self, database_entry, **kwargs):
        """Change the given database entry so that it interprets the passed
        key-value pairs as new values where the keys represent the attributes
        of this entry. If no keywords arguments are given, :exc:`ValueError` is
        raised.

        """
        cmd = commands.EditEntry(database_entry, **kwargs)
        if self._enable_history:
            self._command_manager.do(cmd)
        else:
            cmd()
        self._cache[database_entry.id] = database_entry

    def remove_many(self, database_entries):
        """Remove a row of database entries "at once". If this method is used,
        only one entry is saved in the undo history.

        Parameters
        ----------
        database_entries : iterable of sunpy.database.tables.DatabaseEntry
            The database entries that will be removed from the database.
        """
        cmds = []
        for database_entry in database_entries:
            cmd = commands.RemoveEntry(self.session, database_entry)
            if self._enable_history:
                cmds.append(cmd)
            else:
                cmd()
            try:
                del self._cache[database_entry.id]
            except KeyError:
                pass

        if cmds:
            self._command_manager.do(cmds)

    def remove(self, database_entry):
        """Remove the given database entry from the database table."""
        remove_entry_cmd = commands.RemoveEntry(self.session, database_entry)
        if self._enable_history:
            self._command_manager.do(remove_entry_cmd)
        else:
            remove_entry_cmd()
        try:
            del self._cache[database_entry.id]
        except KeyError:
            # entry cannot be removed because it was already removed or never
            # existed in the database. This can be safely ignored, the user
            # doesn't even know there's a cache here
            pass

    def clear(self):
        """Remove all entries from the databse. This operation can be undone
        using the :meth:`undo` method.

        """
        cmds = []
        for entry in self:
            for tag in entry.tags:
                cmds.append(commands.RemoveTag(self.session, entry, tag))
            # TODO: also remove all FITS header entries and all FITS header
            # comments from each entry before removing the entry itself!
        # remove all entries from all helper tables
        database_tables = [
            tables.JSONDump, tables.Tag, tables.FitsHeaderEntry,
            tables.FitsKeyComment]
        for table in database_tables:
            for entry in self.session.query(table):
                cmds.append(commands.RemoveEntry(self.session, entry))
        for entry in self:
            cmds.append(commands.RemoveEntry(self.session, entry))
            del self._cache[entry.id]
        if self._enable_history:
            self._command_manager.do(cmds)
        else:
            for cmd in cmds:
                cmd()

    def clear_histories(self):
        """Clears all entries from the undo and redo history.

        See Also
        --------
        :meth:`sunpy.database.commands.CommandManager.clear_histories`
        """
        self._command_manager.clear_histories()  # pragma: no cover

    def undo(self, n=1):
        """undo the last n commands.

        See Also
        --------
        :meth:`sunpy.database.commands.CommandManager.undo`

        """
        self._command_manager.undo(n)  # pragma: no cover

    def redo(self, n=1):
        """redo the last n commands.

        See Also
        --------
        :meth:`sunpy.database.commands.CommandManager.redo`

        """
        self._command_manager.redo(n)  # pragma: no cover

    def __getitem__(self, key):
        if isinstance(key, slice):
            entries = []
            start = 0 if key.start is None else key.start
            stop = len(self) if key.stop is None else key.stop
            step = 1 if key.step is None else key.step
            for i in xrange(start, stop, step):
                try:
                    entry = self[i]
                except IndexError:
                    break
                else:
                    self._cache[entry.id]
                    entries.append(entry)
            return entries
        # support negative indices
        if key < 0 < abs(key) <= len(self):
            key %= len(self)
        for i, entry in enumerate(self):
            if i == key:
                # "touch" the entry in the cache to intentionally cause
                # possible side-effects
                self._cache[entry.id]
                return entry
        raise IndexError

    def __contains__(self, database_entry):
        """Return True if the given database_entry entry is saved in the
        database, False otherwise.

        """
        (ret,), = self.session.query(
            exists().where(tables.DatabaseEntry.id == database_entry.id))
        return ret

    def __iter__(self):
        """iterate over all database entries that have been saved."""
        return iter(self.session.query(tables.DatabaseEntry))

    def __len__(self):
        """Get the number of rows in the table."""
        return self.session.query(tables.DatabaseEntry).count()

########NEW FILE########
__FILENAME__ = serialize
from operator import attrgetter
from datetime import datetime
import json

from sunpy.net import vso
from sunpy.database import attrs as db_attrs
from sunpy.net.attr import Attr, AttrOr, AttrAnd


__all__ = ['dump_query', 'load_query']


class QueryEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, datetime):
            return o.strftime("%Y-%m-%d %H:%M:%S")
        elif isinstance(o, Attr):
            if isinstance(o, (AttrAnd, AttrOr)):
                # sort by dictionary keys to be order-invariant
                values = sorted(o.attrs, key=attrgetter('__class__.__name__'))
            elif isinstance(o, vso.attrs.Wave):
                values = o.min, o.max, o.unit
            elif isinstance(o, vso.attrs.Time):
                values = o.start, o.end, o.near
            elif isinstance(o, (vso.attrs._VSOSimpleAttr, db_attrs.Starred)):
                values = o.value
            elif isinstance(o, db_attrs.Tag):
                values = o.tagname, o.inverted,
            elif isinstance(o, db_attrs.Path):
                values = o.value, o.inverted,
            elif isinstance(o, db_attrs.DownloadTime):
                values = o.start, o.end, o.inverted,
            elif isinstance(o, db_attrs.FitsHeaderEntry):
                values = o.key, o.value, o.inverted,
            else:
                assert False
            return {o.__class__.__name__: values}
        return json.JSONEncoder.default(self, o)


def query_decode(json_object):
    simple_attrs = [
        'Provider', 'Source', 'Instrument', 'Physobs', 'Pixels', 'Level',
        'Resolution', 'Detector', 'Filter', 'Sample', 'Quicklook', 'PScale']
    for key in simple_attrs:
        if key in json_object:
            Attr = getattr(vso.attrs, key)
            return Attr(json_object[key])
    for key in ['Wave', 'Time']:
        if key in json_object:
            Attr = getattr(vso.attrs, key)
            return Attr(*json_object[key])
    for key in ['Tag', 'Path', 'DownloadTime', 'FitsHeaderEntry']:
        if key in json_object:
            Attr = getattr(db_attrs, key)
            values, inverted = json_object[key][:-1], json_object[key][-1]
            if inverted:
                return ~Attr(*values)
            return Attr(*values)
    if 'Starred' in json_object:
        if json_object['Starred']:
            return ~db_attrs.Starred()
        return db_attrs.Starred()
    for key in ['AttrOr', 'AttrAnd']:
        if key in json_object:
            Attr = getattr(vso.attrs, key)
            return Attr(json_object[key])
    return json_object


def dump_query(query):  # pragma: no cover
    return json.dumps(query, cls=QueryEncoder)


def load_query(dump):  # pragma: no cover
    return json.loads(dump, object_hook=query_decode)

########NEW FILE########
__FILENAME__ = tables
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from __future__ import absolute_import

from time import strptime, mktime
from datetime import datetime
import fnmatch
import os
from itertools import imap

from astropy.units import Unit, nm, equivalencies
from sqlalchemy import Column, Integer, Float, String, DateTime, Boolean,\
    Table, ForeignKey
from sqlalchemy.orm import relationship
from sqlalchemy.ext.declarative import declarative_base

from sunpy.time import parse_time
from sunpy.io import fits, file_tools as sunpy_filetools
from sunpy.util import print_table

__all__ = [
    'WaveunitNotFoundError', 'WaveunitNotConvertibleError', 'JSONDump',
    'FitsHeaderEntry', 'FitsKeyComment', 'Tag', 'DatabaseEntry',
    'entries_from_query_result', 'entries_from_file', 'entries_from_dir',
    'display_entries']

Base = declarative_base()

# required for the many-to-many relation on tags:entries
association_table = Table('association', Base.metadata,
    Column('tag_name', String, ForeignKey('tags.name')),
    Column('entry_id', Integer, ForeignKey('data.id'))
)


class WaveunitNotFoundError(Exception):
    """This exception is raised if a wavelength unit cannot be found in a FITS
    header or in a VSO query result block.

    """
    def __init__(self, obj):
        self.obj = obj

    def __str__(self):  # pragma: no cover
        return 'the wavelength unit cannot be found in {0}'.format(self.obj)


class WaveunitNotConvertibleError(Exception):
    """This exception is raised if a wavelength cannot be converted to an
    astropy.units.Unit instance.

    """
    def __init__(self, waveunit):
        self.waveunit = waveunit

    def __str__(self):  # pragma: no cover
        return (
            'the waveunit {0!r} cannot be converted to an '
            'astropy.units.Unit instance'.format(self.waveunit))


# TODO: move this function outside this package (sunpy.util? sunpy.time?)
def timestamp2datetime(format, string):
    return datetime.fromtimestamp(mktime(strptime(string, format)))


class JSONDump(Base):
    __tablename__ = 'jsondumps'

    dump = Column(String, nullable=False, primary_key=True)

    def __init__(self, dump):
        self.dump = dump

    def __eq__(self, other):
        return self.dump == other.dump

    def __ne__(self, other):
        return not (self == other)

    def __str__(self):
        return self.dump

    def __repr__(self):  # pragma: no cover
        return '<{0}(dump {1!r})>'.format(self.__class__.__name__, self.dump)


class FitsHeaderEntry(Base):
    __tablename__ = 'fitsheaderentries'

    dbentry_id = Column(Integer, ForeignKey('data.id'))
    id = Column(Integer, primary_key=True)
    key = Column(String, nullable=False)
    value = Column(String)

    def __init__(self, key, value):
        self.key = key
        self.value = value

    def __eq__(self, other):
        return (
            (self.id == other.id or self.id is None or other.id is None) and
            self.key == other.key and
            self.value == other.value)

    def __ne__(self, other):
        return not (self == other)

    def __repr__(self):  # pragma: no cover
        return '<{0}(id {1}, key {2!r}, value {3!r})>'.format(
            self.__class__.__name__, self.id, self.key, self.value)


class FitsKeyComment(Base):
    __tablename__ = 'fitskeycomments'

    dbentry_id = Column(Integer, ForeignKey('data.id'))
    id = Column(Integer, primary_key=True)
    key = Column(String, nullable=False)
    value = Column(String)

    def __init__(self, key, value):
        self.key = key
        self.value = value

    def __eq__(self, other):
        return (
            (self.id == other.id or self.id is None or other.id is None) and
            self.key == other.key and
            self.value == other.value)

    def __ne__(self, other):
        return not (self == other)

    def __repr__(self):  # pragma: no cover
        return '<{0}(id {1}, key {2!r}, value {3!r})>'.format(
            self.__class__.__name__, self.id, self.key, self.value)


class Tag(Base):
    __tablename__ = 'tags'

    name = Column(String, nullable=False, primary_key=True)

    def __init__(self, name):
        self.name = name

    def __eq__(self, other):
        return self.name == other.name

    def __ne__(self, other):
        return not (self == other)

    def __str__(self):
        return self.name

    def __repr__(self):  # pragma: no cover
        return '<{0}(name {1!r})>'.format(self.__class__.__name__, self.name)


class DatabaseEntry(Base):
    """
    DatabaseEntry()

    The class :class:`DatabaseEntry` represents the main table of the database
    and each instance represents one record that *can* be saved in the
    database.

    Parameters
    ----------
    id : int
        A unique ID number. By default it is None, but automatically set to the
        maximum number plus one when this entry is added to the database.
    source : string
        The source is the name of an observatory or the name of a network of
        observatories.
    provider : string
        The name of the server which provides the retrieved data.
    physobs : string
        A physical observable identifier used by VSO.
    fileid : string
        The file ID is a string defined by the data provider that should point
        to a specific data product. The association of fileid to the specific
        data may change sometime, if the fileid always points to the latest
        calibrated data.
    observation_time_start : datetime
        The date and time when the observation of the data started.
    observation_time_end : datetime
        The date and time when the observation of the data ended.
    instrument : string
        The instrument which was used to observe the data.
    size : float
        The size of the data in kilobytes.
    wavemin : float
        The value of the measured wave length.
    wavemax : float
        This is the same value as ``wavemin``. The value is stored twice,
        because each ``suds.sudsobject.QueryResponseBlock`` which is used by
        the vso package contains both these values.
    path : string
        A local file path where the according FITS file is saved.
    download_time : datetime
        The date and time when the files connected to a query have been
        downloaded. Note: this is not the date and time when this entry has
        been added to a database!
    starred : bool
        Entries can be starred to mark them. By default, this value is False.
    fits_header_entries : list
        A list of ``FitsHeaderEntry`` instances.
    tags : list
        A list of ``Tag`` instances. Use `sunpy.database.Database.tag` to
        add a new tag or multiple tags to a specific entry.

    """
    __tablename__ = 'data'

    # FIXME: primary key is data provider + file ID + download_time!
    id = Column(Integer, primary_key=True)
    source = Column(String)
    provider = Column(String)
    physobs = Column(String)
    fileid = Column(String)
    observation_time_start = Column(DateTime)
    observation_time_end = Column(DateTime)
    instrument = Column(String)
    size = Column(Float)
    wavemin = Column(Float)
    wavemax = Column(Float)
    path = Column(String)
    download_time = Column(DateTime)
    starred = Column(Boolean, default=False)
    fits_header_entries = relationship('FitsHeaderEntry')
    fits_key_comments = relationship('FitsKeyComment')
    tags = relationship('Tag', secondary=association_table, backref='data')

    @classmethod
    def _from_query_result_block(cls, qr_block, default_waveunit=None):
        """Make a new :class:`DatabaseEntry` instance from a VSO query result
        block. The values of :attr:`wavemin` and :attr:`wavemax` are converted
        to nm (nanometres).

        Parameters
        ----------
        qr_block : suds.sudsobject.QueryResponseBlock
            A query result block is usually not created directly; instead,
            one gets instances of ``suds.sudsobject.QueryResponseBlock`` by
            iterating over a VSO query result.
        default_waveunit : str, optional
            The wavelength unit that is used if it cannot be found in the
            `qr_block`.

        Examples
        --------
        >>> from sunpy.net import vso
        >>> client = vso.VSOClient()
        >>> qr = client.query(
        ...     vso.attrs.Time('2001/1/1', '2001/1/2'),
        ...     vso.attrs.Instrument('eit'))
        >>> entry = DatabaseEntry.from_query_result_block(qr[0])
        >>> entry.source
        'SOHO'
        >>> entry.provider
        'SDAC'
        >>> entry.physobs
        'intensity'
        >>> entry.fileid
        '/archive/soho/private/data/processed/eit/lz/2001/01/efz20010101.010014'
        >>> entry.observation_time_start, entry.observation_time_end
        (datetime.datetime(2001, 1, 1, 1, 0, 14), datetime.datetime(2001, 1, 1, 1, 0, 21))
        >>> entry.instrument
        'EIT'
        >>> entry.size
        2059.0
        >>> entry.wavemin, entry.wavemax
        (17.1, 17.1)

        """
        time_start = timestamp2datetime('%Y%m%d%H%M%S', qr_block.time.start)
        time_end = timestamp2datetime('%Y%m%d%H%M%S', qr_block.time.end)
        wave = qr_block.wave
        unit = None
        if wave.waveunit is None:
            if default_waveunit is not None:
                unit = Unit(default_waveunit)
        else:
            # some query response blocks store the unit "kev",
            # but AstroPy only understands "keV". See issue #766.
            waveunit = wave.waveunit
            if waveunit == "kev":
                waveunit = "keV"
            unit = Unit(waveunit)
        if wave.wavemin is None:
            wavemin = None
        else:
            if unit is None:
                raise WaveunitNotFoundError(qr_block)
            wavemin = unit.to(nm, float(wave.wavemin), equivalencies.spectral())
        if wave.wavemax is None:
            wavemax = None
        else:
            if unit is None:
                raise WaveunitNotFoundError(qr_block)
            wavemax = unit.to(nm, float(wave.wavemax), equivalencies.spectral())
        source = str(qr_block.source) if qr_block.source is not None else None
        provider = str(qr_block.provider) if qr_block.provider is not None else None
        fileid = str(qr_block.fileid) if qr_block.fileid is not None else None
        instrument = str(qr_block.instrument) if qr_block.instrument is not None else None
        physobs = getattr(qr_block, 'physobs', None)
        if physobs is not None:
            physobs = str(physobs)
        return cls(
            source=source, provider=provider, physobs=physobs, fileid=fileid,
            observation_time_start=time_start, observation_time_end=time_end,
            instrument=instrument, size=qr_block.size,
            wavemin=wavemin, wavemax=wavemax)

    def __eq__(self, other):
        wavemins_equal = self.wavemin is None and other.wavemin is None or\
            self.wavemin is not None and other.wavemin is not None and\
            round(self.wavemin, 10) == round(other.wavemin, 10)
        wavemaxs_equal = self.wavemax is None and other.wavemax is None or\
            self.wavemax is not None and other.wavemax is not None and\
            round(self.wavemax, 10) == round(other.wavemax, 10)
        return (
            (self.id == other.id or self.id is None or other.id is None) and
            self.source == other.source and
            self.provider == other.provider and
            self.physobs == other.physobs and
            self.fileid == other.fileid and
            self.observation_time_start == other.observation_time_start and
            self.observation_time_end == other.observation_time_end and
            self.instrument == other.instrument and
            self.size == other.size and
            wavemins_equal and
            wavemaxs_equal and
            self.path == other.path and
            self.download_time == other.download_time and
            bool(self.starred) == bool(other.starred) and
            self.fits_header_entries == other.fits_header_entries and
            self.tags == other.tags)

    def __ne__(self, other):  # pragma: no cover
        return not (self == other)

    def __repr__(self):  # pragma: no cover
        attrs = [
            'id', 'source', 'provider', 'physobs', 'fileid',
            'observation_time_start', 'observation_time_end', 'instrument',
            'size', 'wavemin', 'wavemax', 'path', 'download_time', 'starred',
            'fits_header_entries', 'tags']
        ret = '<{0}('.format(self.__class__.__name__)
        for attr in attrs:
            value = getattr(self, attr, None)
            if value:
                ret += '{0} {1!r}, '.format(attr, value)
        ret = ret.rstrip(', ')
        ret += ')>'
        return ret


def entries_from_query_result(qr, default_waveunit=None):
    """Use a query response returned from :meth:`sunpy.net.vso.VSOClient.query`
    or :meth:`sunpy.net.vso.VSOClient.query_legacy` to generate instances of
    :class:`DatabaseEntry`. Return an iterator over those instances.

    Parameters
    ----------
    qr : sunpy.net.vso.vso.QueryResponse
        The query response from which to build the database entries.

    default_waveunit : str, optional
        See :meth:`sunpy.database.DatabaseEntry.from_query_result_block`.

    Examples
    --------
    >>> from sunpy.net import vso
    >>> from sunpy.database import entries_from_query_result
    >>> client = vso.VSOClient()
    >>> qr = client.query(
    ...     vso.attrs.Time('2001/1/1', '2001/1/2'),
    ...     vso.attrs.Instrument('eit'))
    >>> entries = entries_from_query_result(qr)
    >>> entry = entries.next()
    >>> entry.source
    'SOHO'
    >>> entry.provider
    'SDAC'
    >>> entry.physobs
    'intensity'
    >>> entry.fileid
    '/archive/soho/private/data/processed/eit/lz/2001/01/efz20010101.010014'
    >>> entry.observation_time_start, entry.observation_time_end
    (datetime.datetime(2001, 1, 1, 1, 0, 14), datetime.datetime(2001, 1, 1, 1, 0, 21))
    >>> entry.instrument
    'EIT'
    >>> entry.size
    2059.0
    >>> entry.wavemin, entry.wavemax
    (17.1, 17.1)

    """
    for block in qr:
        yield DatabaseEntry._from_query_result_block(block, default_waveunit)


def entries_from_file(file, default_waveunit=None):
    """Use the headers of a FITS file to generate an iterator of
    :class:`sunpy.database.tables.DatabaseEntry` instances. Gathered
    information will be saved in the attribute `fits_header_entries`. If the
    key INSTRUME, WAVELNTH or DATE-OBS / DATE_OBS is available, the attribute
    `instrument`, `wavemin` and `wavemax` or `observation_time_start` is set,
    respectively. If the wavelength unit can be read, the values of `wavemin`
    and `wavemax` are converted to nm (nanometres). The value of the `file`
    parameter is used to set the attribute `path` of each generated database
    entry.

    Parameters
    ----------
    file : str or file-like object
        Either a path pointing to a FITS file or a an opened file-like object.
        If an opened file object, its mode must be one of the following rb,
        rb+, or ab+.

    default_waveunit : str, optional
        The wavelength unit that is used for a header if it cannot be
        found.

    Raises
    ------
    sunpy.database.WaveunitNotFoundError
        If `default_waveunit` is not given and the wavelength unit cannot
        be found in one of the FITS headers

    sunpy.WaveunitNotConvertibleError
        If a wavelength unit could be found but cannot be used to create an
        instance of the type ``astropy.units.Unit``. This can be the case
        for example if a FITS header has the key `WAVEUNIT` with the value
        `nonsense`.

    Examples
    --------
    >>> entries = list(entries_from_file(sunpy.data.sample.SWAP_LEVEL1_IMAGE))
    >>> len(entries)
    1
    >>> entry = entries.pop()
    >>> entry.instrument
    'SWAP'
    >>> entry.observation_time_start, entry.observation_time_end
    (datetime.datetime(2012, 1, 1, 0, 16, 7, 836000), None)
    >>> entry.wavemin, entry.wavemax
    (17.400000000000002, 17.400000000000002)
    >>> len(entry.fits_header_entries)
    112

    """
    headers = fits.get_header(file)
    if isinstance(file, (str, unicode)):
        filename = file
    else:
        filename = getattr(file, 'name', None)
    for header in headers:
        entry = DatabaseEntry(path=filename)
        for key, value in header.iteritems():
            # Yes, it is possible to have an empty key in a FITS file.
            # Example: sunpy.data.sample.EIT_195_IMAGE
            # Don't ask me why this could be a good idea.
            if key == '':
                value = str(value)
            elif key == 'KEYCOMMENTS':
                for k, v in value.iteritems():
                    entry.fits_key_comments.append(FitsKeyComment(k, v))
                continue
            entry.fits_header_entries.append(FitsHeaderEntry(key, value))
        waveunit = fits.extract_waveunit(header)
        if waveunit is None:
            waveunit = default_waveunit
        unit = None
        if waveunit is not None:
            try:
                unit = Unit(waveunit)
            except ValueError:
                raise WaveunitNotConvertibleError(waveunit)
        for header_entry in entry.fits_header_entries:
            key, value = header_entry.key, header_entry.value
            if key == 'INSTRUME':
                entry.instrument = value
            elif key == 'WAVELNTH':
                if unit is None:
                    raise WaveunitNotFoundError(file)
                # use the value of `unit` to convert the wavelength to nm
                entry.wavemin = entry.wavemax = unit.to(
                    nm, value, equivalencies.spectral())
            # NOTE: the key DATE-END or DATE_END is not part of the official
            # FITS standard, but many FITS files use it in their header
            elif key in ('DATE-END', 'DATE_END'):
                entry.observation_time_end = parse_time(value)
            elif key in ('DATE-OBS', 'DATE_OBS'):
                entry.observation_time_start = parse_time(value)
        yield entry


def entries_from_dir(fitsdir, recursive=False, pattern='*',
        default_waveunit=None):
    """Search the given directory for FITS files and use the corresponding FITS
    headers to generate instances of :class:`DatabaseEntry`. FITS files are
    detected by reading the content of each file, the `pattern` argument may be
    used to avoid reading entire directories if one knows that all FITS files
    have the same filename extension.

    Parameters
    ----------
    fitsdir : string
        The directory where to look for FITS files.

    recursive : bool, optional
        If True, the given directory will be searched recursively. Otherwise,
        only the given directory and no subdirectories are searched. The
        default is `False`, i.e. the given directory is not searched
        recursively.

    pattern : string, optional
        The pattern can be used to filter the list of filenames before the
        files are attempted to be read. The default is to collect all files.
        This value is passed to the function :func:`fnmatch.filter`, see its
        documentation for more information on the supported syntax.

    default_waveunit : str, optional
        See
        :meth:`sunpy.database.tables.DatabaseEntry.add_fits_header_entries_from_file`.

    Returns
    -------
    generator of (DatabaseEntry, str) pairs
        A generator where each item is a tuple consisting of a
        :class:`DatabaseEntry` instance and the absolute path to the filename
        which was used to make the database entry.

    Examples
    --------
    >>> from pprint import pprint
    >>> from sunpy.data.test import rootdir as fitsdir
    >>> entries = list(entries_from_dir(fitsdir))
    >>> len(entries)
    2
    >>> # and now search `fitsdir` recursive
    >>> entries = list(entries_from_dir(fitsdir, True))
    >>> len(entries)
    15
    >>> # print the first 5 items of the FITS header of the first found file
    >>> first_entry, filename = entries[0]
    >>> pprint(first_entry.fits_header_entries[:5])
    [<FitsHeaderEntry(id None, key 'SIMPLE', value True)>,
     <FitsHeaderEntry(id None, key 'BITPIX', value -64)>,
     <FitsHeaderEntry(id None, key 'NAXIS', value 2)>,
     <FitsHeaderEntry(id None, key 'NAXIS1', value 128)>,
     <FitsHeaderEntry(id None, key 'NAXIS2', value 128)>]

    """
    for dirpath, dirnames, filenames in os.walk(fitsdir):
        filename_paths = (os.path.join(dirpath, name) for name in filenames)
        for path in fnmatch.filter(filename_paths, pattern):
            try:
                filetype = sunpy_filetools._detect_filetype(path)
            except (
                    sunpy_filetools.UnrecognizedFileTypeError,
                    sunpy_filetools.InvalidJPEG2000FileExtension):
                continue
            if filetype == 'fits':
                for entry in entries_from_file(path, default_waveunit):
                    yield entry, path
        if not recursive:
            break


def display_entries(database_entries, columns):
    """Generate a table to display the database entries.

    Parameters
    ----------
    database_entries : iterable of :class:`DatabaseEntry` instances
        The database entries will be the rows in the resulting table.

    columns : iterable of str
        The columns that will be displayed in the resulting table. Possible
        values for the strings are all attributes of :class:`DatabaseEntry`.

    Returns
    -------
    str
        A formatted table that can be printed on the console or written to a
        file.

    """
    header = [columns]
    rulers = [['-' * len(col) for col in columns]]
    data = []
    for entry in database_entries:
        row = []
        for col in columns:
            if col == 'starred':
                row.append('Yes' if entry.starred else 'No')
            elif col == 'tags':
                row.append(', '.join(imap(str, entry.tags)) or 'N/A')
            # do not display microseconds in datetime columns
            elif col in (
                    'observation_time_start',
                    'observation_time_end',
                    'download_time'):
                time = getattr(entry, col, None)
                if time is None:
                    formatted_time = 'N/A'
                else:
                    formatted_time = time.strftime('%Y-%m-%d %H:%M:%S')
                row.append(formatted_time)
            else:
                row.append(str(getattr(entry, col) or 'N/A'))
        if not row:
            raise TypeError('at least one column must be given')
        data.append(row)
    if not data:
        raise TypeError('given iterable is empty')
    return print_table(header + rulers + data)

########NEW FILE########
__FILENAME__ = test_attrs
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from datetime import datetime

import pytest

from sunpy.database.database import Database
from sunpy.database import tables
from sunpy.database.attrs import walker, Starred, Tag, Path, DownloadTime,\
    FitsHeaderEntry
from sunpy.net.attr import DummyAttr, AttrAnd, AttrOr
from sunpy.net import vso


@pytest.fixture
def obj():
    return object()


@pytest.fixture
def session():
    database = Database('sqlite:///:memory:')
    for i in xrange(1, 11):
        entry = tables.DatabaseEntry()
        database.add(entry)
        # every entry has a fake download time of 2005-06-15 i:00:00
        database.edit(entry, download_time=datetime(2005, 6, 15, i))
        # every second entry gets starred
        if i % 2 == 0:
            database.star(entry)
        # every third entry is stored in the path /tmp
        if i % 3 == 0:
            database.edit(entry, path='/tmp')
        # every fifth entry gets the tag 'foo'
        if i % 5 == 0:
            database.tag(entry, 'foo')
    # the last entry gets the FITS header entry INSTRUME=EIT
    entry.fits_header_entries.append(tables.FitsHeaderEntry('INSTRUME', 'EIT'))
    database.commit()
    return database.session


@pytest.fixture
def vso_session():
    client = vso.VSOClient()
    qr = client.query(
        vso.attrs.Time((2011, 9, 20, 1), (2011, 9, 20, 2)),
        vso.attrs.Instrument('RHESSI'))
    entries = tables.entries_from_query_result(qr)
    database = Database('sqlite:///:memory:')
    for entry in entries:
        database.add(entry)
    database.commit()
    return database.session


def test_starred_nonzero():
    assert Starred()


def test_starred_invert():
    assert not ~Starred()


def test_starred_and_different_types(obj):
    assert Starred() & obj == AttrAnd([Starred(), obj])


def test_starred_and_same_types():
    assert ~Starred() & ~Starred() == ~Starred()
    assert ~Starred() & Starred() == ~Starred()
    assert Starred() & ~Starred() == ~Starred()
    assert Starred() & Starred() == Starred()


def test_starred_or_different_types(obj):
    assert Starred() | obj == AttrOr([Starred(), obj])


def test_starred_or_same_types():
    assert ~Starred() | ~Starred() == ~Starred()
    assert ~Starred() | Starred() == Starred()
    assert Starred() | ~Starred() == Starred()
    assert Starred() | Starred() == Starred()


def test_starred_equality():
    assert Starred() == Starred()
    assert Starred() != ~Starred()
    assert ~Starred() != Starred()
    assert ~Starred() == ~Starred()


def test_starred_repr():
    assert repr(Starred()) == '<Starred()>'
    assert repr(~Starred()) == '<~Starred()>'


def test_tag_repr():
    assert repr(Tag('foo')) == "<Tag('foo')>"
    assert repr(~Tag('foo')) == "<~Tag('foo')>"


def test_path_repr():
    assert repr(Path('/tmp')) == "<Path('/tmp')>"
    assert repr(~Path('/tmp')) == "<~Path('/tmp')>"


def test_downloadtime_repr():
    download_time = DownloadTime('2008-12-8', datetime(2009, 6, 12))
    expected_repr = (
        '<DownloadTime(datetime.datetime(2008, 12, 8, 0, 0), '
        'datetime.datetime(2009, 6, 12, 0, 0))>')
    assert repr(download_time) == expected_repr


def test_inverted_downloadtime_repr():
    download_time = ~DownloadTime('2008-12-8', datetime(2009, 6, 12))
    expected_repr = (
        '<~DownloadTime(datetime.datetime(2008, 12, 8, 0, 0), '
        'datetime.datetime(2009, 6, 12, 0, 0))>')
    assert repr(download_time) == expected_repr


def test_fitsheaderentry_repr():
    header_entry = FitsHeaderEntry('key', 'value')
    assert repr(header_entry) == "<FitsHeaderEntry('key', 'value')>"
    assert repr(~header_entry) == "<~FitsHeaderEntry('key', 'value')>"


def test_walker_create_dummy(session):
    with pytest.raises(TypeError):
        walker.create(DummyAttr(), session)


def test_walker_create_starred_true(session):
    entries = walker.create(Starred(), session)
    tag = tables.Tag('foo')
    tag.id = 1
    fits_header_entry = tables.FitsHeaderEntry('INSTRUME', 'EIT')
    fits_header_entry.id = 1
    assert len(entries) == 5
    assert entries == [
        tables.DatabaseEntry(
            id=2, starred=True, download_time=datetime(2005, 6, 15, 2)),
        tables.DatabaseEntry(
            id=4, starred=True, download_time=datetime(2005, 6, 15, 4)),
        tables.DatabaseEntry(
            id=6, path='/tmp', starred=True,
            download_time=datetime(2005, 6, 15, 6)),
        tables.DatabaseEntry(
            id=8, starred=True, download_time=datetime(2005, 6, 15, 8)),
        tables.DatabaseEntry(
            id=10, starred=True, tags=[tag],
            download_time=datetime(2005, 6, 15, 10),
            fits_header_entries=[fits_header_entry])]


def test_walker_create_starred_false(session):
    entries = walker.create(~Starred(), session)
    tag = tables.Tag('foo')
    tag.id = 1
    assert len(entries) == 5
    assert entries == [
        tables.DatabaseEntry(id=1, download_time=datetime(2005, 6, 15, 1)),
        tables.DatabaseEntry(
            id=3, path='/tmp', download_time=datetime(2005, 6, 15, 3)),
        tables.DatabaseEntry(
            id=5, tags=[tag], download_time=datetime(2005, 6, 15, 5)),
        tables.DatabaseEntry(
            id=7, download_time=datetime(2005, 6, 15, 7)),
        tables.DatabaseEntry(
            id=9, path='/tmp', download_time=datetime(2005, 6, 15, 9))]


def test_walker_create_tag_positive(session):
    entries = walker.create(Tag('foo'), session)
    tag = tables.Tag('foo')
    tag.id = 1
    fits_header_entry = tables.FitsHeaderEntry('INSTRUME', 'EIT')
    fits_header_entry.id = 1
    assert len(entries) == 2
    assert entries == [
        tables.DatabaseEntry(
            id=5, tags=[tag], download_time=datetime(2005, 6, 15, 5)),
        tables.DatabaseEntry(
            id=10, starred=True, tags=[tag],
            download_time=datetime(2005, 6, 15, 10),
            fits_header_entries=[fits_header_entry])]


def test_walker_create_tag_negative(session):
    entries = walker.create(~Tag('foo'), session)
    assert len(entries) == 8
    assert entries == [
        tables.DatabaseEntry(id=1, download_time=datetime(2005, 6, 15, 1)),
        tables.DatabaseEntry(
            id=2, starred=True, download_time=datetime(2005, 6, 15, 2)),
        tables.DatabaseEntry(
            id=3, path='/tmp', download_time=datetime(2005, 6, 15, 3)),
        tables.DatabaseEntry(
            id=4, starred=True, download_time=datetime(2005, 6, 15, 4)),
        tables.DatabaseEntry(
            id=6, path='/tmp', starred=True,
            download_time=datetime(2005, 6, 15, 6)),
        tables.DatabaseEntry(
            id=7, download_time=datetime(2005, 6, 15, 7)),
        tables.DatabaseEntry(
            id=8, starred=True, download_time=datetime(2005, 6, 15, 8)),
        tables.DatabaseEntry(
            id=9, path='/tmp', download_time=datetime(2005, 6, 15, 9))]


def test_walker_create_anded_query(session):
    entries = walker.create(Tag('foo') & Starred(), session)
    assert len(entries) == 1
    tag = tables.Tag('foo')
    tag.id = 1
    fits_header_entry = tables.FitsHeaderEntry('INSTRUME', 'EIT')
    fits_header_entry.id = 1
    assert tables.DatabaseEntry(
        id=10, starred=True, tags=[tag],
        download_time=datetime(2005, 6, 15, 10),
        fits_header_entries=[fits_header_entry]) in entries


def test_walker_create_ored_query(session):
    entries = walker.create(Tag('foo') | Starred(), session)
    assert len(entries) == 6
    tag = tables.Tag('foo')
    tag.id = 1
    fits_header_entry = tables.FitsHeaderEntry('INSTRUME', 'EIT')
    fits_header_entry.id = 1
    assert tables.DatabaseEntry(
        id=2, starred=True, download_time=datetime(2005, 6, 15, 2)) in entries
    assert tables.DatabaseEntry(
        id=4, starred=True, download_time=datetime(2005, 6, 15, 4)) in entries
    assert tables.DatabaseEntry(
        id=5, tags=[tag], download_time=datetime(2005, 6, 15, 5)) in entries
    assert tables.DatabaseEntry(
        id=6, path='/tmp', starred=True,
        download_time=datetime(2005, 6, 15, 6)) in entries
    assert tables.DatabaseEntry(
        id=8, starred=True, download_time=datetime(2005, 6, 15, 8)) in entries
    assert tables.DatabaseEntry(
        id=10, starred=True, tags=[tag],
        download_time=datetime(2005, 6, 15, 10),
        fits_header_entries=[fits_header_entry]) in entries


def test_walker_create_complex_query(session):
    query = Tag('foo') & Starred() | ~Tag('foo') & ~Starred()
    entries = walker.create(query, session)
    assert len(entries) == 5
    tag = tables.Tag('foo')
    tag.id = 1
    fits_header_entry = tables.FitsHeaderEntry('INSTRUME', 'EIT')
    fits_header_entry.id = 1
    assert tables.DatabaseEntry(
        id=1, download_time=datetime(2005, 6, 15, 1)) in entries
    assert tables.DatabaseEntry(
        id=3, path='/tmp', download_time=datetime(2005, 6, 15, 3)) in entries
    assert tables.DatabaseEntry(
        id=7, download_time=datetime(2005, 6, 15, 7)) in entries
    assert tables.DatabaseEntry(
        id=9, path='/tmp', download_time=datetime(2005, 6, 15, 9)) in entries
    assert tables.DatabaseEntry(
        id=10, starred=True, tags=[tag],
        download_time=datetime(2005, 6, 15, 10),
        fits_header_entries=[fits_header_entry]) in entries


def test_walker_create_path_attr_notfound(session):
    assert walker.create(Path('doesnotexist'), session) == []


def test_walker_create_path_attr_exists(session):
    entries = walker.create(Path('/tmp'), session)
    assert len(entries) == 3
    assert tables.DatabaseEntry(
        id=3, path='/tmp', download_time=datetime(2005, 6, 15, 3)) in entries
    assert tables.DatabaseEntry(
        id=6, path='/tmp', starred=True,
        download_time=datetime(2005, 6, 15, 6)) in entries
    assert tables.DatabaseEntry(
        id=9, path='/tmp', download_time=datetime(2005, 6, 15, 9)) in entries


def test_walker_create_path_inverted(session):
    tag = tables.Tag('foo')
    tag.id = 1
    fits_header_entry = tables.FitsHeaderEntry('INSTRUME', 'EIT')
    fits_header_entry.id = 1
    entries = walker.create(~Path('/tmp'), session)
    assert len(entries) == 7
    assert entries == [
        tables.DatabaseEntry(
            id=1, download_time=datetime(2005, 6, 15, 1)),
        tables.DatabaseEntry(
            id=2, starred=True, download_time=datetime(2005, 6, 15, 2)),
        tables.DatabaseEntry(
            id=4, starred=True, download_time=datetime(2005, 6, 15, 4)),
        tables.DatabaseEntry(
            id=5, tags=[tag], download_time=datetime(2005, 6, 15, 5)),
        tables.DatabaseEntry(id=7, download_time=datetime(2005, 6, 15, 7)),
        tables.DatabaseEntry(
            id=8, starred=True, download_time=datetime(2005, 6, 15, 8)),
        tables.DatabaseEntry(
            id=10, starred=True, tags=[tag],
            download_time=datetime(2005, 6, 15, 10),
            fits_header_entries=[fits_header_entry])]


def test_walker_create_downloadtime_notfound(session):
    download_time = DownloadTime(
        datetime(2005, 6, 15, 11), datetime(2005, 6, 15, 11))
    entries = walker.create(download_time, session)
    assert entries == []


def test_walker_create_downloadtime_exists(session):
    download_time = DownloadTime(
        datetime(2005, 6, 15, 7), datetime(2005, 6, 15, 9))
    entries = walker.create(download_time, session)
    assert entries == [
        tables.DatabaseEntry(id=7, download_time=datetime(2005, 6, 15, 7)),
        tables.DatabaseEntry(
            id=8, starred=True, download_time=datetime(2005, 6, 15, 8)),
        tables.DatabaseEntry(
            id=9, path='/tmp', download_time=datetime(2005, 6, 15, 9))]


def test_walker_create_downloadtime_inverted(session):
    tag = tables.Tag('foo')
    tag.id = 1
    fits_header_entry = tables.FitsHeaderEntry('INSTRUME', 'EIT')
    fits_header_entry.id = 1
    download_time = ~DownloadTime(
        datetime(2005, 6, 15, 7), datetime(2005, 6, 15, 9))
    entries = walker.create(download_time, session)
    assert len(entries) == 7
    assert entries == [
        tables.DatabaseEntry(
            id=1, download_time=datetime(2005, 6, 15, 1)),
        tables.DatabaseEntry(
            id=2, starred=True, download_time=datetime(2005, 6, 15, 2)),
        tables.DatabaseEntry(
            id=3, path='/tmp', download_time=datetime(2005, 6, 15, 3)),
        tables.DatabaseEntry(
            id=4, starred=True, download_time=datetime(2005, 6, 15, 4)),
        tables.DatabaseEntry(
            id=5, tags=[tag], download_time=datetime(2005, 6, 15, 5)),
        tables.DatabaseEntry(
            id=6, starred=True, path='/tmp',
            download_time=datetime(2005, 6, 15, 6)),
        tables.DatabaseEntry(
            id=10, starred=True, tags=[tag],
            download_time=datetime(2005, 6, 15, 10),
            fits_header_entries=[fits_header_entry])]


def test_walker_create_fitsheader(session):
    tag = tables.Tag('foo')
    tag.id = 1
    entries = walker.create(FitsHeaderEntry('INSTRUME', 'EIT'), session)
    fits_header_entry = tables.FitsHeaderEntry('INSTRUME', 'EIT')
    fits_header_entry.id = 1
    assert len(entries) == 1
    assert entries == [tables.DatabaseEntry(
        id=10, starred=True, tags=[tag],
        download_time=datetime(2005, 6, 15, 10),
        fits_header_entries=[fits_header_entry])]


def test_walker_create_fitsheader_inverted(session):
    tag = tables.Tag('foo')
    tag.id = 1
    entries = walker.create(~FitsHeaderEntry('INSTRUME', 'EIT'), session)
    assert len(entries) == 9
    assert entries == [
        tables.DatabaseEntry(
            id=1, download_time=datetime(2005, 6, 15, 1)),
        tables.DatabaseEntry(
            id=2, starred=True, download_time=datetime(2005, 6, 15, 2)),
        tables.DatabaseEntry(
            id=3, path='/tmp', download_time=datetime(2005, 6, 15, 3)),
        tables.DatabaseEntry(
            id=4, starred=True, download_time=datetime(2005, 6, 15, 4)),
        tables.DatabaseEntry(
            id=5, tags=[tag], download_time=datetime(2005, 6, 15, 5)),
        tables.DatabaseEntry(
            id=6, starred=True, path='/tmp',
            download_time=datetime(2005, 6, 15, 6)),
        tables.DatabaseEntry(id=7, download_time=datetime(2005, 6, 15, 7)),
        tables.DatabaseEntry(
            id=8, starred=True, download_time=datetime(2005, 6, 15, 8)),
        tables.DatabaseEntry(
            id=9, path='/tmp', download_time=datetime(2005, 6, 15, 9))]


@pytest.mark.online
def test_walker_create_vso_instrument(vso_session):
    entries = walker.create(vso.attrs.Instrument('RHESSI'), vso_session)
    assert entries == [
        tables.DatabaseEntry(id=1, source=u'RHESSI', provider=u'LSSP',
            physobs=u'intensity',
            fileid=u'/hessidata/2011/09/20/hsi_20110920_010920',
            observation_time_start=datetime(2011, 9, 20, 1, 9, 20),
            observation_time_end=datetime(2011, 9, 20, 2, 27, 40),
            instrument=u'RHESSI', size=-1.0, wavemin=0.4132806430668068,
            wavemax=7.293187818826002e-05),
        tables.DatabaseEntry(id=2, source=u'RHESSI', provider=u'LSSP',
            physobs=u'intensity',
            fileid=u'/hessidata/2011/09/19/hsi_20110919_233340',
            observation_time_start=datetime(2011, 9, 19, 23, 33, 40),
            observation_time_end=datetime(2011, 9, 20, 1, 9, 20),
            instrument=u'RHESSI', size=-1.0, wavemin=0.4132806430668068,
            wavemax=7.293187818826002e-05)]


@pytest.mark.online
def test_walker_create_wave(vso_session):
    entries = walker.create(vso.attrs.Wave(0, 10), vso_session)
    assert len(entries) == 2
    entries = walker.create(vso.attrs.Wave(5, 10), vso_session)
    assert len(entries) == 0


@pytest.mark.online
def test_walker_create_time(vso_session):
    time = vso.attrs.Time(
        datetime(2011, 9, 17, 0, 0, 0), datetime(2011, 9, 20, 0, 0, 0))
    entries = walker.create(time, vso_session)
    assert len(entries) == 1
    assert entries == [
        tables.DatabaseEntry(id=2, source=u'RHESSI', provider=u'LSSP',
            physobs=u'intensity',
            fileid=u'/hessidata/2011/09/19/hsi_20110919_233340',
            observation_time_start=datetime(2011, 9, 19, 23, 33, 40),
            observation_time_end=datetime(2011, 9, 20, 1, 9, 20),
            instrument=u'RHESSI', size=-1.0, wavemin=0.4132806430668068,
            wavemax=7.293187818826002e-05)]

########NEW FILE########
__FILENAME__ = test_caching
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from __future__ import absolute_import

from collections import deque

import pytest

from sunpy.database.caching import BaseCache, LRUCache, LFUCache


def test_custom_cache():
    class FIFO(BaseCache):
        def __init__(self, maxsize=float('inf')):
            self.queue = deque([], maxsize)
            BaseCache.__init__(self, maxsize)

        @property
        def to_be_removed(self):
            try:
                return self.queue[0]
            except IndexError:
                return None

        def remove(self):
            del self.queue[0]

        def __getitem__(self, key):
            for k, value in self.queue:
                if k == key:
                    return value
            raise KeyError

        def __setitem__(self, key, value):
            self.queue.append((key, value))

        def __len__(self):
            return len(self.queue)
    cache = FIFO(3)
    cache[1] = 'a'
    cache[2] = 'b'
    cache[3] = 'c'
    assert cache.to_be_removed == (1, 'a')
    cache[4] = 'd'
    assert len(cache) == 3
    assert cache[2] == 'b'
    assert cache[3] == 'c'
    assert cache[4] == 'd'


def test_lru_cache():
    lrucache = LRUCache(3)
    lrucache[1] = 'a'
    lrucache[2] = 'b'
    lrucache[3] = 'c'
    assert lrucache.to_be_removed == (1, 'a')
    lrucache[1]
    lrucache[3]
    assert lrucache.to_be_removed == (2, 'b')
    lrucache[4] = 'd'
    assert len(lrucache) == 3
    assert lrucache.to_be_removed == (1, 'a')
    assert lrucache == {1: 'a', 3: 'c', 4: 'd'}


def test_lru_cache_missing_item():
    with pytest.raises(KeyError):
        LRUCache()[0]


def test_lfu_cache():
    lfucache = LFUCache(3)
    lfucache[1] = 'a'
    lfucache[2] = 'b'
    lfucache[3] = 'c'
    assert lfucache.to_be_removed == (1, 'a')
    lfucache[1]
    lfucache[2]
    assert lfucache.to_be_removed == (3, 'c')
    lfucache[4] = 'd'
    assert len(lfucache) == 3
    assert lfucache.to_be_removed == (4, 'd')
    assert lfucache == {1: 'a', 2: 'b', 4: 'd'}

########NEW FILE########
__FILENAME__ = test_commands
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from __future__ import absolute_import

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import pytest

from sunpy.database.commands import AddEntry, RemoveEntry, EditEntry,\
    AddTag, RemoveTag, NoSuchEntryError, NonRemovableTagError,\
    EmptyCommandStackError, CommandManager
from sunpy.database.tables import DatabaseEntry, Tag


@pytest.fixture
def session():
    # always create an in-memory database with its own new table in each test
    engine = create_engine('sqlite:///:memory:')
    Session = sessionmaker()
    DatabaseEntry.metadata.create_all(bind=engine)
    return Session(bind=engine)


@pytest.fixture
def command_manager():
    return CommandManager()


def test_add_entry_repr(session):
    entry = DatabaseEntry(id=5)
    repr_result = repr(AddEntry(session, entry))
    expected_repr_result = (
        '<AddEntry('
            'session <sqlalchemy.orm.session.Session object at {0:#x}>, '
            'entry id 5)>'.format(id(session)))
    assert repr_result == expected_repr_result


def test_add_entry(session):
    assert not session.new
    entry = DatabaseEntry()
    AddEntry(session, entry)()
    assert len(session.new) == 1
    assert entry.id is None
    session.commit()
    assert not session.new
    assert entry.id == 1


def test_add_entry_undo(session):
    entry = DatabaseEntry()
    cmd = AddEntry(session, entry)
    cmd()
    assert session.query(DatabaseEntry).count() == 1
    assert entry.id == 1
    cmd.undo()
    assert entry in session.deleted
    assert session.query(DatabaseEntry).count() == 0


def test_add_removed_entry(session):
    entry = DatabaseEntry()
    AddEntry(session, entry)()
    session.commit()
    RemoveEntry(session, entry)()
    session.commit()
    AddEntry(session, entry)()
    session.commit()
    assert not session.new
    assert entry.id == 1


def test_add_entry_undo_precommit(session):
    entry = DatabaseEntry()
    cmd = AddEntry(session, entry)
    cmd()
    cmd.undo()
    session.commit()
    assert session.query(DatabaseEntry).count() == 0


def test_edit_entry_repr():
    entry = DatabaseEntry(id=7)
    expected_repr_result = "<EditEntry(kwargs {'foo': 'bar'}, entry id 7)>"
    assert repr(EditEntry(entry, foo='bar')) == expected_repr_result


def test_edit_entry_invalid(session):
    with pytest.raises(ValueError):
        EditEntry(DatabaseEntry())


def test_edit_entry(session):
    entry = DatabaseEntry()
    session.add(entry)
    session.commit()
    assert entry.id == 1
    EditEntry(entry, id=42)()
    assert entry.id == 42


def test_edit_entry_undo(session):
    entry = DatabaseEntry()
    session.add(entry)
    session.commit()
    cmd = EditEntry(entry, id=42)
    cmd()
    session.commit()
    assert entry.id == 42
    cmd.undo()
    session.commit()
    assert entry.id == 1


def test_remove_entry_repr(session):
    entry = DatabaseEntry(id=3)
    expected_repr_result = (
        '<RemoveEntry('
            'session <sqlalchemy.orm.session.Session object at {0:#x}>, '
            'entry <DatabaseEntry(id 3)>)>'.format(id(session)))
    assert repr(RemoveEntry(session, entry)) == expected_repr_result


def test_remove_existing_entry(session):
    entry = DatabaseEntry()
    session.add(entry)
    assert session.query(DatabaseEntry).count() == 1
    assert entry.id == 1
    RemoveEntry(session, entry)()
    assert entry in session.deleted
    assert session.query(DatabaseEntry).count() == 0


def test_remove_nonexisting_entry(session):
    with pytest.raises(NoSuchEntryError):
        RemoveEntry(session, DatabaseEntry())()


def test_remove_entry_undo(session):
    entry = DatabaseEntry()
    session.add(entry)
    cmd = RemoveEntry(session, entry)
    session.commit()
    cmd()
    assert session.query(DatabaseEntry).count() == 0
    cmd.undo()
    assert session.query(DatabaseEntry).count() == 1


def test_add_tag_repr(session):
    entry = DatabaseEntry(id=12)
    tag = Tag('spam')
    expected_repr_result = (
        "<AddTag("
            "tag 'spam', "
            "session <sqlalchemy.orm.session.Session object at {0:#x}>, "
            "entry id 12)>".format(id(session)))
    assert repr(AddTag(session, entry, tag)) == expected_repr_result


def test_add_tag(session):
    tag = Tag('tag')
    entry = DatabaseEntry()
    assert entry.tags == []
    cmd = AddTag(session, entry, tag)
    cmd()
    assert tag in entry.tags


def test_add_removed_tag(session):
    entry = DatabaseEntry()
    tag = Tag('tag')
    entry.tags.append(tag)
    session.add(tag)
    session.commit()
    session.delete(tag)
    AddTag(session, entry, tag)()
    assert tag in entry.tags


def test_add_tag_undo_unsaved_entry(session):
    tag = Tag('tag')
    entry = DatabaseEntry()
    cmd = AddTag(session, entry, tag)
    cmd()
    cmd.undo()
    assert entry.tags == []
    cmd()
    assert tag in entry.tags


def test_remove_tag_repr(session):
    entry = DatabaseEntry(id=8)
    tag = Tag('foo')
    expected_repr_result = (
        "<RemoveTag("
            "tag 'foo', "
            "session <sqlalchemy.orm.session.Session object at {0:#x}>, "
            "entry id 8)>".format(id(session)))
    assert repr(RemoveTag(session, entry, tag)) == expected_repr_result


def test_remove_nonexisting_tag(session):
    cmd = RemoveTag(session, DatabaseEntry(), Tag('tag'))
    with pytest.raises(NonRemovableTagError):
        cmd()


def test_remove_tag_undo(session):
    tag = Tag('tag')
    entry = DatabaseEntry()
    entry.tags.append(tag)
    session.add(entry)
    session.commit()
    assert tag in entry.tags
    cmd = RemoveTag(session, entry, tag)
    cmd()
    assert tag not in entry.tags
    cmd.undo()
    assert tag in entry.tags


def test_cmd_manager_pop_undo_cmd(session, command_manager):
    cmd = AddEntry(session, DatabaseEntry())
    command_manager.do(cmd)
    popped_cmd = command_manager.pop_undo_command()
    assert popped_cmd == cmd


def test_cmd_manager_pop_undo_cmd_empty_stack(command_manager):
    with pytest.raises(EmptyCommandStackError):
        command_manager.pop_undo_command()


def test_cmd_manager_pop_redo_cmd(command_manager):
    with pytest.raises(EmptyCommandStackError):
        command_manager.pop_redo_command()


def test_cmd_manager_pop_redo_cmd_empty_stack(session, command_manager):
    cmd = AddEntry(session, DatabaseEntry())
    command_manager.do(cmd)
    command_manager.undo()
    popped_cmd = command_manager.pop_redo_command()
    assert popped_cmd == cmd


def test_cmd_manager_redo_stack_empty_after_call(session, command_manager):
    command_manager.do(AddEntry(session, DatabaseEntry()))
    command_manager.do(AddEntry(session, DatabaseEntry()))
    assert len(command_manager.undo_commands) == 2
    session.commit()
    command_manager.undo(2)
    assert not command_manager.undo_commands
    assert len(command_manager.redo_commands) == 2
    command_manager.do(AddEntry(session, DatabaseEntry()))
    assert not command_manager.redo_commands


def test_cmd_manager_redo(session, command_manager):
    assert command_manager.undo_commands == []
    assert command_manager.redo_commands == []
    command_manager.do(AddEntry(session, DatabaseEntry()))
    command_manager.do(AddEntry(session, DatabaseEntry()))
    assert len(command_manager.undo_commands) == 2
    assert command_manager.redo_commands == []
    session.commit()
    command_manager.undo(2)
    assert command_manager.undo_commands == []
    assert len(command_manager.redo_commands) == 2
    command_manager.redo(2)
    assert len(command_manager.undo_commands) == 2
    assert command_manager.redo_commands == []


def test_undo_redo_multiple_cmds_at_once(session, command_manager):
    assert command_manager.undo_commands == []
    command_manager.do([
        AddEntry(session, DatabaseEntry()),
        AddEntry(session, DatabaseEntry()),
        AddEntry(session, DatabaseEntry())])
    assert len(command_manager.undo_commands) == 1
    assert session.query(DatabaseEntry).count() == 3
    command_manager.undo()
    assert command_manager.undo_commands == []
    assert session.query(DatabaseEntry).count() == 0
    command_manager.redo()
    assert command_manager.redo_commands == []
    assert session.query(DatabaseEntry).count() == 3

########NEW FILE########
__FILENAME__ = test_database
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from __future__ import absolute_import

import glob
import ConfigParser
import os.path
import sys

import pytest
import sqlalchemy

import sunpy
from sunpy.database import Database, EntryAlreadyAddedError,\
    EntryAlreadyStarredError, EntryAlreadyUnstarredError, NoSuchTagError,\
    EntryNotFoundError, TagAlreadyAssignedError, disable_undo
from sunpy.database.tables import DatabaseEntry, Tag, FitsHeaderEntry,\
    FitsKeyComment, JSONDump
from sunpy.database.commands import EmptyCommandStackError, NoSuchEntryError
from sunpy.database.caching import LRUCache, LFUCache
from sunpy.database import attrs
from sunpy.net import vso, hek
from sunpy.data.sample import RHESSI_EVENT_LIST
from sunpy.data.test.waveunit import waveunitdir
from sunpy.io import fits


@pytest.fixture
def database_using_lrucache():
    return Database('sqlite:///:memory:', LRUCache, cache_size=3)


@pytest.fixture
def database_using_lfucache():
    return Database('sqlite:///:memory:', LFUCache, cache_size=3)


@pytest.fixture
def database():
    return Database('sqlite:///:memory:')


@pytest.fixture
def query_result():
    return vso.VSOClient().query(
        vso.attrs.Time('20130801T200000', '20130801T200030'),
        vso.attrs.Instrument('PLASTIC'))

@pytest.fixture
def download_qr():
    return vso.VSOClient().query(
        vso.attrs.Time('2012-03-29', '2012-03-29'),
        vso.attrs.Instrument('AIA'))


@pytest.fixture
def empty_query():
    return [
        vso.attrs.Time((2012, 7, 3), (2012, 7, 4)),
        vso.attrs.Instrument('EIT')]


@pytest.fixture
def download_query():
    return [
        vso.attrs.Time((2013, 5, 19, 2), (2013, 5, 19, 2), (2013, 5, 19, 2)),
        vso.attrs.Instrument('VIRGO') | vso.attrs.Instrument('SECCHI')]


@pytest.fixture
def filled_database():
    database = Database('sqlite:///:memory:')
    for i in xrange(1, 11):
        entry = DatabaseEntry()
        database.add(entry)
        # every fourth entry gets the tag 'foo'
        if i % 4 == 0:
            database.tag(entry, 'foo')
        # every fifth entry gets the tag 'bar'
        if i % 5 == 0:
            database.tag(entry, 'bar')
    database.commit()
    return database

def test_config_url(monkeypatch):
    monkeypatch.setattr("sunpy.config", ConfigParser.SafeConfigParser())
    url = 'sqlite:///'
    sunpy.config.add_section('database')
    sunpy.config.set('database', 'url', url)
    database = Database()
    assert database.url == url

def test_config_url_none(monkeypatch):
    monkeypatch.setattr("sunpy.config", ConfigParser.SafeConfigParser())
    with pytest.raises(ConfigParser.NoSectionError):
        Database()

def test_tags_unique(database):
    entry = DatabaseEntry()
    entry.tags = [Tag('foo')]
    database.add(entry)
    database.commit()
    entry.tags.append(Tag('foo'))
    with pytest.raises(sqlalchemy.orm.exc.FlushError):
        database.commit()


def test_setting_cache_size(database_using_lrucache):
    assert database_using_lrucache.cache_maxsize == 3
    assert database_using_lrucache.cache_size == 0
    for _ in xrange(5):
        database_using_lrucache.add(DatabaseEntry())
    assert len(database_using_lrucache) == 3
    assert database_using_lrucache.cache_size == 3
    assert database_using_lrucache.cache_maxsize == 3
    database_using_lrucache.set_cache_size(5)
    assert database_using_lrucache.cache_size == 3
    assert database_using_lrucache.cache_maxsize == 5
    for _ in xrange(5):
        database_using_lrucache.add(DatabaseEntry())
    assert len(database_using_lrucache) == 5
    assert database_using_lrucache.cache_size == 5
    assert database_using_lrucache.cache_maxsize == 5


def test_setting_cache_size_shrinking(database_using_lrucache):
    assert database_using_lrucache.cache_maxsize == 3
    assert database_using_lrucache.cache_size == 0
    for _ in xrange(5):
        database_using_lrucache.add(DatabaseEntry())
    assert len(database_using_lrucache) == 3
    assert database_using_lrucache.cache_maxsize == 3
    assert database_using_lrucache.cache_size == 3
    database_using_lrucache.set_cache_size(2)
    assert database_using_lrucache.cache_maxsize == 2
    assert database_using_lrucache.cache_size == 2
    assert len(database_using_lrucache) == 2
    assert list(database_using_lrucache) == [
        DatabaseEntry(id=4),
        DatabaseEntry(id=5)]
    for _ in xrange(5):
        database_using_lrucache.add(DatabaseEntry())
    assert len(database_using_lrucache) == 2
    assert database_using_lrucache.cache_maxsize == 2
    assert database_using_lrucache.cache_size == 2


def test_setting_cache_size_undo(database_using_lrucache):
    assert database_using_lrucache.cache_maxsize == 3
    assert database_using_lrucache.cache_size == 0
    for _ in xrange(5):
        database_using_lrucache.add(DatabaseEntry())
    assert len(database_using_lrucache) == 3
    database_using_lrucache.set_cache_size(1)
    assert database_using_lrucache.cache_size == 1
    assert len(database_using_lrucache) == 1
    database_using_lrucache.undo()
    assert len(database_using_lrucache) == 3


def test_get_entry_by_id_invalid(database):
    with pytest.raises(EntryNotFoundError):
        database.get_entry_by_id(0)


def test_get_entry_by_id_zero(filled_database):
    with pytest.raises(EntryNotFoundError):
        filled_database.get_entry_by_id(0)


def test_get_entry_by_id_accessible(filled_database):
    assert filled_database.get_entry_by_id(1) == DatabaseEntry(id=1)


def test_tags_property(database):
    assert database.tags == []


def test_get_existing_tag(database):
    entry = DatabaseEntry()
    database.tag(entry, 'tag')
    database.add(entry)
    expected_tag = Tag('tag')
    expected_tag.id = 1
    assert database.get_tag('tag') == expected_tag


def test_get_nonexting_tag(database):
    with pytest.raises(NoSuchTagError):
        database.get_tag('foo')


def test_tag_missing_tags_arg(database):
    with pytest.raises(TypeError):
        database.tag(DatabaseEntry())


def test_tag_new_tag(database):
    entry = DatabaseEntry()
    database.tag(entry, 'tag')
    assert len(entry.tags) == 1
    database.add(entry)
    assert len(database.tags) == 1
    tag = entry.tags[0]
    assert tag.name == 'tag'
    assert tag in database.tags


def test_tag_existing_tag(database):
    entry1 = DatabaseEntry()
    entry2 = DatabaseEntry()
    database.tag(entry1, 'tag')
    database.tag(entry2, 'tag')
    assert entry1.tags == entry2.tags


def test_tag_duplicate(database):
    entry = DatabaseEntry()
    database.add(entry)
    database.tag(entry, 'tag')
    database.commit()
    with pytest.raises(TagAlreadyAssignedError):
        database.tag(entry, 'tag')


def test_tag_duplicates_before_adding(database):
    entry1 = DatabaseEntry()
    entry2 = DatabaseEntry()
    database.tag(entry1, 'tag')
    database.tag(entry2, 'tag')
    database.add(entry1)
    database.add(entry2)
    with pytest.raises(sqlalchemy.orm.exc.FlushError):
        database.commit()


def test_tag_undo(database):
    entry = DatabaseEntry()
    database.add(entry)
    database.tag(entry, 'tag')
    assert len(database.tags) == 1
    assert len(entry.tags) == 1
    database.undo()
    assert len(entry.tags) == 0
    assert len(database.tags) == 0
    database.redo()
    assert len(database.tags) == 1
    assert len(entry.tags) == 1


def remove_nonexisting_tag(database):
    with pytest.raises(NoSuchTagError):
        database.remove_tag('foo')


def test_remove_tag(filled_database):
    foo = Tag('foo')
    foo.id = 1
    fourth_entry = filled_database.get_entry_by_id(4)
    assert foo in fourth_entry.tags
    eighth_entry = filled_database.get_entry_by_id(8)
    assert foo in eighth_entry.tags
    filled_database.remove_tag(fourth_entry, 'foo')
    assert foo not in fourth_entry.tags
    assert foo in filled_database.tags
    filled_database.remove_tag(eighth_entry, 'foo')
    assert foo not in eighth_entry.tags
    assert foo not in filled_database.tags


def test_remove_tag_undo_redo(filled_database):
    foo = Tag('foo')
    foo.id = 1
    fourth_entry = filled_database.get_entry_by_id(4)
    assert foo in fourth_entry.tags
    filled_database.remove_tag(fourth_entry, 'foo')
    assert foo not in fourth_entry.tags
    assert foo in filled_database.tags
    filled_database.undo()
    assert foo in fourth_entry.tags
    assert foo in filled_database.tags
    filled_database.redo()
    assert foo not in fourth_entry.tags
    assert foo in filled_database.tags
    eighth_entry = filled_database.get_entry_by_id(8)
    filled_database.remove_tag(eighth_entry, 'foo')
    assert foo not in eighth_entry.tags
    assert foo not in filled_database.tags
    filled_database.undo()
    assert foo not in fourth_entry.tags
    assert foo in eighth_entry.tags
    assert foo in filled_database.tags
    filled_database.redo()
    assert foo not in eighth_entry.tags
    assert foo not in filled_database.tags


def test_star_entry(database):
    entry = DatabaseEntry()
    assert not entry.starred
    database.star(entry)
    assert entry.starred


def test_star_already_starred_entry(database):
    entry = DatabaseEntry()
    database.star(entry)
    with pytest.raises(EntryAlreadyStarredError):
        database.star(entry)


def test_star_undo(database):
    entry = DatabaseEntry()
    assert not entry.starred
    database.star(entry)
    assert entry.starred
    database.undo()
    assert not entry.starred
    database.redo()
    assert entry.starred


def unstar_entry(database):
    entry = DatabaseEntry()
    assert not entry.starred
    database.star(entry)
    assert entry.starred
    database.unstar(entry)
    assert not entry.starred


def test_unstar_already_unstarred_entry(database):
    with pytest.raises(EntryAlreadyUnstarredError):
        database.unstar(DatabaseEntry())


def test_unstar_already_unstarred_entry_ignore(database):
    entry = DatabaseEntry()
    database.unstar(entry, True)
    assert not entry.starred


def test_unstar_undo(database):
    entry = DatabaseEntry()
    entry.starred = True
    database.unstar(entry)
    assert not entry.starred
    database.undo()
    assert entry.starred
    database.redo()
    assert not entry.starred


def test_add_many(database):
    assert len(database) == 0
    database.add_many((DatabaseEntry() for _ in xrange(5)))
    assert len(database) == 5
    database.undo()
    with pytest.raises(EmptyCommandStackError):
        database.undo()
    assert len(database) == 0
    database.redo()
    assert len(database) == 5


def test_add_many_with_existing_entry(database):
    evil_entry = DatabaseEntry()
    database.add(evil_entry)
    assert len(database) == 1
    with pytest.raises(EntryAlreadyAddedError):
        database.add_many([evil_entry])


def test_add_entry(database):
    entry = DatabaseEntry()
    assert entry.id is None
    database.add(entry)
    database.commit()
    assert entry.id == 1


def test_add_already_existing_entry(database):
    entry = DatabaseEntry()
    database.add(entry)
    database.commit()
    with pytest.raises(EntryAlreadyAddedError):
        database.add(entry)


def test_add_already_existing_entry_ignore(database):
    entry = DatabaseEntry()
    database.add(entry)
    database.add(entry, True)
    database.commit()
    assert entry.id == 1


@pytest.mark.online
def test_add_entry_from_hek_qr(database):
    hek_res = hek.HEKClient().query(
        hek.attrs.Time('2011/08/09 07:23:56', '2011/08/09 07:24:00'),
        hek.attrs.EventType('FL'))
    assert len(database) == 0
    database.add_from_hek_query_result(hek_res)
    assert len(database) == 2133


@pytest.mark.online
@pytest.mark.skipif(
        sys.version_info[:2] == (2,6),
        reason='for some unknown reason, this test fails on Python 2.6')
def test_download_from_qr(database, download_qr, tmpdir):
    assert len(database) == 0
    database.download_from_vso_query_result(
        download_qr, path=str(tmpdir.join('{file}.fits')))
    fits_pattern = str(tmpdir.join('*.fits'))
    num_of_fits_headers = sum(
        len(fits.get_header(file)) for file in glob.glob(fits_pattern))
    assert len(database) == num_of_fits_headers > 0
    for entry in database:
        assert os.path.dirname(entry.path) == str(tmpdir)
    database.undo()
    assert len(database) == 0
    database.redo()
    assert len(database) == num_of_fits_headers > 0


@pytest.mark.online
def test_add_entry_from_qr(database, query_result):
    assert len(database) == 0
    database.add_from_vso_query_result(query_result)
    assert len(database) == 10
    database.undo()
    assert len(database) == 0
    database.redo()
    assert len(database) == 10


@pytest.mark.online
def test_add_entries_from_qr_duplicates(database, query_result):
    assert len(database) == 0
    database.add_from_vso_query_result(query_result)
    assert len(database) == 10
    with pytest.raises(EntryAlreadyAddedError):
        database.add_from_vso_query_result(query_result)


@pytest.mark.online
def test_add_entries_from_qr_ignore_duplicates(database, query_result):
    assert len(database) == 0
    database.add_from_vso_query_result(query_result)
    assert len(database) == 10
    database.add_from_vso_query_result(query_result, True)
    assert len(database) == 20


def test_add_fom_path(database):
    assert len(database) == 0
    database.add_from_dir(waveunitdir)
    assert len(database) == 4
    database.undo()
    assert len(database) == 0
    database.redo()
    assert len(database) == 4


def test_add_fom_path_duplicates(database):
    database.add_from_dir(waveunitdir)
    assert len(database) == 4
    with pytest.raises(EntryAlreadyAddedError):
        database.add_from_dir(waveunitdir)


def test_add_fom_path_ignore_duplicates(database):
    database.add_from_dir(waveunitdir)
    assert len(database) == 4
    database.add_from_dir(waveunitdir, ignore_already_added=True)
    assert len(database) == 8


def test_add_from_file(database):
    assert len(database) == 0
    database.add_from_file(RHESSI_EVENT_LIST)
    assert len(database) == 11
    # make sure that all entries have the same fileid
    fileid = database[0].fileid
    for entry in database:
        assert entry.fileid == fileid


def test_add_from_file_duplicates(database):
    database.add_from_file(RHESSI_EVENT_LIST)
    with pytest.raises(EntryAlreadyAddedError):
        database.add_from_file(RHESSI_EVENT_LIST)


def test_add_from_file_ignore_duplicates(database):
    assert len(database) == 0
    database.add_from_file(RHESSI_EVENT_LIST)
    assert len(database) == 11
    database.add_from_file(RHESSI_EVENT_LIST, True)
    assert len(database) == 22


def test_edit_entry(database):
    entry = DatabaseEntry()
    database.add(entry)
    database.commit()
    assert entry.id == 1
    database.edit(entry, id=42)
    assert entry.id == 42


def test_remove_many_entries(filled_database):
    bar = Tag('bar')
    bar.id = 2
    # required to check if `remove_many` adds any entries to undo-history
    filled_database.clear_histories()
    filled_database.remove_many(filled_database[:8])
    assert len(filled_database) == 2
    assert list(filled_database) == [
        DatabaseEntry(id=9),
        DatabaseEntry(id=10, tags=[bar])]
    filled_database.undo()
    assert len(filled_database) == 10
    with pytest.raises(EmptyCommandStackError):
        filled_database.undo()


def test_remove_existing_entry(database):
    entry = DatabaseEntry()
    database.add(entry)
    assert database.session.query(DatabaseEntry).count() == 1
    assert entry.id == 1
    database.remove(entry)
    assert database.session.query(DatabaseEntry).count() == 0


def test_remove_nonexisting_entry(database):
    with pytest.raises(NoSuchEntryError):
        database.remove(DatabaseEntry())


def test_clear_empty_database(database):
    database.clear()


def test_clear_database(filled_database):
    assert len(filled_database) == 10
    filled_database.clear()
    assert not filled_database
    assert filled_database.session.query(JSONDump).all() == []
    assert filled_database.session.query(FitsHeaderEntry).all() == []
    assert filled_database.session.query(FitsKeyComment).all() == []
    assert filled_database.session.query(Tag).all() == []
    filled_database.undo()
    assert len(filled_database) == 10
    filled_database.redo()
    assert not filled_database
    assert filled_database.session.query(JSONDump).all() == []
    assert filled_database.session.query(FitsHeaderEntry).all() == []
    assert filled_database.session.query(FitsKeyComment).all() == []
    assert filled_database.session.query(Tag).all() == []


def test_getitem_notfound(database):
    with pytest.raises(IndexError):
        database[23]


def test_getitem_one(filled_database):
    first_entry = DatabaseEntry(id=1)
    assert filled_database[0] == first_entry


def test_getitem_getall(filled_database):
    entries = filled_database[:]
    assert entries == list(filled_database)


def test_getitem_custom(filled_database):
    entries = filled_database[1:5:2]
    foo = Tag('foo')
    foo.id = 1
    assert entries == [
        DatabaseEntry(id=2), DatabaseEntry(id=4, tags=[foo])]


def test_getitem_exceeding_range(filled_database):
    entries = filled_database[7:1000]
    foo = Tag('foo')
    foo.id = 1
    bar = Tag('bar')
    bar.id = 2
    assert entries == [
        DatabaseEntry(id=8, tags=[foo]),
        DatabaseEntry(id=9),
        DatabaseEntry(id=10, tags=[bar])]


def test_getitem_negative_index(filled_database):
    entry = filled_database[-4]
    assert entry == DatabaseEntry(id=7)


def test_getitem_negative_indices_slice(filled_database):
    entries = filled_database[-2:-8:-2]
    bar = Tag('bar')
    bar.id = 2
    assert entries == [
        DatabaseEntry(id=9),
        DatabaseEntry(id=7),
        DatabaseEntry(id=5, tags=[bar])]


def test_contains_exists(database):
    entry = DatabaseEntry()
    database.add(entry)
    database.commit()
    assert entry in database


def test_contains_precommit(database):
    entry = DatabaseEntry()
    database.add(entry)
    assert entry not in database


def test_contains_notexists(database):
    assert DatabaseEntry() not in database


def test_iter(database):
    entry1 = DatabaseEntry()
    entry2 = DatabaseEntry()
    database.add(entry1)
    database.add(entry2)
    expected_entries = [entry1, entry2]
    entries = list(database)
    assert entries == expected_entries


def test_len(database):
    assert len(database) == 0
    database.session.add(DatabaseEntry())
    assert len(database) == 1


def test_lru_cache(database_using_lrucache):
    assert not database_using_lrucache._cache
    entry1, entry2, entry3 = DatabaseEntry(), DatabaseEntry(), DatabaseEntry()
    database_using_lrucache.add(entry1)
    database_using_lrucache.add(entry2)
    database_using_lrucache.add(entry3)
    assert len(database_using_lrucache) == 3
    assert database_using_lrucache._cache.items() == [
        (1, entry1), (2, entry2), (3, entry3)]
    database_using_lrucache.get_entry_by_id(1)
    database_using_lrucache.get_entry_by_id(3)
    entry4 = DatabaseEntry()
    database_using_lrucache.add(entry4)
    assert len(database_using_lrucache) == 3
    assert database_using_lrucache._cache.items() == [
        (1, entry1), (3, entry3), (4, entry4)]


def test_lfu_cache(database_using_lfucache):
    assert not database_using_lfucache._cache
    entry1, entry2, entry3 = DatabaseEntry(), DatabaseEntry(), DatabaseEntry()
    database_using_lfucache.add(entry1)
    database_using_lfucache.add(entry2)
    database_using_lfucache.add(entry3)
    assert len(database_using_lfucache) == 3
    assert database_using_lfucache._cache.items() == [
        (1, entry1), (2, entry2), (3, entry3)]
    # access the entries #1 and #2 to increment their counters
    database_using_lfucache.get_entry_by_id(1)
    database_using_lfucache.get_entry_by_id(2)
    entry4 = DatabaseEntry()
    database_using_lfucache.add(entry4)
    assert len(database_using_lfucache) == 3
    assert database_using_lfucache._cache.items() == [
        (1, entry1), (2, entry2), (4, entry4)]


def test_query_missing_arg(database):
    with pytest.raises(TypeError):
        database.query()


def test_query_unexpected_kwarg(database):
    with pytest.raises(TypeError):
        database.query(attrs.Starred(), foo=42)


def test_query(filled_database):
    foo = Tag('foo')
    foo.id = 1
    bar = Tag('bar')
    bar.id = 2
    entries = filled_database.query(
        attrs.Tag('foo') | attrs.Tag('bar'), sortby='id')
    assert len(entries) == 4
    assert entries == [
        DatabaseEntry(id=4, tags=[foo]),
        DatabaseEntry(id=5, tags=[bar]),
        DatabaseEntry(id=8, tags=[foo]),
        DatabaseEntry(id=10, tags=[bar])]


def test_download_missing_arg(database):
    with pytest.raises(TypeError):
        database.download()


def test_download_unexpected_kwarg(database):
    with pytest.raises(TypeError):
        database.download(vso.attrs.Source('SOHO'), foo=42)


@pytest.mark.online
def test_download_empty_query_result(database, empty_query):
    database.download(*empty_query)
    with pytest.raises(EmptyCommandStackError):
        database.undo()
    assert len(database) == 0


@pytest.mark.online
@pytest.mark.skipif(
        sys.version_info[:2] == (2,6),
        reason='for some unknown reason, this test fails on Python 2.6')
def test_download(database, download_query, tmpdir):
    assert len(database) == 0
    database.default_waveunit = 'angstrom'
    database.download(
        *download_query, path=str(tmpdir.join('{file}.fits')), progress=True)
    fits_pattern = str(tmpdir.join('*.fits'))
    num_of_fits_headers = sum(
        len(fits.get_header(file)) for file in glob.glob(fits_pattern))
    assert len(database) == num_of_fits_headers
    for entry in database:
        assert os.path.dirname(entry.path) == str(tmpdir)
    database.undo()
    assert len(database) == 0
    database.redo()
    assert len(database) == 4


@pytest.mark.online
@pytest.mark.skipif(
        sys.version_info[:2] == (2,6),
        reason='for some unknown reason, this test fails on Python 2.6')
def test_download_duplicates(database, download_query, tmpdir):
    assert len(database) == 0
    database.default_waveunit = 'angstrom'
    database.download(
        *download_query, path=str(tmpdir.join('{file}.fits')), progress=True)
    assert len(database) == 4 
    download_time = database[0].download_time
    database.download(*download_query, path=str(tmpdir.join('{file}.fits')))
    assert len(database) == 4
    assert database[0].download_time != download_time


def test_fetch_missing_arg(database):
    with pytest.raises(TypeError):
        database.fetch()


def test_fetch_unexpected_kwarg(database):
    with pytest.raises(TypeError):
        database.fetch(vso.attrs.Source('SOHO'), foo=42)


@pytest.mark.online
@pytest.mark.skipif(
        sys.version_info[:2] == (2,6),
        reason='for some unknown reason, this test fails on Python 2.6')
def test_fetch(database, download_query, tmpdir):
    assert len(database) == 0
    database.default_waveunit = 'angstrom'
    database.fetch(*download_query, path=str(tmpdir.join('{file}.fits')))
    assert len(database) == 4
    download_time = database[0].download_time
    database.fetch(*download_query, path=str(tmpdir.join('{file}.fits')))
    assert len(database) == 4
    assert database[0].download_time == download_time


@pytest.mark.online
@pytest.mark.skipif(
        sys.version_info[:2] == (2,6),
        reason='for some unknown reason, this test fails on Python 2.6')
def test_disable_undo(database, download_query, tmpdir):
    entry = DatabaseEntry()
    with disable_undo(database) as db:
        db.set_cache_size(5)
        db.add(entry)
        db.commit()
        db.remove(entry)
        db.default_waveunit = 'angstrom'
        db.download(*download_query, path=str(tmpdir.join('{file}.fits')))
        entry = db[0]
        db.tag(entry, 'foo', 'bar')
        db.remove_tag(entry, 'foo')
        db.star(entry)
        db.unstar(entry)
        db.add_many([entry, entry], ignore_already_added=True)
        db.add(entry, ignore_already_added=True)
        db.add_from_dir(str(tmpdir))
        db.clear()
    with pytest.raises(EmptyCommandStackError):
        database.undo()

########NEW FILE########
__FILENAME__ = test_serialize
import json

from sunpy.net import vso
from sunpy.database import attrs as db_attrs
from sunpy.database.serialize import QueryEncoder, query_decode


def test_vso_wave():
    attr = vso.attrs.Wave(100, 200)
    expected = '{"Wave": [100.0, 200.0, "Angstrom"]}'
    assert json.dumps(attr, cls=QueryEncoder) == expected


def test_vso_time():
    attr = vso.attrs.Time((1999, 12, 31), (2000, 1, 1))
    expected = '{"Time": ["1999-12-31 00:00:00", "2000-01-01 00:00:00", null]}'
    assert json.dumps(attr, cls=QueryEncoder) == expected


def test_vso_simple_attr():
    attr = vso.attrs.Instrument('EIT')
    expected = '{"Instrument": "EIT"}'
    assert json.dumps(attr, cls=QueryEncoder) == expected


def test_starred():
    expected = '{"Starred": true}'
    assert json.dumps(db_attrs.Starred(), cls=QueryEncoder) == expected


def test_starred_inverted():
    expected = '{"Starred": false}'
    assert json.dumps(~db_attrs.Starred(), cls=QueryEncoder) == expected


def test_tag():
    expected = '{"Tag": ["foo", false]}'
    assert json.dumps(db_attrs.Tag('foo'), cls=QueryEncoder) == expected


def test_tag_inverted():
    expected = '{"Tag": ["foo", true]}'
    assert json.dumps(~db_attrs.Tag('foo'), cls=QueryEncoder) == expected


def test_path():
    expected = '{"Path": ["bar", false]}'
    assert json.dumps(db_attrs.Path('bar'), cls=QueryEncoder) == expected


def test_path_inverted():
    expected = '{"Path": ["bar", true]}'
    assert json.dumps(~db_attrs.Path('bar'), cls=QueryEncoder) == expected


def test_download_time():
    attr = db_attrs.DownloadTime((1991, 8, 25, 3, 15, 40), (2001, 3, 5))
    expected = (
        '{"DownloadTime": '
        '["1991-08-25 03:15:40", "2001-03-05 00:00:00", false]}')
    assert json.dumps(attr, cls=QueryEncoder) == expected


def test_download_time_inverted():
    attr = ~db_attrs.DownloadTime((1991, 8, 25, 3, 15, 40), (2001, 3, 5))
    expected = (
        '{"DownloadTime": '
        '["1991-08-25 03:15:40", "2001-03-05 00:00:00", true]}')
    assert json.dumps(attr, cls=QueryEncoder) == expected


def test_fits_header_entry():
    attr = db_attrs.FitsHeaderEntry('key', 'value')
    expected = '{"FitsHeaderEntry": ["key", "value", false]}'
    assert json.dumps(attr, cls=QueryEncoder) == expected


def test_fits_header_entry_inverted():
    attr = ~db_attrs.FitsHeaderEntry('key', 'value')
    expected = '{"FitsHeaderEntry": ["key", "value", true]}'
    assert json.dumps(attr, cls=QueryEncoder) == expected


def test_attr_or():
    attr = vso.attrs.Source('SOHO') | vso.attrs.Provider('SDAC')
    expected = '{"AttrOr": [{"Provider": "SDAC"}, {"Source": "SOHO"}]}'
    assert json.dumps(attr, cls=QueryEncoder) == expected


def test_attr_and():
    attr = vso.attrs.Source('SOHO') & vso.attrs.Provider('SDAC')
    expected = '{"AttrAnd": [{"Provider": "SDAC"}, {"Source": "SOHO"}]}'
    assert json.dumps(attr, cls=QueryEncoder) == expected


def test_decode_wave():
    dump = '{"Wave": [10.0, 20.0, "Angstrom"]}'
    assert json.loads(dump, object_hook=query_decode) == vso.attrs.Wave(10, 20)


def test_decode_time():
    dump = '{"Time": ["1999-12-31 00:00:00", "2000-01-01 00:00:00", null]}'
    expected = vso.attrs.Time((1999, 12, 31), (2000, 1, 1))
    assert json.loads(dump, object_hook=query_decode) == expected


def test_decode_simple_attr():
    dump = '{"Instrument": "EIT"}'
    expected = vso.attrs.Instrument('EIT')
    assert json.loads(dump, object_hook=query_decode) == expected


def test_decode_starred():
    dump = '{"Starred": false}'
    assert json.loads(dump, object_hook=query_decode) == db_attrs.Starred()


def test_decode_starred_inverted():
    dump = '{"Starred": true}'
    assert json.loads(dump, object_hook=query_decode) == ~db_attrs.Starred()


def test_decode_tag():
    dump = '{"Tag": ["foo", false]}'
    assert json.loads(dump, object_hook=query_decode) == db_attrs.Tag('foo')


def test_decode_path():
    dump = '{"Path": ["bar", false]}'
    assert json.loads(dump, object_hook=query_decode) == db_attrs.Path('bar')


def test_decode_download_time():
    dump = (
        '{"DownloadTime": '
        '["1991-08-25 03:15:40", "2001-03-05 00:00:00", true]}')
    expected = ~db_attrs.DownloadTime((1991, 8, 25, 3, 15, 40), (2001, 3, 5))
    assert json.loads(dump, object_hook=query_decode) == expected


def test_decode_fits_header_entry():
    dump = '{"FitsHeaderEntry": ["key", "value", false]}'
    expected = db_attrs.FitsHeaderEntry('key', 'value')
    assert json.loads(dump, object_hook=query_decode) == expected


def test_decode_or():
    dump = '{"AttrOr": [{"Source": "SOHO"}, {"Provider": "SDAC"}]}'
    expected = vso.attrs.Source('SOHO') | vso.attrs.Provider('SDAC')
    assert json.loads(dump, object_hook=query_decode) == expected


def test_decode_and():
    dump = '{"AttrAnd": [{"Source": "SOHO"}, {"Provider": "SDAC"}]}'
    expected = vso.attrs.Source('SOHO') & vso.attrs.Provider('SDAC')
    assert json.loads(dump, object_hook=query_decode) == expected

########NEW FILE########
__FILENAME__ = test_tables
# Author: Simon Liedtke <liedtke.simon@googlemail.com>
#
# This module was developed with funding provided by
# the Google Summer of Code (2013).

from collections import Hashable
from datetime import datetime
import os.path

from sunpy.database import Database
from sunpy.database.tables import FitsHeaderEntry, FitsKeyComment, Tag,\
    DatabaseEntry, entries_from_query_result, entries_from_dir,\
    entries_from_file, display_entries, WaveunitNotFoundError
from sunpy.net import vso
from sunpy.data.test import rootdir as testdir
from sunpy.data.test.waveunit import waveunitdir, MQ_IMAGE
from sunpy.data.sample import RHESSI_IMAGE, EIT_195_IMAGE

import pytest


@pytest.fixture
def query_result():
    client = vso.VSOClient()
    return client.query_legacy('2001/1/1', '2001/1/2', instrument='EIT')


@pytest.fixture
def qr_with_none_waves():
    return vso.VSOClient().query(
        vso.attrs.Time('20121224T120049.8', '20121224T120049.8'),
        vso.attrs.Provider('SDAC'), vso.attrs.Instrument('VIRGO'))


@pytest.fixture
def qr_block_with_missing_physobs():
    return vso.VSOClient().query(
        vso.attrs.Time('20130805T120000', '20130805T121000'),
        vso.attrs.Instrument('SWAVES'), vso.attrs.Source('STEREO_A'),
        vso.attrs.Provider('SSC'), vso.attrs.Wave(10, 160, 'kHz'))[0]


@pytest.fixture
def qr_block_with_kev_unit():
    return vso.VSOClient().query(
        vso.attrs.Time((2011, 9, 20, 1), (2011, 9, 20, 2)),
        vso.attrs.Instrument('RHESSI'))[0]


def test_fits_header_entry_equality():
    assert FitsHeaderEntry('key', 'value') == FitsHeaderEntry('key', 'value')
    assert not (FitsHeaderEntry('key', 'value') == FitsHeaderEntry('k', 'v'))


def test_fits_header_entry_inequality():
    assert FitsHeaderEntry('key', 'value') != FitsHeaderEntry('k', 'v')
    assert not (FitsHeaderEntry('k', 'v') != FitsHeaderEntry('k', 'v'))


def test_fits_header_entry_hashability():
    assert isinstance(FitsHeaderEntry('key', 'value'), Hashable)


def test_tag_equality():
    assert Tag('abc') == Tag('abc')
    assert not (Tag('abc') == Tag('xyz'))


def test_tag_inequality():
    assert Tag('abc') != Tag('xyz')
    assert not (Tag('abc') != Tag('abc'))


def test_tag_hashability():
    assert isinstance(Tag(''), Hashable)


@pytest.mark.online
def test_entry_from_qr_block(query_result):
    entry = DatabaseEntry._from_query_result_block(query_result[0])
    expected_entry = DatabaseEntry(
        source='SOHO', provider='SDAC', physobs='intensity',
        fileid='/archive/soho/private/data/processed/eit/lz/2001/01/efz20010101.010014',
        observation_time_start=datetime(2001, 1, 1, 1, 0, 14),
        observation_time_end=datetime(2001, 1, 1, 1, 0, 21),
        instrument='EIT', size=2059.0, wavemin=17.1, wavemax=17.1)
    assert entry == expected_entry


@pytest.mark.online
def test_entry_from_qr_block_with_missing_physobs(qr_block_with_missing_physobs):
    entry = DatabaseEntry._from_query_result_block(qr_block_with_missing_physobs)
    expected_entry = DatabaseEntry(
        source='STEREO_A', provider='SSC',
        fileid='swaves/2013/swaves_average_20130805_a_hfr.dat',
        observation_time_start=datetime(2013, 8, 5),
        observation_time_end=datetime(2013, 8, 6), instrument='SWAVES',
        size=3601.08, wavemin=2398339664000.0, wavemax=18737028625.0)
    assert entry == expected_entry


@pytest.mark.online
def test_entry_from_qr_block_kev(qr_block_with_kev_unit):
    # See issue #766.
    entry = DatabaseEntry._from_query_result_block(qr_block_with_kev_unit)
    assert entry.source == 'RHESSI'
    assert entry.provider == 'LSSP'
    assert entry.fileid == '/hessidata/2011/09/20/hsi_20110920_010920'
    assert entry.observation_time_start == datetime(2011, 9, 20, 1, 9, 20)
    assert entry.observation_time_end == datetime(2011, 9, 20, 2, 27, 40)
    assert entry.instrument == 'RHESSI'
    assert entry.size == -1
    assert round(entry.wavemin, 3) == 0.413
    assert round(entry.wavemax, 7) == 0.0000729


def test_entries_from_file():
    entries = list(entries_from_file(MQ_IMAGE))
    assert len(entries) == 1
    entry = entries[0]
    assert len(entry.fits_header_entries) == 31
    expected_fits_header_entries = [
        FitsHeaderEntry('SIMPLE', True),
        FitsHeaderEntry('BITPIX', 16),
        FitsHeaderEntry('NAXIS', 2),
        FitsHeaderEntry('NAXIS1', 1500),
        FitsHeaderEntry('NAXIS2', 1340),
        FitsHeaderEntry('CONTACT', 'Isabelle.Buale@obspm.fr'),
        FitsHeaderEntry('DATE_OBS', '2013-08-12T08:42:53.000'),
        FitsHeaderEntry('DATE_END', '2013-08-12T08:42:53.000'),
        FitsHeaderEntry('FILENAME', 'mq130812.084253.fits'),
        FitsHeaderEntry('INSTITUT', 'Observatoire de Paris'),
        FitsHeaderEntry('INSTRUME', 'Spectroheliograph'),
        FitsHeaderEntry('OBJECT', 'FS'),
        FitsHeaderEntry('OBS_MODE', 'SCAN'),
        FitsHeaderEntry('PHYSPARA', 'Intensity'),
        FitsHeaderEntry('NBREG', 1),
        FitsHeaderEntry('NBLAMBD', 1),
        FitsHeaderEntry('WAVELNTH', 6563),
        FitsHeaderEntry('WAVEUNIT', 'angstrom'),
        FitsHeaderEntry('POLARANG', 0),
        FitsHeaderEntry('THEMISFF', 3),
        FitsHeaderEntry('LONGTRC', 258.78),
        FitsHeaderEntry('LONGCARR', 258.78),
        FitsHeaderEntry('LONGITUD', 258.78),
        FitsHeaderEntry('LATITUD', 6.50107),
        FitsHeaderEntry('LATIRC', 6.50107),
        FitsHeaderEntry('INDLAMD', 1),
        FitsHeaderEntry('INDREG', 1),
        FitsHeaderEntry('SEQ_IND', 1),
        FitsHeaderEntry('SVECTOR', 0),
        FitsHeaderEntry('COMMENT', ''),
        FitsHeaderEntry('HISTORY', '')]
    assert entry.fits_header_entries == expected_fits_header_entries
    assert entry.fits_key_comments == [
        FitsKeyComment('SIMPLE', 'Written by IDL:  Mon Aug 12 08:48:08 2013'),
        FitsKeyComment('BITPIX', 'Integer*2 (short integer)')]
    assert entry.instrument == 'Spectroheliograph'
    assert entry.observation_time_start == datetime(2013, 8, 12, 8, 42, 53)
    assert entry.observation_time_end == datetime(2013, 8, 12, 8, 42, 53)
    assert round(entry.wavemin, 1) == 656.3
    assert round(entry.wavemax, 1) == 656.3
    assert entry.path == MQ_IMAGE


def test_entries_from_file_withoutwaveunit():
    # does not raise `WaveunitNotFoundError`, because no wavelength information
    # is present in this file
    entries_from_file(RHESSI_IMAGE).next()
    with pytest.raises(WaveunitNotFoundError):
        entries_from_file(EIT_195_IMAGE).next()


def test_entries_from_dir():
    entries = list(entries_from_dir(waveunitdir))
    assert len(entries) == 4
    for entry, filename in entries:
        if filename.endswith('na120701.091058.fits'):
            break
    assert entry.path == os.path.join(waveunitdir, filename)
    assert filename.startswith(waveunitdir)
    assert len(entry.fits_header_entries) == 42
    assert entry.fits_header_entries == [
        FitsHeaderEntry('SIMPLE', True),
        FitsHeaderEntry('BITPIX', -32),
        FitsHeaderEntry('NAXIS', 3),
        FitsHeaderEntry('NAXIS1', 256),
        FitsHeaderEntry('NAXIS2', 256),
        FitsHeaderEntry('NAXIS3', 1),
        FitsHeaderEntry('DATE', '27-OCT-82'),
        FitsHeaderEntry('DATE-OBS', '2012-07-01'),
        FitsHeaderEntry('DATE_OBS', '2012-07-01T09:10:58.200Z'),
        FitsHeaderEntry('DATE_END', '2012-07-01T09:10:58.200Z'),
        FitsHeaderEntry('WAVELNTH', 1.98669),
        FitsHeaderEntry('WAVEUNIT', 'm'),
        FitsHeaderEntry('PHYSPARA', 'STOKESI'),
        FitsHeaderEntry('OBJECT', 'FS'),
        FitsHeaderEntry('OBS_TYPE', 'RADIO'),
        FitsHeaderEntry('OBS_MODE', 'IMAGE'),
        FitsHeaderEntry('LONGITUD', 0.0),
        FitsHeaderEntry('LATITUDE', 0.0),
        FitsHeaderEntry('INSTITUT', 'MEUDON'),
        FitsHeaderEntry('CMP_NAME', 'ROUTINE'),
        FitsHeaderEntry('CONTACT', ' A. KERDRAON'),
        FitsHeaderEntry('TELESCOP', 'NRH'),
        FitsHeaderEntry('INSTRUME', 'NRH2'),
        FitsHeaderEntry('FILENAME', 'nrh2_1509_h80_20120701_091058c02_i.fts'),
        FitsHeaderEntry('NRH_DATA', '2DB'),
        FitsHeaderEntry('ORIGIN', 'wrfits'),
        FitsHeaderEntry('FREQ', 150.9),
        FitsHeaderEntry('FREQUNIT', 6),
        FitsHeaderEntry('BSCALE', 1.0),
        FitsHeaderEntry('BZERO', 0.0),
        FitsHeaderEntry('BUNIT', 'K'),
        FitsHeaderEntry('EXPTIME', 1168576512),
        FitsHeaderEntry('CTYPE1', 'Solar-X'),
        FitsHeaderEntry('CTYPE2', 'Solar-Y'),
        FitsHeaderEntry('CTYPE3', 'StokesI'),
        FitsHeaderEntry('CRPIX1', 128),
        FitsHeaderEntry('CRPIX2', 128),
        FitsHeaderEntry('CDELT1', 0.015625),
        FitsHeaderEntry('CDELT2', 0.015625),
        FitsHeaderEntry('SOLAR_R', 64.0),
        FitsHeaderEntry('COMMENT', ''),
        FitsHeaderEntry('HISTORY', '')]
    assert entry.fits_key_comments == [
        FitsKeyComment('WAVEUNIT', 'in meters'),
        FitsKeyComment('NAXIS2', 'number of rows'),
        FitsKeyComment('CDELT2', 'pixel scale y, in solar radius/pixel'),
        FitsKeyComment('CRPIX1', 'SUN CENTER X, pixels'),
        FitsKeyComment('CRPIX2', 'SUN CENTER Y, pixels'),
        FitsKeyComment('SOLAR_R', 'SOLAR RADIUS, pixels'),
        FitsKeyComment('NAXIS1', 'number of columns'),
        FitsKeyComment('CDELT1', 'pixel scale x, in solar radius/pixel'),
        FitsKeyComment('NAXIS3', 'StokesI'),
        FitsKeyComment('TELESCOP', 'Nancay Radioheliograph'),
        FitsKeyComment('INSTRUME', 'Nancay 2D-images Radioheliograph'),
        FitsKeyComment('BUNIT', 'Brightness temperature'),
        FitsKeyComment('BITPIX', 'IEEE 32-bit floating point values'),
        FitsKeyComment('DATE', 'Date of file creation'),
        FitsKeyComment('FREQUNIT', 'in MHz'),
        FitsKeyComment('EXPTIME', 'in seconds')]


def test_entries_from_dir_recursively_true():
    entries = list(
        entries_from_dir(testdir, True, default_waveunit='angstrom'))
    assert len(entries) == 28


def test_entries_from_dir_recursively_false():
    entries = list(
        entries_from_dir(testdir, False, default_waveunit='angstrom'))
    assert len(entries) == 11


@pytest.mark.online
def test_entries_from_query_result(query_result):
    entries = list(entries_from_query_result(query_result))
    assert len(entries) == 122
    snd_entry = entries[1]
    expected_entry = DatabaseEntry(
        source='SOHO', provider='SDAC', physobs='intensity',
        fileid='/archive/soho/private/data/processed/eit/lz/2001/01/efz20010101.070014',
        observation_time_start=datetime(2001, 1, 1, 7, 0, 14),
        observation_time_end=datetime(2001, 1, 1, 7, 0, 21),
        instrument='EIT', size=2059.0, wavemin=17.1, wavemax=17.1)
    assert snd_entry == expected_entry


@pytest.mark.online
def test_entry_from_query_results_with_none_wave(qr_with_none_waves):
    # does not raise WaveunitNotFoundError because neither wavemin nor wavemax
    # are given
    list(entries_from_query_result(qr_with_none_waves))


@pytest.mark.online
def test_entry_from_query_results_with_none_wave_and_default_unit(
        qr_with_none_waves):
    entries = list(entries_from_query_result(qr_with_none_waves, 'nm'))
    assert len(entries) == 4
    assert entries == [
        DatabaseEntry(
            source='SOHO', provider='SDAC', physobs='intensity',
            fileid='/archive/soho/private/data/processed/virgo/level1/1212/HK/121222_1.H01',
            observation_time_start=datetime(2012, 12, 23, 23, 59, 3),
            observation_time_end=datetime(2012, 12, 24, 23, 59, 2),
            instrument='VIRGO', size=155.0, wavemin=None,
            wavemax=None),
        DatabaseEntry(
            source='SOHO', provider='SDAC', physobs='intensity',
            fileid='/archive/soho/private/data/processed/virgo/level1/1212/LOI/121224_1.L01',
            observation_time_end=datetime(2012, 12, 24, 23, 59, 2),
            observation_time_start=datetime(2012, 12, 23, 23, 59, 3),
            instrument='VIRGO', size=329.0, wavemin=None,
            wavemax=None),
        DatabaseEntry(
            source='SOHO', provider='SDAC', physobs ='intensity',
            fileid='/archive/soho/private/data/processed/virgo/level1/1212/SPM/121222_1.S02',
            observation_time_start=datetime(2012, 12, 23, 23, 59, 3),
            observation_time_end=datetime(2012, 12, 24, 23, 59, 2),
            instrument='VIRGO', size=87.0, wavemin=None,
            wavemax=None),
        DatabaseEntry(
            source='SOHO', provider='SDAC', physobs='intensity',
            fileid='/archive/soho/private/data/processed/virgo/level1/1212/DIARAD/121222_1.D01',
            observation_time_start=datetime(2012, 12, 24, 0, 1, 58),
            observation_time_end=datetime(2012, 12, 25, 0, 1, 57),
            instrument='VIRGO', size=14.0, wavemin=None,
            wavemax=None)]


def test_display_entries_missing_entries():
    with pytest.raises(TypeError):
        display_entries([], ['some', 'columns'])


def test_display_entries_empty_db():
    with pytest.raises(TypeError):
        display_entries(Database('sqlite:///'), ['id'])


def test_display_entries_missing_columns():
    with pytest.raises(TypeError):
        display_entries([DatabaseEntry()], [])


def test_display_entries():
    entries = [
        DatabaseEntry(
            id=1, source='SOHO', provider='SDAC', physobs='intensity',
            fileid='/archive/soho/...',
            observation_time_start=datetime(2001, 1, 1, 7, 0, 14),
            observation_time_end=datetime(2001, 1, 1, 7, 0, 21),
            instrument='EIT', size=259.0, wavemin=171.0,
            wavemax=171.0, tags=[Tag('foo'), Tag('bar')]),
        DatabaseEntry(
            id=2, source='GONG', provider='NSO', physobs='LOS_velocity',
            fileid='pptid=11010...',
            observation_time_start=datetime(2010, 1, 1, 0, 59),
            observation_time_end=datetime(2010, 1, 1, 1),
            download_time=datetime(2014, 6, 15, 3, 42, 55, 123456),
            instrument='Merged gong', size=944.0,
            wavemin=6768.0, wavemax=6768.0, starred=True)]
    columns = [
        'id', 'source', 'provider', 'physobs', 'fileid', 'download_time',
        'observation_time_start', 'instrument', 'size',
        'wavemin', 'path', 'starred', 'tags']
    table = display_entries(entries, columns)
    filedir = os.path.dirname(os.path.realpath(__file__))
    with open(os.path.join(filedir,'test_table.txt'), 'r') as f:
        stored_table = f.read()
    assert table.strip() == stored_table.strip()

########NEW FILE########
__FILENAME__ = coalignment
"""
This module provides routines for the coalignment of images and mapcubes.

Currently this module provides image coalignment by template matching. 
Which is partially inspired by the SSWIDL routine 
`tr_get_disp.pro <http://hesperia.gsfc.nasa.gov/ssw/trace/idl/util/routines/tr_get_disp.pro>`_.

In this implementation, the template matching is handled via 
the scikit-image routine :func:`skimage.feature.match_template`.

References
----------
Template matching algorithm: 

 * http://scribblethink.org/Work/nvisionInterface/nip.html
 * J.P. Lewis, Fast Template Matching, Vision Interface 95, Canadian Image
   Processing and Pattern Recognition Society, Quebec City, Canada, May 15-19,
   1995, p. 120-123 http://www.scribblethink.org/Work/nvisionInterface/vi95_lewis.pdf.
"""

import numpy as np
from scipy.ndimage.interpolation import shift
from copy import deepcopy

# Image co-registration by matching templates
from skimage.feature import match_template

# SunPy imports
from sunpy.map.mapbase import GenericMap

__author__ = 'J. Ireland'

__all__ = ['calculate_shift', 'clip_edges', 'calculate_clipping',
           'match_template_to_layer', 'find_best_match_location',
           'get_correlation_shifts', 'parabolic_turning_point',
           'repair_image_nonfinite', 'mapcube_coalign_by_match_template']


def _default_fmap_function(data):
    """This function ensures that the data are floats.  It is the default data
    manipulation function for the coalignment method.
    """
    return np.float64(data)


def calculate_shift(this_layer, template):
    """Calculates the pixel shift required to put the template in the "best"
    position on a layer.

    Parameters
    ----------
    this_layer : ndarray
        A numpy array of size (ny, nx), where the first two dimensions are
        spatial dimensions.

    template : ndarray
        A numpy array of size (N, M) where N < ny and M < nx.

    Returns
    -------
    shifts : tuple
        Pixel shifts (yshift, xshift) relative to the offset of the template
        to the input array.
    """
    # Repair any NANs, Infs, etc in the layer and the template
    this_layer = repair_image_nonfinite(this_layer)
    template = repair_image_nonfinite(template)

    # Calculate the correlation array matching the template to this layer
    corr = match_template_to_layer(this_layer, template)

    # Calculate the y and x shifts in pixels
    return find_best_match_location(corr)


#
# Remove the edges of a datacube
#
def clip_edges(data, yclips, xclips):
    """Clips off the y and x edges of a 2d array according to a list of pixel
    values.  This function is useful for removing data at the edge of
    2d images that may be affected by shifts from solar de-rotation and
    layer co-registration, leaving an image unaffected by edge effects.

    Parameters
    ----------
    data : ndarray
        A numpy array of shape (ny, nx).

    yclips : ndarray
        The amount to clip in the y-direction of the data.

    xclips : ndarray
        The amount to clip in the x-direction of the data.

    Returns
    -------
    image : ndarray
        A 2d image with edges clipped off according to the positive and
        negative ceiling values in the yclips and xclips arrays.
    """
    # Datacube shape
    ny = data.shape[0]
    nx = data.shape[1]
    return data[yclips[0]: ny - yclips[1], xclips[0]: nx - xclips[1]]


#
# Return the upper and lower clipping values for the y and x directions an
# input set of pixel shifts y and x
#
def calculate_clipping(y, x):
    """Return the upper and lower clipping values for the y and x directions.

    Parameters
    ----------
    y : ndarray
        An array of pixel shifts in the y-direction for an image.

    x : ndarray
        An array of pixel shifts in the x-direction for an image.

    Returns
    -------
    clipping : ([int, int], [int, int])
        The number of (integer) pixels that need to be clipped off at each
        edge in an image. The first element in the tuple is a list that gives
        the number of pixels to clip in the y-direction.  The first element in
        that list is the number of rows to clip at the lower edge of the image
        in y.  The clipped image has "clipping[0][0]" rows removed from its
        lower edge when compared to the original image.  The second element in
        that list is the number of rows to clip at the upper edge of the image
        in y.  The clipped image has "clipping[0][1]" rows removed from its
        upper edge when compared to the original image.  The second element in
        the "clipping" tuple applies similarly to the x-direction (image
        columns).
    """
    return [_lower_clip(y), _upper_clip(y)], [_lower_clip(x), _upper_clip(x)]


#
# Helper functions for clipping edges
#
def _upper_clip(z):
    """Find smallest integer bigger than all the positive entries in the input
    array.
    """
    zupper = 0
    zcond = z >= 0
    if np.any(zcond):
        zupper = np.max(np.ceil(z[zcond]))
    return zupper


def _lower_clip(z):
    """Find smallest positive integer bigger than the absolute values of the
    negative entries in the input array.
    """
    zlower = 0
    zcond = z <= 0
    if np.any(zcond):
        zlower = np.max(np.ceil(-z[zcond]))
    return zlower


def match_template_to_layer(layer, template):
    """Calculate the correlation array that describes how well the template
    matches the layer. All inputs are assumed to be numpy arrays.  This
    function requires the "match_template" function in scikit image.

    Parameters
    ----------
    layer : ndarray
        A numpy array of size (ny, nx).

    template : ndarray
        A numpy array of size (N, M) where N < ny and M < nx.

    Returns
    -------
    correlationarray : ndarray
        A correlation array between the layer and the template.
        The values in the array range between 0 and 1.
    """
    return match_template(layer, template)


def find_best_match_location(corr):
    """Calculate an estimate of the location of the peak of the correlation
    result in image pixels.

    Parameters
    ----------
    corr : ndarray
        A 2-d correlation array.

    Returns
    -------
    shift : tuple
        The shift amounts (y, x) in image pixels.  Subpixel values are
        possible.
    """
    # Get the index of the maximum in the correlation function
    ij = np.unravel_index(np.argmax(corr), corr.shape)
    cor_max_x, cor_max_y = ij[::-1]

    # Get the correlation function around the maximum
    array_around_maximum = corr[np.max([0, cor_max_y - 1]): np.min([cor_max_y + 2, corr.shape[0] - 1]), 
                                  np.max([0, cor_max_x - 1]): np.min([cor_max_x + 2, corr.shape[1] - 1])]
    y_shift_relative_to_maximum, x_shift_relative_to_maximum = \
    get_correlation_shifts(array_around_maximum)

    # Get shift relative to correlation array
    y_shift_relative_to_correlation_array = y_shift_relative_to_maximum + cor_max_y
    x_shift_relative_to_correlation_array = x_shift_relative_to_maximum + cor_max_x

    return y_shift_relative_to_correlation_array, x_shift_relative_to_correlation_array


def get_correlation_shifts(array):
    """Estimate the location of the maximum of a fit to the input array.  The
    estimation in the x and y directions are done separately. The location
    estimates can be used to implement subpixel shifts between two different
    images.

    Parameters
    ----------
    array : ndarray
        An array with at least one dimension that has three elements.  The
        input array is at most a 3 x 3 array of correlation values calculated
        by matching a template to an image.

    Returns
    -------
    peakloc : tuple
        The (y, x) location of the peak of a parabolic fit, in image pixels.
    """
    # Check input shape
    ny = array.shape[0]
    nx = array.shape[1]
    if nx > 3 or ny > 3:
        print 'Input array is too big in at least one dimension. Returning Nones'
        return None, None

    # Find where the maximum of the input array is
    ij = np.unravel_index(np.argmax(array), array.shape)
    x_max_location, y_max_location = ij[::-1]

    # Estimate the location of the parabolic peak if there is enough data.
    # Otherwise, just return the location of the maximum in a particular
    # direction.
    if ny == 3:
        y_location = parabolic_turning_point(array[:, x_max_location])
    else:
        y_location = 1.0 * y_max_location

    if nx == 3:
        x_location = parabolic_turning_point(array[y_max_location, :])
    else:
        x_location = 1.0 * x_max_location

    return y_location, x_location


def parabolic_turning_point(y):
    """Find the location of the turning point for a parabola
    y(x) = ax^2 + bx + c, given input values y(-1), y(0), y(1).
    The maximum is located at x0 = -b / 2a .  Assumes
    that the input array represents an equally spaced sampling at the
    locations y(-1), y(0) and y(1).

    Parameters
    ----------
    y : ndarray
        A one dimensional numpy array of shape 3 with entries that sample the
        parabola at -1, 0, and 1.

    Returns
    -------
    location : float
        A digit, the location of the parabola maximum.
    """
    numerator = -0.5 * y.dot([-1, 0, 1])
    denominator = y.dot([1, -2, 1])
    return numerator / denominator


def repair_image_nonfinite(image):
    """Return a new image in which all the nonfinite entries of the original
    image have been replaced by the local mean.

    Parameters
    ----------
    image : ndarray
        A two-dimensional ndarray.

    Returns
    -------
    repaired_image : ndarray
        A two-dimensional ndarray of the same shape as the input that has all
        the non-finite entries replaced by a local mean.  The algorithm
        repairs one non-finite entry at every pass.  At each pass, the next
        non-finite value is replaced by the mean of its finite valued nearest
        neighbours.
    """
    repaired_image = deepcopy(image)
    nx = repaired_image.shape[1]
    ny = repaired_image.shape[0]
    bad_index = np.where(np.logical_not(np.isfinite(repaired_image)))
    while bad_index[0].size != 0:
        by = bad_index[0][0]
        bx = bad_index[1][0]

        # x locations taking in to account the boundary
        x = bx
        if bx == 0:
            x = 1
        if bx == nx - 1:
            x = nx - 2

        # y locations taking in to account the boundary
        y = by
        if by == 0:
            y = 1
        if by == ny - 1:
            y = ny - 2

        # Get the sub array around the bad index, and find the local mean
        # ignoring nans
        subarray = repaired_image[y - 1: y + 2, x - 1: x + 2]
        repaired_image[by, bx] = np.mean(subarray[np.isfinite(subarray)])
        bad_index = np.where(np.logical_not(np.isfinite(repaired_image)))
    return repaired_image


# Coalignment by matching a template
def mapcube_coalign_by_match_template(mc, template=None, layer_index=0,
                               func=_default_fmap_function, clip=True,
                               return_displacements_only=False,
                               apply_displacements=None,
                               with_displacements=False):
    """Co-register the layers in a mapcube according to a template taken from
    that mapcube.  This method REQUIRES that scikit-image be installed.
    When using this functionality, it is a good idea to check that the
    shifts that were applied to were reasonable and expected.  One way of
    checking this is to animate the original mapcube, animate the coaligned
    mapcube, and compare the differences you see to the calculated shifts.


    Parameters
    ----------
    mc : sunpy.map.MapCube
        A mapcube of shape (ny, nx, nt), where nt is the number of layers in
        the mapcube.

    template : {None | sunpy.map.Map | ndarray}
        The template used in the matching.  If an ndarray is passed, the
        ndarray has to have two dimensions.

    layer_index : int
        The template is assumed to refer to the map in the mapcube indexed by
        the value of "layer_index".  Displacements of all maps in the mapcube
        are assumed to be relative to this layer.  The displacements of the
        template relative to this layer are therefore (0, 0).

    func : function
        A function which is applied to the data values before the coalignment
        method is applied.  This can be useful in coalignment, because it is
        sometimes better to co-align on a function of the data rather than the
        data itself.  The calculated shifts are applied to the original data.
        Examples of useful functions to consider for EUV images are the
        logarithm or the square root.  The function is of the form
        func = F(data).  The default function ensures that the data are
        floats.

    clip : bool
        If True, thenclip off x, y edges in the datacube that are potentially
        affected by edges effects.

    return_displacements_only : bool
        If True return ONLY the x and y displacements applied to the input
        data in units of arcseconds.  The return value is a dictionary of the
        form {"x": xdisplacement, "y": ydisplacement}.

    apply_displacements : {None | dict}
        If not None, then use the displacements supplied by the user.  Must be
        in the same format as that returned using the
        return_displacements_only option.  Can be used when you want to appl
        the same displacements to multiple mapcubes.

    with_displacements : bool
        If True, return the x and y displacements applied to the input data in
        the same format as that returned using the return_displacements_only
        option, along with the coaligned mapcube.  The format of the return is
        (mapcube, displacements).

    Returns
    -------
    output : {sunpy.map.MapCube | dict | tuple}
        The results of the mapcube coalignment.  The output depends on the
        value of the parameters "return_displacements_only" and
        "with_displacements".

    Examples
    --------
    >>> import numpy as np
    >>> from sunpy.image.coalignment import mapcube_coalign_by_match_template as mc_coalign
    >>> coaligned_mc = mc_coalign(mc)
    >>> coaligned_mc = mc_coalign(mc, layer_index=-1)
    >>> coaligned_mc = mc_coalign(mc, clip=False)
    >>> coaligned_mc = mc_coalign(mc, template=sunpy_map)
    >>> coaligned_mc = mc_coalign(mc, template=two_dimensional_ndarray)
    >>> coaligned_mc = mc_coalign(mc, func=np.log)
    >>> displacements = mc_coalign(mc, return_displacements_only=True)
    >>> coaligned_mc, displacements = mc_coalign(mc, with_displacements=True)
    >>> coaligned_mc = mc_coalign(mc, apply_displacements=displacements)
    """
    # Size of the data
    ny = mc.maps[layer_index].shape[0]
    nx = mc.maps[layer_index].shape[1]
    nt = len(mc.maps)

    # Storage for the pixel shifts and the shifts in arcseconds
    xshift_keep = np.zeros((nt))
    yshift_keep = np.zeros_like(xshift_keep)

    # Use the displacements supplied
    if apply_displacements is not None:
        xshift_arcseconds = apply_displacements["x"]
        yshift_arcseconds = apply_displacements["y"]
        for i, m in enumerate(mc.maps):
            xshift_keep[i] = xshift_arcseconds[i] / m.scale['x']
            yshift_keep[i] = yshift_arcseconds[i] / m.scale['y']
    else:
        xshift_arcseconds = np.zeros_like(xshift_keep)
        yshift_arcseconds = np.zeros_like(xshift_keep)

        # Calculate a template.  If no template is passed then define one
        # from the the index layer.
        if template is None:
            tplate = mc.maps[layer_index].data[ny / 4: 3 * ny / 4,
                                             nx / 4: 3 * nx / 4]
        elif isinstance(template, GenericMap):
            tplate = template.data
        elif isinstance(template, np.ndarray):
            tplate = template
        else:
            raise ValueError('Invalid template.')

        # Apply the function to the template
        tplate = func(tplate)

        # Match the template and calculate shifts
        for i, m in enumerate(mc.maps):
            # Get the next 2-d data array
            this_layer = func(m.data)

            # Calculate the y and x shifts in pixels
            yshift, xshift = calculate_shift(this_layer, tplate)

            # Keep shifts in pixels
            yshift_keep[i] = yshift
            xshift_keep[i] = xshift

        # Calculate shifts relative to the template layer
        yshift_keep = yshift_keep - yshift_keep[layer_index]
        xshift_keep = xshift_keep - xshift_keep[layer_index]

        for i, m in enumerate(mc.maps):
            # Calculate the shifts required in physical units, which are
            # presumed to be arcseconds.
            xshift_arcseconds[i] = xshift_keep[i] * m.scale['x']
            yshift_arcseconds[i] = yshift_keep[i] * m.scale['y']

    # Return only the displacements
    if return_displacements_only:
        return {"x": xshift_arcseconds, "y": yshift_arcseconds}

    # New mapcube for the new data
    newmc = deepcopy(mc)

    # Shift the data and construct the mapcube
    for i, m in enumerate(newmc.maps):
        shifted_data = shift(m.data, [-yshift_keep[i], -xshift_keep[i]])
        if clip:
            yclips, xclips = calculate_clipping(yshift_keep, xshift_keep)
            shifted_data = clip_edges(shifted_data, yclips, xclips)

        # Update the mapcube image data
        newmc.maps[i].data = shifted_data

        # Adjust the positioning information accordingly.
        newmc.maps[i].meta['crpix1'] = newmc.maps[i].meta['crpix1'] + xshift_arcseconds[i]
        newmc.maps[i].meta['crpix2'] = newmc.maps[i].meta['crpix2'] + yshift_arcseconds[i]

    # Return the mapcube, or optionally, the mapcube and the displacements
    # used to create the mapcube.
    if with_displacements:
        return newmc, {"x": xshift_arcseconds, "y": yshift_arcseconds}
    else:
        return newmc

########NEW FILE########
__FILENAME__ = rescale
"""Image resampling methods"""
from __future__ import absolute_import

import numpy as np
import scipy.interpolate
import scipy.ndimage

__all__ = ['resample', 'reshape_image_to_4d_superpixel']

def resample(orig, dimensions, method='linear', center=False, minusone=False):
    """Returns a new ndarray that has been resampled up or down
    
    Arbitrary resampling of source array to new dimension sizes.
    Currently only supports maintaining the same number of dimensions.
    To use 1-D arrays, first promote them to shape (x,1).
    
    Uses the same parameters and creates the same co-ordinate lookup points
    as IDL''s congrid routine, which apparently originally came from a 
    VAX/VMS routine of the same name.
    
    Parameters
    ----------
    dimensions : tuple
        Dimensions that new ndarray should have.
    method : {'neighbor' | 'nearest' | 'linear' | 'spline'}
        Method to use for resampling interpolation.
            * neighbor - Closest value from original data
            * nearest and linear - Uses n x 1-D interpolations using
              scipy.interpolate.interp1d
            * spline - Uses ndimage.map_coordinates
    center : bool
        If True, interpolation points are at the centers of the bins,
        otherwise points are at the front edge of the bin.
    minusone : bool
        For inarray.shape = (i,j) & new dimensions = (x,y), if set to False
        inarray is resampled by factors of (i/x) * (j/y), otherwise inarray 
        is resampled by(i-1)/(x-1) * (j-1)/(y-1)
        This prevents extrapolation one element beyond bounds of input 
        array.

    Returns
    -------
    out : ndarray
        A new ndarray which has been resampled to the desired dimensions.
    
    References
    ----------
    | http://www.scipy.org/Cookbook/Rebinning (Original source, 2011/11/19)
    """

    # Verify that number dimensions requested matches original shape
    if len(dimensions) != orig.ndim:
        raise UnequalNumDimensions("Number of dimensions must remain the same "
                                   "when calling resample.")

    #@note: will this be okay for integer (e.g. JPEG 2000) data?
    if not orig.dtype in [np.float64, np.float32]:
        orig = orig.astype(np.float64)

    dimensions = np.asarray(dimensions, dtype=np.float64)
    m1 = np.array(minusone, dtype=np.int64) # array(0) or array(1)
    offset = np.float64(center * 0.5)       # float64(0.) or float64(0.5)

    # Resample data
    if method == 'neighbor':
        data = _resample_neighbor(orig, dimensions, offset, m1)
    elif method in ['nearest','linear']:
        data = _resample_nearest_linear(orig, dimensions, method, 
                                             offset, m1)
    elif method == 'spline':
        data = _resample_spline(orig, dimensions, offset, m1)
    else:
        raise UnrecognizedInterpolationMethod("Unrecognized interpolation "
                                              "method requested.")
    
    return data
    
def _resample_nearest_linear(orig, dimensions, method, offset, m1):
    """Resample Map using either linear or nearest interpolation"""

    dimlist = []
    
    # calculate new dims
    for i in range(orig.ndim):
        base = np.arange(dimensions[i])
        dimlist.append((orig.shape[i] - m1) / (dimensions[i] - m1) *
                       (base + offset) - offset)

    # specify old coordinates
    old_coords = [np.arange(i, dtype=np.float) for i in orig.shape]

    # first interpolation - for ndims = any
    mint = scipy.interpolate.interp1d(old_coords[-1], orig, bounds_error=False,
                                      fill_value=min(old_coords[-1]), kind=method)

    new_data = mint(dimlist[-1])

    trorder = [orig.ndim - 1] + range(orig.ndim - 1)
    for i in xrange(orig.ndim - 2, -1, -1):
        new_data = new_data.transpose(trorder)

        mint = scipy.interpolate.interp1d(old_coords[i], new_data,
            bounds_error=False, fill_value=min(old_coords[i]), kind=method)
        new_data = mint(dimlist[i])

    if orig.ndim > 1:
        # need one more transpose to return to original dimensions
        new_data = new_data.transpose(trorder)

    return new_data

def _resample_neighbor(orig, dimensions, offset, m1):
    """Resample Map using closest-value interpolation"""
    
    dimlist = []
    
    for i in xrange(orig.ndim):
        base = np.indices(dimensions)[i]
        dimlist.append((orig.shape[i] - m1) / (dimensions[i] - m1) *
                       (base + offset) - offset)
    cd = np.array(dimlist).round().astype(int)
    
    return orig[list(cd)]

def _resample_spline(orig, dimensions, offset, m1):
    """Resample Map using spline-based interpolation"""
    
    oslices = [slice(0, j) for j in orig.shape]
    old_coords = np.ogrid[oslices] #pylint: disable=W0612
    nslices = [slice(0, j) for j in list(dimensions)]
    newcoords = np.mgrid[nslices]

    newcoords_dims = range(np.rank(newcoords))
    
    #make first index last
    newcoords_dims.append(newcoords_dims.pop(0))
    newcoords_tr = newcoords.transpose(newcoords_dims) #pylint: disable=W0612

    # makes a view that affects newcoords
    newcoords_tr += offset

    deltas = (np.asarray(orig.shape) - m1) / (dimensions - m1)
    newcoords_tr *= deltas

    newcoords_tr -= offset

    return scipy.ndimage.map_coordinates(orig, newcoords)

def reshape_image_to_4d_superpixel(img,dimensions):
    """Re-shape the two dimension input image into a a four dimensional
    array whose 1st and third dimensions express the number of original
    pixels in the x and y directions form one superpixel. The reshaping
    makes it very easy to perform operations on super-pixels.  Taken from
    http://mail.scipy.org/pipermail/numpy-discussion/2010-July/051760.html
    """
    # check that the dimensions divide into the image size exactly
    if img.shape[1] % dimensions[0] != 0:
        print('Sum value in x direction must divide exactly into image'
              ' x-dimension size.')
        return None
    if img.shape[0] % dimensions[1] != 0:
        print('Sum value in y direction must divide exactly into image'
              ' x-dimension size.')
        return None
   
    # Reshape up to a higher dimensional array which is useful for higher
    # level operations
    return img.reshape(img.shape[1] / dimensions[0],
                       dimensions[0],
                       img.shape[0] / dimensions[1],
                       dimensions[1])
    
class UnrecognizedInterpolationMethod(ValueError):
    """Unrecognized interpolation method specified."""
    pass

class UnequalNumDimensions(ValueError):
    """Number of dimensions does not match input array"""
    pass

########NEW FILE########
__FILENAME__ = test_coalignment
# Author: Jack Ireland
#
# Testing functions for a mapcube coalignment functionality.  This
# functionality relies on the scikit-image function "match_template".
#

import numpy as np
from numpy.testing import assert_allclose
from scipy.ndimage.interpolation import shift
from sunpy import AIA_171_IMAGE
from sunpy import map
from sunpy.image.coalignment import parabolic_turning_point, \
repair_image_nonfinite, _default_fmap_function, _lower_clip, _upper_clip, \
calculate_clipping, get_correlation_shifts, find_best_match_location, \
match_template_to_layer, clip_edges, calculate_shift, \
mapcube_coalign_by_match_template

# Map and template we will use in testing
testmap = map.Map(AIA_171_IMAGE)
test_layer = testmap.data
ny = test_layer.shape[0]
nx = test_layer.shape[1]
test_template = test_layer[1 + ny / 4 : 1 + 3 * ny / 4,
                            2 + nx / 4 : 2 + 3 * nx / 4]

# Used in testing the clipping
clip_test_array = np.asarray([0.2, -0.3, -1.0001])


def test_parabolic_turning_point():
    assert(parabolic_turning_point(np.asarray([6.0, 2.0, 0.0])) == 1.5)


def test_repair_image_nonfinite():
    for i in range(0, 9):
        for non_number in [np.nan, np.inf]:
            a = np.ones((9))
            a[i] = non_number
            b = a.reshape(3, 3)
            c = repair_image_nonfinite(b)
            assert(np.isfinite(c).all())


def test_match_template_to_layer():
    result = match_template_to_layer(test_layer, test_template)
    assert(result.shape[0] == 513)
    assert(result.shape[1] == 513)
    assert_allclose(np.max(result), 1.00, rtol=1e-2, atol=0)


def test_get_correlation_shifts():
    # Input array is 3 by 3, the most common case
    test_array = np.zeros((3, 3))
    test_array[1, 1] = 1
    test_array[2, 1] = 0.6
    test_array[1, 2] = 0.2
    y_test, x_test = get_correlation_shifts(test_array)
    assert_allclose(y_test, 0.214285714286, rtol=1e-2, atol=0)
    assert_allclose(x_test, 0.0555555555556, rtol=1e-2, atol=0)

    # Input array is smaller in one direction than the other.
    test_array = np.zeros((2, 2))
    test_array[0, 0] = 0.1
    test_array[0, 1] = 0.2
    test_array[1, 0] = 0.4
    test_array[1, 1] = 0.3
    y_test, x_test = get_correlation_shifts(test_array)
    assert_allclose(y_test, 1.0, rtol=1e-2, atol=0)
    assert_allclose(x_test, 0.0, rtol=1e-2, atol=0)

    # Input array is too big in either direction
    test_array = np.zeros((4, 3))
    y_test, x_test = get_correlation_shifts(test_array)
    assert(y_test == None)
    assert(x_test == None)
    test_array = np.zeros((3, 4))
    y_test, x_test = get_correlation_shifts(test_array)
    assert(y_test == None)
    assert(x_test == None)

def test_find_best_match_location():
    result = match_template_to_layer(test_layer, test_template)
    y_test, x_test = find_best_match_location(result)
    assert_allclose(y_test, 257.0, rtol=1e-3, atol=0)
    assert_allclose(x_test, 258.0, rtol=1e-3, atol=0)

def test_lower_clip():
    assert(_lower_clip(clip_test_array) == 2.0)
    # No element is less than zero
    test_array = np.asarray([1.1, 0.1, 3.0])
    assert(_lower_clip(test_array) == 0)


def test_upper_clip():
    assert(_upper_clip(clip_test_array) == 1.0)
    # No element is greater than zero
    test_array = np.asarray([-1.1, -0.1, -3.0])
    assert(_upper_clip(test_array) == 0)


def test_calculate_clipping():
    answer = calculate_clipping(clip_test_array, clip_test_array)
    assert(answer == ([2.0, 1.0], [2.0, 1.0]))


def test_clip_edges():
    a = np.zeros(shape=(341, 156))
    yclip = [4, 0]
    xclip = [1, 2]
    new_a = clip_edges(a, yclip, xclip)
    assert(a.shape[0] - (yclip[0] + yclip[1]) == 337)
    assert(a.shape[1] - (xclip[0] + xclip[1]) == 153)


def test_calculate_shift():
    result = calculate_shift(test_layer, test_template)
    assert_allclose(result[0], 257.0,  rtol=1e-3, atol=0)
    assert_allclose(result[1], 258.0,  rtol=1e-3, atol=0)


def test__default_fmap_function():
    assert(_default_fmap_function([1,2,3]).dtype == np.float64(1).dtype)


def test_mapcube_coalign_by_match_template():
    # take the AIA image and shift it
    # Pixel displacements have the y-displacement as the first entry
    pixel_displacements = np.asarray([1.6, 10.1])
    known_displacements = {'x':np.asarray([0.0, pixel_displacements[1] * testmap.scale['x']]), 'y':np.asarray([0.0, pixel_displacements[0] * testmap.scale['y']])}

    # Create a map that has been shifted a known amount. 
    d1 = shift(testmap.data, pixel_displacements)
    m1 = map.Map((d1, testmap.meta))

    # Create the mapcube
    mc = map.Map([testmap, m1], cube=True)

    # Test to see if the code can recover the displacements. Do the coalignment
    # using the "return_displacements_only" option
    test_displacements = mapcube_coalign_by_match_template(mc, return_displacements_only=True)
    # Assert
    assert_allclose(test_displacements['x'], known_displacements['x'], rtol=5e-2, atol=0)
    assert_allclose(test_displacements['y'], known_displacements['y'], rtol=5e-2, atol=0 )
    
    # Test setting the template as a ndarray
    template_ndarray = testmap.data[ny / 4: 3 * ny / 4, nx / 4: 3 * nx / 4]
    test_displacements = mapcube_coalign_by_match_template(mc, template=template_ndarray, return_displacements_only=True)
    # Assert
    assert_allclose(test_displacements['x'], known_displacements['x'], rtol=5e-2, atol=0)
    assert_allclose(test_displacements['y'], known_displacements['y'], rtol=5e-2, atol=0 )

    # Test setting the template as GenericMap
    submap = testmap.submap([nx / 4, 3 * nx / 4], [ny / 4, 3 * ny / 4], units='pixels')
    test_displacements = mapcube_coalign_by_match_template(mc, template=submap, return_displacements_only=True)
    # Assert
    assert_allclose(test_displacements['x'], known_displacements['x'], rtol=5e-2, atol=0)
    assert_allclose(test_displacements['y'], known_displacements['y'], rtol=5e-2, atol=0 )

    # Test setting the template as something other than a ndarray and a
    # GenericMap.  This should throw a ValueError.
    try:
        test_displacements = mapcube_coalign_by_match_template(mc, template='broken')
    except ValueError:
        pass

    # Test passing in displacements
    test_apply_displacements = {'x':-test_displacements['x'], 'y':-test_displacements['y']}
    test_displacements = mapcube_coalign_by_match_template(mc,
                                                           apply_displacements=test_apply_displacements,
                                                           return_displacements_only=True)
    assert_allclose(test_displacements['x'], test_apply_displacements['x'], rtol=5e-2, atol=0)
    assert_allclose(test_displacements['y'], test_apply_displacements['y'], rtol=5e-2, atol=0)

    # Test returning using the "with_displacements" option
    test_output = mapcube_coalign_by_match_template(mc, with_displacements=True)
    # Assert
    assert(isinstance(test_output[0], map.MapCube))
    assert_allclose(test_output[1]['x'], known_displacements['x'], rtol=5e-2, atol=0)
    assert_allclose(test_output[1]['y'], known_displacements['y'], rtol=5e-2, atol=0 )

    # Test returning with no extra options - the code returns a mapcube only
    test_output = mapcube_coalign_by_match_template(mc)
    assert(isinstance(test_output, map.MapCube))

    # Test returning with no clipping.  Output layers should have the same size
    # as the original input layer.
    test_mc = mapcube_coalign_by_match_template(mc, clip=False)
    assert(test_mc[0].data.shape == testmap.data.shape)
    assert(test_mc[1].data.shape == testmap.data.shape)

########NEW FILE########
__FILENAME__ = test_resample
# Author: Tomas Meszaros <exo@tty.sk>

from sunpy import AIA_171_IMAGE
from sunpy import map
from sunpy.image.rescale import reshape_image_to_4d_superpixel


AIA_MAP = map.Map(AIA_171_IMAGE)

def resample_meta(dimensions, method, center, minusone):
    map_resampled = AIA_MAP.resample(dimensions)
    return map_resampled.shape

def resample_method(method):
    assert resample_meta((512, 512), method, False, False) == (512, 512)
    assert resample_meta((2056, 2056), method, False, False) == (2056, 2056)
    assert resample_meta((512, 512), method, False, True) == (512, 512)
    assert resample_meta((2056, 2056), method, False, True) == (2056, 2056)
    assert resample_meta((512, 512), method, True, False) == (512, 512)
    assert resample_meta((2056, 2056), method, True, False) == (2056, 2056)
    assert resample_meta((512, 512), method, True, True) == (512, 512)
    assert resample_meta((2056, 2056), method, True, True) == (2056, 2056)

def test_resample_neighbor():
    resample_method('neighbor')

def test_resample_nearest():
    resample_method('nearest')

def test_resample_linear():
    resample_method('linear')

def test_resample_spline():
    resample_method('spline')

def test_reshape():
    assert reshape_image_to_4d_superpixel(AIA_MAP.data, (512, 512)) is not None
    assert reshape_image_to_4d_superpixel(AIA_MAP.data, (600, 512)) is None
    assert reshape_image_to_4d_superpixel(AIA_MAP.data, (512, 600)) is None

########NEW FILE########
__FILENAME__ = goes
from __future__ import absolute_import

from sunpy.net import hek
from sunpy.time import parse_time

__all__ = ['get_goes_event_list']

def get_goes_event_list(trange,goes_class_filter=None):
    """A function to retrieve a list of flares detected by GOES within a given time range.

    Parameters
    ----------
    trange: a SunPy TimeRange object

    goes_class_filter: (optional) string
        a string specifying a minimum GOES class for inclusion in the list, e.g. 'M1', 'X2'."""
    
    
    #use HEK module to search for GOES events
    client=hek.HEKClient()
    event_type='FL'
    tstart=trange.start()
    tend=trange.end()

    #query the HEK for a list of events detected by the GOES instrument between tstart and tend (using a GOES-class filter)
    if goes_class_filter:
        result=client.query(hek.attrs.Time(tstart,tend),hek.attrs.EventType(event_type),hek.attrs.FL.GOESCls > goes_class_filter,hek.attrs.OBS.Observatory == 'GOES')
    else:
        result=client.query(hek.attrs.Time(tstart,tend),hek.attrs.EventType(event_type),hek.attrs.OBS.Observatory == 'GOES')

    #want to condense the results of the query into a more manageable dictionary
    #keep event data, start time, peak time, end time, GOES-class, location, active region source (as per GOES list standard)
    #make this into a list of dictionaries
    goes_event_list=[]

    for r in result:
        goes_event={}
        goes_event['event_date'] = parse_time(r['event_starttime']).date().strftime('%Y-%m-%d')
        goes_event['start_time'] =parse_time(r['event_starttime'])
        goes_event['peak_time'] = parse_time(r['event_peaktime'])
        goes_event['end_time'] = parse_time(r['event_endtime'])
        goes_event['goes_class'] = str(r['fl_goescls'])
        goes_event['goes_location'] = r['event_coord1'],r['event_coord2']
        goes_event['noaa_active_region'] = r['ar_noaanum']
        goes_event_list.append(goes_event)

    return goes_event_list

########NEW FILE########
__FILENAME__ = iris
"""
Some very beta tools for IRIS
"""

import sunpy.io
import sunpy.time
import sunpy.map

__all__ = ['SJI_to_cube']

def SJI_to_cube(filename, start=0, stop=None, hdu=0):
    """
    Read a SJI file and return a MapCube
    
    .. warning::
        This function is a very early beta and is not stable. Further work is 
        on going to improve SunPy IRIS support.
    
    Parameters
    ----------
    filename: string
        File to read
    
    start: int
        Temporal axis index to create MapCube from
    
    stop: int
        Temporal index to stop MapCube at
    
    hdu: int
        Choose hdu index

    Returns
    -------
    iris_cube: sunpy.map.MapCube
        A map cube of the SJI sequence
    """
    
    hdus = sunpy.io.read_file(filename)
    #Get the time delta
    time_range = sunpy.time.TimeRange(hdus[hdu][1]['STARTOBS'], hdus[hdu][1]['ENDOBS'])
    splits = time_range.split(hdus[hdu][0].shape[0])

    if not stop:
        stop = len(splits)

    headers = [hdus[hdu][1]]*(stop-start)
    datas = hdus[hdu][0][start:stop]
    
    #Make the cube:
    iris_cube = sunpy.map.Map(zip(datas,headers),cube=True)
    #Set the date/time
    for i,m in enumerate(iris_cube):
        m.meta['DATE-OBS'] = splits[i].center().isoformat()
    
    return iris_cube

########NEW FILE########
__FILENAME__ = lyra
from __future__ import absolute_import

import datetime
import calendar
import sqlite3
import numpy as np
import os
from sunpy.time import parse_time
import urllib

def download_lytaf_database(lytaf_dir=''):
    """download the latest version of the Proba-2 pointing database from the Proba2 Science Center"""
    #dl=sunpy.net.download.Downloader()
    #dl.download('http://proba2.oma.be/lyra/data/lytaf/annotation_ppt.db',path=lytaf_dir)
    url='http://proba2.oma.be/lyra/data/lytaf/annotation_ppt.db'
    destination=os.path.join(lytaf_dir,'annotation_ppt.db')
    urllib.urlretrieve(url,destination)
    
    return


def get_lytaf_events(timerange,lytaf_dir=''):
    """returns a list of LYRA pointing events that occured during a timerange"""
    #timerange is a TimeRange object
    #start_ts and end_ts need to be unix timestamps
    st_timerange = timerange.start()
    start_ts=calendar.timegm(st_timerange.timetuple())
    en_timerange=timerange.end()
    end_ts=calendar.timegm(en_timerange.timetuple())
    
    #involves executing SQLite commands from within python.
    #connect to the SQlite database
    #conn=sqlite3.connect(lytaf_dir + 'annotation_ppt.db')
    conn=sqlite3.connect(os.path.join(lytaf_dir,'annotation_ppt.db'))
    cursor=conn.cursor()

    #create a substitute tuple out of the start and end times for using in the database query
    query_tup=(start_ts,end_ts,start_ts,end_ts,start_ts,end_ts)

    #search only for events within the time range of interest (the lightcurve start/end). Return records ordered by start time
    result=(cursor.execute('select * from event WHERE((begin_time > ? AND begin_time < ?) OR (end_time > ? AND end_time < ?)' +  
                'OR (begin_time < ? AND end_time > ?)) ORDER BY begin_time ASC',query_tup))
    
    #get all records from the query in python list format. 
    list=result.fetchall()

    #times are in unix time - want to use datetime instead
    output=[]

    for l in list:
        #create a dictionary for each entry of interest
        lar_entry={'roi_description':'LYRA LYTAF event',
        'start_time':datetime.datetime.utcfromtimestamp(l[1]),
        'ref_time':datetime.datetime.utcfromtimestamp(l[2]),
        'end_time':datetime.datetime.utcfromtimestamp(l[3]),
        'event_type_id':l[4],'event_type_description':_lytaf_event2string(l[4])[0]}

        #create output tuple for each entry in list
        #entry=(insertion_time,start_time,ref_time,end_time,event_type,event_type_info[0])
        #output a list of dictionaries
        output.append(lar_entry)
    
    return output

def split_series_using_lytaf(timearray,data,lar):
    """
    Proba-2 analysis code for splitting up LYRA timeseries around locations where LARs
    (and other data events) are observed.

    Inputs
    ------
    timearray - an array of times that can be understood by the SunPy parse_time function
    data - data array corresponding to the given time array
    lar - list of events obtained from querying the LYTAF database using lyra.get_lytaf_events()

    Output
    ------
    A list of dictionaries. Each dictionary contains a sub-series corresponding to an interval of 'good data'
    """    
    #lar is a dictionary with tags:
    #'start_time'
    #'end_time'
    #'ref_time'
    #'roi_description'
    #'event_type_description'
    #'event_type_id'

    
    n=len(timearray)
    mask=np.ones(n)
    el=len(lar)

    #make the input time array a list of datetime objects
    datetime_array=[]
    for tim in timearray:
        datetime_array.append(parse_time(tim))
        

        #scan through each entry retrieved from the LYTAF database
    for j in range(0,el):
        #want to mark all times with events as bad in the mask, i.e. = 0
        start_dt=lar[j]['start_time']
        end_dt=lar[j]['end_time']

        #find the start and end indices for each event
        start_ind=np.searchsorted(datetime_array,start_dt)
        end_ind=np.searchsorted(datetime_array,end_dt)

        #append the mask to mark event as 'bad'
        mask[start_ind:end_ind] = 0


    diffmask=np.diff(mask)
    tmp_discontinuity=np.where(diffmask != 0.)
    #disc contains the indices of mask where there are discontinuities
    disc = tmp_discontinuity[0]

    if len(disc) == 0:
        print 'No events found within time series interval. Returning original series.'
        return [{'subtimes':datetime_array,'subdata':data}]
    
    #-1 in diffmask means went from good data to bad
    #+1 means went from bad data to good

    #want to get the data between a +1 and the next -1

    #if the first discontinuity is a -1 then the start of the series was good. 
    if diffmask[disc[0]] == -1.0:
        #make sure we can always start from disc[0] below
        disc=np.insert(disc,0,0)
    
    split_series=[]

    limit=len(disc)
    #now extract the good data regions and ignore the bad ones
    for h in range(0,limit,2):

        if h == limit-1:
            #can't index h+1 here. Go to end of series
            subtimes=datetime_array[disc[h]:-1]
            subdata=data[disc[h]:-1]
            subseries={'subtimes':subtimes,'subdata':subdata}
            split_series.append(subseries)
        else:
            subtimes=datetime_array[disc[h]:disc[h+1]]
            subdata=data[disc[h]:disc[h+1]]
            subseries={'subtimes':subtimes,'subdata':subdata}
            split_series.append(subseries)

    return split_series

def _lytaf_event2string(integers):
    if type(integers) == int:
        integers=[integers]
    #else:
    #    n=len(integers)
    out=[]

    for i in integers:
        if i == 1:
            out.append('LAR')
        if i == 2:
            out.append('N/A')
        if i == 3:
            out.append('UV occult.')
        if i == 4:
            out.append('Vis. occult.')
        if i == 5:
            out.append('Offpoint')
        if i == 6:
            out.append('SAA')
        if i == 7:
            out.append('Auroral zone')
        if i == 8:
            out.append('Moon in LYRA')
        if i == 9:
            out.append('Moon in SWAP')
        if i == 10:
            out.append('Venus in LYRA')
        if i == 11:
            out.append('Venus in SWAP')

    return out

########NEW FILE########
__FILENAME__ = rhessi
# -*- coding: utf-8 -*-
"""
    Provides programs to process and analyze RHESSI data.

    .. warning:: This module is still in development!

"""

from __future__ import absolute_import

import urllib
import csv
from datetime import datetime
from datetime import timedelta

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates

from astropy.io import fits

import sunpy
from sunpy.time import TimeRange, parse_time
import sunpy.sun.constants as sun
from sunpy.sun.sun import solar_semidiameter_angular_size
from sunpy.sun.sun import sunearth_distance

__all__ = ['get_obssumm_dbase_file', 'parse_obssumm_dbase_file', 'get_obssum_filename', 'get_obssumm_file', 'parse_obssumm_file', 'backprojection']

# Measured fixed grid parameters
grid_pitch = (4.52467, 7.85160, 13.5751, 23.5542, 40.7241, 70.5309, 122.164, 
              211.609, 366.646)
grid_orientation = (3.53547, 2.75007, 3.53569, 2.74962, 3.92596, 2.35647, 
                    0.786083, 0.00140674, 1.57147)

data_servers = ('http://hesperia.gsfc.nasa.gov/hessidata/', 
                'http://hessi.ssl.berkeley.edu/hessidata/',
                'http://soleil.i4ds.ch/hessidata/')

lc_linecolors = ('black', 'pink', 'green', 'blue', 'brown', 'red', 
                     'navy', 'orange', 'green')

def get_obssumm_dbase_file(time_range):
    """
    Download the RHESSI observing summary database file. This file lists the 
    name of observing summary files for specific time ranges. 
    
    Parameters
    ----------
    time_range : str, TimeRange
        A TimeRange or time range compatible string

    Returns
    -------
    value : tuple
        Return a tuple (filename, headers) where filename is the local file 
        name under which the object can be found, and headers is 
        whatever the info() method of the object returned by urlopen.

    Examples
    --------
    >>> import sunpy.instr.rhessi as rhessi
    >>> rhessi.get_obssumm_dbase_file(('2011/04/04', '2011/04/05'))

    References
    ----------
    | http://hesperia.gsfc.nasa.gov/ssw/hessi/doc/guides/hessi_data_access.htm#Observing Summary Data

    .. note::
        This API is currently limited to providing data from whole days only.

    """
    
    #    http://hesperia.gsfc.nasa.gov/hessidata/dbase/hsi_obssumm_filedb_200311.txt
    
    _time_range = TimeRange(time_range)
    data_location = 'dbase/'
    
    url_root = data_servers[0] + data_location
    url = url_root + _time_range.t1.strftime("hsi_obssumm_filedb_%Y%m.txt")
    
    f = urllib.urlretrieve(url)
    
    return f
      
def parse_obssumm_dbase_file(filename):
    """
    Parse the RHESSI observing summary database file. This file lists the 
    name of observing summary files for specific time ranges along with other 
    info
    
    Parameters
    ----------
    filename : str
        The filename of the obssumm dbase file

    Returns
    -------
    out : dict
        Return a dict containing the parsed data in the dbase file

    Examples
    --------
    >>> import sunpy.instr.rhessi as rhessi
    >>> f = rhessi.get_obssumm_dbase_file(('2011/04/04', '2011/04/05'))
    >>> rhessi.parse_obssumm_dbase_file(f[0])

    References
    ----------
    | http://hesperia.gsfc.nasa.gov/ssw/hessi/doc/guides/hessi_data_access.htm#Observing Summary Data

    .. note::
        This API is currently limited to providing data from whole days only.

    """
    with open(filename, "rb") as fd:
        reader = csv.reader(fd, delimiter=' ', skipinitialspace=True)
        headerline = reader.next()
        headerline = reader.next()
        headerline = reader.next()
        headerline = reader.next()
        
        obssumm_filename = []
        orbit_start = []
        orbit_end = []
        start_time = []
        end_time = []
        status_flag = []
        number_of_packets = []
        
        for row in reader:
            obssumm_filename.append(row[0])
            orbit_start.append(int(row[1]))
            orbit_end.append(int(row[2]))
            start_time.append(datetime.strptime(row[3], '%d-%b-%y'))
            end_time.append(datetime.strptime(row[5], '%d-%b-%y'))
            status_flag.append(int(row[7]))
            number_of_packets.append(int(row[8]))

        return {
            headerline[0].lower(): obssumm_filename,
            headerline[1].lower(): orbit_start,
            headerline[2].lower(): orbit_end,
            headerline[3].lower(): start_time,
            headerline[4].lower(): end_time,
            headerline[5].lower(): status_flag,
            headerline[6].lower(): number_of_packets
        }

def get_obssum_filename(time_range):
    """
    Download the RHESSI observing summary data from one of the RHESSI 
    servers, parses it, and returns the name of the obssumm file relevant for
    the time range

    Parameters
    ----------
    time_range : str, TimeRange 
        A TimeRange or time range compatible string

    Returns
    -------
    out : string
        Returns the filename of the observation summary file

    Examples
    --------
    >>> import sunpy.instr.rhessi as rhessi
    >>> rhessi.get_obssumm_filename(('2011/04/04', '2011/04/05'))

    .. note::
        This API is currently limited to providing data from whole days only.

    """
    # need to download and inspect the dbase file to determine the filename
    # for the observing summary data
    f = get_obssumm_dbase_file(time_range)
    data_location = 'metadata/catalog/'
   
    result = parse_obssumm_dbase_file(f[0])
    _time_range = TimeRange(time_range)
    
    index_number = int(_time_range.t1.strftime('%d')) - 1
    
    return data_servers[0] + data_location + result.get('filename')[index_number] + 's'

def get_obssumm_file(time_range):
    """
    Download the RHESSI observing summary data from one of the RHESSI 
    servers. 

    Parameters
    ----------
    time_range : str, TimeRange
        A TimeRange or time range compatible string

    Returns
    -------
    out : tuple
        Return a tuple (filename, headers) where filename is the local file 
        name under which the object can be found, and headers is 
        whatever the info() method of the object returned by urlopen.

    Examples
    --------
    >>> import sunpy.instr.rhessi as rhessi
    >>> rhessi.get_obssumm_file(('2011/04/04', '2011/04/05'))

    .. note::
        This API is currently limited to providing data from whole days only.

    """
    
    time_range = TimeRange(time_range)
    data_location = 'metadata/catalog/'
    
    #TODO need to check which is the closest servers
    url_root = data_servers[0] + data_location
        
    url = url_root + get_obssum_filename(time_range)
    
    print('Downloading file: ' + url)
    f = urllib.urlretrieve(url)
    
    return f

def parse_obssumm_file(filename):
    """
    Parse a RHESSI observation summary file.

    Parameters
    ----------
    filename : str
        The filename of a RHESSI fits file.

    Returns
    -------
    out : dict
        Returns a dictionary.

    Examples
    --------
    >>> import sunpy.instr.rhessi as rhessi
    >>> f = rhessi.get_obssumm_file(('2011/04/04', '2011/04/05'))
    >>> data = rhessi.parse_obssumm_file(f[0])

    """

    afits = fits.open(filename)
    header = afits[0].header
    
    reference_time_ut = parse_time(afits[5].data.field('UT_REF')[0])
    time_interval_sec = afits[5].data.field('TIME_INTV')[0]
    # label_unit = fits[5].data.field('DIM1_UNIT')[0]
    # labels = fits[5].data.field('DIM1_IDS')
    labels = ['3 - 6 keV', '6 - 12 keV', '12 - 25 keV', '25 - 50 keV', 
              '50 - 100 keV', '100 - 300 keV', '300 - 800 keV', '800 - 7000 keV',
              '7000 - 20000 keV']

    lightcurve_data = np.array(afits[6].data.field('countrate'))

    dim = np.array(lightcurve_data[:,0]).size
 
    time_array = [reference_time_ut + timedelta(0,time_interval_sec*a) for a in np.arange(dim)]

    #TODO generate the labels for the dict automatically from labels
    data = {'time': time_array, 'data': lightcurve_data, 'labels': labels}
       
    return header, data

def _backproject(calibrated_event_list, detector=8, pixel_size=(1.,1.), image_dim=(64,64)):
    """
    Given a stacked calibrated event list fits file create a back 
    projection image for an individual detectors. This function is used by
    backprojection.

    Parameters
    ----------
    calibrated_event_list : string
        filename of a RHESSI calibrated event list
    detector : int
        the detector number
    pixel_size : 2-tuple
        the size of the pixels in arcseconds. Default is (1,1).
    image_dim : 2-tuple
        the size of the output image in number of pixels

    Returns
    -------
    out : ndarray
        Return a backprojection image.

    Examples
    --------
    >>> import sunpy.instr.rhessi as rhessi

    """
    afits = fits.open(calibrated_event_list)
    
    #info_parameters = fits[2]    
    #detector_efficiency = info_parameters.data.field('cbe_det_eff$$REL')
    
    afits = fits.open(calibrated_event_list)

    fits_detector_index = detector + 2
    detector_index = detector - 1
    grid_angle = np.pi/2. - grid_orientation[detector_index]
    harm_ang_pitch = grid_pitch[detector_index]/1

    phase_map_center = afits[fits_detector_index].data.field('phase_map_ctr')
    this_roll_angle = afits[fits_detector_index].data.field('roll_angle')
    modamp = afits[fits_detector_index].data.field('modamp')
    grid_transmission = afits[fits_detector_index].data.field('gridtran')
    count = afits[fits_detector_index].data.field('count')

    tempa = (np.arange(image_dim[0]*image_dim[1]) %  image_dim[0]) - (image_dim[0]-1)/2.
    tempb = tempa.reshape(image_dim[0],image_dim[1]).transpose().reshape(image_dim[0]*image_dim[1])

    pixel = np.array(zip(tempa,tempb))*pixel_size[0]
    phase_pixel = (2*np.pi/harm_ang_pitch)* ( np.outer(pixel[:,0], np.cos(this_roll_angle - grid_angle)) - 
                                              np.outer(pixel[:,1], np.sin(this_roll_angle - grid_angle))) + phase_map_center
    phase_modulation = np.cos(phase_pixel)    
    gridmod = modamp * grid_transmission
    probability_of_transmission = gridmod*phase_modulation + grid_transmission
    bproj_image = np.inner(probability_of_transmission, count).reshape(image_dim)
        
    return bproj_image

def backprojection(calibrated_event_list, pixel_size=(1.,1.), image_dim=(64,64)):
    """
    Given a stacked calibrated event list fits file create a back 
    projection image.
    
    .. warning:: The image is not in the right orientation!

    Parameters
    ----------
    calibrated_event_list : string
        filename of a RHESSI calibrated event list
    detector : int
        the detector number
    pixel_size : 2-tuple
        the size of the pixels in arcseconds. Default is (1,1).
    image_dim : 2-tuple
        the size of the output image in number of pixels

    Returns
    -------
    out : RHESSImap
        Return a backprojection map.

    Examples
    --------
    >>> import sunpy.instr.rhessi as rhessi
    >>> map = rhessi.backprojection(sunpy.RHESSI_EVENT_LIST)
    >>> map.show()

    """
    
    calibrated_event_list = sunpy.RHESSI_EVENT_LIST
    afits = fits.open(calibrated_event_list)
    info_parameters = afits[2]
    xyoffset = info_parameters.data.field('USED_XYOFFSET')[0]
    time_range = TimeRange(info_parameters.data.field('ABSOLUTE_TIME_RANGE')[0])
    
    image = np.zeros(image_dim)
    
    #find out what detectors were used
    det_index_mask = afits[1].data.field('det_index_mask')[0]    
    detector_list = (np.arange(9)+1) * np.array(det_index_mask)
    for detector in detector_list:
        if detector > 0:
            image = image + _backproject(calibrated_event_list, detector=detector, pixel_size=pixel_size, image_dim=image_dim)
    
    dict_header = {
        "DATE-OBS": time_range.center().strftime("%Y-%m-%d %H:%M:%S"), 
        "CDELT1": pixel_size[0],
        "NAXIS1": image_dim[0],
        "CRVAL1": xyoffset[0],
        "CRPIX1": image_dim[0]/2 + 0.5, 
        "CUNIT1": "arcsec",
        "CTYPE1": "HPLN-TAN",
        "CDELT2": pixel_size[1],
        "NAXIS2": image_dim[1],
        "CRVAL2": xyoffset[1],
        "CRPIX2": image_dim[0]/2 + 0.5,
        "CUNIT2": "arcsec",
        "CTYPE2": "HPLT-TAN",
        "HGLT_OBS": 0,
        "HGLN_OBS": 0,
        "RSUN_OBS": solar_semidiameter_angular_size(time_range.center()),
        "RSUN_REF": sun.radius,
        "DSUN_OBS": sunearth_distance(time_range.center()) * sunpy.sun.constants.au
    }
    
    header = sunpy.map.MapHeader(dict_header)
    result_map = sunpy.map.Map(image, header)
            
    return result_map

########NEW FILE########
__FILENAME__ = test_goes
from __future__ import absolute_import

import datetime

import pytest
from sunpy.time import TimeRange
from sunpy.instr import goes

@pytest.mark.online
def test_goes_event_list():
    trange=TimeRange('2011-06-07 00:00','2011-06-08 00:00')
    result=goes.get_goes_event_list(trange,goes_class_filter='M1')
    assert type(result) == list
    assert type(result[0]) == dict
    assert type(result[0]['event_date'] == str)
    assert type(result[0]['goes_location'] == tuple)
    assert type(result[0]['peak_time'] == datetime)
    assert type(result[0]['start_time'] == datetime)
    assert type(result[0]['end_time'] == datetime)
    assert type(result[0]['goes_class'] == str)
    assert type(result[0]['noaa_active_region'] == int)
    assert result[0]['event_date'] == '2011-06-07'
    assert result[0]['goes_location'] == (54, -21)
    assert result[0]['start_time'] == datetime.datetime(2011,6,7,6,16)
    assert result[0]['peak_time'] == datetime.datetime(2011,6,7,6,41)
    assert result[0]['end_time'] == datetime.datetime(2011,6,7,6,59)
    assert result[0]['goes_class'] == 'M2.5'
    assert result[0]['noaa_active_region'] == 11226

########NEW FILE########
__FILENAME__ = test_iris
# -*- coding: utf-8 -*-

import os

import numpy as np

import sunpy.data.test
import sunpy.map

from sunpy.instr import iris

def test_SJI_to_cube():
    test_data = os.path.join(sunpy.data.test.rootdir,'iris_l2_20130801_074720_4040000014_SJI_1400_t000.fits')
    iris_cube = iris.SJI_to_cube(test_data, start=0, stop=None, hdu=0)
    
    assert isinstance(iris_cube, sunpy.map.MapCube)
    assert isinstance(iris_cube.maps[0], sunpy.map.sources.IRISMap)
    assert len(iris_cube.maps) == 2
    assert iris_cube.maps[0].meta['DATE-OBS'] != iris_cube.maps[1].meta['DATE-OBS']

def test_iris_rot():
    test_data = os.path.join(sunpy.data.test.rootdir,'iris_l2_20130801_074720_4040000014_SJI_1400_t000.fits')
    iris_cube = iris.SJI_to_cube(test_data, start=0, stop=None, hdu=0)
    irismap = iris_cube.maps[0]
    irismap_rot = irismap.iris_rot()
    
    assert isinstance(irismap_rot, sunpy.map.sources.IRISMap)
    assert not np.allclose(irismap_rot.data, irismap.data)
    
    assert irismap_rot.meta['pc1_1'] == 1
    assert irismap_rot.meta['pc1_2'] == 0
    assert irismap_rot.meta['pc2_1'] == 0
    assert irismap_rot.meta['pc2_2'] == 1

########NEW FILE########
__FILENAME__ = test_lyra
from __future__ import absolute_import

from sunpy.time import TimeRange, parse_time
from sunpy.instr import lyra
import tempfile
import os
import pytest
import datetime
import numpy as np

@pytest.mark.online
def test_lytaf_utils():
    '''test the downloading of the LYTAF file and subsequent queries'''
    tmp_dir=tempfile.mkdtemp()
    lyra.download_lytaf_database(lytaf_dir=tmp_dir)
    assert os.path.exists(os.path.join(tmp_dir,'annotation_ppt.db'))

    #try doing a query on the temporary database
    lar=lyra.get_lytaf_events(TimeRange('2010-06-13 02:00','2010-06-13 06:00'),lytaf_dir=tmp_dir)
    assert type(lar) == list
    assert type(lar[0]) == dict
    assert type(lar[0]['start_time']) == datetime.datetime
    assert type(lar[0]['end_time']) == datetime.datetime
    assert type(lar[0]['roi_description']) == str
    assert type(lar[0]['event_type_description']) == str
    assert lar[0]['start_time'] == parse_time('2010-06-13 02:07:04')
    assert lar[0]['end_time'] == parse_time('2010-06-13 02:10:04')
    assert lar[0]['event_type_description'] == 'LAR'

    #test split_series_using_lytaf
    #construct a dummy signal for testing purposes
    basetime=parse_time('2010-06-13 02:00')
    seconds=3600
    dummy_time = [basetime + datetime.timedelta(0, s) for s in range(seconds)]
    dummy_data=np.random.random(seconds)

    split=lyra.split_series_using_lytaf(dummy_time, dummy_data, lar)
    assert type(split) == list
    assert len(split) == 4
    assert split[0]['subtimes'][0] == datetime.datetime(2010, 6, 13, 2, 0)
    assert split[0]['subtimes'][-1] == datetime.datetime(2010, 6, 13, 2, 7, 2)
    assert split[3]['subtimes'][0] == datetime.datetime(2010, 6, 13, 2, 59, 41)
    assert split[3]['subtimes'][-1] == datetime.datetime(2010, 6, 13, 2, 59, 58)
    

########NEW FILE########
__FILENAME__ = ana
"""
ANA File Reader

.. warning::
    This code currently has an unresolved bug under Windows, it may not work as expected.

Notes
-----
ANA is a script that allows people to access compressed ana files.
It accesses a C library, based on Michiel van Noort's
IDL DLM library 'f0' which contains a cleaned up version of the original
anarw routines.

Created by Tim van Werkhoven (t.i.m.vanwerkhoven@gmail.com) on 2009-02-11.
Copyright (c) 2009--2011 Tim van Werkhoven.
"""
 
from __future__ import absolute_import
import os

try:
    from sunpy.io import _pyana
except ImportError: # pragma: no cover
    _pyana = None # pragma: no cover

from sunpy.io.header import FileHeader

__all__ = ['read', 'get_header', 'write']

def read(filename, debug=False):
    """
    Loads an ANA file and returns the data and a header in a list of (data,
    header) tuples.
    
    Parameters
    ----------
    filename: string
        Name of file to be read.
    debug: bool, optional
        Prints versbose debug information.
    
    Returns
    -------
    out: list
        A list of (data, header) tuples
    
    Examples
    --------
    >>> data = sunpy.io.ana.read(filename)
    
    """
    if not os.path.isfile(filename):
        raise IOError("File does not exist!")

    if _pyana is None:
        raise ImportError("C extension for ANA is missing, please rebuild") # pragma: no cover

    data = _pyana.fzread(filename, debug)
    return [(data['data'],FileHeader(data['header']))]

def get_header(filename, debug=False):
    """
    Loads an ANA file and only return the header consisting of the dimensions,
    size (defined as the product of all dimensions times the size of the
    datatype, this not relying on actual filesize) and comments.

    Parameters
    ----------
    filename: string
        Name of file to be read.
    debug: bool, optional
        Prints versbose debug information.
    
    Returns
    -------
    out: list
        A list of FileHeader headers

    Examples
    --------    
    >>> header = sunpy.io.ana.get_header(filename)
    """
    if _pyana is None:
        raise ImportError("C extension for ANA is missing, please rebuild")# pragma: no cover

    data = _pyana.fzread(filename, debug)
    return [FileHeader(data['header'])]

def write(filename, data, comments=False, compress=1, debug=False):
    """
    Saves a 2D numpy array as an ANA file and returns the bytes written or NULL

    Parameters
    ----------
    filename: string
        Name of file to be created.
    data: numpy array
        Name of data to be stored.
    comments: FileHeader, optional
        The comments to be stored as a header.
    compress: int, optional
        To compress the data or not.
        1 is to compress, 0 is uncompressed
    debug: bool, optional
        Prints versbose debug information.
    
    Returns
    -------
    out: ANA compressed archive
        A new ANA compressed archive containing the data and header.    

    Examples
    --------    
    >>> written = sunpy.io.ana.write(filename, data, comments=Falsem, compress=1)
    """
    if _pyana is None:
        raise ImportError("C extension for ANA is missing, please rebuild")# pragma: no cover

    if comments:
        return _pyana.fzwrite(filename, data, compress, comments, debug)
    else:
        return _pyana.fzwrite(filename, data, compress, '', debug)

########NEW FILE########
__FILENAME__ = file_tools
from __future__ import absolute_import

import re

try:
    from . import fits
except ImportError:
    fits = None

try:
    from . import jp2
except ImportError:
    jp2 = None

try:
    from . import ana
except ImportError:
    ana = None

__all__ = ['read_file', 'read_file_header', 'write_file']

# File formats supported by SunPy
_known_extensions = {
    ('fts', 'fits'): 'fits',
    ('jp2', 'j2k', 'jpc', 'jpt'): 'jp2',
    ('fz', 'f0'): 'ana'
}

#Define a dict which raises a custom error message if the value is None
class Readers(dict):
    def __init__(self, *args):
        dict.__init__(self, *args)

    def __getitem__(self, key):
        val = dict.__getitem__(self, key)
        if val is None:
            raise ReaderError("The Reader sunpy.io.%s is not avalible, please check that you have the required dependancies installed."%key)
        return val

#Map the readers
_readers = Readers({
            'fits':fits,
            'jp2':jp2,
            'ana':ana
})

def read_file(filepath, filetype=None, **kwargs):
    """
    Automatically determine the filetype and read the file

    Parameters
    ----------
    filepath : string
        The file to be read

    filetype: string
        Supported reader or extension to manually specify the filetype.
        Supported readers are ('jp2', 'fits', 'ana')

    Returns
    -------
    pairs : list
        A list of (data, header) tuples.
    """
    if filetype:
        return _readers[filetype].read(filepath, **kwargs)

    for extension, readername in _known_extensions.items():
        if filepath.endswith(extension) or filetype in extension:
            return _readers[readername].read(filepath, **kwargs)

    # If filetype is not apparent from extension, attempt to detect
    readername = _detect_filetype(filepath)
    return _readers[readername].read(filepath, **kwargs)

def read_file_header(filepath, filetype=None, **kwargs):
    """
    Reads the header from a given file

    This should always return a instance of io.header.FileHeader

    Parameters
    ----------

    filepath :  string
        The file from which the header is to be read.

    filetype: string
        Supported reader or extension to manually specify the filetype.
        Supported readers are ('jp2', 'fits')

    Returns
    -------

    headers : list
        A list of headers
    """
    if filetype:
        return _readers[filetype].get_header(filepath, **kwargs)

    for extension, readername in _known_extensions.items():
        if filepath.endswith(extension) or filetype in extension:
            return _readers[readername].get_header(filepath, **kwargs)

    readername = _detect_filetype(filepath)
    return _readers[readername].get_header(filepath, **kwargs)

def write_file(fname, data, header, filetype='auto', **kwargs):
    """
    Write a file from a data & header pair using one of the defined file types.

    Parameters
    ----------
    fname : string
        Filename of file to save

    data : ndarray
        Data to save to a fits file

    header : OrderedDict
        Meta data to save with the data

    filetype : string
        {'auto', 'fits', 'jp2'} Filetype to savem if auto fname extension will
        be detected, else specifiy a supported file extension.
    
    Notes
    -----
    * Other keyword arguments will be passes to the writer function used.
    * This routine currently only supports saving a single HDU.
    """
    if filetype == 'auto':
        for extension, readername in _known_extensions.items():
            if fname.endswith(extension):
                return _readers[readername].write(fname, data, header, **kwargs)

    else:
        for extension, readername in _known_extensions.items():
            if filetype in extension:
                return _readers[readername].write(fname, data, header, **kwargs)

    #Nothing has matched, panic
    raise ValueError("This filetype is not supported" )

def _detect_filetype(filepath):
    """
    Attempts to determine the type of data contained in a file.

    This is only used for reading because it opens the file to check the data.
    """

    # Open file and read in first two lines
    with open(filepath) as fp:
        line1 = fp.readline()
        line2 = fp.readline()
        #Some FITS files do not have line breaks at the end of header cards.
        fp.seek(0)
        first80 = fp.read(80)

    # FITS
    #
    # Checks for "KEY_WORD  =" at beginning of file
    match = re.match(r"[A-Z0-9_]{0,8} *=", first80)

    if match is not None:
        return 'fits'

    # JPEG 2000
    #
    # Checks for one of two signatures found at beginning of all JP2 files.
    # Adapted from ExifTool
    # [1] http://www.sno.phy.queensu.ca/~phil/exiftool/
    # [2] http://www.jpeg.org/public/fcd15444-2.pdf
    # [3] ftp://ftp.remotesensing.org/jpeg2000/fcd15444-1.pdf
    jp2_signatures = ["\x00\x00\x00\x0cjP  \x0d\x0a\x87\x0a",
                      "\x00\x00\x00\x0cjP\x1a\x1a\x0d\x0a\x87\x0a"]

    for sig in jp2_signatures:
        if line1 + line2 == sig:
            return 'jp2'

    # Raise an error if an unsupported filetype is encountered
    raise UnrecognizedFileTypeError("The requested filetype is not currently "
                                    "supported by SunPy.")

class UnrecognizedFileTypeError(IOError):
    """Exception to raise when an unknown file type is encountered"""
    pass

class ReaderError(ImportError):
    """Exception to raise when an unknown file type is encountered"""
    pass

class InvalidJPEG2000FileExtension(IOError):
    pass
########NEW FILE########
__FILENAME__ = fits
"""
FITS File Reader

Notes
-----
FITS
    [1] FITS files allow comments to be attached to every value in the header.
    This is implemented in this module as a KEYCOMMENTS dictionary in the
    sunpy header. To add a comment to the file on write, add a comment to this
    dictionary with the same name as a key in the header (upcased).

PyFITS
    [1] Due to the way PyFITS works with images the header dictionary may
    differ depending on whether is accessed before or after the fits[0].data
    is requested. If the header is read before the data then the original
    header will be returned. If the header is read after the data has been
    accessed then the data will have been scaled and a modified header
    reflecting these changes will be returned: BITPIX may differ and
    BSCALE and B_ZERO may be dropped in the modified version.

    [2] The verify('fix') call attempts to handle violations of the FITS
    standard. For example, nan values will be converted to "nan" strings.
    Attempting to cast a pyfits header to a dictionary while it contains
    invalid header tags will result in an error so verifying it early on
    makes the header easier to work with later.

References
----------
| http://stackoverflow.com/questions/456672/class-factory-in-python
| http://stsdas.stsci.edu/download/wikidocs/The_PyFITS_Handbook.pdf

"""
from __future__ import absolute_import

import os
import re
import itertools
import collections

from astropy.io import fits

from sunpy.io.header import FileHeader

__all__ = ['read', 'get_header', 'write', 'extract_waveunit']

__author__ = "Keith Hughitt, Stuart Mumford, Simon Liedtke"
__email__ = "keith.hughitt@nasa.gov"

def read(filepath, hdus=None):
    """
    Read a fits file

    Parameters
    ----------
    filepath : string
        The fits file to be read
    hdu: int or iterable
        The HDU indexes to read from the file

    Returns
    -------
    pairs : list
        A list of (data, header) tuples

    Notes
    -----
    This routine reads all the HDU's in a fits file and returns a list of the
    data and a FileHeader instance for each one.
    Also all comments in the original file are concatenated into a single
    'comment' key in the returned FileHeader.
    """
    hdulist = fits.open(filepath)
    if hdus is not None:
        if isinstance(hdus, int):
            hdulist = hdulist[hdus]
        elif isinstance(hdus, collections.Iterable):
            hdulist = [hdulist[i] for i in hdus]
    try:
        hdulist.verify('silentfix')

        headers = get_header(hdulist)
        pairs = []
        for hdu,header in itertools.izip(hdulist, headers):
            pairs.append((hdu.data, header))
    finally:
        hdulist.close()

    return pairs

def get_header(afile):
    """
    Read a fits file and return just the headers for all HDU's. In each header,
    the key WAVEUNIT denotes the wavelength unit which is used to describe the
    value of the key WAVELNTH.

    Parameters
    ----------
    afile : string or fits.HDUList
        The file to be read, or HDUList to process

    Returns
    -------
    headers : list
        A list of FileHeader headers
    """
    if isinstance(afile,fits.HDUList):
        hdulist = afile
        close = False
    else:
        hdulist = fits.open(afile)
        hdulist.verify('silentfix')
        close=True

    try:
        headers= []
        for hdu in hdulist:
            try:
                comment = "".join(hdu.header['COMMENT']).strip()
            except KeyError:
                comment = ""
            try:
                history = "".join(hdu.header['HISTORY']).strip()
            except KeyError:
                history = ""

            header = FileHeader(hdu.header)
            header['COMMENT'] = comment
            header['HISTORY'] = history

            #Strip out KEYCOMMENTS to a dict, the hard way
            keydict = {}
            for card in hdu.header.cards:
                if card.comment != '':
                    keydict.update({card.keyword:card.comment})
            header['KEYCOMMENTS'] = keydict
            header['WAVEUNIT'] = extract_waveunit(header)

            headers.append(header)
    finally:
        if close:
            hdulist.close()
    return headers

def write(fname, data, header, **kwargs):
    """
    Take a data header pair and write a fits file

    Parameters
    ----------
    fname: str
        File name, with extension

    data: ndarray
        n-dimensional data array

    header: dict
        A header dictionary
    """
    #Copy header so the one in memory is left alone while changing it for write
    header = header.copy()

    #The comments need to be added to the header seperately from the normal
    # kwargs. Find and deal with them:
    fits_header = fits.Header()
    # Check Header
    key_comments = header.pop('KEYCOMMENTS', False)

    for k,v in header.items():
        if isinstance(v, fits.header._HeaderCommentaryCards):
            if k == 'comments':
                comments = str(v).split('\n')
                for com in comments:
                    fits_header.add_comments(com)
            elif k == 'history':
                hists = str(v).split('\n')
                for hist in hists:
                    fits_header.add_history(hist)
            elif k != '':
                fits_header.append(fits.Card(k, str(v).split('\n')))
        else:
            fits_header.append(fits.Card(k,v))


    if isinstance(key_comments, dict):
        for k,v in key_comments.items():
            fits_header.comments[k] = v
    elif key_comments:
        raise TypeError("KEYCOMMENTS must be a dictionary")

    fitskwargs = {'output_verify':'fix'}
    fitskwargs.update(kwargs)
    fits.writeto(os.path.expanduser(fname), data, header=fits_header,
                   **fitskwargs)


def extract_waveunit(header):
    """Attempt to read the wavelength unit from a given FITS header.

    Parameters
    ----------
    header : FileHeader
        One :class:`sunpy.io.header.FileHeader` instance which was created by
        reading a FITS file. :func:`sunpy.io.fits.get_header` returns a list of
        such instances.

    Returns
    -------
    waveunit : str
        The wavelength unit that could be found or ``None`` otherwise.

    Examples
    --------
    The goal of this function is to return a string that can be used in
    conjunction with the astropy.units module so that the return value can be
    directly passed to ``astropy.units.Unit``::

        >>> import astropy.units
        >>> waveunit = extract_waveunit(header)
        >>> if waveunit is not None:
        ...     unit = astropy.units.Unit(waveunit)

    """
    # algorithm: try the following procedures in the following order and return
    # as soon as a waveunit could be detected
    # 1. read header('WAVEUNIT'). If None, go to step 2.
    # 1.1 -9 -> 'nm'
    # 1.2 -10 -> 'angstrom'
    # 1.3 0 -> go to step 2
    # 1.4 if neither of the above, return the value itself in lowercase
    # 2. parse waveunit_comment
    # 2.1 'in meters' -> 'm'
    # 3. parse wavelnth_comment
    # 3.1 "[$UNIT] ..." -> $UNIT
    # 3.2 "Observed wavelength ($UNIT)" -> $UNIT
    def parse_waveunit_comment(waveunit_comment):
        if waveunit_comment == 'in meters':
            return 'm'

    waveunit_comment = header['KEYCOMMENTS'].get('WAVEUNIT')
    wavelnth_comment = header['KEYCOMMENTS'].get('WAVELNTH')
    waveunit = header.get('WAVEUNIT')
    if waveunit is not None:
        metre_submultiples = {
            0: parse_waveunit_comment(waveunit_comment),
            -1: 'dm',
            -2: 'cm',
            -3: 'mm',
            -6: 'um',
            -9: 'nm',
            -10: 'angstrom',
            -12: 'pm',
            -15: 'fm',
            -18: 'am',
            -21: 'zm',
            -24: 'ym'}
        waveunit = metre_submultiples.get(waveunit, str(waveunit).lower())
    elif waveunit_comment is not None:
        waveunit = parse_waveunit_comment(waveunit_comment)
    elif wavelnth_comment is not None:
        # supported formats (where $UNIT is the unit like "nm" or "Angstrom"):
        #   "Observed wavelength ($UNIT)"
        #   "[$UNIT] ..."
        parentheses_pattern = r'Observed wavelength \((\w+?)\)$'
        brackets_pattern = r'^\[(\w+?)\]'
        for pattern in [parentheses_pattern, brackets_pattern]:
            m = re.search(pattern, wavelnth_comment)
            if m is not None:
                waveunit = m.group(1)
                break
    if waveunit == '': 
        return None # To fix problems associated with HMI FITS.        
    return waveunit

########NEW FILE########
__FILENAME__ = header
# -*- coding: utf-8 -*-
from __future__ import absolute_import
from sunpy.util.odict import OrderedDict

__all__ = ['FileHeader']

class FileHeader(OrderedDict):
    """ FileHeader is designed to provide a consistent interface to all other
    sunpy classes that expect a generic file.
    
    Open read all file types should format their header into a FileHeader """
    def __init__(self, *args, **kwargs):
        OrderedDict.__init__(self, *args, **kwargs)
########NEW FILE########
__FILENAME__ = jp2
"""JPEG 2000 File Reader"""
from __future__ import absolute_import

__author__ = "Keith Hughitt"
__email__ = "keith.hughitt@nasa.gov"

from xml.etree import cElementTree as ET

from glymur import Jp2k

from sunpy.util.xml import xml_to_dict
from sunpy.io.header import FileHeader

__all__ = ['read', 'get_header', 'write']

def read(filepath):
    """
    Reads a JPEG2000 file

    Parameters
    ----------
    filepath : string
        The file to be read

    j2k_to_image : string
        binary to use for reading?

    Returns
    -------
    pairs : list
        A list of (data, header) tuples
    """
    header = get_header(filepath)

    data = Jp2k(filepath).read()[::-1]

    return [(data, header[0])]

def get_header(filepath):
    """
    Reads the header from the file

    Parameters
    ----------
    filepath : string
        The file to be read

    Returns
    -------
    headers : list
        A list of headers read from the file
    """
    jp2 = Jp2k(filepath)
    xml_box = [box for box in jp2.box if box.box_id == 'xml ']
    xmlstring = ET.tostring(xml_box[0].xml.find('fits'))
    pydict = xml_to_dict(xmlstring)["fits"]

    #Fix types
    for k, v in pydict.items():
        if v.isdigit():
            pydict[k] = int(v)
        elif _is_float(v):
            pydict[k] = float(v)

    # Remove newlines from comment
    if 'comment' in pydict:
        pydict['comment'] = pydict['comment'].replace("\n", "")

    return [FileHeader(pydict)]

def write(fname, data, header):
    """
    Place holder for required file writer
    """
    raise NotImplementedError("No jp2 writer is implemented")

def _is_float(s):
    """Check to see if a string value is a valid float"""
    try:
        float(s)
        return True
    except ValueError:
        return False

########NEW FILE########
__FILENAME__ = test_ana
"""
General ANA Tests
"""
import unittest
from sunpy.io import ana
import numpy as np
            
class anaTests(unittest.TestCase):
    """Basic ANA tests"""
    def setUp(self):
            # Create a test image, store it, reread it and compare
            self.img_size = (456, 345)
            self.img_src = np.arange(np.product(self.img_size))
            self.img_src.shape = self.img_size
            self.img_i8 = self.img_src*2**8/self.img_src.max()
            self.img_i8 = self.img_i8.astype(np.int8)
            self.img_i16 = self.img_src*2**16/self.img_src.max()
            self.img_i16 = self.img_i16.astype(np.int16)
            self.img_f32 = self.img_src*1.0/self.img_src.max()
            self.img_f32 = self.img_f32.astype(np.float32)
    
    def runTests(self):
        unittest.TextTestRunner(verbosity=2).run(self.suite())
    
    def suite(self):
        suite = unittest.TestLoader().loadTestsFromTestCase(anaTests)
        return suite
    
    def testi8c(self):
        # Test int 8 compressed functions
        ana.write('/tmp/pyana-testi8c', self.img_i8, 'testcase', 0)
        self.img_i8c_rec = ana.read('/tmp/pyana-testi8c')
        self.assert_(np.sum(self.img_i8c_rec[0][0] - self.img_i8) == 0,
             msg="Storing 8 bits integer data with compression failed (diff: %d)" % (np.sum(self.img_i8c_rec[0][0] - self.img_i8)))
    
    def testi8u(self):
        # Test int 8 uncompressed functions
        ana.write('/tmp/pyana-testi8u', self.img_i8, 'testcase', 0)
        self.img_i8u_rec = ana.read('/tmp/pyana-testi8u')
        self.assert_(np.sum(self.img_i8u_rec[0][0] - self.img_i8) == 0,
            msg="Storing 8 bits integer data without compression failed (diff: %d)" % (np.sum(self.img_i8u_rec[0][0] - self.img_i8)))
    
    def testi16c(self):
        # Test int 16 compressed functions
        ana.write('/tmp/pyana-testi16c', self.img_i16, 'testcase', 0)
        self.img_i16c_rec = ana.read('/tmp/pyana-testi16c')
        self.assert_(np.sum(self.img_i16c_rec[0][0] - self.img_i16) == 0,
            msg="Storing 16 bits integer data with compression failed (diff: %d)" % (np.sum(self.img_i16c_rec[0][0] - self.img_i16)))
    
    def testi16u(self):
        # Test int 16 uncompressed functions
        ana.write('/tmp/pyana-testi16u', self.img_i16, 'testcase', 0)
        self.img_i16u_rec = ana.read('/tmp/pyana-testi16u')
        self.assert_(np.sum(self.img_i16u_rec[0][0] - self.img_i16) == 0,
            msg="Storing 16 bits integer data without compression failed (diff: %d)" % (np.sum(self.img_i16u_rec[0][0] - self.img_i16)))
    
    def testf32u(self):
        # Test float 32 uncompressed functions
        ana.write('/tmp/pyana-testf32u', self.img_f32, 'testcase', 0)
        self.img_f32u_rec = ana.read('/tmp/pyana-testf32u')
        self.assert_(np.sum(self.img_f32u_rec[0][0]- self.img_f32) == 0,
            msg="Storing 32 bits float data without compression failed (diff: %g)" % (1.0*np.sum(self.img_f32u_rec[0][0] - self.img_f32)))
    
    def testf32c(self):
        # Test if float 32 compressed functions
        #TODO: Bug with same code. Needs to be tracked down.
    
#        ana.write('/tmp/pyana-testf32c', self.img_f32, 1, 'testcase', 0)
#        self.img_f32c_rec = ana.read('/tmp/pyana-testf32c', 1)
#        self.assert_(np.sum(self.img_f32c_rec[0][1]- self.img_f32) == 0,
#            msg="Storing 32 bits float data without compression failed (diff: %g)" % (1.0*np.sum(self.img_f32c_rec[0][1] - self.img_f32)))    
           self.assertRaises(RuntimeError, ana.write, '/tmp/pyana-testf32c', self.img_f32, 'testcase', 1)
########NEW FILE########
__FILENAME__ = test_filetools
# -*- coding: utf-8 -*-
import numpy as np
import os

import sunpy
import sunpy.io
import sunpy.data.test
#==============================================================================
# Test, read, get_header and write through the file independant layer
#==============================================================================
class TestFiletools():

    def test_read_file_fits(self):
        #Test read FITS
        aiapair = sunpy.io.read_file(sunpy.AIA_171_IMAGE)
        assert isinstance(aiapair, list)
        assert len(aiapair) == 1
        assert len(aiapair[0]) == 2
        assert isinstance(aiapair[0][0], np.ndarray)
        assert isinstance(aiapair[0][1], sunpy.io.header.FileHeader)

        #Test read multi HDU list
        pairs = sunpy.io.read_file(sunpy.RHESSI_IMAGE)
        assert isinstance(pairs, list)
        assert len(pairs) == 4
        assert all([len(p) == 2 for p in pairs])
        assert all([isinstance(p[0], np.ndarray) for p in pairs])
        assert all([isinstance(p[1],
                               sunpy.io.header.FileHeader) for p in pairs])

    def test_read_file_jp2(self):
        #Test read jp2
        pair = sunpy.io.read_file(os.path.join(sunpy.data.test.rootdir,
                               "2013_06_24__17_31_30_84__SDO_AIA_AIA_193.jp2"))
                               
        assert isinstance(pair, list)
        assert len(pair) == 1
        assert len(pair[0]) == 2
        assert isinstance(pair[0][0], np.ndarray)
        assert isinstance(pair[0][1], sunpy.io.header.FileHeader)

    def test_read_file_header_fits(self):
        #Test FITS
        hlist = sunpy.io.read_file_header(sunpy.AIA_171_IMAGE)
        assert isinstance(hlist, list)
        assert len(hlist) == 1
        assert isinstance(hlist[0], sunpy.io.header.FileHeader)

    def test_read_file_header_jp2(self):
        #Test jp2
        hlist = sunpy.io.read_file_header(os.path.join(sunpy.data.test.rootdir,
                               "2013_06_24__17_31_30_84__SDO_AIA_AIA_193.jp2"))
        assert isinstance(hlist, list)
        assert len(hlist) == 1
        assert isinstance(hlist[0], sunpy.io.header.FileHeader)


    def test_write_file_fits(self):
        #Test write FITS
        aiapair = sunpy.io.read_file(sunpy.AIA_171_IMAGE)[0]
        sunpy.io.write_file("aia_171_image.fits", aiapair[0], aiapair[1],
                            clobber=True)
        assert os.path.exists("aia_171_image.fits")
        outpair = sunpy.io.read_file(sunpy.AIA_171_IMAGE)[0]
        assert np.all(np.equal(outpair[0], aiapair[0]))
        assert outpair[1] == aiapair[1]
        os.remove("aia_171_image.fits")
    
    def test_read_file_ana(self):
        ana_data = sunpy.io.read_file(os.path.join(sunpy.data.test.rootdir,"test_ana.fz"))
        assert isinstance(ana_data, list)
        assert len(ana_data) == 1
        assert len(ana_data[0]) == 2
        assert isinstance(ana_data[0][0], np.ndarray)
        assert isinstance(ana_data[0][1], sunpy.io.header.FileHeader)
    
    def test_read_file__header_ana(self):
        ana_data = sunpy.io.read_file_header(os.path.join(sunpy.data.test.rootdir,"test_ana.fz"))
        assert isinstance(ana_data, list)
        assert len(ana_data) == 1
        assert isinstance(ana_data[0], sunpy.io.header.FileHeader)
    
    def test_write_file_ana(self):
        ana = sunpy.io.read_file(os.path.join(sunpy.data.test.rootdir,"test_ana.fz"))[0]
        sunpy.io.write_file("ana_test_write.fz", ana[0], str(ana[1]))
        assert os.path.exists("ana_test_write.fz")
        outpair = sunpy.io.read_file(os.path.join(sunpy.data.test.rootdir,"test_ana.fz"))
        assert np.all(np.equal(outpair[0][1], ana[1]))
        assert outpair[0][1] == ana[1]
        os.remove("ana_test_write.fz")
        
    #TODO: Test write jp2

########NEW FILE########
__FILENAME__ = test_fits
import sunpy.io.fits
from sunpy.io.fits import get_header, extract_waveunit
from sunpy.data.sample import RHESSI_IMAGE, EIT_195_IMAGE, AIA_171_IMAGE,\
    SWAP_LEVEL1_IMAGE
from sunpy.data.test.waveunit import MEDN_IMAGE, MQ_IMAGE, NA_IMAGE, SVSM_IMAGE

def read_hdus():
    pairs = sunpy.io.fits.read(RHESSI_IMAGE)
    assert len(pairs) == 4

def read_hdu_int():
    pairs = sunpy.io.fits.read(RHESSI_IMAGE, hdus=1)
    assert len(pairs) == 1

def read_hdus_list():
    pairs = sunpy.io.fits.read(RHESSI_IMAGE, hdus=[1,2])
    assert len(pairs) == 2

def read_hdus_gen():
    pairs = sunpy.io.fits.read(RHESSI_IMAGE, hdus=xrange(0,1))
    assert len(pairs) == 2

def test_extract_waveunit_missing_waveunit_key_and_missing_wavelnth_comment():
    waveunit = extract_waveunit(get_header(RHESSI_IMAGE)[0])
    assert waveunit is None


def test_missing_waveunit_in_wavelnth_comment():
    # the comment of the key WAVELNTH has the value
    # '171 = Fe IX/X, 195 = Fe XII,' which contains no unit information
    waveunit = extract_waveunit(get_header(EIT_195_IMAGE)[0])
    assert waveunit is None


def test_extract_waveunit_from_waveunit_key():
    # the key WAVEUNIT can be accessed and returned directly
    waveunit = extract_waveunit(get_header(AIA_171_IMAGE)[0])
    assert waveunit == 'angstrom'


def test_extract_waveunit_minus9():
    # value of WAVEUNIT is -9
    waveunit = extract_waveunit(get_header(MEDN_IMAGE)[0])
    assert waveunit == 'nm'


def test_extract_waveunit_minus10():
    # value of WAVEUNIT is -10
    waveunit = extract_waveunit(get_header(MQ_IMAGE)[0])
    assert waveunit == 'angstrom'


def test_extract_waveunit_waveunitcomment():
    # comment of WAVEUNIT is: "in meters"
    waveunit = extract_waveunit(get_header(NA_IMAGE)[0])
    assert waveunit == 'm'


def test_extract_waveunit_wavelnthcomment_brackets():
    # WAVELNTH comment is: "[Angstrom] bandpass peak response"
    waveunit = extract_waveunit(get_header(SWAP_LEVEL1_IMAGE)[0])
    assert waveunit == 'angstrom'


def test_extract_waveunit_wavelnthcomment_parentheses():
    # WAVELNTH comment is: "Observed wavelength (nm)"
    waveunit = extract_waveunit(get_header(SVSM_IMAGE)[0])
    assert waveunit == 'nm'

########NEW FILE########
__FILENAME__ = test_jp2
"""
JPEG2000 reading tests
"""
from __future__ import absolute_import

#pylint: disable=C0103,R0904,W0201,W0212,W0232,E1103
import numpy as np
import pytest
from sunpy.data.test import AIA_193_JP2
from sunpy.io.jp2 import get_header
from sunpy.io.header import FileHeader
from sunpy.map import GenericMap
from sunpy.map import Map

# SunPy's JPEG2000 capabilities rely on the glymur library.  First we check to
# make sure that glymur imports correctly before proceeding.
try:
    import glymur
except ImportError:
    glymur_imports = False
else:
    glymur_imports = True

@pytest.mark.skipif("glymur_imports is False")
def test_read_data():
    """Tests the reading of the JP2 data"""
    data = glymur.Jp2k(AIA_193_JP2).read()
    assert isinstance(data, np.ndarray)

@pytest.mark.skipif("glymur_imports is False")
def test_read_header():
    """Tests the reading of the JP2 header"""
    header = get_header(AIA_193_JP2)[0]
    assert isinstance(header, FileHeader)

@pytest.mark.skipif("glymur_imports is False")
def test_read_file():
    """Tests the reading of the complete JP2 file and its conversion into a
    SunPy map"""
    map_ = Map(AIA_193_JP2)
    assert isinstance(map_, GenericMap)

########NEW FILE########
__FILENAME__ = lightcurve
"""
LightCurve is a generic LightCurve class from which all other LightCurve classes 
inherit from.
"""
from __future__ import absolute_import

#pylint: disable=E1101,E1121,W0404,W0612,W0613
__authors__ = ["Keith Hughitt"]
__email__ = "keith.hughitt@nasa.gov"

import os.path
import shutil
import urllib2
import warnings
from datetime import datetime

import numpy as np
import matplotlib.pyplot as plt
import pandas

import sunpy
from sunpy.time import is_time, TimeRange, parse_time
from sunpy.util.cond_dispatch import ConditionalDispatch, run_cls
from sunpy.util.odict import OrderedDict

__all__ = ['LightCurve']

class LightCurve(object):
    """
    LightCurve(filepath)

    A generic light curve object.

    Parameters
    ----------
    args : filepath, url, or start and end dates
        The input for a LightCurve object should either be a filepath, a URL,
        or a date range to be queried for the particular instrument.

    Attributes
    ----------
    meta : string, dict
        The comment string or header associated with the light curve input
    data : pandas.DataFrame
        An pandas DataFrame prepresenting one or more fields as they vary with 
        respect to time.

    Examples
    --------
    >>> import sunpy
    >>> import datetime
    >>> import numpy as np

    >>> base = datetime.datetime.today()
    >>> dates = [base - datetime.timedelta(minutes=x) for x in range(0, 24 * 60)]

    >>> intensity = np.sin(np.arange(0, 12 * np.pi, step=(12 * np.pi) / 24 * 60))

    >>> light_curve = sunpy.lightcurve.LightCurve.create(
    ...    {"param1": intensity}, index=dates
    ... )

    >>> light_curve.peek()

    References
    ----------
    | http://pandas.pydata.org/pandas-docs/dev/dsintro.html

    """
    _cond_dispatch = ConditionalDispatch()
    create = classmethod(_cond_dispatch.wrapper())

    def __init__(self, data, meta=None):
        self.data = pandas.DataFrame(data)
	if meta == '' or meta is None:
	     self.meta = OrderedDict()
	else:	
             self.meta = OrderedDict(meta)
	
    
    @property
    def header(self):
        """
        Return the lightcurves metadata

        .. deprecated:: 0.4.0
            Use .meta instead
        """
        warnings.warn("""lightcurve.header has been renamed to lightcurve.meta
for compatability with map, please use meta instead""", Warning)
        return self.meta

    @classmethod
    def from_time(cls, time, **kwargs):
        '''Called by Conditional Dispatch object when valid time is passed as input to create method.'''
	date = parse_time(time)
        url = cls._get_url_for_date(date, **kwargs)
        filepath = cls._download(
            url, kwargs, err="Unable to download data for specified date"
        )
        return cls.from_file(filepath)

    @classmethod
    def from_range(cls, start, end, **kwargs):
        '''Called by Conditional Dispatch object when start and end time are passed as input to create method.'''
        url = cls._get_url_for_date_range(parse_time(start), parse_time(end))
        filepath = cls._download(
            url, kwargs, 
            err = "Unable to download data for specified date range"
        )
        result = cls.from_file(filepath)
        result.data = result.data.truncate(start,end)
        return result

    @classmethod
    def from_timerange(cls, timerange, **kwargs):
        '''Called by Conditional Dispatch object when time range is passed as input to create method.'''
        url = cls._get_url_for_date_range(timerange)
        filepath = cls._download(
            url, kwargs,
            err = "Unable to download data for specified date range"
        )
        result = cls.from_file(filepath)
        result.data = result.data.truncate(timerange.start(), timerange.end())
        return result

    @classmethod
    def from_file(cls, filename):
        '''Used to return Light Curve object by reading the given filename

	Parameters:
	    filename: Path of the file to be read.

	'''

        filename = os.path.expanduser(filename)
        meta, data = cls._parse_filepath(filename)
        if data.empty:
            raise ValueError("No data found!")
        else:               
            return cls(data, meta)

    @classmethod
    def from_url(cls, url, **kwargs):
        '''
	Downloads a file from the given url, reads and returns a Light Curve object.

	Parameters:
	    url : string 
	        Uniform Resource Locator pointing to the file.

	    kwargs :Dict
	        Dict object containing other related parameters to assist in download.

        '''
        try:
            filepath = cls._download(url, kwargs)
        except (urllib2.HTTPError, urllib2.URLError, ValueError):
            err = ("Unable to read location %s.") % url
            raise ValueError(err)
        return cls.from_file(filepath)

    @classmethod
    def from_data(cls, data, index=None, meta=None):
        '''
	Called by Conditional Dispatch object to create Light Curve object when corresponding data is passed
	to create method.
	'''

	return cls(
            pandas.DataFrame(data, index=index),
            meta
        )

    @classmethod
    def from_yesterday(cls):
        return cls.from_url(cls._get_default_uri())

    @classmethod
    def from_dataframe(cls, dataframe, meta=None):
        '''
	Called by Conditional Dispatch object to create Light Curve object when Pandas DataFrame is passed
	to create method.
	'''

	return cls(dataframe, meta)

    def plot(self, axes=None, **plot_args):
        """Plot a plot of the light curve

        Parameters
        ----------
        axes: matplotlib.axes object or None
            If provided the image will be plotted on the given axes. Else the 
            current matplotlib axes will be used.

        **plot_args : dict
            Any additional plot arguments that should be used
            when plotting the image.

        """

        #Get current axes
        if axes is None:
            axes = plt.gca()

        axes = self.data.plot(ax=axes, **plot_args)

        return axes

    def peek(self, **kwargs):
        """Displays the light curve in a new figure"""

        figure = plt.figure()

        self.plot(**kwargs)

        figure.show()

        return figure

    @staticmethod
    def _download(uri, kwargs, 
                  err='Unable to download data at specified URL'):
        """Attempts to download data at the specified URI"""
                    
        _filename = os.path.basename(uri).split("?")[0]
        
        # user specifies a download directory
        if "directory" in kwargs:
            download_dir = os.path.expanduser(kwargs["directory"])
        else:
            download_dir = sunpy.config.get("downloads", "download_dir")

        # overwrite the existing file if the keyword is present
        if "overwrite" in kwargs:
            overwrite = kwargs["overwrite"]
        else:
            overwrite = False

        # If the file is not already there, download it
        filepath = os.path.join(download_dir, _filename)

        if not(os.path.isfile(filepath)) or (overwrite and 
                                             os.path.isfile(filepath)):
            try:
                response = urllib2.urlopen(uri)
            except (urllib2.HTTPError, urllib2.URLError):
                raise urllib2.URLError(err)
            with open(filepath, 'wb') as fp:
                shutil.copyfileobj(response, fp)
        else:
            warnings.warn("Using existing file rather than downloading, use overwrite=True to override.", RuntimeWarning)

        return filepath

    @classmethod
    def _get_default_uri(cls):
        """Default data to load when none is specified"""
        msg = "No default action set for %s"
        raise NotImplementedError(msg % cls.__name__)

    @classmethod
    def _get_url_for_date(cls, date, **kwargs):
        """Returns a URL to the data for the specified date"""
        msg = "Date-based downloads not supported for for %s"
        raise NotImplementedError(msg % cls.__name__)

    @classmethod
    def _get_url_for_date_range(cls, *args, **kwargs):
        """Returns a URL to the data for the specified date range"""
        msg = "Date-range based downloads not supported for for %s"
        raise NotImplementedError(msg % cls.__name__)

    @staticmethod
    def _parse_csv(filepath):
        """Place holder method to parse CSV files."""
        msg = "Generic CSV parsing not yet implemented for LightCurve"
        raise NotImplementedError(msg)

    @staticmethod
    def _parse_fits(filepath):
        """Place holder method to parse FITS files."""
        msg = "Generic FITS parsing not yet implemented for LightCurve"
        raise NotImplementedError(msg)

    @classmethod
    def _parse_filepath(cls, filepath):
        """Check the file extension to see how to parse the file"""
        filename, extension = os.path.splitext(filepath)

        if extension.lower() in (".csv", ".txt"):
            return cls._parse_csv(filepath)
        else:
            return cls._parse_fits(filepath)

    def truncate(self, a, b=None):
        """Returns a truncated version of the timeseries object"""
        if isinstance(a, TimeRange):
            time_range = a
        else:
            time_range = TimeRange(a,b)

        truncated = self.data.truncate(time_range.start(), time_range.end())
        return LightCurve(truncated, self.meta.copy())

    def extract(self, a):
        """Extract a set of particular columns from the DataFrame"""
        # TODO allow the extract function to pick more than one column
        if isinstance(self, pandas.Series):
            return self
        else:
            return LightCurve(self.data[a], self.meta.copy())

    def time_range(self):
        """Returns the start and end times of the LightCurve as a TimeRange
        object"""
        return TimeRange(self.data.index[0], self.data.index[-1])

# What's happening here is the following: The ConditionalDispatch is just an
# unbound callable object, that is, it does not know which class it is attached
# to. What we do against that is return a wrapper and make that a classmethod -
# thus we get the class as whose member it is called as as the first argument,
# this is why in the type signature we always have type as the first type.

# We then use run_cls, which just returns a wrapper that interprets the first
# argument as the class the function should be called of. So,
# x = run_cls("foo") returns something that turns x(cls, 1) into cls.foo(1).
# Because this has *args, **kwargs as its signature we need to disable the
# check of ConditionalDispatch that makes sure the function and the
# conditional need to have the same signature - but they still do have to.

LightCurve._cond_dispatch.add(
    run_cls("from_time"),
    lambda cls, time, **kwargs: is_time(time),
    # type is here because the class parameter is a class,
    # i.e. an instance of type (which is the base meta-class).
    [type, (basestring, datetime, tuple)],
    False
)

LightCurve._cond_dispatch.add(
    run_cls("from_range"),
    lambda cls, time1, time2, **kwargs: is_time(time1) and is_time(time2),
    [type, (basestring, datetime, tuple), (basestring, datetime, tuple)],
    False
)

LightCurve._cond_dispatch.add(
    run_cls("from_timerange"),
    lambda cls, timerange, **kwargs: True,
    [type, TimeRange],
    False
)

LightCurve._cond_dispatch.add(
    run_cls("from_file"),
    lambda cls, filename: os.path.exists(os.path.expanduser(filename)),
    [type, basestring],
    False
)

LightCurve._cond_dispatch.add(
    run_cls("from_url"),
    lambda cls, url, **kwargs: True,
    [type, basestring],
    False
)

LightCurve._cond_dispatch.add(
    run_cls("from_data"),
    lambda cls, data, index=None, meta=None: True,
    [type, (list, dict, np.ndarray, pandas.Series), object, object],
    False
)

LightCurve._cond_dispatch.add(
    run_cls("from_dataframe"),
    lambda cls, dataframe, meta=None: True,
    [type, pandas.DataFrame, object],
    False
)

LightCurve._cond_dispatch.add(
    run_cls("from_yesterday"),
    lambda cls: True,
    [type],
    False
)

########NEW FILE########
__FILENAME__ = eve
# -*- coding: utf-8 -*-
"""Provides programs to process and analyze EVE data."""
from __future__ import absolute_import

import os
import numpy
from datetime import datetime  

import matplotlib.pyplot as plt
from pandas.io.parsers import read_csv
from os.path import basename

from sunpy.lightcurve import LightCurve
from sunpy.util.odict import OrderedDict

__all__ = ['EVELightCurve']

class EVELightCurve(LightCurve):
    """
    SDO EVE LightCurve.
    
    Examples
    --------
    >>> import sunpy
    
    >>> eve = sunpy.lightcurve.EVELightCurve.create()
    >>> eve = sunpy.lightcurve.EVELightCurve.create('2012/06/20')
    >>> eve = sunpy.lightcurve.EVELightCurve.create(sunpy.data.test.EVE_AVERAGES_CSV)
    >>> eve = sunpy.lightcurve.EVELightCurve.create("http://lasp.colorado.edu/eve/data_access/quicklook/quicklook_data/L0CS/LATEST_EVE_L0CS_DIODES_1m.txt")
    >>> eve.peek(subplots=True)
    
    References
    ----------
    | http://lasp.colorado.edu/home/eve/data/data-access/
    """

    def peek(self, column = None, **kwargs):
        figure = plt.figure()
        # Choose title if none was specified
        if not kwargs.has_key("title") and column is None:
            if len(self.data.columns) > 1:
                kwargs['title'] = 'EVE (1 minute data)'
            else:
                if self._filename is not None:
                    base = self._filename.replace('_', ' ')
                    kwargs['title'] = os.path.splitext(base)[0]
                else:
                    kwargs['title'] = 'EVE Averages'

        if column is None:
            self.plot(**kwargs)
        else:
            data = self.data[column]
            if not kwargs.has_key("title"):
                kwargs['title'] = 'EVE ' + column.replace('_', ' ')
            data.plot(**kwargs)
        figure.show()
        return figure
        
    @staticmethod
    def _get_default_uri():
        """Load latest level 0CS if no other data is specified"""
        return "http://lasp.colorado.edu/eve/data_access/evewebdata/quicklook/L0CS/LATEST_EVE_L0CS_DIODES_1m.txt"
    
    @staticmethod
    def _get_url_for_date(date):
        """Returns a URL to the EVE data for the specified date
        
            @NOTE: currently only supports downloading level 0 data
            .TODO: No data available prior to 2010/03/01!
        """
        base_url = 'http://lasp.colorado.edu/eve/data_access/evewebdata/quicklook/L0CS/SpWx/'
        return base_url + date.strftime('%Y/%Y%m%d') + '_EVE_L0CS_DIODES_1m.txt'
    
    @classmethod
    def _parse_csv(cls, filepath):
        """Parses an EVE CSV file"""
        cls._filename = basename(filepath)
        with open(filepath, 'rb') as fp:
            # Determine type of EVE CSV file and parse
            line1 = fp.readline()
            fp.seek(0)

            if line1.startswith("Date"):
                return cls._parse_average_csv(fp)
            elif line1.startswith(";"):
                return cls._parse_level_0cs(fp)
    
    @staticmethod
    def _parse_average_csv(fp):
        """Parses an EVE Averages file"""
        return "", read_csv(fp, sep=",", index_col=0, parse_dates=True)
    
    @staticmethod
    def _parse_level_0cs(fp):
        """Parses and EVE Level 0CS file"""
	is_missing_data = False      #boolean to check for missing data
	missing_data_val = numpy.nan
        header = []
        fields = []
        line = fp.readline()
        # Read header at top of file
        while line.startswith(";"):
            header.append(line)
	    if '; Missing data:' in line :     
		    is_missing_data = True
		    missing_data_val = line.split(':')[1].strip()

		   
            line = fp.readline()
	
	meta = OrderedDict()
	for hline in header :
		if hline == '; Format:\n' or hline == '; Column descriptions:\n':
			continue
		elif ('Created' in hline) or ('Source' in hline):
			meta[hline.split(':',1)[0].replace(';',' ').strip()] = hline.split(':',1)[1].strip()
		elif ':' in hline :
			meta[hline.split(':')[0].replace(';',' ').strip()] = hline.split(':')[1].strip()

        fieldnames_start = False
        for hline in header:
            if hline.startswith("; Format:"):
                fieldnames_start = False
            if fieldnames_start:
                fields.append(hline.split(":")[0].replace(';', ' ').strip())        
            if hline.startswith("; Column descriptions:"):
                fieldnames_start = True

        # Next line is YYYY DOY MM DD        
        date_parts = line.split(" ")
                
        year = int(date_parts[0])
        month = int(date_parts[2])
        day = int(date_parts[3])
        #last_pos = fp.tell()
        #line = fp.readline()
        #el = line.split()
        #len
        
        # function to parse date column (HHMM)
        parser = lambda x: datetime(year, month, day, int(x[0:2]), int(x[2:4]))

        data = read_csv(fp, sep="\s*", names=fields, index_col=0, date_parser=parser, header = None)
	if is_missing_data :   #If missing data specified in header
		data[data == float(missing_data_val)] = numpy.nan
	
        #data.columns = fields
        return meta, data

########NEW FILE########
__FILENAME__ = goes
# -*- coding: utf-8 -*-
"""Provides programs to process and analyze GOES X-ray data."""
from __future__ import absolute_import

import datetime
import matplotlib.dates
from matplotlib import pyplot as plt  
from astropy.io import fits as pyfits
from numpy import nan
from numpy import floor
from pandas import DataFrame

from sunpy.lightcurve import LightCurve
from sunpy.time import parse_time, TimeRange, is_time_in_given_format

__all__ = ['GOESLightCurve']

class GOESLightCurve(LightCurve):
    """
    GOES X-ray LightCurve. Provides GOES data back to 1981-01-01. Most recent data is
    usually available one or two days late.

    Examples
    --------
    >>> from sunpy import lightcurve as lc
    
    >>> goes = lc.GOESLightCurve.create()
    >>> goes = lc.GOESLightCurve.create('2012/06/01', '2012/06/05')
    >>> goes.peek()

    References
    ----------
    | http://umbra.nascom.nasa.gov/goes/fits/
    """

    def peek(self, title="GOES Xray Flux", **kwargs):
        """Plots GOES light curve is the usual manner"""
        figure = plt.figure()
        axes = plt.gca()

        dates = matplotlib.dates.date2num(self.data.index)

        axes.plot_date(dates, self.data['xrsa'], '-', 
                     label='0.5--4.0 $\AA$', color='blue', lw=2)
        axes.plot_date(dates, self.data['xrsb'], '-', 
                     label='1.0--8.0 $\AA$', color='red', lw=2)

        axes.set_yscale("log")
        axes.set_ylim(1e-9, 1e-2)
        axes.set_title(title)
        axes.set_ylabel('Watts m$^{-2}$')
        axes.set_xlabel(datetime.datetime.isoformat(self.data.index[0])[0:10])

        ax2 = axes.twinx()
        ax2.set_yscale("log")
        ax2.set_ylim(1e-9, 1e-2)
        ax2.set_yticks((1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2))
        ax2.set_yticklabels((' ','A','B','C','M','X',' '))

        axes.yaxis.grid(True, 'major')
        axes.xaxis.grid(False, 'major')
        axes.legend()

        # @todo: display better tick labels for date range (e.g. 06/01 - 06/05)
        formatter = matplotlib.dates.DateFormatter('%H:%M')
        axes.xaxis.set_major_formatter(formatter)

        axes.fmt_xdata = matplotlib.dates.DateFormatter('%H:%M')
        figure.autofmt_xdate()
        figure.show()

        return figure

    @classmethod
    def _get_default_uri(cls):
        """Retrieve latest GOES data if no other data is specified"""
        today = datetime.datetime.today()
        days_back = 3
        time_range = TimeRange(today - datetime.timedelta(days=days_back), today - datetime.timedelta(days=days_back-1))
        return cls._get_url_for_date_range(time_range)

    @classmethod
    def _get_goes_sat_num(self,start,end):
        """Parses the query time to determine which GOES satellite to use."""
       
        goes_operational={
        2:TimeRange('1981-01-01','1983-04-30'),
        5:TimeRange('1983-05-02','1984-07-31'),
        6:TimeRange('1983-06-01','1994-08-18'),
        7:TimeRange('1994-01-01','1996-08-13'),
        8:TimeRange('1996-03-21','2003-06-18'),
        9:TimeRange('1997-01-01','1998-09-08'),
        10:TimeRange('1998-07-10','2009-12-01'),
        11:TimeRange('2006-06-20','2008-02-15'),
        12:TimeRange('2002-12-13','2007-05-08'),
        13:TimeRange('2006-08-01','2006-08-01'),
        14:TimeRange('2009-12-02','2012-11-04'),
        15:TimeRange('2010-09-01',datetime.datetime.utcnow())}
	    
        sat_list = []
        for sat_num in goes_operational:
            if ((start > goes_operational[sat_num].start() and start < goes_operational[sat_num].end()) and
                (end > goes_operational[sat_num].start() and end < goes_operational[sat_num].end())):
                    #if true then the satellite with sat_num is available
                    sat_list.append(sat_num)
    
        if not sat_list:
            #if no satellites were found then raise an exception
            raise Exception, 'No operational GOES satellites within time range'
        else:
            return sat_list
        
    @staticmethod
    def _get_url_for_date_range(*args, **kwargs):
        """Returns a URL to the GOES data for the specified date.

        Parameters
        ----------
        args : TimeRange, datetimes, date strings
            Date range should be specified using a TimeRange, or start
            and end dates at datetime instances or date strings.
        satellite_number : int
            GOES satellite number (default = 15)
        data_type : string
            Data type to return for the particular GOES satellite. Supported
            types depend on the satellite number specified. (default = xrs_2s) 
        """
        # TimeRange
        if len(args) == 1 and isinstance(args[0], TimeRange):
            time_range = args[0]
            start = args[0].start()
            end = args[0].end()
        elif len(args) == 2:
            # Start & End date
            start = parse_time(args[0])
            end = parse_time(args[1])
            time_range = TimeRange(start, end)
            if end < start:
                raise ValueError('start time (argument 1) > end time (argument 2)')

        #find out which satellite and datatype to query from the query times
        sat_num = GOESLightCurve._get_goes_sat_num(start,end)
        base_url = 'http://umbra.nascom.nasa.gov/goes/fits/'

        if start < parse_time('1999/01/15'):
            url = (base_url + "%s/go%02d%s.fits") % (start.strftime("%Y"), 
                sat_num[0], start.strftime("%y%m%d"))
        else:
            url = (base_url + "%s/go%02d%s.fits") % (start.strftime("%Y"), 
                sat_num[0], start.strftime("%Y%m%d"))
            
        return url
        
    @staticmethod
    def _parse_fits(filepath):
        """Parses a GOES FITS file from http://umbra.nascom.nasa.gov/goes/fits/"""
        fits = pyfits.open(filepath)
        header = fits[0].header
        if len(fits) == 4:
            if is_time_in_given_format(fits[0].header['DATE-OBS'], '%d/%m/%Y'):
                start_time = datetime.datetime.strptime(fits[0].header['DATE-OBS'], '%d/%m/%Y')
            elif is_time_in_given_format(fits[0].header['DATE-OBS'], '%d/%m/%y'):
                start_time = datetime.datetime.strptime(fits[0].header['DATE-OBS'], '%d/%m/%y')
            else:
               raise ValueError("Date not recognized")
            xrsb = fits[2].data['FLUX'][0][:,0]
            xrsa = fits[2].data['FLUX'][0][:,1]
            seconds_from_start = fits[2].data['TIME'][0]
        elif 1 <= len(fits) <= 3:
            start_time = parse_time(header['TIMEZERO'])
            seconds_from_start = fits[0].data[0]
            xrsb = fits[0].data[1]
            xrsa = fits[0].data[2]
        else:
            raise ValueError("Don't know how to parse this file")
            
        times = [start_time + datetime.timedelta(seconds = int(floor(s)), 
                    microseconds = int((s - floor(s))*1e6)) for s in seconds_from_start]

        # remove bad values as defined in header comments
        xrsb[xrsb == -99999] = nan
        xrsa[xrsa == -99999] = nan

        # fix byte ordering
        newxrsa = xrsa.byteswap().newbyteorder()
        newxrsb = xrsb.byteswap().newbyteorder()
        
        data = DataFrame({'xrsa': newxrsa, 'xrsb': newxrsb}, index=times)
        
        return header, data

########NEW FILE########
__FILENAME__ = logical
# -*- coding: utf-8 -*-
"""Provides a logical lightcurve.  Only two values are allowed - True or False.
Useful for keeping track of when an event occurred, usually labeled as 
"True"."""
from __future__ import absolute_import

import numpy as np

from sunpy.lightcurve import LightCurve
from scipy.ndimage import label
from sunpy.time import TimeRange

__all__ = ['LogicalLightCurve']

#
#
# Logical Lightcurve
# TODO
# Change the init to accept a list of TimeRange objects.  Durations between the
# start and end time of each TimeRange object are labeled 'True'.
class LogicalLightCurve(LightCurve):
    """
    Logical LightCurve.
    
    Originated from a need to analyze the times of HEK
    results, where 'True' indicates an event was observed, and 'False'
    indicates an event was not observed.
    
    Examples
    --------
    >>> import sunpy.lightcurve as lightcurve
    >>> import datetime
    
    >>> base = datetime.datetime.today()
    >>> dates = [base - datetime.timedelta(minutes=x) for x in range(0, 24 * 60)]
    >>> z = [True for x in range(0, 24 * 60)]
    >>> light_curve = lightcurve.LogicalLightCurve.create({"param1": z}, index=dates)
    """

    def complement(self):
        """ Define the complement of the passed lightcurve """
        return LogicalLightCurve.create(np.invert(self.data),
                                        header = self.header)

    def times(self):
        """Label all the periods of time that have the value 'True'. Return
        a list of TimeRange objects """

        labeling = label(self.data)
        timeranges = []
        for i in xrange(1, labeling[1]+1):
            eventindices = (labeling[0] == i).nonzero()
            timeranges.append( TimeRange(self.data.index[ eventindices[0][0] ],
                                         self.data.index[ eventindices[0][-1] ]) )
        return timeranges

########NEW FILE########
__FILENAME__ = lyra
# -*- coding: utf-8 -*-
"""Provides programs to process and analyze PROBA2/LYRA data."""
from __future__ import absolute_import

import datetime
import urlparse
import sys

from matplotlib import pyplot as plt
from astropy.io import fits
import pandas

from sunpy.lightcurve import LightCurve 
from sunpy.time import parse_time
from sunpy.util.odict import OrderedDict

__all__ = ['LYRALightCurve']

class LYRALightCurve(LightCurve):
    """
    Proba-2 LYRA LightCurve.

    Examples
    --------
    >>> import sunpy
    
    >>> lyra = sunpy.lightcurve.LYRALightCurve.create()
    >>> lyra = sunpy.lightcurve.LYRALightCurve.create('~/Data/lyra/lyra_20110810-000000_lev2_std.fits')
    >>> lyra = sunpy.lightcurve.LYRALightCurve.create('2011/08/10')
    >>> lyra = sunpy.lightcurve.LYRALightCurve.create("http://proba2.oma.be/lyra/data/bsd/2011/08/10/lyra_20110810-000000_lev2_std.fits")
    >>> lyra.peek()

    References
    ----------
    | http://proba2.sidc.be/data/LYRA
    """

    def peek(self, names=3, **kwargs):
        """Plots the LYRA data

        See: http://pandas.sourceforge.net/visualization.html
        """
        lyranames = (('Lyman alpha','Herzberg cont.','Al filter','Zr filter'),
                 ('120-123nm','190-222nm','17-80nm + <5nm','6-20nm + <2nm'))

        # Choose title if none was specified
        #if not kwargs.has_key("title"):
        #    if len(self.data.columns) > 1:
        #        kwargs['title'] = 'LYRA data'
        #    else:
        #        if self._filename is not None:
        #            base = self._filename
        #            kwargs['title'] = os.path.splitext(base)[0]
        #        else:
        #            kwargs['title'] = 'LYRA data'

        """Shows a plot of all four light curves"""
        figure = plt.figure()
        plt.subplots_adjust(left=0.17,top=0.94,right=0.94,bottom=0.15)
        axes = plt.gca()
        
        axes = self.data.plot(ax=axes, subplots=True, sharex=True, **kwargs)
        #plt.legend(loc='best')

        for i, name in enumerate(self.data.columns):
            if names < 3:
                name = lyranames[names][i]
            else:
                name = lyranames[0][i] + ' \n (' + lyranames[1][i] + ')'
            axes[i].set_ylabel( "%s %s" % (name, "\n (W/m**2)"),fontsize=9.5)

        axes[0].set_title("LYRA ("+ self.data.index[0].strftime('%Y-%m-%d') +")")
        axes[-1].set_xlabel("Time")
        for axe in axes:
            axe.locator_params(axis='y',nbins=6)

        figure.show()

        return figure


    @staticmethod
    def _get_url_for_date(date):
        """Returns a URL to the LYRA data for the specified date
        """
        dt = parse_time(date or datetime.datetime.utcnow())

        # Filename
        filename = "lyra_%s000000_lev%d_%s.fits" % (dt.strftime('%Y%m%d-'),
                                                    2, 'std')
        # URL
        base_url = "http://proba2.oma.be/lyra/data/bsd/"
        url_path = urlparse.urljoin(dt.strftime('%Y/%m/%d/'), filename)
        return urlparse.urljoin(base_url, url_path)

    @classmethod
    def _get_default_uri(cls):
        """Look for and download today's LYRA data"""
        return cls._get_url_for_date(datetime.datetime.utcnow())

    @staticmethod
    def _parse_fits(filepath):
        """Loads LYRA data from a FITS file"""
        # Open file with PyFITS
        hdulist = fits.open(filepath)
        fits_record = hdulist[1].data
        #secondary_header = hdulist[1].header

        # Start and end dates.  Different LYRA FITS files have
        # different tags for the date obs.
        if 'date-obs' in hdulist[0].header:
            start_str = hdulist[0].header['date-obs']
        elif 'date_obs' in hdulist[0].header:
            start_str = hdulist[0].header['date_obs']
        #end_str = hdulist[0].header['date-end']

        #start = datetime.datetime.strptime(start_str, '%Y-%m-%dT%H:%M:%S.%f')
        start = parse_time(start_str)
        #end = datetime.datetime.strptime(end_str, '%Y-%m-%dT%H:%M:%S.%f')

        # First column are times
        times = [start + datetime.timedelta(0, n) for n in fits_record.field(0)]

        # Rest of columns are the data
        table = {}

        for i, col in enumerate(fits_record.columns[1:-1]):
            #temporary patch for big-endian data bug on pandas 0.13
            if fits_record.field(i+1).dtype.byteorder == '>' and sys.byteorder =='little':
                table[col.name] = fits_record.field(i + 1).byteswap().newbyteorder()
            else:
                table[col.name] = fits_record.field(i + 1)

        # Return the header and the data
        return OrderedDict(hdulist[0].header), pandas.DataFrame(table, index=times)



########NEW FILE########
__FILENAME__ = noaa
# -*- coding: utf-8 -*-
"""Provides programs to process and analyze NOAA Solar Cycle data."""
from __future__ import absolute_import

import datetime

import matplotlib
from matplotlib import pyplot as plt  
from pandas.io.parsers import read_csv
import numpy as np

import sunpy
from sunpy.lightcurve import LightCurve
from sunpy.time import parse_time, TimeRange

__all__ = ['NOAAIndicesLightCurve', 'NOAAPredictIndicesLightCurve']

class NOAAIndicesLightCurve(LightCurve):
    """NOAA Solar Cycle monthly indices. 
        
    Solar activity is measured by a number of different values. The NOAA Solar Weather
    Prediction Center (SWPC) publishes the following indices. All of these indices are 
    also provided as a 13-month running smoothed value.
      
    * The SWO sunspot number is issued by the NOAA Space Weather Prediction Center (SWPC) 
    * The RI sunspot number is the official International Sunspot Number and is issued by the `Solar Influence Data Analysis Center (SDIC) <http://sidc.oma.be>`_ in Brussels, Belgium.
    * The ratio between the SWO and RI indices.
    * Radio flux at 10.7 cm is produced by `Penticon/Ottawa <http://www.ngdc.noaa.gov/stp/solar/flux.html>`_ and the units are in sfu.
    * The Ap Geomagnetic Index is produced by the United States Air Force (USAF).

    Examples
    --------
    >>> from sunpy import lightcurve as lc
    >>> noaa = lc.NOAAIndicesLightCurve.create()
    >>> noaa.peek()

    References
    ----------
    | http://www.swpc.noaa.gov/Data/index.html#indices
    | http://www.swpc.noaa.gov/ftpdir/weekly/README3
    | http://www.swpc.noaa.gov/ftpdir/weekly/RecentIndices.txt
    | http://www.swpc.noaa.gov/SolarCycle/
    """

    def peek(self, axes=None, type='sunspot SWO', **plot_args):
        """Plots NOAA Indices as a function of time"""
        figure = plt.figure()
        axes = plt.gca()
       
        if type == 'sunspot SWO':
            axes = self.data['sunspot SWO'].plot()
            self.data['sunspot SWO smooth'].plot()
            axes.set_ylabel('Sunspot Number')
        if type == 'sunspot RI':
            axes = self.data['sunspot RI'].plot()
            self.data['sunspot RI smooth'].plot()
            axes.set_ylabel('Sunspot Number')
        if type == 'sunspot compare':
            axes = self.data['sunspot RI'].plot()
            self.data['sunspot SWO'].plot()
            axes.set_ylabel('Sunspot Number')
        if type == 'radio':
            axes = self.data['radio flux'].plot()
            self.data['radio flux smooth'].plot()
            axes.set_ylabel('Radio Flux [sfu]')
        if type == 'geo':
            axes = self.data['geomagnetic ap'].plot()
            self.data['geomagnetic ap smooth'].plot()
            axes.set_ylabel('Geomagnetic AP Index')
     
        axes.set_ylim(0)
        axes.set_title('Solar Cycle Progression')

        axes.yaxis.grid(True, 'major')
        axes.xaxis.grid(True, 'major')
        axes.legend()
       
        figure.show()
        return figure

    @classmethod
    def _get_default_uri(cls):
        """Return the url to download indices"""
        return "http://www.swpc.noaa.gov/ftpdir/weekly/RecentIndices.txt"

    @staticmethod
    def _get_url_for_date_range(*args, **kwargs):
        """Returns a URL for the specified date."""
        return NOAAIndicesLightCurve._get_default_uri()
        
    @staticmethod
    def _parse_csv(filepath):
        """Parses an NOAA indices csv"""
        header = []
        with open(filepath, 'r') as fp:
            line = fp.readline()
            # Read header at top of file
            while line.startswith((":", "#")):
                header += line
                line = fp.readline()	
            fields = ('yyyy', 'mm', 'sunspot SWO', 'sunspot RI', 'sunspot ratio', 'sunspot SWO smooth', 'sunspot RI smooth', 'radio flux', 'radio flux smooth', 'geomagnetic ap', 'geomagnetic smooth')
            data = read_csv(fp, delim_whitespace=True, names = fields, comment='#', dtype={'yyyy':np.str, 'mm':np.str})
            data = data.dropna(how='any')
            timeindex = [datetime.datetime.strptime(x + '/' + y, '%Y/%m') for x,y in zip(data['yyyy'], data['mm'])]
            data['time']=timeindex
            data = data.set_index('time')
            data = data.drop('mm',1)
            data = data.drop('yyyy',1)
            return {'comments': header}, data

class NOAAPredictIndicesLightCurve(LightCurve):
    """NOAA Solar Cycle Predicted Progression

    The predictions are updated monthly and are produced by ISES. Observed values are 
    initially the preliminary values which are replaced with the final values as they 
    become available.
        
    The following predicted values are available.
              
    * The predicted RI sunspot number is the official International Sunspot Number and is issued by the `Solar Influence Data Analysis Center (SDIC) <http://sidc.oma.be>`_ in Brussels, Belgium.  
    * The predicted radio flux at 10.7 cm is produced by `Penticon/Ottawa <http://www.ngdc.noaa.gov/stp/solar/flux.html>`_ and the units are in sfu.
    
    Examples
    --------
    >>> from sunpy import lightcurve as lc
    >>> noaa = lc.NOAAPredictIndicesLightCurve.create()
    >>> noaa.peek()

    References
    ----------
    | http://www.swpc.noaa.gov/Data/index.html#indices
    | http://www.swpc.noaa.gov/ftpdir/weekly/Predict.txt
    | http://www.swpc.noaa.gov/SolarCycle/
    """

    def peek(self, axes=None, **plot_args):
        """Plots NOAA Indices as a function of time"""
        figure = plt.figure()
        axes = plt.gca()

        axes = self.data['sunspot'].plot(color='b')
        self.data['sunspot low'].plot(linestyle='--', color='b')
        self.data['sunspot high'].plot(linestyle='--', color='b')

        axes.set_ylim(0)
        axes.set_title('Solar Cycle Sunspot Number Prediction')
        axes.set_ylabel('Sunspot Number')
        #axes.set_xlabel(datetime.datetime.isoformat(self.data.index[0])[0:10])

        axes.yaxis.grid(True, 'major')
        axes.xaxis.grid(True, 'major')
        axes.legend()
        
        figure.show()
        return figure

    @classmethod
    def _get_default_uri(cls):
        """Return the url to download indices"""
        return "http://www.swpc.noaa.gov/ftpdir/weekly/Predict.txt"

    @staticmethod
    def _get_url_for_date_range(*args, **kwargs):
        """Returns a URL for the specified date."""
        return NOAAPredictIndicesLightCurve._get_default_uri()

    @staticmethod
    def _parse_csv(filepath):
        """Parses an NOAA indices csv"""
        header = ''
        with open(filepath, 'r') as fp:
            line = fp.readline()
            # Read header at top of file
            while line.startswith((":", "#")):
                header += line
                line = fp.readline()
            fields = ('yyyy', 'mm', 'sunspot', 'sunspot low', 'sunspot high', 'radio flux', 'radio flux low', 'radio flux high')
            data = read_csv(filepath, delim_whitespace=True, names = fields, comment='#', skiprows=2, dtype={'yyyy':np.str, 'mm':np.str})
            data = data.dropna(how='any')
            timeindex = [datetime.datetime.strptime(x + '/' + y, '%Y/%m') for x,y in zip(data['yyyy'], data['mm'])]
            data['time']=timeindex
            data = data.set_index('time')
            data = data.drop('mm',1)
            data = data.drop('yyyy',1)
            return {'comments': header}, data

########NEW FILE########
__FILENAME__ = norh
"""Provides programs to process and analyse NoRH lightcurve data."""

from __future__ import absolute_import

import datetime
import urlparse
import numpy as np
import matplotlib.pyplot as plt

from astropy.io import fits
import pandas

import sunpy
from sunpy.lightcurve import LightCurve
from sunpy.time import parse_time
from sunpy.util.odict import OrderedDict


__all__ = ['NoRHLightCurve']

class NoRHLightCurve(LightCurve):
    """
    Nobeyama Radioheliograph LightCurve.

    Examples
    --------
    >>> import sunpy
    
    >>> norh = sunpy.lightcurve.NoRHLightCurve.create('~/Data/norh/tca110607')
    >>> norh = sunpy.lightcurve.NoRHLightCurve.create('2011/08/10')
    >>> norh = sunpy.lightcurve.NoRHLightCurve.create('2011/08/10',wavelength='34')
    >>> norh.peek()

    References
    ----------
    | http://solar.nro.nao.ac.jp/norh/
    """

    def peek(self, **kwargs):
        """Plots the NoRH lightcurve"""
        plt.figure()
        axes = plt.gca()
        data_lab=self.meta['OBS-FREQ'][0:2] + ' ' + self.meta['OBS-FREQ'][2:5]
        axes.plot(self.data.index,self.data,label=data_lab)
        axes.set_yscale("log")
        axes.set_ylim(1e-4,1)
        axes.set_title('Nobeyama Radioheliograph')
        axes.set_xlabel('Start time: ' + self.data.index[0].strftime('%Y-%m-%d %H:%M:%S UT'))
        axes.set_ylabel('Correlation')
        axes.legend()
        plt.show()

    @classmethod
    def _get_url_for_date(cls,date, **kwargs):
        """This method retrieves the url for NoRH correlation data for the given date."""
        #default urllib password anonymous@ is not accepted by the NoRH FTP server.
        #include an accepted password in base url
        baseurl='ftp://anonymous:mozilla@example.com@solar-pub.nao.ac.jp/pub/nsro/norh/data/tcx/'
        #date is a datetime object
        if 'wavelength' in kwargs:
            if kwargs['wavelength'] == '34':
                final_url=urlparse.urljoin(baseurl,date.strftime('%Y/%m/' + 'tcz' + '%y%m%d')) 
        else:
            final_url=urlparse.urljoin(baseurl, date.strftime('%Y/%m/' + 'tca' + '%y%m%d')) 
        
        return final_url

    @staticmethod
    def _parse_fits(filepath):
        """This method parses NoRH tca and tcz correlation files."""
        hdulist=fits.open(filepath)
        header=OrderedDict(hdulist[0].header)
        #for these NoRH files, the time series data is recorded in the primary HDU
        data=hdulist[0].data

        #No explicit time array in FITS file, so construct the time array from the FITS header
        obs_start_time=parse_time(header['DATE-OBS'] + 'T' + header['CRVAL1'])
        length=len(data)
        cadence=np.float(header['CDELT1'])
        sec_array=np.linspace(0, length-1, (length/cadence))

        norh_time=[]
        for s in sec_array:
            norh_time.append(obs_start_time + datetime.timedelta(0,s))

        return header, pandas.DataFrame(data, index=norh_time)

########NEW FILE########
__FILENAME__ = rhessi
# -*- coding: utf-8 -*-
"""Provides programs to process and analyze RHESSI X-ray data."""
from __future__ import absolute_import

import datetime
import matplotlib.dates
import matplotlib.pyplot as plt
from pandas import DataFrame

from sunpy.lightcurve import LightCurve
from sunpy.time import TimeRange, parse_time
from sunpy.instr import rhessi

__all__ = ['RHESSISummaryLightCurve']


class RHESSISummaryLightCurve(LightCurve):
    """
    RHESSI X-ray Summary LightCurve.

    Examples
    --------
    >>> from sunpy import lightcurve as lc
    >>> rhessi = lc.RHESSISummaryLightCurve.create()
    >>> rhessi = lc.RHESSISummaryLightCurve.create('2012/06/01', '2012/06/05')
    >>> rhessi.peek()

    References
    ----------
    | http://sprg.ssl.berkeley.edu/~jimm/hessi/hsi_obs_summ_soc.html#hsi_obs_summ_rate
    """

    def peek(self, title="RHESSI Observing Summary Count Rate", **kwargs):
        """Plots RHESSI Count Rate light curve"""
        figure = plt.figure()
        axes = plt.gca()

        dates = matplotlib.dates.date2num(self.data.index)

        lc_linecolors = ('black', 'pink', 'green', 'blue', 'brown', 'red',
                         'navy', 'orange', 'green')

        for item, frame in self.data.iteritems():
            axes.plot_date(dates, frame.values, '-', label=item, lw=2)

        axes.set_yscale("log")
        axes.set_xlabel(datetime.datetime.isoformat(self.data.index[0])[0:10])

        axes.set_title('RHESSI Observing Summary Count Rates, Corrected')
        axes.set_ylabel('Corrected Count Rates s$^{-1}$ detector$^{-1}$')

        axes.yaxis.grid(True, 'major')
        axes.xaxis.grid(False, 'major')
        axes.legend()

        # @todo: display better tick labels for date range (e.g. 06/01 - 06/05)
        formatter = matplotlib.dates.DateFormatter('%H:%M')
        axes.xaxis.set_major_formatter(formatter)

        axes.fmt_xdata = matplotlib.dates.DateFormatter('%H:%M')
        figure.autofmt_xdate()
        figure.show()

    @classmethod
    def _get_default_uri(cls):
        """Retrieve the latest RHESSI data."""
        today = datetime.datetime.today()
        days_back = 3
        time_range = TimeRange(today - datetime.timedelta(days=days_back),
                               today - datetime.timedelta(days=days_back - 1))
        return cls._get_url_for_date_range(time_range)

    @staticmethod
    def _get_url_for_date_range(*args, **kwargs):
        """Returns a URL to the RHESSI data for the specified date range.

        Parameters
        ----------
        args : TimeRange, datetimes, date strings
            Date range should be specified using a TimeRange, or start
            and end dates at datetime instances or date strings.
        """
        if len(args) == 1 and isinstance(args[0], TimeRange):
            time_range = args[0]
        elif len(args) == 2:
            time_range = TimeRange(parse_time(args[0]), parse_time(args[1]))
            if time_range.end() < time_range.start():
                raise ValueError('start time > end time')
        url = rhessi.get_obssum_filename(time_range)
        return url

    @staticmethod
    def _parse_fits(filepath):
        """Parses a RHESSI FITS file"""
        header, d = rhessi.parse_obssumm_file(filepath)
        data = DataFrame(d['data'], columns=d['labels'], index=d['time'])

        return header, data

########NEW FILE########
__FILENAME__ = test_eve
"""
EVE tests
"""
from __future__ import absolute_import

import pytest

#pylint: disable=C0103,R0904,W0201,W0232,E1103
import sunpy
import sunpy.lightcurve
from sunpy.data.test import (EVE_AVERAGES_CSV)

@pytest.mark.online
def test_eve():
    eve = sunpy.lightcurve.EVELightCurve.create('2013/04/15')
    assert isinstance(eve, sunpy.lightcurve.EVELightCurve)

@pytest.mark.online
def test_txt():
    """Check support for parsing EVE TXT files """
    eve = sunpy.lightcurve.EVELightCurve.create(
    "http://lasp.colorado.edu/eve/data_access/quicklook/quicklook_data/L0CS/LATEST_EVE_L0CS_DIODES_1m.txt") 
    assert isinstance(eve, sunpy.lightcurve.EVELightCurve)        

def test_csv_parsing():
    """Check support for parsing EVE CSV files"""
    csv = sunpy.lightcurve.EVELightCurve.create(EVE_AVERAGES_CSV)
    assert isinstance(csv, sunpy.lightcurve.sources.eve.EVELightCurve)

########NEW FILE########
__FILENAME__ = test_goes

"""
GOES LightCurve Tests
"""
from __future__ import absolute_import

import pytest
import sunpy.lightcurve
from sunpy.time import TimeRange

timerange_a = TimeRange('2008/06/01', '2008/06/02')
timerange_b = TimeRange('1995/06/03', '1995/06/04')

class TestGOESLightCurve():
    
    @pytest.mark.online
    def test_goes_range(self):
        """Test creation with two times"""
        lc1 = sunpy.lightcurve.GOESLightCurve.create(timerange_a.start(), timerange_a.end())
        assert isinstance(lc1, sunpy.lightcurve.GOESLightCurve)

    @pytest.mark.online
    def test_goes_timerange(self):
        """Test creation with a TimeRange"""
        lc1 = sunpy.lightcurve.GOESLightCurve.create(timerange_a)
        assert isinstance(lc1, sunpy.lightcurve.GOESLightCurve)
    
    @pytest.mark.online
    def test_goes_default(self):
        """Test creation with no input"""
        lc1 = sunpy.lightcurve.GOESLightCurve.create()
        assert isinstance(lc1, sunpy.lightcurve.GOESLightCurve)
    
    @pytest.mark.online
    def test_data(self):
        """Test presence of data"""
        lc1 = sunpy.lightcurve.GOESLightCurve.create(timerange_b)
        lc2 = sunpy.lightcurve.GOESLightCurve.create(timerange_a)
        assert lc1.data.empty == False
        assert lc2.data.empty == False

    @pytest.mark.online
    def test_header(self):
        """Test presence of GOES satellite number in header"""
        lc1 = sunpy.lightcurve.GOESLightCurve.create(timerange_b)
        lc2 = sunpy.lightcurve.GOESLightCurve.create(timerange_a)
        assert lc1.header['TELESCOP'] == 'GOES 7'
        assert lc2.header['TELESCOP'] == 'GOES 10'
    
    def test_goes_url(self):
        """Test creation with url"""
        url = 'http://umbra.nascom.nasa.gov/goes/fits/1995/go07950603.fits'
        lc1 = sunpy.lightcurve.GOESLightCurve.create(url)
        assert isinstance(lc1, sunpy.lightcurve.GOESLightCurve)
    
    @pytest.mark.online
    def compare(self,lc1,lc2):
        try:
            (lc1.data == lc2.data)
        except:
            raise Exception

    @pytest.mark.online
    def test_filename(self):
        """Compare data from two different time ranges to make 
        sure they are not the same"""
        lc1 = sunpy.lightcurve.GOESLightCurve.create(timerange_a)
        lc2 = sunpy.lightcurve.GOESLightCurve.create(timerange_b)
        #If the dataframes are non-idential it raises an error, if they are
        #identical it returns True
        with pytest.raises((Exception)):
            self.compare(lc1, lc2)
    
    def test_goes_sat_numbers(self):
        """Test the ability to return GOES satellite availability"""
        g = sunpy.lightcurve.GOESLightCurve
        assert g._get_goes_sat_num(timerange_a.start(), timerange_a.end()) == [10]
        assert g._get_goes_sat_num(timerange_b.start(), timerange_b.end()) == [7]
    
    def test_get_url(self):
        """Test the getting of urls"""
        g = sunpy.lightcurve.GOESLightCurve
        # time ranges create urls with either 4 digit or 2 digit years
        assert g._get_url_for_date_range(timerange_b) == 'http://umbra.nascom.nasa.gov/goes/fits/1995/go07950603.fits'
        assert g._get_url_for_date_range(timerange_a) == 'http://umbra.nascom.nasa.gov/goes/fits/2008/go1020080601.fits'
########NEW FILE########
__FILENAME__ = test_lightcurve
"""
Generic LightCurve Tests
"""
from __future__ import absolute_import

#
# @TODO:
#   time deltas instead of datetimes?

#pylint: disable=C0103,R0904,W0201,W0232,E1101,E1103
import numpy as np
import pytest
import datetime
import sunpy
import sunpy.lightcurve
from sunpy.data.test import (EVE_AVERAGES_CSV)

# Generate input test data
base = datetime.datetime.today()
dates = [base - datetime.timedelta(minutes=x) for x in range(0, 24 * 60)]

@pytest.mark.parametrize(("data", "index"), [
    (range(24 * 60), dates),
    (np.arange(24 * 60), dates),
    ({"param": range(24 * 60)}, dates)
])
def test_input(data, index):
    """Tests different types of expected input"""
    sunpy.lightcurve.LightCurve.create(data, index=index)

@pytest.mark.parametrize(("bad_input"), [
    (None),
    (EVE_AVERAGES_CSV)
])
def test_unimplemented(bad_input):
    """Tests input that has not been implemented for the generic LC class"""
    with pytest.raises((TypeError, NotImplementedError)):
        sunpy.lightcurve.LightCurve.create(bad_input)

########NEW FILE########
__FILENAME__ = test_lyra
"""
Lyra Tests
"""
from __future__ import absolute_import

import pytest

#pylint: disable=C0103,R0904,W0201,W0232,E1103
import sunpy
import matplotlib as mpl
from matplotlib.testing.decorators import cleanup

@pytest.mark.online
def test_lyra():
    lyra = sunpy.lightcurve.LYRALightCurve.create(
    "http://proba2.oma.be/lyra/data/bsd/2011/08/10/lyra_20110810-000000_lev2_std.fits")
    assert isinstance(lyra, sunpy.lightcurve.LYRALightCurve)

#@cleanup    
#def test_peek():
#    lyra = sunpy.lightcurve.LYRALightCurve.create(
#    "http://proba2.oma.be/lyra/data/bsd/2011/08/10/lyra_20110810-000000_lev2_std.fits")
#    assert isinstance(lyra.peek(),mpl.figure.Figure)

########NEW FILE########
__FILENAME__ = test_noaa
"""
NOAA LightCurve Tests
"""
from __future__ import absolute_import

import pytest
import sunpy.lightcurve
from sunpy.time import TimeRange

timerange_a = TimeRange('2004/01/01', '2007/01/01')

class TestNOAAIndicesLightCurve():
    
    @pytest.mark.online
    def test_create(self):
       lc = sunpy.lightcurve.NOAAIndicesLightCurve.create()
       assert isinstance(lc, sunpy.lightcurve.NOAAIndicesLightCurve)
    
    @pytest.mark.online
    def test_isempty(self):
        lc = sunpy.lightcurve.NOAAIndicesLightCurve.create()
        assert lc.data.empty == False

    @pytest.mark.online
    def test_url(self):
        """Test creation with url"""
        url = 'http://www.swpc.noaa.gov/ftpdir/weekly/RecentIndices.txt'
        lc1 = sunpy.lightcurve.NOAAIndicesLightCurve.create(url)
        assert isinstance(lc1, sunpy.lightcurve.NOAAIndicesLightCurve)

    @pytest.mark.online
    def test_goes_timerange(self):
        """Test creation with a TimeRange"""
        lc1 = sunpy.lightcurve.NOAAIndicesLightCurve.create(timerange_a)
        assert isinstance(lc1, sunpy.lightcurve.NOAAIndicesLightCurve)

    def test_get_url(self):
        """Test the getting of url"""
        g = sunpy.lightcurve.NOAAIndicesLightCurve
        assert g._get_url_for_date_range(timerange_a) == 'http://www.swpc.noaa.gov/ftpdir/weekly/RecentIndices.txt'
        
    @pytest.mark.online
    def test_header(self):
        """Test presence of GOES satellite number in header"""
        lc1 = sunpy.lightcurve.NOAAIndicesLightCurve.create()
        assert lc1.header.keys() == ['comments']

    
class TestNOAAPredictIndicesLightCurve():

    @pytest.mark.online
    def test_create(self):
        """Test creation with no input"""
        lc = sunpy.lightcurve.NOAAPredictIndicesLightCurve.create()
        assert isinstance(lc, sunpy.lightcurve.NOAAPredictIndicesLightCurve)
    
    @pytest.mark.online
    def test_isempty(self):
        """Test presence of data"""
        lc = sunpy.lightcurve.NOAAPredictIndicesLightCurve.create()
        assert lc.data.empty == False

    @pytest.mark.online
    def test_goes_timerange(self):
        """Test creation with a TimeRange"""
        lc1 = sunpy.lightcurve.NOAAPredictIndicesLightCurve.create(timerange_a)
        assert isinstance(lc1, sunpy.lightcurve.NOAAPredictIndicesLightCurve)

    @pytest.mark.online
    def test_url(self):
        """Test creation with url"""
        url = 'http://www.swpc.noaa.gov/ftpdir/weekly/Predict.txt'
        lc1 = sunpy.lightcurve.NOAAPredictIndicesLightCurve.create(url)
        assert isinstance(lc1, sunpy.lightcurve.NOAAPredictIndicesLightCurve)

    def test_get_url(self):
        """Test the getting of url"""
        g = sunpy.lightcurve.NOAAPredictIndicesLightCurve
        assert g._get_url_for_date_range(timerange_a) == 'http://www.swpc.noaa.gov/ftpdir/weekly/Predict.txt'
        
    @pytest.mark.online
    def test_header(self):
        """Test presence of GOES satellite number in header"""
        lc1 = sunpy.lightcurve.NOAAPredictIndicesLightCurve.create()
        assert lc1.header.keys() == ['comments']
########NEW FILE########
__FILENAME__ = test_norh
"""
NoRH Tests
"""
from __future__ import absolute_import

import pytest
import sunpy
import pandas

@pytest.mark.online
def test_norh():
    norh=sunpy.lightcurve.NoRHLightCurve.create('2012-07-06')
    assert isinstance(norh, sunpy.lightcurve.NoRHLightCurve)
    assert norh.meta['OBS-FREQ'] == '17GHZ'
    assert norh.time_range().start() == pandas.Timestamp('2012-07-05 21:59:50.710000')
    assert norh.time_range().end() == pandas.Timestamp('2012-07-06 06:19:49.710000')

    norh34=sunpy.lightcurve.NoRHLightCurve.create('2012-07-06',wavelength='34')
    assert isinstance(norh34, sunpy.lightcurve.NoRHLightCurve)
    assert norh34.meta['OBS-FREQ'] == '34GHZ'
    assert norh34.time_range().start() == pandas.Timestamp('2012-07-05 21:59:50.760000')
    assert norh34.time_range().end() == pandas.Timestamp('2012-07-06 06:19:49.760000')

########NEW FILE########
__FILENAME__ = test_rhessi

"""
RHESSI LightCurve Tests
"""
from __future__ import absolute_import

import pytest
import sunpy.lightcurve
from sunpy.time import TimeRange
from numpy import all


class TestRHESSISummaryLightCurve():

    @pytest.fixture
    def timerange_a(self):
        return TimeRange('2008/06/01', '2008/06/02')

    @pytest.fixture
    def timerange_b(self):
        return TimeRange('2004/06/03', '2004/06/04')

    @pytest.mark.online
    def test_hsi_range(self, timerange_a):
        """Test creation with two times"""
        lc1 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_a.start(), timerange_a.end())
        assert isinstance(lc1, sunpy.lightcurve.RHESSISummaryLightCurve)

    @pytest.mark.online
    def test_hsi_timerange(self, timerange_a):
        """Test creation with a TimeRange"""
        lc1 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_a)
        assert isinstance(lc1, sunpy.lightcurve.RHESSISummaryLightCurve)

    @pytest.mark.online
    def test_hsi_default(self):
        """Test creation with no input"""
        lc1 = sunpy.lightcurve.RHESSISummaryLightCurve.create()
        assert isinstance(lc1, sunpy.lightcurve.RHESSISummaryLightCurve)

    @pytest.mark.online
    def test_data(self, timerange_a, timerange_b):
        """Test presence of data"""
        lc1 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_b)
        lc2 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_a)
        assert not lc1.data.empty
        assert not lc2.data.empty

    @pytest.mark.online
    def test_header(self, timerange_b):
        """Test presence of TELESCOP in header"""
        lc1 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_b)
        assert lc1.header['TELESCOP'] == 'HESSI'

    @pytest.mark.online
    def test_hsi_url(self):
        """Test creation with url"""
        url = 'http://hesperia.gsfc.nasa.gov/hessidata/metadata/catalog/hsi_obssumm_20030302_146.fits'
        lc1 = sunpy.lightcurve.RHESSISummaryLightCurve.create(url)
        assert isinstance(lc1, sunpy.lightcurve.RHESSISummaryLightCurve)

    @pytest.mark.online
    def test_filename(self, timerange_a, timerange_b):
        """Compare data from two different time ranges to make
        sure they are not the same"""
        lc1 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_a)
        lc2 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_b)
        assert not all(lc1.data[lc1.data.columns[0]] == lc2.data[lc2.data.columns[0]])

    @pytest.mark.online
    def test_get_url(self, timerange_a, timerange_b):
        """Test the getting of urls"""
        g = sunpy.lightcurve.RHESSISummaryLightCurve
        assert g._get_url_for_date_range(timerange_a) == 'http://hesperia.gsfc.nasa.gov/hessidata/metadata/catalog/hsi_obssumm_20080601_068.fits'
        assert g._get_url_for_date_range(timerange_b) == 'http://hesperia.gsfc.nasa.gov/hessidata/metadata/catalog/hsi_obssumm_20040603_110.fits'

########NEW FILE########
__FILENAME__ = compositemap
"""A Composite Map class

Author: `Keith Hughitt <keith.hughitt@nasa.gov>`
"""
from __future__ import absolute_import

import matplotlib.pyplot as plt

from sunpy.map import GenericMap

from sunpy.util import expand_list

__all__ = ['CompositeMap']

__author__ = "Keith Hughitt"
__email__ = "keith.hughitt@nasa.gov"

class CompositeMap(object):
    """
    CompositeMap(map1 [,map2,..])

    A Composite Map class

    Parameters
    ----------
    args : [sunpy.map | string]
        One or more map of filepaths

    Methods
    -------
    add_map(map, zorder=None, alpha=1, levels=False)
        Adds a map to the CompositeMap
    remove_map(index)
        Removes and returns the map with the given index
    list_maps()
        Prints a list of the currently included maps
    get_alpha(index=None)
        Gets the alpha-channel value for a layer in the composite image
    get_zorder(index=None)
        Gets the layering preference (z-order) for a map within the composite.
    get_colors(index=None)
        Gets the colors for a map within the CompositeMap.
    get_norm(index=None)
        Gets the normalization for a map within the CompositeMap.
    get_levels(index=None)
        Gets the list of contour levels for a map within the CompositeMap
    set_norm(self, index, norm)
        Sets the norm for a layer in the composite image
    set_levels(index, levels, percent=False)
        Sets the contour levels for a layer in the CompositeMap       
    set_colors(index=None, cm)
        Sets the color map for a layer in the CompositeMap
    set_alpha(index=None, alpha)
        Sets the alpha-channel value for a layer in the CompositeMap
    set_zorder(index=None, zorder)
        Set the layering preference (z-order) for a map within the CompositeMap
    plot(figure=None, overlays=None, draw_limb=False, gamma=1.0, 
    draw_grid=False, colorbar=True, basic_plot=False,title="SunPy Plot", 
    matplot_args)
        Plots the composite map object using matplotlib

    Examples
    --------
    >>> import sunpy
    >>> sunpy.Map(sunpy.AIA_171_IMAGE, sunpy.RHESSI_IMAGE, composite=True)
    >>> comp_map = sunpy.Map(sunpy.AIA_171_IMAGE, sunpy.EIT_195_IMAGE,
                             composite=True)
    >>> comp_map.add_map(sunpy.Map(sunpy.RHESSI_IMAGE))
    >>> comp_map.peek()

    """    
    def __init__(self, *args, **kwargs):
        """
        Create a CompositeMap
        
        Parameters
        ----------
        Maps: SunPy Maps
            A sequence of maps
        """
        
        self._maps = expand_list(args)
        
        for m in self._maps:
            if not isinstance(m, GenericMap):
                raise ValueError(
                           'CompositeMap expects pre-constructed map objects.')
        
        # Default alpha and zorder values
        alphas = [1] * len(self._maps)
        zorders = range(0, 10 * len(self._maps), 10)
        levels = [False] * len(self._maps)
        
        # Set z-order and alpha values for the map     
        for i, m in enumerate(self._maps):
            m.zorder = zorders[i]
            m.alpha = alphas[i]
            m.levels = levels[i]

    def add_map(self, amap, zorder=None, alpha=1, levels=False):
        """Adds a map to the CompositeMap
        
        Parameters
        ----------
        map : sunpy.map
            Map instance to be added
        zorder : int
            The index to use when determining where the map should lie along
            the z-axis; maps with higher z-orders appear above maps with lower
            z-orders.
        alpha : float
            Opacity at which the map should be displayed. An alpha value of 0
            results in a fully transparent image while an alpha value of 1
            results in a fully opaque image. Values between result in semi-
            transparent images.

        """
        if zorder is None:
            zorder = max([m.zorder for m in self._maps]) + 10
        
        amap.zorder = zorder
        amap.alpha = alpha
        amap.levels = levels
        
        self._maps.append(amap)
        
    def remove_map(self, index):
        """Removes and returns the map with the given index"""
        return self._maps.pop(index)
    
    def list_maps(self):
        """Prints a list of the currently included maps"""
        print [m.__class__ for m in self._maps]
    
    def get_map(self, index):
        """ Returns the map with given index """
        return self._maps[index]
        
    def get_alpha(self, index=None):
        """Gets the alpha-channel value for a layer in the composite image"""
        if index is None:
            return [_map.alpha for _map in self._maps]
        else:
            return self._maps[index].alpha
        
    def get_zorder(self, index=None):
        """Gets the layering preference (z-order) for a map within the
        composite.
        """
        if index is None:
            return [_map.zorder for _map in self._maps]
        else:
            return self._maps[index].zorder

    def get_colors(self, index=None):
        """Gets the colors for a map within the compositemap."""
        if index is None:
            return [_map.cmap for _map in self._maps]
        else:
            return self._maps[index].cmap

    def get_mpl_color_normalizer(self, index=None):
        """Gets the color normalizer for a map within the
        composite.
        """
        if index is None:
            return [_map.mpl_color_normalizer for _map in self._maps]
        else:
            return self._maps[index].mpl_color_normalizer
            
    def get_levels(self, index=None):
        """Gets the list of contour levels for a map within the
        composite.
        """
        if index is None:
            return [_map.levels for _map in self._maps]
        else:
            return self._maps[index].levels

    def set_mpl_color_normalizer(self, index, norm):
        """Sets the normalizer for a layer in the composite image."""
        self._maps[index].mpl_color_normalizer = norm

    def set_levels(self, index, levels, percent=False):
        """Sets the contour levels for a layer in the composite image"""
        if percent is False: 
            self._maps[index].levels = levels
        else:
            self._maps[index].levels = [self._maps[index].max()*level/100.0 for level in levels]

    def set_colors(self, index, cm):
        """Sets the color map for a layer in the composite image"""
        self._maps[index].cmap = cm

    def set_alpha(self, index, alpha):
        """Sets the alpha-channel value for a layer in the composite image"""
        if 0 <= alpha <= 1:
            self._maps[index].alpha = alpha
        else:
            raise OutOfRangeAlphaValue("Alpha value must be between 0 and 1.")
        
    def set_zorder(self, index, zorder):
        """Set the layering preference (z-order) for a map within the
        composite.
        """
        self._maps[index].zorder = zorder

    def draw_limb(self, index=None, axes=None):
        """Draws a circle representing the solar limb 
        
            Parameters
            ----------
            index: integer
                Map index to use to plot limb.
                
            axes: matplotlib.axes object or None
                Axes to plot limb on or None to use current axes.
        
            Returns
            -------
            matplotlib.axes object
        """
        if index is None:
            for i,amap in enumerate(self._maps):
                if hasattr(amap,'rsun_arcseconds'):
                    index = i
                    break
                
        index_check = hasattr(self._maps[index],'rsun_arcseconds')
        if not index_check or index is None:
            raise ValueError("Specified index does not have all the required attributes to draw limb.")
            
        return self._maps[index].draw_limb(axes=axes)
        
    def draw_grid(self, index=None, axes=None, grid_spacing=20):
        """Draws a grid over the surface of the Sun
        
        Parameters
        ----------
        index: integer
            Index to determine which map to use to draw grid.
            
        axes: matplotlib.axes object or None
            Axes to plot limb on or None to use current axes.
        
        grid_spacing: float
            Spacing (in degrees) for longitude and latitude grid.
        
        Returns
        -------
        matplotlib.axes object
        """
        needed_attrs = ['rsun_meters', 'dsun', 'heliographic_latitude',
                            'heliographic_longitude']
        if index is None:
            for i, amap in enumerate(self._maps):
                if all([hasattr(amap,k) for k in needed_attrs]):
                    index = i
                    break
        
        index_check = all([hasattr(self._maps[index],k) for k in needed_attrs])
        if not index_check or index is None:
            raise ValueError("Specified index does not have all the required attributes to draw grid.")
        
        ax = self._maps[index].draw_grid(axes=axes, grid_spacing=grid_spacing)
        return ax
        
    def plot(self, axes=None, gamma=None, annotate=True, # pylint: disable=W0613
             title="SunPy Composite Plot", **matplot_args):
        """Plots the composite map object using matplotlib
        
        Parameters
        ----------
        axes: matplotlib.axes object or None
            If provided the image will be plotted on the given axes. Else the 
            current matplotlib axes will be used.
 
        gamma : float
            Gamma value to use for the color map

        annotate : bool
            If true, the data is plotted at it's natural scale; with
            title and axis labels.
            
        **matplot_args : dict
            Matplotlib Any additional imshow arguments that should be used
            when plotting the image.
            
        Returns
        -------
        ret : List
            List of axes image or quad contour sets that have been plotted.
        """
        
        #Get current axes
        if not axes:
            axes = plt.gca()
        
        if annotate:
            # x-axis label
            if self._maps[0].coordinate_system['x'] == 'HG':
                xlabel = 'Longitude [%s]' % self._maps[0].units['x']
            else:
                xlabel = 'X-position [%s]' % self._maps[0].units['x']
    
            # y-axis label
            if self._maps[0].coordinate_system['y'] == 'HG':
                ylabel = 'Latitude [%s]' % self._maps[0].units['y']
            else:
                ylabel = 'Y-position [%s]' % self._maps[0].units['y']
                
            axes.set_xlabel(xlabel)
            axes.set_ylabel(ylabel)
    
            axes.set_title(title)
        
        #Define a list of plotted objects
        ret = []
        # Plot layers of composite map
        for m in self._maps:
            # Parameters for plotting
            params = {
                "origin": "lower",
                "extent": m.xrange + m.yrange,
                "cmap": m.cmap,
                "norm": m.mpl_color_normalizer,
                "alpha": m.alpha,
                "zorder": m.zorder,
            }
            params.update(matplot_args)
            
            if m.levels is False:
                ret.append(axes.imshow(m.data, **params))
            
            # Use contour for contour data, and imshow otherwise
            if m.levels is not False:
                # Set data with values <= 0 to transparent
                # contour_data = np.ma.masked_array(m, mask=(m <= 0))
                ret.append(axes.contour(m.data, m.levels, **params))
                #Set the label of the first line so a legend can be created
                ret[-1].collections[0].set_label(m.name)
                                
        # Adjust axes extents to include all data
        axes.axis('image')
        
        #Set current image (makes colorbar work)
        plt.sci(ret[0])
        return ret
        
    def peek(self, gamma=None, colorbar=True, basic_plot=False, draw_limb=True,
             draw_grid=False, **matplot_args):
        """Displays the map in a new figure

        Parameters
        ----------
        gamma : float
            Gamma value to use for the color map
            
        colorbar : bool or int
            Whether to display a colorbar next to the plot.
            If specified as an integer a colorbar is plotted for that index.
            
        basic_plot : bool
            If true, the data is plotted by itself at it's natural scale; no
            title, labels, or axes are shown.
            
        **matplot_args : dict
            Matplotlib Any additional imshow arguments that should be used
            when plotting the image.
        """
        
        # Create a figure and add title and axes
        figure = plt.figure(frameon=not basic_plot)

        # Basic plot
        if basic_plot:
            axes = plt.Axes(figure, [0., 0., 1., 1.])
            axes.set_axis_off()
            figure.add_axes(axes)
            matplot_args.update({'annotate':False})
        else:
            axes = figure.add_subplot(111)

        ret = self.plot(axes=axes,**matplot_args)        
        
        if not isinstance(colorbar, bool) and isinstance(colorbar, int):
            figure.colorbar(ret[colorbar])
        elif colorbar:
            plt.colorbar()
        if draw_limb:
            self.draw_limb(axes=axes)
        
        if isinstance(draw_grid, bool):
            if draw_grid:
                self.draw_grid(axes=axes)
        
        elif isinstance(draw_grid, (int, long, float)):
            self.draw_grid(axes=axes, grid_spacing=draw_grid)
        else:
            raise TypeError("draw_grid should be bool, int, long or float")

        figure.show()
        
        return figure

        
class OutOfRangeAlphaValue(ValueError):
    """Exception to raise when an alpha value outside of the range 0-1 is
    requested.
    """
    pass

########NEW FILE########
__FILENAME__ = header
"""
MapHeader is a generalized header class that deals with header parsing and
normalization.
"""
from __future__ import absolute_import

from sunpy.util.odict import OrderedDict

__all__ = ['MapMeta']

class MapMeta(OrderedDict):
    """
    A class to hold meta data associated with a Map derivative.
    
    This class handles everything a lower case. This allows case insensitive 
    indexing.
    """
    def __init__(self, adict, *args):
        """Creates a new MapHeader instance"""
        # Store all keys as upper-case to allow for case-insensitive indexing
        #OrderedDict can be instanciated from a list of lists or a tuple of tuples
        if isinstance(adict, list) or isinstance(adict, tuple):
            tags = dict((k.upper(), v) for k, v in adict)
        elif isinstance(adict, dict):
            tags = dict((k.upper(), v) for k, v in adict.items())
        else:
            raise TypeError("Can not create a MapMeta from this type input")
            
        super(MapMeta, self).__init__(tags, *args)

    def __contains__(self, key):
        """Overide __contains__"""
        return OrderedDict.__contains__(self, key.lower())

    def __getitem__(self, key):
        """Overide [] indexing"""
        return OrderedDict.__getitem__(self, key.lower())

    def __setitem__(self, key, value):
        """Overide [] indexing"""
        return OrderedDict.__setitem__(self, key.lower(), value)
        
    def get(self, key, default=None):
        """Overide .get() indexing"""
        return OrderedDict.get(self, key.lower(), default)

    def has_key(self, key):
        """Overide .has_key() to perform case-insensitively"""
        return key.lower() in self

    def pop(self, key, default=None):
        """Overide .pop() to perform case-insensitively"""
        return OrderedDict.pop(self, key.lower(), default)

    def update(self, d2):
        """Overide .update() to perform case-insensitively"""
        return OrderedDict.update(self, dict((k.lower(), v) for k, v in d2.items()))

    def setdefault(self, key, default=None):
        """Overide .setdefault() to perform case-insensitively"""
        return OrderedDict.setdefault(self, key.lower(), default)
########NEW FILE########
__FILENAME__ = mapbase
"""
Map is a generic Map class from which all other Map classes inherit from.
"""
from __future__ import absolute_import

#pylint: disable=E1101,E1121,W0404,W0613
__authors__ = ["Russell Hewett, Stuart Mumford, Keith Hughitt, Steven Christe"]
__email__ = "stuart@mumford.me.uk"

from copy import deepcopy
import warnings

import numpy as np
import matplotlib.pyplot as plt
import scipy.ndimage.interpolation
from matplotlib import patches
from matplotlib import cm

import astropy.nddata

try:
    import sunpy.image.Crotate as Crotate
except ImportError:
    pass

import sunpy.io as io
import sunpy.wcs as wcs
from sunpy.visualization import toggle_pylab
# from sunpy.io import read_file, read_file_header
from sunpy.sun import constants
from sunpy.time import parse_time, is_time
from sunpy.image.rescale import reshape_image_to_4d_superpixel
from sunpy.image.rescale import resample as sunpy_image_resample

#from sunpy.util.cond_dispatch import ConditionalDispatch
#from sunpy.util.create import Parent

__all__ = ['GenericMap']

"""
Questions
---------
* Should we use Helioviewer or VSO's data model? (e.g. map.meas, map.wavelength
or something else?)
* Should 'center' be renamed to 'offset' and crpix1 & 2 be used for 'center'?
"""

class GenericMap(astropy.nddata.NDData):
    """
    A Generic spatially-aware 2D data array

    Parameters
    ----------
    data : numpy.ndarray, list
        A 2d list or ndarray containing the map data
    header : dict
        A dictionary of the original image header tags

    Attributes
    ----------
    cmap : matplotlib.colors.Colormap
        A color map used for plotting with matplotlib.
    mpl_color_normalizer : matplotlib.colors.Normalize
        A matplotlib normalizer used to scale the image plot.

    Examples
    --------
    >>> aia = sunpy.make_map(sunpy.AIA_171_IMAGE)
    >>> aia.T
    AIAMap([[ 0.3125,  1.    , -1.1875, ..., -0.625 ,  0.5625,  0.5   ],
    [-0.0625,  0.1875,  0.375 , ...,  0.0625,  0.0625, -0.125 ],
    [-0.125 , -0.8125, -0.5   , ..., -0.3125,  0.5625,  0.4375],
    ...,
    [ 0.625 ,  0.625 , -0.125 , ...,  0.125 , -0.0625,  0.6875],
    [-0.625 , -0.625 , -0.625 , ...,  0.125 , -0.0625,  0.6875],
    [ 0.    ,  0.    , -1.1875, ...,  0.125 ,  0.    ,  0.6875]])
    >>> aia.units['x']
    'arcsec'
    >>> aia.peek()

    References
    ----------
    | http://docs.scipy.org/doc/numpy/reference/arrays.classes.html
    | http://docs.scipy.org/doc/numpy/user/basics.subclassing.html
    | http://docs.scipy.org/doc/numpy/reference/ufuncs.html
    | http://www.scipy.org/Subclasses

    """

    def __init__(self, data, header, **kwargs):

        astropy.nddata.NDData.__init__(self, data, meta=header, **kwargs)

        # Correct possibly missing meta keywords
        self._fix_date()
        self._fix_naxis()

        # Setup some attributes
        self._name = self.observatory + " " + str(self.measurement)
        self._nickname = self.detector

        # Visualization attributes
        self.cmap = cm.gray

        # Validate header
        # TODO: This should be a function of the header, not of the map
        self._validate()

        # Set mpl.colors.Normalize instance for plot scaling
        self.mpl_color_normalizer = self._get_mpl_normalizer()

    def __getitem__(self, key):
        """ This should allow indexing by physical coordinate """
        raise NotImplementedError(
    "The ability to index Map by physical coordinate is not yet implemented.")

    def __repr__(self):
        if not hasattr(self, 'observatory'):
            return self.data.__repr__()
        return (
"""SunPy %s
---------
Observatory:\t %s
Instrument:\t %s
Detector:\t %s
Measurement:\t %s
Obs Date:\t %s
dt:\t\t %f
Dimension:\t [%d, %d]
[dx, dy] =\t [%f, %f]

""" % (self.__class__.__name__,
       self.observatory, self.instrument, self.detector, self.measurement,
       self.date, self.exposure_time,
       self.data.shape[1], self.data.shape[0], self.scale['x'], self.scale['y'])
     + self.data.__repr__())


    #Some numpy extraction
    @property
    def shape(self):
        return self.data.shape

    @property
    def dtype(self):
        return self.data.dtype

    @property
    def size(self):
        return self.data.size

    @property
    def ndim(self):
        return self.data.ndim

    def std(self, *args, **kwargs):
        return self.data.std(*args, **kwargs)

    def mean(self, *args, **kwargs):
        return self.data.mean(*args, **kwargs)

    def min(self, *args, **kwargs):
        return self.data.min(*args, **kwargs)

    def max(self, *args, **kwargs):
        return self.data.max(*args, **kwargs)

# #### Keyword attribute and other attribute definitions #### #

    @property
    def name(self):
        """Human-readable description of map-type"""
        return self._name
    @name.setter
    def name(self, n):
        self._name = n

    @property
    def nickname(self):
        """An abbreviated human-readable description of the map-type; part of the Helioviewer data model"""
        return self._nickname
    @nickname.setter
    def nickname(self, n):
        self._nickname = n

    @property
    def date(self):
        """Image observation time"""
        return self.meta.get('date-obs', None)
#    @date.setter
#    def date(self, new_date):
#        self.meta['date-obs'] = new_date
#        #propagate change to malformed FITS keywords
#        if is_time(self.meta.get('date_obs', None)):
#            self.meta['date_obs'] = new_date

    @property
    def detector(self):
        """Detector name"""
        return self.meta.get('detector', "")

    @property
    def dsun(self):
        """The observer distance from the Sun."""
        return self.meta.get('dsun_obs', constants.au)

    @property
    def exposure_time(self):
        """Exposure time of the image in seconds."""
        return self.meta.get('exptime', 0.0)

    @property
    def instrument(self):
        """Instrument name"""
        return self.meta.get('instrume', "")

    @property
    def measurement(self):
        """Measurement name, defaults to the wavelength of image"""
        return self.meta.get('wavelnth', "")

    @property
    def wavelength(self):
        """wavelength of the observation"""
        return self.meta.get('wavelnth', "")

    @property
    def observatory(self):
        """Observatory or Telescope name"""
        return self.meta.get('obsrvtry', self.meta.get('telescop', ""))

    @property
    def xrange(self):
        """Return the X range of the image in arcsec from edge to edge."""
        xmin = self.center['x'] - self.shape[1] / 2. * self.scale['x']
        xmax = self.center['x'] + self.shape[1] / 2. * self.scale['x']
        return [xmin, xmax]

    @property
    def yrange(self):
        """Return the Y range of the image in arcsec from edge to edge."""
        ymin = self.center['y'] - self.shape[0] / 2. * self.scale['y']
        ymax = self.center['y'] + self.shape[0] / 2. * self.scale['y']
        return [ymin, ymax]

    @property
    def center(self):
        """Returns the offset between the center of the Sun and the center of 
        the map."""
        return {'x': wcs.get_center(self.shape[1], self.scale['x'], 
                                    self.reference_pixel['x'], 
                                    self.reference_coordinate['x']),
                'y': wcs.get_center(self.shape[0], self.scale['y'],
                                    self.reference_pixel['y'],
                                    self.reference_coordinate['y']),}

    @property
    def rsun_meters(self):
        """Radius of the sun in meters"""
        return self.meta.get('rsun_ref', constants.radius)

    @property
    def rsun_arcseconds(self):
        """Radius of the sun in arcseconds"""
        return self.meta.get('rsun_obs', self.meta.get('solar_r',
                                         self.meta.get('radius', constants.average_angular_size.to('arcsec').value)))

    @property
    def coordinate_system(self):
        """Coordinate system used for x and y axes (ctype1/2)"""
        return {'x': self.meta.get('ctype1', 'HPLN-TAN'),
                'y': self.meta.get('ctype2', 'HPLT-TAN'),}

    @property
    def carrington_longitude(self):
        """Carrington longitude (crln_obs)"""
        return self.meta.get('crln_obs', 0.)

    @property
    def heliographic_latitude(self):
        """Heliographic latitude in degrees"""
        return self.meta.get('hglt_obs', self.meta.get('crlt_obs',
                                         self.meta.get('solar_b0', 0.)))

    @property
    def heliographic_longitude(self):
        """Heliographic longitude in degrees"""
        return self.meta.get('hgln_obs', 0.)

    @property
    def reference_coordinate(self):
        """Reference point WCS axes in data units (crval1/2)"""
        return {'x': self.meta.get('crval1', 0.),
                'y': self.meta.get('crval2', 0.),}

    @property
    def reference_pixel(self):
        """Reference point axes in pixels (crpix1/2)"""
        return {'x': self.meta.get('crpix1', (self.meta.get('naxis1') + 1) / 2.),
                'y': self.meta.get('crpix2', (self.meta.get('naxis2') + 1) / 2.),}

    @property
    def scale(self):
        """Image scale along the x and y axes in units/pixel (cdelt1/2)"""
        return {'x': self.meta.get('cdelt1', 1.),
                'y': self.meta.get('cdelt2', 1.),}

    @property
    def units(self):
        """Image coordinate units along the x and y axes (cunit1/2)."""
        return {'x': self.meta.get('cunit1', 'arcsec'),
                'y': self.meta.get('cunit2', 'arcsec'),}

    #TODO: This needs to be WCS compliant!
    @property
    def rotation_angle(self):
        """The Rotation angle of each axis"""
        return {'x': self.meta.get('crota1', 0.),
                'y': self.meta.get('crota2', 0.),}

# #### Miscellaneous #### #

    def _fix_date(self):
        # Check commonly used but non-standard FITS keyword for observation time
        # and correct the keyword if we can.  Keep updating old one for
        # backwards compatibility.
        if is_time(self.meta.get('date_obs', None)):
            self.meta['date-obs'] = self.meta['date_obs']

    def _fix_naxis(self):
        # If naxis is not specified, get it from the array shape
        if 'naxis1' not in self.meta:
            self.meta['naxis1'] = self.shape[1]
        if 'naxis2' not in self.meta:
            self.meta['naxis2'] = self.shape[0]
        if 'naxis' not in self.meta:
            self.meta['naxis'] = self.ndim

    def _fix_bitpix(self):
        # Bit-depth
        #
        #   8    Character or unsigned binary integer
        #  16    16-bit twos-complement binary integer
        #  32    32-bit twos-complement binary integer
        # -32    IEEE single precision floating point
        # -64    IEEE double precision floating point
        #
        if 'bitpix' not in self.meta:
            float_fac = -1 if self.dtype.kind == "f" else 1
            self.meta['bitpix'] = float_fac * 8 * self.dtype.itemsize

    def _validate(self):
        """Validates the meta-information associated with a Map.

        This function includes very basic validation checks which apply to
        all of the kinds of files that SunPy can read. Datasource-specific
        validation should be handled in the relevant file in the
        sunpy.map.sources package."""
#        if (self.dsun <= 0 or self.dsun >= 40 * constants.au):
#            raise InvalidHeaderInformation("Invalid value for DSUN")
        pass

# #### Data conversion routines #### #

    def data_to_pixel(self, value, dim):
        """Convert pixel-center data coordinates to pixel values"""
        #TODO: This function should be renamed. It is confusing as data
        # coordinates are in something like arcsec but this function just changes how you
        # count pixels
        if dim not in ['x', 'y']:
            raise ValueError("Invalid dimension. Must be one of 'x' or 'y'.")

        size = self.shape[dim == 'x']  # 1 if dim == 'x', 0 if dim == 'y'.

        return (value - self.center[dim]) / self.scale[dim] + ((size - 1) / 2.)

    def pixel_to_data(self, x=None, y=None):
        """Convert from pixel coordinates to data coordinates (e.g. arcsec)"""
        width = self.shape[1]
        height = self.shape[0]

        if (x is not None) & (x > width-1):
            raise ValueError("X pixel value larger than image width (%s)." % width)
        if (x is not None) & (y > height-1):
            raise ValueError("Y pixel value larger than image height (%s)." % height)
        if (x is not None) & (x < 0):
            raise ValueError("X pixel value cannot be less than 0.")
        if (x is not None) & (y < 0):
            raise ValueError("Y pixel value cannot be less than 0.")

        scale = np.array([self.scale['x'], self.scale['y']])
        crpix = np.array([self.reference_pixel['x'], self.reference_pixel['y']])
        crval = np.array([self.reference_coordinate['x'], self.reference_coordinate['y']])
        coordinate_system = [self.coordinate_system['x'], self.coordinate_system['y']]
        x,y = wcs.convert_pixel_to_data(self.shape, scale, crpix, crval, x = x, y = y)

        return x, y

# #### I/O routines #### #

    def save(self, filepath, filetype='auto', **kwargs):
        """Saves the SunPy Map object to a file.

        Currently SunPy can only save files in the FITS format. In the future
        support will be added for saving to other formats.

        Parameters
        ----------
        filepath : string
            Location to save file to.

        filetype : string
            'auto' or any supported file extension
        """
        io.write_file(filepath, self.data, self.meta, filetype=filetype,
                      **kwargs)

# #### Image processing routines #### #

    def resample(self, dimensions, method='linear'):
        """Returns a new Map that has been resampled up or down

        Arbitrary resampling of the Map to new dimension sizes.

        Uses the same parameters and creates the same co-ordinate lookup points
        as IDL''s congrid routine, which apparently originally came from a
        VAX/VMS routine of the same name.

        Parameters
        ----------
        dimensions : tuple
            Dimensions that new Map should have.
            Note: the first argument corresponds to the 'x' axis and the second
            argument corresponds to the 'y' axis.
        method : {'neighbor' | 'nearest' | 'linear' | 'spline'}
            Method to use for resampling interpolation.
                * neighbor - Closest value from original data
                * nearest and linear - Uses n x 1-D interpolations using
                  scipy.interpolate.interp1d
                * spline - Uses ndimage.map_coordinates

        Returns
        -------
        out : Map
            A new Map which has been resampled to the desired dimensions.

        References
        ----------
        | http://www.scipy.org/Cookbook/Rebinning (Original source, 2011/11/19)
        """

        # Note: because the underlying ndarray is transposed in sense when
        #   compared to the Map, the ndarray is transposed, resampled, then
        #   transposed back
        # Note: "center" defaults to True in this function because data
        #   coordinates in a Map are at pixel centers

        # Make a copy of the original data and perform resample
        new_data = sunpy_image_resample(self.data.copy().T, dimensions,
                                    method, center=True)
        new_data = new_data.T

        # Note that 'x' and 'y' correspond to 1 and 0 in self.shape,
        # respectively
        scale_factor_x = (float(self.shape[1]) / dimensions[0])
        scale_factor_y = (float(self.shape[0]) / dimensions[1])

        new_map = deepcopy(self)
        # Update image scale and number of pixels
        new_meta = self.meta.copy()

        # Update metadata
        new_meta['cdelt1'] *= scale_factor_x
        new_meta['cdelt2'] *= scale_factor_y
        new_meta['crpix1'] = (dimensions[0] + 1) / 2.
        new_meta['crpix2'] = (dimensions[1] + 1) / 2.
        new_meta['crval1'] = self.center['x']
        new_meta['crval2'] = self.center['y']

        # Create new map instance
        new_map.data = new_data
        new_map.meta = new_meta
        return new_map
    
    def rotate(self, angle=None, rmatrix=None, scale=1.0, rotation_center=None, recenter=True,
               missing=0.0, interpolation='bicubic', interp_param=-0.5):
        """Returns a new rotated, rescaled and shifted map.

        Parameters
        ----------
        angle: float
           The angle to rotate the image by (radians). Specify angle or matrix.
        rmatrix: NxN
            Linear transformation rotation matrix. Specify angle or matrix.
        scale: float
           A scale factor for the image, default is no scaling
        rotation_center: tuple
           The point in the image to rotate around (Axis of rotation).
           Default: center of the array
        recenter: bool, or array-like
           Move the centroid (axis of rotation) to the center of the array
           or recenter coords.
           Default: True, recenter to the center of the array.
        missing: float
           The numerical value to fill any missing points after rotation.
           Default: 0.0
        interpolation: {'nearest' | 'bilinear' | 'spline' | 'bicubic'}
            Interpolation method to use in the transform.
            Spline uses the
            scipy.ndimage.interpolation.affline_transform routine.
            nearest, bilinear and bicubic all replicate the IDL rot() function.
            Default: 'bicubic'
        interp_par: Int or Float
            Optional parameter for controlling the interpolation.
            Spline interpolation requires an integer value between 1 and 5 for
            the degree of the spline fit.
            Default: 3
            BiCubic interpolation requires a flaot value between -1 and 0.
            Default: 0.5
            Other interpolation options ingore the argument.

        Returns
        -------
        New rotated, rescaled, translated map

        Notes
        -----
        Apart from interpolation='spline' all other options use a compiled
        C-API extension. If for some reason this is not compiled correctly this
        routine will fall back upon the scipy implementation of order = 3.
        For more infomation see:
        http://sunpy.readthedocs.org/en/latest/guide/troubleshooting.html#crotate-warning
        """
        assert angle is None or rmatrix is None
        #Interpolation parameter Sanity
        assert interpolation in ['nearest','spline','bilinear','bicubic']
        #Set defaults based on interpolation
        if interp_param is None:
            if interpolation is 'spline':
                interp_param = 3
            elif interpolation is 'bicubic':
                interp_param = 0.5
            else:
                interp_param = 0 #Default value for nearest or bilinear

        #Make sure recenter is a vector with shape (2,1)
        if not isinstance(recenter, bool):
            recenter = np.array(recenter).reshape(2,1)

        #Define Size and center of array
        center = (np.array(self.data.shape)-1)/2.0

        #If rotation_center is not set (None or False),
        #set rotation_center to the center of the image.
        if rotation_center is None:
            rotation_center = center
        else:
            #Else check rotation_center is a vector with shape (2,1)
            rotation_center = np.array(rotation_center).reshape(2,1)

        #recenter to the rotation_center if recenter is True
        if isinstance(recenter, bool):
            #if rentre is False then this will be (0,0)
            shift = np.array(rotation_center) - np.array(center)
        else:
            #recenter to recenter vector otherwise
            shift = np.array(recenter) - np.array(center)

        image = self.data.copy()

        if not angle is None:
            #Calulate the parameters for the affline_transform
            c = np.cos(angle)
            s = np.sin(angle)
            mati = np.array([[c, s],[-s, c]]) / scale   # res->orig
        if not rmatrix is None:
            mati = rmatrix / scale   # res->orig
        center = np.array([center]).transpose()  # the center of rotn
        shift = np.array([shift]).transpose()    # the shift
        kpos = center - np.dot(mati, (center + shift))
        # kpos and mati are the two transform constants, kpos is a 2x2 array
        rsmat, offs =  mati, np.squeeze((kpos[0,0], kpos[1,0]))

        if interpolation == 'spline':
            # This is the scipy call
            data = scipy.ndimage.interpolation.affine_transform(image, rsmat,
                           offset=offs, order=interp_param, mode='constant',
                           cval=missing)
        else:
            #Use C extension Package
            if not 'Crotate' in globals():
                warnings.warn("""The C extension sunpy.image.Crotate is not
installed, falling back to the interpolation='spline' of order=3""" ,Warning)
                data = scipy.ndimage.interpolation.affine_transform(image, rsmat,
                           offset=offs, order=3, mode='constant',
                           cval=missing)
            else:
                #Set up call parameters depending on interp type.
                if interpolation == 'nearest':
                    interp_type = Crotate.NEAREST
                elif interpolation == 'bilinear':
                    interp_type = Crotate.BILINEAR
                elif interpolation == 'bicubic':
                    interp_type = Crotate.BICUBIC
                #Make call to extension
                data = Crotate.affine_transform(image,
                                      rsmat, offset=offs,
                                      kernel=interp_type, cubic=interp_param,
                                      mode='constant', cval=missing)

        #Return a new map
        #Copy Header
        new_map = deepcopy(self)

        # Create new map instance
        new_map.data = data
        return new_map

    def submap(self, range_a, range_b, units="data"):
        """Returns a submap of the map with the specified range

        Parameters
        ----------
        range_a : list
            The range of the Map to select across either the x axis.
        range_b : list
            The range of the Map to select across either the y axis.
        units : {'data' | 'pixels'}, optional
            The units for the supplied ranges.

        Returns
        -------
        out : Map
            A new map instance is returned representing to specified sub-region

        Examples
        --------
        >>> aia.submap([-5,5],[-5,5])
        AIAMap([[ 341.3125,  266.5   ,  329.375 ,  330.5625,  298.875 ],
        [ 347.1875,  273.4375,  247.4375,  303.5   ,  305.3125],
        [ 322.8125,  302.3125,  298.125 ,  299.    ,  261.5   ],
        [ 334.875 ,  289.75  ,  269.25  ,  256.375 ,  242.3125],
        [ 273.125 ,  241.75  ,  248.8125,  263.0625,  249.0625]])

        >>> aia.submap([0,5],[0,5], units='pixels')
        AIAMap([[ 0.3125, -0.0625, -0.125 ,  0.    , -0.375 ],
        [ 1.    ,  0.1875, -0.8125,  0.125 ,  0.3125],
        [-1.1875,  0.375 , -0.5   ,  0.25  , -0.4375],
        [-0.6875, -0.3125,  0.8125,  0.0625,  0.1875],
        [-0.875 ,  0.25  ,  0.1875,  0.    , -0.6875]])
        """
        if units is "data":
            # Check edges (e.g. [:512,..] or [:,...])
            if range_a[0] is None:
                range_a[0] = self.xrange[0]
            if range_a[1] is None:
                range_a[1] = self.xrange[1]
            if range_b[0] is None:
                range_b[0] = self.yrange[0]
            if range_b[1] is None:
                range_b[1] = self.yrange[1]

            #x_pixels = [self.data_to_pixel(elem, 'x') for elem in range_a]
            x_pixels = [np.ceil(self.data_to_pixel(range_a[0], 'x')),
                        np.floor(self.data_to_pixel(range_a[1], 'x')) + 1]
            #y_pixels = [self.data_to_pixel(elem, 'y') for elem in range_b]
            y_pixels = [np.ceil(self.data_to_pixel(range_b[0], 'y')),
                        np.floor(self.data_to_pixel(range_b[1], 'y')) + 1]
        elif units is "pixels":
            # Check edges
            if range_a[0] is None:
                range_a[0] = 0
            if range_a[1] is None:
                range_a[1] = self.shape[1]
            if range_b[0] is None:
                range_b[0] = 0
            if range_b[1] is None:
                range_b[1] = self.shape[0]

            x_pixels = range_a
            y_pixels = range_b
        else:
            raise ValueError(
                "Invalid unit. Must be one of 'data' or 'pixels'")


        # Get ndarray representation of submap
        xslice = slice(x_pixels[0], x_pixels[1])
        yslice = slice(y_pixels[0], y_pixels[1])
        new_data = self.data[yslice, xslice].copy()

        # Make a copy of the header with updated centering information
        new_map = deepcopy(self)
        new_map.meta['crpix1'] = self.reference_pixel['x'] - x_pixels[0]
        new_map.meta['crpix2'] = self.reference_pixel['y'] - y_pixels[0]
        new_map.meta['naxis1'] = new_data.shape[1]
        new_map.meta['naxis2'] = new_data.shape[0]

        # Create new map instance
        new_map.data = new_data
        return new_map

    def superpixel(self, dimensions, method='sum'):
        """Returns a new map consisting of superpixels formed from the
        original data.  Useful for increasing signal to noise ratio in images.

        Parameters
        ----------
        dimensions : tuple
            One superpixel in the new map is equal to (dimension[0],
            dimension[1]) pixels of the original map
            Note: the first argument corresponds to the 'x' axis and the second
            argument corresponds to the 'y' axis.
        method : {'sum' | 'average'}
            What each superpixel represents compared to the original data
                * sum - add up the original data
                * average - average the sum over the number of original pixels

        Returns
        -------
        out : Map
            A new Map which has superpixels of the required size.

        References
        ----------
        | http://mail.scipy.org/pipermail/numpy-discussion/2010-July/051760.html
        """

        # Note: because the underlying ndarray is transposed in sense when
        #   compared to the Map, the ndarray is transposed, resampled, then
        #   transposed back
        # Note: "center" defaults to True in this function because data
        #   coordinates in a Map are at pixel centers

        # Make a copy of the original data and perform reshaping
        reshaped = reshape_image_to_4d_superpixel(self.data.copy().T,
                                                  dimensions)
        if method == 'sum':
            new_data = reshaped.sum(axis=3).sum(axis=1)
        elif method == 'average':
            new_data = ((reshaped.sum(axis=3).sum(axis=1)) /
                    np.float32(dimensions[0] * dimensions[1]))
        new_data = new_data.T


        # Update image scale and number of pixels
        new_map = deepcopy(self)
        new_meta = new_map.meta

        # Note that 'x' and 'y' correspond to 1 and 0 in self.shape,
        # respectively
        new_nx = self.shape[1] / dimensions[0]
        new_ny = self.shape[0] / dimensions[1]

        # Update metadata
        new_meta['cdelt1'] = dimensions[0] * self.scale['x']
        new_meta['cdelt2'] = dimensions[1] * self.scale['y']
        new_meta['crpix1'] = (new_nx + 1) / 2.
        new_meta['crpix2'] = (new_ny + 1) / 2.
        new_meta['crval1'] = self.center['x']
        new_meta['crval2'] = self.center['y']

        # Create new map instance
        new_map.data = new_data
        return new_map

# #### Visualization #### #

    def draw_grid(self, axes=None, grid_spacing=20, **kwargs):
        """Draws a grid over the surface of the Sun

        Parameters
        ----------
        axes: matplotlib.axes object or None
        Axes to plot limb on or None to use current axes.

        grid_spacing: float
            Spacing (in degrees) for longitude and latitude grid.

        Returns
        -------
        matplotlib.axes object

        Notes
        -----
        keyword arguments are passed onto matplotlib.pyplot.plot
        """

        if not axes:
            axes = plt.gca()

        x, y = self.pixel_to_data()
        rsun = self.rsun_meters
        dsun = self.dsun

        b0 = self.heliographic_latitude
        l0 = self.heliographic_longitude
        units = [self.units['x'], self.units['y']]

        #Prep the plot kwargs
        plot_kw = {'color':'white',
                   'linestyle':'dotted',
                   'zorder':100}
        plot_kw.update(kwargs)

        #TODO: This function could be optimized. Does not need to convert the entire image
        # coordinates
        #lon_self, lat_self = wcs.convert_hpc_hg(rsun, dsun, angle_units = units[0], b0, l0, x, y)
        lon_self, lat_self = wcs.convert_hpc_hg(x, y, b0_deg=b0, l0_deg=l0, dsun_meters=dsun, angle_units='arcsec')
        # define the number of points for each latitude or longitude line
        num_points = 20

        #TODO: The following code is ugly. Fix it.
        lon_range = [lon_self.min(), lon_self.max()]
        lat_range = [lat_self.min(), lat_self.max()]
        if np.isfinite(lon_range[0]) == False:
            lon_range[0] = -90 + self.heliographic_longitude
        if np.isfinite(lon_range[1]) == False:
            lon_range[1] = 90 + self.heliographic_longitude
        if np.isfinite(lat_range[0]) == False:
            lat_range[0] = -90 + self.heliographic_latitude
        if np.isfinite(lat_range[1]) == False:
            lat_range[1] = 90 + self.heliographic_latitude

        hg_longitude_deg = np.linspace(lon_range[0], lon_range[1], num=num_points)
        hg_latitude_deg = np.arange(lat_range[0], lat_range[1]+grid_spacing, grid_spacing)

        # draw the latitude lines
        for lat in hg_latitude_deg:
            hg_latitude_deg_mesh, hg_longitude_deg_mesh = np.meshgrid(
                lat * np.ones(num_points), hg_longitude_deg)
            x, y = wcs.convert_hg_hpc(hg_longitude_deg_mesh, hg_latitude_deg_mesh, b0_deg=b0, l0_deg=l0,
                    dsun_meters=dsun, angle_units=units[0], occultation=False)

            axes.plot(x, y, **plot_kw)

        hg_longitude_deg = np.arange(lon_range[0], lon_range[1]+grid_spacing, grid_spacing)
        hg_latitude_deg = np.linspace(lat_range[0], lat_range[1], num=num_points)

        # draw the longitude lines
        for lon in hg_longitude_deg:
            hg_longitude_deg_mesh, hg_latitude_deg_mesh = np.meshgrid(
                lon * np.ones(num_points), hg_latitude_deg)
            x, y = wcs.convert_hg_hpc(hg_longitude_deg_mesh, hg_latitude_deg_mesh, b0_deg=b0, l0_deg=l0,
                    dsun_meters=dsun, angle_units=units[0], occultation=False)
            axes.plot(x, y, **plot_kw)

        axes.set_ylim(self.yrange)
        axes.set_xlim(self.xrange)

        return axes

    def draw_limb(self, axes=None, **kwargs):
        """Draws a circle representing the solar limb

            Parameters
            ----------
            axes: matplotlib.axes object or None
                Axes to plot limb on or None to use current axes.

            Returns
            -------
            matplotlib.axes object

            Notes
            -----
            keyword arguments are passed onto the Circle Patch, see:
            http://matplotlib.org/api/artist_api.html#matplotlib.patches.Patch
            http://matplotlib.org/api/artist_api.html#matplotlib.patches.Circle
        """

        if not axes:
            axes = plt.gca()

        c_kw = {'radius':self.rsun_arcseconds,
                'fill':False,
                'color':'white',
                'zorder':100
                }
        c_kw.update(kwargs)

        circ = patches.Circle([0, 0], **c_kw)
        axes.add_artist(circ)

        return axes

    @toggle_pylab
    def peek(self, draw_limb=True, draw_grid=False, gamma=None,
                   colorbar=True, basic_plot=False, **matplot_args):
        """Displays the map in a new figure

        Parameters
        ----------
        draw_limb : bool
            Whether the solar limb should be plotted.
        draw_grid : bool or number
            Whether solar meridians and parallels are plotted. If float then sets
            degree difference between parallels and meridians.
        gamma : float
            Gamma value to use for the color map
        colorbar : bool
            Whether to display a colorbar next to the plot
        basic_plot : bool
            If true, the data is plotted by itself at it's natural scale; no
            title, labels, or axes are shown.
        **matplot_args : dict
            Matplotlib Any additional imshow arguments that should be used
            when plotting the image.
        """

        # Create a figure and add title and axes
        figure = plt.figure(frameon=not basic_plot)

        # Basic plot
        if basic_plot:
            axes = plt.Axes(figure, [0., 0., 1., 1.])
            axes.set_axis_off()
            figure.add_axes(axes)
            matplot_args.update({'annotate':False})

        # Normal plot
        else:
            axes = figure.gca()

        im = self.plot(axes=axes,**matplot_args)

        if colorbar and not basic_plot:
            figure.colorbar(im)

        if draw_limb:
            self.draw_limb(axes=axes)

        if isinstance(draw_grid, bool):
            if draw_grid:
                self.draw_grid(axes=axes)
        elif isinstance(draw_grid, (int, long, float)):
            self.draw_grid(axes=axes, grid_spacing=draw_grid)
        else:
            raise TypeError("draw_grid should be bool, int, long or float")

        figure.show()

        return figure

    @toggle_pylab
    def plot(self, gamma=None, annotate=True, axes=None, **imshow_args):
        """ Plots the map object using matplotlib, in a method equivalent
        to plt.imshow() using nearest neighbour interpolation.

        Parameters
        ----------
        gamma : float
            Gamma value to use for the color map

        annotate : bool
            If true, the data is plotted at it's natural scale; with
            title and axis labels.

        axes: matplotlib.axes object or None
            If provided the image will be plotted on the given axes. Else the
            current matplotlib axes will be used.

        **imshow_args : dict
            Any additional imshow arguments that should be used
            when plotting the image.

        Examples
        --------
        #Simple Plot with color bar
        plt.figure()
        aiamap.plot()
        plt.colorbar()

        #Add a limb line and grid
        aia.plot()
        aia.draw_limb()
        aia.draw_grid()
        """

        #Get current axes
        if not axes:
            axes = plt.gca()

        # Normal plot
        if annotate:
            axes.set_title("%s %s" % (self.name, parse_time(self.date).strftime("%Y-%m-%d %H:%M:%S.%f")))

            # x-axis label
            if self.coordinate_system['x'] == 'HG':
                xlabel = 'Longitude [%s]' % self.units['x']
            else:
                xlabel = 'X-position [%s]' % self.units['x']

            # y-axis label
            if self.coordinate_system['y'] == 'HG':
                ylabel = 'Latitude [%s]' % self.units['y']
            else:
                ylabel = 'Y-position [%s]' % self.units['y']

            axes.set_xlabel(xlabel)
            axes.set_ylabel(ylabel)

        # Determine extent
        extent = self.xrange + self.yrange

        cmap = deepcopy(self.cmap)
        if gamma is not None:
            cmap.set_gamma(gamma)

            #make imshow kwargs a dict

        kwargs = {'origin':'lower',
                  'cmap':cmap,
                  'norm':self.mpl_color_normalizer,
                  'extent':extent,
                  'interpolation':'nearest'}
        kwargs.update(imshow_args)

        ret = axes.imshow(self.data, **kwargs)

        #Set current image (makes colorbar work)
        plt.sci(ret)
        return ret

    def _get_mpl_normalizer(self):
        """
        Returns a default mpl.colors.Normalize instance for plot scaling.

        Not yet implemented.
        """
        return None


class InvalidHeaderInformation(ValueError):
    """Exception to raise when an invalid header tag value is encountered for a
    FITS/JPEG 2000 file."""
    pass

########NEW FILE########
__FILENAME__ = mapcube
"""A Python MapCube Object"""
from __future__ import absolute_import
#pylint: disable=W0401,W0614,W0201,W0212,W0404

import numpy as np
import matplotlib.animation
import matplotlib.pyplot as plt

from sunpy.map import GenericMap

from sunpy.visualization.mapcubeanimator import MapCubeAnimator
from sunpy.util import expand_list

__all__ = ['MapCube']

class MapCube(object):
    """
    MapCube(input)

    A series of spatially aligned Maps.

    Parameters
    ----------
    args : {List}
        A list of Map instances
    sortby : {"date", None}
        Method by which the MapCube should be sorted along the z-axis.
    derotate : {None}
        Apply a derotation to the data (Not Implemented)

    To coalign a mapcube so that solar features remain on the same pixels,
    please see the "Coalignment of mapcubes" note below.

    Attributes
    ----------
    maps : {List}
        This attribute holds the list of Map instances obtained from parameter args.

    Examples
    --------
    >>> mapcube = sunpy.map.Map('images/*.fits', mapcube=True)

    Mapcubes can be co-aligned using the routines in sunpy.image.coalignment.
    """
    #pylint: disable=W0613,E1101
    def __init__(self, *args, **kwargs):
        """Creates a new Map instance"""

        # Hack to get around Python 2.x not backporting PEP 3102.
        sortby = kwargs.pop('sortby', 'date')
        derotate = kwargs.pop('derotate', False)

        self.maps = expand_list(args)

        for m in self.maps:
            if not isinstance(m, GenericMap):
                raise ValueError(
                           'CompositeMap expects pre-constructed map objects.')

        # Optionally sort data
        if sortby is not None:
            if sortby is 'date':
                self.maps.sort(key=self._sort_by_date())
            else:
                raise ValueError("Only sort by date is supported")

        if derotate:
            self._derotate()

    def __getitem__(self, key):
        """Overiding indexing operation"""
        return self.maps[key]

    # Sorting methods
    @classmethod
    def _sort_by_date(cls):
        return lambda m: m.date # maps.sort(key=attrgetter('date'))

    def _derotate(self):
        """Derotates the layers in the MapCube"""
        pass

    def plot(self, gamma=None, axes=None, resample=None, annotate=True,
             interval=200, **kwargs):
        """
        A animation plotting routine that animates each element in the
        MapCube

        Parameters
        ----------
        gamma: float
            Gamma value to use for the color map

        axes: mpl axes
            axes to plot the animation on, if none uses current axes

        resample: list or False
            Draws the map at a lower resolution to increase the speed of
            animation. Specify a list as a fraction i.e. [0.25, 0.25] to
            plot at 1/4 resolution.
            [Note: this will only work where the map arrays are the same size]

        annotate: bool
            Annotate the figure with scale and titles

        interval: int
            Animation interval in ms

        Examples
        --------
        >>> cube = sunpy.Map(files, cube=True)
        >>> ani = cube.plot(colorbar=True)
        >>> plt.show()

        Plot the map at 1/2 original resolution

        >>> cube = sunpy.Map(files, cube=True)
        >>> ani = cube.plot(resample=[0.5, 0.5], colorbar=True)
        >>> plt.show()

        Save an animation of the MapCube

        >>> cube = sunpy.Map(res, cube=True)

        >>> ani = cube.plot(controls=False)

        >>> Writer = animation.writers['ffmpeg']
        >>> writer = Writer(fps=10, metadata=dict(artist='SunPy'), bitrate=1800)

        >>> ani.save('mapcube_animation.mp4', writer=writer)
        """
        if not axes:
            axes = plt.gca()
        fig = axes.get_figure()

        # Normal plot
        def annotate_frame(i):
            axes.set_title("%s %s" % (self[i].name, self[i].date))

            # x-axis label
            if self[0].coordinate_system['x'] == 'HG':
                xlabel = 'Longitude [%s]' % self[i].units['x']
            else:
                xlabel = 'X-position [%s]' % self[i].units['x']

            # y-axis label
            if self[0].coordinate_system['y'] == 'HG':
                ylabel = 'Latitude [%s]' % self[i].units['y']
            else:
                ylabel = 'Y-position [%s]' % self[i].units['y']

            axes.set_xlabel(xlabel)
            axes.set_ylabel(ylabel)

        if gamma is not None:
            self[0].cmap.set_gamma(gamma)

        if resample:
            #This assumes that the maps a homogenous!
            #TODO: Update this!
            resample = np.array(len(self.maps)-1) * np.array(resample)
            ani_data = [x.resample(resample) for x in self]
        else:
            ani_data = self

        im = ani_data[0].plot(**kwargs)

        def updatefig(i, im, annotate, ani_data):

            im.set_array(ani_data[i].data)
            im.set_cmap(self[i].cmap)
            im.set_mpl_color_normalizer(self[i].mpl_color_normalizer)
            im.set_extent(self.xrange + self.yrange)
            if annotate:
                annotate_frame(i)

        ani = matplotlib.animation.FuncAnimation(fig, updatefig,
                                            frames=range(0,len(self.maps)),
                                            fargs=[im,annotate,ani_data],
                                            interval=interval,
                                            blit=False)

        return ani

    def peek(self, gamma=None, resample=None, **kwargs):
        """
        A animation plotting routine that animates each element in the
        MapCube

        Parameters
        ----------
        gamma: float
            Gamma value to use for the color map

        fig: mpl.figure
            Figure to use to create the explorer

        resample: list or False
            Draws the map at a lower resolution to increase the speed of
            animation. Specify a list as a fraction i.e. [0.25, 0.25] to
            plot at 1/4 resolution.
            [Note: this will only work where the map arrays are the same size]

        annotate: bool
            Annotate the figure with scale and titles

        interval: int
            Animation interval in ms

        colorbar: bool
            Plot colorbar

        Returns
        -------
        Returns a MapCubeAnimator object

        See Also
        --------
        sunpy.visualization.mapcubeanimator.MapCubeAnimator

        Examples
        --------
        >>> cube = sunpy.Map(files, cube=True)
        >>> ani = cube.plot(colorbar=True)
        >>> plt.show()

        Plot the map at 1/2 original resolution

        >>> cube = sunpy.Map(files, cube=True)
        >>> ani = cube.plot(resample=[0.5, 0.5], colorbar=True)
        >>> plt.show()

        Decide you want an animation:

        >>> cube = sunpy.Map(files, cube=True)
        >>> ani = cube.plot(resample=[0.5, 0.5], colorbar=True)
        >>> mplani = ani.get_animation()
        """

        if gamma is not None:
            self[0].cmap.set_gamma(gamma)

        if resample:
            #This assumes that the maps a homogenous!
            #TODO: Update this!
            resample = np.array(len(self.maps)-1) * np.array(resample)
            for amap in self.maps:
                amap.resample(resample)

        return MapCubeAnimator(self, **kwargs)

########NEW FILE########
__FILENAME__ = map_factory
from __future__ import absolute_import

__authors__ = ["Russell Hewett, Stuart Mumford"]
__email__ = "stuart@mumford.me.uk"

import os
import glob
import urllib2

import numpy as np

import sunpy
from sunpy.map import GenericMap
from sunpy.map.header import MapMeta
from sunpy.map.compositemap import CompositeMap
from sunpy.map.mapcube import MapCube

from sunpy.io.file_tools import read_file
from sunpy.io.header import FileHeader

from sunpy.util.net import download_file
from sunpy.util import expand_list
from sunpy.util import Deprecated

from sunpy.util.datatype_factory_base import BasicRegistrationFactory
from sunpy.util.datatype_factory_base import NoMatchError
from sunpy.util.datatype_factory_base import MultipleMatchError
from sunpy.util.datatype_factory_base import ValidationFunctionError

# Make a mock DatabaseEntry class if sqlalchemy is not installed

try:
    from sunpy.database.tables import DatabaseEntry
except ImportError:
    class DatabaseEntry:
        pass

__all__ = ['Map', 'MapFactory']

class MapFactory(BasicRegistrationFactory):
    """
    Map factory class.  Used to create a variety of Map objects.  Valid map types
    are specified by registering them with the factory.


    Examples
    --------
    >>> import sunpy.map
    >>> mymap = sunpy.map.Map(sunpy.AIA_171_IMAGE)

    The SunPy Map factory accepts a wide variety of inputs for creating maps

    * Preloaded tuples of (data, header) pairs

    >>> mymap = sunpy.map.Map((data, header))

    headers are some base of `dict` or `collections.OrderedDict`, including `sunpy.io.header.FileHeader` or `sunpy.map.header.MapMeta` classes.

    * data, header pairs, not in tuples

    >>> mymap = sunpy.map.Map(data, header)

    * File names

    >>> mymap = sunpy.map.Map('file1.fits')

    * All fits files in a directory by giving a directory

    >>> mymap = sunpy.map.Map('local_dir/sub_dir')

    * Some regex globs

    >>> mymap = sunpy.map.Map('eit_*.fits')

    * URLs

    >>> mymap = sunpy.map.Map(url_str)

    * DatabaseEntry

    >>> mymap = sunpy.map.Map(db_result)

    * Lists of any of the above

    >>> mymap = sunpy.Map(['file1.fits', 'file2.fits', 'file3.fits', 'directory1/'])

    * Any mixture of the above not in a list

    >>> mymap = sunpy.Map((data, header), data2, header2, 'file1.fits', url_str, 'eit_*.fits')
    """

    def _read_file(self, fname, **kwargs):
        """ Read in a file name and return the list of (data, meta) pairs in
            that file. """

        # File gets read here.  This needs to be generic enough to seamlessly
        #call a fits file or a jpeg2k file, etc
        pairs = read_file(fname, **kwargs)

        new_pairs = []
        for pair in pairs:
            filedata, filemeta = pair
            assert isinstance(filemeta, FileHeader)
            #This tests that the data is more than 1D
            if len(np.shape(filedata)) > 1:
                data = filedata
                meta = MapMeta(filemeta)
                new_pairs.append((data, meta))
        return new_pairs

    def _parse_args(self, *args, **kwargs):
        """
        Parses an args list for data-header pairs.  args can contain any mixture
        of the following entries:
        * tuples of data,header
        * data, header not in a tuple
        * filename, which will be read
        * directory, from which all files will be read
        * glob, from which all files will be read
        * url, which will be downloaded and read
        * lists containing any of the above.

        Example
        -------
        self._parse_args(data, header,
                         (data, header),
                         ['file1', 'file2', 'file3'],
                         'file4',
                         'directory1',
                         '*.fits')

        """

        data_header_pairs = list()
        already_maps = list()

        # Account for nested lists of items
        args = expand_list(args)

        # For each of the arguments, handle each of the cases
        i = 0
        while i < len(args):

            arg = args[i]

            # Data-header pair in a tuple
            if ((type(arg) in [tuple, list]) and
                 len(arg) == 2 and
                 isinstance(arg[0],np.ndarray) and
                 isinstance(arg[1],dict)):
                data_header_pairs.append(arg)

            # Data-header pair not in a tuple
            elif (isinstance(arg, np.ndarray) and
                  isinstance(args[i+1],dict)):
                pair = (args[i], args[i+1])
                data_header_pairs.append(pair)
                i += 1 # an extra increment to account for the data-header pairing

            # File name
            elif (isinstance(arg,basestring) and
                  os.path.isfile(os.path.expanduser(arg))):
                path = os.path.expanduser(arg)
                pairs = self._read_file(path, **kwargs)
                data_header_pairs += pairs

            # Directory
            elif (isinstance(arg,basestring) and
                  os.path.isdir(os.path.expanduser(arg))):
                path = os.path.expanduser(arg)
                files = [os.path.join(path, elem) for elem in os.listdir(path)]
                for afile in files:
                    data_header_pairs += self._read_file(afile, **kwargs)

            # Glob
            elif (isinstance(arg,basestring) and '*' in arg):
                files = glob.glob( os.path.expanduser(arg) )
                for afile in files:
                    data_header_pairs += self._read_file(afile, **kwargs)

            # Already a Map
            elif isinstance(arg, GenericMap):
                already_maps.append(arg)

            # A URL
            elif (isinstance(arg,basestring) and
                  _is_url(arg)):
                default_dir = sunpy.config.get("downloads", "download_dir")
                url = arg
                path = download_file(url, default_dir)
                pairs = self._read_file(path, **kwargs)
                data_header_pairs += pairs

            # A database Entry
            elif isinstance(arg, DatabaseEntry):
                data_header_pairs += self._read_file(arg.path, **kwargs)

            else:
                raise ValueError("File not found or invalid input")

            i += 1
        #TODO:
        # In the end, if there are aleady maps it should be put in the same
        # order as the input, currently they are not.
        return data_header_pairs, already_maps


    def __call__(self, *args, **kwargs):
        """ Method for running the factory. Takes arbitrary arguments and
        keyword arguments and passes them to a sequence of pre-registered types
        to determine which is the correct Map-type to build.

        Arguments args and kwargs are passed through to the validation
        function and to the constructor for the final type.  For Map types,
        validation function must take a data-header pair as an argument.

        Parameters
        ----------

        composite : boolean, optional
            Indicates if collection of maps should be returned as a CompositeMap

        cube : boolean, optional
            Indicates if collection of maps should be returned as a MapCube

        silence_errors : boolean, optional
            If set, ignore data-header pairs which cause an exception.

        """

        # Hack to get around Python 2.x not backporting PEP 3102.
        composite = kwargs.pop('composite', False)
        cube = kwargs.pop('cube', False)
        silence_errors = kwargs.pop('silence_errors', False)

        data_header_pairs, already_maps = self._parse_args(*args, **kwargs)

        new_maps = list()

        # Loop over each registered type and check to see if WidgetType
        # matches the arguments.  If it does, use that type.
        for pair in data_header_pairs:
            data, header = pair
            meta = MapMeta(header)

            try:
                new_map = self._check_registered_widgets(data, meta, **kwargs)
            except (NoMatchError, MultipleMatchError, ValidationFunctionError):
                if not silence_errors:
                    raise
            except:
                raise

            new_maps.append(new_map)

        new_maps += already_maps

        # If the list is meant to be a cube, instantiate a map cube
        if cube:
            return MapCube(new_maps, **kwargs)

        # If the list is meant to be a composite mape, instantiate one
        if composite:
            return CompositeMap(new_maps, **kwargs)

        if len(new_maps) == 1:
            return new_maps[0]

        return new_maps

    def _check_registered_widgets(self, data, meta, **kwargs):

        candidate_widget_types = list()

        for key in self.registry:

            # Call the registered validation function for each registered class
            if self.registry[key](data, meta, **kwargs):
                candidate_widget_types.append(key)

        n_matches = len(candidate_widget_types)

        if n_matches == 0:
            if self.default_widget_type is None:
                raise NoMatchError("No types match specified arguments and no default is set.")
            else:
                candidate_widget_types = [self.default_widget_type]
        elif n_matches > 1:
            raise MultipleMatchError("Too many candidate types idenfitied ({0}).  Specify enough keywords to guarantee unique type identification.".format(n_matches))

        # Only one is found
        WidgetType = candidate_widget_types[0]

        return WidgetType(data, meta, **kwargs)


def _is_url(arg):
    try:
        urllib2.urlopen(arg)
    except:
        return False
    return True

@Deprecated("Please use the new factory sunpy.Map")
def make_map(*args, **kwargs):
    __doc__ = MapFactory.__doc__
    return Map(*args, **kwargs)

class InvalidMapInput(ValueError):
    """Exception to raise when input variable is not a Map instance and does
    not point to a valid Map input file."""
    pass

class InvalidMapType(ValueError):
    """Exception to raise when an invalid type of map is requested with make_map
    """
    pass

class NoMapsFound(ValueError):
    """Exception to raise when input does not point to any valid maps or files
    """
    pass

Map = MapFactory(default_widget_type=GenericMap,
                 additional_validation_functions=['is_datasource_for'])

########NEW FILE########
__FILENAME__ = hinode
"""Hinode XRT and SOT Map subclass definitions"""
#pylint: disable=W0221,W0222,E1101,E1121

__author__ = ["Jack Ireland, Jose Ivan Campos-Rozo, David Perez-Suarez"]
__email__ = "jack.ireland@nasa.gov"

import numpy as np
from matplotlib import colors

from sunpy.map import GenericMap
from sunpy.cm import cm


__all__ = ['XRTMap', 'SOTMap']

def _lower_list(L):
    return [item.lower() for item in L]

class XRTMap(GenericMap):
    """XRT Image Map definition
    
    References
    ----------
    For a description of XRT headers
    """
    #TODO: get a link for the XRT FITS headers
    # Add in some information about the the possible filter wheel measurements
    filter_wheel1_measurements = ["Al_med", "Al_poly", "Be_med",
                                  "Be_thin", "C_poly", "Open"]
    filter_wheel2_measurements = ["Open", "Al_mesh", "Al_thick",
                                  "Be_thick", "Gband", "Ti_poly"]
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        fw1 = header.get('EC_FW1_')
        if fw1.lower() not in _lower_list(self.filter_wheel1_measurements):
            raise ValueError('Unpexpected filter wheel 1 in header.')
        fw1 = fw1.replace("_", " ")    
            
        fw2 = header.get('EC_FW2_')
        if fw2.lower() not in _lower_list(self.filter_wheel2_measurements):
            raise ValueError('Unpexpected filter wheel 2 in header.')
        fw2 = fw2.replace("_", " ")
        
        self.meta['detector'] = "XRT"
#        self.meta['instrume'] = "XRT"
        self.meta['telescop'] = "Hinode"
        
        self._name = "{0} {1}-{2}".format(self.detector, fw1, fw2)
        self._nickname = self.detector
        
        self.cmap = cm.get_cmap(name='hinodexrt')

    def _get_mpl_normalizer(self):
        """Returns a Normalize object to be used with XRT data"""
        # byte-scaled images have most likely already been scaled
        if self.dtype == np.uint8:
            return None

        mean = self.mean()
        std = self.std()
        
        vmin = max(0, mean - 3 * std)
        vmax = min(self.max(), mean + 3 * std)
        
        return colors.Normalize(vmin, vmax)


    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an XRT image"""
        return header.get('instrume') == 'XRT'
        
class SOTMap(GenericMap):
	"""SOT Image Map definition
	
	References
	----------
	For a description of SOT headers
	"""
	#TODO: get a link for the SOT FITS headers
	# Add in some information about the the possible instrument, observation type,
	# observable ion and wavelength
	
	Instruments = ['SOT/WB','SOT/NB','SOT/SP','SOT/CT']

	Waves = ['6302A', 'BFI no move', 'CN bandhead 3883',
                 'Ca II H line', 'G band 4305', 'NFI no move',
                 'TF Fe I 6302', 'TF Mg I 5172', 'TF Na I 5896', 
                 'blue cont 4504', 'green cont 5550', 'red cont 6684']

	Observation_Type = ['FG (simple)', 'FG focus scan',
                            'FG shuttered I and V', 'FG shutterless I and V',
                            'FG shutterless I and V with 0.2s intervals',
                            'FG shutterless Stokes', 'SP IQUV 4D array']
	
	def __init__(self, data, header, **kwargs):
		GenericMap.__init__(self, data, header, **kwargs)

		self.meta['detector'] = "SOT"
		self.meta['telescop'] = "Hinode"
		
		self._name = self.observatory + '/' + self.instrument
		self._nickname = self.detector

                #TODO (add other options, Now all threated as intensity. This followes Hinode SDC archive)
		# StokesQUV -> grey, Velocity -> EIS, Width -> EIS, Mag Field Azi -> IDL 5 (STD gamma II)
                #'WB' -> red
		#'NB'(0 = red); (>0 = gray), # nb has 1 stokes I, the rest quv 
                #'SP' (<=1 = red); (>1 = gray) #sp has 2 stokes I, the rest quv
		color = {'SOT/WB': 'intensity', 
			 'SOT/NB': 'intensity', # For the 1st dimmension
			 'SOT/SP': 'intensity', # For the 1st 2 dimmensions
			 }

		self.cmap = cm.get_cmap('hinodesot' + color[self.instrument])


	@classmethod
	def is_datasource_for(cls, data, header, **kwargs):
		"""Determines if header corresponds to an SOT image"""
		
		return header.get('instrume') in cls.Instruments

########NEW FILE########
__FILENAME__ = iris
from __future__ import absolute_import

import numpy as np

from sunpy.map import GenericMap

__all__ = ['IRISMap']

class IRISMap(GenericMap):
    """
    A 2D IRIS Map
    """
    
    def __init__(self, data, header, **kwargs):    
        GenericMap.__init__(self, data, header, **kwargs)

    def iris_rot(self, missing=0.0, interpolation='bicubic', interp_param=-0.5):
        """
        Return aligned map based on header keywords
        
        Parameters
        ----------
        missing: float
           The numerical value to fill any missing points after rotation.
           Default: 0.0
        interpolation: {'nearest' | 'bilinear' | 'spline' | 'bicubic'}
            Interpolation method to use in the transform. 
            Spline uses the 
            scipy.ndimage.interpolation.affline_transform routine.
            nearest, bilinear and bicubic all replicate the IDL rot() function.
            Default: 'bicubic'
        interp_par: Int or Float
            Optional parameter for controlling the interpolation.
            Spline interpolation requires an integer value between 1 and 5 for 
            the degree of the spline fit.
            Default: 3
            BiCubic interpolation requires a flaot value between -1 and 0.
            Default: 0.5
            Other interpolation options ingore the argument.
            
        Returns
        -------
        New rotated, rescaled, translated map
        
        Notes
        -----
        Apart from interpolation='spline' all other options use a compiled 
        C-API extension. If for some reason this is not compiled correctly this
        routine will fall back upon the scipy implementation of order = 3.
        For more infomation see:
            http://sunpy.readthedocs.org/en/latest/guide/troubleshooting.html#crotate-warning
        """
        
        cords = np.matrix([[self.meta['pc1_1'], self.meta['pc1_2']],
                           [self.meta['pc2_1'], self.meta['pc2_2']]])
        center = [self.meta['CRPIX1'], self.meta['CRPIX2']]
        
        #Return a new map
        img2 = self.rotate(rmatrix=cords, rotation_center=center, recenter=False, 
                           missing=missing, interpolation=interpolation,
                           interp_param=interp_param)
             
        # modify the header to show the fact it's been corrected
        img2.meta['pc1_1'] = 1
        img2.meta['pc1_2'] = 0
        img2.meta['pc2_1'] = 0
        img2.meta['pc2_2'] = 1
    
        return img2    

    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an AIA image"""
        tele = header.get('TELESCOP', '').startswith('IRIS')
        obs = header.get('INSTRUME', '').startswith('SJI')
        return tele and obs
########NEW FILE########
__FILENAME__ = proba2
"""PROBA2 Map subclass definitions"""
#pylint: disable=W0221,W0222,E1101,E1121

__author__ = "Keith Hughitt"
__email__ = "keith.hughitt@nasa.gov"

from sunpy.map import GenericMap
from sunpy.cm import cm

__all__ = ['SWAPMap']

class SWAPMap(GenericMap):
    """SWAP Image Map definition
    
    References
    ----------
    For a description of SWAP headers
    http://proba2.oma.be/index.html/swap/swap-analysis-manual/article/data-products?menu=23
    """
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)

		# It needs to be verified that these must actually be set and are not
		# already in the header.
        self.meta['detector'] = "SWAP"
#        self.meta['instrme'] = "SWAP"
        self.meta['obsrvtry'] = "PROBA2"
        
        self._name = self.detector + " " + str(self.measurement)
        self._nickname = self.detector
        
        self.cmap = cm.get_cmap(name='sdoaia171')
            

    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an SWAP image"""
        return header.get('instrume') == 'SWAP'

########NEW FILE########
__FILENAME__ = rhessi
"""RHESSI Map subclass definitions"""
#pylint: disable=W0221,W0222,E1121

__author__ = "Steven Christe"
__email__ = "steven.d.christe@nasa.gov"

from sunpy.map import GenericMap

__all__ = ['RHESSIMap']

class RHESSIMap(GenericMap):
    """RHESSI Image Map definition
    
    References
    ----------
    For a description of RHESSI image fits headers
    ???

    TODO: Currently (8/29/2011), cannot read fits files containing more than one 
    image (schriste)
    """
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        self._name = "RHESSI %d - %d keV" % (self.measurement[0], self.measurement[1])
        self._nickname = self.detector
        
        # Fix some broken/misapplied keywords
        if self.meta['ctype1'] == 'arcsec':
            self.meta['cunit1'] = 'arcsec'
            self.meta['ctype1'] = 'HPLN-TAN'
        if self.meta['ctype2'] == 'arcsec':
            self.meta['cunit2'] = 'arcsec'
            self.meta['ctype2'] = 'HPLT-TAN'
        
    @property
    def measurement(self):
        return [self.meta['energy_l'], self.meta['energy_h']]
    
    @property
    def detector(self):
        return self.meta['telescop']    
    
    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an RHESSI image"""
        return header.get('instrume') == 'RHESSI'


########NEW FILE########
__FILENAME__ = sdo
"""SDO Map subclass definitions"""
#pylint: disable=W0221,W0222,E1101,E1121

__author__ = "Keith Hughitt"
__email__ = "keith.hughitt@nasa.gov"

import numpy as np
from matplotlib import colors

from sunpy.map import GenericMap
from sunpy.cm import cm

__all__ = ['AIAMap', 'HMIMap']

class AIAMap(GenericMap):
    """AIA Image Map definition
    
    References
    ----------
    For a description of AIA headers
    http://jsoc.stanford.edu/doc/keywords/AIA/AIA02840_A_AIA-SDO_FITS_Keyword_Documents.pdf
    """
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        # Fill in some missing info
        self.meta['detector'] = "AIA"
#        self.meta['instrme'] = "AIA"
        
        self._nickname = self.detector
        self._name = self.detector + " " + str(self.measurement)
        
        self.cmap = cm.get_cmap('sdoaia%d' % self.wavelength)
    
    @property
    def observatory(self):
        return self.meta['telescop'].split('/')[0]
        
    @property
    def processing_level(self):
        return self.meta['lvl_num']

    def _get_mpl_normalizer(self):
        """Returns a Normalize object to be used with AIA data"""
        # byte-scaled images have most likely already been scaled
        if self.data.dtype == np.uint8:
            return None

        mean = self.mean()
        std = self.std()
        
        vmin = max(0, mean - 3 * std)
        vmax = min(self.max(), mean + 3 * std)
        
        return colors.Normalize(vmin, vmax)
    
    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an AIA image"""
        return header.get('instrume', '').startswith('AIA')
        
class HMIMap(GenericMap):
    """HMI Image Map definition"""
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        self.meta['detector'] = "HMI"
#        self.meta['instrme'] = "HMI"
#        self.meta['obsrvtry'] = "SDO"

        self._name = self.detector + " " + str(self.measurement)
        self._nickname = self.detector
    
    @property
    def measurement(self):
        return self.meta['content'].split(" ")[0].lower()
    
    @property
    def observatory(self):
        return self.meta['telescop'].split('/')[0]    

    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an HMI image"""
        return header.get('instrume', '').startswith('HMI') 

########NEW FILE########
__FILENAME__ = soho
"""SOHO Map subclass definitions"""
#pylint: disable=W0221,W0222,E1101,E1121

__author__ = "Keith Hughitt"
__email__ = "keith.hughitt@nasa.gov"

import numpy as np
from matplotlib import colors

from sunpy.map import GenericMap
from sunpy.sun import constants
from sunpy.sun import sun
from sunpy.cm import cm

__all__ = ['EITMap', 'LASCOMap', 'MDIMap']

def _dsunAtSoho(date, rad_d, rad_1au = None):
    """Determines the distance to the Sun from SOhO following
    d_{\sun,Object} =
            D_{\sun\earth} \frac{\tan(radius_{1au}[rad])}{\tan(radius_{d}[rad])}
    though tan x ~ x for x << 1
    d_{\sun,Object} =
            D_{\sun\eart} \frac{radius_{1au}[rad]}{radius_{d}[rad]}
    since radius_{1au} and radius_{d} are dividing each other we can use [arcsec]
    instead. 

    ---
    TODO: Does this apply just to observations on the same Earth-Sun line?
    If not it can be moved outside here.
    """
    if not rad_1au:
        rad_1au = sun.solar_semidiameter_angular_size(date)
    dsun = sun.sunearth_distance(date) * constants.au * (rad_1au / rad_d)
    #return scalar value not astropy.quantity
    return dsun.value


class EITMap(GenericMap):
    """EIT Image Map definition"""
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        # Fill in some missing info
        self.meta['detector'] = "EIT"
        self._fix_dsun()
        
        self._name = self.detector + " " + str(self.measurement)
        self._nickname = self.detector
        
        self.cmap = cm.get_cmap('sohoeit%d' % self.wavelength)
    
    @property
    def rsun_arcseconds(self):
        return self.meta['solar_r'] * self.meta['cdelt1']
        
    def _fix_dsun(self):
        dsun = _dsunAtSoho(self.date, self.rsun_arcseconds)
        self.meta['dsun_obs'] = dsun

    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an EIT image"""
        return header.get('instrume') == 'EIT'

    def _get_mpl_normalizer(self):
        """Returns a Normalize object to be used with EIT data"""
        # byte-scaled images have most likely already been scaled
        # THIS WARNING IS KNOWN TO APPLY TO 0.3 code only.
        # NOT TESTED in sunpy 0.4 when the glymur library
        # is used instead of pyopenjpeg.  It seems that EIT JP2 files read by
        # pyopenjpeg and openjpeg using the j2k_to_image command, returns 
        # np.float32 arrays.  For comparison, AIA JP2 files read the same way
        # return np.uint8 arrays.  EIT JP2 files have already been
        # byte-scaled when they are created by the Helioviewer Project.
        # SunPy 0.3 and lower code assumes that if datatype of the data array
        # was np.uint8 then the image was highly likely to be byte-scaled
        # Since the data datatype was in fact a np.float32, then the byte-scaling
        # was never picked up.
        if self.data.dtype == np.float32:
            return None
        
        mean = self.mean()
        std = self.std()
        
        vmin = 1
        vmax = min(self.max(), mean + 5 * std)

        return colors.LogNorm(vmin, vmax)

class LASCOMap(GenericMap):
    """LASCO Image Map definition"""
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        # Fill in some missing or broken info
        datestr = "%sT%s" % (self.meta.get('date-obs',self.meta.get('date_obs')),
                     self.meta.get('time-obs',self.meta.get('time_obs')))
        self.meta['date-obs'] = datestr

        # If non-standard Keyword is present, correct it too, for compatibility.
        if 'date_obs' in self.meta:
            self.meta['date_obs'] = self.meta['date-obs']

        self._name = self.instrument + " " + self.detector + " " + self.measurement
        self._nickname = self.instrument + "-" + self.detector
        self.cmap = cm.get_cmap('soholasco%s' % self.detector[1])
        
    @property
    def measurement(self):
        # TODO: This needs to do more than white-light.  Should give B, pB, etc.
        return "white-light"

    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an LASCO image"""
        return header.get('instrume') == 'LASCO'
        
class MDIMap(GenericMap):
    """MDI Image Map definition"""
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        # Fill in some missing or broken info
        self.meta['detector'] = "MDI"
        self._fix_dsun()
        
        self._name = self.detector + " " + self.measurement
        self._nickname = self.detector + " " + self.measurement
        
    @property
    def measurement(self):
        # TODO: This needs to do more than white-light.  Should give B, pB, etc.
        return "magnetogram" if self.meta['dpc_obsr'].find('Mag') != -1 else "continuum"
        
    def _fix_dsun(self):
        """ Solar radius in arc-seconds at 1 au
            previous value radius_1au = 959.644
            radius = constants.average_angular_size
            There are differences in the keywords in the test FITS data and in
            the Helioviewer JPEG2000 files.  In both files, MDI stores the
            the radius of the Sun in image pixels, and a pixel scale size.
            The names of these keywords are different in the FITS versus the
            JP2 file.  The code below first looks for the keywords relevant to
            a FITS file, and then a JPEG2000 file.  For more information on
            MDI FITS header keywords please go to http://soi.stanford.edu/,
            http://soi.stanford.edu/data/ and
            http://soi.stanford.edu/magnetic/Lev1.8/ .
        """
        scale = self.meta.get('xscale', self.meta.get('cdelt1'))
        radius_in_pixels = self.meta.get('r_sun', self.meta.get('radius'))
        radius = scale * radius_in_pixels
        self.meta['radius'] = radius
        
        if not radius:
#            radius = sun.angular_size(self.date)
            self.meta['dsun_obs'] = constants.au
        else:
            self.meta['dsun_obs'] = _dsunAtSoho(self.date, radius)
        
    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an MDI image"""
        return header.get('instrume') == 'MDI'


########NEW FILE########
__FILENAME__ = stereo
"""STEREO Map subclass definitions"""
#pylint: disable=W0221,W0222,E1121

__author__ = "Keith Hughitt"
__email__ = "keith.hughitt@nasa.gov"

from sunpy.map import GenericMap
from sunpy.cm import cm

__all__ = ['EUVIMap', 'CORMap']

class EUVIMap(GenericMap):
    """EUVI Image Map definition"""
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        self._name = self.observatory + " " + self.detector + " " + str(self.measurement)
        self._nickname = "{0}-{1}".format(self.detector, self.observatory[-1])
        
        self.cmap = cm.get_cmap('sohoeit%d' % self.wavelength)

        # Try to identify when the FITS meta data ddes not have the correct
        # date FITS keyword
        if ('date_obs' in self.meta) and not('date-obs' in self.meta):
            self.meta['date-obs'] = self.meta['date_obs']

    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an EUVI image"""
        return header.get('detector') == 'EUVI'
        
class CORMap(GenericMap):
    """COR Image Map definition"""
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        self._name = self.observatory + " " + self.detector + " " + str(self.measurement)
        self._nickname = "{0}-{1}".format(self.detector, self.observatory[-1])
        
        self.cmap = cm.get_cmap('stereocor%s' % self.detector[-1])

        # Try to identify when the FITS meta data ddes not have the correct
        # date FITS keyword
        if ('date_obs' in self.meta) and not('date-obs' in self.meta):
            self.meta['date-obs'] = self.meta['date_obs']
        
    @property
    def measurement(self):
        # TODO: This needs to do more than white-light.  Should give B, pB, etc.
        return "white-light"

    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an COR image"""
        return header.get('detector', '').startswith('COR')
        

########NEW FILE########
__FILENAME__ = trace
"""TRACE Map subclass definitions"""
#pylint: disable=W0221,W0222,E1101,E1121

__author__ = "Jack Ireland"
__email__ = "jack.ireland@nasa.gov"

from sunpy.map import GenericMap
from sunpy.cm import cm
import numpy as np
from matplotlib import colors

__all__ = ['TRACEMap']

class TRACEMap(GenericMap):
    """TRACE Image Map definition
References
----------
For a description of TRACE headers see:
http://trace.lmsal.com/Project/Instrument/cal/
For a description of the TRACE mission, TRACE analysis guide (SSWIDL),
and all things TRACE (images, movies, galleries, science results), please
see:
http://trace.lmsal.com/
Note that this map definition is currently only being tested on JPEG2000
files. TRACE FITS data is stored in a more complex format. Typically
TRACE data is stored in hourly "tri" files that store all the data taken
by TRACE in the hour indicated by the filename. Those files must first be
understood and parsed to obtain the science data. The ability to do this
is not yet in SunPy, but is available in SSWIDL. Please refer to the links
above concerning how to read "tri" files in SSWIDL.
"""
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)

        # It needs to be verified that these must actually be set and are not
        # already in the header.
        self.meta['detector'] = "TRACE"
        self.meta['obsrvtry'] = "TRACE"

        # Name that will appear at the top of a TRACE image plot
        self._name = self.detector + " " + self.measurement
        self._nickname = self.detector

        # Colour maps
        self.cmap = cm.get_cmap('trace' + self.measurement)

    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an TRACE image"""
        return header.get('instrume') == 'TRACE'
    
    @property
    def measurement(self):
        return str(self.meta['wave_len'])

    def _get_mpl_normalizer(self):
        """Returns a Normalize object to be used with TRACE data"""
        # byte-scaled images have most likely already been scaled
        if self.dtype == np.uint8:
            return None
        
        mean = self.mean()
        std = self.std()
        
        vmin = 1
        vmax = min(self.max(), mean + 5 * std)

        return colors.LogNorm(vmin, vmax)

########NEW FILE########
__FILENAME__ = yohkoh
"""Yohkoh SXT Map subclass definitions"""
#pylint: disable=W0221,W0222,E1101,E1121

__author__ = "Jack Ireland"
__email__ = "jack.ireland@nasa.gov"

import numpy as np
from matplotlib import colors

from sunpy.map import GenericMap
from sunpy.cm import cm
from sunpy.sun import constants

__all__ = ['SXTMap']

class SXTMap(GenericMap):
    """SXT Image Map definition
    
    References
    ----------
    For a description of SXT headers
    http://proba2.oma.be/index.html/swap/swap-analysis-manual/article/data-products?menu=23
    """
    
    def __init__(self, data, header, **kwargs):
        
        GenericMap.__init__(self, data, header, **kwargs)
        
        self.meta['detector'] = "SXT"
        self.meta['telescop'] = "Yohkoh"
        
        self._name = self.observatory + " " + self.wavelength_string
    
        self.cmap = cm.get_cmap(name='yohkohsxt' + self.wavelength_string[0:2].lower())
    
        # 2012/12/19 - the SXT headers do not have a value of the distance from
        # the spacecraft to the center of the Sun.  The FITS keyword 'DSUN_OBS'
        # appears to refer to the observed diameter of the Sun.  Until such 
        # time as that is calculated and properly included in the file, we will 
        # use simple trigonometry to calculate the distance of the center of 
        # the Sun from the spacecraft.  Note that the small angle approximation
        # is used, and the solar radius stored in SXT FITS files is in arcseconds.
        self.meta['dsun_apparent'] = constants.au
        if 'solar_r' in self.meta:
            self.meta['dsun_apparent'] = constants.radius/(np.deg2rad(self.meta['solar_r']/3600.0))
   
    @property
    def dsun(self):
        """ For Yohkoh Maps, dsun_obs is not always defined. Uses approximation
        defined above it is not defined."""
        return self.meta.get('dsun_obs', self.meta['dsun_apparent'])
    
    @property
    def wavelength_string(self):
        s = self.meta.get('wavelnth', '')
        if s == 'Al.1':
            s = 'Al01' 
        elif s.lower() ==  'open':
            s = 'white light'
        return s

    def _get_mpl_normalizer(self):
        """Returns a Normalize object to be used with SXT data"""
        # byte-scaled images have most likely already been scaled
        if self.dtype == np.uint8:
            return None

        mean = self.mean()
        std = self.std()
        
        vmin = max(0, mean - 3 * std)
        vmax = min(self.max(), mean + 3 * std)
        
        return colors.Normalize(vmin, vmax)

    @classmethod
    def is_datasource_for(cls, data, header, **kwargs):
        """Determines if header corresponds to an SXT image"""
        return header.get('instrume') == 'SXT'

########NEW FILE########
__FILENAME__ = test_header
# -*- coding: utf-8 -*-

import sunpy
import sunpy.map
#==============================================================================
# Test, read, get_header and write through the file independant layer
#==============================================================================
class TestMapMeta():
    
    def test_upcasing(self):
        meta = sunpy.map.MapMeta({'wibble':1, 'WOBBLE':2})
        #__getitem__
        assert meta['wibble'] == meta['WIBBLE']
        #get
        assert meta.get('wibble') == meta.get('WIBBLE')
        #has_key
        assert meta.has_key('wibble') == meta.has_key('WIBBLE')
        #Copy
        meta2 = meta.copy()
        assert meta2 == meta
        #pop
        assert meta.pop('wibble') == meta2.pop('WIBBLE')
        #update
        meta.update({'spam':'eggs'})
        meta2.update({'SPAM':'eggs'})
        assert meta == meta2
        #setdefault
        meta.setdefault('dave',3)
        meta2.setdefault('DAVE',3)
        assert meta.get('DAVE') == meta2.get('dave')
        #__setitem__
        meta['wibble'] = 10
        assert meta['wibble'] == 10
        meta['WIBBLE'] = 20
        assert meta['wibble'] == 20
        #__contains__
        assert 'wibble' in meta
        assert 'WIBBLE' in meta

########NEW FILE########
__FILENAME__ = test_mapbase
"""
Map tests
"""
from __future__ import absolute_import

#pylint: disable=C0103,R0904,W0201,W0212,W0232,E1103
import sunpy
import sunpy.map

from astropy.io import fits
import numpy as np

from itertools import izip

import pytest
     
# Try different dimensions to ensure that the resample method works
# correctly in cases where the dimensions of the original map are
# are exactly divisible by those of the output map as well as the cases
# in which they aren't.
resample_params = [
    ('linear', (100, 200)),
    ('neighbor', (128, 256)),
    ('nearest', (512, 128)),
    ('spline', (200, 200)),
]

class TestGenericMap:
    """Tests the Map class"""
    def setup_class(self):
        self.file = sunpy.AIA_171_IMAGE
        self.map = sunpy.map.Map(self.file)
        self.fits = fits.open(self.file)
        self.fits.verify('silentfix')

        # include full comment
        fits_comment = self.fits[0].header['COMMENT']

        # PyFITS 2.x
        if isinstance(fits_comment[0], basestring):
            comments = [val for val in fits_comment]
        else:
            # PyFITS 3.x
            comments = [card.value for card in fits_comment]
        comment = "".join(comments).strip()

        # touch data to apply scaling up front
        self.fits[0].data

        self.fits[0].header['COMMENT'] = comment

    def teardown_class(self):
        self.map = None
        self.fits = None

    def test_data_to_pixel(self):
        """Make sure conversion from data units to pixels is accurate"""
        # Check conversion of reference pixel
        # Note: FITS pixels starts from 1,1
        assert self.map.data_to_pixel(self.map.meta['crval1'], 'x') == self.map.meta['crpix1'] - 1
        assert self.map.data_to_pixel(self.map.meta['crval2'], 'y') == self.map.meta['crpix2'] - 1

        # Check conversion of map center
        assert self.map.data_to_pixel(self.map.center['x'], 'x') == (self.map.meta['naxis1'] - 1) / 2.
        assert self.map.data_to_pixel(self.map.center['y'], 'y') == (self.map.meta['naxis2'] - 1) / 2.

        # Check conversion of map edges
        # Note: data coords are at pixel centers, so edges are 0.5 pixels wider
        assert self.map.data_to_pixel(self.map.xrange[0], 'x') == 0. - 0.5
        assert self.map.data_to_pixel(self.map.yrange[0], 'y') == 0. - 0.5
        assert self.map.data_to_pixel(self.map.xrange[1], 'x') == (self.map.meta['naxis1'] - 1) + 0.5
        assert self.map.data_to_pixel(self.map.yrange[1], 'y') == (self.map.meta['naxis2'] - 1) + 0.5

    def test_data_range(self):
        """Make sure xrange and yrange work"""
        assert self.map.xrange[1] - self.map.xrange[0] == self.map.meta['cdelt1'] * self.map.meta['naxis1']
        assert self.map.yrange[1] - self.map.yrange[0] == self.map.meta['cdelt2'] * self.map.meta['naxis2']

        assert np.average(self.map.xrange) == self.map.center['x']
        assert np.average(self.map.yrange) == self.map.center['y']

    def test_submap(self):
        """Check data and header information for a submap"""
        width = self.map.shape[1]
        height = self.map.shape[0]

        # Create a submap of the top-right quadrant of the image
        submap = self.map.submap([height/2.,height], [width/2.,width],
                                 units='pixels')

        # Expected offset for center
        offset = {
            "x": self.map.meta['crpix1'] - width / 2.,
            "y": self.map.meta['crpix2'] - height / 2.,
        }

        # Check to see if submap properties were updated properly
        assert submap.reference_pixel['x'] == offset['x']
        assert submap.reference_pixel['y'] == offset['y']
        assert submap.shape[0] == width / 2.
        assert submap.shape[1] == height / 2.

        # Check to see if header was updated
        assert submap.meta['naxis1'] == width / 2.
        assert submap.meta['naxis2'] == height / 2.

        # Check data
        assert (self.map.data[height/2:height,
                                     width/2:width] == submap.data).all()

    def test_fits_data_comparison(self):
        """Make sure the data is the same in pyfits and SunPy"""
        assert (self.map.data == self.fits[0].data).all()

#TODO: What the really?
#    def test_original_header_comparison(self):
#        """Make sure the header is the same in pyfits and SunPy.
#
#        PyFITS makes a number of changes to the data and header when reading
#        it in including applying scaling and removing the comment from the
#        header cards to handle it separately.
#
#        The manipulations in the setup_class method and here attempt to
#        level the playing field some so that the rest of the things that
#        should be the same can be tested.
#
#        // Keith, July 2012
#        """
#
#        # Access fits data once to apply scaling-related changes and update
#        # header information in fits[0].header
#        #self.fits[0].data #pylint: disable=W0104
#
#        fits_header = dict(self.fits[0].header)
#        map_header = self.map.meta
#
#        # Ignore fields modified by PyFITS
#        for key in ['COMMENT', 'BZERO', 'BSCALE', 'BITPIX']:
#            if key in fits_header:
#                del fits_header[key]
#            if key in map_header:
#                del map_header[key]
#
#        # Remove empty field (newline?) that is added when data is accesed for first time
#        if '' in fits_header:
#            fits_header.pop('')
#
#        for k,v in map_header.items():
#            if v != fits_header[k]:
#                print k
#
#        assert map_header == fits_header

    @pytest.mark.parametrize('sample_method,new_dimensions', resample_params)
    def test_resample_dimensions(self, sample_method, new_dimensions):
        """Check that resampled map has expected dimensions."""
        resampled_map = self.map.resample(new_dimensions, method=sample_method)
        assert resampled_map.shape[1] == new_dimensions[0]
        assert resampled_map.shape[0] == new_dimensions[1]

    @pytest.mark.parametrize('sample_method,new_dimensions', resample_params)
    def test_resample_metadata(self, sample_method, new_dimensions):
        """
        Check that the resampled map has correctly adjusted metadata.
        """
        resampled_map = self.map.resample(new_dimensions, method=sample_method)
        assert float(resampled_map.meta['cdelt1']) / self.map.meta['cdelt1'] \
            == float(self.map.shape[1]) / resampled_map.shape[1]
        assert float(resampled_map.meta['cdelt2']) / self.map.meta['cdelt2'] \
            == float(self.map.shape[0]) / resampled_map.shape[0]
        assert resampled_map.meta['crpix1'] == (resampled_map.shape[1] + 1) / 2.
        assert resampled_map.meta['crpix2'] == (resampled_map.shape[0] + 1) / 2.
        assert resampled_map.meta['crval1'] == self.map.center['x']
        assert resampled_map.meta['crval2'] == self.map.center['y']
        for key in self.map.meta:
            if key not in ('cdelt1', 'cdelt2', 'crpix1', 'crpix2',
                           'crval1', 'crval2'):
                assert resampled_map.meta[key] == self.map.meta[key]
            

    def test_superpixel(self):

        dimensions = (2, 2)
        superpixel_map_sum = self.map.superpixel(dimensions)
        assert superpixel_map_sum.shape[0] == self.map.shape[0]/dimensions[1]
        assert superpixel_map_sum.shape[1] == self.map.shape[1]/dimensions[0]
        assert superpixel_map_sum.data[0][0] == self.map.data[0][0] + self.map.data[0][1] + self.map.data[1][0] + self.map.data[1][1]

        dimensions = (2, 2)
        superpixel_map_avg = self.map.superpixel(dimensions, 'average')
        assert superpixel_map_avg.shape[0] == self.map.shape[0]/dimensions[1]
        assert superpixel_map_avg.shape[1] == self.map.shape[1]/dimensions[0]
        assert superpixel_map_avg.data[0][0] == (self.map.data[0][0] + self.map.data[0][1] + self.map.data[1][0] + self.map.data[1][1])/4.0


    def test_rotate(self):

        rotated_map_1 = self.map.rotate(0.5)
        rotated_map_2 = rotated_map_1.rotate(0.5)
        rotated_map_3 = self.map.rotate(0, scale=1.5)
        rotated_map_4 = self.map.rotate(np.pi/2, scale=1.5)
        rotated_map_5 = self.map.rotate(np.pi, scale=1.5)
        assert rotated_map_2.shape == rotated_map_1.shape == self.map.shape
        assert dict(rotated_map_2.meta) == dict(rotated_map_1.meta) == dict(self.map.meta)
        # Rotation of a map by non-integral multiple of pi/2 cuts off the corners
        # and assigns the value of 0 to corner pixels. This results in reduction
        # of the mean and an increase in standard deviation.
        assert rotated_map_2.mean() < rotated_map_1.mean() < self.map.mean()
        assert rotated_map_2.std() > rotated_map_1.std() > self.map.std()
        assert rotated_map_3.mean() > self.map.mean()
        # Mean and std should be equal when angle of rotation is integral multiple
        # of pi/2
        assert int(rotated_map_3.mean()) == int(rotated_map_4.mean()) == int(rotated_map_5.mean())
        assert int(rotated_map_3.std()) == int(rotated_map_4.std()) == int(rotated_map_5.std())

########NEW FILE########
__FILENAME__ = test_map_factory
# -*- coding: utf-8 -*-
"""
Created on Fri Jun 21 15:05:09 2013

@author: stuart
"""
import os
import glob
import numpy as np
import sys

import pytest

import sunpy
import sunpy.map
import sunpy.data.test

try:
    import sqlalchemy
    import sunpy.database
    HAS_SQLALCHEMY = True
except ImportError:
    HAS_SQLALCHEMY = False

filepath = sunpy.data.test.rootdir
a_list_of_many = glob.glob(os.path.join(filepath, "EIT", "*"))
a_fname = a_list_of_many[0]
#==============================================================================
# Map Factory Tests
#==============================================================================
class TestMap:
    def test_mapcube(self):
        #Test making a MapCube
        cube = sunpy.map.Map(a_list_of_many, cube=True)
        assert isinstance(cube, sunpy.map.MapCube)

    def test_composite(self):
        #Test making a CompositeMap
        comp = sunpy.map.Map(sunpy.AIA_171_IMAGE, sunpy.RHESSI_IMAGE,
                         composite=True)
        assert isinstance(comp, sunpy.map.CompositeMap)

    def test_patterns(self):
        ## Test different Map pattern matching ##
        # File name
        eitmap = sunpy.map.Map(a_fname)
        assert isinstance(eitmap, sunpy.map.GenericMap)
        # Directory
        maps = sunpy.map.Map(os.path.join(filepath, "EIT"))
        assert isinstance(maps, list)
        assert ([isinstance(amap,sunpy.map.GenericMap) for amap in maps])
        # Glob
        maps = sunpy.map.Map(os.path.join(filepath, "EIT", "*"))
        assert isinstance(maps, list)
        assert ([isinstance(amap,sunpy.map.GenericMap) for amap in maps])
        # Already a Map
        amap = sunpy.map.Map(maps[0])
        assert isinstance(amap, sunpy.map.GenericMap)
        # A list of filenames
        maps = sunpy.map.Map(a_list_of_many)
        assert isinstance(maps, list)
        assert ([isinstance(amap,sunpy.map.GenericMap) for amap in maps])
        # Data-header pair in a tuple
        pair_map = sunpy.map.Map((amap.data, amap.meta))
        assert isinstance(pair_map, sunpy.map.GenericMap)
        # Data-header pair not in a tuple
        pair_map = sunpy.map.Map(amap.data, amap.meta)
        assert isinstance(pair_map, sunpy.map.GenericMap)
        #Custom Map
        data = np.arange(0,100).reshape(10,10)
        header = {'cdelt1': 10, 'cdelt2': 10, 'telescop':'sunpy'}
        pair_map = sunpy.map.Map(data, header)
        assert isinstance(pair_map, sunpy.map.GenericMap)

    # requires sqlalchemy to run properly
    @pytest.mark.skipif('not HAS_SQLALCHEMY')
    def test_databaseentry(self):
        db = sunpy.database.Database(url='sqlite://', default_waveunit='angstrom')
        db.add_from_file(a_fname)

        res = db.get_entry_by_id(1)
        db_map = sunpy.map.Map(res)
        assert isinstance(db_map, sunpy.map.GenericMap)

    @pytest.mark.online
    def test_url_pattern(self):
        # A URL
        amap = sunpy.map.Map("https://raw.github.com/sunpy/sunpy/master/sunpy/data/sample/AIA20110319_105400_0171.fits")
        assert isinstance(amap, sunpy.map.GenericMap)

    def test_save(self):
        #Test save out
        eitmap = sunpy.map.Map(a_fname)
        eitmap.save("eit_save.fits", filetype='fits', clobber=True)
        backin = sunpy.map.Map("eit_save.fits")
        assert isinstance(backin, sunpy.map.sources.EITMap)
        os.remove("eit_save.fits")

#==============================================================================
# Sources Tests
#==============================================================================
    def test_sdo(self):
        #Test an AIAMap
        aia = sunpy.map.Map(sunpy.AIA_171_IMAGE)
        assert isinstance(aia,sunpy.map.sources.AIAMap)
        #Test a HMIMap

    def test_soho(self):
        #Test EITMap, LASCOMap & MDIMap
        eit = sunpy.map.Map(os.path.join(filepath, "EIT", "efz20040301.000010_s.fits"))
        assert isinstance(eit,sunpy.map.sources.EITMap)

        lasco = sunpy.map.Map(os.path.join(filepath, "lasco_c2_25299383_s.fts"))
        assert isinstance(lasco,sunpy.map.sources.LASCOMap)

        mdi_c = sunpy.map.Map(os.path.join(filepath, "mdi_fd_Ic_6h_01d.5871.0000_s.fits"))
        assert isinstance(mdi_c,sunpy.map.sources.MDIMap)

        mdi_m = sunpy.map.Map(os.path.join(filepath, "mdi_fd_M_96m_01d.5874.0005_s.fits"))
        assert isinstance(mdi_m,sunpy.map.sources.MDIMap)

    def test_stereo(self):
        #Test EUVIMap & CORMap
        euvi = sunpy.map.Map(os.path.join(filepath, "euvi_20090615_000900_n4euA_s.fts"))
        assert isinstance(euvi,sunpy.map.sources.EUVIMap)

        cor = sunpy.map.Map(os.path.join(filepath, "cor1_20090615_000500_s4c1A.fts"))
        assert isinstance(cor,sunpy.map.sources.CORMap)

    def test_rhessi(self):
        #Test RHESSIMap
        rhessi = sunpy.map.Map(sunpy.RHESSI_IMAGE)
        assert isinstance(rhessi,sunpy.map.sources.RHESSIMap)
    
    def test_sot(self):
        #Test SOTMap
        sot = sunpy.map.Map(os.path.join(filepath , "FGMG4_20110214_030443.7.fits"))
        assert isinstance(sot,sunpy.map.sources.SOTMap)

        #Test SWAPMap

        #Test XRTMap

        #Test SXTMap

########NEW FILE########
__FILENAME__ = attr
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>
#
# This module was developed with funding provided by
# the ESA Summer of Code (2011).
#
# pylint: disable=C0103,R0903

"""
Allow representation of queries as logic expressions. This module makes
sure that attributes that are combined using the two logic operations AND (&)
and OR (|) always are in disjunctive normal form, that is, there are only two
levels - the first being disjunction and the second being conjunction. In other
words, every combinations of attributes looks like this:
(a AND b AND c) OR (d AND e).

Walkers are used to traverse the tree that results from combining attributes.
They are implemented using sunpy.util.multimethod. Multimethods are functions
that are not assigned to classes but still dispatch by type of one or more
of their arguments. For more information about multimethods, refer to
sunpy.util.multimethod.

Please note that & is evaluated first, so A & B | C is equivalent to
(A & B) | C.
"""

from __future__ import absolute_import

from sunpy.util.multimethod import MultiMethod

# XXX: Maybe allow other normal forms.

class Attr(object):
    """ This is the base for all attributes. """
    def __and__(self, other):
        if isinstance(other, AttrOr):
            return AttrOr([elem & self for elem in other.attrs])
        if self.collides(other):
            return NotImplemented
        return AttrAnd([self, other])

    def __hash__(self):
        return hash(frozenset(vars(self).iteritems()))

    def __or__(self, other):
        # Optimization.
        if self == other:
            return self
        return AttrOr([self, other])

    def collides(self, other):
        raise NotImplementedError

    def __eq__(self, other):
        return dict(vars(self)) == dict(vars(other))


class DummyAttr(Attr):
    """ Empty attribute. Useful for building up queries. Returns other
    attribute when ORed or ANDed. It can be considered an empty query
    that you can use as an initial value if you want to build up your
    query in a loop.

    So, if we wanted an attr matching all the time intervals between the times
    stored as (from, to) tuples in a list, we could do.

    attr = DummyAttr()
    for from\_, to in times:
        attr |= Time(from\_, to)
    """
    def __and__(self, other):
        return other

    def __or__(self, other):
        return other

    def collides(self, other):
        return False

    def __hash__(self):
        return hash(None)

    def __eq__(self, other):
        return isinstance(other, DummyAttr)


class AttrAnd(Attr):
    """ Attribute representing attributes ANDed together. """
    def __init__(self, attrs):
        Attr.__init__(self)
        self.attrs = attrs

    def __and__(self, other):
        if any(other.collides(elem) for elem in self.attrs):
            return NotImplemented
        if isinstance(other, AttrAnd):
            return AttrAnd(self.attrs + other.attrs)
        if isinstance(other, AttrOr):
            return AttrOr([elem & self for elem in other.attrs])
        return AttrAnd(self.attrs + [other])

    __rand__ = __and__

    def __repr__(self):
        return "<AttrAnd(%r)>" % self.attrs

    def __eq__(self, other):
        if not isinstance(other, AttrAnd):
            return False
        return set(self.attrs) == set(other.attrs)

    def __hash__(self):
        return hash(frozenset(self.attrs))

    def collides(self, other):
        return any(elem.collides(other) for elem in self.attrs)


class AttrOr(Attr):
    """ Attribute representing attributes ORed together. """
    def __init__(self, attrs):
        Attr.__init__(self)
        self.attrs = attrs

    def __or__(self, other):
        if isinstance(other, AttrOr):
            return AttrOr(self.attrs + other.attrs)
        return AttrOr(self.attrs + [other])

    __ror__ = __or__

    def __and__(self, other):
        return AttrOr([elem & other for elem in self.attrs])

    __rand__ = __and__

    def __xor__(self, other):
        new = AttrOr([])
        for elem in self.attrs:
            try:
                new |= elem ^ other
            except TypeError:
                pass
        return new

    def __contains__(self, other):
        for elem in self.attrs:
            try:
                if other in elem:
                    return True
            except TypeError:
                pass
        return False

    def __repr__(self):
        return "<AttrOr(%r)>" % self.attrs

    def __eq__(self, other):
        if not isinstance(other, AttrOr):
            return False
        return set(self.attrs) == set(other.attrs)

    def __hash__(self):
        return hash(frozenset(self.attrs))

    def collides(self, other):
        return all(elem.collides(other) for elem in self.attrs)


class ValueAttr(Attr):
    def __init__(self, attrs):
        Attr.__init__(self)
        self.attrs = attrs

    def __repr__(self):
        return "<ValueAttr(%r)>" % (self.attrs)

    def __hash__(self):
        return hash(frozenset(self.attrs.iteritems()))

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self.attrs == other.attrs

    def collides(self, other):
        if not isinstance(other, self.__class__):
            return False
        return any(k in other.attrs for k in self.attrs)


class AttrWalker(object):
    def __init__(self):
        self.applymm = MultiMethod(lambda *a, **kw: (a[1], ))
        self.createmm = MultiMethod(lambda *a, **kw: (a[1], ))

    def add_creator(self, *types):
        def _dec(fun):
            for type_ in types:
                self.createmm.add(fun, (type_, ))
            return fun
        return _dec

    def add_applier(self, *types):
        def _dec(fun):
            for type_ in types:
                self.applymm.add(fun, (type_, ))
            return fun
        return _dec

    def add_converter(self, *types):
        def _dec(fun):
            for type_ in types:
                self.applymm.add(self.cv_apply(fun), (type_, ))
                self.createmm.add(self.cv_create(fun), (type_, ))
            return fun
        return _dec

    def cv_apply(self, fun):
        def _fun(*args, **kwargs):
            args = list(args)
            args[1] = fun(args[1])
            return self.applymm(*args, **kwargs)
        return _fun

    def cv_create(self, fun):
        def _fun(*args, **kwargs):
            args = list(args)
            args[1] = fun(args[1])
            return self.createmm(*args, **kwargs)
        return _fun

    def create(self, *args, **kwargs):
        return self.createmm(self, *args, **kwargs)

    def apply(self, *args, **kwargs):
        return self.applymm(self, *args, **kwargs)

    def super_create(self, *args, **kwargs):
        return self.createmm.super(self, *args, **kwargs)

    def super_apply(self, *args, **kwargs):
        return self.applymm.super(self, *args, **kwargs)


def and_(*args):
    """ Trick operator precendence.

    and_(foo < bar, bar < baz)
    """
    value = DummyAttr()
    for elem in args:
        value &= elem
    return value

def or_(*args):
    """ Trick operator precendence.

    or_(foo < bar, bar < baz)
    """
    value = DummyAttr()
    for elem in args:
        value |= elem
    return value

########NEW FILE########
__FILENAME__ = download
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>
#
# This module was developed with funding provided by
# the ESA Summer of Code (2011).


from __future__ import absolute_import

import os
import re
import urllib2
import threading

from functools import partial
from contextlib import closing
from collections import defaultdict, deque

import sunpy as spy

def default_name(path, sock, url):
    name = sock.headers.get('Content-Disposition', url.rsplit('/', 1)[-1])
    return os.path.join(path, name)


class Downloader(object):
    def __init__(self, max_conn=5, max_total=20):
        self.max_conn = max_conn
        self.max_total = max_total
        self.conns = 0
        
        self.connections = defaultdict(int)  # int() -> 0
        self.q = defaultdict(deque)
        
        self.buf = 9096

        self.done_lock = threading.Semaphore(0)
        self.mutex = threading.Lock()
    
    def _start_download(self, url, path, callback, errback):
        try:
            server = self._get_server(url)
        
	    with self.mutex:
                self.connections[server] += 1
                self.conns += 1
	            
            with closing(urllib2.urlopen(url)) as sock:
                fullname = path(sock, url)

                with open(fullname, 'wb') as fd:
                    while True:
                        rec = sock.read(self.buf)
                        if not rec:
                            with self.mutex:
                                self._close(callback, [{'path': fullname}], server)
                            break
                        else:
                            fd.write(rec)
        except Exception, e:
            if errback is not None:
                with self.mutex:
                    self._close(errback, [e], server)

    def _attempt_download(self, url, path, callback, errback):
        """ Attempt download. If max. connection limit reached, queue for download later.
        """

        num_connections = self.connections[self._get_server(url)]
        
        # If max downloads has not been exceeded, begin downloading
        if num_connections < self.max_conn and self.conns < self.max_total:
            th = threading.Thread(
                target=partial(self._start_download, url,
                               path, callback, errback)
            )
            th.daemon = True
            th.start()
            return True
        return False

    def _get_server(self, url):
        """Returns the server name for a given URL.
        
        Examples: http://server.com, server.org, ftp.server.org, etc.
        """
        return re.search('(\w+://)?([\w\.]+)', url).group(2)
        
    def _default_callback(self, *args):
        """Default callback to execute on a successfull download"""
        pass
        
    def _default_error_callback(self, e):
        """Default callback to execute on a failed download"""
        raise e

    def wait(self):
        self.done_lock.acquire()

    def stop(self):
        self.done_lock.release()

    def init(self):
        pass

    def download(self, url, path=None, callback=None, errback=None):
        """Downloads a file at a specified URL.
        
        Parameters
        ----------
        url : string
            URL of file to download
        path : function, string
            Location to save file to. Can specify either a directory as a string
            or a function with signature: (path, url).
            Defaults to directory specified in sunpy configuration
        callback : function
            Function to call when download is successfully completed
        errback : function
            Function to call when download fails
            
        Returns
        -------
        out : None
        """
        # Load balancing?
        # @todo: explain

        server = self._get_server(url)
        
        # Create function to compute the filepath to download to if not set
        default_dir = spy.config.get("downloads", "download_dir")

        if path is None:
            path = partial(default_name, default_dir)
        elif isinstance(path, basestring):
            path = partial(default_name, path)
        
        # Use default callbacks if none were specified
        if callback is None:
            callback = self._default_callback
        if errback is None:
            errback = self._default_error_callback
        
        # Attempt to download file from URL
        if not self._attempt_download(url, path, callback, errback):
            # If there are too many concurrent downloads, queue for later
            self.q[server].append((url, path, callback, errback))
    
    def _close(self, callback, args, server):
        """ Called after download is done. Activated queued downloads, call callback.
        """
        callback(*args)

        self.connections[server] -= 1
        self.conns -= 1

        if self.q[server]:
            self._attempt_download(*self.q[server].pop())
        else:
            for k, v in self.q.iteritems():  # pylint: disable=W0612
                while v:
                    if self._attempt_download(*v[0]):
                        v.popleft()
                        if self.conns == self.max_total:
                            return
                    else:
                        break

########NEW FILE########
__FILENAME__ = attrs
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>
#
# This module was developed with funding provided by
# the ESA Summer of Code (2011).

# The template can be found in tools/hektemplate.py
# Unless you are editing the template, DO NOT EDIT THIS FILE.
# ALL CHANGES WILL BE LOST THE NEXT TIME IT IS GENERATED FROM THE TEMPLATE.

"""
Attributes that can be used to construct HEK queries. They are different to
the VSO ones in that a lot of them are wrappers that conveniently expose
the comparisions by overloading Python operators. So, e.g., you are able
to say AR & AR.NumSpots < 5 to find all active regions with less than 5 spots.
As with the VSO query, you can use the fundamental logic operators AND and OR
to construct queries of almost arbitrary complexity. Note that complex queries
result in multiple requests to the server which might make them less efficient.
"""

from __future__ import absolute_import

from datetime import datetime
from sunpy.net import attr
from sunpy.time import parse_time

class _ParamAttr(attr.Attr):
    """ A _ParamAttr is used to represent equality or inequality checks
    for certain parameters. It stores the attribute's name, the operator to
    compare with, and the value to compare to. """
    def __init__(self, name, op, value):
        attr.Attr.__init__(self)
        self.name = name
        self.op = op
        self.value = value
    
    def collides(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self.op == other.op and self.name == other.name


# XXX: Why is this here but never used.
class _BoolParamAttr(_ParamAttr):
    def __init__(self, name, value='true'):
        _ParamAttr.__init__(self, name, '=', value)
    
    def __neg__(self):
        if self.value == 'true':
            return _BoolParamAttr(self.name, 'false')
        else:
            return _BoolParamAttr(self.name)
    
    def __pos__(self):
        return _BoolParamAttr(self.name)


class _ListAttr(attr.Attr):
    """ A _ListAttr is used when the server expects a list of things with
    the name (GET parameter name) key. By adding the _ListAttr to the query,
    item is added to that list. """
    def __init__(self, key, item):
        attr.Attr.__init__(self)
        
        self.key = key
        self.item = item
    
    def collides(self, other):
        return False
    
    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return vars(self) == vars(other)
    
    def __hash__(self):
        return hash(tuple(vars(self).itervalues()))


class EventType(attr.Attr):
    def __init__(self, item):
        attr.Attr.__init__(self)
        self.item = item
    
    def collides(self, other):
        return isinstance(other, EventType)
    
    def __or__(self, other):
        if isinstance(other, EventType):
            return EventType(self.item + ',' + other.item)
        else:
            return super(EventType, self).__or__(other)


# XXX: XOR
class Time(attr.Attr):
    """ Restrict query to time range between start and end. """
    def __init__(self, start, end):
        attr.Attr.__init__(self)
        self.start = start
        self.end = end
    
    def collides(self, other):
        return isinstance(other, Time)
    
    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return vars(self) == vars(other)
    
    def __hash__(self):
        return hash(tuple(vars(self).itervalues()))
    
    @classmethod
    def dt(cls, start, end):
        return cls(datetime(*start), datetime(*end))


# pylint: disable=R0913
class SpatialRegion(attr.Attr):
    def __init__(
        self, x1=-1200, y1=-1200, x2=1200, y2=1200, sys='helioprojective'):
        attr.Attr.__init__(self)
        
        self.x1 = x1
        self.y1 = y1
        self.x2 = x2
        self.y2 = y2
        self.sys = sys
    
    def collides(self, other):
        return isinstance(other, SpatialRegion)
    
    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return vars(self) == vars(other)
    
    def __hash__(self):
        return hash(tuple(vars(self).itervalues()))


class Contains(attr.Attr):
    def __init__(self, *types):
        attr.Attr.__init__(self)
        self.types = types
    
    def collides(self, other):
        return False
    
    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return vars(self) == vars(other)
    
    def __hash__(self):
        return hash(tuple(vars(self).itervalues()))


class _ComparisonParamAttrWrapper(object):
    def __init__(self, name):
        self.name = name
    
    def __lt__(self, other):
        return _ParamAttr(self.name, '<', other)
    
    def __le__(self, other):
        return _ParamAttr(self.name, '<=', other)
    
    def __gt__(self, other):
        return _ParamAttr(self.name, '>', other)
    
    def __ge__(self, other):
        return _ParamAttr(self.name, '>=', other)
    
    def __eq__(self, other):
        return _ParamAttr(self.name, '=', other)
    
    def __ne__(self, other):
        return _ParamAttr(self.name, '!=', other)


class _StringParamAttrWrapper(_ComparisonParamAttrWrapper):
    def like(self, other):
        return _ParamAttr(self.name, 'like', other)


class _NumberParamAttrWrapper(_ComparisonParamAttrWrapper):
    pass


# The walker is what traverses the attribute tree and converts it to a format
# that is understood by the server we are querying. The HEK walker builds up
# a dictionary of GET parameters to be sent to the server.
walker = attr.AttrWalker()

@walker.add_applier(Contains)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    dct['type'] = 'contains'
    if not Contains in state:
        state[Contains] = 1
    
    nid = state[Contains]
    n = 0
    for n, type_ in enumerate(root.types):
        dct['event_type%d' % (nid + n)] = type_
    state[Contains] += n
    return dct

@walker.add_creator(
    Time, SpatialRegion, EventType, _ParamAttr, attr.AttrAnd, Contains)
# pylint: disable=E0102,C0103,W0613
def _c(wlk, root, state):
    value = {}
    wlk.apply(root, state, value)
    return [value]

@walker.add_applier(Time)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    dct['event_starttime'] = parse_time(root.start).strftime('%Y-%m-%dT%H:%M:%S')
    dct['event_endtime'] = parse_time(root.end).strftime('%Y-%m-%dT%H:%M:%S')
    return dct

@walker.add_applier(SpatialRegion)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    dct['x1'] = root.x1
    dct['y1'] = root.y1
    dct['x2'] = root.x2
    dct['y2'] = root.y2
    dct['event_coordsys'] = root.sys
    return dct

@walker.add_applier(EventType)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    if dct.get('type', None) == 'contains':
        raise ValueError
    dct['event_type'] = root.item
    return dct

@walker.add_applier(_ParamAttr)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    if not _ParamAttr in state:
        state[_ParamAttr] = 0
    
    nid = state[_ParamAttr]
    dct['param%d' % nid] = root.name
    dct['op%d' % nid] = root.op
    dct['value%d' % nid] = root.value
    state[_ParamAttr] += 1
    return dct

@walker.add_applier(attr.AttrAnd)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    for attribute in root.attrs:
        wlk.apply(attribute, state, dct)

@walker.add_creator(attr.AttrOr)
# pylint: disable=E0102,C0103,W0613
def _c(wlk, root, state):
    blocks = []
    for attribute in root.attrs:
        blocks.extend(wlk.create(attribute, state))
    return blocks



@apply
class AR(EventType):
    CompactnessCls = _StringParamAttrWrapper('AR_CompactnessCls')
    IntensKurt = _StringParamAttrWrapper('AR_IntensKurt')
    IntensMax = _StringParamAttrWrapper('AR_IntensMax')
    IntensMean = _StringParamAttrWrapper('AR_IntensMean')
    IntensMin = _StringParamAttrWrapper('AR_IntensMin')
    IntensSkew = _StringParamAttrWrapper('AR_IntensSkew')
    IntensTotal = _StringParamAttrWrapper('AR_IntensTotal')
    IntensUnit = _StringParamAttrWrapper('AR_IntensUnit')
    IntensVar = _StringParamAttrWrapper('AR_IntensVar')
    McIntoshCls = _StringParamAttrWrapper('AR_McIntoshCls')
    MtWilsonCls = _StringParamAttrWrapper('AR_MtWilsonCls')
    NOAANum = _StringParamAttrWrapper('AR_NOAANum')
    NOAAclass = _StringParamAttrWrapper('AR_NOAAclass')
    NumSpots = _StringParamAttrWrapper('AR_NumSpots')
    PenumbraCls = _StringParamAttrWrapper('AR_PenumbraCls')
    Polarity = _StringParamAttrWrapper('AR_Polarity')
    SpotAreaRaw = _StringParamAttrWrapper('AR_SpotAreaRaw')
    SpotAreaRawUncert = _StringParamAttrWrapper('AR_SpotAreaRawUncert')
    SpotAreaRawUnit = _StringParamAttrWrapper('AR_SpotAreaRawUnit')
    SpotAreaRepr = _StringParamAttrWrapper('AR_SpotAreaRepr')
    SpotAreaReprUncert = _StringParamAttrWrapper('AR_SpotAreaReprUncert')
    SpotAreaReprUnit = _StringParamAttrWrapper('AR_SpotAreaReprUnit')
    ZurichCls = _StringParamAttrWrapper('AR_ZurichCls')
    def __init__(self):
        EventType.__init__(self, 'ar')

@apply
class CE(EventType):
    Accel = _StringParamAttrWrapper('CME_Accel')
    AccelUncert = _StringParamAttrWrapper('CME_AccelUncert')
    AccelUnit = _StringParamAttrWrapper('CME_AccelUnit')
    AngularWidth = _StringParamAttrWrapper('CME_AngularWidth')
    AngularWidthUnit = _StringParamAttrWrapper('CME_AngularWidthUnit')
    Mass = _StringParamAttrWrapper('CME_Mass')
    MassUncert = _StringParamAttrWrapper('CME_MassUncert')
    MassUnit = _StringParamAttrWrapper('CME_MassUnit')
    RadialLinVel = _StringParamAttrWrapper('CME_RadialLinVel')
    RadialLinVelMax = _StringParamAttrWrapper('CME_RadialLinVelMax')
    RadialLinVelMin = _StringParamAttrWrapper('CME_RadialLinVelMin')
    RadialLinVelStddev = _StringParamAttrWrapper('CME_RadialLinVelStddev')
    RadialLinVelUncert = _StringParamAttrWrapper('CME_RadialLinVelUncert')
    RadialLinVelUnit = _StringParamAttrWrapper('CME_RadialLinVelUnit')
    def __init__(self):
        EventType.__init__(self, 'ce')

@apply
class CD(EventType):
    Area = _StringParamAttrWrapper('CD_Area')
    AreaUncert = _StringParamAttrWrapper('CD_AreaUncert')
    AreaUnit = _StringParamAttrWrapper('CD_AreaUnit')
    Mass = _StringParamAttrWrapper('CD_Mass')
    MassUncert = _StringParamAttrWrapper('CD_MassUncert')
    MassUnit = _StringParamAttrWrapper('CD_MassUnit')
    Volume = _StringParamAttrWrapper('CD_Volume')
    VolumeUncert = _StringParamAttrWrapper('CD_VolumeUncert')
    VolumeUnit = _StringParamAttrWrapper('CD_VolumeUnit')
    def __init__(self):
        EventType.__init__(self, 'cd')

CH = EventType('ch')

CW = EventType('cw')

@apply
class FI(EventType):
    BarbsL = _StringParamAttrWrapper('FI_BarbsL')
    BarbsR = _StringParamAttrWrapper('FI_BarbsR')
    BarbsTot = _StringParamAttrWrapper('FI_BarbsTot')
    Chirality = _StringParamAttrWrapper('FI_Chirality')
    Length = _StringParamAttrWrapper('FI_Length')
    LengthUnit = _StringParamAttrWrapper('FI_LengthUnit')
    Tilt = _StringParamAttrWrapper('FI_Tilt')
    def __init__(self):
        EventType.__init__(self, 'fi')

FE = EventType('fe')

FA = EventType('fa')

@apply
class FL(EventType):
    EFoldTime = _StringParamAttrWrapper('FL_EFoldTime')
    EFoldTimeUnit = _StringParamAttrWrapper('FL_EFoldTimeUnit')
    Fluence = _StringParamAttrWrapper('FL_Fluence')
    FluenceUnit = _StringParamAttrWrapper('FL_FluenceUnit')
    GOESCls = _StringParamAttrWrapper('FL_GOESCls')
    PeakEM = _StringParamAttrWrapper('FL_PeakEM')
    PeakEMUnit = _StringParamAttrWrapper('FL_PeakEMUnit')
    PeakFlux = _StringParamAttrWrapper('FL_PeakFlux')
    PeakFluxUnit = _StringParamAttrWrapper('FL_PeakFluxUnit')
    PeakTemp = _StringParamAttrWrapper('FL_PeakTemp')
    PeakTempUnit = _StringParamAttrWrapper('FL_PeakTempUnit')
    def __init__(self):
        EventType.__init__(self, 'fl')

LP = EventType('lp')

OS = EventType('os')

@apply
class SS(EventType):
    SpinRate = _StringParamAttrWrapper('SS_SpinRate')
    SpinRateUnit = _StringParamAttrWrapper('SS_SpinRateUnit')
    def __init__(self):
        EventType.__init__(self, 'ss')

@apply
class EF(EventType):
    AspectRatio = _StringParamAttrWrapper('EF_AspectRatio')
    AxisLength = _StringParamAttrWrapper('EF_AxisLength')
    AxisOrientation = _StringParamAttrWrapper('EF_AxisOrientation')
    AxisOrientationUnit = _StringParamAttrWrapper('EF_AxisOrientationUnit')
    FluxUnit = _StringParamAttrWrapper('EF_FluxUnit')
    LengthUnit = _StringParamAttrWrapper('EF_LengthUnit')
    NegEquivRadius = _StringParamAttrWrapper('EF_NegEquivRadius')
    NegPeakFluxOnsetRate = _StringParamAttrWrapper('EF_NegPeakFluxOnsetRate')
    OnsetRateUnit = _StringParamAttrWrapper('EF_OnsetRateUnit')
    PosEquivRadius = _StringParamAttrWrapper('EF_PosEquivRadius')
    PosPeakFluxOnsetRate = _StringParamAttrWrapper('EF_PosPeakFluxOnsetRate')
    ProximityRatio = _StringParamAttrWrapper('EF_ProximityRatio')
    SumNegSignedFlux = _StringParamAttrWrapper('EF_SumNegSignedFlux')
    SumPosSignedFlux = _StringParamAttrWrapper('EF_SumPosSignedFlux')
    def __init__(self):
        EventType.__init__(self, 'ef')

CJ = EventType('cj')

PG = EventType('pg')

OT = EventType('ot')

NR = EventType('nr')

@apply
class SG(EventType):
    AspectRatio = _StringParamAttrWrapper('SG_AspectRatio')
    Chirality = _StringParamAttrWrapper('SG_Chirality')
    MeanContrast = _StringParamAttrWrapper('SG_MeanContrast')
    Orientation = _StringParamAttrWrapper('SG_Orientation')
    PeakContrast = _StringParamAttrWrapper('SG_PeakContrast')
    Shape = _StringParamAttrWrapper('SG_Shape')
    def __init__(self):
        EventType.__init__(self, 'sg')

SP = EventType('sp')

CR = EventType('cr')

@apply
class CC(EventType):
    AxisUnit = _StringParamAttrWrapper('CC_AxisUnit')
    MajorAxis = _StringParamAttrWrapper('CC_MajorAxis')
    MinorAxis = _StringParamAttrWrapper('CC_MinorAxis')
    TiltAngleMajorFromRadial = _StringParamAttrWrapper('CC_TiltAngleMajorFromRadial')
    TiltAngleUnit = _StringParamAttrWrapper('CC_TiltAngleUnit')
    def __init__(self):
        EventType.__init__(self, 'cc')

ER = EventType('er')

@apply
class TO(EventType):
    Shape = _StringParamAttrWrapper('TO_Shape')
    def __init__(self):
        EventType.__init__(self, 'to')

@apply
class Wave(object):
    DisplMaxAmpl = _StringParamAttrWrapper('WaveDisplMaxAmpl')
    DisplMinAmpl = _StringParamAttrWrapper('WaveDisplMinAmpl')
    DisplUnit = _StringParamAttrWrapper('WaveDisplUnit')
    lMaxPower = _StringParamAttrWrapper('WavelMaxPower')
    lMaxPowerUncert = _StringParamAttrWrapper('WavelMaxPowerUncert')
    lMaxRange = _StringParamAttrWrapper('WavelMaxRange')
    lMinRange = _StringParamAttrWrapper('WavelMinRange')
    lUnit = _StringParamAttrWrapper('WavelUnit')


@apply
class Veloc(object):
    MaxAmpl = _StringParamAttrWrapper('VelocMaxAmpl')
    MaxPower = _StringParamAttrWrapper('VelocMaxPower')
    MaxPowerUncert = _StringParamAttrWrapper('VelocMaxPowerUncert')
    MinAmpl = _StringParamAttrWrapper('VelocMinAmpl')
    Unit = _StringParamAttrWrapper('VelocUnit')


@apply
class Freq(object):
    MaxRange = _StringParamAttrWrapper('FreqMaxRange')
    MinRange = _StringParamAttrWrapper('FreqMinRange')
    PeakPower = _StringParamAttrWrapper('FreqPeakPower')
    Unit = _StringParamAttrWrapper('FreqUnit')


@apply
class Intens(object):
    MaxAmpl = _StringParamAttrWrapper('IntensMaxAmpl')
    MinAmpl = _StringParamAttrWrapper('IntensMinAmpl')
    Unit = _StringParamAttrWrapper('IntensUnit')


@apply
class Area(object):
    AtDiskCenter = _StringParamAttrWrapper('Area_AtDiskCenter')
    AtDiskCenterUncert = _StringParamAttrWrapper('Area_AtDiskCenterUncert')
    Raw = _StringParamAttrWrapper('Area_Raw')
    Uncert = _StringParamAttrWrapper('Area_Uncert')
    Unit = _StringParamAttrWrapper('Area_Unit')


@apply
class BoundBox(object):
    C1LL = _StringParamAttrWrapper('BoundBox_C1LL')
    C1UR = _StringParamAttrWrapper('BoundBox_C1UR')
    C2LL = _StringParamAttrWrapper('BoundBox_C2LL')
    C2UR = _StringParamAttrWrapper('BoundBox_C2UR')


@apply
class Bound(object):
    ox_C1LL = _StringParamAttrWrapper('BoundBox_C1LL')
    ox_C1UR = _StringParamAttrWrapper('BoundBox_C1UR')
    ox_C2LL = _StringParamAttrWrapper('BoundBox_C2LL')
    ox_C2UR = _StringParamAttrWrapper('BoundBox_C2UR')
    CCNsteps = _StringParamAttrWrapper('Bound_CCNsteps')
    CCStartC1 = _StringParamAttrWrapper('Bound_CCStartC1')
    CCStartC2 = _StringParamAttrWrapper('Bound_CCStartC2')


@apply
class OBS(object):
    ChannelID = _StringParamAttrWrapper('OBS_ChannelID')
    DataPrepURL = _StringParamAttrWrapper('OBS_DataPrepURL')
    FirstProcessingDate = _StringParamAttrWrapper('OBS_FirstProcessingDate')
    IncludesNRT = _StringParamAttrWrapper('OBS_IncludesNRT')
    Instrument = _StringParamAttrWrapper('OBS_Instrument')
    LastProcessingDate = _StringParamAttrWrapper('OBS_LastProcessingDate')
    LevelNum = _StringParamAttrWrapper('OBS_LevelNum')
    MeanWavel = _StringParamAttrWrapper('OBS_MeanWavel')
    Observatory = _StringParamAttrWrapper('OBS_Observatory')
    Title = _StringParamAttrWrapper('OBS_Title')
    WavelUnit = _StringParamAttrWrapper('OBS_WavelUnit')


@apply
class Skel(object):
    Curvature = _StringParamAttrWrapper('Skel_Curvature')
    Nsteps = _StringParamAttrWrapper('Skel_Nsteps')
    StartC1 = _StringParamAttrWrapper('Skel_StartC1')
    StartC2 = _StringParamAttrWrapper('Skel_StartC2')


@apply
class FRM(object):
    Contact = _StringParamAttrWrapper('FRM_Contact')
    HumanFlag = _StringParamAttrWrapper('FRM_HumanFlag')
    Identifier = _StringParamAttrWrapper('FRM_Identifier')
    Institute = _StringParamAttrWrapper('FRM_Institute')
    Name = _StringParamAttrWrapper('FRM_Name')
    ParamSet = _StringParamAttrWrapper('FRM_ParamSet')
    SpecificID = _StringParamAttrWrapper('FRM_SpecificID')
    URL = _StringParamAttrWrapper('FRM_URL')
    VersionNumber = _StringParamAttrWrapper('FRM_VersionNumber')


@apply
class Event(object):
    C1Error = _StringParamAttrWrapper('Event_C1Error')
    C2Error = _StringParamAttrWrapper('Event_C2Error')
    ClippedSpatial = _StringParamAttrWrapper('Event_ClippedSpatial')
    ClippedTemporal = _StringParamAttrWrapper('Event_ClippedTemporal')
    Coord1 = _StringParamAttrWrapper('Event_Coord1')
    Coord2 = _StringParamAttrWrapper('Event_Coord2')
    Coord3 = _StringParamAttrWrapper('Event_Coord3')
    CoordSys = _StringParamAttrWrapper('Event_CoordSys')
    CoordUnit = _StringParamAttrWrapper('Event_CoordUnit')
    MapURL = _StringParamAttrWrapper('Event_MapURL')
    MaskURL = _StringParamAttrWrapper('Event_MaskURL')
    Npixels = _StringParamAttrWrapper('Event_Npixels')
    PixelUnit = _StringParamAttrWrapper('Event_PixelUnit')
    Probability = _StringParamAttrWrapper('Event_Probability')
    TestFlag = _StringParamAttrWrapper('Event_TestFlag')
    Type = _StringParamAttrWrapper('Event_Type')


@apply
class Outflow(object):
    Length = _StringParamAttrWrapper('Outflow_Length')
    LengthUnit = _StringParamAttrWrapper('Outflow_LengthUnit')
    OpeningAngle = _StringParamAttrWrapper('Outflow_OpeningAngle')
    Speed = _StringParamAttrWrapper('Outflow_Speed')
    SpeedUnit = _StringParamAttrWrapper('Outflow_SpeedUnit')
    TransSpeed = _StringParamAttrWrapper('Outflow_TransSpeed')
    Width = _StringParamAttrWrapper('Outflow_Width')
    WidthUnit = _StringParamAttrWrapper('Outflow_WidthUnit')


@apply
class Misc(object):
    KB_Archivist = _StringParamAttrWrapper('KB_Archivist')
    MaxMagFieldStrength = _StringParamAttrWrapper('MaxMagFieldStrength')
    MaxMagFieldStrengthUnit = _StringParamAttrWrapper('MaxMagFieldStrengthUnit')
    OscillNPeriods = _StringParamAttrWrapper('OscillNPeriods')
    OscillNPeriodsUncert = _StringParamAttrWrapper('OscillNPeriodsUncert')
    PeakPower = _StringParamAttrWrapper('PeakPower')
    PeakPowerUnit = _StringParamAttrWrapper('PeakPowerUnit')
    RasterScanType = _StringParamAttrWrapper('RasterScanType')

########NEW FILE########
__FILENAME__ = hek
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>
#
# This module was developed with funding provided by
# the ESA Summer of Code (2011).
#
# pylint: disable=C0103,R0903

""" Facilities to interface with the HEK. """

from __future__ import absolute_import

import json

from itertools import chain
from urllib2 import urlopen
from urllib import urlencode
from datetime import datetime
from sunpy.net import attr
from sunpy.net.hek import attrs
from sunpy.net.vso import attrs as v_attrs
from sunpy.util import unique
from sunpy.util.xml import xml_to_dict

__all__ = ['HEKClient']

DEFAULT_URL = 'http://www.lmsal.com/hek/her'

def _freeze(obj):
    """ Create hashable representation of result dict. """
    if isinstance(obj, dict):
        return tuple((k, _freeze(v)) for k, v in obj.iteritems())
    if isinstance(obj, list):
        return tuple(_freeze(elem) for elem in obj)
    return obj


class HEKClient(object):
    """ Client to interact with the HEK. """
    # FIXME: Expose fields in .attrs with the right types
    # that is, not all StringParamWrapper!
    
    default = {
        'cosec': '2',
        'cmd': 'search',
        'type': 'column',
        'event_type': '**',
    }
    # Default to full disk.
    attrs.walker.apply(attrs.SpatialRegion(), {}, default)
    
    def __init__(self, url=DEFAULT_URL):
        self.url = url
    
    def _download(self, data):
        """ Download all data, even if pagiated. """
        page = 1        
        results = []
        
        while True:
            data['page'] = page
            fd = urlopen(self.url, urlencode(data))
            try:
                result = json.load(fd)
            finally:
                fd.close()
            results.extend(result['result'])
            
            if not result['overmax']:
                return map(Response, results)
            page += 1
    
    def query(self, *query):
        """ Retrieve information about records matching the criteria
        given in the query expression. If multiple arguments are passed,
        they are connected with AND. """
        query = attr.and_(*query)
        
        data = attrs.walker.create(query, {})
        ndata = []
        for elem in data:
            new = self.default.copy()
            new.update(elem)
            ndata.append(new)
        
        if len(ndata) == 1:
            return self._download(ndata[0])
        else:
            return self._merge(self._download(data) for data in ndata)
    
    def _merge(self, responses):
        """ Merge responses, removing duplicates. """
        return list(unique(chain.from_iterable(responses), _freeze))


class Response(dict):
    @property
    def vso_time(self):
        return v_attrs.Time(
            datetime.strptime(self['event_starttime'], "%Y-%m-%dT%H:%M:%S"),
            datetime.strptime(self['event_endtime'], "%Y-%m-%dT%H:%M:%S")
        )
    
    @property
    def vso_instrument(self):
        if self['obs_instrument'] == 'HEK':
            raise ValueError("No instrument contained.")
        return v_attrs.Instrument(self['obs_instrument'])
    
    @property
    def vso_all(self):
        return attr.and_(self.vso_time, self.vso_instrument)
    
    def get_voevent(self, as_dict=True, 
                    base_url="http://www.lmsal.com/hek/her?"):
        """Retrieves the VOEvent object associated with a given event and
        returns it as either a Python dictionary or an XML string."""

        # Build URL
        params = {                                                      
            "cmd": "export-voevent",
            "cosec": 1,
            "ivorn": self['kb_archivid']
        }
        url = base_url + urlencode(params)
        
        # Query and read response
        response = urlopen(url).read()

        # Return a string or dict
        if as_dict:
            return xml_to_dict(response)
        else:
            return response


if __name__ == '__main__':
    import pprint
    from sunpy.net.hek import attrs as a

    c = HEKClient()
    b = c.query(
        a.Time((2010, 1, 1), (2010, 1, 2)) | a.Time((2010, 1, 3), (2010, 1, 4)),
        a.AR, a.FL
    )
    pprint(b[0].vso_all)

########NEW FILE########
__FILENAME__ = hek2vso
# -*- coding: utf-8 -*-
# Author:   Michael Malocha <mjm159@humboldt.edu>
# Last Edit:  August 10th, 2013
#
# This module was developed with funding from the GSOC 2013 summer of code
#
#pylint: disable=W0142

"""
This module translates the results of a HEK query into a VSO query
and returns the results from the VSO query to the user.
"""

from __future__ import absolute_import

import sys
from astropy import units

from sunpy.net import hek
from sunpy.net import vso
from sunpy.util.progressbar import TTYProgressBar

__author__ = 'Michael Malocha'
__version__ = 'Aug 10th, 2013'

__all__ = ['wave_unit_catcher', 'translate_results_to_query', 'vso_attribute_parse', 'H2VClient']

def wave_unit_catcher(wavelength, wave_units):
    """
    Catch and convert wavelength to angstroms.

    Designed to discover the units of the wavelength passed in and convert
    it into angstroms. Returns an integer or None.

    Parameters
    ----------
    wavelength : int
        Wavelength value.
    units : str
        Units of the wavelength.

    Examples
    --------
    >>> wave_unit_catcher(2.11e-06, 'cm')
    210.99999999999997

    >>> wave_unit_catcher(9.4e-07, 'cm')
    93.99999999999999

    >>> wave_unit_catcher(5e-08, 'mm')
    0.4999999999999999
    """
    try:
        converted_value = getattr(units, wave_units).to(units.angstrom,
                                                        wavelength)
    except AttributeError:
        raise AttributeError("'%s' is not a supported unit" % wave_units)
    return converted_value


def translate_results_to_query(results):
    """
    Formulate VSO queries from HEK results.

    Take the results from a HEK query either in the form of a single HEK
    response or a list containing multiple HEK responses then translates
    them into a VSO compatible query.

    Parameters
    ----------
    results : sunpy.net.hek.hek.Response or list of sunpy.net.hek.hek.Response
        The HEK results from a HEK query to be translated.

    Examples
    --------
    >>> h = hek.HEKClient()
    >>> h2v = H2VClient()
    >>> q = h.query(hek.attrs.Time('2011/08/09 07:23:56', '2011/08/09 12:40:29'), hek.attrs.EventType('FL'))
    >>> len(q)
    19

    >>> translate_results_to_query(q[0]) # doctest: +ELLIPSIS
    [[<Time(datetime.datetime(2011, 8, 8, 1, 30, 4), datetime.datetime(2011, 8, 10, 0, 0, 4), None)>, <Source(u'SDO')>, <Instrument(u'AIA')>, <sunpy.net.vso.attrs.Wave at 0x...>]]

    >>> translate_results_to_query(q) # doctest: +ELLIPSIS
    [[<Time(datetime.datetime(2011, 8, 8, 1, 30, 4), datetime.datetime(2011, 8, 10, 0, 0, 4), None)>, <Source(u'SDO')>, <Instrument(u'AIA')>, <sunpy.net.vso.attrs.Wave at 0x...>],
    ...
    [<Time(datetime.datetime(2011, 8, 9, 8, 1, 21), datetime.datetime(2011, 8, 9, 8, 16, 45), None)>, <Source(u'SDO')>, <Instrument(u'AIA')>, <sunpy.net.vso.attrs.Wave at 0x...>]]
    """
    queries = []
    if type(results) is list:
        for result in results:
            query = vso_attribute_parse(result)
            queries.append(query)
    else:
        query = vso_attribute_parse(results)
        queries.append(query)
    return queries


def vso_attribute_parse(phrase):
    """
    Parses VSO attributes from a HEK result.

    This is a simple function to parse HEK query result and generate a list
    containing VSO relevant attributes.

    Parameters
    ----------
    phrase: dictionary containing a sunpy.net.hek.hek.Response
        The single HEK result to be parsed for VSO attribute data.

    Examples
    --------
    >>> h = hek.HEKClient()
    >>> h2v = H2VClient()
    >>> q = h.query(hek.attrs.Time('2011/08/09 07:23:56', '2011/08/09 12:40:29'), hek.attrs.EventType('FL'))
    >>> len(q)
    19

    >>> vso_attribute_parse(q[9])
    [<Time(datetime.datetime(2011, 8, 9, 7, 22, 38), datetime.datetime(2011, 8, 9, 8, 32, 2), None)>,
    <Source(u'SDO')>,
    <Instrument(u'AIA')>,
    <sunpy.net.vso.attrs.Wave at 0x10628f950>]
    """
    try:
        query = [vso.attrs.Time(phrase['event_starttime'],
                                phrase['event_endtime']),
                 vso.attrs.Source(phrase['obs_observatory']),
                 vso.attrs.Instrument(phrase['obs_instrument'])]
        avg_wave_len = wave_unit_catcher(phrase['obs_meanwavel'],
                                         phrase['obs_wavelunit'])
        query.append(vso.attrs.Wave(avg_wave_len, avg_wave_len))
    except KeyError, TypeError:
        raise TypeError("'%s' is an improper data type" % type(phrase))
    return query


class H2VClient(object):
    """
    Class to handle HEK to VSO translations

    Though the single step functions exists outside this class where
    translation is also possible, this class provides a framework where
    all the necessary functionality is easily accessed, along with a few
    additional and helpful methods.

    Examples
    --------
    >>> from sunpy.net import hek2vso
    >>> h2v = hek2vso.H2VClient()
    """

    def __init__(self):
        self.hek_client = hek.HEKClient()
        self.hek_results = ''
        self.vso_client = vso.VSOClient()
        self.vso_results = []
        self.num_of_records = 0

    def full_query(self, client_query, limit=None, progress=False):
        """
        An encompassing method that takes a HEK query and returns a VSO result

        Takes a list containing a HEK style query, passes it to a HEKClient
        instance, translates it, queries the VSO webservice, then returns
        the VSO results inside a structured list.

        Parameters
        ----------
        client_query: list
            The list containing the HEK style query.
        limit: int
            An approximate limit to the desired number of VSO results.

        Examples
        --------
        >>> from sunpy.net import hek, hek2vso
        >>> h2v = hek2vso.H2VClient()
        >>> q = h2v.full_query((hek.attrs.Time('2011/08/09 07:23:56', '2011/08/09 12:40:29'), hek.attrs.EventType('FL')))
        """
        self._quick_clean()
        if progress:
            sys.stdout.write('\rQuerying HEK webservice...')
            sys.stdout.flush()
        self.hek_results = self.hek_client.query(*client_query)
        self._quick_clean()
        return self.translate_and_query(self.hek_results,
                                        limit=limit, progress=progress)

    def translate_and_query(self, hek_results, limit=None, progress=False):
        """
        Translates HEK results, makes a VSO query, then returns the results.

        Takes the results from a HEK query, translates them, then makes a VSO
        query, returning the results in a list organized by their
        corresponding HEK query.

        Parameters
        ----------
        hek_results: sunpy.net.hek.hek.Response or list of Responses
            The results from a HEK query in the form of a list.
        limit: int
            An approximate limit to the desired number of VSO results.
        progress: Boolean
            A flag to turn off the progress bar, defaults to "off"

        Examples
        --------
        >>> from sunpy.net import hek, hek2vso
        >>> h = hek.HEKClient()
        >>> tstart = '2011/08/09 07:23:56'
        >>> tend = '2011/08/09 12:40:29'
        >>> event_type = 'FL'
        >>> q = h.query(hek.attrs.Time(tstart, tend), hek.attrs.EventType(event_type))
        >>> h2v = hek2vso.H2VClient()
        >>> res = h2v.translate_and_query(q)
        """
        vso_query = translate_results_to_query(hek_results)
        result_size = len(vso_query)
        if progress:
            sys.stdout.write('\rQuerying VSO webservice')
            sys.stdout.flush()
            pbar = TTYProgressBar(result_size)

        for query in vso_query:
            temp = self.vso_client.query(*query)
            self.vso_results.append(temp)
            self.num_of_records += len(temp)
            if limit is not None:
                if self.num_of_records >= limit:
                    break
            if progress:
                pbar.poke()

        if progress:
            pbar.finish()

        return self.vso_results

    def _quick_clean(self):
        """
        A simple method to quickly sterilize the instance variables.

        Used to bleach local variables before a new query is made. Not
        intended to be run by user.
        """
        self.vso_results = []
        self.num_of_records = 0

########NEW FILE########
__FILENAME__ = hec
"""
Access the Helio Event Catalogue
"""
from sunpy.net.proxyfix import WellBehavedHttpTransport
from sunpy.net.helio import parser
from sunpy.time import parse_time
from suds.client import Client as C
import suds
from astropy.io.votable.table import parse_single_table
import io

__author__ = 'Michael Malocha'
__version__ = 'September 22nd, 2013'

__all__ = ['HECClient']


        

def suds_unwrapper(wrapped_data):
    """
    Removes suds wrapping from returned xml data

    When grabbing data via votable_interceptor.last_payload from the suds.client.Client
    module, it returns the xml data in an un-helpful "<s:Envelope>" that needs
    to be removed. This function politely cleans it up.

    Parameters
    ----------
    wrapped_data: str
        Contains the wrapped xml results from a WSDL query

    Returns
    -------
    unwrapped: str
        The xml results with the wrapper removed

    Examples
    --------
    >>> from sunpy.net.helio import hec  Todo: Fix this example!
    >>> from suds.client import Client
    >>> from sunpy.net.proxyfix import WellBehavedHttpTransport
    >>> votable_interceptor = hec.VotableInterceptor()
    >>> client = Client(hec.parser.wsdl_retriever(), plugins=[self.votable_interceptor], transport=WellBehavedHttpTransport())
    >>> client.service.getTableNames()
    >>> temp = client.last_received().str()
    >>> print temp
    <?xml version="1.0" encoding="UTF-8"?>
    <S:Envelope ..... >
       <S:Body>
          <helio:queryResponse ... >
             <VOTABLE xmlns="http://www.ivoa.net/xml/VOTable/v1.1" version="1.1">
                <RESOURCE>
                ...
                </RESOURCE>
             </VOTABLE>
          </helio:queryResponse>
       </S:Body>
    </S:Envelope>
    >>> temp = hec.suds_unwrapper(temp)
    >>> print temp
    <?xml version="1.0" encoding="UTF-8"?>
    <VOTABLE xmlns="http://www.ivoa.net/xml/VOTable/v1.1" version="1.1">
        <RESOURCE>
        ...
        </RESOURCE>
     </VOTABLE>
    """
    HEADER = '<?xml version="1.0" encoding="UTF-8"?>\n'
    CATCH_1 = '<VOTABLE'
    CATCH_2 = '</VOTABLE>\n'
    # Now going to find the locations of the CATCHes in the wrapped_data
    pos_1 = wrapped_data.find(CATCH_1)
    pos_2 = wrapped_data.find(CATCH_2)
    unwrapped = HEADER + wrapped_data[pos_1:pos_2] + CATCH_2
    return unwrapped


def votable_handler(xml_table):
    """
    Returns a VOtable object from a VOtable style xml string

    In order to get a VOtable object, it has to be parsed from an xml file or
    file-like object. This function creates a file-like object via the
    StringIO module, writes the xml data to it, then passes the file-like
    object to parse_single_table() from the astropy.io.votable.table module
    and thereby creates a VOtable object.

    Parameters
    ----------
    xml_table: str
        Contains the VOtable style xml data

    Returns
    -------
    votable: astropy.io.votable.tree.Table
        A properly formatted VOtable object

    Examples
    --------
    >>> temp = hec.suds_unwrapper(xml_string)
    >>> type(temp)
    unicode
    >>> temp = hec.votable_handler(temp)
    >>> type(temp)
    astropy.io.votable.tree.Table
    """
    fake_file = io.StringIO()
    fake_file.write(xml_table)
    votable = parse_single_table(fake_file)
    fake_file.close()
    return votable


class VotableInterceptor(suds.plugin.MessagePlugin):
    '''
    Adapted example from http://stackoverflow.com/questions/15259929/configure-suds-to-use-custom-response-xml-parser-for-big-response-payloads
    '''
    def __init__(self, *args, **kwargs):
        self.last_payload = None

    def received(self, context):
        #recieved xml as a string
        self.last_payload = unicode(suds_unwrapper(context.reply))
        #clean up reply to prevent parsing
        context.reply = ""
        return context


class HECClient(object):
    """
    A client class used to interface with and query HELIO webservices.
    """

    def __init__(self, link=None):
        """
        The constructor; establishes the webservice link for the client

        Initializes the client with a weblink

        Parameters
        ----------
        link: str
            Contains URL to valid WSDL endpoint

        Examples
        --------
        >>> hc = hec.HECClient()
        """
        if link is None:
            # The default wsdl file
            link = parser.wsdl_retriever()

        self.votable_interceptor = VotableInterceptor()
        self.hec_client = C(link, plugins=[self.votable_interceptor], transport=WellBehavedHttpTransport())

    def time_query(self, start_time, end_time, table=None, max_records=None):
        """
        The simple interface to query the wsdl service.

        Used to utilize the service's TimeQuery() method, this is a simple
        interface between the sunpy module library and the web-service's API.

        Parameters
        ----------
        start_time: str
            The datetime where the query window opens

        end_time: str
            The datetime where the query window closes

        table: str
            The table to query from. If the table is unknown, the user will be
            prompted to pick from a list of tables.

        max_records: int
            The maximum number of desired records.

        Returns
        -------
        results: astropy.io.votable.tree.Table
            Table containing the results from the query

        Examples
        --------
        >>> hc = hec.HECClient()
        >>> start = '2005/01/03'
        >>> end = '2005/12/03'
        >>> temp = hc.time_query(start, end, max_records=10)
        >>> print temp.array
        [ (31463, '2005-01-03T01:37:36', '2005-01-03T01:37:54', '2005-01-03T01:39:00', 717, 982.0, 113.0, 989.0, 84, 22, 9456, 6, 5010320)
         (31464, '2005-01-03T01:51:36', '2005-01-03T01:59:18', '2005-01-03T02:17:24', 717, 989.0, 117.0, 996.0, 1548, 656, 2286912, 12, 5010301)
         (31465, '2005-01-03T03:26:28', '2005-01-03T03:42:50', '2005-01-03T03:46:04', 717, 994.0, 117.0, 1001.0, 1176, 38, 157800, 6, 5010332)
         (31466, '2005-01-03T03:46:04', '2005-01-03T04:07:10', '2005-01-03T04:07:52', 715, -154.0, 124.0, 198.0, 1308, 1328, 2049360, 12, 5010302)
         (31467, '2005-01-03T05:00:24', '2005-01-03T05:00:30', '2005-01-03T05:19:36', 715, -139.0, 107.0, 176.0, 1152, 224, 894816, 6, 5010313)
         (31468, '2005-01-03T06:40:48', '2005-01-03T06:42:46', '2005-01-03T06:50:12', 717, 990.0, 105.0, 996.0, 564, 23, 50782, 6, 5010314)
         (31469, '2005-01-03T08:27:56', '2005-01-03T08:28:26', '2005-01-03T08:29:08', 717, 971.0, 104.0, 977.0, 72, 36, 11197, 6, 5010334)
         (31470, '2005-01-03T09:31:00', '2005-01-03T09:33:34', '2005-01-03T09:34:52', 717, 960.0, 99.0, 965.0, 232, 108, 56486, 6, 5010322)
         (31471, '2005-01-03T09:34:52', '2005-01-03T09:59:46', '2005-01-03T10:06:04', 717, 994.0, 108.0, 1000.0, 1872, 40, 55920, 6, 5010336)
         (31472, '2005-01-03T11:06:48', '2005-01-03T11:07:18', '2005-01-03T11:15:56', 717, 974.0, 116.0, 981.0, 548, 2160, 2240376, 12, 5010304)]
        """
        while table is None:
            table = self.make_table_list()
        start_time = parse_time(start_time)
        end_time = parse_time(end_time)
        self.hec_client.service.TimeQuery(STARTTIME=start_time.isoformat(),
                                          ENDTIME=end_time.isoformat(),
                                          FROM=table,
                                          MAXRECORDS=max_records)
        results = votable_handler(self.votable_interceptor.last_payload)
        return results

    def get_table_names(self):
        """
        Returns a list of the available tables to query

        Returns the names of all the tables that can be queried via the webservice

        Returns
        -------
        tables.array: numpy.ma.core.MaskedArray
            A VOtable table of available tables names

        Examples
        --------
        >>> hc = hec.HECClient()
        >>> print hc.get_table_names()
        [('hi_cme_list',) ('cactus_stereoa_cme',) ('aad_gle',)
            ...
         ('wind_sw_crossing_time',) ('ulysses_hxr_flare',)
         ('wind_typeii_soho_cme',)]

        """
        self.hec_client.service.getTableNames()
        tables = votable_handler(self.votable_interceptor.last_payload)
        return tables.array

    def make_table_list(self):
        """
        Creates a list of table names and prompts the user for a choice

        This takes the table of table names from get_table_names(), creates a
        list of the names, sorts them, then presents the tables in a
        convenient menu for the user to choose from. It returns a string
        containing the name of the table that the user picked.

        Returns
        -------
        temp: str
            contains the name of the table that the user picked.

        Examples
        --------
        >>> hc.make_table_list()
          1) aad_gle
          2) aastar_list
          3) apstar_list
          4) bas_magnetic_storms
          ...
        108) wind_waves_type_ii_burst
        109) yohkoh_flare_list
        110) yohkoh_hxr_flare
        111) yohkoh_sxt_trace_list

        Please enter a table number between 1 and 111 ('e' to exit): 108
        'wind_waves_type_ii_burst'
        """
        table_list = []
        tables = self.get_table_names()
        for i in tables:
            table = i[0]
            if len(table) > 0:
                table_list.append(table)
        table_list.sort()
        for index, table in enumerate(table_list):
            print ('{number:3d}) {table}'.format(number = index + 1, table = table))
        while True:
            input = raw_input("\nPlease enter a table number between 1 and %i "
                              "('e' to exit): " % len(table_list))
            if input.lower() == "e" or input.lower() == "exit":
                temp = None
                break
            temp = [int(s) for s in input.split() if s.isdigit()]
            temp = temp[0] - 1
            if temp in range(0, len(table_list)):
                temp = table_list[temp]
                break
            else:
                print "Choice outside of bounds"
        return temp


########NEW FILE########
__FILENAME__ = parser
# -*- coding: utf-8 -*-
# Author:   Michael Malocha <mjm159@humboldt.edu>
# Last Edit:  September 22nd, 2013
#
# This module was developed with funding from the GSOC 2013 summer of code
#

"""
This module is meant to parse the HELIO registry and return WSDL endpoints to
facilitate the interfacing between further modules and HELIO.
"""
from __future__ import absolute_import
from urllib2 import urlopen, URLError
#import sunpy.util.etree as EL 
import xml.etree.ElementTree as EL
from sunpy.net.helio import registry_links as RL
from bs4 import BeautifulSoup
from contextlib import closing

__author__ = 'Michael Malocha'
__version__ = 'September 22nd, 2013'

# Lifespan in seconds before a link times-out
LINK_TIMEOUT = 3


def webservice_parser(service='HEC'):
    """
    Quickly parses important contents from HELIO registry.

    Uses the link contained in registry_links in with 'service' appended
    and scrapes the web-service links contained on that webpage.

    Parameters
    ----------
    service: str
        Indicates which particular HELIO service is used. Defaults to HEC.

    Returns
    -------
    links: list or NoneType
        List of urls to registries containing WSDL endpoints.

    Examples
    --------
    >>> from sunpy.net.helio import parser
    >>> parser.webservice_parser()
    ['http://msslkz.mssl.ucl.ac.uk/helio-hec/HelioService',
    'http://festung3.oats.inaf.it:8080/helio-hec/HelioService',
    'http://festung1.oats.inaf.it:8080/helio-hec/HelioService',
    'http://hec.helio-vo.eu/helio_hec/HelioService',
    'http://msslkz.mssl.ucl.ac.uk/helio-hec/HelioLongQueryService',
    'http://festung3.oats.inaf.it:8080/helio-hec/HelioLongQueryService',
    'http://festung1.oats.inaf.it:8080/helio-hec/HelioLongQueryService',
    'http://hec.helio-vo.eu/helio_hec/HelioLongQueryService']
    """
    link = RL.LINK + service.lower()
    xml = link_test(link)
    if xml is None:
        return xml
    root = EL.fromstring(xml)
    links = []

    #WARNING: getiterator is deprecated in Python 2.7+
    #Fix for 3.x support
    for interface in root.getiterator('interface'):
        service_type = interface.attrib
        key = service_type.keys()
        if len(key) > 0:
            value = service_type[key[0]]
            if value == 'vr:WebService':
                for url in interface.getiterator('accessURL'):
                    if url.text not in links:
                        links.append(url.text)
    return links


def endpoint_parser(link):
    """
    Takes a link to a list of endpoints and parses the WSDL links.

    Feeding 1 result from webservice_parser() into endpoint_parser() at a time
    will return a list of WSDL endpoints that are contained on the page from
    that link that was passed in.

    Parameters
    ----------
    link: str
        A url to a page containing links to WSDL files.

    Returns
    -------
    endpoints: list or NoneType
        A list containing all of the available WSDL endpoints from the passed
        in url.

    Examples
    --------
    >>> from sunpy.net.helio import parser
    >>> parser.endpoint_parser('http://msslkz.mssl.ucl.ac.uk/helio-hec/HelioService')
    ['http://msslkz.mssl.ucl.ac.uk:80/helio-hec/HelioService?wsdl',
    'http://msslkz.mssl.ucl.ac.uk:80/helio-hec/HelioService1_0?wsdl',
    'http://msslkz.mssl.ucl.ac.uk:80/helio-hec/HelioService1_0b?wsdl',
    'http://msslkz.mssl.ucl.ac.uk:80/helio-hec/HelioLongQueryService?wsdl',
    'http://msslkz.mssl.ucl.ac.uk:80/helio-hec/HelioLongQueryService1_0?wsdl',
    'http://msslkz.mssl.ucl.ac.uk:80/helio-hec/HelioLongQueryService1_1?wsdl',
    'http://msslkz.mssl.ucl.ac.uk:80/helio-hec/HelioLongQueryService1_0b?wsdl',
    'http://msslkz.mssl.ucl.ac.uk:80/helio-hec/HelioTavernaService?wsdl']
    """
    endpoint_page = link_test(link)
    if endpoint_page is None:
        return None
    soup = BeautifulSoup(endpoint_page)
    endpoints = []
    for web_link in soup.find_all('a'):
        endpoints.append(web_link.get('href'))
    return endpoints


def taverna_parser(link):
    """
    Takes a link to a list of endpoints and parses the taverna WSDL links.

    Takes a url to a page containing a list of endpoints, then passes that url
    to endpoint_parser(). Upon receiving the resulting list from the parser
    taverna_parser() goes through the list and finds all the WSDL links for
    the taverna web-service. It then returns a list containing the filtered
    links.

    Parameters
    ----------
    link: str
        A url to a page containing links to WSDL files.

    Returns
    -------
    taverna_links: list or NoneType
        A list containing WSDL links for a taverna web-service

    Examples
    --------
    >>> from sunpy.net.helio import parser
    >>> parser.taverna_parser('http://msslkz.mssl.ucl.ac.uk/helio-hec/HelioService')
    ['http://msslkz.mssl.ucl.ac.uk:80/helio-hec/HelioTavernaService?wsdl']
    """
    endpoints = endpoint_parser(link)
    taverna_links = []
    if endpoints is None:
        return None
    for web_link in endpoints:
        if 'Taverna' in web_link:
            taverna_links.append(web_link)
    if len(taverna_links) == 0:
        return None
    return taverna_links


def link_test(link):
    """
    Just a quick function to test a link.

    Quickly checks to see if the URL is a valid link; if it is it returns the
    downloaded contents of that page.

    Parameters
    ----------
    link: str
        A string containing a URL

    Returns
    -------
    webpage: str or NoneType
        String containing the webresults

    Examples
    --------
    >>> from sunpy.net.helio import parser
    >>> parser.link_test('http://msslkz.mssl.ucl.ac.uk/helio-hec/HelioService')
    u'<html>\n<head>...</body>\n</html>\n'

    >>> print parser.link_test('http://rrnx.invalid_url5523.com')
    None
    """
    try:
        with closing(urlopen(link)) as fd:
            return fd.read()
    except (ValueError, URLError):
        return None


def wsdl_retriever(service='HEC'):
    """
    Retrieves a link to a taverna WSDL file

    This is essentially the master method, from it all the other functions get
    called and it essentially knits everything together. It gets a list of
    service links via webservice_parser(), then filters the results via
    taverna_parser(). Finally it tests all the returned taverna WSDL links
    and returns the first live taverna endpoint.

    Parameters
    ----------
    service: str
        Indicates which particular HELIO service is used. Defaults to HEC.

    Returns
    -------
    wsdl: str
        URL to a single live taverna endpoint

    Examples
    --------
    >>> from sunpy.net.helio import parser
    >>> parser.wsdl_retriever()
    'http://msslkz.mssl.ucl.ac.uk:80/helio_hec/HelioTavernaService?wsdl'

    Notes
    -----
    * Currently only support for HEC exists, but it was designed so that it
        could be expanded at a later date
    * There is a 3 second timeout lifespan on links, so there is potential for
        this function to take a while to return. Timeout duration can be
        controlled through the LINK_TIMEOUT value
    """
    service_links = webservice_parser(service=service)
    wsdl = None
    wsdl_links = None
    if service_links is None:
        return None
    for link in service_links:
        wsdl_links = taverna_parser(link)
    if wsdl_links is None:
        return None
    for end_point in wsdl_links:
        if end_point is not None and link_test(end_point) is not None:
            wsdl = end_point
            break
    return wsdl

########NEW FILE########
__FILENAME__ = registry_links
# contains links to HELIO registries
from __future__ import absolute_import

"""
The following link is the link to the registry, should it be necessary, this
link can easily be replaced or swapped.
"""
LINK = 'http://msslkz.mssl.ucl.ac.uk/helio_registry/viewResourceEntry_body.jsp?' \
       'XML=true&IVORN=ivo%3A%2F%2Fhelio-vo.eu%2F'
########NEW FILE########
__FILENAME__ = helioviewer
"""
This module provides a wrapper around the Helioviewer API.
"""
from __future__ import absolute_import

#pylint: disable=E1101,F0401,W0231

__author__ = ["Keith Hughitt"]
__email__ = "keith.hughitt@nasa.gov"

import os
import urllib
import urllib2

import json

import sunpy
from sunpy.time import parse_time
from sunpy.util.net import download_fileobj

__all__ = ['HelioviewerClient']

class HelioviewerClient:
    """Helioviewer.org Client"""
    def __init__(self, url="http://helioviewer.org/api/"):
        self._api = url

    def get_data_sources(self, **kwargs):
        """Returns a structured list of datasources available at Helioviewer.org"""
        params = {"action": "getDataSources"}
        params.update(kwargs)

        return self._get_json(params)

    def get_closest_image(self, date, **kwargs):
        """Finds the closest image available for the specified source and date.

        For more information on what types of requests are available and the
        expected usage for the response, consult the Helioviewer
        API documenation: http://helioviewer.org/api

        Parameters
        ----------
        date : datetime, string
            A string or datetime object for the desired date of the image
        observatory : string
            (Optional) Observatory name
        instrument : string
            (Optional) instrument name
        detector : string
            (Optional) detector name
        measurement : string
            (Optional) measurement name
        sourceId : int
            (Optional) data source id

        Returns
        -------
        out : dict
            A dictionary containing metainformation for the closest image matched

        Examples
        --------
        >>> from sunpy.net import HelioviewerClient

        >>> client = HelioviewerClient()
        >>> metadata = client.get_closest_image('2012/01/01', sourceId=11)
        >>> print(metadata['date'])
        """
        params = {
            "action": "getClosestImage",
            "date": self._format_date(date)
        }
        params.update(kwargs)

        response = self._get_json(params)

        # Cast date string to DateTime
        response['date'] = parse_time(response['date'])

        return response

    def download_jp2(self, date, directory=None, overwrite=False, **kwargs):
        """
        Downloads the JPEG 2000 that most closely matches the specified time and
        data source.

        The data source may be specified either using it's sourceId from the
        get_data_sources query, or a combination of observatory, instrument,
        detector and measurement.

        Parameters
        ----------
        date : datetime, string
            A string or datetime object for the desired date of the image
        directory : string
            (Optional) Directory to download JPEG 2000 image to.
        observatory : string
            (Optional) Observatory name
        instrument : string
            (Optional) instrument name
        detector : string
            (Optional) detector name
        measurement : string
            (Optional) measurement name
        sourceId : int
            (Optional) data source id
        jpip : bool
            (Optional) Returns a JPIP URI if set to True

        Returns
        -------
        out : string
            Returns a filepath to the downloaded JPEG 2000 image or a URL if
            the "jpip" parameter is set to True.

        Examples
        --------
        >>> import sunpy
        >>> from sunpy.net import helioviewer
        >>> hv = helioviewer.HelioviewerClient()
        >>> filepath = hv.download_jp2('2012/07/03 14:30:00', observatory='SDO', instrument='AIA', detector='AIA', measurement='171')
        >>> aia = sunpy.make_map(filepath)
        >>> aia.show()

        >>> data_sources = hv.get_data_sources()
        >>> hv.download_jp2('2012/07/03 14:30:00', sourceId=data_sources['SOHO']['LASCO']['C2']['white-light']['sourceId'])
        """
        params = {
            "action": "getJP2Image",
            "date": self._format_date(date)
        }
        params.update(kwargs)

        # JPIP URL response
        if 'jpip' in kwargs:
            return self._get_json(params)

        return self._get_file(params, directory, overwrite=overwrite)

    def download_png(self, date, image_scale, layers, directory=None,
                     overwrite=False, **kwargs):
        """Downloads a PNG image using data from Helioviewer.org.

        Returns a single image containing all layers/image types requested.
        If an image is not available for the date requested the closest
        available image is returned. The region to be included in the
        image may be specified using either the top-left and bottom-right
        coordinates in arc-seconds, or a center point in arc-seconds and a
        width and height in pixels. See the Helioviewer.org API Coordinates
        Appendix for more infomration about working with coordinates in
        Helioviewer.org.

        Parameters
        ----------
        date : datetime, string
            A string or datetime object for the desired date of the image
        image_scale : float
            The zoom scale of the image. Default scales that can be used are
            0.6, 1.2, 2.4, and so on, increasing or decreasing by a factor
            of 2. The full-res scale of an AIA image is 0.6.
        layers : string
            Each layer string is comma-separated with these values, e.g.:
            "[sourceId,visible,opacity]" or "[obs,inst,det,meas,visible,opacity]"
            Mulitple layer string are by commas: "[layer1],[layer2],[layer3]"
        directory : string
            (Optional)  Directory to download JPEG 2000 image to.
        x1 : float
            (Optional) The offset of the image's left boundary from the center
            of the sun, in arcseconds.
        y1 : float
            (Optional) The offset of the image's top boundary from the center
            of the sun, in arcseconds.
        x2 : float
            (Optional) The offset of the image's right boundary from the
            center of the sun, in arcseconds.
        y2 : float
            (Optional) The offset of the image's bottom boundary from the
            center of the sun, in arcseconds.
        x0 : float
            (Optional) The horizontal offset from the center of the Sun.
        y0 : float
            (Optional) The vertical offset from the center of the Sun.
        width : int
            (Optional) Width of the image in pixels (Maximum: 1920).
        height : int
            (Optional) Height of the image in pixels (Maximum: 1200).
        watermark
            (Optional) Whether or not the include the timestamps and the
            Helioviewer.org logo in the image (Default=True).

        Returns
        -------
        out : string
            filepath to the PNG image

        Examples
        --------
        >>> from sunpy.net.helioviewer import HelioviewerClient
        >>> hv = HelioviewerClient()
        >>> hv.download_png('2012/07/16 10:08:00', 2.4, "[SDO,AIA,AIA,171,1,100]", x0=0, y0=0, width=1024, height=1024)
        '/home/user/sunpy/data/2012_07_16_10_08_00_AIA_171.png
        >>> hv.download_png('2012/07/16 10:08:00', 4.8, "[SDO,AIA,AIA,171,1,100],[SOHO,LASCO,C2,white-light,1,100]", x1=-2800, x2=2800, y1=-2800, y2=2800, directory='~/Desktop')
        '/home/user/Desktop/2012_07_16_10_08_00_AIA_171__LASCO_C2.png'
        """
        params = {
            "action": "takeScreenshot",
            "date": self._format_date(date),
            "imageScale": image_scale,
            "layers": layers,
            "display": True
        }
        params.update(kwargs)

        return self._get_file(params, directory, overwrite=overwrite)

    def is_online(self):
        """Returns True if Helioviewer is online and available"""
        try:
            self.get_data_sources()
        except urllib2.URLError:
            return False

        return True

    def _get_json(self, params):
        """Returns a JSON result as a string"""
        response = self._request(params).read()
        return json.loads(response)

    def _get_file(self, params, directory=None, overwrite=False):
        """Downloads a file and return the filepath to that file"""
        # Query Helioviewer.org
        if directory is None:
            directory = sunpy.config.get('downloads', 'download_dir')
        else:
            directory = os.path.abspath(os.path.expanduser(directory))

        response = self._request(params)
        try:
            filepath = download_fileobj(response, directory, overwrite=overwrite)
        finally:
            response.close()

        return filepath

    def _request(self, params):
        """Sends an API request and returns the result

        Parameters
        ----------
        params : dict
            Parameters to send

        Returns
        -------
        out : result of request
        """
        response = urllib2.urlopen(self._api, urllib.urlencode(params))

        return response

    def _format_date(self, date):
        """Formats a date for Helioviewer API requests"""
        return parse_time(date).strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + "Z"

########NEW FILE########
__FILENAME__ = jsoc
# -*- coding: utf-8 -*-
"""
Created on Wed Mar 26 13:44:56 2014

@author: stuart
"""
from __future__ import print_function, absolute_import

import os
import time
import urlparse
import warnings

import requests
import astropy.time

from sunpy import config
from sunpy.time import parse_time, TimeRange
from sunpy.net.download import Downloader
from sunpy.net.vso.vso import Results

__all__ = ['JSOCClient']

JSOC_URL = 'http://jsoc.stanford.edu/cgi-bin/ajax/jsoc_fetch'
BASE_DL_URL = 'http://jsoc.stanford.edu'

class JSOCClient(object):
    """
    This is a Client to the JSOC Data Export service.

    This is not warm and fluffy like the VSO, this is hardcore crazy. It is
    more efficient for large scale SDO queries than the VSO, however harder to
    use.

    Notes
    -----
    This Client mocks input to this site: http://jsoc.stanford.edu/ajax/exportdata.html
    Therefore that is a good resource if things are mis-behaving.
    The full list of 'series' is availible through this site: http://jsoc.stanford.edu/

    You can build more complex queries by specifiying parameters to POST to JSOC via keyword
    arguments. You can generate these kwargs using the Export Data page at JSOC.

    Examples
    --------
    Query JSOC for some HMI data at 45 second cadence

    >>> from sunpy.net import jsoc
    >>> client = jsoc.JSOCClient()
    >>> IDs = client.jsoc_query('2012/1/1T00:00:00', '2012/1/1T00:00:45', 'hmi.m_45s')

    The returned `IDs` is a list of JSOC request identifiers, you can check the status of
    a request thus:

    >>> status = client.check_status(IDs)

    Once the request has been staged (Status 1) you can download the data:

    >>> res = client.get(IDs)

    This returns a Results instance which can be used to watch the progress
    of the download.
    """

    def jsoc_query(self, start_time, end_time, series, **kwargs):
        """
        Build a JSOC query and submit it to JSOC for processing.

        Parameters
        ----------
        start_time: datetime, astropy.time or string
            The time in UTC (or astropy.time object) specifying the start time.

        end_time: datetime, astropy.time or string
            The time in UTC (or astropy.time object) specifying the end time.

        series: string
            A string representing the JSOC data series to download e.g. 'hmi.M_45s'

        notify: string
            An email address to get a notification to when JSOC has staged your request

        protocol: string
            The type of download to request one of ("FITS", "JPEG", "MPG", "MP4", or "as-is").
            Only FITS is supported, the others will require extra keywords.

        compression: string
            'rice' or None, download FITS files with RICE compression.

        kwargs: dict
            Extra keywords to put into the POST payload to JSOC.

        Returns
        -------
        requestIDs: list
            A list of the requestIDs generated from your query
        """

        # A little (hidden) debug feature
        return_resp = kwargs.pop('return_resp', False)

        start_time = self._process_time(start_time)
        end_time = self._process_time(end_time)

        # Do a multi-request
        responses = self._multi_request(start_time, end_time, series, **kwargs)

        for i, response in enumerate(responses):
            #TODD: catch non 200 return
            if response.json()['status'] != 2:
                warnings.warn(
                Warning("Query {0} retuned status {1} with error {2}".format(i,
                                                     response.json()['status'],
                                                     response.json()['error'])))
                responses.pop(i)
        #Extract the IDs from the JSON
        requestIDs = [response.json()['requestid'] for response in responses]

        if return_resp:
            return responses

        return requestIDs

    def check_request(self, requestIDs):
        """
        Check the status of a request and print out a messgae about it

        Parameters
        ----------
        requestIDs: list or string
            A list of requestIDs to check

        Returns
        -------
        status: list
            A list of status' that were returned by JSOC
        """
        # Convert IDs to a list if not already
        if not astropy.utils.misc.isiterable(requestIDs) or isinstance(requestIDs, basestring):
            requestIDs = [requestIDs]

        allstatus = []
        for request_id in requestIDs:
            u = self._request_status(request_id)
            status = int(u.json()['status'])

            if status == 0: #Data ready to download
                print("Request {0} was exported at {1} and is ready to download.".format(u.json()['requestid'],
                                                                                       u.json()['exptime']))
            elif status == 1:
                print("Request {0} was submitted {1} seconds ago, it is not ready to download.".format(
                                                             u.json()['requestid'], u.json()['wait']))
            else:
                print("Request returned status: {0} with error: {1}".format(
                                    u.json()['status'], u.json()['error']))

            allstatus.append(status)

        return allstatus


    def wait_get(self, requestIDs, path=None, overwrite=False, progress=True,
            max_conn=5, sleep=10):
        """
        Same as get() excepts it will wait until the download has been staged.

        Parameters
        ----------
        requestIDs: list or string
            One or many requestID strings

        path: string
            Path to save data to, defaults to SunPy download dir

        overwrite: bool
            Replace files with the same name if True

        progress: bool
            Print progress info to terminal

        max_conns: int
            Maximum number of download connections.

        downloader: sunpy.download.Downloder instance
            A Custom downloader to use

        sleep: int
            The number of seconds to wait between calls to JSOC to check the status
            of the request.

        Returns
        -------
        downloader: a sunpy.net.download.Downloader instance
            A Downloader instance
        """
        # Convert IDs to a list if not already
        if not astropy.utils.misc.isiterable(requestIDs) or isinstance(requestIDs, basestring):
            requestIDs = [requestIDs]

        r = Results(lambda x: None)

        while requestIDs:
            for i, request_id in enumerate(requestIDs):
                u = self._request_status(request_id)

                if progress:
                    self.check_request(request_id)

                if u.status_code == 200 and u.json()['status'] == '0':
                    rID = requestIDs.pop(i)
                    r = self.get(rID, path=path, overwrite=overwrite,
                             progress=progress, results=r)

                else:
                    time.sleep(sleep)

        return r

    def get(self, requestIDs, path=None, overwrite=False, progress=True,
            max_conn=5, downloader=None, results=None):
        """
        Query JSOC to see if request_id is ready for download.

        If the request is ready for download download it.

        Parameters
        ----------
        requestIDs: list or string
            One or many requestID strings

        path: string
            Path to save data to, defaults to SunPy download dir

        overwrite: bool
            Replace files with the same name if True

        progress: bool
            Print progress info to terminal

        max_conns: int
            Maximum number of download connections.

        downloader: sunpy.download.Downloder instance
            A Custom downloader to use

        results: Results instance
            A Results manager to use.

        Returns
        -------
        res: Results
            A Results instance or None if no URLs to download
        """

        # Convert IDs to a list if not already
        if not astropy.utils.misc.isiterable(requestIDs) or isinstance(requestIDs, basestring):
            requestIDs = [requestIDs]

        if path is None:
            path = config.get('downloads','download_dir')

        if downloader is None:
            downloader = Downloader(max_conn=max_conn, max_total=max_conn)

        # A Results object tracks the number of downloads requested and the
        # number that have been completed.
        if results is None:
            results = Results(lambda x: None)

        urls = []
        for request_id in requestIDs:
            u = self._request_status(request_id)

            if u.status_code == 200 and u.json()['status'] == '0':
                for ar in u.json()['data']:
                    if overwrite or not os.path.isfile(os.path.join(path, ar['filename'])):
                        urls.append(urlparse.urljoin(BASE_DL_URL + u.json()['dir']+'/', ar['filename']))
                if progress:
                    print("{0} URLs found for Download. Totalling {1}MB".format(len(urls), u.json()['size']))

            else:
                if progress:
                    self.check_request(request_id)

        if urls:
            for url, rcall in list(zip(urls, list(map(lambda x: results.require([x]), urls)))):
                downloader.download(url, callback=rcall, path=path)

        else:
            #Make Results think it has finished.
            results.require([])

        results.poke()
        return results

    def _process_time(self, time):
        """
        Take a UTC time string or datetime instance and generate a astropy.time
        object in TAI frame. Alternatively convert a astropy time object to TAI

        Parameters
        ----------
        time: basestring or datetime or astropy.time
            Input time

        Returns
        -------
        datetime, in TAI
        """
        # Convert from any input (in UTC) to TAI
        if isinstance(time, basestring):
            time = parse_time(time)
        time = astropy.time.Time(time, scale='utc')
        time = astropy.time.Time(time, scale='tai')

        return time.datetime


    def _make_query_payload(self, start_time, end_time, series, notify='',
                          protocol='FITS', compression='rice', **kwargs):
        """
        Build the POST payload for the query parameters
        """

        if protocol.upper() == 'FITS' and compression and compression.lower() == 'rice':
            jprotocol = 'FITS,compress Rice'
        elif protocol.upper() == 'FITS':
            jprotocol = 'FITS, **NONE**'
        else:
            jprotocol = protocol

        payload = {'ds':'{0}[{1}-{2}]'.format(series, start_time.strftime("%Y.%m.%d_%H:%M:%S_TAI"),
                                           end_time.strftime("%Y.%m.%d_%H:%M:%S_TAI")),
                   'format':'json',
                   'method':'url',
                   'notify':notify,
                   'op':'exp_request',
                   'process':'n=0|no_op',
                   'protocol':jprotocol,
                   'requestor':'none',
                   'filenamefmt':'{0}.{{T_REC:A}}.{{CAMERA}}.{{segment}}'.format(series)}

        payload.update(kwargs)

        return payload

    def _send_jsoc_request(self, start_time, end_time, series, notify='',
                          protocol='FITS', compression='rice', **kwargs):
        """
        Request that JSOC stages data for download

        This routine puts in a POST request to JSOC
        """

        payload = self._make_query_payload(start_time, end_time, series, notify=notify,
                          protocol=protocol, compression=compression, **kwargs)

        r = requests.post(JSOC_URL, data=payload)

        if r.status_code != 200:
            raise Exception("JSOC POST Request returned code {0}".format(r.status_code))

        return r, r.json()

    def _multi_request(self, start_time, end_time, series, **kwargs):
        """
        Make a series of requests to avoid the 100GB limit
        """
        tr = TimeRange(start_time, end_time)
        returns = []

        response, json_response = self._send_jsoc_request(start_time, end_time, series, **kwargs)

        if json_response['status'] == 3 and json_response['error'] == 'Request exceeds max byte limit of 100000MB':
            returns.append(self._multi_request(tr.start(), tr.center(), series, **kwargs)[0])
            returns.append(self._multi_request(tr.center(), tr.end(), series, **kwargs)[0])
        else:
            returns.append(response)

        return returns

    def _request_status(self, request_id):
        """
        GET the status of a request ID
        """
        payload = {'op':'exp_status', 'requestid':request_id}
        u = requests.get(JSOC_URL, params=payload)

        return u

########NEW FILE########
__FILENAME__ = test_jsoc
# -*- coding: utf-8 -*-
"""
Created on Wed Mar 26 20:17:06 2014

@author: stuart
"""
import datetime
import astropy.time
import pytest

from sunpy.time import parse_time
from sunpy.net.jsoc import JSOCClient
from sunpy.net.vso.vso import Results

client = JSOCClient()

def test_payload():
    start = parse_time('2012/1/1T00:00:00')
    end = parse_time('2012/1/1T00:00:45')

    payload = client._make_query_payload(start, end, 'hmi.M_42s')

    payload_expected = {
       'ds':'{0}[{1}-{2}]'.format('hmi.M_42s', start.strftime("%Y.%m.%d_%H:%M:%S_TAI"),
                                       end.strftime("%Y.%m.%d_%H:%M:%S_TAI")),
       'format':'json',
       'method':'url',
       'notify':'',
       'op':'exp_request',
       'process':'n=0|no_op',
       'protocol':'FITS,compress Rice',
       'requestor':'none',
       'filenamefmt':'{0}.{{T_REC:A}}.{{CAMERA}}.{{segment}}'.format('hmi.M_42s')
       }

    assert payload == payload_expected

def test_payload_nocompression():
    start = parse_time('2012/1/1T00:00:00')
    end = parse_time('2012/1/1T00:00:45')

    payload = client._make_query_payload(start, end, 'hmi.M_42s', compression=None)

    payload_expected = {
       'ds':'{0}[{1}-{2}]'.format('hmi.M_42s', start.strftime("%Y.%m.%d_%H:%M:%S_TAI"),
                                       end.strftime("%Y.%m.%d_%H:%M:%S_TAI")),
       'format':'json',
       'method':'url',
       'notify':'',
       'op':'exp_request',
       'process':'n=0|no_op',
       'protocol':'FITS, **NONE**',
       'requestor':'none',
       'filenamefmt':'{0}.{{T_REC:A}}.{{CAMERA}}.{{segment}}'.format('hmi.M_42s')
       }

    assert payload == payload_expected

def test_payload_protocol():
    start = parse_time('2012/1/1T00:00:00')
    end = parse_time('2012/1/1T00:00:45')

    payload = client._make_query_payload(start, end, 'hmi.M_42s', protocol='as-is')

    payload_expected = {
       'ds':'{0}[{1}-{2}]'.format('hmi.M_42s', start.strftime("%Y.%m.%d_%H:%M:%S_TAI"),
                                       end.strftime("%Y.%m.%d_%H:%M:%S_TAI")),
       'format':'json',
       'method':'url',
       'notify':'',
       'op':'exp_request',
       'process':'n=0|no_op',
       'protocol':'as-is',
       'requestor':'none',
       'filenamefmt':'{0}.{{T_REC:A}}.{{CAMERA}}.{{segment}}'.format('hmi.M_42s')
       }

    assert payload == payload_expected

def test_process_time_string():
    start = client._process_time('2012/1/1T00:00:00')
    assert start == datetime.datetime(year=2012, month=1, day=1, second=34)

def test_process_time_datetime():
    start = client._process_time(datetime.datetime(year=2012, month=1, day=1))
    assert start == datetime.datetime(year=2012, month=1, day=1, second=34)

def test_process_time_astropy():
    start = client._process_time(astropy.time.Time('2012-01-01T00:00:00', format='isot', scale='utc'))
    assert start == datetime.datetime(year=2012, month=1, day=1, second=34)

def test_process_time_astropy_tai():
    start = client._process_time(astropy.time.Time('2012-01-01T00:00:00', format='isot', scale='tai'))
    assert start == datetime.datetime(year=2012, month=1, day=1, second=0)

@pytest.mark.online
def test_status_request():
    r = client._request_status('none')
    assert r.json() == {u'status': 4, u'error': u"Bad RequestID 'none' provided."}

@pytest.mark.online
def test_post():
    responses = client.jsoc_query('2012/1/1T00:00:00', '2012/1/1T00:00:45', 'hmi.M_45s')
    assert isinstance(responses, list)
    assert responses[0][:4] == 'JSOC'

@pytest.mark.online
def test_post_pass():
    responses = client.jsoc_query('2012/1/1T00:00:00', '2012/1/1T00:00:45', 'hmi.M_45s', return_resp=True)
    responses[0].json()['status'] == 2
    responses[0].json()['protocol'] == 'FITS,compress Rice'
    responses[0].json()['method'] == 'url'

@pytest.mark.online
def test_post_fail(recwarn):
    client.jsoc_query('2012/1/1T00:00:00', '2012/1/1T00:00:45', 'none', return_resp=True)
    w = recwarn.pop(Warning)
    assert issubclass(w.category, Warning)
    assert "Query 0 retuned status 4 with error Cannot export series 'none' - it does not exist." in str(w.message)
    assert w.filename
    assert w.lineno

@pytest.mark.online
def test_request_status_fail():
    resp = client._request_status('none')
    assert resp.json() == {u'status': 4, u'error': u"Bad RequestID 'none' provided."}

@pytest.mark.online
def test_wait_get():
    responses = client.jsoc_query('2012/1/3T00:00:00', '2012/1/3T00:00:45', 'hmi.M_45s')
    res = client.wait_get(responses[0])
    assert isinstance(res, Results)
    assert res.total == 2

########NEW FILE########
__FILENAME__ = proxyfix
from suds.transport.http import HttpTransport as SudsHttpTransport 

class WellBehavedHttpTransport(SudsHttpTransport): 
    """HttpTransport which properly obeys the ``*_proxy`` environment variables.""" 

    def u2handlers(self): 
        """Return a list of specific handlers to add. 

        The urllib2 logic regarding ``build_opener(*handlers)`` is: 

        - It has a list of default handlers to use 

        - If a subclass or an instance of one of those default handlers is given 
            in ``*handlers``, it overrides the default one. 

        Suds uses a custom {'protocol': 'proxy'} mapping in self.proxy, and adds 
        a ProxyHandler(self.proxy) to that list of handlers. 
        This overrides the default behaviour of urllib2, which would otherwise 
        use the system configuration (environment variables on Linux, System 
        Configuration on Mac OS, ...) to determine which proxies to use for 
        the current protocol, and when not to use a proxy (no_proxy). 

        Thus, passing an empty list will use the default ProxyHandler which 
        behaves correctly. 

        This method comes from http://stackoverflow.com/a/12433606/1087595
        """ 
        return []

########NEW FILE########
__FILENAME__ = test_attr
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

from __future__ import absolute_import

from sunpy.net import attr

def test_dummyattr():
    one = attr.DummyAttr()
    other = attr.ValueAttr({'a': 'b'})
    assert (one | other) is other
    assert (one & other) is other
########NEW FILE########
__FILENAME__ = test_download
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

#pylint: disable=W0613

from __future__ import absolute_import

import pytest

import os
import tempfile
import threading

from functools import partial

import sunpy

from sunpy.net.download import Downloader, default_name


class CalledProxy(object):
    def __init__(self, fn):
        self.fn = fn
        self.fired = False

    def __call__(self, *args, **kwargs):
        self.fn(*args, **kwargs)
        self.fired = True


class MockConfig(object):
    def __init__(self):
        self.dct = {}

    def add_section(self, name, dct):
        self.dct[name] = dct

    def get(self, one, other):
        return self.dct[one][other]


def wait_for(n, callback): #pylint: disable=W0613
    items = []
    def _fun(handler):
        items.append(handler)
        if len(items) == n:
            callback(items)
    return _fun


def path_fun(*args, **kwargs):
    raise ValueError

@pytest.mark.online
def test_path_exception():
    x = threading.Event()
    dw = Downloader(1, 2)
    dw.download(
        "http://google.at", path_fun, errback=wait_for(1, lambda a: x.set())
    )
    th = threading.Thread(target=dw.wait)
    th.daemon = True
    th.start()
    x.wait(10)
    assert x.isSet()
    dw.stop()

@pytest.mark.online
def test_download_http():
    items = []
    lck = threading.Lock()

    def wait_for(n, callback):  # pylint: disable=W0613
        def _fun(handler):
            with lck:
                items.append(handler)
                if len(items) == n:
                    callback(items)
        return _fun

    tmp = tempfile.mkdtemp()
    path_fun = partial(default_name, tmp)

    dw = Downloader(1, 1)
    _stop = lambda _: dw.stop()

    timeout = CalledProxy(dw.stop)
    timer = threading.Timer(60, timeout)
    timer.start()

    on_finish = wait_for(3, lambda _: dw.stop())
    dw.download('http://ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min.js', path_fun, on_finish)
    dw.download('http://ajax.googleapis.com/ajax/libs/webfont/1.4.2/webfont.js', path_fun, on_finish)
    dw.download('https://raw.github.com/sunpy/sunpy/master/INSTALL.txt', path_fun, on_finish)
    # dw.download('ftp://speedtest.inode.at/speedtest-100mb', path_fun, on_finish)

    dw.wait()
    timer.cancel()

    assert len(items) == 3
    assert not timeout.fired

    for item in items:
        assert os.path.exists(item['path'])

@pytest.mark.online
def test_download_default_dir():
    _config = sunpy.config

    try:
        tmpdir = tempfile.mkdtemp()

        sunpy.config = MockConfig()
        sunpy.config.add_section(
            "downloads", {"download_dir": tmpdir}
        )

        dw = Downloader(1, 1)
        _stop = lambda _: dw.stop()

        timeout = CalledProxy(dw.stop)
        errback = CalledProxy(_stop)
        dw.download(
            'http://ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min.js',
            callback=_stop,
            errback=errback
        )

        timer = threading.Timer(10, timeout)
        timer.start()
        dw.wait()
        timer.cancel()

        assert not timeout.fired
        assert not errback.fired
        assert os.path.exists(os.path.join(tmpdir, 'jquery.min.js'))
    finally:
        sunpy.config = _config

@pytest.mark.online
def test_download_dir():
    tmpdir = tempfile.mkdtemp()

    dw = Downloader(1, 1)
    _stop = lambda _: dw.stop()
    timeout = CalledProxy(dw.stop)
    errback = CalledProxy(_stop)

    dw.download(
        'http://ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min.js',
        tmpdir,
        callback=_stop,
        errback=errback
    )

    timer = threading.Timer(10, timeout)
    timer.start()
    dw.wait()
    timer.cancel()
    assert not timeout.fired
    assert not errback.fired
    assert os.path.exists(os.path.join(tmpdir, 'jquery.min.js'))

########NEW FILE########
__FILENAME__ = test_hek
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

#pylint: disable=W0613

from __future__ import absolute_import

import pytest

from sunpy.net import hek
from sunpy.net import attr

def pytest_funcarg__foostrwrap(request):
    return hek.attrs._StringParamAttrWrapper("foo")

def test_eventtype_collide():
    with pytest.raises(TypeError):
        hek.attrs.AR & hek.attrs.CE
    with pytest.raises(TypeError):
        (hek.attrs.AR & hek.attrs.Time((2011, 1, 1), (2011, 1, 2))) & hek.attrs.CE
        with pytest.raises(TypeError):
            (hek.attrs.AR | hek.attrs.Time((2011, 1, 1), (2011, 1, 2))) & hek.attrs.CE


def test_eventtype_or():
    assert (hek.attrs.AR | hek.attrs.CE).item == "ar,ce"


def test_paramattr():
    res = hek.attrs.walker.create(hek.attrs._ParamAttr("foo", "=", "bar"), {})
    assert len(res) == 1
    assert res[0] == {'value0': 'bar', 'op0': '=', 'param0': 'foo'}


def test_stringwrapper_eq(foostrwrap):
    res = hek.attrs.walker.create(foostrwrap == "bar", {})
    assert len(res) == 1
    assert res[0] == {'value0': 'bar', 'op0': '=', 'param0': 'foo'}

def test_stringwrapper_lt(foostrwrap):
    res = hek.attrs.walker.create(foostrwrap < "bar", {})
    assert len(res) == 1
    assert res[0] == {'value0': 'bar', 'op0': '<', 'param0': 'foo'}

def test_stringwrapper_gt(foostrwrap):
    res = hek.attrs.walker.create(foostrwrap > "bar", {})
    assert len(res) == 1
    assert res[0] == {'value0': 'bar', 'op0': '>', 'param0': 'foo'}
    
def test_stringwrapper_le(foostrwrap):
    res = hek.attrs.walker.create(foostrwrap <= "bar", {})
    assert len(res) == 1
    assert res[0] == {'value0': 'bar', 'op0': '<=', 'param0': 'foo'}

def test_stringwrapper_ge(foostrwrap):
    res = hek.attrs.walker.create(foostrwrap >= "bar", {})
    assert len(res) == 1
    assert res[0] == {'value0': 'bar', 'op0': '>=', 'param0': 'foo'}

def test_stringwrapper_ne(foostrwrap):
    res = hek.attrs.walker.create(foostrwrap != "bar", {})
    assert len(res) == 1
    assert res[0] == {'value0': 'bar', 'op0': '!=', 'param0': 'foo'}

def test_stringwrapper_like(foostrwrap):
    res = hek.attrs.walker.create(foostrwrap.like("bar"), {})
    assert len(res) == 1
    assert res[0] == {'value0': 'bar', 'op0': 'like', 'param0': 'foo'}

def test_err_dummyattr_create():
    with pytest.raises(TypeError):
        hek.attrs.walker.create(attr.DummyAttr(), {})

def test_err_dummyattr_apply():
    with pytest.raises(TypeError):
        hek.attrs.walker.apply(attr.DummyAttr(), {})

########NEW FILE########
__FILENAME__ = test_hek2vso
# -*- coding: utf-8 -*-
# Author: Michael Malocha
# e-mail: mmalocha13@gmail.com
# Version: June 11th, 2013
#

"""
This module was built to test the HEK2VSO class.
"""

__author__ = 'Michael Malocha'
__version__ = 'June 11th, 2013'

import pytest

from sunpy.net import hek
from sunpy.net import vso
from sunpy.net import hek2vso

from sunpy.time import parse_time

import numpy as np

startTime = '2011/08/09 07:23:56'
endTime = '2011/08/09 12:40:29'
eventType = 'FL'
instrument = 'eit'

hekTime = hek.attrs.Time(startTime, endTime)
hekEvent = hek.attrs.EventType(eventType)

@pytest.fixture
def h2v_client():
    return hek2vso.H2VClient()

@pytest.fixture
def hek_client():
    return hek.HEKClient()

@pytest.fixture
def vso_client():
    vso.VSOClient()

def test_wave_unit_catcher():
    """Make sure that inter-unit conversion of wavelengths is accurate"""
    # Implementing the example test cases
    test_wavel = [
        hek2vso.wave_unit_catcher(2.11e-06, 'cm'), 
        hek2vso.wave_unit_catcher(9.4e-07, 'cm'),
        hek2vso.wave_unit_catcher(5e-08, 'mm') 
    ]
    test_values = [211.0, 94.0, 0.5]

    assert np.allclose(test_wavel, test_values, rtol=1e-05, atol=1e-8)

@pytest.mark.online
def test_translate_results_to_query():
    """Make sure that conversion of HEK results to VSO queries is accurate"""
    h = hek.HEKClient()
    h2v = hek2vso.H2VClient()
    hek_query = h.query(hekTime, hekEvent)
    vso_query = hek2vso.translate_results_to_query(hek_query)

    if isinstance(hek_query, list):
        # Comparing length of two lists
        assert len(hek_query) == len(vso_query)
        #Comparing types of both queries
        assert type(hek_query) == type(vso_query)

@pytest.mark.online
def test_vso_attribute_parse():
    """Make sure that Parsing of VSO attributes from HEK queries is accurate"""
    h = hek.HEKClient()
    h2v = hek2vso.H2VClient()
    hek_query = h.query(hekTime, hekEvent)
    vso_query = hek2vso.vso_attribute_parse(hek_query[0])

    # Cheking Time
    # TODO

    # Checking Observatory
    assert vso_query[1].value == hek_query[0]['obs_observatory']
    
    # Checking Instrument
    assert vso_query[2].value == hek_query[0]['obs_instrument']
    
    # Checking Wavelength
    assert vso_query[3].min == hek2vso.wave_unit_catcher(hek_query[0]['obs_meanwavel'], hek_query[0]['obs_wavelunit'])
    assert vso_query[3].max == hek2vso.wave_unit_catcher(hek_query[0]['obs_meanwavel'], hek_query[0]['obs_wavelunit'])
    assert vso_query[3].unit == 'Angstrom'

class TestH2VClient:
    """Tests the H2V class"""
    # TODO

########NEW FILE########
__FILENAME__ = test_helio
from __future__ import absolute_import

import xml.etree.ElementTree

import pytest

from sunpy.net.helio import hec
import sunpy.net.helio.parser as p
from sunpy.net.helio import registry_links as RL

def test_suds_unwrapper():
    suds_output = """<?xml version="1.0" encoding="UTF-8"?>
    <S:Envelope ..... >
       <S:Body>
          <helio:queryResponse ... >
             <VOTABLE xmlns="http://www.ivoa.net/xml/VOTable/v1.1" version="1.1">
                <RESOURCE>
                ...
                </RESOURCE>
             </VOTABLE>
          </helio:queryResponse>
       </S:Body>
    </S:Envelope>
    """
    expected_output = """<?xml version="1.0" encoding="UTF-8"?>
<VOTABLE xmlns="http://www.ivoa.net/xml/VOTable/v1.1" version="1.1">
                <RESOURCE>
                ...
                </RESOURCE>
             </VOTABLE>
"""
    assert hec.suds_unwrapper(suds_output) == expected_output

@pytest.mark.online
def test_webservice_parser():
    result = p.webservice_parser()
    assert isinstance(result,list)

########NEW FILE########
__FILENAME__ = test_helioviewer
"""
Helioviewer Client tests
"""
from __future__ import absolute_import

#pylint: disable=C0103,R0904,W0201,W0212,W0232,E1103
import sunpy
import sunpy.map
import pytest
from sunpy.net.helioviewer import HelioviewerClient

# If server is not accessible, skip Helioviewer tests
client = HelioviewerClient()
if not client.is_online():
    __SKIP_TESTS__ = True
    print("Skipping Helioviewer.org tests (server inaccessible)")
else:
    __SKIP_TESTS__ = False

class TestHelioviewerClient:
    """Tests the Helioviewer.org API Client class"""
    def setup_class(self):
        self.client = client
        self.sources = self.client.get_data_sources()

    def teardown_class(self):
        self.client = None
    
    @pytest.mark.online
    @pytest.mark.skipif("__SKIP_TESTS__ is True")
    def test_get_datasources(self):
        """Makes sure datasource query returns a valid result and source id
        is casted to an integer"""
        assert type(self.sources['SDO']['AIA']['AIA']['171']['sourceId']) is int
    
    @pytest.mark.online
    @pytest.mark.skipif("__SKIP_TESTS__ is True")
    def test_get_closest_image(self):
        """Tests getClosestImage API method"""
        # check basic query
        im1 = self.client.get_closest_image('1994/01/01', 
                                              observatory='SOHO', 
                                              instrument='EIT', 
                                              detector='EIT', 
                                              measurement='195')
        assert im1['width'] == im1['height'] == 1024
        
        # result should be same when using source id to query
        source_id = self.sources['SOHO']['EIT']['EIT']['195']['sourceId']
        
        im2 = self.client.get_closest_image('1994/01/01', sourceId=source_id)
        
        assert im1 == im2
    
    @pytest.mark.online
    @pytest.mark.skipif("__SKIP_TESTS__ is True")
    def test_download_jp2(self):
        """Tests getJP2Image API method"""
        filepath = self.client.download_jp2('2020/01/01', observatory='SOHO', 
                                            instrument='MDI', detector='MDI',
                                            measurement='continuum')
        map_ = sunpy.map.Map(filepath)
        assert isinstance(map_, sunpy.map.GenericMap)

########NEW FILE########
__FILENAME__ = test_util
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

import sunpy.util.net

def test_content_disposition_ascii():
    ret = sunpy.util.net.get_content_disposition("Content-Disposition: attachment; filename=foo.txt")
    assert ret == u"foo.txt"
    assert isinstance(ret, unicode)


def test_content_disposition_unicode():
    ret = sunpy.util.net.get_content_disposition("Content-Disposition: attachment; filename*= UTF-8''%e2%82%ac%20rates")
    assert ret == u" rates"
    assert isinstance(ret, unicode)

def test_slugify():
    assert sunpy.util.net.slugify(u"b c", u"b_c")

########NEW FILE########
__FILENAME__ = test_vso
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

#pylint: disable=W0613

from __future__ import absolute_import

import datetime
import pytest

from sunpy.time import TimeRange
from sunpy.net import vso
from sunpy.net.vso import attrs as va
from sunpy.net import attr

def pytest_funcarg__eit(request):
    return va.Instrument('eit')


def pytest_funcarg__client(request):
    return vso.VSOClient()


def pytest_funcarg__iclient(request):
    return vso.InteractiveVSOClient()


def test_simpleattr_apply():
    a = attr.ValueAttr({('test', ): 1})
    dct = {}
    va.walker.apply(a, None, dct)
    assert dct['test'] == 1

def test_Time_timerange():
    t = va.Time(TimeRange('2012/1/1','2012/1/2'))
    assert isinstance(t, va.Time)
    assert t.min == datetime.datetime(2012, 1, 1)
    assert t.max == datetime.datetime(2012, 1, 2)

def test_input_error():
    with pytest.raises(ValueError):
        va.Time('2012/1/1')

@pytest.mark.online
def test_simpleattr_create(client):
    a = attr.ValueAttr({('instrument', ): 'eit'})
    assert va.walker.create(a, client.api)[0].instrument == 'eit'


def test_simpleattr_and_duplicate():
    attr = va.Instrument('foo')
    pytest.raises(TypeError, lambda: attr & va.Instrument('bar'))
    attr |= va.Source('foo')
    pytest.raises(TypeError, lambda: attr & va.Instrument('bar'))
    otherattr = va.Instrument('foo') | va.Source('foo')
    pytest.raises(TypeError, lambda: attr & otherattr)
    pytest.raises(TypeError, lambda: (attr | otherattr) & va.Instrument('bar'))
    tst = va.Instrument('foo') & va.Source('foo')
    pytest.raises(TypeError, lambda: tst & tst)


def test_simpleattr_or_eq():
    attr = va.Instrument('eit')
    
    assert attr | attr == attr
    assert attr | va.Instrument('eit') == attr


def test_complexattr_apply():
    tst = {('test', 'foo'): 'a', ('test', 'bar'): 'b'}
    a = attr.ValueAttr(tst)
    dct = {'test': {}}
    va.walker.apply(a, None, dct)
    assert dct['test'] == {'foo': 'a', 'bar': 'b'}


@pytest.mark.online
def test_complexattr_create(client):
    a = attr.ValueAttr({('time', 'start'): 'test'})
    assert va.walker.create(a, client.api)[0].time.start == 'test'


def test_complexattr_and_duplicate():
    attr = va.Time((2011, 1, 1), (2011, 1, 1, 1))
    pytest.raises(
        TypeError,
        lambda: attr & va.Time((2011, 2, 1), (2011, 2, 1, 1))
    )
    attr |= va.Source('foo')
    pytest.raises(
        TypeError,
        lambda: attr & va.Time((2011, 2, 1), (2011, 2, 1, 1))
    )


def test_complexattr_or_eq():
    attr = va.Time((2011, 1, 1), (2011, 1, 1, 1))
    
    assert attr | attr == attr
    assert attr | va.Time((2011, 1, 1), (2011, 1, 1, 1)) == attr


def test_attror_and():
    attr = va.Instrument('foo') | va.Instrument('bar')
    one = attr & va.Source('bar')
    other = (
        (va.Instrument('foo') & va.Source('bar')) | 
        (va.Instrument('bar') & va.Source('bar'))
    )
    assert one == other


def test_wave_toangstrom():
    frequency = [
        ('Hz', 1),
        ('kHz', 1e3),
        ('MHz', 1e6),
        ('GHz', 1e9)]

    energy = [
        ('eV', 1),
        ('keV', 1e3),
        ('MeV', 1e6)]

    for name, factor in energy:
        w = va.Wave(62 / factor, 62 / factor, name)
        assert int(w.min) == 199
    
    w = va.Wave(62, 62, 'eV')
    assert int(w.min) == 199
    w = va.Wave(62e-3, 62e-3, 'keV')
    assert int(w.min) == 199

    for name, factor in frequency:
        w = va.Wave(1.506e16 / factor, 1.506e16 / factor, name)
        assert int(w.min) == 199
    
    w = va.Wave(1.506e16, 1.506e16, 'Hz')
    assert int(w.min) == 199
    w = va.Wave(1.506e7, 1.506e7, 'GHz')
    assert int(w.min) == 199


def test_time_xor():
    one = va.Time((2010, 1, 1), (2010, 1, 2))
    a = one ^ va.Time((2010, 1, 1, 1), (2010, 1, 1, 2))
    
    assert a == attr.AttrOr(
        [va.Time((2010, 1, 1), (2010, 1, 1, 1)),
         va.Time((2010, 1, 1, 2), (2010, 1, 2))]
    )
    
    a ^= va.Time((2010, 1, 1, 4), (2010, 1, 1, 5))
    assert a == attr.AttrOr(
        [va.Time((2010, 1, 1), (2010, 1, 1, 1)),
         va.Time((2010, 1, 1, 2), (2010, 1, 1, 4)),
         va.Time((2010, 1, 1, 5), (2010, 1, 2))]
    )


def test_wave_xor():
    one = va.Wave(0, 1000)
    a = one ^ va.Wave(200, 400)
    
    assert a == attr.AttrOr([va.Wave(0, 200), va.Wave(400, 1000)])
    
    a ^= va.Wave(600, 800)
    
    assert a == attr.AttrOr(
        [va.Wave(0, 200), va.Wave(400, 600), va.Wave(800, 1000)])


def test_err_dummyattr_create():
    with pytest.raises(TypeError):
        va.walker.create(attr.DummyAttr(), None, {})


def test_err_dummyattr_apply():
    with pytest.raises(TypeError):
        va.walker.apply(attr.DummyAttr(), None, {})

def test_wave_repr():
    """Tests the __repr__ method of class vso.attrs.Wave"""
    wav = vso.attrs.Wave(12, 16)
    moarwav = vso.attrs.Wave(15, 12, "Angstrom")
    assert repr(wav) == "<Wave(12.0, 16.0, 'Angstrom')>"
    assert repr(moarwav) == "<Wave(12.0, 15.0, 'Angstrom')>"


########NEW FILE########
__FILENAME__ = attrs
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>
#
# This module was developed with funding provided by
# the ESA Summer of Code (2011).
#
# pylint: disable=C0103,R0903

"""
Attributes that can be used to construct VSO queries. Attributes are the
fundamental building blocks of queries that, together with the two
operations of AND and OR (and in some rare cases XOR) can be used to
construct complex queries. Most attributes can only be used once in an
AND-expression, if you still attempt to do so it is called a collision,
for a quick example think about how the system should handle
Instrument('aia') & Instrument('eit').
"""
from __future__ import absolute_import

from datetime import datetime

from sunpy.time import TimeRange
from sunpy.net.attr import (
    Attr, ValueAttr, AttrWalker, AttrAnd, AttrOr, DummyAttr, ValueAttr
)
from sunpy.util import to_angstrom
from sunpy.util.multimethod import MultiMethod
from sunpy.time import parse_time

__all__ = ['Wave', 'Time', 'Extent', 'Field', 'Provider', 'Source',
           'Instrument', 'Physobs', 'Pixels', 'Level', 'Resolution',
           'Detector', 'Filter', 'Sample', 'Quicklook', 'PScale']

TIMEFORMAT = '%Y%m%d%H%M%S'

class _Range(object):
    def __init__(self, min_, max_, create):
        self.min = min_
        self.max = max_
        self.create = create
    
    def __xor__(self, other):
        if not isinstance(other, self.__class__):
            return NotImplemented
        
        new = DummyAttr()
        if self.min < other.min:            
            new |= self.create(self.min, min(other.min, self.max))
        if other.max < self.max:
            new |= self.create(other.max, self.max)
        return new
    
    def __contains__(self, other):
        return self.min <= other.min and self.max >= other.max


class Wave(Attr, _Range):
    def __init__(self, wavemin, wavemax, waveunit='Angstrom'):
        self.min, self.max = sorted(
            to_angstrom(v, waveunit) for v in [float(wavemin), float(wavemax)]
        )
        self.unit = 'Angstrom'
        
        Attr.__init__(self)
        _Range.__init__(self, self.min, self.max, self.__class__)
    
    def collides(self, other):
        return isinstance(other, self.__class__)

    def __repr__(self):
	return '<Wave({0!r}, {1!r}, {2!r})>'.format(self.min, self.max, self.unit)


class Time(Attr, _Range):
    def __init__(self, start, end=None, near=None):
        if end is None and not isinstance(start, TimeRange):
            raise ValueError("Specify start and end or start has to be a TimeRange")
        if isinstance(start, TimeRange):
            self.start = start.t1
            self.end = start.t2
        else:
            self.start = parse_time(start)
            self.end = parse_time(end)
        self.near = None if near is None else parse_time(near)

        _Range.__init__(self, self.start, self.end, self.__class__)
        Attr.__init__(self)
    
    def collides(self, other):
        return isinstance(other, self.__class__)
    
    def __xor__(self, other):
        if not isinstance(other, self.__class__):
            raise TypeError
        if self.near is not None or other.near is not None:
            raise TypeError
        return _Range.__xor__(self, other)
    
    def pad(self, timedelta):
        return Time(self.start - timedelta, self.start + timedelta)
    
    def __repr__(self):
        return '<Time(%r, %r, %r)>' % (self.start, self.end, self.near)


class Extent(Attr):
    # pylint: disable=R0913
    def __init__(self, x, y, width, length, atype):
        Attr.__init__(self)
        
        self.x = x
        self.y = y
        self.width = width
        self.length = length
        self.type = atype
    
    def collides(self, other):
        return isinstance(other, self.__class__)


class Field(ValueAttr):
    def __init__(self, fielditem):
        ValueAttr.__init__(self, {
            ('field', 'fielditem'): fielditem
        })


class _VSOSimpleAttr(Attr):
    """ A _SimpleAttr is an attribute that is not composite, i.e. that only
    has a single value, such as, e.g., Instrument('eit'). """
    def __init__(self, value):
        Attr.__init__(self)
        
        self.value = value
    
    def collides(self, other):
        return isinstance(other, self.__class__)
    
    def __repr__(self):
        return "<%s(%r)>" % (self.__class__.__name__, self.value)


class Provider(_VSOSimpleAttr):
    pass


class Source(_VSOSimpleAttr):
    pass


class Instrument(_VSOSimpleAttr):
    pass


class Physobs(_VSOSimpleAttr):
    pass


class Pixels(_VSOSimpleAttr):
    pass


class Level(_VSOSimpleAttr):
    pass


class Resolution(_VSOSimpleAttr):
    pass


class Detector(_VSOSimpleAttr):
    pass


class Filter(_VSOSimpleAttr):
    pass


class Sample(_VSOSimpleAttr):
    pass


class Quicklook(_VSOSimpleAttr):
    pass


class PScale(_VSOSimpleAttr):
    pass


# The walker specifies how the Attr-tree is converted to a query the
# server can handle.
walker = AttrWalker()

# The _create functions make a new VSO query from the attribute tree,
# the _apply functions take an existing query-block and update it according
# to the attribute tree passed in as root. Different attributes require
# different functions for conversion into query blocks.

@walker.add_creator(ValueAttr, AttrAnd)
# pylint: disable=E0102,C0103,W0613
def _create(wlk, root, api):
    """ Implementation detail. """
    value = api.factory.create('QueryRequestBlock')
    wlk.apply(root, api, value)
    return [value]

@walker.add_applier(ValueAttr)
# pylint: disable=E0102,C0103,W0613
def _apply(wlk, root, api, queryblock):
    """ Implementation detail. """
    for k, v in root.attrs.iteritems():
        lst = k[-1]
        rest = k[:-1]
        
        block = queryblock
        for elem in rest:
            block = block[elem]
        block[lst] = v

@walker.add_applier(AttrAnd)
# pylint: disable=E0102,C0103,W0613
def _apply(wlk, root, api, queryblock):
    """ Implementation detail. """
    for attr in root.attrs:
        wlk.apply(attr, api, queryblock)

@walker.add_creator(AttrOr)
# pylint: disable=E0102,C0103,W0613
def _create(wlk, root, api):
    """ Implementation detail. """
    blocks = []
    for attr in root.attrs:
        blocks.extend(wlk.create(attr, api))
    return blocks


# Converters take a type unknown to the walker and convert it into one
# known to it. All of those convert types into ValueAttrs, which are
# handled above by just assigning according to the keys and values of the
# attrs member.
walker.add_converter(Extent)(
    lambda x: ValueAttr(
        dict((('extent', k), v) for k, v in vars(x).iteritems())
    )
)

walker.add_converter(Time)(
    lambda x: ValueAttr({
            ('time', 'start'): x.start.strftime(TIMEFORMAT),
            ('time', 'end'): x.end.strftime(TIMEFORMAT) ,
            ('time', 'near'): (
                x.near.strftime(TIMEFORMAT) if x.near is not None else None),
    })
)

walker.add_converter(_VSOSimpleAttr)(
    lambda x: ValueAttr({(x.__class__.__name__.lower(), ): x.value})
)

walker.add_converter(Wave)(
    lambda x: ValueAttr({
            ('wave', 'wavemin'): x.min,
            ('wave', 'wavemax'): x.max,
            ('wave', 'waveunit'): x.unit,
    })
)

# The idea of using a multi-method here - that means a method which dispatches
# by type but is not attached to said class - is that the attribute classes are
# designed to be used not only in the context of VSO but also elsewhere (which
# AttrAnd and AttrOr obviously are - in the HEK module). If we defined the
# filter method as a member of the attribute classes, we could only filter
# one type of data (that is, VSO data).
filter_results = MultiMethod(lambda *a, **kw: (a[0], ))

# If we filter with ANDed together attributes, the only items are the ones
# that match all of them - this is implementing  by ANDing the pool of items
# with the matched items - only the ones that match everything are there
# after this.
@filter_results.add_dec(AttrAnd)
def _(attr, results):
    res = set(results)
    for elem in attr.attrs:
        res &= filter_results(elem, res)
    return res

# If we filter with ORed attributes, the only attributes that should be
# removed are the ones that match none of them. That's why we build up the
# resulting set by ORing all the matching items.
@filter_results.add_dec(AttrOr)
def _(attr, results):
    res = set()
    for elem in attr.attrs:
        res |= filter_results(elem, results)
    return res

# Filter out items by comparing attributes.
@filter_results.add_dec(_VSOSimpleAttr)
def _(attr, results):
    attrname = attr.__class__.__name__.lower()
    return set(
        item for item in results
        # Some servers seem to obmit some fields. No way to filter there.
        if not hasattr(item, attrname) or
        getattr(item, attrname).lower() == attr.value.lower()
    )

# The dummy attribute does not filter at all.
@filter_results.add_dec(DummyAttr, Field)
def _(attr, results):
    return set(results)


@filter_results.add_dec(Wave)
def _(attr, results):
    return set(
        it for it in results
        if
        it.wave.wavemax is not None
        and
        attr.min <= to_angstrom(float(it.wave.wavemax), it.wave.waveunit)
        and
        it.wave.wavemin is not None
        and
        attr.max >= to_angstrom(float(it.wave.wavemin), it.wave.waveunit)
    )

@filter_results.add_dec(Time)
def _(attr, results):
    return set(
        it for it in results
        if
        it.time.end is not None
        and
        attr.min <= datetime.strptime(it.time.end, TIMEFORMAT)
        and
        it.time.start is not None
        and
        attr.max >= datetime.strptime(it.time.start, TIMEFORMAT)
    )

@filter_results.add_dec(Extent)
def _(attr, results):
    return set(
        it for it in results
        if
        it.extent.type is not None
        and
        it.extent.type.lower() == attr.type.lower()
    )

########NEW FILE########
__FILENAME__ = vso
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>
#
# This module was developed with funding provided by
# the ESA Summer of Code (2011).
#
#pylint: disable=W0401,C0103,R0904,W0141

from __future__ import absolute_import
from __future__ import division

"""
This module provides a wrapper around the VSO API.
"""

import re
import os
import sys
import threading

from datetime import datetime, timedelta
from functools import partial
from collections import defaultdict
from suds import client, TypeNotFound

from sunpy import config
from sunpy.net import download
from sunpy.net.proxyfix import WellBehavedHttpTransport
from sunpy.util.progressbar import TTYProgressBar as ProgressBar
from sunpy.util.net import get_filename, slugify
from sunpy.net.attr import and_, Attr
from sunpy.net.vso import attrs
from sunpy.net.vso.attrs import walker, TIMEFORMAT
from sunpy.util import print_table, replacement_filename
from sunpy.time import parse_time

DEFAULT_URL = 'http://docs.virtualsolar.org/WSDL/VSOi_rpc_literal.wsdl'
DEFAULT_PORT = 'nsoVSOi'
RANGE = re.compile(r'(\d+)(\s*-\s*(\d+))?(\s*([a-zA-Z]+))?')


# TODO: Name
class NoData(Exception):
    """ Risen for callbacks of VSOClient that are unable to supply
    information for the request. """
    pass


class _Str(str):
    """ Subclass of string that contains a meta attribute for the
    record_item associated with the file. """
    pass


# ----------------------------------------


class Results(object):
    """ Returned by VSOClient.get. Use .wait to wait
    for completion of download.
    """
    def __init__(self, callback, n=0, done=None):
        self.callback = callback
        self.n = self.total = n
        self.map_ = {}
        self.done = done
        self.evt = threading.Event()
        self.errors = []
        self.lock = threading.RLock()

        self.progress = None

    def submit(self, keys, value):
        """
        Submit

        Parameters
        ----------
        keys : list
            names under which to save the value
        value : object
            value to save
        """
        for key in keys:
            self.map_[key] = value
        self.poke()

    def poke(self):
        """ Signal completion of one item that was waited for. This can be
        because it was submitted, because it lead to an error or for any
        other reason. """
        with self.lock:
            self.n -= 1
            if self.progress is not None:
               self.progress.poke()
            if not self.n:
                if self.done is not None:
                    self.map_ = self.done(self.map_)
                self.callback(self.map_)
                self.evt.set()

    def require(self, keys):
        """ Require that keys be submitted before the Results object is
        finished (i.e., wait returns). Returns a callback method that can
        be used to submit the result by simply calling it with the result.

        keys : list
            name of keys under which to save the result
        """
        with self.lock:
            self.n += 1
            self.total += 1
            return partial(self.submit, keys)

    def wait(self, timeout=100, progress=False):
        """ Wait for result to be complete and return it. """
        # Giving wait a timeout somehow circumvents a CPython bug that the
        # call gets ininterruptible.
        if progress:
            with self.lock:
                self.progress = ProgressBar(self.total, self.total - self.n)
                self.progress.start()
                self.progress.draw()

        while not self.evt.wait(timeout):
            pass
        if progress:
            self.progress.finish()

        return self.map_

    def add_error(self, exception):
        """ Signal a required result cannot be submitted because of an
        error. """
        self.errors.append(exception)
        self.poke()


def _parse_waverange(string):
    min_, max_, unit = RANGE.match(string).groups()[::2]
    return {
        'wave_wavemin': min_,
        'wave_wavemax': min_ if max_ is None else max_,
        'wave_waveunit': 'Angstrom' if unit is None else unit,
    }


def _parse_date(string):
    start, end = string.split(' - ')
    return {'time_start': start.strip(), 'time_end': end.strip()}


def iter_records(response):
    for prov_item in response.provideritem:
        if not hasattr(prov_item, 'record') or not prov_item.record:
            continue
        for record_item in prov_item.record.recorditem:
            yield record_item


def iter_errors(response):
    for prov_item in response.provideritem:
        if not hasattr(prov_item, 'record') or not prov_item.record:
            yield prov_item


class QueryResponse(list):
    def __init__(self, lst, queryresult=None):
        super(QueryResponse, self).__init__(lst)
        self.queryresult = queryresult
        self.errors = []

    def query(self, *query):
        """ Furtherly reduce the query response by matching it against
        another query, e.g. response.query(attrs.Instrument('aia')). """
        query = and_(*query)
        return QueryResponse(
            attrs.filter_results(query, self), self.queryresult
        )

    @classmethod
    def create(cls, queryresult):
        return cls(iter_records(queryresult), queryresult)

    def total_size(self):
        """ Total size of data in KB. May be less than the actual
        size because of inaccurate data providers. """
        # Warn about -1 values?
        return sum(record.size for record in self if record.size > 0)

    def num_records(self):
        """ Return number of records. """
        return len(self)

    def time_range(self):
        """ Return total time-range all records span across. """
        return (
            datetime.strptime(
                min(record.time.start for record in self
                  if record.time.start is not None), TIMEFORMAT),
            datetime.strptime(
                max(record.time.end for record in self
                  if record.time.end is not None), TIMEFORMAT)
        )

    def show(self):
        """Print out human-readable summary of records retrieved"""

        table = [
          [
            str(datetime.strptime(record.time.start, TIMEFORMAT))
              if record.time.start is not None else 'N/A',
            str(datetime.strptime(record.time.end, TIMEFORMAT))
              if record.time.end is not None else 'N/A',
            record.source,
            record.instrument,
            record.extent.type
              if record.extent.type is not None else 'N/A'
          ] for record in self]
        table.insert(0, ['----------','--------','------','----------','----'])
        table.insert(0, ['Start time','End time','Source','Instrument','Type'])

        print(print_table(table, colsep = '  ', linesep='\n'))

    def add_error(self, exception):
        self.errors.append(exception)

class DownloadFailed(Exception):
    pass

class MissingInformation(Exception):
    pass

class UnknownMethod(Exception):
    pass

class MultipleChoices(Exception):
    pass

class UnknownVersion(Exception):
    pass

class UnknownStatus(Exception):
    pass

class VSOClient(object):
    """ Main VSO Client. """
    method_order = [
        'URL-TAR_GZ', 'URL-ZIP', 'URL-TAR', 'URL-FILE', 'URL-packaged'
    ]
    def __init__(self, url=None, port=None, api=None):
        if api is None:
            if url is None:
                url = DEFAULT_URL
            if port is None:
                port = DEFAULT_PORT

            api = client.Client(url, transport = WellBehavedHttpTransport())
            api.set_options(port=port)
        self.api = api

    def make(self, atype, **kwargs):
        """ Create new SOAP object with attributes specified in kwargs.
        To assign subattributes, use foo__bar=1 to assign
        ['foo']['bar'] = 1. """
        obj = self.api.factory.create(atype)
        for k, v in kwargs.iteritems():
            split = k.split('__')
            tip = split[-1]
            rest = split[:-1]

            item = obj
            for elem in rest:
                item = item[elem]

            if isinstance(v, dict):
                # Do not throw away type information for dicts.
                for k, v in v.iteritems():
                    item[tip][k] = v
            else:
                item[tip] = v
        return obj

    def query(self, *query):
        """ Query data from the VSO with the new API. Takes a variable number
        of attributes as parameter, which are chained together using AND.

        The new query language allows complex queries to be easily formed.

        Examples
        --------
        Query all data from eit or aia between 2010-01-01T00:00 and
        2010-01-01T01:00.

        >>> client.query(
        ...    vso.Time(datetime(2010, 1, 1), datetime(2010, 1, 1, 1)),
        ...    vso.Instrument('eit') | vso.Instrument('aia')
        ... )

        Returns
        -------
        out : :py:class:`QueryResult` (enhanced list) of matched items. Return value of same type as the one of :py:meth:`VSOClient.query`.
        """
        query = and_(*query)

        responses = []
        for block in walker.create(query, self.api):
            try:
                responses.append(
                    self.api.service.Query(
                        self.make('QueryRequest', block=block)
                    )
                )
            except TypeNotFound:
                pass
            except Exception as ex:
                response = QueryResponse.create(self.merge(responses))
                response.add_error(ex)

        return QueryResponse.create(self.merge(responses))

    def merge(self, queryresponses):
        """ Merge responses into one. """
        if len(queryresponses) == 1:
            return queryresponses[0]

        fileids = set()
        providers = {}

        for queryresponse in queryresponses:
            for provideritem in queryresponse.provideritem:
                provider = provideritem.provider
                if not hasattr(provideritem, 'record'):
                    continue
                if not hasattr(provideritem.record, 'recorditem'):
                    continue
                if not provideritem.provider in providers:
                    providers[provider] = provideritem
                    fileids |= set(
                        record_item.fileid
                        for record_item in provideritem.record.recorditem
                    )
                else:
                    for record_item in provideritem.record.recorditem:
                        if record_item.fileid not in fileids:
                            fileids.add(record_item.fileid)
                            providers[provider].record.recorditem.append(
                                record_item
                            )
                            providers[provider].no_of_records_found += 1
                            providers[provider].no_of_records_returned += 1
        return self.make('QueryResponse', provideritem=providers.values())

    @staticmethod
    def mk_filename(pattern, response, sock, url, overwrite=False):
        name = get_filename(sock, url)
        if not name:
            if not isinstance(response.fileid, unicode):
                name = unicode(response.fileid, "ascii", "ignore")
            else:
                name = response.fileid

        fs_encoding = sys.getfilesystemencoding()
        if fs_encoding is None:
            fs_encoding = "ascii"
        name = slugify(name).encode(fs_encoding, "ignore")

        if not name:
            name = "file"

        fname = pattern.format(file=name, **dict(response))

        if not overwrite and os.path.exists(fname):
            fname = replacement_filename(fname)

        dir_ = os.path.abspath(os.path.dirname(fname))
        if not os.path.exists(dir_):
            os.makedirs(dir_)
        return fname

    # pylint: disable=R0914
    def query_legacy(self, tstart=None, tend=None, **kwargs):
        """
        Query data from the VSO mocking the IDL API as close as possible.
        Either tstart and tend or date_start and date_end or date have
        to be supplied.

        Parameters
        ----------
        tstart : datetime.datetime
            Start of the time-range in which records are searched.
        tend : datetime.datetime
            Start of the time-range in which records are searched.
        date : str
            (start date) - (end date)
        start_date : datetime
            the start date
        end_date : datetime
            the end date
        wave : str
            (min) - (max) (unit)
        min_wave : str
            minimum spectral range
        max_wave : str
            maximum spectral range
        unit_wave : str
            spectral range units (Angstrom, GHz, keV)
        extent : str
            VSO 'extent type' ... (FULLDISK, CORONA, LIMB, etc)
        physobj : str
            VSO 'physical observable'
        provider : str
            VSO ID for the data provider (SDAC, NSO, SHA, MSU, etc)
        source : str
            spacecraft or observatory (SOHO, YOHKOH, BBSO, etc)
            synonyms : spacecraft, observatory
        instrument : str
            instrument ID (EIT, SXI-0, SXT, etc)
            synonyms : telescope, inst
        detector : str
            detector ID (C3, EUVI, COR2, etc.)
        layout : str
            layout of the data (image, spectrum, time_series, etc.)

        level : str
            level of the data product (numeric range, see below)
        pixels : str
            number of pixels (numeric range, see below)
        resolution : str
            effective resolution (1 = full, 0.5 = 2x2 binned, etc)
            numeric range, see below.
        pscale : str
            pixel scale, in arcseconds (numeric range, see below)
        near_time : datetime
            return record closest to the time.  See below.
        sample : int
            attempt to return only one record per SAMPLE seconds.  See below.

        Numeric Ranges:

            - May be entered as a string or any numeric type for equality matching
            - May be a string of the format '(min) - (max)' for range matching
            - May be a string of the form '(operator) (number)' where operator is one of: lt gt le ge < > <= >=


        Examples
        --------
        Query all data from eit between 2010-01-01T00:00 and
        2010-01-01T01:00.

        >>> qr = client.query_legacy(
        ...     datetime(2010, 1, 1), datetime(2010, 1, 1, 1), instrument='eit')

        Returns
        -------
        out : :py:class:`QueryResult` (enhanced list) of matched items. Return value of same type as the one of :py:class:`VSOClient.query`.
        """
        sdk = lambda key: lambda value: {key: value}
        ALIASES = {
            'wave_min': sdk('wave_wavemin'),
            'wave_max': sdk('wave_wavemax'),
            'wave_type': sdk('wave_wavetype'),
            'wave_unit': sdk('wave_waveunit'),
            'min_wave': sdk('wave_wavemin'),
            'max_wave': sdk('wave_wavemax'),
            'type_wave': sdk('wave_wavetype'),
            'unit_wave': sdk('wave_waveunit'),
            'wave': _parse_waverange,
            'inst': sdk('instrument'),
            'telescope': sdk('instrument'),
            'spacecraft': sdk('source'),
            'observatory': sdk('source'),
            'start_date': sdk('time_start'),
            'end_date': sdk('time_end'),
            'start': sdk('time_start'),
            'end': sdk('time_end'),
            'near_time': sdk('time_near'),
            'date': _parse_date,
            'layout': sdk('datatype'),
        }
        if tstart is not None:
            kwargs.update({'time_start': tstart})
        if tend is not None:
            kwargs.update({'time_end': tend})

        queryreq = self.api.factory.create('QueryRequest')
        for key, value in kwargs.iteritems():
            for k, v in ALIASES.get(key, sdk(key))(value).iteritems():
                if k.startswith('time'):
                    v = parse_time(v).strftime(TIMEFORMAT)
                attr = k.split('_')
                lst = attr[-1]
                rest = attr[:-1]

                # pylint: disable=E1103
                item = queryreq.block
                for elem in rest:
                    try:
                        item = item[elem]
                    except KeyError:
                        raise ValueError("Unexpected argument %s." % key)
                if lst not in item:
                    raise ValueError("Unexpected argument %s." % key)
                if item[lst]:
                    raise ValueError("Got multiple values for %s." % k)
                item[lst] = v
        try:
            return QueryResponse.create(self.api.service.Query(queryreq))
        except TypeNotFound:
            return QueryResponse([])

    def latest(self):
        """ Return newest record (limited to last week). """
        return self.query_legacy(
            datetime.utcnow()  - timedelta(7),
            datetime.utcnow(),
            time_near=datetime.utcnow()
        )

    def get(self, query_response, path=None, methods=('URL-FILE',), downloader=None, site=None):
        """
        Download data specified in the query_response.

        Parameters
        ----------
        query_response : sunpy.net.vso.QueryResponse
            QueryResponse containing the items to be downloaded.
        path : str
            Specify where the data is to be downloaded. Can refer to arbitrary
            fields of the QueryResponseItem (instrument, source, time, ...) via
            string formatting, moreover the file-name of the file downloaded can
            be refered to as file, e.g.
            "{source}/{instrument}/{time.start}/{file}".
        methods : {list of str}
            Methods acceptable to user.
        downloader : sunpy.net.downloader.Downloader
            Downloader used to download the data.
        site: str
            There are a number of caching mirrors for SDO and other
            instruments, some available ones are listed below.

            =============== ========================================================
            NSO             National Solar Observatory, Tucson (US)
            SAO  (aka CFA)  Smithonian Astronomical Observatory, Harvard U. (US)
            SDAC (aka GSFC) Solar Data Analysis Center, NASA/GSFC (US)
            ROB             Royal Observatory of Belgium (Belgium)
            MPS             Max Planck Institute for Solar System Research (Germany)
            UCLan           University of Central Lancashire (UK)
            IAS             Institut Aeronautique et Spatial (France)
            KIS             Kiepenheuer-Institut fur Sonnenphysik Germany)
            NMSU            New Mexico State University (US)
            =============== ========================================================

        Returns
        -------
        out : :py:class:`Results` object that supplies a list of filenames with meta attributes containing the respective QueryResponse.

        Examples
        --------
        >>> res = get(qr).wait() # doctest:+SKIP
        """
        if downloader is None:
            downloader = download.Downloader()
            downloader.init()
            res = Results(
                lambda _: downloader.stop(), 1,
                lambda mp: self.link(query_response, mp)
            )
        else:
            res = Results(
                lambda _: None, 1, lambda mp: self.link(query_response, mp)
            )
        if path is None:
            path = os.path.join(config.get('downloads','download_dir'),
                                '{file}')
        fileids = VSOClient.by_fileid(query_response)
        if not fileids:
            res.poke()
            return res
        # Adding the site parameter to the info
        info = {}
        if site is not None:
            info['site']=site

        self.download_all(
            self.api.service.GetData(
                self.make_getdatarequest(query_response, methods, info)
                ),
            methods, downloader, path,
            fileids, res
        )
        res.poke()
        return res

    @staticmethod
    def link(query_response, maps):
        """ Return list of paths with records associated with them in
        the meta attribute. """
        if not maps:
            return []
        ret = []

        for record_item in query_response:
            try:
                item = _Str(maps[record_item.fileid]['path'])
            except KeyError:
                continue
            # pylint: disable=W0201
            item.meta = record_item
            ret.append(item)
        return ret

    def make_getdatarequest(self, response, methods=None, info=None):
        """ Make datarequest with methods from response. """
        if methods is None:
            methods = self.method_order + ['URL']

        return self.create_getdatarequest(
            dict((k, [x.fileid for x in v])
                 for k, v in self.by_provider(response).iteritems()),
            methods, info
        )

    def create_getdatarequest(self, maps, methods, info=None):
        """ Create datarequest from maps mapping data provider to
        fileids and methods, """
        if info is None:
            info = {}

        return self.make(
            'VSOGetDataRequest',
            request__method__methodtype=methods,
            request__info=info,
            request__datacontainer__datarequestitem=[
                self.make('DataRequestItem', provider=k, fileiditem__fileid=[v])
                for k, v in maps.iteritems()
            ]
        )

    # pylint: disable=R0913,R0912
    def download_all(self, response, methods, dw, path, qr, res, info=None):
        GET_VERSION = [
            ('0.8', (5, 8)),
            ('0.7', (1, 4)),
            ('0.6', (0, 3)),
        ]
        for dresponse in response.getdataresponseitem:
            for version, (from_, to) in GET_VERSION:
                if getattr(dresponse, version, '0.6') >= version:
                    break
            else:
                res.add_error(UnknownVersion(dresponse))
                continue

            # If from_ and to are uninitialized, the else block of the loop
            # continues the outer loop and thus this code is never reached.
            # pylint: disable=W0631
            code = (
                dresponse.status[from_:to]
                if hasattr(dresponse, 'status') else '200'
            )
            if code == '200':
                for dataitem in dresponse.getdataitem.dataitem:
                    try:
                        self.download(
                            dresponse.method.methodtype[0],
                            dataitem.url,
                            dw,
                            res.require(map(str, dataitem.fileiditem.fileid)),
                            res.add_error,
                            path,
                            qr[dataitem.fileiditem.fileid[0]]
                        )
                    except NoData:
                        res.add_error(DownloadFailed(dresponse))
                        continue
                    except Exception:
                        # FIXME: Is this a good idea?
                        res.add_error(DownloadFailed(dresponse))
            elif code == '300' or code == '412' or code == '405':
                if code == '300':
                    try:
                        methods = self.multiple_choices(
                            dresponse.method.methodtype, dresponse
                        )
                    except NoData:
                        res.add_error(MultipleChoices(dresponse))
                        continue
                elif code == '412':
                    try:
                        info = self.missing_information(
                            info, dresponse.info
                        )
                    except NoData:
                        res.add_error(MissingInformation(dresponse))
                        continue
                elif code == '405':
                    try:
                        methods = self.unknown_method(dresponse)
                    except NoData:
                        res.add_error(UnknownMethod(dresponse))
                        continue

                files = []
                for dataitem in dresponse.getdataitem.dataitem:
                    files.extend(dataitem.fileiditem.fileid)

                request = self.create_getdatarequest(
                    {dresponse.provider: files}, methods, info
                )

                self.download_all(
                    self.api.service.GetData(request), methods, dw, path,
                    qr, res, info
                )
            else:
                res.add_error(UnknownStatus(dresponse))

    def download(self, method, url, dw, callback, errback, *args):
        """ Override to costumize download action. """
        if method.startswith('URL'):
            return dw.download(url, partial(self.mk_filename, *args),
                        callback, errback
            )
        raise NoData

    @staticmethod
    def by_provider(response):
        map_ = defaultdict(list)
        for record in response:
            map_[record.provider].append(record)
        return map_

    @staticmethod
    def by_fileid(response):
        return dict(
            (record.fileid, record) for record in response
        )

    # pylint: disable=W0613
    def multiple_choices(self, choices, response):
        """ Override to pick between multiple download choices. """
        for elem in self.method_order:
            if elem in choices:
                return [elem]
        raise NoData

    # pylint: disable=W0613
    def missing_information(self, info, field):
        """ Override to provide missing information. """
        raise NoData

    # pylint: disable=W0613
    def unknown_method(self, response):
        """ Override to pick a new method if the current one is unknown. """
        raise NoData


class InteractiveVSOClient(VSOClient):
    """ Client for use in the REPL. Prompts user for data if required. """
    def multiple_choices(self, choices, response):
        while True:
            for n, elem in enumerate(choices):
                print "(%d) %s" % (n + 1, elem)
            try:
                choice = raw_input("Method number: ")
            except KeyboardInterrupt:
                raise NoData
            if not choice:
                raise NoData
            try:
                choice = int(choice) - 1
            except ValueError:
                continue
            if choice == -1:
                raise NoData
            elif choice >= 0:
                try:
                    return [choices[choice]]
                except IndexError:
                    continue

    def missing_information(self, info, field):
        choice = raw_input(field + ': ')
        if not choice:
            raise NoData
        return choice

    def search(self, *args, **kwargs):
        """ When passed an Attr object, perform new-style query;
        otherwise, perform legacy query.
        """
        if isinstance(args[0], Attr):
            return self.query(*args)
        else:
            return self.query_legacy(*args, **kwargs)

    def get(self, query_response, path=None, methods=('URL-FILE',), downloader=None):
        """The path expands ``~`` to refer to the user's home directory.
        If the given path is an already existing directory, ``{file}`` is
        appended to this path. After that, all received parameters (including
        the updated path) are passed to :meth:`VSOClient.get`.

        """
        if path is not None:
            path = os.path.abspath(os.path.expanduser(path))
            if os.path.exists(path) and os.path.isdir(path):
                path = os.path.join(path, '{file}')
        return VSOClient.get(self, query_response, path, methods, downloader)


g_client = None
def search(*args, **kwargs):
    # pylint: disable=W0603
    global g_client
    if g_client is None:
        g_client = InteractiveVSOClient()
    return g_client.search(*args, **kwargs)

search.__doc__ = InteractiveVSOClient.search.__doc__

def get(query_response, path=None, methods=('URL-FILE',), downloader=None):
    # pylint: disable=W0603
    global g_client
    if g_client is None:
        g_client = InteractiveVSOClient()
    return g_client.get(query_response, path, methods, downloader)

get.__doc__ = VSOClient.get.__doc__

if __name__ == "__main__":
    from sunpy.net import vso

    client = VSOClient()
    result = client.query(
        vso.attrs.Time((2011, 1, 1), (2011, 1, 1, 10)),
        vso.attrs.Instrument('aia')
    )
    #res = client.get(result, path="/download/path").wait()

########NEW FILE########
__FILENAME__ = differential_rotation
from __future__ import division

__all__ = ['diff_rot']
import numpy as np
import datetime

__author__ = ["Jose Ivan Campos Rozo","Stuart Mumford"]
__all__ = ['diff_rot']


def diff_rot(ddays,latitude,rot_type='howard',frame_time='sidereal'):
    """
    This function computes the change in longitude over days in degrees.

    Parameters
    -----------
    ddays: float or timedelta
        Number of days to rotate over, or timedelta object.
        
    latitude: float or array-like
        heliographic coordinate latitude in Degrees.
        
    rot_type: {'howard' | 'snodgrass' | 'allen'}
        howard: Use values for small magnetic features from Howard et al.
        snodgrass: Use Values from Snodgrass et. al
        allen: Use values from Allen, Astrophysical Quantities, and simplier equation.
    
    frame_time: {'sidereal' | 'synodic'}
        Choose 'type of day' time reference frame.

    Returns
    -------
    longditude_delta: ndarray            
        The change in longitude over days (units=degrees)
    
    Notes
    -----
    * IDL code equavalent: http://hesperia.gsfc.nasa.gov/ssw/gen/idl/solar/diff_rot.pro
    * Howard rotation: http://adsabs.harvard.edu/abs/1990SoPh..130..295H
    * A review of rotation parameters (including Snodgrass values): http://link.springer.com/article/10.1023%2FA%3A1005226402796
    
    Examples
    --------
    Default rotation calculation over two days at 30 degrees latitude:
    
    >>> rotation = diff_rot(2, 30)
    
    Default rotation over two days for a number of latitudes:
    
    >>> rotation = diff_rot(2, np.linspace(-70, 70, 20))
    
    With rotation type 'allen':
    
    >>> rotation = diff_rot(2, np.linspace(-70, 70, 20), 'allen')
    """
    
    if not isinstance(ddays,datetime.timedelta):
        delta = datetime.timedelta(days=ddays)
    
    delta_seconds = (delta.microseconds + (delta.seconds + delta.days * 24 * 3600) *
                    10**6) / 10**6
    delta_days = delta_seconds / 24 / 3600
    
    sin2l = (np.sin(np.deg2rad(latitude)))**2
    sin4l = sin2l**2
    
    rot_params = {'howard': [2.894, -0.428, -0.370],
                  'snodgrass': [2.851, -0.343, -0.474]
                  }
                  
    if rot_type not in ['howard', 'allen', 'snodgrass']:
        raise ValueError("""rot_type must equal one of 
                        { 'howard' | 'allen' | 'snodgrass' }""")  
                        
    elif rot_type == 'allen':
        rotation_deg = delta_days * (14.44 - (3.0 * sin2l))
        
    else:
        A, B, C = rot_params[rot_type] 
        
        #This is in micro-radians / sec
        rotation_rate = A + B * sin2l + C * sin4l 
        rotation_deg = rotation_rate * 1e-6  * delta_seconds / np.deg2rad(1)
    
    if frame_time == 'synodic':
        rotation_deg -= 0.9856 * delta_days
    
    return np.round(rotation_deg,4)

########NEW FILE########
__FILENAME__ = test_differential_rotation
from __future__ import absolute_import
import unittest
import numpy as np
from sunpy.physics.transforms.differential_rotation import diff_rot
#pylint: disable=C0103,R0904,W0201,W0212,W0232,E1103

class DiffRotTest(unittest.TestCase):
    """ Please note the numbers in these tests are not checked for physical
    accuracy, only that they are the values the function was outputting upon
    implementation. """
    
    def test_single(self):
        rot = diff_rot(10, 30)
        self.failUnless(rot == 136.8216)
        
    def test_array(self):
        rot = diff_rot(10, np.linspace(-70, 70, 2))
        self.failUnless(np.array_equal(rot, np.array([110.2725,  110.2725])))
        
    def test_synodic(self):
        rot = diff_rot(10, 30, rot_type='howard', frame_time='synodic')
        self.failUnless(rot == 126.9656)
        
    def test_sidereal(self):
        rot = diff_rot(10, 30, rot_type='howard', frame_time='sidereal')
        self.failUnless(rot == 136.8216)
        
    def test_howard(self):
        rot = diff_rot(10, 30, rot_type='howard')
        self.failUnless(rot == 136.8216)
 
    def test_allen(self):
        rot = diff_rot(10, 30, rot_type='allen')
        self.failUnless(rot == 136.9)
    
    def test_snodgrass(self):
        rot = diff_rot(10, 30, rot_type='snodgrass')
        self.failUnless(rot == 135.4232)
    
    def test_fail(self):
        self.assertRaises(ValueError, diff_rot, 10, 30, rot_type='garbage')

########NEW FILE########
__FILENAME__ = chaincode
__authors__ = ["David PS"]
__email__ = "dps.helio-?-gmail.com"

import numpy as np

class Chaincode(np.ndarray):
    """
    Chaincode(origin, chaincode, xdelta=1, ydelta=1)

    A tool to infer some information from chaincodes produced
    by HELIO Feature Catalogue or Heliphyisics Events Knowledgebase

    Parameters
    ----------
    origin : numpy.ndarray, list
        The 2 points of the origin of the chaincode
    chaincode : string
        A list of the numbers (0-7) that indicate the path of the 
        chaincode.  0 moves horizontally to the left and the rest
        follows anticlockwise.
    xdelta : Float
    ydelta : Float
        The scale to convert between pixels and flat coordinates

    Returns
    -------
    cc.coordinates : numpy.ndarray
        An array containing all the x and y coordinates of the cc
        such [[x0, x1, x2, ..., xn], [y0 ,y1, y2, ..., yn]]

    Examples
    --------
    >>> cc = Chaincode([-88, 812], "44464655567670006011212222324", 
    ...     xdelta=2.629, ydelta=2.629)
    
    >>> fig = plt.figure()
    >>> ax = fig.add_subplot(111)
    >>> x,y = zip(cc.coordinates)
    >>> ax.plot(x[0], y[0], 'go-')
    >>> fig.show()
    """
    def __new__(cls, origin, chaincode, **kargs):
        if isinstance(origin, list):
            obj = np.asarray(origin).view(cls)
        else:
            raise TypeError('Invalid input')
        return obj

    def __init__(self, origin, chaincode, xdelta=1, ydelta=1):
        x_steps = [-1, -1, 0, 1, 1, 1, 0, -1]
        y_steps = [0, -1, -1, -1, 0, 1, 1, 1]
        self.coordinates = np.ndarray((2, len(chaincode)+1))
        self.coordinates[:,0] = origin
        if chaincode.isdigit():
            for index, step in enumerate(chaincode):
                self.coordinates[:,index + 1] = self.coordinates[:,index] + \
                    [[x_steps[int(step)] * xdelta, y_steps[int(step)] * ydelta]]

    def matchend(self, end):
        return np.alltrue(np.equal(self.coordinates[:,-1], np.asarray(end)))

    def matchany(self, coordinates, index):
        return np.alltrue(np.allclose(self.coordinates[:,index], np.asarray(coordinates)))

    def BoundingBox(self):
        '''
        Extract the coordinates of the chaincode
        [[x0,x1],[y0,y1]]
        '''
        bb = np.zeros((2,2))
        bb[:,0] = self.coordinates.min(1)
        bb[:,1] = self.coordinates.max(1)
        return bb

    def area(self):# should we add a mask for possible not flat objects (eg. Sun)?
        # Check whehter it is a closed object
        pass

    def length(self):
        pass

    def subBoundingBox(self, xedge=None, yedge=None):
        '''
        Extract the x or y boundaries of the chaincode from
        a defined limits xedge or yedge.
        '''
# It needs to check whether the input are lists and with 2 elements..
#        try:
#            if (type(xedge) == list) or (type(yedge) == list):
#                
        if xedge != None:
            edge = xedge
            IndexMask = 0  # we want to mask X
            IndexValue = 1 # we want to extract the MinMax from Y
        elif yedge != None:
            edge = yedge
            IndexMask = 1
            IndexValue = 0
        else:
            print "Not edges input"
            return None
        mask = (self.coordinates[IndexMask,:] >= edge[0]) & \
            (self.coordinates[IndexMask,:] <= edge[1])
# Should the edges be included?
        mx = np.ma.masked_array(self.coordinates[IndexValue,:], mask=(~mask))
        return [mx.min(), mx.max()]

########NEW FILE########
__FILENAME__ = roi
from __future__ import absolute_import

from sunpy.time import TimeRange
from sunpy.time import parse_time

__all__ = ['roi']

class roi:
    """
    A generalized region of interest (ROI) object
    
    Parameters
    ----------
    times : list (optional)
        A list of 1 or 2 parse_time-readable times
    description : str (optional)
        A text description of the ROI
    source : str (optional)
        A description of where this ROI comes from (e.g. the instrument, 'RHESSI', 'LYRA LYTAF')
    
    Attributes
    ----------
    start_time : datetime object containing the start time of the ROI
    end_time : datetime object containing the end time of the ROI
    description: A string descriptor of the ROI event type (e.g. 'attenuator change', 'LAR', 'SAA', 'flare')
    source: A string descriptor of the ROI source (e.g. 'LYRA', 'RHESSI')
    
    
    Methods
    -------
    time_range()
        Return a time range object from the start and end times of the ROI

    Examples
    --------
    >>> from sunpy.roi import roi
    
    >>> result = roi(times=['2011-02-15 04:34:09','2011-02-15 04:48:21'],description='UV occult.',source='LYRA LYTAF')
    
    >>> result = roi(times='2013-05-12 03:12:00') 
    """
    
    def __init__(self, times=None, description=None, source=None):
        # time could be a list with one or two elements
        if times and type(times) == list:
            if len(times) == 1:
                #if only one time given, make start and end times the same
                self.start_time = parse_time(times[0])
                self.end_time = parse_time(times[0])
            elif len(times) == 2:
                self.start_time = parse_time(times[0])
                self.end_time = parse_time(times[1])
            else:
                self.start_time = None
                self.end_time = None

        elif type(times) == str:
            self.start_time = parse_time(times)
            self.end_time = parse_time(times)
        else:
            self.start_time = None
            self.end_time = None

        
        #description of the ROI event type
        if description:
            self.description = str(description)
        else:
            self.description = None
            
        # optional description of where the ROI came from
        if source == None:
            self.source = "Unknown"
        else:
            self.source = source
                        
    def time_range(self):
        """Returns a TimeRange using the start and end times"""
        if self.start_time and self.end_time:
            return TimeRange(self.start_time, self.end_time)
        
    def __repr__(self):
        """Print out info on the ROI"""
        if not self.start_time:
            startstring = 'None'
        else:
            startstring=self.start_time.isoformat()

        if not self.end_time:
            endstring = 'None'
        else:
            endstring = self.end_time.isoformat()

        
        return('SunPy Region-of-interest (ROI) object' +
        '\n-------------------------------------' +
        '\nSource: \t\t' + self.source +
        '\nStart time:\t\t' + startstring +
        '\nEnd time: \t\t' + endstring +
        '\nEvent description:\t' + str(self.description))

 

########NEW FILE########
__FILENAME__ = test_chaincode
# Author: David PS <dps.helio -?- gmail.com>

import unittest
from sunpy.roi.chaincode import Chaincode
import numpy as np

class CCTests(unittest.TestCase):

    def testEnds(self):
        cc = Chaincode([0, 0], "2460") # Can I test more than one path? How?
        end = [0, 0]
        self.failUnless(cc.matchend(end))

    def testEndsFalse(self):
        cc = Chaincode([0, 0], "24460")
        end = [0, 0]
        self.failIf(cc.matchend(end))

    def testSecondCoordinate(self):
        cc = Chaincode([0, 0], "0023")
        second = [-2, 0]
        self.failUnless(cc.matchany(second, 2))

    def testSecondCoordinateFails(self):
        cc = Chaincode([1, 0], "0023")
        second = [-2, 0]
        self.failIf(cc.matchany(second, 2))

    def testScaleSecond(self):
        cc = Chaincode([0, 0], "0723", xdelta=0.5, ydelta=0.5)
        second = [-1, 0.5]
        self.failUnless(cc.matchany(second, 2))

    def testScaleEnd(self):
        cc = Chaincode([1.2, 3],"0723", xdelta=2.629, ydelta=2.629)
        end = [-1.429, 0.371]
        self.failUnless(cc.matchany(end, -1))

    def testnparray(self):
        # Let's test that the shape of the array matchs the expected
        # To do so we need to use np.array, instead of lists.
        cc = Chaincode([0, 0], "2460")
        shape = (2,5)
        self.failUnless(cc.coordinates.shape == shape)

    def testBoundingBox(self): #needs of np.array... I think
        cc = Chaincode([0, 0], "00033344")
        boundingbox = [[-3, 2], [-3, 0]] # [[x0,x1],[y0,y1]] (like cc)
        self.failUnless(np.all(cc.BoundingBox() == np.array(boundingbox)))

    def testBoundingBoxFalse(self):
        cc = Chaincode([0, 0], "002")
        boundingbox = [[-1, 0], [-1, 0]]
        self.failIf(np.all(cc.BoundingBox() != np.array(boundingbox)))
                      
    def testSubBoundingBoxX(self):
        cc = Chaincode([0, 0], "44464660012075602223") 
        self.failUnless(cc.subBoundingBox(xedge=[0.1, 2]) == [0, 3])

    def testSubBoundingBoxY(self):
        cc = Chaincode([0, 0], "44464660012075602223") 
        self.failUnless(cc.subBoundingBox(yedge=[-1, 0.5]) == [0, 3])



def main():
    unittest.main()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_roi
from __future__ import absolute_import

import sunpy
from sunpy.roi import roi

def test_roi_instance():
    region=roi(times=['2012-06-20 05:00','2012-06-20 07:00'],description='dummy_roi')
    assert isinstance(region,sunpy.roi.roi)

    

########NEW FILE########
__FILENAME__ = callisto
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

from __future__ import absolute_import

import datetime
import urllib2

import numpy as np

from astropy.io import fits

from collections import defaultdict

from bs4 import BeautifulSoup

from scipy.optimize import leastsq
from scipy.ndimage import gaussian_filter1d

from sunpy.time import parse_time
from sunpy.util import polyfun_at, minimal_pairs
from sunpy.util.cond_dispatch import ConditionalDispatch, run_cls
from sunpy.util.net import download_file

from sunpy.spectra.spectrogram import LinearTimeSpectrogram, REFERENCE


__all__ = ['CallistoSpectrogram']

TIME_STR = "%Y%m%d%H%M%S"
DEFAULT_URL = 'http://soleil.i4ds.ch/solarradio/data/2002-20yy_Callisto/'
_DAY = datetime.timedelta(days=1)

DATA_SIZE = datetime.timedelta(seconds=15*60)

def parse_filename(href):
    name = href.split('.')[0]
    try:
        inst, date, time, no = name.rsplit('_')
        dstart = datetime.datetime.strptime(date + time, TIME_STR)
    except ValueError:
        # If the split fails, the file name does not match out
        # format,so we skip it and continue to the next
        # iteration of the loop.
        return None
    return inst, no, dstart


PARSERS = [
    # Everything starts with ""
    ("", parse_filename)
]
def query(start, end, instruments=None, url=DEFAULT_URL):
    """ Get URLs for callisto data from instruments between start and end.
    
    Parameters
    ----------
    start : parse_time compatible
    end : parse_time compatible
    instruments : sequence
        Sequence of instruments whose data is requested.
    url : str
        Base URL for the request.
    """
    day = datetime.datetime(start.year, start.month, start.day)
    while day <= end:
        directory = url + '%d/%02d/%02d/' % (day.year, day.month, day.day)
        opn = urllib2.urlopen(directory)
        try:
            soup = BeautifulSoup(opn)
            for link in soup.find_all("a"):
                href = link.get("href")
                for prefix, parser in PARSERS:
                    if href.startswith(prefix):
                        break

                result = parser(href)
                if result is None:
                    continue
                inst, no, dstart = result

                if (instruments is not None and
                    inst not in instruments and 
                    (inst, int(no)) not in instruments):
                    continue
                
                dend = dstart + DATA_SIZE
                if dend > start and dstart < end:
                    yield directory + href
        finally:
            opn.close()
        day += _DAY


def download(urls, directory):
    """ Download files from urls into directory.
    
    Parameters
    ----------
    urls : list of str
        urls of the files to retrieve
    directory : str
        directory to save them in
    """
    return [download_file(url, directory) for url in urls]


def _parse_header_time(date, time):
    """ Return datetime object from date and time fields of header. """
    if time is not None:
        date = date + 'T' + time
    return parse_time(date)


class CallistoSpectrogram(LinearTimeSpectrogram):
    """ Classed used for dynamic spectra coming from the Callisto network.
    
    
    Attributes
    ----------
    header : fits.Header
        main header of the FITS file
    axes_header : fits.Header
        header foe the axes table
    swapped : boolean
        flag that specifies whether originally in the file the x-axis was
        frequency
    """
    # XXX: Determine those from the data.
    SIGMA_SUM = 75
    SIGMA_DELTA_SUM = 20
    _create = ConditionalDispatch.from_existing(LinearTimeSpectrogram._create)
    create = classmethod(_create.wrapper())
    # Contrary to what pylint may think, this is not an old-style class.
    # pylint: disable=E1002,W0142,R0902

    # This needs to list all attributes that need to be
    # copied to maintain the object and how to handle them.
    COPY_PROPERTIES = LinearTimeSpectrogram.COPY_PROPERTIES + [
        ('header', REFERENCE),
        ('swapped', REFERENCE),
        ('axes_header', REFERENCE)
    ]
    
    # List of instruments retrieved in July 2012 from
    # http://soleil.i4ds.ch/solarradio/data/2002-20yy_Callisto/
    INSTRUMENTS = set([
        'ALASKA', 'ALMATY', 'BIR', 'DARO', 'HB9SCT', 'HUMAIN',
        'HURBANOVO', 'KASI', 'KENYA', 'KRIM', 'MALAYSIA', 'MRT1',
        'MRT2', 'OOTY', 'OSRA', 'SWMC', 'TRIEST', 'UNAM'
    ])

    def save(self, filepath):
        """ Save modified spectrogram back to filepath.
        
        Parameters
        ----------
        filepath : str
            path to save the spectrogram to
        """
        main_header = self.get_header()
        data = fits.PrimaryHDU(self, header=main_header)
        ## XXX: Update axes header.

        freq_col = fits.Column(
            name="frequency", format="D8.3", array=self.freq_axis
        )
        time_col = fits.Column(
            name="time", format="D8.3", array=self.time_axis
        )
        cols = fits.ColDefs([freq_col, time_col])
        table = fits.new_table(cols, header=self.axes_header)

        hdulist = fits.HDUList([data, table])
        hdulist.writeto(filepath)   

    def get_header(self):
        """ Return updated header. """
        header = self.header.copy()

        if self.swapped:
            header['NAXIS2'] = self.shape[1] # pylint: disable=E1101
            header['NAXIS1'] = self.shape[0] # pylint: disable=E1101
        else:
            header['NAXIS1'] = self.shape[1] # pylint: disable=E1101
            header['NAXIS2'] = self.shape[0] # pylint: disable=E1101
        return header

    @classmethod
    def read(cls, filename, **kwargs):
        """ Read in FITS file and return a new CallistoSpectrogram. 
        Any unknown (i.e. any except filename) keyword arguments get
        passed to fits.open.
        
        Parameters
        ----------
        filename : str
            path of the file to read
        """
        fl = fits.open(filename, **kwargs)
        data = fl[0].data
        axes = fl[1]
        header = fl[0].header

        start = _parse_header_time(
            header['DATE-OBS'], header.get('TIME-OBS', header.get('TIME$_OBS'))
        )
        end = _parse_header_time(
            header['DATE-END'], header.get('TIME-END', header.get('TIME$_END'))
        )

        swapped = "time" not in header["CTYPE1"].lower()
        
        # Swap dimensions so x-axis is always time.
        if swapped:
            t_delt = header["CDELT2"]
            t_init = header["CRVAL2"] - t_delt * header["CRPIX2"]
            t_label = header["CTYPE2"]

            f_delt = header["CDELT1"]
            f_init = header["CRVAL1"] - t_delt * header["CRPIX1"]
            f_label = header["CTYPE1"]
            data = data.transpose()
        else:
            t_delt = header["CDELT1"]
            t_init = header["CRVAL1"] - t_delt * header["CRPIX1"]
            t_label = header["CTYPE1"]

            f_delt = header["CDELT2"]
            f_init = header["CRVAL2"] - t_delt * header["CRPIX2"]
            f_label = header["CTYPE2"]

        # Table may contain the axes data. If it does, the other way of doing
        # it might be very wrong.
        if axes is not None:
            try:
                # It's not my fault. Neither supports __contains__ nor .get
                tm = axes.data['time']
            except KeyError:
                tm = None
            try:
                fq = axes.data['frequency']
            except KeyError:
                fq = None 
        
        if tm is not None:
            # Fix dimensions (whyever they are (1, x) in the first place)
            time_axis = np.squeeze(tm)
        else:
            # Otherwise, assume it's linear.
            time_axis = \
                np.linspace(0, data.shape[1] - 1) * t_delt + t_init # pylint: disable=E1101

        if fq is not None:  
            freq_axis = np.squeeze(fq)
        else:
            freq_axis = \
                np.linspace(0, data.shape[0] - 1) * f_delt + f_init # pylint: disable=E1101

        content = header["CONTENT"]
        instruments = set([header["INSTRUME"]])
    
        return cls(
            data, time_axis, freq_axis, start, end, t_init, t_delt,
            t_label, f_label, content, instruments, 
            header, axes.header, swapped
        )

        
    def __init__(self, data, time_axis, freq_axis, start, end,
            t_init=None, t_delt=None, t_label="Time", f_label="Frequency",
            content="", instruments=None, header=None, axes_header=None,
            swapped=False):
        # Because of how object creation works, there is no avoiding
        # unused arguments in this case.
        # pylint: disable=W0613

        super(CallistoSpectrogram, self).__init__(
            data, time_axis, freq_axis, start, end,
            t_init, t_delt, t_label, f_label,
            content, instruments
        )

        self.header = header
        self.axes_header = axes_header
        self.swapped = swapped

    @classmethod
    def is_datasource_for(cls, header):
        """ Check if class supports data from the given FITS file.
        
        Parameters
        ----------
        header : fits.Header
            main header of the FITS file
        """
        return header.get('instrume', '').strip() in cls.INSTRUMENTS

    def remove_border(self):
        """ Remove duplicate entries on the borders. """
        left = 0
        while self.freq_axis[left] == self.freq_axis[0]:
            left += 1
        right = self.shape[0] - 1
        while self.freq_axis[right] == self.freq_axis[-1]:
            right -= 1
        return self[left-1:right+2, :]

    @classmethod
    def read_many(cls, filenames, sort_by=None):
        """ Return list of CallistoSpectrogram objects read from filenames.
        
        Parameters
        ----------
        filenames : list of str
            list of paths to read from
        sort_by : str
            optional attribute of the resulting objects to sort from, e.g.
            start to sort by starting time.
        """
        objs = map(cls.read, filenames)
        if sort_by is not None:
            objs.sort(key=lambda x: getattr(x, sort_by))
        return objs
    
    @classmethod
    def from_range(cls, instrument, start, end, **kwargs):
        """ Automatically download data from instrument between start and
        end and join it together.
        
        Parameters
        ----------
        instrument : str
            instrument to retrieve the data from
        start : parse_time compatible
            start of the measurement
        end : parse_time compatible
            end of the measurement
        """
        kw = {
            'maxgap': None,
            'fill': cls.JOIN_REPEAT,
        }
        
        kw.update(kwargs)
        start = parse_time(start)
        end = parse_time(end)
        urls = query(start, end, [instrument])
        data = map(cls.from_url, urls)
        freq_buckets = defaultdict(list)
        for elem in data:
            freq_buckets[tuple(elem.freq_axis)].append(elem)
        try:
            return cls.combine_frequencies(
                [cls.join_many(elem, **kw) for elem in freq_buckets.itervalues()]
            )
        except ValueError:
            raise ValueError("No data found.")
    
    def _overlap(self, other):
        """ Find frequency and time overlap of two spectrograms. """
        one, two = self.intersect_time([self, other])
        ovl = one.freq_overlap(two)
        return one.clip_freq(*ovl), two.clip_freq(*ovl)
    
    @staticmethod
    def _to_minimize(a, b):
        """
        Function to be minimized for matching to frequency channels.
        """
        def _fun(p):
            if p[0] <= 0.2 or abs(p[1]) >= a.max():
                return float("inf")
            return a - (p[0] * b + p[1])
        return _fun
    
    def _homogenize_params(self, other, maxdiff=1):
        """
        Return triple with a tuple of indices (in self and other, respectively),
        factors and constants at these frequencies.
        
        Parameters
        ----------
        other : CallistoSpectrogram
            Spectrogram to be homogenized with the current one.
        maxdiff : float
            Threshold for which frequencies are considered equal.
        """
            
        pairs_indices = [
            (x, y) for x, y, d in minimal_pairs(self.freq_axis, other.freq_axis)
            if d <= maxdiff
        ]
        
        pairs_data = [
            (self[n_one, :], other[n_two, :]) for n_one, n_two in pairs_indices
        ]
        
        # XXX: Maybe unnecessary.
        pairs_data_gaussian = [
            (gaussian_filter1d(a, 15), gaussian_filter1d(b, 15))
            for a, b in pairs_data
        ]
        
        # If we used integer arithmetic, we would accept more invalid
        # values.
        pairs_data_gaussian64 = np.float64(pairs_data_gaussian)
        least = [
            leastsq(self._to_minimize(a,b), [1, 0])[0]
            for a, b in pairs_data_gaussian64
        ]
        
        factors = [x for x, y in least]
        constants = [y for x, y in least]
        
        return pairs_indices, factors, constants
    
    def homogenize(self, other, maxdiff=1):
        """ Return overlapping part of self and other as (self, other) tuple.
        Homogenize intensities so that the images can be used with
        combine_frequencies. Note that this works best when most of the 
        picture is signal, so use :py:meth:`in_interval` to select the subset
        of your image before applying this method.
        
        Parameters
        ----------
        other : CallistoSpectrogram
            Spectrogram to be homogenized with the current one.
        maxdiff : float
            Threshold for which frequencies are considered equal.
        """
        one, two = self._overlap(other)
        pairs_indices, factors, constants = one._homogenize_params(
            two, maxdiff
        )
        # XXX: Maybe (xd.freq_axis[x] + yd.freq_axis[y]) / 2.
        pairs_freqs = [one.freq_axis[x] for x, y in pairs_indices]
        
        # XXX: Extrapolation does not work this way.
        # XXX: Improve.
        f1 = np.polyfit(pairs_freqs, factors, 3)
        f2 = np.polyfit(pairs_freqs, constants, 3)
        
        return (
            one,
            two * polyfun_at(f1, two.freq_axis)[:, np.newaxis] +
                polyfun_at(f2, two.freq_axis)[:, np.newaxis]
        )
    
    def extend(self, minutes=15, **kwargs):
        """ Request subsequent files from the server. If minutes is negative,
        retrieve preceding files. """
        if len(self.instruments) != 1:
            raise ValueError
        
        instrument = iter(self.instruments).next()
        if minutes > 0:
            data = CallistoSpectrogram.from_range(
                instrument,
                self.end, self.end + datetime.timedelta(minutes=minutes)
            )
        else:
            data = CallistoSpectrogram.from_range(
                instrument,
                self.start - datetime.timedelta(minutes=-minutes), self.start
            )
        
        data = data.clip_freq(self.freq_axis[-1], self.freq_axis[0])
        return CallistoSpectrogram.join_many([self, data], **kwargs)
    
    @classmethod
    def from_url(cls, url):
        """ Return CallistoSpectrogram read from URL.
        
        Parameters
        ----------
        url : str
            URL to retrieve the data from
        """
        return cls.read(url)


CallistoSpectrogram._create.add(
    run_cls('from_range'),
    lambda cls, instrument, start, end: True,
    check=False
)

CallistoSpectrogram.create.im_func.__doc__ = (
    """ Create CallistoSpectrogram from given input dispatching to the
    appropriate from_* function.

Possible signatures:

""" + CallistoSpectrogram._create.generate_docs()
)

if __name__ == "__main__":
    opn = CallistoSpectrogram.read("callisto/BIR_20110922_103000_01.fit")
    opn.subtract_bg().clip(0).plot(ratio=2).show()
    print "Press return to exit"
    raw_input()

########NEW FILE########
__FILENAME__ = swaves
# -*- coding: utf-8 -*-
# Author: David Perez-Suarez <dps.helio-?-gmail.com>

from __future__ import absolute_import

import os
import datetime

import numpy as np

from sunpy.util.cond_dispatch import ConditionalDispatch
from sunpy.spectra.spectrogram import LinearTimeSpectrogram, REFERENCE, get_day

__all__ = ['SWavesSpectrogram']

#def query(start, end, spacecraft=None, band=None):
#    start = datetime.datetime(start.year, start.month, start.day)
#    end = datetime.datetime(end.year, end.month, end.day)
#    if spacecraft is None:
#        spacecraft = ['STEREO_A','STEREO_B']
#    elif spacecraft.upper() != 'STEREO_B':
#        spacecraft = ['STEREO_A']
#    else:
#        spacecraft = ['STEREO_B']
#    if band is None:
#        band = 'All'
#        min_wave = 0.00001
#        max_wave = 0.016
#    elif band.lower() != 'low':
#        band = 'high' # range 0.125 - 16 MHz
#        min_wave = 0.00017 
#        max_wave = 0.016
#    elif band.lower() == 'low': # range 10 - 160 KHz
#        min_wave = 0.00001
#        max_wave = 0.00012
#    print "Getting data from %s satellite, band %s" % (spacecraft,band)
#    client = vso.InteractiveVSOClient()
#    result = client.query_legacy(tstart=start,\
#                                     tend=end,instrument='SWAVES',source=spacecraft.upper(),\
#                                     min_wave=min_wave,max_wave=max_wave,unit_wave='GHz')
#    if len(result) > 0:
#        listfiles = client.get(result).wait()


class SWavesSpectrogram(LinearTimeSpectrogram):
    _create = ConditionalDispatch.from_existing(LinearTimeSpectrogram._create)
    create = classmethod(_create.wrapper())
    COPY_PROPERTIES = LinearTimeSpectrogram.COPY_PROPERTIES + [
        ('bg', REFERENCE)
    ]

    @staticmethod
    def swavesfile_to_date(filename):
        _, name = os.path.split(filename)
        date = name.split('_')[2]
        return datetime.datetime(
            int(date[0:4]), int(date[4:6]), int(date[6:])
        )

    @classmethod
    def read(cls, filename, **kwargs):
        """ Read in FITS file and return a new SWavesSpectrogram. """
        data = np.genfromtxt(filename, skip_header=2)
        time_axis = data[:, 0] * 60.
        data = data[:, 1:].transpose()
        header = np.genfromtxt(filename, skip_footer=time_axis.size)
        freq_axis = header[0, :]
        bg = header[1, :]
        start = cls.swavesfile_to_date(filename)
        end = start + datetime.timedelta(seconds=time_axis[-1])
        t_delt = 60.
        t_init = (start - get_day(start)).seconds
        content = ''
        t_label = 'Time [UT]'
        f_label = 'Frequency [KHz]'
        
        freq_axis = freq_axis[::-1]
        data = data[::-1, :]
        
        return cls(data, time_axis, freq_axis, start, end, t_init, t_delt,
            t_label, f_label, content, bg)

        
    def __init__(self, data, time_axis, freq_axis, start, end,
            t_init, t_delt, t_label, f_label, content, bg):
        # Because of how object creation works, there is no avoiding
        # unused arguments in this case.
        # pylint: disable=W0613
        
        super(SWavesSpectrogram, self).__init__(
            data, time_axis, freq_axis, start, end,
            t_init, t_delt, t_label, f_label,
            content, set(["SWAVES"])
        )
        self.bg = bg


SWavesSpectrogram.create.im_func.__doc__ = (
    """ Create SWavesSpectrogram from given input dispatching to the
    appropriate from_* function.

Possible signatures:

""" + SWavesSpectrogram._create.generate_docs()
)

if __name__ == "__main__":
    opn = SWavesSpectrogram.read("/home/florian/swaves_average_20120705_a_hfr.dat")
    opn.plot(min_=0, linear=False).show()
    print "Press return to exit"
    raw_input()

########NEW FILE########
__FILENAME__ = spectrogram
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

""" Classes for spectral analysis. """

from __future__ import division
from __future__ import print_function
from __future__ import absolute_import
from __future__ import unicode_literals

import datetime

from random import randint
from itertools import izip
from copy import copy
from math import floor

import numpy as np
from numpy import ma

from scipy import ndimage

from matplotlib import pyplot as plt
from matplotlib.figure import Figure
from matplotlib.ticker import FuncFormatter, MaxNLocator, IndexLocator
from matplotlib.colorbar import Colorbar

#import sunpy

from sunpy.time import parse_time, get_day
from sunpy.util import to_signed, common_base, merge
from sunpy.util.cond_dispatch import ConditionalDispatch
from sunpy.util.create import Parent
from sunpy.spectra.spectrum import Spectrum

__all__ = ['Spectrogram', 'LinearTimeSpectrogram']

# 1080 because that usually is the maximum vertical pixel count on modern
# screens nowadays (2012).
DEFAULT_YRES = 1080

# This should not be necessary, as observations do not take more than a day
# but it is used for completeness' and extendibility's sake.
# XXX: Leap second?
SECONDS_PER_DAY = 86400

# Used for COPY_PROPERTIES
REFERENCE = 0
COPY = 1
DEEPCOPY = 2

def figure(*args, **kwargs):
    """ Create new SpectroFigure, a figure extended with features
    useful for analysis of spectrograms. Compare pyplot.figure. """
    kw = {
        'FigureClass': SpectroFigure,
    }
    kw.update(kwargs)
    return plt.figure(*args, **kw)


def _min_delt(arr):
    deltas = (arr[:-1] - arr[1:])
    # Multiple values at the same frequency are just thrown away
    # in the process of linearizaion
    return deltas[deltas != 0].min()

def _list_formatter(lst, fun=None):
    """ Return function that takes x, pos and returns fun(lst[x]) if
    fun is not None, else lst[x] or "" if x is out of range. """
    def _fun(x, pos):
        x = int(x)
        if x >= len(lst) or x < 0:
            return ""

        elem = lst[x]
        if fun is None:
            return elem
        return fun(elem)
    return _fun


def _union(sets):
    """ Return union of sets. """
    union = set()
    for s in sets:
        union |= s
    return union


class _LinearView(object):
    """ Helper class for frequency channel linearization.

    Attributes
    ----------
    arr : Spectrogram
        Spectrogram to linearize.
    delt : float
        Delta between frequency channels in linearized spectrogram. Defaults to
        (minimum delta / 2.) because of the Shannon sampling theorem.
    """
    def __init__(self, arr, delt=None):
        self.arr = arr
        if delt is None:
            # NyquistShannon sampling theorem
            delt = _min_delt(arr.freq_axis) / 2.

        self.delt = delt

        midpoints = (self.arr.freq_axis[:-1] + self.arr.freq_axis[1:]) / 2
        self.midpoints = np.concatenate([midpoints, arr.freq_axis[-1:]])

        self.max_mp_delt = np.min(self.midpoints[1:] - self.midpoints[:-1])

        self.freq_axis = np.arange(
            self.arr.freq_axis[0], self.arr.freq_axis[-1], -self.delt
        )
        self.time_axis = self.arr.time_axis

        self.shape = (len(self), arr.data.shape[1])

    def __len__(self):
        return 1 + (self.arr.freq_axis[0] - self.arr.freq_axis[-1]) / self.delt

    def _find(self, arr, item):
        if item < 0:
            item = item % len(self)
        if item >= len(self):
            raise IndexError

        freq_offset = item * self.delt
        freq = self.arr.freq_axis[0] - freq_offset
        # The idea is that when we take the biggest delta in the mid points,
        # we do not have to search anything that is between the beginning and
        # the first item that can possibly be that frequency.
        min_mid = max(0, (freq - self.midpoints[0]) // self.max_mp_delt)
        for n, mid in enumerate(self.midpoints[min_mid:]):
            if mid <= freq:
                return arr[min_mid + n]
        return arr[min_mid + n]

    def __getitem__(self, item):
        return self._find(self.arr, item)

    def get_freq(self, item):
        return self._find(self.arr.freq_axis, item)

    def make_mask(self, max_dist):
        mask = np.zeros(self.shape, dtype=np.bool)
        for n, item in enumerate(xrange(len(self))):
            freq = self.arr.freq_axis[0] - item * self.delt
            if abs(self.get_freq(item) - freq) > max_dist:
                mask[n, :] = True
        return mask


class SpectroFigure(Figure):
    def _init(self, data, freqs):
        self.data = data
        self.freqs = freqs

    def ginput_to_time(self, inp):
        return [
            self.data.start + datetime.timedelta(seconds=secs)
            for secs in self.ginput_to_time_secs(inp)
        ]

    def ginput_to_time_secs(self, inp):
        return np.array([float(self.data.time_axis[x]) for x, y in inp])

    def ginput_to_time_offset(self, inp):
        v = self.ginput_to_time_secs(inp)
        return v - v.min()

    def ginput_to_freq(self, inp):
        return np.array([self.freqs[y] for x, y in inp])

    def time_freq(self, points=0):
        inp = self.ginput(points)
        min_ = self.ginput_to_time_secs(inp).min()
        start = self.data.start + datetime.timedelta(seconds=min_)
        return TimeFreq(
            start, self.ginput_to_time_offset(inp), self.ginput_to_freq(inp)
        )


class TimeFreq(object):
    """ Class to use for plotting frequency vs time.

    Attributes
    ----------
    start : datetime
        Start time of the plot.
    time : array
        Time of the data points as offset from start in seconds.
    freq : array
        Frequency of the data points in MHz.
    """
    def __init__(self, start, time, freq):
        self.start = start
        self.time = time
        self.freq = freq

    def plot(self, time_fmt="%H:%M:%S", **kwargs):
        figure = plt.gcf()
        axes = figure.add_subplot(111)
        axes.plot(self.time, self.freq, **kwargs)
        xa = axes.get_xaxis()
        ya = axes.get_yaxis()
        xa.set_major_formatter(
            FuncFormatter(
                lambda x, pos: (
                    self.start + datetime.timedelta(seconds=x)
                    ).strftime(time_fmt)
            )
        )

        axes.set_xlabel("Time [UT]")
        axes.set_ylabel("Frequency [MHz]")

        xa = axes.get_xaxis()
        for tl in xa.get_ticklabels():
            tl.set_fontsize(10)
            tl.set_rotation(30)
        figure.add_axes(axes)
        figure.subplots_adjust(bottom=0.2)
        figure.subplots_adjust(left=0.2)

        return figure

    def peek(self, *args, **kwargs):
        plt.figure()
        ret = self.plot(*args, **kwargs)
        plt.show()
        return ret


class Spectrogram(Parent):
    """
    Base class for spectral analysis in SunPy.

    .. warning:: This module is under development! Use at your own risk.

    Attributes
    ----------
    data : np.ndarray
        two-dimensional array of the image data of the spectrogram.
    time_axis : np.ndarray
        one-dimensional array containing the offset from the start
        for each column of data.
    freq_axis : np.ndarray
        one-dimensional array containing information about the
        frequencies each row of the image corresponds to.
    start : datetime
        starting time of the measurement
    end : datetime
        end time of the measurement
    t_init : int
        offset from the start of the day the measurement began. If None
        gets automatically set from start.
    t_label : str
        label for the time axis
    f_label : str
        label for the frequency axis
    content : str
        header for the image
    instruments : set of str
        instruments that recorded the data, may be more than one if
        it was constructed using combine_frequencies or join_many.
    """
    # Contrary to what pylint may think, this is not an old-style class.
    # pylint: disable=E1002,W0142,R0902

    # This needs to list all attributes that need to be
    # copied to maintain the object and how to handle them.
    COPY_PROPERTIES = [
        ('time_axis', COPY),
        ('freq_axis', COPY),
        ('instruments', COPY),
        ('start', REFERENCE),
        ('end', REFERENCE),
        ('t_label', REFERENCE),
        ('f_label', REFERENCE),
        ('content', REFERENCE),
        ('t_init', REFERENCE),
    ]
    _create = ConditionalDispatch.from_existing(Parent._create)

    @property
    def shape(self):
        return self.data.shape

    @property
    def dtype(self):
        return self.data.dtype

    def _get_params(self):
        """ Implementation detail. """
        return dict(
            (name, getattr(self, name)) for name, _ in self.COPY_PROPERTIES
        )

    def _slice(self, y_range, x_range):
        """ Return new spectrogram reduced to the values passed
        as slices. Implementation detail. """
        data = self.data[y_range, x_range]
        params = self._get_params()

        soffset = 0 if x_range.start is None else x_range.start
        eoffset = self.shape[1] if x_range.stop is None else x_range.stop # pylint: disable=E1101
        eoffset -= 1

        fsoffset = 0 if y_range.start is None else y_range.start
        feoffset = self.shape[0] if y_range.stop is None else y_range.stop # pylint: disable=E1101

        params.update({
            'time_axis': self.time_axis[
                x_range.start:x_range.stop:x_range.step
            ] - self.time_axis[soffset],
            'freq_axis': self.freq_axis[
                y_range.start:y_range.stop:y_range.step],
            'start': self.start + datetime.timedelta(
                seconds=self.time_axis[soffset]),
            'end': self.start + datetime.timedelta(
                seconds=self.time_axis[eoffset]),
            't_init': self.t_init + self.time_axis[soffset],
        })
        return self.__class__(data, **params)

    def _with_data(self, data):
        new = copy(self)
        new.data = data
        return new

    def __init__(self, data, time_axis, freq_axis, start, end, t_init=None,
        t_label="Time", f_label="Frequency", content="", instruments=None):
        # Because of how object creation works, there is no avoiding
        # unused arguments in this case.
        self.data = data

        if t_init is None:
            diff = start - get_day(start)
            t_init = diff.seconds
        if instruments is None:
            instruments = set()

        self.start = start
        self.end = end

        self.t_label = t_label
        self.f_label = f_label

        self.t_init = t_init

        self.time_axis = time_axis
        self.freq_axis = freq_axis

        self.content = content
        self.instruments = instruments

    def time_formatter(self, x, pos):
        """ This returns the label for the tick of value x at
        a specified pos on the time axis. """
        # Callback, cannot avoid unused arguments.
        # pylint: disable=W0613
        x = int(x)
        if x >= len(self.time_axis) or x < 0:
            return ""
        return self.format_time(
            self.start + datetime.timedelta(
                seconds=float(self.time_axis[x])
            )
        )

    @staticmethod
    def format_time(time):
        """ Override to configure default plotting """
        return time.strftime("%H:%M:%S")

    @staticmethod
    def format_freq(freq):
        """ Override to configure default plotting """
        return "%.1f" % freq

    def peek(self, *args, **kwargs):
        figure()
        ret = self.plot(*args, **kwargs)
        plt.show()
        return ret

    def plot(self, figure=None, overlays=[], colorbar=True, vmin=None, vmax=None,
             linear=True, showz=True, yres=DEFAULT_YRES,
             max_dist=None, **matplotlib_args):
        """
        Plot spectrogram onto figure.

        Parameters
        ----------
        figure : matplotlib.figure.Figure
            Figure to plot the spectrogram on. If None, new Figure is created.
        overlays : list
            List of overlays (functions that receive figure and axes and return
            new ones) to be applied after drawing.
        colorbar : bool
            Flag that determines whether or not to draw a colorbar. If existing
            figure is passed, it is attempted to overdraw old colorbar.
        vmin : float
            Clip intensities lower than vmin before drawing.
        vmax : float
            Clip intensities higher than vmax before drawing.
        linear :  bool
            If set to True, "stretch" image to make frequency axis linear.
        showz : bool
            If set to True, the value of the pixel that is hovered with the
            mouse is shown in the bottom right corner.
        yres : int or None
            To be used in combination with linear=True. If None, sample the
            image with half the minimum frequency delta. Else, sample the
            image to be at most yres pixels in vertical dimension. Defaults
            to 1080 because that's a common screen size.
        max_dist : float or None
            If not None, mask elements that are further than max_dist away
            from actual data points (ie, frequencies that actually have data
            from the receiver and are not just nearest-neighbour interpolated).
        """
        # [] as default argument is okay here because it is only read.
        # pylint: disable=W0102,R0914
        if linear:
            delt = yres
            if delt is not None:
                delt = max(
                    (self.freq_axis[0] - self.freq_axis[-1]) / (yres - 1),
                    _min_delt(self.freq_axis) / 2.
                )
                delt = float(delt)

            data = _LinearView(self.clip_values(vmin, vmax), delt)
            freqs = np.arange(
                self.freq_axis[0], self.freq_axis[-1], -data.delt
            )
        else:
            data = np.array(self.clip_values(vmin, vmax))
            freqs = self.freq_axis

        figure = plt.gcf()

        if figure.axes:
            axes = figure.axes[0]
        else:
            axes = figure.add_subplot(111)

        params = {
            'origin': 'lower',
            'aspect': 'auto',
        }
        params.update(matplotlib_args)
        if linear and max_dist is not None:
            toplot = ma.masked_array(data, mask=data.make_mask(max_dist))
            pass
        else:
            toplot = data
        im = axes.imshow(toplot, **params)

        xa = axes.get_xaxis()
        ya = axes.get_yaxis()

        xa.set_major_formatter(
            FuncFormatter(self.time_formatter)
        )

        if linear:
            # Start with a number that is divisible by 5.
            init = (self.freq_axis[0] % 5) / data.delt
            nticks = 15.
            # Calculate MHz difference between major ticks.
            dist = (self.freq_axis[0] - self.freq_axis[-1]) / nticks
            # Round to next multiple of 10, at least ten.
            dist = max(round(dist, -1), 10)
            # One pixel in image space is data.delt MHz, thus we can convert
            # our distance between the major ticks into image space by dividing
            # it by data.delt.

            ya.set_major_locator(
                IndexLocator(
                    dist / data.delt, init
                )
            )
            ya.set_minor_locator(
                IndexLocator(
                    dist / data.delt / 10, init
                )
            )
            def freq_fmt(x, pos):
                # This is necessary because matplotlib somehow tries to get
                # the mid-point of the row, which we do not need here.
                x = x + 0.5
                return self.format_freq(self.freq_axis[0] - x * data.delt)
        else:
            freq_fmt = _list_formatter(freqs, self.format_freq)
            ya.set_major_locator(MaxNLocator(integer=True, steps=[1, 5, 10]))

        ya.set_major_formatter(
            FuncFormatter(freq_fmt)
        )

        axes.set_xlabel(self.t_label)
        axes.set_ylabel(self.f_label)
        # figure.suptitle(self.content)

        figure.suptitle(
            ' '.join([
                get_day(self.start).strftime("%d %b %Y"),
                'Radio flux density',
                '(' + ', '.join(self.instruments) + ')',
            ])
        )

        for tl in xa.get_ticklabels():
            tl.set_fontsize(10)
            tl.set_rotation(30)
        figure.add_axes(axes)
        figure.subplots_adjust(bottom=0.2)
        figure.subplots_adjust(left=0.2)

        if showz:
            axes.format_coord = self._mk_format_coord(
                data, figure.gca().format_coord)

        if colorbar:
            if len(figure.axes) > 1:
                Colorbar(figure.axes[1], im).set_label("Intensity")
            else:
                figure.colorbar(im).set_label("Intensity")

        for overlay in overlays:
            figure, axes = overlay(figure, axes)

        for ax in figure.axes:
            ax.autoscale()
        if isinstance(figure, SpectroFigure):
            figure._init(self, freqs)
        return axes

    def __getitem__(self, key):
        only_y = not isinstance(key, tuple)

        if only_y:
            return self.data[key]
        elif isinstance(key[0], slice) and isinstance(key[1], slice):
            return self._slice(key[0], key[1])
        elif isinstance(key[1], slice):
            # return Spectrum( # XXX: Right class
            #     super(Spectrogram, self).__getitem__(key),
            #     self.time_axis[key[1].start:key[1].stop:key[1].step]
            # )
            return np.array(self.data[key])
        elif isinstance(key[0], slice):
            return Spectrum(
                self.data[key],
                self.freq_axis[key[0].start:key[0].stop:key[0].step]
            )

        return self.data[key]

    def clip_freq(self, vmin=None, vmax=None):
        """ Return a new spectrogram only consisting of frequencies
        in the interval [min\_, max\_].

        Parameters
        ----------
        min\_ : float
            All frequencies in the result are greater or equal to this.
        max\_ : float
            All frequencies in the result are smaller or equal to this.
        """
        left = 0
        if vmax is not None:
            while self.freq_axis[left] > vmax:
                left += 1

        right = len(self.freq_axis) - 1

        if vmin is not None:
            while self.freq_axis[right] < vmin:
                right -= 1

        return self[left:right + 1, :]


    def auto_find_background(self, amount=0.05):
        # pylint: disable=E1101,E1103
        data = self.data.astype(to_signed(self.dtype))
        # Subtract average value from every frequency channel.
        tmp = (data - np.average(self.data, 1).reshape(self.shape[0], 1))
        # Get standard deviation at every point of time.
        # Need to convert because otherwise this class's __getitem__
        # is used which assumes two-dimensionality.
        sdevs = np.asarray(np.std(tmp, 0))

        # Get indices of values with lowest standard deviation.
        cand = sorted(xrange(self.shape[1]), key=lambda y: sdevs[y])
        # Only consider the best 5 %.
        return cand[:max(1, int(amount * len(cand)))]

    def auto_const_bg(self):
        """ Automatically determine background. """
        realcand = self.auto_find_background()
        bg = np.average(self.data[:, realcand], 1)
        return bg.reshape(self.shape[0], 1)

    def subtract_bg(self):
        """ Perform constant background subtraction. """
        return self._with_data(self.data - self.auto_const_bg())

    def randomized_auto_const_bg(self, amount):
        """ Automatically determine background. Only consider a randomly
        chosen subset of the image.

        Parameters
        ----------
        amount : int
            Size of random sample that is considered for calculation of
            the background.
        """
        cols = [randint(0, self.shape[1] - 1) for _ in xrange(amount)]

        # pylint: disable=E1101,E1103
        data = self.data.astype(to_signed(self.dtype))
        # Subtract average value from every frequency channel.
        tmp = (data - np.average(self.data, 1).reshape(self.shape[0], 1))
        # Get standard deviation at every point of time.
        # Need to convert because otherwise this class's __getitem__
        # is used which assumes two-dimensionality.
        tmp = tmp[:, cols]
        sdevs = np.asarray(np.std(tmp, 0))

        # Get indices of values with lowest standard deviation.
        cand = sorted(xrange(amount), key=lambda y: sdevs[y])
        # Only consider the best 5 %.
        realcand = cand[:max(1, int(0.05 * len(cand)))]

        # Average the best 5 %
        bg = np.average(self[:, [cols[r] for r in realcand]], 1)

        return bg.reshape(self.shape[0], 1)

    def randomized_subtract_bg(self, amount):
        """ Perform randomized constant background subtraction.
        Does not produce the same result every time it is run.

        Parameters
        ----------
        amount : int
            Size of random sample that is considered for calculation of
            the background.
        """
        return self._with_data(self.data - self.randomized_auto_const_bg(amount))

    def clip_values(self, vmin=None, vmax=None, out=None):
        """
        Clip intensities to be in the interval [min\_, max\_].

        Any values greater than the maximum will be assigned the maximum,
        any values lower than the minimum will be assigned the minimum.
        If either is left out or None, do not clip at that side of the interval.

        Parameters
        ----------
        min\_ : int or float
            New minimum value for intensities.
        max\_ : int or float
            New maximum value for intensities
        """
        # pylint: disable=E1101
        if vmin is None:
            vmin = int(self.data.min())

        if vmax is None:
            vmax = int(self.data.max())

        return self._with_data(self.data.clip(vmin, vmax, out))

    def rescale(self, vmin=0, vmax=1, dtype=np.dtype('float32')):
        u"""
        Rescale intensities to [min\_, max\_].
        Note that min\_  max\_ and spectrogram.min()  spectrogram.max().

        Parameters
        ----------
        min\_ : float or int
            New minimum value in the resulting spectogram.
        max\_ : float or int
            New maximum value in the resulting spectogram.
        dtype : np.dtype
            Data-type of the resulting spectogram.
        """
        if vmax == vmin:
            raise ValueError("Maximum and minimum must be different.")
        if self.data.max() == self.data.min():
            raise ValueError("Spectrogram needs to contain distinct values.")
        data = self.data.astype(dtype) # pylint: disable=E1101
        return self._with_data(
            vmin + (vmax - vmin) * (data - self.data.min()) / # pylint: disable=E1101
            (self.data.max() - self.data.min()) # pylint: disable=E1101
        )

    def interpolate(self, frequency):
        """
        Linearly interpolate intensity at unknown frequency using linear
        interpolation of its two neighbours.

        Parameters
        ----------
        frequency : float or int
            Unknown frequency for which to lineary interpolate the intensities.
            freq_axis[0] >= frequency >= self_freq_axis[-1]
        """
        lfreq, lvalue = None, None
        for freq, value in izip(self.freq_axis, self.data[:, :]):
            if freq < frequency:
                break
            lfreq, lvalue = freq, value
        else:
            raise ValueError("Frequency not in interpolation range")
        if lfreq is None:
            raise ValueError("Frequency not in interpolation range")
        diff = frequency - freq # pylint: disable=W0631
        ldiff = lfreq - frequency
        return (ldiff * value + diff * lvalue) / (diff + ldiff) # pylint: disable=W0631

    def linearize_freqs(self, delta_freq=None):
        """ Rebin frequencies so that the frequency axis is linear.

        Parameters
        ----------
        delta_freq : float
            Difference between consecutive values on the new frequency axis.
            Defaults to half of smallest delta in current frequency axis.
            Compare Nyquist-Shannon sampling theorem.
        """
        if delta_freq is None:
            # NyquistShannon sampling theorem
            delta_freq = _min_delt(self.freq_axis) / 2.
        nsize = (self.freq_axis.max() - self.freq_axis.min()) / delta_freq + 1
        new = np.zeros((nsize, self.shape[1]), dtype=self.data.dtype)

        freqs = self.freq_axis - self.freq_axis.max()
        freqs = freqs / delta_freq

        midpoints = np.round((freqs[:-1] + freqs[1:]) / 2)
        fillto = np.concatenate(
            [midpoints - 1, np.round([freqs[-1]]) - 1]
        )
        fillfrom = np.concatenate(
            [np.round([freqs[0]]), midpoints - 1]
        )

        fillto = np.abs(fillto)
        fillfrom = np.abs(fillfrom)

        for row, from_, to_ in izip(self, fillfrom, fillto):
            new[from_: to_] = row

        vrs = self._get_params()
        vrs.update({
            'freq_axis': np.linspace(
                self.freq_axis.max(), self.freq_axis.min(), nsize
            )
        })

        return self.__class__(new, **vrs)

    def freq_overlap(self, other):
        """ Get frequency range present in both spectrograms. Returns
        (min, max) tuple.

        Parameters
        ----------
        other : Spectrogram
            other spectrogram with which to look for frequency overlap
        """
        lower = max(self.freq_axis[-1], other.freq_axis[-1])
        upper = min(self.freq_axis[0], other.freq_axis[0])
        if lower > upper:
            raise ValueError("No overlap.")
        return lower, upper

    def time_to_x(self, time):
        """ Return x-coordinate in spectrogram that corresponds to the
        passed datetime value.

        Parameters
        ----------
        time : parse_time compatible
            Datetime to find the x coordinate for.
        """
        diff = time - self.start
        diff_s = SECONDS_PER_DAY * diff.days + diff.seconds
        if self.time_axis[-1] < diff_s < 0:
            raise ValueError("Out of bounds")
        for n, elem in enumerate(self.time_axis):
            if diff_s < elem:
                return n - 1
        # The last element is the searched one.
        return n

    def at_freq(self, freq):
        return self[np.nonzero(self.freq_axis == freq)[0], :]

    @staticmethod
    def _mk_format_coord(spec, fmt_coord):
        def format_coord(x, y):
            shape = map(int, spec.shape)

            xint, yint = int(x), int(y)
            if 0 <= xint < shape[1] and 0 <= yint < shape[0]:
                pixel = spec[yint][xint]
            else:
                pixel = ""

            return '%s z=%s' % (
                fmt_coord(x, y),
                pixel
            )
        return format_coord


class LinearTimeSpectrogram(Spectrogram):
    """ Spectrogram evenly sampled in time.

    Attributes
    ----------
    t_delt : float
        difference between the items on the time axis
    """
    # pylint: disable=E1002
    COPY_PROPERTIES = Spectrogram.COPY_PROPERTIES + [
        ('t_delt', REFERENCE),
    ]

    def __init__(self, data, time_axis, freq_axis, start, end,
        t_init=None, t_delt=None, t_label="Time", f_label="Frequency",
        content="", instruments=None):
        if t_delt is None:
            t_delt = _min_delt(freq_axis)

        super(LinearTimeSpectrogram, self).__init__(
            data, time_axis, freq_axis, start, end, t_init, t_label, f_label,
            content, instruments
        )
        self.t_delt = t_delt

    @staticmethod
    def make_array(shape, dtype=np.dtype('float32')):
        """ Function to create an array with shape and dtype.

        Parameters
        ----------
        shape : tuple
            shape of the array to create
        dtype : np.dtype
            data-type of the array to create
        """
        return np.zeros(shape, dtype=dtype)

    @staticmethod
    def memmap(filename):
        """ Return function that takes shape and dtype and returns a
        memory mapped array.

        Parameters
        ----------
        filename : str
            File to store the memory mapped array in.
        """
        return (
            lambda shape, dtype=np.dtype('float32'): np.memmap(
                filename, mode="write", shape=shape, dtype=dtype
            )
        )

    def resample_time(self, new_delt):
        """ Rescale image so that the difference in time between pixels is
        new_delt seconds.

        Parameters
        ----------
        new_delt : float
            New delta between consecutive values.
        """
        if self.t_delt == new_delt:
            return self
        factor = self.t_delt / float(new_delt)

        # The last data-point does not change!
        new_size = floor((self.shape[1] - 1) * factor + 1) # pylint: disable=E1101
        data = ndimage.zoom(self.data, (1, new_size / self.shape[1])) # pylint: disable=E1101

        params = self._get_params()
        params.update({
            'time_axis': np.linspace(
                self.time_axis[0],
                self.time_axis[(new_size - 1) * new_delt / self.t_delt],
                new_size
            ),
            't_delt': new_delt,
        })
        return self.__class__(data, **params)

    JOIN_REPEAT = object()

    @classmethod
    def join_many(cls, specs, mk_arr=None, nonlinear=False,
        maxgap=0, fill=JOIN_REPEAT):
        """ Produce new Spectrogram that contains spectrograms
        joined together in time.

        Parameters
        ----------
        specs : list
            List of spectrograms to join together in time.
        nonlinear : bool
            If True, leave out gaps between spectrograms. Else, fill them with
            the value specified in fill.
        maxgap : float, int or None
            Largest gap to allow in second. If None, allow gap of arbitrary
            size.
        fill : float or int
            Value to fill missing values (assuming nonlinear=False) with.
            Can be LinearTimeSpectrogram.JOIN_REPEAT to repeat the values for
            the time just before the gap.
        mk_array: function
            Function that is called to create the resulting array. Can be set
            to LinearTimeSpectrogram.memap(filename) to create a memory mapped
            result array.
        """
        # XXX: Only load header and load contents of files
        # on demand.
        mask = None

        if mk_arr is None:
            mk_arr = cls.make_array

        specs = sorted(specs, key=lambda x: x.start)

        freqs = specs[0].freq_axis
        if not all(np.array_equal(freqs, sp.freq_axis) for sp in specs):
            raise ValueError("Frequency channels do not match.")

        # Smallest time-delta becomes the common time-delta.
        min_delt = min(sp.t_delt for sp in specs)
        dtype_ = max(sp.dtype for sp in specs)

        specs = [sp.resample_time(min_delt) for sp in specs]
        size = sum(sp.shape[1] for sp in specs)

        data = specs[0]
        init = data.t_init
        start_day = data.start

        xs = []
        last = data
        for elem in specs[1:]:
            e_init = (
                SECONDS_PER_DAY * (
                    get_day(elem.start) - get_day(start_day)
                ).days + elem.t_init
            )
            x = int((e_init - last.t_init) / min_delt)
            xs.append(x)
            diff = last.shape[1] - x

            if maxgap is not None and -diff > maxgap / min_delt:
                raise ValueError("Too large gap.")

            # If we leave out undefined values, we do not want to
            # add values here if x > t_res.
            if nonlinear:
                size -= max(0, diff)
            else:
                size -= diff

            last = elem

        # The non existing element after the last one starts after
        # the last one. Needed to keep implementation below sane.
        xs.append(specs[-1].shape[1])

        # We do that here so the user can pass a memory mapped
        # array if they'd like to.
        arr = mk_arr((data.shape[0], size), dtype_)
        time_axis = np.zeros((size,))
        sx = 0
        # Amount of pixels left out due to nonlinearity. Needs to be
        # considered for correct time axes.
        sd = 0
        for x, elem in izip(xs, specs):
            diff = x - elem.shape[1]
            e_time_axis = elem.time_axis

            elem = elem.data

            if x > elem.shape[1]:
                if nonlinear:
                    x = elem.shape[1]
                else:
                    # If we want to stay linear, fill up the missing
                    # pixels with placeholder zeros.
                    filler = np.zeros((data.shape[0], diff))
                    if fill is cls.JOIN_REPEAT:
                        filler[:, :] = elem[:, -1, np.newaxis]
                    else:
                        filler[:] = fill
                    minimum = e_time_axis[-1]
                    e_time_axis = np.concatenate([
                        e_time_axis,
                        np.linspace(
                            minimum + min_delt,
                            minimum + diff * min_delt,
                            diff
                        )
                    ])
                    elem = np.concatenate([elem, filler], 1)
            arr[:, sx:sx + x] = elem[:, :x]

            if diff > 0:
                if mask is None:
                    mask = np.zeros((data.shape[0], size), dtype=np.uint8)
                mask[:, sx + x - diff:sx + x] = 1
            time_axis[sx:sx + x] = e_time_axis[:x] + data.t_delt * (sx + sd)
            if nonlinear:
                sd += max(0, diff)
            sx += x
        params = {
            'time_axis': time_axis,
            'freq_axis': data.freq_axis,
            'start': data.start,
            'end': specs[-1].end,
            't_delt': data.t_delt,
            't_init': data.t_init,
            't_label': data.t_label,
            'f_label': data.f_label,
            'content': data.content,
            'instruments': _union(spec.instruments for spec in specs),
        }
        if mask is not None:
            arr = ma.array(arr, mask=mask)
        if nonlinear:
            del params['t_delt']
            return Spectrogram(arr, **params)
        return common_base(specs)(arr, **params)

    def time_to_x(self, time):
        """ Return x-coordinate in spectrogram that corresponds to the
        passed datetime value.

        Parameters
        ----------
        time : parse_time compatible
            Datetime to find the x coordinate for.
        """
        # This is impossible for frequencies because that mapping
        # is not injective.
        time = parse_time(time)
        diff = time - self.start
        diff_s = SECONDS_PER_DAY * diff.days + diff.seconds
        result = diff_s // self.t_delt
        if 0 <= result <= self.shape[1]: # pylint: disable=E1101
            return result
        raise ValueError("Out of range.")

    @staticmethod
    def intersect_time(specs):
        """ Return slice of spectrograms that is present in all of the ones
        passed.

        Parameters
        ----------
        specs : list
            List of spectrograms of which to find the time intersections.
        """
        delt = min(sp.t_delt for sp in specs)
        start = max(sp.t_init for sp in specs)

        # XXX: Could do without resampling by using
        # sp.t_init below, not sure if good idea.
        specs = [sp.resample_time(delt) for sp in specs]
        cut = [sp[:, (start - sp.t_init) / delt:] for sp in specs]

        length = min(sp.shape[1] for sp in cut)
        return [sp[:, :length] for sp in cut]

    @classmethod
    def combine_frequencies(cls, specs):
        """ Return new spectrogram that contains frequencies from all the
        spectrograms in spec. Only returns time intersection of all of them.

        Parameters
        ----------
        spec : list
            List of spectrograms of which to combine the frequencies into one.
        """
        if not specs:
            raise ValueError("Need at least one spectrogram.")

        specs = cls.intersect_time(specs)

        one = specs[0]

        dtype_ = max(sp.dtype for sp in specs)
        fsize = sum(sp.shape[0] for sp in specs)

        new = np.zeros((fsize, one.shape[1]), dtype=dtype_)

        freq_axis = np.zeros((fsize,))


        for n, (data, row) in enumerate(merge(
            [
                [(sp, n) for n in xrange(sp.shape[0])] for sp in specs
            ],
            key=lambda x: x[0].freq_axis[x[1]]
        )):
            new[n, :] = data[row, :]
            freq_axis[n] = data.freq_axis[row]
        params = {
            'time_axis': one.time_axis, # Should be equal
            'freq_axis': freq_axis,
            'start': one.start,
            'end': one.end,
            't_delt': one.t_delt,
            't_init': one.t_init,
            't_label': one.t_label,
            'f_label': one.f_label,
            'content': one.content,
            'instruments': _union(spec.instruments for spec in specs)
        }
        return common_base(specs)(new, **params)

    def check_linearity(self, err=None, err_factor=None):
        """ Check linearity of time axis. If err is given, tolerate absolute
        derivation from average delta up to err. If err_factor is given,
        tolerate up to err_factor * average_delta. If both are given,
        TypeError is raised. Default to err=0.

        Parameters
        ----------
        err : float
            Absolute difference each delta is allowed to diverge from the
            average. Cannot be used in combination with err_factor.
        err_factor : float
            Relative difference each delta is allowed to diverge from the
            average, i.e. err_factor * average. Cannot be used in combination
            with err.
        """
        deltas = self.time_axis[:-1] - self.time_axis[1:]
        avg = np.average(deltas)
        if err is None and err_factor is None:
            err = 0
        elif err is None:
            err = abs(err_factor * avg)
        elif err_factor is not None:
            raise TypeError("Only supply err or err_factor, not both")
        return (abs(deltas - avg) <= err).all()

    def in_interval(self, start=None, end=None):
        """ Return part of spectrogram that lies in [start, end).

        Parameters
        ----------
        start : None or datetime or parse_time compatible string or time string
            Start time of the part of the spectrogram that is returned. If the
            measurement only spans over one day, a colon seperated string
            representing the time can be passed.
        end : None or datetime or parse_time compatible string or time string
            See start.
        """
        if start is not None:
            try:
                start = parse_time(start)
            except ValueError:
                # XXX: We could do better than that.
                if get_day(self.start) != get_day(self.end):
                    raise TypeError(
                        "Time ambiguous because data spans over more than one day"
                    )
                start = datetime.datetime(
                    self.start.year, self.start.month, self.start.day,
                    *map(int, start.split(":"))
                )
            start = self.time_to_x(start)
        if end is not None:
            try:
                end = parse_time(end)
            except ValueError:
                if get_day(self.start) != get_day(self.end):
                    raise TypeError(
                        "Time ambiguous because data spans over more than one day"
                    )
                end = datetime.datetime(
                    self.start.year, self.start.month, self.start.day,
                    *map(int, end.split(":"))
                )
            end = self.time_to_x(end)
        return self[:, start:end]


########NEW FILE########
__FILENAME__ = spectrum
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

from __future__ import absolute_import

import numpy as np

from matplotlib import pyplot as plt

__all__ = ['Spectrum']

class Spectrum(np.ndarray):
    """
    Class representing a spectrum.
    
    Attributes
    ----------
    freq_axis : np.ndarray
        one-dimensional array with the frequency values at every data point

    data\ : np.ndarray
        one-dimensional array which the intensity at a particular frequency at every data-point.
    """
    def __new__(cls, data, *args, **kwargs):
        return np.asarray(data).view(cls)

    def __init__(self, data, freq_axis):
        self.data = data
        self.freq_axis = freq_axis

    def plot(self, axes=None, **matplot_args):
        """
        Plot spectrum onto current axes. Behaves like matplotlib.pylot.plot()
        
        Parameters
        ----------
        axes: matplotlib.axes object or None
            If provided the spectrum will be plotted on the given axes. 
            Else the current matplotlib axes will be used.
        """
        
        #Get current axes
        if not axes:
            axes = plt.gca()
        
        params = {}
        params.update(matplot_args)
        
        #This is taken from mpl.pyplot.plot() as we are trying to
        #replicate that functionality
        
        # allow callers to override the hold state by passing hold=True|False
        washold = axes.ishold()
        hold = matplot_args.pop('hold', None)
        
        if hold is not None:
            axes.hold(hold)
        try:
            lines = axes.plot(self.freq_axis, self, **params)
        finally:
            axes.hold(washold)

        return lines
    
    def peek(self, **matplot_args):
        """
        Plot spectrum onto a new figure.
        """
        
        figure = plt.figure()
        
        lines = self.plot(**matplot_args)
        
        figure.show()
        
        return figure


########NEW FILE########
__FILENAME__ = test_callisto
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

from __future__ import absolute_import

import os
import shutil
from tempfile import mkdtemp
from datetime import datetime

import pytest

import numpy as np
from numpy.testing import assert_array_almost_equal

import sunpy.data.test
from sunpy.data.sample import CALLISTO_IMAGE
from sunpy.spectra.sources.callisto import (
    CallistoSpectrogram, query, download, minimal_pairs
)


def test_read():
    ca = CallistoSpectrogram.read(CALLISTO_IMAGE)
    assert ca.start == datetime(2011, 9, 22, 10, 30, 0, 51000)
    assert (
        ca.t_init ==
        (datetime(2011, 9, 22, 10, 30) - datetime(2011, 9, 22)).seconds
    )
    assert ca.shape == (200, 3600)
    assert ca.t_delt == 0.25
    # Test linearity of time axis.
    assert np.array_equal(
        ca.time_axis, np.linspace(0, 0.25 * (ca.shape[1] - 1), ca.shape[1])
    )
    assert ca.dtype == np.uint8

@pytest.mark.online
def test_query():
    URL = 'http://soleil.i4ds.ch/solarradio/data/2002-20yy_Callisto/2011/09/22/'

    result = list(query(
        datetime(2011, 9, 22, 5), datetime(2011, 9, 22, 6), set(["BIR"])
    ))
    RESULTS = [
        "BIR_20110922_050000_01.fit.gz",
        "BIR_20110922_051500_01.fit.gz",
        "BIR_20110922_053000_01.fit.gz",
        "BIR_20110922_050000_03.fit.gz",
        "BIR_20110922_051500_03.fit.gz",
        "BIR_20110922_053000_03.fit.gz",
        "BIR_20110922_054500_03.fit.gz",
    ]
    
    RESULTS.sort()
    # Should be sorted anyway, but better to assume as little as possible.
    result.sort()

    assert result == [URL + res for res in RESULTS]

@pytest.mark.online
def test_query_number():
    URL = 'http://soleil.i4ds.ch/solarradio/data/2002-20yy_Callisto/2011/09/22/'

    result = list(query(
        datetime(2011, 9, 22, 5), datetime(2011, 9, 22, 6), set([("BIR", 1)])
    ))
    RESULTS = [
        "BIR_20110922_050000_01.fit.gz",
        "BIR_20110922_051500_01.fit.gz",
        "BIR_20110922_053000_01.fit.gz",
    ]

    RESULTS.sort()
    # Should be sorted anyway, but better to assume as little as possible.
    result.sort()

    assert result == [URL + res for res in RESULTS]

@pytest.mark.online
def test_download():
    directory = mkdtemp()
    try:
        result = query(
            datetime(2011, 9, 22, 5), datetime(2011, 9, 22, 6), set([("BIR", 1)])
        )
        RESULTS = [
            "BIR_20110922_050000_01.fit.gz",
            "BIR_20110922_051500_01.fit.gz",
            "BIR_20110922_053000_01.fit.gz",
        ]
        download(result, directory)
        assert sorted(os.listdir(directory)) == RESULTS
    finally:
        shutil.rmtree(directory)


def test_create_file():
    ca = CallistoSpectrogram.create(CALLISTO_IMAGE)
    assert np.array_equal(ca.data, CallistoSpectrogram.read(CALLISTO_IMAGE).data)


def test_create_file_kw():
    ca = CallistoSpectrogram.create(filename=CALLISTO_IMAGE)
    assert np.array_equal(ca.data, CallistoSpectrogram.read(CALLISTO_IMAGE).data)

@pytest.mark.online
def test_create_url():
    URL = (
        "http://soleil.i4ds.ch/solarradio/data/2002-20yy_Callisto/2011/09/22/"
        "BIR_20110922_050000_01.fit.gz"
    )
    ca = CallistoSpectrogram.create(URL)
    assert np.array_equal(ca.data, CallistoSpectrogram.read(URL).data)

@pytest.mark.online
def test_create_url_kw():
    URL = (
        "http://soleil.i4ds.ch/solarradio/data/2002-20yy_Callisto/2011/09/22/"
        "BIR_20110922_050000_01.fit.gz"
    )
    ca = CallistoSpectrogram.create(url=URL)
    assert np.array_equal(ca.data, CallistoSpectrogram.read(URL).data)


def test_create_single_glob():
    PATTERN = os.path.join(
        os.path.dirname(CALLISTO_IMAGE),
        "BIR_*"
    )
    ca = CallistoSpectrogram.create(PATTERN)
    assert np.array_equal(ca.data, CallistoSpectrogram.read(CALLISTO_IMAGE).data)


def test_create_single_glob_kw():
    PATTERN = os.path.join(
        os.path.dirname(CALLISTO_IMAGE),
        "BIR_*"
    )
    ca = CallistoSpectrogram.create(singlepattern=PATTERN)
    assert np.array_equal(ca.data, CallistoSpectrogram.read(CALLISTO_IMAGE).data)


def test_create_glob_kw():
    PATTERN = os.path.join(
        os.path.dirname(CALLISTO_IMAGE),
        "BIR_*"
    )
    ca = CallistoSpectrogram.create(pattern=PATTERN)[0]
    assert np.array_equal(ca.data, CallistoSpectrogram.read(CALLISTO_IMAGE).data)

def test_create_glob():
    PATTERN = os.path.join(
        os.path.dirname(sunpy.data.test.__file__),
        "BIR_*"
    )
    ca = CallistoSpectrogram.create(PATTERN)
    assert len(ca) == 2

def test_minimum_pairs_commotative():
    A = [0, 1, 2]
    B = [1, 2, 3]
    first = list(minimal_pairs(A, B))
    assert first == [(b, a, d) for a, b, d in minimal_pairs(B, A)]

def test_minimum_pairs_end():
    assert (
        list(minimal_pairs([0, 1, 2, 4], [1, 2, 3, 4])) ==
        [(1, 0, 0), (2, 1, 0), (3, 3, 0)]
    )

def test_minimum_pairs_end_more():
    assert (
        list(minimal_pairs([0, 1, 2, 4, 8], [1, 2, 3, 4])) ==
        [(1, 0, 0), (2, 1, 0), (3, 3, 0)]
    )

def test_minimum_pairs_end_diff():
    assert (
        list(minimal_pairs([0, 1, 2, 8], [1, 2, 3, 4])) ==
        [(1, 0, 0), (2, 1, 0), (3, 3, 4)]
    )

def test_closest():
    assert (
        list(minimal_pairs([50, 60], [0, 10, 20, 30, 40, 51, 52])) ==
        [(0, 5, 1), (1, 6, 8)]
    )

def test_homogenize_factor():
    a = np.float64(np.random.randint(0, 255, 3600))[np.newaxis, :]
    
    c1 = CallistoSpectrogram(
        a,
        np.arange(3600),
        np.array([1]),
        datetime(2011, 1, 1),
        datetime(2011, 1, 1, 1),
        0,
        1,
        'Time',
        'Frequency',
        'Test',
        None,
        None,
        None,
        False
    )
    b = 2 * a
    c2 = CallistoSpectrogram(
        b,
        np.arange(3600),
        np.array([1]),
        datetime(2011, 1, 1),
        datetime(2011, 1, 1, 1),
        0,
        1,
        'Time',
        'Frequency',
        'Test',
        None,
        None,
        None,
        False
    )
    
    pairs_indices, factors, constants = c1._homogenize_params(
        c2, 0
    )
    
    assert pairs_indices == [(0, 0)]
    assert_array_almost_equal(factors, [0.5], 2)
    assert_array_almost_equal(constants, [0], 2)
    assert_array_almost_equal(factors[0] * b + constants[0], a)

def test_homogenize_constant():
    a = np.float64(np.random.randint(0, 255, 3600))[np.newaxis, :]
    
    c1 = CallistoSpectrogram(
        a,
        np.arange(3600),
        np.array([1]),
        datetime(2011, 1, 1),
        datetime(2011, 1, 1, 1),
        0,
        1,
        'Time',
        'Frequency',
        'Test',
        None,
        None,
        None,
        False
    )
    b = a + 10
    c2 = CallistoSpectrogram(
        b,
        np.arange(3600),
        np.array([1]),
        datetime(2011, 1, 1),
        datetime(2011, 1, 1, 1),
        0,
        1,
        'Time',
        'Frequency',
        'Test',
        None,
        None,
        None,
        False
    )
    
    pairs_indices, factors, constants = c1._homogenize_params(
        c2, 0
    )
    
    assert pairs_indices == [(0, 0)]
    assert_array_almost_equal(factors, [1], 2)
    assert_array_almost_equal(constants, [-10], 2)
    assert_array_almost_equal(factors[0] * b + constants[0], a)

def test_homogenize_both():
    a = np.float64(np.random.randint(0, 255, 3600))[np.newaxis, :]
    
    c1 = CallistoSpectrogram(
        a,
        np.arange(3600),
        np.array([1]),
        datetime(2011, 1, 1),
        datetime(2011, 1, 1, 1),
        0,
        1,
        'Time',
        'Frequency',
        'Test',
        None,
        None,
        None,
        False
    )
    b = 2 * a + 1
    c2 = CallistoSpectrogram(
        b,
        np.arange(3600),
        np.array([1]),
        datetime(2011, 1, 1),
        datetime(2011, 1, 1, 1),
        0,
        1,
        'Time',
        'Frequency',
        'Test',
        None,
        None,
        None,
        False
    )
    
    pairs_indices, factors, constants = c1._homogenize_params(
        c2, 0
    )
    
    assert pairs_indices == [(0, 0)]
    assert_array_almost_equal(factors, [0.5], 2)
    assert_array_almost_equal(constants, [-0.5], 2)
    assert_array_almost_equal(factors[0] * b + constants[0], a)

def test_homogenize_rightfq():
    a = np.float64(np.random.randint(0, 255, 3600))[np.newaxis, :]
        
    c1 = CallistoSpectrogram(
        a,
        np.arange(3600),
        np.array([1]),
        datetime(2011, 1, 1),
        datetime(2011, 1, 1, 1),
        0,
        1,
        'Time',
        'Frequency',
        'Test',
        None,
        None,
        None,
        False
    )
    b = 2 * a + 1
    c2 = CallistoSpectrogram(
        np.concatenate([
            np.arange(3600)[np.newaxis, :], b,
            np.arange(3600)[np.newaxis, :]
            ], 0),
        np.arange(3600),
        np.array([0, 1, 2]),
        datetime(2011, 1, 1),
        datetime(2011, 1, 1, 1),
        0,
        1,
        'Time',
        'Frequency',
        'Test',
        None,
        None,
        None,
        False
    )
    pairs_indices, factors, constants = c1._homogenize_params(
        c2, 0
    )    
    assert pairs_indices == [(0, 1)]
    assert_array_almost_equal(factors, [0.5], 2)
    assert_array_almost_equal(constants, [-0.5], 2)
    assert_array_almost_equal(factors[0] * b + constants[0], a)

@pytest.mark.online
def test_extend():
    im = CallistoSpectrogram.create(CALLISTO_IMAGE)
    im2 = im.extend()
    # Not too stable test, but works.
    assert im2.data.shape == (200, 7196)

########NEW FILE########
__FILENAME__ = test_spectrogram
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

from __future__ import absolute_import

from datetime import datetime
import pytest

import numpy as np

from numpy.testing import assert_array_almost_equal

from sunpy.spectra.spectrogram import(
    Spectrogram, LinearTimeSpectrogram, _LinearView
)


def is_linear(arr):
    return np.array_equal(arr, np.linspace(arr[0], arr[-1], len(arr)))


def dict_eq(one, other):
    ks = set(one.keys())
    if ks != set(other.keys()):
        return False
    for key in ks:
        if isinstance(one[key], np.ndarray):
            if not np.array_equal(one[key], other[key]):
                return False
        else:
            if one[key] != other[key]:
                return False
    return True


def mk_spec(image):
    return Spectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10), datetime(2010, 10, 10, 1), 0
    )


def test_subtract_bg():
    # The idea is to generate background and add a random signal, perform
    # background subtraction and see if the signal comes out again.
    bg = np.linspace(0, 200, 200).astype(np.uint16)
    bg.shape = (200, 1)
    bg = bg + np.zeros((200, 3600))

    signal = np.random.rand(200, 1800) * 255
    signal = signal.astype(np.uint16)

    image = bg
    image[:, 1800:] += signal

    spectrogram = mk_spec(image)
    sbg = spectrogram.subtract_bg()
    assert np.array_equal(
        spectrogram.subtract_bg()[:, 1800:].data, signal
    )

    assert dict_eq(spectrogram._get_params(), sbg._get_params())


def test_auto_const_bg():
    # The idea is to generate background and add a random signal, perform
    # background subtraction and see if the signal comes out again.
    x = np.linspace(0, 200, 200).astype(np.uint16)
    bg = x.reshape(200, 1)
    bg = bg + np.zeros((200, 3600))

    signal = np.random.rand(200, 1800) * 255
    signal = signal.astype(np.uint16)

    image = bg
    image[:, 1800:] += signal

    spectrogram = mk_spec(image)
    sbg = spectrogram.auto_const_bg()
    assert np.array_equal(sbg, x.reshape(200, 1))


def test_randomized_auto_const_bg():
    # The idea is to generate background and add a random signal, perform
    # background subtraction and see if the signal comes out again.
    # As this is a Monte-Carlo probabilistic algorithm this test might
    # fail occasionally.
    x = np.linspace(0, 200, 200).astype(np.uint16)
    bg = x.reshape(200, 1)
    bg = bg + np.zeros((200, 3600))

    signal = np.random.rand(200, 1800) * 255
    signal = signal.astype(np.uint16)

    image = bg
    image[:, 1800:] += signal

    spectrogram = mk_spec(image)
    sbg = spectrogram.randomized_auto_const_bg(1500)
    assert np.array_equal(sbg, x.reshape(200, 1))


def test_slice_time_axis():
    rnd = np.random.rand(200, 3600)
    spectrogram = mk_spec(rnd)
    new = spectrogram[:, 59:3599]
    assert new.shape == (200, 3600 - 59 - 1)
    assert new.t_init == 59
    assert np.array_equal(new.time_axis,
        np.linspace(0, 3600 - 60 - 1, 3600 - 59 - 1)
    )
    assert new.start == datetime(2010, 10, 10, 0, 0, 59)
    assert np.array_equal(new.data, rnd[:, 59:3599])


def test_slice_freq_axis():
    rnd = np.random.rand(200, 3600)
    spectrogram = mk_spec(rnd)
    new = spectrogram[100:150, :]
    assert new.shape == (50, 3600)
    assert np.array_equal(new.freq_axis, np.linspace(100, 149, 50))
    assert np.array_equal(new.data, rnd[100:150, :])


def test_slice_both_axis():
    rnd = np.random.rand(200, 3600)
    spectrogram = mk_spec(rnd)
    new = spectrogram[100:, 59:]
    assert new.shape == (100, 3600 - 59)
    assert new.t_init == 59
    assert np.array_equal(new.time_axis, np.linspace(0, 3600 - 60, 3600 - 59))
    assert new.start == datetime(2010, 10, 10, 0, 0, 59)
    assert np.array_equal(new.freq_axis, np.linspace(100, 199, 100))
    assert np.array_equal(new.data, rnd[100:, 59:])


def test_time_to_x():
    image = np.zeros((200, 3600))
    spectrogram = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10), datetime(2010, 10, 10, 1), 0, 1
    )
    ret = spectrogram.time_to_x(datetime(2010, 10, 10, 0, 0, 59))
    assert isinstance(ret, int)
    assert ret == 59


def test_time_to_x_nonlinear():
    image = np.zeros((200, 3600))
    spectrogram = Spectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10), datetime(2010, 10, 10, 1)
    )
    ret = spectrogram.time_to_x(datetime(2010, 10, 10, 0, 0, 59))
    assert isinstance(ret, int)
    assert ret == 59


def test_join():
    image = np.random.rand(200, 3600)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10), datetime(2010, 10, 10, 0, 30), 0, 0.5,
    )

    image = np.random.rand(200, 3600)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10, 0, 29),
        datetime(2010, 10, 10, 1, 29), 1799, 1,
    )

    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=False, maxgap=0
    )
    # The - 1 is because resampling other procuces an image of size
    # 2 * 3600 - 1
    # The - 2 is because there is one second overlap.
    assert z.shape == (200, 3 * 3600 - 2 - 1)

    assert np.array_equal(z.data[:, :3598], one.data[:, :-2])
    # assert np.array_equal(z[:, 3598:], ndimage.zoom(other, (1, 2)))
    assert z.start == one.start
    assert z.end == other.end
    assert is_linear(z.time_axis)
    assert isinstance(z, LinearTimeSpectrogram)


def test_join_dtype():
    image = np.random.rand(200, 3600).astype(np.uint8)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10), datetime(2010, 10, 10, 0, 30), 0, 0.5,
    )

    image = np.random.rand(200, 3600).astype(np.uint8)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10, 0, 29),
        datetime(2010, 10, 10, 1, 29), 1799, 1,
    )
    
    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=False, maxgap=0
    )
    assert z.dtype == np.dtype('uint8')


def test_join_different_dtype():
    image = np.random.rand(200, 3600).astype(np.uint16)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10), datetime(2010, 10, 10, 0, 30), 0, 0.5,
    )

    image = np.random.rand(200, 3600).astype(np.uint8)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10, 0, 29),
        datetime(2010, 10, 10, 1, 29), 1799, 1,
    )
    
    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=False, maxgap=0
    )
    assert z.dtype == np.dtype('uint16')


def test_join_midnight():
    image = np.random.rand(200, 3600)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10, 23, 30),
        datetime(2010, 10, 10, 23, 59, 59), 84600, 0.5,
    )

    image = np.random.rand(200, 3600)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 11, 0, 0), datetime(2010, 10, 11, 1), 0, 1,
    )

    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=False, maxgap=0
    )
    # The - 1 is because resampling other procuces an image of size
    # 2 * 3600 - 1
    assert z.shape == (200, 3 * 3600 - 1)

    assert np.array_equal(z.data[:, :3600], one.data)
    assert is_linear(z.time_axis)
    assert isinstance(z, LinearTimeSpectrogram)


def test_join_month():
    image = np.random.rand(200, 3600)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2012, 7, 31, 23, 30),
        datetime(2012, 7, 31, 23, 59, 59), 84600, 0.5,
    )

    image = np.random.rand(200, 3600)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2012, 8, 1), datetime(2012, 8, 1, 1), 0, 1,
    )

    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=False, maxgap=0
    )
    # The - 1 is because resampling other procuces an image of size
    # 2 * 3600 - 1
    assert z.shape == (200, 3 * 3600 - 1)

    assert np.array_equal(z.data[:, :3600], one.data)
    assert is_linear(z.time_axis)
    assert isinstance(z, LinearTimeSpectrogram)


def test_join_year():
    image = np.random.rand(200, 3600)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2012, 12, 31, 23, 30),
        datetime(2013, 1, 1, 0, 0, 0), 84600, 0.5,
    )

    image = np.random.rand(200, 3600)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2013, 1, 1), datetime(2013, 1, 1, 1), 0, 1,
    )

    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=False, maxgap=0
    )
    # The - 1 is because resampling other procuces an image of size
    # 2 * 3600 - 1
    assert z.shape == (200, 3 * 3600 - 1)

    assert np.array_equal(z.data[:, :3600], one.data)
    assert is_linear(z.time_axis)
    assert isinstance(z, LinearTimeSpectrogram)


def test_join_over_midnight():
    image = np.random.rand(200, 3600)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10, 23, 45),
        datetime(2010, 10, 11, 0, 15,), 85500, 0.5,
    )
    image = np.random.rand(200, 3600)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 11, 0, 15), datetime(2010, 10, 11, 1, 15), 900, 1,
    )

    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=False, maxgap=0
    )
    oz = other.resample_time(0.5)

    # The - 1 is because resampling other procuces an image of size
    # 2 * 3600 - 1
    assert z.shape == (200, 3 * 3600 - 1)

    assert np.array_equal(z.data[:, :3600], one.data)
    assert np.array_equal(z.time_axis[:3600], one.time_axis)
    assert is_linear(z.time_axis)
    assert isinstance(z, LinearTimeSpectrogram)


def test_join_gap():
    image = np.random.rand(200, 3600)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10, 23, 45),
        datetime(2010, 10, 11, 0, 15,), 85500, 0.5,
    )

    image = np.random.rand(200, 3600)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 11, 0, 15, 1),
        datetime(2010, 10, 11, 1, 15), 901, 1,
    )
    with pytest.raises(ValueError) as excinfo:
        LinearTimeSpectrogram.join_many(
            [one, other], nonlinear=False, maxgap=0
        )

    assert excinfo.value.message == "Too large gap."


def test_join_with_gap():
    image = np.random.rand(200, 3600)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10, 23, 45),
        datetime(2010, 10, 11, 0, 15,), 85500, 0.5,
    )

    image = np.random.rand(200, 3600)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 11, 0, 15), datetime(2010, 10, 11, 1, 15), 901, 1,
    )

    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=False, maxgap=1, fill=0
    )

    # The - 1 is because resampling other produces an image of size
    # 2 * 3600 - 1
    # The + 2 is because there is one second without data inserted.
    assert z.shape == (200, 3 * 3600 + 2 - 1)

    assert np.array_equal(z.data[:, :3600], one.data)
    # Second data to unpack masked array
    assert (z.data.data[:, 3600:3602] == 0).all()
    assert is_linear(z.time_axis)
    assert isinstance(z, LinearTimeSpectrogram)


def test_join_with_gap_fill():
    image = np.random.rand(200, 3600)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10, 23, 45),
        datetime(2010, 10, 11, 0, 15,), 85500, 0.5,
    )

    image = np.random.rand(200, 3600)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 11, 0, 15), datetime(2010, 10, 11, 1, 15), 901, 1,
    )

    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=False, maxgap=2, fill=np.NaN
    )
    # The - 1 is because resampling other procuces an image of size
    # 2 * 3600 - 1
    # The + 2 is because there is one second without data inserted.
    assert z.shape == (200, 3 * 3600 + 2 - 1)

    assert np.array_equal(z.data[:, :3600], one.data)

    print type(z.data)

    # Second data to unpack masked array
    assert np.isnan(z.data.data[:, 3600:3602]).all()
    assert is_linear(z.time_axis)
    assert isinstance(z, LinearTimeSpectrogram)


def test_join_nonlinear():
    image = np.random.rand(200, 3600)
    one = LinearTimeSpectrogram(
        image, np.linspace(0, 0.5 * (image.shape[1] - 1), image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 10, 23, 45),
        datetime(2010, 10, 11, 0, 15,), 85500, 0.5,
    )

    image = np.random.rand(200, 3600)
    other = LinearTimeSpectrogram(
        image, np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 10, 11, 0, 15),
        datetime(2010, 10, 11, 1, 15), 901, 1,
    )

    oz = other.resample_time(0.5)

    z = LinearTimeSpectrogram.join_many(
        [one, other], nonlinear=True, maxgap=2
    )

    # The - 1 is because resampling other procuces an image of size
    # 2 * 3600 - 1
    assert z.shape == (200, 3 * 3600 - 1)

    assert np.array_equal(z.data[:, :3600], one.data)
    assert np.array_equal(z.time_axis[:3600], one.time_axis)
    assert np.array_equal(z.time_axis[3600:], oz.time_axis + 1801)
    assert isinstance(z, Spectrogram)


def test_auto_t_init():
    image = np.random.rand(200, 3600)
    assert Spectrogram(image,
        np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30)
    ).t_init == 900


def test_rescale():
    image = np.random.rand(200, 3600) * 43
    spec = Spectrogram(image,
        np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30)
    )

    nspec = spec.rescale()

    assert dict_eq(spec._get_params(), nspec._get_params())
    assert_array_almost_equal(nspec.data.max(), 1)
    assert nspec.data.min() == 0


def test_rescale_error():
    image = np.zeros((200, 3600))
    spec = Spectrogram(image,
        np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30)
    )

    with pytest.raises(ValueError) as excinfo:
        spec.rescale(0, 1)
    assert (
        excinfo.value.message ==
        "Spectrogram needs to contain distinct values."
    )


def test_rescale_error2():
    image = np.random.rand(200, 3600) * 43
    spec = Spectrogram(image,
        np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.linspace(0, image.shape[0] - 1, image.shape[0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30)
    )

    with pytest.raises(ValueError) as excinfo:
        spec.rescale(1, 1)
    assert excinfo.value.message == "Maximum and minimum must be different."


def test_resample():
    image = np.array([[0, 1, 2], [0, 1, 2]])
    spec = LinearTimeSpectrogram(
        image, np.array([0, 1, 2]), np.array([0]),
        datetime(2012, 1, 1), datetime(2012, 1, 1, 0, 0, 3),
        0, 1
    )
    r = spec.resample_time(0.5)
    assert r.shape[1] == 5
    assert np.array_equal(r.time_axis, np.linspace(0, 2, 5))


def test_upsample():
    image = np.array([[0, 1, 2, 3], [0, 1, 2, 3]])
    spec = LinearTimeSpectrogram(
        image, np.array([0, 1, 2]), np.array([0]),
        datetime(2012, 1, 1), datetime(2012, 1, 1, 0, 0, 3),
        0, 1
    )
    r = spec.resample_time(2)
    assert r.shape[1] == 2


def test_combine_freqs():
    image = np.random.rand(5, 3600)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.array([8, 6, 4, 2, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        0.25
    )
    image = np.random.rand(5, 3600)
    spec2 = LinearTimeSpectrogram(image,
        np.linspace(0, image.shape[1] - 1, image.shape[1]),
        np.array([9, 7, 5, 3, 1]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        0.25
    )
    comb = LinearTimeSpectrogram.combine_frequencies([spec, spec2])
    stuff = [spec, spec2]

    # print comb

    for freq in xrange(10):
        assert np.array_equal(
            comb[9 - freq, :], stuff[freq % 2][4 - freq // 2, :]
        )


def test_join_diff_freq():
    image = np.random.rand(5, 3600)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 0.25 * (image.shape[1] - 1), image.shape[1]),
        np.array([8, 6, 4, 2, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        0.25
    )
    image = np.random.rand(5, 3600)
    spec2 = LinearTimeSpectrogram(image,
        np.linspace(0, 0.25 * (image.shape[1] - 1), image.shape[1]),
        np.array([9, 7, 5, 3, 1]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        1800,
        0.25
    )
    
    with pytest.raises(ValueError) as excinfo:
        LinearTimeSpectrogram.join_many([spec, spec2])
    assert excinfo.value.message == "Frequency channels do not match."


def test_intersect_time():
    image = np.random.rand(5, 3600)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 0.25 * (image.shape[1] - 1), image.shape[1]),
        np.array([8, 6, 4, 2, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        0.25
    )
    image = np.random.rand(5, 3600)
    spec2 = LinearTimeSpectrogram(image,
        np.linspace(0, 0.25 * (image.shape[1] - 1), image.shape[1]),
        np.array([9, 7, 5, 3, 1]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        901,
        0.25
    )

    one, other = LinearTimeSpectrogram.intersect_time(
        [spec, spec2]
    )

    assert one.shape[1] == other.shape[1]
    assert one.shape[1] == 3596
    assert np.array_equal(one.data, spec.data[:, 4:])
    assert np.array_equal(other.data, spec2.data[:, :-4])

    assert np.array_equal(one.time_axis, other.time_axis)
    assert one.t_init == other.t_init
    assert is_linear(one.time_axis)
    assert is_linear(other.time_axis)

def test_check_linearity():
    image = np.random.rand(5, 3600)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 0.25 * (image.shape[1] - 1), image.shape[1]),
        np.array([8, 6, 4, 2, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        0.25
    )
    assert spec.check_linearity()
    spec.time_axis[1] += 0.1
    assert not spec.check_linearity()
    assert spec.check_linearity(0.1)
    spec.time_axis[1] -= 0.1
    # The average stays (almost) the same because there are 3600 items.
    spec.time_axis[1] += 0.2 * 0.25
    assert spec.check_linearity(None, 0.2)


def test_flatten():
    flat = np.arange(5 * 3600)
    image = flat.reshape(5, 3600)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 0.25 * (image.shape[1] - 1), image.shape[1]),
        np.array([8, 6, 4, 2, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        0.25
    )
    assert np.array_equal(flat, spec.data.flatten())


def test_in_interval():
    image = np.random.rand(5, 900)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 1 * (image.shape[1] - 1), image.shape[1]),
        np.array([8, 6, 4, 2, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        1
    )
    
    assert np.array_equal(spec.in_interval("00:15", "00:30").data, spec.data)


def test_in_interval2():
    image = np.random.rand(5, 900)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 1 * (image.shape[1] - 1), image.shape[1]),
        np.array([8, 6, 4, 2, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        1
    )
    
    assert np.array_equal(
        spec.in_interval("2010-01-01T00:15:00", "00:30").data, spec.data
    )


def test_linearize():
    image = np.random.rand(5, 900)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 1 * (image.shape[1] - 1), image.shape[1]),
        np.array([20, 10, 5, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        1
    )
    # 0   1   2   3   4   5  6  7  8
    # -------- ----------- ----- ---
    # 20 17.5 15 12.5 10 7.5 5 2.5 0
    
    linear = spec.linearize_freqs()
    assert ((linear.freq_axis[:-1] - linear.freq_axis[1:]) == 2.5).all()
    
    assert (linear[0] == image[0, :]).all()
    assert (linear[1] == image[0, :]).all()
    assert (linear[2] == image[0, :]).all()
    assert (linear[3] == image[1, :]).all()
    assert (linear[4] == image[1, :]).all()
    assert (linear[5] == image[1, :]).all()    
    assert (linear[6] == image[2, :]).all()
    assert (linear[7] == image[2, :]).all()
    assert (linear[8] == image[3, :]).all()


def test_linear_view():
    image = np.random.rand(5, 900)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 1 * (image.shape[1] - 1), image.shape[1]),
        np.array([20, 10, 5, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        1
    )
    
    linear = _LinearView(spec)
    # assert ((linear.freq_axis[:-1] - linear.freq_axis[1:]) == 2.5).all()
    
    assert (linear[0] == image[0, :]).all()
    assert (linear[1] == image[0, :]).all()
    assert (linear[2] == image[0, :]).all()
    assert (linear[3] == image[1, :]).all()
    assert (linear[4] == image[1, :]).all()
    assert (linear[5] == image[1, :]).all()    
    assert (linear[6] == image[2, :]).all()
    assert (linear[7] == image[2, :]).all()
    assert (linear[8] == image[3, :]).all()


def test_linear_view_indexerror():
    image = np.random.rand(5, 900)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 1 * (image.shape[1] - 1), image.shape[1]),
        np.array([20, 10, 5, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        1
    )
    
    linear = _LinearView(spec)
    # assert ((linear.freq_axis[:-1] - linear.freq_axis[1:]) == 2.5).all()
    with pytest.raises(IndexError):
        linear[9]


def test_linear_view_negative():
    image = np.random.rand(5, 900)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 1 * (image.shape[1] - 1), image.shape[1]),
        np.array([20, 10, 5, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        1
    )
    
    linear = _LinearView(spec)
    # assert ((linear.freq_axis[:-1] - linear.freq_axis[1:]) == 2.5).all()
    assert (linear[8] == image[3, :]).all()
    assert (linear[-1] == image[3, :]).all()


def test_linear_view_freqs():
    image = np.random.rand(5, 900)
    spec = LinearTimeSpectrogram(image,
        np.linspace(0, 1 * (image.shape[1] - 1), image.shape[1]),
        np.array([20, 10, 5, 0]),
        datetime(2010, 1, 1, 0, 15),
        datetime(2010, 1, 1, 0, 30),
        900,
        1
    )
    
    linear = _LinearView(spec)
    # assert ((linear.freq_axis[:-1] - linear.freq_axis[1:]) == 2.5).all()
    
    assert linear.get_freq(0) == 20
    assert linear.get_freq(1) == 20
    assert linear.get_freq(2) == 20
    assert linear.get_freq(3) == 10
    assert linear.get_freq(4) == 10
    assert linear.get_freq(5) == 10
    assert linear.get_freq(6) == 5
    assert linear.get_freq(7) == 5
    assert linear.get_freq(8) == 0

########NEW FILE########
__FILENAME__ = constants
"""
Fundamental Solar Physical Constants
------------------------------------
These constants are taken from various sources. The structure of this module is heavily 
based on if not directly copied from the SciPy constants module but contains olar 
Physical constants.

Object
------
    physical_constants : dict
        A dictionary containing physical constants. Keys are the names
        of physical constants, values are tuples (value, units, uncertainty). The dictionary
        contains the following solar physical constants:

    average density:
         The average density of the Sun.
    average_angular_size: 
        The average angular size of the Sun as seen from Earth in arcseconds.
    effective temperature:
        The effective black-body temperature of the Sun in Kelvin.  
    oblateness: 
        The ellipticity of the Sun.
    escape velocity: 
        The velocity which an object needs to escape from the gravitational pull of the Sun.
    luminosity:
        The luminosity of the Sun.
    mass:
        The mass of the Sun.
    mass conversion rate:
        The rate at which the Sun converts mass to energy.
    mean energy production:
        The mean rate at which the Sun produces energy.
    mean intensity:
        The mean intensity of the Sun.
    metallicity: 
        The metallicity of the Sun.
    radius:
        The radius of the Sun at the equator.
    solar flux unit:
        The definition of a solar flux unit.
    sunspot cycle:
        The average duration of the solar activity cycle.
    surface area:
        The surface area of the Sun. 
    surface gravity:
        The gravitational acceleration at the surface of the Sun as measured at the equator.
    visual magnitude:
       A measure of the Sun's brightness as seen by an observer on Earth without the
       presence of the atmosphere.
    volume:
        The volume of the Sun.

Attributes
----------
A number of variables from physical_constants are made available for convenience as 
attributes. 

Websites
--------
| http://books.google.com/books?id=4SWENr1tIJ0C&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q=sfu&f=false

"""

from __future__ import absolute_import

from sunpy.sun import _si as _con # pylint: disable=E0611

physical_constants = _con.physical_constants

def constant(key) :
    """
    The constant in physical_constants index by key
    
    Parameters
    ----------
    key : Python string or unicode
        Key in dictionary in `physical_constants`

    Returns
    -------
    constant : constant
        constant in `physical_constants` corresponding to `key`

    See Also
    --------
    _constants : Contains the description of `physical_constants`, which, as a
        dictionary literal object, does not itself possess a docstring.

    Examples
    --------
    >>> from sunpy.sun import constants
    >>> constants.constant('mass')
    
    """
    return physical_constants[key]

def value(key) :
    """
    Value in physical_constants indexed by key

    Parameters
    ----------
    key : Python string or unicode
        Key in dictionary `physical_constants`

    Returns
    -------
    value : float
        Value in `physical_constants` corresponding to `key`

    See Also
    --------
    _constants : Contains the description of `physical_constants`, which, as a
        dictionary literal object, does not itself possess a docstring.

    Examples
    --------
    >>> from sunpy.sun import constants
    >>> constants.uncertainty('mass')
        1.9884e30

    """
    return constant(key).value

def unit(key) :
    """
    Unit in physical_constants indexed by key

    Parameters
    ----------
    key : Python string or unicode
        Key in dictionary `physical_constants`

    Returns
    -------
    unit : Python string
        Unit in `physical_constants` corresponding to `key`

    See Also
    --------
    _constants : Contains the description of `physical_constants`, which, as a
        dictionary literal object, does not itself possess a docstring.

    Examples
    --------
    >>> from sunpy.sun import constants
    >>> constants.uncertainty('mass')
    'kg'

    """
    return constant(key).unit

def uncertainty(key) :
    """
    Relative uncertainty in physical_constants indexed by key

    Parameters
    ----------
    key : Python string or unicode
        Key in dictionary `physical_constants`

    Returns
    -------
    prec : float
        Relative uncertainty in `physical_constants` corresponding to `key`

    See Also
    --------
    _constants : Contains the description of `physical_constants`, which, as a
        dictionary literal object, does not itself possess a docstring.

    Examples
    --------
    >>> from sunpy.sun import constants
    >>> constants.uncertainty('mass')
    

    """
    return constant(key).uncertainty

def find(sub=None, disp=False):
    """
    Return list of physical_constants keys containing a given string

    Parameters
    ----------
    sub : str, unicode
        Sub-string to search keys for.  By default, return all keys.
    disp : bool
        If True, print the keys that are found, and return None.
        Otherwise, return the list of keys without printing anything.

    Returns
    -------
    keys : None or list
        If `disp` is False, the list of keys is returned. Otherwise, None
        is returned.

    See Also
    --------
    _constants : Contains the description of `physical_constants`, which, as a
        dictionary literal object, does not itself possess a docstring.

    """
    if sub is None:
        result = physical_constants.keys()
    else:
        result = [key for key in physical_constants \
                 if sub.lower() in key.lower()]

    result.sort()
    if disp:
        for key in result:
            print key
        return
    else:
        return result


def print_all(key = None):
    """
    Prints out the complete list of physical_constants to the screen or
    one single value
    
    Parameters
    ----------
    key : Python string or unicode
        Key in dictionary `physical_constants`

    Returns
    -------
    None

    See Also
    --------
    _constants : Contains the description of `physical_constants`, which, as a
        dictionary literal object, does not itself possess a docstring.

    """
    column_width = [25, 20, 20, 20]
    table_width = (column_width[0] + column_width[1] + column_width[2] 
                   + column_width[3])
    format_string = ('{0:<' + str(column_width[0]) + '}' + '{1:>' + 
                    str(column_width[1]) + '}' + '{2:>' + str(column_width[2]) 
                    + '}' + '{3:>' + str(column_width[3]) + '}')
    print(format_string.format('Name', 'Value', 'Units', 'Error'))
    print(('{:-^' + str(table_width) + '}').format(''))

    if key is None:
        for key in physical_constants:
            print(format_string.format(key, str(value(key)), unit(key), 
                                       str(uncertainty(key))))
    else: 
        print(format_string.format(key, str(value(key)), unit(key), 
                                   str(uncertainty(key))))

# Spectral class is not included in physical constants since it is not a number
spectral_classification = 'G2V'

au = astronomical_unit = constant('mean distance')

# The following variables from _constants are brought out by making them 
# accessible through a call such as sun.volume
mass = constant('mass')
equatorial_radius = radius = constant('radius')
volume = constant('volume')
surface_area = constant('surface area')
average_density = density = constant('average density')
equatorial_surface_gravity = surface_gravity = constant('surface gravity')
effective_temperature = constant('effective temperature')
luminosity = constant('luminosity')
mass_conversion_rate = constant('mass conversion rate')
escape_velocity = constant('escape velocity')

sfu = constant('solar flux unit')

# Observable parameters
average_angular_size = constant('average angular size')

########NEW FILE########
__FILENAME__ = models
"""
Solar Physical Models
---------------------
This module contains standard models of the sun from various sources. All data is saved
in pandas DataFrames with two added attributes

* source : names the source of the data
* units : a dictionary with the units of each of the columns

Object
------
    interior : pandas.DataFrame
        The standard model of the solar interior
    evolution : pandas.DatFrame
        The evolution as a function of time of the Sun

.. todo:: Need source for evolution model.

"""

from __future__ import absolute_import

import pandas
from astropy.units import Quantity
import sunpy.sun.constants as con

# Solar radius measured outside earth's atmosphere in arcseconds

# Standard Model - Interior Structure
# adapted from Turck-Chieze et al. (1988)
# Composition X = 0.7046, Y = 0.2757, Z = 0.0197

# Radius -  R_sun
_radius = [0, 0.01, 0.022, 0.061, 0.090, 0.120,
          0.166, 0.202, 0.246, 0.281, 0.317,
          0.370, 0.453, 0.611, 0.7304, 0.862,
          0.965, 1.0000]
          
# mass -  M_sun
_mass = [0, 0.0001, 0.002, 0.020, 0.057, 0.115,
        0.235, 0.341, 0.470, 0.562, 0.647, 0.748,
        0.854, 0.951, 0.9809, 0.9964, 0.9999, 1.000]

# luminosity -  L_sun
_luminosity = [0, 0.0009, 0.009, 0.154, 0.365, 
              0.594, 0.845, 0.940, 0.985, 0.997,
              0.992, 0.9996, 1.000, 1.0, 1.0, 1.0, 1.0, 1.0]
              
# temperature -  10^6 K
_temperature = [15.513, 15.48, 15.36, 14.404, 
               13.37, 12.25, 10.53, 9.30, 8.035,
               7.214, 6.461, 5.531, 4.426, 2.981,
               2.035, 0.884, 0.1818, 0.005770]
               
# density -  g cm^-3
_density = [147.74, 146.66, 142.73, 116.10, 93.35,
           72.73, 48.19, 34.28, 21.958, 15.157,
           10.157, 5.566, 2.259, 0.4483, 0.1528,
           0.042, 0.00361, 1.99e-7]

_d = {'mass': _mass, 'luminosity': _luminosity, 'temperature': _temperature, 'density': _density}
interior = pandas.DataFrame(_d, index = _radius)
interior.index.name = 'radius'
interior.units = {'radius': con.radius, 'mass': con.mass, 'luminosity': con.luminosity, 
                  'temperature': Quantity(1e6, 'K'), 'density': Quantity(1, 'g cm**-3')}
interior.source = 'Turck-Chieze et al. (1988)'

# time -  10^9 years
_time = [0, 0.143, 0.856, 1.863, 2.193, 3.020,
         3.977, 4.587, 5.506, 6.074, 6.577, 7.027,
         7.728, 8.258, 8.7566, 9.805]
         
# luminosity -  L_sun
_tluminosity = [0.7688, 0.7248, 0.7621, 0.8156,
               0.8352, 0.8855, 0.9522, 1.0, 1.079,
               1.133, 1.186, 1.238, 1.318, 1.399,
               1.494, 1.760]
               
# radius -  R_sun
_tradius = [0.872, 0.885, 0.902, 0.924, 0.932,
           0.953, 0.981, 1.0, 1.035, 1.059, 1.082,
           1.105, 1.143, 1.180, 1.224, 1.361]
           
# central temperature -  10^6 K
_tcentral_temperature = [13.35, 13.46, 13.68, 14.08, 14.22, 14.60,
                        15.12, 15.51, 16.18, 16.65, 17.13, 17.62,
                        18.42, 18.74, 18.81, 19.25]

_t = {'luminosity': _tluminosity, 'radius': _tradius, 'central temperature': _tcentral_temperature}
evolution = pandas.DataFrame(_t, index = _tradius)
evolution.index.name = 'time'
evolution.units = {'radius': con.radius, 'luminosity': con.luminosity, 
                  'central temperature': Quantity(1e6, 'K'), 'time': Quantity(1e9, 'year')}
evolution.source = 'Unknown'
########NEW FILE########
__FILENAME__ = position
"""
Position of the Sun.
"""
from __future__ import absolute_import

import numpy as np
#pylint: disable=E1101,E1121

__all__ = ['position']

__authors__ = ["Jack Ireland"]
__email__ = "jack.ireland@nasa.gov"

"""
Questions (now an issue: https://github.com/sunpy/sunpy/issues/394)
---------
1. should the output always be an ndarray?
2. Should we be able to return position in other coordinate systems as well?
   If so, then this function should be to something like radec, and pos() should
   accept "radec" as input and call the current function.
3. Where do all of the "magic" numbers come from? If possible document them, and
   they may be useful elsewhere, consider putting them in sunpy.constants (could
   any of them already exist there or in scipy constants?)
"""

def position(date, radian=False):
    """
    Routine to calculate the right ascension (RA) and declination (dec) of 
    the Sun.
    
    Based in Solarsoft/IDL routine SUNPOS (Sept 1997)
    
    Parameters
    ----------
    date : scalar, numpy.ndarray ???
        Julian dates as input
    radian : boolean
        if True, return the results in radians

    Attributes
    ----------
    none

    Examples
    --------
    >>> from sunpy.sun import pos
    >>> pos.pos(0.0)
    >>> (array([ 241.65373812]), array([-21.68562104]), array([ 243.82420808]), array([ 24.31513199]))'
    >>> pos.pos(2455813.024259259, radian=True)
    >>> (array([ 2.90902792]), array([ 0.09958257]), array([ 2.88897706]), array([ 0.40905761]))
    
    See Also
    --------
    numpy.ndarray
    
    References
    ----------
    | http://docs.scipy.org/doc/numpy/reference/arrays.classes.html
    """
    
    # Make sure the input is a ndarray
    if np.isscalar(date):
        date = date + np.zeros([1])

    # Degrees to radians
    deg2rad = np.pi / 180.0
    
    # Time in Julian centuries from 1900.0
    time = (date - 2415020.0) / 36525.0

    # Sun's mean longitude
    longitude = (279.696678 + np.mod(36000.768925 * time, 360.0)) * 3600.0
    
    # Allow for ellipticity of the orbit (equation of centre)
    # using the Earth's mean anomaly ME
    me = 358.4758440 + np.mod(35999.0497500 * time, 360.0)
    ellcor  = ((6910.10 - 17.20 * time) * np.sin(me * deg2rad) + 
               72.30 * np.sin(2.00 * me * deg2rad))
    longitude = longitude + ellcor

    # Allow for the Venus perturbations using the mean anomaly of Venus MV
    mv = 212.6032190 + np.mod(58517.8038750 * time, 360.0)
    vencorr = (4.80 * np.cos((299.10170 + mv - me) * deg2rad) + 
               5.50 * np.cos((148.31330 + 2.00 * mv - 2.00 * me ) * deg2rad) + 
               2.50 * np.cos((315.94330 + 2.00 * mv - 3.00 * me ) * deg2rad) + 
               1.60 * np.cos((345.25330 + 3.00 * mv - 4.00 * me ) * deg2rad) + 
               1.00 * np.cos((318.150   + 3.00 * mv - 5.00 * me ) * deg2rad))
    longitude = longitude + vencorr

    # Allow for the Mars perturbations using the mean anomaly of Mars MM
    mm = 319.5294250 + np.mod( 19139.8585000 * time,  360.00)
    marscorr = (2.00 * np.cos((343.88830 - 2.00 * mm + 2.00 * me) * deg2rad ) + 
                1.80 * np.cos((200.40170 - 2.00 * mm + me) * deg2rad))
    longitude = longitude + marscorr

    # Allow for the Jupiter perturbations using the mean anomaly of
    # Jupiter MJ
    mj = 225.3283280 + np.mod( 3034.69202390 * time, 360.00 )
    jupcorr = (7.20 * np.cos(( 179.53170 - mj + me ) * deg2rad) +
               2.60 * np.cos((263.21670 - mj) * deg2rad) +
               2.70 * np.cos(( 87.14500 - 2.00 * mj + 2.00 * me) * deg2rad) +
               1.60 * np.cos((109.49330 - 2.00 * mj + me) * deg2rad))
    longitude = longitude + jupcorr
    
    # Allow for the Moons perturbations using the mean elongation of
    # the Moon from the Sun
    moon_me = 350.73768140 + np.mod(445267.114220 * time, 360.00)
    mooncorr = 6.50 * np.sin(moon_me * deg2rad)
    longitude = longitude + mooncorr

    # Allow for long period terms
    longterm = + 6.40 * np.sin((231.190 + 20.200 * time) * deg2rad)
    longitude = longitude + longterm
    longitude = np.mod(longitude + 2592000.00, 1296000.00)
    longmed = longitude / 3600.00

    # Allow for Aberration ??????
    longitude = longitude - 20.50

    # Allow for Nutation using the longitude of the Moons mean node OMEGA
    omega = 259.1832750 - np.mod(1934.1420080 * time, 360.00)
    longitude = longitude - 17.20 * np.sin(omega * deg2rad)

    # Form the True Obliquity
    obliquity = (23.4522940 - 0.01301250 * 
                 time + (9.20 * np.cos(omega * deg2rad)) / 3600.00)

    # Form Right Ascension and Declination
    longitude = longitude / 3600.00
    ra = np.arctan2(np.sin(longitude * deg2rad) * np.cos(obliquity * deg2rad), 
                    np.cos(longitude * deg2rad))

    # ?
    neg = np.where(ra < 0.0)
    if len(neg) > 0:
        ra[neg] = ra[neg] + 2.0 * np.pi

    dec = np.arcsin(np.sin(longitude * deg2rad) * np.sin(obliquity * deg2rad))

    if radian:
        obliquity = obliquity * deg2rad 
        longmed = longmed * deg2rad
    else:
        ra = ra / deg2rad
        dec = dec / deg2rad

    return ra, dec, longmed, obliquity
########NEW FILE########
__FILENAME__ = sun
"""Provides Sun-related parameters

The following code is heavily based on IDL function get_sun.pro which itself 
is based on algorithms presented in the book Astronomical Formulae for 
Calculators, by Jean Meeus.

A correct answer set to compare to

Solar Ephemeris for  1-JAN-01  00:00:00
 
Distance (AU) = 0.98330468
Semidiameter (arc sec) = 975.92336
True (long, lat) in degrees = (280.64366, 0.00000)
Apparent (long, lat) in degrees = (280.63336, 0.00000)
True (RA, Dec) in hrs, deg = (18.771741, -23.012449)
Apparent (RA, Dec) in hrs, deg = (18.770994, -23.012593)
Heliographic long. and lat. of disk center in deg = (217.31269, -3.0416292)
Position angle of north pole in deg = 2.0102649
Carrington Rotation Number = 1971.4091        check!

"""
from __future__ import absolute_import

import numpy as np

import astropy.units as u

from sunpy.time import parse_time, julian_day, julian_centuries
from sunpy.sun import constants

__all__ = ["print_params"
           ,"heliographic_solar_center"
           ,"solar_north"
           ,"apparent_declination"
           ,"apparent_rightascenscion"
           ,"apparent_obliquity_of_ecliptic"
           ,"true_declination"
           ,"true_rightascenscion"
           ,"true_obliquity_of_ecliptic"
           ,"apparent_latitude"
           ,"true_latitude"
           ,"apparent_longitude"
           ,"sunearth_distance"
           ,"true_anomaly"
           ,"true_longitude"
           ,"equation_of_center"
           ,"geometric_mean_longitude"
           ,"carrington_rotation_number"
           ,"mean_anomaly"
           ,"longitude_Sun_perigee"
           ,"mean_ecliptic_longitude"
           ,"eccentricity_SunEarth_orbit"
           ,"position"
           ,"solar_semidiameter_angular_size"
           ,"solar_cycle_number"]

__authors__ = ["Steven Christe"]
__email__ = "steven.d.christe@nasa.gov"

def solar_cycle_number(t=None):
    time = parse_time(t)
    result = (time.year + 8) % 28 + 1
    return result

def solar_semidiameter_angular_size(t=None):
    r"""
    Return the angular size of the semi-diameter of the Sun as 
    a function of time as viewed from Earth (in arcsec)
    
    .. math::
    
        Radius_{\odot}[rad]=\tan^{-1}\left(\frac{<Radius_{\odot}[m]>}{D_{\odot \oplus}(t)[m]}\right)

    since :math:`tan(x) \approx x` when :math:`x << 1`

    .. math::
    
        Radius_{\odot}[rad]=\frac{<Radius_{\odot}[m]>}{D_{\odot \oplus}(t)[m]}

    """
    solar_semidiameter_rad = constants.radius / (sunearth_distance(t) * constants.au)
    return np.rad2deg(solar_semidiameter_rad * u.degree) * 60. * 60.
 
def position(t=None):
    """Returns the position of the Sun (right ascension and declination)
    on the celestial sphere using the equatorial coordinate system in arcsec.
    """
    ra = true_rightascenscion(t)
    dec = true_declination(t)
    result = [ra,dec]
    return result
    
def eccentricity_SunEarth_orbit(t=None):
    """Returns the eccentricity of the Sun Earth Orbit."""
    T = julian_centuries(t)
    result = 0.016751040 - 0.00004180 * T - 0.0000001260 * T ** 2
    return result

def mean_ecliptic_longitude(t=None):
    """Returns the mean ecliptic longitude."""
    T = julian_centuries(t)
    result = 279.696680 + 36000.76892 * T + 0.0003025 * T ** 2
    result = result % 360.0
    return result

def longitude_Sun_perigee(t=None): # pylint: disable=W0613 
    # T = julian_centuries(t)
    return 1
    
def mean_anomaly(t=None):
    """Returns the mean anomaly (the angle through which the Sun has moved
    assuming a circular orbit) as a function of time."""
    T = julian_centuries(t)
    result = 358.475830 + 35999.049750 * T - 0.0001500 * T ** 2 - 0.00000330 * T ** 3
    result = result % 360.0
    return result

def carrington_rotation_number(t=None):
    """Return the Carrington Rotation number"""
    jd = julian_day(t)
    result = (1. / 27.2753) * (jd - 2398167.0) + 1.0
    return result

def geometric_mean_longitude(t=None):
    """Returns the geometric mean longitude (in degrees)"""   
    T = julian_centuries(t)
    result = 279.696680 + 36000.76892 * T + 0.0003025 * T ** 2
    result = result % 360.0
    return result
  
def equation_of_center(t=None):
    """Returns the Sun's equation of center (in degrees)"""
    T = julian_centuries(t)
    mna = mean_anomaly(t) 
    result = ((1.9194600 - 0.0047890 * T - 0.0000140 * T
    ** 2) * np.sin(np.radians(mna)) + (0.0200940 - 0.0001000 * T) *
    np.sin(np.radians(2 * mna)) + 0.0002930 * np.sin(np.radians(3 * mna)))
    return result

def true_longitude(t=None): 
    """Returns the Sun's true geometric longitude (in degrees) 
    (Refered to the mean equinox of date.  Question: Should the higher
    accuracy terms from which app_long is derived be added to true_long?)"""
    result = (equation_of_center(t) + geometric_mean_longitude(t)) % 360.0
    return result

def true_anomaly(t=None):
    """Returns the Sun's true anomaly (in degress)."""
    result = (mean_anomaly(t) + equation_of_center(t)) % 360.0
    return result

def sunearth_distance(t=None):
    """Returns the Sun Earth distance (AU). There are a set of higher 
    accuracy terms not included here."""  
    ta = true_anomaly(t)
    e = eccentricity_SunEarth_orbit(t)
    result = 1.00000020 * (1.0 - e ** 2) / (1.0 + e * np.cos(np.radians(ta)))
    return result

def apparent_longitude(t=None):
    """Returns the apparent longitude of the Sun."""
    T = julian_centuries(t)
    omega = 259.18 - 1934.142 * T
    true_long = true_longitude(t)        
    result = true_long - 0.00569 - 0.00479 * np.sin(np.radians(omega))
    return result

def true_latitude(t=None): # pylint: disable=W0613
    '''Returns the true latitude. Never more than 1.2 arcsec from 0, 
    set to 0 here.'''
    return 0.0

def apparent_latitude(t=None): # pylint: disable=W0613
    return 0

def true_obliquity_of_ecliptic(t=None):
    T = julian_centuries(t)
    result = 23.452294 - 0.0130125 * T - 0.00000164 * T ** 2 + 0.000000503 * T ** 3
    return result

def true_rightascenscion(t=None):
    true_long = true_longitude(t)
    ob = true_obliquity_of_ecliptic(t)
    result = np.cos(np.radians(ob)) * np.sin(np.radians(true_long))
    return result

def true_declination(t=None):
    result = np.cos(np.radians(true_longitude(t)))
    return result

def apparent_obliquity_of_ecliptic(t=None):
    omega = apparent_longitude(t)
    result = true_obliquity_of_ecliptic(t) + 0.00256 * np.cos(np.radians(omega))
    return result

def apparent_rightascenscion(t=None):
    """Returns the apparent right ascenscion of the Sun."""
    y = np.cos(np.radians(apparent_obliquity_of_ecliptic(t))) * np.sin(np.radians(apparent_longitude(t)))
    x = np.cos(np.radians(apparent_longitude(t)))
    rpol = np.rad2deg(np.arctan2(y, x))
    app_ra = rpol % 360.0
    if app_ra < 0: app_ra += 360.0
    result = app_ra/15.0
    return result

def apparent_declination(t=None):
    """Returns the apparent declination of the Sun."""
    ob = apparent_obliquity_of_ecliptic(t)
    app_long = apparent_longitude(t)
    result = np.degrees(np.arcsin(np.sin(np.radians(ob))) * np.sin(np.radians(app_long)))
    return result

def solar_north(t=None):
    """Returns the position of the Solar north pole in degrees."""
    T = julian_centuries(t)
    ob1 = true_obliquity_of_ecliptic(t)
    # in degrees
    i = 7.25
    k = 74.3646 + 1.395833 * T
    lamda = true_longitude(t) - 0.00569
    omega = apparent_longitude(t)
    lamda2 = lamda - 0.00479 * np.sin(np.radians(omega))
    diff = np.radians(lamda - k)
    x = np.degrees(np.arctan(-np.cos(np.radians(lamda2)*np.tan(np.radians(ob1)))))
    y = np.degrees(np.arctan(-np.cos(diff) * np.tan(np.radians(i))))
    result = x + y
    return result

def heliographic_solar_center(t=None):
    """Returns the position of the solar center in heliographic coordinates."""
    jd = julian_day(t)
    T = julian_centuries(t)
    # Heliographic coordinates in degrees
    theta = (jd - 2398220)*360/25.38
    i = 7.25
    k = 74.3646 + 1.395833 * T
    lamda = true_longitude(t) - 0.00569
    #omega = apparent_longitude(t)
    #lamda2 = lamda - 0.00479 * math.sin(np.radians(omega))
    diff = np.radians(lamda - k)
    # Latitude at center of disk (deg):    
    he_lat = np.degrees(np.arcsin(np.sin(diff)*np.sin(np.radians(i))))
    # Longitude at center of disk (deg):
    y = -np.sin(diff)*np.cos(np.radians(i))
    x = -np.cos(diff)
    rpol = np.rad2deg(np.arctan2(y, x))
    he_lon = rpol - theta
    he_lon = he_lon % 360
    if he_lon < 0:
        he_lon = he_lon + 360.0

    return [he_lon, he_lat]

def print_params(t=None):
    """Print out a summary of Solar ephemeris"""
    time = parse_time(t)
    print('Solar Ephemeris for ' + time.ctime())
    print('')
    print('Distance (AU) = ' + str(sunearth_distance(t)))
    print('Semidiameter (arc sec) = ' + str(solar_semidiameter_angular_size(t)))
    print('True (long,lat) in degrees = (' + str(true_longitude(t)) + ',' 
                                                 + str(true_latitude(t)) + ')')
    print('Apparent (long, lat) in degrees = (' + str(apparent_longitude(t)) + ',' 
                                                 + str(apparent_latitude(t)) + ')')
    print('True (RA, Dec) = (' + str(true_rightascenscion(t)) + ','
          + str(true_declination(t)))
    print('Apparent (RA, Dec) = (' + str(apparent_rightascenscion(t)) + ','
          + str(apparent_declination(t)))
    print('Heliographic long. and lat of disk center in deg = (' + str(heliographic_solar_center(t)) + ')')
    print('Position angle of north pole in deg = ' + str(solar_north(t)))
    print('Carrington Rotation Number = ' + str(carrington_rotation_number(t)))

########NEW FILE########
__FILENAME__ = test_sun
from __future__ import absolute_import

from sunpy.sun import sun
from numpy.testing import assert_array_almost_equal

def test_sunearth_distance():
    # Source for these values
    # wolframalpha.com
    # http://www.wolframalpha.com/input/?i=earth-sun+distance+on+2010%2F02%2F04
    assert_array_almost_equal(sun.sunearth_distance("2010/02/04"), 0.9858, decimal=3)
    assert_array_almost_equal(sun.sunearth_distance("2009/04/13"), 1.003, decimal=3)
    assert_array_almost_equal(sun.sunearth_distance("2008/06/20"), 1.016, decimal=3)
    assert_array_almost_equal(sun.sunearth_distance("2007/08/15"), 1.013, decimal=3)
    assert_array_almost_equal(sun.sunearth_distance("2007/10/02"), 1.001, decimal=3)
    assert_array_almost_equal(sun.sunearth_distance("2006/12/27"), 0.9834, decimal=3)

def test_true_longitude():
    # source: http://www.satellite-calculations.com/Satellite/suncalc.htm
    # values are deviating a little because of lack of time parameter in
    # true_longitude function
    assert_array_almost_equal(sun.true_longitude("2002/12/23"), 270.978, decimal=0)
    assert_array_almost_equal(sun.true_longitude("2003/01/29"), 308.661, decimal=0)
    assert_array_almost_equal(sun.true_longitude("2004/05/12"), 51.617, decimal=0)
    assert_array_almost_equal(sun.true_longitude("2006/07/04"), 101.910, decimal=0)
    assert_array_almost_equal(sun.true_longitude("2007/09/16"), 172.767, decimal=0)
    assert_array_almost_equal(sun.true_longitude("2009/02/11"), 322.394, decimal=0)

def test_apparent_declination():
    assert_array_almost_equal(sun.apparent_declination("2002/12/22"), -22.964, decimal=0)
    assert_array_almost_equal(sun.apparent_declination("2003/1/12"), -21.743, decimal=0)
    assert_array_almost_equal(sun.apparent_declination("2004/02/13"), -13.478, decimal=0)
    assert_array_almost_equal(sun.apparent_declination("2005/12/3"), -22.152, decimal=0)
    assert_array_almost_equal(sun.apparent_declination("2013/02/26"), -8.547, decimal=0)
    assert_array_almost_equal(sun.apparent_declination("2014/05/1"), 15.141, decimal=0)

def test_mean_anomaly():
    assert_array_almost_equal(sun.mean_anomaly("2002/12/12"), 337.538, decimal=0)
    assert_array_almost_equal(sun.mean_anomaly("2003/03/25"), 79.055, decimal=0)
    assert_array_almost_equal(sun.mean_anomaly("2005/06/05"), 150.492, decimal=0)
    assert_array_almost_equal(sun.mean_anomaly("2006/11/17"), 312.860, decimal=0)
    assert_array_almost_equal(sun.mean_anomaly("2008/07/29"), 203.933, decimal=0)
    assert_array_almost_equal(sun.mean_anomaly("2011/01/31"), 26.742, decimal=0)

########NEW FILE########
__FILENAME__ = _cgs
"""
CGS values of solar physics constants.
"""
from __future__ import absolute_import

import numpy as np
from . import _si

__all__ = ['physical_constants']

# This is really just to make the name shorter, so we can stick to a 
# maximum of 79 characters per line.
si_consts = _si.physical_constants

physical_constants = {}

physical_constants['mass'] = (si_consts['mass'][0] * 1.0e3, 'g', -1)
physical_constants['radius'] = (si_consts['radius'][0] * 1.0e2, 'cm', -1)
physical_constants['diameter'] = (physical_constants['radius'][0] * 2.0, 
                                  'cm', -1)
physical_constants['volume'] = (4 / 3. * np.pi *
                                physical_constants['radius'][0] ** 3, 'cm^3', 
                                -1)
physical_constants['surface area'] = (4 * np.pi *
                                      physical_constants['radius'][0] ** 2,
                                      'cm^2', -1)
physical_constants['average density'] = (physical_constants['mass'][0] / 
                                         physical_constants['volume'][0],
                                         'g cm^-3', -1)
physical_constants['center density'] = (si_consts['center density'][0] * 
                                        1.0e-3, 'g cm^-3', -1)
physical_constants['mean intensity'] = (si_consts['mean intensity'][0] * 
                                        1.0e3, 'erg cm^-2 sr^-1', -1)
physical_constants['effective temperature'] = si_consts['effective temperature']
physical_constants['center temperature'] = si_consts['center temperature']
physical_constants['luminosity'] = (si_consts['luminosity'][0] * 1.0e7, 
                                    'erg s^-1', -1)
physical_constants['absolute magnitude'] = si_consts['absolute magnitude']
physical_constants['visual magnitude'] = si_consts['visual magnitude']
physical_constants['mass conversion rate'] = (si_consts['mass conversion rate']
                                              [0] * 1.0e3, 'g s^-1', -1)
physical_constants['mean energy production'] = (
    si_consts['mean energy production'][0] * 1.0e4, 'erg g^-1', -1)
physical_constants['ellipticity'] = si_consts['ellipticity']
physical_constants['GM'] = (si_consts['GM'][0] * 1.0e6, 'cm^3 s^-2', -1)
physical_constants['surface gravity'] = (si_consts['surface gravity'][0] *
                                         1.0e-3, 'g cm^-3', -1)
physical_constants['escape velocity'] = (si_consts['escape velocity'][0] *
                                         1.0e5, 'cm s^-1', -1)
physical_constants['sunspot cycle'] = si_consts['sunspot cycle']
physical_constants['metallicity'] = si_consts['metallicity']

physical_constants['solar flux unit'] = (si_consts['solar flux unit'][0] *
                                         1.0e3, 'erg s^-1 cm^-2 Hz^-1', -1)
physical_constants['average_angular_size'] = si_consts['average_angular_size']

standard_model = _si.standard_model

########NEW FILE########
__FILENAME__ = _si
"""
Collection of solar physical constants.

The list is not meant to be comprehensive, but just a convenient list for 
everyday use.

.. todo:: Need better sources for some constants as well as error values.

"""

from __future__ import absolute_import

from astropy.constants import Constant
import astropy.constants as astrocon

__all__ = ['physical_constants']

physical_constants = {}

physical_constants['mass'] = astrocon.M_sun
physical_constants['radius'] = astrocon.R_sun
physical_constants['luminosity'] = astrocon.L_sun
physical_constants['mean distance'] = astrocon.au

# following needs error estimate if appropriate
physical_constants['perihelion distance'] = Constant('perihelion', "Perihelion Distance", 1.471e11, 'm', 0, 
                                     "Allen's Astrophysical Quantities 4th Ed.", system='si')

# following needs error estimate if appropriate
physical_constants['aphelion distance'] = Constant('aphelion', "Aphelion Distance", 1.521e11, 'm', 0, 
                                     "Allen's Astrophysical Quantities 4th Ed.", system='si')

physical_constants['age'] = Constant('age', "Age of the Sun", 4.6e9, 'year', 0.1e9, 
                                     "Allen's Astrophysical Quantities 4th Ed.", system='si')

# A solar flux  (sfu) is traditional measure of solar radio flux.
physical_constants['solar flux unit'] = Constant('sfu', "Solar flux unit", 1e-22, 
                                                 'W m**-2 Hz**-1', 0, "Allen's Astrophysical Quantities 4th Ed.", system='si')

# following needs error estimate if appropriate
physical_constants['visual magnitude'] = Constant('V', "Apparent visual magnitude", -26.75, 
                                                 '', 0, "Allen's Astrophysical Quantities 4th Ed.", system='si')

# The Sun as viewed from Earth
physical_constants['average angular size'] = Constant('theta', "Semidiameter", 959.63, 
                                                 'arcsec', 0, "Allen's Astrophysical Quantities 4th Ed.", system='si')

# following needs error estimate if appropriate
physical_constants['surface area'] = Constant('A', "Surface area", 6.087e18, 
                                                 'm**2', 0, "Allen's Astrophysical Quantities 4th Ed.", system='si')

# following needs error estimate if appropriate
physical_constants['average density'] = Constant('rho', "Mean density", 1409, 
                                                 'kg m**-3', 0, "Allen's Astrophysical Quantities 4th Ed.", system='si')

# following needs error estimate if appropriate
physical_constants['surface gravity'] = Constant('g', "Surface gravity", 274, 
                                                 'm s**-1', 0, "Allen's Astrophysical Quantities 4th Ed.", system='si')

# following needs error estimate if appropriate
physical_constants['moment of inertia'] = Constant('I', "Moment of inertia", 5.7e54, 
                                                 'kg m**-2', 0, "Allen's Astrophysical Quantities 4th Ed.", system='si')

# following needs error estimate if appropriate
physical_constants['volume'] = Constant('V', "Volume", 1.4122e27, 
                                                 'm**3', 0, "Allen's Astrophysical Quantities 4th Ed.", system='si')

# following needs error estimate if appropriate
physical_constants['escape velocity'] = Constant('v', "Escape velocity at surface", 6.177e5, 
                                                 'm s**-1', 0, "Allen's Astrophysical Quantities 4th Ed.", system='si')

physical_constants['oblateness'] = Constant('v', "oblateness", 8.01, 
                                                 'marcsec', 0.14, "Fivian et al. 2008", system='si')  

#the following constants need references and error estimates if appropriate
physical_constants['metallicity'] = Constant('Z', "Metallicity", 0.0122, '', 0.0, 
                                                 'Asplund et al. 2006', system='si')

physical_constants['sunspot cycle'] = Constant('v', "Average duration of sunspot cycle", 11.4, 
                                                 'year', 0, "", system='si')                                      

physical_constants['average intensity'] = Constant('I', "Mean Intensity", 2.009e7, 
                                                 'W m**-2 sr**-1', 0, "", system='si')                                      
        
physical_constants['effective temperature'] = Constant('T', "The effective black-body temperature of the Sun in Kelvin. ", 5778.0, 
                                                 'K', 0, "", system='si')

physical_constants['mass conversion rate'] = Constant('dm/dt', "Mass conversion rate", 4300e6, 
                                                 'kg s**-1', 0, "", system='si')
########NEW FILE########
__FILENAME__ = conftest
from functools import partial
import urllib2

import pytest

GOOGLE_URL = 'http://www.google.com'


def site_reachable(url):
    try:
        urllib2.urlopen(url, timeout=1)
    except urllib2.URLError:
        return False
    else:
        return True


is_online = partial(site_reachable, GOOGLE_URL)


def pytest_runtest_setup(item):
    """pytest hook to skip all tests that have the mark 'online' if the
    client is online (simply detected by checking whether http://www.google.com
    can be requested).

    """
    if isinstance(item, item.Function):
        if 'online' in item.keywords and not is_online():
            msg = 'skipping test {0} (reason: client seems to be offline)'
            pytest.skip(msg.format(item.name))

########NEW FILE########
__FILENAME__ = helpers
# -*- coding: utf-8 -*-
import warnings

import pytest

@pytest.fixture
def warnings_as_errors(request):
    warnings.simplefilter('error')

    request.addfinalizer(lambda *args: warnings.resetwarnings())
########NEW FILE########
__FILENAME__ = test_main
import os.path

import pytest

import sunpy.tests

root_dir = os.path.dirname(os.path.abspath(sunpy.__file__))

def test_main_nonexisting_module():
    with pytest.raises(ImportError):
        sunpy.tests.main('doesnotexist')


def test_main_stdlib_module():
    """This test makes sure that the module is really searched within the
    sunpy package.

    """
    with pytest.raises(ImportError):
        sunpy.tests.main('random')


def test_main_noargs(monkeypatch):
    monkeypatch.setattr(pytest, 'main', lambda x: x)
    args = sunpy.tests.main()
    assert args == [root_dir]


def test_main_submodule(monkeypatch):
    monkeypatch.setattr(pytest, 'main', lambda x: x)
    args = sunpy.tests.main('map')
    assert args == [os.path.join(root_dir, 'map', 'tests')]


def test_main_with_cover(monkeypatch):
    monkeypatch.setattr(pytest, 'main', lambda x: x)
    args = sunpy.tests.main('map', cover=True)
    covpath = os.path.abspath(
        os.path.join(sunpy.tests.testdir, os.path.join(os.pardir, 'map')))
    assert args == ['--cov', covpath, os.path.join(root_dir, 'map', 'tests')]


def test_main_with_show_uncovered_lines(monkeypatch):
    monkeypatch.setattr(pytest, 'main', lambda x: x)
    args = sunpy.tests.main('map', show_uncovered_lines=True)
    assert args == [
        '--cov-report', 'term-missing',
        os.path.join(root_dir, 'map', 'tests')]


def test_main_exclude_online(monkeypatch):
    monkeypatch.setattr(pytest, 'main', lambda x: x)
    args = sunpy.tests.main('map', online=sunpy.tests.EXCLUDE_ONLINE)
    assert args == ['-k-online', os.path.join(root_dir, 'map', 'tests')]


def test_main_only_online(monkeypatch):
    monkeypatch.setattr(pytest, 'main', lambda x: x)
    args = sunpy.tests.main('map', online=sunpy.tests.ONLY_ONLINE)
    assert args == ['-k', 'online', os.path.join(root_dir, 'map', 'tests')]


def test_main_invalid_online_parameter():
    with pytest.raises(ValueError) as excinfo:
        sunpy.tests.main(online='blabla')
    assert excinfo.exconly() == (
        'ValueError: `online` parameter must have one of the following '
        'values: sunpy.tests.INCLUDE_ONLINE, sunpy.tests.EXCLUDE_ONLINE, '
        'sunpy.tests.ONLY_ONLINE')

########NEW FILE########
__FILENAME__ = julian
from __future__ import absolute_import

from sunpy.time import parse_time

__all__ = ['julian_day', 'julian_centuries']

# The number of days between Jan 1 1900 and the Julian reference date of 
# 12:00 noon Jan 1, 4713 BC
JULIAN_DAY_ON_NOON01JAN1900 = 2415021.0

def julian_day(t=None):
    """Returns the (fractional) Julian day defined as the number of days 
    between the queried day and the reference date of 12:00 (noon) Jan 1, 4713 
    BC."""

    # Good online reference for fractional julian day
    # http://www.stevegs.com/jd_calc/jd_calc.htm    
    JULIAN_REF_DAY = parse_time('1900/1/1 12:00:00')
    time = parse_time(t)
    
    tdiff = time - JULIAN_REF_DAY
    
    julian = tdiff.days + JULIAN_DAY_ON_NOON01JAN1900
   
    result = julian + 1 / 24. * (time.hour + time.minute / 60.0 + 
                                 time.second / (60. * 60.))

    # This is because the days in datetime objects start at 00:00, 
    # not 12:00 as for Julian days.
    if time.hour >= 12:
        result = result - 0.5
    else:
        result = result + 0.5

    return result

def julian_centuries(t=None):
    """Returns the number of Julian centuries since 1900 January 0.5."""
    DAYS_IN_YEAR = 36525.0

    return (julian_day(t) - JULIAN_DAY_ON_NOON01JAN1900) / DAYS_IN_YEAR

########NEW FILE########
__FILENAME__ = test_julian
from __future__ import absolute_import

from datetime import datetime

from numpy.testing import assert_almost_equal
import pytest

from sunpy.time import julian

DATETIME_DATE_1 = datetime(1900, 1, 1, 12, 00, 00)
STRING_DATE_1 = '1900/1/1 12:00:00'

DATETIME_DATE_2 = datetime(1984, 11, 23, 14, 21, 05)
STRING_DATE_2 = '1984/11/23 14:21:05'

DATETIME_DATE_3 = datetime(2174, 2, 11, 05, 02, 00)
STRING_DATE_3 = '2174/02/11 05:02:00'

DATETIME_DATE_4 = datetime(814, 1, 28, 23, 59, 59)
STRING_DATE_4 = '0814/01/28 23:59:59'

LANDING = datetime(1966, 2, 3)

def test__all__():
    """should return __all__"""

    assert julian.__all__ == ["julian_day", "julian_centuries"]
    
def test_constant():
    """should return JULIAN_DAY_ON_NOON01JAN1900"""

    assert julian.JULIAN_DAY_ON_NOON01JAN1900 == 2415021.0


def test_julian_day():
    assert julian.julian_day('1900-01-01 12:00') == 2415021.0
    assert julian.julian_day(LANDING) == 2439159.5
    result = julian.julian_day('2000-03-01 15:30:26')
    assert_almost_equal(result, 2451605.1461111, decimal=3)

def test_julian_day1():
    """should return julian day for date 1"""
    expected_day = julian.JULIAN_DAY_ON_NOON01JAN1900
    assert julian.julian_day(DATETIME_DATE_1) == expected_day
    assert julian.julian_day(STRING_DATE_1) == expected_day

def test_julian_day2():
    """should return julian day for date 2"""
    
    expected_day = 2446028.097974537
    assert julian.julian_day(DATETIME_DATE_2) == expected_day
    assert julian.julian_day(STRING_DATE_2) == expected_day

def test_julian_day3():
    """should return julian day for date 3"""
    
    expected_day = 2515138.7097222223
    assert julian.julian_day(DATETIME_DATE_3) == expected_day
    assert julian.julian_day(STRING_DATE_3) == expected_day

def test_julian_day4():
    """should return julian day for date 4"""
    
    expected_day = 2018395.499988426
    assert julian.julian_day(DATETIME_DATE_4) == expected_day
    assert julian.julian_day(STRING_DATE_4) == expected_day
    
def test_julian_day5():
    """should raise value error when passed empty string"""
    
    pytest.raises(ValueError, julian.julian_day, '')
    
def test_julian_day6():
    """should raise value error when passed non-date string"""
    
    pytest.raises(ValueError, julian.julian_day, 'A lovely bunch of coconuts')
    
def test_julian_centuries1():
    """should return julian century for date 1"""
    
    expected_century = 0.0
    assert julian.julian_centuries(DATETIME_DATE_1) == expected_century
    assert julian.julian_centuries(STRING_DATE_1) == expected_century
    
def test_julian_centuries2():
    """should return julian century for date 2"""
    
    expected_century = 0.8489280759626815
    assert julian.julian_centuries(DATETIME_DATE_2) == expected_century
    assert julian.julian_centuries(STRING_DATE_2) == expected_century
    
def test_julian_centuries3():
    """should return julian century for date 3"""
    
    expected_century = 2.7410735036884954
    assert julian.julian_centuries(DATETIME_DATE_3) == expected_century
    assert julian.julian_centuries(STRING_DATE_3) == expected_century
    
def test_julian_centuries4():
    """should return julian century for date 4"""
    
    expected_century = -10.859014374033512
    assert julian.julian_centuries(DATETIME_DATE_4) == expected_century
    assert julian.julian_centuries(STRING_DATE_4) == expected_century

    
def test_julian_centuries5():
    """should raise value error when passed empty string"""
    
    pytest.raises(ValueError, julian.julian_centuries, '')
    
def test_julian_centuries6():
    """should raise value error when passed non-date string"""
    
    pytest.raises(ValueError, julian.julian_centuries, 'Are you suggesting coconuts migrate?')
    
########NEW FILE########
__FILENAME__ = test_time
from __future__ import absolute_import

from datetime import datetime

from sunpy import time
from sunpy.time import parse_time

LANDING = datetime(1966, 2, 3)

def test_parse_time_24():
    assert parse_time("2010-10-10T24:00:00") == datetime(2010, 10, 11)

def test_parse_time_24_2():
    assert parse_time("2010-10-10T24:00:00.000000") == datetime(2010, 10, 11)

def test_parse_time_trailing_zeros():
    # see issue #289 at https://github.com/sunpy/sunpy/issues/289
    assert parse_time('2010-10-10T00:00:00.00000000') == datetime(2010, 10, 10)

def test_parse_time_tuple():
    assert parse_time((1966, 2, 3)) == LANDING

def test_parse_time_int():
    assert parse_time(765548612.0) == datetime(2003, 4, 5, 12, 23, 32)
    assert parse_time(1009685652.0) == datetime(2010, 12, 30, 4, 14, 12)

def test_parse_time_ISO():
    assert parse_time('1966-02-03') == LANDING
    assert (
        parse_time('1966-02-03T20:17:40') == datetime(1966, 2, 3, 20, 17, 40)
    )
    assert (
        parse_time('19660203T201740') == datetime(1966, 2, 3, 20, 17, 40)
    )
    
    lst = [
        ('2007-05-04T21:08:12.999999', datetime(2007, 5, 4, 21, 8, 12, 999999)),
        ('20070504T210812.999999', datetime(2007, 5, 4, 21, 8, 12, 999999)),
        ('2007/05/04 21:08:12.999999', datetime(2007, 5, 4, 21, 8, 12, 999999)),
        ('2007-05-04 21:08:12.999999', datetime(2007, 5, 4, 21, 8, 12, 999999)),
        ('2007/05/04 21:08:12', datetime(2007, 5, 4, 21, 8, 12)),
        ('2007-05-04 21:08:12', datetime(2007, 5, 4, 21, 8, 12)),
        ('2007-05-04 21:08', datetime(2007, 5, 4, 21, 8)),
        ('2007-05-04T21:08:12', datetime(2007, 5, 4, 21, 8, 12)),
        ('20070504T210812', datetime(2007, 5, 4, 21, 8, 12)),
        ('2007-May-04 21:08:12', datetime(2007, 5, 4, 21, 8, 12)),
        ('2007-May-04 21:08', datetime(2007, 5, 4, 21, 8)),
        ('2007-May-04', datetime(2007, 5, 4)),
        ('2007-05-04', datetime(2007, 5, 4)),
        ('2007/05/04', datetime(2007, 5, 4)),
        ('04-May-2007', datetime(2007, 5, 4)),
        ('20070504_210812', datetime(2007, 5, 4, 21, 8, 12))
    ]
    
    for k, v in lst:
        assert parse_time(k) == v
    
def test_break_time():
    assert time.break_time(datetime(2007, 5, 4, 21, 8, 12)) == '20070504_210812'

def test_day_of_year():
    # Note that 2012 is a leap year, 2011 is a standard year
    # test that it starts at 1
    assert time.day_of_year('2011/01/01') == 1.0
    # test fractional day
    assert time.day_of_year('2011/01/01 06:00') == 1.25
    assert time.day_of_year('2011/01/01 12:00') == 1.50
    assert time.day_of_year('2011/01/01 18:00') == 1.75
    # test correct number of days in a (standard) year
    assert time.day_of_year('2011/12/31') == 365
    # test correct number of days in a (leap) year
    assert time.day_of_year('2012/12/31') == 366
    # test a few extra dates in standard year
    assert time.day_of_year('2011/08/01') == 213
    assert time.day_of_year('2011/04/10') == 100
    assert time.day_of_year('2011/01/31') == 31
    assert time.day_of_year('2011/09/30') == 273
    # test a few extra dates in a leap year
    assert time.day_of_year('2012/08/01') == 214
    assert time.day_of_year('2012/04/10') == 101
    assert time.day_of_year('2012/01/31') == 31
    assert time.day_of_year('2012/09/30') == 274
    
    
    

########NEW FILE########
__FILENAME__ = test_timerange
# -*- coding: utf-8 -*-

from __future__ import absolute_import

import datetime

import pytest

import sunpy.time

start = datetime.datetime(year=2012, month=1, day=1)
end = datetime.datetime(year=2012, month=1, day=2)
delta = end - start

@pytest.mark.parametrize("inputs", [
    ('2012/1/1','2012/1/2'),
    ('2012/1/1',24*60*60),
    ('2012/1/1',datetime.timedelta(days=1))
])
def test_timerange_inputs(inputs):
    timerange = sunpy.time.TimeRange(*inputs)
    assert isinstance(timerange, sunpy.time.TimeRange)
    assert timerange.t1 == start
    assert timerange.t2 == end
    assert timerange.dt == delta

@pytest.mark.parametrize("ainput", [
    ('2012/1/1','2012/1/2'),
    ('2012/1/1',24*60*60),
    ('2012/1/1',datetime.timedelta(days=1)),
    (sunpy.time.TimeRange('2012/1/1','2012/1/2'))
])
def test_timerange_input(ainput):
    timerange = sunpy.time.TimeRange(ainput)
    assert isinstance(timerange, sunpy.time.TimeRange)
    assert timerange.t1 == start
    assert timerange.t2 == end
    assert timerange.dt == delta

def test_center():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    assert timerange.center() == datetime.datetime(year=2012,day=1,month=1,hour=12)

def test_split():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    expect = [sunpy.time.TimeRange('2012/1/1T00:00:00','2012/1/1T12:00:00'),
              sunpy.time.TimeRange('2012/1/1T12:00:00','2012/1/2T00:00:00')]
    split = timerange.split(n=2)
    #Doing direct comparisons seem to not work
    assert all([wi.t1 == ex.t1 and wi.t2 == ex.t2 for wi, ex in zip(split, expect)])

def test_split_n_0():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    with pytest.raises(ValueError):
        timerange.split(n=0)

def test_window():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    window = timerange.window(12*60*60, 10)
    expect = [sunpy.time.TimeRange('2012/1/1T00:00:00','2012/1/1T00:00:10'),
              sunpy.time.TimeRange('2012/1/1T12:00:00','2012/1/1T12:00:10'),
              sunpy.time.TimeRange('2012/1/2T00:00:00','2012/1/2T00:00:10')]
    assert isinstance(window, list)
    #Doing direct comparisons seem to not work
    assert all([wi.t1 == ex.t1 and wi.t2 == ex.t2 for wi, ex in zip(window, expect)])

def test_window_timedelta():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    window = timerange.window(datetime.timedelta(hours=12), datetime.timedelta(seconds=10))
    expect = [sunpy.time.TimeRange('2012/1/1T00:00:00','2012/1/1T00:00:10'),
              sunpy.time.TimeRange('2012/1/1T12:00:00','2012/1/1T12:00:10'),
              sunpy.time.TimeRange('2012/1/2T00:00:00','2012/1/2T00:00:10')]
    assert isinstance(window, list)
    #Doing direct comparisons seem to not work
    assert all([wi.t1 == ex.t1 and wi.t2 == ex.t2 for wi, ex in zip(window, expect)])

def test_days():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    assert timerange.days() == 1

def test_start():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    assert timerange.start() == start

def test_end():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    assert timerange.end() == end

def test_seconds():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    assert timerange.seconds() == 24*60*60

def test_minutes():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    assert timerange.minutes() == 24*60

def test_next():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    timerange.next()
    assert isinstance(timerange, sunpy.time.TimeRange)
    assert timerange.t1 == start + delta
    assert timerange.t2 == end + delta
    assert timerange.dt == delta

def test_previous():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    timerange.previous()
    assert isinstance(timerange, sunpy.time.TimeRange)
    assert timerange.t1 == start - delta
    assert timerange.t2 == end - delta
    assert timerange.dt == delta

def test_extend():
    timerange = sunpy.time.TimeRange('2012/1/1','2012/1/2')
    timerange.extend(delta, delta)
    assert isinstance(timerange, sunpy.time.TimeRange)
    assert timerange.t1 == start + delta
    assert timerange.t2 == end + delta
    assert timerange.dt == delta
    
def test_contains():
    before = datetime.datetime(year=1990, month=1, day=1)
    after = datetime.datetime(year=2022, month=1, day=1)
    between = datetime.datetime(year=2014, month=5, day=4)
    timerange = sunpy.time.TimeRange('2014/05/03 12:00', '2014/05/05 21:00')
    assert between in timerange
    assert before not in timerange
    assert after not in timerange
    assert timerange.t1 in timerange
    assert timerange.t2 in timerange
    assert '2014/05/04 15:21' in timerange
    assert '1975/4/13' not in timerange
    assert '2100/1/1'not in timerange
    assert '2014/05/03 12:00' in timerange
    assert '2014/05/05 21:00' in timerange
########NEW FILE########
__FILENAME__ = time
import re
from datetime import datetime
from datetime import timedelta

__all__ = ['find_time', 'extract_time', 'parse_time', 'is_time', 'day_of_year', 'break_time', 'get_day', 'is_time_in_given_format']

# Mapping of time format codes to regular expressions.
REGEX = {
    '%Y': '(?P<year>\d{4})',
    '%j': '(?P<dayofyear>\d{3})',
    '%m': '(?P<month>\d{1,2})',
    '%d': '(?P<day>\d{1,2})',
    '%H': '(?P<hour>\d{1,2})',
    '%M': '(?P<minute>\d{1,2})',
    '%S': '(?P<second>\d{1,2})',
    '%f': '(?P<microsecond>\d+)',
    '%b': '(?P<month_str>[a-zA-Z]+)',
}

TIME_FORMAT_LIST = [
    "%Y-%m-%dT%H:%M:%S.%f",    # Example 2007-05-04T21:08:12.999999
    "%Y/%m/%dT%H:%M:%S.%f",    # Example 2007/05/04T21:08:12.999999
    "%Y-%m-%dT%H:%M:%S.%fZ",   # Example 2007-05-04T21:08:12.999Z
    "%Y-%m-%dT%H:%M:%S",       # Example 2007-05-04T21:08:12
    "%Y/%m/%dT%H:%M:%S",       # Example 2007/05/04T21:08:12
    "%Y%m%dT%H%M%S.%f",        # Example 20070504T210812.999999
    "%Y%m%dT%H%M%S",           # Example 20070504T210812
    "%Y/%m/%d %H:%M:%S",       # Example 2007/05/04 21:08:12
    "%Y/%m/%d %H:%M",          # Example 2007/05/04 21:08
    "%Y/%m/%d %H:%M:%S.%f",    # Example 2007/05/04 21:08:12.999999
    "%Y-%m-%d %H:%M:%S.%f",    # Example 2007-05-04 21:08:12.999999
    "%Y-%m-%d %H:%M:%S",       # Example 2007-05-04 21:08:12
    "%Y-%m-%d %H:%M",          # Example 2007-05-04 21:08
    "%Y-%b-%d %H:%M:%S",       # Example 2007-May-04 21:08:12
    "%Y-%b-%d %H:%M",          # Example 2007-May-04 21:08
    "%Y-%b-%d",                # Example 2007-May-04
    "%Y-%m-%d",                # Example 2007-05-04
    "%Y/%m/%d",                # Example 2007/05/04
    "%d-%b-%Y",                # Example 04-May-2007
    "%Y%m%d_%H%M%S",           # Example 20070504_210812
    "%Y:%j:%H:%M:%S",          # Example 2012:124:21:08:12
    "%Y:%j:%H:%M:%S.%f",       # Example 2012:124:21:08:12.999999
]


def _group_or_none(match, group, fun):
    try:
        ret = match.group(group)
    except IndexError:
        return None
    else:
        return fun(ret)

def _n_or_eq(a, b):
    return a is None or a == b

def _regex_parse_time(inp, format):
    # Parser for finding out the minute value so we can adjust the string
    # from 24:00:00 to 00:00:00 the next day because strptime does not
    # understand the former.
    for key, value in REGEX.iteritems():
        format = format.replace(key, value)
    match = re.match(format, inp)
    if match is None:
        return None, None
    try:
        hour = match.group("hour")
    except IndexError:
        return inp, timedelta(days=0)
    if match.group("hour") == "24":
        if not all(_n_or_eq(_group_or_none(match, g, int), 00)
            for g in ["minute", "second", "microsecond"]
        ):
            raise ValueError
        from_, to = match.span("hour")
        return inp[:from_] + "00" + inp[to:], timedelta(days=1)
    return inp, timedelta(days=0)


def find_time(string, format):
    """ Return iterator of occurences of date formatted with format
    in string. Currently supported format codes: """
    re_format = format
    for key, value in REGEX.iteritems():
        re_format = re_format.replace(key, value)
    matches = re.finditer(re_format, string)
    for match in matches:
        try:
            matchstr = string[slice(*match.span())]
            dt = datetime.strptime(matchstr, format)
        except ValueError:
            continue
        else:
            yield dt


find_time.__doc__ += ', '.join(REGEX.keys())

def _iter_empty(iter):
    try:
        iter.next()
    except StopIteration:
        return True
    return False


def extract_time(string):
    """ Find subset of string that corresponds to a datetime and return
    its value as a a datetime. If more than one or none is found, raise
    ValueError. """
    matched = None
    bestmatch = None
    for time_format in TIME_FORMAT_LIST:
        found = find_time(string, time_format)
        try:
            match = found.next()
        except StopIteration:
            continue
        else:
            if matched is not None:
                if time_format.startswith(matched):
                    # Already matched is a substring of the one just matched.
                    matched = time_format
                    bestmatch = match
                elif not matched.startswith(time_format):
                    # If just matched is substring of time_format, just ignore
                    # just matched.
                    raise ValueError("Ambiguous string")
            else:
                matched = time_format
                bestmatch = match
            if not _iter_empty(found):
                raise ValueError("Ambiguous string")
    if not matched:
        raise ValueError("Time not found")
    return bestmatch


def parse_time(time_string):
    """Given a time string will parse and return a datetime object.
    Similar to the anytim function in IDL.

    Parameters
    ----------
    time_string : string
        Datestring to parse

    Returns
    -------
    out : datetime
        DateTime corresponding to input date string

    Examples
    --------
    >>> sunpy.time.parse_time('2012/08/01')
    >>> sunpy.time.parse_time('2005-08-04T00:01:02.000Z')

    .. todo::
        add ability to parse tai (International Atomic Time seconds since
        Jan 1, 1958)
    """
    if isinstance(time_string, datetime):
        return time_string
    elif isinstance(time_string, tuple):
        return datetime(*time_string)
    elif isinstance(time_string, int) or isinstance(time_string, float):
        return datetime(1979, 1, 1) + timedelta(0, time_string)
    else:
        # remove trailing zeros and the final dot to allow any
        # number of zeros. This solves issue #289
        if '.' in time_string:
            time_string = time_string.rstrip("0").rstrip(".")
        for time_format in TIME_FORMAT_LIST: 
            try:
                try:
                    ts, time_delta = _regex_parse_time(time_string, time_format)
                except TypeError:
                    break
                if ts is None:
                    continue
                return datetime.strptime(ts, time_format) + time_delta
            except ValueError:
                pass
    
        raise ValueError("%s is not a valid time string!" % time_string)
    

def is_time(time_string):
    """
    Returns true if the input is a valid date/time representation
    
    Parameters
    ----------
    time_string : string
        Datestring to parse

    Returns
    -------
    out : bool
        True if can be parsed by parse_time

    Examples
    --------
    >>> sunpy.time.parse_time('2012/08/01')
    >>> sunpy.time.parse_time('2005-08-04T00:01:02.000Z')

    .. todo::
    
        add ability to parse tai (International Atomic Time seconds since Jan 1, 1958)
    
    """
    if time_string is None:
        return False
    elif isinstance(time_string, datetime):
        return True

    try:
        parse_time(time_string)
    except ValueError:
        return False
    else:
        return True


def day_of_year(time_string):
    """Returns the (fractional) day of year.
        
    Parameters
    ----------
    time_string : string
        A parse_time compatible string

    Returns
    -------
    out : float
        The fractional day of year (where Jan 1st is 1).

    Examples
    --------
    >>> sunpy.time.day_of_year('2012/01/01')
    1.00
    >>> sunpy.time.day_of_year('2012/08/01')
    214.00
    >>> sunpy.time.day_of_year('2005-08-04T00:18:02.000Z')
    216.01252314814815

    """
    SECONDS_IN_DAY = 60 * 60 * 24.0
    time = parse_time(time_string)
    time_diff = time - datetime(time.year, 1, 1, 0, 0, 0)
    return time_diff.days + time_diff.seconds / SECONDS_IN_DAY + 1

def break_time(t=None):
    """Given a time returns a string. Useful for naming files."""
    #TODO: should be able to handle a time range
    return parse_time(t).strftime("%Y%m%d_%H%M%S")

def get_day(dt):
    """ Return datetime for the beginning of the day of given datetime. """
    return datetime(dt.year, dt.month, dt.day)

def is_time_in_given_format(time_string, time_format):
    """Tests whether a time string is formatted according to the given time format."""
    try:
        datetime.strptime(time_string, time_format)
        return True
    except ValueError:
        return False

########NEW FILE########
__FILENAME__ = timerange
from __future__ import absolute_import

from datetime import timedelta
from datetime import datetime

from sunpy.time import parse_time

__all__ = ['TimeRange']


class TimeRange:
    """
    Timerange(a, b) or Timerange((a, b))

    An object to handle time ranges.

    Parameters
    ----------
    a : the start time specified as a time string, or datetime object
        A 2d list or ndarray containing the map data
    b : the end time specified as a time string or datetime object
        or the length of the time range specified as a timedelta object, or
        number of seconds

    Attributes
    ----------
    t1 : datetime
        The start time of the time range
    t2 : datetime
        The end time of the time range
    dt : timediff
        The difference in time between the start time and end time

    Examples
    --------
    >>> time_range = TimeRange('2010/03/04 00:10', '2010/03/04 00:20')

    >>> time_range = TimeRange('2010/03/04 00:10', 400)

    References
    ----------
    | http://docs.scipy.org/doc/numpy/reference/arrays.classes.html

    """
    def __init__(self, a, b=None):
        """Creates a new TimeRange instance"""
        # If a is a TimeRange object, copy attributes to new instance.
        if isinstance(a, TimeRange):
            self.__dict__ = a.__dict__.copy()
            return

        # Normalize different input types
        if b is None:
            x = a[0]
            y = a[1]
        else:
            x = a
            y = b

        # Start time
        self.t1 = parse_time(x)

        # End date
        if isinstance(y, str):
            self.t2 = parse_time(y)

        # Datetime
        if isinstance(y, datetime):
            self.t2 = y

        # Timedelta
        if isinstance(y, timedelta):
            self.t2 = self.t1 + y

        # Seconds offset
        if isinstance(y, (float, int)):
            self.t2 = self.t1 + timedelta(0, y)

        self.dt = self.t2 - self.t1

    def __repr__(self):
        """
        Returns a human-readable representation of the TimeRange instance."""
        TIME_FORMAT = "%Y/%m/%d %H:%M:%S"

        t1 = self.t1.strftime(TIME_FORMAT)
        t2 = self.t2.strftime(TIME_FORMAT)
        center = self.center().strftime(TIME_FORMAT)

        return ('    Start:'.ljust(11) + t1 +
                '\n    End:'.ljust(12) + t2 +
                '\n    Center:'.ljust(12) + center +
                '\n    Duration:'.ljust(12) + str(self.days()) + ' days or' +
                '\n    '.ljust(12) + str(self.minutes()) + ' minutes or' +
                '\n    '.ljust(12) + str(self.seconds()) + ' seconds' +
                '\n')

    def center(self):
        """Gets the center of the TimeRange instance"""
        return self.t1 + self.dt / 2

    def split(self, n=2):
        """Splits the TimeRange into multiple equally sized parts

        Accepts a value greater than or equal to 1 as input, and
        returns an array of equally sized TimeRange objects between
        t1 and t2.

        Raises a ValueError if requested amount is less than 1

        """

        if n <= 0:
            raise ValueError('n must be greater than or equal to 1')
        subsections = []
        previous_time = self.start()
        next_time = None
        for _ in range(n):
            next_time = previous_time + self.dt/n
            next_range = TimeRange(previous_time, next_time)
            subsections.append(next_range)
            previous_time = next_time
        return subsections

    def window(self, cadence, window):
        """
        Split the TimeRange up into a series of TimeRange windows,
        'window' long, between the start and end with a cadence of 'cadence'.

        Parameters
        ----------
        cadence: int or timedelta
            Cadence in seconds or a timedelta instance
        window: int or timedelta
            The length of the Time's, assumed to be seconds if int.

        Returns
        -------
        times: list
            A list of TimeRange objects, that are window long and seperated by
            cadence.

        Examples
        --------
        To get one 12 second long window every hour within the timerange:

        >>> TimeRange.window(60*60, window=12)
        """
        if not isinstance(window, timedelta):
            window = timedelta(seconds=window)
        if not isinstance(cadence, timedelta):
            cadence = timedelta(seconds=cadence)

        n = 1
        times = [TimeRange(self.t1, self.t1 + window)]
        while times[-1].t2 < self.t2:
            times.append(TimeRange(self.t1 + cadence*n,
                                   self.t1 + cadence*n + window))
            n += 1
        return times

    def days(self):
        """Gets the number of days elapsed."""
        return self.dt.days

    def start(self):
        """Gets the start date"""
        return self.t1

    def end(self):
        """Gets the start date"""
        return self.t2

    def seconds(self):
        """Gets the number of seconds elapsed."""
        return (self.dt.microseconds +
               (self.dt.seconds + self.dt.days * 24 * 3600) * 1e6) / 1e6

    def minutes(self):
        """Gets the number of minutes elapsed."""
        return self.seconds() / 60.0

    def next(self):
        """Shift the time range forward by the amount of time elapsed"""
        self.t1 = self.t1 + self.dt
        self.t2 = self.t2 + self.dt

        return self

    def previous(self):
        """Shift the time range backward by the amount of time elapsed"""
        self.t1 = self.t1 - self.dt
        self.t2 = self.t2 - self.dt

        return self

    def extend(self, t_backwards, t_forwards):
        """Extend the time range forwards and backwards by arbitrary amounts"""
        # Only a timedelta object is acceptable here
        self.t1 = self.t1 + t_backwards
        self.t2 = self.t2 + t_forwards

    def __contains__(self, time):
        """
        Checks whether the given time lies within this range.
        Both limits are inclusive (i.e. __contains__(t1) and __contains__(t2)
        always return true)

        Parameters
        ----------
        time: datetime or str
            The time to be checked

        Returns
        -------
        true if time lies between t1 and t2, false otherwise.

        Example
        -------
        >>> time_range = TimeRange('2014/05/04 13:54', '2018/02/03 12:12')
        >>> time in time_range
        """
        t = parse_time(time)
        return t >= self.t1 and t <= self.t2

########NEW FILE########
__FILENAME__ = cond_dispatch
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

""" Offer a callable object that dispatches based on arbitrary conditions
and function signature. That means, whenever it is called, it finds the
registered methods that match the input's signature and then checks for
user-defined conditions and types. 

First, we need to create a new ConditionalDispatch

>>> fun = ConditionalDispatch()

We then can start adding branches, in this case we add a branch for 
even integers, in which case the function applied is a muliplication by
three.

>>> fun.add(lambda x: 3 * x, lambda x: x % 2 == 0, [int])

By adding the other branch (odd), the function can be used for all integers.
In the case of an odd integer, we double the input. Please note that the system
has no way of verifying the conditions are mutually exclusive. In some cases
it can even be useful to use not mutually exclusive conditions, in which case
the branch that was added the earliest is executed.

>>> fun.add(lambda x: 2 * x, lambda x: x % 2 == 1, [int])

We can verify this is working.

>>> fun(2)
6
>>> fun(3)
6

And that using a float, e.g., does not.

>>> fun(3.2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/florian/Projects/sunpy/sunpy/util/cond_dispatch.py", line 128, in __call__
    "There are no functions matching your input parameter "
TypeError: There are no functions matching your input parameter signature.


We can then add a branch for floats, giving the condition None that means
that this branch is always executed for floats.

>>> fun.add(lambda y: 5 * y, None, [float])

Also note that the float branch takes y, while the integer branch takes x.
Thus, if the user explicitely passes fun(x=1) using a keyword argument, only
the integer branch is considered. This can be useful if the user wants
control over which kind of data they are passing the the function.

>>> fun(2.0)
10.0
>>> fun(y=2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/florian/Projects/sunpy/sunpy/util/cond_dispatch.py", line 128, in __call__
    "There are no functions matching your input parameter "
TypeError: There are no functions matching your input parameter signature.
>>> fun(y=2.5)
12.5
>>> fun(x=2)
6
>>> fun(x=2.5)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/florian/Projects/sunpy/sunpy/util/cond_dispatch.py", line 128, in __call__
    "There are no functions matching your input parameter "
TypeError: There are no functions matching your input parameter signature.
"""

from __future__ import absolute_import

import inspect

from itertools import izip, chain, repeat

__all__ = ['run_cls', 'matches_types', 'arginize', 'correct_argspec',
           'matches_signature', 'ConditionalDispatch', 'fmt_argspec_types']


def run_cls(name):
    """ run_cls("foo")(cls, *args, **kwargs) -> cls.foo(*args, **kwargs) """
    fun = lambda cls, *args, **kwargs: getattr(cls, name)(*args, **kwargs)
    fun.__name__ = name
    fun.run_cls = True
    return fun    


def matches_types(fun, types, args, kwargs):
    """ See if args and kwargs match are instances of types. types are given
    in the order they are defined in the function. kwargs are automatically
    converted into that order. """
    return all(
        isinstance(obj, cls) for obj, cls in izip(
            arginize(fun, args, kwargs), types
        )
    )


def arginize(fun, a, kw):
    """ Turn args and kwargs into args by considering the function
    signature. """
    args, varargs, keywords, defaults = correct_argspec(fun)
    if varargs is not None:
        raise ValueError
    names = args[len(a):]
    if defaults:
        defs = dict(izip(args[-len(defaults):], defaults))
    else:
        defs = {}
    return list(a) + [kw.get(name, defs.get(name, None)) for name in names]


def correct_argspec(fun):
    """ Remove first argument if method is bound. """
    args, varargs, keywords, defaults = inspect.getargspec(fun)
    if inspect.ismethod(fun):
        args = args[1:]
    return args, varargs, keywords, defaults 


def matches_signature(fun, a, kw):
    """ Check whether function can be called with a as args and kw as kwargs.
    """
    args, varargs, keywords, defaults = correct_argspec(fun)
    if varargs is None and len(a) > len(args):
        return False
    skw = set(kw)
    sargs = set(args[len(a):])
    
    # There mayn't be unexpected parameters unless there is a **kwargs
    # in fun's signature.
    if keywords is None and skw - sargs != set():
        return False
    rest = set(args[len(a):])  - set(kw)
    
    # If there are any arguments that weren't passed but do not have
    # defaults, the signature does not match.
    defs = set() if defaults is None else set(defaults)
    if keywords is None and rest > defs:
        return False
    return True


class ConditionalDispatch(object):
    def __init__(self):
        self.funcs = []
        self.nones = []
    
    @classmethod
    def from_existing(cls, cond_dispatch):
        new = cls()
        new.funcs = cond_dispatch.funcs[:]
        new.nones = cond_dispatch.nones[:]
        return new

    def add_dec(self, condition):
        def _dec(fun):
            self.add(fun, condition)
            return fun
        return _dec
    
    def add(self, fun, condition=None, types=None, check=True):
        """ Add fun to ConditionalDispatch under the condition that the
        arguments must match. If condition is left out, the function is 
        executed for every input that matches the signature. Functions are
        considered in the order they are added, but ones with condition=None
        are considered as the last: that means, a function with condition None
        serves as an else branch for that signature.
        conditions must be mutually exclusive because otherwise which will
        be executed depends on the order they are added in. Function signatures
        of fun and condition must match (if fun is bound, the bound parameter
        needs to be left out in condition). """
        if condition is None:
            self.nones.append((fun, types))
        elif check and correct_argspec(fun) != correct_argspec(condition):
            raise ValueError(
                "Signature of condition must match signature of fun."
            )
        else:
            self.funcs.append((fun, condition, types))
    
    def __call__(self, *args, **kwargs):
        matched = False
        for fun, condition, types in self.funcs:
            if (matches_signature(condition, args, kwargs) and
                (types is None or matches_types(condition, types, args, kwargs))):
                matched = True
                if condition(*args, **kwargs):
                    return fun(*args, **kwargs)
        for fun, types in self.nones:
            if (matches_signature(fun, args, kwargs) and
                (types is None or matches_types(fun, types, args, kwargs))):
                return fun(*args, **kwargs)
        
        if matched:
            raise TypeError(
                "Your input did not fulfill the condition for any function."
            )
        else:
            raise TypeError(
                "There are no functions matching your input parameter "
                "signature."
            )
    
    def wrapper(self):
        return lambda *args, **kwargs: self(*args, **kwargs)
    
    def get_signatures(self, prefix="", start=0):
        """ Return an iterator containing all possible function signatures.
        If prefix is given, use it as function name in signatures, else
        leave it out. If start is given, leave out first n elements.

        If start is -1, leave out first element if the function was created
        by run_cls. """
        for fun, condition, types in self.funcs:
            if start == -1:
                st = getattr(fun, 'run_cls', 0)
            else:
                st = start

            if types is not None:
                yield prefix + fmt_argspec_types(condition, types, st)
            else:
                args, varargs, keywords, defaults = correct_argspec(condition)
                args = args[st:]
                yield prefix + inspect.formatargspec(
                    args, varargs, keywords, defaults
                )
        
        for fun, types in self.nones:
            if types is not None:
                yield prefix + fmt_argspec_types(fun, types, st)
            else:
                args, varargs, keywords, defaults = correct_argspec(condition)
                args = args[st:]
                yield prefix + inspect.formatargspec(
                    args, varargs, keywords, defaults
                )

    def generate_docs(self):
        fns = (item[0] for item in chain(self.funcs, self.nones))
        return '\n\n'.join("%s -> :py:meth:`%s`" % (sig, fun.__name__)
            for sig, fun in
            # The 1 prevents the cls from incorrectly being shown in the
            # documentation.
            izip(self.get_signatures("create", -1), fns)
        )


def fmt_argspec_types(fun, types, start=0):
    args, varargs, keywords, defaults = correct_argspec(fun)
    
    args = args[start:]
    types = types[start:]

    NULL = object()
    if defaults is None:
        defaults = []
    defs = chain(repeat(NULL, len(args) - len(defaults)), defaults)
    
    spec = []
    for key, value, type_ in izip(args, defs, types):
        if value is NULL:
            spec.append("%s: %s" % (key, type_.__name__))
        else:
            spec.append("%s: %s = %s" % (key, type_.__name__, value))
    if varargs is not None:
        spec.append('*%s' % varargs)
    if keywords is not None:
        spec.append('**%s' % keywords)
    return '(' + ', '.join(spec) + ')'

########NEW FILE########
__FILENAME__ = config
"""SunPy configuration file functionality"""
import os
import tempfile
import ConfigParser

import sunpy as spy

__all__ = ['load_config', 'print_config']

def load_config():
    """
    Read the sunpyrc configuration file. If one does not exists in the user's
    home directory then read in the defaults from module
    """
    config = ConfigParser.SafeConfigParser()

    # Get locations of SunPy configuration files to be loaded
    config_files = _find_config_files()

    # Read in configuration files
    config.read(config_files)

    # Specify the working directory as a default so that the user's home
    # directory can be located in an OS-independent manner
    if not config.has_option('general', 'working_dir'):
        config.set('general', 'working_dir', os.path.join(_get_home(), "sunpy"))

    # Use absolute filepaths and adjust OS-dependent paths as needed
    filepaths = [
        ('downloads', 'download_dir')
    ]
    _fix_filepaths(config, filepaths)

    # check for sunpy working directory and create it if it doesn't exist
    if not os.path.isdir(config.get('downloads', 'download_dir')):
        os.mkdir(config.get('downloads', 'download_dir'))

    return config

def print_config():
    """Print current configuration options"""
    print("FILES USED:")
    for file_ in _find_config_files():
        print("  " + file_)

    print ("\nCONFIGURATION:")
    for section in spy.config.sections():
        print("  [%s]" % section)
        for option in spy.config.options(section):
            print("  %s = %s" % (option, spy.config.get(section, option)))
        print("")

def _is_writable_dir(p):
    """Checks to see if a directory is writable"""
    return os.path.isdir(p) and os.access(p, os.W_OK)

def _get_home():
    """Find user's home directory if possible.
    Otherwise raise error.

    """
    path = path=os.path.expanduser("~")

    if not os.path.isdir(path):
        for evar in ('HOME', 'USERPROFILE', 'TMP'):
            try:
                path = os.environ[evar]
                if os.path.isdir(path):
                    break
            except KeyError:
                pass
    if path:
        return path
    else:
        raise RuntimeError('please define environment variable $HOME')

def _find_config_files():
    """Finds locations of SunPy configuration files"""
    config_files = []
    config_filename = 'sunpyrc'

    # find default configuration file
    module_dir = os.path.dirname(spy.__file__)
    config_files.append(os.path.join(module_dir, 'data', 'sunpyrc'))

    # if a user configuration file exists, add that to list of files to read
    # so that any values set there will overide ones specified in the default
    # config file
    config_path = _get_user_configdir()

    if os.path.exists(os.path.join(config_path, config_filename)):
        config_files.append(os.path.join(config_path, config_filename))

    return config_files

def _get_user_configdir():
    """
    Return the string representing the configuration dir.
    The default is "HOME/.sunpy".  You can override this with the
    SUNPY_CONFIGDIR environment variable
    """
    configdir = os.environ.get('SUNPY_CONFIGDIR')

    if configdir is not None:
        if not _is_writable_dir(configdir):
            raise RuntimeError('Could not write to SUNPY_CONFIGDIR="%s"' %
                               configdir)
        return configdir

    h = _get_home()
    p = os.path.join(_get_home(), '.sunpy')

    if os.path.exists(p):
        if not _is_writable_dir(p):
            raise RuntimeError("'%s' is not a writable dir; you must set %s/."
                               "sunpy to be a writable dir.  You can also set "
                               "environment variable SUNPY_CONFIGDIR to any "
                               "writable directory where you want matplotlib "
                               "data stored " % (h, h))
    else:
        if not _is_writable_dir(h):
            raise RuntimeError("Failed to create %s/.sunpy; consider setting "
                               "SUNPY_CONFIGDIR to a writable directory for "
                               "sunpy configuration data" % h)

        os.mkdir(p)

    return p

def _fix_filepaths(config, filepaths):
    """Converts relative filepaths to absolute filepaths"""
    # Parse working_dir
    working_dir = _expand_filepath(config.get("general", "working_dir"))
    config.set('general', 'working_dir', working_dir)

    for f in filepaths:
        val = config.get(*f)

        filepath = _expand_filepath(val, working_dir)

        # Create dir if it doesn't already exist
        if not os.path.isdir(filepath):
            os.makedirs(filepath)

        # Replace config value with full filepath
        params = f + (filepath,)
        config.set(*params)

def _expand_filepath(filepath, working_dir=""):
    """Checks a filepath and expands it if necessary"""
    # Expand home directory
    if filepath[0] == "~":
        return os.path.abspath(os.path.expanduser(filepath))
    # Check for /tmp
    elif filepath == "/tmp":
        return tempfile.gettempdir()
    # Relative filepaths
    elif not filepath.startswith("/"):
        return os.path.join(working_dir, filepath)
    # Absolute filepath
    else:
        return filepath

########NEW FILE########
__FILENAME__ = counter
# Licensed under MIT license.
"""
Backport of the Python 2.7 collections.Counter class to Python 2.5.

See http://docs.python.org/2/library/collections.html#counter-objects for more
information.

Source: http://code.activestate.com/recipes/576611/
"""
__all__ = ['Counter']
try:
    from collections import Counter
except ImportError:
    from operator import itemgetter
    from heapq import nlargest
    from itertools import repeat, ifilter

    class Counter(dict):
        '''Dict subclass for counting hashable objects.  Sometimes called a bag
        or multiset.  Elements are stored as dictionary keys and their counts
        are stored as dictionary values.

        >>> Counter('zyzygy')
        Counter({'y': 3, 'z': 2, 'g': 1})

        '''

        def __init__(self, iterable=None, **kwds):
            '''Create a new, empty Counter object.  And if given, count elements
            from an input iterable.  Or, initialize the count from another mapping
            of elements to their counts.

            >>> c = Counter()                           # a new, empty counter
            >>> c = Counter('gallahad')                 # a new counter from an iterable
            >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping
            >>> c = Counter(a=4, b=2)                   # a new counter from keyword args

            '''        
            self.update(iterable, **kwds)

        def __missing__(self, key):
            return 0

        def most_common(self, n=None):
            '''List the n most common elements and their counts from the most
            common to the least.  If n is None, then list all element counts.

            >>> Counter('abracadabra').most_common(3)
            [('a', 5), ('r', 2), ('b', 2)]

            '''        
            if n is None:
                return sorted(self.iteritems(), key=itemgetter(1), reverse=True)
            return nlargest(n, self.iteritems(), key=itemgetter(1))

        def elements(self):
            '''Iterator over elements repeating each as many times as its count.

            >>> c = Counter('ABCABC')
            >>> sorted(c.elements())
            ['A', 'A', 'B', 'B', 'C', 'C']

            If an element's count has been set to zero or is a negative number,
            elements() will ignore it.

            '''
            for elem, count in self.iteritems():
                for _ in repeat(None, count):
                    yield elem

        # Override dict methods where the meaning changes for Counter objects.

        @classmethod
        def fromkeys(cls, iterable, v=None):
            raise NotImplementedError(
                'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')

        def update(self, iterable=None, **kwds):
            '''Like dict.update() but add counts instead of replacing them.

            Source can be an iterable, a dictionary, or another Counter instance.

            >>> c = Counter('which')
            >>> c.update('witch')           # add elements from another iterable
            >>> d = Counter('watch')
            >>> c.update(d)                 # add elements from another counter
            >>> c['h']                      # four 'h' in which, witch, and watch
            4

            '''        
            if iterable is not None:
                if hasattr(iterable, 'iteritems'):
                    if self:
                        self_get = self.get
                        for elem, count in iterable.iteritems():
                            self[elem] = self_get(elem, 0) + count
                    else:
                        dict.update(self, iterable) # fast path when counter is empty
                else:
                    self_get = self.get
                    for elem in iterable:
                        self[elem] = self_get(elem, 0) + 1
            if kwds:
                self.update(kwds)

        def copy(self):
            'Like dict.copy() but returns a Counter instance instead of a dict.'
            return Counter(self)

        def __delitem__(self, elem):
            'Like dict.__delitem__() but does not raise KeyError for missing values.'
            if elem in self:
                dict.__delitem__(self, elem)

        def __repr__(self):
            if not self:
                return '%s()' % self.__class__.__name__
            items = ', '.join(map('%r: %r'.__mod__, self.most_common()))
            return '%s({%s})' % (self.__class__.__name__, items)

        # Multiset-style mathematical operations discussed in:
        #       Knuth TAOCP Volume II section 4.6.3 exercise 19
        #       and at http://en.wikipedia.org/wiki/Multiset
        #
        # Outputs guaranteed to only include positive counts.
        #
        # To strip negative and zero counts, add-in an empty counter:
        #       c += Counter()

        def __add__(self, other):
            '''Add counts from two counters.

            >>> Counter('abbb') + Counter('bcc')
            Counter({'b': 4, 'c': 2, 'a': 1})


            '''
            if not isinstance(other, Counter):
                return NotImplemented
            result = Counter()
            for elem in set(self) | set(other):
                newcount = self[elem] + other[elem]
                if newcount > 0:
                    result[elem] = newcount
            return result

        def __sub__(self, other):
            ''' Subtract count, but keep only results with positive counts.

            >>> Counter('abbbc') - Counter('bccd')
            Counter({'b': 2, 'a': 1})

            '''
            if not isinstance(other, Counter):
                return NotImplemented
            result = Counter()
            for elem in set(self) | set(other):
                newcount = self[elem] - other[elem]
                if newcount > 0:
                    result[elem] = newcount
            return result

        def __or__(self, other):
            '''Union is the maximum of value in either of the input counters.

            >>> Counter('abbb') | Counter('bcc')
            Counter({'b': 3, 'c': 2, 'a': 1})

            '''
            if not isinstance(other, Counter):
                return NotImplemented
            _max = max
            result = Counter()
            for elem in set(self) | set(other):
                newcount = _max(self[elem], other[elem])
                if newcount > 0:
                    result[elem] = newcount
            return result

        def __and__(self, other):
            ''' Intersection is the minimum of corresponding counts.

            >>> Counter('abbb') & Counter('bcc')
            Counter({'b': 1})

            '''
            if not isinstance(other, Counter):
                return NotImplemented
            _min = min
            result = Counter()
            if len(self) < len(other):
                self, other = other, self
            for elem in ifilter(self.__contains__, other):
                newcount = _min(self[elem], other[elem])
                if newcount > 0:
                    result[elem] = newcount
            return result


    if __name__ == '__main__':
        import doctest
        print doctest.testmod()

########NEW FILE########
__FILENAME__ = create
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

from __future__ import absolute_import

import os
import glob

from sunpy import config
from sunpy.util.net import download_file

from sunpy.util.cond_dispatch import ConditionalDispatch, run_cls

__all__ = ['Parent']

class Parent(object):
    _create = ConditionalDispatch()

    @classmethod
    def read(cls, filename):
        raise NotImplementedError

    @classmethod
    def read_many(cls, filenames):
        return map(cls.read, filenames)

    @classmethod
    def from_glob(cls, pattern):
        """ Read out files using glob (e.g., ~/BIR_2011*) pattern. Returns
        list of objects made from all matched files.
        """
        return cls.read_many(glob.glob(pattern))

    @classmethod
    def from_single_glob(cls, singlepattern):
        """ Read out a single file using glob (e.g., ~/BIR_2011*) pattern.
        If more than one file matches the pattern, raise ValueError.
        """
        matches = glob.glob(os.path.expanduser(singlepattern))
        if len(matches) != 1:
            raise ValueError("Invalid number of matches: %d" % len(matches))
        return cls.read(matches[0])

    @classmethod
    def from_files(cls, filenames):
        """ Return list of object read from given list of
        filenames. """
        filenames = map(os.path.expanduser, filenames)
        return cls.read_many(filenames)

    @classmethod
    def from_file(cls, filename):
        """ Return object from file. """
        filename = os.path.expanduser(filename)
        return cls.read(filename)

    @classmethod
    def from_dir(cls, directory):
        """ Return list that contains all files in the directory read in. """
        directory = os.path.expanduser(directory)
        return cls.read_many(
            (os.path.join(directory, elem) for elem in os.listdir(directory))
        )

    @classmethod
    def from_url(cls, url):
        """ Return object read from URL.

        Parameters
        ----------
        url : str
            URL to retrieve the data from
        """
        default_dir = config.get("downloads", "download_dir")
        path = download_file(url, default_dir)
        return cls.read(path)


Parent._create.add(
    run_cls('from_file'),
    lambda cls, filename: os.path.isfile(os.path.expanduser(filename)),
    [type, basestring], check=False
)
Parent._create.add(
# pylint: disable=W0108
# The lambda is necessary because introspection is peformed on the
# argspec of the function.
    run_cls('from_dir'),
    lambda cls, directory: os.path.isdir(os.path.expanduser(directory)),
    [type, basestring], check=False
)
# If it is not a kwarg and only one matches, do not return a list.
Parent._create.add(
    run_cls('from_single_glob'),
    lambda cls, singlepattern: ('*' in singlepattern and
                           len(glob.glob(
                               os.path.expanduser(singlepattern))) == 1),
    [type, basestring], check=False
)
# This case only gets executed under the condition that the previous one wasn't.
# This is either because more than one file matched, or because the user
# explicitely used pattern=, in both cases we want a list.
Parent._create.add(
    run_cls('from_glob'),
    lambda cls, pattern: '*' in pattern and glob.glob(
        os.path.expanduser(pattern)
        ),
    [type, basestring], check=False
)
Parent._create.add(
    run_cls('from_files'),
    lambda cls, filenames: True,
    types=[type, list], check=False
)
Parent._create.add(
    run_cls('from_url'),
    lambda cls, url: True,
    types=[type, basestring], check=False
)

########NEW FILE########
__FILENAME__ = datatype_factory_base
class BasicRegistrationFactory(object):
    """ Generalized registerable factory type.

    Widgets (classes) can be registered with an instance of this class.
    Arguments to the factory's `__call__` method are then passed to a function
    specified by the registered factory, which validates the input and returns
    a instance of the class that best matches the inputs.

    Attributes
    ----------

    registry : dict
        Dictionary mapping classes (key) to function (value) which validates
        input.

    default_widget_type : type
        Class of the default widget.  Defaults to None.

    validation_functions : list of strings
        List of function names that are valid validation functions.

    Parameters
    ----------

    default_widget_type : type, optional

    additional_validation_functions : list of strings, optional
        List of strings corresponding to additional validation function names.

    Notes
    -----

    * A valid validation function must be a classmethod of the registered widget
      and it must return True or False.

    """

    def __init__(self, default_widget_type=None, additional_validation_functions=[]):

        self.registry = dict()

        self.default_widget_type = default_widget_type

        self.validation_functions = ['_factory_validation_function'] + additional_validation_functions

    def __call__(self, *args, **kwargs):
        """ Method for running the factory.

        Arguments args and kwargs are passed through to the validation
        function and to the constructor for the final type.
        """

        # Any preprocessing and massaging of inputs can happen here

        return self._check_registered_widget(*args, **kwargs)

    def _check_registered_widget(self, *args, **kwargs):
        """ Implementation of a basic check to see if arguments match a widget."""

        candidate_widget_types = list()

        for key in self.registry:

            # Call the registered validation function for each registered class
            if self.registry[key](*args, **kwargs):
                candidate_widget_types.append(key)

        n_matches = len(candidate_widget_types)

        if n_matches == 0:
            if self.default_widget_type is None:
                raise NoMatchError("No types match specified arguments and no default is set.")
            else:
                candidate_widget_types = [self.default_widget_type]
        elif n_matches > 1:
            raise MultipleMatchError("Too many candidate types idenfitied ({0}).  Specify enough keywords to guarantee unique type identification.".format(n_matches))

        # Only one is found
        WidgetType = candidate_widget_types[0]

        return WidgetType(*args, **kwargs)

    def register(self, WidgetType, validation_function=None, is_default=False):
        """ Register a widget with the factory.

        If `validation_function` is not specified, tests `WidgetType` for
        existence of any function in in the list `self.validation_functions`,
        which is a list of strings which must be callable class attribut

        Parameters
        ----------

        WidgetType : type
            Widget to register.

        validation_function : function, optional
            Function to validate against.  Defaults to None, which indicates
            that a classmethod in validation_functions is used.

        is_default : bool, optional
            Sets WidgetType to be the default widget.

        """
        if is_default:
            self.default_widget_type = WidgetType

        elif validation_function is not None:
            if not callable(validation_function):
                raise AttributeError("Keyword argument 'validation_function' must be callable.")

            self.registry[WidgetType] = validation_function

        else:
            found = False
            for vfunc_str in self.validation_functions:
                if hasattr(WidgetType, vfunc_str):
                    vfunc = getattr(WidgetType, vfunc_str)

                    # check if classmethod: stackoverflow #19227724
                    _classmethod = vfunc.__self__ is WidgetType

                    if _classmethod:
                        self.registry[WidgetType] = vfunc
                        found = True
                        break
                    else:
                        raise ValidationFunctionError("{0}.{1} must be a classmethod.".format(WidgetType.__name__, vfunc_str))

            if not found:
                raise ValidationFunctionError("No proper validation function for class {0} found.".format(WidgetType.__name__))

    def unregister(self, WidgetType):
        """ Remove a widget from the factory's registry."""
        self.registry.pop(WidgetType)


class NoMatchError(StandardError):
    """Exception for when no candidate class is found."""


class MultipleMatchError(StandardError):
    """Exception for when too many candidate classes are found."""


class ValidationFunctionError(AttributeError):
    """Exception for when no candidate class is found."""

########NEW FILE########
__FILENAME__ = multimethod
# Copyright (c) 2011 Florian Mayer <florian.mayer@bitsrc.org>

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

"""
Multimethod implementation in pure Python.
"""

from __future__ import absolute_import

from warnings import warn
from itertools import izip

__all__ = ['TypeWarning', 'MultiMethod']

SILENT = 0
WARN = 1
FAIL = 2

def _fmt_t(types):
    return ', '.join(type_.__name__ for type_ in types)


class TypeWarning(UserWarning):
    pass


class MultiMethod(object):
    """ A multimethod is a callable object that decides which code to execute
    based on the type of one or more of its arguments. 
    
    Parameters
    ----------
    get : function
        function which receives args and kwargs and returns a tuple of
        values to consider for dispatch.
    """
    def __init__(self, get):
        self.get = get
        
        self.methods = []
        self.cache = {}
    
    def add(self, fun, types, override=SILENT):
        """ Add fun to the multimethod. It will be executed if get returns
        values of the types passed as types. Must return tuples of same
        length for any input.
        
        Parameters
        ----------
        fun : function
            function to be added to the multimethod
        types : tuple of classes
            types for which the function is executed
        override : SILENT, WARN or FAIL
            control behaviour when overriding existing definitions.
            If it is set to SILENT, prior definitions are silently
            overriden, if it is set to WARN a TypeWarning
            will be issued, and with FAIL a TypeError is raised when
            attempting to override an existing definition.
        """
        overriden = False
        if override:
            for signature, _ in self.methods:
                if all(issubclass(a, b) for a, b in izip(types, signature)):
                    overriden = True
        if overriden and override == FAIL:
            raise TypeError
        elif overriden and override == WARN:
            # pylint: disable=W0631
            warn(
                'Definition (%s) overrides prior definition (%s).' %
                (_fmt_t(types), _fmt_t(signature)),
                TypeWarning,
                stacklevel=3
            )
        elif overriden:
            raise ValueError('Invalid value for override.')
        self.methods.append((types, fun))
    
    def add_dec(self, *types, **kwargs):
        """ Return a decorator that adds the function it receives to the
        multimethod with the types passed as \*args. Using keyword arg
        override to control overriding behaviour. Compare add.
        """
        self.cache = {}
        def _dec(fun):
            self.add(fun, types, kwargs.get('override', SILENT))
            return fun
        return _dec
    
    def __call__(self, *args, **kwargs):
        objs = self.get(*args, **kwargs)
        
        # pylint: disable=W0141
        types = tuple(map(type, objs))
        
        # This code is duplicate for performace reasons.
        cached = self.cache.get(types, None)
        if cached is not None:
            return cached(*args, **kwargs)
        
        for signature, fun in reversed(self.methods):
            if all(issubclass(ty, sig) for ty, sig in zip(types, signature)):
                self.cache[types] = fun
                return fun(*args, **kwargs)
        raise TypeError('%r' % types)
    
    # XXX: Other Python implementations.
    def super(self, *args, **kwargs):
        """ Like __call__, only that when you give it super(cls, obj) items,
        it will skip the multimethod for cls and use the one for its parent
        class. The normal __call__ does not consider this for performance
        reasons. """
        objs = self.get(*args, **kwargs)
        types = tuple(
            [
                x.__thisclass__.__mro__[1] if isinstance(x, super) else type(x)
                for x in objs
            ]
        )
        nargs = [
            x.__self__ if isinstance(x, super) else x
            for x in args
        ]
        
        for k, elem in kwargs.iteritems():
            if isinstance(elem, super):
                kwargs[k] = elem.__self__
        
        # This code is duplicate for performace reasons.
        cached = self.cache.get(types, None)
        if cached is not None:
            return cached(*nargs, **kwargs)
        
        for signature, fun in reversed(self.methods):
            if all(issubclass(ty, sig) for ty, sig in zip(types, signature)):
                self.cache[types] = fun
                return fun(*nargs, **kwargs)
        raise TypeError

########NEW FILE########
__FILENAME__ = net
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

from __future__ import division
from __future__ import print_function
from __future__ import absolute_import
from __future__ import unicode_literals

import os
import re
import sys
import shutil

# For Content-Disposition parsing
from urllib2 import urlopen
from urlparse import urlparse
from email.parser import FeedParser
from unicodedata import normalize
from itertools import ifilter

from sunpy.util import replacement_filename

__all__ = ['slugify','get_content_disposition', 'get_filename',
           'get_system_filename', 'get_system_filename_slugify',
           'download_file', 'download_fileobj']

# Characters not allowed in slugified version.
_punct_re = re.compile(r'[:\t !"#$%&\'()*\-/<=>?@\[\\\]^_`{|},.]+')

def slugify(text, delim=u'_', encoding="ascii"):
    """ Slugify given unicode text. """
    text = normalize('NFKD', text)
    return unicode(delim).join(ifilter(None, (
        word.encode(encoding, 'ignore')
        for word in _punct_re.split(text.lower())        
        )))


def get_content_disposition(content_disposition):
    """ Get content disposition filename from given header. Do not include
    "Content-Disposition:". Returns a unicode string! """
    parser = FeedParser()
    parser.feed(b'Content-Disposition: ' + content_disposition)
    name = parser.close().get_filename()
    if not isinstance(name, unicode):
        name = name.decode('latin1', 'ignore')
    return name


def get_filename(sock, url):
    """ Get filename from given urllib2.urlopen object and URL.
    First, tries Content-Disposition, if unavailable, extracts
    name from URL. """
    name = None
    # NOTE: This gives bytes on 2 and unicode on 3.
    # How does 3.x know the encoding?
    cd = sock.headers.get('Content-Disposition', None)
    if cd is not None:
        try:
            name = get_content_disposition(cd)
        except IndexError:
            pass

    if not name:
        parsed = urlparse(url)
        name = parsed.path.rstrip('/').rsplit('/', 1)[-1]
    return unicode(name)


def get_system_filename(sock, url, default=u"file"):
    """ Get filename from given urllib2.urlopen object and URL.
    First, attempts to extract Content-Disposition, second, extract
    from URL, eventually fall back to default. Returns bytestring
    in file system encoding. """
    name = get_filename(sock, url)
    if not name:
        name = default.decode("ascii", "ignore")
    return name.encode(sys.getfilesystemencoding(), 'ignore')


def get_system_filename_slugify(sock, url, default=u"file"):
    """ Get filename from given urllib2.urlopen object and URL.
    First, attempts to extract Content-Disposition, second, extract
    from URL, eventually fall back to default. Returns bytestring
    in file system encoding, normalized so it shouldn't violate
    operating system restrictions. """
    return slugify(get_system_filename(sock, url, default))


def download_file(url, directory, default=u'file', overwrite=False):
    """ Download file from url into directory. Try to get filename from
    Content-Disposition header, otherwise get from path of url. Fall
    back to default if both fail. Only overwrite existing files when
    overwrite is True. """
    opn = urlopen(url)
    try:
        path = download_fileobj(opn, directory, url, default, overwrite)
    finally:
        opn.close()
    return path


def download_fileobj(opn, directory, url='', default=u"file", overwrite=False):
    """ Download file from url into directory. Try to get filename from
    Content-Disposition header, otherwise get from path of url if given.
    Fall back to default if both fail. Only overwrite existing files when
    overwrite is True. """
    filename = get_system_filename(opn, url, default)
    path = os.path.join(directory, filename)
    if not overwrite and os.path.exists(path):
        path = replacement_filename(path)
    with open(path, 'wb') as fd:
        shutil.copyfileobj(opn, fd)
    return path

########NEW FILE########
__FILENAME__ = odict
from __future__ import absolute_import

__all__ = ['OrderedDict']

from astropy.utils.compat.odict import OrderedDict
########NEW FILE########
__FILENAME__ = progressbar
from __future__ import absolute_import
from __future__ import division

import sys

__all__ = ['TTYProgressBar']

class TTYProgressBar(object):
    """
    A simple progress bar to visualize progress on a TTY (teletypewriter).
    It needs to support '\b' to delete characters.

    The ProgressBar interface is start, finish, draw and poke.
    """
    SYMBOL = '='
    LEFT_BORDER = '['
    RIGHT_BORDER = ']'
    def __init__(self, n, current=0, width=40, output=sys.stdout):
        """
        Parameters
        ----------
        n : int
            Total number of items until completion
        current : int
            Current state of completion
        width : int
            Width of progress bar.
        output : file
            Teletypewriter to print on.
        """
        self.n = n
        self.current = current
        self.width = width
        # Future division, so it is float.
        self.step = self.n / self.width
        self.output = output

    def start(self):
        """
        Draw empty bar to output.
        """
        self.output.write(
            self.LEFT_BORDER + " " * (len(self.SYMBOL) * self.width) +
                self.RIGHT_BORDER
        )
        self.output.flush()
        self.output.write("\b" * (self.width+len(self.RIGHT_BORDER)))

    def finish(self):
        """
        Finish the bar, the ProgressBar cannot be used after this
        method was called.
        """
        print

    def _draw_one(self):
        """
        Advance progress bar by one.
        """
        self.output.write(self.SYMBOL)

    def draw(self):
        """
        Draw current state of progress bar onto and empty bar.
        """
        cur = self.current
        self.current = 0
        for _ in xrange(cur):
            self.poke()

    def poke(self, n=1):
        """
        Increase finished items by n. May advance the progress bar by one
        or more fields.
        """
        if self.current > self.n:
            raise ValueError("ProgressBar overflowed.")

        diff = int((self.current + n) / self.step) - int(self.current / self.step)
        for _ in xrange(diff):
            self._draw_one()
        self.current += n

########NEW FILE########
__FILENAME__ = sysinfo
from __future__ import absolute_import

import platform
import datetime


__all__ = ['get_sys_dict', 'system_info']


    
def get_sys_dict():
    """
    Test which packages are installed on system.
    
    Returns
    -------
    sys_prop: dict
        A dictionary containing the programs and versions installed on this 
        machine
    
    """    
    
    try:
        from sunpy.version import version as sunpy_version
        from sunpy.version import git_description as sunpy_git_description
    except ImportError:
        sunpy_version = 'Missing version.py; re-run setup.py'
        sunpy_git_description = 'N/A'

    # Dependencies
    try:
        from numpy import __version__ as numpy_version
    except ImportError:
        numpy_version = "NOT INSTALLED"

    try:
        from scipy import __version__ as scipy_version
    except ImportError:
        scipy_version = "NOT INSTALLED"
        
    try:
        from matplotlib import __version__ as matplotlib_version
    except ImportError:
        matplotlib_version = "NOT INSTALLED"

    try:
        from astropy import __version__ as astropy_version
    except ImportError:
        astropy_version = "NOT INSTALLED"
        
    try:
        from pandas import __version__ as pandas_version
    except ImportError:
        pandas_version = "NOT INSTALLED"

    try:
        from bs4 import __version__ as bs4_version
    except ImportError:
        bs4_version = "NOT INSTALLED"
        
    try:
        from PyQt4.QtCore import PYQT_VERSION_STR as pyqt_version
    except ImportError:
        pyqt_version = "NOT INSTALLED"

    try:
        from suds import __version__ as suds_version
    except ImportError:
        suds_version = "NOT INSTALLED"
      
    try:
        from sqlalchemy import __version__ as sqlalchemy_version
    except ImportError:
        sqlalchemy_version = "NOT INSTALLED"
        
    try:
        from requests import __version__ as requests_version
    except ImportError:
        requests_version = "NOT INSTALLED"

        
        
    sys_prop = {'Time':datetime.datetime.utcnow().strftime("%A, %d. %B %Y %I:%M%p UT"),
                'System':platform.system(), 'Processor':platform.processor(), 
                'SunPy':sunpy_version, 'SunPy_git':sunpy_git_description,
                'Arch':platform.architecture()[0], "Python":platform.python_version(),
                'NumPy':numpy_version, 
                'SciPy':scipy_version, 'matplotlib':matplotlib_version,
                'Astropy':astropy_version, 'Pandas':pandas_version, 
                'beautifulsoup':bs4_version, 'PyQt':pyqt_version,
                'SUDS':suds_version, 'Sqlalchemy':sqlalchemy_version, 'Requests':requests_version
                }
    return sys_prop
    
def system_info():
    """
    Takes dictionary from sys_info() and prints the contents in an attractive fashion   

    """    
    sys_prop = get_sys_dict()

# title
    print("==========================================================")
    print(" SunPy Installation Information\n")
    print("==========================================================\n")  
        
    
# general properties      
    print("###########")
    print(" General")
    print("###########")   
    # OS and architecture information
    
    for sys_info in ['Time', 'System', 'Processor', 'Arch', 'SunPy', 'SunPy_git']:
        print '%s : %s' % (sys_info, sys_prop[sys_info])
        
    if sys_prop['System'] == "Linux":
        distro = " ".join(platform.linux_distribution())
        print("OS: %s (Linux %s %s)" %  (distro, platform.release(), sys_prop['Processor']))
    elif sys_prop['System'] == "Darwin":
        print("OS: Mac OS X %s (%s)" %  (platform.mac_ver()[0], sys_prop['Processor']))
    elif sys_prop['System'] == "Windows":
        print("OS: Windows %s %s (%s)" %  (platform.release(), 
                                        platform.version(), sys_prop['Processor']))
    else:
        print ("Unknown OS (%s)" % sys_prop['Processor'])

    
    print "\n"
# required libraries
    print("###########")
    print(" Required Libraries ")
    print("###########")
    
    for sys_info in ['Python', 'NumPy', 'SciPy',
              'matplotlib', 'Astropy', 'Pandas']:
        print '%s: %s' % (sys_info, sys_prop[sys_info])
    
    print "\n"
    
# recommended
    print("###########")    
    print(" Recommended Libraries ") 
    print("###########")
        
    for sys_info in ['beautifulsoup', 'PyQt', 'SUDS', 
                     'Sqlalchemy', 'Requests']:
        print '%s: %s' % (sys_info, sys_prop[sys_info])
########NEW FILE########
__FILENAME__ = test_cond_dispatch
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

from __future__ import absolute_import

import pytest

from sunpy.util.cond_dispatch import ConditionalDispatch

def pytest_funcarg__oddeven(request):
    f = ConditionalDispatch()
    # Multiply even numbers by two.
    f.add(lambda x: 2 * x, lambda x: x % 2 == 0)
    # Mulitply odd numbers by three.
    f.add(lambda x: 3 * x, lambda x: x % 2 == 1)    
    return f


def test_dispatch(oddeven):
    assert oddeven(2) == 4
    assert oddeven(3) == 9


def test_wrong_sig(oddeven):
    with pytest.raises(TypeError) as exc_info:
        oddeven(y=2)
    assert exc_info.value.message == (
        "There are no functions matching your input parameter "
        "signature."
    )


def test_nocond():
    f = ConditionalDispatch()
    # Multiply even numbers by two.
    f.add(lambda x: 2 * x, lambda x: x % 2 == 0)
    with pytest.raises(TypeError) as exc_info:
        f(3)
    assert exc_info.value.message == (
        "Your input did not fulfill the condition for any function."
    )


def test_else():
    f = ConditionalDispatch()
    # Multiply even numbers by two.
    f.add(lambda x: 2 * x, lambda x: x % 2 == 0)
    f.add(lambda x: 3 * x)
    assert f(2) == 4
    assert f(3) == 9


def test_else2():
    # This verifies else branches do not catch cases that are covered
    # by cases added later.
    f = ConditionalDispatch()
    # Because gcd(2, 3) == 1, 2 | x and 3 | x are mutually exclusive.
    f.add(lambda x: 2 * x, lambda x: x % 2 == 0)
    f.add(lambda x: 3 * x)
    f.add(lambda x: 4 * x, lambda x: x % 3 == 0)
    assert f(2) == 4
    assert f(3) == 12
    assert f(5) == 15


def test_types():
    f = ConditionalDispatch()
    f.add(lambda x: 2 * x, lambda x: x % 2 == 0, [int])
    with pytest.raises(TypeError):
        f(2.0)

########NEW FILE########
__FILENAME__ = test_datatype_factory_base
import pytest

from sunpy.util.datatype_factory_base import BasicRegistrationFactory
from sunpy.util.datatype_factory_base import NoMatchError
from sunpy.util.datatype_factory_base import MultipleMatchError
from sunpy.util.datatype_factory_base import ValidationFunctionError


class BaseWidget(object):
    def __init__(self, *args, **kwargs):
        pass


class DefaultWidget(BaseWidget):
    pass


class StandardWidget(BaseWidget):
    @classmethod
    def _factory_validation_function(cls, *args, **kwargs):
        return kwargs.get('style') == 'standard'


class DuplicateStandardWidget(BaseWidget):
    @classmethod
    def _factory_validation_function(cls, *args, **kwargs):
        return kwargs.get('style') == 'standard'


class FancyWidget(BaseWidget):
    @classmethod
    def _factory_validation_function(cls, *args, **kwargs):
        return kwargs.get('style') == 'fancy' and 'feature' in kwargs


class ExternallyValidatedWidget(BaseWidget):
    pass


def external_validation_function(*args, **kwargs):
    return kwargs.get('style') == 'external'


class UnvalidatedWidget(BaseWidget):
    pass


class MissingClassMethodWidget(BaseWidget):
    def _factory_validation_function(cls, *args, **kwargs):
        return kwargs.get('style') == 'missing'


class DifferentValidationWidget(BaseWidget):
    @classmethod
    def different_validation_function(cls, *args, **kwargs):
        return kwargs.get('style') == 'different'


class MissingClassMethodDifferentValidationWidget(BaseWidget):
    def different_validation_function(cls, *args, **kwargs):
        return kwargs.get('style') == 'missing-different'


class TestBasicRegistrationFactory(object):

    def test_default_factory(self):

        DefaultFactory = BasicRegistrationFactory()

        DefaultFactory.register(DefaultWidget, is_default=True)
        assert DefaultFactory.default_widget_type == DefaultWidget

        DefaultFactory.register(StandardWidget)
        DefaultFactory.register(FancyWidget)
        DefaultFactory.register(ExternallyValidatedWidget,
                                validation_function=external_validation_function)

        assert type(DefaultFactory()) is DefaultWidget
        assert type(DefaultFactory(style='standard')) is StandardWidget
        assert type(DefaultFactory(style='fancy')) is DefaultWidget
        assert type(DefaultFactory(style='fancy', feature="present")) is FancyWidget
        assert type(DefaultFactory(style='external')) is ExternallyValidatedWidget

        with pytest.raises(ValidationFunctionError):
            DefaultFactory.register(UnvalidatedWidget)

        with pytest.raises(ValidationFunctionError):
            DefaultFactory.register(MissingClassMethodWidget)

    def test_no_default_factory(self):

        NoDefaultFactory = BasicRegistrationFactory()

        NoDefaultFactory.register(StandardWidget)
        NoDefaultFactory.register(FancyWidget)

        with pytest.raises(NoMatchError):
            NoDefaultFactory()

        # Raises because all requirements are not met for FancyWidget and no
        # default is present.
        with pytest.raises(NoMatchError):
            NoDefaultFactory(style='fancy')

        assert type(NoDefaultFactory(style='standard')) is StandardWidget
        assert type(NoDefaultFactory(style='fancy', feature='present')) is FancyWidget

    def test_multiple_match_factory(self):

        MultipleMatchFactory = BasicRegistrationFactory()

        MultipleMatchFactory.register(StandardWidget)
        MultipleMatchFactory.register(DuplicateStandardWidget)

        with pytest.raises(MultipleMatchError):
            MultipleMatchFactory(style='standard')

    def test_extra_validation_factory(self):
        ExtraValidationFactory = BasicRegistrationFactory(additional_validation_functions=['different_validation_function'])

        ExtraValidationFactory.register(DifferentValidationWidget)

        assert type(ExtraValidationFactory(style='different')) is DifferentValidationWidget

        with pytest.raises(ValidationFunctionError):
            ExtraValidationFactory.register(MissingClassMethodDifferentValidationWidget)

########NEW FILE########
__FILENAME__ = test_multimethod
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>

from __future__ import absolute_import

import pytest

from sunpy.util.multimethod import MultiMethod, FAIL, WARN, TypeWarning

def test_super():
    class String(str):
        pass
    
    mm = MultiMethod(lambda *a: a)
    
    @mm.add_dec(str, str)
    def foo(foo, bar):
        return 'String'
    
    @mm.add_dec(String, str)
    def foo(foo, bar):
        return 'Fancy', mm.super(super(String, foo), bar)
    
    assert mm('foo', 'bar') == 'String'
    assert mm(String('foo'), 'bar') == ('Fancy', 'String')


def test_override(recwarn):
    class String(str):
        pass
    
    mm = MultiMethod(lambda *a: a)
    
    @mm.add_dec(str, str)
    def foo(foo, bar):
        return 'String'
    
    pytest.raises(
        TypeError, mm.add_dec(String, str, override=FAIL), lambda x, y: None
    )
    
    mm.add_dec(String, str, override=WARN)(lambda x, y: None)
    w = recwarn.pop(TypeWarning)
    assert (
        'Definition (String, str)'
        ' overrides prior definition (str, str).' in w.message
    )

########NEW FILE########
__FILENAME__ = test_sysinfo
# -*- coding: utf-8 -*-
import sunpy

def test_sysinfo():

    output = sunpy.util.get_sys_dict()    
    
    assert isinstance(output, dict)
########NEW FILE########
__FILENAME__ = test_util
"""This module tests the functions implemented in sunpy.util.util."""

from __future__ import absolute_import

from sunpy.util import util
import numpy as np
import warnings

def test_to_signed():
    """
    This should return a signed type that can hold uint32.
    """
    assert util.to_signed(np.dtype('uint32')) == np.dtype('int64')

def test_goes_flare_class():
    """
    This should convert a list of GOES classes into a list of numbers.
    """
    lst = ['A1.0', 'M1.0', 'C1.0', 'B1.0', 'X1.0']
    assert util.goes_flare_class(lst) == \
        [1.0e-08, 1.0e-05, 1.0e-06, 1.0e-07, 1.0e-04]

def test_unique():
    """
    This should add the unique values of itr to unique_list.
    """
    itr = [6, 1, 2, 1, 7, 41.2, '41.2', 1, '41.2']
    unique_list = []
    for elem in util.unique(itr):
        unique_list.append(elem)
    assert unique_list == [6, 1, 2, 7, 41.2, '41.2']

def test_unique_key():
    """
    This should add each element of itr to unique_list if no preceding
    element is congruent to it in mod 10.
    """
    itr = [7, 3, 17, 104, 6, 1006, 117, 14, 10]
    unique_list = []
    for elem in util.unique(itr, lambda x: x % 10):
        unique_list.append(elem)
    assert unique_list == [7, 3, 104, 6, 10]

def test_print_table():
    """
    This should return a string representation of lst with table elements
    left-justified and with columns separated by dashes.
    """
    lst = [['n', 'sqrt(n)', 'n^2'], \
           ['1', '1', '1'], \
           ['4', '2', '16'], \
           ['3', '1.732', '9']]
    expected = ('n|sqrt(n)|n^2\n'
               '1|1      |1  \n'
               '4|2      |16 \n'
               '3|1.732  |9  ')
    assert util.print_table(lst, colsep='|') == expected

def test_findpeaks():
    """
    This should return the indices of the local maxima of numpy array
    data (relative to index 1).
    """
    data = np.array([1.0, 3.5, 3.0, 4.0, -9.0, 0.0, 0.5, 0.3, 9.5])
    assert np.array_equal(util.findpeaks(data), np.array([0, 2, 5]))

def test_polyfun_at():
    """
    This should evaluate the polynomial x^3 + 5x^2 - 6x + 3 at x = 5.
    """
    coeff = [1, 5, -6, 3]
    assert util.polyfun_at(coeff, 5) == 223

def test_minimal_pairs():
    """
    This should return the pairs of elements from list1 and list2 with
    minimal difference between their values.
    """
    list1 = [0, 5, 10, 15, 20, 25]
    list2 = [3, 12, 19, 21, 26, 29]
    assert list(util.minimal_pairs(list1, list2)) == [(1, 0, 2), (2, 1, 2),
                                                      (4, 2, 1), (5, 4, 1)]

def test_find_next():
    """
    This should return a generator yielding the nearest larger element in
    list2 for each element in list1 (or None if none exists after the
    previous element yielded from list2).
    """
    list1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]
    list2 = [0, 2, 3, 5, 0, 0, 5, 9, 10, 15]
    assert list(util.find_next(list1, list2, None)) == [(1, 2), (2, 3), (3, 5),
                        (4, 5), (5, 9), (6, 10), (7, 15), (8, None), (9, None)]

def test_common_base():
    """
    This should return the base class common to each object in objs.
    """
    class TestA(object):
        """Base test class."""
        pass
    class TestB(TestA):
        """First inherited class."""
        pass
    class TestC(TestA):
        """Second inherited class."""
        pass
    inst_b = TestB()
    inst_c = TestC()
    objs = [inst_b, inst_c]
    assert util.common_base(objs) == TestA

def test_merge():
    """
    This should return a sorted (from greatest to least) merged list
    from list1 and list2.
    """
    list1 = [13, 11, 9, 7, 5, 3, 1]
    list2 = [14, 12, 10, 8, 6, 4, 2]
    result = list(util.merge([list1, list2]))
    assert result[::-1] == sorted(result)

def test_replacement_filename():
    """
    This should return a replacement path for the current file.
    """
    assert util.replacement_filename(__file__).endswith('test_util.0.py')

def test_expand_list():
    """
    This should return an expanded version of list lst.
    """
    lst = [1, 2, 3, [4, 5, 6], 7, (8, 9)]
    assert util.expand_list(lst) == [1, 2, 3, 4, 5, 6, 7, 8, 9]

def test_deprecated():
    """
    This should trigger a deprecation warning.
    """
    depr = util.Deprecated()
    with warnings.catch_warnings(record=True) as current_warnings:
        depr_func = depr(lambda x: x)
        depr_func(1)
        assert len(current_warnings) == 1

########NEW FILE########
__FILENAME__ = test_xml
from __future__ import absolute_import

from xml.parsers.expat import ExpatError
from xml.dom.minidom import Document
from xml.dom.minidom import parseString

import pytest

from sunpy.util import xml


def test_xml_to_dict1():
    """
    should return dict of xml string
    """
    source_xml = "<outer>\
          <inner1>one</inner1>\
          <inner2>two</inner2>\
        </outer>"
        
    xml_dict = xml.xml_to_dict(source_xml)
    expected_dict = {u'outer': {u'inner2': u'two', u'inner1': u'one'}}
    
    assert xml_dict == expected_dict
    
def test_xml_to_dict2():
    """
    should return dict of xml string 
    and if a tag is duplicated it takes the last one.    
    """
    source_xml = "<outer>\
                    <inner1>one-one</inner1>\
                    <inner1>one-two</inner1>\
                    <inner2>two-one</inner2>\
                    <inner2>two-two</inner2>\
                 </outer>"
                
    xml_dict = xml.xml_to_dict(source_xml)
    expected_dict = {u'outer': {u'inner2': u'two-two', u'inner1': u'one-two'}}
    
    assert xml_dict == expected_dict
    
def test_xml_to_dict3():
    """
    should return dict of xml string 
    with empty value if there are no inner elements   
    """
    source_xml = "<outer/>"
                
    xml_dict = xml.xml_to_dict(source_xml)
    expected_dict = {u'outer': ''}
        
    assert xml_dict == expected_dict
    
def test_xml_to_dict4():
    """
    should return dict of xml string 
    with empty value if there are no inner elements   
    """
    source_xml = "<outer></outer>"
                
    xml_dict = xml.xml_to_dict(source_xml)
    expected_dict = {u'outer': ''}
        
    assert xml_dict == expected_dict
    
def test_xml_to_dict5():
    """
    should return dict of xml string 
    with 2 layer nesting 
    """
    source_xml = "<outer>\
                    <mid1>\
                        <inner1>one-one</inner1>\
                    </mid1>\
                    <mid2>\
                        <inner2>two-one</inner2>\
                    </mid2>\
                 </outer>"
                
    xml_dict = xml.xml_to_dict(source_xml)
    expected_dict = {u'outer': {u'mid2': {u'inner2': u'two-one'}, u'mid1': {u'inner1': u'one-one'}}}
        
    assert xml_dict == expected_dict
    
def test_xml_to_dict6():
    """
    should return dict of xml string 
    with 2 layer nesting and if a tag is duplicated it takes the last one.
    """
    source_xml = "<outer>\
                    <mid>\
                        <inner1>one-one</inner1>\
                    </mid>\
                    <mid>\
                        <inner2>two-one</inner2>\
                    </mid>\
                 </outer>"
                
    xml_dict = xml.xml_to_dict(source_xml)
    expected_dict = {u'outer': {u'mid': {u'inner2': u'two-one'}}}
        
    assert xml_dict == expected_dict

def test_xml_to_dict7():
    """
    should raise TypeError when passed None 
    """
    assert pytest.raises(TypeError, xml.xml_to_dict, None)
    
def test_xml_to_dict8():
    """
    should raise TypeError when passed non string
    """
    assert pytest.raises(TypeError, xml.xml_to_dict, 9)
    
def test_xml_to_dict9():
    """
    should raise ExpatError when passed empty string
    """
    assert pytest.raises(ExpatError, xml.xml_to_dict, "")
    
def test_xml_to_dict10():
    """
    should raise ExpatError when passed space
    """
    assert pytest.raises(ExpatError, xml.xml_to_dict, " ")
    
    
def test_get_node_text1():
    """
    should raise NotTextNodeError if there is a non text node.
    """
    doc = Document()
    outer = doc.createElement("outer")
    doc.appendChild(outer)
    pytest.raises(xml.NotTextNodeError, xml.get_node_text, doc)
  
def test_get_node_text2():
    """
    should return empty string for a node with no child nodes. 
    """
    assert xml.get_node_text(Document()) == ""
  
def test_get_node_text3():
    """
    should return node text
    """
    node = parseString("<outer>one</outer>")
    text_node = node.childNodes[0]

    assert xml.get_node_text(text_node) == "one"
    
def test_get_node_text4():
    """
     should raise AttributeError when sent None
    """
    assert pytest.raises(AttributeError, xml.get_node_text, None)
    
def test_get_node_text5():
    """
     should raise AttributeError when sent wrong type
    """
    assert pytest.raises(AttributeError, xml.get_node_text, "wrong type")

def test_node_to_dict1():
    """
    should return dict of node
    """
    
    doc = Document()

    outer = doc.createElement("outer")
    doc.appendChild(outer)
   
    inner1 = doc.createElement("inner1")
    inner2 = doc.createElement("inner2")
    outer.appendChild(inner1)
    outer.appendChild(inner2)
    
    inner1_text = doc.createTextNode("one")
    inner2_text = doc.createTextNode("two")
    inner1.appendChild(inner1_text)
    inner2.appendChild(inner2_text)
    
    expected_dict = {'outer': {'inner2': 'two', 'inner1': 'one'}}
    xml_dict = xml.node_to_dict(doc)
    
    assert xml_dict == expected_dict
    
def test_node_to_dict2():
    """
    should return dict of node double nested
    """
    
    doc = Document()

    outer = doc.createElement("outer")
    doc.appendChild(outer)
    
    mid1 = doc.createElement("mid1")
    outer.appendChild(mid1)
    mid2 = doc.createElement("mid2")
    outer.appendChild(mid2)
    
    inner1 = doc.createElement("inner1")
    inner2 = doc.createElement("inner2")
    mid1.appendChild(inner1)
    mid2.appendChild(inner2)
    
    inner1_text = doc.createTextNode("one")
    inner2_text = doc.createTextNode("two")
    inner1.appendChild(inner1_text)
    inner2.appendChild(inner2_text)
    
    expected_dict = {'outer': {'mid2': {'inner2': 'two'}, 'mid1': {'inner1': 'one'}}}
    xml_dict = xml.node_to_dict(doc)
    
    assert xml_dict == expected_dict
    
def test_node_to_dict3():
    """
    should return empty dict when sent empty doc
    """
    expected_dict = {}
    xml_dict = xml.node_to_dict(Document())
    
    assert xml_dict == expected_dict
    
def test_node_to_dict4():
    """
    should raise AttributeError when sent wrong type
    """
    assert pytest.raises(AttributeError, xml.node_to_dict, 9)
    
def test_node_to_dict5():
    """
    should raise AttributeError when sent None
    """
    assert pytest.raises(AttributeError, xml.node_to_dict, None)
    
    
########NEW FILE########
__FILENAME__ = unit_conversion
from __future__ import absolute_import

import numpy as np
from scipy.constants import constants
from astropy import units

__all__ = ['degrees_to_hours', 'degrees_to_arc', 'kelvin_to_keV', 
           'keV_to_kelvin', 'to_angstrom']
           
def degrees_to_hours(angle):
    """Converts an angle from the degree notation to the hour, arcmin, arcsec 
    notation (returned as a tuple)."""
    hour = int(np.floor(angle / 15))
    remainder = angle / 15.0 - hour
    arcminute = int(np.floor(remainder * 60))
    remainder =  remainder * 60 - arcminute
    arcsecond = remainder * 60.0
    return [hour, arcminute, arcsecond]

def degrees_to_arc(angle):
    """Converts decimal degrees to degree, arcminute, 
    arcsecond (returned as a tuple)."""
    degree = int(np.floor(angle))
    remainder = angle - degree
    arcminute = int(np.floor(remainder * 60))
    remainder =  remainder * 60 - arcminute
    arcsecond = remainder * 60.0
    return [degree, arcminute, arcsecond]

def to_angstrom(value, unit):
    """Given a value with a unit (given in a string), convert to angstroms"""
    value_quantity = value * units.Unit(unit)
    return value_quantity.to(units.angstrom, equivalencies=units.spectral()).value

def kelvin_to_keV(temperature):
    """Convert from temperature expressed in Kelvin to a 
    temperature expressed in keV"""
    return temperature / (constants.e / constants.k * 1000.0) 

def keV_to_kelvin(temperature):
    """Convert from temperature expressed in keV to a temperature 
    expressed in Kelvin"""
    return temperature * (constants.e / constants.k * 1000.0) 
########NEW FILE########
__FILENAME__ = util
from __future__ import absolute_import

import os
import types
import warnings
from itertools import izip, imap, count

import numpy as np

__all__ = ['to_signed', 'unique', 'print_table',
           'replacement_filename', 'goes_flare_class', 'merge', 'common_base',
           'minimal_pairs', 'polyfun_at', 
           'expand_list', 'expand_list_generator', 'Deprecated']

def to_signed(dtype):
    """ Return dtype that can hold data of passed dtype but is signed.
    Raise ValueError if no such dtype exists.
    
    Parameters
    ----------
    dtype : np.dtype
        dtype whose values the new dtype needs to be able to represent.
    """
    if dtype.kind == "u":
        if dtype.itemsize == 8:
            raise ValueError("Cannot losslessy convert uint64 to int.")
        dtype = "int%d" % (min(dtype.itemsize * 2 * 8, 64))
    return np.dtype(dtype)

def goes_flare_class(gcls):
    """Convert GOES classes into a number to aid size comparison.  Units are
    watts per meter squared."""
    def calc(gcls):
        powers_of_ten = {'A':1e-08, 'B':1e-07, 'C':1e-06, 'M':1e-05, 'X':1e-04}
        power = gcls[0].upper()
        if power in powers_of_ten:
            return powers_of_ten[power] * float(gcls[1:])
        else:
            return None

    if isinstance(gcls, types.StringType):
        return calc(gcls)
    if isinstance(gcls, types.ListType):
        return [calc(x) for x in gcls]


def unique(itr, key=None):
    items = set()
    if key is None:
        for elem in itr:
            if elem not in items:
                yield elem
                items.add(elem)
    else:
        for elem in itr:
            x = key(elem)
            if x not in items:
                yield elem
                items.add(x)

def print_table(lst, colsep=' ', linesep='\n'):
    width = [max(imap(len, col)) for col in izip(*lst)]
    return linesep.join(
        colsep.join(
            col.ljust(n) for n, col in izip(width, row)
        ) for row in lst
    )


def findpeaks(a):
    """ Find local maxima in 1D. Use findpeaks(-a) for minima. """
    return np.nonzero((a[1:-1] > a[:-2]) & (a[1:-1] > a[2:]))[0]


def polyfun_at(coeff, p):
    """ Return value of polynomial with coefficients (highest first) at
    point (can also be an np.ndarray for more than one point) p. """
    return np.sum(k * p ** n for n, k in enumerate(reversed(coeff)))


def minimal_pairs(one, other):
    """ Find pairs of values in one and other with minimal distance.
    Assumes one and other are sorted in the same sort sequence.
    
    one, other : sequence
        Sequence of scalars to find pairs from.
    """
    lbestdiff = bestdiff = bestj = besti = None
    for i, freq in enumerate(one):
        lbestj = bestj
        
        bestdiff, bestj = None, None
        for j, o_freq in enumerate(other[lbestj:]):
            j = lbestj + j if lbestj else j
            diff = abs(freq - o_freq)
            if bestj is not None and diff > bestdiff:
                break
            
            if bestj is None or bestdiff > diff:
                bestj = j
                bestdiff = diff
        
        if lbestj is not None and lbestj != bestj:
            yield (besti, lbestj, lbestdiff)
            besti = i
            lbestdiff = bestdiff
        elif lbestdiff is None or bestdiff < lbestdiff:
            besti = i
            lbestdiff = bestdiff
    
    yield (besti, bestj, lbestdiff)


DONT = object()
def find_next(one, other, pad=DONT):
    """ Given two sorted sequences one and other, for every element
    in one, return the one larger than it but nearest to it in other.
    If no such exists and pad is not DONT, return value of pad as "partner".
    """
    n = 0
    for elem1 in one:
        for elem2 in other[n:]:
            n += 1
            if elem2 > elem1:
                yield elem1, elem2
                break
        else:
            if pad is not DONT:
                yield elem1, pad


def common_base(objs):
    """ Find class that every item of objs is an instance of. """
    for cls in objs[0].__class__.__mro__:
        if all(isinstance(obj, cls) for obj in objs):
            break
    return cls


def merge(items, key=(lambda x: x)):
    """ Given sorted lists of iterables, return new iterable that returns
    elemts of all iterables sorted with respect to key. """
    state = {}
    for item in map(iter, items):
        try:
            first = item.next()
        except StopIteration:
            continue
        else:
            state[item] = (first, key(first))
    
    while state:
        for item, (value, tk) in state.iteritems():
            # Value is biggest.
            if all(tk >= k for it, (v, k)
                in state.iteritems() if it is not item):
                yield value
                break
        try:
            n = item.next()
            state[item] = (n, key(n))
        except StopIteration:
            del state[item]

def replacement_filename(path):
    """ Return replacement path for already used path. Enumerates
    until an unused filename is found. E.g., "/home/florian/foo.fits"
    becomes "/home/florian/foo.0.fits", if that is used
    "/home/florian/foo.1.fits", etc. """
    if not os.path.exists(path):
        return path
    else:
        dir_, filename = os.path.split(path)
        base, ext = os.path.splitext(filename)
        for c in count():
            name = base + '.' + str(c) + ext
            newpath = os.path.join(dir_, name)
            if not os.path.exists(newpath):
                return newpath


#==============================================================================
# expand list from :http://stackoverflow.com/a/2185971/2486799
#==============================================================================
def expand_list(input):
	return [item for item in expand_list_generator(input)]

def expand_list_generator(input):    
    for item in input:
       if type(item) in [list, tuple]:
           for nested_item in expand_list_generator(item):
               yield nested_item
       else:
           yield item

#==============================================================================
# Deprecation decorator: http://code.activestate.com/recipes/391367-deprecated/
# and http://www.artima.com/weblogs/viewpost.jsp?thread=240845
#==============================================================================
class Deprecated(object):
    """ Use this decorator to deprecate a function or method, you can pass an
    additional message to the decorator:
    
    @Deprecated("no more")
    """
    def __init__(self, message=""):
        self.message = message

    def __call__(self, func):
        def newFunc(*args, **kwargs):
            warnings.warn("Call to deprecated function %s. \n %s" %(
                                                                func.__name__,
                                                                self.message),
                          category=Warning, stacklevel=2)
            return func(*args, **kwargs)
        
        newFunc.__name__ = func.__name__
        newFunc.__doc__ = func.__doc__
        newFunc.__dict__.update(func.__dict__)
        return newFunc
########NEW FILE########
__FILENAME__ = xml
"""XML helper functions"""

from __future__ import absolute_import
from xml.dom.minidom import parseString #pylint: disable=E0611,F0401

__all__ = ['NotTextNodeError', 'xml_to_dict', 'node_to_dict', 'get_node_text']

#
# Converting XML to a Dictionary
# Author: Christoph Dietze
# URL   : http://code.activestate.com/recipes/116539/
#
class NotTextNodeError(Exception):
    pass

def xml_to_dict(xmlstring):
    """
    Converts an XML string to a Python dictionary
    
    .. Warning::
        This method does not support multiple inner nodes of the same name but
        with different values.  It always takes the last value.
    
    Examples
    --------
    ::

        <outer>
            <inner>one</inner>
            <inner>two</inner>
        </outer>
            
    gives you the dict::

       {u'outer': {u'inner': u'two'}}  
    """
    return node_to_dict(parseString(xmlstring))

def node_to_dict(node):
    """
    node_to_dict() scans through the children of node and makes a dictionary 
    from the content.
    
    Three cases are differentiated:
    
    1. If the node contains no other nodes, it is a text-node and 
       {nodeName: text} is merged into the dictionary.
    2. If the node has the attribute "method" set to "true", then it's children 
       will be appended to a list and this list is merged to the dictionary in 
       the form: {nodeName:list}.
    3. Else, node_to_dict() will call itself recursively on the nodes children 
       (merging {nodeName: node_to_dict()} to the dictionary).

    """
    dic = {} 
    for n in node.childNodes:
        if n.nodeType != n.ELEMENT_NODE:
            continue
        if n.getAttribute("multiple") == "true":
            # node with multiple children: put them in a list
            l = []
            for c in n.childNodes:
                if c.nodeType != n.ELEMENT_NODE:
                    continue
                l.append(node_to_dict(c))
                dic.update({n.nodeName: l})
            continue
            
        try:
            text = get_node_text(n)
        except NotTextNodeError:
            # 'normal' node
            dic.update({n.nodeName: node_to_dict(n)})
            continue
    
        # text node
        dic.update({n.nodeName: text})
        continue
    return dic

def get_node_text(node):
    """
    scans through all children of node and gathers the text. if node has 
    non-text child-nodes, then NotTextNodeError is raised.
    """
    t = ""
    for n in node.childNodes:
        if n.nodeType == n.TEXT_NODE:
            t += n.nodeValue
        else:
            raise NotTextNodeError
    return t

########NEW FILE########
__FILENAME__ = imageanimator
# -*- coding: utf-8 -*-

import matplotlib.pyplot as plt
import matplotlib.widgets as widgets
import matplotlib.animation as mplanim

from mpl_toolkits.axes_grid1 import make_axes_locatable
import mpl_toolkits.axes_grid1.axes_size as Size

__all__ = ['BaseFuncAnimator', 'ImageAnimator']

class SliderPB(widgets.Slider):
    __doc__= widgets.Slider.__doc__

    def __init__(self, ax, label, valmin, valmax, valinit=0.5, valfmt='%1.2f',
                 closedmin=True, closedmax=True, slidermin=None,
                 slidermax=None, dragging=True, **kwargs):

        widgets.Slider.__init__(self, ax, label, valmin, valmax, valinit=valinit,
                                valfmt=valfmt, closedmin=closedmin,
                                closedmax=closedmax, slidermin=slidermin,
                                slidermax=slidermax, dragging=dragging, **kwargs)

        self.changed_args = {}

    def set_val(self, val):
        xy = self.poly.xy
        xy[2] = val, 1
        xy[3] = val, 0
        self.poly.xy = xy
        self.valtext.set_text(self.valfmt % val)
        if self.drawon:
            self.ax.figure.canvas.draw()
        self.val = val
        if not self.eventson:
            return
        for cid, func in self.observers.items():
            func(val, *self.changed_args[cid])

    def on_changed(self, func, *args):
        """
        When the slider value is changed, call *func* with the new
        slider position

        A connection id is returned which can be used to disconnect
        """
        cid = self.cnt
        self.observers[cid] = func
        self.changed_args[cid] = args
        self.cnt += 1
        return cid

class ButtonPB(widgets.Button):
    def __init__(self, ax, label, image=None,
                 color='0.85', hovercolor='0.95'):

        widgets.Button.__init__(self, ax, label, image=image,
                 color=color, hovercolor=hovercolor)

        self.clicked_args = {}

    def on_clicked(self, func, *args):
        """
        When the button is clicked, call this *func* with event

        A connection id is returned which can be used to disconnect
        """
        cid = self.cnt
        self.observers[cid] = func
        self.clicked_args[cid] = args
        self.cnt += 1
        return cid

    def _release(self, event):
        if self.ignore(event):
            return
        if event.canvas.mouse_grabber != self.ax:
            return
        event.canvas.release_mouse(self.ax)
        if not self.eventson:
            return
        if event.inaxes != self.ax:
            return
        for cid, func in self.observers.items():
            func(event, *self.clicked_args[cid])

class BaseFuncAnimator(object):
    """
    Create a matplotlib backend independant data explorer which allows
    definition of figure update functions for each slider.

    The following keyboard shortcuts are defined in the viewer:

    - 'left': previous step on active slider
    - 'right': next step on active slider
    - 'top': change the active slider up one
    - 'bottom': change the active slider down one
    - 'p': play/pause active slider

    This viewer can have user defined buttones added by specifing the labels and
    functions called when those buttons are clicked as keyword argumets.

    To make this class useful the subclass must implement `_plot_start_image`
    which must define a `self.im` attribute which is an instance of AxesImage

    Parameters
    ----------
    data: iterable
        Some arbitary data

    slider_functions: list
        A list of functions to call when that slider is changed.
        These functions will have `val`, the axes image object and the slider
        widget instance passed to them, i.e.: update_slider(val, im, slider)

    slider_ranges: list
        list of [min,max] pairs to set the ranges for each slider.

    fig: mpl.figure
        Figure to use

    interval: int
        Animation interval in ms

    colorbar: bool
        Plot colorbar

    button_labels: list
        List of strings to label buttons

    button_func: list
        List of functions to map to the buttons

    Extra keywords are passed to imshow.
    """

    def __init__(self, data, slider_functions, slider_ranges, fig=None,
                 interval=200, colorbar=False, **kwargs):

        #Allow the user to specify the button func:
        self.button_func = kwargs.pop('button_func', [])
        self.button_labels = kwargs.pop('button_labels', [])
        self.num_buttons = len(self.button_labels)

        if not fig:
            fig = plt.figure()
        self.fig = fig

        self.data = data
        self.interval = interval
        self.if_colorbar = colorbar
        self.imshow_kwargs = kwargs

        if len(slider_functions) != len(slider_ranges):
            raise ValueError("You must specify the same number of functions as extents")
        self.num_sliders = len(slider_functions)
        self.slider_functions = slider_functions
        self.slider_ranges = slider_ranges

        #Set active slider
        self.active_slider = 0

        #Set a blank timer
        self.timer = None

        #Set up axes
        self._make_axes_grid()
        self._add_widgets()
        self._set_active_slider(0)

        #Set the current axes to the main axes so commands like plt.ylabel() work.
        plt.sca(self.axes)

        #Do Plot
        self.im = self.plot_start_image(self.axes)

        #Connect fig events
        self._connect_fig_events()

    def label_slider(self, i, label):
        """
        Change the Slider label

        Parameters
        ----------
        i: int
            The index of the slider to change (0 is bottom)
        label: str
            The label to set
        """
        self.sliders[i]._slider.label.set_text(label)

    def get_animation(self, axes=None, slider=0, startframe=0, endframe=None,
                      stepframe=1, **kwargs):
        """
        Return a matplotlib.animation.FuncAnimation instance for the selected
        slider.

        This will allow easy saving of the animation to a file.

        Parameters
        ----------

        slider: int
            The slider to animate along

        startframe: int
            The frame to start the animation

        endframe: int
            The frame to end the animation

        stepframe: int
            The step between frames
        """
        if not axes:
            axes = plt.gca()
        anim_fig = axes.get_figure()

        if endframe is None:
            endframe=self.slider_ranges[slider][1]

        im = self.plot_start_image(axes)

        anim_kwargs = {'frames':range(startframe, endframe, stepframe),
                       'fargs':[im, self.sliders[slider]._slider]}
        anim_kwargs.update(kwargs)

        ani = mplanim.FuncAnimation(anim_fig, self.slider_functions[slider], **anim_kwargs)

        return ani

    def plot_start_image(self, ax):
        """
        This method creates the inital image on the mpl axes

        .. warning::
            This method needs to be implemeted in subclasses

        Parameters
        ----------
        ax: mpl axes
            This is the axes on which to plot the image

        Returns
        -------
        AxesImage:
            A AxesImage object, the instance returned from a plt.imshow() command.
        """
        raise NotImplementedError("Please define your setup function")

    def _connect_fig_events(self):
        self.fig.canvas.mpl_connect('button_press_event', self._mouse_click)
        self.fig.canvas.mpl_connect('key_press_event', self._key_press)

    def _add_colorbar(self, im):
        self.colorbar = plt.colorbar(im, self.cax)

#==============================================================================
#   Figure event callback functions
#==============================================================================
    def _mouse_click(self, event):
        if event.inaxes in self.sliders:
            slider = self.sliders.index(event.inaxes)
            self._set_active_slider(slider)

    def _key_press(self, event):
        if event.key == 'left':
            self._previous(self.sliders[self.active_slider]._slider)
        elif event.key == 'right':
            self._step(self.sliders[self.active_slider]._slider)
        elif event.key == 'up':
            self._set_active_slider((self.active_slider+1)%self.num_sliders)
        elif event.key == 'down':
            self._set_active_slider((self.active_slider-1)%self.num_sliders)
        elif event.key == 'p':
            self._click_slider_button(event, self.slider_buttons[self.active_slider]._button,
                               self.sliders[self.active_slider]._slider)

#==============================================================================
#   Active Sider methods
#==============================================================================
    def _set_active_slider(self, ind):
        self._dehighlight_slider(self.active_slider)
        self._highliget_slider(ind)
        self.active_slider = ind

    def _highliget_slider(self, ind):
        ax = self.sliders[ind]
        [a.set_linewidth(2.0) for n,a in ax.spines.items()]
        self.fig.canvas.draw()

    def _dehighlight_slider(self, ind):
        ax = self.sliders[ind]
        [a.set_linewidth(1.0) for n,a in ax.spines.items()]
        self.fig.canvas.draw()

#==============================================================================
#   Build the figure and place the widgets
#==============================================================================
    def _get_main_axes(self):
        """ Allow replacement of main axes by subclassing """
        return self.fig.add_subplot(111)

    def _make_axes_grid(self):
        self.axes = self._get_main_axes()

        #Split up the current axes so there is space for a start and a stop button
        self.divider = make_axes_locatable(self.axes)
        pad = 0.01 # Padding between axes
        pad_size = Size.Fraction(pad, Size.AxesX(self.axes))
        large_pad_size = Size.Fraction(0.1, Size.AxesY(self.axes))

        #Define size of useful axes cells, 50% each in x 20% for buttons in y.
        small_x = Size.Fraction((1.-2.*pad)/10, Size.AxesX(self.axes))
        ysize = Size.Fraction((1.-2.*pad)/15., Size.AxesY(self.axes))

        #Set up grid, 3x3 with cells for padding.
        if self.num_buttons > 0:
            xsize = Size.Fraction((1.-2.*pad)/self.num_buttons, Size.AxesX(self.axes))
            horiz = [xsize] + [pad_size, xsize]*(self.num_buttons-1) + \
                    [Size.Fraction(0.1, Size.AxesY(self.axes)), small_x]
            vert = [ysize, pad_size] * self.num_sliders + \
                   [large_pad_size, large_pad_size, Size.AxesY(self.axes)]
        else:
            vert = [ysize, pad_size] * self.num_sliders + \
                   [large_pad_size, Size.AxesY(self.axes)]
            horiz = [Size.Fraction(0.8, Size.AxesX(self.axes))] + \
                    [Size.Fraction(0.1, Size.AxesX(self.axes))]*2

        self.divider.set_horizontal(horiz)
        self.divider.set_vertical(vert)
        self.button_ny = len(vert) - 3


        #If we are going to add a colorbar it will need an axis next to the plot
        if self.if_colorbar:
            nx1 = -3
            self.cax = self.fig.add_axes((0.,0.,0.141,1.))
            locator = self.divider.new_locator(nx=-2, ny=len(vert)-1, nx1=-1)
            self.cax.set_axes_locator(locator)
        else:
            #Main figure spans all horiz and is in the top (2) in vert.
            nx1 = -1

        self.axes.set_axes_locator(self.divider.new_locator(nx=0, ny=len(vert)-1,
                                                  nx1=nx1))

    def _add_widgets(self):
        self.buttons = []
        for i in range(0,self.num_buttons):
            x = i*2
            #The i+1/10. is a bug that if you make two axes directly ontop of
            #one another then the divider doesn't work.
            self.buttons.append(self.fig.add_axes((0.,0.,0.+i/10.,1.)))
            locator = self.divider.new_locator(nx=x, ny=self.button_ny)
            self.buttons[-1].set_axes_locator(locator)
            self.buttons[-1]._button = widgets.Button(self.buttons[-1],
                                                         self.button_labels[i])
            self.buttons[-1]._button.on_clicked(self.button_func[i])

        self.sliders = []
        self.slider_buttons = []
        for i in range(self.num_sliders):
            x = i * 2
            self.sliders.append(self.fig.add_axes((0.,0.,0.01+i/10.,1.)))
            if self.num_buttons == 0:
                nx1 = 1
            else:
                nx1 = -3
            locator = self.divider.new_locator(nx=0, ny=x, nx1=nx1)
            self.sliders[-1].set_axes_locator(locator)
            sframe = SliderPB(self.sliders[-1], "%i"%i,
                                    self.slider_ranges[i][0],
                                    self.slider_ranges[i][1]-1,
                                    valinit=self.slider_ranges[i][0],
                                    valfmt = '%i')
            sframe.on_changed(self._slider_changed, sframe)
            sframe.slider_ind = i
            sframe.cval = sframe.val
            self.sliders[-1]._slider = sframe

            self.slider_buttons.append(self.fig.add_axes((0., 0., 0.05+x/10., 1.)))
            if self.num_buttons == 0:
                nx = 2
            else:
                nx = 2 + 2*(self.num_buttons-1)
            locator = self.divider.new_locator(nx=nx, ny=x)

            self.slider_buttons[-1].set_axes_locator(locator)
            butt = ButtonPB(self.slider_buttons[-1],">")
            butt.on_clicked(self._click_slider_button, butt, sframe)
            butt.clicked = False
            self.slider_buttons[-1]._button = butt

#==============================================================================
#   Widget callbacks
#==============================================================================
    def _click_slider_button(self, event, button, slider):
        self._set_active_slider(slider.slider_ind)
        if button.clicked:
            self._stop_play(event)
            button.clicked = False
            button.label.set_text(">")
        else:
            button.clicked = True
            self._start_play(event, button, slider)
            button.label.set_text("||")
        self.fig.canvas.draw()

    def _start_play(self, event, button, slider):
        if not self.timer:
            self.timer = self.fig.canvas.new_timer()
            self.timer.interval = self.interval
            self.timer.add_callback(self._step, slider)
            self.timer.start()

    def _stop_play(self, event):
        if self.timer:
            self.timer.remove_callback(self._step)
            self.timer = None

    def _step(self, slider):
        s = slider
        if s.val >= s.valmax:
            s.set_val(s.valmin)
        else:
            s.set_val(s.val+1)
        self.fig.canvas.draw()

    def _previous(self, slider):
        s = slider
        if s.val <= s.valmin:
            s.set_val(s.valmax)
        else:
            s.set_val(s.val-1)
        self.fig.canvas.draw()

    def _slider_changed(self, val, slider):
        self.slider_functions[slider.slider_ind](val, self.im, slider)

class ImageAnimator(BaseFuncAnimator):
    """
    Create a matplotlib backend independant data explorer

    The following keyboard shortcuts are defined in the viewer:

    - 'left': previous step on active slider
    - 'right': next step on active slider
    - 'top': change the active slider up one
    - 'bottom': change the active slider down one
    - 'p': play/pause active slider

    This viewer can have user defined buttones added by specifing the labels and
    functions called when those buttons are clicked as keyword argumets.

    Parameters
    ----------
    data: ndarray
        The data to be visualised > 2D

    image_axes: list
        The two axes that make the image

    fig: mpl.figure
        Figure to use

    axis_range: list of lists
        List of [min, max] pairs for each axis, if min==max axis length
        will be used.

    interval: int
        Animation interval in ms

    colorbar: bool
        Plot colorbar

    button_labels: list
        List of strings to label buttons

    button_func: list
        List of functions to map to the buttons

    Extra keywords are passed to imshow.
    """

    def __init__(self, data, image_axes=[-2,-1], axis_range=None, **kwargs):

        self.naxis = data.ndim
        self.num_sliders = self.naxis - 2
        if len(image_axes) != 2:
            raise ValueError("There can only be two spatial axes")

        all_axes = list(range(self.naxis))
        #Handle negative indexes
        self.image_axes = [all_axes[i] for i in image_axes]

        slider_axes = list(range(self.naxis))
        [slider_axes.remove(x) for x in self.image_axes]

        if len(slider_axes) != self.num_sliders:
            raise ValueError("Specific the same number of axes as sliders!")
        self.slider_axes = slider_axes

        #Verify that combined slider_axes and image_axes make all axes
        ax = self.slider_axes + self.image_axes
        ax.sort()
        if ax != list(range(self.naxis)):
            raise ValueError("spatial_axes and sider_axes mismatch")

        self.axis_range = self._parse_axis_range(axis_range, data)

        #create data slice
        self.frame_slice = [slice(None)]*self.naxis
        for i in self.slider_axes:
            self.frame_slice[i] = 0

        base_kwargs = {'slider_functions':[self._updateimage]*self.num_sliders,
                       'slider_ranges': [self.axis_range[i] for i in self.slider_axes]}
        base_kwargs.update(kwargs)
        BaseFuncAnimator.__init__(self, data, **base_kwargs)

    def label_slider(self, i, label):
        """
        Change the Slider label

        Parameters
        ----------
        i: int
            The index of the slider to change (0 is bottom)
        label: str
            The label to set
        """
        self.sliders[i]._slider.label.set_text(label)

    def plot_start_image(self, ax):
        #Create extent arg
        extent = []
        #reverse because numpy is in y-x and extent is x-y
        for i in self.image_axes[::-1]:
            extent += self.axis_range[i]

        imshow_args = {'interpolation':'nearest',
                       'origin':'lower',
                       'extent':extent,
                       }

        imshow_args.update(self.imshow_kwargs)
        im = ax.imshow(self.data[self.frame_slice], **imshow_args)
        if self.if_colorbar:
            self._add_colorbar(im)

        return im

    def _parse_axis_range(self, axis_range, data):
        #Make axes_range argument and replace instances where min == max
        if not axis_range:
            axis_range = [[0, i] for i in data.shape]
        else:
            if len(axis_range) != data.ndim:
                raise ValueError("axis_range must equal number of axes")
            for i,d in enumerate(data.shape):
                if axis_range[i] is None or axis_range[i][0] == axis_range[i][1]:
                    axis_range[i] = [0, d]
        return axis_range

    def _updateimage(self, val, im, slider):
        val = int(val)
        ax = self.slider_axes[slider.slider_ind]
        self.frame_slice[ax] = val
        if val != slider.cval:
            im.set_array(self.data[self.frame_slice])
            slider.cval = val
########NEW FILE########
__FILENAME__ = mapcubeanimator
# -*- coding: utf-8 -*-

__all__ = ['MapCubeAnimator']

from sunpy.visualization import imageanimator

class MapCubeAnimator(imageanimator.BaseFuncAnimator):
    """
    Create an interactive viewer for a MapCube

    The following keyboard shortcuts are defined in the viewer:

    - 'left': previous step on active slider
    - 'right': next step on active slider
    - 'top': change the active slider up one
    - 'bottom': change the active slider down one
    - 'p': play/pause active slider

    Parameters
    ----------
    mapcube: sunpy.map.MapCube
        A MapCube

    annotate: bool
        Annotate the figure with scale and titles

    fig: mpl.figure
        Figure to use

    interval: int
        Animation interval in ms

    colorbar: bool
        Plot colorbar

    Notes
    -----
    Extra keywords are passed to `mapcube[0].plot()` i.e. the `plot()` routine of
    the first map in the cube.
    """
    def __init__(self, mapcube, annotate=True, **kwargs):

        self.mapcube = mapcube
        self.annotate = annotate
        slider_functions = [self.updatefig]
        slider_ranges = [[0,len(mapcube.maps)]]

        imageanimator.BaseFuncAnimator.__init__(self, mapcube.maps, slider_functions,
                                        slider_ranges, **kwargs)

        if annotate:
            self._annotate_plot(0)

    def updatefig(self, val, im, slider):
        i = int(val)
        im.set_array(self.data[i].data)
        im.set_cmap(self.mapcube[i].cmap)
        im.set_norm(self.mapcube[i].mpl_color_normalizer)
        # Having this line in means the plot will resize for non-homogenous
        # maps. However it also means that if you zoom in on the plot bad
        # things happen.
        # im.set_extent(self.mapcube[i].xrange + self.mapcube[i].yrange)
        if self.annotate:
            self._annotate_plot(i)

    def _annotate_plot(self, ind):
        """
        Annotate the image.

        This may overwrite some stuff in `GenericMap.plot()`
        """
        # Normal plot
        self.axes.set_title("%s %s" % (self.data[ind].name, self.data[ind].date))

        # x-axis label
        if self.data[ind].coordinate_system['x'] == 'HG':
            xlabel = 'Longitude [%s]' % self.data[ind].units['x']
        else:
            xlabel = 'X-position [%s]' % self.data[ind].units['x']

        # y-axis label
        if self.data[ind].coordinate_system['y'] == 'HG':
            ylabel = 'Latitude [%s]' % self.data[ind].units['y']
        else:
            ylabel = 'Y-position [%s]' % self.data[ind].units['y']

        self.axes.set_xlabel(xlabel)
        self.axes.set_ylabel(ylabel)

    def plot_start_image(self, ax):
        im = self.mapcube[0].plot(annotate=self.annotate, axes=ax,
                                       **self.imshow_kwargs)
        return im

########NEW FILE########
__FILENAME__ = plotting
# -*- coding: utf-8 -*-
"""
Some Independant plotting tools, mainly animation UI based.
"""
__author__ = "Stuart Mumford"
__email__ = "stuartmumford@physics.org"
__all__ = ['ControlFuncAnimation', 'add_controls']

import matplotlib.animation as animation
import matplotlib.pyplot as plt
import matplotlib.widgets as widgets
from mpl_toolkits.axes_grid1 import make_axes_locatable
import mpl_toolkits.axes_grid1.axes_size as Size

class ControlFuncAnimation(animation.FuncAnimation):
    """ This is a slight modification to the animation class to allow pausing
    starting and stopping."""
    def __init__(self, fig, func, frames=None, init_func=None, fargs=None,
            save_count=None, auto_start=True, **kwargs):
        self.fig = fig #This should be done.
        animation.FuncAnimation.__init__(self, fig, func, frames=frames,
                                         init_func=init_func, fargs=fargs,
                                         save_count=save_count, **kwargs)
                                         
        self._started = False #Set to false _start will start animation
        if not auto_start:
            self._fig.canvas.mpl_disconnect(self._first_draw_id)
            self._first_draw_id = None
    
    def _start(self, *args):
        if not self._started:
            if self.event_source is None:
                self.event_source = self.fig.canvas.new_timer()
                self.event_source.interval = self._interval
            animation.FuncAnimation._start(self)
            self._started = True
    
    def _stop(self, *args):
        if self.event_source:
            animation.FuncAnimation._stop(self, *args)
        self._started = False

def add_controls(axes=None, slider=False):
    """ Adds Start/Stop controls to an axes having been given a animation 
    instance. """
    
    #If No axes specified use current axes.
    if not axes:
        axes = plt.gca()
    fig = axes.get_figure()
    
    #Split up the current axes so there is space for a start and a stop button
    divider = make_axes_locatable(axes)
    pad = 0.1 # Padding between axes
    pad_size = Size.Fraction(pad, Size.AxesX(axes))

    #Define size of usefult axes cells, 50% each in x 20% for buttons in y.
    xsize = Size.Fraction((1.-2.*pad)/3., Size.AxesX(axes))
    ysize = Size.Fraction((1.-2.*pad)/15., Size.AxesY(axes))

    #Set up grid, 3x3 with cells for padding.
    divider.set_horizontal([xsize, pad_size, xsize, pad_size, xsize])
    if slider:
        divider.set_vertical([ysize, pad_size, ysize, pad_size, Size.AxesY(axes)])
        bny = 2
    else:
        divider.set_vertical([ysize, pad_size, Size.AxesY(axes)])
        bny = 0
        
    #Main figure spans all horiz and is in the top (2) in vert.
    axes.set_axes_locator(divider.new_locator(0, len(divider.get_vertical())-1,
                                              nx1=-1))
    
    #Add two axes for buttons and make them 50/50 spilt at the bottom.
    bax1 = fig.add_axes((0.,0.,1.,1.))
    locator = divider.new_locator(nx=0, ny=bny)
    bax1.set_axes_locator(locator)
    bax2 = fig.add_axes((0.,0.,0.8,1.))
    locator = divider.new_locator(nx=2, ny=bny)
    bax2.set_axes_locator(locator)
    bax3 = fig.add_axes((0.,0.,0.7,1.))
    locator = divider.new_locator(nx=4, ny=bny)
    bax3.set_axes_locator(locator)
    
    start = widgets.Button(bax1, "Start")
    stop = widgets.Button(bax2, "Stop")
    step = widgets.Button(bax3, "Step")
    #Make dummy refernce to prevent garbage collection
    bax1._button = start
    bax2._button = stop
    bax3._button = step
    
    if slider:
        bax4 = fig.add_axes((0.,0.,0.6,1.))
        locator = divider.new_locator(nx=0, ny=0, nx1=-1)
        bax4.set_axes_locator(locator)
        sframe = widgets.Slider(bax4, 'Frame', 0, 10, valinit=0, valfmt = '%i')        
        bax4._slider = sframe
    
        return axes, bax1, bax2, bax3, bax4
    return axes, bax1, bax2, bax3
########NEW FILE########
__FILENAME__ = visualization
# -*- coding: utf-8 -*-
from matplotlib import pyplot

def toggle_pylab(fn):
    """ A decorator to prevent functions from opening matplotlib windows
        unexpectedly when sunpy is run in interactive shells like ipython 
        --pylab. 

        Toggles the value of matplotlib.pyplot.isinteractive() to preserve the
        users' expections of pylab's behaviour in general. """

    if pyplot.isinteractive():
        def fn_itoggle(*args, **kwargs):
            pyplot.ioff()
            ret = fn(*args, **kwargs)
            pyplot.ion()
            return ret
        return fn_itoggle
    else:
        return fn
########NEW FILE########
__FILENAME__ = test_wcs
from __future__ import absolute_import

#pylint: disable=E1103
import numpy as np
from numpy.testing import assert_allclose

import sunpy
import sunpy.map
import sunpy.wcs as wcs
import sunpy.sun as sun

img = sunpy.map.Map(sunpy.AIA_171_IMAGE)

# the following known_answers come from equivalent queries to IDL
# WCS implementation (http://hesperia.gsfc.nasa.gov/ssw/gen/idl/wcs/)

wcs.wcs.rsun_meters = img.rsun_meters

#def test_convert_pixel_to_data():
#    actual = 
#    reference_pixel = [img.reference_pixel['x'], img.reference_pixel['y']]
#    reference_coordinate = [img.reference_coordinate['x'], img.reference_coordinate['y']]
#    scale = [img.scale['x'], img.scale['y']]
#    actual = wcs.convert_pixel_to_data(scale,img.shape, reference_pixel, reference_coordinate, 0, 0)

def test_convert_angle_units():
    actual = np.array([wcs._convert_angle_units(), wcs._convert_angle_units('arcsec'),
        wcs._convert_angle_units('arcmin'), wcs._convert_angle_units('degrees'), 
        wcs._convert_angle_units('mas')])
    desired = np.array([np.deg2rad(1) / (60 * 60), np.deg2rad(1) / (60 * 60), 
        np.deg2rad(1) / 60.0, np.deg2rad(1), np.deg2rad(1) / (60 * 60 * 1000)])
    assert_allclose(actual, desired, rtol=1e-2, atol=0)
    
def test_conv_hpc_hcc():
    coord = [40.0, 32.0]
    result = wcs.convert_hpc_hcc(coord[0], coord[1], angle_units=img.units['x'])
    known_answer = [28748691, 22998953]
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)
    
    # Test the dsun_meters parameter for a distance of 0.5 AU
    dist = 0.5 * sun.constants.au.si.value
    result = wcs.convert_hpc_hcc(coord[0], coord[1], dist, img.units['x'])
    known_answer = [14370494, 11496395]
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)
    
    # Make sure that z coordinate is returned if parameter z is True
    result = wcs.convert_hpc_hcc(coord[0], coord[1], angle_units=img.units['x'],
                                                     z=True)
    known_answer = [28748691, 22998953, 695016924]
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)
 
def test_conv_hcc_hpc():
    coord = [28748691, 22998953]
    result = wcs.convert_hcc_hpc(coord[0], coord[1], dsun_meters=img.dsun, 
        angle_units=img.units['x'])
    known_answer = [40.0, 32.0]
    assert_allclose(result, known_answer, rtol=1e-4, atol=0)
 
def test_conv_hcc_hg():
    coord = [13.0, 58.0]
    result = wcs.convert_hcc_hg(coord[0], coord[1], b0_deg=img.heliographic_latitude, l0_deg=img.heliographic_longitude)
    known_answer = [1.0791282e-06, -7.0640732]
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)

    # Make sure that r value is returned if radius=True
    result = wcs.convert_hcc_hg(coord[0], coord[1], b0_deg=img.heliographic_latitude,
                                l0_deg=img.heliographic_longitude, radius=True)
    known_answer = [1.0791282e-06, -7.0640732, sun.constants.radius.si.value]
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)
 
def test_conv_hg_hcc():
    coord = [34.0, 96.0]
    result = wcs.convert_hg_hcc(coord[0], coord[1], b0_deg=img.heliographic_latitude, 
                                l0_deg=img.heliographic_longitude)
    known_answer = [-40653538.0, 6.7903529e8]
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)

    # Test the radius parameter using half of the Sun's radius
    known_answer = [x / 2.0 for x in known_answer]
    radius = sun.constants.radius.si.value / 2.0
    result = wcs.convert_hg_hcc(coord[0], coord[1], b0_deg=img.heliographic_latitude, 
                                l0_deg=img.heliographic_longitude, r=radius)
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)

    # Make sure that z coordinates are returned if z=True
    known_answer = [-40653538.0, 6.7903529e8, -1.4487837e8]
    result = wcs.convert_hg_hcc(coord[0], coord[1], b0_deg=img.heliographic_latitude, 
                                l0_deg=img.heliographic_longitude, z=True)
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)

    # If z < 0, using occultation should make the return coordinates nan
    coord2 = [55.0, 56.0]
    known_answer = [[np.nan, 3.1858718e8], [np.nan, 5.9965928e8]]
    coords = zip(coord, coord2)
    result = wcs.convert_hg_hcc(*coords, b0_deg=img.heliographic_latitude, 
                                l0_deg=img.heliographic_longitude, occultation=True)
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)
    
def test_conv_hg_hpc():
    coord = [34.0, 45.0]
    result = wcs.convert_hg_hpc(coord[0], coord[1], dsun_meters=img.dsun,
                                b0_deg=img.heliographic_latitude,
                                l0_deg=img.heliographic_longitude, angle_units=img.units['x'])
    known_answer = [381.737592, 747.072612]
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)

    # Test to make sure occultation parameter works
    coord = [34.0, 96.0]
    coord2 = [55.0, 56.0]
    known_answer = [[np.nan, 441.65710359], [np.nan, 831.30194808]]
    coords = zip(coord, coord2)
    result = wcs.convert_hg_hpc(*coords, dsun_meters=img.dsun,
                b0_deg=img.heliographic_latitude, l0_deg=img.heliographic_longitude,
                angle_units=img.units['x'], occultation=True)
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)

def test_conv_hpc_hg():
    coord = [382, 748]
    known_answer = [34.091299, 45.095130]
    result = wcs.convert_hpc_hg(coord[0], coord[1], dsun_meters=img.dsun,
                                b0_deg=img.heliographic_latitude,
                                l0_deg=img.heliographic_longitude, angle_units=img.units['x'])
    assert_allclose(result, known_answer, rtol=1e-2, atol=0)

def test_convert_to_coord():
    x, y = (34.0, 96.0)
    b0_deg = img.heliographic_latitude
    l0_deg = img.heliographic_longitude
    units = img.units['x']
    dsun=img.dsun
    def check_conversion(from_coord, to_coord, expected):
        # Make sure that wcs.convert_to_coord returns the expected value
        assert_allclose(wcs.convert_to_coord(x, y, from_coord, to_coord,
            b0_deg=b0_deg, l0_deg=l0_deg, dsun_meters=dsun, angle_units=units),
            expected, rtol=1e-2, atol=0)
    check_conversion('hcc', 'hg', wcs.convert_hcc_hg(x, y, b0_deg=b0_deg,
                                                     l0_deg=l0_deg))
    check_conversion('hpc', 'hg', wcs.convert_hpc_hg(x, y, b0_deg=b0_deg,
                        l0_deg=l0_deg, dsun_meters=dsun, angle_units=units))
    check_conversion('hg', 'hcc', wcs.convert_hg_hcc(x, y, b0_deg=b0_deg,
                                                        l0_deg=l0_deg))
    check_conversion('hcc', 'hpc', wcs.convert_hcc_hpc(x, y, dsun_meters=dsun,
                                                       angle_units=units))
    check_conversion('hg', 'hpc', wcs.convert_hg_hpc(x, y, b0_deg=b0_deg,
                        l0_deg=l0_deg, dsun_meters=dsun, angle_units=units))
    check_conversion('hpc', 'hcc', wcs.convert_hpc_hcc(x, y, dsun_meters=dsun,
                                                       angle_units=units))

def test_convert_back():
    # Make sure transformation followed by inverse transformation returns
    # the original coordinates
    coord = [40.0, 32.0]
    assert_allclose(wcs.convert_hcc_hpc(*wcs.convert_hpc_hcc(*coord)),
                    coord, rtol=1e-2, atol=0)
    coord = [13.0, 58.0]
    assert_allclose(wcs.convert_hg_hcc(*wcs.convert_hcc_hg(*coord)),
                    coord, rtol=1e-2, atol=0)
    coord = [34.0, 45.0]
    assert_allclose(wcs.convert_hpc_hg(*wcs.convert_hg_hpc(*coord)),
                    coord, rtol=1e-2, atol=0)

########NEW FILE########
__FILENAME__ = wcs
from __future__ import absolute_import

import numpy as np
import sunpy.sun as sun

import astropy.units

rsun_meters = sun.constants.radius.si.value

__all__ = ['_convert_angle_units', 'convert_pixel_to_data', 'convert_hpc_hg',
           'convert_data_to_pixel', 'convert_hpc_hcc', 'convert_hcc_hpc',
           'convert_hcc_hg', 'convert_hg_hcc', 'proj_tan',
           'convert_hg_hpc',  'convert_to_coord', 
           'get_center']

def _convert_angle_units(unit='arcsec'):
    """Determine the conversion factor between the data units and radians."""
    if unit == 'degrees':
        return np.deg2rad(1)
    elif unit == 'arcmin':
        return np.deg2rad(1) / 60.0
    elif unit == 'arcsec':
        return np.deg2rad(1) / (60 * 60.0)
    elif unit == 'mas':
        return np.deg2rad(1) / (60 * 60 * 1000.0)
    else:
	raise ValueError("The units specified are either invalid or is not supported at this time.")

def convert_pixel_to_data(size, scale, reference_pixel, 
                          reference_coordinate, x=None, y=None):
    """Calculate the data coordinate for particular pixel indices.
    
    Parameters
    ----------
    size : 2d ndarray
        Number of pixels in width and height.
    scale : 2d ndarray
        The size of a pixel (dx,dy) in data coordinates (equivalent to WCS/CDELT)
    reference_pixel : 2d ndarray
        The reference pixel (x,y) at which the reference coordinate is given (equivalent to WCS/CRPIX)
    reference_coordinate : 2d ndarray
        The data coordinate (x, y) as measured at the reference pixel (equivalent to WCS/CRVAL)
    x,y : int or ndarray
        The pixel values at which data coordinates are requested. If none are given, 
        returns coordinates for every pixel.
        
    Returns
    -------
    out : ndarray
        The data coordinates at pixel (x,y).
    
    Notes
    -----
    This function assumes a gnomic projection which is correct for a detector at the focus
    of an optic observing the Sun.
        
    Examples
    --------
    
    """
    cdelt = np.array(scale)
    crpix = np.array(reference_pixel)
    crval = np.array(reference_coordinate)
    
    # first assume that coord is just [x,y]
    if (x is None) and (y is None):
        x, y = np.meshgrid(np.arange(size[0]), np.arange(size[1]))

    # note that crpix[] counts pixels starting at 1
    
    coordx = (x - (crpix[0] - 1)) * cdelt[0] + crval[0]
    coordy = (y - (crpix[1] - 1)) * cdelt[1] + crval[1]
    
    # Correct for Gnomic projection
    coordx, coordy = proj_tan(coordx, coordy)
    
    return coordx, coordy

def get_center(size, scale, reference_pixel, reference_coordinate):
    """Returns the center of the image in data coordinates.
      
    Parameters
    ----------
    size : 2d ndarray
        Number of pixels in width and height.
    scale : 2d ndarray
        The size of a pixel (dx,dy) in data coordinates (equivalent to WCS/CDELT)
    reference_pixel : 2d ndarray
        The reference pixel (x,y) at which the reference coordinate is given (equivalent to WCS/CRPIX)
    reference_coordinate : 2d ndarray
        The data coordinate (x, y) as measured at the reference pixel (equivalent to WCS/CRVAL)
          
    Returns
    -------
    out : ndarray
        The data coordinates
            
    Examples
    --------
    
    """
    return scale * (size - 1) / 2. + reference_coordinate - (reference_pixel - 1) * scale

def convert_data_to_pixel(x, y, scale, reference_pixel, reference_coordinate):
    """Calculate the pixel indices for a given data coordinate.

    Parameters
    ----------
    x, y : float
        Data coordinate in same units as reference coordinate
    scale : 2d ndarray
        The size of a pixel (dx,dy) in data coordinates (equivalent to WCS/CDELT)
    reference_pixel : 2d ndarray
        The reference pixel (x,y) at which the reference coordinate is given (equivalent to WCS/CRPIX)
    reference_coordinate : 2d ndarray
        The data coordinate (x, y) as measured at the reference pixel (equivalent to WCS/CRVAL)
        
    Returns
    -------
    out : ndarray
        The  pixel coordinates (x,y) at that data coordinate.
            
    Examples
    --------
    
    """
    
    # TODO: Needs to check what coordinate system the data is given in
    cdelt = np.array(scale)
    crpix = np.array(reference_pixel)
    crval = np.array(reference_coordinate)
    # De-apply any tabular projections.
    # coord = inv_proj_tan(coord)
    
    # note that crpix[] counts pixels starting at 1
    pixelx = (x - crval[0]) / cdelt[0] + (crpix[1] - 1)
    pixely = (y - crval[1]) / cdelt[1] + (crpix[1] - 1)

    return pixelx, pixely

def convert_hpc_hcc(x, y, dsun_meters=None, angle_units='arcsec', z=False):
    """Converts from Helioprojective-Cartesian (HPC) coordinates into 
    Heliocentric-Cartesian (HCC) coordinates. Returns all three dimensions, x, y, z in
    meters.
    
    Parameters
    ----------
    x, y : float
        Data coordinate in angle units (default is arcsec)
    dsun_meters : float
        Distance from the observer to the Sun in meters. Default is 1 AU.
    angle_units : str
        Units of the data coordinates (e.g. arcsec, arcmin, degrees). Default is arcsec.
    z : Bool
        If true return the z coordinate as well.

    Returns
    -------
    out : ndarray
        The data coordinates (x,y,z) in heliocentric cartesian coordinates in meters.
    
    Notes
    -----
    Implements Eq. (15) of Thompson (2006), A&A, 449, 791.
            
    Examples
    --------
    >>> sunpy.wcs.convert_hpc_hcc(40.0, 32.0, z=True)
    (28876152.176423457, 23100922.071266972, 694524220.8157959)
    
    """
    
    c = np.array([_convert_angle_units(unit=angle_units), 
                  _convert_angle_units(unit=angle_units)])

    cosx = np.cos(x * c[0])
    sinx = np.sin(x * c[0])
    cosy = np.cos(y * c[1])
    siny = np.sin(y * c[1])

    if dsun_meters is None:
        dsun_meters = sun.constants.au.si.value
    elif isinstance(dsun_meters, astropy.units.Quantity):
        dsun_meters = dsun_meters.si.value
        
    q = dsun_meters * cosy * cosx
    distance = q ** 2 - dsun_meters ** 2 + rsun_meters ** 2
    # distance[np.where(distance < 0)] = np.sqrt(-1)
    distance = q - np.sqrt(distance)

    rx = distance * cosy * sinx
    ry = distance * siny
    rz = dsun_meters - distance * cosy * cosx

    if np.all(z == True):
        return rx, ry, rz
    else:
        return rx, ry

def convert_hcc_hpc(x, y, dsun_meters=None, angle_units='arcsec'):
    """Convert Heliocentric-Cartesian (HCC) to angular 
    Helioprojective-Cartesian (HPC) coordinates (in degrees).

    Parameters
    ----------
    x, y : float (meters)
        Data coordinate in meters.
    dsun_meters : float
        Distance from the observer to the Sun in meters. Default is 1 AU.
    angle_units : str
        Units of the data coordinates (e.g. arcsec, arcmin, degrees). Default is arcsec.
    
    Returns
    -------
    out : ndarray
        The  data coordinates (x,y) in helioprojective cartesian coordinates in arcsec.
    
    Notes
    -----
    Implements Eq. (16) of Thompson (2006), A&A, 449, 791.
            
    Examples
    --------
    >>> sunpy.wcs.convert_hcc_hpc(28748691, 22998953)
    (39.823439773829705, 31.858751644835717)
    
    """
    
    # Calculate the z coordinate by assuming that it is on the surface of the Sun
    z = np.sqrt(rsun_meters ** 2 - x ** 2 - y ** 2)
    
    if dsun_meters is None:
        dsun_meters = sun.constants.au.si.value
    elif isinstance(dsun_meters, astropy.units.Quantity):
        dsun_meters = dsun_meters.si.value

    zeta = dsun_meters - z
    distance = np.sqrt(x**2 + y**2 + zeta**2)
    hpcx = np.rad2deg(np.arctan2(x, zeta))
    hpcy = np.rad2deg(np.arcsin(y / distance))
    
    if angle_units == 'arcsec':
        hpcx = 60 * 60 * hpcx
        hpcy = 60 * 60 * hpcy
    elif angle_units == 'arcmin':
        hpcx = 60 * hpcx
        hpcy = 60 * hpcy
        
    return hpcx, hpcy

def convert_hcc_hg(x, y, z=None, b0_deg=0, l0_deg=0, radius=False):
    """Convert from Heliocentric-Cartesian (HCC) (given in meters) to
    Stonyhurst Heliographic coordinates (HG) given in degrees, with
    radial output in meters.

    Parameters
    ----------
    x, y : float (meters)
        Data coordinate in meters.
    z : float (meters)
        Data coordinate in meters.  If None, then the z-coordinate is assumed
        to be on the Sun.
    b0_deg : float (degrees)
        Tilt of the solar North rotational axis toward the observer 
        (heliographic latitude of the observer). Usually given as SOLAR_B0, 
        HGLT_OBS, or CRLT_OBS. Default is 0.
    l0_deg : float (degrees)
        Carrington longitude of central meridian as seen from Earth. Default is 0.
    radius : Bool
        If true, forces the output to return a triple of (lon, lat, r). If
        false, return (lon, lat) only.

    Returns
    -------
    out : ndarray (degrees, meters)
        if radius is false, return the data coordinates (lon, lat).  If
        radius=True, return the data cordinates (lon, lat, r).  The quantities
        (lon, lat) are the heliographic coordinates in degrees.  The quantity
        'r' is the heliographic radius in meters.
    
    Notes
    -----
    Implements Eq. (12) of Thompson (2006), A&A, 449, 791.
            
    Examples
    --------
    >>> sunpy.wcs.convert_hcc_hg(230000.0,45000000.0,
    z=695508000.0 + 8000000.0, radius=True)
    (0.01873188196651189, 3.6599471896203317, 704945784.41465974)
    """
    if z is None:
        z = np.sqrt(rsun_meters**2 - x**2 - y**2) 

    cosb = np.cos(np.deg2rad(b0_deg))
    sinb = np.sin(np.deg2rad(b0_deg))

    hecr = np.sqrt(x**2 + y**2 + z**2)
    hgln = np.arctan2(x, z * cosb - y * sinb) + np.deg2rad(l0_deg)
    hglt = np.arcsin((y * cosb + z * sinb) / hecr)

    if radius:
        return np.rad2deg(hgln), np.rad2deg(hglt), hecr
    else:
        return np.rad2deg(hgln), np.rad2deg(hglt)

def convert_hg_hcc(hglon_deg, hglat_deg, b0_deg=0, l0_deg=0, occultation=False,
                   z=False, r=rsun_meters):
    """Convert from Stonyhurst Heliographic coordinates (given in degrees) to 
    Heliocentric-Cartesian coordinates (given in meters).
    
    Parameters
    ----------
    hglon_deg, hglat_deg : float (degrees)
        Heliographic longitude and latitude in degrees.
    b0_deg : float (degrees)
        Tilt of the solar North rotational axis toward the observer 
        (heliographic latitude of the observer). Usually given as SOLAR_B0, 
        HGLT_OBS, or CRLT_OBS. Default is 0.
    l0_deg : float (degrees)
        Carrington longitude of central meridian as seen from Earth. Default is 0.
    occultation : Bool
        If true set all points behind the Sun (e.g. not visible) to Nan.
    z : Bool
        If true return the z coordinate as well.
    r : float (meters)
        Heliographic radius
    
    Returns
    -------
    out : ndarray (meters)
        The data coordinates in Heliocentric-Cartesian coordinates.
    
    Notes
    -----
    Implements Eq. (11) of Thompson (2006), A&A, 449, 791, with the default
    assumption that the value 'r' in Eq. (11) is identical to the radius of the
    Sun.
                
    Examples
    --------
    >>> sunpy.wcs.convert_hg_hcc(0.01873188196651189, 3.6599471896203317,
    r=704945784.41465974, z=True)
    (230000.0, 45000000.0, 703508000.0)
    """
    lon = np.deg2rad(hglon_deg)
    lat = np.deg2rad(hglat_deg)
    
    cosb = np.cos(np.deg2rad(b0_deg))
    sinb = np.sin(np.deg2rad(b0_deg))

    lon = lon - np.deg2rad(l0_deg)

    cosx = np.cos(lon)
    sinx = np.sin(lon)
    cosy = np.cos(lat)
    siny = np.sin(lat)
    
    # Perform the conversion.
    x = r * cosy * sinx
    y = r * (siny * cosb - cosy * cosx * sinb)
    zz = r * (siny * sinb + cosy * cosx * cosb)
    
    if occultation:
        x[zz < 0] = np.nan
        y[zz < 0] = np.nan

    if np.all(z == True):
        return x, y, zz
    else:
        return x, y

def convert_hg_hpc(hglon_deg, hglat_deg, b0_deg=0, l0_deg=0, dsun_meters=None, angle_units='arcsec', 
                   occultation=False):
    """Convert from Heliographic coordinates (HG) to Helioprojective-Cartesian 
    (HPC).
    
    Parameters
    ----------
    hglon_deg, hglat_deg : float (degrees)
        Heliographic longitude and latitude in degrees.
    b0_deg : float (degrees)
        Tilt of the solar North rotational axis toward the observer 
        (heliographic latitude of the observer). Usually given as SOLAR_B0, 
        HGLT_OBS, or CRLT_OBS. Default is 0.
    l0_deg : float (degrees)
        Carrington longitude of central meridian as seen from Earth. Default is 0.
    occultation : Bool
        If true set all points behind the Sun (e.g. not visible) to Nan.
    dsun_meters : float (meters)
        Distance between the observer and the Sun.
    angle_units : str 
        
    
    Returns
    -------
    out : ndarray (arcsec)
        The data coordinates (x,y) in Helioprojective-Cartesian coordinates.
    
    Notes
    -----
    Uses equations 11 and 16 in Thompson (2006), A&A, 449, 791-803. 
            
    Examples
    --------
    >>> sunpy.wcs.convert_hg_hpc(34.0, 45.0, b0_deg=-7.064078, l0_deg=0.0)
    (380.05656560308898, 743.78281283290016)
    """
    
    tempx, tempy = convert_hg_hcc(hglon_deg, hglat_deg, b0_deg=b0_deg, l0_deg=l0_deg, occultation=occultation)
    x, y = convert_hcc_hpc(tempx, tempy, dsun_meters=dsun_meters, angle_units=angle_units)
    return x, y

def convert_hpc_hg(x, y, b0_deg=0, l0_deg=0, dsun_meters=None, angle_units='arcsec'):
    """Convert from Helioprojective-Cartesian (HPC) to Heliographic coordinates 
    (HG) in degrees.
    
    Parameters
    ----------
    x, y : float ()
        Data coordinate in angle units.
    b0 : float (degrees)
        Tilt of the solar North rotational axis toward the observer 
        (heliographic latitude of the observer). Usually given as SOLAR_B0, 
        HGLT_OBS, or CRLT_OBS. Default is 0.
    l0 : float (degrees)
        Carrington longitude of central meridian as seen from Earth. Default is 0.
    dsun_meters : float (meters)
        Distance between the observer and the Sun.
    angle_units : str
        Units used for input x and y. Default is arcsec.
    
    Returns
    -------
    out : ndarray (degrees)
        The  data coordinates (hglongitude, hglatitude) in Heliographic coordinates.
    
    Notes
    -----
    Uses equations 15 and 12 in Thompson (2006), A&A, 449, 791-803. 
            
    Examples
    --------
    >>> sunpy.wcs.convert_hg_hpc(382, 748, b0_deg=-7.064078, l0_deg=0.0)
    (34.504653439914669, 45.443143275518182)
    """
    
    tempx, tempy = convert_hpc_hcc(x, y, dsun_meters=dsun_meters, angle_units=angle_units)
    lon, lat = convert_hcc_hg(tempx, tempy, b0_deg=b0_deg, l0_deg=l0_deg)
    return lon, lat

def proj_tan(x, y, force=False):
    """Applies the gnomonic (TAN) projection to intermediate relative 
    coordinates. This function is not currently implemented!"""
    # if pixels are within 3 degrees of the Sun then skip the calculatin unless 
    # force is True. This applies to all sdo images so this function is just 
    # here as a place holder for the future
    # TODO: write proj_tan function
    return x, y
    
def convert_to_coord(x, y, from_coord, to_coord, b0_deg=0, l0_deg=0, dsun_meters=None, angle_units='arcsec'):
    """Apply a coordinate transform to coordinates. Right now can only do hpc 
    to hcc to hg"""
    
    if (from_coord == 'hcc') and (to_coord == 'hg'):
        rx, ry = convert_hcc_hg(x, y, b0_deg=b0_deg, l0_deg=l0_deg)
    elif (from_coord == 'hpc') and (to_coord == 'hg'):
        rx, ry = convert_hpc_hg(x, y, b0_deg=b0_deg, l0_deg=l0_deg, dsun_meters=dsun_meters, angle_units=angle_units)
    elif (from_coord == 'hg') and (to_coord == 'hcc'):
        rx, ry = convert_hg_hcc(x, y, b0_deg=b0_deg, l0_deg=l0_deg)
    elif (from_coord == 'hcc') and (to_coord == 'hpc'):
        rx, ry = convert_hcc_hpc(x, y, dsun_meters=dsun_meters, angle_units=angle_units)
    elif (from_coord == 'hg') and (to_coord == 'hpc'):
        rx, ry = convert_hg_hpc(x, y, b0_deg=b0_deg, l0_deg=l0_deg, dsun_meters=dsun_meters, angle_units=angle_units)
    elif (from_coord == 'hpc') and (to_coord == 'hcc'):
        rx, ry = convert_hpc_hcc(x, y, dsun_meters=dsun_meters, angle_units=angle_units)
    
    return rx, ry

########NEW FILE########
__FILENAME__ = hektemplate
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>
#
# This module was developed with funding provided by
# the ESA Summer of Code (2011).

# The template can be found in tools/hektemplate.py
# Unless you are editing the template, DO NOT EDIT THIS FILE.
# ALL CHANGES WILL BE LOST THE NEXT TIME IT IS GENERATED FROM THE TEMPLATE.

"""
Attributes that can be used to construct HEK queries. They are different to
the VSO ones in that a lot of them are wrappers that conveniently expose
the comparisions by overloading Python operators. So, e.g., you are able
to say AR & AR.NumSpots < 5 to find all active regions with less than 5 spots.
As with the VSO query, you can use the fundamental logic operators AND and OR
to construct queries of almost arbitrary complexity. Note that complex queries
result in multiple requests to the server which might make them less efficient.
"""

from __future__ import absolute_import

from datetime import datetime
from sunpy.net import attr
from sunpy.time import parse_time

class _ParamAttr(attr.Attr):
    """ A _ParamAttr is used to represent equality or inequality checks
    for certain parameters. It stores the attribute's name, the operator to
    compare with, and the value to compare to. """
    def __init__(self, name, op, value):
        attr.Attr.__init__(self)
        self.name = name
        self.op = op
        self.value = value
    
    def collides(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self.op == other.op and self.name == other.name


# XXX: Why is this here but never used.
class _BoolParamAttr(_ParamAttr):
    def __init__(self, name, value='true'):
        _ParamAttr.__init__(self, name, '=', value)
    
    def __neg__(self):
        if self.value == 'true':
            return _BoolParamAttr(self.name, 'false')
        else:
            return _BoolParamAttr(self.name)
    
    def __pos__(self):
        return _BoolParamAttr(self.name)


class _ListAttr(attr.Attr):
    """ A _ListAttr is used when the server expects a list of things with
    the name (GET parameter name) key. By adding the _ListAttr to the query,
    item is added to that list. """
    def __init__(self, key, item):
        attr.Attr.__init__(self)
        
        self.key = key
        self.item = item
    
    def collides(self, other):
        return False
    
    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return vars(self) == vars(other)
    
    def __hash__(self):
        return hash(tuple(vars(self).itervalues()))


class EventType(attr.Attr):
    def __init__(self, item):
        attr.Attr.__init__(self)
        self.item = item
    
    def collides(self, other):
        return isinstance(other, EventType)
    
    def __or__(self, other):
        if isinstance(other, EventType):
            return EventType(self.item + ',' + other.item)
        else:
            return super(EventType, self).__or__(other)


# XXX: XOR
class Time(attr.Attr):
    """ Restrict query to time range between start and end. """
    def __init__(self, start, end):
        attr.Attr.__init__(self)
        self.start = start
        self.end = end
    
    def collides(self, other):
        return isinstance(other, Time)
    
    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return vars(self) == vars(other)
    
    def __hash__(self):
        return hash(tuple(vars(self).itervalues()))
    
    @classmethod
    def dt(cls, start, end):
        return cls(datetime(*start), datetime(*end))


# pylint: disable=R0913
class SpatialRegion(attr.Attr):
    def __init__(
        self, x1=-1200, y1=-1200, x2=1200, y2=1200, sys='helioprojective'):
        attr.Attr.__init__(self)
        
        self.x1 = x1
        self.y1 = y1
        self.x2 = x2
        self.y2 = y2
        self.sys = sys
    
    def collides(self, other):
        return isinstance(other, SpatialRegion)
    
    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return vars(self) == vars(other)
    
    def __hash__(self):
        return hash(tuple(vars(self).itervalues()))


class Contains(attr.Attr):
    def __init__(self, *types):
        attr.Attr.__init__(self)
        self.types = types
    
    def collides(self, other):
        return False
    
    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return vars(self) == vars(other)
    
    def __hash__(self):
        return hash(tuple(vars(self).itervalues()))


class _ComparisonParamAttrWrapper(object):
    def __init__(self, name):
        self.name = name
    
    def __lt__(self, other):
        return _ParamAttr(self.name, '<', other)
    
    def __le__(self, other):
        return _ParamAttr(self.name, '<=', other)
    
    def __gt__(self, other):
        return _ParamAttr(self.name, '>', other)
    
    def __ge__(self, other):
        return _ParamAttr(self.name, '>=', other)
    
    def __eq__(self, other):
        return _ParamAttr(self.name, '=', other)
    
    def __ne__(self, other):
        return _ParamAttr(self.name, '!=', other)


class _StringParamAttrWrapper(_ComparisonParamAttrWrapper):
    def like(self, other):
        return _ParamAttr(self.name, 'like', other)


class _NumberParamAttrWrapper(_ComparisonParamAttrWrapper):
    pass


# The walker is what traverses the attribute tree and converts it to a format
# that is understood by the server we are querying. The HEK walker builds up
# a dictionary of GET parameters to be sent to the server.
walker = attr.AttrWalker()

@walker.add_applier(Contains)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    dct['type'] = 'contains'
    if not Contains in state:
        state[Contains] = 1
    
    nid = state[Contains]
    n = 0
    for n, type_ in enumerate(root.types):
        dct['event_type%d' % (nid + n)] = type_
    state[Contains] += n
    return dct

@walker.add_creator(
    Time, SpatialRegion, EventType, _ParamAttr, attr.AttrAnd, Contains)
# pylint: disable=E0102,C0103,W0613
def _c(wlk, root, state):
    value = {}
    wlk.apply(root, state, value)
    return [value]

@walker.add_applier(Time)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    dct['event_starttime'] = parse_time(root.start).strftime('%Y-%m-%dT%H:%M:%S')
    dct['event_endtime'] = parse_time(root.end).strftime('%Y-%m-%dT%H:%M:%S')
    return dct

@walker.add_applier(SpatialRegion)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    dct['x1'] = root.x1
    dct['y1'] = root.y1
    dct['x2'] = root.x2
    dct['y2'] = root.y2
    dct['event_coordsys'] = root.sys
    return dct

@walker.add_applier(EventType)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    if dct.get('type', None) == 'contains':
        raise ValueError
    dct['event_type'] = root.item
    return dct

@walker.add_applier(_ParamAttr)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    if not _ParamAttr in state:
        state[_ParamAttr] = 0
    
    nid = state[_ParamAttr]
    dct['param%d' % nid] = root.name
    dct['op%d' % nid] = root.op
    dct['value%d' % nid] = root.value
    state[_ParamAttr] += 1
    return dct

@walker.add_applier(attr.AttrAnd)
# pylint: disable=E0102,C0103,W0613
def _a(wlk, root, state, dct):
    for attribute in root.attrs:
        wlk.apply(attribute, state, dct)

@walker.add_creator(attr.AttrOr)
# pylint: disable=E0102,C0103,W0613
def _c(wlk, root, state):
    blocks = []
    for attribute in root.attrs:
        blocks.extend(wlk.create(attribute, state))
    return blocks


########NEW FILE########
__FILENAME__ = hek_mkcls
# -*- coding: utf-8 -*-
# Author: Florian Mayer <florian.mayer@bitsrc.org>
#
# This module was developed with funding provided by
# the ESA Summer of Code (2011).

# The template can be found in tools/hektemplate.py

"""
This script is used to generate sunpy.net.hek.attrs. The rationale for using
code-generation in lieu of dynamic magic is that code-generation ensures
that tools (e.g. IPython) are able to automatically complete names of members.
Considering the sheer amount of members it is essential that users are able
to use completion.

Events are EventType objects. When they are directly ORed together, they are
joined together so that only one query is sent to the query. They may not
be ANDed together because an event cannot be of multiple types.	

Events also have attributes which are _StringParamAttrWrapper, that means that
they overload the Python operators for strings and return a _ParamAttr for
them. _ParamAttrs are used to specify the values of parameters of the event.
So, AR.NumSpots == 1 returns a _ParamAttr that when encountered in a query
sets the GET parameters in a way that only active regions with only one spot
are returned. _StringParamAttrWrapper support <, <= ,>, >=, ==, != and like.
_ComparisonParamAttrWrapper support all the operations mentioned above
barring like.
"""

# XXX: Maybe split into three modules and import them all into one so
# we do not need a template but generate one module in its entirety.

from __future__ import absolute_import

import shutil
import sys
import os

from collections import defaultdict

EVENTS = [
    'AR', 'CME', 'CD', 'CH', 'CW', 'FI', 'FE', 'FA', 'FL', 'LP', 'OS', 'SS',
    'EF', 'CJ', 'PG', 'OT', 'NR', 'SG', 'SP', 'CR', 'CC', 'ER', 'TO'
]

# For some reason, the event type is "ce" but all its attributes start with
# "CME". This dict is here to consider this.
NAMES = defaultdict(lambda: None, {
    'CME': 'CE'
})
# These are just groups for attributes that are not _ListAttrs themselves.
OTHER = ['Area', 'BoundBox', 'Bound', 'OBS', 'Skel', 'FRM', 'Event', 'Outflow']
# There is no underscore after Wave in the names of the API, so we do not 
# need to remove it.
OTHER_NOPAD = ['Wave', 'Veloc', 'Freq', 'Intens']
# Every attribute that neither starts with something in EVENTS, OTHER or
# OTHER_NOPAD, is put into the Misc class.

# XXX: Not all of them actually are string. We just use string for now because
# that is the type that has the most functionality.
fields = {
    'AR_CompactnessCls': '_StringParamAttrWrapper',
    'AR_IntensKurt': '_StringParamAttrWrapper',
    'AR_IntensMax': '_StringParamAttrWrapper',
    'AR_IntensMean': '_StringParamAttrWrapper',
    'AR_IntensMin': '_StringParamAttrWrapper',
    'AR_IntensSkew': '_StringParamAttrWrapper',
    'AR_IntensTotal': '_StringParamAttrWrapper',
    'AR_IntensUnit': '_StringParamAttrWrapper',
    'AR_IntensVar': '_StringParamAttrWrapper',
    'AR_McIntoshCls': '_StringParamAttrWrapper',
    'AR_MtWilsonCls': '_StringParamAttrWrapper',
    'AR_NOAANum': '_StringParamAttrWrapper',
    'AR_NOAAclass': '_StringParamAttrWrapper',
    'AR_NumSpots': '_StringParamAttrWrapper',
    'AR_PenumbraCls': '_StringParamAttrWrapper',
    'AR_Polarity': '_StringParamAttrWrapper',
    'AR_SpotAreaRaw': '_StringParamAttrWrapper',
    'AR_SpotAreaRawUncert': '_StringParamAttrWrapper',
    'AR_SpotAreaRawUnit': '_StringParamAttrWrapper',
    'AR_SpotAreaRepr': '_StringParamAttrWrapper',
    'AR_SpotAreaReprUncert': '_StringParamAttrWrapper',
    'AR_SpotAreaReprUnit': '_StringParamAttrWrapper',
    'AR_ZurichCls': '_StringParamAttrWrapper',
    'Area_AtDiskCenter': '_StringParamAttrWrapper',
    'Area_AtDiskCenterUncert': '_StringParamAttrWrapper',
    'Area_Raw': '_StringParamAttrWrapper',
    'Area_Uncert': '_StringParamAttrWrapper',
    'Area_Unit': '_StringParamAttrWrapper',
    'BoundBox_C1LL': '_StringParamAttrWrapper',
    'BoundBox_C1UR': '_StringParamAttrWrapper',
    'BoundBox_C2LL': '_StringParamAttrWrapper',
    'BoundBox_C2UR': '_StringParamAttrWrapper',
    'Bound_CCNsteps': '_StringParamAttrWrapper',
    'Bound_CCStartC1': '_StringParamAttrWrapper',
    'Bound_CCStartC2': '_StringParamAttrWrapper',
    'CC_AxisUnit': '_StringParamAttrWrapper',
    'CC_MajorAxis': '_StringParamAttrWrapper',
    'CC_MinorAxis': '_StringParamAttrWrapper',
    'CC_TiltAngleMajorFromRadial': '_StringParamAttrWrapper',
    'CC_TiltAngleUnit': '_StringParamAttrWrapper',
    'CD_Area': '_StringParamAttrWrapper',
    'CD_AreaUncert': '_StringParamAttrWrapper',
    'CD_AreaUnit': '_StringParamAttrWrapper',
    'CD_Mass': '_StringParamAttrWrapper',
    'CD_MassUncert': '_StringParamAttrWrapper',
    'CD_MassUnit': '_StringParamAttrWrapper',
    'CD_Volume': '_StringParamAttrWrapper',
    'CD_VolumeUncert': '_StringParamAttrWrapper',
    'CD_VolumeUnit': '_StringParamAttrWrapper',
    'CME_Accel': '_StringParamAttrWrapper',
    'CME_AccelUncert': '_StringParamAttrWrapper',
    'CME_AccelUnit': '_StringParamAttrWrapper',
    'CME_AngularWidth': '_StringParamAttrWrapper',
    'CME_AngularWidthUnit': '_StringParamAttrWrapper',
    'CME_Mass': '_StringParamAttrWrapper',
    'CME_MassUncert': '_StringParamAttrWrapper',
    'CME_MassUnit': '_StringParamAttrWrapper',
    'CME_RadialLinVel': '_StringParamAttrWrapper',
    'CME_RadialLinVelMax': '_StringParamAttrWrapper',
    'CME_RadialLinVelMin': '_StringParamAttrWrapper',
    'CME_RadialLinVelStddev': '_StringParamAttrWrapper',
    'CME_RadialLinVelUncert': '_StringParamAttrWrapper',
    'CME_RadialLinVelUnit': '_StringParamAttrWrapper',
    'EF_AspectRatio': '_StringParamAttrWrapper',
    'EF_AxisLength': '_StringParamAttrWrapper',
    'EF_AxisOrientation': '_StringParamAttrWrapper',
    'EF_AxisOrientationUnit': '_StringParamAttrWrapper',
    'EF_FluxUnit': '_StringParamAttrWrapper',
    'EF_LengthUnit': '_StringParamAttrWrapper',
    'EF_NegEquivRadius': '_StringParamAttrWrapper',
    'EF_NegPeakFluxOnsetRate': '_StringParamAttrWrapper',
    'EF_OnsetRateUnit': '_StringParamAttrWrapper',
    'EF_PosEquivRadius': '_StringParamAttrWrapper',
    'EF_PosPeakFluxOnsetRate': '_StringParamAttrWrapper',
    'EF_ProximityRatio': '_StringParamAttrWrapper',
    'EF_SumNegSignedFlux': '_StringParamAttrWrapper',
    'EF_SumPosSignedFlux': '_StringParamAttrWrapper',
    'Event_C1Error': '_StringParamAttrWrapper',
    'Event_C2Error': '_StringParamAttrWrapper',
    'Event_ClippedSpatial': '_StringParamAttrWrapper',
    'Event_ClippedTemporal': '_StringParamAttrWrapper',
    'Event_Coord1': '_StringParamAttrWrapper',
    'Event_Coord2': '_StringParamAttrWrapper',
    'Event_Coord3': '_StringParamAttrWrapper',
    'Event_CoordSys': '_StringParamAttrWrapper',
    'Event_CoordUnit': '_StringParamAttrWrapper',
    'Event_MapURL': '_StringParamAttrWrapper',
    'Event_MaskURL': '_StringParamAttrWrapper',
    'Event_Npixels': '_StringParamAttrWrapper',
    'Event_PixelUnit': '_StringParamAttrWrapper',
    'Event_Probability': '_StringParamAttrWrapper',
    'Event_TestFlag': '_StringParamAttrWrapper',
    'Event_Type': '_StringParamAttrWrapper',
    'FI_BarbsL': '_StringParamAttrWrapper',
    'FI_BarbsR': '_StringParamAttrWrapper',
    'FI_BarbsTot': '_StringParamAttrWrapper',
    'FI_Chirality': '_StringParamAttrWrapper',
    'FI_Length': '_StringParamAttrWrapper',
    'FI_LengthUnit': '_StringParamAttrWrapper',
    'FI_Tilt': '_StringParamAttrWrapper',
    'FL_EFoldTime': '_StringParamAttrWrapper',
    'FL_EFoldTimeUnit': '_StringParamAttrWrapper',
    'FL_Fluence': '_StringParamAttrWrapper',
    'FL_FluenceUnit': '_StringParamAttrWrapper',
    'FL_GOESCls': '_StringParamAttrWrapper',
    'FL_PeakEM': '_StringParamAttrWrapper',
    'FL_PeakEMUnit': '_StringParamAttrWrapper',
    'FL_PeakFlux': '_StringParamAttrWrapper',
    'FL_PeakFluxUnit': '_StringParamAttrWrapper',
    'FL_PeakTemp': '_StringParamAttrWrapper',
    'FL_PeakTempUnit': '_StringParamAttrWrapper',
    'FRM_Contact': '_StringParamAttrWrapper',
    'FRM_HumanFlag': '_StringParamAttrWrapper',
    'FRM_Identifier': '_StringParamAttrWrapper',
    'FRM_Institute': '_StringParamAttrWrapper',
    'FRM_Name': '_StringParamAttrWrapper',
    'FRM_ParamSet': '_StringParamAttrWrapper',
    'FRM_SpecificID': '_StringParamAttrWrapper',
    'FRM_URL': '_StringParamAttrWrapper',
    'FRM_VersionNumber': '_StringParamAttrWrapper',
    'FreqMaxRange': '_StringParamAttrWrapper',
    'FreqMinRange': '_StringParamAttrWrapper',
    'FreqPeakPower': '_StringParamAttrWrapper',
    'FreqUnit': '_StringParamAttrWrapper',
    'IntensMaxAmpl': '_StringParamAttrWrapper',
    'IntensMinAmpl': '_StringParamAttrWrapper',
    'IntensUnit': '_StringParamAttrWrapper',
    'KB_Archivist': '_StringParamAttrWrapper',
    'MaxMagFieldStrength': '_StringParamAttrWrapper',
    'MaxMagFieldStrengthUnit': '_StringParamAttrWrapper',
    'OBS_ChannelID': '_StringParamAttrWrapper',
    'OBS_DataPrepURL': '_StringParamAttrWrapper',
    'OBS_FirstProcessingDate': '_StringParamAttrWrapper',
    'OBS_IncludesNRT': '_StringParamAttrWrapper',
    'OBS_Instrument': '_StringParamAttrWrapper',
    'OBS_LastProcessingDate': '_StringParamAttrWrapper',
    'OBS_LevelNum': '_StringParamAttrWrapper',
    'OBS_MeanWavel': '_StringParamAttrWrapper',
    'OBS_Observatory': '_StringParamAttrWrapper',
    'OBS_Title': '_StringParamAttrWrapper',
    'OBS_WavelUnit': '_StringParamAttrWrapper',
    'OscillNPeriods': '_StringParamAttrWrapper',
    'OscillNPeriodsUncert': '_StringParamAttrWrapper',
    'Outflow_Length': '_StringParamAttrWrapper',
    'Outflow_LengthUnit': '_StringParamAttrWrapper',
    'Outflow_OpeningAngle': '_StringParamAttrWrapper',
    'Outflow_Speed': '_StringParamAttrWrapper',
    'Outflow_SpeedUnit': '_StringParamAttrWrapper',
    'Outflow_TransSpeed': '_StringParamAttrWrapper',
    'Outflow_Width': '_StringParamAttrWrapper',
    'Outflow_WidthUnit': '_StringParamAttrWrapper',
    'PeakPower': '_StringParamAttrWrapper',
    'PeakPowerUnit': '_StringParamAttrWrapper',
    'RasterScanType': '_StringParamAttrWrapper',
    'SG_AspectRatio': '_StringParamAttrWrapper',
    'SG_Chirality': '_StringParamAttrWrapper',
    'SG_MeanContrast': '_StringParamAttrWrapper',
    'SG_Orientation': '_StringParamAttrWrapper',
    'SG_PeakContrast': '_StringParamAttrWrapper',
    'SG_Shape': '_StringParamAttrWrapper',
    'SS_SpinRate': '_StringParamAttrWrapper',
    'SS_SpinRateUnit': '_StringParamAttrWrapper',
    'Skel_Curvature': '_StringParamAttrWrapper',
    'Skel_Nsteps': '_StringParamAttrWrapper',
    'Skel_StartC1': '_StringParamAttrWrapper',
    'Skel_StartC2': '_StringParamAttrWrapper',
    'TO_Shape': '_StringParamAttrWrapper',
    'VelocMaxAmpl': '_StringParamAttrWrapper',
    'VelocMaxPower': '_StringParamAttrWrapper',
    'VelocMaxPowerUncert': '_StringParamAttrWrapper',
    'VelocMinAmpl': '_StringParamAttrWrapper',
    'VelocUnit': '_StringParamAttrWrapper',
    'WaveDisplMaxAmpl': '_StringParamAttrWrapper',
    'WaveDisplMinAmpl': '_StringParamAttrWrapper',
    'WaveDisplUnit': '_StringParamAttrWrapper',
    'WavelMaxPower': '_StringParamAttrWrapper',
    'WavelMaxPowerUncert': '_StringParamAttrWrapper',
    'WavelMaxRange': '_StringParamAttrWrapper',
    'WavelMinRange': '_StringParamAttrWrapper',
    'WavelUnit': '_StringParamAttrWrapper'
}

def mk_gen(rest):
    """ Generate Misc class. """
    ret = ''
    ret += '@apply\nclass Misc(object):\n'
    for elem in sorted(rest):
        ret += '    %s = %s(%r)\n' %(elem, fields[elem], elem)
    return ret

def mk_cls(key, used, pad=1, nokeys=True, init=True, name=None, base='EventType'):
    if name is None:
        name = key
    
    keys = sorted(
        [(k, v) for k, v in fields.iteritems() if k.startswith(key)]
    )
    used.update(set([k for k, v in keys]))
    if not keys:
        if not nokeys:
            raise ValueError
        return '%s = EventType(%r)' % (key, name.lower())
    ret = ''
    ret += '@apply\nclass %s(%s):\n' % (name, base)
    for k, v in keys:
        ret += '    %s = %s(%r)\n' % (k[len(key) + pad:], v, k)
    if init:
        ret += '''    def __init__(self):
        EventType.__init__(self, %r)''' % name.lower()
    return ret

if __name__ == '__main__':
    BUFFER = 4096
    used = set()
    tmpl = (
        os.path.join(os.path.dirname(__file__), 'hektemplate.py')
        if len(sys.argv) <= 2 else sys.argv[2]
    )
    dest = (
        os.path.join(
            os.path.dirname(__file__), os.pardir, 'sunpy', 'net', 'hek',
            'attrs.py')
        if len(sys.argv) <= 1 else sys.argv[1]
    )
    
    if dest == '-':
        fd = sys.stdout
    else:
        fd = open(dest, 'w')
    
    tmplfd = open(tmpl)
    
    while True:
        buf = tmplfd.read(BUFFER)
        if not buf:
            break
        fd.write(buf)
    
    fd.write('\n\n')
    fd.write('\n\n'.join(mk_cls(evt, used, name=NAMES[evt]) for evt in EVENTS))
    fd.write('\n\n')
    fd.write('\n\n'.join(mk_cls(evt, used, 0, 0, 0, NAMES[evt], 'object') for evt in OTHER_NOPAD))
    fd.write('\n\n')
    fd.write('\n\n'.join(mk_cls(evt, used, 1, 0, 0, NAMES[evt], 'object') for evt in OTHER))
    fd.write('\n\n')
    fd.write(mk_gen(set(fields) - used))
    


########NEW FILE########
